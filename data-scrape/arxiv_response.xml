<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Amisinformation%26id_list%3D%26start%3D0%26max_results%3D3000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:misinformation&amp;id_list=&amp;start=0&amp;max_results=3000</title>
  <id>http://arxiv.org/api/0Fli2DiSkCrto8sHkhseJxHMEko</id>
  <updated>2025-07-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">2046</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">3000</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2306.08871v1</id>
    <updated>2023-06-15T05:59:11Z</updated>
    <published>2023-06-15T05:59:11Z</published>
    <title>Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated
  Misinformation in the Medical Domain</title>
    <summary>  The pervasive influence of misinformation has far-reaching and detrimental
effects on both individuals and society. The COVID-19 pandemic has witnessed an
alarming surge in the dissemination of medical misinformation. However,
existing datasets pertaining to misinformation predominantly focus on textual
information, neglecting the inclusion of visual elements, and tend to center
solely on COVID-19-related misinformation, overlooking misinformation
surrounding other diseases. Furthermore, the potential of Large Language Models
(LLMs), such as the ChatGPT developed in late 2022, in generating
misinformation has been overlooked in previous works. To overcome these
limitations, we present Med-MMHL, a novel multi-modal misinformation detection
dataset in a general medical domain encompassing multiple diseases. Med-MMHL
not only incorporates human-generated misinformation but also includes
misinformation generated by LLMs like ChatGPT. Our dataset aims to facilitate
comprehensive research and development of methodologies for detecting
misinformation across diverse diseases and various scenarios, including human
and LLM-generated misinformation detection at the sentence, document, and
multi-modal levels. To access our dataset and code, visit our GitHub
repository: \url{https://github.com/styxsys0927/Med-MMHL}.
</summary>
    <author>
      <name>Yanshen Sun</name>
    </author>
    <author>
      <name>Jianfeng He</name>
    </author>
    <author>
      <name>Shuo Lei</name>
    </author>
    <author>
      <name>Limeng Cui</name>
    </author>
    <author>
      <name>Chang-Tien Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2306.08871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.08871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.13480v2</id>
    <updated>2024-02-17T13:31:59Z</updated>
    <published>2024-01-24T14:28:55Z</published>
    <title>The Dynamics of (Not) Unfollowing Misinformation Spreaders</title>
    <summary>  Many studies explore how people 'come into' misinformation exposure. But much
less is known about how people 'come out of' misinformation exposure. Do people
organically sever ties to misinformation spreaders? And what predicts doing so?
Over six months, we tracked the frequency and predictors of ~900K followers
unfollowing ~5K health misinformation spreaders on Twitter. We found that
misinformation ties are persistent. Monthly unfollowing rates are just 0.52%.
In other words, 99.5% of misinformation ties persist each month. Users are also
31% more likely to unfollow non-misinformation spreaders than they are to
unfollow misinformation spreaders. Although generally infrequent, the factors
most associated with unfollowing misinformation spreaders are (1) redundancy
and (2) ideology. First, users initially following many spreaders, or who
follow spreaders that tweet often, are most likely to unfollow later. Second,
liberals are more likely to unfollow than conservatives. Overall, we observe a
strong persistence of misinformation ties. The fact that users rarely unfollow
misinformation spreaders suggests a need for external nudges and the importance
of preventing exposure from arising in the first place.
</summary>
    <author>
      <name>Joshua Ashkinaze</name>
    </author>
    <author>
      <name>Eric Gilbert</name>
    </author>
    <author>
      <name>Ceren Budak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3589334.3645445</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3589334.3645445" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WWW 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.13480v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.13480v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.09445v1</id>
    <updated>2022-02-18T22:01:49Z</updated>
    <published>2022-02-18T22:01:49Z</published>
    <title>Identifying the Adoption or Rejection of Misinformation Targeting
  COVID-19 Vaccines in Twitter Discourse</title>
    <summary>  Although billions of COVID-19 vaccines have been administered, too many
people remain hesitant. Misinformation about the COVID-19 vaccines, propagating
on social media, is believed to drive hesitancy towards vaccination. However,
exposure to misinformation does not necessarily indicate misinformation
adoption. In this paper we describe a novel framework for identifying the
stance towards misinformation, relying on attitude consistency and its
properties. The interactions between attitude consistency, adoption or
rejection of misinformation and the content of microblogs are exploited in a
novel neural architecture, where the stance towards misinformation is organized
in a knowledge graph. This new neural framework is enabling the identification
of stance towards misinformation about COVID-19 vaccines with state-of-the-art
results. The experiments are performed on a new dataset of misinformation
towards COVID-19 vaccines, called CoVaxLies, collected from recent Twitter
discourse. Because CoVaxLies provides a taxonomy of the misinformation about
COVID-19 vaccines, we are able to show which type of misinformation is mostly
adopted and which is mostly rejected.
</summary>
    <author>
      <name>Maxwell Weinzierl</name>
    </author>
    <author>
      <name>Sanda Harabagiu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3485447.3512039</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3485447.3512039" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in TheWebConf 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.09445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.09445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.05656v1</id>
    <updated>2023-11-09T00:05:27Z</updated>
    <published>2023-11-09T00:05:27Z</published>
    <title>Combating Misinformation in the Age of LLMs: Opportunities and
  Challenges</title>
    <summary>  Misinformation such as fake news and rumors is a serious threat on
information ecosystems and public trust. The emergence of Large Language Models
(LLMs) has great potential to reshape the landscape of combating
misinformation. Generally, LLMs can be a double-edged sword in the fight. On
the one hand, LLMs bring promising opportunities for combating misinformation
due to their profound world knowledge and strong reasoning abilities. Thus, one
emergent question is: how to utilize LLMs to combat misinformation? On the
other hand, the critical challenge is that LLMs can be easily leveraged to
generate deceptive misinformation at scale. Then, another important question
is: how to combat LLM-generated misinformation? In this paper, we first
systematically review the history of combating misinformation before the advent
of LLMs. Then we illustrate the current efforts and present an outlook for
these two fundamental questions respectively. The goal of this survey paper is
to facilitate the progress of utilizing LLMs for fighting misinformation and
call for interdisciplinary efforts from different stakeholders for combating
LLM-generated misinformation.
</summary>
    <author>
      <name>Canyu Chen</name>
    </author>
    <author>
      <name>Kai Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages for the main paper, 35 pages including 656 references, more
  resources on "LLMs Meet Misinformation" are on the website:
  https://llm-misinformation.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.05656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.05656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.12466v1</id>
    <updated>2023-06-21T17:30:02Z</updated>
    <published>2023-06-21T17:30:02Z</published>
    <title>Misinformation as Information Pollution</title>
    <summary>  Social media feed algorithms are designed to optimize online social
engagements for the purpose of maximizing advertising profits, and therefore
have an incentive to promote controversial posts including misinformation. By
thinking about misinformation as information pollution, we can draw parallels
with environmental policy for countering pollution such as carbon taxes.
Similar to pollution, a Pigouvian tax on misinformation provides economic
incentives for social media companies to control the spread of misinformation
more effectively to avoid or reduce their misinformation tax, while preserving
some degree of freedom in platforms' response. In this paper, we highlight a
bird's eye view of a Pigouvian misinformation tax and discuss the key questions
and next steps for implementing such a taxing scheme.
</summary>
    <author>
      <name>Ashkan Kazemi</name>
    </author>
    <author>
      <name>Rada Mihalcea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.12466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.12466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.02095v2</id>
    <updated>2024-11-04T05:36:32Z</updated>
    <published>2023-10-03T14:39:13Z</published>
    <title>A Survey on the Role of Crowds in Combating Online Misinformation:
  Annotators, Evaluators, and Creators</title>
    <summary>  Online misinformation poses a global risk with significant real-world
consequences. To combat misinformation, current research relies on
professionals like journalists and fact-checkers for annotating and debunking
misinformation, and develops automated machine learning methods for detecting
misinformation. Complementary to these approaches, recent research has
increasingly concentrated on utilizing the power of ordinary social media
users, a.k.a. "crowd", who act as eyes-on-the-ground proactively questioning
and countering misinformation. Notably, recent studies show that 96% of
counter-misinformation responses originate from them. Acknowledging their
prominent role, we present the first systematic and comprehensive survey of
research papers that actively leverage the crowds to combat misinformation.
  We first identify 88 papers related to crowd-based efforts, following a
meticulous annotation process adhering to the PRISMA framework. We then present
key statistics related to misinformation, counter-misinformation, and crowd
input in different formats and topics. Upon holistic analysis of the papers, we
introduce a novel taxonomy of the roles played by the crowds: (i)annotators who
actively identify misinformation; (ii)evaluators who assess
counter-misinformation effectiveness; (iii)creators who create
counter-misinformation. This taxonomy explores the crowd's capabilities in
misinformation detection, identifies prerequisites for effective
counter-misinformation, and analyzes crowd-generated counter-misinformation.
Then, we delve into (i)distinguishing individual, collaborative, and
machine-assisted labeling for annotators; (ii)analyzing the effectiveness of
counter-misinformation through surveys, interviews, and in-lab experiments for
evaluators; and (iii)characterizing creation patterns and creator profiles for
creators. Finally, we outline potential future research in this field.
</summary>
    <author>
      <name>Bing He</name>
    </author>
    <author>
      <name>Yibo Hu</name>
    </author>
    <author>
      <name>Yeon-Chang Lee</name>
    </author>
    <author>
      <name>Soyoung Oh</name>
    </author>
    <author>
      <name>Gaurav Verma</name>
    </author>
    <author>
      <name>Srijan Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Survey is accepted by ACM Transactions on Knowledge Discovery from
  Data (ACM TKDD) Journal. GitHub repository with the curated list of papers:
  https://github.com/claws-lab/awesome-crowd-combat-misinformation</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.02095v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.02095v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.00124v2</id>
    <updated>2021-04-03T17:34:31Z</updated>
    <published>2021-03-31T21:12:29Z</published>
    <title>Misinformation detection in Luganda-English code-mixed social media text</title>
    <summary>  The increasing occurrence, forms, and negative effects of misinformation on
social media platforms has necessitated more misinformation detection tools.
Currently, work is being done addressing COVID-19 misinformation however, there
are no misinformation detection tools for any of the 40 distinct indigenous
Ugandan languages. This paper addresses this gap by presenting basic language
resources and a misinformation detection data set based on code-mixed
Luganda-English messages sourced from the Facebook and Twitter social media
platforms. Several machine learning methods are applied on the misinformation
detection data set to develop classification models for detecting whether a
code-mixed Luganda-English message contains misinformation or not. A 10-fold
cross validation evaluation of the classification methods in an experimental
misinformation detection task shows that a Discriminative Multinomial Naive
Bayes (DMNB) method achieves the highest accuracy and F-measure of 78.19% and
77.90% respectively. Also, Support Vector Machine and Bagging ensemble
classification models achieve comparable results. These results are promising
since the machine learning models are based on n-gram features from only the
misinformation detection dataset.
</summary>
    <author>
      <name>Peter Nabende</name>
    </author>
    <author>
      <name>David Kabiito</name>
    </author>
    <author>
      <name>Claire Babirye</name>
    </author>
    <author>
      <name>Hewitt Tusiime</name>
    </author>
    <author>
      <name>Joyce Nakatumba-Nabende</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at African NLP workshop @EACL 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.00124v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00124v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.00976v1</id>
    <updated>2021-02-01T16:59:31Z</updated>
    <published>2021-02-01T16:59:31Z</published>
    <title>Can Predominant Credible Information Suppress Misinformation in Crises?
  Empirical Studies of Tweets Related to Prevention Measures during COVID-19</title>
    <summary>  During COVID-19, misinformation on social media affects the adoption of
appropriate prevention behaviors. It is urgent to suppress the misinformation
to prevent negative public health consequences. Although an array of studies
has proposed misinformation suppression strategies, few have investigated the
role of predominant credible information during crises. None has examined its
effect quantitatively using longitudinal social media data. Therefore, this
research investigates the temporal correlations between credible information
and misinformation, and whether predominant credible information can suppress
misinformation for two prevention measures (i.e. topics), i.e. wearing masks
and social distancing using tweets collected from February 15 to June 30, 2020.
We trained Support Vector Machine classifiers to retrieve relevant tweets and
classify tweets containing credible information and misinformation for each
topic. Based on cross-correlation analyses of credible and misinformation time
series for both topics, we find that the previously predominant credible
information can lead to the decrease of misinformation (i.e. suppression) with
a time lag. The research findings provide empirical evidence for suppressing
misinformation with credible information in complex online environments and
suggest practical strategies for future information management during crises
and emergencies.
</summary>
    <author>
      <name>Yan Wang</name>
    </author>
    <author>
      <name>Shangde Gao</name>
    </author>
    <author>
      <name>Wenyu Gao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/1468-5973.12385</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/1468-5973.12385" rel="related"/>
    <link href="http://arxiv.org/abs/2102.00976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.00976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11702v4</id>
    <updated>2021-07-08T08:39:43Z</updated>
    <published>2021-06-22T12:17:53Z</published>
    <title>Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study
  of COVID-19 Infodemic</title>
    <summary>  The spreading COVID-19 misinformation over social media already draws the
attention of many researchers. According to Google Scholar, about 26000
COVID-19 related misinformation studies have been published to date. Most of
these studies focusing on 1) detect and/or 2) analysing the characteristics of
COVID-19 related misinformation. However, the study of the social behaviours
related to misinformation is often neglected. In this paper, we introduce a
fine-grained annotated misinformation tweets dataset including social
behaviours annotation (e.g. comment or question to the misinformation). The
dataset not only allows social behaviours analysis but also suitable for both
evidence-based or non-evidence-based misinformation classification task. In
addition, we introduce leave claim out validation in our experiments and
demonstrate the misinformation classification performance could be
significantly different when applying to real-world unseen misinformation.
</summary>
    <author>
      <name>Ye Jiang</name>
    </author>
    <author>
      <name>Xingyi Song</name>
    </author>
    <author>
      <name>Carolina Scarton</name>
    </author>
    <author>
      <name>Ahmet Aker</name>
    </author>
    <author>
      <name>Kalina Bontcheva</name>
    </author>
    <link href="http://arxiv.org/abs/2106.11702v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11702v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.09449v1</id>
    <updated>2022-02-18T22:09:38Z</updated>
    <published>2022-02-18T22:09:38Z</published>
    <title>VaccineLies: A Natural Language Resource for Learning to Recognize
  Misinformation about the COVID-19 and HPV Vaccines</title>
    <summary>  Billions of COVID-19 vaccines have been administered, but many remain
hesitant. Misinformation about the COVID-19 vaccines and other vaccines,
propagating on social media, is believed to drive hesitancy towards
vaccination. The ability to automatically recognize misinformation targeting
vaccines on Twitter depends on the availability of data resources. In this
paper we present VaccineLies, a large collection of tweets propagating
misinformation about two vaccines: the COVID-19 vaccines and the Human
Papillomavirus (HPV) vaccines. Misinformation targets are organized in
vaccine-specific taxonomies, which reveal the misinformation themes and
concerns. The ontological commitments of the Misinformation taxonomies provide
an understanding of which misinformation themes and concerns dominate the
discourse about the two vaccines covered in VaccineLies. The organization into
training, testing and development sets of VaccineLies invites the development
of novel supervised methods for detecting misinformation on Twitter and
identifying the stance towards it. Furthermore, VaccineLies can be a stepping
stone for the development of datasets focusing on misinformation targeting
additional vaccines.
</summary>
    <author>
      <name>Maxwell Weinzierl</name>
    </author>
    <author>
      <name>Sanda Harabagiu</name>
    </author>
    <link href="http://arxiv.org/abs/2202.09449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.09449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.12589v1</id>
    <updated>2022-07-26T00:40:26Z</updated>
    <published>2022-07-26T00:40:26Z</published>
    <title>Folk Models of Misinformation on Social Media</title>
    <summary>  In this paper we investigate what folk models of misinformation exist through
semi-structured interviews with a sample of 235 social media users. Work on
social media misinformation does not investigate how ordinary users - the
target of misinformation - deal with it; rather, the focus is mostly on the
anxiety, tensions, or divisions misinformation creates. Studying the aspects of
creation, diffusion and amplification also overlooks how misinformation is
internalized by users on social media and thus is quick to prescribe
"inoculation" strategies for the presumed lack of immunity to misinformation.
How users grapple with social media content to develop "natural immunity" as a
precursor to misinformation resilience remains an open question. We have
identified at least five folk models that conceptualize misinformation as
either: political (counter)argumentation, out-of-context narratives, inherently
fallacious information, external propaganda, or simply entertainment. We use
the rich conceptualizations embodied in these folk models to uncover how social
media users minimize adverse reactions to misinformation encounters in their
everyday lives.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Amy Devine</name>
    </author>
    <author>
      <name>Emma Pieroni</name>
    </author>
    <author>
      <name>Peter Jachim</name>
    </author>
    <link href="http://arxiv.org/abs/2207.12589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21608v1</id>
    <updated>2025-05-27T17:57:44Z</updated>
    <published>2025-05-27T17:57:44Z</published>
    <title>How does Misinformation Affect Large Language Model Behaviors and
  Preferences?</title>
    <summary>  Large Language Models (LLMs) have shown remarkable capabilities in
knowledge-intensive tasks, while they remain vulnerable when encountering
misinformation. Existing studies have explored the role of LLMs in combating
misinformation, but there is still a lack of fine-grained analysis on the
specific aspects and extent to which LLMs are influenced by misinformation. To
bridge this gap, we present MisBench, the current largest and most
comprehensive benchmark for evaluating LLMs' behavior and knowledge preference
toward misinformation. MisBench consists of 10,346,712 pieces of
misinformation, which uniquely considers both knowledge-based conflicts and
stylistic variations in misinformation. Empirical results reveal that while
LLMs demonstrate comparable abilities in discerning misinformation, they still
remain susceptible to knowledge conflicts and stylistic variations. Based on
these findings, we further propose a novel approach called Reconstruct to
Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our
study provides valuable insights into LLMs' interactions with misinformation,
and we believe MisBench can serve as an effective benchmark for evaluating
LLM-based detectors and enhancing their reliability in real-world applications.
Codes and data are available at https://github.com/GKNL/MisBench.
</summary>
    <author>
      <name>Miao Peng</name>
    </author>
    <author>
      <name>Nuo Chen</name>
    </author>
    <author>
      <name>Jianheng Tang</name>
    </author>
    <author>
      <name>Jia Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2025 Main Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.21608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.01217v1</id>
    <updated>2023-03-02T12:59:01Z</updated>
    <published>2023-03-02T12:59:01Z</published>
    <title>Synthetic Misinformers: Generating and Combating Multimodal
  Misinformation</title>
    <summary>  With the expansion of social media and the increasing dissemination of
multimedia content, the spread of misinformation has become a major concern.
This necessitates effective strategies for multimodal misinformation detection
(MMD) that detect whether the combination of an image and its accompanying text
could mislead or misinform. Due to the data-intensive nature of deep neural
networks and the labor-intensive process of manual annotation, researchers have
been exploring various methods for automatically generating synthetic
multimodal misinformation - which we refer to as Synthetic Misinformers - in
order to train MMD models. However, limited evaluation on real-world
misinformation and a lack of comparisons with other Synthetic Misinformers
makes difficult to assess progress in the field. To address this, we perform a
comparative study on existing and new Synthetic Misinformers that involves (1)
out-of-context (OOC) image-caption pairs, (2) cross-modal named entity
inconsistency (NEI) as well as (3) hybrid approaches and we evaluate them
against real-world misinformation; using the COSMOS benchmark. The comparative
study showed that our proposed CLIP-based Named Entity Swapping can lead to MMD
models that surpass other OOC and NEI Misinformers in terms of multimodal
accuracy and that hybrid approaches can lead to even higher detection accuracy.
Nevertheless, after alleviating information leakage from the COSMOS evaluation
protocol, low Sensitivity scores indicate that the task is significantly more
challenging than previous studies suggested. Finally, our findings showed that
NEI-based Synthetic Misinformers tend to suffer from a unimodal bias, where
text-only MMDs can outperform multimodal ones.
</summary>
    <author>
      <name>Stefanos-Iordanis Papadopoulos</name>
    </author>
    <author>
      <name>Christos Koutlis</name>
    </author>
    <author>
      <name>Symeon Papadopoulos</name>
    </author>
    <author>
      <name>Panagiotis C. Petrantonakis</name>
    </author>
    <link href="http://arxiv.org/abs/2303.01217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.01217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.08025v2</id>
    <updated>2025-03-24T18:03:00Z</updated>
    <published>2024-08-15T08:48:54Z</published>
    <title>Disagreement as a way to study misinformation and its effects</title>
    <summary>  Misinformation -- false or misleading information -- is considered a
significant societal concern due to its associated "misinformation effects,"
such as political polarization, erosion of trust in institutions, problematic
behavior, and public health challenges. However, the prevailing concept is
misaligned with what is studied. While misinformation focuses on instances of
information about factual matters, the broad spectrum of effects often
manifests at a societal level and is shaped by a wide range of interdependent
factors such as identity, values, opinions, epistemologies, and disagreements.
Unsurprisingly, misinformation effects can occur without the prevalence of
misinformation, and misinformation does not necessarily increase the effects
studied. Here, we propose using disagreement - conflicting attitudes and
beliefs between individuals and communities - as a way to study misinformation
effects because it addresses the identified conceptual limitations of
misinformation. Furthermore, unlike misinformation, disagreement does not
require researchers to determine whether a given information is false or
misleading. Thus, it can be studied and, more importantly, measured without the
need to make a normative judgment about a given information, even when the
specific topic is entirely removed, as we show in a longitudinal disagreement
measurement. We demonstrate that disagreement, as a holistic concept, provides
better explanations for the occurrence of misinformation effects, enhances
precision in developing appropriate interventions, and offers a promising
approach for evaluating them through quantification. Finally, we show how
disagreement addresses current misinformation research questions and conclude
with recommendations for research practice.
</summary>
    <author>
      <name>Damian Hodel</name>
    </author>
    <author>
      <name>Jevin West</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.37016/mr-2020-174</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.37016/mr-2020-174" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Harvard Kennedy School (HKS) Misinformation Review on
  March 20 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.08025v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.08025v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15321v1</id>
    <updated>2025-02-21T09:17:38Z</updated>
    <published>2025-02-21T09:17:38Z</published>
    <title>Crisis, Country, and Party Lines: Politicians' Misinformation Behavior
  and Public Engagement</title>
    <summary>  Politicians with large media visibility and social media audiences have a
significant influence on public discourse. Consequently, their dissemination of
misinformation can have profound implications for society. This study
investigated the misinformation-sharing behavior of 3,277 politicians and
associated public engagement by using data from X (formerly Twitter) during
2020-2021. The analysis was grounded in a novel and comprehensive dataset
including over 400,000 tweets covering multiple levels of governance-national
executive, national legislative, and regional executive-in Germany, Italy, the
UK, and the USA, representing distinct clusters of misinformation resilience.
Striking cross-country differences in misinformation-sharing behavior and
public engagement were observed. Politicians in Italy (4.9%) and the USA (2.2%)
exhibited the highest rates of misinformation sharing, primarily among
far-right and conservative legislators. Public engagement with misinformation
also varied significantly. In the USA, misinformation attracted over 2.5 times
the engagement of reliable information. In Italy, engagement levels were
similar across content types. Italy is unique in crisis-related misinformation,
particularly regarding COVID-19, which surpassed general misinformation in both
prevalence and audience engagement. These insights underscore the critical
roles of political affiliation, governance level, and crisis contexts in
shaping the dynamics of misinformation. The study expands the literature by
providing a cross-national, multi-level perspective, shedding light on how
political actors influence the proliferation of misinformation during crisis.
</summary>
    <author>
      <name>Jingyuan Yu</name>
    </author>
    <author>
      <name>Emese Domahidi</name>
    </author>
    <author>
      <name>Duccio Gamannossi degl'Innocenti</name>
    </author>
    <author>
      <name>Fabiana Zollo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.15321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.02471v2</id>
    <updated>2020-08-06T03:11:38Z</updated>
    <published>2020-06-03T18:28:57Z</published>
    <title>Can WhatsApp Benefit from Debunked Fact-Checked Stories to Reduce
  Misinformation?</title>
    <summary>  WhatsApp was alleged to be widely used to spread misinformation and
propaganda during elections in Brazil and India. Due to the private encrypted
nature of the messages on WhatsApp, it is hard to track the dissemination of
misinformation at scale. In this work, using public WhatsApp data, we observe
that misinformation has been largely shared on WhatsApp public groups even
after they were already fact-checked by popular fact-checking agencies. This
represents a significant portion of misinformation spread in both Brazil and
India in the groups analyzed. We posit that such misinformation content could
be prevented if WhatsApp had a means to flag already fact-checked content. To
this end, we propose an architecture that could be implemented by WhatsApp to
counter such misinformation. Our proposal respects the current end-to-end
encryption architecture on WhatsApp, thus protecting users' privacy while
providing an approach to detect the misinformation that benefits from
fact-checking efforts.
</summary>
    <author>
      <name>Julio C. S. Reis</name>
    </author>
    <author>
      <name>Philipe de Freitas Melo</name>
    </author>
    <author>
      <name>Kiran Garimella</name>
    </author>
    <author>
      <name>Fabr√≠cio Benevenuto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprint version of an accepted manuscript on The Harvard
  Kennedy School (HKS) Misinformation Review. Please, consider to cite it
  instead of this one</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.02471v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02471v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.14806v1</id>
    <updated>2020-07-29T12:46:45Z</updated>
    <published>2020-07-29T12:46:45Z</published>
    <title>Towards Domain-Specific Characterization of Misinformation</title>
    <summary>  The rapid dissemination of health misinformation poses an increasing risk to
public health. To best understand the way of combating health misinformation,
it is important to acknowledge how the fundamental characteristics of
misinformation differ from domain to domain. This paper presents a pathway
towards domain-specific characterization of misinformation so that we can
address the concealed behavior of health misinformation compared to others and
take proper initiative accordingly for combating it. With this aim, we have
mentioned several possible approaches to identify discriminating features of
medical misinformation from other types of misinformation. Thereafter, we
briefly propose a research plan followed by possible challenges to meet up. The
findings of the proposed research idea will provide new directions to the
misinformation research community.
</summary>
    <author>
      <name>Fariha Afsana</name>
    </author>
    <author>
      <name>Muhammad Ashad Kabir</name>
    </author>
    <author>
      <name>Naeemul Hassan</name>
    </author>
    <author>
      <name>Manoranjan Paul</name>
    </author>
    <link href="http://arxiv.org/abs/2007.14806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.14806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.16777v1</id>
    <updated>2023-03-27T22:24:05Z</updated>
    <published>2023-03-27T22:24:05Z</published>
    <title>Not cool, calm or collected: Using emotional language to detect COVID-19
  misinformation</title>
    <summary>  COVID-19 misinformation on social media platforms such as twitter is a threat
to effective pandemic management. Prior works on tweet COVID-19 misinformation
negates the role of semantic features common to twitter such as charged
emotions. Thus, we present a novel COVID-19 misinformation model, which uses
both a tweet emotion encoder and COVID-19 misinformation encoder to predict
whether a tweet contains COVID-19 misinformation. Our emotion encoder was
fine-tuned on a novel annotated dataset and our COVID-19 misinformation encoder
was fine-tuned on a subset of the COVID-HeRA dataset. Experimental results show
superior results using the combination of emotion and misinformation encoders
as opposed to a misinformation classifier alone. Furthermore, extensive result
analysis was conducted, highlighting low quality labels and mismatched label
distributions as key limitations to our study.
</summary>
    <author>
      <name>Gabriel Asher</name>
    </author>
    <author>
      <name>Phil Bohlman</name>
    </author>
    <author>
      <name>Karsten Kleyensteuber</name>
    </author>
    <link href="http://arxiv.org/abs/2303.16777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.00022v1</id>
    <updated>2024-08-16T16:14:36Z</updated>
    <published>2024-08-16T16:14:36Z</published>
    <title>Detecting Misinformation in Multimedia Content through Cross-Modal
  Entity Consistency: A Dual Learning Approach</title>
    <summary>  The landscape of social media content has evolved significantly, extending
from text to multimodal formats. This evolution presents a significant
challenge in combating misinformation. Previous research has primarily focused
on single modalities or text-image combinations, leaving a gap in detecting
multimodal misinformation. While the concept of entity consistency holds
promise in detecting multimodal misinformation, simplifying the representation
to a scalar value overlooks the inherent complexities of high-dimensional
representations across different modalities. To address these limitations, we
propose a Multimedia Misinformation Detection (MultiMD) framework for detecting
misinformation from video content by leveraging cross-modal entity consistency.
The proposed dual learning approach allows for not only enhancing
misinformation detection performance but also improving representation learning
of entity consistency across different modalities. Our results demonstrate that
MultiMD outperforms state-of-the-art baseline models and underscore the
importance of each modality in misinformation detection. Our research provides
novel methodological and technical insights into multimodal misinformation
detection.
</summary>
    <author>
      <name>Zhe Fu</name>
    </author>
    <author>
      <name>Kanlun Wang</name>
    </author>
    <author>
      <name>Wangjiaxuan Xin</name>
    </author>
    <author>
      <name>Lina Zhou</name>
    </author>
    <author>
      <name>Shi Chen</name>
    </author>
    <author>
      <name>Yaorong Ge</name>
    </author>
    <author>
      <name>Daniel Janies</name>
    </author>
    <author>
      <name>Dongsong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to PACIS 2024. 15 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">https://aisel.aisnet.org/pacis2024/track07_secprivacy/track07_secprivacy/2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2409.00022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.00022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.08070v1</id>
    <updated>2025-01-14T12:35:21Z</updated>
    <published>2025-01-14T12:35:21Z</published>
    <title>The Phase Model of Misinformation Interventions</title>
    <summary>  Misinformation is a challenging problem. This paper provides the first
systematic interdisciplinary investigation of technical and non-technical
interventions against misinformation. It combines interviews and a survey to
understand which interventions are accepted across academic disciplines and
approved by misinformation experts. Four interventions are supported by more
than two in three misinformation experts: promoting media literacy, education
in schools and universities, finding information about claims, and finding
sources for claims. The most controversial intervention is deleting
misinformation. We discuss the potentials and risks of all interventions.
Education-based interventions are perceived as the most helpful by
misinformation experts. Interventions focused on providing evidence are also
widely perceived as helpful. We discuss them as scalable and always available
interventions that empower users to independently identify misinformation. We
also introduce the Phase Model of Misinformation Interventions that helps
practitioners make informed decisions about which interventions to focus on and
how to best combine interventions.
</summary>
    <author>
      <name>Hendrik Heuer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3711088</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3711088" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CSCW 2025, April 2025</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ACM Hum.-Comput. Interact. 9, 2, Article CSCW190 (April
  2025), 28 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2501.08070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.08070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02135v1</id>
    <updated>2025-03-03T23:53:04Z</updated>
    <published>2025-03-03T23:53:04Z</published>
    <title>Does the Story Matter? Applying Narrative Theory to an Educational
  Misinformation Escape Room Game</title>
    <summary>  Rapid spread of harmful misinformation has led to a dire need for effective
media literacy interventions, to which educational games have been suggested as
a possible solution. Researchers and educators have created several games that
increase media literacy and resilience to misinformation. However, the existing
body of misinformation education games rarely focus upon the socio-emotional
influences that factor into misinformation belief. Misinformation correction
and serious games have both explored narrative as a method to engage with
people on an emotional basis. To this end, we investigated how 123 young adults
(mean age = 22.98) experienced narrative transportation and identification in
two narrative-centered misinformation escape room games developed for library
settings. We found that propensity for certain misinformation contexts, such as
engagement with fan culture and likelihood to share on social media platforms,
significantly affected how participants experienced specific measures of
narrative immersion within the games. We discuss design implications for
tailoring educational interventions to specific misinformation contexts.
</summary>
    <author>
      <name>Nisha Devasia</name>
    </author>
    <author>
      <name>Runhua Zhao</name>
    </author>
    <author>
      <name>Jin Ha Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2503.02135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02333v2</id>
    <updated>2025-06-07T06:59:18Z</updated>
    <published>2025-03-04T06:45:17Z</published>
    <title>Examining the Mental Health Impact of Misinformation on Social Media
  Using a Hybrid Transformer-Based Approach</title>
    <summary>  Social media has significantly reshaped interpersonal communication,
fostering connectivity while also enabling the proliferation of misinformation.
The unchecked spread of false narratives has profound effects on mental health,
contributing to increased stress, anxiety, and misinformation-driven paranoia.
This study presents a hybrid transformer-based approach using a RoBERTa-LSTM
classifier to detect misinformation, assess its impact on mental health, and
classify disorders linked to misinformation exposure. The proposed models
demonstrate accuracy rates of 98.4, 87.8, and 77.3 in detecting misinformation,
mental health implications, and disorder classification, respectively.
Furthermore, Pearson's Chi-Squared Test for Independence (p-value = 0.003871)
validates the direct correlation between misinformation and deteriorating
mental well-being. This study underscores the urgent need for better
misinformation management strategies to mitigate its psychological
repercussions. Future research could explore broader datasets incorporating
linguistic, demographic, and cultural variables to deepen the understanding of
misinformation-induced mental health distress.
</summary>
    <author>
      <name>Sarvesh Arora</name>
    </author>
    <author>
      <name>Sarthak Arora</name>
    </author>
    <author>
      <name>Deepika Kumar</name>
    </author>
    <author>
      <name>Vallari Agrawal</name>
    </author>
    <author>
      <name>Vedika Gupta</name>
    </author>
    <author>
      <name>Dipit Vasdev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper has data authenticity problems, ALL AUTHORS confirm they wish
  to withdraw this paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.02333v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02333v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07136v1</id>
    <updated>2019-03-17T17:37:39Z</updated>
    <published>2019-03-17T17:37:39Z</published>
    <title>Human-Misinformation interaction: Understanding the interdisciplinary
  approach needed to computationally combat false information</title>
    <summary>  The prevalence of new technologies and social media has amplified the effects
of misinformation on our societies. Thus, it is necessary to create
computational tools to mitigate their effects effectively. This study aims to
provide a critical overview of computational approaches concerned with
combating misinformation. To this aim, I offer an overview of scholarly
definitions of misinformation. I adopt a framework for studying misinformation
that suggests paying attention to the source, content, and consumers as the
three main elements involved in the process of misinformation and I provide an
overview of literature from disciplines of psychology, media studies, and
cognitive sciences that deal with each of these elements. Using the framework,
I overview the existing computational methods that deal with 1) misinformation
detection and fact-checking using Content 2) Identifying untrustworthy Sources
and social bots, and 3) Consumer-facing tools and methods aiming to make humans
resilient to misinformation. I find that the vast majority of works in computer
science and information technology is concerned with the crucial tasks of
detection and verification of content and sources of misinformation. Moreover,
I find that computational research focusing on Consumers of Misinformation in
Human-Computer Interaction (HCI) and related fields are very sparse and often
do not deal with the subtleties of this process. The majority of existing
interfaces and systems are less concerned with the usability of the tools
rather than the robustness and accuracy of the detection methods. Using this
survey, I call for an interdisciplinary approach towards human-misinformation
interaction that focuses on building methods and tools that robustly deal with
such complex psychological/social phenomena.
</summary>
    <author>
      <name>Alireza Karduni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.07136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.06455v2</id>
    <updated>2020-10-15T03:23:26Z</updated>
    <published>2020-10-13T15:10:26Z</published>
    <title>Characterizing and Comparing COVID-19 Misinformation Across Languages,
  Countries and Platforms</title>
    <summary>  Misinformation/disinformation about COVID-19 has been rampant on social media
around the world. In this study, we investigate COVID-19 misinformation/
disinformation on social media in multiple languages - Farsi (Persian),
Chinese, and English, about multiple countries - Iran, China, and the United
States (US), and on multiple platforms such as Twitter, Facebook, Instagram,
Weibo, and WhatsApp. Misinformation, especially about a global pandemic, is a
global problem yet it is common for studies of COVID-19 misinformation on
social media to focus on a single language, like English, a single country,
like the US, or a single platform, like Twitter. We utilized opportunistic
sampling to compile 200 specific items of viral and yet debunked misinformation
across these languages, countries and platforms emerged between January 1 and
August 31. We then categorized this collection based both on the topics of the
misinformation and the underlying roots of that misinformation. Our
multi-cultural and multilingual team observed that the nature of COVID-19
misinformation on social media varied in substantial ways across different
languages/countries depending on the cultures, beliefs/religions, popularity of
social media, types of platforms, freedom of speech and the power of people
versus governments. We observe that politics is at the root of most of the
collected misinformation across all three languages in this dataset. We further
observe the different impact of government restrictions on platforms and
platform restrictions on content in Iran, China, and the US and their impact on
a key question of our age: how do we control misinformation without silencing
the voices we need to hold governments accountable?
</summary>
    <author>
      <name>Golshan Madraki</name>
    </author>
    <author>
      <name>Isabella Grasso</name>
    </author>
    <author>
      <name>Jacqueline Otala</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <author>
      <name>Jeanna Matthews</name>
    </author>
    <link href="http://arxiv.org/abs/2010.06455v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.06455v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.12468v1</id>
    <updated>2020-09-25T22:48:51Z</updated>
    <published>2020-09-25T22:48:51Z</published>
    <title>Investigating Misinformation in Online Marketplaces: An Audit Study on
  Amazon</title>
    <summary>  Search and recommendation systems are ubiquitous and irreplaceable tools in
our daily lives. Despite their critical role in selecting and ranking the most
relevant information, they typically do not consider the veracity of
information presented to the user. In this paper, we introduce an audit
methodology to investigate the extent of misinformation presented in search
results and recommendations on online marketplaces. We investigate the factors
and personalization attributes that influence the amount of misinformation in
searches and recommendations. Recently, several media reports criticized Amazon
for hosting and recommending items that promote misinformation on topics such
as vaccines. Motivated by those reports, we apply our algorithmic auditing
methodology on Amazon to verify those claims. Our audit study investigates (a)
factors that might influence the search algorithms of Amazon and (b)
personalization attributes that contribute to amplifying the amount of
misinformation recommended to users in their search results and
recommendations. Our audit study collected ~526k search results and ~182k
homepage recommendations, with ~8.5k unique items. Each item is annotated for
its stance on vaccines' misinformation (pro, neutral, or anti). Our study
reveals that (1) the selection and ranking by the default Featured search
algorithm of search results that have misinformation stances are positively
correlated with the stance of search queries and customers' evaluation of items
(ratings and reviews), (2) misinformation stances of search results are neither
affected by users' activities nor by interacting (browsing, wish-listing,
shopping) with items that have a misinformation stance, and (3) a filter bubble
built-in users' homepages have a misinformation stance positively correlated
with the misinformation stance of items that a user interacts with.
</summary>
    <author>
      <name>Eslam Hussein</name>
    </author>
    <author>
      <name>Hoda Eldardiry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures, submitted to ASONAM</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.12468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.12468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.12191v1</id>
    <updated>2021-03-22T21:44:32Z</updated>
    <published>2021-03-22T21:44:32Z</published>
    <title>Using an Epidemiological Model to Study the Spread of Misinformation
  during the Black Lives Matter Movement</title>
    <summary>  The proliferation of social media platforms like Twitter has heightened the
consequences of the spread of misinformation. To understand and model the
spread of misinformation, in this paper, we leveraged the SEIZ (Susceptible,
Exposed, Infected, Skeptics) epidemiological model to describe the underlying
process that delineates the spread of misinformation on Twitter. Compared to
the other epidemiological models, this model produces broader results because
it includes the additional Skeptics (Z) compartment, wherein a user may be
exposed to an item of misinformation but not engage in any reaction to it, and
the additional Exposed (E) compartment, wherein the user may need some time
before deciding to spread a misinformation item. We analyzed misinformation
regarding the unrest in Washington, D.C. in the month of March 2020 which was
propagated by the use of the #DCblackout hashtag by different users across the
U.S. on Twitter. Our analysis shows that misinformation can be modeled using
the concept of epidemiology. To the best of our knowledge, this research is the
first to attempt to apply the SEIZ epidemiological model to the spread of a
specific item of misinformation, which is a category distinct from that of
rumor, and a hoax on online social media platforms. Applying a mathematical
model can help to understand the trends and dynamics of the spread of
misinformation on Twitter and ultimately help to develop techniques to quickly
identify and control it.
</summary>
    <author>
      <name>Maryam Maleki</name>
    </author>
    <author>
      <name>Esther Mead</name>
    </author>
    <author>
      <name>Mohammad Arani</name>
    </author>
    <author>
      <name>Nitin Agarwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted on the International Conference on Fake News,
  Social Media Manipulation and Misinformation 2021 (ICFNSMMM 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.12191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.12191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01350v1</id>
    <updated>2022-03-02T19:00:33Z</updated>
    <published>2022-03-02T19:00:33Z</published>
    <title>Partisan Asymmetries in Exposure to Misinformation</title>
    <summary>  Health misinformation is believed to have contributed to vaccine hesitancy
during the Covid-19 pandemic, highlighting concerns about the role of social
media in polarization and social stability. While previous research has
identified a link between political partisanship and misinformation sharing
online, the interaction between partisanship and how much misinformation people
see within their social networks has not been well studied. As a result, we do
not know whether partisanship drives exposure to misinformation or people
selectively share misinformation despite being exposed to factual content. We
study Twitter discussions about the Covid-19 pandemic, classifying users
ideologically along political and factual dimensions. We find partisan
asymmetries in both sharing behaviors and exposure, with conservatives more
likely to see and share misinformation and moderate liberals seeing the most
factual content. We identify multi-dimensional echo chambers that expose users
to ideologically congruent content; however, the interaction between political
and factual dimensions creates conditions for the highly polarized users --
hardline conservatives and liberals -- to amplify misinformation. Despite this,
misinformation receives less attention than factual content and political
moderates, who represent the bulk of users in our sample, help filter out
misinformation, reducing the amount of low factuality content in the
information ecosystem. Identifying the extent of polarization and how political
ideology can exacerbate misinformation can potentially help public health
experts and policy makers improve their messaging to promote consensus.
</summary>
    <author>
      <name>Ashwin Rao</name>
    </author>
    <author>
      <name>Fred Morstatter</name>
    </author>
    <author>
      <name>Kristina Lerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.01350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.08483v2</id>
    <updated>2024-01-18T11:43:52Z</updated>
    <published>2023-10-12T16:42:53Z</published>
    <title>Understanding the Humans Behind Online Misinformation: An Observational
  Study Through the Lens of the COVID-19 Pandemic</title>
    <summary>  The proliferation of online misinformation has emerged as one of the biggest
threats to society. Considerable efforts have focused on building
misinformation detection models, still the perils of misinformation remain
abound. Mitigating online misinformation and its ramifications requires a
holistic approach that encompasses not only an understanding of its intricate
landscape in relation to the complex issue and topic-rich information ecosystem
online, but also the psychological drivers of individuals behind it. Adopting a
time series analytic technique and robust causal inference-based design, we
conduct a large-scale observational study analyzing over 32 million COVID-19
tweets and 16 million historical timeline tweets. We focus on understanding the
behavior and psychology of users disseminating misinformation during COVID-19
and its relationship with the historical inclinations towards sharing
misinformation on Non-COVID domains before the pandemic. Our analysis
underscores the intricacies inherent to cross-domain misinformation, and
highlights that users' historical inclination toward sharing misinformation is
positively associated with their present behavior pertaining to misinformation
sharing on emergent topics and beyond. This work may serve as a valuable
foundation for designing user-centric inoculation strategies and
ecologically-grounded agile interventions for effectively tackling online
misinformation.
</summary>
    <author>
      <name>Mohit Chandra</name>
    </author>
    <author>
      <name>Anush Mattapalli</name>
    </author>
    <author>
      <name>Munmun De Choudhury</name>
    </author>
    <link href="http://arxiv.org/abs/2310.08483v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.08483v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.09354v1</id>
    <updated>2023-12-14T21:34:59Z</updated>
    <published>2023-12-14T21:34:59Z</published>
    <title>Older Adults' Experiences with Misinformation on Social Media</title>
    <summary>  Older adults habitually encounter misinformation on social media, but there
is little knowledge about their experiences with it. In this study, we combined
a qualitative survey (n=119) with in-depth interviews (n=21) to investigate how
older adults in America conceptualize, discern, and contextualize social media
misinformation. As misinformation on social media in the past was driven
towards influencing voting outcomes, we were particularly interested to
approach our study from a voting intention perspective. We found that 62% of
the participants intending to vote Democrat saw a manipulative political
purpose behind the spread of misinformation while only 5% of those intending to
vote Republican believed misinformation has a political dissent purpose.
Regardless of the voting intentions, most participants relied on source
heuristics combined with fact-checking to discern truth from misinformation on
social media. The biggest concern about the misinformation, among all the
participants, was that it increasingly leads to biased reasoning influenced by
personal values and feelings instead of reasoning based on objective evidence.
The participants intending to vote Democrat were in 74% of the cases concerned
that misinformation will cause escalation of extremism in the future, while
those intending to vote Republican, were undecided, or planned to abstain were
concerned that misinformation will further erode the trust in democratic
institutions, specifically in the context of public health and free and fair
elections. During our interviews, we found that 63% of the participants who
intended to vote Republican, were fully aware and acknowledged that Republican
or conservative voices often time speak misinformation, even though they are
closely aligned to their political ideology.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Jennifer Vander Loop</name>
    </author>
    <link href="http://arxiv.org/abs/2312.09354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.09354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.00633v1</id>
    <updated>2024-08-01T15:17:33Z</updated>
    <published>2024-08-01T15:17:33Z</published>
    <title>DisTrack: a new Tool for Semi-automatic Misinformation Tracking in
  Online Social Networks</title>
    <summary>  Introduction: This article introduces DisTrack, a methodology and a tool
developed for tracking and analyzing misinformation within Online Social
Networks (OSNs). DisTrack is designed to combat the spread of misinformation
through a combination of Natural Language Processing (NLP) Social Network
Analysis (SNA) and graph visualization. The primary goal is to detect
misinformation, track its propagation, identify its sources, and assess the
influence of various actors within the network.
  Methods: DisTrack's architecture incorporates a variety of methodologies
including keyword search, semantic similarity assessments, and graph generation
techniques. These methods collectively facilitate the monitoring of
misinformation, the categorization of content based on alignment with known
false claims, and the visualization of dissemination cascades through detailed
graphs. The tool is tailored to capture and analyze the dynamic nature of
misinformation spread in digital environments.
  Results: The effectiveness of DisTrack is demonstrated through three case
studies focused on different themes: discredit/hate speech, anti-vaccine
misinformation, and false narratives about the Russia-Ukraine conflict. These
studies show DisTrack's capabilities in distinguishing posts that propagate
falsehoods from those that counteract them, and tracing the evolution of
misinformation from its inception.
  Conclusions: The research confirms that DisTrack is a valuable tool in the
field of misinformation analysis. It effectively distinguishes between
different types of misinformation and traces their development over time. By
providing a comprehensive approach to understanding and combating
misinformation in digital spaces, DisTrack proves to be an essential asset for
researchers and practitioners working to mitigate the impact of false
information in online social environments.
</summary>
    <author>
      <name>Guillermo Villar-Rodr√≠guez</name>
    </author>
    <author>
      <name>√Ålvaro Huertas-Garc√≠a</name>
    </author>
    <author>
      <name>Alejandro Mart√≠n</name>
    </author>
    <author>
      <name>Javier Huertas-Tato</name>
    </author>
    <author>
      <name>David Camacho</name>
    </author>
    <link href="http://arxiv.org/abs/2408.00633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.00633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.08985v1</id>
    <updated>2025-01-15T18:04:21Z</updated>
    <published>2025-01-15T18:04:21Z</published>
    <title>Personality Modeling for Persuasion of Misinformation using AI Agent</title>
    <summary>  The proliferation of misinformation on social media platforms has highlighted
the need to understand how individual personality traits influence
susceptibility to and propagation of misinformation. This study employs an
innovative agent-based modeling approach to investigate the relationship
between personality traits and misinformation dynamics. Using six AI agents
embodying different dimensions of the Big Five personality traits
(Extraversion, Agreeableness, and Neuroticism), we simulated interactions
across six diverse misinformation topics. The experiment, implemented through
the AgentScope framework using the GLM-4-Flash model, generated 90 unique
interactions, revealing complex patterns in how personality combinations affect
persuasion and resistance to misinformation. Our findings demonstrate that
analytical and critical personality traits enhance effectiveness in
evidence-based discussions, while non-aggressive persuasion strategies show
unexpected success in misinformation correction. Notably, agents with critical
traits achieved a 59.4% success rate in HIV-related misinformation discussions,
while those employing non-aggressive approaches maintained consistent
persuasion rates above 40% across different personality combinations. The study
also revealed a non-transitive pattern in persuasion effectiveness, challenging
conventional assumptions about personality-based influence. These results
provide crucial insights for developing personality-aware interventions in
digital environments and suggest that effective misinformation countermeasures
should prioritize emotional connection and trust-building over confrontational
approaches. The findings contribute to both theoretical understanding of
personality-misinformation dynamics and practical strategies for combating
misinformation in social media contexts.
</summary>
    <author>
      <name>Qianmin Lou</name>
    </author>
    <author>
      <name>Wentao Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2501.08985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.08985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.00941v1</id>
    <updated>2021-07-02T10:02:36Z</updated>
    <published>2021-07-02T10:02:36Z</published>
    <title>Misinformation Detection on YouTube Using Video Captions</title>
    <summary>  Millions of people use platforms such as YouTube, Facebook, Twitter, and
other mass media. Due to the accessibility of these platforms, they are often
used to establish a narrative, conduct propaganda, and disseminate
misinformation. This work proposes an approach that uses state-of-the-art NLP
techniques to extract features from video captions (subtitles). To evaluate our
approach, we utilize a publicly accessible and labeled dataset for classifying
videos as misinformation or not. The motivation behind exploring video captions
stems from our analysis of videos metadata. Attributes such as the number of
views, likes, dislikes, and comments are ineffective as videos are hard to
differentiate using this information. Using caption dataset, the proposed
models can classify videos among three classes (Misinformation, Debunking
Misinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the
relevance of the misinformation class, we re-formulate our classification
problem as a two-class classification - Misinformation vs. others (Debunking
Misinformation and Neutral). In our experiments, the proposed models can
classify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.
</summary>
    <author>
      <name>Raj Jagtap</name>
    </author>
    <author>
      <name>Abhinav Kumar</name>
    </author>
    <author>
      <name>Rahul Goel</name>
    </author>
    <author>
      <name>Shakshi Sharma</name>
    </author>
    <author>
      <name>Rajesh Sharma</name>
    </author>
    <author>
      <name>Clint P. George</name>
    </author>
    <link href="http://arxiv.org/abs/2107.00941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.00941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.11911v1</id>
    <updated>2022-05-24T09:15:51Z</updated>
    <published>2022-05-24T09:15:51Z</published>
    <title>Evaluating the Effect of Enhanced Text-Visualization Integration on
  Combating Misinformation in Data Story</title>
    <summary>  Misinformation has disruptive effects on our lives. Many researchers have
looked into means to identify and combat misinformation in text or data
visualization. However, there is still a lack of understanding of how
misinformation can be introduced when text and visualization are combined to
tell data stories, not to mention how to improve the lay public's awareness of
possible misperceptions about facts in narrative visualization. In this paper,
we first analyze where misinformation could possibly be injected into the
production-consumption process of data stories through a literature survey.
Then, as a first step towards combating misinformation in data stories, we
explore possible defensive design methods to enhance the reader's awareness of
information misalignment when data facts are scripted and visualized. More
specifically, we conduct a between-subjects crowdsourcing study to investigate
the impact of two design methods enhancing text-visualization integration,
i.e., explanatory annotation and interactive linking, on users' awareness of
misinformation in data stories. The study results show that although most
participants still can not find misinformation, the two design methods can
significantly lower the perceived credibility of the text or visualizations.
Our work informs the possibility of fighting an infodemic through defensive
design methods.
</summary>
    <author>
      <name>Chengbo Zheng</name>
    </author>
    <author>
      <name>Xiaojuan Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at PacificVis2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.11911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.11911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08419v2</id>
    <updated>2021-01-29T20:15:18Z</updated>
    <published>2021-01-21T03:16:29Z</published>
    <title>Auditing E-Commerce Platforms for Algorithmically Curated Vaccine
  Misinformation</title>
    <summary>  There is a growing concern that e-commerce platforms are amplifying
vaccine-misinformation. To investigate, we conduct two-sets of algorithmic
audits for vaccine misinformation on the search and recommendation algorithms
of Amazon -- world's leading e-retailer. First, we systematically audit
search-results belonging to vaccine-related search-queries without logging into
the platform -- unpersonalized audits. We find 10.47% of search-results promote
misinformative health products. We also observe ranking-bias, with Amazon
ranking misinformative search-results higher than debunking search-results.
Next, we analyze the effects of personalization due to account-history, where
history is built progressively by performing various real-world user-actions,
such as clicking a product. We find evidence of filter-bubble effect in
Amazon's recommendations; accounts performing actions on misinformative
products are presented with more misinformation compared to accounts performing
actions on neutral and debunking products. Interestingly, once user clicks on a
misinformative product, homepage recommendations become more contaminated
compared to when user shows an intention to buy that product.
</summary>
    <author>
      <name>Prerna Juneja</name>
    </author>
    <author>
      <name>Tanushree Mitra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3411764.3445250</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3411764.3445250" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CHI Conference on Human Factors in Computing Systems 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.08419v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08419v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14748v1</id>
    <updated>2021-03-26T21:53:38Z</updated>
    <published>2021-03-26T21:53:38Z</published>
    <title>Analysing the Effect of Recommendation Algorithms on the Amplification
  of Misinformation</title>
    <summary>  Recommendation algorithms have been pointed out as one of the major culprits
of misinformation spreading in the digital sphere. However, it is still unclear
how these algorithms really propagate misinformation, e.g., it has not been
shown which particular recommendation approaches are more prone to suggest
misinforming items, or which internal parameters of the algorithms could be
influencing more on their misinformation propagation capacity.
  Motivated by this fact, in this paper we present an analysis of the effect of
some of the most popular recommendation algorithms on the spread of
misinformation in Twitter. A set of guidelines on how to adapt these algorithms
is provided based on such analysis and a comprehensive review of the research
literature. A dataset is also generated and released to the scientific
community to stimulate discussions on the future design and development of
recommendation algorithms to counter misinformation. The dataset includes
editorially labelled news items and claims regarding their misinformation
nature.
</summary>
    <author>
      <name>Miriam Fern√°ndez</name>
    </author>
    <author>
      <name>Alejandro Bellog√≠n</name>
    </author>
    <author>
      <name>Iv√°n Cantador</name>
    </author>
    <link href="http://arxiv.org/abs/2103.14748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.13661v2</id>
    <updated>2023-10-26T20:45:39Z</updated>
    <published>2023-05-23T04:10:26Z</published>
    <title>On the Risk of Misinformation Pollution with Large Language Models</title>
    <summary>  In this paper, we comprehensively investigate the potential misuse of modern
Large Language Models (LLMs) for generating credible-sounding misinformation
and its subsequent impact on information-intensive applications, particularly
Open-Domain Question Answering (ODQA) systems. We establish a threat model and
simulate potential misuse scenarios, both unintentional and intentional, to
assess the extent to which LLMs can be utilized to produce misinformation. Our
study reveals that LLMs can act as effective misinformation generators, leading
to a significant degradation in the performance of ODQA systems. To mitigate
the harm caused by LLM-generated misinformation, we explore three defense
strategies: prompting, misinformation detection, and majority voting. While
initial results show promising trends for these defensive strategies, much more
work needs to be done to address the challenge of misinformation pollution. Our
work highlights the need for further research and interdisciplinary
collaboration to address LLM-generated misinformation and to promote
responsible use of LLMs.
</summary>
    <author>
      <name>Yikang Pan</name>
    </author>
    <author>
      <name>Liangming Pan</name>
    </author>
    <author>
      <name>Wenhu Chen</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Min-Yen Kan</name>
    </author>
    <author>
      <name>William Yang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2023 (Findings; Long Paper)</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.13661v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.13661v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.02377v2</id>
    <updated>2024-01-09T13:44:43Z</updated>
    <published>2023-08-04T15:18:28Z</published>
    <title>Sowing 'Seeds of Doubt': Cottage Industries of Election and Medical
  Misinformation in Brazil and the United States</title>
    <summary>  We conducted ethnographic research with 31 misinformation creators and
consumers in Brazil and the US before, during, and after a major election to
understand the consumption and production of election and medical
misinformation. This study contributes to research on misinformation ecosystems
by focusing on poorly understood small players, or "micro-influencers", who
create misinformation in peer-to-peer networks. We detail four key tactics that
micro-influencers use. First, they typically disseminate "gray area" content
rather than expert-falsified claims, using subtle aesthetic and rhetorical
tactics to evade moderation. Second, they post in small, closed groups where
members feel safe and predisposed to trust content. Third, they explicitly
target misinformation consumers' emotional and social needs. Finally, they post
a high volume of short, repetitive content to plant seeds of doubt and build
trust in influencers as unofficial experts. We discuss the implications these
micro-influencers have for misinformation interventions and platforms' efforts
to moderate misinformation.
</summary>
    <author>
      <name>Amelia Hassoun</name>
    </author>
    <author>
      <name>Gabrielle Borenstein</name>
    </author>
    <author>
      <name>Beth Goldberg</name>
    </author>
    <author>
      <name>Jacob McAuliffe</name>
    </author>
    <author>
      <name>Katy Osborn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 13 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.02377v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.02377v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.08254v1</id>
    <updated>2024-05-14T01:01:44Z</updated>
    <published>2024-05-14T01:01:44Z</published>
    <title>Detecting Fallacies in Climate Misinformation: A Technocognitive
  Approach to Identifying Misleading Argumentation</title>
    <summary>  Misinformation about climate change is a complex societal issue requiring
holistic, interdisciplinary solutions at the intersection between technology
and psychology. One proposed solution is a "technocognitive" approach,
involving the synthesis of psychological and computer science research.
Psychological research has identified that interventions in response to
misinformation require both fact-based (e.g., factual explanations) and
technique-based (e.g., explanations of misleading techniques) content. However,
little progress has been made on documenting and detecting fallacies in climate
misinformation. In this study, we apply a previously developed critical
thinking methodology for deconstructing climate misinformation, in order to
develop a dataset mapping different types of climate misinformation to
reasoning fallacies. This dataset is used to train a model to detect fallacies
in climate misinformation. Our study shows F1 scores that are 2.5 to 3.5 better
than previous works. The fallacies that are easiest to detect include fake
experts and anecdotal arguments, while fallacies that require background
knowledge, such as oversimplification, misrepresentation, and slothful
induction, are relatively more difficult to detect. This research lays the
groundwork for development of solutions where automatically detected climate
misinformation can be countered with generative technique-based corrections.
</summary>
    <author>
      <name>Francisco Zanartu</name>
    </author>
    <author>
      <name>John Cook</name>
    </author>
    <author>
      <name>Markus Wagner</name>
    </author>
    <author>
      <name>Julian Garcia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41598-024-76139-w</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41598-024-76139-w" rel="related"/>
    <link href="http://arxiv.org/abs/2405.08254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.08254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.09613v2</id>
    <updated>2025-04-17T09:37:38Z</updated>
    <published>2024-08-18T23:43:12Z</published>
    <title>How Do Social Bots Participate in Misinformation Spread? A Comprehensive
  Dataset and Analysis</title>
    <summary>  The social media platform is an ideal medium to spread misinformation, where
social bots might accelerate the spread. This paper is the first to explore the
interplay between social bots and misinformation on the Sina Weibo platform. We
construct a large-scale dataset that contains annotations of misinformation and
social bots. From the misinformation perspective, this dataset is multimodal,
containing 11,393 pieces of misinformation and 16,416 pieces of real
information. From the social bot perspective, this dataset contains 65,749
social bots and 345,886 genuine accounts, where we propose a weak-supervised
annotator to annotate automatically. Extensive experiments prove that the
dataset is the most comprehensive, misinformation and real information are
distinguishable, and social bots have high annotation quality. Further analysis
illustrates that: (i) social bots are deeply involved in information spread;
(ii) misinformation with the same topics has similar content, providing the
basis of echo chambers, and social bots amplify this phenomenon; and (iii)
social bots generate similar content aiming to manipulate public opinions.
</summary>
    <author>
      <name>Herun Wan</name>
    </author>
    <author>
      <name>Minnan Luo</name>
    </author>
    <author>
      <name>Zihan Ma</name>
    </author>
    <author>
      <name>Guang Dai</name>
    </author>
    <author>
      <name>Xiang Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2408.09613v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.09613v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04028v1</id>
    <updated>2025-05-07T00:07:04Z</updated>
    <published>2025-05-07T00:07:04Z</published>
    <title>Appeal and Scope of Misinformation Spread by AI Agents and Humans</title>
    <summary>  This work examines the influence of misinformation and the role of AI agents,
called bots, on social network platforms. To quantify the impact of
misinformation, it proposes two new metrics based on attributes of tweet
engagement and user network position: Appeal, which measures the popularity of
the tweet, and Scope, which measures the potential reach of the tweet. In
addition, it analyzes 5.8 million misinformation tweets on the COVID-19 vaccine
discourse over three time periods: Pre-Vaccine, Vaccine Launch, and
Post-Vaccine. Results show that misinformation was more prevalent during the
first two periods. Human-generated misinformation tweets tend to have higher
appeal and scope compared to bot-generated ones. Tweedie regression analysis
reveals that human-generated misinformation tweets were most concerning during
Vaccine Launch week, whereas bot-generated misinformation reached its highest
appeal and scope during the Pre-Vaccine period.
</summary>
    <author>
      <name>Lynnette Hui Xian Ng</name>
    </author>
    <author>
      <name>Wenqi Zhou</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to AMCIS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.04028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18555v1</id>
    <updated>2025-05-24T06:45:45Z</updated>
    <published>2025-05-24T06:45:45Z</published>
    <title>Unraveling Misinformation Propagation in LLM Reasoning</title>
    <summary>  Large Language Models (LLMs) have demonstrated impressive capabilities in
reasoning, positioning them as promising tools for supporting human
problem-solving. However, what happens when their performance is affected by
misinformation, i.e., incorrect inputs introduced by users due to oversights or
gaps in knowledge? Such misinformation is prevalent in real-world interactions
with LLMs, yet how it propagates within LLMs' reasoning process remains
underexplored. Focusing on mathematical reasoning, we present a comprehensive
analysis of how misinformation affects intermediate reasoning steps and final
answers. We also examine how effectively LLMs can correct misinformation when
explicitly instructed to do so. Even with explicit instructions, LLMs succeed
less than half the time in rectifying misinformation, despite possessing
correct internal knowledge, leading to significant accuracy drops (10.02% -
72.20%). Further analysis shows that applying factual corrections early in the
reasoning process most effectively reduces misinformation propagation, and
fine-tuning on synthesized data with early-stage corrections significantly
improves reasoning factuality. Our work offers a practical approach to
mitigating misinformation propagation.
</summary>
    <author>
      <name>Yiyang Feng</name>
    </author>
    <author>
      <name>Yichen Wang</name>
    </author>
    <author>
      <name>Shaobo Cui</name>
    </author>
    <author>
      <name>Boi Faltings</name>
    </author>
    <author>
      <name>Mina Lee</name>
    </author>
    <author>
      <name>Jiawei Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 14 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.18555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.02350v1</id>
    <updated>2025-06-03T01:09:55Z</updated>
    <published>2025-06-03T01:09:55Z</published>
    <title>Truth over Tricks: Measuring and Mitigating Shortcut Learning in
  Misinformation Detection</title>
    <summary>  Misinformation detection models often rely on superficial cues (i.e.,
\emph{shortcuts}) that correlate with misinformation in training data but fail
to generalize to the diverse and evolving nature of real-world misinformation.
This issue is exacerbated by large language models (LLMs), which can easily
generate convincing misinformation through simple prompts. We introduce
TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning
in misinformation detection. TruthOverTricks categorizes shortcut behaviors
into intrinsic shortcut induction and extrinsic shortcut injection, and
evaluates seven representative detectors across 14 popular benchmarks, along
with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.
Empirical results reveal that existing detectors suffer severe performance
degradation when exposed to both naturally occurring and adversarially crafted
shortcuts. To address this, we propose SMF, an LLM-augmented data augmentation
framework that mitigates shortcut reliance through paraphrasing, factual
summarization, and sentiment normalization. SMF consistently enhances
robustness across 16 benchmarks, encouraging models to rely on deeper semantic
understanding rather than shortcut cues. To promote the development of
misinformation detectors, we have published the resources publicly at
https://github.com/whr000001/TruthOverTricks.
</summary>
    <author>
      <name>Herun Wan</name>
    </author>
    <author>
      <name>Jiaying Wu</name>
    </author>
    <author>
      <name>Minnan Luo</name>
    </author>
    <author>
      <name>Zhi Zeng</name>
    </author>
    <author>
      <name>Zhixiong Su</name>
    </author>
    <link href="http://arxiv.org/abs/2506.02350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.02350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.06433v1</id>
    <updated>2023-03-11T15:55:01Z</updated>
    <published>2023-03-11T15:55:01Z</published>
    <title>Reinforcement Learning-based Counter-Misinformation Response Generation:
  A Case Study of COVID-19 Vaccine Misinformation</title>
    <summary>  The spread of online misinformation threatens public health, democracy, and
the broader society. While professional fact-checkers form the first line of
defense by fact-checking popular false claims, they do not engage directly in
conversations with misinformation spreaders. On the other hand, non-expert
ordinary users act as eyes-on-the-ground who proactively counter misinformation
-- recent research has shown that 96% counter-misinformation responses are made
by ordinary users. However, research also found that 2/3 times, these responses
are rude and lack evidence. This work seeks to create a counter-misinformation
response generation model to empower users to effectively correct
misinformation. This objective is challenging due to the absence of datasets
containing ground-truth of ideal counter-misinformation responses, and the lack
of models that can generate responses backed by communication theories. In this
work, we create two novel datasets of misinformation and counter-misinformation
response pairs from in-the-wild social media and crowdsourcing from
college-educated students. We annotate the collected data to distinguish poor
from ideal responses that are factual, polite, and refute misinformation. We
propose MisinfoCorrect, a reinforcement learning-based framework that learns to
generate counter-misinformation responses for an input misinformation post. The
model rewards the generator to increase the politeness, factuality, and
refutation attitude while retaining text fluency and relevancy. Quantitative
and qualitative evaluation shows that our model outperforms several baselines
by generating high-quality counter-responses. This work illustrates the promise
of generative text models for social good -- here, to help create a safe and
reliable information ecosystem. The code and data is accessible on
https://github.com/claws-lab/MisinfoCorrect.
</summary>
    <author>
      <name>Bing He</name>
    </author>
    <author>
      <name>Mustaque Ahamad</name>
    </author>
    <author>
      <name>Srijan Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at: ACM Web Conference 2023 (WWW'23). Code and data at:
  https://github.com/claws-lab/MisinfoCorrect</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.06433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.06433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.13954v1</id>
    <updated>2023-06-24T12:56:56Z</updated>
    <published>2023-06-24T12:56:56Z</published>
    <title>Characterizing the Emotion Carriers of COVID-19 Misinformation and Their
  Impact on Vaccination Outcomes in India and the United States</title>
    <summary>  The COVID-19 Infodemic had an unprecedented impact on health behaviors and
outcomes at a global scale. While many studies have focused on a qualitative
and quantitative understanding of misinformation, including sentiment analysis,
there is a gap in understanding the emotion-carriers of misinformation and
their differences across geographies. In this study, we characterized emotion
carriers and their impact on vaccination rates in India and the United States.
A manually labelled dataset was created from 2.3 million tweets and collated
with three publicly available datasets (CoAID, AntiVax, CMU) to train deep
learning models for misinformation classification. Misinformation labelled
tweets were further analyzed for behavioral aspects by leveraging Plutchik
Transformers to determine the emotion for each tweet. Time series analysis was
conducted to study the impact of misinformation on spatial and temporal
characteristics. Further, categorical classification was performed using
transformer models to assign categories for the misinformation tweets.
Word2Vec+BiLSTM was the best model for misinformation classification, with an
F1-score of 0.92. The US had the highest proportion of misinformation tweets
(58.02%), followed by the UK (10.38%) and India (7.33%). Disgust, anticipation,
and anger were associated with an increased prevalence of misinformation
tweets. Disgust was the predominant emotion associated with misinformation
tweets in the US, while anticipation was the predominant emotion in India. For
India, the misinformation rate exhibited a lead relationship with vaccination,
while in the US it lagged behind vaccination. Our study deciphered that
emotions acted as differential carriers of misinformation across geography and
time. These carriers can be monitored to develop strategic interventions for
countering misinformation, leading to improved public health.
</summary>
    <author>
      <name>Ridam Pal</name>
    </author>
    <author>
      <name>Sanjana S</name>
    </author>
    <author>
      <name>Deepak Mahto</name>
    </author>
    <author>
      <name>Kriti Agrawal</name>
    </author>
    <author>
      <name>Gopal Mengi</name>
    </author>
    <author>
      <name>Sargun Nagpal</name>
    </author>
    <author>
      <name>Akshaya Devadiga</name>
    </author>
    <author>
      <name>Tavpritesh Sethi</name>
    </author>
    <link href="http://arxiv.org/abs/2306.13954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.13954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05243v1</id>
    <updated>2021-04-12T07:25:49Z</updated>
    <published>2021-04-12T07:25:49Z</published>
    <title>On Unifying Misinformation Detection</title>
    <summary>  In this paper, we introduce UnifiedM2, a general-purpose misinformation model
that jointly models multiple domains of misinformation with a single, unified
setup. The model is trained to handle four tasks: detecting news bias,
clickbait, fake news, and verifying rumors. By grouping these tasks together,
UnifiedM2learns a richer representation of misinformation, which leads to
state-of-the-art or comparable performance across all tasks. Furthermore, we
demonstrate that UnifiedM2's learned representation is helpful for few-shot
learning of unseen misinformation tasks/datasets and model's generalizability
to unseen events.
</summary>
    <author>
      <name>Nayeon Lee</name>
    </author>
    <author>
      <name>Belinda Z. Li</name>
    </author>
    <author>
      <name>Sinong Wang</name>
    </author>
    <author>
      <name>Pascale Fung</name>
    </author>
    <author>
      <name>Hao Ma</name>
    </author>
    <author>
      <name>Wen-tau Yih</name>
    </author>
    <author>
      <name>Madian Khabsa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NAACL2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.05243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.19866v1</id>
    <updated>2024-11-29T17:27:54Z</updated>
    <published>2024-11-29T17:27:54Z</published>
    <title>Misinformation Dissemination: Effects of Network Density in Segregated
  Communities</title>
    <summary>  Understanding the relationship between network features and misinformation
propagation is crucial for mitigating the spread of false information. Here, we
investigate how network density and segregation affect the dissemination of
misinformation using a susceptible-infectious-recovered framework. We find that
a higher density consistently increases the proportion of misinformation
believers. In segregated networks, our results reveal that minorities affect
the majority: denser minority groups increase the number of believers in the
majority, demonstrating how the structure of a segregated minority can
influence misinformation dynamics within the majority group.
</summary>
    <author>
      <name>Soroush Karimi</name>
    </author>
    <author>
      <name>Marcos Oliveira</name>
    </author>
    <author>
      <name>Diogo Pacheco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, Social Simulation Conference 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.19866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.19866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.16210v1</id>
    <updated>2025-01-27T17:01:41Z</updated>
    <published>2025-01-27T17:01:41Z</published>
    <title>New Frontiers in Fighting Misinformation</title>
    <summary>  Despite extensive research and development of tools and technologies for
misinformation tracking and detection, we often find ourselves largely on the
losing side of the battle against misinformation. In an era where
misinformation poses a substantial threat to public discourse, trust in
information sources, and societal and political stability, it is imperative
that we regularly revisit and reorient our work strategies. While we have made
significant strides in understanding how and why misinformation spreads, we
must now broaden our focus and explore how technology can help realise new
approaches to address this complex challenge more efficiently.
</summary>
    <author>
      <name>Harith Alani</name>
    </author>
    <author>
      <name>Gr√©goire Burel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages - Viewpoint article</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.16210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.16210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.12627v1</id>
    <updated>2025-03-16T19:43:25Z</updated>
    <published>2025-03-16T19:43:25Z</published>
    <title>Online Misinformation Detection in Live Streaming Videos</title>
    <summary>  Online misinformation detection is an important issue and methods are
proposed to detect and curb misinformation in various forms. However, previous
studies are conducted in an offline manner. We claim a realistic misinformation
detection setting that has not been studied yet is online misinformation
detection in live streaming videos (MDLS). In the proposal, we formulate the
problem of MDLS and illustrate the importance and the challenge of the task.
Besides, we propose feasible ways of developing the problem into AI challenges
as well as potential solutions to the problem.
</summary>
    <author>
      <name>Rui Cao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First prize winner in the Smart City Challenge in the 16th ACM
  international WSDM conference(WSDM), 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.12627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.12627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00397v1</id>
    <updated>2019-09-01T13:32:08Z</updated>
    <published>2019-09-01T13:32:08Z</published>
    <title>Misinformation spreading on correlated multiplex networks</title>
    <summary>  The numerous expanding online social networks offer fast channels for
misinformation spreading, which could have a serious impact on socioeconomic
systems. Researchers across multiple areas have paid attention to this issue
with a view of addressing it. However, no systematical theoretical study has
been performed to date on observing misinformation spreading on correlated
multiplex networks. In this study, we propose a multiplex network-based
misinformation spreading model, considering the fact that each individual can
obtain misinformation from multiple platforms. Subsequently, we develop a
heterogeneous edge-base compartmental theory to comprehend the spreading
dynamics of our proposed model. In addition, we establish an analytical method
based on stability analysis to obtain the misinformation outbreak threshold. On
the basis of these theories, we finally analyze the influence of different
dynamical and structural parameters on the misinformation spreading dynamics.
Results show that the misinformation outbreak size $R(\infty)$ grows
continuously with the effective transmission probability $\beta$ once $\beta$
exceeds a certain value, that is, the outbreak threshold $\beta_c$. A large
average degrees, strong degree heterogeneity, or positive inter-layer
correlation will reduce $\beta_c$, accelerating the outbreak of misinformation.
Besides, increasing the degree heterogeneity or a more positive inter-layer
correlation will both enlarge (reduce) $R(\infty)$ for small (large) values of
$\beta$. Our systematic theoretical analysis results agree well with the
numerical simulation results. Our proposed model and accurate theoretical
analysis will serve as a useful framework to understand and predict the
spreading dynamics of misinformation on multiplex networks, and thereby pave
the way to address this serious issue.
</summary>
    <author>
      <name>Jiajun Xian</name>
    </author>
    <author>
      <name>Dan Yang</name>
    </author>
    <author>
      <name>Liming Pan</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Zhen Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.5121394</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.5121394" rel="related"/>
    <link href="http://arxiv.org/abs/1909.00397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00791v4</id>
    <updated>2020-09-19T07:11:39Z</updated>
    <published>2020-08-03T11:44:22Z</published>
    <title>Characterizing COVID-19 Misinformation Communities Using a Novel Twitter
  Dataset</title>
    <summary>  From conspiracy theories to fake cures and fake treatments, COVID-19 has
become a hot-bed for the spread of misinformation online. It is more important
than ever to identify methods to debunk and correct false information online.
In this paper, we present a methodology and analyses to characterize the two
competing COVID-19 misinformation communities online: (i) misinformed users or
users who are actively posting misinformation, and (ii) informed users or users
who are actively spreading true information, or calling out misinformation. The
goals of this study are two-fold: (i) collecting a diverse set of annotated
COVID-19 Twitter dataset that can be used by the research community to conduct
meaningful analysis; and (ii) characterizing the two target communities in
terms of their network structure, linguistic patterns, and their membership in
other communities. Our analyses show that COVID-19 misinformed communities are
denser, and more organized than informed communities, with a possibility of a
high volume of the misinformation being part of disinformation campaigns. Our
analyses also suggest that a large majority of misinformed users may be
anti-vaxxers. Finally, our sociolinguistic analyses suggest that COVID-19
informed users tend to use more narratives than misinformed users.
</summary>
    <author>
      <name>Shahan Ali Memon</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, In Proceedings of The 5th International Workshop on Mining
  Actionable Insights from Social Networks (MAISoN 2020), co-located with CIKM</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.00791v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00791v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.09532v1</id>
    <updated>2021-09-20T13:31:15Z</updated>
    <published>2021-09-20T13:31:15Z</published>
    <title>Characterizing User Susceptibility to COVID-19 Misinformation on Twitter</title>
    <summary>  Though significant efforts such as removing false claims and promoting
reliable sources have been increased to combat COVID-19 "misinfodemic", it
remains an unsolved societal challenge if lacking a proper understanding of
susceptible online users, i.e., those who are likely to be attracted by,
believe and spread misinformation. This study attempts to answer {\it who}
constitutes the population vulnerable to the online misinformation in the
pandemic, and what are the robust features and short-term behavior signals that
distinguish susceptible users from others. Using a 6-month longitudinal user
panel on Twitter collected from a geopolitically diverse network-stratified
samples in the US, we distinguish different types of users, ranging from social
bots to humans with various level of engagement with COVID-related
misinformation. We then identify users' online features and situational
predictors that correlate with their susceptibility to COVID-19 misinformation.
This work brings unique contributions: First, contrary to the prior studies on
bot influence, our analysis shows that social bots' contribution to
misinformation sharing was surprisingly low, and human-like users'
misinformation behaviors exhibit heterogeneity and temporal variability. While
the sharing of misinformation was highly concentrated, the risk of occasionally
sharing misinformation for average users remained alarmingly high. Second, our
findings highlight the political sensitivity activeness and responsiveness to
emotionally-charged content among susceptible users. Third, we demonstrate a
feasible solution to efficiently predict users' transient susceptibility solely
based on their short-term news consumption and exposure from their networks.
Our work has an implication in designing effective intervention mechanism to
mitigate the misinformation dissipation.
</summary>
    <author>
      <name>Xian Teng</name>
    </author>
    <author>
      <name>Yu-Ru Lin</name>
    </author>
    <author>
      <name>Wen-Ting Chung</name>
    </author>
    <author>
      <name>Ang Li</name>
    </author>
    <author>
      <name>Adriana Kovashka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted into ICWSM 2022, 9 figures (main text)</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.09532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09665v1</id>
    <updated>2021-01-24T07:21:50Z</updated>
    <published>2021-01-24T07:21:50Z</published>
    <title>Corrective Information Does Not Necessarily Curb Social Disruption</title>
    <summary>  The spread of misinformation can cause social confusion. The authenticity of
information on a social networking service (SNS) is unknown, and false
information can be easily spread. Consequently, many studies have been
conducted on methods to control the spread of misinformation on social
networking sites. However, few studies have examined the impact of the spread
of misinformation and its corrections on society. This study models the impact
of the reduction of misinformation and the diffusion of corrective information
on social disruption, and it identifies the features of this impact. In this
study, we analyzed misinformation regarding the shortage of toilet paper during
the 2020 COVID-19 epidemic, its corrections, and the excessive purchasing
caused by this information. First, we analyze the amount of misinformation and
corrective information spread on SNS, and we create a regression model to
estimate the real-world impact of misinformation and its correction. This model
is used to analyze the change in real-world impact corresponding to the change
in the diffusion of misinformation and corrective information. Our analysis
shows that the corrective information was spread to a much greater extent than
the misinformation. In addition, our model reveals that the corrective
information was what caused the excessive purchasing behavior. As a result of
our further analysis, we found that the amount of diffusion of corrective
information required to minimize the impact on the real world depends on the
amount of the diffusion of misinformation.
</summary>
    <author>
      <name>Ryusuke Iizuka</name>
    </author>
    <author>
      <name>Fujio Toriumi</name>
    </author>
    <author>
      <name>Mao Nishiguchi</name>
    </author>
    <author>
      <name>Masanori Takano</name>
    </author>
    <author>
      <name>Mitsuo Yoshida</name>
    </author>
    <link href="http://arxiv.org/abs/2101.09665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.02314v3</id>
    <updated>2021-11-17T03:51:30Z</updated>
    <published>2021-08-04T23:27:10Z</published>
    <title>Automatic Detection of COVID-19 Vaccine Misinformation with Graph Link
  Prediction</title>
    <summary>  Enormous hope in the efficacy of vaccines became recently a successful
reality in the fight against the COVID-19 pandemic. However, vaccine hesitancy,
fueled by exposure to social media misinformation about COVID-19 vaccines
became a major hurdle. Therefore, it is essential to automatically detect where
misinformation about COVID-19 vaccines on social media is spread and what kind
of misinformation is discussed, such that inoculation interventions can be
delivered at the right time and in the right place, in addition to
interventions designed to address vaccine hesitancy. This paper is addressing
the first step in tackling hesitancy against COVID-19 vaccines, namely the
automatic detection of known misinformation about the vaccines on Twitter, the
social media platform that has the highest volume of conversations about
COVID-19 and its vaccines. We present CoVaxLies, a new dataset of tweets judged
relevant to several misinformation targets about COVID-19 vaccines on which a
novel method of detecting misinformation was developed. Our method organizes
CoVaxLies in a Misinformation Knowledge Graph as it casts misinformation
detection as a graph link prediction problem. The misinformation detection
method detailed in this paper takes advantage of the link scoring functions
provided by several knowledge embedding methods. The experimental results
demonstrate the superiority of this method when compared with
classification-based methods, widely used currently.
</summary>
    <author>
      <name>Maxwell A. Weinzierl</name>
    </author>
    <author>
      <name>Sanda M. Harabagiu</name>
    </author>
    <link href="http://arxiv.org/abs/2108.02314v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.02314v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.12296v3</id>
    <updated>2022-08-17T18:18:02Z</updated>
    <published>2021-10-23T20:45:24Z</published>
    <title>Cybersecurity Misinformation Detection on Social Media: Case Studies on
  Phishing Reports and Zoom's Threats</title>
    <summary>  Prior work has extensively studied misinformation related to news, politics,
and health, however, misinformation can also be about technological topics.
While less controversial, such misinformation can severely impact companies'
reputations and revenues, and users' online experiences. Recently, social media
has also been increasingly used as a novel source of knowledgebase for
extracting timely and relevant security threats, which are fed to the threat
intelligence systems for better performance. However, with possible campaigns
spreading false security threats, these systems can become vulnerable to
poisoning attacks. In this work, we proposed novel approaches for detecting
misinformation about cybersecurity and privacy threats on social media,
focusing on two topics with different types of misinformation: phishing
websites and Zoom's security &amp; privacy threats. We developed a framework for
detecting inaccurate phishing claims on Twitter. Using this framework, we could
label about 9% of URLs and 22% of phishing reports as misinformation. We also
proposed another framework for detecting misinformation related to Zoom's
security and privacy threats on multiple platforms. Our classifiers showed
great performance with more than 98% accuracy. Employing these classifiers on
the posts from Facebook, Instagram, Reddit, and Twitter, we found respectively
that about 18%, 3%, 4%, and 3% of posts were misinformation. In addition, we
studied the characteristics of misinformation posts, their authors, and their
timelines, which helped us identify campaigns.
</summary>
    <author>
      <name>Mohit Singhal</name>
    </author>
    <author>
      <name>Nihal Kumarswamy</name>
    </author>
    <author>
      <name>Shreyasi Kinhekar</name>
    </author>
    <author>
      <name>Shirin Nilizadeh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1609/icwsm.v17i1.22189</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1609/icwsm.v17i1.22189" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the 17th International AAAI Conference on Web and Social
  Media (ICWSM 2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.12296v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.12296v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01379v1</id>
    <updated>2021-12-02T16:12:38Z</updated>
    <published>2021-12-02T16:12:38Z</published>
    <title>Sentinel node approach to monitoring online COVID-19 misinformation</title>
    <summary>  Understanding how different online communities engage with COVID-19
misinformation is critical for public health response, as misinformation
confined to a small, isolated community of users poses a different public
health risk than misinformation being consumed by a large population spanning
many diverse communities. Here we take a longitudinal approach that leverages
tools from network science to study COVID-19 misinformation on Twitter. Our
approach provides a means to examine the breadth of misinformation engagement
using modest data needs and computational resources. We identify influential
accounts from different Twitter communities discussing COVID-19, and follow
these `sentinel nodes' longitudinally from July 2020 to January 2021. We
characterize sentinel nodes in terms of a linked-media preference score, and
use a standardized similarity score to examine alignment of tweets within and
between communities. We find that media preference is strongly correlated with
the amount of misinformation propagated by sentinel nodes. Engagement with
sensationalist misinformation topics is largely confined to a cluster of
sentinel nodes that includes influential conspiracy theorist accounts, while
misinformation relating to COVID-19 severity generated widespread engagement
across multiple communities. Our findings indicate that misinformation
downplaying COVID-19 severity is of particular concern for public health
response.
</summary>
    <author>
      <name>Matthew T. Osborne</name>
    </author>
    <author>
      <name>Samuel S. Malloy</name>
    </author>
    <author>
      <name>Erik C. Nisbet</name>
    </author>
    <author>
      <name>Robert M. Bond</name>
    </author>
    <author>
      <name>Joseph H. Tien</name>
    </author>
    <link href="http://arxiv.org/abs/2112.01379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.05289v1</id>
    <updated>2022-11-10T01:52:12Z</updated>
    <published>2022-11-10T01:52:12Z</published>
    <title>Combating Health Misinformation in Social Media: Characterization,
  Detection, Intervention, and Open Issues</title>
    <summary>  Social media has been one of the main information consumption sources for the
public, allowing people to seek and spread information more quickly and easily.
However, the rise of various social media platforms also enables the
proliferation of online misinformation. In particular, misinformation in the
health domain has significant impacts on our society such as the COVID-19
infodemic. Therefore, health misinformation in social media has become an
emerging research direction that attracts increasing attention from researchers
of different disciplines. Compared to misinformation in other domains, the key
differences of health misinformation include the potential of causing actual
harm to humans' bodies and even lives, the hardness to identify for normal
people, and the deep connection with medical science. In addition, health
misinformation on social media has distinct characteristics from conventional
channels such as television on multiple dimensions including the generation,
dissemination, and consumption paradigms. Because of the uniqueness and
importance of combating health misinformation in social media, we conduct this
survey to further facilitate interdisciplinary research on this problem. In
this survey, we present a comprehensive review of existing research about
online health misinformation in different disciplines. Furthermore, we also
systematically organize the related literature from three perspectives:
characterization, detection, and intervention. Lastly, we conduct a deep
discussion on the pressing open issues of combating health misinformation in
social media and provide future directions for multidisciplinary researchers.
</summary>
    <author>
      <name>Canyu Chen</name>
    </author>
    <author>
      <name>Haoran Wang</name>
    </author>
    <author>
      <name>Matthew Shapiro</name>
    </author>
    <author>
      <name>Yunyu Xiao</name>
    </author>
    <author>
      <name>Fei Wang</name>
    </author>
    <author>
      <name>Kai Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 241 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.00957v1</id>
    <updated>2023-05-01T17:44:52Z</updated>
    <published>2023-05-01T17:44:52Z</published>
    <title>Behavioral Forensics in Social Networks: Identifying Misinformation,
  Disinformation and Refutation Spreaders Using Machine Learning</title>
    <summary>  With the ever-increasing spread of misinformation on online social networks,
it has become very important to identify the spreaders of misinformation
(unintentional), disinformation (intentional), and misinformation refutation.
It can help in educating the first, stopping the second, and soliciting the
help of the third category, respectively, in the overall effort to counter
misinformation spread. Existing research to identify spreaders is limited to
binary classification (true vs false information spreaders). However, people's
intention (whether naive or malicious) behind sharing misinformation can only
be understood after observing their behavior after exposure to both the
misinformation and its refutation which the existing literature lacks to
consider. In this paper, we propose a labeling mechanism to label people as one
of the five defined categories based on the behavioral actions they exhibit
when exposed to misinformation and its refutation. However, everyone does not
show behavioral actions but is part of a network. Therefore, we use their
network features, extracted through deep learning-based graph embedding models,
to train a machine learning model for the prediction of the classes. We name
our approach behavioral forensics since it is an evidence-based investigation
of suspicious behavior which is spreading misinformation and disinformation in
our case. After evaluating our proposed model on a real-world Twitter dataset,
we achieved 77.45% precision and 75.80% recall in detecting the malicious
actors, who shared the misinformation even after receiving its refutation. Such
behavior shows intention, and hence these actors can rightfully be called
agents of disinformation spread.
</summary>
    <author>
      <name>Euna Mehnaz Khan</name>
    </author>
    <author>
      <name>Ayush Ram</name>
    </author>
    <author>
      <name>Bhavtosh Rath</name>
    </author>
    <author>
      <name>Emily Vraga</name>
    </author>
    <author>
      <name>Jaideep Srivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, and 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.00957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.00957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.13788v5</id>
    <updated>2024-04-23T22:59:13Z</updated>
    <published>2023-09-25T00:45:07Z</published>
    <title>Can LLM-Generated Misinformation Be Detected?</title>
    <summary>  The advent of Large Language Models (LLMs) has made a transformative impact.
However, the potential that LLMs such as ChatGPT can be exploited to generate
misinformation has posed a serious concern to online safety and public trust. A
fundamental research question is: will LLM-generated misinformation cause more
harm than human-written misinformation? We propose to tackle this question from
the perspective of detection difficulty. We first build a taxonomy of
LLM-generated misinformation. Then we categorize and validate the potential
real-world methods for generating misinformation with LLMs. Then, through
extensive empirical investigation, we discover that LLM-generated
misinformation can be harder to detect for humans and detectors compared to
human-written misinformation with the same semantics, which suggests it can
have more deceptive styles and potentially cause more harm. We also discuss the
implications of our discovery on combating misinformation in the age of LLMs
and the countermeasures.
</summary>
    <author>
      <name>Canyu Chen</name>
    </author>
    <author>
      <name>Kai Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Proceedings of ICLR 2024. 9 pages for main paper, 40
  pages including appendix. The code, results, dataset for this paper and more
  resources on "LLMs Meet Misinformation" have been released on the project
  website: https://llm-misinformation.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.13788v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.13788v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.18195v1</id>
    <updated>2023-11-30T02:27:34Z</updated>
    <published>2023-11-30T02:27:34Z</published>
    <title>COVID-19 Vaccine Misinformation in Middle Income Countries</title>
    <summary>  This paper introduces a multilingual dataset of COVID-19 vaccine
misinformation, consisting of annotated tweets from three middle-income
countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset
includes annotations for 5,952 tweets, assessing their relevance to COVID-19
vaccines, presence of misinformation, and the themes of the misinformation. To
address challenges posed by domain specificity, the low-resource setting, and
data imbalance, we adopt two approaches for developing COVID-19 vaccine
misinformation detection models: domain-specific pre-training and text
augmentation using a large language model. Our best misinformation detection
models demonstrate improvements ranging from 2.7 to 15.9 percentage points in
macro F1-score compared to the baseline models. Additionally, we apply our
misinformation detection models in a large-scale study of 19 million unlabeled
tweets from the three countries between 2020 and 2022, showcasing the practical
application of our dataset and models for detecting and analyzing vaccine
misinformation in multiple countries and languages. Our analysis indicates that
percentage changes in the number of new COVID-19 cases are positively
associated with COVID-19 vaccine misinformation rates in a staggered manner for
Brazil and Indonesia, and there are significant positive associations between
the misinformation rates across the three countries.
</summary>
    <author>
      <name>Jongin Kim</name>
    </author>
    <author>
      <name>Byeo Rhee Bak</name>
    </author>
    <author>
      <name>Aditya Agrawal</name>
    </author>
    <author>
      <name>Jiaxi Wu</name>
    </author>
    <author>
      <name>Veronika J. Wirtz</name>
    </author>
    <author>
      <name>Traci Hong</name>
    </author>
    <author>
      <name>Derry Wijaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2023 (Main conference), 9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.18195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.18195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.14171v3</id>
    <updated>2024-04-08T08:30:06Z</updated>
    <published>2024-03-21T06:47:28Z</published>
    <title>MMIDR: Teaching Large Language Model to Interpret Multimodal
  Misinformation via Knowledge Distillation</title>
    <summary>  Automatic detection of multimodal misinformation has gained a widespread
attention recently. However, the potential of powerful Large Language Models
(LLMs) for multimodal misinformation detection remains underexplored. Besides,
how to teach LLMs to interpret multimodal misinformation in cost-effective and
accessible way is still an open question. To address that, we propose MMIDR, a
framework designed to teach LLMs in providing fluent and high-quality textual
explanations for their decision-making process of multimodal misinformation. To
convert multimodal misinformation into an appropriate instruction-following
format, we present a data augmentation perspective and pipeline. This pipeline
consists of a visual information processing module and an evidence retrieval
module. Subsequently, we prompt the proprietary LLMs with processed contents to
extract rationales for interpreting the authenticity of multimodal
misinformation. Furthermore, we design an efficient knowledge distillation
approach to distill the capability of proprietary LLMs in explaining multimodal
misinformation into open-source LLMs. To explore several research questions
regarding the performance of LLMs in multimodal misinformation detection tasks,
we construct an instruction-following multimodal misinformation dataset and
conduct comprehensive experiments. The experimental findings reveal that our
MMIDR exhibits sufficient detection performance and possesses the capacity to
provide compelling rationales to support its assessments.
</summary>
    <author>
      <name>Longzheng Wang</name>
    </author>
    <author>
      <name>Xiaohan Xu</name>
    </author>
    <author>
      <name>Lei Zhang</name>
    </author>
    <author>
      <name>Jiarui Lu</name>
    </author>
    <author>
      <name>Yongxiu Xu</name>
    </author>
    <author>
      <name>Hongbo Xu</name>
    </author>
    <author>
      <name>Minghao Tang</name>
    </author>
    <author>
      <name>Chuang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.14171v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.14171v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.06122v2</id>
    <updated>2025-04-08T16:41:24Z</updated>
    <published>2024-11-09T09:12:39Z</published>
    <title>Characteristics of Political Misinformation Over the Past Decade</title>
    <summary>  Although misinformation tends to spread online, it can have serious
real-world consequences. In order to develop automated tools to detect and
mitigate the impact of misinformation, researchers must leverage algorithms
that can adapt to the modality (text, images and video), the source, and the
content of the false information. However, these characteristics tend to change
dynamically across time, making it challenging to develop robust algorithms to
fight misinformation spread. Therefore, this paper uses natural language
processing to find common characteristics of political misinformation over a
twelve year period. The results show that misinformation has increased
dramatically in recent years and that it has increasingly started to be shared
from sources with primary information modalities of text and images (e.g.,
Facebook and Instagram), although video sharing sources containing
misinformation are starting to increase (e.g., TikTok). Moreover, it was
discovered that statements expressing misinformation contain more negative
sentiment than accurate information. However, the sentiment associated with
both accurate and inaccurate information has trended downward, indicating a
generally more negative tone in political statements across time. Finally,
recurring misinformation categories were uncovered that occur over multiple
years, which may imply that people tend to share inaccurate statements around
information they fear or don't understand (Science and Medicine, Crime,
Religion), impacts them directly (Policy, Election Integrity, Economic) or
Public Figures who are salient in their daily lives. Together, it is hoped that
these insights will assist researchers in developing algorithms that are
temporally invariant and capable of detecting and mitigating misinformation
across time.
</summary>
    <author>
      <name>Erik J Schlicht</name>
    </author>
    <link href="http://arxiv.org/abs/2411.06122v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.06122v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.04999v1</id>
    <updated>2024-12-06T12:52:18Z</updated>
    <published>2024-12-06T12:52:18Z</published>
    <title>'Debunk-It-Yourself': Health Professionals' Strategies for Responding to
  Misinformation on TikTok</title>
    <summary>  Misinformation is "sticky" in nature, requiring a considerable effort to undo
its influence. One such effort is debunking or exposing the falsity of
information. As an abundance of misinformation is on social media, platforms do
bear some debunking responsibility in order to preserve their trustworthiness
as information providers. A subject of interpretation, platforms poorly meet
this responsibility and allow dangerous health misinformation to influence many
of their users. This open route to harm did not sit well with health
professional users, who recently decided to take the debunking into their own
hands. To study this individual debunking effort - which we call
'Debunk-It-Yourself (DIY)' - we conducted an exploratory survey n=14 health
professionals who wage a misinformation counter-influence campaign through
videos on TikTok. We focused on two topics, nutrition and mental health, which
are the ones most often subjected to misinformation on the platform. Our
thematic analysis reveals that the counterinfluence follows a common process of
initiation, selection, creation, and "stitching" or duetting a debunking video
with a misinformation video. The 'Debunk-It-Yourself' effort was underpinned by
three unique aspects: (i) it targets trending misinformation claims perceived
to be of direct harm to people's health; (ii) it offers a symmetric response to
the misinformation; and (iii) it is strictly based on scientific evidence and
claimed clinical experience. Contrasting the 'Debunk-It-Yourself' effort with
the one TikTok and other platforms (reluctantly) put in moderation, we offer
recommendations for a structured response against the misinformation's
influence by the users themselves.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Jennifer Vander Loop</name>
    </author>
    <author>
      <name>Amy Devine</name>
    </author>
    <author>
      <name>Peter Jachim</name>
    </author>
    <author>
      <name>Sanchari Das</name>
    </author>
    <link href="http://arxiv.org/abs/2412.04999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.04999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09103v1</id>
    <updated>2025-03-12T06:43:25Z</updated>
    <published>2025-03-12T06:43:25Z</published>
    <title>VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for
  Detecting LLM-Generated Vaccine Misinformation</title>
    <summary>  Recent advancements in Large Language Models (LLMs) have significantly
improved text generation capabilities. However, they also present challenges,
particularly in generating vaccine-related misinformation, which poses risks to
public health. Despite research on human-authored misinformation, a notable gap
remains in understanding how LLMs contribute to vaccine misinformation and how
best to detect it. Existing benchmarks often overlook vaccine-specific
misinformation and the diverse roles of misinformation spreaders. This paper
introduces VaxGuard, a novel dataset designed to address these challenges.
VaxGuard includes vaccine-related misinformation generated by multiple LLMs and
provides a comprehensive framework for detecting misinformation across various
roles. Our findings show that GPT-3.5 and GPT-4o consistently outperform other
LLMs in detecting misinformation, especially when dealing with subtle or
emotionally charged narratives. On the other hand, PHI3 and Mistral show lower
performance, struggling with precision and recall in fear-driven contexts.
Additionally, detection performance tends to decline as input text length
increases, indicating the need for improved methods to handle larger content.
These results highlight the importance of role-specific detection strategies
and suggest that VaxGuard can serve as a key resource for improving the
detection of LLM-generated vaccine misinformation.
</summary>
    <author>
      <name>Syed Talal Ahmad</name>
    </author>
    <author>
      <name>Haohui Lu</name>
    </author>
    <author>
      <name>Sidong Liu</name>
    </author>
    <author>
      <name>Annie Lau</name>
    </author>
    <author>
      <name>Amin Beheshti</name>
    </author>
    <author>
      <name>Mark Dras</name>
    </author>
    <author>
      <name>Usman Naseem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.09103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.06486v2</id>
    <updated>2018-09-20T20:49:08Z</updated>
    <published>2018-09-18T00:22:26Z</published>
    <title>On Misinformation Containment in Online Social Networks</title>
    <summary>  The widespread online misinformation could cause public panic and serious
economic damages. The misinformation containment problem aims at limiting the
spread of misinformation in online social networks by launching competing
campaigns. Motivated by realistic scenarios, we present the first analysis of
the misinformation containment problem for the case when an arbitrary number of
cascades are allowed. This paper makes four contributions. First, we provide a
formal model for multi-cascade diffusion and introduce an important concept
called as cascade priority. Second, we show that the misinformation containment
problem cannot be approximated within a factor of
$\Omega(2^{\log^{1-\epsilon}n^4})$ in polynomial time unless $NP \subseteq
DTIME(n^{\polylog{n}})$. Third, we introduce several types of cascade priority
that are frequently seen in real social networks. Finally, we design novel
algorithms for solving the misinformation containment problem. The
effectiveness of the proposed algorithm is supported by encouraging
experimental results.
</summary>
    <author>
      <name>Guangmo Tong</name>
    </author>
    <author>
      <name>Weili Wu</name>
    </author>
    <author>
      <name>Ding-Zhu Du</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.06486v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.06486v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.08743v2</id>
    <updated>2022-04-25T17:37:48Z</updated>
    <published>2020-10-17T08:34:57Z</published>
    <title>Drink Bleach or Do What Now? Covid-HeRA: A Study of Risk-Informed Health
  Decision Making in the Presence of COVID-19 Misinformation</title>
    <summary>  Given the widespread dissemination of inaccurate medical advice related to
the 2019 coronavirus pandemic (COVID-19), such as fake remedies, treatments and
prevention suggestions, misinformation detection has emerged as an open problem
of high importance and interest for the research community. Several works study
health misinformation detection, yet little attention has been given to the
perceived severity of misinformation posts. In this work, we frame health
misinformation as a risk assessment task. More specifically, we study the
severity of each misinformation story and how readers perceive this severity,
i.e., how harmful a message believed by the audience can be and what type of
signals can be used to recognize potentially malicious fake news and detect
refuted claims. To address our research questions, we introduce a new benchmark
dataset, accompanied by detailed data analysis. We evaluate several traditional
and state-of-the-art models and show there is a significant gap in performance
when applying traditional misinformation classification models to this task. We
conclude with open challenges and future directions.
</summary>
    <author>
      <name>Arkin Dharawat</name>
    </author>
    <author>
      <name>Ismini Lourentzou</name>
    </author>
    <author>
      <name>Alex Morales</name>
    </author>
    <author>
      <name>ChengXiang Zhai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to AAAI ICWSM'22 Datasets Track</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.08743v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08743v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.00885v3</id>
    <updated>2020-11-03T20:37:11Z</updated>
    <published>2020-05-22T19:08:14Z</published>
    <title>CoAID: COVID-19 Healthcare Misinformation Dataset</title>
    <summary>  As the COVID-19 virus quickly spreads around the world, unfortunately,
misinformation related to COVID-19 also gets created and spreads like wild
fire. Such misinformation has caused confusion among people, disruptions in
society, and even deadly consequences in health problems. To be able to
understand, detect, and mitigate such COVID-19 misinformation, therefore, has
not only deep intellectual values but also huge societal impacts. To help
researchers combat COVID-19 health misinformation, therefore, we present CoAID
(Covid-19 heAlthcare mIsinformation Dataset), with diverse COVID-19 healthcare
misinformation, including fake news on websites and social platforms, along
with users' social engagement about such news. CoAID includes 4,251 news,
296,000 related user engagements, 926 social platform posts about COVID-19, and
ground truth labels. The dataset is available at:
https://github.com/cuilimeng/CoAID.
</summary>
    <author>
      <name>Limeng Cui</name>
    </author>
    <author>
      <name>Dongwon Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2006.00885v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00885v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00188v1</id>
    <updated>2022-04-30T07:29:48Z</updated>
    <published>2022-04-30T07:29:48Z</published>
    <title>Finding Strategies Against Misinformation in Social Media: A Qualitative
  Study</title>
    <summary>  Misinformation spread through social media has become a fundamental challenge
in modern society. Recent studies have evaluated various strategies for
addressing this problem, such as by modifying social media platforms or
educating people about misinformation, to varying degrees of success. Our goal
is to develop a new strategy for countering misinformation: intelligent tools
that encourage social media users to foster metacognitive skills "in the wild."
As a first step, we conducted focus groups with social media users to discover
how they can be best supported in combating misinformation. Qualitative
analyses of the discussions revealed that people find it difficult to detect
misinformation. Findings also indicated a need for but lack of resources to
support cross-validation of information. Moreover, misinformation had a nuanced
emotional impact on people. Suggestions for the design of intelligent tools
that support social media users in information selection, information
engagement, and emotional response management are presented.
</summary>
    <author>
      <name>Jacqueline Urakami</name>
    </author>
    <author>
      <name>Yeongdae Kim</name>
    </author>
    <author>
      <name>Hiroki Oura</name>
    </author>
    <author>
      <name>Katie Seaborn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3491101.3519661</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3491101.3519661" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CHI EA '22</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CHI EA '22: CHI Conference on Human Factors in Computing Systems
  Extended Abstracts, April 2022, Article No.: 242</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.00188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01243v1</id>
    <updated>2022-05-02T22:47:49Z</updated>
    <published>2022-05-02T22:47:49Z</published>
    <title>Meaningful Context, a Red Flag, or Both? Users' Preferences for Enhanced
  Misinformation Warnings on Twitter</title>
    <summary>  Warning users about misinformation on social media is not a simple usability
task. Soft moderation has to balance between debunking falsehoods and avoiding
moderation bias while preserving the social media consumption flow. Platforms
thus employ minimally distinguishable warning tags with generic text under a
suspected misinformation content. This approach resulted in an unfavorable
outcome where the warnings "backfired" and users believed the misinformation
more, not less. In response, we developed enhancements to the misinformation
warnings where users are advised on the context of the information hazard and
exposed to standard warning iconography. We ran an A/B evaluation with the
Twitter's original warning tags in a 337 participant usability study. The
majority of the participants preferred the enhancements as a nudge toward
recognizing and avoiding misinformation. The enhanced warning tags were most
favored by the politically left-leaning and to a lesser degree moderate
participants, but they also appealed to roughly a third of the right-leaning
participants. The education level was the only demographic factor shaping
participants' preferences. We use our findings to propose user-tailored
improvements in the soft moderation of misinformation on social media.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Amy Devine</name>
    </author>
    <author>
      <name>Emma Pieroni</name>
    </author>
    <author>
      <name>Peter Jacnim</name>
    </author>
    <link href="http://arxiv.org/abs/2205.01243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.12758v2</id>
    <updated>2020-12-15T01:13:08Z</updated>
    <published>2020-11-25T14:22:36Z</published>
    <title>Encounters with Visual Misinformation and Labels Across Platforms: An
  Interview and Diary Study to Inform Ecosystem Approaches to Misinformation
  Interventions</title>
    <summary>  Since 2016, the amount of academic research with the keyword "misinformation"
has more than doubled [2]. This research often focuses on article headlines
shown in artificial testing environments, yet misinformation largely spreads
through images and video posts shared in highly-personalized platform contexts.
A foundation of qualitative research is necessary to begin filling this gap to
ensure platforms' visual misinformation interventions are aligned with users'
needs and understanding of information in their personal contexts, across
platforms. In two studies, we combined in-depth interviews (n=15) with diary
and co-design methods (n=23) to investigate how a broad mix of Americans
exposed to misinformation during COVID-19 understand their visual information
environments, including encounters with interventions such as Facebook
fact-checking labels. Analysis reveals a deep division in user attitudes about
platform labeling interventions for visual information which are perceived by
many as overly paternalistic, biased, and punitive. Alongside these findings,
we discuss our methods as a model for continued independent qualitative
research on cross-platform user experiences of misinformation that inform
interventions.
</summary>
    <author>
      <name>Emily Saltz</name>
    </author>
    <author>
      <name>Claire Leibowicz</name>
    </author>
    <author>
      <name>Claire Wardle</name>
    </author>
    <link href="http://arxiv.org/abs/2011.12758v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12758v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.05626v1</id>
    <updated>2021-01-09T22:52:21Z</updated>
    <published>2021-01-09T22:52:21Z</published>
    <title>Eating Garlic Prevents COVID-19 Infection: Detecting Misinformation on
  the Arabic Content of Twitter</title>
    <summary>  The rapid growth of social media content during the current pandemic provides
useful tools for disseminating information which has also become a root for
misinformation. Therefore, there is an urgent need for fact-checking and
effective techniques for detecting misinformation in social media. In this
work, we study the misinformation in the Arabic content of Twitter. We
construct a large Arabic dataset related to COVID-19 misinformation and
gold-annotate the tweets into two categories: misinformation or not. Then, we
apply eight different traditional and deep machine learning models, with
different features including word embeddings and word frequency. The word
embedding models (\textsc{FastText} and word2vec) exploit more than two million
Arabic tweets related to COVID-19. Experiments show that optimizing the area
under the curve (AUC) improves the models' performance and the Extreme Gradient
Boosting (XGBoost) presents the highest accuracy in detecting COVID-19
misinformation online.
</summary>
    <author>
      <name>Sarah Alqurashi</name>
    </author>
    <author>
      <name>Btool Hamoui</name>
    </author>
    <author>
      <name>Abdulaziz Alashaikh</name>
    </author>
    <author>
      <name>Ahmad Alhindi</name>
    </author>
    <author>
      <name>Eisa Alanazi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.05626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.05626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.12105v1</id>
    <updated>2022-07-25T12:15:45Z</updated>
    <published>2022-07-25T12:15:45Z</published>
    <title>Ego-graph Replay based Continual Learning for Misinformation Engagement
  Prediction</title>
    <summary>  Online social network platforms have a problem with misinformation. One
popular way of addressing this problem is via the use of machine learning based
automated misinformation detection systems to classify if a post is
misinformation. Instead of post hoc detection, we propose to predict if a user
will engage with misinformation in advance and design an effective graph neural
network classifier based on ego-graphs for this task. However, social networks
are highly dynamic, reflecting continual changes in user behaviour, as well as
the content being posted. This is problematic for machine learning models which
are typically trained on a static training dataset, and can thus become
outdated when the social network changes. Inspired by the success of continual
learning on such problems, we propose an ego-graphs replay strategy in
continual learning (EgoCL) using graph neural networks to effectively address
this issue. We have evaluated the performance of our method on user engagement
with misinformation on two Twitter datasets across nineteen misinformation and
conspiracy topics. Our experimental results show that our approach EgoCL has
better performance in terms of predictive accuracy and computational resources
than the state of the art.
</summary>
    <author>
      <name>Hongbo Bo</name>
    </author>
    <author>
      <name>Ryan McConville</name>
    </author>
    <author>
      <name>Jun Hong</name>
    </author>
    <author>
      <name>Weiru Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IJCNN 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.12105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.07518v1</id>
    <updated>2022-10-14T05:00:10Z</updated>
    <published>2022-10-14T05:00:10Z</published>
    <title>Counterfactual Neural Temporal Point Process for Estimating Causal
  Influence of Misinformation on Social Media</title>
    <summary>  Recent years have witnessed the rise of misinformation campaigns that spread
specific narratives on social media to manipulate public opinions on different
areas, such as politics and healthcare. Consequently, an effective and
efficient automatic methodology to estimate the influence of the misinformation
on user beliefs and activities is needed. However, existing works on
misinformation impact estimation either rely on small-scale psychological
experiments or can only discover the correlation between user behaviour and
misinformation. To address these issues, in this paper, we build up a causal
framework that model the causal effect of misinformation from the perspective
of temporal point process. To adapt the large-scale data, we design an
efficient yet precise way to estimate the Individual Treatment Effect(ITE) via
neural temporal point process and gaussian mixture models. Extensive
experiments on synthetic dataset verify the effectiveness and efficiency of our
model. We further apply our model on a real-world dataset of social media posts
and engagements about COVID-19 vaccines. The experimental results indicate that
our model recognized identifiable causal effect of misinformation that hurts
people's subjective emotions toward the vaccines.
</summary>
    <author>
      <name>Yizhou Zhang</name>
    </author>
    <author>
      <name>Defu Cao</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 figures, already accepted by NeurIPS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.07518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.07518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.12508v1</id>
    <updated>2022-11-22T05:38:29Z</updated>
    <published>2022-11-22T05:38:29Z</published>
    <title>Time-Aware Datasets are Adaptive Knowledgebases for the New Normal</title>
    <summary>  Recent advances in text classification and knowledge capture in language
models have relied on availability of large-scale text datasets. However,
language models are trained on static snapshots of knowledge and are limited
when that knowledge evolves. This is especially critical for misinformation
detection, where new types of misinformation continuously appear, replacing old
campaigns. We propose time-aware misinformation datasets to capture
time-critical phenomena. In this paper, we first present evidence of evolving
misinformation and show that incorporating even simple time-awareness
significantly improves classifier accuracy. Second, we present COVID-TAD, a
large-scale COVID-19 misinformation da-taset spanning 25 months. It is the
first large-scale misinformation dataset that contains multiple snapshots of a
datastream and is orders of magnitude bigger than related misinformation
datasets. We describe the collection and labeling pro-cess, as well as
preliminary experiments.
</summary>
    <author>
      <name>Abhijit Suprem</name>
    </author>
    <author>
      <name>Sanjyot Vaidya</name>
    </author>
    <author>
      <name>Joao Eduardo Ferreira</name>
    </author>
    <author>
      <name>Calton Pu</name>
    </author>
    <link href="http://arxiv.org/abs/2211.12508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.12508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.04272v1</id>
    <updated>2022-12-08T13:57:06Z</updated>
    <published>2022-12-08T13:57:06Z</published>
    <title>A Modality-level Explainable Framework for Misinformation Checking in
  Social Networks</title>
    <summary>  The widespread of false information is a rising concern worldwide with
critical social impact, inspiring the emergence of fact-checking organizations
to mitigate misinformation dissemination. However, human-driven verification
leads to a time-consuming task and a bottleneck to have checked trustworthy
information at the same pace they emerge. Since misinformation relates not only
to the content itself but also to other social features, this paper addresses
automatic misinformation checking in social networks from a multimodal
perspective. Moreover, as simply naming a piece of news as incorrect may not
convince the citizen and, even worse, strengthen confirmation bias, the
proposal is a modality-level explainable-prone misinformation classifier
framework. Our framework comprises a misinformation classifier assisted by
explainable methods to generate modality-oriented explainable inferences.
Preliminary findings show that the misinformation classifier does benefit from
multimodal information encoding and the modality-oriented explainable mechanism
increases both inferences' interpretability and completeness.
</summary>
    <author>
      <name>V√≠tor Louren√ßo</name>
    </author>
    <author>
      <name>Aline Paes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to publication at LatinX in AI workshop at the Thirty-sixth
  Conference on Neural Information Processing Systems, LXAI @ NeurIPS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.04272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.04272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00639v1</id>
    <updated>2023-08-25T10:06:05Z</updated>
    <published>2023-08-25T10:06:05Z</published>
    <title>Misinformation Concierge: A Proof-of-Concept with Curated Twitter
  Dataset on COVID-19 Vaccination</title>
    <summary>  We demonstrate the Misinformation Concierge, a proof-of-concept that provides
actionable intelligence on misinformation prevalent in social media.
Specifically, it uses language processing and machine learning tools to
identify subtopics of discourse and discern non/misleading posts; presents
statistical reports for policy-makers to understand the big picture of
prevalent misinformation in a timely manner; and recommends rebuttal messages
for specific pieces of misinformation, identified from within the corpus of
data - providing means to intervene and counter misinformation promptly. The
Misinformation Concierge proof-of-concept using a curated dataset is accessible
at: https://demo-frontend-uy34.onrender.com/
</summary>
    <author>
      <name>Shakshi Sharma</name>
    </author>
    <author>
      <name>Anwitaman Datta</name>
    </author>
    <author>
      <name>Vigneshwaran Shankaran</name>
    </author>
    <author>
      <name>Rajesh Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprinted version of our CIKM paper. Please cite our CIKM
  paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.00639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.12699v2</id>
    <updated>2024-12-25T03:52:46Z</updated>
    <published>2023-11-21T16:03:51Z</published>
    <title>Explore the Potential of LLMs in Misinformation Detection: An Empirical
  Study</title>
    <summary>  Large Language Models (LLMs) have garnered significant attention for their
powerful ability in natural language understanding and reasoning. In this
paper, we present a comprehensive empirical study to explore the performance of
LLMs on misinformation detection tasks. This study stands as the pioneering
investigation into the understanding capabilities of multiple LLMs regarding
both content and propagation across social media platforms. Our empirical
studies on eight misinformation detection datasets show that LLM-based
detectors can achieve comparable performance in text-based misinformation
detection but exhibit notably constrained capabilities in comprehending
propagation structure compared to existing models in propagation-based
misinformation detection. Our experiments further demonstrate that LLMs exhibit
great potential to enhance existing misinformation detection models. These
findings highlight the potential ability of LLMs to detect misinformation.
</summary>
    <author>
      <name>Mengyang Chen</name>
    </author>
    <author>
      <name>Lingwei Wei</name>
    </author>
    <author>
      <name>Han Cao</name>
    </author>
    <author>
      <name>Wei Zhou</name>
    </author>
    <author>
      <name>Songlin Hu</name>
    </author>
    <link href="http://arxiv.org/abs/2311.12699v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.12699v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.09359v1</id>
    <updated>2023-12-14T21:46:47Z</updated>
    <published>2023-12-14T21:46:47Z</published>
    <title>Children, Parents, and Misinformation on Social Media</title>
    <summary>  Children encounter misinformation on social media in a similar capacity as
their parents. Unlike their parents, children are an exceptionally vulnerable
population because their cognitive abilities and emotional regulation are still
maturing, rendering them more susceptible to misinformation and falsehoods
online. Yet, little is known about children's experience with misinformation as
well as what their parents think of the misinformation's effect on child
development. To answer these questions, we combined a qualitative survey of
parents (n=87) with semi-structured interviews of both parents and children
(n=12). We found that children usually encounter deep fakes, memes with
political context, or celebrity/influencer rumors on social media. Children
revealed they "ask Siri" whether a social media video or post is true or not
before they search on Google or ask their parents about it. Parents expressed
discontent that their children are impressionable to misinformation, stating
that the burden falls on them to help their children develop critical thinking
skills for navigating falsehoods on social media. Here, the majority of parents
felt that schools should also teach these skills as well as media literacy to
their children. Misinformation, according to both parents and children affects
the family relationships especially with grandparents with different political
views than theirs.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Jennifer Vander Loop</name>
    </author>
    <link href="http://arxiv.org/abs/2312.09359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.09359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.11853v1</id>
    <updated>2024-04-02T20:09:27Z</updated>
    <published>2024-04-02T20:09:27Z</published>
    <title>Decoding the Sociotechnical Dimensions of Digital Misinformation: A
  Comprehensive Literature Review</title>
    <summary>  This paper presents a systematic literature review in Computer Science that
provide an overview of the initiatives related to digital misinformation. This
is an exploratory study that covers research from 1993 to 2020, focusing on the
investigation of the phenomenon of misinformation. The review consists of 788
studies from SCOPUS, IEEE, and ACM digital libraries, synthesizing the primary
research directions and sociotechnical challenges. These challenges are
classified into Physical, Empirical, Syntactic, Semantic, Pragmatic, and Social
dimensions, drawing from Organizational Semiotics. The mapping identifies
issues related to the concept of misinformation, highlights deficiencies in
mitigation strategies, discusses challenges in approaching stakeholders, and
unveils various sociotechnical aspects relevant to understanding and mitigating
the harmful effects of digital misinformation. As contributions, this study
present a novel categorization of mitigation strategies, a sociotechnical
taxonomy for classifying types of false information and elaborate on the
inter-relation of sociotechnical aspects and their impacts.
</summary>
    <author>
      <name>Alisson Andrey Puska</name>
    </author>
    <author>
      <name>Luiz Adolpho Baroni</name>
    </author>
    <author>
      <name>Roberto Pereira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This study is part of the main author's PhD, focusing on digital
  misinformation's sociotechnical aspects. It contributes to understanding and
  combating misinformation. The full thesis is available at:
  https://acervodigital.ufpr.br/xmlui/handle/1884/85764, providing further
  insight into the methodological rigor, theoretical underpinnings, and
  highlighting the need for multidisciplinary approaches</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.11853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.11853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.00292v1</id>
    <updated>2024-08-01T05:40:58Z</updated>
    <published>2024-08-01T05:40:58Z</published>
    <title>Everything We Hear: Towards Tackling Misinformation in Podcasts</title>
    <summary>  Advances in generative AI, the proliferation of large multimodal models
(LMMs), and democratized open access to these technologies have direct
implications for the production and diffusion of misinformation. In this
prequel, we address tackling misinformation in the unique and increasingly
popular context of podcasts. The rise of podcasts as a popular medium for
disseminating information across diverse topics necessitates a proactive
strategy to combat the spread of misinformation. Inspired by the proven
effectiveness of \textit{auditory alerts} in contexts like collision alerts for
drivers and error pings in mobile phones, our work envisions the application of
auditory alerts as an effective tool to tackle misinformation in podcasts. We
propose the integration of suitable auditory alerts to notify listeners of
potential misinformation within the podcasts they are listening to, in
real-time and without hampering listening experiences. We identify several
opportunities and challenges in this path and aim to provoke novel
conversations around instruments, methods, and measures to tackle
misinformation in podcasts.
</summary>
    <author>
      <name>Sachin Pathiyan Cherumanal</name>
    </author>
    <author>
      <name>Ujwal Gadiraju</name>
    </author>
    <author>
      <name>Damiano Spina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3678957.3678959</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3678957.3678959" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM ICMI'24 (Third Place Blue Sky Paper)</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.00292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.00292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.12558v1</id>
    <updated>2024-08-22T17:17:43Z</updated>
    <published>2024-08-22T17:17:43Z</published>
    <title>Exploring the Role of Audio in Multimodal Misinformation Detection</title>
    <summary>  With the rapid development of deepfake technology, especially the deep audio
fake technology, misinformation detection on the social media scene meets a
great challenge. Social media data often contains multimodal information which
includes audio, video, text, and images. However, existing multimodal
misinformation detection methods tend to focus only on some of these
modalities, failing to comprehensively address information from all modalities.
To comprehensively address the various modal information that may appear on
social media, this paper constructs a comprehensive multimodal misinformation
detection framework. By employing corresponding neural network encoders for
each modality, the framework can fuse different modality information and
support the multimodal misinformation detection task. Based on the constructed
framework, this paper explores the importance of the audio modality in
multimodal misinformation detection tasks on social media. By adjusting the
architecture of the acoustic encoder, the effectiveness of different acoustic
feature encoders in the multimodal misinformation detection tasks is
investigated. Furthermore, this paper discovers that audio and video
information must be carefully aligned, otherwise the misalignment across
different audio and video modalities can severely impair the model performance.
</summary>
    <author>
      <name>Moyang Liu</name>
    </author>
    <author>
      <name>Yukun Liu</name>
    </author>
    <author>
      <name>Ruibo Fu</name>
    </author>
    <author>
      <name>Zhengqi Wen</name>
    </author>
    <author>
      <name>Jianhua Tao</name>
    </author>
    <author>
      <name>Xuefei Liu</name>
    </author>
    <author>
      <name>Guanjun Li</name>
    </author>
    <link href="http://arxiv.org/abs/2408.12558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.12558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.20543v1</id>
    <updated>2024-10-27T18:21:14Z</updated>
    <published>2024-10-27T18:21:14Z</published>
    <title>Investigation into the Spread of Misinformation about UK Prime Ministers
  on Twitter</title>
    <summary>  Misinformation presents threats to societal mental well-being, public health
initiatives, as well as satisfaction in democracy. Those who spread
misinformation can leverage cognitive biases to make others more likely to
believe and share their misinformation unquestioningly. For example, by sharing
misinformation whilst claiming to be someone from a highly respectable
profession, a propagandist may seek to increase the effectiveness of their
campaign using authority bias. Using retweet data from the spread of
misinformation about two former UK Prime Ministers (Boris Johnson and Theresa
May), we find that 3.1% of those who retweeted such misinformation claimed to
be teachers or lecturers (20.7% of those who claimed to have a profession in
their Twitter bio field in our sample), despite such professions representing
under 1.15% of the UK population. Whilst polling data shows teachers and
healthcare workers are amongst the most trusted professions in society, these
were amongst the most popular professions that those in our sample claimed to
have.
</summary>
    <author>
      <name>Junade Ali</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18178/ijssh.2023.V13.1141</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18178/ijssh.2023.V13.1141" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJSSH 2023 Vol.13(3): 181-186</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2410.20543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.20543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.00724v1</id>
    <updated>2025-03-02T04:31:42Z</updated>
    <published>2025-03-02T04:31:42Z</published>
    <title>Unmasking Digital Falsehoods: A Comparative Analysis of LLM-Based
  Misinformation Detection Strategies</title>
    <summary>  The proliferation of misinformation on social media has raised significant
societal concerns, necessitating robust detection mechanisms. Large Language
Models such as GPT-4 and LLaMA2 have been envisioned as possible tools for
detecting misinformation based on their advanced natural language understanding
and reasoning capabilities. This paper conducts a comparison of LLM-based
approaches to detecting misinformation between text-based, multimodal, and
agentic approaches. We evaluate the effectiveness of fine-tuned models,
zero-shot learning, and systematic fact-checking mechanisms in detecting
misinformation across different topic domains like public health, politics, and
finance. We also discuss scalability, generalizability, and explainability of
the models and recognize key challenges such as hallucination, adversarial
attacks on misinformation, and computational resources. Our findings point
towards the importance of hybrid approaches that pair structured verification
protocols with adaptive learning techniques to enhance detection accuracy and
explainability. The paper closes by suggesting potential avenues of future
work, including real-time tracking of misinformation, federated learning, and
cross-platform detection models.
</summary>
    <author>
      <name>Tianyi Huang</name>
    </author>
    <author>
      <name>Jingyuan Yi</name>
    </author>
    <author>
      <name>Peiyang Yu</name>
    </author>
    <author>
      <name>Xiaochuan Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.00724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.00724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02038v1</id>
    <updated>2025-03-03T20:30:22Z</updated>
    <published>2025-03-03T20:30:22Z</published>
    <title>Persuasion at Play: Understanding Misinformation Dynamics in
  Demographic-Aware Human-LLM Interactions</title>
    <summary>  Existing challenges in misinformation exposure and susceptibility vary across
demographic groups, as some populations are more vulnerable to misinformation
than others. Large language models (LLMs) introduce new dimensions to these
challenges through their ability to generate persuasive content at scale and
reinforcing existing biases. This study investigates the bidirectional
persuasion dynamics between LLMs and humans when exposed to misinformative
content. We analyze human-to-LLM influence using human-stance datasets and
assess LLM-to-human influence by generating LLM-based persuasive arguments.
Additionally, we use a multi-agent LLM framework to analyze the spread of
misinformation under persuasion among demographic-oriented LLM agents. Our
findings show that demographic factors influence susceptibility to
misinformation in LLMs, closely reflecting the demographic-based patterns seen
in human susceptibility. We also find that, similar to human demographic
groups, multi-agent LLMs exhibit echo chamber behavior. This research explores
the interplay between humans and LLMs, highlighting demographic differences in
the context of misinformation and offering insights for future interventions.
</summary>
    <author>
      <name>Angana Borah</name>
    </author>
    <author>
      <name>Rada Mihalcea</name>
    </author>
    <author>
      <name>Ver√≥nica P√©rez-Rosas</name>
    </author>
    <link href="http://arxiv.org/abs/2503.02038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09598v2</id>
    <updated>2025-05-27T16:40:26Z</updated>
    <published>2025-03-12T17:59:18Z</published>
    <title>How to Protect Yourself from 5G Radiation? Investigating LLM Responses
  to Implicit Misinformation</title>
    <summary>  As Large Language Models (LLMs) are widely deployed in diverse scenarios, the
extent to which they could tacitly spread misinformation emerges as a critical
safety concern. Current research primarily evaluates LLMs on explicit false
statements, overlooking how misinformation often manifests subtly as
unchallenged premises in real-world interactions. We curated EchoMist, the
first comprehensive benchmark for implicit misinformation, where false
assumptions are embedded in the query to LLMs. EchoMist targets circulated,
harmful, and ever-evolving implicit misinformation from diverse sources,
including realistic human-AI conversations and social media interactions.
Through extensive empirical studies on 15 state-of-the-art LLMs, we find that
current models perform alarmingly poorly on this task, often failing to detect
false premises and generating counterfactual explanations. We also investigate
two mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability
to counter implicit misinformation. Our findings indicate that EchoMist remains
a persistent challenge and underscore the critical need to safeguard against
the risk of implicit misinformation.
</summary>
    <author>
      <name>Ruohao Guo</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Alan Ritter</name>
    </author>
    <link href="http://arxiv.org/abs/2503.09598v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09598v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.10266v1</id>
    <updated>2025-05-15T13:18:04Z</updated>
    <published>2025-05-15T13:18:04Z</published>
    <title>Characterizing AI-Generated Misinformation on Social Media</title>
    <summary>  AI-generated misinformation (e.g., deepfakes) poses a growing threat to
information integrity on social media. However, prior research has largely
focused on its potential societal consequences rather than its real-world
prevalence. In this study, we conduct a large-scale empirical analysis of
AI-generated misinformation on the social media platform X. Specifically, we
analyze a dataset comprising N=91,452 misleading posts, both AI-generated and
non-AI-generated, that have been identified and flagged through X's Community
Notes platform. Our analysis yields four main findings: (i) AI-generated
misinformation is more often centered on entertaining content and tends to
exhibit a more positive sentiment than conventional forms of misinformation,
(ii) it is more likely to originate from smaller user accounts, (iii) despite
this, it is significantly more likely to go viral, and (iv) it is slightly less
believable and harmful compared to conventional misinformation. Altogether, our
findings highlight the unique characteristics of AI-generated misinformation on
social media. We discuss important implications for platforms and future
research.
</summary>
    <author>
      <name>Chiara Drolsbach</name>
    </author>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <link href="http://arxiv.org/abs/2505.10266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.10266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.17511v1</id>
    <updated>2025-05-23T06:05:56Z</updated>
    <published>2025-05-23T06:05:56Z</published>
    <title>Multi-agent Systems for Misinformation Lifecycle : Detection, Correction
  And Source Identification</title>
    <summary>  The rapid proliferation of misinformation in digital media demands solutions
that go beyond isolated Large Language Model(LLM) or AI Agent based detection
methods. This paper introduces a novel multi-agent framework that covers the
complete misinformation lifecycle: classification, detection, correction, and
source verification to deliver more transparent and reliable outcomes. In
contrast to single-agent or monolithic architectures, our approach employs five
specialized agents: an Indexer agent for dynamically maintaining trusted
repositories, a Classifier agent for labeling misinformation types, an
Extractor agent for evidence based retrieval and ranking, a Corrector agent for
generating fact-based correction and a Verification agent for validating
outputs and tracking source credibility. Each agent can be individually
evaluated and optimized, ensuring scalability and adaptability as new types of
misinformation and data sources emerge. By decomposing the misinformation
lifecycle into specialized agents - our framework enhances scalability,
modularity, and explainability. This paper proposes a high-level system
overview, agent design with emphasis on transparency, evidence-based outputs,
and source provenance to support robust misinformation detection and correction
at scale.
</summary>
    <author>
      <name>Aditya Gautam</name>
    </author>
    <link href="http://arxiv.org/abs/2505.17511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.17511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.10635v5</id>
    <updated>2022-07-12T18:11:02Z</updated>
    <published>2021-04-21T16:59:54Z</published>
    <title>Online misinformation is linked to early COVID-19 vaccination hesitancy
  and refusal</title>
    <summary>  Widespread uptake of vaccines is necessary to achieve herd immunity. However,
uptake rates have varied across U.S. states during the first six months of the
COVID-19 vaccination program. Misbeliefs may play an important role in vaccine
hesitancy, and there is a need to understand relationships between
misinformation, beliefs, behaviors, and health outcomes. Here we investigate
the extent to which COVID-19 vaccination rates and vaccine hesitancy are
associated with levels of online misinformation about vaccines. We also look
for evidence of directionality from online misinformation to vaccine hesitancy.
We find a negative relationship between misinformation and vaccination uptake
rates. Online misinformation is also correlated with vaccine hesitancy rates
taken from survey data. Associations between vaccine outcomes and
misinformation remain significant when accounting for political as well as
demographic and socioeconomic factors. While vaccine hesitancy is strongly
associated with Republican vote share, we observe that the effect of online
misinformation on hesitancy is strongest across Democratic rather than
Republican counties. Granger causality analysis shows evidence for a
directional relationship from online misinformation to vaccine hesitancy. Our
results support a need for interventions that address misbeliefs, allowing
individuals to make better-informed health decisions.
</summary>
    <author>
      <name>Francesco Pierri</name>
    </author>
    <author>
      <name>Brea Perry</name>
    </author>
    <author>
      <name>Matthew R. DeVerna</name>
    </author>
    <author>
      <name>Kai-Cheng Yang</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <author>
      <name>John Bryden</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature Scientific Reports 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.10635v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10635v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09739v2</id>
    <updated>2019-03-17T18:37:34Z</updated>
    <published>2018-07-25T17:36:25Z</published>
    <title>Vulnerable to Misinformation? Verifi!</title>
    <summary>  We present Verifi2, a visual analytic system to support the investigation of
misinformation on social media. On the one hand, social media platforms empower
individuals and organizations by democratizing the sharing of information. On
the other hand, even well-informed and experienced social media users are
vulnerable to misinformation. To address the issue, various models and studies
have emerged from multiple disciplines to detect and understand the effects of
misinformation. However, there is still a lack of intuitive and accessible
tools that help social media users distinguish misinformation from verified
news. In this paper, we present Verifi2, a visual analytic system that uses
state-of-the-art computational methods to highlight salient features from text,
social network, and images. By exploring news on a source level through
multiple coordinated views in Verifi2, users can interact with the complex
dimensions that characterize misinformation and contrast how real and
suspicious news outlets differ on these dimensions. To evaluate Verifi2, we
conduct interviews with experts in digital media, journalism, education,
psychology, and computing who study misinformation. Our interviews show
promising potential for Verifi2 to serve as an educational tool on
misinformation. Furthermore, our interview results highlight the complexity of
the problem of combating misinformation and call for more work from the
visualization community.
</summary>
    <author>
      <name>Alireza Karduni</name>
    </author>
    <author>
      <name>Isaac Cho</name>
    </author>
    <author>
      <name>Ryan Wesslen</name>
    </author>
    <author>
      <name>Sashank Santhanam</name>
    </author>
    <author>
      <name>Svitlana Volkova</name>
    </author>
    <author>
      <name>Dustin Arendt</name>
    </author>
    <author>
      <name>Samira Shaikh</name>
    </author>
    <author>
      <name>Wenwen Dou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09739v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09739v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.06811v1</id>
    <updated>2021-06-12T16:26:04Z</updated>
    <published>2021-06-12T16:26:04Z</published>
    <title>Case Study on Detecting COVID-19 Health-Related Misinformation in Social
  Media</title>
    <summary>  COVID-19 pandemic has generated what public health officials called an
infodemic of misinformation. As social distancing and stay-at-home orders came
into effect, many turned to social media for socializing. This increase in
social media usage has made it a prime vehicle for the spreading of
misinformation. This paper presents a mechanism to detect COVID-19
health-related misinformation in social media following an interdisciplinary
approach. Leveraging social psychology as a foundation and existing
misinformation frameworks, we defined misinformation themes and associated
keywords incorporated into the misinformation detection mechanism using applied
machine learning techniques. Next, using the Twitter dataset, we explored the
performance of the proposed methodology using multiple state-of-the-art machine
learning classifiers. Our method shows promising results with at most 78%
accuracy in classifying health-related misinformation versus true information
using uni-gram-based NLP feature generations from tweets and the Decision Tree
classifier. We also provide suggestions on alternatives for countering
misinformation and ethical consideration for the study.
</summary>
    <author>
      <name>Mir Mehedi A. Pritom</name>
    </author>
    <author>
      <name>Rosana Montanez Rodriguez</name>
    </author>
    <author>
      <name>Asad Ali Khan</name>
    </author>
    <author>
      <name>Sebastian A. Nugroho</name>
    </author>
    <author>
      <name>Esra'a Alrashydah</name>
    </author>
    <author>
      <name>Beatrice N. Ruiz</name>
    </author>
    <author>
      <name>Anthony Rios</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.06811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.07803v3</id>
    <updated>2023-09-19T23:47:18Z</updated>
    <published>2021-10-15T01:55:18Z</published>
    <title>Attacking Open-domain Question Answering by Injecting Misinformation</title>
    <summary>  With a rise in false, inaccurate, and misleading information in propaganda,
news, and social media, real-world Question Answering (QA) systems face the
challenges of synthesizing and reasoning over misinformation-polluted contexts
to derive correct answers. This urgency gives rise to the need to make QA
systems robust to misinformation, a topic previously unexplored. We study the
risk of misinformation to QA models by investigating the sensitivity of
open-domain QA models to corpus pollution with misinformation documents. We
curate both human-written and model-generated false documents that we inject
into the evidence corpus of QA models and assess the impact on the performance
of these systems. Experiments show that QA models are vulnerable to even small
amounts of evidence contamination brought by misinformation, with large
absolute performance drops on all models. Misinformation attack brings more
threat when fake documents are produced at scale by neural models or the
attacker targets hacking specific questions of interest. To defend against such
a threat, we discuss the necessity of building a misinformation-aware QA system
that integrates question-answering and misinformation detection in a joint
fashion.
</summary>
    <author>
      <name>Liangming Pan</name>
    </author>
    <author>
      <name>Wenhu Chen</name>
    </author>
    <author>
      <name>Min-Yen Kan</name>
    </author>
    <author>
      <name>William Yang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AACL-IJCNLP 2023 (main conference, long paper)</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.07803v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.07803v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.02172v2</id>
    <updated>2022-08-11T15:53:20Z</updated>
    <published>2022-02-04T15:00:00Z</published>
    <title>Facebook's Architecture Undermines Vaccine Misinformation Removal
  Efforts</title>
    <summary>  Misinformation promotes distrust in science, undermines public health, and
may drive civil unrest. Vaccine misinformation, in particular, has stalled
efforts to overcome the COVID-19 pandemic, prompting social media platforms'
attempts to reduce it. Some have questioned whether "soft" content moderation
remedies -- e.g., flagging and downranking misinformation -- were successful,
suggesting that the addition of "hard" content remedies -- e.g., deplatforming
and content bans -- is necessary. We therefore examined whether Facebook's
vaccine misinformation content removal policies were effective. Here, we show
that Facebook's policies reduced the number of anti-vaccine posts but also
caused several perverse effects: pro-vaccine content was also removed,
engagement with remaining anti-vaccine content repeatedly recovered to
pre-policy levels, and this content became more misinformative, more
politically polarised, and more likely to be seen in users' newsfeeds. We
explain these results as an unintended consequence of Facebook's design goal:
promoting community formation. Members of communities dedicated to vaccine
refusal appear to seek out misinformation from multiple sources. Community
administrators make use of several channels afforded by the Facebook platform
to disseminate misinformation. Our findings suggest the need to address how
social media platform architecture enables community formation and mobilisation
around misinformative topics when managing the spread of online content.
</summary>
    <author>
      <name>David A. Broniatowski</name>
    </author>
    <author>
      <name>Jiayan Gu</name>
    </author>
    <author>
      <name>Amelia M. Jamison</name>
    </author>
    <author>
      <name>Joseph R. Simons</name>
    </author>
    <author>
      <name>Lorien C. Abroms</name>
    </author>
    <link href="http://arxiv.org/abs/2202.02172v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02172v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12537v1</id>
    <updated>2022-03-23T16:58:33Z</updated>
    <published>2022-03-23T16:58:33Z</published>
    <title>Socially Fair Mitigation of Misinformation on Social Networks via
  Constraint Stochastic Optimization</title>
    <summary>  Recent social networks' misinformation mitigation approaches tend to
investigate how to reduce misinformation by considering a whole-network
statistical scale. However, unbalanced misinformation exposures among
individuals urge to study fair allocation of mitigation resources. Moreover,
the network has random dynamics which change over time. Therefore, we introduce
a stochastic and non-stationary knapsack problem, and we apply its resolution
to mitigate misinformation in social network campaigns. We further propose a
generic misinformation mitigation algorithm that is robust to different social
networks' misinformation statistics, allowing a promising impact in real-world
scenarios. A novel loss function ensures fair mitigation among users. We
achieve fairness by intelligently allocating a mitigation incentivization
budget to the knapsack, and optimizing the loss function. To this end, a team
of Learning Automata (LA) drives the budget allocation. Each LA is associated
with a user and learns to minimize its exposure to misinformation by performing
a non-stationary and stochastic walk over its state space. Our results show how
our LA-based method is robust and outperforms similar misinformation mitigation
methods in how the mitigation is fairly influencing the network users.
</summary>
    <author>
      <name>Ahmed Abouzeid</name>
    </author>
    <author>
      <name>Ole-Christoffer Granmo</name>
    </author>
    <author>
      <name>Christian Webersik</name>
    </author>
    <author>
      <name>Morten Goodwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">These 14-pages paper is a version with appendices so that I can cite
  appendices in the original version of the paper which was accepted and
  submitted to AAAI22</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.12537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.15359v1</id>
    <updated>2022-06-30T15:33:20Z</updated>
    <published>2022-06-30T15:33:20Z</published>
    <title>Two-Stage Classifier for COVID-19 Misinformation Detection Using BERT: a
  Study on Indonesian Tweets</title>
    <summary>  The COVID-19 pandemic has caused globally significant impacts since the
beginning of 2020. This brought a lot of confusion to society, especially due
to the spread of misinformation through social media. Although there were
already several studies related to the detection of misinformation in social
media data, most studies focused on the English dataset. Research on COVID-19
misinformation detection in Indonesia is still scarce. Therefore, through this
research, we collect and annotate datasets for Indonesian and build prediction
models for detecting COVID-19 misinformation by considering the tweet's
relevance. The dataset construction is carried out by a team of annotators who
labeled the relevance and misinformation of the tweet data. In this study, we
propose the two-stage classifier model using IndoBERT pre-trained language
model for the Tweet misinformation detection task. We also experiment with
several other baseline models for text classification. The experimental results
show that the combination of the BERT sequence classifier for relevance
prediction and Bi-LSTM for misinformation detection outperformed other machine
learning models with an accuracy of 87.02%. Overall, the BERT utilization
contributes to the higher performance of most prediction models. We release a
high-quality COVID-19 misinformation Tweet corpus in the Indonesian language,
indicated by the high inter-annotator agreement.
</summary>
    <author>
      <name>Douglas Raevan Faisal</name>
    </author>
    <author>
      <name>Rahmad Mahendra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 5 figures, submitted to Elsevier Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.15359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09578v4</id>
    <updated>2022-10-02T20:16:51Z</updated>
    <published>2022-08-20T02:09:35Z</published>
    <title>Contrastive Domain Adaptation for Early Misinformation Detection: A Case
  Study on COVID-19</title>
    <summary>  Despite recent progress in improving the performance of misinformation
detection systems, classifying misinformation in an unseen domain remains an
elusive challenge. To address this issue, a common approach is to introduce a
domain critic and encourage domain-invariant input features. However, early
misinformation often demonstrates both conditional and label shifts against
existing misinformation data (e.g., class imbalance in COVID-19 datasets),
rendering such methods less effective for detecting early misinformation. In
this paper, we propose contrastive adaptation network for early misinformation
detection (CANMD). Specifically, we leverage pseudo labeling to generate
high-confidence target examples for joint training with source data. We
additionally design a label correction component to estimate and correct the
label shifts (i.e., class priors) between the source and target domains.
Moreover, a contrastive adaptation loss is integrated in the objective function
to reduce the intra-class discrepancy and enlarge the inter-class discrepancy.
As such, the adapted model learns corrected class priors and an invariant
conditional distribution across both domains for improved estimation of the
target data distribution. To demonstrate the effectiveness of the proposed
CANMD, we study the case of COVID-19 early misinformation detection and perform
extensive experiments using multiple real-world datasets. The results suggest
that CANMD can effectively adapt misinformation detection systems to the unseen
COVID-19 target domain with significant improvements compared to the
state-of-the-art baselines.
</summary>
    <author>
      <name>Zhenrui Yue</name>
    </author>
    <author>
      <name>Huimin Zeng</name>
    </author>
    <author>
      <name>Ziyi Kou</name>
    </author>
    <author>
      <name>Lanyu Shang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CIKM 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.09578v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09578v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09429v1</id>
    <updated>2023-05-16T13:39:52Z</updated>
    <published>2023-05-16T13:39:52Z</published>
    <title>Efficacy of Educational Misinformation Games</title>
    <summary>  Misinformation has become a significant issue in today's society, with the
proliferation of false information through various mediums such as social media
and traditional news sources. The rapid spread of misinformation has made it
increasingly difficult for people to separate truth from fiction, and this has
the potential to cause significant harm to individuals and society as a whole.
In addition, there currently exists an information gap with regard to internet
education, with many schools across America not having the teaching personnel
nor resources to adequately educate their students about the dangers of the
internet, specifically with regard to misinformation in the political sphere.
To address the dangers of misinformation, some game developers have created
video games that aim to educate players on the issue and help them develop
critical thinking skills. These games can be used to raise awareness about the
importance of verifying information before sharing it. By doing so, they can
help reduce the spread of misinformation and promote a more informed and
discerning public. They can also provide players with a safe and controlled
environment to practice these skills and build confidence in their ability to
evaluate information. However, these existing games often suffer from various
shortcomings such as failing to adequately address how misinformation
specifically exploits the biases within people to be effective and rarely
covering how evolving modern technologies like sophisticated chatbots and deep
fakes have made individuals even more vulnerable to misinformation. The purpose
of this study is to create an educational misinformation game to address this
information gap and investigate its efficacy as an educational tool while also
iterating on the designs for previous games in the space.
</summary>
    <author>
      <name>William Shi</name>
    </author>
    <link href="http://arxiv.org/abs/2305.09429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.12363v1</id>
    <updated>2023-09-06T02:50:16Z</updated>
    <published>2023-09-06T02:50:16Z</published>
    <title>Investigating Online Financial Misinformation and Its Consequences: A
  Computational Perspective</title>
    <summary>  The rapid dissemination of information through digital platforms has
revolutionized the way we access and consume news and information, particularly
in the realm of finance. However, this digital age has also given rise to an
alarming proliferation of financial misinformation, which can have detrimental
effects on individuals, markets, and the overall economy. This research paper
aims to provide a comprehensive survey of online financial misinformation,
including its types, sources, and impacts. We first discuss the characteristics
and manifestations of financial misinformation, encompassing false claims and
misleading content. We explore various case studies that illustrate the
detrimental consequences of financial misinformation on the economy. Finally,
we highlight the potential impact and implications of detecting financial
misinformation. Early detection and mitigation strategies can help protect
investors, enhance market transparency, and preserve financial stability. We
emphasize the importance of greater awareness, education, and regulation to
address the issue of online financial misinformation and safeguard individuals
and businesses from its harmful effects. In conclusion, this research paper
sheds light on the pervasive issue of online financial misinformation and its
wide-ranging consequences. By understanding the types, sources, and impacts of
misinformation, stakeholders can work towards implementing effective detection
and prevention measures to foster a more informed and resilient financial
ecosystem.
</summary>
    <author>
      <name>Aman Rangapur</name>
    </author>
    <author>
      <name>Haoran Wang</name>
    </author>
    <author>
      <name>Kai Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.12363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.12363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1; I.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.11351v4</id>
    <updated>2025-01-29T17:52:33Z</updated>
    <published>2024-02-17T18:01:43Z</published>
    <title>Modeling the amplification of epidemic spread by individuals exposed to
  misinformation on social media</title>
    <summary>  Understanding how misinformation affects the spread of disease is crucial for
public health, especially given recent research indicating that misinformation
can increase vaccine hesitancy and discourage vaccine uptake. However, it is
difficult to investigate the interaction between misinformation and epidemic
outcomes due to the dearth of data-informed holistic epidemic models. Here, we
employ an epidemic model that incorporates a large, mobility-informed physical
contact network as well as the distribution of misinformed individuals across
counties derived from social media data. The model allows us to simulate
various scenarios to understand how epidemic spreading can be affected by
misinformation spreading through one particular social media platform. Using
this model, we compare a worst-case scenario, in which individuals become
misinformed after a single exposure to low-credibility content, to a best-case
scenario where the population is highly resilient to misinformation. We
estimate the additional portion of the U.S. population that would become
infected over the course of the COVID-19 epidemic in the worst-case scenario.
This work can provide policymakers with insights about the potential harms of
exposure to online vaccine misinformation.
</summary>
    <author>
      <name>Matthew R. DeVerna</name>
    </author>
    <author>
      <name>Francesco Pierri</name>
    </author>
    <author>
      <name>Yong-Yeol Ahn</name>
    </author>
    <author>
      <name>Santo Fortunato</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <link href="http://arxiv.org/abs/2402.11351v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.11351v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.10238v1</id>
    <updated>2024-06-02T19:27:56Z</updated>
    <published>2024-06-02T19:27:56Z</published>
    <title>Early Detection of Misinformation for Infodemic Management: A Domain
  Adaptation Approach</title>
    <summary>  An infodemic refers to an enormous amount of true information and
misinformation disseminated during a disease outbreak. Detecting misinformation
at the early stage of an infodemic is key to manage it and reduce its harm to
public health. An early stage infodemic is characterized by a large volume of
unlabeled information concerning a disease. As a result, conventional
misinformation detection methods are not suitable for this misinformation
detection task because they rely on labeled information in the infodemic domain
to train their models. To address the limitation of conventional methods,
state-of-the-art methods learn their models using labeled information in other
domains to detect misinformation in the infodemic domain. The efficacy of these
methods depends on their ability to mitigate both covariate shift and concept
shift between the infodemic domain and the domains from which they leverage
labeled information. These methods focus on mitigating covariate shift but
overlook concept shift, rendering them less effective for the task. In
response, we theoretically show the necessity of tackling both covariate shift
and concept shift as well as how to operationalize each of them. Built on the
theoretical analysis, we develop a novel misinformation detection method that
addresses both covariate shift and concept shift. Using two real-world
datasets, we conduct extensive empirical evaluations to demonstrate the
superior performance of our method over state-of-the-art misinformation
detection methods as well as prevalent domain adaptation methods that can be
tailored to solve the misinformation detection task.
</summary>
    <author>
      <name>Minjia Mao</name>
    </author>
    <author>
      <name>Xiaohang Zhao</name>
    </author>
    <author>
      <name>Xiao Fang</name>
    </author>
    <link href="http://arxiv.org/abs/2406.10238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.10238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.03829v1</id>
    <updated>2024-10-04T18:00:28Z</updated>
    <published>2024-10-04T18:00:28Z</published>
    <title>Misinformation with Legal Consequences (MisLC): A New Task Towards
  Harnessing Societal Harm of Misinformation</title>
    <summary>  Misinformation, defined as false or inaccurate information, can result in
significant societal harm when it is spread with malicious or even innocuous
intent. The rapid online information exchange necessitates advanced detection
mechanisms to mitigate misinformation-induced harm. Existing research, however,
has predominantly focused on assessing veracity, overlooking the legal
implications and social consequences of misinformation. In this work, we take a
novel angle to consolidate the definition of misinformation detection using
legal issues as a measurement of societal ramifications, aiming to bring
interdisciplinary efforts to tackle misinformation and its consequence. We
introduce a new task: Misinformation with Legal Consequence (MisLC), which
leverages definitions from a wide range of legal domains covering 4 broader
legal topics and 11 fine-grained legal issues, including hate speech, election
laws, and privacy regulations. For this task, we advocate a two-step dataset
curation approach that utilizes crowd-sourced checkworthiness and expert
evaluations of misinformation. We provide insights about the MisLC task through
empirical evidence, from the problem definition to experiments and expert
involvement. While the latest large language models and retrieval-augmented
generation are effective baselines for the task, we find they are still far
from replicating expert performance.
</summary>
    <author>
      <name>Chu Fei Luo</name>
    </author>
    <author>
      <name>Radin Shayanfar</name>
    </author>
    <author>
      <name>Rohan Bhambhoria</name>
    </author>
    <author>
      <name>Samuel Dahan</name>
    </author>
    <author>
      <name>Xiaodan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8.5 pages of main body, 20 pages total; Accepted to Findings of EMNLP
  2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.03829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.03829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.11813v1</id>
    <updated>2024-11-11T08:20:56Z</updated>
    <published>2024-11-11T08:20:56Z</published>
    <title>Heterogeneous population and its resilience to misinformation in
  vaccination uptake: A dual ODE and network approach</title>
    <summary>  Misinformation about vaccination poses a significant public health threat by
reducing vaccination rates and increasing disease burden. Understanding
population heterogeneity can aid in recognizing and mitigating the effects of
such misinformation, especially when vaccine effectiveness is low. Our research
quantifies the impact of misinformation on vaccination uptake and explores its
effects in heterogeneous versus homogeneous populations. We employed a dual
approach combining ordinary differential equations (ODE) and complex network
models to analyze how different epidemiological parameters influence disease
spread and vaccination behaviour. Our results indicate that misinformation
significantly lowers vaccination rates, particularly in homogeneous
populations, while heterogeneous populations demonstrate greater resilience.
Among network topologies, small-world networks achieve higher vaccination rates
under varying vaccine efficacies, while scale-free networks experience reduced
vaccine coverage with higher misinformation amplification. Notably, cumulative
infection remains independent of disease transmission rate when the vaccine is
partially effective. In small-world networks, cumulative infection shows high
stochasticity across vaccination rates and misinformation parameters, while
cumulative vaccination is highest with higher and lower misinformation. To
control disease spread, public health efforts should address misinformation,
particularly in homogeneous populations and scale-free networks. Building
resilience by promoting reliable vaccine information can boost vaccination
rates. Focusing campaigns on small-world networks can result in higher vaccine
uptake.
</summary>
    <author>
      <name>Komal Tanwar</name>
    </author>
    <author>
      <name>Viney Kumar</name>
    </author>
    <author>
      <name>Jai Prakash Tripathi</name>
    </author>
    <link href="http://arxiv.org/abs/2411.11813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.11813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.15677v1</id>
    <updated>2024-11-24T01:07:13Z</updated>
    <published>2024-11-24T01:07:13Z</published>
    <title>How Media Competition Fuels the Spread of Misinformation</title>
    <summary>  Competition among news sources may encourage some sources to share fake news
and misinformation to influence the public. While sharing misinformation may
lead to a short-term gain in audience engagement, it may damage the reputation
of these sources, resulting in a loss of audience. To understand the rationale
behind sharing misinformation, we model the competition as a zero-sum
sequential game, where each news source influences individuals based on its
credibility-how trustworthy the public perceives it-and the individual's
opinion and susceptibility. In this game, news sources can decide whether to
share factual information to enhance their credibility or disseminate
misinformation for greater immediate attention at the cost of losing
credibility. We employ the quantal response equilibrium concept, which accounts
for the bounded rationality of human decision-making, allowing for imperfect or
probabilistic choices. Our analysis shows that the resulting equilibria for
this game reproduce the credibility-bias distribution observed in real-world
news sources, with hyper-partisan sources more likely to spread misinformation
than centrist ones. It further illustrates that disseminating misinformation
can polarize the public. Notably, our model reveals that when one player
increases misinformation dissemination, the other player is likely to follow,
exacerbating the spread of misinformation. We conclude by discussing potential
strategies to mitigate the spread of fake news and promote a more factual and
reliable information landscape.
</summary>
    <author>
      <name>Arash Amini</name>
    </author>
    <author>
      <name>Yigit Ege Bayiz</name>
    </author>
    <author>
      <name>Eun-Ju Lee</name>
    </author>
    <author>
      <name>Zeynep Somer-Topcu</name>
    </author>
    <author>
      <name>Radu Marculescu</name>
    </author>
    <author>
      <name>Ufuk Topcu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.15677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.15677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.09055v1</id>
    <updated>2025-07-11T22:19:39Z</updated>
    <published>2025-07-11T22:19:39Z</published>
    <title>Analysing Health Misinformation with Advanced Centrality Metrics in
  Online Social Networks</title>
    <summary>  The rapid spread of health misinformation on online social networks (OSNs)
during global crises such as the COVID-19 pandemic poses challenges to public
health, social stability, and institutional trust. Centrality metrics have long
been pivotal in understanding the dynamics of information flow, particularly in
the context of health misinformation. However, the increasing complexity and
dynamism of online networks, especially during crises, highlight the
limitations of these traditional approaches. This study introduces and compares
three novel centrality metrics: dynamic influence centrality (DIC), health
misinformation vulnerability centrality (MVC), and propagation centrality (PC).
These metrics incorporate temporal dynamics, susceptibility, and multilayered
network interactions. Using the FibVID dataset, we compared traditional and
novel metrics to identify influential nodes, propagation pathways, and
misinformation influencers. Traditional metrics identified 29 influential
nodes, while the new metrics uncovered 24 unique nodes, resulting in 42
combined nodes, an increase of 44.83%. Baseline interventions reduced health
misinformation by 50%, while incorporating the new metrics increased this to
62.5%, an improvement of 25%. To evaluate the broader applicability of the
proposed metrics, we validated our framework on a second dataset, Monant
Medical Misinformation, which covers a diverse range of health misinformation
discussions beyond COVID-19. The results confirmed that the advanced metrics
generalised successfully, identifying distinct influential actors not captured
by traditional methods. In general, the findings suggest that a combination of
traditional and novel centrality measures offers a more robust and
generalisable framework for understanding and mitigating the spread of health
misinformation in different online network contexts.
</summary>
    <author>
      <name>Mkululi Sikosana</name>
    </author>
    <author>
      <name>Sean Maudsley-Barton</name>
    </author>
    <author>
      <name>Oluwaseun Ajao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pdig.0000888</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pdig.0000888" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, 2 figures, 3 tables, journal article in PLOS Digital Health
  (2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.09055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.09055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.10085v1</id>
    <updated>2022-10-18T18:27:47Z</updated>
    <published>2022-10-18T18:27:47Z</published>
    <title>Auditing YouTube's Recommendation Algorithm for Misinformation Filter
  Bubbles</title>
    <summary>  In this paper, we present results of an auditing study performed over YouTube
aimed at investigating how fast a user can get into a misinformation filter
bubble, but also what it takes to "burst the bubble", i.e., revert the bubble
enclosure. We employ a sock puppet audit methodology, in which pre-programmed
agents (acting as YouTube users) delve into misinformation filter bubbles by
watching misinformation promoting content. Then they try to burst the bubbles
and reach more balanced recommendations by watching misinformation debunking
content. We record search results, home page results, and recommendations for
the watched videos. Overall, we recorded 17,405 unique videos, out of which we
manually annotated 2,914 for the presence of misinformation. The labeled data
was used to train a machine learning model classifying videos into three
classes (promoting, debunking, neutral) with the accuracy of 0.82. We use the
trained model to classify the remaining videos that would not be feasible to
annotate manually.
  Using both the manually and automatically annotated data, we observe the
misinformation bubble dynamics for a range of audited topics. Our key finding
is that even though filter bubbles do not appear in some situations, when they
do, it is possible to burst them by watching misinformation debunking content
(albeit it manifests differently from topic to topic). We also observe a sudden
decrease of misinformation filter bubble effect when misinformation debunking
videos are watched after misinformation promoting videos, suggesting a strong
contextuality of recommendations. Finally, when comparing our results with a
previous similar study, we do not observe significant improvements in the
overall quantity of recommended misinformation content.
</summary>
    <author>
      <name>Ivan Srba</name>
    </author>
    <author>
      <name>Robert Moro</name>
    </author>
    <author>
      <name>Matus Tomlein</name>
    </author>
    <author>
      <name>Branislav Pecher</name>
    </author>
    <author>
      <name>Jakub Simko</name>
    </author>
    <author>
      <name>Elena Stefancova</name>
    </author>
    <author>
      <name>Michal Kompan</name>
    </author>
    <author>
      <name>Andrea Hrckova</name>
    </author>
    <author>
      <name>Juraj Podrouzek</name>
    </author>
    <author>
      <name>Adrian Gavornik</name>
    </author>
    <author>
      <name>Maria Bielikova</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3568392</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3568392" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Just accepted to ACM Transactions on Recommender Systems (ACM TORS).
  arXiv admin note: substantial text overlap with arXiv:2203.13769</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Recommender Systems. 1, 1, Article 6 (March
  2023), 33 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.10085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.10085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.10113v1</id>
    <updated>2020-10-20T08:11:47Z</updated>
    <published>2020-10-20T08:11:47Z</published>
    <title>Is this pofma? Analysing public opinion and misinformation in a COVID-19
  Telegram group chat</title>
    <summary>  We analyse a Singapore-based COVID-19 Telegram group with more than 10,000
participants. First, we study the group's opinion over time, focusing on four
dimensions: participation, sentiment, topics, and psychological features. We
find that engagement peaked when the Ministry of Health raised the disease
alert level, but this engagement was not sustained. Second, we search for
government-identified misinformation in the group. We find that
government-identified misinformation is rare, and that messages discussing
these pieces of misinformation express skepticism.
</summary>
    <author>
      <name>Lynnette Hui Xian Ng</name>
    </author>
    <author>
      <name>Loke Jia Yuan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.36190/2020.12</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.36190/2020.12" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop Proceedings of the 14th International AAAI Conference on
  Web and Social Media 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.10113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.10113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.14907v1</id>
    <updated>2020-04-30T16:06:02Z</updated>
    <published>2020-04-30T16:06:02Z</published>
    <title>You are right. I am ALARMED -- But by Climate Change Counter Movement</title>
    <summary>  The world is facing the challenge of climate crisis. Despite the consensus in
scientific community about anthropogenic global warming, the web is flooded
with articles spreading climate misinformation. These articles are carefully
constructed by climate change counter movement (cccm) organizations to
influence the narrative around climate change. We revisit the literature on
climate misinformation in social sciences and repackage it to introduce in the
community of NLP. Despite considerable work in detection of fake news, there is
no misinformation dataset available that is specific to the domain.of climate
change. We try to bridge this gap by scraping and releasing articles with known
climate change misinformation.
</summary>
    <author>
      <name>Shraey Bhatia</name>
    </author>
    <author>
      <name>Jey Han Lau</name>
    </author>
    <author>
      <name>Timothy Baldwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.14907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.14907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.08830v2</id>
    <updated>2021-01-27T20:23:36Z</updated>
    <published>2020-06-15T23:48:50Z</published>
    <title>Examining the Global Spread of COVID-19 Misinformation</title>
    <summary>  The global COVID-19 pandemic has led to the online proliferation of health-,
political-, and conspiratorial-based misinformation. Understanding the reach
and belief in this misinformation is vital to managing this crisis, as well as
future crises. The results from our global survey finds a troubling reach of
and belief in COVID-related misinformation, as well as a correlation with those
that primarily consume news from social media, and, in the United States, a
strong correlation with political leaning.
</summary>
    <author>
      <name>Sophie Nightingale</name>
    </author>
    <author>
      <name>Hany Farid</name>
    </author>
    <link href="http://arxiv.org/abs/2006.08830v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.08830v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.16302v1</id>
    <updated>2023-03-28T20:53:21Z</updated>
    <published>2023-03-28T20:53:21Z</published>
    <title>Retracted Articles about COVID-19 Vaccines Enable Vaccine Misinformation
  on Twitter</title>
    <summary>  Retracted scientific articles about COVID-19 vaccines have proliferated false
claims about vaccination harms and discouraged vaccine acceptance. Our study
analyzed the topical content of 4,876 English-language tweets about retracted
COVID-19 vaccine research and found that 27.4% of tweets contained
retraction-related misinformation. Misinformed tweets either ignored the
retraction, or less commonly, politicized the retraction using conspiratorial
rhetoric. To address this, Twitter and other social media platforms should
expand their efforts to address retraction-related misinformation.
</summary>
    <author>
      <name>Rod Abhari</name>
    </author>
    <author>
      <name>Esteban Villa-Turek</name>
    </author>
    <author>
      <name>Nicholas Vincent</name>
    </author>
    <author>
      <name>Henry Dambanemuya</name>
    </author>
    <author>
      <name>Em≈ëke-√Ågnes Horv√°t</name>
    </author>
    <link href="http://arxiv.org/abs/2303.16302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05582v1</id>
    <updated>2025-06-05T20:57:33Z</updated>
    <published>2025-06-05T20:57:33Z</published>
    <title>Combating Misinformation in the Arab World: Challenges &amp; Opportunities</title>
    <summary>  Misinformation and disinformation pose significant risks globally, with the
Arab region facing unique vulnerabilities due to geopolitical instabilities,
linguistic diversity, and cultural nuances. We explore these challenges through
the key facets of combating misinformation: detection, tracking, mitigation and
community-engagement. We shed light on how connecting with grass-roots
fact-checking organizations, understanding cultural norms, promoting social
correction, and creating strong collaborative information networks can create
opportunities for a more resilient information ecosystem in the Arab world.
</summary>
    <author>
      <name>Azza Abouzied</name>
    </author>
    <author>
      <name>Firoj Alam</name>
    </author>
    <author>
      <name>Raian Ali</name>
    </author>
    <author>
      <name>Paolo Papotti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3737450</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3737450" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">disinformation, misinformation, factuality, harmfulness, fake news</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.10026v3</id>
    <updated>2024-01-13T18:26:49Z</updated>
    <published>2022-10-18T17:49:53Z</published>
    <title>Diverse Misinformation: Impacts of Human Biases on Detection of
  Deepfakes on Networks</title>
    <summary>  Social media platforms often assume that users can self-correct against
misinformation. However, social media users are not equally susceptible to all
misinformation as their biases influence what types of misinformation might
thrive and who might be at risk. We call "diverse misinformation" the complex
relationships between human biases and demographics represented in
misinformation. To investigate how users' biases impact their susceptibility
and their ability to correct each other, we analyze classification of deepfakes
as a type of diverse misinformation. We chose deepfakes as a case study for
three reasons: 1) their classification as misinformation is more objective; 2)
we can control the demographics of the personas presented; 3) deepfakes are a
real-world concern with associated harms that must be better understood. Our
paper presents an observational survey (N=2,016) where participants are exposed
to videos and asked questions about their attributes, not knowing some might be
deepfakes. Our analysis investigates the extent to which different users are
duped and which perceived demographics of deepfake personas tend to mislead. We
find that accuracy varies by demographics, and participants are generally
better at classifying videos that match them. We extrapolate from these results
to understand the potential population-level impacts of these biases using a
mathematical model of the interplay between diverse misinformation and crowd
correction. Our model suggests that diverse contacts might provide "herd
correction" where friends can protect each other. Altogether, human biases and
the attributes of misinformation matter greatly, but having a diverse social
group may help reduce susceptibility to misinformation.
</summary>
    <author>
      <name>Juniper Lovato</name>
    </author>
    <author>
      <name>Laurent H√©bert-Dufresne</name>
    </author>
    <author>
      <name>Jonathan St-Onge</name>
    </author>
    <author>
      <name>Randall Harp</name>
    </author>
    <author>
      <name>Gabriela Salazar Lopez</name>
    </author>
    <author>
      <name>Sean P. Rogers</name>
    </author>
    <author>
      <name>Ijaz Ul Haq</name>
    </author>
    <author>
      <name>Jeremiah Onaolapo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary appendix available upon request for the time being</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.10026v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.10026v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11864v1</id>
    <updated>2022-12-22T16:59:33Z</updated>
    <published>2022-12-22T16:59:33Z</published>
    <title>How can we combat online misinformation? A systematic overview of
  current interventions and their efficacy</title>
    <summary>  The spread of misinformation is a pressing global problem that has elicited a
range of responses from researchers, policymakers, civil society and industry.
Over the past decade, these stakeholders have developed many interventions to
tackle misinformation that vary across factors such as which effects of
misinformation they hope to target, at what stage in the misinformation
lifecycle they are aimed at, and who they are implemented by. These
interventions also differ in how effective they are at reducing susceptibility
to (and curbing the spread of) misinformation. In recent years, a vast amount
of scholarly work on misinformation has become available, which extends across
multiple disciplines and methodologies. It has become increasingly difficult to
comprehensively map all of the available interventions, assess their efficacy,
and understand the challenges, opportunities and tradeoffs associated with
using them. Few papers have systematically assessed and compared the various
interventions, which has led to a lack of understanding in civic and
policymaking discourses. With this in mind, we develop a new hierarchical
framework for understanding interventions against misinformation online. The
framework comprises three key elements: Interventions that Prepare people to be
less susceptible; Interventions that Curb the spread and effects of
misinformation; and Interventions that Respond to misinformation. We outline
how different interventions are thought to work, categorise them, and summarise
the available evidence on their efficacy; offering researchers, policymakers
and practitioners working to combat online misinformation both an analytical
framework that they can use to understand and evaluate different interventions
(and which could be extended to address new interventions that we do not
describe here) and a summary of the range of interventions that have been
proposed to date.
</summary>
    <author>
      <name>Pica Johansson</name>
    </author>
    <author>
      <name>Florence Enock</name>
    </author>
    <author>
      <name>Scott Hale</name>
    </author>
    <author>
      <name>Bertie Vidgen</name>
    </author>
    <author>
      <name>Cassidy Bereskin</name>
    </author>
    <author>
      <name>Helen Margetts</name>
    </author>
    <author>
      <name>Jonathan Bright</name>
    </author>
    <link href="http://arxiv.org/abs/2212.11864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.09768v1</id>
    <updated>2021-07-20T20:58:23Z</updated>
    <published>2021-07-20T20:58:23Z</published>
    <title>Checkovid: A COVID-19 misinformation detection system on Twitter using
  network and content mining perspectives</title>
    <summary>  During the COVID-19 pandemic, social media platforms were ideal for
communicating due to social isolation and quarantine. Also, it was the primary
source of misinformation dissemination on a large scale, referred to as the
infodemic. Therefore, automatic debunking misinformation is a crucial problem.
To tackle this problem, we present two COVID-19 related misinformation datasets
on Twitter and propose a misinformation detection system comprising
network-based and content-based processes based on machine learning algorithms
and NLP techniques. In the network-based process, we focus on social
properties, network characteristics, and users. On the other hand, we classify
misinformation using the content of the tweets directly in the content-based
process, which contains text classification models (paragraph-level and
sentence-level) and similarity models. The evaluation results on the
network-based process show the best results for the artificial neural network
model with an F1 score of 88.68%. In the content-based process, our novel
similarity models, which obtained an F1 score of 90.26%, show an improvement in
the misinformation classification results compared to the network-based models.
In addition, in the text classification models, the best result was achieved
using the stacking ensemble-learning model by obtaining an F1 score of 95.18%.
Furthermore, we test our content-based models on the Constraint@AAAI2021
dataset, and by getting an F1 score of 94.38%, we improve the baseline results.
Finally, we develop a fact-checking website called Checkovid that uses each
process to detect misinformative and informative claims in the domain of
COVID-19 from different perspectives.
</summary>
    <author>
      <name>Sajad Dadgar</name>
    </author>
    <author>
      <name>Mehdi Ghatee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 Pages, 18 Figures, 7 Tables, Submitted for Review Process in a
  Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.09768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.09416v1</id>
    <updated>2022-05-19T09:30:39Z</updated>
    <published>2022-05-19T09:30:39Z</published>
    <title>A Weakly-Supervised Iterative Graph-Based Approach to Retrieve COVID-19
  Misinformation Topics</title>
    <summary>  The COVID-19 pandemic has been accompanied by an `infodemic' -- of accurate
and inaccurate health information across social media. Detecting misinformation
amidst dynamically changing information landscape is challenging; identifying
relevant keywords and posts is arduous due to the large amount of human effort
required to inspect the content and sources of posts. We aim to reduce the
resource cost of this process by introducing a weakly-supervised iterative
graph-based approach to detect keywords, topics, and themes related to
misinformation, with a focus on COVID-19. Our approach can successfully detect
specific topics from general misinformation-related seed words in a few seed
texts. Our approach utilizes the BERT-based Word Graph Search (BWGS) algorithm
that builds on context-based neural network embeddings for retrieving
misinformation-related posts. We utilize Latent Dirichlet Allocation (LDA)
topic modeling for obtaining misinformation-related themes from the texts
returned by BWGS. Furthermore, we propose the BERT-based Multi-directional Word
Graph Search (BMDWGS) algorithm that utilizes greater starting context
information for misinformation extraction. In addition to a qualitative
analysis of our approach, our quantitative analyses show that BWGS and BMDWGS
are effective in extracting misinformation-related content compared to common
baselines in low data resource settings. Extracting such content is useful for
uncovering prevalent misconceptions and concerns and for facilitating precision
public health messaging campaigns to improve health behaviors.
</summary>
    <author>
      <name>Harry Wang</name>
    </author>
    <author>
      <name>Sharath Chandra Guntuku</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at CySoc2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.09416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.09416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.08423v2</id>
    <updated>2022-04-01T05:16:55Z</updated>
    <published>2021-06-15T20:32:10Z</published>
    <title>COVID-19 Vaccine Misinformation Campaigns and Social Media Narratives</title>
    <summary>  COVID-19 vaccine hesitancy has increased concerns about vaccine uptake
required to overcome the pandemic and protect public health. A critical factor
associated with anti-vaccine attitudes is the information shared on social
media. In this work, we investigate misinformation communities and narratives
that can contribute to COVID-19 vaccine hesitancy. During the pandemic,
anti-science and political misinformation/conspiracies have been rampant on
social media. Therefore, we investigate misinformation and conspiracy groups
and their characteristic behaviours in Twitter data collected on COVID-19
vaccines. We identify if any suspicious coordinated efforts are present in
promoting vaccine misinformation, and find two suspicious groups - one
promoting a 'Great Reset' conspiracy which suggests that the pandemic is
orchestrated by world leaders to take control of the economy, with vaccine
related misinformation and strong anti-vaccine and anti-social messages such as
no lock-downs; and another promoting the Bioweapon theory. Misinformation
promoted is largely from the anti-vaccine and far-right communities in the
3-core of the retweet graph, with its tweets proportion of conspiracy and
questionable sources to reliable sources being much higher. In comparison with
the mainstream and health news, the right-leaning community is more influenced
by the anti-vaccine and far-right communities, which is also reflected in the
disparate vaccination rates in left and right U.S. states. The misinformation
communities are also more vocal, either in vaccine or other discussions,
relative to remaining communities, besides other behavioral differences.
</summary>
    <author>
      <name>Karishma Sharma</name>
    </author>
    <author>
      <name>Yizhou Zhang</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICWSM 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.08423v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08423v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13769v1</id>
    <updated>2022-03-25T16:49:57Z</updated>
    <published>2022-03-25T16:49:57Z</published>
    <title>An Audit of Misinformation Filter Bubbles on YouTube: Bubble Bursting
  and Recent Behavior Changes</title>
    <summary>  The negative effects of misinformation filter bubbles in adaptive systems
have been known to researchers for some time. Several studies investigated,
most prominently on YouTube, how fast a user can get into a misinformation
filter bubble simply by selecting wrong choices from the items offered. Yet, no
studies so far have investigated what it takes to burst the bubble, i.e.,
revert the bubble enclosure. We present a study in which pre-programmed agents
(acting as YouTube users) delve into misinformation filter bubbles by watching
misinformation promoting content (for various topics). Then, by watching
misinformation debunking content, the agents try to burst the bubbles and reach
more balanced recommendation mixes. We recorded the search results and
recommendations, which the agents encountered, and analyzed them for the
presence of misinformation. Our key finding is that bursting of a filter bubble
is possible, albeit it manifests differently from topic to topic. Moreover, we
observe that filter bubbles do not truly appear in some situations. We also
draw a direct comparison with a previous study. Sadly, we did not find much
improvements in misinformation occurrences, despite recent pledges by YouTube.
</summary>
    <author>
      <name>Matus Tomlein</name>
    </author>
    <author>
      <name>Branislav Pecher</name>
    </author>
    <author>
      <name>Jakub Simko</name>
    </author>
    <author>
      <name>Ivan Srba</name>
    </author>
    <author>
      <name>Robert Moro</name>
    </author>
    <author>
      <name>Elena Stefancova</name>
    </author>
    <author>
      <name>Michal Kompan</name>
    </author>
    <author>
      <name>Andrea Hrckova</name>
    </author>
    <author>
      <name>Juraj Podrouzek</name>
    </author>
    <author>
      <name>Maria Bielikova</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3460231.3474241</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3460231.3474241" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RecSys '21: Fifteenth ACM Conference on Recommender System</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">RecSys '21: Fifteenth ACM Conference on Recommender Systems, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.13769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.15237v1</id>
    <updated>2022-06-30T12:34:24Z</updated>
    <published>2022-06-30T12:34:24Z</published>
    <title>Adherence to Misinformation on Social Media Through Socio-Cognitive and
  Group-Based Processes</title>
    <summary>  Previous work suggests that people's preference for different kinds of
information depends on more than just accuracy. This could happen because the
messages contained within different pieces of information may either be
well-liked or repulsive. Whereas factual information must often convey
uncomfortable truths, misinformation can have little regard for veracity and
leverage psychological processes which increase its attractiveness and
proliferation on social media. In this review, we argue that when
misinformation proliferates, this happens because the social media environment
enables adherence to misinformation by reducing, rather than increasing, the
psychological cost of doing so. We cover how attention may often be shifted
away from accuracy and towards other goals, how social and individual cognition
is affected by misinformation and the cases under which debunking it is most
effective, and how the formation of online groups affects information
consumption patterns, often leading to more polarization and radicalization.
Throughout, we make the case that polarization and misinformation adherence are
closely tied. We identify ways in which the psychological cost of adhering to
misinformation can be increased when designing anti-misinformation
interventions or resilient affordances, and we outline open research questions
that the CSCW community can take up in further understanding this cost.
</summary>
    <author>
      <name>Alexandros Efstratiou</name>
    </author>
    <author>
      <name>Emiliano De Cristofaro</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">25th ACM Conference on Computer Supported Cooperative Work and
  Social Computing (CSCW 2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.15237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.03242v3</id>
    <updated>2023-08-06T05:37:37Z</updated>
    <published>2023-02-07T04:03:55Z</published>
    <title>Combating Online Misinformation Videos: Characterization, Detection, and
  Future Directions</title>
    <summary>  With information consumption via online video streaming becoming increasingly
popular, misinformation video poses a new threat to the health of the online
information ecosystem. Though previous studies have made much progress in
detecting misinformation in text and image formats, video-based misinformation
brings new and unique challenges to automatic detection systems: 1) high
information heterogeneity brought by various modalities, 2) blurred distinction
between misleading video manipulation and nonmalicious artistic video editing,
and 3) new patterns of misinformation propagation due to the dominant role of
recommendation systems on online video platforms. To facilitate research on
this challenging task, we conduct this survey to present advances in
misinformation video detection. We first analyze and characterize the
misinformation video from three levels including signals, semantics, and
intents. Based on the characterization, we systematically review existing works
for detection from features of various modalities to techniques for clue
integration. We also introduce existing resources including representative
datasets and useful tools. Besides summarizing existing studies, we discuss
related areas and outline open issues and future directions to encourage and
guide more research on misinformation video detection. The corresponding
repository is at https://github.com/ICTMCG/Awesome-Misinfo-Video-Detection.
</summary>
    <author>
      <name>Yuyan Bu</name>
    </author>
    <author>
      <name>Qiang Sheng</name>
    </author>
    <author>
      <name>Juan Cao</name>
    </author>
    <author>
      <name>Peng Qi</name>
    </author>
    <author>
      <name>Danding Wang</name>
    </author>
    <author>
      <name>Jintao Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3581783.3612426</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3581783.3612426" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM Multimedia 2023 (MM 2023). 11 pages, 4 figures, and
  89 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.03242v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.03242v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.08889v1</id>
    <updated>2023-03-15T19:19:49Z</updated>
    <published>2023-03-15T19:19:49Z</published>
    <title>Characterizing and Predicting Social Correction on Twitter</title>
    <summary>  Online misinformation has been a serious threat to public health and society.
Social media users are known to reply to misinformation posts with
counter-misinformation messages, which have been shown to be effective in
curbing the spread of misinformation. This is called social correction.
However, the characteristics of tweets that attract social correction versus
those that do not remain unknown. To close the gap, we focus on answering the
following two research questions: (1) ``Given a tweet, will it be countered by
other users?'', and (2) ``If yes, what will be the magnitude of countering
it?''. This exploration will help develop mechanisms to guide users'
misinformation correction efforts and to measure disparity across users who get
corrected. In this work, we first create a novel dataset with 690,047 pairs of
misinformation tweets and counter-misinformation replies. Then, stratified
analysis of tweet linguistic and engagement features as well as tweet posters'
user attributes are conducted to illustrate the factors that are significant in
determining whether a tweet will get countered. Finally, predictive classifiers
are created to predict the likelihood of a misinformation tweet to get
countered and the degree to which that tweet will be countered. The code and
data is accessible on https://github.com/claws-lab/social-correction-twitter.
</summary>
    <author>
      <name>Yingchen Ma</name>
    </author>
    <author>
      <name>Bing He</name>
    </author>
    <author>
      <name>Nathan Subrahmanian</name>
    </author>
    <author>
      <name>Srijan Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at: 15th ACM Web Science Conference 2023
  (WebSci'23). Code and data at:
  https://github.com/claws-lab/social-correction-twitter</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.08889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.08889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08588v2</id>
    <updated>2023-08-04T23:10:52Z</updated>
    <published>2023-04-17T20:08:33Z</published>
    <title>Designing Policies for Truth: Combating Misinformation with Transparency
  and Information Design</title>
    <summary>  Misinformation has become a growing issue on online social platforms (OSPs),
especially during elections or pandemics. To combat this, OSPs have implemented
various policies, such as tagging, to notify users about potentially misleading
information. However, these policies are often transparent and therefore
susceptible to being exploited by content creators, who may not be willing to
invest effort into producing authentic content, causing the viral spread of
misinformation. Instead of mitigating the reach of existing misinformation,
this work focuses on a solution of prevention, aiming to stop the spread of
misinformation before it has a chance to gain momentum. We propose a Bayesian
persuaded branching process ($\operatorname{BP}^2$) to model the strategic
interactions among the OSP, the content creator, and the user. The
misinformation spread on OSP is modeled by a multi-type branching process,
where users' positive and negative comments influence the misinformation
spreading. Using a Lagrangian induced by Bayesian plausibility, we characterize
the OSP's optimal policy under the perfect Bayesian equilibrium. The convexity
of the Lagrangian implies that the OSP's optimal policy is simply the fully
informative tagging policy: revealing the content's accuracy to the user. Such
a tagging policy solicits the best effort from the content creator in reducing
misinformation, even though the OSP exerts no direct control over the content
creator. We corroborate our findings using numerical simulations.
</summary>
    <author>
      <name>Ya-Ting Yang</name>
    </author>
    <author>
      <name>Tao Li</name>
    </author>
    <author>
      <name>Quanyan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to WiOpt 23; 8 pages, 3 figures; update note: fixed typos</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.08588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.18089v1</id>
    <updated>2023-10-27T12:21:55Z</updated>
    <published>2023-10-27T12:21:55Z</published>
    <title>Lost in Translation -- Multilingual Misinformation and its Evolution</title>
    <summary>  Misinformation and disinformation are growing threats in the digital age,
spreading rapidly across languages and borders. This paper investigates the
prevalence and dynamics of multilingual misinformation through an analysis of
over 250,000 unique fact-checks spanning 95 languages. First, we find that
while the majority of misinformation claims are only fact-checked once, 11.7%,
corresponding to more than 21,000 claims, are checked multiple times. Using
fact-checks as a proxy for the spread of misinformation, we find 33% of
repeated claims cross linguistic boundaries, suggesting that some
misinformation permeates language barriers. However, spreading patterns exhibit
strong homophily, with misinformation more likely to spread within the same
language. To study the evolution of claims over time and mutations across
languages, we represent fact-checks with multilingual sentence embeddings and
cluster semantically similar claims. We analyze the connected components and
shortest paths connecting different versions of a claim finding that claims
gradually drift over time and undergo greater alteration when traversing
languages. Overall, this novel investigation of multilingual misinformation
provides key insights. It quantifies redundant fact-checking efforts,
establishes that some claims diffuse across languages, measures linguistic
homophily, and models the temporal and cross-lingual evolution of claims. The
findings advocate for expanded information sharing between fact-checkers
globally while underscoring the importance of localized verification.
</summary>
    <author>
      <name>Dorian Quelle</name>
    </author>
    <author>
      <name>Calvin Cheng</name>
    </author>
    <author>
      <name>Alexandre Bovet</name>
    </author>
    <author>
      <name>Scott A. Hale</name>
    </author>
    <link href="http://arxiv.org/abs/2310.18089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.18089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.04052v1</id>
    <updated>2023-12-07T05:20:15Z</updated>
    <published>2023-12-07T05:20:15Z</published>
    <title>Multimodal Misinformation Detection in a South African Social Media
  Environment</title>
    <summary>  With the constant spread of misinformation on social media networks, a need
has arisen to continuously assess the veracity of digital content. This need
has inspired numerous research efforts on the development of misinformation
detection (MD) models. However, many models do not use all information
available to them and existing research contains a lack of relevant datasets to
train the models, specifically within the South African social media
environment. The aim of this paper is to investigate the transferability of
knowledge of a MD model between different contextual environments. This
research contributes a multimodal MD model capable of functioning in the South
African social media environment, as well as introduces a South African
misinformation dataset. The model makes use of multiple sources of information
for misinformation detection, namely: textual and visual elements. It uses
bidirectional encoder representations from transformers (BERT) as the textual
encoder and a residual network (ResNet) as the visual encoder. The model is
trained and evaluated on the Fakeddit dataset and a South African
misinformation dataset. Results show that using South African samples in the
training of the model increases model performance, in a South African
contextual environment, and that a multimodal model retains significantly more
knowledge than both the textual and visual unimodal models. Our study suggests
that the performance of a misinformation detection model is influenced by the
cultural nuances of its operating environment and multimodal models assist in
the transferability of knowledge between different contextual environments.
Therefore, local data should be incorporated into the training process of a
misinformation detection model in order to optimize model performance.
</summary>
    <author>
      <name>Amica De Jager</name>
    </author>
    <author>
      <name>Vukosi Marivate</name>
    </author>
    <author>
      <name>Abioudun Modupe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-49002-6_19</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-49002-6_19" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Intelligence Research. SACAIR 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.04052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.04052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.14952v1</id>
    <updated>2024-03-22T05:05:45Z</updated>
    <published>2024-03-22T05:05:45Z</published>
    <title>Evidence-Driven Retrieval Augmented Response Generation for Online
  Misinformation</title>
    <summary>  The proliferation of online misinformation has posed significant threats to
public interest. While numerous online users actively participate in the combat
against misinformation, many of such responses can be characterized by the lack
of politeness and supporting facts. As a solution, text generation approaches
are proposed to automatically produce counter-misinformation responses.
Nevertheless, existing methods are often trained end-to-end without leveraging
external knowledge, resulting in subpar text quality and excessively repetitive
responses. In this paper, we propose retrieval augmented response generation
for online misinformation (RARG), which collects supporting evidence from
scientific sources and generates counter-misinformation responses based on the
evidences. In particular, our RARG consists of two stages: (1) evidence
collection, where we design a retrieval pipeline to retrieve and rerank
evidence documents using a database comprising over 1M academic articles; (2)
response generation, in which we align large language models (LLMs) to generate
evidence-based responses via reinforcement learning from human feedback (RLHF).
We propose a reward function to maximize the utilization of the retrieved
evidence while maintaining the quality of the generated text, which yields
polite and factual responses that clearly refutes misinformation. To
demonstrate the effectiveness of our method, we study the case of COVID-19 and
perform extensive experiments with both in- and cross-domain datasets, where
RARG consistently outperforms baselines by generating high-quality
counter-misinformation responses.
</summary>
    <author>
      <name>Zhenrui Yue</name>
    </author>
    <author>
      <name>Huimin Zeng</name>
    </author>
    <author>
      <name>Yimeng Lu</name>
    </author>
    <author>
      <name>Lanyu Shang</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NAACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.14952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.14952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.07293v1</id>
    <updated>2024-06-11T14:23:36Z</updated>
    <published>2024-06-11T14:23:36Z</published>
    <title>Exploring Cognitive Bias Triggers in COVID-19 Misinformation Tweets: A
  Bot vs. Human Perspective</title>
    <summary>  During the COVID-19 pandemic, the proliferation of misinformation on social
media has been rapidly increasing. Automated Bot authors are believed to be
significant contributors of this surge. It is hypothesized that Bot authors
deliberately craft online misinformation aimed at triggering and exploiting
human cognitive biases, thereby enhancing tweet engagement and persuasive
influence. This study investigates this hypothesis by studying triggers of
biases embedded in Bot-authored misinformation and comparing them with their
counterparts, Human-authored misinformation. We complied a Misinfo Dataset that
contains COVID-19 vaccine-related misinformation tweets annotated by author
identities, Bots vs Humans, from Twitter during the vaccination period from
July 2020 to July 2021. We developed an algorithm to computationally automate
the extraction of triggers for eight cognitive biase. Our analysis revealed
that the Availability Bias, Cognitive Dissonance, and Confirmation Bias were
most commonly present in misinformation, with Bot-authored tweets exhibiting a
greater prevalence, with distinct patterns in utilizing bias triggers between
Humans and Bots. We further linked these bias triggers with engagement metrics,
inferring their potential influence on tweet engagement and persuasiveness.
Overall, our findings indicate that bias-triggering tactics have been more
influential on Bot-authored tweets than Human-authored tweets. While certain
bias triggers boosted engagement for Bot-authored tweets, some other bias
triggers unexpectedly decreased it. Conversely, triggers of most biases
appeared to be unrelated to the engagement of Human-authored tweets. Our work
sheds light on the differential utilization and effect of persuasion strategies
between Bot-authored and Human-authored misinformation from the lens of human
biases, offering insights for the development of effective counter-measures.
</summary>
    <author>
      <name>Lynnette Hui Xian Ng</name>
    </author>
    <author>
      <name>Wenqi Zhou</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <link href="http://arxiv.org/abs/2406.07293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.07293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.01549v1</id>
    <updated>2024-08-02T19:31:30Z</updated>
    <published>2024-08-02T19:31:30Z</published>
    <title>Reducing COVID-19 Misinformation Spread by Introducing Information
  Diffusion Delay Using Agent-based Modeling</title>
    <summary>  With the explosive growth of the Coronavirus Pandemic (COVID-19),
misinformation on social media has developed into a global phenomenon with
widespread and detrimental societal effects. Despite recent progress and
efforts in detecting COVID-19 misinformation on social media networks, this
task remains challenging due to the complexity, diversity, multi-modality, and
high costs of fact-checking or annotation. In this research, we introduce a
systematic and multidisciplinary agent-based modeling approach to limit the
spread of COVID-19 misinformation and interpret the dynamic actions of users
and communities in evolutionary online (or offline) social media networks. Our
model was applied to a Twitter network associated with an armed protest
demonstration against the COVID-19 lockdown in Michigan state in May, 2020. We
implemented a one-median problem to categorize the Twitter network into six key
communities (nodes) and identified information exchange (links) within the
network. We measured the response time to COVID-19 misinformation spread in the
network and employed a cybernetic organizational method to monitor the Twitter
network. The overall misinformation mitigation strategy was evaluated, and
agents were allocated to interact with the network based on the measured
response time and feedback. The proposed model prioritized the communities
based on the agents response times at the operational level. It then optimized
agent allocation to limit the spread of COVID19 related misinformation from
different communities, improved the information diffusion delay threshold to up
to 3 minutes, and ultimately enhanced the mitigation process to reduce
misinformation spread across the entire network.
</summary>
    <author>
      <name>Mustafa Alassad</name>
    </author>
    <author>
      <name>Nitin Agarwal</name>
    </author>
    <link href="http://arxiv.org/abs/2408.01549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.01549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.04786v2</id>
    <updated>2025-03-10T01:43:38Z</updated>
    <published>2025-02-27T14:15:43Z</published>
    <title>Analyzing the temporal dynamics of linguistic features contained in
  misinformation</title>
    <summary>  Consumption of misinformation can lead to negative consequences that impact
the individual and society. To help mitigate the influence of misinformation on
human beliefs, algorithmic labels providing context about content accuracy and
source reliability have been developed. Since the linguistic features used by
algorithms to estimate information accuracy can change across time, it is
important to understand their temporal dynamics. As a result, this study uses
natural language processing to analyze PolitiFact statements spanning between
2010 and 2024 to quantify how the sources and linguistic features of
misinformation change between five-year time periods. The results show that
statement sentiment has decreased significantly over time, reflecting a
generally more negative tone in PolitiFact statements. Moreover, statements
associated with misinformation realize significantly lower sentiment than
accurate information. Additional analysis shows that recent time periods are
dominated by sources from online social networks and other digital forums, such
as blogs and viral images, that contain high levels of misinformation
containing negative sentiment. In contrast, most statements during early time
periods are attributed to individual sources (i.e., politicians) that are
relatively balanced in accuracy ratings and contain statements with neutral or
positive sentiment. Named-entity recognition was used to identify that
presidential incumbents and candidates are relatively more prevalent in
statements containing misinformation, while US states tend to be present in
accurate information. Finally, entity labels associated with people and
organizations are more common in misinformation, while accurate statements are
more likely to contain numeric entity labels, such as percentages and dates.
</summary>
    <author>
      <name>Erik J Schlicht</name>
    </author>
    <link href="http://arxiv.org/abs/2503.04786v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.04786v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.12593v2</id>
    <updated>2020-12-28T07:19:01Z</updated>
    <published>2020-12-23T10:50:30Z</published>
    <title>Attention and misinformation sharing on social media</title>
    <summary>  The behaviour of sharing information on social media should be fulfilled only
when a user is exhibiting attentive behaviour. So that the useful information
can be consumed constructively, and misinformation can be identified and
ignored. Attentive behaviour is related to users' cognitive abilities in their
processing of set information. The work described in this paper examines the
issue of attentive factors that affect users' behaviour when they share
misinformation on social media. The research aims to identify the significance
of prevailing attention factors towards sharing misinformation on social media.
We used a closed-ended questionnaire which consisted of a psychometric scale to
measure attention behaviour with participants (n = 112). The regression
equation results are obtained as: y=(19,533-0,390+e) from a set of regression
analyses shows that attention factors have a significant negative correlation
effect for users to share misinformation on social media. Along with the
findings of the analysis results, we propose that attentive factors are
incorporated into a social media application's future design that could
intervene in user attention and avoid potential harm caused by the spread of
misinformation.
</summary>
    <author>
      <name>Zaid Amin</name>
    </author>
    <author>
      <name>Nazlena Mohamad Ali</name>
    </author>
    <author>
      <name>Alan F. Smeaton</name>
    </author>
    <link href="http://arxiv.org/abs/2012.12593v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12593v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.03313v1</id>
    <updated>2021-05-03T14:30:49Z</updated>
    <published>2021-05-03T14:30:49Z</published>
    <title>Looking for COVID-19 misinformation in multilingual social media texts</title>
    <summary>  This paper presents the Multilingual COVID-19 Analysis Method (CMTA) for
detecting and observing the spread of misinformation about this disease within
texts. CMTA proposes a data science (DS) pipeline that applies machine learning
models for processing, classifying (Dense-CNN) and analyzing (MBERT)
multilingual (micro)-texts. DS pipeline data preparation tasks extract features
from multilingual textual data and categorize it into specific information
classes (i.e., 'false', 'partly false', 'misleading'). The CMTA pipeline has
been experimented with multilingual micro-texts (tweets), showing
misinformation spread across different languages. To assess the performance of
CMTA and put it in perspective, we performed a comparative analysis of CMTA
with eight monolingual models used for detecting misinformation. The comparison
shows that CMTA has surpassed various monolingual models and suggests that it
can be used as a general method for detecting misinformation in multilingual
micro-texts. CMTA experimental results show misinformation trends about
COVID-19 in different languages during the first pandemic months.
</summary>
    <author>
      <name>Raj Ratn Pranesh</name>
    </author>
    <author>
      <name>Mehrdad Farokhnejad</name>
    </author>
    <author>
      <name>Ambesh Shekhar</name>
    </author>
    <author>
      <name>Genoveva Vargas-Solar</name>
    </author>
    <link href="http://arxiv.org/abs/2105.03313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.04666v2</id>
    <updated>2020-06-10T08:49:30Z</updated>
    <published>2020-06-08T15:13:44Z</published>
    <title>Misinformation Has High Perplexity</title>
    <summary>  Debunking misinformation is an important and time-critical task as there
could be adverse consequences when misinformation is not quashed promptly.
However, the usual supervised approach to debunking via misinformation
classification requires human-annotated data and is not suited to the fast
time-frame of newly emerging events such as the COVID-19 outbreak. In this
paper, we postulate that misinformation itself has higher perplexity compared
to truthful statements, and propose to leverage the perplexity to debunk false
claims in an unsupervised manner. First, we extract reliable evidence from
scientific and news sources according to sentence similarity to the claims.
Second, we prime a language model with the extracted evidence and finally
evaluate the correctness of given claims based on the perplexity scores at
debunking time. We construct two new COVID-19-related test sets, one is
scientific, and another is political in content, and empirically verify that
our system performs favorably compared to existing systems. We are releasing
these datasets publicly to encourage more research in debunking misinformation
on COVID-19 and other topics.
</summary>
    <author>
      <name>Nayeon Lee</name>
    </author>
    <author>
      <name>Yejin Bang</name>
    </author>
    <author>
      <name>Andrea Madotto</name>
    </author>
    <author>
      <name>Pascale Fung</name>
    </author>
    <link href="http://arxiv.org/abs/2006.04666v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04666v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.00700v1</id>
    <updated>2021-11-01T04:48:07Z</updated>
    <published>2021-11-01T04:48:07Z</published>
    <title>Analyzing Behavioral Changes of Twitter Users After Exposure to
  Misinformation</title>
    <summary>  Social media platforms have been exploited to disseminate misinformation in
recent years. The widespread online misinformation has been shown to affect
users' beliefs and is connected to social impact such as polarization. In this
work, we focus on misinformation's impact on specific user behavior and aim to
understand whether general Twitter users changed their behavior after being
exposed to misinformation. We compare the before and after behavior of exposed
users to determine whether the frequency of the tweets they posted, or the
sentiment of their tweets underwent any significant change. Our results
indicate that users overall exhibited statistically significant changes in
behavior across some of these metrics. Through language distance analysis, we
show that exposed users were already different from baseline users before the
exposure. We also study the characteristics of two specific user groups,
multi-exposure and extreme change groups, which were potentially highly
impacted. Finally, we study if the changes in the behavior of the users after
exposure to misinformation tweets vary based on the number of their followers
or the number of followers of the tweet authors, and find that their behavioral
changes are all similar.
</summary>
    <author>
      <name>Yichen Wang</name>
    </author>
    <author>
      <name>Richard Han</name>
    </author>
    <author>
      <name>Tamara Lehman</name>
    </author>
    <author>
      <name>Qin Lv</name>
    </author>
    <author>
      <name>Shivakant Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3487351.3492718</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3487351.3492718" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to FOSINT-SI, co-located with ASONAM 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.00700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.10414v1</id>
    <updated>2020-05-21T01:34:08Z</updated>
    <published>2020-05-21T01:34:08Z</published>
    <title>Analysis of misinformation during the COVID-19 outbreak in China:
  cultural, social and political entanglements</title>
    <summary>  COVID-19 resulted in an infodemic, which could erode public trust, impede
virus containment, and outlive the pandemic itself. The evolving and fragmented
media landscape is a key driver of the spread of misinformation. Using
misinformation identified by the fact-checking platform by Tencent and posts on
Weibo, our results showed that the evolution of misinformation follows an
issue-attention cycle, pertaining to topics such as city lockdown, cures, and
preventions, and school reopening. Sources of authority weigh in on these
topics, but their influence is complicated by peoples' pre-existing beliefs and
cultural practices. Finally, social media has a complicated relationship with
established or legacy media systems. Sometimes they reinforce each other, but
in general, social media may have a topic cycle of its own making. Our findings
shed light on the distinct characteristics of misinformation during the
COVID-19 and offer insights into combating misinformation in China and across
the world at large.
</summary>
    <author>
      <name>Yan Leng</name>
    </author>
    <author>
      <name>Yujia Zhai</name>
    </author>
    <author>
      <name>Shaojing Sun</name>
    </author>
    <author>
      <name>Yifei Wu</name>
    </author>
    <author>
      <name>Jordan Selzer</name>
    </author>
    <author>
      <name>Sharon Strover</name>
    </author>
    <author>
      <name>Julia Fensel</name>
    </author>
    <author>
      <name>Alex Pentland</name>
    </author>
    <author>
      <name>Ying Ding</name>
    </author>
    <link href="http://arxiv.org/abs/2005.10414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.02607v1</id>
    <updated>2021-06-03T16:34:54Z</updated>
    <published>2021-06-03T16:34:54Z</published>
    <title>Defending Democracy: Using Deep Learning to Identify and Prevent
  Misinformation</title>
    <summary>  The rise in online misinformation in recent years threatens democracies by
distorting authentic public discourse and causing confusion, fear, and even, in
extreme cases, violence. There is a need to understand the spread of false
content through online networks for developing interventions that disrupt
misinformation before it achieves virality. Using a Deep Bidirectional
Transformer for Language Understanding (BERT) and propagation graphs, this
study classifies and visualizes the spread of misinformation on a social media
network using publicly available Twitter data. The results confirm prior
research around user clusters and the virality of false content while improving
the precision of deep learning models for misinformation detection. The study
further demonstrates the suitability of BERT for providing a scalable model for
false information detection, which can contribute to the development of more
timely and accurate interventions to slow the spread of misinformation in
online environments.
</summary>
    <author>
      <name>Anusua Trivedi</name>
    </author>
    <author>
      <name>Alyssa Suhm</name>
    </author>
    <author>
      <name>Prathamesh Mahankal</name>
    </author>
    <author>
      <name>Subhiksha Mukuntharaj</name>
    </author>
    <author>
      <name>Meghana D. Parab</name>
    </author>
    <author>
      <name>Malvika Mohan</name>
    </author>
    <author>
      <name>Meredith Berger</name>
    </author>
    <author>
      <name>Arathi Sethumadhavan</name>
    </author>
    <author>
      <name>Ashish Jaiman</name>
    </author>
    <author>
      <name>Rahul Dodhia</name>
    </author>
    <link href="http://arxiv.org/abs/2106.02607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.02607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.03805v1</id>
    <updated>2021-08-09T04:46:41Z</updated>
    <published>2021-08-09T04:46:41Z</published>
    <title>Learning to Detect Few-Shot-Few-Clue Misinformation</title>
    <summary>  The quality of digital information on the web has been disquieting due to the
lack of careful manual review. Consequently, a large volume of false textual
information has been disseminating for a long time since the prevalence of
social media. The potential negative influence of misinformation on the public
is a growing concern. Therefore, it is strongly motivated to detect online
misinformation as early as possible. Few-shot-few-clue learning applies in this
misinformation detection task when the number of annotated statements is quite
few (called few shots) and the corresponding evidence is also quite limited in
each shot (called few clues). Within the few-shot-few-clue framework, we
propose a Bayesian meta-learning algorithm to extract the shared patterns among
different topics (i.e.different tasks) of misinformation. Moreover, we derive a
scalable method, i.e., amortized variational inference, to optimize the
Bayesian meta-learning algorithm. Empirical results on three benchmark datasets
demonstrate the superiority of our algorithm. This work focuses more on
optimizing parameters than designing detection models, and will generate fresh
insights into data-efficient detection of online misinformation at early
stages.
</summary>
    <author>
      <name>Qiang Zhang</name>
    </author>
    <author>
      <name>Hongbin Huang</name>
    </author>
    <author>
      <name>Shangsong Liang</name>
    </author>
    <author>
      <name>Zaiqiao Meng</name>
    </author>
    <author>
      <name>Emine Yilmaz</name>
    </author>
    <link href="http://arxiv.org/abs/2108.03805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.00737v2</id>
    <updated>2021-10-24T10:29:45Z</updated>
    <published>2021-10-02T06:42:30Z</published>
    <title>A Survey of COVID-19 Misinformation: Datasets, Detection Techniques and
  Open Issues</title>
    <summary>  Misinformation during pandemic situations like COVID-19 is growing rapidly on
social media and other platforms. This expeditious growth of misinformation
creates adverse effects on the people living in the society. Researchers are
trying their best to mitigate this problem using different approaches based on
Machine Learning (ML), Deep Learning (DL), and Natural Language Processing
(NLP). This survey aims to study different approaches of misinformation
detection on COVID-19 in recent literature to help the researchers in this
domain. More specifically, we review the different methods used for COVID-19
misinformation detection in their research with an overview of data
pre-processing and feature extraction methods to get a better understanding of
their work. We also summarize the existing datasets which can be used for
further research. Finally, we discuss the limitations of the existing methods
and highlight some potential future research directions along this dimension to
combat the spreading of misinformation during a pandemic.
</summary>
    <author>
      <name>A. R. Sana Ullah</name>
    </author>
    <author>
      <name>Anupam Das</name>
    </author>
    <author>
      <name>Anik Das</name>
    </author>
    <author>
      <name>Muhammad Ashad Kabir</name>
    </author>
    <author>
      <name>Kai Shu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13278-022-00921-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13278-022-00921-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Social Network Analysis and Mining, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.00737v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00737v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.11647v1</id>
    <updated>2022-10-21T00:33:39Z</updated>
    <published>2022-10-21T00:33:39Z</published>
    <title>Approaches to Identify Vulnerabilities to Misinformation: A Research
  Agenda</title>
    <summary>  Given the prevalence of online misinformation and our scarce cognitive
capacity, Internet users have been shown to frequently fall victim to such
information. As some studies have investigated psychological factors that make
people susceptible to believe or share misinformation, some ongoing research
further put these findings into practice by objectively identifying when and
which users are vulnerable to misinformation. In this position paper, we
highlight two ongoing avenues of research to identify vulnerable users:
detecting cognitive biases and exploring misinformation spreaders. We also
discuss the potential implications of these objective approaches: discovering
more cohorts of vulnerable users and prompting interventions to more
effectively address the right group of users. Lastly, we point out two of the
understudied contexts for misinformation vulnerability research as
opportunities for future research.
</summary>
    <author>
      <name>Nattapat Boonprakong</name>
    </author>
    <author>
      <name>Benjamin Tag</name>
    </author>
    <author>
      <name>Tilman Dingler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Position paper to a CHI 2022 workshop: Designing Credibility Tools To
  Combat Mis/Disinformation</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.11647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.11647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13865v1</id>
    <updated>2022-10-25T09:40:48Z</updated>
    <published>2022-10-25T09:40:48Z</published>
    <title>Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for
  Misinformation</title>
    <summary>  Misinformation emerges in times of uncertainty when credible information is
limited. This is challenging for NLP-based fact-checking as it relies on
counter-evidence, which may not yet be available. Despite increasing interest
in automatic fact-checking, it is still unclear if automated approaches can
realistically refute harmful real-world misinformation. Here, we contrast and
compare NLP fact-checking with how professional fact-checkers combat
misinformation in the absence of counter-evidence. In our analysis, we show
that, by design, existing NLP task definitions for fact-checking cannot refute
misinformation as professional fact-checkers do for the majority of claims. We
then define two requirements that the evidence in datasets must fulfill for
realistic fact-checking: It must be (1) sufficient to refute the claim and (2)
not leaked from existing fact-checking articles. We survey existing
fact-checking datasets and find that all of them fail to satisfy both criteria.
Finally, we perform experiments to demonstrate that models trained on a
large-scale fact-checking dataset rely on leaked evidence, which makes them
unsuitable in real-world scenarios. Taken together, we show that current NLP
fact-checking cannot realistically combat real-world misinformation because it
depends on unrealistic assumptions about counter-evidence in the data.
</summary>
    <author>
      <name>Max Glockner</name>
    </author>
    <author>
      <name>Yufang Hou</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.13865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.04617v1</id>
    <updated>2022-11-09T00:28:13Z</updated>
    <published>2022-11-09T00:28:13Z</published>
    <title>Countering Misinformation on Social Networks Using Graph Alterations</title>
    <summary>  We restrict the propagation of misinformation in a social-media-like
environment while preserving the spread of correct information. We model the
environment as a random network of users in which each news item propagates in
the network in consecutive cascades. Existing studies suggest that the cascade
behaviors of misinformation and correct information are affected differently by
user polarization and reflexivity. We show that this difference can be used to
alter network dynamics in a way that selectively hinders the spread of
misinformation content. To implement these alterations, we introduce an
optimization-based probabilistic dropout method that randomly removes
connections between users to achieve minimal propagation of misinformation. We
use disciplined convex programming to optimize these removal probabilities over
a reduced space of possible network alterations. We test the algorithm's
effectiveness using simulated social networks. In our tests, we use both
synthetic network structures based on stochastic block models, and natural
network structures that are generated using random sampling of a dataset
collected from Twitter. The results show that on average the algorithm
decreases the cascade size of misinformation content by up to $70\%$ in
synthetic network tests and up to $45\%$ in natural network tests while
maintaining a branching ratio of at least $1.5$ for correct information.
</summary>
    <author>
      <name>Yigit E. Bayiz</name>
    </author>
    <author>
      <name>Ufuk Topcu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.04617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.04617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05128v1</id>
    <updated>2023-01-12T16:31:48Z</updated>
    <published>2023-01-12T16:31:48Z</published>
    <title>Abortion Misinformation on TikTok: Rampant Content, Lax Moderation, and
  Vivid User Experiences</title>
    <summary>  The scientific effort devoted to health misinformation mostly focuses on the
implications of misleading vaccines and communicable disease claims with
respect to public health. However, the proliferation of abortion misinformation
following the Supreme Court's decision to overturn Roe v. Wade banning legal
abortion in the US highlighted a gap in scientific attention to individual
health-related misinformation. To address this gap, we conducted a study with
60 TikTok users to uncover their experiences with abortion misinformation and
the way they conceptualize, assess, and respond to misleading video content on
this platform. Our findings indicate that users mostly encounter short-term
videos suggesting herbal "at-home" remedies for pregnancy termination. While
many of the participants were cautious about scientifically debunked "abortion
alternatives," roughly 30% of the entire sample believed in their safety and
efficacy. Even an explicit debunking label attached to a misleading abortion
video about the harms of "at-home" did not help a third of the participants to
dismiss a video about self-administering abortion as misinformation. We discuss
the implications of our findings for future participation on TikTok and other
polarizing topics debated on social media.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Jennifer Vander Loop</name>
    </author>
    <author>
      <name>Peter Jachim</name>
    </author>
    <author>
      <name>Amy Devine</name>
    </author>
    <author>
      <name>Emma Pieroni</name>
    </author>
    <link href="http://arxiv.org/abs/2301.05128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.07836v1</id>
    <updated>2023-02-15T18:20:15Z</updated>
    <published>2023-02-15T18:20:15Z</published>
    <title>Assessing enactment of content regulation policies: A post hoc
  crowd-sourced audit of election misinformation on YouTube</title>
    <summary>  With the 2022 US midterm elections approaching, conspiratorial claims about
the 2020 presidential elections continue to threaten users' trust in the
electoral process. To regulate election misinformation, YouTube introduced
policies to remove such content from its searches and recommendations. In this
paper, we conduct a 9-day crowd-sourced audit on YouTube to assess the extent
of enactment of such policies. We recruited 99 users who installed a browser
extension that enabled us to collect up-next recommendation trails and search
results for 45 videos and 88 search queries about the 2020 elections. We find
that YouTube's search results, irrespective of search query bias, contain more
videos that oppose rather than support election misinformation. However,
watching misinformative election videos still lead users to a small number of
misinformative videos in the up-next trails. Our results imply that while
YouTube largely seems successful in regulating election misinformation, there
is still room for improvement.
</summary>
    <author>
      <name>Prerna Juneja</name>
    </author>
    <author>
      <name>Md Momen Bhuiyan</name>
    </author>
    <author>
      <name>Tanushree Mitra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3544548.3580846</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3544548.3580846" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.07836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.07836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.11649v1</id>
    <updated>2023-04-23T13:34:54Z</updated>
    <published>2023-04-23T13:34:54Z</published>
    <title>Impact of misinformation in the evolution of collective cooperation</title>
    <summary>  Human societies are organized and developed through collective cooperative
behaviors, in which interactions between individuals are governed by the
underlying social connections. It is well known that, based on the information
in their environment, individuals can form collective cooperation by
strategically imitating superior behaviors and changing unfavorable
surroundings in self-organizing ways. However, facing the tough situation that
some humans and social bots keep spreading misinformation, we still lack the
systematic investigation on the impact of such proliferation of misinformation
on the evolution of social cooperation. Here we study this problem by virtue of
classical evolutionary game theory. We find that misinformation generally
impedes the emergence of collective cooperation compared to scenarios with
completely true information, although the level of cooperation is slightly
higher when the benefits provided by cooperators are reduced below a proven
threshold. We further show that this possible advantage shrinks as social
connections become denser, suggesting that misinformation is more detrimental
to the formation of collective cooperation when 'social viscosity' is low. Our
results uncover the quantitative effect of misinformation on the social
cooperative behavior in the complex networked society, and pave the way for
designing possible interventions to improve collective cooperation.
</summary>
    <author>
      <name>Yao Meng</name>
    </author>
    <author>
      <name>Mark Broom</name>
    </author>
    <author>
      <name>Aming Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.11649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.04913v1</id>
    <updated>2023-05-08T17:49:21Z</updated>
    <published>2023-05-08T17:49:21Z</published>
    <title>Information Mutation and Spread of Misinformation in Timely Gossip
  Networks</title>
    <summary>  We consider a network of $n$ user nodes that receives updates from a source
and employs an age-based gossip protocol for faster dissemination of version
updates to all nodes. When a node forwards its packet to another node, the
packet information gets mutated with probability $p$ during transmission,
creating misinformation. The receiver node does not know whether an incoming
packet information is different from the packet information originally at the
sender node. We assume that truth prevails over misinformation, and therefore,
when a receiver encounters both accurate information and misinformation
corresponding to the same version, the accurate information gets chosen for
storage at the node. We study the expected fraction of nodes with correct
information in the network and version age at the nodes in this setting using
stochastic hybrid systems (SHS) modelling and study their properties. We
observe that very high or very low gossiping rates help curb misinformation,
and misinformation spread is higher with moderate gossiping rates. We support
our theoretical findings with simulation results which shed further light on
the behavior of above quantities.
</summary>
    <author>
      <name>Priyanka Kaswan</name>
    </author>
    <author>
      <name>Sennur Ulukus</name>
    </author>
    <link href="http://arxiv.org/abs/2305.04913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.04913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.02568v1</id>
    <updated>2023-10-04T04:02:32Z</updated>
    <published>2023-10-04T04:02:32Z</published>
    <title>Stand for Something or Fall for Everything: Predict Misinformation
  Spread with Stance-Aware Graph Neural Networks</title>
    <summary>  Although pervasive spread of misinformation on social media platforms has
become a pressing challenge, existing platform interventions have shown limited
success in curbing its dissemination. In this study, we propose a stance-aware
graph neural network (stance-aware GNN) that leverages users' stances to
proactively predict misinformation spread. As different user stances can form
unique echo chambers, we customize four information passing paths in
stance-aware GNN, while the trainable attention weights provide explainability
by highlighting each structure's importance. Evaluated on a real-world dataset,
stance-aware GNN outperforms benchmarks by 32.65% and exceeds advanced GNNs
without user stance by over 4.69%. Furthermore, the attention weights indicate
that users' opposition stances have a higher impact on their neighbors'
behaviors than supportive ones, which function as social correction to halt
misinformation propagation. Overall, our study provides an effective predictive
model for platforms to combat misinformation, and highlights the impact of user
stances in the misinformation propagation.
</summary>
    <author>
      <name>Zihan Chen</name>
    </author>
    <author>
      <name>Jingyi Sun</name>
    </author>
    <author>
      <name>Rong Liu</name>
    </author>
    <author>
      <name>Feng Mai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by the 2023 International Conference on Information Systems
  (ICIS 2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.02568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.02568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.0; J.4; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.12065v2</id>
    <updated>2024-02-13T21:31:46Z</updated>
    <published>2023-10-18T15:57:36Z</published>
    <title>A Persuasive Approach to Combating Misinformation</title>
    <summary>  Bayesian Persuasion is proposed as a tool for social media platforms to
combat the spread of misinformation. Since platforms can use machine learning
to predict the popularity and misinformation features of to-be-shared posts,
and users are largely motivated to share popular content, platforms can
strategically signal this informational advantage to change user beliefs and
persuade them not to share misinformation. We characterize the optimal
signaling scheme with imperfect predictions as a linear program and give
sufficient and necessary conditions on the classifier to ensure optimal
platform utility is non-decreasing and continuous. Next, this interaction is
considered under a performative model, wherein platform intervention affects
the user's future behaviour. The convergence and stability of optimal signaling
under this performative process are fully characterized. Lastly, we
experimentally validate that our approach significantly reduces misinformation
in both the single round and performative setting and discuss the broader scope
of using information design to combat misinformation.
</summary>
    <author>
      <name>Safwan Hossain</name>
    </author>
    <author>
      <name>Andjela Mladenovic</name>
    </author>
    <author>
      <name>Yiling Chen</name>
    </author>
    <author>
      <name>Gauthier Gidel</name>
    </author>
    <link href="http://arxiv.org/abs/2310.12065v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.12065v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.19834v2</id>
    <updated>2024-07-26T11:21:24Z</updated>
    <published>2023-10-29T13:07:33Z</published>
    <title>AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination
  Datasets based Recommendation System</title>
    <summary>  Misinformation has emerged as a major societal threat in recent years in
general; specifically in the context of the COVID-19 pandemic, it has wrecked
havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable
solutions for combating misinformation are the need of the hour. This work
explored how existing information obtained from social media and augmented with
more curated fact checked data repositories can be harnessed to facilitate
automated rebuttal of misinformation at scale. While the ideas herein can be
generalized and reapplied in the broader context of misinformation mitigation
using a multitude of information sources and catering to the spectrum of social
media platforms, this work serves as a proof of concept, and as such, it is
confined in its scope to only rebuttal of tweets, and in the specific context
of misinformation regarding COVID-19. It leverages two publicly available
datasets, viz. FaCov (fact-checked articles) and misleading (social media
Twitter) data on COVID-19 Vaccination.
</summary>
    <author>
      <name>Shakshi Sharma</name>
    </author>
    <author>
      <name>Anwitaman Datta</name>
    </author>
    <author>
      <name>Rajesh Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Please cite our published paper on IEEE Transactions on Computational
  Social Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.19834v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.19834v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.00671v1</id>
    <updated>2023-11-01T17:21:09Z</updated>
    <published>2023-11-01T17:21:09Z</published>
    <title>Emotion Detection for Misinformation: A Review</title>
    <summary>  With the advent of social media, an increasing number of netizens are sharing
and reading posts and news online. However, the huge volumes of misinformation
(e.g., fake news and rumors) that flood the internet can adversely affect
people's lives, and have resulted in the emergence of rumor and fake news
detection as a hot research topic. The emotions and sentiments of netizens, as
expressed in social media posts and news, constitute important factors that can
help to distinguish fake news from genuine news and to understand the spread of
rumors. This article comprehensively reviews emotion-based methods for
misinformation detection. We begin by explaining the strong links between
emotions and misinformation. We subsequently provide a detailed analysis of a
range of misinformation detection methods that employ a variety of emotion,
sentiment and stance-based features, and describe their strengths and
weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based
misinformation detection based on large language models and suggest future
research directions, including data collection (multi-platform, multilingual),
annotation, benchmark, multimodality, and interpretability.
</summary>
    <author>
      <name>Zhiwei Liu</name>
    </author>
    <author>
      <name>Tianlin Zhang</name>
    </author>
    <author>
      <name>Kailai Yang</name>
    </author>
    <author>
      <name>Paul Thompson</name>
    </author>
    <author>
      <name>Zeping Yu</name>
    </author>
    <author>
      <name>Sophia Ananiadou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.inffus.2024.102300</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.inffus.2024.102300" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.00671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.00671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.16285v1</id>
    <updated>2024-01-29T16:42:34Z</updated>
    <published>2024-01-29T16:42:34Z</published>
    <title>Capturing Pertinent Symbolic Features for Enhanced Content-Based
  Misinformation Detection</title>
    <summary>  Preventing the spread of misinformation is challenging. The detection of
misleading content presents a significant hurdle due to its extreme linguistic
and domain variability. Content-based models have managed to identify deceptive
language by learning representations from textual data such as social media
posts and web articles. However, aggregating representative samples of this
heterogeneous phenomenon and implementing effective real-world applications is
still elusive. Based on analytical work on the language of misinformation, this
paper analyzes the linguistic attributes that characterize this phenomenon and
how representative of such features some of the most popular misinformation
datasets are. We demonstrate that the appropriate use of pertinent symbolic
knowledge in combination with neural language models is helpful in detecting
misleading content. Our results achieve state-of-the-art performance in
misinformation datasets across the board, showing that our approach offers a
valid and robust alternative to multi-task transfer learning without requiring
any additional training data. Furthermore, our results show evidence that
structured knowledge can provide the extra boost required to address a complex
and unpredictable real-world problem like misinformation detection, not only in
terms of accuracy but also time efficiency and resource utilization.
</summary>
    <author>
      <name>Flavio Merenda</name>
    </author>
    <author>
      <name>Jos√© Manuel G√≥mez-P√©rez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3587259.3627566</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3587259.3627566" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at K-CAP'23: The 12th Knowledge Capture Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.16285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.16285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.11943v2</id>
    <updated>2024-06-20T20:20:30Z</updated>
    <published>2024-02-19T08:32:27Z</published>
    <title>LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with
  External Knowledge Augmentation</title>
    <summary>  The rise of multimodal misinformation on social platforms poses significant
challenges for individuals and societies. Its increased credibility and broader
impact compared to textual misinformation make detection complex, requiring
robust reasoning across diverse media types and profound knowledge for accurate
verification. The emergence of Large Vision Language Model (LVLM) offers a
potential solution to this problem. Leveraging their proficiency in processing
visual and textual information, LVLM demonstrates promising capabilities in
recognizing complex information and exhibiting strong reasoning skills. In this
paper, we first investigate the potential of LVLM on multimodal misinformation
detection. We find that even though LVLM has a superior performance compared to
LLMs, its profound reasoning may present limited power with a lack of evidence.
Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal
Misinformation Detection with External Knowledge Augmentation. LEMMA leverages
LVLM intuition and reasoning capabilities while augmenting them with external
knowledge to enhance the accuracy of misinformation detection. Our method
improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and
Fakeddit datasets respectively.
</summary>
    <author>
      <name>Keyang Xuan</name>
    </author>
    <author>
      <name>Li Yi</name>
    </author>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Ruochen Wu</name>
    </author>
    <author>
      <name>Yi R. Fung</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <link href="http://arxiv.org/abs/2402.11943v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.11943v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.16241v1</id>
    <updated>2024-09-24T17:04:12Z</updated>
    <published>2024-09-24T17:04:12Z</published>
    <title>LLM Echo Chamber: personalized and automated disinformation</title>
    <summary>  Recent advancements have showcased the capabilities of Large Language Models
like GPT4 and Llama2 in tasks such as summarization, translation, and content
review. However, their widespread use raises concerns, particularly around the
potential for LLMs to spread persuasive, humanlike misinformation at scale,
which could significantly influence public opinion. This study examines these
risks, focusing on LLMs ability to propagate misinformation as factual. To
investigate this, we built the LLM Echo Chamber, a controlled digital
environment simulating social media chatrooms, where misinformation often
spreads. Echo chambers, where individuals only interact with like minded
people, further entrench beliefs. By studying malicious bots spreading
misinformation in this environment, we can better understand this phenomenon.
We reviewed current LLMs, explored misinformation risks, and applied sota
finetuning techniques. Using Microsoft phi2 model, finetuned with our custom
dataset, we generated harmful content to create the Echo Chamber. This setup,
evaluated by GPT4 for persuasiveness and harmfulness, sheds light on the
ethical concerns surrounding LLMs and emphasizes the need for stronger
safeguards against misinformation.
</summary>
    <author>
      <name>Tony Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.16241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.16241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.20385v1</id>
    <updated>2024-09-30T15:20:58Z</updated>
    <published>2024-09-30T15:20:58Z</published>
    <title>Wait, but Tylenol is Acetaminophen... Investigating and Improving
  Language Models' Ability to Resist Requests for Misinformation</title>
    <summary>  Background: Large language models (LLMs) are trained to follow directions,
but this introduces a vulnerability to blindly comply with user requests even
if they generate wrong information. In medicine, this could accelerate the
generation of misinformation that impacts human well-being.
  Objectives/Methods: We analyzed compliance to requests to generate misleading
content about medications in settings where models know the request is
illogical. We investigated whether in-context directions and instruction-tuning
of LLMs to prioritize logical reasoning over compliance reduced misinformation
risk.
  Results: While all frontier LLMs complied with misinformation requests, both
prompt-based and parameter-based approaches can improve the detection of logic
flaws in requests and prevent the dissemination of medical misinformation.
  Conclusion: Shifting LLMs to prioritize logic over compliance could reduce
risks of exploitation for medical misinformation.
</summary>
    <author>
      <name>Shan Chen</name>
    </author>
    <author>
      <name>Mingye Gao</name>
    </author>
    <author>
      <name>Kuleen Sasse</name>
    </author>
    <author>
      <name>Thomas Hartvigsen</name>
    </author>
    <author>
      <name>Brian Anthony</name>
    </author>
    <author>
      <name>Lizhou Fan</name>
    </author>
    <author>
      <name>Hugo Aerts</name>
    </author>
    <author>
      <name>Jack Gallifant</name>
    </author>
    <author>
      <name>Danielle Bitterman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for Review</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.20385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.20385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.12699v1</id>
    <updated>2024-10-16T16:02:39Z</updated>
    <published>2024-10-16T16:02:39Z</published>
    <title>Rescuing Counterspeech: A Bridging-Based Approach to Combating
  Misinformation</title>
    <summary>  Social media has a misinformation problem, and counterspeech -- fighting bad
speech with more speech -- has been an ineffective solution. Here, we argue
that bridging-based ranking -- an algorithmic approach to promoting content
favored by users of diverse viewpoints -- is a promising approach to helping
counterspeech combat misinformation. By identifying counterspeech that is
favored both by users who are inclined to agree and by users who are inclined
to disagree with a piece of misinformation, bridging promotes counterspeech
that persuades the users most likely to believe the misinformation.
Furthermore, this algorithmic approach leverages crowd-sourced votes, shifting
discretion from platforms back to users and enabling counterspeech at the speed
and scale required to combat misinformation online. Bridging is respectful of
users' autonomy and encourages broad participation in healthy exchanges; it
offers a way for the free speech tradition to persist in modern speech
environments.
</summary>
    <author>
      <name>Kenny Peng</name>
    </author>
    <author>
      <name>James Grimmelmann</name>
    </author>
    <link href="http://arxiv.org/abs/2410.12699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.12699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18122v2</id>
    <updated>2025-05-26T19:18:59Z</updated>
    <published>2024-10-12T09:46:36Z</published>
    <title>Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution
  Generalization of Misinformation Detection Models</title>
    <summary>  This article introduces misinfo-general, a benchmark dataset for evaluating
misinformation models' ability to perform out-of-distribution generalization.
Misinformation changes rapidly, much more quickly than moderators can annotate
at scale, resulting in a shift between the training and inference data
distributions. As a result, misinformation detectors need to be able to perform
out-of-distribution generalization, an attribute they currently lack. Our
benchmark uses distant labelling to enable simulating covariate shifts in
misinformation content. We identify time, event, topic, publisher, political
bias, misinformation type as important axes for generalization, and we evaluate
a common class of baseline models on each. Using article metadata, we show how
this model fails desiderata, which is not necessarily obvious from
classification metrics. Finally, we analyze properties of the data to ensure
limited presence of modelling shortcuts. We make the dataset and accompanying
code publicly available: https://github.com/ioverho/misinfo-general
</summary>
    <author>
      <name>Ivo Verhoeven</name>
    </author>
    <author>
      <name>Pushkar Mishra</name>
    </author>
    <author>
      <name>Ekaterina Shutova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18122v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18122v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.04931v1</id>
    <updated>2025-02-07T13:52:37Z</updated>
    <published>2025-02-07T13:52:37Z</published>
    <title>Breaking the News: A LLM-based Game where Players Act as Influencer or
  Debunker for Raising Awareness About Misinformation</title>
    <summary>  Game-based interventions are widely used to combat misinformation online by
employing the "inoculation approach". However, most current interventions are
designed as single-player games, presenting players with limited predefined
choices. Such restrictions reduce replayability and may lead to an overly
simplistic understanding of the processes of misinformation phenomenon and the
debunking. This study seeks to address these issues, and empower people to
better understand the opinion influencing and misinformation debunking
processes. We did this by creating a Player versus Player (PvP) game where
participants attempt to either generate or debunk misinformation to convince
LLM-represented public opinion. Using a within-subjects mixed-methods study
design (N=47), we found that this game significantly raised participants' media
literacy and improved their ability to identify misinformation. Our qualitative
exploration revealed how participants' use of debunking and content creation
strategies deepened their understanding of the nature of disinformation. We
demonstrate how LLMs can be integrated into PvP games to foster greater
understanding of contrasting viewpoints and highlight social challenges.
</summary>
    <author>
      <name>Huiyun Tang</name>
    </author>
    <author>
      <name>Songqi Sun</name>
    </author>
    <author>
      <name>Kexin Nie</name>
    </author>
    <author>
      <name>Ang Li</name>
    </author>
    <author>
      <name>Anastasia Sergeeva</name>
    </author>
    <author>
      <name>Ray LC</name>
    </author>
    <link href="http://arxiv.org/abs/2502.04931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.04931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17332v1</id>
    <updated>2025-04-24T07:48:26Z</updated>
    <published>2025-04-24T07:48:26Z</published>
    <title>Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation
  Detection</title>
    <summary>  In the digital era, social media has become a major conduit for information
dissemination, yet it also facilitates the rapid spread of misinformation.
Traditional misinformation detection methods primarily focus on surface-level
features, overlooking the crucial roles of human empathy in the propagation
process. To address this gap, we propose the Dual-Aspect Empathy Framework
(DAE), which integrates cognitive and emotional empathy to analyze
misinformation from both the creator and reader perspectives. By examining
creators' cognitive strategies and emotional appeals, as well as simulating
readers' cognitive judgments and emotional responses using Large Language
Models (LLMs), DAE offers a more comprehensive and human-centric approach to
misinformation detection. Moreover, we further introduce an empathy-aware
filtering mechanism to enhance response authenticity and diversity.
Experimental results on benchmark datasets demonstrate that DAE outperforms
existing methods, providing a novel paradigm for multimodal misinformation
detection.
</summary>
    <author>
      <name>Zihan Wang</name>
    </author>
    <author>
      <name>Lu Yuan</name>
    </author>
    <author>
      <name>Zhengxuan Zhang</name>
    </author>
    <author>
      <name>Qing Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2504.17332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.08048v2</id>
    <updated>2025-05-14T15:19:43Z</updated>
    <published>2025-05-12T20:34:50Z</published>
    <title>Partisan Fact-Checkers' Warnings Can Effectively Correct Individuals'
  Misbeliefs About Political Misinformation</title>
    <summary>  Political misinformation, particularly harmful when it aligns with
individuals' preexisting beliefs and political ideologies, has become
widespread on social media platforms. In response, platforms like Facebook and
X introduced warning messages leveraging fact-checking results from third-party
fact-checkers to alert users against false content. However, concerns persist
about the effectiveness of these fact-checks, especially when fact-checkers are
perceived as politically biased. To address these concerns, this study presents
findings from an online human-subject experiment (N=216) investigating how the
political stances of fact-checkers influence their effectiveness in correcting
misbeliefs about political misinformation. Our findings demonstrate that
partisan fact-checkers can decrease the perceived accuracy of political
misinformation and correct misbeliefs without triggering backfire effects. This
correction is even more pronounced when the misinformation aligns with
individuals' political ideologies. Notably, while previous research suggests
that fact-checking warnings are less effective for conservatives than liberals,
our results suggest that explicitly labeled partisan fact-checkers, positioned
as political counterparts to conservatives, are particularly effective in
reducing conservatives' misbeliefs toward pro-liberal misinformation.
</summary>
    <author>
      <name>Sian Lee</name>
    </author>
    <author>
      <name>Haeseung Seo</name>
    </author>
    <author>
      <name>Aiping Xiong</name>
    </author>
    <author>
      <name>Dongwon Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in International AAAI Conference on Web and Social Media
  (ICWSM) 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.08048v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.08048v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16836v2</id>
    <updated>2025-06-04T03:33:32Z</updated>
    <published>2025-05-22T16:05:06Z</published>
    <title>Fact-R1: Towards Explainable Video Misinformation Detection with Deep
  Reasoning</title>
    <summary>  The rapid spread of multimodal misinformation on social media has raised
growing concerns, while research on video misinformation detection remains
limited due to the lack of large-scale, diverse datasets. Existing methods
often overfit to rigid templates and lack deep reasoning over deceptive
content. To address these challenges, we introduce FakeVV, a large-scale
benchmark comprising over 100,000 video-text pairs with fine-grained,
interpretable annotations. In addition, we further propose Fact-R1, a novel
framework that integrates deep reasoning with collaborative rule-based
reinforcement learning. Fact-R1 is trained through a three-stage process: (1)
misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference
alignment via Direct Preference Optimization (DPO), and (3) Group Relative
Policy Optimization (GRPO) using a novel verifiable reward function. This
enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those
observed in advanced text-based reinforcement learning systems, but in the more
complex multimodal misinformation setting. Our work establishes a new paradigm
for misinformation detection, bridging large-scale video understanding,
reasoning-guided alignment, and interpretable verification.
</summary>
    <author>
      <name>Fanrui Zhang</name>
    </author>
    <author>
      <name>Dian Li</name>
    </author>
    <author>
      <name>Qiang Zhang</name>
    </author>
    <author>
      <name> Chenjun</name>
    </author>
    <author>
      <name> sinbadliu</name>
    </author>
    <author>
      <name>Junxiong Lin</name>
    </author>
    <author>
      <name>Jiahong Yan</name>
    </author>
    <author>
      <name>Jiawei Liu</name>
    </author>
    <author>
      <name>Zheng-Jun Zha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 27 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.16836v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16836v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18221v1</id>
    <updated>2025-05-23T08:52:58Z</updated>
    <published>2025-05-23T08:52:58Z</published>
    <title>Evidence-Grounded Multimodal Misinformation Detection with
  Attention-Based GNNs</title>
    <summary>  Multimodal out-of-context (OOC) misinformation is misinformation that
repurposes real images with unrelated or misleading captions. Detecting such
misinformation is challenging because it requires resolving the context of the
claim before checking for misinformation. Many current methods, including LLMs
and LVLMs, do not perform this contextualization step. LLMs hallucinate in
absence of context or parametric knowledge. In this work, we propose a
graph-based method that evaluates the consistency between the image and the
caption by constructing two graph representations: an evidence graph, derived
from online textual evidence, and a claim graph, from the claim in the caption.
Using graph neural networks (GNNs) to encode and compare these representations,
our framework then evaluates the truthfulness of image-caption pairs. We create
datasets for our graph-based method, evaluate and compare our baseline model
against popular LLMs on the misinformation detection task. Our method scores
$93.05\%$ detection accuracy on the evaluation set and outperforms the
second-best performing method (an LLM) by $2.82\%$, making a case for smaller
and task-specific methods.
</summary>
    <author>
      <name>Sharad Duwal</name>
    </author>
    <author>
      <name>Mir Nafis Sharear Shopnil</name>
    </author>
    <author>
      <name>Abhishek Tyagi</name>
    </author>
    <author>
      <name>Adiba Mahbub Proma</name>
    </author>
    <link href="http://arxiv.org/abs/2505.18221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.24479v1</id>
    <updated>2025-05-30T11:29:10Z</updated>
    <published>2025-05-30T11:29:10Z</published>
    <title>Leveraging Knowledge Graphs and LLMs for Structured Generation of
  Misinformation</title>
    <summary>  The rapid spread of misinformation, further amplified by recent advances in
generative AI, poses significant threats to society, impacting public opinion,
democratic stability, and national security. Understanding and proactively
assessing these threats requires exploring methodologies that enable structured
and scalable misinformation generation. In this paper, we propose a novel
approach that leverages knowledge graphs (KGs) as structured semantic resources
to systematically generate fake triplets. By analyzing the structural
properties of KGs, such as the distance between entities and their predicates,
we identify plausibly false relationships. These triplets are then used to
guide large language models (LLMs) in generating misinformation statements with
varying degrees of credibility. By utilizing structured semantic relationships,
our deterministic approach produces misinformation inherently challenging for
humans to detect, drawing exclusively upon publicly available KGs (e.g.,
WikiGraphs).
  Additionally, we investigate the effectiveness of LLMs in distinguishing
between genuine and artificially generated misinformation. Our analysis
highlights significant limitations in current LLM-based detection methods,
underscoring the necessity for enhanced detection strategies and a deeper
exploration of inherent biases in generative models.
</summary>
    <author>
      <name>Sania Nayab</name>
    </author>
    <author>
      <name>Marco Simoni</name>
    </author>
    <author>
      <name>Giulio Rossolini</name>
    </author>
    <link href="http://arxiv.org/abs/2505.24479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.24479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.00509v1</id>
    <updated>2025-05-31T11:02:26Z</updated>
    <published>2025-05-31T11:02:26Z</published>
    <title>Goal-Aware Identification and Rectification of Misinformation in
  Multi-Agent Systems</title>
    <summary>  Large Language Model-based Multi-Agent Systems (MASs) have demonstrated
strong advantages in addressing complex real-world tasks. However, due to the
introduction of additional attack surfaces, MASs are particularly vulnerable to
misinformation injection. To facilitate a deeper understanding of
misinformation propagation dynamics within these systems, we introduce
MisinfoTask, a novel dataset featuring complex, realistic tasks designed to
evaluate MAS robustness against such threats. Building upon this, we propose
ARGUS, a two-stage, training-free defense framework leveraging goal-aware
reasoning for precise misinformation rectification within information flows.
Our experiments demonstrate that in challenging misinformation scenarios, ARGUS
exhibits significant efficacy across various injection attacks, achieving an
average reduction in misinformation toxicity of approximately 28.17% and
improving task success rates under attack by approximately 10.33%. Our code and
dataset is available at: https://github.com/zhrli324/ARGUS.
</summary>
    <author>
      <name>Zherui Li</name>
    </author>
    <author>
      <name>Yan Mi</name>
    </author>
    <author>
      <name>Zhenhong Zhou</name>
    </author>
    <author>
      <name>Houcheng Jiang</name>
    </author>
    <author>
      <name>Guibin Zhang</name>
    </author>
    <author>
      <name>Kun Wang</name>
    </author>
    <author>
      <name>Junfeng Fang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.00509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.00509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.16355v1</id>
    <updated>2025-06-19T14:32:24Z</updated>
    <published>2025-06-19T14:32:24Z</published>
    <title>Social Media Can Reduce Misinformation When Public Scrutiny is High</title>
    <summary>  Misinformation poses a growing global threat to institutional trust,
democratic stability, and public decision-making. While prior research has
often portrayed social media as a channel for spreading falsehoods, less is
known about the conditions under which it may instead constrain misinformation
by enhancing transparency and accountability. Here we show this dual potential
in the context of local governments' GDP reporting in China, where data
falsifications are widespread. Analyzing official reports from 2011 to 2019, we
find that local governments have overstated GDP on average. However, after
adopting social media for public communications, the extent of misreporting
declines significantly but only in regions where the public scrutiny over
political matters is high. In such regions, social media increases the cost of
misinformation by facilitating greater information disclosure and bottom-up
monitoring. In contrast, in regions with low public scrutiny, adopting social
media can exacerbate data manipulation. These findings challenge the prevailing
view that social media primarily amplifies misinformation and instead highlight
the importance of civic engagement as a moderating force. Our findings show a
boundary condition for the spread of misinformation and offer insights for
platform design and public policy aimed at promoting accuracy and institutional
accountability.
</summary>
    <author>
      <name>Gavin Wang</name>
    </author>
    <author>
      <name>Haofei Qin</name>
    </author>
    <author>
      <name>Xiao Tang</name>
    </author>
    <author>
      <name>Lynn Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2506.16355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.16355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.01984v1</id>
    <updated>2025-06-26T18:17:35Z</updated>
    <published>2025-06-26T18:17:35Z</published>
    <title>Multimodal Misinformation Detection Using Early Fusion of Linguistic,
  Visual, and Social Features</title>
    <summary>  Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.
</summary>
    <author>
      <name>Gautam Kishore Shahi</name>
    </author>
    <link href="http://arxiv.org/abs/2507.01984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.01984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.18203v1</id>
    <updated>2025-07-24T08:58:47Z</updated>
    <published>2025-07-24T08:58:47Z</published>
    <title>Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to
  Misinformation</title>
    <summary>  Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.
</summary>
    <author>
      <name>Kyubeen Han</name>
    </author>
    <author>
      <name>Junseo Jang</name>
    </author>
    <author>
      <name>Hongjin Kim</name>
    </author>
    <author>
      <name>Geunyeong Jeong</name>
    </author>
    <author>
      <name>Harksoo Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2025 Main Accepted</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.18203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.18203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06314v1</id>
    <updated>2017-06-20T08:36:56Z</updated>
    <published>2017-06-20T08:36:56Z</published>
    <title>Mining Significant Microblogs for Misinformation Identification: An
  Attention-based Approach</title>
    <summary>  With the rapid growth of social media, massive misinformation is also
spreading widely on social media, such as microblog, and bring negative effects
to human life. Nowadays, automatic misinformation identification has drawn
attention from academic and industrial communities. For an event on social
media usually consists of multiple microblogs, current methods are mainly based
on global statistical features. However, information on social media is full of
noisy and outliers, which should be alleviated. Moreover, most of microblogs
about an event have little contribution to the identification of
misinformation, where useful information can be easily overwhelmed by useless
information. Thus, it is important to mine significant microblogs for a
reliable misinformation identification method. In this paper, we propose an
Attention-based approach for Identification of Misinformation (AIM). Based on
the attention mechanism, AIM can select microblogs with largest attention
values for misinformation identification. The attention mechanism in AIM
contains two parts: content attention and dynamic attention. Content attention
is calculated based textual features of each microblog. Dynamic attention is
related to the time interval between the posting time of a microblog and the
beginning of the event. To evaluate AIM, we conduct a series of experiments on
the Weibo dataset and the Twitter dataset, and the experimental results show
that the proposed AIM model outperforms the state-of-the-art methods.
</summary>
    <author>
      <name>Qiang Liu</name>
    </author>
    <author>
      <name>Feng Yu</name>
    </author>
    <author>
      <name>Shu Wu</name>
    </author>
    <author>
      <name>Liang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1706.06314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07729v2</id>
    <updated>2021-04-15T13:56:54Z</updated>
    <published>2020-12-14T17:24:59Z</published>
    <title>"Thought I'd Share First" and Other Conspiracy Theory Tweets from the
  COVID-19 Infodemic: Exploratory Study</title>
    <summary>  Background: The COVID-19 outbreak has left many people isolated within their
homes; these people are turning to social media for news and social connection,
which leaves them vulnerable to believing and sharing misinformation.
Health-related misinformation threatens adherence to public health messaging,
and monitoring its spread on social media is critical to understanding the
evolution of ideas that have potentially negative public health impacts.
Results: Analysis using model-labeled data was beneficial for increasing the
proportion of data matching misinformation indicators. Random forest classifier
metrics varied across the four conspiracy theories considered (F1 scores
between 0.347 and 0.857); this performance increased as the given conspiracy
theory was more narrowly defined. We showed that misinformation tweets
demonstrate more negative sentiment when compared to nonmisinformation tweets
and that theories evolve over time, incorporating details from unrelated
conspiracy theories as well as real-world events. Conclusions: Although we
focus here on health-related misinformation, this combination of approaches is
not specific to public health and is valuable for characterizing misinformation
in general, which is an important first step in creating targeted messaging to
counteract its spread. Initial messaging should aim to preempt generalized
misinformation before it becomes widespread, while later messaging will need to
target evolving conspiracy theories and the new facets of each as they become
incorporated.
</summary>
    <author>
      <name>Dax Gerts</name>
    </author>
    <author>
      <name>Courtney D. Shelley</name>
    </author>
    <author>
      <name>Nidhi Parikh</name>
    </author>
    <author>
      <name>Travis Pitts</name>
    </author>
    <author>
      <name>Chrysm Watson Ross</name>
    </author>
    <author>
      <name>Geoffrey Fairchild</name>
    </author>
    <author>
      <name>Nidia Yadria Vaquera Chavez</name>
    </author>
    <author>
      <name>Ashlynn R. Daughton</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2196/26527</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2196/26527" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JMIR Pub Hlth Surv 2021 7(4)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.07729v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07729v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.09338v2</id>
    <updated>2021-08-08T17:03:44Z</updated>
    <published>2021-06-17T09:19:45Z</published>
    <title>Investigating Misinformation Dissemination on Social Media in Pakistan</title>
    <summary>  Fake news and misinformation are one of the most significant challenges
brought about by advances in communication technologies. We chose to research
the spread of fake news in Pakistan because of some unfortunate incidents that
took place during 2020. These included the downplaying of the severity of the
COVID-19 pandemic, and protests by right-wing political movements. We observed
that fake news and misinformation contributed significantly to these events and
especially affected low-literate and low-income populations. We conducted a
cross-platform comparison of misinformation on WhatsApp, Twitter and YouTube
with a primary focus on messages shared in public WhatsApp groups, and analysed
the characteristics of misinformation, techniques used to make is believable,
and how users respond to it. To the best of our knowledge, this is the first
attempt to compare misinformation on all three platforms in Pakistan. Data
collected over a span of eight months helped us identify fake news and
misinformation related to politics, religion and health, among other
categories. Common elements which were used by fake news creators in Pakistan
to make false content seem believable included: appeals to emotion, conspiracy
theories, political and religious polarization, incorrect facts and
impersonation of credible sources.
</summary>
    <author>
      <name>Danyal Haroon</name>
    </author>
    <author>
      <name>Hammad Arif</name>
    </author>
    <author>
      <name>Ahmed Abdullah Tariq</name>
    </author>
    <author>
      <name>fareeda nawaz</name>
    </author>
    <author>
      <name>Ihsan Ayyub Qazi</name>
    </author>
    <author>
      <name>Maryam mustafa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">i want to further work on it</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.09338v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.09338v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.01508v1</id>
    <updated>2022-06-19T15:14:06Z</updated>
    <published>2022-06-19T15:14:06Z</published>
    <title>Understanding misinformation in India: The case for a meaningful
  regulatory approach for social media platforms</title>
    <summary>  For research, this paper has included numerous literature that are covering a
variety of information on the topics of misinformation, social media and fake
news, regulation of misinformation and social media platforms, all presented
for India. Studies including thematic analysis of misinformation, brief history
on social media and its amplification of misinformation, current and past
policy interventions by the Indian government, history of self-regulations in
industries, and an analysis of regulatory approaches in the Indian context.
This paper aims at introducing a coherent reading into the context of
misinformation in the country and the subsequent social and business
disruptions that will follow. Utilizing lessons from history around industry
regulations, existing policy research and framework analysis to convince the
reader of the nature of policy intervention that will bode well for all
stakeholders involved. The literature sources have been mentioned in their
respective sections for reference. The research utilized the PASTEL framework
to analyse data collected from other research efforts covering the topic of
misinformation and regulation across academic whitepapers and news media blogs
and articles, all available freely on the public domain. Relevant secondary
data, in terms of information, previous analysis in other research efforts, and
literature work included in respective sections in the paper have been
reproduced, shared and/or indicated wherever necessary.
</summary>
    <author>
      <name>Gandharv Dhruv Madan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.01508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.01508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.12692v1</id>
    <updated>2023-05-22T04:00:38Z</updated>
    <published>2023-05-22T04:00:38Z</published>
    <title>MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta
  Learning</title>
    <summary>  With emerging topics (e.g., COVID-19) on social media as a source for the
spreading misinformation, overcoming the distributional shifts between the
original training domain (i.e., source domain) and such target domains remains
a non-trivial task for misinformation detection. This presents an elusive
challenge for early-stage misinformation detection, where a good amount of data
and annotations from the target domain is not available for training. To
address the data scarcity issue, we propose MetaAdapt, a meta learning based
approach for domain adaptive few-shot misinformation detection. MetaAdapt
leverages limited target examples to provide feedback and guide the knowledge
transfer from the source to the target domain (i.e., learn to adapt). In
particular, we train the initial model with multiple source tasks and compute
their similarity scores to the meta task. Based on the similarity scores, we
rescale the meta gradients to adaptively learn from the source tasks. As such,
MetaAdapt can learn how to adapt the misinformation detection model and exploit
the source data for improved performance in the target domain. To demonstrate
the efficiency and effectiveness of our method, we perform extensive
experiments to compare MetaAdapt with state-of-the-art baselines and large
language models (LLMs) such as LLaMA, where MetaAdapt achieves better
performance in domain adaptive few-shot misinformation detection with
substantially reduced parameters on real-world datasets.
</summary>
    <author>
      <name>Zhenrui Yue</name>
    </author>
    <author>
      <name>Huimin Zeng</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Lanyu Shang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.12692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.12692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.11282v3</id>
    <updated>2025-01-24T15:54:18Z</updated>
    <published>2023-11-19T09:52:44Z</published>
    <title>Differential impact from individual versus collective misinformation
  tagging on the diversity of Twitter (X) information engagement and mobility</title>
    <summary>  Fears about the destabilizing impact of misinformation online have motivated
individuals and platforms to respond. Individuals have increasingly challenged
others' online claims with fact-checks in pursuit of a healthier information
ecosystem and to break down echo chambers of self-reinforcing opinion. Using
Twitter (now X) data, here we show the consequences of individual
misinformation tagging: tagged posters had explored novel political information
and expanded topical interests immediately prior, but being tagged caused
posters to retreat into information bubbles. These unintended consequences were
softened by a collective verification system for misinformation moderation. In
Twitter's new feature, Community Notes, misinformation tagging was
peer-reviewed by other fact-checkers before revelation to the poster. With
collective misinformation tagging, posters were less likely to retreat from
diverse information engagement. Detailed comparison demonstrated differences in
toxicity, sentiment, readability, and delay in individual versus collective
misinformation tagging messages. These findings provide evidence for
differential impacts from individual versus collective moderation strategies on
the diversity of information engagement and mobility across the information
ecosystem.
</summary>
    <author>
      <name>Junsol Kim</name>
    </author>
    <author>
      <name>Zhao Wang</name>
    </author>
    <author>
      <name>Haohan Shi</name>
    </author>
    <author>
      <name>Hsin-Keng Ling</name>
    </author>
    <author>
      <name>James Evans</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41467-025-55868-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41467-025-55868-0" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was published in Nature Communications. Supplementary
  information is available at
  https://www.nature.com/articles/s41467-025-55868-0#Sec19</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature Communications 16, 973 (2025)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2311.11282v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.11282v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.15525v1</id>
    <updated>2024-02-19T21:50:42Z</updated>
    <published>2024-02-19T21:50:42Z</published>
    <title>Detecting misinformation through Framing Theory: the Frame Element-based
  Model</title>
    <summary>  In this paper, we delve into the rapidly evolving challenge of misinformation
detection, with a specific focus on the nuanced manipulation of narrative
frames - an under-explored area within the AI community. The potential for
Generative AI models to generate misleading narratives underscores the urgency
of this problem. Drawing from communication and framing theories, we posit that
the presentation or 'framing' of accurate information can dramatically alter
its interpretation, potentially leading to misinformation. We highlight this
issue through real-world examples, demonstrating how shifts in narrative frames
can transmute fact-based information into misinformation. To tackle this
challenge, we propose an innovative approach leveraging the power of
pre-trained Large Language Models and deep neural networks to detect
misinformation originating from accurate facts portrayed under different
frames. These advanced AI techniques offer unprecedented capabilities in
identifying complex patterns within unstructured data critical for examining
the subtleties of narrative frames. The objective of this paper is to bridge a
significant research gap in the AI domain, providing valuable insights and
methodologies for tackling framing-induced misinformation, thus contributing to
the advancement of responsible and trustworthy AI technologies. Several
experiments are intensively conducted and experimental results explicitly
demonstrate the various impact of elements of framing theory proving the
rationale of applying framing theory to increase the performance in
misinformation detection.
</summary>
    <author>
      <name>Guan Wang</name>
    </author>
    <author>
      <name>Rebecca Frederick</name>
    </author>
    <author>
      <name>Jinglong Duan</name>
    </author>
    <author>
      <name>William Wong</name>
    </author>
    <author>
      <name>Verica Rupar</name>
    </author>
    <author>
      <name>Weihua Li</name>
    </author>
    <author>
      <name>Quan Bai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.15525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.15525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.11093v2</id>
    <updated>2025-05-31T09:54:19Z</updated>
    <published>2024-06-16T22:49:11Z</published>
    <title>RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation
  Detection Using In-Context Learning Based on Emotional Information</title>
    <summary>  Misinformation is prevalent in various fields such as education, politics,
health, etc., causing significant harm to society. However, current methods for
cross-domain misinformation detection rely on effort- and resource-intensive
fine-tuning and complex model structures. With the outstanding performance of
LLMs, many studies have employed them for misinformation detection.
Unfortunately, they focus on in-domain tasks and do not incorporate significant
sentiment and emotion features (which we jointly call {\em affect}). In this
paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework
to address cross-domain misinformation detection using in-context learning
based on affective information. RAEmoLLM includes three modules. (1) In the
index construction module, we apply an emotional LLM to obtain affective
embeddings from all domains to construct a retrieval database. (2) The
retrieval module uses the database to recommend top K examples (text-label
pairs) from source domain data for target domain contents. (3) These examples
are adopted as few-shot demonstrations for the inference module to process the
target domain content. The RAEmoLLM can effectively enhance the general
performance of LLMs in cross-domain misinformation detection tasks through
affect-based retrieval, without fine-tuning. We evaluate our framework on three
misinformation benchmarks. Results show that RAEmoLLM achieves significant
improvements compared to the other few-shot methods on three datasets, with the
highest increases of 15.64%, 31.18%, and 15.73% respectively. This project is
available at https://github.com/lzw108/RAEmoLLM.
</summary>
    <author>
      <name>Zhiwei Liu</name>
    </author>
    <author>
      <name>Kailai Yang</name>
    </author>
    <author>
      <name>Qianqian Xie</name>
    </author>
    <author>
      <name>Christine de Kock</name>
    </author>
    <author>
      <name>Sophia Ananiadou</name>
    </author>
    <author>
      <name>Eduard Hovy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL 2025 (Main)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.11093v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.11093v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.19196v1</id>
    <updated>2024-07-27T07:30:47Z</updated>
    <published>2024-07-27T07:30:47Z</published>
    <title>Why Misinformation is Created? Detecting them by Integrating Intent
  Features</title>
    <summary>  Various social media platforms, e.g., Twitter and Reddit, allow people to
disseminate a plethora of information more efficiently and conveniently.
However, they are inevitably full of misinformation, causing damage to diverse
aspects of our daily lives. To reduce the negative impact, timely
identification of misinformation, namely Misinformation Detection (MD), has
become an active research topic receiving widespread attention. As a complex
phenomenon, the veracity of an article is influenced by various aspects. In
this paper, we are inspired by the opposition of intents between misinformation
and real information. Accordingly, we propose to reason the intent of articles
and form the corresponding intent features to promote the veracity
discrimination of article features. To achieve this, we build a hierarchy of a
set of intents for both misinformation and real information by referring to the
existing psychological theories, and we apply it to reason the intent of
articles by progressively generating binary answers with an encoder-decoder
structure. We form the corresponding intent features and integrate it with the
token features to achieve more discriminative article features for MD. Upon
these ideas, we suggest a novel MD method, namely Detecting Misinformation by
Integrating Intent featuRes (DM-INTER). To evaluate the performance of
DM-INTER, we conduct extensive experiments on benchmark MD datasets. The
experimental results validate that DM-INTER can outperform the existing
baseline MD methods.
</summary>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Ximing Li</name>
    </author>
    <author>
      <name>Changchun Li</name>
    </author>
    <author>
      <name>Bo Fu</name>
    </author>
    <author>
      <name>Songwen Pei</name>
    </author>
    <author>
      <name>Shengsheng Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures. Accepted by CIKM 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.19196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.19196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.06311v1</id>
    <updated>2024-10-08T19:43:37Z</updated>
    <published>2024-10-08T19:43:37Z</published>
    <title>A Comparative Study of Hybrid Models in Health Misinformation Text
  Classification</title>
    <summary>  This study evaluates the effectiveness of machine learning (ML) and deep
learning (DL) models in detecting COVID-19-related misinformation on online
social networks (OSNs), aiming to develop more effective tools for countering
the spread of health misinformation during the pan-demic. The study trained and
tested various ML classifiers (Naive Bayes, SVM, Random Forest, etc.), DL
models (CNN, LSTM, hybrid CNN+LSTM), and pretrained language models
(DistilBERT, RoBERTa) on the "COVID19-FNIR DATASET". These models were
evaluated for accuracy, F1 score, recall, precision, and ROC, and used
preprocessing techniques like stemming and lemmatization. The results showed
SVM performed well, achieving a 94.41% F1-score. DL models with Word2Vec
embeddings exceeded 98% in all performance metrics (accuracy, F1 score, recall,
precision &amp; ROC). The CNN+LSTM hybrid models also exceeded 98% across
performance metrics, outperforming pretrained models like DistilBERT and
RoBERTa. Our study concludes that DL and hybrid DL models are more effective
than conventional ML algorithms for detecting COVID-19 misinformation on OSNs.
The findings highlight the importance of advanced neural network approaches and
large-scale pretraining in misinformation detection. Future research should
optimize these models for various misinformation types and adapt to changing
OSNs, aiding in combating health misinformation.
</summary>
    <author>
      <name>Mkululi Sikosana</name>
    </author>
    <author>
      <name>Oluwaseun Ajao</name>
    </author>
    <author>
      <name>Sean Maudsley-Barton</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3677117.3685007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3677117.3685007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 tables presented at the OASIS workshop of the ACM
  Hypertext and Social Media Conference 2024</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 4th International Workshop on Open
  Challenges in Online Social Networks (pp. 18-25) 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2410.06311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.06311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.09949v2</id>
    <updated>2024-10-15T02:46:27Z</updated>
    <published>2024-10-13T18:16:50Z</published>
    <title>MisinfoEval: Generative AI in the Era of "Alternative Facts"</title>
    <summary>  The spread of misinformation on social media platforms threatens democratic
processes, contributes to massive economic losses, and endangers public health.
Many efforts to address misinformation focus on a knowledge deficit model and
propose interventions for improving users' critical thinking through access to
facts. Such efforts are often hampered by challenges with scalability, and by
platform users' personal biases. The emergence of generative AI presents
promising opportunities for countering misinformation at scale across
ideological barriers.
  In this paper, we introduce a framework (MisinfoEval) for generating and
comprehensively evaluating large language model (LLM) based misinformation
interventions. We present (1) an experiment with a simulated social media
environment to measure effectiveness of misinformation interventions, and (2) a
second experiment with personalized explanations tailored to the demographics
and beliefs of users with the goal of countering misinformation by appealing to
their pre-existing values. Our findings confirm that LLM-based interventions
are highly effective at correcting user behavior (improving overall user
accuracy at reliability labeling by up to 41.72%). Furthermore, we find that
users favor more personalized interventions when making decisions about news
reliability and users shown personalized interventions have significantly
higher accuracy at identifying misinformation.
</summary>
    <author>
      <name>Saadia Gabriel</name>
    </author>
    <author>
      <name>Liang Lyu</name>
    </author>
    <author>
      <name>James Siderius</name>
    </author>
    <author>
      <name>Marzyeh Ghassemi</name>
    </author>
    <author>
      <name>Jacob Andreas</name>
    </author>
    <author>
      <name>Asu Ozdaglar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2024. Correspondence can be sent to skgabrie at cs dot ucla dot
  edu</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.09949v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.09949v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.05773v2</id>
    <updated>2020-11-12T04:20:37Z</updated>
    <published>2020-11-11T13:48:44Z</published>
    <title>The Role of the Crowd in Countering Misinformation: A Case Study of the
  COVID-19 Infodemic</title>
    <summary>  Fact checking by professionals is viewed as a vital defense in the fight
against misinformation.While fact checking is important and its impact has been
significant, fact checks could have limited visibility and may not reach the
intended audience, such as those deeply embedded in polarized communities.
Concerned citizens (i.e., the crowd), who are users of the platforms where
misinformation appears, can play a crucial role in disseminating fact-checking
information and in countering the spread of misinformation. To explore if this
is the case, we conduct a data-driven study of misinformation on the Twitter
platform, focusing on tweets related to the COVID-19 pandemic, analyzing the
spread of misinformation, professional fact checks, and the crowd response to
popular misleading claims about COVID-19. In this work, we curate a dataset of
false claims and statements that seek to challenge or refute them. We train a
classifier to create a novel dataset of 155,468 COVID-19-related tweets,
containing 33,237 false claims and 33,413 refuting arguments.Our findings show
that professional fact-checking tweets have limited volume and reach. In
contrast, we observe that the surge in misinformation tweets results in a quick
response and a corresponding increase in tweets that refute such
misinformation. More importantly, we find contrasting differences in the way
the crowd refutes tweets, some tweets appear to be opinions, while others
contain concrete evidence, such as a link to a reputed source. Our work
provides insights into how misinformation is organically countered in social
platforms by some of their users and the role they play in amplifying
professional fact checks.These insights could lead to development of tools and
mechanisms that can empower concerned citizens in combating misinformation. The
code and data can be found in
http://claws.cc.gatech.edu/covid_counter_misinformation.html.
</summary>
    <author>
      <name>Nicholas Micallef</name>
    </author>
    <author>
      <name>Bing He</name>
    </author>
    <author>
      <name>Srijan Kumar</name>
    </author>
    <author>
      <name>Mustaque Ahamad</name>
    </author>
    <author>
      <name>Nasir Memon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PrePrint - IEEE BigData 2020. The code and data can be found in
  http://claws.cc.gatech.edu/covid_counter_misinformation.html</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.05773v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05773v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.11169v4</id>
    <updated>2024-09-03T05:51:40Z</updated>
    <published>2024-03-17T10:59:09Z</published>
    <title>Correcting misinformation on social media with a large language model</title>
    <summary>  Real-world misinformation, often multimodal, can be partially or fully
factual but misleading using diverse tactics like conflating correlation with
causation. Such misinformation is severely understudied, challenging to
address, and harms various social domains, particularly on social media, where
it can spread rapidly. High-quality and timely correction of misinformation
that identifies and explains its (in)accuracies effectively reduces false
beliefs. Despite the wide acceptance of manual correction, it is difficult to
be timely and scalable. While LLMs have versatile capabilities that could
accelerate misinformation correction, they struggle due to a lack of recent
information, a tendency to produce false content, and limitations in addressing
multimodal information. We propose MUSE, an LLM augmented with access to and
credibility evaluation of up-to-date information. By retrieving evidence as
refutations or supporting context, MUSE identifies and explains content
(in)accuracies with references. It conducts multimodal retrieval and interprets
visual content to verify and correct multimodal content. Given the absence of a
comprehensive evaluation approach, we propose 13 dimensions of misinformation
correction quality. Then, fact-checking experts evaluate responses to social
media content that are not presupposed to be misinformation but broadly include
(partially) incorrect and correct posts that may (not) be misleading. Results
demonstrate MUSE's ability to write high-quality responses to potential
misinformation--across modalities, tactics, domains, political leanings, and
for information that has not previously been fact-checked online--within
minutes of its appearance on social media. Overall, MUSE outperforms GPT-4 by
37% and even high-quality responses from laypeople by 29%. Our work provides a
general methodological and evaluative framework to correct misinformation at
scale.
</summary>
    <author>
      <name>Xinyi Zhou</name>
    </author>
    <author>
      <name>Ashish Sharma</name>
    </author>
    <author>
      <name>Amy X. Zhang</name>
    </author>
    <author>
      <name>Tim Althoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.11169v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.11169v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.11697v2</id>
    <updated>2024-05-21T15:35:26Z</updated>
    <published>2024-05-19T23:05:53Z</published>
    <title>AMMeBa: A Large-Scale Survey and Dataset of Media-Based Misinformation
  In-The-Wild</title>
    <summary>  The prevalence and harms of online misinformation is a perennial concern for
internet platforms, institutions and society at large. Over time, information
shared online has become more media-heavy and misinformation has readily
adapted to these new modalities. The rise of generative AI-based tools, which
provide widely-accessible methods for synthesizing realistic audio, images,
video and human-like text, have amplified these concerns. Despite intense
public interest and significant press coverage, quantitative information on the
prevalence and modality of media-based misinformation remains scarce. Here, we
present the results of a two-year study using human raters to annotate online
media-based misinformation, mostly focusing on images, based on claims assessed
in a large sample of publicly-accessible fact checks with the ClaimReview
markup. We present an image typology, designed to capture aspects of the image
and manipulation relevant to the image's role in the misinformation claim. We
visualize the distribution of these types over time. We show the rise of
generative AI-based content in misinformation claims, and that its commonality
is a relatively recent phenomenon, occurring significantly after heavy press
coverage. We also show "simple" methods dominated historically, particularly
context manipulations, and continued to hold a majority as of the end of data
collection in November 2023. The dataset, Annotated Misinformation, Media-Based
(AMMeBa), is publicly-available, and we hope that these data will serve as both
a means of evaluating mitigation methods in a realistic setting and as a
first-of-its-kind census of the types and modalities of online misinformation.
</summary>
    <author>
      <name>Nicholas Dufour</name>
    </author>
    <author>
      <name>Arkanath Pathak</name>
    </author>
    <author>
      <name>Pouya Samangouei</name>
    </author>
    <author>
      <name>Nikki Hariri</name>
    </author>
    <author>
      <name>Shashi Deshetti</name>
    </author>
    <author>
      <name>Andrew Dudfield</name>
    </author>
    <author>
      <name>Christopher Guess</name>
    </author>
    <author>
      <name>Pablo Hern√°ndez Escayola</name>
    </author>
    <author>
      <name>Bobby Tran</name>
    </author>
    <author>
      <name>Mevan Babakar</name>
    </author>
    <author>
      <name>Christoph Bregler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Grammar, spelling corrections. Minor rewording and clarification of
  one sentence. 24 pages, 31 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.11697v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.11697v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.01464v1</id>
    <updated>2019-01-05T20:23:02Z</updated>
    <published>2019-01-05T20:23:02Z</published>
    <title>The Value of Misinformation and Disinformation</title>
    <summary>  Information is a critical dimension in warfare. Inaccurate information such
as misinformation or disinformation further complicates military operations. In
this paper, we examine the value of misinformation and disinformation to a
military leader who through investment in people, programs and technology is
able to affect the accuracy of information communicated between other actors.
We model the problem as a partially observable stochastic game with three
agents, a leader and two followers. We determine the value to the leader of
misinformation or disinformation being communicated between two (i) adversarial
followers and (ii) allied followers. We demonstrate that only under certain
conditions, the prevalent intuition that the leader would benefit from less
(more) accurate communication between adversarial (allied) followers is valid.
We analyzed why the intuition may fail and show a holistic paradigm taking into
account both the reward structures and policies of agents is necessary in order
to correctly determine the value of misinformation and disinformation. Our
research identifies efficient targeted investments to affect the accuracy of
information communicated between followers to the leader's advantage.
</summary>
    <author>
      <name>Yanling Chang</name>
    </author>
    <author>
      <name>Matthew F. Keblis</name>
    </author>
    <author>
      <name>Ran Li</name>
    </author>
    <author>
      <name>Eleftherios Iakovou</name>
    </author>
    <author>
      <name>Chelsea C. White III</name>
    </author>
    <link href="http://arxiv.org/abs/1901.01464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.01543v1</id>
    <updated>2019-09-04T03:41:44Z</updated>
    <published>2019-09-04T03:41:44Z</published>
    <title>Towards Automatic Detection of Misinformation in Online Medical Videos</title>
    <summary>  Recent years have witnessed a significant increase in the online sharing of
medical information, with videos representing a large fraction of such online
sources. Previous studies have however shown that more than half of the
health-related videos on platforms such as YouTube contain misleading
information and biases. Hence, it is crucial to build computational tools that
can help evaluate the quality of these videos so that users can obtain accurate
information to help inform their decisions. In this study, we focus on the
automatic detection of misinformation in YouTube videos. We select prostate
cancer videos as our entry point to tackle this problem. The contribution of
this paper is twofold. First, we introduce a new dataset consisting of 250
videos related to prostate cancer manually annotated for misinformation.
Second, we explore the use of linguistic, acoustic, and user engagement
features for the development of classification models to identify
misinformation. Using a series of ablation experiments, we show that we can
build automatic models with accuracies of up to 74%, corresponding to a 76.5%
precision and 73.2% recall for misinformative instances.
</summary>
    <author>
      <name>Rui Hou</name>
    </author>
    <author>
      <name>Ver√≥nica P√©rez-Rosas</name>
    </author>
    <author>
      <name>Stacy Loeb</name>
    </author>
    <author>
      <name>Rada Mihalcea</name>
    </author>
    <link href="http://arxiv.org/abs/1909.01543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.01543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01462v2</id>
    <updated>2021-01-21T13:55:23Z</updated>
    <published>2020-10-04T01:36:14Z</published>
    <title>Right and left, partisanship predicts (asymmetric) vulnerability to
  misinformation</title>
    <summary>  We analyze the relationship between partisanship, echo chambers, and
vulnerability to online misinformation by studying news sharing behavior on
Twitter. While our results confirm prior findings that online misinformation
sharing is strongly correlated with right-leaning partisanship, we also uncover
a similar, though weaker trend among left-leaning users. Because of the
correlation between a user's partisanship and their position within a partisan
echo chamber, these types of influence are confounded. To disentangle their
effects, we perform a regression analysis and find that vulnerability to
misinformation is most strongly influenced by partisanship for both left- and
right-leaning users.
</summary>
    <author>
      <name>Dimitar Nikolov</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.37016/mr-2020-55</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.37016/mr-2020-55" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Harvard Kennedy School Misinformation Review, Volume 1, Issue 7,
  2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.01462v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01462v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02164v3</id>
    <updated>2021-09-09T15:55:04Z</updated>
    <published>2020-12-03T18:47:34Z</published>
    <title>People Still Care About Facts: Twitter Users Engage More with Factual
  Discourse than Misinformation--A Comparison Between COVID and General
  Narratives on Twitter</title>
    <summary>  Misinformation entails the dissemination of falsehoods that leads to the slow
fracturing of society via decreased trust in democratic processes,
institutions, and science. The public has grown aware of the role of social
media as a superspreader of untrustworthy information, where even pandemics
have not been immune. In this paper, we focus on COVID-19 misinformation and
examine a subset of 2.1M tweets to understand misinformation as a function of
engagement, tweet content (COVID-19- vs. non-COVID-19-related), and veracity
(misleading or factual). Using correlation analysis, we show the most relevant
feature subsets among over 126 features that most heavily correlate with
misinformation or facts. We found that (i) factual tweets, regardless of
whether COVID-related, were more engaging than misinformation tweets; and (ii)
features that most heavily correlated with engagement varied depending on the
veracity and content of the tweet.
</summary>
    <author>
      <name>Mirela Silva</name>
    </author>
    <author>
      <name>Fabr√≠cio Ceschin</name>
    </author>
    <author>
      <name>Prakash Shrestha</name>
    </author>
    <author>
      <name>Christopher Brant</name>
    </author>
    <author>
      <name>Shlok Gilda</name>
    </author>
    <author>
      <name>Juliana Fernandes</name>
    </author>
    <author>
      <name>Catia S. Silva</name>
    </author>
    <author>
      <name>Andr√© Gr√©gio</name>
    </author>
    <author>
      <name>Daniela Oliveira</name>
    </author>
    <author>
      <name>Luiz Giovanini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.02164v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02164v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.11055v1</id>
    <updated>2020-12-21T00:02:04Z</updated>
    <published>2020-12-21T00:02:04Z</published>
    <title>Social Media COVID-19 Misinformation Interventions Viewed Positively,
  But Have Limited Impact</title>
    <summary>  Amidst COVID-19 misinformation spreading, social media platforms like
Facebook and Twitter rolled out design interventions, including banners linking
to authoritative resources and more specific "false information" labels. In
late March 2020, shortly after these interventions began to appear, we
conducted an exploratory mixed-methods survey (N = 311) to learn: what are
social media users' attitudes towards these interventions, and to what extent
do they self-report effectiveness? We found that most participants indicated a
positive attitude towards interventions, particularly post-specific labels for
misinformation. Still, the majority of participants discovered or corrected
misinformation through other means, most commonly web searches, suggesting room
for platforms to do more to stem the spread of COVID-19 misinformation.
</summary>
    <author>
      <name>Christine Geeng</name>
    </author>
    <author>
      <name>Tiona Francisco</name>
    </author>
    <author>
      <name>Jevin West</name>
    </author>
    <author>
      <name>Franziska Roesner</name>
    </author>
    <link href="http://arxiv.org/abs/2012.11055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.11055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00784v1</id>
    <updated>2020-08-03T11:25:47Z</updated>
    <published>2020-08-03T11:25:47Z</published>
    <title>COVID-19 Misinformation and Disinformation on Social Networks -- The
  Limits of Veritistic Countermeasures</title>
    <summary>  The COVID-19 pandemic has been the subject of a vast amount of
misinformation, particularly in digital information environments, and major
social media platforms recently publicized some of the countermeasures they are
adopting. This presents an opportunity to examine the nature of the
misinformation and disinformation being produced, and the theoretical and
technological paradigm used to counter it. I argue that this approach is based
on a conception of misinformation as epistemic pollution that can only justify
a limited and potentially inadequate response , and that some of the measures
undertaken in practice outrun this. In fact, social networks manage ecological
and architectural conditions that influence discourse on their platforms in
ways that should motivate reconsideration of the justifications that ground
epistemic interventions to combat misinformation, and the types of intervention
that they warrant. The editorial role of platforms should not be framed solely
as the management of epistemic pollution, but instead as managing the epistemic
environment in which narratives and social epistemic processes take place.
There is an element of inevitable epistemic paternalism involved in this, and
exploration of the independent constraints on its justifiability can help
determine proper limits of its exercise in practice.
</summary>
    <author>
      <name>Andrew Buzzell</name>
    </author>
    <link href="http://arxiv.org/abs/2008.00784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.00779v1</id>
    <updated>2021-04-01T21:50:19Z</updated>
    <published>2021-04-01T21:50:19Z</published>
    <title>Misinformation Warning Labels: Twitter's Soft Moderation Effects on
  COVID-19 Vaccine Belief Echoes</title>
    <summary>  Twitter, prompted by the rapid spread of alternative narratives, started
actively warning users about the spread of COVID-19 misinformation. This form
of soft moderation comes in two forms: as a warning cover before the Tweet is
displayed to the user and as a warning tag below the Tweet. This study
investigates how each of the soft moderation forms affects the perceived
accuracy of COVID-19 vaccine misinformation on Twitter. The results suggest
that the warning covers work, but not the tags, in reducing the perception of
accuracy of COVID-19 vaccine misinformation on Twitter. "Belief echoes" do
exist among Twitter users, unfettered by any warning labels, in relationship to
the perceived safety and efficacy of the COVID-19 vaccine as well as the
vaccination hesitancy for themselves and their children. The implications of
these results are discussed in the context of usable security affordances for
combating misinformation on social media.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Raniem Alsaadi</name>
    </author>
    <author>
      <name>Peter Jachim</name>
    </author>
    <author>
      <name>Emma Pieroni</name>
    </author>
    <link href="http://arxiv.org/abs/2104.00779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.13657v3</id>
    <updated>2020-04-02T16:32:15Z</updated>
    <published>2020-03-30T17:44:42Z</published>
    <title>Analysing the Extent of Misinformation in Cancer Related Tweets</title>
    <summary>  Twitter has become one of the most sought after places to discuss a wide
variety of topics, including medically relevant issues such as cancer. This
helps spread awareness regarding the various causes, cures and prevention
methods of cancer. However, no proper analysis has been performed, which
discusses the validity of such claims. In this work, we aim to tackle the
misinformation spread in such platforms. We collect and present a dataset
regarding tweets which talk specifically about cancer and propose an
attention-based deep learning model for automated detection of misinformation
along with its spread. We then do a comparative analysis of the linguistic
variation in the text corresponding to misinformation and truth. This analysis
helps us gather relevant insights on various social aspects related to
misinformed tweets.
</summary>
    <author>
      <name>Rakesh Bal</name>
    </author>
    <author>
      <name>Sayan Sinha</name>
    </author>
    <author>
      <name>Swastika Dutta</name>
    </author>
    <author>
      <name>Rishabh Joshi</name>
    </author>
    <author>
      <name>Sayan Ghosh</name>
    </author>
    <author>
      <name>Ritam Dutt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 14th International Conference on Web and Social
  Media (ICWSM-20)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICWSM 2020, 14, 924-928</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.13657v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13657v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04682v2</id>
    <updated>2020-05-28T07:31:05Z</updated>
    <published>2020-05-10T14:55:50Z</published>
    <title>Exposure to Social Engagement Metrics Increases Vulnerability to
  Misinformation</title>
    <summary>  News feeds in virtually all social media platforms include engagement
metrics, such as the number of times each post is liked and shared. We find
that exposure to these social engagement signals increases the vulnerability of
users to misinformation. This finding has important implications for the design
of social media interactions in the misinformation age. To reduce the spread of
misinformation, we call for technology platforms to rethink the display of
social engagement metrics. Further research is needed to investigate whether
and how engagement metrics can be presented without amplifying the spread of
low-credibility information.
</summary>
    <author>
      <name>Mihai Avram</name>
    </author>
    <author>
      <name>Nicholas Micallef</name>
    </author>
    <author>
      <name>Sameer Patil</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.37016/mr-2020-033</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.37016/mr-2020-033" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">HKS Misinformation Review Vol. 1 (No. 5), 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.04682v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04682v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02462v1</id>
    <updated>2021-03-03T15:13:25Z</updated>
    <published>2021-03-03T15:13:25Z</published>
    <title>University of Copenhagen Participation in TREC Health Misinformation
  Track 2020</title>
    <summary>  In this paper, we describe our participation in the TREC Health
Misinformation Track 2020. We submitted $11$ runs to the Total Recall Task and
13 runs to the Ad Hoc task. Our approach consists of 3 steps: (1) we create an
initial run with BM25 and RM3; (2) we estimate credibility and misinformation
scores for the documents in the initial run; (3) we merge the relevance,
credibility and misinformation scores to re-rank documents in the initial run.
To estimate credibility scores, we implement a classifier which exploits
features based on the content and the popularity of a document. To compute the
misinformation score, we apply a stance detection approach with a pretrained
Transformer language model. Finally, we use different approaches to merge
scores: weighted average, the distance among score vectors and rank fusion.
</summary>
    <author>
      <name>Lucas Chaves Lima</name>
    </author>
    <author>
      <name>Dustin Brandon Wright</name>
    </author>
    <author>
      <name>Isabelle Augenstein</name>
    </author>
    <author>
      <name>Maria Maistro</name>
    </author>
    <link href="http://arxiv.org/abs/2103.02462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.15715v5</id>
    <updated>2022-03-31T18:09:44Z</updated>
    <published>2021-06-29T20:39:17Z</published>
    <title>No Calm in The Storm: Investigating QAnon Website Relationships</title>
    <summary>  QAnon is a far-right conspiracy theory whose followers largely organize
online. In this work, we use web crawls seeded from two of the largest QAnon
hotbeds on the Internet, Voat and 8kun, to build a QAnon-centered domain-based
hyperlink graph. We use this graph to identify, understand, and learn about the
set of websites that spread QAnon content online. Specifically, we curate the
largest list of QAnon centered websites to date, from which we document the
types of QAnon sites, their hosting providers, as well as their popularity. We
further analyze QAnon websites' connection to mainstream news and
misinformation online, highlighting the outsized role misinformation websites
play in spreading the conspiracy. Finally, we leverage the observed
relationship between QAnon and misinformation sites to build a highly accurate
random forest classifier that distinguishes between misinformation and
authentic news sites. Our results demonstrate new and effective ways to study
the growing presence of conspiracy theories and misinformation on the Internet.
</summary>
    <author>
      <name>Hans W. A. Hanley</name>
    </author>
    <author>
      <name>Deepak Kumar</name>
    </author>
    <author>
      <name>Zakir Durumeric</name>
    </author>
    <link href="http://arxiv.org/abs/2106.15715v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15715v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07968v1</id>
    <updated>2021-12-15T08:40:27Z</updated>
    <published>2021-12-15T08:40:27Z</published>
    <title>Science Factionalism: How Group Identity Language Affects Public
  Engagement with Misinformation and Debunking Narratives on a Popular Q&amp;A
  Platform in China</title>
    <summary>  Misinformation and intergroup bias are two pathologies challenging informed
citizenship. This paper examines how identity language is used in
misinformation and debunking messages about controversial science on Chinese
digital public sphere, and their impact on how the public engage with science.
We collected an eight-year time series dataset of public discussion (N=6039) on
one of the most controversial science issues in China (GMO) from a popular Q&amp;A
platform, Zhihu. We found that both misinformation and debunking messages use a
substantial amount of group identity languages when discussing the
controversial science issue, which we define as science factionalism --
discussion about science is divided by factions that are formed upon science
attitudes. We found that posts that use science factionalism receive more
digital votes and comments, even among the science-savvy community in China.
Science factionalism also increases the use of negativity in public discourse.
We discussed the implications of how science factionalism interacts with the
digital attention economy to affect public engagement with science
misinformation.
</summary>
    <author>
      <name>Kaiping Chen</name>
    </author>
    <author>
      <name>Yepeng Jin</name>
    </author>
    <author>
      <name>Anqi Shao</name>
    </author>
    <link href="http://arxiv.org/abs/2112.07968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07076v3</id>
    <updated>2023-05-01T20:18:08Z</updated>
    <published>2022-01-18T16:00:03Z</published>
    <title>Mitigating Misinformation Spread on Blockchain Enabled Social Media
  Networks</title>
    <summary>  The paper develops a blockchain protocol for a social media network (BE-SMN)
to mitigate the spread of misinformation. BE-SMN is derived based on the
information transmission-time distribution by modeling the misinformation
transmission as double-spend attacks on blockchain. The misinformation
distribution is then incorporated into the SIR (Susceptible, Infectious, or
Recovered) model, which substitutes the single rate parameter in the
traditional SIR model. Then, on a multi-community network, we study the
propagation of misinformation numerically and show that the proposed blockchain
enabled social media network outperforms the baseline network in flattening the
curve of the infected population.
</summary>
    <author>
      <name>Rui Luo</name>
    </author>
    <author>
      <name>Vikram Krishnamurthy</name>
    </author>
    <author>
      <name>Erik Blasch</name>
    </author>
    <link href="http://arxiv.org/abs/2201.07076v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07076v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07706v2</id>
    <updated>2022-07-31T00:50:37Z</updated>
    <published>2022-02-15T20:14:54Z</published>
    <title>Misinformation Detection in Social Media Video Posts</title>
    <summary>  With the growing adoption of short-form video by social media platforms,
reducing the spread of misinformation through video posts has become a critical
challenge for social media providers. In this paper, we develop methods to
detect misinformation in social media posts, exploiting modalities such as
video and text. Due to the lack of large-scale public data for misinformation
detection in multi-modal datasets, we collect 160,000 video posts from Twitter,
and leverage self-supervised learning to learn expressive representations of
joint visual and textual data. In this work, we propose two new methods for
detecting semantic inconsistencies within short-form social media video posts,
based on contrastive learning and masked language modeling. We demonstrate that
our new approaches outperform current state-of-the-art methods on both
artificial data generated by random-swapping of positive samples and in the
wild on a new manually-labeled test set for semantic misinformation.
</summary>
    <author>
      <name>Kehan Wang</name>
    </author>
    <author>
      <name>David Chan</name>
    </author>
    <author>
      <name>Seth Z. Zhao</name>
    </author>
    <author>
      <name>John Canny</name>
    </author>
    <author>
      <name>Avideh Zakhor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We discovered an error in our dataset construction where retweets
  were not properly filtered. This resulted in test data leakage in training
  data, and the results reported are affected</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.07706v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07706v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.00710v2</id>
    <updated>2023-01-21T01:48:59Z</updated>
    <published>2022-03-01T19:15:16Z</published>
    <title>Understanding Effects of Algorithmic vs. Community Label on Perceived
  Accuracy of Hyper-partisan Misinformation</title>
    <summary>  Hyper-partisan misinformation has become a major public concern. In order to
examine what type of misinformation label can mitigate hyper-partisan
misinformation sharing on social media, we conducted a 4 (label type:
algorithm, community, third-party fact-checker, and no label) X 2 (post
ideology: liberal vs. conservative) between-subjects online experiment (N =
1,677) in the context of COVID-19 health information. The results suggest that
for liberal users, all labels reduced the perceived accuracy and believability
of fake posts regardless of the posts' ideology. In contrast, for conservative
users, the efficacy of the labels depended on whether the posts were
ideologically consistent: algorithmic labels were more effective in reducing
the perceived accuracy and believability of fake conservative posts compared to
community labels, whereas all labels were effective in reducing their belief in
liberal posts. Our results shed light on the differing effects of various
misinformation labels dependent on people's political ideology.
</summary>
    <author>
      <name>Chenyan Jia</name>
    </author>
    <author>
      <name>Alexander Boltz</name>
    </author>
    <author>
      <name>Angie Zhang</name>
    </author>
    <author>
      <name>Anqing Chen</name>
    </author>
    <author>
      <name>Min Kyung Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3555096</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3555096" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the ACM: Human-Computer Interaction. Issue CSCW
  (CSCW 2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.00710v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00710v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.10080v1</id>
    <updated>2022-04-21T13:22:48Z</updated>
    <published>2022-04-21T13:22:48Z</published>
    <title>Identifying and Characterizing Active Citizens who Refute Misinformation
  in Social Media</title>
    <summary>  The phenomenon of misinformation spreading in social media has developed a
new form of active citizens who focus on tackling the problem by refuting posts
that might contain misinformation. Automatically identifying and characterizing
the behavior of such active citizens in social media is an important task in
computational social science for complementing studies in misinformation
analysis. In this paper, we study this task across different social media
platforms (i.e., Twitter and Weibo) and languages (i.e., English and Chinese)
for the first time. To this end, (1) we develop and make publicly available a
new dataset of Weibo users mapped into one of the two categories (i.e.,
misinformation posters or active citizens); (2) we evaluate a battery of
supervised models on our new Weibo dataset and an existing Twitter dataset
which we repurpose for the task; and (3) we present an extensive analysis of
the differences in language use between the two user categories.
</summary>
    <author>
      <name>Yida Mu</name>
    </author>
    <author>
      <name>Pu Niu</name>
    </author>
    <author>
      <name>Nikolaos Aletras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM WebSci 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.10080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.12294v1</id>
    <updated>2022-04-26T13:18:27Z</updated>
    <published>2022-04-26T13:18:27Z</published>
    <title>Monant Medical Misinformation Dataset: Mapping Articles to Fact-Checked
  Claims</title>
    <summary>  False information has a significant negative influence on individuals as well
as on the whole society. Especially in the current COVID-19 era, we witness an
unprecedented growth of medical misinformation. To help tackle this problem
with machine learning approaches, we are publishing a feature-rich dataset of
approx. 317k medical news articles/blogs and 3.5k fact-checked claims. It also
contains 573 manually and more than 51k automatically labelled mappings between
claims and articles. Mappings consist of claim presence, i.e., whether a claim
is contained in a given article, and article stance towards the claim. We
provide several baselines for these two tasks and evaluate them on the manually
labelled part of the dataset. The dataset enables a number of additional tasks
related to medical misinformation, such as misinformation characterisation
studies or studies of misinformation diffusion between sources.
</summary>
    <author>
      <name>Ivan Srba</name>
    </author>
    <author>
      <name>Branislav Pecher</name>
    </author>
    <author>
      <name>Matus Tomlein</name>
    </author>
    <author>
      <name>Robert Moro</name>
    </author>
    <author>
      <name>Elena Stefancova</name>
    </author>
    <author>
      <name>Jakub Simko</name>
    </author>
    <author>
      <name>Maria Bielikova</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3477495.3531726</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3477495.3531726" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures, SIGIR 2022 Resource paper track</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGIR Conference on Research and Development in Information
  Retrieval (SIGIR 2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.12294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.12294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13568v2</id>
    <updated>2022-04-29T15:02:59Z</updated>
    <published>2022-04-28T15:31:13Z</published>
    <title>Justice in Misinformation Detection Systems: An Analysis of Algorithms,
  Stakeholders, and Potential Harms</title>
    <summary>  Faced with the scale and surge of misinformation on social media, many
platforms and fact-checking organizations have turned to algorithms for
automating key parts of misinformation detection pipelines. While offering a
promising solution to the challenge of scale, the ethical and societal risks
associated with algorithmic misinformation detection are not well-understood.
In this paper, we employ and extend upon the notion of informational justice to
develop a framework for explicating issues of justice relating to
representation, participation, distribution of benefits and burdens, and
credibility in the misinformation detection pipeline. Drawing on the framework:
(1) we show how injustices materialize for stakeholders across three
algorithmic stages in the pipeline; (2) we suggest empirical measures for
assessing these injustices; and (3) we identify potential sources of these
harms. This framework should help researchers, policymakers, and practitioners
reason about potential harms or risks associated with these algorithms and
provide conceptual guidance for the design of algorithmic fairness audits in
this domain.
</summary>
    <author>
      <name>Terrence Neumann</name>
    </author>
    <author>
      <name>Maria De-Arteaga</name>
    </author>
    <author>
      <name>Sina Fazelpour</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3531146.3533205</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3531146.3533205" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM Conference on Fairness, Accountability, and
  Transparenct (FAccT), 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.13568v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13568v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.09964v1</id>
    <updated>2022-09-20T19:47:27Z</updated>
    <published>2022-09-20T19:47:27Z</published>
    <title>The Morbid Realities of Social Media: An Investigation into the
  Misinformation Shared by the Deceased Victims of COVID-19</title>
    <summary>  Social media platforms have had considerable impact on the real world
especially during the Covid-19 pandemic. Misinformation related to Covid-19
might have caused significant impact on the population specifically due to its
association with dangerous beliefs such as anti-vaccination and Covid denial.
In this work, we study a unique dataset of Facebook posts by users who shared
and believed in Covid-19 misinformation before succumbing to Covid-19 often
resulting in death. We aim to characterize the dominant themes and sources
present in the victim's posts along with identifying the role of the platform
in handling deadly narratives. Our analysis reveals the overwhelming
politicization of Covid-19 through the prevalence of anti-government themes
propagated by right-wing political and media ecosystem. Furthermore, we
highlight the failures of Facebook's implementation and completeness of soft
moderation actions intended to warn users of misinformation. Results from this
study bring insights into the responsibility of political elites in shaping
public discourse and the platform's role in dampening the reach of harmful
misinformation.
</summary>
    <author>
      <name>Hussam Habib</name>
    </author>
    <author>
      <name>Rishab Nithyanand</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1609/icwsm.v17i1.22147</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1609/icwsm.v17i1.22147" rel="related"/>
    <link href="http://arxiv.org/abs/2209.09964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.06517v2</id>
    <updated>2024-04-02T10:01:15Z</updated>
    <published>2023-01-16T16:55:06Z</published>
    <title>The Landscape of User-centered Misinformation Interventions -- A
  Systematic Literature Review</title>
    <summary>  Misinformation is one of the key challenges facing society today.
User-centered misinformation interventions as digital countermeasures that
exert a direct influence on users represent a promising means to deal with the
large amounts of information available. While an extensive body of research on
this topic exists, researchers are confronted with a diverse research landscape
spanning multiple disciplines. This review systematizes the landscape of
user-centered misinformation interventions to facilitate knowledge transfer,
identify trends, and enable informed decision-making. Over 5,700 scholarly
publications were screened and a systematic literature review (N=163) was
conducted. A taxonomy was derived regarding intervention design (e.g., (binary)
label), user interaction (active or passive), and timing (e.g., post exposure
to misinformation). We provide a structured overview of approaches across
multiple disciplines, and derive six overarching challenges for future
research.
</summary>
    <author>
      <name>Katrin Hartwig</name>
    </author>
    <author>
      <name>Frederic Doell</name>
    </author>
    <author>
      <name>Christian Reuter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3674724</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3674724" rel="related"/>
    <link href="http://arxiv.org/abs/2301.06517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.06517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.07788v1</id>
    <updated>2023-02-15T17:20:02Z</updated>
    <published>2023-02-15T17:20:02Z</published>
    <title>Fight Fire with Fire: Hacktivists' Take on Social Media Misinformation</title>
    <summary>  In this study, we interviewed 22 prominent hacktivists to learn their take on
the increased proliferation of misinformation on social media. We found that
none of them welcomes the nefarious appropriation of trolling and memes for the
purpose of political (counter)argumentation and dissemination of propaganda.
True to the original hacker ethos, misinformation is seen as a threat to the
democratic vision of the Internet, and as such, it must be confronted on the
face with tried hacktivists' methods like deplatforming the "misinformers" and
doxing or leaking data about their funding and recruitment. The majority of the
hacktivists also recommended interventions for raising misinformation literacy
in addition to targeted hacking campaigns. We discuss the implications of these
findings relative to the emergent recasting of hacktivism in defense of a
constructive and factual social media discourse.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Benjamin Kessell</name>
    </author>
    <link href="http://arxiv.org/abs/2302.07788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.07788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.01516v1</id>
    <updated>2023-07-04T06:56:18Z</updated>
    <published>2023-07-04T06:56:18Z</published>
    <title>Noisy Games: A Study on the Effect of Noise on Game Specifications</title>
    <summary>  We consider misinformation games, i.e., multi-agent interactions where the
players are misinformed with regards to the game that they play, essentially
having an \emph{incorrect} understanding of the game setting, without being
aware of their misinformation. In this paper, we introduce and study a new
family of misinformation games, called Noisy games, where misinformation is due
to structured (white) noise that affects additively the payoff values of
players. We analyse the general properties of Noisy games and derive
theoretical formulas related to ``behavioural consistency'', i.e., the
probability that the players behaviour will not be significantly affected by
the noise. We show several properties of these formulas, and present an
experimental evaluation that validates and visualises these results.
</summary>
    <author>
      <name>Constantinos Varsos</name>
    </author>
    <author>
      <name>Giorgos Flouris</name>
    </author>
    <author>
      <name>Marina Bitsaki</name>
    </author>
    <link href="http://arxiv.org/abs/2307.01516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.01516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.11029v1</id>
    <updated>2023-09-20T03:19:11Z</updated>
    <published>2023-09-20T03:19:11Z</published>
    <title>Trustworthiness Evaluations of Search Results: The Impact of Rank and
  Misinformation</title>
    <summary>  Users rely on search engines for information in critical contexts, such as
public health emergencies. Understanding how users evaluate the trustworthiness
of search results is therefore essential. Research has identified rank and the
presence of misinformation as factors impacting perceptions and click behavior
in search. Here, we elaborate on these findings by measuring the effects of
rank and misinformation, as well as warning banners, on the perceived
trustworthiness of individual results in search. We conducted three online
experiments (N=3196) using Covid-19-related queries to address this question.
We show that although higher-ranked results are clicked more often, they are
not more trusted. We also show that misinformation did not change trust in
accurate results below it. However, a warning about unreliable sources
backfired, decreasing trust in accurate information but not misinformation.
This work addresses concerns about how people evaluate information in search,
and illustrates the dangers of generic prevention approaches.
</summary>
    <author>
      <name>Sterling Williams-Ceci</name>
    </author>
    <author>
      <name>Michael Macy</name>
    </author>
    <author>
      <name>Mor Naaman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 10 figures, 4 supplementary files</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.11029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.11029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.05487v2</id>
    <updated>2023-11-10T13:50:37Z</updated>
    <published>2023-11-09T16:22:10Z</published>
    <title>News and Misinformation Consumption in Europe: A Longitudinal
  Cross-Country Perspective</title>
    <summary>  The Internet and social media have transformed news availability and
accessibility, reshaping information consumption and production. However, they
can also facilitate the rapid spread of misinformation, posing significant
societal challenges. To combat misinformation effectively, it is crucial to
understand the online information environment and news consumption patterns.
Most existing research has primarily focused on single topics or individual
countries, lacking cross-country comparisons. This study investigated
information consumption in four European countries, analyzing three years of
Twitter activity from news outlet accounts in France, Germany, Italy, and the
UK and focusing on the role of misinformation sources. Our work offers a
perspective on how topics of European significance are interpreted across
various countries. Results indicate that reliable sources dominate the
information landscape, although unreliable content is still present across all
countries and topics. While most users engage with reliable sources, a small
percentage consume questionable content. Interestingly, few users have a mixed
information diet, bridging the gap between questionable and reliable news in
the similarity network. Cross-country comparisons revealed differences in
audience overlap of news sources, offering valuable guidance for policymakers
and scholars in developing effective and tailored solutions to combat
misinformation.
</summary>
    <author>
      <name>Anees Baqir</name>
    </author>
    <author>
      <name>Alessandro Galeazzi</name>
    </author>
    <author>
      <name>Fabiana Zollo</name>
    </author>
    <link href="http://arxiv.org/abs/2311.05487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.05487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.14200v1</id>
    <updated>2023-11-23T20:57:26Z</updated>
    <published>2023-11-23T20:57:26Z</published>
    <title>Prebunking Design as a Defense Mechanism Against Misinformation
  Propagation on Social Networks</title>
    <summary>  The growing reliance on social media for news consumption necessitates
effective countermeasures to mitigate the rapid spread of misinformation.
Prebunking, a proactive method that arms users with accurate information before
they come across false content, has garnered support from journalism and
psychology experts. We formalize the problem of optimal prebunking as
optimizing the timing of delivering accurate information, ensuring users
encounter it before receiving misinformation while minimizing the disruption to
user experience. Utilizing a susceptible-infected epidemiological process to
model the propagation of misinformation, we frame optimal prebunking as a
policy synthesis problem with safety constraints. We then propose a policy that
approximates the optimal solution to a relaxed problem. The experiments show
that this policy cuts the user experience cost of repeated information delivery
in half, compared to delivering accurate information immediately after
identifying a misinformation propagation.
</summary>
    <author>
      <name>Yigit Ege Bayiz</name>
    </author>
    <author>
      <name>Ufuk Topcu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures, Submitted to PERCOM 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.14200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.14200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.02191v1</id>
    <updated>2024-01-04T10:47:07Z</updated>
    <published>2024-01-04T10:47:07Z</published>
    <title>Characterizing Fake News Targeting Corporations</title>
    <summary>  Misinformation proliferates in the online sphere, with evident impacts on the
political and social realms, influencing democratic discourse and posing risks
to public health and safety. The corporate world is also a prime target for
fake news dissemination. While recent studies have attempted to characterize
corporate misinformation and its effects on companies, their findings often
suffer from limitations due to qualitative or narrative approaches and a narrow
focus on specific industries. To address this gap, we conducted an analysis
utilizing social media quantitative methods and crowd-sourcing studies to
investigate corporate misinformation across a diverse array of industries
within the S\&amp;P 500 companies. Our study reveals that corporate misinformation
encompasses topics such as products, politics, and societal issues. We
discovered companies affected by fake news also get reputable news coverage but
less social media attention, leading to heightened negativity in social media
comments, diminished stock growth, and increased stress mentions among employee
reviews. Additionally, we observe that a company is not targeted by fake news
all the time, but there are particular times when a critical mass of fake news
emerges. These findings hold significant implications for regulators, business
leaders, and investors, emphasizing the necessity to vigilantly monitor the
escalating phenomenon of corporate misinformation.
</summary>
    <author>
      <name>Ke Zhou</name>
    </author>
    <author>
      <name>Sanja Scepanovic</name>
    </author>
    <author>
      <name>Daniele Quercia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ICWSM 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.02191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.02191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.04481v1</id>
    <updated>2024-01-09T10:38:13Z</updated>
    <published>2024-01-09T10:38:13Z</published>
    <title>Fighting Fire with Fire: Adversarial Prompting to Generate a
  Misinformation Detection Dataset</title>
    <summary>  The recent success in language generation capabilities of large language
models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns
about their possible misuse in inducing mass agitation and communal hatred via
generating fake news and spreading misinformation. Traditional means of
developing a misinformation ground-truth dataset does not scale well because of
the extensive manual effort required to annotate the data. In this paper, we
propose an LLM-based approach of creating silver-standard ground-truth datasets
for identifying misinformation. Specifically speaking, given a trusted news
article, our proposed approach involves prompting LLMs to automatically
generate a summarised version of the original article. The prompts in our
proposed approach act as a controlling mechanism to generate specific types of
factual incorrectness in the generated summaries, e.g., incorrect quantities,
false attributions etc. To investigate the usefulness of this dataset, we
conduct a set of experiments where we train a range of supervised models for
the task of misinformation detection.
</summary>
    <author>
      <name>Shrey Satapara</name>
    </author>
    <author>
      <name>Parth Mehta</name>
    </author>
    <author>
      <name>Debasis Ganguly</name>
    </author>
    <author>
      <name>Sandip Modha</name>
    </author>
    <link href="http://arxiv.org/abs/2401.04481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.04481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.03642v1</id>
    <updated>2024-02-06T02:39:59Z</updated>
    <published>2024-02-06T02:39:59Z</published>
    <title>Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish
  Misinformation</title>
    <summary>  The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide
high-quality, annotated, 5-way stance data extracted from Twitter, suitable for
analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus
2.0 iteration, we extend this framework to encompass Russian and Spanish. The
former is of current significance due to prevalent misinformation amid
escalating tensions with the West and the violent incursion into Ukraine. The
latter, meanwhile, represents an enormous community that has been largely
overlooked on major social media platforms. By incorporating an additional
3,874 Spanish and Russian tweets over 41 misinformation claims, our objective
is to support research focused on these issues. To demonstrate the value of
this data, we employed zero-shot cross-lingual transfer on multilingual BERT,
yielding results on par with the initial Stanceosaurus study with a macro F1
score of 43 for both languages. This underlines the viability of stance
classification as an effective tool for identifying multicultural
misinformation.
</summary>
    <author>
      <name>Anton Lavrouk</name>
    </author>
    <author>
      <name>Ian Ligon</name>
    </author>
    <author>
      <name>Tarek Naous</name>
    </author>
    <author>
      <name>Jonathan Zheng</name>
    </author>
    <author>
      <name>Alan Ritter</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WNUT2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.03642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.03642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.12913v1</id>
    <updated>2024-03-19T17:11:25Z</updated>
    <published>2024-03-19T17:11:25Z</published>
    <title>Fact Checking Chatbot: A Misinformation Intervention for Instant
  Messaging Apps and an Analysis of Trust in the Fact Checkers</title>
    <summary>  In Singapore, there has been a rise in misinformation on mobile instant
messaging services (MIMS). MIMS support both small peer-to-peer networks and
large groups. Misinformation in the former may spread due to recipients' trust
in the sender while in the latter, misinformation can directly reach a wide
audience. The encryption of MIMS makes it difficult to address misinformation
directly. As such, chatbots have become an alternative solution where users can
disclose their chat content directly to fact checking services. To understand
how effective fact checking chatbots are as an intervention and how trust in
three different fact checkers (i.e., Government, News Outlets, and Artificial
Intelligence) may affect this trust, we conducted a within-subjects experiment
with 527 Singapore residents. We found mixed results for the fact checkers but
support for the chatbot intervention overall. We also found a striking
contradiction between participants' trust in the fact checkers and their
behaviour towards them. Specifically, those who reported a high level of trust
in the government performed worse and tended to follow the fact checking tool
less when it was endorsed by the government.
</summary>
    <author>
      <name>Gionnieve Lim</name>
    </author>
    <author>
      <name>Simon T. Perrault</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-94-024-2225-2_11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-94-024-2225-2_11" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mobile Communication and Online Falsehoods in Asia, 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.12913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.12913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.01467v1</id>
    <updated>2024-04-01T20:33:29Z</updated>
    <published>2024-04-01T20:33:29Z</published>
    <title>Transnational Network Dynamics of Problematic Information Diffusion</title>
    <summary>  This study maps the spread of two cases of COVID-19 conspiracy theories and
misinformation in Spanish and French in Latin American and French-speaking
communities on Facebook, and thus contributes to understanding the dynamics,
reach and consequences of emerging transnational misinformation networks. The
findings show that co-sharing behavior of public Facebook groups created
transnational networks by sharing videos of Medicos por la Verdad (MPV)
conspiracy theories in Spanish and hydroxychloroquine-related misinformation
sparked by microbiologist Didier Raoult (DR) in French, usually igniting the
surge of locally led interest groups across the Global South. Using inferential
methods, the study shows how these networks are enabled primarily by shared
cultural and thematic attributes among Facebook groups, effectively creating
very large, networked audiences. The study contributes to the understanding of
how potentially harmful conspiracy theories and misinformation transcend
national borders through non-English speaking online communities, further
highlighting the overlooked role of transnationalism in global misinformation
diffusion and the potentially disproportionate harm that it causes in
vulnerable communities across the globe.
</summary>
    <author>
      <name>Esteban Villa-Turek</name>
    </author>
    <author>
      <name>Rod Abhari</name>
    </author>
    <author>
      <name>Erik C. Nisbet</name>
    </author>
    <author>
      <name>Yu Xu</name>
    </author>
    <author>
      <name>Ayse Deniz Lokmanoglu</name>
    </author>
    <link href="http://arxiv.org/abs/2404.01467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.01467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.15673v1</id>
    <updated>2024-04-24T06:03:07Z</updated>
    <published>2024-04-24T06:03:07Z</published>
    <title>Augmented CARDS: A machine learning approach to identifying triggers of
  climate change misinformation on Twitter</title>
    <summary>  Misinformation about climate change poses a significant threat to societal
well-being, prompting the urgent need for effective mitigation strategies.
However, the rapid proliferation of online misinformation on social media
platforms outpaces the ability of fact-checkers to debunk false claims.
Automated detection of climate change misinformation offers a promising
solution. In this study, we address this gap by developing a two-step
hierarchical model, the Augmented CARDS model, specifically designed for
detecting contrarian climate claims on Twitter. Furthermore, we apply the
Augmented CARDS model to five million climate-themed tweets over a six-month
period in 2022. We find that over half of contrarian climate claims on Twitter
involve attacks on climate actors or conspiracy theories. Spikes in climate
contrarianism coincide with one of four stimuli: political events, natural
events, contrarian influencers, or convinced influencers. Implications for
automated responses to climate misinformation are discussed.
</summary>
    <author>
      <name>Cristian Rojas</name>
    </author>
    <author>
      <name>Frank Algra-Maschio</name>
    </author>
    <author>
      <name>Mark Andrejevic</name>
    </author>
    <author>
      <name>Travis Coan</name>
    </author>
    <author>
      <name>John Cook</name>
    </author>
    <author>
      <name>Yuan-Fang Li</name>
    </author>
    <link href="http://arxiv.org/abs/2404.15673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.15673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.05599v1</id>
    <updated>2024-07-08T04:21:58Z</updated>
    <published>2024-07-08T04:21:58Z</published>
    <title>Generative Debunking of Climate Misinformation</title>
    <summary>  Misinformation about climate change causes numerous negative impacts,
necessitating corrective responses. Psychological research has offered various
strategies for reducing the influence of climate misinformation, such as the
fact-myth-fallacy-fact-structure. However, practically implementing corrective
interventions at scale represents a challenge. Automatic detection and
correction of misinformation offers a solution to the misinformation problem.
This study documents the development of large language models that accept as
input a climate myth and produce a debunking that adheres to the
fact-myth-fallacy-fact (``truth sandwich'') structure, by incorporating
contrarian claim classification and fallacy detection into an LLM prompting
framework. We combine open (Mixtral, Palm2) and proprietary (GPT-4) LLMs with
prompting strategies of varying complexity. Experiments reveal promising
performance of GPT-4 and Mixtral if combined with structured prompts. We
identify specific challenges of debunking generation and human evaluation, and
map out avenues for future work. We release a dataset of high-quality
truth-sandwich debunkings, source code and a demo of the debunking system.
</summary>
    <author>
      <name>Francisco Zanartu</name>
    </author>
    <author>
      <name>Yulia Otmakhova</name>
    </author>
    <author>
      <name>John Cook</name>
    </author>
    <author>
      <name>Lea Frermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepter to ClimateNLP 2024 workshop at ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.05599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.05599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.15186v2</id>
    <updated>2025-05-05T10:03:05Z</updated>
    <published>2024-08-27T16:41:13Z</published>
    <title>Easy-access online social media metrics can foster the identification of
  misinformation sharing users</title>
    <summary>  Misinformation poses a significant challenge studied extensively by
researchers, yet acquiring data to identify primary sharers is time-consuming
and challenging. To address this, we propose a low-barrier approach to
differentiate social media users who are more likely to share misinformation
from those who are less likely. Leveraging insights from previous studies, we
demonstrate that easy-access online social network metrics -- average daily
tweet count, and account age -- can be leveraged to help identify potential low
factuality content spreaders on X (previously known as Twitter). We find that
higher tweet frequency is positively associated with low factuality in shared
content, while account age is negatively associated with it. We also find that
some of the effects, namely the effect of the number of accounts followed and
the number of tweets produced, differ depending on the number of followers a
user has. Our findings show that relying on these easy-access social network
metrics could serve as a low-barrier approach for initial identification of
users who are more likely to spread misinformation, and therefore contribute to
combating misinformation effectively on social media platforms.
</summary>
    <author>
      <name>J√∫lia Sz√°mely</name>
    </author>
    <author>
      <name>Alessandro Galeazzi</name>
    </author>
    <author>
      <name>J√∫lia Koltai</name>
    </author>
    <author>
      <name>Elisa Omodei</name>
    </author>
    <link href="http://arxiv.org/abs/2408.15186v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.15186v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.18393v1</id>
    <updated>2024-09-27T02:16:09Z</updated>
    <published>2024-09-27T02:16:09Z</published>
    <title>Social media algorithms can curb misinformation, but do they?</title>
    <summary>  A recent article in $\textit{Science}$ by Guess et al. estimated the effect
of Facebook's news feed algorithm on exposure to misinformation and political
information among Facebook users. However, its reporting and conclusions did
not account for a series of temporary emergency changes to Facebook's news feed
algorithm in the wake of the 2020 U.S. presidential election that were designed
to diminish the spread of voter-fraud misinformation. Here, we demonstrate that
these emergency measures systematically reduced the amount of misinformation in
the control group of the study, which was using the news feed algorithm. This
issue may have led readers to misinterpret the results of the study and to
conclude that the Facebook news feed algorithm used outside of the study period
mitigates political misinformation as compared to reverse chronological feed.
</summary>
    <author>
      <name>Chhandak Bagchi</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <author>
      <name>Jennifer Lundquist</name>
    </author>
    <author>
      <name>Monideepa Tarafdar</name>
    </author>
    <author>
      <name>Anthony Paik</name>
    </author>
    <author>
      <name>Przemyslaw A. Grabowicz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.13787981</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.13787981" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Science eLetter 26 Sep 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2409.18393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.18393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18390v2</id>
    <updated>2025-03-29T21:19:38Z</updated>
    <published>2024-10-24T03:02:03Z</published>
    <title>Monolingual and Multilingual Misinformation Detection for Low-Resource
  Languages: A Comprehensive Survey</title>
    <summary>  In today's global digital landscape, misinformation transcends linguistic
boundaries, posing a significant challenge for moderation systems. Most
approaches to misinformation detection are monolingual, focused on
high-resource languages, i.e., a handful of world languages that have benefited
from substantial research investment. This survey provides a comprehensive
overview of the current research on misinformation detection in low-resource
languages, both in monolingual and multilingual settings. We review existing
datasets, methodologies, and tools used in these domains, identifying key
challenges related to: data resources, model development, cultural and
linguistic context, and real-world applications. We examine emerging
approaches, such as language-generalizable models and multi-modal techniques,
and emphasize the need for improved data collection practices,
interdisciplinary collaboration, and stronger incentives for socially
responsible AI research. Our findings underscore the importance of systems
capable of addressing misinformation across diverse linguistic and cultural
contexts.
</summary>
    <author>
      <name>Xinyu Wang</name>
    </author>
    <author>
      <name>Wenbo Zhang</name>
    </author>
    <author>
      <name>Sarah Rajtmajer</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18390v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18390v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18670v1</id>
    <updated>2024-10-24T12:00:51Z</updated>
    <published>2024-10-24T12:00:51Z</published>
    <title>Health Misinformation in Social Networks: A Survey of IT Approaches</title>
    <summary>  In this paper, we present a comprehensive survey on the pervasive issue of
medical misinformation in social networks from the perspective of information
technology. The survey aims at providing a systematic review of related
research and helping researchers and practitioners navigate through this
fast-changing field. Specifically, we first present manual and automatic
approaches for fact-checking. We then explore fake news detection methods,
using content, propagation features, or source features, as well as mitigation
approaches for countering the spread of misinformation. We also provide a
detailed list of several datasets on health misinformation and of publicly
available tools. We conclude the survey with a discussion on the open
challenges and future research directions in the battle against health
misinformation.
</summary>
    <author>
      <name>Vasiliki Papanikou</name>
    </author>
    <author>
      <name>Panagiotis Papadakos</name>
    </author>
    <author>
      <name>Theodora Karamanidou</name>
    </author>
    <author>
      <name>Thanos G. Stavropoulos</name>
    </author>
    <author>
      <name>Evaggelia Pitoura</name>
    </author>
    <author>
      <name>Panayiotis Tsaparas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint -- Under review in the ACM Transactions on Computing for
  Healthcare (HEALTH) journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.00825v2</id>
    <updated>2025-03-08T23:01:13Z</updated>
    <published>2024-10-29T02:34:52Z</published>
    <title>Transparent Tagging for Strategic Social Nudges on User-Generated
  Misinformation</title>
    <summary>  Social network platforms (SNP) rely heavily on user-generated content to
attract users, yet they have limited control over content provision, which
leads to misinformation. As countermeasures, SNPs have implemented policies to
notify users by tagging the content and influencing users' responses to the
tagged content. The population-level response creates a social nudge to the
content provider that encourages it to supply more authentic content. Yet, when
designing tags to leverage social nudges, SNP must be cautious about
misdetection, which impairs its ability to create social nudges. We establish a
Bayesian persuaded branching process to study SNP's tagging policy design under
misdetection. Misinformation circulation is modeled by a multi-type branching
process, where users are persuaded through tags to give positive/negative
comments that influence misinformation spread. When translated into posterior
belief space, the SNP's problem is reduced to an equality-constrained
optimization, the optimal condition of which is given by the Lagrangian
characterization. The key finding is that SNP's optimal policy is transparent
tagging, albeit misdetection, which nudges the provider not to generate
misinformation.
</summary>
    <author>
      <name>Ya-Ting Yang</name>
    </author>
    <author>
      <name>Tao Li</name>
    </author>
    <author>
      <name>Quanyan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.00825v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.00825v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.18817v1</id>
    <updated>2024-11-27T23:40:16Z</updated>
    <published>2024-11-27T23:40:16Z</published>
    <title>The Collaborative Practices and Motivations of Online Communities
  Dedicated to Voluntary Misinformation Response</title>
    <summary>  Responding to misinformation online can be an exhausting and thankless task.
It takes time and energy to write effective content, puts users at risk of
online harassment, and strains personal relationships. Despite these
challenges, there are people who voluntarily respond to misinformation online,
and some have established communities on platforms such as Reddit, Discord, and
X (formerly Twitter) dedicated to these efforts. In this work, we interviewed 8
people who participate in such communities to understand the type of support
they receive from each other in these discussion spaces. Interviewees described
that their communities helped them sustain motivation, save time, and improve
their communication skills. Common practices included sharing sources and
citations, providing emotional support, giving others advice, and signaling
positive feedback. We present our findings as three case studies and discuss
opportunities for future work to support collaborative practices in online
communities dedicated to misinformation response. Our work surfaces how
resource sharing, social motivation, and decentralization can make
misinformation correction more sustainable, rewarding, and effective for online
citizens.
</summary>
    <author>
      <name>Jina Yoon</name>
    </author>
    <author>
      <name>Shreya Sathyanarayanan</name>
    </author>
    <author>
      <name>Franziska Roesner</name>
    </author>
    <author>
      <name>Amy X. Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3701209</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3701209" rel="related"/>
    <link href="http://arxiv.org/abs/2411.18817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.18817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.18618v1</id>
    <updated>2024-12-13T20:22:36Z</updated>
    <published>2024-12-13T20:22:36Z</published>
    <title>Exploring Text Representations for Online Misinformation</title>
    <summary>  Mis- and disinformation, commonly collectively called fake news, continue to
menace society. Perhaps, the impact of this age-old problem is presently most
plain in politics and healthcare. However, fake news is affecting an increasing
number of domains. It takes many different forms and continues to shapeshift as
technology advances. Though it arguably most widely spreads in textual form,
e.g., through social media posts and blog articles. Thus, it is imperative to
thwart the spread of textual misinformation, which necessitates its initial
detection. This thesis contributes to the creation of representations that are
useful for detecting misinformation. Firstly, it develops a novel method for
extracting textual features from news articles for misinformation detection.
These features harness the disparity between the thematic coherence of
authentic and false news stories. In other words, the composition of themes
discussed in both groups significantly differs as the story progresses.
Secondly, it demonstrates the effectiveness of topic features for fake news
detection, using classification and clustering. Clustering is particularly
useful because it alleviates the need for a labelled dataset, which can be
labour-intensive and time-consuming to amass. More generally, it contributes
towards a better understanding of misinformation and ways of detecting it using
Machine Learning and Natural Language Processing.
</summary>
    <author>
      <name>Martins Samuel Dogo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Masters Thesis, 106 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.18618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.00752v1</id>
    <updated>2025-02-02T11:18:05Z</updated>
    <published>2025-02-02T11:18:05Z</published>
    <title>Zero-Shot Warning Generation for Misinformative Multimodal Content</title>
    <summary>  The widespread prevalence of misinformation poses significant societal
concerns. Out-of-context misinformation, where authentic images are paired with
false text, is particularly deceptive and easily misleads audiences. Most
existing detection methods primarily evaluate image-text consistency but often
lack sufficient explanations, which are essential for effectively debunking
misinformation. We present a model that detects multimodal misinformation
through cross-modality consistency checks, requiring minimal training time.
Additionally, we propose a lightweight model that achieves competitive
performance using only one-third of the parameters. We also introduce a
dual-purpose zero-shot learning task for generating contextualized warnings,
enabling automated debunking and enhancing user comprehension. Qualitative and
human evaluations of the generated warnings highlight both the potential and
limitations of our approach.
</summary>
    <author>
      <name>Giovanni Pio Delvecchio</name>
    </author>
    <author>
      <name>Huy Hong Nguyen</name>
    </author>
    <author>
      <name>Isao Echizen</name>
    </author>
    <link href="http://arxiv.org/abs/2502.00752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.00752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.18500v1</id>
    <updated>2025-02-20T23:24:17Z</updated>
    <published>2025-02-20T23:24:17Z</published>
    <title>Too Little, Too Late: Moderation of Misinformation around the
  Russo-Ukrainian Conflict</title>
    <summary>  In this study, we examine the role of Twitter as a first line of defense
against misinformation by tracking the public engagement with, and the
platforms response to, 500 tweets concerning the RussoUkrainian conflict which
were identified as misinformation. Using a realtime sample of 543 475 of their
retweets, we find that users who geolocate themselves in the U.S. both produce
and consume the largest portion of misinformation, however accounts claiming to
be in Ukraine are the second largest source. At the time of writing, 84% of
these tweets were still available on the platform, especially those having an
anti-Russia narrative. For those that did receive some sanctions, the
retweeting rate has already stabilized, pointing to ineffectiveness of the
measures to stem their spread. These findings point to the need for a change in
the existing anti-misinformation system ecosystem. We propose several design
and research guidelines for its possible improvement.
</summary>
    <author>
      <name>Gautam Kishore Shahi</name>
    </author>
    <author>
      <name>Yelena Mejova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.18500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.18500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02328v1</id>
    <updated>2025-03-04T06:38:29Z</updated>
    <published>2025-03-04T06:38:29Z</published>
    <title>Limited Effectiveness of LLM-based Data Augmentation for COVID-19
  Misinformation Stance Detection</title>
    <summary>  Misinformation surrounding emerging outbreaks poses a serious societal
threat, making robust countermeasures essential. One promising approach is
stance detection (SD), which identifies whether social media posts support or
oppose misleading claims. In this work, we finetune classifiers on COVID-19
misinformation SD datasets consisting of claims and corresponding tweets.
Specifically, we test controllable misinformation generation (CMG) using large
language models (LLMs) as a method for data augmentation. While CMG
demonstrates the potential for expanding training datasets, our experiments
reveal that performance gains over traditional augmentation methods are often
minimal and inconsistent, primarily due to built-in safeguards within LLMs. We
release our code and datasets to facilitate further research on misinformation
detection and generation.
</summary>
    <author>
      <name>Eun Cheol Choi</name>
    </author>
    <author>
      <name>Ashwin Balasubramanian</name>
    </author>
    <author>
      <name>Jinhu Qi</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3701716.3715521</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3701716.3715521" rel="related"/>
    <link href="http://arxiv.org/abs/2503.02328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14626v1</id>
    <updated>2025-03-18T18:24:23Z</updated>
    <published>2025-03-18T18:24:23Z</published>
    <title>An Explainable Framework for Misinformation Identification via Critical
  Question Answering</title>
    <summary>  Natural language misinformation detection approaches have been, to date,
largely dependent on sequence classification methods, producing opaque systems
in which the reasons behind classification as misinformation are unclear. While
an effort has been made in the area of automated fact-checking to propose
explainable approaches to the problem, this is not the case for automated
reason-checking systems. In this paper, we propose a new explainable framework
for both factual and rational misinformation detection based on the theory of
Argumentation Schemes and Critical Questions. For that purpose, we create and
release NLAS-CQ, the first corpus combining 3,566 textbook-like natural
language argumentation scheme instances and 4,687 corresponding answers to
critical questions related to these arguments. On the basis of this corpus, we
implement and validate our new framework which combines classification with
question answering to analyse arguments in search of misinformation, and
provides the explanations in form of critical questions to the human user.
</summary>
    <author>
      <name>Ramon Ruiz-Dolz</name>
    </author>
    <author>
      <name>John Lawrence</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13777v1</id>
    <updated>2025-04-18T16:26:02Z</updated>
    <published>2025-04-18T16:26:02Z</published>
    <title>Beyond Misinformation: A Conceptual Framework for Studying AI
  Hallucinations in (Science) Communication</title>
    <summary>  This paper proposes a conceptual framework for understanding AI
hallucinations as a distinct form of misinformation. While misinformation
scholarship has traditionally focused on human intent, generative AI systems
now produce false yet plausible outputs absent of such intent. I argue that
these AI hallucinations should not be treated merely as technical failures but
as communication phenomena with social consequences. Drawing on a
supply-and-demand model and the concept of distributed agency, the framework
outlines how hallucinations differ from human-generated misinformation in
production, perception, and institutional response. I conclude by outlining a
research agenda for communication scholars to investigate the emergence,
dissemination, and audience reception of hallucinated content, with attention
to macro (institutional), meso (group), and micro (individual) levels. This
work urges communication researchers to rethink the boundaries of
misinformation theory in light of probabilistic, non-human actors increasingly
embedded in knowledge production.
</summary>
    <author>
      <name>Anqi Shao</name>
    </author>
    <link href="http://arxiv.org/abs/2504.13777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09526v1</id>
    <updated>2025-05-14T16:22:41Z</updated>
    <published>2025-05-14T16:22:41Z</published>
    <title>Evaluation Metrics for Misinformation Warning Interventions: Challenges
  and Prospects</title>
    <summary>  Misinformation has become a widespread issue in the 21st century, impacting
numerous areas of society and underscoring the need for effective intervention
strategies. Among these strategies, user-centered interventions, such as
warning systems, have shown promise in reducing the spread of misinformation.
Many studies have used various metrics to evaluate the effectiveness of these
warning interventions. However, no systematic review has thoroughly examined
these metrics in all studies. This paper provides a comprehensive review of
existing metrics for assessing the effectiveness of misinformation warnings,
categorizing them into four main groups: behavioral impact, trust and
credulity, usability, and cognitive and psychological effects. Through this
review, we identify critical challenges in measuring the effectiveness of
misinformation warnings, including inconsistent use of cognitive and
attitudinal metrics, the lack of standardized metrics for affective and
emotional impact, variations in user trust, and the need for more inclusive
warning designs. We present an overview of these metrics and propose areas for
future research.
</summary>
    <author>
      <name>Hussaini Zubairu</name>
    </author>
    <author>
      <name>Abdelrahaman Abdou</name>
    </author>
    <author>
      <name>Ashraf Matrawy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18596v2</id>
    <updated>2025-05-27T11:22:44Z</updated>
    <published>2025-05-24T08:44:33Z</published>
    <title>Debate-to-Detect: Reformulating Misinformation Detection as a Real-World
  Debate with Large Language Models</title>
    <summary>  The proliferation of misinformation in digital platforms reveals the
limitations of traditional detection methods, which mostly rely on static
classification and fail to capture the intricate process of real-world
fact-checking. Despite advancements in Large Language Models (LLMs) that
enhance automated reasoning, their application to misinformation detection
remains hindered by issues of logical inconsistency and superficial
verification. In response, we introduce Debate-to-Detect (D2D), a novel
Multi-Agent Debate (MAD) framework that reformulates misinformation detection
as a structured adversarial debate. Inspired by fact-checking workflows, D2D
assigns domain-specific profiles to each agent and orchestrates a five-stage
debate process, including Opening Statement, Rebuttal, Free Debate, Closing
Statement, and Judgment. To transcend traditional binary classification, D2D
introduces a multi-dimensional evaluation mechanism that assesses each claim
across five distinct dimensions: Factuality, Source Reliability, Reasoning
Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets
demonstrate significant improvements over baseline methods, and the case study
highlight D2D's capability to iteratively refine evidence while improving
decision transparency, representing a substantial advancement towards robust
and interpretable misinformation detection. The code will be open-sourced in a
future release.
</summary>
    <author>
      <name>Chen Han</name>
    </author>
    <author>
      <name>Wenzhen Zheng</name>
    </author>
    <author>
      <name>Xijin Tang</name>
    </author>
    <link href="http://arxiv.org/abs/2505.18596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05924v1</id>
    <updated>2025-06-06T09:46:09Z</updated>
    <published>2025-06-06T09:46:09Z</published>
    <title>Generating Grounded Responses to Counter Misinformation via Learning
  Efficient Fine-Grained Critiques</title>
    <summary>  Fake news and misinformation poses a significant threat to society, making
efficient mitigation essential. However, manual fact-checking is costly and
lacks scalability. Large Language Models (LLMs) offer promise in automating
counter-response generation to mitigate misinformation, but a critical
challenge lies in their tendency to hallucinate non-factual information.
Existing models mainly rely on LLM self-feedback to reduce hallucination, but
this approach is computationally expensive. In this paper, we propose
MisMitiFact, Misinformation Mitigation grounded in Facts, an efficient
framework for generating fact-grounded counter-responses at scale. MisMitiFact
generates simple critique feedback to refine LLM outputs, ensuring responses
are grounded in evidence. We develop lightweight, fine-grained critique models
trained on data sourced from readily available fact-checking sites to identify
and correct errors in key elements such as numerals, entities, and topics in
LLM generations. Experiments show that MisMitiFact generates counter-responses
of comparable quality to LLMs' self-feedback while using significantly smaller
critique models. Importantly, it achieves ~5x increase in feedback generation
throughput, making it highly suitable for cost-effective, large-scale
misinformation mitigation. Code and LLM prompt templates are at
https://github.com/xxfwin/MisMitiFact.
</summary>
    <author>
      <name>Xiaofei Xu</name>
    </author>
    <author>
      <name>Xiuzhen Zhang</name>
    </author>
    <author>
      <name>Ke Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to IJCAI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22529v1</id>
    <updated>2025-06-27T12:32:19Z</updated>
    <published>2025-06-27T12:32:19Z</published>
    <title>MisinfoTeleGraph: Network-driven Misinformation Detection for German
  Telegram Messages</title>
    <summary>  Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.
</summary>
    <author>
      <name>Lu Kalkbrenner</name>
    </author>
    <author>
      <name>Veronika Solopova</name>
    </author>
    <author>
      <name>Steffen Zeiler</name>
    </author>
    <author>
      <name>Robert Nickel</name>
    </author>
    <author>
      <name>Dorothea Kolossa</name>
    </author>
    <link href="http://arxiv.org/abs/2506.22529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.05288v1</id>
    <updated>2025-07-05T09:52:21Z</updated>
    <published>2025-07-05T09:52:21Z</published>
    <title>A Survey on Proactive Defense Strategies Against Misinformation in Large
  Language Models</title>
    <summary>  The widespread deployment of large language models (LLMs) across critical
domains has amplified the societal risks posed by algorithmically generated
misinformation. Unlike traditional false content, LLM-generated misinformation
can be self-reinforcing, highly plausible, and capable of rapid propagation
across multiple languages, which traditional detection methods fail to mitigate
effectively. This paper introduces a proactive defense paradigm, shifting from
passive post hoc detection to anticipatory mitigation strategies. We propose a
Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of
training and deployed data; (2) Inference Reliability, embedding
self-corrective mechanisms during reasoning; and (3) Input Robustness,
enhancing the resilience of model interfaces against adversarial attacks.
Through a comprehensive survey of existing techniques and a comparative
meta-analysis, we demonstrate that proactive defense strategies offer up to
63\% improvement over conventional methods in misinformation prevention,
despite non-trivial computational overhead and generalization challenges. We
argue that future research should focus on co-designing robust knowledge
foundations, reasoning certification, and attack-resistant interfaces to ensure
LLMs can effectively counter misinformation across varied domains.
</summary>
    <author>
      <name>Shuliang Liu</name>
    </author>
    <author>
      <name>Hongyi Liu</name>
    </author>
    <author>
      <name>Aiwei Liu</name>
    </author>
    <author>
      <name>Bingchen Duan</name>
    </author>
    <author>
      <name>Qi Zheng</name>
    </author>
    <author>
      <name>Yibo Yan</name>
    </author>
    <author>
      <name>He Geng</name>
    </author>
    <author>
      <name>Peijie Jiang</name>
    </author>
    <author>
      <name>Jia Liu</name>
    </author>
    <author>
      <name>Xuming Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL 2025 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.05288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.05288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.09149v1</id>
    <updated>2025-07-12T05:44:06Z</updated>
    <published>2025-07-12T05:44:06Z</published>
    <title>Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models
  Informed by the Elaboration Likelihood Model (ELM)</title>
    <summary>  Health misinformation during the COVID-19 pandemic has significantly
challenged public health efforts globally. This study applies the Elaboration
Likelihood Model (ELM) to enhance misinformation detection on social media
using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory
(LSTM) model. The model aims to enhance the detection accuracy and reliability
of misinformation classification by integrating ELM-based features such as text
readability, sentiment polarity, and heuristic cues (e.g., punctuation
frequency). The enhanced model achieved an accuracy of 97.37%, precision of
96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined
model incorporating feature engineering further improved performance, achieving
a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of
99.80%. These findings highlight the value of ELM features in improving
detection performance, offering valuable contextual information. This study
demonstrates the practical application of psychological theories in developing
advanced machine learning algorithms to address health misinformation
effectively.
</summary>
    <author>
      <name>Mkululi Sikosana</name>
    </author>
    <author>
      <name>Sean Maudsley-Barton</name>
    </author>
    <author>
      <name>Oluwaseun Ajao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages, 2 Figures, 3 Tables conference paper to appear in
  proceedings of International Conference on Artificial Intelligence, Computer,
  Data Sciences and Applications (ACDSA'25)</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.09149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.09149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.7400v1</id>
    <updated>2013-03-28T13:04:30Z</updated>
    <published>2013-03-28T13:04:30Z</published>
    <title>Policy and Planning for Large Infrastructure Projects: Problems, Causes,
  Cures</title>
    <summary>  This paper argues, first, that a major problem in the planning of large
infrastructure projects is the high level of misinformation about costs and
benefits that decision makers face in deciding whether to build, and the high
risks such misinformation generates. Second, it explores the causes of
misinformation and risk, mainly in the guise of optimism bias and strategic
misrepresentation. Finally, the paper presents a number of measures aimed at
improving planning and decision making for large infrastructure projects,
including changed incentive structures and better planning methods. Thus the
paper is organized as a simple triptych consisting in problems, causes, and
cures.
</summary>
    <author>
      <name>Bent Flyvbjerg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1303.6571,
  arXiv:1303.6654, arXiv:1303.6571, arXiv:1302.3642</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.7400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.7400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08034v1</id>
    <updated>2021-07-16T17:56:27Z</updated>
    <published>2021-07-16T17:56:27Z</published>
    <title>Pilot Study Suggests Online Media Literacy Programming Reduces Belief in
  False News in Indonesia</title>
    <summary>  Amidst the threat of digital misinformation, we offer a pilot study regarding
the efficacy of an online social media literacy campaign aimed at empowering
individuals in Indonesia with skills to help them identify misinformation. We
found that users who engaged with our online training materials and educational
videos were more likely to identify misinformation than those in our control
group (total $N$=1000). Given the promising results of our preliminary study,
we plan to expand efforts in this area, and build upon lessons learned from
this pilot study.
</summary>
    <author>
      <name>Pamela Bilo Thomas</name>
    </author>
    <author>
      <name>Clark Hogan-Taylor</name>
    </author>
    <author>
      <name>Michael Yankoski</name>
    </author>
    <author>
      <name>Tim Weninger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.08034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.14625v2</id>
    <updated>2022-03-28T05:15:46Z</updated>
    <published>2021-07-30T13:46:36Z</published>
    <title>Single-Leader-Multiple-Followers Stackelberg Security Game with
  Hypergame Framework</title>
    <summary>  In this paper, we employ a hypergame framework to analyze the
single-leader-multiple-followers (SLMF) Stackelberg security game with two
typical misinformed situations: misperception and deception. We provide a
stability criterion with the help of hyper Nash equilibrium (HNE) to
investigate both strategic stability and cognitive stability of equilibria in
SLMF games with misinformation. In fact, we find mild stable conditions such
that the equilibria with misperception and deception can become HNE. Moreover,
we discuss the robustness of the equilibria to reveal whether players have the
ability to keep their profits under the influence of some misinformation.
</summary>
    <author>
      <name>Zhaoyang Cheng</name>
    </author>
    <author>
      <name>Guanpu Chen</name>
    </author>
    <author>
      <name>Yiguang Hong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIFS.2022.3155294</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIFS.2022.3155294" rel="related"/>
    <link href="http://arxiv.org/abs/2107.14625v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.14625v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.09784v1</id>
    <updated>2020-05-19T23:00:17Z</updated>
    <published>2020-05-19T23:00:17Z</published>
    <title>Images and Misinformation in Political Groups: Evidence from WhatsApp in
  India</title>
    <summary>  WhatsApp is a key medium for the spread of news and rumors, often shared as
images. We study a large collection of politically-oriented WhatsApp groups in
India, focusing on the period leading up to the 2019 Indian national elections.
By labeling samples of random and popular images, we find that around 13% of
shared images are known misinformation and most fall into three types of
images. Machine learning methods can be used to predict whether a viral image
is misinformation, but are brittle to shifts in content over time.
</summary>
    <author>
      <name>Kiran Garimella</name>
    </author>
    <author>
      <name>Dean Eckles</name>
    </author>
    <link href="http://arxiv.org/abs/2005.09784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.02248v1</id>
    <updated>2022-10-05T13:26:08Z</updated>
    <published>2022-10-05T13:26:08Z</published>
    <title>Crowding out the truth? A simple model of misinformation, polarization
  and meaningful social interactions</title>
    <summary>  This paper provides a simple theoretical framework to evaluate the effect of
key parameters of ranking algorithms, namely popularity and personalization
parameters, on measures of platform engagement, misinformation and
polarization. The results show that an increase in the weight assigned to
online social interactions (e.g., likes and shares) and to personalized content
may increase engagement on the social media platform, while at the same time
increasing misinformation and/or polarization. By exploiting Facebook's 2018
"Meaningful Social Interactions" algorithmic ranking update, we also provide
direct empirical support for some of the main predictions of the model.
</summary>
    <author>
      <name>Fabrizio Germano</name>
    </author>
    <author>
      <name>Vicen√ß G√≥mez</name>
    </author>
    <author>
      <name>Francesco Sobbrio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages + appendices, 11 figures, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.02248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91C99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.02964v1</id>
    <updated>2023-06-05T15:27:16Z</updated>
    <published>2023-06-05T15:27:16Z</published>
    <title>Beyond Harm: an Ethical Framework to Tackle Misinformation on Social
  Media</title>
    <summary>  This paper aims to build an actionable framework for permissible online
content moderation to combat misinformation. Often strong content moderation
policies are invoked when misinformation causes harm. By adopting Mill's
ethical framework, I show the complexities involved in permissible content
moderation. The conclusion will be that, besides invoking the notion of harm,
we should also introduce the idea of cognitive autonomy and adopt useful tools,
such as cognitive nudging, to promote a healthier epistemic environment online.
</summary>
    <author>
      <name>Marianna Ganapini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages - draft</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.02964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.02964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.17158v1</id>
    <updated>2025-06-20T17:03:10Z</updated>
    <published>2025-06-20T17:03:10Z</published>
    <title>How online misinformation works: a costly signalling perspective</title>
    <summary>  This chapter explores how online communication, particularly on social media,
reshapes the reputational incentives that motivate speakers to communicate
truthfully. Drawing on costly signalling theory (CST), it examines how online
contexts alter the social mechanisms that sustain honest communication. Key
characteristics of online spaces are identified and discussed, namely (i) the
presence of novel speech acts like reposting, (ii) the gamification of
communication, (iii) information overload, (iv) the presence of anonymous and
unaccountable sources and (v) the increased reach and persistence of online
communication. Both epistemic pitfalls and potential benefits of these features
are discussed, identifying promising avenues for further empirical
investigation, and underscoring CST's value for understanding and tackling
online misinformation.
</summary>
    <author>
      <name>Neri Marsili</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 1 figure, written for "Misinformation and Other Epistemic
  Pathologies", edited by Mihaela Popa-Wyatt, Cambridge University Press</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.17158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.17158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.13636v1</id>
    <updated>2025-07-18T03:48:55Z</updated>
    <published>2025-07-18T03:48:55Z</published>
    <title>Duplicating Deceit: Inauthentic Behavior Among Indian Misinformation
  Duplicators on X/Twitter</title>
    <summary>  This paper investigates inauthentic duplication on social media, where
multiple accounts share identical misinformation tweets. Leveraging a dataset
of misinformation verified by AltNews, an Indian fact-checking organization, we
analyze over 12 million posts from 5,493 accounts known to have duplicated such
content. Contrary to common assumptions that bots are primarily responsible for
spreading false information, fewer than 1\% of these accounts exhibit bot-like
behavior. We present TweeXster, a framework for detecting and analyzing
duplication campaigns, revealing clusters of accounts involved in repeated and
sometimes revived dissemination of false or abusive content.
</summary>
    <author>
      <name>Ashfaq Ali Shafin</name>
    </author>
    <author>
      <name>Bogdan Carbunar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, accepted in 17th International Conference on
  Advances in Social Networks Analysis and Mining (ASONAM 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.13636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.13636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04221v1</id>
    <updated>2017-01-16T09:59:45Z</updated>
    <published>2017-01-16T09:59:45Z</published>
    <title>It's Always April Fools' Day! On the Difficulty of Social Network
  Misinformation Classification via Propagation Features</title>
    <summary>  Given the huge impact that Online Social Networks (OSN) had in the way people
get informed and form their opinion, they became an attractive playground for
malicious entities that want to spread misinformation, and leverage their
effect. In fact, misinformation easily spreads on OSN and is a huge threat for
modern society, possibly influencing also the outcome of elections, or even
putting people's life at risk (e.g., spreading "anti-vaccines" misinformation).
Therefore, it is of paramount importance for our society to have some sort of
"validation" on information spreading through OSN. The need for a wide-scale
validation would greatly benefit from automatic tools.
  In this paper, we show that it is difficult to carry out an automatic
classification of misinformation considering only structural properties of
content propagation cascades. We focus on structural properties, because they
would be inherently difficult to be manipulated, with the the aim of
circumventing classification systems. To support our claim, we carry out an
extensive evaluation on Facebook posts belonging to conspiracy theories (as
representative of misinformation), and scientific news (representative of
fact-checked content). Our findings show that conspiracy content actually
reverberates in a way which is hard to distinguish from the one scientific
content does: for the classification mechanisms we investigated, classification
F1-score never exceeds 0.65 during content propagation stages, and is still
less than 0.7 even after propagation is complete.
</summary>
    <author>
      <name>Mauro Conti</name>
    </author>
    <author>
      <name>Daniele Lain</name>
    </author>
    <author>
      <name>Riccardo Lazzeretti</name>
    </author>
    <author>
      <name>Giulio Lovisotto</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.04221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09918v1</id>
    <updated>2017-11-27T19:00:08Z</updated>
    <published>2017-11-27T19:00:08Z</published>
    <title>Leveraging the Crowd to Detect and Reduce the Spread of Fake News and
  Misinformation</title>
    <summary>  Online social networking sites are experimenting with the following
crowd-powered procedure to reduce the spread of fake news and misinformation:
whenever a user is exposed to a story through her feed, she can flag the story
as misinformation and, if the story receives enough flags, it is sent to a
trusted third party for fact checking. If this party identifies the story as
misinformation, it is marked as disputed. However, given the uncertain number
of exposures, the high cost of fact checking, and the trade-off between flags
and exposures, the above mentioned procedure requires careful reasoning and
smart algorithms which, to the best of our knowledge, do not exist to date.
  In this paper, we first introduce a flexible representation of the above
procedure using the framework of marked temporal point processes. Then, we
develop a scalable online algorithm, Curb, to select which stories to send for
fact checking and when to do so to efficiently reduce the spread of
misinformation with provable guarantees. In doing so, we need to solve a novel
stochastic optimal control problem for stochastic differential equations with
jumps, which is of independent interest. Experiments on two real-world datasets
gathered from Twitter and Weibo show that our algorithm may be able to
effectively reduce the spread of fake news and misinformation.
</summary>
    <author>
      <name>Jooyeon Kim</name>
    </author>
    <author>
      <name>Behzad Tabibian</name>
    </author>
    <author>
      <name>Alice Oh</name>
    </author>
    <author>
      <name>Bernhard Schoelkopf</name>
    </author>
    <author>
      <name>Manuel Gomez-Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the 11th ACM International Conference on Web Search and
  Data Mining (WSDM 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.09918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05149v3</id>
    <updated>2024-09-14T19:33:54Z</updated>
    <published>2019-01-16T06:24:37Z</published>
    <title>Beyond Uniform Reverse Sampling: A Hybrid Sampling Technique for
  Misinformation Prevention</title>
    <summary>  Online misinformation has been considered as one of the top global risks as
it may cause serious consequences such as economic damages and public panic.
The misinformation prevention problem aims at generating a positive cascade
with appropriate seed nodes in order to compete against the misinformation. In
this paper, we study the misinformation prevention problem under the prominent
independent cascade model. Due to the #P-hardness in computing influence, the
core problem is to design effective sampling methods to estimate the function
value. The main contribution of this paper is a novel sampling method.
Different from the classic reverse sampling technique which treats all nodes
equally and samples the node uniformly, the proposed method proceeds with a
hybrid sampling process which is able to attach high weights to the users who
are prone to be affected by the misinformation. Consequently, the new sampling
method is more powerful in generating effective samples used for computing seed
nodes for the positive cascade. Based on the new hybrid sample technique, we
design an algorithm offering a $(1-1/e-\epsilon)$-approximation. We
experimentally evaluate the proposed method on extensive datasets and show that
it significantly outperforms the state-of-the-art solutions.
</summary>
    <author>
      <name>Gunagmo Tong</name>
    </author>
    <author>
      <name>Ding-Zhu Du</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Type fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.05149v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05149v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.07819v5</id>
    <updated>2022-11-10T10:54:57Z</updated>
    <published>2021-11-15T15:01:55Z</published>
    <title>Testing the Generalization of Neural Language Models for COVID-19
  Misinformation Detection</title>
    <summary>  A drastic rise in potentially life-threatening misinformation has been a
by-product of the COVID-19 pandemic. Computational support to identify false
information within the massive body of data on the topic is crucial to prevent
harm. Researchers proposed many methods for flagging online misinformation
related to COVID-19. However, these methods predominantly target specific
content types (e.g., news) or platforms (e.g., Twitter). The methods'
capabilities to generalize were largely unclear so far. We evaluate fifteen
Transformer-based models on five COVID-19 misinformation datasets that include
social media posts, news articles, and scientific papers to fill this gap. We
show tokenizers and models tailored to COVID-19 data do not provide a
significant advantage over general-purpose ones. Our study provides a realistic
assessment of models for detecting COVID-19 misinformation. We expect that
evaluating a broad spectrum of datasets and models will benefit future research
in developing misinformation detection systems.
</summary>
    <author>
      <name>Jan Philip Wahle</name>
    </author>
    <author>
      <name>Nischal Ashok</name>
    </author>
    <author>
      <name>Terry Ruas</name>
    </author>
    <author>
      <name>Norman Meuschke</name>
    </author>
    <author>
      <name>Tirthankar Ghosal</name>
    </author>
    <author>
      <name>Bela Gipp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-96957-8_33</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-96957-8_33" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">iConference 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.07819v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.07819v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06122v1</id>
    <updated>2018-01-18T16:54:01Z</updated>
    <published>2018-01-18T16:54:01Z</published>
    <title>Anatomy of an online misinformation network</title>
    <summary>  Massive amounts of fake news and conspiratorial content have spread over
social media before and after the 2016 US Presidential Elections despite
intense fact-checking efforts. How do the spread of misinformation and
fact-checking compete? What are the structural and dynamic characteristics of
the core of the misinformation diffusion network, and who are its main
purveyors? How to reduce the overall amount of misinformation? To explore these
questions we built Hoaxy, an open platform that enables large-scale, systematic
studies of how misinformation and fact-checking spread and compete on Twitter.
Hoaxy filters public tweets that include links to unverified claims or
fact-checking articles. We perform k-core decomposition on a diffusion network
obtained from two million retweets produced by several hundred thousand
accounts over the six months before the election. As we move from the periphery
to the core of the network, fact-checking nearly disappears, while social bots
proliferate. The number of users in the main core reaches equilibrium around
the time of the election, with limited churn and increasingly dense
connections. We conclude by quantifying how effectively the network can be
disrupted by penalizing the most central nodes. These findings provide a first
look at the anatomy of a massive online misinformation diffusion network.
</summary>
    <author>
      <name>Chengcheng Shao</name>
    </author>
    <author>
      <name>Pik-Mai Hui</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Xinwen Jiang</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <author>
      <name>Giovanni Luca Ciampaglia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0196087</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0196087" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 11 figures, submitted to PLOS ONE</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PLoS ONE, 13(4): e0196087. 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.06122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.12309v4</id>
    <updated>2020-10-22T03:03:29Z</updated>
    <published>2020-03-26T09:48:24Z</published>
    <title>COVID-19 on Social Media: Analyzing Misinformation in Twitter
  Conversations</title>
    <summary>  The ongoing Coronavirus (COVID-19) pandemic highlights the
inter-connectedness of our present-day globalized world. With social distancing
policies in place, virtual communication has become an important source of
(mis)information. As increasing number of people rely on social media platforms
for news, identifying misinformation and uncovering the nature of online
discourse around COVID-19 has emerged as a critical task. To this end, we
collected streaming data related to COVID-19 using the Twitter API, starting
March 1, 2020. We identified unreliable and misleading contents based on
fact-checking sources, and examined the narratives promoted in misinformation
tweets, along with the distribution of engagements with these tweets. In
addition, we provide examples of the spreading patterns of prominent
misinformation tweets. The analysis is presented and updated on a publically
accessible dashboard (https://usc-melady.github.io/COVID-19-Tweet-Analysis) to
track the nature of online discourse and misinformation about COVID-19 on
Twitter from March 1 - June 5, 2020. The dashboard provides a daily list of
identified misinformation tweets, along with topics, sentiments, and emerging
trends in the COVID-19 Twitter discourse. The dashboard is provided to improve
visibility into the nature and quality of information shared online, and
provide real-time access to insights and information extracted from the
dataset.
</summary>
    <author>
      <name>Karishma Sharma</name>
    </author>
    <author>
      <name>Sungyong Seo</name>
    </author>
    <author>
      <name>Chuizheng Meng</name>
    </author>
    <author>
      <name>Sirisha Rambhatla</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2003.12309v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.12309v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05710v2</id>
    <updated>2020-08-24T19:13:30Z</updated>
    <published>2020-05-12T12:07:35Z</published>
    <title>An Exploratory Study of COVID-19 Misinformation on Twitter</title>
    <summary>  During the COVID-19 pandemic, social media has become a home ground for
misinformation. To tackle this infodemic, scientific oversight, as well as a
better understanding by practitioners in crisis management, is needed. We have
conducted an exploratory study into the propagation, authors and content of
misinformation on Twitter around the topic of COVID-19 in order to gain early
insights. We have collected all tweets mentioned in the verdicts of
fact-checked claims related to COVID-19 by over 92 professional fact-checking
organisations between January and mid-July 2020 and share this corpus with the
community. This resulted in 1 500 tweets relating to 1 274 false and 276
partially false claims, respectively. Exploratory analysis of author accounts
revealed that the verified twitter handle(including Organisation/celebrity) are
also involved in either creating (new tweets) or spreading (retweet) the
misinformation. Additionally, we found that false claims propagate faster than
partially false claims. Compare to a background corpus of COVID-19 tweets,
tweets with misinformation are more often concerned with discrediting other
information on social media. Authors use less tentative language and appear to
be more driven by concerns of potential harm to others. Our results enable us
to suggest gaps in the current scientific coverage of the topic as well as
propose actions for authorities and social media users to counter
misinformation.
</summary>
    <author>
      <name>Gautam Kishore Shahi</name>
    </author>
    <author>
      <name>Anne Dirkson</name>
    </author>
    <author>
      <name>Tim A. Majchrzak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, nine figures, four tables. Submitted for peer review,
  revision 1</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.05710v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05710v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.00747v1</id>
    <updated>2021-03-01T04:28:39Z</updated>
    <published>2021-03-01T04:28:39Z</published>
    <title>Combat COVID-19 Infodemic Using Explainable Natural Language Processing
  Models</title>
    <summary>  Misinformation of COVID-19 is prevalent on social media as the pandemic
unfolds, and the associated risks are extremely high. Thus, it is critical to
detect and combat such misinformation. Recently, deep learning models using
natural language processing techniques, such as BERT (Bidirectional Encoder
Representations from Transformers), have achieved great successes in detecting
misinformation. In this paper, we proposed an explainable natural language
processing model based on DistilBERT and SHAP (Shapley Additive exPlanations)
to combat misinformation about COVID-19 due to their efficiency and
effectiveness. First, we collected a dataset of 984 claims about COVID-19 with
fact checking. By augmenting the data using back-translation, we doubled the
sample size of the dataset and the DistilBERT model was able to obtain good
performance (accuracy: 0.972; areas under the curve: 0.993) in detecting
misinformation about COVID-19. Our model was also tested on a larger dataset
for AAAI2021 - COVID-19 Fake News Detection Shared Task and obtained good
performance (accuracy: 0.938; areas under the curve: 0.985). The performance on
both datasets was better than traditional machine learning models. Second, in
order to boost public trust in model prediction, we employed SHAP to improve
model explainability, which was further evaluated using a between-subjects
experiment with three conditions, i.e., text (T), text+SHAP explanation (TSE),
and text+SHAP explanation+source and evidence (TSESE). The participants were
significantly more likely to trust and share information related to COVID-19 in
the TSE and TSESE conditions than in the T condition. Our results provided good
implications in detecting misinformation about COVID-19 and improving public
trust.
</summary>
    <author>
      <name>Jackie Ayoub</name>
    </author>
    <author>
      <name>X. Jessie Yang</name>
    </author>
    <author>
      <name>Feng Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2103.00747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.05985v4</id>
    <updated>2024-04-10T14:13:29Z</updated>
    <published>2022-11-11T03:26:37Z</published>
    <title>Using Persuasive Writing Strategies to Explain and Detect Health
  Misinformation</title>
    <summary>  Nowadays, the spread of misinformation is a prominent problem in society. Our
research focuses on aiding the automatic identification of misinformation by
analyzing the persuasive strategies employed in textual documents. We introduce
a novel annotation scheme encompassing common persuasive writing tactics to
achieve our objective. Additionally, we provide a dataset on health
misinformation, thoroughly annotated by experts utilizing our proposed scheme.
Our contribution includes proposing a new task of annotating pieces of text
with their persuasive writing strategy types. We evaluate fine-tuning and
prompt-engineering techniques with pre-trained language models of the BERT
family and the generative large language models of the GPT family using
persuasive strategies as an additional source of information. We evaluate the
effects of employing persuasive strategies as intermediate labels in the
context of misinformation detection. Our results show that those strategies
enhance accuracy and improve the explainability of misinformation detection
models. The persuasive strategies can serve as valuable insights and
explanations, enabling other models or even humans to make more informed
decisions regarding the trustworthiness of the information.
</summary>
    <author>
      <name>Danial Kamali</name>
    </author>
    <author>
      <name>Joseph Romain</name>
    </author>
    <author>
      <name>Huiyi Liu</name>
    </author>
    <author>
      <name>Wei Peng</name>
    </author>
    <author>
      <name>Jingbo Meng</name>
    </author>
    <author>
      <name>Parisa Kordjamshidi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at LREC-CoLING-2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05985v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05985v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07981v1</id>
    <updated>2023-01-19T10:16:56Z</updated>
    <published>2023-01-19T10:16:56Z</published>
    <title>Continuously Reliable Detection of New-Normal Misinformation: Semantic
  Masking and Contrastive Smoothing in High-Density Latent Regions</title>
    <summary>  Toxic misinformation campaigns have caused significant societal harm, e.g.,
affecting elections and COVID-19 information awareness. Unfortunately, despite
successes of (gold standard) retrospective studies of misinformation that
confirmed their harmful effects after the fact, they arrive too late for timely
intervention and reduction of such harm. By design, misinformation evades
retrospective classifiers by exploiting two properties we call new-normal: (1)
never-seen-before novelty that cause inescapable generalization challenges for
previous classifiers, and (2) massive but short campaigns that end before they
can be manually annotated for new classifier training. To tackle these
challenges, we propose UFIT, which combines two techniques: semantic masking of
strong signal keywords to reduce overfitting, and intra-proxy smoothness
regularization of high-density regions in the latent space to improve
reliability and maintain accuracy. Evaluation of UFIT on public new-normal
misinformation data shows over 30% improvement over existing approaches on
future (and unseen) campaigns. To the best of our knowledge, UFIT is the first
successful effort to achieve such high level of generalization on new-normal
misinformation data with minimal concession (1 to 5%) of accuracy compared to
oracles trained with full knowledge of all campaigns.
</summary>
    <author>
      <name>Abhijit Suprem</name>
    </author>
    <author>
      <name>Joao Eduardo Ferreira</name>
    </author>
    <author>
      <name>Calton Pu</name>
    </author>
    <link href="http://arxiv.org/abs/2301.07981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.07759v1</id>
    <updated>2023-04-16T12:14:38Z</updated>
    <published>2023-04-16T12:14:38Z</published>
    <title>MisRoB√ÜRTa: Transformers versus Misinformation</title>
    <summary>  Misinformation is considered a threat to our democratic values and
principles. The spread of such content on social media polarizes society and
undermines public discourse by distorting public perceptions and generating
social unrest while lacking the rigor of traditional journalism. Transformers
and transfer learning proved to be state-of-the-art methods for multiple
well-known natural language processing tasks. In this paper, we propose
MisRoB{\AE}RTa, a novel transformer-based deep neural ensemble architecture for
misinformation detection. MisRoB{\AE}RTa takes advantage of two transformers
(BART \&amp; RoBERTa) to improve the classification performance. We also
benchmarked and evaluated the performances of multiple transformers on the task
of misinformation detection. For training and testing, we used a large
real-world news articles dataset labeled with 10 classes, addressing two
shortcomings in the current research: increasing the size of the dataset from
small to large, and moving the focus of fake news detection from binary
classification to multi-class classification. For this dataset, we manually
verified the content of the news articles to ensure that they were correctly
labeled. The experimental results show that the accuracy of transformers on the
misinformation detection problem was significantly influenced by the method
employed to learn the context, dataset size, and vocabulary dimension. We
observe empirically that the best accuracy performance among the classification
models that use only one transformer is obtained by BART, while DistilRoBERTa
obtains the best accuracy in the least amount of time required for fine-tuning
and training. The proposed MisRoB{\AE}RTa outperforms the other transformer
models in the task of misinformation detection. To arrive at this conclusion,
we performed ample ablation and sensitivity testing with MisRoB{\AE}RTa on two
datasets.
</summary>
    <author>
      <name>Ciprian-Octavian TruicƒÉ</name>
    </author>
    <author>
      <name>Elena-Simona Apostol</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/math10040569</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/math10040569" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mathematics, 10(4):1-25(569), 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.07759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.07759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.13098v1</id>
    <updated>2023-09-22T15:10:36Z</updated>
    <published>2023-09-22T15:10:36Z</published>
    <title>Topological Data Mapping of Online Hate Speech, Misinformation, and
  General Mental Health: A Large Language Model Based Study</title>
    <summary>  The advent of social media has led to an increased concern over its potential
to propagate hate speech and misinformation, which, in addition to contributing
to prejudice and discrimination, has been suspected of playing a role in
increasing social violence and crimes in the United States. While literature
has shown the existence of an association between posting hate speech and
misinformation online and certain personality traits of posters, the general
relationship and relevance of online hate speech/misinformation in the context
of overall psychological wellbeing of posters remain elusive. One difficulty
lies in the lack of adequate data analytics tools capable of adequately
analyzing the massive amount of social media posts to uncover the underlying
hidden links. Recent progresses in machine learning and large language models
such as ChatGPT have made such an analysis possible. In this study, we
collected thousands of posts from carefully selected communities on the social
media site Reddit. We then utilized OpenAI's GPT3 to derive embeddings of these
posts, which are high-dimensional real-numbered vectors that presumably
represent the hidden semantics of posts. We then performed various
machine-learning classifications based on these embeddings in order to
understand the role of hate speech/misinformation in various communities.
Finally, a topological data analysis (TDA) was applied to the embeddings to
obtain a visual map connecting online hate speech, misinformation, various
psychiatric disorders, and general mental health.
</summary>
    <author>
      <name>Andrew Alexander</name>
    </author>
    <author>
      <name>Hongbin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.13098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.13098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.12509v2</id>
    <updated>2024-01-24T01:56:12Z</updated>
    <published>2024-01-23T06:02:03Z</published>
    <title>Digital cloning of online social networks for language-sensitive
  agent-based modeling of misinformation spread</title>
    <summary>  We develop a simulation framework for studying misinformation spread within
online social networks that blends agent-based modeling and natural language
processing techniques. While many other agent-based simulations exist in this
space, questions over their fidelity and generalization to existing networks in
part hinders their ability to provide actionable insights. To partially address
these concerns, we create a 'digital clone' of a known misinformation sharing
network by downloading social media histories for over ten thousand of its
users. We parse these histories to both extract the structure of the network
and model the nuanced ways in which information is shared and spread among its
members. Unlike many other agent-based methods in this space, information
sharing between users in our framework is sensitive to topic of discussion,
user preferences, and online community dynamics. To evaluate the fidelity of
our method, we seed our cloned network with a set of posts recorded in the base
network and compare propagation dynamics between the two, observing reasonable
agreement across the twin networks over a variety of metrics. Lastly, we
explore how the cloned network may serve as a flexible, low-cost testbed for
misinformation countermeasure evaluation and red teaming analysis. We hope the
tools explored here augment existing efforts in the space and unlock new
opportunities for misinformation countermeasure evaluation, a field that may
become increasingly important to consider with the anticipated rise of
misinformation campaigns fueled by generative artificial intelligence.
</summary>
    <author>
      <name>Prateek Puri</name>
    </author>
    <author>
      <name>Gabriel Hassler</name>
    </author>
    <author>
      <name>Anton Shenk</name>
    </author>
    <author>
      <name>Sai Katragadda</name>
    </author>
    <link href="http://arxiv.org/abs/2401.12509v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.12509v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.08772v3</id>
    <updated>2025-02-27T03:19:48Z</updated>
    <published>2024-06-13T03:04:28Z</published>
    <title>MMFakeBench: A Mixed-Source Multimodal Misinformation Detection
  Benchmark for LVLMs</title>
    <summary>  Current multimodal misinformation detection (MMD) methods often assume a
single source and type of forgery for each sample, which is insufficient for
real-world scenarios where multiple forgery sources coexist. The lack of a
benchmark for mixed-source misinformation has hindered progress in this field.
To address this, we introduce MMFakeBench, the first comprehensive benchmark
for mixed-source MMD. MMFakeBench includes 3 critical sources: textual veracity
distortion, visual veracity distortion, and cross-modal consistency distortion,
along with 12 sub-categories of misinformation forgery types. We further
conduct an extensive evaluation of 6 prevalent detection methods and 15 Large
Vision-Language Models (LVLMs) on MMFakeBench under a zero-shot setting. The
results indicate that current methods struggle under this challenging and
realistic mixed-source MMD setting. Additionally, we propose MMD-Agent, a novel
approach to integrate the reasoning, action, and tool-use capabilities of LVLM
agents, significantly enhancing accuracy and generalization. We believe this
study will catalyze future research into more realistic mixed-source multimodal
misinformation and provide a fair evaluation of misinformation detection
methods.
</summary>
    <author>
      <name>Xuannan Liu</name>
    </author>
    <author>
      <name>Zekun Li</name>
    </author>
    <author>
      <name>Peipei Li</name>
    </author>
    <author>
      <name>Huaibo Huang</name>
    </author>
    <author>
      <name>Shuhan Xia</name>
    </author>
    <author>
      <name>Xing Cui</name>
    </author>
    <author>
      <name>Linzhi Huang</name>
    </author>
    <author>
      <name>Weihong Deng</name>
    </author>
    <author>
      <name>Zhaofeng He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICLR 2025, Project page:
  https://liuxuannan.github.io/MMFakeBench.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.08772v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.08772v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.02710v1</id>
    <updated>2024-07-02T23:11:27Z</updated>
    <published>2024-07-02T23:11:27Z</published>
    <title>WARNING This Contains Misinformation: The Effect of Cognitive Factors,
  Beliefs, and Personality on Misinformation Warning Tag Attitudes</title>
    <summary>  Social media platforms enhance the propagation of online misinformation by
providing large user bases with a quick means to share content. One way to
disrupt the rapid dissemination of misinformation at scale is through warning
tags, which label content as potentially false or misleading. Past warning tag
mitigation studies yield mixed results for diverse audiences, however. We
hypothesize that personalizing warning tags to the individual characteristics
of their diverse users may enhance mitigation effectiveness. To reach the goal
of personalization, we need to understand how people differ and how those
differences predict a person's attitudes and self-described behaviors toward
tags and tagged content. In this study, we leverage Amazon Mechanical Turk (n =
132) and undergraduate students (n = 112) to provide this foundational
understanding. Specifically, we find attitudes towards warning tags and
self-described behaviors are positively influenced by factors such as
Personality Openness and Agreeableness, Need for Cognitive Closure (NFCC),
Cognitive Reflection Test (CRT) score, and Trust in Medical Scientists.
Conversely, Trust in Religious Leaders, Conscientiousness, and political
conservatism were negatively correlated with these attitudes and behaviors. We
synthesize our results into design insights and a future research agenda for
more effective and personalized misinformation warning tags and misinformation
mitigation strategies more generally.
</summary>
    <author>
      <name>Robert Kaufman</name>
    </author>
    <author>
      <name>Aaron Broukhim</name>
    </author>
    <author>
      <name>Michael Haupt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.02710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.02710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.14321v1</id>
    <updated>2024-07-19T13:57:11Z</updated>
    <published>2024-07-19T13:57:11Z</published>
    <title>Multimodal Misinformation Detection using Large Vision-Language Models</title>
    <summary>  The increasing proliferation of misinformation and its alarming impact have
motivated both industry and academia to develop approaches for misinformation
detection and fact checking. Recent advances on large language models (LLMs)
have shown remarkable performance in various tasks, but whether and how LLMs
could help with misinformation detection remains relatively underexplored. Most
of existing state-of-the-art approaches either do not consider evidence and
solely focus on claim related features or assume the evidence to be provided.
Few approaches consider evidence retrieval as part of the misinformation
detection but rely on fine-tuning models. In this paper, we investigate the
potential of LLMs for misinformation detection in a zero-shot setting. We
incorporate an evidence retrieval component into the process as it is crucial
to gather pertinent information from various sources to detect the veracity of
claims. To this end, we propose a novel re-ranking approach for multimodal
evidence retrieval using both LLMs and large vision-language models (LVLM). The
retrieved evidence samples (images and texts) serve as the input for an
LVLM-based approach for multimodal fact verification (LVLM4FV). To enable a
fair evaluation, we address the issue of incomplete ground truth for evidence
samples in an existing evidence retrieval dataset by annotating a more complete
set of evidence samples for both image and text retrieval. Our experimental
results on two datasets demonstrate the superiority of the proposed approach in
both evidence retrieval and fact verification tasks and also better
generalization capability across dataset compared to the supervised baseline.
</summary>
    <author>
      <name>Sahar Tahmasebi</name>
    </author>
    <author>
      <name>Eric M√ºller-Budack</name>
    </author>
    <author>
      <name>Ralph Ewerth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in: Conference on Information and Knowledge
  Management (CIKM) 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.14321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.14321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.19192v1</id>
    <updated>2024-07-27T07:16:07Z</updated>
    <published>2024-07-27T07:16:07Z</published>
    <title>Harmfully Manipulated Images Matter in Multimodal Misinformation
  Detection</title>
    <summary>  Nowadays, misinformation is widely spreading over various social media
platforms and causes extremely negative impacts on society. To combat this
issue, automatically identifying misinformation, especially those containing
multimodal content, has attracted growing attention from the academic and
industrial communities, and induced an active research topic named Multimodal
Misinformation Detection (MMD). Typically, existing MMD methods capture the
semantic correlation and inconsistency between multiple modalities, but neglect
some potential clues in multimodal content. Recent studies suggest that
manipulated traces of the images in articles are non-trivial clues for
detecting misinformation. Meanwhile, we find that the underlying intentions
behind the manipulation, e.g., harmful and harmless, also matter in MMD.
Accordingly, in this work, we propose to detect misinformation by learning
manipulation features that indicate whether the image has been manipulated, as
well as intention features regarding the harmful and harmless intentions of the
manipulation. Unfortunately, the manipulation and intention labels that make
these features discriminative are unknown. To overcome the problem, we propose
two weakly supervised signals as alternatives by introducing additional
datasets on image manipulation detection and formulating two classification
tasks as positive and unlabeled learning problems. Based on these ideas, we
propose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD
(HAMI-M3D). Extensive experiments across three benchmark datasets can
demonstrate that HAMI-M3D can consistently improve the performance of any MMD
baselines.
</summary>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Shengsheng Wang</name>
    </author>
    <author>
      <name>Changchun Li</name>
    </author>
    <author>
      <name>Renchu Guan</name>
    </author>
    <author>
      <name>Ximing Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM MM 2024. Code:
  https://github.com/wangbing1416/HAMI-M3D</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.19192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.19192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.17637v1</id>
    <updated>2024-09-26T08:38:15Z</updated>
    <published>2024-09-26T08:38:15Z</published>
    <title>Intervention strategies for misinformation sharing on social media: A
  bibliometric analysis</title>
    <summary>  Widely distributed misinformation shared across social media channels is a
pressing issue that poses a significant threat to many aspects of society's
well-being. Inaccurate shared information causes confusion, can adversely
affect mental health, and can lead to mis-informed decision-making. Therefore,
it is important to implement proactive measures to intervene and curb the
spread of misinformation where possible. This has prompted scholars to
investigate a variety of intervention strategies for misinformation sharing on
social media. This study explores the typology of intervention strategies for
addressing misinformation sharing on social media, identifying 4 important
clusters - cognition-based, automated-based, information-based, and
hybrid-based. The literature selection process utilized the PRISMA method to
ensure a systematic and comprehensive analysis of relevant literature while
maintaining transparency and reproducibility. A total of 139 articles published
from 2013-2023 were then analyzed. Meanwhile, bibliometric analyses were
conducted using performance analysis and science mapping techniques for the
typology development. A comparative analysis of the typology was conducted to
reveal patterns and evolution in the field. This provides valuable insights for
both theory and practical applications. Overall, the study concludes that
scholarly contributions to scientific research and publication help to address
research gaps and expand knowledge in this field. Understanding the evolution
of intervention strategies for misinformation sharing on social media can
support future research that contributes to the development of more effective
and sustainable solutions to this persistent problem.
</summary>
    <author>
      <name>Juanita Zainudin</name>
    </author>
    <author>
      <name>Nazlena Mohamad Ali</name>
    </author>
    <author>
      <name>Alan F. Smeaton</name>
    </author>
    <author>
      <name>Mohamad Taha Ijab</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2024.3469248</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2024.3469248" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 5 figures, 8 tables, to appear in IEEE Access</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.17637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.17637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13802v2</id>
    <updated>2025-03-09T16:39:06Z</updated>
    <published>2025-01-23T16:21:15Z</published>
    <title>Enhancing LLMs for Governance with Human Oversight: Evaluating and
  Aligning LLMs on Expert Classification of Climate Misinformation for
  Detecting False or Misleading Claims about Climate Change</title>
    <summary>  Climate misinformation is a problem that has the potential to be
substantially aggravated by the development of Large Language Models (LLMs). In
this study we evaluate the potential for LLMs to be part of the solution for
mitigating online dis/misinformation rather than the problem. Employing a
public expert annotated dataset and a curated sample of social media content we
evaluate the performance of proprietary vs. open source LLMs on climate
misinformation classification task, comparing them to existing climate-focused
computer-assisted tools and expert assessments. Results show (1) open-source
models substantially under-perform in classifying climate misinformation
compared to proprietary models, (2) existing climate-focused computer-assisted
tools leveraging expert-annotated datasets continues to outperform many of
proprietary models, including GPT-4o, and (3) demonstrate the efficacy and
generalizability of fine-tuning GPT-3.5-turbo on expert annotated dataset in
classifying claims about climate change at the equivalency of climate change
experts with over 20 years of experience in climate communication. These
findings highlight 1) the importance of incorporating human-oversight, such as
incorporating expert-annotated datasets in training LLMs, for governance tasks
that require subject-matter expertise like classifying climate misinformation,
and 2) the potential for LLMs in facilitating civil society organizations to
engage in various governance tasks such as classifying false or misleading
claims in domains beyond climate change such as politics and health science.
</summary>
    <author>
      <name>Mowafak Allaham</name>
    </author>
    <author>
      <name>Ayse D. Lokmanoglu</name>
    </author>
    <author>
      <name>P. Sol Hart</name>
    </author>
    <author>
      <name>Erik C. Nisbet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on AI Governance: Alignment, Morality and Law
  (AIGOV) 2025. AAAI Conference on Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13802v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13802v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.12740v4</id>
    <updated>2025-07-09T10:27:15Z</updated>
    <published>2025-02-18T10:56:30Z</published>
    <title>IPSR Model: Misinformation Intervention through Prebunking in Social
  Systems</title>
    <summary>  The rapid dissemination of misinformation through online social networks
poses a growing threat to public understanding and societal stability.
Prebunking, a proactive strategy based on inoculation theory, has recently
emerged as an effective intervention to build cognitive resilience against
misinformation before exposure. In this work, we investigate the impact of
prebunking on misinformation dynamics using a compartmental modeling framework.
We first analyze the classical Ignorant-Spreader-Stifler (ISR) model, its
parameters are determined using empirical rumor data from Twitter. We then
propose an extended model, the Ignorant-Prebunked-Spreader-Stifler (IPSR)
model, which incorporates prebunking as a preventive state and includes a
forgetting mechanism to account for the decay of cognitive immunity over time.
Using mean-field approximations, we derive steady-state solutions and examine
the effect of prebunking on the spreading of misinformation. We further
investigate the robustness of the IPSR model by varying network size and
average degree. In addition, we analyze the model's behavior on Watts-Strogatz
and Barabasi-Albert networks to assess the role of small-world and scale-free
structures in shaping intervention outcomes. Our results show that the
inclusion of prebunking significantly reduces the scale of misinformation
outbreaks across different network structures. These findings highlight the
efficacy of prebunking as a scalable intervention strategy and underscore the
utility of compartmental models in understanding and mitigating
information-based contagion in complex networks.
</summary>
    <author>
      <name>Robert Rai</name>
    </author>
    <author>
      <name>Rajesh Sharma</name>
    </author>
    <author>
      <name>Chandrakala Meena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.12740v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.12740v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13079v1</id>
    <updated>2025-04-17T16:46:11Z</updated>
    <published>2025-04-17T16:46:11Z</published>
    <title>Retrieval-Augmented Generation with Conflicting Evidence</title>
    <summary>  Large language model (LLM) agents are increasingly employing
retrieval-augmented generation (RAG) to improve the factuality of their
responses. However, in practice, these systems often need to handle ambiguous
user queries and potentially conflicting information from multiple sources
while also suppressing inaccurate information from noisy or irrelevant
documents. Prior work has generally studied and addressed these challenges in
isolation, considering only one aspect at a time, such as handling ambiguity or
robustness to noise and misinformation. We instead consider multiple factors
simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and
Misinformation in Documents), a new dataset that simulates complex and
realistic scenarios for conflicting evidence for a user query, including
ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent
approach in which LLM agents debate over the merits of an answer over multiple
rounds, allowing an aggregator to collate responses corresponding to
disambiguated entities while discarding misinformation and noise, thereby
handling diverse sources of conflict jointly. We demonstrate the effectiveness
of MADAM-RAG using both closed and open-source models on AmbigDocs -- which
requires presenting all valid answers for ambiguous queries -- improving over
strong RAG baselines by up to 11.40% and on FaithEval -- which requires
suppressing misinformation -- where we improve by up to 15.80% (absolute) with
Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for
existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match
score). While MADAM-RAG begins to address these conflicting factors, our
analysis indicates that a substantial gap remains especially when increasing
the level of imbalance in supporting evidence and misinformation.
</summary>
    <author>
      <name>Han Wang</name>
    </author>
    <author>
      <name>Archiki Prasad</name>
    </author>
    <author>
      <name>Elias Stengel-Eskin</name>
    </author>
    <author>
      <name>Mohit Bansal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Our data and code is available at:
  https://github.com/HanNight/RAMDocs</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.13079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18685v1</id>
    <updated>2025-05-24T13:04:23Z</updated>
    <published>2025-05-24T13:04:23Z</published>
    <title>From Generation to Detection: A Multimodal Multi-Task Dataset for
  Benchmarking Health Misinformation</title>
    <summary>  Infodemics and health misinformation have significant negative impact on
individuals and society, exacerbating confusion and increasing hesitancy in
adopting recommended health measures. Recent advancements in generative AI,
capable of producing realistic, human like text and images, have significantly
accelerated the spread and expanded the reach of health misinformation,
resulting in an alarming surge in its dissemination. To combat the infodemics,
most existing work has focused on developing misinformation datasets from
social media and fact checking platforms, but has faced limitations in topical
coverage, inclusion of AI generation, and accessibility of raw content. To
address these issues, we present MM Health, a large scale multimodal
misinformation dataset in the health domain consisting of 34,746 news article
encompassing both textual and visual information. MM Health includes
human-generated multimodal information (5,776 articles) and AI generated
multimodal information (28,880 articles) from various SOTA generative AI
models. Additionally, We benchmarked our dataset against three tasks
(reliability checks, originality checks, and fine-grained AI detection)
demonstrating that existing SOTA models struggle to accurately distinguish the
reliability and origin of information. Our dataset aims to support the
development of misinformation detection across various health scenarios,
facilitating the detection of human and machine generated content at multimodal
levels.
</summary>
    <author>
      <name>Zhihao Zhang</name>
    </author>
    <author>
      <name>Yiran Zhang</name>
    </author>
    <author>
      <name>Xiyue Zhou</name>
    </author>
    <author>
      <name>Liting Huang</name>
    </author>
    <author>
      <name>Imran Razzak</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Usman Naseem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.18685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.04037v1</id>
    <updated>2021-10-08T11:14:44Z</updated>
    <published>2021-10-08T11:14:44Z</published>
    <title>Simulations for novel problems in recommendation: analyzing
  misinformation and data characteristics</title>
    <summary>  In this position paper, we discuss recent applications of simulation
approaches for recommender systems tasks. In particular, we describe how they
were used to analyze the problem of misinformation spreading and understand
which data characteristics affect the performance of recommendation algorithms
more significantly. We also present potential lines of future work where
simulation methods could advance the work in the recommendation community.
</summary>
    <author>
      <name>Alejandro Bellog√≠n</name>
    </author>
    <author>
      <name>Yashar Deldjoo</name>
    </author>
    <link href="http://arxiv.org/abs/2110.04037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.04037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.05371v1</id>
    <updated>2023-04-06T05:09:39Z</updated>
    <published>2023-04-06T05:09:39Z</published>
    <title>Those Aren't Your Memories, They're Somebody Else's: Seeding
  Misinformation in Chat Bot Memories</title>
    <summary>  One of the new developments in chit-chat bots is a long-term memory mechanism
that remembers information from past conversations for increasing engagement
and consistency of responses. The bot is designed to extract knowledge of
personal nature from their conversation partner, e.g., stating preference for a
particular color. In this paper, we show that this memory mechanism can result
in unintended behavior. In particular, we found that one can combine a personal
statement with an informative statement that would lead the bot to remember the
informative statement alongside personal knowledge in its long term memory.
This means that the bot can be tricked into remembering misinformation which it
would regurgitate as statements of fact when recalling information relevant to
the topic of conversation. We demonstrate this vulnerability on the BlenderBot
2 framework implemented on the ParlAI platform and provide examples on the more
recent and significantly larger BlenderBot 3 model. We generate 150 examples of
misinformation, of which 114 (76%) were remembered by BlenderBot 2 when
combined with a personal statement. We further assessed the risk of this
misinformation being recalled after intervening innocuous conversation and in
response to multiple questions relevant to the injected memory. Our evaluation
was performed on both the memory-only and the combination of memory and
internet search modes of BlenderBot 2. From the combinations of these
variables, we generated 12,890 conversations and analyzed recalled
misinformation in the responses. We found that when the chat bot is questioned
on the misinformation topic, it was 328% more likely to respond with the
misinformation as fact when the misinformation was in the long-term memory.
</summary>
    <author>
      <name>Conor Atkins</name>
    </author>
    <author>
      <name>Benjamin Zi Hao Zhao</name>
    </author>
    <author>
      <name>Hassan Jameel Asghar</name>
    </author>
    <author>
      <name>Ian Wood</name>
    </author>
    <author>
      <name>Mohamed Ali Kaafar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in 21st International Conference on Applied
  Cryptography and Network Security, ACNS 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.05371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.05371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.04852v1</id>
    <updated>2024-03-07T19:06:42Z</updated>
    <published>2024-03-07T19:06:42Z</published>
    <title>Corrective or Backfire: Characterizing and Predicting User Response to
  Social Correction</title>
    <summary>  Online misinformation poses a global risk with harmful implications for
society. Ordinary social media users are known to actively reply to
misinformation posts with counter-misinformation messages, which is shown to be
effective in containing the spread of misinformation. Such a practice is
defined as "social correction". Nevertheless, it remains unknown how users
respond to social correction in real-world scenarios, especially, will it have
a corrective or backfire effect on users. Investigating this research question
is pivotal for developing and refining strategies that maximize the efficacy of
social correction initiatives. To fill this gap, we conduct an in-depth study
to characterize and predict the user response to social correction in a
data-driven manner through the lens of X (Formerly Twitter), where the user
response is instantiated as the reply that is written toward a
counter-misinformation message. Particularly, we first create a novel dataset
with 55, 549 triples of misinformation tweets, counter-misinformation replies,
and responses to counter-misinformation replies, and then curate a taxonomy to
illustrate different kinds of user responses. Next, fine-grained statistical
analysis of reply linguistic and engagement features as well as repliers' user
attributes is conducted to illustrate the characteristics that are significant
in determining whether a reply will have a corrective or backfire effect.
Finally, we build a user response prediction model to identify whether a social
correction will be corrective, neutral, or have a backfire effect, which
achieves a promising F1 score of 0.816. Our work enables stakeholders to
monitor and predict user responses effectively, thus guiding the use of social
correction to maximize their corrective impact and minimize backfire effects.
The code and data is accessible on
https://github.com/claws-lab/response-to-social-correction.
</summary>
    <author>
      <name>Bing He</name>
    </author>
    <author>
      <name>Yingchen Ma</name>
    </author>
    <author>
      <name>Mustaque Ahamad</name>
    </author>
    <author>
      <name>Srijan Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at: The ACM Web Science Conference 2024 (ACM WebSci 2024).
  Code and data at: https://github.com/claws-lab/response-to-social-correction</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.04852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.04852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01400v1</id>
    <updated>2018-02-05T14:17:21Z</updated>
    <published>2018-02-05T14:17:21Z</published>
    <title>Polarization and Fake News: Early Warning of Potential Misinformation
  Targets</title>
    <summary>  Users polarization and confirmation bias play a key role in misinformation
spreading on online social media. Our aim is to use this information to
determine in advance potential targets for hoaxes and fake news. In this paper,
we introduce a general framework for promptly identifying polarizing content on
social media and, thus, "predicting" future fake news topics. We validate the
performances of the proposed methodology on a massive Italian Facebook dataset,
showing that we are able to identify topics that are susceptible to
misinformation with 77% accuracy. Moreover, such information may be embedded as
a new feature in an additional classifier able to recognize fake news with 91%
accuracy. The novelty of our approach consists in taking into account a series
of characteristics related to users behavior on online social media, making a
first, important step towards the smoothing of polarization and the mitigation
of misinformation phenomena.
</summary>
    <author>
      <name>Michela Del Vicario</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <author>
      <name>Antonio Scala</name>
    </author>
    <author>
      <name>Fabiana Zollo</name>
    </author>
    <link href="http://arxiv.org/abs/1802.01400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.04887v2</id>
    <updated>2019-12-05T20:29:36Z</updated>
    <published>2019-03-04T22:23:33Z</published>
    <title>QuickStop: A Markov Optimal Stopping Approach for Quickest
  Misinformation Detection</title>
    <summary>  This paper combines data-driven and model-driven methods for real-time
misinformation detection. Our algorithm, named QuickStop, is an optimal
stopping algorithm based on a probabilistic information spreading model
obtained from labeled data. The algorithm consists of an offline machine
learning algorithm for learning the probabilistic information spreading model
and an online optimal stopping algorithm to detect misinformation. The online
detection algorithm has both low computational and memory complexities. Our
numerical evaluations with a real-world dataset show that QuickStop outperforms
existing misinformation detection algorithms in terms of both accuracy and
detection time (number of observations needed for detection). Our evaluations
with synthetic data further show that QuickStop is robust to (offline) learning
errors.
</summary>
    <author>
      <name>Honghao Wei</name>
    </author>
    <author>
      <name>Xiaohan Kang</name>
    </author>
    <author>
      <name>Weina Wang</name>
    </author>
    <author>
      <name>Lei Ying</name>
    </author>
    <link href="http://arxiv.org/abs/1903.04887v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.04887v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09805v2</id>
    <updated>2020-02-20T18:32:33Z</updated>
    <published>2019-08-26T17:23:22Z</published>
    <title>The Limitations of Stylometry for Detecting Machine-Generated Fake News</title>
    <summary>  Recent developments in neural language models (LMs) have raised concerns
about their potential misuse for automatically spreading misinformation. In
light of these concerns, several studies have proposed to detect
machine-generated fake news by capturing their stylistic differences from
human-written text. These approaches, broadly termed stylometry, have found
success in source attribution and misinformation detection in human-written
texts. However, in this work, we show that stylometry is limited against
machine-generated misinformation. While humans speak differently when trying to
deceive, LMs generate stylistically consistent text, regardless of underlying
motive. Thus, though stylometry can successfully prevent impersonation by
identifying text provenance, it fails to distinguish legitimate LM applications
from those that introduce false information. We create two benchmarks
demonstrating the stylistic similarity between malicious and legitimate uses of
LMs, employed in auto-completion and editing-assistance settings. Our findings
highlight the need for non-stylometry approaches in detecting machine-generated
misinformation, and open up the discussion on the desired evaluation
benchmarks.
</summary>
    <author>
      <name>Tal Schuster</name>
    </author>
    <author>
      <name>Roei Schuster</name>
    </author>
    <author>
      <name>Darsh J Shah</name>
    </author>
    <author>
      <name>Regina Barzilay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Computational Linguistics journal (squib). Previously
  posted with title "Are We Safe Yet? The Limitations of Distributional
  Features for Fake News Detection"</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00544v1</id>
    <updated>2020-10-01T16:58:12Z</updated>
    <published>2020-10-01T16:58:12Z</published>
    <title>Designing Indicators to Combat Fake Media</title>
    <summary>  The growth of misinformation technology necessitates the need to identify
fake videos. One approach to preventing the consumption of these fake videos is
provenance which allows the user to authenticate media content to its original
source. This research designs and investigates the use of provenance indicators
to help users identify fake videos. We first interview users regarding their
experiences with different misinformation modes (text, image, video) to guide
the design of indicators within users' existing perspectives. Then, we conduct
a participatory design study to develop and design fake video indicators.
Finally, we evaluate participant-designed indicators via both expert
evaluations and quantitative surveys with a large group of end-users. Our
results provide concrete design guidelines for the emerging issue of fake
videos. Our findings also raise concerns regarding users' tendency to
overgeneralize from misinformation warning messages, suggesting the need for
further research on warning design in the ongoing fight against misinformation.
</summary>
    <author>
      <name>Imani N. Sherman</name>
    </author>
    <author>
      <name>Elissa M. Redmiles</name>
    </author>
    <author>
      <name>Jack W. Stokes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.00544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.08768v2</id>
    <updated>2021-03-13T20:26:35Z</updated>
    <published>2020-10-17T11:21:40Z</published>
    <title>ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation
  Detection</title>
    <summary>  In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset
for misinformation detection composed of tweets containing claims from 27th
January till the end of April 2020. We collected 138 verified claims, mostly
from popular fact-checking websites, and identified 9.4K relevant tweets to
those claims. Tweets were manually-annotated by veracity to support research on
misinformation detection, which is one of the major problems faced during a
pandemic. ArCOV19-Rumors supports two levels of misinformation detection over
Twitter: verifying free-text claims (called claim-level verification) and
verifying claims expressed in tweets (called tweet-level verification). Our
dataset covers, in addition to health, claims related to other topical
categories that were influenced by COVID-19, namely, social, politics, sports,
entertainment, and religious. Moreover, we present benchmarking results for
tweet-level verification on the dataset. We experimented with SOTA models of
versatile approaches that either exploit content, user profiles features,
temporal features and propagation structure of the conversational threads for
tweet verification.
</summary>
    <author>
      <name>Fatima Haouari</name>
    </author>
    <author>
      <name>Maram Hasanain</name>
    </author>
    <author>
      <name>Reem Suwaileh</name>
    </author>
    <author>
      <name>Tamer Elsayed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work was accepted at the Sixth Arabic Natural Language
  Processing Workshop (EACL/WANLP 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.08768v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08768v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07730v1</id>
    <updated>2021-05-17T10:58:35Z</updated>
    <published>2021-05-17T10:58:35Z</published>
    <title>The State of Infodemic on Twitter</title>
    <summary>  Following the wave of misinterpreted, manipulated and malicious information
growing on the Internet, the misinformation surrounding COVID-19 has become a
paramount issue. In the context of the current COVID-19 pandemic, social media
posts and platforms are at risk of rumors and misinformation in the face of the
serious uncertainty surrounding the virus itself. At the same time, the
uncertainty and new nature of COVID-19 means that other unconfirmed information
that may appear "rumored" may be an important indicator of the behavior and
impact of this new virus. Twitter, in particular, has taken a center stage in
this storm where Covid-19 has been a much talked about subject. We have
presented an exploratory analysis of the tweets and the users who are involved
in spreading misinformation and then delved into machine learning models and
natural language processing techniques to identify if a tweet contains
misinformation.
</summary>
    <author>
      <name>Drishti Jain</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Indraprastha Institute of Information Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Tavpritesh Sethi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Indraprastha Institute of Information Technology</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.07730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04170v2</id>
    <updated>2018-01-17T17:44:51Z</updated>
    <published>2016-10-13T16:48:45Z</published>
    <title>Network segregation in a model of misinformation and fact checking</title>
    <summary>  Misinformation under the form of rumor, hoaxes, and conspiracy theories
spreads on social media at alarming rates. One hypothesis is that, since social
media are shaped by homophily, belief in misinformation may be more likely to
thrive on those social circles that are segregated from the rest of the
network. One possible antidote is fact checking which, in some cases, is known
to stop rumors from spreading further. However, fact checking may also backfire
and reinforce the belief in a hoax. Here we take into account the combination
of network segregation, finite memory and attention, and fact-checking efforts.
We consider a compartmental model of two interacting epidemic processes over a
network that is segregated between gullible and skeptic users. Extensive
simulation and mean-field analysis show that a more segregated network
facilitates the spread of a hoax only at low forgetting rates, but has no
effect when agents forget at faster rates. This finding may inform the
development of mitigation techniques and overall inform on the risks of
uncontrolled misinformation online.
</summary>
    <author>
      <name>Marcella Tambuscio</name>
    </author>
    <author>
      <name>Diego F. M. Oliveira</name>
    </author>
    <author>
      <name>Giovanni Luca Ciampaglia</name>
    </author>
    <author>
      <name>Giancarlo Ruffo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s42001-018-0018-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s42001-018-0018-9" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J Comput Soc Sc (2018) 1: 261</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.04170v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04170v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.01284v3</id>
    <updated>2020-06-30T22:30:37Z</updated>
    <published>2020-06-01T21:48:22Z</published>
    <title>Independent Component Analysis for Trustworthy Cyberspace during High
  Impact Events: An Application to Covid-19</title>
    <summary>  Social media has become an important communication channel during high impact
events, such as the COVID-19 pandemic. As misinformation in social media can
rapidly spread, creating social unrest, curtailing the spread of misinformation
during such events is a significant data challenge. While recent solutions that
are based on machine learning have shown promise for the detection of
misinformation, most widely used methods include approaches that rely on either
handcrafted features that cannot be optimal for all scenarios, or those that
are based on deep learning where the interpretation of the prediction results
is not directly accessible. In this work, we propose a data-driven solution
that is based on the ICA model, such that knowledge discovery and detection of
misinformation are achieved jointly. To demonstrate the effectiveness of our
method and compare its performance with deep learning methods, we developed a
labeled COVID-19 Twitter dataset based on socio-linguistic criteria.
</summary>
    <author>
      <name>Zois Boukouvalas</name>
    </author>
    <author>
      <name>Christine Mallinson</name>
    </author>
    <author>
      <name>Evan Crothers</name>
    </author>
    <author>
      <name>Nathalie Japkowicz</name>
    </author>
    <author>
      <name>Aritran Piplai</name>
    </author>
    <author>
      <name>Sudip Mittal</name>
    </author>
    <author>
      <name>Anupam Joshi</name>
    </author>
    <author>
      <name>T√ºlay Adalƒ±</name>
    </author>
    <link href="http://arxiv.org/abs/2006.01284v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.01284v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.05271v1</id>
    <updated>2020-08-12T12:42:57Z</updated>
    <published>2020-08-12T12:42:57Z</published>
    <title>Social Media and Health Misinformation during the US COVID Crisis</title>
    <summary>  Health misinformation has been found to be prevalent on social media,
particularly in new public health crises in which there is limited scientific
information. However, social media can also play a role in limiting and
refuting health misinformation. Using as a case study US President Donald
Trump's controversial comments about the promise and power of UV light- and
disinfectant-based treatments, this data memo examines how these comments were
discussed and responded to on Twitter. We find that these comments fell into
established politically partisan narratives and dominated discussion of both
politics and COVID in the days following. Contestation of the comments was much
more prevalent than support. Supporters attacked media coverage in line with
existing Trump narratives. Contesters responded with humour and shared
mainstream media coverage condemning the comments. These practices would have
strengthened the original misinformation through repetition and done little to
construct a successful refutation for those who might have believed them. This
research adds much-needed knowledge to our understanding of the information
environment surrounding COVID and demonstrates that, despite calls for the
depoliticization of health information in this public health crisis, this is
largely being approached as a political issue along divisive, polarised,
partisan lines.
</summary>
    <author>
      <name>Gillian Bolsover</name>
    </author>
    <author>
      <name>Janet Tokitsu Tizon</name>
    </author>
    <link href="http://arxiv.org/abs/2008.05271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.05271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11694v1</id>
    <updated>2021-04-20T23:19:43Z</updated>
    <published>2021-04-20T23:19:43Z</published>
    <title>Mutual Hyperlinking Among Misinformation Peddlers</title>
    <summary>  The internet promised to democratize access to knowledge and make the world
more open and understanding. The reality of today's internet, however, is far
from this ideal. Misinformation, lies, and conspiracies dominate many social
media platforms. This toxic online world has had real-world implications
ranging from genocide to, election interference, and threats to global public
health. A frustrated public and impatient government regulators are calling for
a more vigorous response to mis- and disinformation campaigns designed to sow
civil unrest and inspire violence against individuals, societies, and
democracies. We describe a large-scale, domain-level analysis that reveals
seemingly coordinated efforts between multiple domains to spread and amplify
misinformation. We also describe how the hyperlinks shared by certain Twitter
users can be used to surface problematic domains. These analyses can be used by
search engines and social media recommendation algorithms to systematically
discover and demote misinformation peddlers.
</summary>
    <author>
      <name>Vibhor Sehgal</name>
    </author>
    <author>
      <name>Ankit Peshin</name>
    </author>
    <author>
      <name>Sadia Afroz</name>
    </author>
    <author>
      <name>Hany Farid</name>
    </author>
    <link href="http://arxiv.org/abs/2104.11694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.10801v1</id>
    <updated>2019-04-24T13:25:46Z</updated>
    <published>2019-04-24T13:25:46Z</published>
    <title>Containing misinformation spreading in temporal social networks</title>
    <summary>  Many researchers from a variety of fields including computer science, network
science and mathematics have focused on how to contain the outbreaks of
Internet misinformation that threaten social systems and undermine societal
health. Most research on this topic treats the connections among individuals as
static, but these connections change in time, and thus social networks are also
temporal networks. Currently there is no theoretical approach to the problem of
containing misinformation outbreaks in temporal networks. We thus propose a
misinformation spreading model for temporal networks and describe it using a
new theoretical approach. We propose a heuristic-containing (HC) strategy based
on optimizing final outbreak size that outperforms simplified strategies such
as those that are random-containing (RC) and targeted-containing (TC). We
verify the effectiveness of our HC strategy on both artificial and real-world
networks by performing extensive numerical simulations and theoretical
analyses. We find that the HC strategy greatly increases the outbreak threshold
and decreases the final outbreak threshold.
</summary>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Yuanhui Ma</name>
    </author>
    <author>
      <name>Tao Wu</name>
    </author>
    <author>
      <name>Yang Dai</name>
    </author>
    <author>
      <name>Xingshu Chen</name>
    </author>
    <author>
      <name>Lidia A. Braunstein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.5114853</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.5114853" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Chaos, (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.10801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09218v2</id>
    <updated>2021-05-27T22:07:08Z</updated>
    <published>2020-09-19T12:24:53Z</published>
    <title>Misinformation and its stakeholders in Europe: a web-based analysis</title>
    <summary>  The rise of the internet and computational power in recent years allowed for
the exponential growth of misinformation phenomena. An issue that was a
non-issue a decade ago, became a challenge for societal cohesion. The emergence
of this new threat has led many stakeholders, especially in Europe, to act in
order to tackle this phenomenon. This paper provides in its first part a
literature review on misinformation in Europe, and in its second part a
webometrics analysis on the identified key stakeholders. In the results we
discuss who those stakeholders are, what actions do they perform to limit
misinformation and whether those actions have an impact.
</summary>
    <author>
      <name>Emmanouil Koulas</name>
    </author>
    <author>
      <name>Marios Anthopoulos</name>
    </author>
    <author>
      <name>Sotiria Grammenou</name>
    </author>
    <author>
      <name>Christos Kaimakamis</name>
    </author>
    <author>
      <name>Konstantinos Kousaris</name>
    </author>
    <author>
      <name>Fotini-Rafailia Panavou</name>
    </author>
    <author>
      <name>Orestis Piskioulis</name>
    </author>
    <author>
      <name>Syed Iftikhar H. Shah</name>
    </author>
    <author>
      <name>Vasilios Peristeras</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09218v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09218v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07951v1</id>
    <updated>2021-01-20T03:49:51Z</updated>
    <published>2021-01-20T03:49:51Z</published>
    <title>This photograph has been altered: Testing the effectiveness of image
  forensic labeling on news image credibility</title>
    <summary>  Despite the ubiquity and proliferation of images and videos in online news
environments, much of the existing research on misinformation and its
correction is solely focused on textual misinformation, and little is known
about how ordinary users evaluate fake or manipulated images and the most
effective ways to label and correct such falsities. We designed a visual
forensic label of image authenticity, Picture-O-Meter, and tested the label's
efficacy in relation to its source and placement in an experiment with 2440
participants. Our findings demonstrate that, despite human beings' general
inability to detect manipulated images on their own, image forensic labels are
an effective tool for counteracting visual misinformation.
</summary>
    <author>
      <name>Cuihua Shen</name>
    </author>
    <author>
      <name>Mona Kasra</name>
    </author>
    <author>
      <name>James O'Brien</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.37016/mr-2020-72</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.37016/mr-2020-72" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Harvard Kennedy School (HKS) Misinformation Review (2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.07951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.11362v1</id>
    <updated>2021-02-22T21:06:52Z</updated>
    <published>2021-02-22T21:06:52Z</published>
    <title>An ontological analysis of misinformation in online social networks</title>
    <summary>  The internet, Online Social Networks (OSNs) and smart phones enable users to
create tremendous amount of information. Users who search for general or
specific knowledge may not have these days problems of information scarce but
misinformation. Misinformation nowadays can refer to a continuous spectrum
between what can be seen as "facts" or "truth", if humans agree on the
existence of such, to false information that everyone agree that it is false.
In this paper, we will look at this spectrum of information/misinformation and
compare between some of the major relevant concepts. While few fact-checking
websites exist to evaluate news articles or some of the popular claims people
exchange, nonetheless this can be seen as a little effort in the mission to tag
online information with their "proper" category or label.
</summary>
    <author>
      <name>Izzat Alsmadi</name>
    </author>
    <author>
      <name>Iyad Alazzam</name>
    </author>
    <author>
      <name>Mohammad A. AlRamahi</name>
    </author>
    <link href="http://arxiv.org/abs/2102.11362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.11362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.00163v1</id>
    <updated>2021-06-01T01:12:44Z</updated>
    <published>2021-06-01T01:12:44Z</published>
    <title>Parlermonium: A Data-Driven UX Design Evaluation of the Parler Platform</title>
    <summary>  This paper evaluates Parler, the controversial social media platform, from
two seemingly orthogonal perspectives: UX design perspective and data science.
UX design researchers explore how users react to the interface/content of their
social media feeds; Data science researchers analyze the misinformation flow in
these feeds to detect alternative narratives and state-sponsored disinformation
campaigns. We took a critical look into the intersection of these approaches to
understand how Parler's interface itself is conductive to the flow of
misinformation and the perception of "free speech" among its audience. Parler
drew widespread attention leading up to and after the 2020 U.S. elections as
the "alternative" place for free speech, as a reaction to other mainstream
social media platform which actively engaged in labeling misinformation with
content warnings. Because platforms like Parler are disruptive to the social
media landscape, we believe the evaluation uniquely uncovers the platform's
conductivity to the spread of misinformation.
</summary>
    <author>
      <name>Emma Pieroni</name>
    </author>
    <author>
      <name>Peter Jachim</name>
    </author>
    <author>
      <name>Nathaniel Jachim</name>
    </author>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <link href="http://arxiv.org/abs/2106.00163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.00163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.04737v1</id>
    <updated>2022-02-09T21:33:53Z</updated>
    <published>2022-02-09T21:33:53Z</published>
    <title>Telegram Monitor: Monitoring Brazilian Political Groups and Channels on
  Telegram</title>
    <summary>  Instant messaging platforms such as Telegram became one of the main means of
communication used by people all over the world. Most of them are home of
several groups and channels that connect thousands of people focused on
political topics. However, they have suffered with misinformation campaigns
with a direct impact on electoral processes around the world. While some
platforms, such as WhatsApp, took restrictive policies and measures to
attenuate the issues arising from the abuse of their systems, others have
emerged as alternatives, presenting little or no restrictions on content
moderation or actions in combating misinformation. Telegram is one of those
systems, which has been attracting more users and gaining popularity. In this
work, we present the "Telegram Monitor", a web-based system that monitors the
political debate in this environment and enables the analysis of the most
shared content in multiple channels and public groups. Our system aims to allow
journalists, researchers, and fact-checking agencies to identify trending
conspiracy theories, misinformation campaigns, or simply to monitor the
political debate in this space along the 2022 Brazilian elections. We hope our
system can assist the combat of misinformation spreading through Telegram in
Brazil.
</summary>
    <author>
      <name>Manoel J√∫nior</name>
    </author>
    <author>
      <name>Philipe Melo</name>
    </author>
    <author>
      <name>Daniel Kansaon</name>
    </author>
    <author>
      <name>Vitor Mafra</name>
    </author>
    <author>
      <name>Kaio S√°</name>
    </author>
    <author>
      <name>Fabr√≠cio Benevenuto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, TheWebConf 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.04737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13883v7</id>
    <updated>2024-09-18T07:35:46Z</updated>
    <published>2022-03-25T19:45:33Z</published>
    <title>Multi-modal Misinformation Detection: Approaches, Challenges and
  Opportunities</title>
    <summary>  As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.
</summary>
    <author>
      <name>Sara Abdali</name>
    </author>
    <author>
      <name>Sina shaham</name>
    </author>
    <author>
      <name>Bhaskar Krishnamachari</name>
    </author>
    <link href="http://arxiv.org/abs/2203.13883v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13883v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02000v1</id>
    <updated>2022-04-05T05:47:15Z</updated>
    <published>2022-04-05T05:47:15Z</published>
    <title>The COVMis-Stance dataset: Stance Detection on Twitter for COVID-19
  Misinformation</title>
    <summary>  During the COVID-19 pandemic, large amounts of COVID-19 misinformation are
spreading on social media. We are interested in the stance of Twitter users
towards COVID-19 misinformation. However, due to the relative recent nature of
the pandemic, only a few stance detection datasets fit our task. We have
constructed a new stance dataset consisting of 2631 tweets annotated with the
stance towards COVID-19 misinformation. In contexts with limited labeled data,
we fine-tune our models by leveraging the MNLI dataset and two existing stance
detection datasets (RumourEval and COVIDLies), and evaluate the model
performance on our dataset. Our experimental results show that the model
performs the best when fine-tuned sequentially on the MNLI dataset and the
combination of the undersampled RumourEval and COVIDLies datasets. Our code and
dataset are publicly available at
https://github.com/yanfangh/covid-rumor-stance
</summary>
    <author>
      <name>Yanfang Hou</name>
    </author>
    <author>
      <name>Peter van der Putten</name>
    </author>
    <author>
      <name>Suzan Verberne</name>
    </author>
    <link href="http://arxiv.org/abs/2204.02000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09548v1</id>
    <updated>2022-04-20T15:35:52Z</updated>
    <published>2022-04-20T15:35:52Z</published>
    <title>Misinformed by Visualization: What Do We Learn From Misinformative
  Visualizations?</title>
    <summary>  Data visualization is powerful in persuading an audience. However, when it is
done poorly or maliciously, a visualization may become misleading or even
deceiving. Visualizations give further strength to the dissemination of
misinformation on the Internet. The visualization research community has long
been aware of visualizations that misinform the audience, mostly associated
with the terms "lie" and "deceptive." Still, these discussions have focused
only on a handful of cases. To better understand the landscape of misleading
visualizations, we open-coded over one thousand real-world visualizations that
have been reported as misleading. From these examples, we discovered 74 types
of issues and formed a taxonomy of misleading elements in visualizations. We
found four directions that the research community can follow to widen the
discussion on misleading visualizations: (1) informal fallacies in
visualizations, (2) exploiting conventions and data literacy, (3) deceptive
tricks in uncommon charts, and (4) understanding the designers' dilemma. This
work lays the groundwork for these research directions, especially in
understanding, detecting, and preventing them.
</summary>
    <author>
      <name>Leo Yu-Ho Lo</name>
    </author>
    <author>
      <name>Ayush Gupta</name>
    </author>
    <author>
      <name>Kento Shigyo</name>
    </author>
    <author>
      <name>Aoyu Wu</name>
    </author>
    <author>
      <name>Enrico Bertini</name>
    </author>
    <author>
      <name>Huamin Qu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at EuroVis 2022. Appendix available at
  https://github.com/leoyuholo/bad-vis-browser/blob/master/misinformed_by_visualization_appendix.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.12690v1</id>
    <updated>2022-04-27T04:15:15Z</updated>
    <published>2022-04-27T04:15:15Z</published>
    <title>Multimodal Pipeline for Collection of Misinformation Data from Telegram</title>
    <summary>  The paper presents the outcomes of AI-COVID19, our project aimed at better
understanding of misinformation flow about COVID-19 across social media
platforms. The specific focus of the study reported in this paper is on
collecting data from Telegram groups which are active in promotion of
COVID-related misinformation. Our corpus collected so far contains around 28
million words, from almost one million messages. Given that a substantial
portion of misinformation flow in social media is spread via multimodal means,
such as images and video, we have also developed a mechanism for utilising such
channels via producing automatic transcripts for videos and automatic
classification for images into such categories as memes, screenshots of posts
and other kinds of images. The accuracy of the image classification pipeline is
around 87%.
</summary>
    <author>
      <name>Jose Sosa</name>
    </author>
    <author>
      <name>Serge Sharoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Language Resources and Evaluation Conference (LREC) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.12690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.12690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.05401v2</id>
    <updated>2024-07-11T15:13:14Z</updated>
    <published>2022-10-11T12:25:26Z</published>
    <title>MiDe22: An Annotated Multi-Event Tweet Dataset for Misinformation
  Detection</title>
    <summary>  The rapid dissemination of misinformation through online social networks
poses a pressing issue with harmful consequences jeopardizing human health,
public safety, democracy, and the economy; therefore, urgent action is required
to address this problem. In this study, we construct a new human-annotated
dataset, called MiDe22, having 5,284 English and 5,064 Turkish tweets with
their misinformation labels for several recent events between 2020 and 2022,
including the Russia-Ukraine war, COVID-19 pandemic, and Refugees. The dataset
includes user engagements with the tweets in terms of likes, replies, retweets,
and quotes. We also provide a detailed data analysis with descriptive
statistics and the experimental results of a benchmark evaluation for
misinformation detection.
</summary>
    <author>
      <name>Cagri Toraman</name>
    </author>
    <author>
      <name>Oguzhan Ozcelik</name>
    </author>
    <author>
      <name>Furkan ≈ûahinu√ß</name>
    </author>
    <author>
      <name>Fazli Can</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at LREC-COLING 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.05401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.05401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.15954v1</id>
    <updated>2022-10-28T07:18:32Z</updated>
    <published>2022-10-28T07:18:32Z</published>
    <title>Stanceosaurus: Classifying Stance Towards Multilingual Misinformation</title>
    <summary>  We present Stanceosaurus, a new corpus of 28,033 tweets in English, Hindi,
and Arabic annotated with stance towards 251 misinformation claims. As far as
we are aware, it is the largest corpus annotated with stance towards
misinformation claims. The claims in Stanceosaurus originate from 15
fact-checking sources that cover diverse geographical regions and cultures.
Unlike existing stance datasets, we introduce a more fine-grained 5-class
labeling strategy with additional subcategories to distinguish implicit stance.
Pre-trained transformer-based stance classifiers that are fine-tuned on our
corpus show good generalization on unseen claims and regional claims from
countries outside the training data. Cross-lingual experiments demonstrate
Stanceosaurus' capability of training multi-lingual models, achieving 53.1 F1
on Hindi and 50.4 F1 on Arabic without any target-language fine-tuning.
Finally, we show how a domain adaptation method can be used to improve
performance on Stanceosaurus using additional RumourEval-2019 data. We make
Stanceosaurus publicly available to the research community and hope it will
encourage further work on misinformation identification across languages and
cultures.
</summary>
    <author>
      <name>Jonathan Zheng</name>
    </author>
    <author>
      <name>Ashutosh Baheti</name>
    </author>
    <author>
      <name>Tarek Naous</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Alan Ritter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2022 main conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.15954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.15954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.09683v4</id>
    <updated>2023-07-03T07:04:45Z</updated>
    <published>2022-12-19T18:11:10Z</published>
    <title>Human-in-the-loop Evaluation for Early Misinformation Detection: A Case
  Study of COVID-19 Treatments</title>
    <summary>  We present a human-in-the-loop evaluation framework for fact-checking novel
misinformation claims and identifying social media messages that support them.
Our approach extracts check-worthy claims, which are aggregated and ranked for
review. Stance classifiers are then used to identify tweets supporting novel
misinformation claims, which are further reviewed to determine whether they
violate relevant policies. To demonstrate the feasibility of our approach, we
develop a baseline system based on modern NLP methods for human-in-the-loop
fact-checking in the domain of COVID-19 treatments. We make our data and
detailed annotation guidelines available to support the evaluation of
human-in-the-loop systems that identify novel misinformation directly from raw
user-generated content.
</summary>
    <author>
      <name>Ethan Mendes</name>
    </author>
    <author>
      <name>Yang Chen</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Alan Ritter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2023 (main conference)</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.09683v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09683v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.10765v1</id>
    <updated>2023-03-19T20:46:46Z</updated>
    <published>2023-03-19T20:46:46Z</published>
    <title>Modeling and Mitigating Online Misinformation: a Suggested Blockchain
  Approach</title>
    <summary>  Misinformation propagation in online social networks has become an
increasingly challenging problem. Although many studies exist to solve the
problem computationally, a permanent and robust solution is yet to be
discovered. In this study, we propose and demonstrate the effectiveness of a
blockchain-machine learning hybrid approach for addressing the issue of
misinformation in a crowdsourced environment. First, we motivate the use of
blockchain for this problem by finding the crucial parts contributing to the
dissemination of misinformation and how blockchain can be useful, respectively.
Second, we propose a method that combines the wisdom of the crowd with a
behavioral classifier to classify the news stories in terms of their
truthfulness while reducing the effects of the actions performed by malicious
users. We conduct experiments and simulations under different scenarios and
attacks to assess the performance of this approach. Finally, we provide a case
study involving a comparison with an existing approach using Twitter Birdwatch
data. Our results suggest that this solution holds promise and warrants further
investigation.
</summary>
    <author>
      <name>Tolga Yilmaz</name>
    </author>
    <author>
      <name>√ñzg√ºr Ulusoy</name>
    </author>
    <link href="http://arxiv.org/abs/2303.10765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.04811v2</id>
    <updated>2023-05-07T13:55:02Z</updated>
    <published>2023-04-10T18:44:41Z</published>
    <title>A Large-Scale Comparative Study of Accurate COVID-19 Information versus
  Misinformation</title>
    <summary>  The COVID-19 pandemic led to an infodemic where an overwhelming amount of
COVID-19 related content was being disseminated at high velocity through social
media. This made it challenging for citizens to differentiate between accurate
and inaccurate information about COVID-19. This motivated us to carry out a
comparative study of the characteristics of COVID-19 misinformation versus
those of accurate COVID-19 information through a large-scale computational
analysis of over 242 million tweets. The study makes comparisons alongside four
key aspects: 1) the distribution of topics, 2) the live status of tweets, 3)
language analysis and 4) the spreading power over time. An added contribution
of this study is the creation of a COVID-19 misinformation classification
dataset. Finally, we demonstrate that this new dataset helps improve
misinformation classification by more than 9\% based on average F1 measure.
</summary>
    <author>
      <name>Yida Mu</name>
    </author>
    <author>
      <name>Ye Jiang</name>
    </author>
    <author>
      <name>Freddy Heppell</name>
    </author>
    <author>
      <name>Iknoor Singh</name>
    </author>
    <author>
      <name>Carolina Scarton</name>
    </author>
    <author>
      <name>Kalina Bontcheva</name>
    </author>
    <author>
      <name>Xingyi Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICWSM TrueHealth 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.04811v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.04811v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08759v1</id>
    <updated>2023-04-18T06:34:33Z</updated>
    <published>2023-04-18T06:34:33Z</published>
    <title>Exoskeleton for the Mind: Exploring Strategies Against Misinformation
  with a Metacognitive Agent</title>
    <summary>  Misinformation is a global problem in modern social media platforms with few
solutions known to be effective. Social media platforms have offered tools to
raise awareness of information, but these are closed systems that have not been
empirically evaluated. Others have developed novel tools and strategies, but
most have been studied out of context using static stimuli, researcher prompts,
or low fidelity prototypes. We offer a new anti-misinformation agent grounded
in theories of metacognition that was evaluated within Twitter. We report on a
pilot study (n=17) and multi-part experimental study (n=57, n=49) where
participants experienced three versions of the agent, each deploying a
different strategy. We found that no single strategy was superior over the
control. We also confirmed the necessity of transparency and clarity about the
agent's underlying logic, as well as concerns about repeated exposure to
misinformation and lack of user engagement.
</summary>
    <author>
      <name>Yeongdae Kim</name>
    </author>
    <author>
      <name>Takane Ueno</name>
    </author>
    <author>
      <name>Katie Seaborn</name>
    </author>
    <author>
      <name>Hiroki Oura</name>
    </author>
    <author>
      <name>Jacqueline Urakami</name>
    </author>
    <author>
      <name>Yuto Sawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3582700.3582725</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3582700.3582725" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages 209-220</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Augmented Humans International Conference
  2023 (AHs '23). Association for Computing Machinery, New York, NY, USA,
  209-220</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.08759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.11279v1</id>
    <updated>2023-04-22T00:06:24Z</updated>
    <published>2023-04-22T00:06:24Z</published>
    <title>Trust and Reliance in Consensus-Based Explanations from an
  Anti-Misinformation Agent</title>
    <summary>  The illusion of consensus occurs when people believe there is consensus
across multiple sources, but the sources are the same and thus there is no
"true" consensus. We explore this phenomenon in the context of an AI-based
intelligent agent designed to augment metacognition on social media.
Misinformation, especially on platforms like Twitter, is a global problem for
which there is currently no good solution. As an explainable AI (XAI) system,
the agent provides explanations for its decisions on the misinformed nature of
social media content. In this late-breaking study, we explored the roles of
trust (attitude) and reliance (behaviour) as key elements of XAI user
experience (UX) and whether these influenced the illusion of consensus.
Findings show no effect of trust, but an effect of reliance on consensus-based
explanations. This work may guide the design of anti-misinformation systems
that use XAI, especially the user-centred design of explanations.
</summary>
    <author>
      <name>Takane Ueno</name>
    </author>
    <author>
      <name>Yeongdae Kim</name>
    </author>
    <author>
      <name>Hiroki Oura</name>
    </author>
    <author>
      <name>Katie Seaborn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3544549.3585713</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3544549.3585713" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Extended Abstracts of the 2023 CHI Conference on Human Factors
  in Computing Systems (CHI EA '23). Association for Computing Machinery, New
  York, NY, USA, Article 296, 1-7</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.11279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.05964v2</id>
    <updated>2024-09-14T07:45:27Z</updated>
    <published>2023-05-10T08:16:36Z</published>
    <title>Interpretable Multimodal Misinformation Detection with Logic Reasoning</title>
    <summary>  Multimodal misinformation on online social platforms is becoming a critical
concern due to increasing credibility and easier dissemination brought by
multimedia content, compared to traditional text-only information. While
existing multimodal detection approaches have achieved high performance, the
lack of interpretability hinders these systems' reliability and practical
deployment. Inspired by NeuralSymbolic AI which combines the learning ability
of neural networks with the explainability of symbolic learning, we propose a
novel logic-based neural model for multimodal misinformation detection which
integrates interpretable logic clauses to express the reasoning process of the
target task. To make learning effective, we parameterize symbolic logical
elements using neural representations, which facilitate the automatic
generation and evaluation of meaningful logic clauses. Additionally, to make
our framework generalizable across diverse misinformation sources, we introduce
five meta-predicates that can be instantiated with different correlations.
Results on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the
feasibility and versatility of our model.
</summary>
    <author>
      <name>Hui Liu</name>
    </author>
    <author>
      <name>Wenya Wang</name>
    </author>
    <author>
      <name>Haoliang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Findings of ACL 23. 9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.05964v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.05964v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.12357v1</id>
    <updated>2023-05-21T05:35:48Z</updated>
    <published>2023-05-21T05:35:48Z</published>
    <title>CoSINT: Designing a Collaborative Capture the Flag Competition to
  Investigate Misinformation</title>
    <summary>  Crowdsourced investigations shore up democratic institutions by debunking
misinformation and uncovering human rights abuses. However, current
crowdsourcing approaches rely on simplistic collaborative or competitive models
and lack technological support, limiting their collective impact. Prior
research has shown that blending elements of competition and collaboration can
lead to greater performance and creativity, but crowdsourced investigations
pose unique analytical and ethical challenges. In this paper, we employed a
four-month-long Research through Design process to design and evaluate a novel
interaction style called collaborative capture the flag competitions (CoCTFs).
We instantiated this interaction style through CoSINT, a platform that enables
a trained crowd to work with professional investigators to identify and
investigate social media misinformation. Our mixed-methods evaluation showed
that CoSINT leverages the complementary strengths of competition and
collaboration, allowing a crowd to quickly identify and debunk misinformation.
We also highlight tensions between competition versus collaboration and discuss
implications for the design of crowdsourced investigations.
</summary>
    <author>
      <name>Sukrit Venkatagiri</name>
    </author>
    <author>
      <name>Anirban Mukhopadhyay</name>
    </author>
    <author>
      <name>David Hicks</name>
    </author>
    <author>
      <name>Aaron Brantly</name>
    </author>
    <author>
      <name>Kurt Luther</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3563657.3595997</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3563657.3595997" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACM Designing Interactive Systems 2023 (DIS 2023). To
  cite this paper please use the official citation available here:
  https://doi.org/10.1145/3563657.3595997</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Designing Interactive Systems Conference 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2305.12357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.12357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.14928v3</id>
    <updated>2023-10-31T07:19:01Z</updated>
    <published>2023-05-24T09:10:20Z</published>
    <title>Towards Reliable Misinformation Mitigation: Generalization, Uncertainty,
  and GPT-4</title>
    <summary>  Misinformation poses a critical societal challenge, and current approaches
have yet to produce an effective solution. We propose focusing on
generalization, uncertainty, and how to leverage recent large language models,
in order to create more practical tools to evaluate information veracity in
contexts where perfect classification is impossible. We first demonstrate that
GPT-4 can outperform prior methods in multiple settings and languages. Next, we
explore generalization, revealing that GPT-4 and RoBERTa-large exhibit
differences in failure modes. Third, we propose techniques to handle
uncertainty that can detect impossible examples and strongly improve outcomes.
We also discuss results on other language models, temperature, prompting,
versioning, explainability, and web retrieval, each one providing practical
insights and directions for future research. Finally, we publish the LIAR-New
dataset with novel paired English and French misinformation data and
Possibility labels that indicate if there is sufficient context for veracity
evaluation. Overall, this research lays the groundwork for future tools that
can drive real-world progress to combat misinformation.
</summary>
    <author>
      <name>Kellin Pelrine</name>
    </author>
    <author>
      <name>Anne Imouza</name>
    </author>
    <author>
      <name>Camille Thibault</name>
    </author>
    <author>
      <name>Meilina Reksoprodjo</name>
    </author>
    <author>
      <name>Caleb Gupta</name>
    </author>
    <author>
      <name>Joel Christoph</name>
    </author>
    <author>
      <name>Jean-Fran√ßois Godbout</name>
    </author>
    <author>
      <name>Reihaneh Rabbany</name>
    </author>
    <link href="http://arxiv.org/abs/2305.14928v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.14928v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.13913v1</id>
    <updated>2023-06-24T09:25:44Z</updated>
    <published>2023-06-24T09:25:44Z</published>
    <title>Temporal Analysis of Misinformation on Parler</title>
    <summary>  Social media platforms have facilitated the rapid spread of dis- and
mis-information. Parler, a US-based fringe social media platform that positions
itself as a champion of free-speech, has had substantial information integrity
issues. In this study, we seek to characterize temporal misinformation trends
on Parler. Comparing a dataset of 189 million posts and comments from Parler
against 1591 rated claims (false, barely true, half true, mostly true, pants on
fire, true) from Politifact, we identified 231,881 accuracy-labeled posts on
Parler. We used BERT-Topic to thematically analyze the Poltifact claims, and
then compared trends in these categories to real world events to contextualize
their distribution. We identified three distinct categories of misinformation
circulating on Parler: COVID-19, the 2020 presidential election, and the Black
Lives Matter movement. Our results are significant, with a surprising 69.2% of
posts in our dataset found to be 'false' and 7.6% 'barely true'. We also found
that when Parler posts ('parleys') containing misinformation were posted
increased around major events (e.g., George Floyd's murder).
</summary>
    <author>
      <name>Eliana Norton</name>
    </author>
    <author>
      <name>Tha√Øs Thomas</name>
    </author>
    <author>
      <name>Akaash Kolluri</name>
    </author>
    <author>
      <name>Torie Hyunsik Kim</name>
    </author>
    <author>
      <name>Dhiraj Murthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.13913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.13913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.08860v1</id>
    <updated>2023-08-17T08:41:32Z</updated>
    <published>2023-08-17T08:41:32Z</published>
    <title>Mitigating Misinformation Spreading in Social Networks Via Edge Blocking</title>
    <summary>  The wide adoption of social media platforms has brought about numerous
benefits for communication and information sharing. However, it has also led to
the rapid spread of misinformation, causing significant harm to individuals,
communities, and society at large. Consequently, there has been a growing
interest in devising efficient and effective strategies to contain the spread
of misinformation. One popular countermeasure is blocking edges in the
underlying network.
  We model the spread of misinformation using the classical Independent Cascade
model and study the problem of minimizing the spread by blocking a given number
of edges. We prove that this problem is computationally hard, but we propose an
intuitive community-based algorithm, which aims to detect well-connected
communities in the network and disconnect the inter-community edges. Our
experiments on various real-world social networks demonstrate that the proposed
algorithm significantly outperforms the prior methods, which mostly rely on
centrality measures.
</summary>
    <author>
      <name>Ahad N. Zehmakan</name>
    </author>
    <author>
      <name>Khushvind Maurya</name>
    </author>
    <link href="http://arxiv.org/abs/2308.08860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.08860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.17711v1</id>
    <updated>2023-10-26T18:12:02Z</updated>
    <published>2023-10-26T18:12:02Z</published>
    <title>Is Explanation the Cure? Misinformation Mitigation in the Short Term and
  Long Term</title>
    <summary>  With advancements in natural language processing (NLP) models, automatic
explanation generation has been proposed to mitigate misinformation on social
media platforms in addition to adding warning labels to identified fake news.
While many researchers have focused on generating good explanations, how these
explanations can really help humans combat fake news is under-explored. In this
study, we compare the effectiveness of a warning label and the state-of-the-art
counterfactual explanations generated by GPT-4 in debunking misinformation. In
a two-wave, online human-subject study, participants (N = 215) were randomly
assigned to a control group in which false contents are shown without any
intervention, a warning tag group in which the false claims were labeled, or an
explanation group in which the false contents were accompanied by GPT-4
generated explanations. Our results show that both interventions significantly
decrease participants' self-reported belief in fake claims in an equivalent
manner for the short-term and long-term. We discuss the implications of our
findings and directions for future NLP-based misinformation debunking
strategies.
</summary>
    <author>
      <name>Yi-Li Hsu</name>
    </author>
    <author>
      <name>Shih-Chieh Dai</name>
    </author>
    <author>
      <name>Aiping Xiong</name>
    </author>
    <author>
      <name>Lun-Wei Ku</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP Findings 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.17711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.17711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.09085v5</id>
    <updated>2024-05-31T15:13:33Z</updated>
    <published>2023-12-14T16:16:50Z</published>
    <title>The Earth is Flat because...: Investigating LLMs' Belief towards
  Misinformation via Persuasive Conversation</title>
    <summary>  Large language models (LLMs) encapsulate vast amounts of knowledge but still
remain vulnerable to external misinformation. Existing research mainly studied
this susceptibility behavior in a single-turn setting. However, belief can
change during a multi-turn conversation, especially a persuasive one.
Therefore, in this study, we delve into LLMs' susceptibility to persuasive
conversations, particularly on factual questions that they can answer
correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which
contains factual questions paired with systematically generated persuasive
misinformation. Then, we develop a testing framework to track LLMs' belief
changes in a persuasive dialogue. Through extensive experiments, we find that
LLMs' correct beliefs on factual knowledge can be easily manipulated by various
persuasive strategies.
</summary>
    <author>
      <name>Rongwu Xu</name>
    </author>
    <author>
      <name>Brian S. Lin</name>
    </author>
    <author>
      <name>Shujian Yang</name>
    </author>
    <author>
      <name>Tianqi Zhang</name>
    </author>
    <author>
      <name>Weiyan Shi</name>
    </author>
    <author>
      <name>Tianwei Zhang</name>
    </author>
    <author>
      <name>Zhixuan Fang</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Han Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL'24 (Main). Camera-ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.09085v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.09085v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.01197v1</id>
    <updated>2024-01-02T13:01:50Z</updated>
    <published>2024-01-02T13:01:50Z</published>
    <title>Uncertainty Resolution in Misinformation Detection</title>
    <summary>  Misinformation poses a variety of risks, such as undermining public trust and
distorting factual discourse. Large Language Models (LLMs) like GPT-4 have been
shown effective in mitigating misinformation, particularly in handling
statements where enough context is provided. However, they struggle to assess
ambiguous or context-deficient statements accurately. This work introduces a
new method to resolve uncertainty in such statements. We propose a framework to
categorize missing information and publish category labels for the LIAR-New
dataset, which is adaptable to cross-domain content with missing information.
We then leverage this framework to generate effective user queries for missing
context. Compared to baselines, our method improves the rate at which generated
questions are answerable by the user by 38 percentage points and classification
performance by over 10 percentage points macro F1. Thus, this approach may
provide a valuable component for future misinformation mitigation pipelines.
</summary>
    <author>
      <name>Yury Orlovskiy</name>
    </author>
    <author>
      <name>Camille Thibault</name>
    </author>
    <author>
      <name>Anne Imouza</name>
    </author>
    <author>
      <name>Jean-Fran√ßois Godbout</name>
    </author>
    <author>
      <name>Reihaneh Rabbany</name>
    </author>
    <author>
      <name>Kellin Pelrine</name>
    </author>
    <link href="http://arxiv.org/abs/2401.01197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.01197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.02379v3</id>
    <updated>2024-03-26T20:27:34Z</updated>
    <published>2024-01-04T17:47:36Z</published>
    <title>Detection and Discovery of Misinformation Sources using Attributed
  Webgraphs</title>
    <summary>  Website reliability labels underpin almost all research in misinformation
detection. However, misinformation sources often exhibit transient behavior,
which makes many such labeled lists obsolete over time. We demonstrate that
Search Engine Optimization (SEO) attributes provide strong signals for
predicting news site reliability. We introduce a novel attributed webgraph
dataset with labeled news domains and their connections to outlinking and
backlinking domains. We demonstrate the success of graph neural networks in
detecting news site reliability using these attributed webgraphs, and show that
our baseline news site reliability classifier outperforms current SoTA methods
on the PoliticalNews dataset, achieving an F1 score of 0.96. Finally, we
introduce and evaluate a novel graph-based algorithm for discovering previously
unknown misinformation news sources.
</summary>
    <author>
      <name>Peter Carragher</name>
    </author>
    <author>
      <name>Evan M. Williams</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1609/icwsm.v18i1.31309</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1609/icwsm.v18i1.31309" rel="related"/>
    <link href="http://arxiv.org/abs/2401.02379v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.02379v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.06920v1</id>
    <updated>2024-01-12T22:27:25Z</updated>
    <published>2024-01-12T22:27:25Z</published>
    <title>Comparing GPT-4 and Open-Source Language Models in Misinformation
  Mitigation</title>
    <summary>  Recent large language models (LLMs) have been shown to be effective for
misinformation detection. However, the choice of LLMs for experiments varies
widely, leading to uncertain conclusions. In particular, GPT-4 is known to be
strong in this domain, but it is closed source, potentially expensive, and can
show instability between different versions. Meanwhile, alternative LLMs have
given mixed results. In this work, we show that Zephyr-7b presents a
consistently viable alternative, overcoming key limitations of commonly used
approaches like Llama-2 and GPT-3.5. This provides the research community with
a solid open-source option and shows open-source models are gradually catching
up on this task. We then highlight how GPT-3.5 exhibits unstable performance,
such that this very widely used model could provide misleading results in
misinformation detection. Finally, we validate new tools including approaches
to structured output and the latest version of GPT-4 (Turbo), showing they do
not compromise performance, thus unlocking them for future research and
potentially enabling more complex pipelines for misinformation mitigation.
</summary>
    <author>
      <name>Tyler Vergho</name>
    </author>
    <author>
      <name>Jean-Francois Godbout</name>
    </author>
    <author>
      <name>Reihaneh Rabbany</name>
    </author>
    <author>
      <name>Kellin Pelrine</name>
    </author>
    <link href="http://arxiv.org/abs/2401.06920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.06920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.08694v2</id>
    <updated>2024-01-30T21:59:08Z</updated>
    <published>2024-01-13T16:36:58Z</published>
    <title>Combining Confidence Elicitation and Sample-based Methods for
  Uncertainty Quantification in Misinformation Mitigation</title>
    <summary>  Large Language Models have emerged as prime candidates to tackle
misinformation mitigation. However, existing approaches struggle with
hallucinations and overconfident predictions. We propose an uncertainty
quantification framework that leverages both direct confidence elicitation and
sampled-based consistency methods to provide better calibration for NLP
misinformation mitigation solutions. We first investigate the calibration of
sample-based consistency methods that exploit distinct features of consistency
across sample sizes and stochastic levels. Next, we evaluate the performance
and distributional shift of a robust numeric verbalization prompt across single
vs. two-step confidence elicitation procedure. We also compare the performance
of the same prompt with different versions of GPT and different numerical
scales. Finally, we combine the sample-based consistency and verbalized methods
to propose a hybrid framework that yields a better uncertainty estimation for
GPT models. Overall, our work proposes novel uncertainty quantification methods
that will improve the reliability of Large Language Models in misinformation
mitigation applications.
</summary>
    <author>
      <name>Mauricio Rivera</name>
    </author>
    <author>
      <name>Jean-Fran√ßois Godbout</name>
    </author>
    <author>
      <name>Reihaneh Rabbany</name>
    </author>
    <author>
      <name>Kellin Pelrine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.08694v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08694v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.12053v4</id>
    <updated>2025-02-28T15:30:38Z</updated>
    <published>2024-01-22T15:43:14Z</published>
    <title>From trust in news to disagreement: is misinformation more
  controversial?</title>
    <summary>  The growing prevalence of fruitless disagreement threatens social cohesion
and constructive public discourse. While polarised discussions often reflect
distrust in the news, the link between disagreement and misinformation remains
unclear. In this study, we used data from "Cartesio", an online experiment
rating the trustworthiness of Italian news articles annotated for reliability
by experts, to develop a disagreement metric that accounts for differences in
mean trust values. Our findings show that while misinformation is rated as less
trustworthy, it is not more controversial. Furthermore, disagreement correlates
with increased commenting on Facebook. This suggests that combating
misinformation alone may not reduce polarisation. Disagreement focuses more on
the divergence of opinions, trust, and their effects on social cohesion. Our
study lays the groundwork for unsupervised news analysis and highlights the
need for platform design that promotes constructive interactions and reduces
divisiveness.
</summary>
    <author>
      <name>Donald Ruggiero Lo Sardo</name>
    </author>
    <author>
      <name>Emanuele Brugnoli</name>
    </author>
    <author>
      <name>Enrico Ubaldi</name>
    </author>
    <author>
      <name>Pietro Gravino</name>
    </author>
    <author>
      <name>Vittorio Loreto</name>
    </author>
    <link href="http://arxiv.org/abs/2401.12053v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.12053v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.11485v1</id>
    <updated>2024-03-18T05:29:31Z</updated>
    <published>2024-03-18T05:29:31Z</published>
    <title>A Browser Extension for in-place Signaling and Assessment of
  Misinformation</title>
    <summary>  The status-quo of misinformation moderation is a central authority, usually
social platforms, deciding what content constitutes misinformation and how it
should be handled. However, to preserve users' autonomy, researchers have
explored democratized misinformation moderation. One proposition is to enable
users to assess content accuracy and specify whose assessments they trust. We
explore how these affordances can be provided on the web, without cooperation
from the platforms where users consume content. We present a browser extension
that empowers users to assess the accuracy of any content on the web and shows
the user assessments from their trusted sources in-situ. Through a two-week
user study, we report on how users perceive such a tool, the kind of content
users want to assess, and the rationales they use in their assessments. We
identify implications for designing tools that enable users to moderate content
for themselves with the help of those they trust.
</summary>
    <author>
      <name>Farnaz Jahanbakhsh</name>
    </author>
    <author>
      <name>David R. Karger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3613904.3642473</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3613904.3642473" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11--16, 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.11485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.11485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.15925v1</id>
    <updated>2024-04-24T15:37:12Z</updated>
    <published>2024-04-24T15:37:12Z</published>
    <title>Inside the echo chamber: Linguistic underpinnings of misinformation on
  Twitter</title>
    <summary>  Social media users drive the spread of misinformation online by sharing posts
that include erroneous information or commenting on controversial topics with
unsubstantiated arguments often in earnest. Work on echo chambers has suggested
that users' perspectives are reinforced through repeated interactions with
like-minded peers, promoted by homophily and bias in information diffusion.
Building on long-standing interest in the social bases of language and
linguistic underpinnings of social behavior, this work explores how
conversations around misinformation are mediated through language use. We
compare a number of linguistic measures, e.g., in-/out-group cues, readability,
and discourse connectives, within and across topics of conversation and user
communities. Our findings reveal increased presence of group identity signals
and processing fluency within echo chambers during discussions of
misinformation. We discuss the specific character of these broader trends
across topics and examine contextual influences.
</summary>
    <author>
      <name>Xinyu Wang</name>
    </author>
    <author>
      <name>Jiayi Li</name>
    </author>
    <author>
      <name>Sarah Rajtmajer</name>
    </author>
    <link href="http://arxiv.org/abs/2404.15925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.15925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01621v1</id>
    <updated>2024-05-31T18:53:39Z</updated>
    <published>2024-05-31T18:53:39Z</published>
    <title>Susceptibility to Misinformation about COVID-19 Vaccines: A Signal
  Detection Analysis</title>
    <summary>  An analysis drawing on Signal Detection Theory suggests that people may fall
for misinformation because they are unable to discern true from false
information (truth insensitivity) or because they tend to accept information
with a particular slant regardless of whether it is true or false (belief
bias). Three preregistered experiments with participants from the United States
and the United Kingdom (N = 961) revealed that (i) truth insensitivity in
responses to (mis)information about COVID-19 vaccines differed as a function of
prior attitudes toward COVID-19 vaccines; (ii) participants exhibited a strong
belief bias favoring attitude-congruent information; (iii) truth insensitivity
and belief bias jointly predicted acceptance of false information about
COVID-19 vaccines, but belief bias was a much stronger predictor; (iv)
cognitive elaboration increased truth sensitivity without reducing belief bias;
and (v) higher levels of confidence in one's beliefs were associated with
greater belief bias. The findings provide insights into why people fall for
misinformation, which is essential for individual-level interventions to reduce
susceptibility to misinformation.
</summary>
    <author>
      <name>Lea S. Nahon</name>
    </author>
    <author>
      <name>Nyx L. Ng</name>
    </author>
    <author>
      <name>Bertram Gawronski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jesp.2024.104632</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jesp.2024.104632" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Experimental Social Psychology, 114, Article 104632
  (2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2406.01621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.11497v3</id>
    <updated>2024-12-17T14:11:19Z</updated>
    <published>2024-06-17T13:01:12Z</published>
    <title>CrAM: Credibility-Aware Attention Modification in LLMs for Combating
  Misinformation in RAG</title>
    <summary>  Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large
Language Models (LLMs) by referencing external documents. However, the
misinformation in external documents may mislead LLMs' generation. To address
this issue, we explore the task of "credibility-aware RAG", in which LLMs
automatically adjust the influence of retrieved documents based on their
credibility scores to counteract misinformation. To this end, we introduce a
plug-and-play method named $\textbf{Cr}$edibility-aware $\textbf{A}$ttention
$\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in
LLMs and adjusts their attention weights based on the credibility of the
documents, thereby reducing the impact of low-credibility documents.
Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and
Qwen1.5-7B show that CrAM improves the RAG performance of LLMs against
misinformation pollution by over 20%, even surpassing supervised fine-tuning
methods.
</summary>
    <author>
      <name>Boyi Deng</name>
    </author>
    <author>
      <name>Wenjie Wang</name>
    </author>
    <author>
      <name>Fengbin Zhu</name>
    </author>
    <author>
      <name>Qifan Wang</name>
    </author>
    <author>
      <name>Fuli Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI25 camera-ready</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.11497v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.11497v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.17114v1</id>
    <updated>2024-06-24T20:01:43Z</updated>
    <published>2024-06-24T20:01:43Z</published>
    <title>Inception: Efficiently Computable Misinformation Attacks on Markov Games</title>
    <summary>  We study security threats to Markov games due to information asymmetry and
misinformation. We consider an attacker player who can spread misinformation
about its reward function to influence the robust victim player's behavior.
Given a fixed fake reward function, we derive the victim's policy under
worst-case rationality and present polynomial-time algorithms to compute the
attacker's optimal worst-case policy based on linear programming and backward
induction. Then, we provide an efficient inception ("planting an idea in
someone's mind") attack algorithm to find the optimal fake reward function
within a restricted set of reward functions with dominant strategies.
Importantly, our methods exploit the universal assumption of rationality to
compute attacks efficiently. Thus, our work exposes a security vulnerability
arising from standard game assumptions under misinformation.
</summary>
    <author>
      <name>Jeremy McMahan</name>
    </author>
    <author>
      <name>Young Wu</name>
    </author>
    <author>
      <name>Yudong Chen</name>
    </author>
    <author>
      <name>Xiaojin Zhu</name>
    </author>
    <author>
      <name>Qiaomin Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Reinforcement Learning Conference (RLC) 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.17114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.17114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.00156v1</id>
    <updated>2024-07-31T20:45:56Z</updated>
    <published>2024-07-31T20:45:56Z</published>
    <title>Measuring Falseness in News Articles based on Concealment and
  Overstatement</title>
    <summary>  This research investigates the extent of misinformation in certain
journalistic articles by introducing a novel measurement tool to assess the
degrees of falsity. It aims to measure misinformation using two metrics
(concealment and overstatement) to explore how information is interpreted as
false. This should help examine how articles containing partly true and partly
false information can potentially harm readers, as they are more challenging to
identify than completely fabricated information. In this study, the full story
provided by the fact-checking website serves as a standardized source of
information for comparing differences between fake and real news. The result
suggests that false news has greater concealment and overstatement, due to
longer and more complex new stories being shortened and ambiguously phrased.
While there are no major distinctions among categories of politics science and
civics, it demonstrates that misinformation lacks crucial details while
simultaneously containing more redundant words. Hence, news articles containing
partial falsity, categorized as misinformation, can deceive inattentive readers
who lack background knowledge. Hopefully, this approach instigates future
fact-checkers, journalists, and the readers to secure high quality articles for
a resilient information environment.
</summary>
    <author>
      <name>Jiyoung Lee</name>
    </author>
    <author>
      <name>Keeheon Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2408.00156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.00156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.09939v2</id>
    <updated>2024-08-20T08:59:22Z</updated>
    <published>2024-08-19T12:21:34Z</published>
    <title>"Image, Tell me your story!" Predicting the original meta-context of
  visual misinformation</title>
    <summary>  To assist human fact-checkers, researchers have developed automated
approaches for visual misinformation detection. These methods assign veracity
scores by identifying inconsistencies between the image and its caption, or by
detecting forgeries in the image. However, they neglect a crucial point of the
human fact-checking process: identifying the original meta-context of the
image. By explaining what is actually true about the image, fact-checkers can
better detect misinformation, focus their efforts on check-worthy visual
content, engage in counter-messaging before misinformation spreads widely, and
make their explanation more convincing. Here, we fill this gap by introducing
the task of automated image contextualization. We create 5Pils, a dataset of
1,676 fact-checked images with question-answer pairs about their original
meta-context. Annotations are based on the 5 Pillars fact-checking framework.
We implement a first baseline that grounds the image in its original
meta-context using the content of the image and textual evidence retrieved from
the open web. Our experiments show promising results while highlighting several
open challenges in retrieval and reasoning. We make our code and data publicly
available.
</summary>
    <author>
      <name>Jonathan Tonglet</name>
    </author>
    <author>
      <name>Marie-Francine Moens</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Code available at https://github.com/UKPLab/5pils</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.09939v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.09939v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.00009v2</id>
    <updated>2024-10-09T19:13:41Z</updated>
    <published>2024-08-15T15:13:16Z</published>
    <title>Web Retrieval Agents for Evidence-Based Misinformation Detection</title>
    <summary>  This paper develops an agent-based automated fact-checking approach for
detecting misinformation. We demonstrate that combining a powerful LLM agent,
which does not have access to the internet for searches, with an online web
search agent yields better results than when each tool is used independently.
Our approach is robust across multiple models, outperforming alternatives and
increasing the macro F1 of misinformation detection by as much as 20 percent
compared to LLMs without search. We also conduct extensive analyses on the
sources our system leverages and their biases, decisions in the construction of
the system like the search tool and the knowledge base, the type of evidence
needed and its impact on the results, and other parts of the overall process.
By combining strong performance with in-depth understanding, we hope to provide
building blocks for future search-enabled misinformation mitigation systems.
</summary>
    <author>
      <name>Jacob-Junqi Tian</name>
    </author>
    <author>
      <name>Hao Yu</name>
    </author>
    <author>
      <name>Yury Orlovskiy</name>
    </author>
    <author>
      <name>Tyler Vergho</name>
    </author>
    <author>
      <name>Mauricio Rivera</name>
    </author>
    <author>
      <name>Mayank Goel</name>
    </author>
    <author>
      <name>Zachary Yang</name>
    </author>
    <author>
      <name>Jean-Francois Godbout</name>
    </author>
    <author>
      <name>Reihaneh Rabbany</name>
    </author>
    <author>
      <name>Kellin Pelrine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 main figure, 8 tables, 10 pages, 12 figures in Appendix, 7 tables
  in Appendix GitHub URL: https://github.com/ComplexData-MILA/webretrieval</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.00009v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.00009v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.11308v1</id>
    <updated>2024-09-17T16:05:09Z</updated>
    <published>2024-09-17T16:05:09Z</published>
    <title>SpMis: An Investigation of Synthetic Spoken Misinformation Detection</title>
    <summary>  In recent years, speech generation technology has advanced rapidly, fueled by
generative models and large-scale training techniques. While these developments
have enabled the production of high-quality synthetic speech, they have also
raised concerns about the misuse of this technology, particularly for
generating synthetic misinformation. Current research primarily focuses on
distinguishing machine-generated speech from human-produced speech, but the
more urgent challenge is detecting misinformation within spoken content. This
task requires a thorough analysis of factors such as speaker identity, topic,
and synthesis. To address this need, we conduct an initial investigation into
synthetic spoken misinformation detection by introducing an open-source
dataset, SpMis. SpMis includes speech synthesized from over 1,000 speakers
across five common topics, utilizing state-of-the-art text-to-speech systems.
Although our results show promising detection capabilities, they also reveal
substantial challenges for practical implementation, underscoring the
importance of ongoing research in this critical area.
</summary>
    <author>
      <name>Peizhuo Liu</name>
    </author>
    <author>
      <name>Li Wang</name>
    </author>
    <author>
      <name>Renqiang He</name>
    </author>
    <author>
      <name>Haorui He</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Huadi Zheng</name>
    </author>
    <author>
      <name>Jie Shi</name>
    </author>
    <author>
      <name>Tong Xiao</name>
    </author>
    <author>
      <name>Zhizheng Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in SLT 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.11308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.11308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.12061v2</id>
    <updated>2024-10-26T20:27:22Z</updated>
    <published>2024-10-15T20:58:42Z</published>
    <title>CrediRAG: Network-Augmented Credibility-Based Retrieval for
  Misinformation Detection in Reddit</title>
    <summary>  Fake news threatens democracy and exacerbates the polarization and divisions
in society; therefore, accurately detecting online misinformation is the
foundation of addressing this issue. We present CrediRAG, the first fake news
detection model that combines language models with access to a rich external
political knowledge base with a dense social network to detect fake news across
social media at scale. CrediRAG uses a news retriever to initially assign a
misinformation score to each post based on the source credibility of similar
news articles to the post title content. CrediRAG then improves the initial
retrieval estimations through a novel weighted post-to-post network connected
based on shared commenters and weighted by the average stance of all shared
commenters across every pair of posts. We achieve 11% increase in the F1-score
in detecting misinformative posts over state-of-the-art methods. Extensive
experiments conducted on curated real-world Reddit data of over 200,000 posts
demonstrate the superior performance of CrediRAG on existing baselines. Thus,
our approach offers a more accurate and scalable solution to combat the spread
of fake news across social media platforms.
</summary>
    <author>
      <name>Ashwin Ram</name>
    </author>
    <author>
      <name>Yigit Ege Bayiz</name>
    </author>
    <author>
      <name>Arash Amini</name>
    </author>
    <author>
      <name>Mustafa Munir</name>
    </author>
    <author>
      <name>Radu Marculescu</name>
    </author>
    <link href="http://arxiv.org/abs/2410.12061v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.12061v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.13909v1</id>
    <updated>2024-10-16T23:58:26Z</updated>
    <published>2024-10-16T23:58:26Z</published>
    <title>Large Language Model-driven Multi-Agent Simulation for News Diffusion
  Under Different Network Structures</title>
    <summary>  The proliferation of fake news in the digital age has raised critical
concerns, particularly regarding its impact on societal trust and democratic
processes. Diverging from conventional agent-based simulation approaches, this
work introduces an innovative approach by employing a large language model
(LLM)-driven multi-agent simulation to replicate complex interactions within
information ecosystems. We investigate key factors that facilitate news
propagation, such as agent personalities and network structures, while also
evaluating strategies to combat misinformation. Through simulations across
varying network structures, we demonstrate the potential of LLM-based agents in
modeling the dynamics of misinformation spread, validating the influence of
agent traits on the diffusion process. Our findings emphasize the advantages of
LLM-based simulations over traditional techniques, as they uncover underlying
causes of information spread -- such as agents promoting discussions -- beyond
the predefined rules typically employed in existing agent-based models.
Additionally, we evaluate three countermeasure strategies, discovering that
brute-force blocking influential agents in the network or announcing news
accuracy can effectively mitigate misinformation. However, their effectiveness
is influenced by the network structure, highlighting the importance of
considering network structure in the development of future misinformation
countermeasures.
</summary>
    <author>
      <name>Xinyi Li</name>
    </author>
    <author>
      <name>Yu Xu</name>
    </author>
    <author>
      <name>Yongfeng Zhang</name>
    </author>
    <author>
      <name>Edward C. Malthouse</name>
    </author>
    <link href="http://arxiv.org/abs/2410.13909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.13909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.15517v1</id>
    <updated>2024-10-20T21:55:13Z</updated>
    <published>2024-10-20T21:55:13Z</published>
    <title>SceneGraMMi: Scene Graph-boosted Hybrid-fusion for Multi-Modal
  Misinformation Veracity Prediction</title>
    <summary>  Misinformation undermines individual knowledge and affects broader societal
narratives. Despite growing interest in the research community in multi-modal
misinformation detection, existing methods exhibit limitations in capturing
semantic cues, key regions, and cross-modal similarities within multi-modal
datasets. We propose SceneGraMMi, a Scene Graph-boosted Hybrid-fusion approach
for Multi-modal Misinformation veracity prediction, which integrates scene
graphs across different modalities to improve detection performance.
Experimental results across four benchmark datasets show that SceneGraMMi
consistently outperforms state-of-the-art methods. In a comprehensive ablation
study, we highlight the contribution of each component, while Shapley values
are employed to examine the explainability of the model's decision-making
process.
</summary>
    <author>
      <name>Swarang Joshi</name>
    </author>
    <author>
      <name>Siddharth Mavani</name>
    </author>
    <author>
      <name>Joel Alex</name>
    </author>
    <author>
      <name>Arnav Negi</name>
    </author>
    <author>
      <name>Rahul Mishra</name>
    </author>
    <author>
      <name>Ponnurangam Kumaraguru</name>
    </author>
    <link href="http://arxiv.org/abs/2410.15517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.15517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.09578v1</id>
    <updated>2024-12-12T18:53:46Z</updated>
    <published>2024-12-12T18:53:46Z</published>
    <title>DISHONEST: Dissecting misInformation Spread using Homogeneous sOcial
  NEtworks and Semantic Topic classification</title>
    <summary>  The emergence of the COVID-19 pandemic resulted in a significant rise in the
spread of misinformation on online platforms such as Twitter. Oftentimes this
growth is blamed on the idea of the "echo chamber." However, the behavior said
to characterize these echo chambers exists in two dimensions. The first is in a
user's social interactions, where they are said to stick with the same clique
of like-minded users. The second is in the content of their posts, where they
are said to repeatedly espouse homogeneous ideas. In this study, we link the
two by using Twitter's network of retweets to study social interactions and
topic modeling to study tweet content. In order to measure the diversity of a
user's interactions over time, we develop a novel metric to track the speed at
which they travel through the social network. The application of these analysis
methods to misinformation-focused data from the pandemic demonstrates
correlation between social behavior and tweet content. We believe this
correlation supports the common intuition about how antisocial users behave,
and further suggests that it holds even in subcommunities already rife with
misinformation.
</summary>
    <author>
      <name>Caleb Stam</name>
    </author>
    <author>
      <name>Emily Saldanha</name>
    </author>
    <author>
      <name>Mahantesh Halappanavar</name>
    </author>
    <author>
      <name>Anurag Acharya</name>
    </author>
    <link href="http://arxiv.org/abs/2412.09578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.09578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.01902v1</id>
    <updated>2025-02-28T07:12:03Z</updated>
    <published>2025-02-28T07:12:03Z</published>
    <title>An Empirical Analysis of LLMs for Countering Misinformation</title>
    <summary>  While Large Language Models (LLMs) can amplify online misinformation, they
also show promise in tackling misinformation. In this paper, we empirically
study the capabilities of three LLMs -- ChatGPT, Gemini, and Claude -- in
countering political misinformation. We implement a two-step, chain-of-thought
prompting approach, where models first identify credible sources for a given
claim and then generate persuasive responses. Our findings suggest that models
struggle to ground their responses in real news sources, and tend to prefer
citing left-leaning sources. We also observe varying degrees of response
diversity among models. Our findings highlight concerns about using LLMs for
fact-checking through only prompt-engineering, emphasizing the need for more
robust guardrails. Our results have implications for both researchers and
non-technical users.
</summary>
    <author>
      <name>Adiba Mahbub Proma</name>
    </author>
    <author>
      <name>Neeley Pate</name>
    </author>
    <author>
      <name>James Druckman</name>
    </author>
    <author>
      <name>Gourab Ghoshal</name>
    </author>
    <author>
      <name>Hangfeng He</name>
    </author>
    <author>
      <name>Ehsan Hoque</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Adiba and Neeley contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.01902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.01902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.04271v1</id>
    <updated>2025-03-06T10:02:25Z</updated>
    <published>2025-03-06T10:02:25Z</published>
    <title>On Fact and Frequency: LLM Responses to Misinformation Expressed with
  Uncertainty</title>
    <summary>  We study LLM judgments of misinformation expressed with uncertainty. Our
experiments study the response of three widely used LLMs (GPT-4o, LlaMA3,
DeepSeek-v2) to misinformation propositions that have been verified false and
then are transformed into uncertain statements according to an uncertainty
typology. Our results show that after transformation, LLMs change their
factchecking classification from false to not-false in 25% of the cases.
Analysis reveals that the change cannot be explained by predictors to which
humans are expected to be sensitive, i.e., modality, linguistic cues, or
argumentation strategy. The exception is doxastic transformations, which use
linguistic cue phrases such as "It is believed ...".To gain further insight, we
prompt the LLM to make another judgment about the transformed misinformation
statements that is not related to truth value. Specifically, we study LLM
estimates of the frequency with which people make the uncertain statement. We
find a small but significant correlation between judgment of fact and
estimation of frequency.
</summary>
    <author>
      <name>Yana van de Sande</name>
    </author>
    <author>
      <name>Gunes A√ßar</name>
    </author>
    <author>
      <name>Thabo van Woudenberg</name>
    </author>
    <author>
      <name>Martha Larson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 3 tables, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.04271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.04271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91F20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.4; K.4; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.23329v1</id>
    <updated>2025-03-30T06:08:33Z</updated>
    <published>2025-03-30T06:08:33Z</published>
    <title>A Multi-Agent Framework with Automated Decision Rule Optimization for
  Cross-Domain Misinformation Detection</title>
    <summary>  Misinformation spans various domains, but detection methods trained on
specific domains often perform poorly when applied to others. With the rapid
development of Large Language Models (LLMs), researchers have begun to utilize
LLMs for cross-domain misinformation detection. However, existing LLM-based
methods often fail to adequately analyze news in the target domain, limiting
their detection capabilities. More importantly, these methods typically rely on
manually designed decision rules, which are limited by domain knowledge and
expert experience, thus limiting the generalizability of decision rules to
different domains. To address these issues, we propose a MultiAgent Framework
for cross-domain misinformation detection with Automated Decision Rule
Optimization (MARO). Under this framework, we first employs multiple expert
agents to analyze target-domain news. Subsequently, we introduce a
question-reflection mechanism that guides expert agents to facilitate
higherquality analysis. Furthermore, we propose a decision rule optimization
approach based on carefully-designed cross-domain validation tasks to
iteratively enhance the effectiveness of decision rules in different domains.
Experimental results and in-depth analysis on commonlyused datasets demonstrate
that MARO achieves significant improvements over existing methods.
</summary>
    <author>
      <name>Hui Li</name>
    </author>
    <author>
      <name>Ante Wang</name>
    </author>
    <author>
      <name>kunquan li</name>
    </author>
    <author>
      <name>Zhihao Wang</name>
    </author>
    <author>
      <name>Liang Zhang</name>
    </author>
    <author>
      <name>Delai Qiu</name>
    </author>
    <author>
      <name>Qingsong Liu</name>
    </author>
    <author>
      <name>Jinsong Su</name>
    </author>
    <link href="http://arxiv.org/abs/2503.23329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.23329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.21540v1</id>
    <updated>2025-05-24T15:46:06Z</updated>
    <published>2025-05-24T15:46:06Z</published>
    <title>Modeling the Impact of Misinformation Dynamics on Antimicrobial
  Resistance</title>
    <summary>  Antimicrobial Resistance (RAM) poses a significant threat to global public
health, making important medicines less useful. While the medical and
biological reasons behind RAM are well studied, we still don't know enough
about how false health information affects people's actions, which can speed up
RAM. This study presents a new mathematical model to investigate the complex
interplay between the spread of misinformation and the dynamics of RAM. We
adapt a multi-strain fake news model, including distinct population
compartments representing individuals susceptible to, believing in, or
skeptical of various ideas related to antibiotic use. The model considers
multiple "strains" of misinformation, such as the wrong belief that antibiotics
are effective for viral infections or not trusting medical advice regarding
prudent antibiotic prescription. Time delays are integrated to reflect the
latency in information processing, behavioral change, and the manifestation of
resistance. Through stability analysis and numerical simulations, this research
aims to identify critical factors and parameters that influence the propagation
of harmful beliefs and their consequent impact on behaviors contributing to
RAM. The findings could help develop public health campaigns to reduce the
negative impact of misinformation on fighting antimicrobial resistance.
</summary>
    <author>
      <name>Laurance Fakih</name>
    </author>
    <author>
      <name>Andrei Halanay</name>
    </author>
    <link href="http://arxiv.org/abs/2505.21540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.21540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.23449v2</id>
    <updated>2025-05-30T11:34:43Z</updated>
    <published>2025-05-29T13:56:21Z</published>
    <title>CMIE: Combining MLLM Insights with External Evidence for Explainable
  Out-of-Context Misinformation Detection</title>
    <summary>  Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.
</summary>
    <author>
      <name>Fanxiao Li</name>
    </author>
    <author>
      <name>Jiaying Wu</name>
    </author>
    <author>
      <name>Canyuan He</name>
    </author>
    <author>
      <name>Wei Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2505.23449v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.23449v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.09221v1</id>
    <updated>2025-06-10T20:25:10Z</updated>
    <published>2025-06-10T20:25:10Z</published>
    <title>In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation</title>
    <summary>  The spread of online misinformation poses serious threats to democratic
societies. Traditionally, expert fact-checkers verify the truthfulness of
information through investigative processes. However, the volume and immediacy
of online content present major scalability challenges. Crowdsourcing offers a
promising alternative by leveraging non-expert judgments, but it introduces
concerns about bias, accuracy, and interpretability. This thesis investigates
how human intelligence can be harnessed to assess the truthfulness of online
information, focusing on three areas: misinformation assessment, cognitive
biases, and automated fact-checking systems. Through large-scale crowdsourcing
experiments and statistical modeling, it identifies key factors influencing
human judgments and introduces a model for the joint prediction and explanation
of truthfulness. The findings show that non-expert judgments often align with
expert assessments, particularly when factors such as timing and experience are
considered. By deepening our understanding of human judgment and bias in
truthfulness assessment, this thesis contributes to the development of more
transparent, trustworthy, and interpretable systems for combating
misinformation.
</summary>
    <author>
      <name>Michael Soprano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis, University of Udine, defended May 2023, 458 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.09221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.09221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22930v1</id>
    <updated>2025-06-28T15:43:06Z</updated>
    <published>2025-06-28T15:43:06Z</published>
    <title>Towards Explainable Bilingual Multimodal Misinformation Detection and
  Localization</title>
    <summary>  The increasing realism of multimodal content has made misinformation more
subtle and harder to detect, especially in news media where images are
frequently paired with bilingual (e.g., Chinese-English) subtitles. Such
content often includes localized image edits and cross-lingual inconsistencies
that jointly distort meaning while remaining superficially plausible. We
introduce BiMi, a bilingual multimodal framework that jointly performs
region-level localization, cross-modal and cross-lingual consistency detection,
and natural language explanation for misinformation analysis. To support
generalization, BiMi integrates an online retrieval module that supplements
model reasoning with up-to-date external context. We further release BiMiBench,
a large-scale and comprehensive benchmark constructed by systematically editing
real news images and subtitles, comprising 104,000 samples with realistic
manipulations across visual and linguistic modalities. To enhance
interpretability, we apply Group Relative Policy Optimization (GRPO) to improve
explanation quality, marking the first use of GRPO in this domain. Extensive
experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in
classification accuracy, +15.9 in localization accuracy, and +2.5 in
explanation BERTScore, advancing state-of-the-art performance in realistic,
multilingual misinformation detection. Code, models, and datasets will be
released.
</summary>
    <author>
      <name>Yiwei He</name>
    </author>
    <author>
      <name>Xiangtai Li</name>
    </author>
    <author>
      <name>Zhenglin Huang</name>
    </author>
    <author>
      <name>Yi Dong</name>
    </author>
    <author>
      <name>Hao Fei</name>
    </author>
    <author>
      <name>Jiangning Zhang</name>
    </author>
    <author>
      <name>Baoyuan Wu</name>
    </author>
    <author>
      <name>Guangliang Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/2506.22930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.07307v2</id>
    <updated>2025-07-27T18:11:34Z</updated>
    <published>2025-07-09T22:10:06Z</published>
    <title>Multi-Agent Retrieval-Augmented Framework for Evidence-Based
  Counterspeech Against Health Misinformation</title>
    <summary>  Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, cross
evaluations show that our system generalizes well across diverse health
misinformation topics and datasets. And human evaluations reveal that
refinement significantly enhances counterspeech quality and obtains human
preference.
</summary>
    <author>
      <name>Anirban Saha Anik</name>
    </author>
    <author>
      <name>Xiaoying Song</name>
    </author>
    <author>
      <name>Elliott Wang</name>
    </author>
    <author>
      <name>Bryan Wang</name>
    </author>
    <author>
      <name>Bengisu Yarimbas</name>
    </author>
    <author>
      <name>Lingzi Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at COLM 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.07307v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.07307v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.03153v1</id>
    <updated>2022-01-10T03:44:31Z</updated>
    <published>2022-01-10T03:44:31Z</published>
    <title>Promoting and countering misinformation during Australia's 2019-2020
  bushfires: A case study of polarisation</title>
    <summary>  During Australia's unprecedented bushfires in 2019-2020, misinformation
blaming arson resurfaced on Twitter using #ArsonEmergency. The extent to which
bots were responsible for disseminating and amplifying this misinformation has
received scrutiny in the media and academic research. Here we study Twitter
communities spreading this misinformation during the population-level event,
and investigate the role of online communities and bots. Our in-depth
investigation of the dynamics of the discussion uses a phased approach --
before and after reporting of bots promoting the hashtag was broadcast by the
mainstream media. Though we did not find many bots, the most bot-like accounts
were social bots, which present as genuine humans. Further, we distilled
meaningful quantitative differences between two polarised communities in the
Twitter discussion, resulting in the following insights. First, Supporters of
the arson narrative promoted misinformation by engaging others directly with
replies and mentions using hashtags and links to external sources. In response,
Opposers retweeted fact-based articles and official information. Second,
Supporters were embedded throughout their interaction networks, but Opposers
obtained high centrality more efficiently despite their peripheral positions.
By the last phase, Opposers and unaffiliated accounts appeared to coordinate,
potentially reaching a broader audience. Finally, unaffiliated accounts shared
the same URLs as Opposers over Supporters by a ratio of 9:1 in the last phase,
having shared mostly Supporter URLs in the first phase. This foiled Supporters'
efforts, highlighting the value of exposing misinformation campaigns. We
speculate that the communication strategies observed here could be discoverable
in other misinformation-related discussions and could inform
counter-strategies.
</summary>
    <author>
      <name>Derek Weber</name>
    </author>
    <author>
      <name>Lucia Falzon</name>
    </author>
    <author>
      <name>Lewis Mitchell</name>
    </author>
    <author>
      <name>Mehwish Nasim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13278-022-00892-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13278-022-00892-x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">80 pages, 29 figures. Submitted to the International Journal of
  Social Network Analysis and Mining (SNAM) expanding upon
  DOI:10.1007/978-3-030-61841-4_11. arXiv admin note: text overlap with
  arXiv:2004.00742</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.03153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.03153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.03162v1</id>
    <updated>2022-09-07T13:55:59Z</updated>
    <published>2022-09-07T13:55:59Z</published>
    <title>Machine Learning-based Automatic Annotation and Detection of COVID-19
  Fake News</title>
    <summary>  COVID-19 impacted every part of the world, although the misinformation about
the outbreak traveled faster than the virus. Misinformation spread through
online social networks (OSN) often misled people from following correct medical
practices. In particular, OSN bots have been a primary source of disseminating
false information and initiating cyber propaganda. Existing work neglects the
presence of bots that act as a catalyst in the spread and focuses on fake news
detection in 'articles shared in posts' rather than the post (textual) content.
Most work on misinformation detection uses manually labeled datasets that are
hard to scale for building their predictive models. In this research, we
overcome this challenge of data scarcity by proposing an automated approach for
labeling data using verified fact-checked statements on a Twitter dataset. In
addition, we combine textual features with user-level features (such as
followers count and friends count) and tweet-level features (such as number of
mentions, hashtags and urls in a tweet) to act as additional indicators to
detect misinformation. Moreover, we analyzed the presence of bots in tweets and
show that bots change their behavior over time and are most active during the
misinformation campaign. We collected 10.22 Million COVID-19 related tweets and
used our annotation model to build an extensive and original ground truth
dataset for classification purposes. We utilize various machine learning models
to accurately detect misinformation and our best classification model achieves
precision (82%), recall (96%), and false positive rate (3.58%). Also, our bot
analysis indicates that bots generated approximately 10% of misinformation
tweets. Our methodology results in substantial exposure of false information,
thus improving the trustworthiness of information disseminated through social
media platforms.
</summary>
    <author>
      <name>Mohammad Majid Akhtar</name>
    </author>
    <author>
      <name>Bibhas Sharma</name>
    </author>
    <author>
      <name>Ishan Karunanayake</name>
    </author>
    <author>
      <name>Rahat Masood</name>
    </author>
    <author>
      <name>Muhammad Ikram</name>
    </author>
    <author>
      <name>Salil S. Kanhere</name>
    </author>
    <link href="http://arxiv.org/abs/2209.03162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.03162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.08832v2</id>
    <updated>2024-07-01T13:48:32Z</updated>
    <published>2024-01-16T21:10:17Z</published>
    <title>From News Sharers to Post Viewers: How Topic Diversity and Conspiracy
  Theories Shape Engagement With Misinformation During a Health Crisis?</title>
    <summary>  Engagement with misinformation on social media poses unprecedented threats to
societal well-being, particularly during health crises when susceptibility to
misinformation is heightened in a multi-topic context. This paper focuses on
the COVID-19 pandemic and addresses a critical gap in understanding online
engagement with multi-topic misinformation at two user levels: news sharers who
share source news items on social media and post viewers who engage with online
news posts. To this end, we conduct a comprehensive analysis of 7273
fact-checked source news claims related to COVID-19 and their associated posts
on X, through the lens of topic diversity and conspiracy theories. We find that
false news, particularly when accompanied by conspiracy theories, exhibits
higher topic diversity than true news. At the news sharer level, false news has
a longer lifetime and receives more posts on X than true news. Additionally,
the integration of conspiracy theories is significantly associated with a
longer lifetime for COVID-19 misinformation. However, topic diversity has no
significant association with news sharer engagement in terms of news lifetime
and the number of posts. At the post viewer level, contrary to the news sharer
level, news posts characterized by heightened topic diversity receive more
reposts, likes, and replies. Notably, post viewers tend to engage more with
misinformation containing conspiracy narratives: false news posts that contain
conspiracy theories, on average, receive 40.8% more reposts, 45.2% more likes,
and 44.1% more replies compared to false news posts without conspiracy
theories. Our findings suggest that news sharers and post viewers exhibit
different engagement patterns on social media regarding topic diversity and
conspiracy theories, offering valuable insights into designing targeted
misinformation intervention strategies at both user levels.
</summary>
    <author>
      <name>Yuwei Chuai</name>
    </author>
    <author>
      <name>Jichang Zhao</name>
    </author>
    <author>
      <name>Gabriele Lenzini</name>
    </author>
    <link href="http://arxiv.org/abs/2401.08832v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08832v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.20224v3</id>
    <updated>2024-08-16T19:43:03Z</updated>
    <published>2024-07-29T17:58:06Z</published>
    <title>Can Editing LLMs Inject Harm?</title>
    <summary>  Knowledge editing has been increasingly adopted to correct the false or
outdated knowledge in Large Language Models (LLMs). Meanwhile, one critical but
under-explored question is: can knowledge editing be used to inject harm into
LLMs? In this paper, we propose to reformulate knowledge editing as a new type
of safety threat for LLMs, namely Editing Attack, and conduct a systematic
investigation with a newly constructed dataset EditAttack. Specifically, we
focus on two typical safety risks of Editing Attack including Misinformation
Injection and Bias Injection. For the risk of misinformation injection, we
first categorize it into commonsense misinformation injection and long-tail
misinformation injection. Then, we find that editing attacks can inject both
types of misinformation into LLMs, and the effectiveness is particularly high
for commonsense misinformation injection. For the risk of bias injection, we
discover that not only can biased sentences be injected into LLMs with high
effectiveness, but also one single biased sentence injection can cause a bias
increase in general outputs of LLMs, which are even highly irrelevant to the
injected sentence, indicating a catastrophic impact on the overall fairness of
LLMs. Then, we further illustrate the high stealthiness of editing attacks,
measured by their impact on the general knowledge and reasoning capacities of
LLMs, and show the hardness of defending editing attacks with empirical
evidence. Our discoveries demonstrate the emerging misuse risks of knowledge
editing techniques on compromising the safety alignment of LLMs and the
feasibility of disseminating misinformation or bias with LLMs as new channels.
</summary>
    <author>
      <name>Canyu Chen</name>
    </author>
    <author>
      <name>Baixiang Huang</name>
    </author>
    <author>
      <name>Zekun Li</name>
    </author>
    <author>
      <name>Zhaorun Chen</name>
    </author>
    <author>
      <name>Shiyang Lai</name>
    </author>
    <author>
      <name>Xiongxiao Xu</name>
    </author>
    <author>
      <name>Jia-Chen Gu</name>
    </author>
    <author>
      <name>Jindong Gu</name>
    </author>
    <author>
      <name>Huaxiu Yao</name>
    </author>
    <author>
      <name>Chaowei Xiao</name>
    </author>
    <author>
      <name>Xifeng Yan</name>
    </author>
    <author>
      <name>William Yang Wang</name>
    </author>
    <author>
      <name>Philip Torr</name>
    </author>
    <author>
      <name>Dawn Song</name>
    </author>
    <author>
      <name>Kai Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first two authors contributed equally. 9 pages for main paper, 36
  pages including appendix. The code, results, dataset for this paper and more
  resources are on the project website: https://llm-editing.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.20224v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.20224v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01511v1</id>
    <updated>2016-03-04T15:59:06Z</updated>
    <published>2016-03-04T15:59:06Z</published>
    <title>Hoaxy: A Platform for Tracking Online Misinformation</title>
    <summary>  Massive amounts of misinformation have been observed to spread in
uncontrolled fashion across social media. Examples include rumors, hoaxes, fake
news, and conspiracy theories. At the same time, several journalistic
organizations devote significant efforts to high-quality fact checking of
online claims. The resulting information cascades contain instances of both
accurate and inaccurate information, unfold over multiple time scales, and
often reach audiences of considerable size. All these factors pose challenges
for the study of the social dynamics of online news sharing. Here we introduce
Hoaxy, a platform for the collection, detection, and analysis of online
misinformation and its related fact-checking efforts. We discuss the design of
the platform and present a preliminary analysis of a sample of public tweets
containing both fake news and fact checking. We find that, in the aggregate,
the sharing of fact-checking content typically lags that of misinformation by
10--20 hours. Moreover, fake news are dominated by very active users, while
fact checking is a more grass-roots activity. With the increasing risks
connected to massive online misinformation, social news observatories have the
potential to help researchers, journalists, and the general public understand
the dynamics of real and fake news sharing.
</summary>
    <author>
      <name>Chengcheng Shao</name>
    </author>
    <author>
      <name>Giovanni Luca Ciampaglia</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2872518.2890098</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2872518.2890098" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, submitted to Third Workshop on Social News On the
  Web</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05924v2</id>
    <updated>2017-07-17T17:17:37Z</updated>
    <published>2017-06-19T13:26:41Z</published>
    <title>"Everything I Disagree With is #FakeNews": Correlating Political
  Polarization and Spread of Misinformation</title>
    <summary>  An important challenge in the process of tracking and detecting the
dissemination of misinformation is to understand the political gap between
people that engage with the so called "fake news". A possible factor
responsible for this gap is opinion polarization, which may prompt the general
public to classify content that they disagree or want to discredit as fake. In
this work, we study the relationship between political polarization and content
reported by Twitter users as related to "fake news". We investigate how
polarization may create distinct narratives on what misinformation actually is.
We perform our study based on two datasets collected from Twitter. The first
dataset contains tweets about US politics in general, from which we compute the
degree of polarization of each user towards the Republican and Democratic
Party. In the second dataset, we collect tweets and URLs that co-occurred with
"fake news" related keywords and hashtags, such as #FakeNews and
#AlternativeFact, as well as reactions towards such tweets and URLs. We then
analyze the relationship between polarization and what is perceived as
misinformation, and whether users are designating information that they
disagree as fake. Our results show an increase in the polarization of users and
URLs associated with fake-news keywords and hashtags, when compared to
information not labeled as "fake news". We discuss the impact of our findings
on the challenges of tracking "fake news" in the ongoing battle against
misinformation.
</summary>
    <author>
      <name>Manoel Horta Ribeiro</name>
    </author>
    <author>
      <name>Pedro H. Calais</name>
    </author>
    <author>
      <name>Virg√≠lio A. F. Almeida</name>
    </author>
    <author>
      <name>Wagner Meira Jr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures, to be presented at DS+J Workshop @ KDD'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.10462v1</id>
    <updated>2021-09-22T00:00:02Z</updated>
    <published>2021-09-22T00:00:02Z</published>
    <title>A Hierarchical Network-Oriented Analysis of User Participation in
  Misinformation Spread on WhatsApp</title>
    <summary>  WhatsApp emerged as a major communication platform in many countries in the
recent years. Despite offering only one-to-one and small group conversations,
WhatsApp has been shown to enable the formation of a rich underlying network,
crossing the boundaries of existing groups, and with structural properties that
favor information dissemination at large. Indeed, WhatsApp has reportedly been
used as a forum of misinformation campaigns with significant social, political
and economic consequences in several countries. In this article, we aim at
complementing recent studies on misinformation spread on WhatsApp, mostly
focused on content properties and propagation dynamics, by looking into the
network that connects users sharing the same piece of content. Specifically, we
present a hierarchical network-oriented characterization of the users engaged
in misinformation spread by focusing on three perspectives: individuals,
WhatsApp groups and user communities, i.e., groupings of users who,
intentionally or not, share the same content disproportionately often. By
analyzing sharing and network topological properties, our study offers valuable
insights into how WhatsApp users leverage the underlying network connecting
different groups to gain large reach in the spread of misinformation on the
platform.
</summary>
    <author>
      <name>Gabriel Peres Nobre</name>
    </author>
    <author>
      <name>Carlos H. G. Ferreira</name>
    </author>
    <author>
      <name>Jussara M. Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper Accepted in Information Processing &amp; Management, Elsevier</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.10462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.10462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.10161v1</id>
    <updated>2022-05-20T13:09:05Z</updated>
    <published>2022-05-20T13:09:05Z</published>
    <title>The role of the Big Geographic Sort in the circulation of misinformation
  among U.S. Reddit users</title>
    <summary>  Past research has attributed the online circulation of misinformation to two
main factors - individual characteristics (e.g., a person's information
literacy) and social media effects (e.g., algorithm-mediated information
diffusion) - and has overlooked a third one: the critical mass created by the
offline self-segregation of Americans into like-minded geographical regions
such as states (a phenomenon called "The Big Sort"). We hypothesized that this
latter factor matters for the online spreading of misinformation not least
because online interactions, despite having the potential of being global, end
up being localized: interaction probability is known to rapidly decay with
distance. Upon analysis of more than 8M Reddit comments containing news links
spanning four years, from January 2016 to December 2019, we found that Reddit
did not work as an "hype machine" for misinformation (as opposed to what
previous work reported for other platforms, circulation was not mainly caused
by platform-facilitated network effects) but worked as a supply-and-demand
system: misinformation news items scaled linearly with the number of users in
each state (with a scaling exponent beta=1, and a goodness of fit R2 = 0.95).
Furthermore, deviations from such a universal pattern were best explained by
state-level personality and cultural factors (R2 = {0.12, 0.39}), rather than
socioeconomic conditions (R2 = {0.15, 0.29}) or, as one would expect, political
characteristics (R2 ={0.06, 0.21}). Higher-than-expected circulation of any
type of news (including reputable news) was found in states characterised by
residents who tend to be less diligent in terms of their personality (low in
conscientiousness) and by loose cultures understating the importance of
adherence to norms (low in cultural tightness).
</summary>
    <author>
      <name>Lia Bozarth</name>
    </author>
    <author>
      <name>Daniele Quercia</name>
    </author>
    <author>
      <name>Licia Capra</name>
    </author>
    <author>
      <name>Sanja Scepanovic</name>
    </author>
    <link href="http://arxiv.org/abs/2205.10161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.10161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01162v2</id>
    <updated>2020-01-13T04:35:08Z</updated>
    <published>2018-07-01T18:58:59Z</published>
    <title>Reverse Prevention Sampling for Misinformation Mitigation in Social
  Networks</title>
    <summary>  In this work, we consider misinformation propagating through a social network
and study the problem of its prevention. In this problem, a "bad" campaign
starts propagating from a set of seed nodes in the network and we use the
notion of a limiting (or "good") campaign to counteract the effect of
misinformation. The goal is to identify a set of $k$ users that need to be
convinced to adopt the limiting campaign so as to minimize the number of people
that adopt the "bad" campaign at the end of both propagation processes.
  This work presents \emph{RPS} (Reverse Prevention Sampling), an algorithm
that provides a scalable solution to the misinformation mitigation problem. Our
theoretical analysis shows that \emph{RPS} runs in $O((k + l)(n + m)(\frac{1}{1
- \gamma}) \log n / \epsilon^2 )$ expected time and returns a $(1 - 1/e -
\epsilon)$-approximate solution with at least $1 - n^{-l}$ probability (where
$\gamma$ is a typically small network parameter and $l$ is a confidence
parameter). The time complexity of \emph{RPS} substantially improves upon the
previously best-known algorithms that run in time $\Omega(m n k \cdot
POLY(\epsilon^{-1}))$. We experimentally evaluate \emph{RPS} on large datasets
and show that it outperforms the state-of-the-art solution by several orders of
magnitude in terms of running time. This demonstrates that misinformation
mitigation can be made practical while still offering strong theoretical
guarantees.
</summary>
    <author>
      <name>Michael Simpson</name>
    </author>
    <author>
      <name>Venkatesh Srinivasan</name>
    </author>
    <author>
      <name>Alex Thomo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1404.0900, arXiv:1212.0884
  by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01162v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01162v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.08355v1</id>
    <updated>2021-02-16T18:45:01Z</updated>
    <published>2021-02-16T18:45:01Z</published>
    <title>Adversarial Targeted Forgetting in Regularization and Generative Based
  Continual Learning Models</title>
    <summary>  Continual (or "incremental") learning approaches are employed when additional
knowledge or tasks need to be learned from subsequent batches or from streaming
data. However these approaches are typically adversary agnostic, i.e., they do
not consider the possibility of a malicious attack. In our prior work, we
explored the vulnerabilities of Elastic Weight Consolidation (EWC) to the
perceptible misinformation. We now explore the vulnerabilities of other
regularization-based as well as generative replay-based continual learning
algorithms, and also extend the attack to imperceptible misinformation. We show
that an intelligent adversary can take advantage of a continual learning
algorithm's capabilities of retaining existing knowledge over time, and force
it to learn and retain deliberately introduced misinformation. To demonstrate
this vulnerability, we inject backdoor attack samples into the training data.
These attack samples constitute the misinformation, allowing the attacker to
capture control of the model at test time. We evaluate the extent of this
vulnerability on both rotated and split benchmark variants of the MNIST dataset
under two important domain and class incremental learning scenarios. We show
that the adversary can create a "false memory" about any task by inserting
carefully-designed backdoor samples to the test instances of that task thereby
controlling the amount of forgetting of any task of its choosing. Perhaps most
importantly, we show this vulnerability to be very acute and damaging: the
model memory can be easily compromised with the addition of backdoor samples
into as little as 1\% of the training data, even when the misinformation is
imperceptible to human eye.
</summary>
    <author>
      <name>Muhammad Umer</name>
    </author>
    <author>
      <name>Robi Polikar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2002.07111</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.08355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.13489v2</id>
    <updated>2021-11-09T23:01:50Z</updated>
    <published>2021-10-26T08:41:03Z</published>
    <title>Visual Selective Attention System to Intervene User Attention in Sharing
  COVID-19 Misinformation</title>
    <summary>  Information sharing on social media must be accompanied by attentive behavior
so that in a distorted digital environment, users are not rushed and distracted
in deciding to share information. The spread of misinformation, especially
those related to the COVID-19, can divide and create negative effects of
falsehood in society. Individuals can also cause feelings of fear, health
anxiety, and confusion in the treatment COVID-19. Although much research has
focused on understanding human judgment from a psychological underline, few
have addressed the essential issue in the screening phase of what technology
can interfere amidst users' attention in sharing information. This research
aims to intervene in the user's attention with a visual selective attention
approach. This study uses a quantitative method through studies 1 and 2 with
pre-and post-intervention experiments. In study 1, we intervened in user
decisions and attention by stimulating ten information and misinformation using
the Visual Selective Attention System (VSAS) tool. In Study 2, we identified
associations of user tendencies in evaluating information using the Implicit
Association Test (IAT). The significant results showed that the user's
attention and decision behavior improved after using the VSAS. The IAT results
show a change in the association of user exposure, where after the intervention
using VSAS, users tend not to share misinformation about COVID-19. The results
are expected to be the basis for developing social media applications to combat
the negative impact of the infodemic COVID-19 misinformation.
</summary>
    <author>
      <name>Zaid Amin</name>
    </author>
    <author>
      <name>Nazlena Mohamad Ali</name>
    </author>
    <author>
      <name>Alan F. Smeaton</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14569/IJACSA.2021.0121005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14569/IJACSA.2021.0121005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications(IJACSA) Volume 12 Issue 10, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.13489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.13489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.06468v1</id>
    <updated>2022-08-12T19:27:18Z</updated>
    <published>2022-08-12T19:27:18Z</published>
    <title>A Re-Conceptualization of Online Misinformation Diffusion</title>
    <summary>  Online social networks facilitate the diffusion of misinformation. Some
theorists construe the problem of misinformation as a problem of knowledge,
hence of ignorance. This assumption leads to solutions in which misinformation
(false belief) is resisted by good information (true belief). We argue that
information is better understood as gossip. We believe that gossip spreads as
part of an economy of social capital that has a specific discursive grammar
that mimics ordinary human gossip. But there are some critical differences.
These differences have immense and divisive social and political effects. If we
shift our focus from the truth or falsity of information, and instead focus on
the social dynamics of gossip we can more effectively respond to the challenges
and dangers of online social networks.
  Our argument has three parts. (1) We briefly critique epistemological and
truth-centered accounts of misinformation. (2) We describe a basic discursive
grammar of gossip as a social practice. (3) We, then, match the properties of
online information with this discursive grammar of gossip. While gossip has a
particular discursive form, its online modes involve a number of unique social
features that will have immense and divisive social and political effects. Our
goal is not to replace current accounts of information diffusion but to augment
these accounts with a descriptive model of gossip. Information diffusion models
should be understood as tools with which to explore the sociology of evolving
online communities in conjunction with offline communities.
</summary>
    <author>
      <name>Brett Bourbon</name>
    </author>
    <author>
      <name>Renita Murimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.06468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.06468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.01675v4</id>
    <updated>2023-02-24T10:28:34Z</updated>
    <published>2022-09-04T19:08:02Z</published>
    <title>One Year of COVID-19 Vaccine Misinformation on Twitter: Longitudinal
  Study</title>
    <summary>  Vaccinations play a critical role in mitigating the impact of COVID-19 and
other diseases. This study explores COVID-19 vaccine misinformation circulating
on Twitter during 2021, when vaccines were being released to the public in an
effort to mitigate the global pandemic. Our findings show a low prevalence of
low-credibility information compared to mainstream news. However, most popular
low-credibility sources had larger reshare volumes than authoritative sources
such as the CDC and WHO. We observed an increasing trend in the prevalence of
low-credibility news relative to mainstream news about vaccines. We also
observed a considerable amount of suspicious YouTube videos shared on Twitter.
Tweets by a small group of about 800 "superspreaders" verified by Twitter
accounted for approximately 35% of all reshares of misinformation on the
average day, with the top superspreader (@RobertKennedyJr) responsible for over
13% of retweets. Low-credibility news and suspicious YouTube videos were more
likely to be shared by automated accounts. Our findings are consistent with the
hypothesis that superspreaders are driven by financial incentives that allow
them to profit from health misinformation. Despite high-profile cases of
deplatformed misinformation superspreaders, our results show that in 2021 a few
individuals still played an outsize role in the spread of low-credibility
vaccine content. Social media policies should consider revoking the verified
status of repeat-spreaders of harmful content, especially during public health
crises.
</summary>
    <author>
      <name>Francesco Pierri</name>
    </author>
    <author>
      <name>Matthew R. DeVerna</name>
    </author>
    <author>
      <name>Kai-Cheng Yang</name>
    </author>
    <author>
      <name>David Axelrod</name>
    </author>
    <author>
      <name>John Bryden</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2196/42227</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2196/42227" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Forthcoming/in press in JMIR</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Medical Internet Research. 30/01/2023:42227 PMID:
  36735835</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2209.01675v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01675v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.09300v1</id>
    <updated>2022-09-09T02:50:47Z</updated>
    <published>2022-09-09T02:50:47Z</published>
    <title>PoxVerifi: An Information Verification System to Combat Monkeypox
  Misinformation</title>
    <summary>  Following recent outbreaks, monkeypox-related misinformation continues to
rapidly spread online. This negatively impacts response strategies and
disproportionately harms LGBTQ+ communities in the short-term, and ultimately
undermines the overall effectiveness of public health responses. In an attempt
to combat monkeypox-related misinformation, we present PoxVerifi, an
open-source, extensible tool that provides a comprehensive approach to
assessing the accuracy of monkeypox related claims. Leveraging information from
existing fact checking sources and published World Health Organization (WHO)
information, we created an open-sourced corpus of 225 rated monkeypox claims.
Additionally, we trained an open-sourced BERT-based machine learning model for
specifically classifying monkeypox information, which achieved 96%
cross-validation accuracy. PoxVerifi is a Google Chrome browser extension
designed to empower users to navigate through monkeypox-related misinformation.
Specifically, PoxVerifi provides users with a comprehensive toolkit to assess
the veracity of headlines on any webpage across the Internet without having to
visit an external site. Users can view an automated accuracy review from our
trained machine learning model, a user-generated accuracy review based on
community-member votes, and have the ability to see similar, vetted, claims.
Besides PoxVerifi's comprehensive approach to claim-testing, our platform
provides an efficient and accessible method to crowdsource accuracy ratings on
monkeypox related-claims, which can be aggregated to create new labeled
misinformation datasets.
</summary>
    <author>
      <name>Akaash Kolluri</name>
    </author>
    <author>
      <name>Kami Vinton</name>
    </author>
    <author>
      <name>Dhiraj Murthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.09300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.10880v5</id>
    <updated>2023-11-13T23:00:00Z</updated>
    <published>2023-01-26T00:20:02Z</published>
    <title>A Golden Age: Conspiracy Theories' Relationship with Misinformation
  Outlets, News Media, and the Wider Internet</title>
    <summary>  Do we live in a "Golden Age of Conspiracy Theories?" In the last few decades,
conspiracy theories have proliferated on the Internet with some having
dangerous real-world consequences. A large contingent of those who participated
in the January 6th attack on the US Capitol fervently believed in the QAnon
conspiracy theory. In this work, we study the relationships amongst five
prominent conspiracy theories (QAnon, COVID, UFO/Aliens, 9/11, and Flat-Earth)
and each of their respective relationships to the news media, both authentic
news and misinformation. Identifying and publishing a set of 755 different
conspiracy theory websites dedicated to our five conspiracy theories, we find
that each set often hyperlinks to the same external domains, with COVID and
QAnon conspiracy theory websites having the largest amount of shared
connections. Examining the role of news media, we further find that not only do
outlets known for spreading misinformation hyperlink to our set of conspiracy
theory websites more often than authentic news websites but also that this
hyperlinking increased dramatically between 2018 and 2021, with the advent of
QAnon and the start of COVID-19 pandemic. Using partial Granger-causality, we
uncover several positive correlative relationships between the hyperlinks from
misinformation websites and the popularity of conspiracy theory websites,
suggesting the prominent role that misinformation news outlets play in
popularizing many conspiracy theories.
</summary>
    <author>
      <name>Hans W. A. Hanley</name>
    </author>
    <author>
      <name>Deepak Kumar</name>
    </author>
    <author>
      <name>Zakir Durumeric</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CSCW 2023; CSCW version</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.10880v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10880v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.07633v2</id>
    <updated>2024-04-05T23:39:52Z</updated>
    <published>2023-04-15T21:11:55Z</published>
    <title>Interpretable Detection of Out-of-Context Misinformation with
  Neural-Symbolic-Enhanced Large Multimodal Model</title>
    <summary>  Recent years have witnessed the sustained evolution of misinformation that
aims at manipulating public opinions. Unlike traditional rumors or fake news
editors who mainly rely on generated and/or counterfeited images, text and
videos, current misinformation creators now more tend to use out-of-context
multimedia contents (e.g. mismatched images and captions) to deceive the public
and fake news detection systems. This new type of misinformation increases the
difficulty of not only detection but also clarification, because every
individual modality is close enough to true information. To address this
challenge, in this paper we explore how to achieve interpretable cross-modal
de-contextualization detection that simultaneously identifies the mismatched
pairs and the cross-modal contradictions, which is helpful for fact-check
websites to document clarifications. The proposed model first symbolically
disassembles the text-modality information to a set of fact queries based on
the Abstract Meaning Representation of the caption and then forwards the
query-image pairs into a pre-trained large vision-language model select the
``evidences" that are helpful for us to detect misinformation. Extensive
experiments indicate that the proposed methodology can provide us with much
more interpretable predictions while maintaining the accuracy same as the
state-of-the-art model on this task.
</summary>
    <author>
      <name>Yizhou Zhang</name>
    </author>
    <author>
      <name>Loc Trinh</name>
    </author>
    <author>
      <name>Defu Cao</name>
    </author>
    <author>
      <name>Zijun Cui</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.07633v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.07633v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09820v5</id>
    <updated>2024-03-20T03:58:34Z</updated>
    <published>2023-05-16T21:51:01Z</published>
    <title>Machine-Made Media: Monitoring the Mobilization of Machine-Generated
  Articles on Misinformation and Mainstream News Websites</title>
    <summary>  As large language models (LLMs) like ChatGPT have gained traction, an
increasing number of news websites have begun utilizing them to generate
articles. However, not only can these language models produce factually
inaccurate articles on reputable websites but disreputable news sites can
utilize LLMs to mass produce misinformation. To begin to understand this
phenomenon, we present one of the first large-scale studies of the prevalence
of synthetic articles within online news media. To do this, we train a
DeBERTa-based synthetic news detector and classify over 15.46 million articles
from 3,074 misinformation and mainstream news websites. We find that between
January 1, 2022, and May 1, 2023, the relative number of synthetic news
articles increased by 57.3% on mainstream websites while increasing by 474% on
misinformation sites. We find that this increase is largely driven by smaller
less popular websites. Analyzing the impact of the release of ChatGPT using an
interrupted-time-series, we show that while its release resulted in a marked
increase in synthetic articles on small sites as well as misinformation news
websites, there was not a corresponding increase on large mainstream news
websites.
</summary>
    <author>
      <name>Hans W. A. Hanley</name>
    </author>
    <author>
      <name>Zakir Durumeric</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICWSM 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.09820v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09820v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.07960v2</id>
    <updated>2024-08-23T07:51:34Z</updated>
    <published>2023-07-16T06:41:01Z</published>
    <title>Did the Roll-Out of Community Notes Reduce Engagement With
  Misinformation on X/Twitter?</title>
    <summary>  Developing interventions that successfully reduce engagement with
misinformation on social media is challenging. One intervention that has
recently gained great attention is X/Twitter's Community Notes (previously
known as "Birdwatch"). Community Notes is a crowdsourced fact-checking approach
that allows users to write textual notes to inform others about potentially
misleading posts on X/Twitter. Yet, empirical evidence regarding its
effectiveness in reducing engagement with misinformation on social media is
missing. In this paper, we perform a large-scale empirical study to analyze
whether the introduction of the Community Notes feature and its roll-out to
users in the U.S. and around the world have reduced engagement with
misinformation on X/Twitter in terms of retweet volume and likes. We employ
Difference-in-Differences (DiD) models and Regression Discontinuity Design
(RDD) to analyze a comprehensive dataset consisting of all fact-checking notes
and corresponding source tweets since the launch of Community Notes in early
2021. Although we observe a significant increase in the volume of fact-checks
carried out via Community Notes, particularly for tweets from verified users
with many followers, we find no evidence that the introduction of Community
Notes significantly reduced engagement with misleading tweets on X/Twitter.
Rather, our findings suggest that Community Notes might be too slow to
effectively reduce engagement with misinformation in the early (and most viral)
stage of diffusion. Our work emphasizes the importance of evaluating
fact-checking interventions in the field and offers important implications to
enhance crowdsourced fact-checking strategies on social media.
</summary>
    <author>
      <name>Yuwei Chuai</name>
    </author>
    <author>
      <name>Haoye Tian</name>
    </author>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <author>
      <name>Gabriele Lenzini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3686967</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3686967" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CSCW2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.07960v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.07960v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.13180v1</id>
    <updated>2023-07-24T23:59:10Z</updated>
    <published>2023-07-24T23:59:10Z</published>
    <title>Navigating the Web of Misinformation: A Framework for Misinformation
  Domain Detection Using Browser Traffic</title>
    <summary>  The proliferation of misinformation and propaganda is a global challenge,
with profound effects during major crises such as the COVID-19 pandemic and the
Russian invasion of Ukraine. Understanding the spread of misinformation and its
social impacts requires identifying the news sources spreading false
information. While machine learning (ML) techniques have been proposed to
address this issue, ML models have failed to provide an efficient
implementation scenario that yields useful results. In prior research, the
precision of deployment in real traffic deteriorates significantly,
experiencing a decrement up to ten times compared to the results derived from
benchmark data sets. Our research addresses this gap by proposing a graph-based
approach to capture navigational patterns and generate traffic-based features
which are used to train a classification model. These navigational and
traffic-based features result in classifiers that present outstanding
performance when evaluated against real traffic. Moreover, we also propose
graph-based filtering techniques to filter out models to be classified by our
framework. These filtering techniques increase the signal-to-noise ratio of the
models to be classified, greatly reducing false positives and the computational
cost of deploying the model. Our proposed framework for the detection of
misinformation domains achieves a precision of 0.78 when evaluated in real
traffic. This outcome represents an improvement factor of over ten times over
those achieved in previous studies.
</summary>
    <author>
      <name>Mayana Pereira</name>
    </author>
    <author>
      <name>Kevin Greene</name>
    </author>
    <author>
      <name>Nilima Pisharody</name>
    </author>
    <author>
      <name>Rahul Dodhia</name>
    </author>
    <author>
      <name>Jacob N. Shapiro</name>
    </author>
    <author>
      <name>Juan Lavista</name>
    </author>
    <link href="http://arxiv.org/abs/2307.13180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.13180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.02037v1</id>
    <updated>2023-07-30T03:03:48Z</updated>
    <published>2023-07-30T03:03:48Z</published>
    <title>Proposing a conceptual framework: social media listening for public
  health behavior</title>
    <summary>  Existing communications and behavioral theories have been adopted to address
health misinformation. Although various theories and models have been used to
investigate the COVID-19 pandemic, there is no framework specially designed for
social listening or misinformation studies using social media data and natural
language processing techniques. This study aimed to propose a novel yet
theory-based conceptual framework for misinformation research. We collected
theories and models used in COVID-19 related studies published in peer-reviewed
journals. The theories and models ranged from health behaviors, communications,
to misinformation. They are analyzed and critiqued for their components,
followed by proposing a conceptual framework with a demonstration. We reviewed
Health Belief Model, Theory of Planned Behavior/Reasoned Action, Communication
for Behavioral Impact, Transtheoretical Model, Uses and Gratifications Theory,
Social Judgment Theory, Risk Information Seeking and Processing Model,
Behavioral and Social Drivers, and Hype Loop. Accordingly, we proposed the
Social Media Listening for Public Health Behavior Conceptual Framework by not
only integrating important attributes of existing theories, but also adding new
attributes. The proposed conceptual framework was demonstrated in the Freedom
Convoy social media listening. The proposed conceptual framework can be used to
better understand public discourse on social media, and it can be integrated
with other data analyses to gather a more comprehensive picture. The framework
will continue to be revised and adopted as health misinformation evolves.
</summary>
    <author>
      <name>Shu-Feng Tsao</name>
    </author>
    <author>
      <name>Helen Chen</name>
    </author>
    <author>
      <name>Samantha Meyer</name>
    </author>
    <author>
      <name>Zahid A. Butt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 2 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.02037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.02037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.12215v4</id>
    <updated>2025-01-27T02:36:13Z</updated>
    <published>2023-08-23T15:52:20Z</published>
    <title>SoK: Machine Learning for Misinformation Detection</title>
    <summary>  We examine the disconnect between scholarship and practice in applying
machine learning to trust and safety problems, using misinformation detection
as a case study. We survey literature on automated detection of misinformation
across a corpus of 248 well-cited papers in the field. We then examine subsets
of papers for data and code availability, design missteps, reproducibility, and
generalizability. Our paper corpus includes published work in security, natural
language processing, and computational social science. Across these disparate
disciplines, we identify common errors in dataset and method design. In
general, detection tasks are often meaningfully distinct from the challenges
that online services actually face. Datasets and model evaluation are often
non-representative of real-world contexts, and evaluation frequently is not
independent of model training. We demonstrate the limitations of current
detection methods in a series of three representative replication studies.
Based on the results of these analyses and our literature survey, we conclude
that the current state-of-the-art in fully-automated misinformation detection
has limited efficacy in detecting human-generated misinformation. We offer
recommendations for evaluating applications of machine learning to trust and
safety problems and recommend future directions for research.
</summary>
    <author>
      <name>Madelyne Xiao</name>
    </author>
    <author>
      <name>Jonathan Mayer</name>
    </author>
    <link href="http://arxiv.org/abs/2308.12215v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.12215v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.16579v1</id>
    <updated>2023-10-25T12:06:55Z</updated>
    <published>2023-10-25T12:06:55Z</published>
    <title>WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming
  Sentences with Contextualized Social Wisdom</title>
    <summary>  In recent years, we witness the explosion of false and unconfirmed
information (i.e., rumors) that went viral on social media and shocked the
public. Rumors can trigger versatile, mostly controversial stance expressions
among social media users. Rumor verification and stance detection are different
yet relevant tasks. Fake news debunking primarily focuses on determining the
truthfulness of news articles, which oversimplifies the issue as fake news
often combines elements of both truth and falsehood. Thus, it becomes crucial
to identify specific instances of misinformation within the articles. In this
research, we investigate a novel task in the field of fake news debunking,
which involves detecting sentence-level misinformation. One of the major
challenges in this task is the absence of a training dataset with
sentence-level annotations regarding veracity. Inspired by the Multiple
Instance Learning (MIL) approach, we propose a model called Weakly Supervised
Detection of Misinforming Sentences (WSDMS). This model only requires bag-level
labels for training but is capable of inferring both sentence-level
misinformation and article-level veracity, aided by relevant social media
conversations that are attentively contextualized with news sentences. We
evaluate WSDMS on three real-world benchmarks and demonstrate that it
outperforms existing state-of-the-art baselines in debunking fake news at both
the sentence and article levels.
</summary>
    <author>
      <name>Ruichao Yang</name>
    </author>
    <author>
      <name>Wei Gao</name>
    </author>
    <author>
      <name>Jing Ma</name>
    </author>
    <author>
      <name>Hongzhan Lin</name>
    </author>
    <author>
      <name>Zhiwei Yang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 2023 Conference on Empirical Methods in Natural Language
  Processing</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2310.16579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.16579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.14315v1</id>
    <updated>2023-11-24T07:06:16Z</updated>
    <published>2023-11-24T07:06:16Z</published>
    <title>Robust Domain Misinformation Detection via Multi-modal Feature Alignment</title>
    <summary>  Social media misinformation harms individuals and societies and is
potentialized by fast-growing multi-modal content (i.e., texts and images),
which accounts for higher "credibility" than text-only news pieces. Although
existing supervised misinformation detection methods have obtained acceptable
performances in key setups, they may require large amounts of labeled data from
various events, which can be time-consuming and tedious. In turn, directly
training a model by leveraging a publicly available dataset may fail to
generalize due to domain shifts between the training data (a.k.a. source
domains) and the data from target domains. Most prior work on domain shift
focuses on a single modality (e.g., text modality) and ignores the scenario
where sufficient unlabeled target domain data may not be readily available in
an early stage. The lack of data often happens due to the dynamic propagation
trend (i.e., the number of posts related to fake news increases slowly before
catching the public attention). We propose a novel robust domain and
cross-modal approach (\textbf{RDCM}) for multi-modal misinformation detection.
It reduces the domain shift by aligning the joint distribution of textual and
visual modalities through an inter-domain alignment module and bridges the
semantic gap between both modalities through a cross-modality alignment module.
We also propose a framework that simultaneously considers application scenarios
of domain generalization (in which the target domain data is unavailable) and
domain adaptation (in which unlabeled target domain data is available).
Evaluation results on two public multi-modal misinformation detection datasets
(Pheme and Twitter Datasets) evince the superiority of the proposed model. The
formal implementation of this paper can be found in this link:
https://github.com/less-and-less-bugs/RDCM
</summary>
    <author>
      <name>Hui Liu</name>
    </author>
    <author>
      <name>Wenya Wang</name>
    </author>
    <author>
      <name>Hao Sun</name>
    </author>
    <author>
      <name>Anderson Rocha</name>
    </author>
    <author>
      <name>Haoliang Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIFS.2023.3326368</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIFS.2023.3326368" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by TIFS 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.14315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.14315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.11524v1</id>
    <updated>2024-01-21T15:29:50Z</updated>
    <published>2024-01-21T15:29:50Z</published>
    <title>Controlling the Misinformation Diffusion in Social Media by the Effect
  of Different Classes of Agents</title>
    <summary>  The rapid and widespread dissemination of misinformation through social
networks is a growing concern in today's digital age. This study focused on
modeling fake news diffusion, discovering the spreading dynamics, and designing
control strategies. A common approach for modeling the misinformation dynamics
is SIR-based models. Our approach is an extension of a model called 'SBFC'
which is a SIR-based model. This model has three states, Susceptible, Believer,
and Fact-Checker. The dynamics and transition between states are based on
neighbors' beliefs, hoax credibility, spreading rate, probability of verifying
the news, and probability of forgetting the current state. Our contribution is
to push this model to real social networks by considering different classes of
agents with their characteristics. We proposed two main strategies for
confronting misinformation diffusion. First, we can educate a minor class, like
scholars or influencers, to improve their ability to verify the news or
remember their state longer. The second strategy is adding fact-checker bots to
the network to spread the facts and influence their neighbors' states. Our
result shows that both of these approaches can effectively control the
misinformation spread.
</summary>
    <author>
      <name>Ali Khodabandeh Yalabadi</name>
    </author>
    <author>
      <name>Mehdi Yazdani-Jahromi</name>
    </author>
    <author>
      <name>Sina Abdidizaji</name>
    </author>
    <author>
      <name>Ivan Garibay</name>
    </author>
    <author>
      <name>Ozlem Ozmen Garibay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at The Computational Social Science Society of the Americas
  (CSS) - 2023, Annual Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.11524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.11524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.03170v1</id>
    <updated>2024-03-05T18:04:59Z</updated>
    <published>2024-03-05T18:04:59Z</published>
    <title>SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context
  Misinformation Detection</title>
    <summary>  Misinformation is a prevalent societal issue due to its potential high risks.
Out-of-context (OOC) misinformation, where authentic images are repurposed with
false text, is one of the easiest and most effective ways to mislead audiences.
Current methods focus on assessing image-text consistency but lack convincing
explanations for their judgments, which is essential for debunking
misinformation. While Multimodal Large Language Models (MLLMs) have rich
knowledge and innate capability for visual reasoning and explanation
generation, they still lack sophistication in understanding and discovering the
subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel
multimodal large language model specifically engineered for OOC misinformation
detection and explanation. SNIFFER employs two-stage instruction tuning on
InstructBLIP. The first stage refines the model's concept alignment of generic
objects with news-domain entities and the second stage leverages language-only
GPT-4 generated OOC-specific instruction data to fine-tune the model's
discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not
only detects inconsistencies between text and image but also utilizes external
knowledge for contextual verification. Our experiments show that SNIFFER
surpasses the original MLLM by over 40% and outperforms state-of-the-art
methods in detection accuracy. SNIFFER also provides accurate and persuasive
explanations as validated by quantitative and human evaluations.
</summary>
    <author>
      <name>Peng Qi</name>
    </author>
    <author>
      <name>Zehong Yan</name>
    </author>
    <author>
      <name>Wynne Hsu</name>
    </author>
    <author>
      <name>Mong Li Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in CVPR 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.03170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.03170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.08391v2</id>
    <updated>2024-05-26T06:15:33Z</updated>
    <published>2024-03-13T10:10:07Z</published>
    <title>Misinformation is not about Bad Facts: An Analysis of the Production and
  Consumption of Fringe Content</title>
    <summary>  What if misinformation is not an information problem at all? To understand
the role of news publishers in potentially unintentionally propagating
misinformation, we examine how far-right and fringe online groups share and
leverage established legacy news media articles to advance their narratives.
Our findings suggest that online fringe ideologies spread through the use of
content that is consensus-based and "factually correct". We found that
Australian news publishers with both moderate and far-right political leanings
contain comparable levels of information completeness and quality; and
furthermore, that far-right Twitter users often share from moderate sources.
However, a stark difference emerges when we consider two additional factors: 1)
the narrow topic selection of articles by far-right users, suggesting that they
cherry pick only news articles that engage with their preexisting worldviews
and specific topics of concern, and 2) the difference between moderate and
far-right publishers when we examine the writing style of their articles.
Furthermore, we can identify users prone to sharing misinformation based on
their communication style. These findings have important implications for
countering online misinformation, as they highlight the powerful role that
personal biases towards specific topics and publishers' writing styles have in
amplifying fringe ideologies online.
</summary>
    <author>
      <name>JooYoung Lee</name>
    </author>
    <author>
      <name>Emily Booth</name>
    </author>
    <author>
      <name>Hany Farid</name>
    </author>
    <author>
      <name>Marian-Andrei Rizoiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.08391v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.08391v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.10700v1</id>
    <updated>2024-05-17T11:14:55Z</updated>
    <published>2024-05-17T11:14:55Z</published>
    <title>SynDy: Synthetic Dynamic Dataset Generation Framework for Misinformation
  Tasks</title>
    <summary>  Diaspora communities are disproportionately impacted by off-the-radar
misinformation and often neglected by mainstream fact-checking efforts,
creating a critical need to scale-up efforts of nascent fact-checking
initiatives. In this paper we present SynDy, a framework for Synthetic Dynamic
Dataset Generation to leverage the capabilities of the largest frontier Large
Language Models (LLMs) to train local, specialized language models. To the best
of our knowledge, SynDy is the first paper utilizing LLMs to create
fine-grained synthetic labels for tasks of direct relevance to misinformation
mitigation, namely Claim Matching, Topical Clustering, and Claim Relationship
Classification. SynDy utilizes LLMs and social media queries to automatically
generate distantly-supervised, topically-focused datasets with synthetic labels
on these three tasks, providing essential tools to scale up human-led
fact-checking at a fraction of the cost of human-annotated data. Training on
SynDy's generated labels shows improvement over a standard baseline and is not
significantly worse compared to training on human labels (which may be
infeasible to acquire). SynDy is being integrated into Meedan's chatbot
tiplines that are used by over 50 organizations, serve over 230K users
annually, and automatically distribute human-written fact-checks via messaging
apps such as WhatsApp. SynDy will also be integrated into our deployed
Co-Insights toolkit, enabling low-resource organizations to launch tiplines for
their communities. Finally, we envision SynDy enabling additional fact-checking
tools such as matching new misinformation claims to high-quality explainers on
common misinformation topics.
</summary>
    <author>
      <name>Michael Shliselberg</name>
    </author>
    <author>
      <name>Ashkan Kazemi</name>
    </author>
    <author>
      <name>Scott A. Hale</name>
    </author>
    <author>
      <name>Shiri Dori-Hacohen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3626772.3657667</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3626772.3657667" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 47th International ACM SIGIR Conference on
  Research and Development in Information Retrieval (SIGIR '24), July 14--18,
  2024, Washington, DC, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2405.10700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.10700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.07159v2</id>
    <updated>2024-07-15T11:18:44Z</updated>
    <published>2024-07-09T18:00:12Z</published>
    <title>Finding Fake News Websites in the Wild</title>
    <summary>  The battle against the spread of misinformation on the Internet is a daunting
task faced by modern society. Fake news content is primarily distributed
through digital platforms, with websites dedicated to producing and
disseminating such content playing a pivotal role in this complex ecosystem.
Therefore, these websites are of great interest to misinformation researchers.
However, obtaining a comprehensive list of websites labeled as producers and/or
spreaders of misinformation can be challenging, particularly in developing
countries. In this study, we propose a novel methodology for identifying
websites responsible for creating and disseminating misinformation content,
which are closely linked to users who share confirmed instances of fake news on
social media. We validate our approach on Twitter by examining various
execution modes and contexts. Our findings demonstrate the effectiveness of the
proposed methodology in identifying misinformation websites, which can aid in
gaining a better understanding of this phenomenon and enabling competent
entities to tackle the problem in various areas of society.
</summary>
    <author>
      <name>Leandro Araujo</name>
    </author>
    <author>
      <name>Joao M. M. Couto</name>
    </author>
    <author>
      <name>Luiz Felipe Nery</name>
    </author>
    <author>
      <name>Isadora C. Rodrigues</name>
    </author>
    <author>
      <name>Jussara M. Almeida</name>
    </author>
    <author>
      <name>Julio C. S. Reis</name>
    </author>
    <author>
      <name>Fabricio Benevenuto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprint version of a submitted manuscript on the Brazilian
  Symposium on Multimedia and the Web (WebMedia)</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.07159v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.07159v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.04463v1</id>
    <updated>2024-08-08T13:45:23Z</updated>
    <published>2024-08-08T13:45:23Z</published>
    <title>Crowd Intelligence for Early Misinformation Prediction on Social Media</title>
    <summary>  Misinformation spreads rapidly on social media, causing serious damage by
influencing public opinion, promoting dangerous behavior, or eroding trust in
reliable sources. It spreads too fast for traditional fact-checking, stressing
the need for predictive methods. We introduce CROWDSHIELD, a crowd
intelligence-based method for early misinformation prediction. We hypothesize
that the crowd's reactions to misinformation reveal its accuracy. Furthermore,
we hinge upon exaggerated assertions/claims and replies with particular
positions/stances on the source post within a conversation thread. We employ
Q-learning to capture the two dimensions -- stances and claims. We utilize deep
Q-learning due to its proficiency in navigating complex decision spaces and
effectively learning network properties. Additionally, we use a
transformer-based encoder to develop a comprehensive understanding of both
content and context. This multifaceted approach helps ensure the model pays
attention to user interaction and stays anchored in the communication's
content. We propose MIST, a manually annotated misinformation detection Twitter
corpus comprising nearly 200 conversation threads with more than 14K replies.
In experiments, CROWDSHIELD outperformed ten baseline systems, achieving an
improvement of ~4% macro-F1 score. We conduct an ablation study and error
analysis to validate our proposed model's performance. The source code and
dataset are available at https://github.com/LCS2-IIITD/CrowdShield.git.
</summary>
    <author>
      <name>Megha Sundriyal</name>
    </author>
    <author>
      <name>Harshit Choudhary</name>
    </author>
    <author>
      <name>Tanmoy Chakraborty</name>
    </author>
    <author>
      <name>Md Shad Akhtar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.04463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.04463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.10168v2</id>
    <updated>2025-04-14T21:13:38Z</updated>
    <published>2024-09-16T10:56:43Z</published>
    <title>Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube
  Search for COVID-19 Misinformation Between the United States and South Africa</title>
    <summary>  Despite being an integral tool for finding health-related information online,
YouTube has faced criticism for disseminating COVID-19 misinformation globally
to its users. Yet, prior audit studies have predominantly investigated YouTube
within the Global North contexts, often overlooking the Global South. To
address this gap, we conducted a comprehensive 10-day geolocation-based audit
on YouTube to compare the prevalence of COVID-19 misinformation in search
results between the United States (US) and South Africa (SA), the countries
heavily affected by the pandemic in the Global North and the Global South,
respectively. For each country, we selected 3 geolocations and placed
sock-puppets, or bots emulating "real" users, that collected search results for
48 search queries sorted by 4 search filters for 10 days, yielding a dataset of
915K results. We found that 31.55% of the top-10 search results contained
COVID-19 misinformation. Among the top-10 search results, bots in SA faced
significantly more misinformative search results than their US counterparts.
Overall, our study highlights the contrasting algorithmic behaviors of YouTube
search between two countries, underscoring the need for the platform to
regulate algorithmic behavior consistently across different regions of the
Globe.
</summary>
    <author>
      <name>Hayoung Jung</name>
    </author>
    <author>
      <name>Prerna Juneja</name>
    </author>
    <author>
      <name>Tanushree Mitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages. Accepted at ICWSM 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.10168v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.10168v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.11369v2</id>
    <updated>2025-07-09T09:29:19Z</updated>
    <published>2024-10-15T07:48:43Z</published>
    <title>Before &amp; After: The Effect of EU's 2022 Code of Practice on
  Disinformation</title>
    <summary>  Over the past few years, the European Commission has made significant steps
to reduce disinformation in cyberspace. One of those steps has been the
introduction of the 2022 "Strengthened Code of Practice on Disinformation".
Signed by leading online platforms, this Strengthened Code of Practice on
Disinformation is an attempt to combat disinformation on the Web. The Code of
Practice includes a variety of measures including the demonetization of
disinformation, urging, for example, advertisers "to avoid the placement of
advertising next to Disinformation content".
  In this work, we set out to explore what was the impact of the Code of
Practice and especially to explore to what extent ad networks continue to
advertise on dis-/mis-information sites. We perform a historical analysis and
find that, although at a hasty glance things may seem to be improving, there is
really no significant reduction in the amount of advertising relationships
among popular misinformation websites and major ad networks. In fact, we show
that ad networks have withdrawn mostly from unpopular misinformation websites
with very few visitors, but still form relationships with highly unreliable
websites that account for the majority of misinformation traffic. To make
matters worse, we show that ad networks continue to place advertisements of
legitimate companies next to misinformation content. We show that major ad
networks place ads in almost 400 misinformation websites in our dataset.
</summary>
    <author>
      <name>Emmanouil Papadogiannakis</name>
    </author>
    <author>
      <name>Panagiotis Papadopoulos</name>
    </author>
    <author>
      <name>Nicolas Kourtellis</name>
    </author>
    <author>
      <name>Evangelos P. Markatos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3696410.3714898</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3696410.3714898" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WWW '25: Proceedings of the ACM on Web Conference 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.11369v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.11369v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.14515v2</id>
    <updated>2025-02-03T18:10:18Z</updated>
    <published>2024-10-18T14:54:40Z</published>
    <title>Efficient Annotator Reliability Assessment and Sample Weighting for
  Knowledge-Based Misinformation Detection on Social Media</title>
    <summary>  Misinformation spreads rapidly on social media, confusing the truth and
targeting potentially vulnerable people. To effectively mitigate the negative
impact of misinformation, it must first be accurately detected before applying
a mitigation strategy, such as X's community notes, which is currently a manual
process. This study takes a knowledge-based approach to misinformation
detection, modelling the problem similarly to one of natural language
inference. The EffiARA annotation framework is introduced, aiming to utilise
inter- and intra-annotator agreement to understand the reliability of each
annotator and influence the training of large language models for
classification based on annotator reliability. In assessing the EffiARA
annotation framework, the Russo-Ukrainian Conflict Knowledge-Based
Misinformation Classification Dataset (RUC-MCD) was developed and made publicly
available. This study finds that sample weighting using annotator reliability
performs the best, utilising both inter- and intra-annotator agreement and
soft-label training. The highest classification performance achieved using
Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.
</summary>
    <author>
      <name>Owen Cook</name>
    </author>
    <author>
      <name>Charlie Grimshaw</name>
    </author>
    <author>
      <name>Ben Wu</name>
    </author>
    <author>
      <name>Sophie Dillon</name>
    </author>
    <author>
      <name>Jack Hicks</name>
    </author>
    <author>
      <name>Luke Jones</name>
    </author>
    <author>
      <name>Thomas Smith</name>
    </author>
    <author>
      <name>Matyas Szert</name>
    </author>
    <author>
      <name>Xingyi Song</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18653/v1/2025.findings-naacl.185</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18653/v1/2025.findings-naacl.185" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 3 tables. Code available here:
  https://github.com/MiniEggz/ruc-misinfo; annotation framework available here:
  https://github.com/MiniEggz/EffiARA</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.14515v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.14515v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.16592v1</id>
    <updated>2024-10-22T00:30:08Z</updated>
    <published>2024-10-22T00:30:08Z</published>
    <title>ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding</title>
    <summary>  The rise of social media and short-form video (SFV) has facilitated a
breeding ground for misinformation. With the emergence of large language
models, significant research has gone into curbing this misinformation problem
with automatic false claim detection for text. Unfortunately, the automatic
detection of misinformation in SFV is a more complex problem that remains
largely unstudied. While text samples are monomodal (only containing words),
SFVs comprise three different modalities: words, visuals, and non-linguistic
audio. In this work, we introduce Video Masked Autoencoders for Misinformation
Guarding (ViMGuard), the first deep-learning architecture capable of
fact-checking an SFV through analysis of all three of its constituent
modalities. ViMGuard leverages a dual-component system. First, Video and Audio
Masked Autoencoders analyze the visual and non-linguistic audio elements of a
video to discern its intention; specifically whether it intends to make an
informative claim. If it is deemed that the SFV has informative intent, it is
passed through our second component: a Retrieval Augmented Generation system
that validates the factual accuracy of spoken words. In evaluation, ViMGuard
outperformed three cutting-edge fact-checkers, thus setting a new standard for
SFV fact-checking and marking a significant stride toward trustworthy news on
social platforms. To promote further testing and iteration, VimGuard was
deployed into a Chrome extension and all code was open-sourced on GitHub.
</summary>
    <author>
      <name>Andrew Kan</name>
    </author>
    <author>
      <name>Christopher Kan</name>
    </author>
    <author>
      <name>Zaid Nabulsi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.16592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.16592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.19202v2</id>
    <updated>2025-06-06T20:55:07Z</updated>
    <published>2024-10-24T23:24:35Z</published>
    <title>Towards Generalizable AI-Assisted Misinformation Inoculation: Protecting
  Confidence Against False Election Narratives</title>
    <summary>  We present a generalizable AI-assisted framework for rapidly generating
effective "prebunking" interventions against misinformation. Like mRNA vaccine
platforms, our approach uses a stable template structure that can be quickly
adapted to counter emerging false narratives. In a preregistered two-wave
experiment with 4,293 U.S. registered voters, we test this framework against
politically-charged election misinformation -- one of the most challenging
domains for misinformation intervention. Our design directly tests scalability
by comparing human-reviewed and purely AI-generated inoculation messages. We
find that LLM-generated prebunking significantly reduced belief in election
rumors (persisting for at least one week) and increased confidence in election
integrity across partisan lines. Purely AI-generated messages proved as
effective as human-reviewed versions, with some achieving larger protective
effects, demonstrating that effective misinformation inoculation can be
achieved at machine speed without proportional human effort, offering a
scalable defense against the accelerating threat of false narratives across all
domains.
</summary>
    <author>
      <name>Mitchell Linegar</name>
    </author>
    <author>
      <name>Betsy Sinclair</name>
    </author>
    <author>
      <name>Sander van der Linden</name>
    </author>
    <author>
      <name>R. Michael Alvarez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2. Updates from previous: main independent variables are now
  post-treatment confidence scores, and we include pre-treatment confidence
  scores as a control, rather than work with difference scores. This improves
  power, but conclusions should otherwise be identical. Clarifications of
  experimental design, scalability claims, more precise phrasing of results</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.19202v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.19202v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4.1; J.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.13280v1</id>
    <updated>2024-12-17T19:15:08Z</updated>
    <published>2024-12-17T19:15:08Z</published>
    <title>Political Fact-Checking Efforts are Constrained by Deficiencies in
  Coverage, Speed, and Reach</title>
    <summary>  Fact-checking has been promoted as a key method for combating political
misinformation. Comparing the spread of election-related misinformation
narratives along with their relevant political fact-checks, this study provides
the most comprehensive assessment to date of the real-world limitations faced
by political fact-checking efforts. To examine barriers to impact, this study
extends recent work from laboratory and experimental settings to the wider
online information ecosystem present during the 2022 U.S. midterm elections.
From analyses conducted within this context, we find that fact-checks as
currently developed and distributed are severely inhibited in election contexts
by constraints on their i. coverage, ii. speed, and, iii. reach. Specifically,
we provide evidence that fewer than half of all prominent election-related
misinformation narratives were fact-checked. Within the subset of fact-checked
claims, we find that the median fact-check was released a full four days after
the initial appearance of a narrative. Using network analysis to estimate user
partisanship and dynamics of information spread, we additionally find evidence
that fact-checks make up less than 1.2\% of narrative conversations and that
even when shared, fact-checks are nearly always shared within,rather than
between, partisan communities. Furthermore, we provide empirical evidence which
runs contrary to the assumption that misinformation moderation is politically
biased against the political right. In full, through this assessment of the
real-world influence of political fact-checking efforts, our findings
underscore how limitations in coverage, speed, and reach necessitate further
examination of the potential use of fact-checks as the primary method for
combating the spread of political misinformation.
</summary>
    <author>
      <name>Morgan Wack</name>
    </author>
    <author>
      <name>Kayla Duskin</name>
    </author>
    <author>
      <name>Damian Hodel</name>
    </author>
    <link href="http://arxiv.org/abs/2412.13280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.13280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14765v1</id>
    <updated>2025-03-18T22:28:39Z</updated>
    <published>2025-03-18T22:28:39Z</published>
    <title>Dynamics of COVID-19 Misinformation: An Analysis of Conspiracy Theories,
  Fake Remedies, and False Reports</title>
    <summary>  This paper makes four scientific contributions to the area of misinformation
detection and analysis on digital platforms, with a specific focus on
investigating how conspiracy theories, fake remedies, and false reports emerge,
propagate, and shape public perceptions in the context of COVID-19. A dataset
of 5,614 posts on the internet that contained misinformation about COVID-19 was
used for this study. These posts were published in 2020 on 427 online sources
(such as social media platforms, news channels, and online blogs) from 193
countries and in 49 languages. First, this paper presents a structured,
three-tier analytical framework that investigates how multiple motives -
including fear, politics, and profit - can lead to a misleading claim. Second,
it emphasizes the importance of narrative structures, systematically
identifying and quantifying the thematic elements that drive conspiracy
theories, fake remedies, and false reports. Third, it presents a comprehensive
analysis of different sources of misinformation, highlighting the varied roles
played by individuals, state-based organizations, media outlets, and other
sources. Finally, it discusses multiple potential implications of these
findings for public policy and health communication, illustrating how insights
gained from motive, narrative, and source analyses can guide more targeted
interventions in the context of misinformation detection on digital platforms.
</summary>
    <author>
      <name>Nirmalya Thakur</name>
    </author>
    <author>
      <name>Mingchen Shao</name>
    </author>
    <author>
      <name>Victoria Knieling</name>
    </author>
    <author>
      <name>Vanessa Su</name>
    </author>
    <author>
      <name>Andrew Bian</name>
    </author>
    <author>
      <name>Hongseok Jeong</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.8; I.5.4; K.4.2; H.2.8; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06269v1</id>
    <updated>2025-03-01T06:32:27Z</updated>
    <published>2025-03-01T06:32:27Z</published>
    <title>EXCLAIM: An Explainable Cross-Modal Agentic System for Misinformation
  Detection with Hierarchical Retrieval</title>
    <summary>  Misinformation continues to pose a significant challenge in today's
information ecosystem, profoundly shaping public perception and behavior. Among
its various manifestations, Out-of-Context (OOC) misinformation is particularly
obscure, as it distorts meaning by pairing authentic images with misleading
textual narratives. Existing methods for detecting OOC misinformation
predominantly rely on coarse-grained similarity metrics between image-text
pairs, which often fail to capture subtle inconsistencies or provide meaningful
explainability. While multi-modal large language models (MLLMs) demonstrate
remarkable capabilities in visual reasoning and explanation generation, they
have not yet demonstrated the capacity to address complex, fine-grained, and
cross-modal distinctions necessary for robust OOC detection. To overcome these
limitations, we introduce EXCLAIM, a retrieval-based framework designed to
leverage external knowledge through multi-granularity index of multi-modal
events and entities. Our approach integrates multi-granularity contextual
analysis with a multi-agent reasoning architecture to systematically evaluate
the consistency and integrity of multi-modal news content. Comprehensive
experiments validate the effectiveness and resilience of EXCLAIM, demonstrating
its ability to detect OOC misinformation with 4.3% higher accuracy compared to
state-of-the-art approaches, while offering explainable and actionable
insights.
</summary>
    <author>
      <name>Yin Wu</name>
    </author>
    <author>
      <name>Zhengxuan Zhang</name>
    </author>
    <author>
      <name>Fuling Wang</name>
    </author>
    <author>
      <name>Yuyu Luo</name>
    </author>
    <author>
      <name>Hui Xiong</name>
    </author>
    <author>
      <name>Nan Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13859v1</id>
    <updated>2025-03-22T19:11:57Z</updated>
    <published>2025-03-22T19:11:57Z</published>
    <title>DoYouTrustAI: A Tool to Teach Students About AI Misinformation and
  Prompt Engineering</title>
    <summary>  AI, especially Large Language Models (LLMs) like ChatGPT, have rapidly
developed and gained widespread adoption in the past five years, shifting user
preference from traditional search engines. However, the generative nature of
LLMs raises concerns about presenting misinformation as fact. To address this,
we developed a web-based application that helps K-12 students enhance critical
thinking by identifying misleading information in LLM responses about major
historical figures. In this paper, we describe the implementation and design
details of the DoYouTrustAI tool, which can be used to provide an interactive
lesson which teaches students about the dangers of misinformation and how
believable generative AI can make it seem. The DoYouTrustAI tool utilizes
prompt engineering to present the user with AI generated summaries about the
life of a historical figure. These summaries can be either accurate accounts of
that persons life, or an intentionally misleading alteration of their history.
The user is tasked with determining the validity of the statement without
external resources. Our research questions for this work were:(RQ1) How can we
design a tool that teaches students about the dangers of misleading information
and of how misinformation can present itself in LLM responses? (RQ2) Can we
present prompt engineering as a topic that is easily understandable for
students? Our findings highlight the need to correct misleading information
before users retain it. Our tool lets users select familiar individuals for
testing to reduce random guessing and presents misinformation alongside known
facts to maintain believability. It also provides pre-configured prompt
instructions to show how different prompts affect AI responses. Together, these
features create a controlled environment where users learn the importance of
verifying AI responses and understanding prompt engineering.
</summary>
    <author>
      <name>Phillip Driscoll</name>
    </author>
    <author>
      <name>Priyanka Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/2504.13859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.21604v1</id>
    <updated>2025-04-30T13:03:17Z</updated>
    <published>2025-04-30T13:03:17Z</published>
    <title>Robust Misinformation Detection by Visiting Potential Commonsense
  Conflict</title>
    <summary>  The development of Internet technology has led to an increased prevalence of
misinformation, causing severe negative effects across diverse domains. To
mitigate this challenge, Misinformation Detection (MD), aiming to detect online
misinformation automatically, emerges as a rapidly growing research topic in
the community. In this paper, we propose a novel plug-and-play augmentation
method for the MD task, namely Misinformation Detection with Potential
Commonsense Conflict (MD-PCC). We take inspiration from the prior studies
indicating that fake articles are more likely to involve commonsense conflict.
Accordingly, we construct commonsense expressions for articles, serving to
express potential commonsense conflicts inferred by the difference between
extracted commonsense triplet and golden ones inferred by the well-established
commonsense reasoning tool COMET. These expressions are then specified for each
article as augmentation. Any specific MD methods can be then trained on those
commonsense-augmented articles. Besides, we also collect a novel
commonsense-oriented dataset named CoMis, whose all fake articles are caused by
commonsense conflict. We integrate MD-PCC with various existing MD backbones
and compare them across both 4 public benchmark datasets and CoMis. Empirical
results demonstrate that MD-PCC can consistently outperform the existing MD
baselines.
</summary>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Ximing Li</name>
    </author>
    <author>
      <name>Changchun Li</name>
    </author>
    <author>
      <name>Bingrui Zhao</name>
    </author>
    <author>
      <name>Bo Fu</name>
    </author>
    <author>
      <name>Renchu Guan</name>
    </author>
    <author>
      <name>Shengsheng Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures. Accepted by IJCAI 2025. Code:
  https://github.com/wangbing1416/MD-PCC</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.21604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.21604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.19768v1</id>
    <updated>2025-05-26T09:50:55Z</updated>
    <published>2025-05-26T09:50:55Z</published>
    <title>T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with
  Monte Carlo Tree Search</title>
    <summary>  Real-world multimodal misinformation often arises from mixed forgery sources,
requiring dynamic reasoning and adaptive verification. However, existing
methods mainly rely on static pipelines and limited tool usage, limiting their
ability to handle such complexity and diversity. To address this challenge, we
propose T2Agent, a novel misinformation detection agent that incorporates an
extensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of
modular tools such as web search, forgery detection, and consistency analysis.
Each tool is described using standardized templates, enabling seamless
integration and future expansion. To avoid inefficiency from using all tools
simultaneously, a Bayesian optimization-based selector is proposed to identify
a task-relevant subset. This subset then serves as the action space for MCTS to
dynamically collect evidence and perform multi-source verification. To better
align MCTS with the multi-source nature of misinformation detection, T2Agent
extends traditional MCTS with multi-source verification, which decomposes the
task into coordinated subtasks targeting different forgery sources. A dual
reward mechanism containing a reasoning trajectory score and a confidence score
is further proposed to encourage a balance between exploration across mixed
forgery sources and exploitation for more reliable evidence. We conduct
ablation studies to confirm the effectiveness of the tree search mechanism and
tool usage. Extensive experiments further show that T2Agent consistently
outperforms existing baselines on challenging mixed-source multimodal
misinformation benchmarks, demonstrating its strong potential as a
training-free approach for enhancing detection accuracy. The code will be
released.
</summary>
    <author>
      <name>Xing Cui</name>
    </author>
    <author>
      <name>Yueying Zou</name>
    </author>
    <author>
      <name>Zekun Li</name>
    </author>
    <author>
      <name>Peipei Li</name>
    </author>
    <author>
      <name>Xinyuan Xu</name>
    </author>
    <author>
      <name>Xuannan Liu</name>
    </author>
    <author>
      <name>Huaibo Huang</name>
    </author>
    <author>
      <name>Ran He</name>
    </author>
    <link href="http://arxiv.org/abs/2505.19768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.19768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07219v2</id>
    <updated>2021-02-18T11:31:57Z</updated>
    <published>2019-03-18T01:15:48Z</published>
    <title>Automatically applying a credibility appraisal tool to track
  vaccination-related communications shared on social media</title>
    <summary>  Background:
  Tools used to appraise the credibility of health information are
time-consuming to apply and require context-specific expertise, limiting their
use for quickly identifying and mitigating the spread of misinformation as it
emerges. Our aim was to estimate the proportion of vaccination-related posts on
Twitter are likely to be misinformation, and how unevenly exposure to
misinformation was distributed among Twitter users.
  Methods:
  Sampling from 144,878 vaccination-related web pages shared on Twitter between
January 2017 and March 2018, we used a seven-point checklist adapted from two
validated tools to appraise the credibility of a small subset of 474. These
were used to train several classifiers (random forest, support vector machines,
and a recurrent neural network with transfer learning), using the text from a
web page to predict whether the information satisfies each of the seven
criteria.
  Results:
  Applying the best performing classifier to the 144,878 web pages, we found
that 14.4% of relevant posts to text-based communications were linked to
webpages of low credibility and made up 9.2% of all potential
vaccination-related exposures. However, the 100 most popular links to
misinformation were potentially seen by between 2 million and 80 million
Twitter users, and for a substantial sub-population of Twitter users engaging
with vaccination-related information, links to misinformation appear to
dominate the vaccination-related information to which they were exposed.
  Conclusions:
  We proposed a new method for automatically appraising the credibility of
webpages based on a combination of validated checklist tools. The results
suggest that an automatic credibility appraisal tool can be used to find
populations at higher risk of exposure to misinformation or applied proactively
to add friction to the sharing of low credibility vaccination information.
</summary>
    <author>
      <name>Zubair Shah</name>
    </author>
    <author>
      <name>Didi Surian</name>
    </author>
    <author>
      <name>Amalie Dyda</name>
    </author>
    <author>
      <name>Enrico Coiera</name>
    </author>
    <author>
      <name>Kenneth D. Mandl</name>
    </author>
    <author>
      <name>Adam G. Dunn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2196/14007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2196/14007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 5 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">https://www.jmir.org/2019/11/e14007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.07219v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07219v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.00765v2</id>
    <updated>2021-04-06T14:09:31Z</updated>
    <published>2020-06-01T07:45:35Z</published>
    <title>Conspiracy vs science: A large-scale analysis of online discussion
  cascades</title>
    <summary>  With the emergence and rapid proliferation of social media platforms and
social networking sites, recent years have witnessed a surge of misinformation
spreading in our daily life. Drawing on a large-scale dataset which covers more
than 1.4M posts and 18M comments, we investigate the propagation of two
distinct narratives--(i) conspiracy information, whose claims are generally
unsubstantiated and thus referred as misinformation to some extent, and (ii)
scientific information, whose origins are generally readily identifiable and
verifiable--in an online social media platform. We find that conspiracy
cascades tend to propagate in a multigenerational branching process while
science cascades are more likely to grow in a breadth-first manner.
Specifically, conspiracy information triggers larger cascades, involves more
users and generations, persists longer, is more viral and bursty than science
information. Content analysis reveals that conspiracy cascades contain more
negative words and emotional words which convey anger, fear, disgust, surprise
and trust. We also find that conspiracy cascades are more concerned with
political and controversial topics. After applying machine learning models, we
achieve an AUC score of nearly 90% in discriminating conspiracy from science
narratives.
  We find that conspiracy cascades are more likely to be controlled by a
broader set of users than science cascades, imposing new challenges on the
management of misinformation. Although political affinity is thought to affect
the consumption of misinformation, there is very little evidence that political
orientation of the information source plays a role during the propagation of
conspiracy information. Our study provides complementing evidence to current
misinformation research and has practical policy implications to stem the
propagation and mitigate the influence of misinformation online.
</summary>
    <author>
      <name>Yafei Zhang</name>
    </author>
    <author>
      <name>Lin Wang</name>
    </author>
    <author>
      <name>Jonathan J. H. Zhu</name>
    </author>
    <author>
      <name>Xiaofan Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11280-021-00862-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11280-021-00862-x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 9 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">World Wide Web 24, 585-606 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2006.00765v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00765v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.13673v4</id>
    <updated>2023-03-24T19:03:20Z</updated>
    <published>2022-05-26T23:27:59Z</published>
    <title>Diffusion of Community Fact-Checked Misinformation on Twitter</title>
    <summary>  The spread of misinformation on social media is a pressing societal problem
that platforms, policymakers, and researchers continue to grapple with. As a
countermeasure, recent works have proposed to employ non-expert fact-checkers
in the crowd to fact-check social media content. While experimental studies
suggest that crowds might be able to accurately assess the veracity of social
media content, an understanding of how crowd fact-checked (mis-)information
spreads is missing. In this work, we empirically analyze the spread of
misleading vs. not misleading community fact-checked posts on social media. For
this purpose, we employ a dataset of community-created fact-checks from
Twitter's Birdwatch pilot and map them to resharing cascades on Twitter.
Different from earlier studies analyzing the spread of misinformation listed on
third-party fact-checking websites (e.g., Snopes), we find that community
fact-checked misinformation is less viral. Specifically, misleading posts are
estimated to receive 36.62% fewer retweets than not misleading posts. A partial
explanation may lie in differences in the fact-checking targets: community
fact-checkers tend to fact-check posts from influential user accounts with many
followers, while expert fact-checks tend to target posts that are shared by
less influential users. We further find that there are significant differences
in virality across different sub-types of misinformation (e.g., factual errors,
missing context, manipulated media). Moreover, we conduct a user study to
assess the perceived reliability of (real-world) community-created fact-checks.
Here, we find that users, to a large extent, agree with community-created
fact-checks. Altogether, our findings offer insights into how misleading vs.
not misleading posts spread and highlight the crucial role of sample selection
when studying misinformation on social media.
</summary>
    <author>
      <name>Chiara Drolsbach</name>
    </author>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CSCW 23</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.13673v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.13673v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.13687v6</id>
    <updated>2023-09-25T11:24:17Z</updated>
    <published>2021-08-31T08:55:47Z</published>
    <title>The Game Theory of Fake News</title>
    <summary>  A great deal of empirical research has examined who falls for misinformation
and why. Here, we introduce a formal game-theoretic model of engagement with
news stories that captures the strategic interplay between (mis)information
consumers and producers. A key insight from the model is that observed patterns
of engagement do not necessarily reflect the preferences of consumers. This is
because producers seeking to promote misinformation can use strategies that
lead moderately inattentive readers to engage more with false stories than true
ones -- even when readers prefer more accurate over less accurate information.
We then empirically test people's preferences for accuracy in the news. In
three studies, we find that people strongly prefer to click and share news they
perceive as more accurate -- both in a general population sample, and in a
sample of users recruited through Twitter who had actually shared links to
misinformation sites online. Despite this preference for accurate news -- and
consistent with the predictions of our model -- we find markedly different
engagement patterns for articles from misinformation versus mainstream news
sites. Using 1,000 headlines from 20 misinformation and 20 mainstream news
sites, we compare Facebook engagement data with 20,000 accuracy ratings
collected in a survey experiment. Engagement with a headline is negatively
correlated with perceived accuracy for misinformation sites, but positively
correlated with perceived accuracy for mainstream sites. Taken together, these
theoretical and empirical results suggest that consumer preferences cannot be
straightforwardly inferred from empirical patterns of engagement.
</summary>
    <author>
      <name>Alexander J. Stewart</name>
    </author>
    <author>
      <name>Antonio A. Arechar</name>
    </author>
    <author>
      <name>David G. Rand</name>
    </author>
    <author>
      <name>Joshua B. Plotkin</name>
    </author>
    <link href="http://arxiv.org/abs/2108.13687v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.13687v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.12264v3</id>
    <updated>2024-06-06T11:17:31Z</updated>
    <published>2022-07-25T15:35:48Z</published>
    <title>Dynamics and triggers of misinformation on vaccines</title>
    <summary>  The Covid-19 pandemic has sparked renewed attention on the prevalence of
misinformation online, whether intentional or not, underscoring the potential
risks posed to individuals' quality of life associated with the dissemination
of misconceptions and enduring myths on health-related subjects. In this study,
we analyze 6 years (2016-2021) of Italian vaccine debate across diverse social
media platforms (Facebook, Instagram, Twitter, YouTube), encompassing all major
news sources - both questionable and reliable. We first use the symbolic
transfer entropy analysis of news production time-series to dynamically
determine which category of sources, questionable or reliable, causally drives
the agenda on vaccines. Then, leveraging deep learning models capable to
accurately classify vaccine-related content based on the conveyed stance and
discussed topic, respectively, we evaluate the focus on various topics by news
sources promoting opposing views and compare the resulting user engagement.
Aside from providing valuable resources for further investigation of
vaccine-related misinformation, particularly in a language (Italian) that
receives less attention in scientific research compared to languages like
English, our study uncovers misinformation not as a parasite of the news
ecosystem that merely opposes the perspectives offered by mainstream media, but
as an autonomous force capable of even overwhelming the production of
vaccine-related content from the latter. While the pervasiveness of
misinformation is evident in the significantly higher engagement of
questionable sources compared to reliable ones, our findings underscore the
importance of consistent and thorough pro-vax coverage. This is especially
crucial in addressing the most sensitive topics where the risk of
misinformation spreading and potentially exacerbating negative attitudes toward
vaccines among the users involved is higher.
</summary>
    <author>
      <name>Emanuele Brugnoli</name>
    </author>
    <author>
      <name>Marco Delmastro</name>
    </author>
    <link href="http://arxiv.org/abs/2207.12264v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12264v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.01458v4</id>
    <updated>2023-10-26T07:19:58Z</updated>
    <published>2023-07-04T03:34:19Z</published>
    <title>CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity
  and Infant Care</title>
    <summary>  The recent advances in natural language processing (NLP), have led to a new
trend of applying large language models (LLMs) to real-world scenarios. While
the latest LLMs are astonishingly fluent when interacting with humans, they
suffer from the misinformation problem by unintentionally generating factually
false statements. This can lead to harmful consequences, especially when
produced within sensitive contexts, such as healthcare. Yet few previous works
have focused on evaluating misinformation in the long-form (LF) generation of
LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have
been shown to perform well in different languages, misinformation evaluation
has been mostly conducted in English. To this end, we present a benchmark,
CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic,
specifically the maternity and infant care domain; and 2) a language other than
English, namely Chinese. Most importantly, we provide an innovative paradigm
for building LF generation evaluation benchmarks that can be transferred to
other knowledge-intensive domains and low-resourced languages. Our proposed
benchmark fills the gap between the extensive usage of LLMs and the lack of
datasets for assessing the misinformation generated by these models. It
contains 1,612 expert-checked questions, accompanied with human-selected
references. Using our benchmark, we conduct extensive experiments and found
that current Chinese LLMs are far from perfect in the topic of maternity and
infant care. In an effort to minimize the reliance on human resources for
performance evaluation, we offer off-the-shelf judgment models for
automatically assessing the LF output of LLMs given benchmark questions.
Moreover, we compare potential solutions for LF generation evaluation and
provide insights for building better automated metrics.
</summary>
    <author>
      <name>Tong Xiang</name>
    </author>
    <author>
      <name>Liangzhi Li</name>
    </author>
    <author>
      <name>Wangyue Li</name>
    </author>
    <author>
      <name>Mingbai Bai</name>
    </author>
    <author>
      <name>Lu Wei</name>
    </author>
    <author>
      <name>Bowen Wang</name>
    </author>
    <author>
      <name>Noa Garcia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2023 Datasets and Benchmarks Track</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.01458v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.01458v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.09486v2</id>
    <updated>2024-01-22T14:27:03Z</updated>
    <published>2023-11-16T01:14:18Z</published>
    <title>The impact of effective participation in stopping misinformation: an
  approach based on branching processes</title>
    <summary>  The emergence of research focused to understand the spreading and impact of
disinformation is increasing year over year. Most times, the purpose of those
who start the spreading of information intentionally false and designed to
cause harm is in catalyzing its fast transformation into misinformation, which
is the false content shared by people who do not realize it is false or
misleading. Our interest is in discussing the role of people who decide to
adopt an active role in stopping the propagation of an information when they
realize that it is false. For this, we formulate two simple probabilistic
models to compare misinformation spreading in the possible scenarios for which
there is a passive or an active environment of aware individuals. With aware
individuals we mean those individuals who realize that a given information is
false or misleading. In the passive environment we assume that if one of an
aware individual is exposed to the misinformation then he/she will not spread
it. In the active environment we assume that if one of an aware individual is
exposed to the misinformation then he/she will not spread it but also he/she
will stop the propagation to other individuals from the individual who
contacted him/her. We appeal to the theory of branching processes to analyse
propagation in both scenarios and we discuss the role and the impact of
effective participation in stopping misinformation. We show that the
propagation reduces drastically provided we assume an active environment, and
we obtain theoretical and computational results to measure such a reduction,
which in turns depends on the proportion of aware individuals and the number of
potential contacts of each individual which is assumed to be random.
</summary>
    <author>
      <name>Luz Marina Gomez</name>
    </author>
    <author>
      <name>Valdivino V. Junior</name>
    </author>
    <author>
      <name>Pablo M. Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 11 figures, revised version accepted for publication at
  Journal of Statistical Mechanics: Theory and Experiment</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.09486v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.09486v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J85, 60K37, 82B26" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.23812v1</id>
    <updated>2025-05-27T15:38:50Z</updated>
    <published>2025-05-27T15:38:50Z</published>
    <title>Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for
  Stance Detection in Misinformative Social Media Content</title>
    <summary>  The rapid evolution of social media has generated an overwhelming volume of
user-generated content, conveying implicit opinions and contributing to the
spread of misinformation. The method aims to enhance the detection of stance
where misinformation can polarize user opinions. Stance detection has emerged
as a crucial approach to effectively analyze underlying biases in shared
information and combating misinformation. This paper proposes a novel method
for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual
cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work
(SPLAENet) in misinformative social media user-generated content. The proposed
method employs a dual cross-attention mechanism and a hierarchical attention
network to capture inter and intra-relationships by focusing on the relevant
parts of source text in the context of reply text and vice versa. We
incorporate emotions to effectively distinguish between different stance
categories by leveraging the emotional alignment or divergence between the
texts. We also employ label fusion that uses distance-metric learning to align
extracted features with stance labels, improving the method's ability to
accurately distinguish between stances. Extensive experiments demonstrate the
significant improvements achieved by SPLAENet over existing state-of-the-art
methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and
17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it
achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the
P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and
11.18\% in F1-score. These results validate the effectiveness of the proposed
method for stance detection in the context of misinformative social media
content.
</summary>
    <author>
      <name>Lata Pangtey</name>
    </author>
    <author>
      <name>Mohammad Zia Ur Rehman</name>
    </author>
    <author>
      <name>Prasad Chaudhari</name>
    </author>
    <author>
      <name>Shubhi Bansal</name>
    </author>
    <author>
      <name>Nagendra Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.engappai.2025.111109</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.engappai.2025.111109" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">journal = {Engineering Applications of Artificial Intelligence},
  volume = {156}, pages = {111109}, year = {2025}</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2505.23812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.23812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.04061v1</id>
    <updated>2025-07-05T14:53:32Z</updated>
    <published>2025-07-05T14:53:32Z</published>
    <title>Consistent and Invariant Generalization Learning for Short-video
  Misinformation Detection</title>
    <summary>  Short-video misinformation detection has attracted wide attention in the
multi-modal domain, aiming to accurately identify the misinformation in the
video format accompanied by the corresponding audio. Despite significant
advancements, current models in this field, trained on particular domains
(source domains), often exhibit unsatisfactory performance on unseen domains
(target domains) due to domain gaps. To effectively realize such domain
generalization on the short-video misinformation detection task, we propose
deep insights into the characteristics of different domains: (1) The detection
on various domains may mainly rely on different modalities (i.e., mainly
focusing on videos or audios). To enhance domain generalization, it is crucial
to achieve optimal model performance on all modalities simultaneously. (2) For
some domains focusing on cross-modal joint fraud, a comprehensive analysis
relying on cross-modal fusion is necessary. However, domain biases located in
each modality (especially in each frame of videos) will be accumulated in this
fusion process, which may seriously damage the final identification of
misinformation. To address these issues, we propose a new DOmain generalization
model via ConsisTency and invariance learning for shORt-video misinformation
detection (named DOCTOR), which contains two characteristic modules: (1) We
involve the cross-modal feature interpolation to map multiple modalities into a
shared space and the interpolation distillation to synchronize multi-modal
learning; (2) We design the diffusion model to add noise to retain core
features of multi modal and enhance domain invariant features through
cross-modal guided denoising. Extensive experiments demonstrate the
effectiveness of our proposed DOCTOR model. Our code is public available at
https://github.com/ghh1125/DOCTOR.
</summary>
    <author>
      <name>Hanghui Guo</name>
    </author>
    <author>
      <name>Weijie Shi</name>
    </author>
    <author>
      <name>Mengze Li</name>
    </author>
    <author>
      <name>Juncheng Li</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Yue Cui</name>
    </author>
    <author>
      <name>Jiajie Xu</name>
    </author>
    <author>
      <name>Jia Zhu</name>
    </author>
    <author>
      <name>Jiawei Shen</name>
    </author>
    <author>
      <name>Zhangze Chen</name>
    </author>
    <author>
      <name>Sirui Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACM MM 2025,15 pages, 16figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.04061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.04061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.1002v1</id>
    <updated>2012-12-05T12:07:05Z</updated>
    <published>2012-12-05T12:07:05Z</published>
    <title>Stochastic Models of Misinformation Distribution in Online Social
  Networks</title>
    <summary>  This report contains results of an experimental study of the distribution of
misinformation in online social networks (OSNs). We consider the classification
of the topologies of OSNs and analyze the parameters identified in order to
relate the topology of a real network with one of the classes. We propose an
algorithm for conducting a search for the percolation cluster in the social
graph.
</summary>
    <author>
      <name>Konstantin Abramov</name>
    </author>
    <author>
      <name>Yuri Monakhov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.1002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.1002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02406v2</id>
    <updated>2017-06-27T01:56:01Z</updated>
    <published>2017-04-08T00:12:10Z</published>
    <title>Impact of misinformation in temporal network epidemiology</title>
    <summary>  We investigate the impact of misinformation about the contact structure on
the ability to predict disease outbreaks. We base our study on 31 empirical
temporal networks and tune the frequencies in errors in the node identities or
timestamps of contacts. We find that for both these spreading scenarios, the
maximal misprediction of both the outbreak size and time to extinction follows
an stretched exponential convergence as a function of the error frequency. We
furthermore determine the temporal-network structural factors influencing the
parameters of this convergence.
</summary>
    <author>
      <name>Petter Holme</name>
    </author>
    <author>
      <name>Luis E C Rocha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/nws.2018.28</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/nws.2018.28" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Net Sci 7 (2019) 52-69</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.02406v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02406v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.05901v1</id>
    <updated>2018-09-16T15:49:14Z</updated>
    <published>2018-09-16T15:49:14Z</published>
    <title>Trends in the Diffusion of Misinformation on Social Media</title>
    <summary>  We measure trends in the diffusion of misinformation on Facebook and Twitter
between January 2015 and July 2018. We focus on stories from 570 sites that
have been identified as producers of false stories. Interactions with these
sites on both Facebook and Twitter rose steadily through the end of 2016.
Interactions then fell sharply on Facebook while they continued to rise on
Twitter, with the ratio of Facebook engagements to Twitter shares falling by
approximately 60 percent. We see no similar pattern for other news, business,
or culture sites, where interactions have been relatively stable over time and
have followed similar trends on the two platforms both before and after the
election.
</summary>
    <author>
      <name>Hunt Allcott</name>
    </author>
    <author>
      <name>Matthew Gentzkow</name>
    </author>
    <author>
      <name>Chuan Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1809.05901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13352v1</id>
    <updated>2020-10-26T05:37:18Z</updated>
    <published>2020-10-26T05:37:18Z</published>
    <title>The Age-related Differences in Web Information Search Process</title>
    <summary>  Older adults' need for quality health information has never been more
critical as during the COVID-19 pandemic. Yet, they are susceptible to the
wide-spread misinformation disseminated through search engines and social
media. To build a search-related behavioral profile of older adults, this
article surveys the empirical research on age-related differences in query
formulation, search strategies, information evaluation, and susceptibility to
misinformation effects. It also decomposes the mechanisms (i.e., cognitive
changes, development goal shift) and moderators (i.e., search task and
interface design) of such differences. To inform the design of information
systems to improve older adults' information search experience, we discuss
opportunities for future research.
</summary>
    <author>
      <name>Zhaopeng Xing</name>
    </author>
    <author>
      <name>Xiaojun Yuan</name>
    </author>
    <author>
      <name>Lisa Vizer</name>
    </author>
    <link href="http://arxiv.org/abs/2010.13352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04311v1</id>
    <updated>2021-04-09T11:33:02Z</updated>
    <published>2021-04-09T11:33:02Z</published>
    <title>Helping People Deal With Disinformation -- A Socio-Technical Perspective</title>
    <summary>  At the latest since the advent of the Internet, disinformation and conspiracy
theories have become ubiquitous. Recent examples like QAnon and Pizzagate prove
that false information can lead to real violence. In this motivation statement
for the Workshop on Human Aspects of Misinformation at CHI 2021, I explain my
research agenda focused on 1. why people believe in disinformation, 2. how
people can be best supported in recognizing disinformation, and 3. what the
potentials and risks of different tools designed to fight disinformation are.
</summary>
    <author>
      <name>Hendrik Heuer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be presented at the Workshop on Human Aspects of
  Misinformation at CHI 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13534v1</id>
    <updated>2022-10-24T18:37:42Z</updated>
    <published>2022-10-24T18:37:42Z</published>
    <title>Classification of Misinformation in New Articles using Natural Language
  Processing and a Recurrent Neural Network</title>
    <summary>  This paper seeks to address the classification of misinformation in news
articles using a Long Short Term Memory Recurrent Neural Network. Articles were
taken from 2018; a year that was filled with reporters writing about President
Donald Trump, Special Counsel Robert Mueller, the Fifa World Cup, and Russia.
The model presented successfully classifies these articles with an accuracy
score of 0.779944. We consider this to be successful because the model was
trained on articles that included languages other than English as well as
incomplete, or fragmented, articles.
</summary>
    <author>
      <name>Brendan Cunha</name>
    </author>
    <author>
      <name>Lydia Manikonda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICWSM Data Conference 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.13534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.03704v1</id>
    <updated>2023-03-07T07:35:30Z</updated>
    <published>2023-03-07T07:35:30Z</published>
    <title>Identifying Misinformation Spreaders: A Graph-Based Semi-Supervised
  Learning Approach</title>
    <summary>  In this paper we proposed a Graph-Based conspiracy source detection method
for the MediaEval task 2022 FakeNews: Corona Virus and Conspiracies Multimedia
Analysis Task. The goal of this study was to apply SOTA graph neural network
methods to the problem of misinformation spreading in online social networks.
We explore three different Graph Neural Network models: GCN, GraphSAGE and
DGCNN. Experimental results demonstrate that DGCNN outperforms in terms of
accuracy.
</summary>
    <author>
      <name>Atta Ullah</name>
    </author>
    <author>
      <name>Rabeeh Ayaz Abbasi</name>
    </author>
    <author>
      <name>Akmal Saeed Khattak</name>
    </author>
    <author>
      <name>Anwar Said</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Multimedia Benchmark Workshop Proceedings 2022:
  https://2022.multimediaeval.com/</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.03704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.03704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91D30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.16305v1</id>
    <updated>2023-09-28T09:56:55Z</updated>
    <published>2023-09-28T09:56:55Z</published>
    <title>Does Explanation Matter? An Exploratory Study on the Effects of Covid 19
  Misinformation Warning Flags on Social Media</title>
    <summary>  We investigate whether adding specific explanations from fact checking
websites enhances trust in these flags. We experimented with 348 American
participants, exposing them to a randomised order of true and false news
headlines related to COVID 19, with and without warning flags and explanation
text. Our findings suggest that warning flags, whether alone or accompanied by
explanatory text, effectively reduce the perceived accuracy of fake news and
the intent to share such headlines. Interestingly, our study also suggests that
incorporating explanatory text in misinformation warning systems could
significantly enhance their trustworthiness, emphasising the importance of
transparency and user comprehension in combating fake news on social media.
</summary>
    <author>
      <name>Dipto Barman</name>
    </author>
    <author>
      <name>Owen Conlan</name>
    </author>
    <link href="http://arxiv.org/abs/2309.16305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.16305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.07545v2</id>
    <updated>2024-04-29T09:12:44Z</updated>
    <published>2023-10-11T14:49:54Z</published>
    <title>Large-Language-Model-Powered Agent-Based Framework for Misinformation
  and Disinformation Research: Opportunities and Open Challenges</title>
    <summary>  This article presents the affordances that Generative Artificial Intelligence
can have in misinformation and disinformation contexts, major threats to our
digitalized society. We present a research framework to generate customized
agent-based social networks for disinformation simulations that would enable
understanding and evaluating the phenomena whilst discussing open challenges.
</summary>
    <author>
      <name>Javier Pastor-Galindo</name>
    </author>
    <author>
      <name>Pantaleone Nespoli</name>
    </author>
    <author>
      <name>Jos√© A. Ruip√©rez-Valiente</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MSEC.2024.3380511</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MSEC.2024.3380511" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Security &amp; Privacy, 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2310.07545v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.07545v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.06443v1</id>
    <updated>2024-02-09T14:39:20Z</updated>
    <published>2024-02-09T14:39:20Z</published>
    <title>Explaining Veracity Predictions with Evidence Summarization: A
  Multi-Task Model Approach</title>
    <summary>  The rapid dissemination of misinformation through social media increased the
importance of automated fact-checking. Furthermore, studies on what deep neural
models pay attention to when making predictions have increased in recent years.
While significant progress has been made in this field, it has not yet reached
a level of reasoning comparable to human reasoning. To address these gaps, we
propose a multi-task explainable neural model for misinformation detection.
Specifically, this work formulates an explanation generation process of the
model's veracity prediction as a text summarization problem. Additionally, the
performance of the proposed model is discussed on publicly available datasets
and the findings are evaluated with related studies.
</summary>
    <author>
      <name>Recep Firat Cekinel</name>
    </author>
    <author>
      <name>Pinar Karagoz</name>
    </author>
    <link href="http://arxiv.org/abs/2402.06443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.06443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.17581v1</id>
    <updated>2024-03-29T17:11:13Z</updated>
    <published>2024-03-29T17:11:13Z</published>
    <title>Deepfake Labels Restore Reality, Especially for Those Who Dislike the
  Speaker</title>
    <summary>  Deepfake videos create dangerous possibilities for public misinformation. In
this experiment (N=204), we investigated whether labeling videos as containing
actual or deepfake statements from US President Biden helps participants later
differentiate between true and fake information. People accurately recalled
93.8% of deepfake videos and 84.2% of actual videos, suggesting that labeling
videos can help combat misinformation. Individuals who identify as Republican
and had lower favorability ratings of Biden performed better in distinguishing
between actual and deepfake videos, a result explained by the elaboration
likelihood model (ELM), which predicts that people who distrust a message
source will more critically evaluate the message.
</summary>
    <author>
      <name>Nathan L. Tenhundfeld</name>
    </author>
    <author>
      <name>Ryan Weber</name>
    </author>
    <author>
      <name>William I. MacKenzie</name>
    </author>
    <author>
      <name>Hannah M. Barr</name>
    </author>
    <author>
      <name>Candice Lanius</name>
    </author>
    <link href="http://arxiv.org/abs/2404.17581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.17581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.00843v1</id>
    <updated>2024-05-01T20:10:44Z</updated>
    <published>2024-05-01T20:10:44Z</published>
    <title>Can a Hallucinating Model help in Reducing Human "Hallucination"?</title>
    <summary>  The prevalence of unwarranted beliefs, spanning pseudoscience, logical
fallacies, and conspiracy theories, presents substantial societal hurdles and
the risk of disseminating misinformation. Utilizing established psychometric
assessments, this study explores the capabilities of large language models
(LLMs) vis-a-vis the average human in detecting prevalent logical pitfalls. We
undertake a philosophical inquiry, juxtaposing the rationality of humans
against that of LLMs. Furthermore, we propose methodologies for harnessing LLMs
to counter misconceptions, drawing upon psychological models of persuasion such
as cognitive dissonance theory and elaboration likelihood theory. Through this
endeavor, we highlight the potential of LLMs as personalized misinformation
debunking agents.
</summary>
    <author>
      <name>Sowmya S Sundaram</name>
    </author>
    <author>
      <name>Balaji Alwar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.00843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.00843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.16489v1</id>
    <updated>2024-06-24T09:55:31Z</updated>
    <published>2024-06-24T09:55:31Z</published>
    <title>Deepfake tweets automatic detection</title>
    <summary>  This study addresses the critical challenge of detecting DeepFake tweets by
leveraging advanced natural language processing (NLP) techniques to distinguish
between genuine and AI-generated texts. Given the increasing prevalence of
misinformation, our research utilizes the TweepFake dataset to train and
evaluate various machine learning models. The objective is to identify
effective strategies for recognizing DeepFake content, thereby enhancing the
integrity of digital communications. By developing reliable methods for
detecting AI-generated misinformation, this work contributes to a more
trustworthy online information environment.
</summary>
    <author>
      <name>Adam Frej</name>
    </author>
    <author>
      <name>Adrian Kaminski</name>
    </author>
    <author>
      <name>Piotr Marciniak</name>
    </author>
    <author>
      <name>Szymon Szmajdzinski</name>
    </author>
    <author>
      <name>Soveatin Kuntur</name>
    </author>
    <author>
      <name>Anna Wroblewska</name>
    </author>
    <link href="http://arxiv.org/abs/2406.16489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.16489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.01168v1</id>
    <updated>2024-08-02T10:35:49Z</updated>
    <published>2024-08-02T10:35:49Z</published>
    <title>Misinforming LLMs: vulnerabilities, challenges and opportunities</title>
    <summary>  Large Language Models (LLMs) have made significant advances in natural
language processing, but their underlying mechanisms are often misunderstood.
Despite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely
on statistical patterns in word embeddings rather than true cognitive
processes. This leads to vulnerabilities such as "hallucination" and
misinformation. The paper argues that current LLM architectures are inherently
untrustworthy due to their reliance on correlations of sequential patterns of
word embedding vectors. However, ongoing research into combining generative
transformer-based models with fact bases and logic programming languages may
lead to the development of trustworthy LLMs capable of generating statements
based on given truth and explaining their self-reasoning process.
</summary>
    <author>
      <name>Bo Zhou</name>
    </author>
    <author>
      <name>Daniel Gei√üler</name>
    </author>
    <author>
      <name>Paul Lukowicz</name>
    </author>
    <link href="http://arxiv.org/abs/2408.01168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.01168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.09689v1</id>
    <updated>2025-02-13T16:48:27Z</updated>
    <published>2025-02-13T16:48:27Z</published>
    <title>Large Language Models and Provenance Metadata for Determining the
  Relevance of Images and Videos in News Stories</title>
    <summary>  The most effective misinformation campaigns are multimodal, often combining
text with images and videos taken out of context -- or fabricating them
entirely -- to support a given narrative. Contemporary methods for detecting
misinformation, whether in deepfakes or text articles, often miss the interplay
between multiple modalities. Built around a large language model, the system
proposed in this paper addresses these challenges. It analyzes both the
article's text and the provenance metadata of included images and videos to
determine whether they are relevant. We open-source the system prototype and
interactive web interface.
</summary>
    <author>
      <name>Tomas Peterka</name>
    </author>
    <author>
      <name>Matyas Bohacek</name>
    </author>
    <link href="http://arxiv.org/abs/2502.09689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.09689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.05481v1</id>
    <updated>2025-03-07T14:51:29Z</updated>
    <published>2025-03-07T14:51:29Z</published>
    <title>Maximum Hallucination Standards for Domain-Specific Large Language
  Models</title>
    <summary>  Large language models (LLMs) often generate inaccurate yet credible-sounding
content, known as hallucinations. This inherent feature of LLMs poses
significant risks, especially in critical domains. I analyze LLMs as a new
class of engineering products, treating hallucinations as a product attribute.
I demonstrate that, in the presence of imperfect awareness of LLM
hallucinations and misinformation externalities, net welfare improves when the
maximum acceptable level of LLM hallucinations is designed to vary with two
domain-specific factors: the willingness to pay for reduced LLM hallucinations
and the marginal damage associated with misinformation.
</summary>
    <author>
      <name>Tingmingke Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.05481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.05481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5007v1</id>
    <updated>2009-06-26T20:57:55Z</updated>
    <published>2009-06-26T20:57:55Z</published>
    <title>Spread of Misinformation in Social Networks</title>
    <summary>  We provide a model to investigate the tension between information aggregation
and spread of misinformation in large societies (conceptualized as networks of
agents communicating with each other). Each individual holds a belief
represented by a scalar. Individuals meet pairwise and exchange information,
which is modeled as both individuals adopting the average of their pre-meeting
beliefs. When all individuals engage in this type of information exchange, the
society will be able to effectively aggregate the initial information held by
all individuals. There is also the possibility of misinformation, however,
because some of the individuals are "forceful," meaning that they influence the
beliefs of (some) of the other individuals they meet, but do not change their
own opinion. The paper characterizes how the presence of forceful agents
interferes with information aggregation. Under the assumption that even
forceful agents obtain some information (however infrequent) from some others
(and additional weak regularity conditions), we first show that beliefs in this
class of societies converge to a consensus among all individuals. This
consensus value is a random variable, however, and we characterize its
behavior. Our main results quantify the extent of misinformation in the society
by either providing bounds or exact results (in some special cases) on how far
the consensus value can be from the benchmark without forceful agents (where
there is efficient information aggregation). The worst outcomes obtain when
there are several forceful agents and forceful agents themselves update their
beliefs only on the basis of information they obtain from individuals most
likely to have received their own information previously.
</summary>
    <author>
      <name>Daron Acemoglu</name>
    </author>
    <author>
      <name>Asuman Ozdaglar</name>
    </author>
    <author>
      <name>Ali ParandehGheibi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Games and Economic Behavior</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.5007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06988v1</id>
    <updated>2017-03-20T22:19:22Z</updated>
    <published>2017-03-20T22:19:22Z</published>
    <title>The Fake News Spreading Plague: Was it Preventable?</title>
    <summary>  In 2010, a paper entitled "From Obscurity to Prominence in Minutes: Political
Speech and Real-time search" won the Best Paper Prize of the Web Science 2010
Conference. Among its findings were the discovery and documentation of what was
termed a "Twitter-bomb", an organized effort to spread misinformation about the
democratic candidate Martha Coakley through anonymous Twitter accounts. In this
paper, after summarizing the details of that event, we outline the recipe of
how social networks are used to spread misinformation. One of the most
important steps in such a recipe is the "infiltration" of a community of users
who are already engaged in conversations about a topic, to use them as organic
spreaders of misinformation in their extended subnetworks. Then, we take this
misinformation spreading recipe and indicate how it was successfully used to
spread fake news during the 2016 U.S. Presidential Election. The main
differences between the scenarios are the use of Facebook instead of Twitter,
and the respective motivations (in 2010: political influence; in 2016:
financial benefit through online advertising). After situating these events in
the broader context of exploiting the Web, we seize this opportunity to address
limitations of the reach of research findings and to start a conversation about
how communities of researchers can increase their impact on real-world societal
issues.
</summary>
    <author>
      <name>Eni Mustafaraj</name>
    </author>
    <author>
      <name>Panagiotis Takis Metaxas</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.08740v2</id>
    <updated>2019-09-23T14:51:16Z</updated>
    <published>2019-09-18T23:53:24Z</published>
    <title>Can WhatsApp Counter Misinformation by Limiting Message Forwarding?</title>
    <summary>  WhatsApp is the most popular messaging app in the world. The closed nature of
the app, in addition to the ease of transferring multimedia and sharing
information to large-scale groups make WhatsApp unique among other platforms,
where an anonymous encrypted messages can become viral, reaching multiple users
in a short period of time. The personal feeling and immediacy of messages
directly delivered to the user's phone on WhatsApp was extensively abused to
spread unfounded rumors and create misinformation campaigns during recent
elections in Brazil and India. WhatsApp has been deploying measures to mitigate
this problem, such as reducing the limit for forwarding a message to at most
five users at once. Despite the welcomed effort to counter the problem, there
is no evidence so far on the real effectiveness of such restrictions. In this
work, we propose a methodology to evaluate the effectiveness of such measures
on the spreading of misinformation circulating on WhatsApp. We use an
epidemiological model and real data gathered from WhatsApp in Brazil, India and
Indonesia to assess the impact of limiting virality features in this kind of
network. Our results suggest that the current efforts deployed by WhatsApp can
offer significant delays on the information spread, but they are ineffective in
blocking the propagation of misinformation campaigns through public groups when
the content has a high viral nature.
</summary>
    <author>
      <name>Philipe de Freitas Melo</name>
    </author>
    <author>
      <name>Carolina Coimbra Vieira</name>
    </author>
    <author>
      <name>Kiran Garimella</name>
    </author>
    <author>
      <name>Pedro O. S. Vaz de Melo</name>
    </author>
    <author>
      <name>Fabr√≠cio Benevenuto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.08740v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08740v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09819v1</id>
    <updated>2021-05-20T15:10:48Z</updated>
    <published>2021-05-20T15:10:48Z</published>
    <title>Characterizing Abhorrent, Misinformative, and Mistargeted Content on
  YouTube</title>
    <summary>  YouTube has revolutionized the way people discover and consume video.
Although YouTube facilitates easy access to hundreds of well-produced and
trustworthy videos, abhorrent, misinformative, and mistargeted content is also
common. The platform is plagued by various types of problematic content: 1)
disturbing videos targeting young children; 2) hateful and misogynistic
content; and 3) pseudoscientific misinformation. While YouTube's recommendation
algorithm plays a vital role in increasing user engagement and YouTube's
monetization, its role in unwittingly promoting problematic content is not
entirely understood. In this thesis, we shed some light on the degree of
problematic content on YouTube and the role of the recommendation algorithm in
the dissemination of such content. Following a data-driven quantitative
approach, we analyze thousands of videos on YouTube, to shed light on: 1) the
risks of YouTube media consumption by young children; 2) the role of the
recommendation algorithm in the dissemination of misogynistic content, by
focusing on the Involuntary Celibates (Incels) community; and 3) user exposure
to pseudoscientific content on various parts of the platform and how this
exposure changes based on the user's watch history. Our analysis reveals that
young children are likely to encounter disturbing content when they randomly
browse the platform. By analyzing the Incel community on YouTube, we find that
Incel activity is increasing over time and that platforms may play an active
role in steering users towards extreme content. Finally, when studying
pseudoscientific misinformation, we find that YouTube suggests more
pseudoscientific content regarding traditional pseudoscientific topics (e.g.,
flat earth) than for emerging ones (like COVID-19) and that these
recommendations are more common on the search results page than on a user's
homepage or the video recommendations section.
</summary>
    <author>
      <name>Kostantinos Papadamou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis. Overlaps with arXiv:1901.07046, arXiv:2001.08293,
  arXiv:2010.11638</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.09819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10365v4</id>
    <updated>2020-08-04T20:26:08Z</updated>
    <published>2019-06-25T07:47:00Z</published>
    <title>Emotion Cognizance Improves Health Fake News Identification</title>
    <summary>  Identifying misinformation is increasingly being recognized as an important
computational task with high potential social impact. Misinformation and fake
contents are injected into almost every domain of news including politics,
health, science, business, etc., among which, the fakeness in health domain
pose serious adverse effects to scare or harm the society. Misinformation
contains scientific claims or content from social media exaggerated with strong
emotion content to attract eyeballs. In this paper, we consider the utility of
the affective character of news articles for fake news identification in the
health domain and present evidence that emotion cognizant representations are
significantly more suited for the task. We outline a technique to leverage
emotion intensity lexicons to develop emotionized text representations, and
evaluate the utility of such a representation for identifying fake news
relating to health in various supervised and unsupervised scenarios. The
consistent and significant empirical gains that we observe over a range of
technique types and parameter settings establish the utility of the emotional
information in news articles, an often overlooked aspect, for the task of
misinformation identification in the health domain.
</summary>
    <author>
      <name>Anoop K</name>
    </author>
    <author>
      <name>Deepak P</name>
    </author>
    <author>
      <name>Lajish V L</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of 24th International Database Engineering &amp;
  Applications Symposium (IDEAS 2020), Incheon, Korea</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10365v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10365v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.05825v1</id>
    <updated>2019-11-13T21:49:17Z</updated>
    <published>2019-11-13T21:49:17Z</published>
    <title>Trustworthy Misinformation Mitigation with Soft Information Nudging</title>
    <summary>  Research in combating misinformation reports many negative results: facts may
not change minds, especially if they come from sources that are not trusted.
Individuals can disregard and justify lies told by trusted sources. This
problem is made even worse by social recommendation algorithms which help
amplify conspiracy theories and information confirming one's own biases due to
companies' efforts to optimize for clicks and watch time over individuals' own
values and public good. As a result, more nuanced voices and facts are drowned
out by a continuous erosion of trust in better information sources. Most
misinformation mitigation techniques assume that discrediting, filtering, or
demoting low veracity information will help news consumers make better
information decisions. However, these negative results indicate that some news
consumers, particularly extreme or conspiracy news consumers will not be
helped.
  We argue that, given this background, technology solutions to combating
misinformation should not simply seek facts or discredit bad news sources, but
instead use more subtle nudges towards better information consumption. Repeated
exposure to such nudges can help promote trust in better information sources
and also improve societal outcomes in the long run. In this article, we will
talk about technological solutions that can help us in developing such an
approach, and introduce one such model called Trust Nudging.
</summary>
    <author>
      <name>Benjamin D. Horne</name>
    </author>
    <author>
      <name>Maur√≠cio Gruppi</name>
    </author>
    <author>
      <name>Sibel Adalƒ±</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at IEEE TPS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.05825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.12742v1</id>
    <updated>2020-08-28T16:55:43Z</updated>
    <published>2020-08-28T16:55:43Z</published>
    <title>Linked Credibility Reviews for Explainable Misinformation Detection</title>
    <summary>  In recent years, misinformation on the Web has become increasingly rampant.
The research community has responded by proposing systems and challenges, which
are beginning to be useful for (various subtasks of) detecting misinformation.
However, most proposed systems are based on deep learning techniques which are
fine-tuned to specific domains, are difficult to interpret and produce results
which are not machine readable. This limits their applicability and adoption as
they can only be used by a select expert audience in very specific settings. In
this paper we propose an architecture based on a core concept of Credibility
Reviews (CRs) that can be used to build networks of distributed bots that
collaborate for misinformation detection. The CRs serve as building blocks to
compose graphs of (i) web content, (ii) existing credibility signals
--fact-checked claims and reputation reviews of websites--, and (iii)
automatically computed reviews. We implement this architecture on top of
lightweight extensions to Schema.org and services providing generic NLP tasks
for semantic similarity and stance detection. Evaluations on existing datasets
of social-media posts, fake news and political speeches demonstrates several
advantages over existing systems: extensibility, domain-independence,
composability, explainability and transparency via provenance. Furthermore, we
obtain competitive results without requiring finetuning and establish a new
state of the art on the Clef'18 CheckThat! Factuality task.
</summary>
    <author>
      <name>Ronald Denaux</name>
    </author>
    <author>
      <name>Jose Manuel Gomez-Perez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 19th International Semantic Web Conference (ISWC
  2020) https://iswc2020.semanticweb.org</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.12742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.12742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.10864v1</id>
    <updated>2021-04-22T05:09:25Z</updated>
    <published>2021-04-22T05:09:25Z</published>
    <title>Misinformation, Believability, and Vaccine Acceptance Over 40 Countries:
  Takeaways From the Initial Phase of The COVID-19 Infodemic</title>
    <summary>  The COVID-19 pandemic has been damaging to the lives of people all around the
world. Accompanied by the pandemic is an infodemic, an abundant and
uncontrolled spreading of potentially harmful misinformation. The infodemic may
severely change the pandemic's course by interfering with public health
interventions such as wearing masks, social distancing, and vaccination. In
particular, the impact of the infodemic on vaccination is critical because it
holds the key to reverting to pre-pandemic normalcy. This paper presents
findings from a global survey on the extent of worldwide exposure to the
COVID-19 infodemic, assesses different populations' susceptibility to false
claims, and analyzes its association with vaccine acceptance. Based on
responses gathered from over 18,400 individuals from 40 countries, we find a
strong association between perceived believability of misinformation and
vaccination hesitancy. Additionally, our study shows that only half of the
online users exposed to rumors might have seen the fact-checked information.
Moreover, depending on the country, between 6% and 37% of individuals
considered these rumors believable. Our survey also shows that poorer regions
are more susceptible to encountering and believing COVID-19 misinformation. We
discuss implications of our findings on public campaigns that proactively
spread accurate information to countries that are more susceptible to the
infodemic. We also highlight fact-checking platforms' role in better
identifying and prioritizing claims that are perceived to be believable and
have wide exposure. Our findings give insights into better handling of risk
communication during the initial phase of a future pandemic.
</summary>
    <author>
      <name>Karandeep Singh</name>
    </author>
    <author>
      <name>Gabriel Lima</name>
    </author>
    <author>
      <name>Meeyoung Cha</name>
    </author>
    <author>
      <name>Chiyoung Cha</name>
    </author>
    <author>
      <name>Juhi Kulshrestha</name>
    </author>
    <author>
      <name>Yong-Yeol Ahn</name>
    </author>
    <author>
      <name>Onur Varol</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0263381</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0263381" rel="related"/>
    <link href="http://arxiv.org/abs/2104.10864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.06416v2</id>
    <updated>2021-09-23T19:57:17Z</updated>
    <published>2021-09-14T03:57:50Z</published>
    <title>MMCoVaR: Multimodal COVID-19 Vaccine Focused Data Repository for Fake
  News Detection and a Baseline Architecture for Classification</title>
    <summary>  The outbreak of COVID-19 has resulted in an "infodemic" that has encouraged
the propagation of misinformation about COVID-19 and cure methods which, in
turn, could negatively affect the adoption of recommended public health
measures in the larger population. In this paper, we provide a new multimodal
(consisting of images, text and temporal information) labeled dataset
containing news articles and tweets on the COVID-19 vaccine. We collected 2,593
news articles from 80 publishers for one year between Feb 16th 2020 to May 8th
2021 and 24184 Twitter posts (collected between April 17th 2021 to May 8th
2021). We combine ratings from two news media ranking sites: Medias Bias Chart
and Media Bias/Fact Check (MBFC) to classify the news dataset into two levels
of credibility: reliable and unreliable. The combination of two filters allows
for higher precision of labeling. We also propose a stance detection mechanism
to annotate tweets into three levels of credibility: reliable, unreliable and
inconclusive. We provide several statistics as well as other analytics like,
publisher distribution, publication date distribution, topic analysis, etc. We
also provide a novel architecture that classifies the news data into
misinformation or truth to provide a baseline performance for this dataset. We
find that the proposed architecture has an F-Score of 0.919 and accuracy of
0.882 for fake news detection. Furthermore, we provide benchmark performance
for misinformation detection on tweet dataset. This new multimodal dataset can
be used in research on COVID-19 vaccine, including misinformation detection,
influence of fake COVID-19 vaccine information, etc.
</summary>
    <author>
      <name>Mingxuan Chen</name>
    </author>
    <author>
      <name>Xinqiao Chu</name>
    </author>
    <author>
      <name>K. P. Subbalakshmi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures. This paper has been accepted for publication in
  ASONAM 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.06416v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.06416v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.10594v1</id>
    <updated>2021-11-20T14:07:10Z</updated>
    <published>2021-11-20T14:07:10Z</published>
    <title>Misrepresenting Scientific Consensus on COVID-19: The Amplification of
  Dissenting Scientists on Twitter</title>
    <summary>  The COVID-19 pandemic has resulted in a slew of misinformation, often
described as an "infodemic". Whereas previous research has focused on the
propagation of unreliable sources as a main vehicle of misinformation, the
present study focuses on exploring the role of scientists whose views oppose
the scientific consensus. Using Nobelists in Physiology and Medicine as a proxy
for scientific consensus, we analyze two separate datasets: 15.8K tweets by
13.1K unique users on COVID-19 vaccines specifically, and 208K tweets by 151K
unique users on COVID-19 broadly which mention the Nobelist names. Our analyses
reveal that dissenting scientists are amplified by a factor of 426 relative to
true scientific consensus in the context of COVID-19 vaccines, and by a factor
of 43 in the context of COVID-19 generally. Although more popular accounts tend
to mention consensus-abiding scientists more, our results suggest that this
false consensus is driven by higher engagement with dissent-mentioning tweets.
Furthermore, false consensus mostly occurs due to traffic spikes following
highly popularized statements of dissenting scientists. We find that dissenting
voices are mainly discussed in French, English-speaking, Turkish, Brazilian,
Argentine, Indian, and Japanese misinformation clusters. This research suggests
that social media platforms should prioritize the exposure of consensus-abiding
scientists as a vehicle of reversing false consensus and addressing
misinformation stemming from seemingly credible sources.
</summary>
    <author>
      <name>Alexandros Efstratiou</name>
    </author>
    <author>
      <name>Tristan Caulfield</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.10594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.10594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06181v1</id>
    <updated>2022-05-11T07:42:56Z</updated>
    <published>2022-05-11T07:42:56Z</published>
    <title>FACTOID: A New Dataset for Identifying Misinformation Spreaders and
  Political Bias</title>
    <summary>  Proactively identifying misinformation spreaders is an important step towards
mitigating the impact of fake news on our society. In this paper, we introduce
a new contemporary Reddit dataset for fake news spreader analysis, called
FACTOID, monitoring political discussions on Reddit since the beginning of
2020. The dataset contains over 4K users with 3.4M Reddit posts, and includes,
beyond the users' binary labels, also their fine-grained credibility level
(very low to very high) and their political bias strength (extreme right to
extreme left). As far as we are aware, this is the first fake news spreader
dataset that simultaneously captures both the long-term context of users'
historical posts and the interactions between them. To create the first
benchmark on our data, we provide methods for identifying misinformation
spreaders by utilizing the social connections between the users along with
their psycho-linguistic features. We show that the users' social interactions
can, on their own, indicate misinformation spreading, while the
psycho-linguistic features are mostly informative in non-neural classification
settings. In a qualitative analysis, we observe that detecting affective mental
processes correlates negatively with right-biased users, and that the openness
to experience factor is lower for those who spread fake news.
</summary>
    <author>
      <name>Flora Sakketou</name>
    </author>
    <author>
      <name>Joan Plepi</name>
    </author>
    <author>
      <name>Riccardo Cervero</name>
    </author>
    <author>
      <name>Henri-Jacques Geiss</name>
    </author>
    <author>
      <name>Paolo Rosso</name>
    </author>
    <author>
      <name>Lucie Flek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to LREC 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.06181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02079v1</id>
    <updated>2015-08-09T20:14:09Z</updated>
    <published>2015-08-09T20:14:09Z</published>
    <title>Facts and Fabrications about Ebola: A Twitter Based Study</title>
    <summary>  Microblogging websites like Twitter have been shown to be immensely useful
for spreading information on a global scale within seconds. The detrimental
effect, however, of such platforms is that misinformation and rumors are also
as likely to spread on the network as credible, verified information. From a
public health standpoint, the spread of misinformation creates unnecessary
panic for the public. We recently witnessed several such scenarios during the
outbreak of Ebola in 2014 [14, 1]. In order to effectively counter the medical
misinformation in a timely manner, our goal here is to study the nature of such
misinformation and rumors in the United States during fall 2014 when a handful
of Ebola cases were confirmed in North America. It is a well known convention
on Twitter to use hashtags to give context to a Twitter message (a tweet). In
this study, we collected approximately 47M tweets from the Twitter streaming
API related to Ebola. Based on hashtags, we propose a method to classify the
tweets into two sets: credible and speculative. We analyze these two sets and
study how they differ in terms of a number of features extracted from the
Twitter API. In conclusion, we infer several interesting differences between
the two sets. We outline further potential directions to using this material
for monitoring and separating speculative tweets from credible ones, to enable
improved public health information.
</summary>
    <author>
      <name>Janani Kalyanam</name>
    </author>
    <author>
      <name>Sumithra Velupillai</name>
    </author>
    <author>
      <name>Son Doan</name>
    </author>
    <author>
      <name>Mike Conway</name>
    </author>
    <author>
      <name>Gert Lanckriet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in SIGKDD BigCHat Workshop 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04310v2</id>
    <updated>2021-06-03T22:30:14Z</updated>
    <published>2020-05-08T22:41:39Z</published>
    <title>Semi-Supervised Multi-aspect Detection of Misinformation using
  Hierarchical Joint Decomposition</title>
    <summary>  Distinguishing between misinformation and real information is one of the most
challenging problems in today's interconnected world. The vast majority of the
state-of-the-art in detecting misinformation is fully supervised, requiring a
large number of high-quality human annotations. However, the availability of
such annotations cannot be taken for granted, since it is very costly,
time-consuming, and challenging to do so in a way that keeps up with the
proliferation of misinformation. In this work, we are interested in exploring
scenarios where the number of annotations is limited. In such scenarios, we
investigate how tapping on a diverse number of resources that characterize a
news article, henceforth referred to as "aspects" can compensate for the lack
of labels. In particular, our contributions in this paper are twofold: 1) We
propose the use of three different aspects: article content, context of social
sharing behaviors, and host website/domain features, and 2) We introduce a
principled tensor based embedding framework that combines all those aspects
effectively. We propose HiJoD a 2-level decomposition pipeline which not only
outperforms state-of-the-art methods with F1-scores of 74% and 81% on Twitter
and Politifact datasets respectively but also is an order of magnitude faster
than similar ensemble approaches.
</summary>
    <author>
      <name>Sara Abdali</name>
    </author>
    <author>
      <name>Neil Shah</name>
    </author>
    <author>
      <name>Evangelos E. Papalexakis</name>
    </author>
    <link href="http://arxiv.org/abs/2005.04310v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04310v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.08705v1</id>
    <updated>2020-05-15T17:02:19Z</updated>
    <published>2020-05-15T17:02:19Z</published>
    <title>Threat from being Social: Vulnerability Analysis of Social Network
  Coupled Smart Grid</title>
    <summary>  Social Networks (SNs) have been gradually applied by utility companies as an
addition to smart grid and are proved to be helpful in smoothing load curves
and reducing energy usage. However, SNs also bring in new threats to smart
grid: misinformation in SNs may cause smart grid users to alter their demand,
resulting in transmission line overloading and in turn leading to catastrophic
impact to the grid. In this paper, we discuss the interdependency in the social
network coupled smart grid and focus on its vulnerability. That is, how much
can the smart grid be damaged when misinformation related to it diffuses in
SNs? To analytically study the problem, we propose the Misinformation Attack
Problem in Social-Smart Grid (MAPSS) that identifies the top critical nodes in
the SN, such that the smart grid can be greatly damaged when misinformation
propagates from those nodes. This problem is challenging as we have to
incorporate the complexity of the two networks concurrently. Nevertheless, we
propose a technique that can explicitly take into account information diffusion
in SN, power flow balance and cascading failure in smart grid integratedly when
evaluating node criticality, based on which we propose various strategies in
selecting the most critical nodes. Also, we introduce controlled load shedding
as a protection strategy to reduce the impact of cascading failure. The
effectiveness of our algorithms are demonstrated by experiments on IEEE bus
test cases as well as the Pegase data set.
</summary>
    <author>
      <name>Tianyi Pan</name>
    </author>
    <author>
      <name>Subhankar Mishra</name>
    </author>
    <author>
      <name>Lan N. Nguyen</name>
    </author>
    <author>
      <name>Gunhee Lee</name>
    </author>
    <author>
      <name>Jungmin Kang</name>
    </author>
    <author>
      <name>Jungtaek Seo</name>
    </author>
    <author>
      <name>My T. Thai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2017.2738565</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2017.2738565" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access 5 (2017): 16774-16783</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.08705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.07849v2</id>
    <updated>2021-06-03T22:32:32Z</updated>
    <published>2021-02-15T21:05:11Z</published>
    <title>Identifying Misinformation from Website Screenshots</title>
    <summary>  Can the look and the feel of a website give information about the
trustworthiness of an article? In this paper, we propose to use a promising,
yet neglected aspect in detecting the misinformativeness: the overall look of
the domain webpage. To capture this overall look, we take screenshots of news
articles served by either misinformative or trustworthy web domains and
leverage a tensor decomposition based semi-supervised classification technique.
The proposed approach i.e., VizFake is insensitive to a number of image
transformations such as converting the image to grayscale, vectorizing the
image and losing some parts of the screenshots. VizFake leverages a very small
amount of known labels, mirroring realistic and practical scenarios, where
labels (especially for known misinformative articles), are scarce and quickly
become dated. The F1 score of VizFake on a dataset of 50k screenshots of news
articles spanning more than 500 domains is roughly 85% using only 5% of ground
truth labels. Furthermore, tensor representations of VizFake, obtained in an
unsupervised manner, allow for exploratory analysis of the data that provides
valuable insights into the problem. Finally, we compare VizFake with deep
transfer learning, since it is a very popular black-box approach for image
classification and also well-known text text-based methods. VizFake achieves
competitive accuracy with deep transfer learning models while being two orders
of magnitude faster and not requiring laborious hyper-parameter tuning.
</summary>
    <author>
      <name>Sara Abdali</name>
    </author>
    <author>
      <name>Rutuja Gurav</name>
    </author>
    <author>
      <name>Siddharth Menon</name>
    </author>
    <author>
      <name>Daniel Fonseca</name>
    </author>
    <author>
      <name>Negin Entezari</name>
    </author>
    <author>
      <name>Neil Shah</name>
    </author>
    <author>
      <name>Evangelos E. Papalexakis</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International AAAI Conference on Web and Social Media (ICWSM)
  2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.07849v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.07849v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07435v2</id>
    <updated>2022-02-17T16:08:09Z</updated>
    <published>2021-06-14T13:56:26Z</published>
    <title>Misinformation versus Facts: Understanding the Influence of News
  Regarding COVID-19 Vaccines on Vaccine Uptake</title>
    <summary>  There is a lot of fact-based information and misinformation in the online
discourses and discussions about the COVID-19 vaccines. Using a sample of
nearly four million geotagged English tweets and the data from the CDC COVID
Data Tracker, we conducted the Fama-MacBeth regression with the Newey-West
adjustment to understand the influence of both misinformation and fact-based
news on Twitter on the COVID-19 vaccine uptake in the U.S. from April 19 when
U.S. adults were vaccine eligible to June 30, 2021, after controlling
state-level factors such as demographics, education, and the pandemic severity.
We identified the tweets related to either misinformation or fact-based news by
analyzing the URLs. One percent increase in fact-related Twitter users is
associated with an approximately 0.87 decrease (B = -0.87, SE = 0.25, p&lt;.001)
in the number of daily new vaccinated people per hundred. No significant
relationship was found between the percentage of fake-news-related users and
the vaccination rate. The negative association between the percentage of
fact-related users and the vaccination rate might be due to a combination of a
larger user-level influence and the negative impact of online social
endorsement on vaccination intent.
</summary>
    <author>
      <name>Hanjia Lyu</name>
    </author>
    <author>
      <name>Zihe Zheng</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Health Data Science, 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.07435v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07435v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11227v1</id>
    <updated>2021-06-21T16:13:44Z</updated>
    <published>2021-06-21T16:13:44Z</published>
    <title>FauxWard: A Graph Neural Network Approach to Fauxtography Detection
  Using Social Media Comments</title>
    <summary>  Online social media has been a popular source for people to consume and share
news content. More recently, the spread of misinformation online has caused
widespread concerns. In this work, we focus on a critical task of detecting
fauxtography on social media where the image and associated text together
convey misleading information. Many efforts have been made to mitigate
misinformation online, but we found that the fauxtography problem has not been
fully addressed by existing work. Solutions focusing on detecting fake images
or misinformed texts alone on social media often fail to identify the
misinformation delivered together by the image and the associated text of a
fauxtography post. In this paper, we develop FauxWard, a novel graph
convolutional neural network framework that explicitly explores the complex
information extracted from a user comment network of a social media post to
effectively identify fauxtography. FauxWard is content-free in the sense that
it does not analyze the visual or textual contents of the post itself, which
makes it robust against sophisticated fauxtography uploaders who intentionally
craft image-centric posts by editing either the text or image content. We
evaluate FauxWard on two real-world datasets collected from mainstream social
media platforms (i.e., Reddit and Twitter). The results show that FauxWard is
both effective and efficient in identifying fauxtography posts on social media.
</summary>
    <author>
      <name>Lanyu Shang</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Daniel Zhang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13278-020-00689-w</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13278-020-00689-w" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Soc. Netw. Anal. Min. 10, 76 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.11227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06495v2</id>
    <updated>2021-10-14T19:54:41Z</updated>
    <published>2021-10-13T04:44:02Z</published>
    <title>Cross-lingual COVID-19 Fake News Detection</title>
    <summary>  The COVID-19 pandemic poses a great threat to global public health.
Meanwhile, there is massive misinformation associated with the pandemic which
advocates unfounded or unscientific claims. Even major social media and news
outlets have made an extra effort in debunking COVID-19 misinformation, most of
the fact-checking information is in English, whereas some unmoderated COVID-19
misinformation is still circulating in other languages, threatening the health
of less-informed people in immigrant communities and developing countries. In
this paper, we make the first attempt to detect COVID-19 misinformation in a
low-resource language (Chinese) only using the fact-checked news in a
high-resource language (English). We start by curating a Chinese real&amp;fake news
dataset according to existing fact-checking information. Then, we propose a
deep learning framework named CrossFake to jointly encode the cross-lingual
news body texts and capture the news content as much as possible. Empirical
results on our dataset demonstrate the effectiveness of CrossFake under the
cross-lingual setting and it also outperforms several monolingual and
cross-lingual fake news detectors. The dataset is available at
https://github.com/YingtongDou/CrossFake.
</summary>
    <author>
      <name>Jiangshu Du</name>
    </author>
    <author>
      <name>Yingtong Dou</name>
    </author>
    <author>
      <name>Congying Xia</name>
    </author>
    <author>
      <name>Limeng Cui</name>
    </author>
    <author>
      <name>Jing Ma</name>
    </author>
    <author>
      <name>Philip S. Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by SDM at ICDM, data is available at
  https://github.com/YingtongDou/CrossFake</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06495v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06495v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.08470v1</id>
    <updated>2021-12-15T20:34:57Z</updated>
    <published>2021-12-15T20:34:57Z</published>
    <title>Insta-VAX: A Multimodal Benchmark for Anti-Vaccine and Misinformation
  Posts Detection on Social Media</title>
    <summary>  Sharing of anti-vaccine posts on social media, including misinformation
posts, has been shown to create confusion and reduce the publics confidence in
vaccines, leading to vaccine hesitancy and resistance. Recent years have
witnessed the fast rise of such anti-vaccine posts in a variety of linguistic
and visual forms in online networks, posing a great challenge for effective
content moderation and tracking. Extending previous work on leveraging textual
information to understand vaccine information, this paper presents Insta-VAX, a
new multi-modal dataset consisting of a sample of 64,957 Instagram posts
related to human vaccines. We applied a crowdsourced annotation procedure
verified by two trained expert judges to this dataset. We then bench-marked
several state-of-the-art NLP and computer vision classifiers to detect whether
the posts show anti-vaccine attitude and whether they contain misinformation.
Extensive experiments and analyses demonstrate the multimodal models can
classify the posts more accurately than the uni-modal models, but still need
improvement especially on visual context understanding and external knowledge
cooperation. The dataset and classifiers contribute to monitoring and tracking
of vaccine discussions for social scientific and public health efforts in
combating the problem of vaccine misinformation.
</summary>
    <author>
      <name>Mingyang Zhou</name>
    </author>
    <author>
      <name>Mahasweta Chakraborti</name>
    </author>
    <author>
      <name>Sijia Qian</name>
    </author>
    <author>
      <name>Zhou Yu</name>
    </author>
    <author>
      <name>Jingwen Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2112.08470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.08470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.03590v1</id>
    <updated>2022-02-08T01:31:34Z</updated>
    <published>2022-02-08T01:31:34Z</published>
    <title>Moral Emotions Shape the Virality of COVID-19 Misinformation on Social
  Media</title>
    <summary>  While false rumors pose a threat to the successful overcoming of the COVID-19
pandemic, an understanding of how rumors diffuse in online social networks is -
even for non-crisis situations - still in its infancy. Here we analyze a large
sample consisting of COVID-19 rumor cascades from Twitter that have been
fact-checked by third-party organizations. The data comprises N=10,610 rumor
cascades that have been retweeted more than 24 million times. We investigate
whether COVID-19 misinformation spreads more viral than the truth and whether
the differences in the diffusion of true vs. false rumors can be explained by
the moral emotions they carry. We observe that, on average, COVID-19
misinformation is more likely to go viral than truthful information. However,
the veracity effect is moderated by moral emotions: false rumors are more viral
than the truth if the source tweets embed a high number of other-condemning
emotion words, whereas a higher number of self-conscious emotion words is
linked to a less viral spread. The effects are pronounced both for health
misinformation and false political rumors. These findings offer insights into
how true vs. false rumors spread and highlight the importance of considering
emotions from the moral emotion families in social media content.
</summary>
    <author>
      <name>Kirill Solovev</name>
    </author>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <link href="http://arxiv.org/abs/2202.03590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.03590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.11684v2</id>
    <updated>2022-03-08T12:30:03Z</updated>
    <published>2022-02-23T18:47:34Z</published>
    <title>MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation
  Social Network Dataset</title>
    <summary>  Misinformation is becoming increasingly prevalent on social media and in news
articles. It has become so widespread that we require algorithmic assistance
utilising machine learning to detect such content. Training these machine
learning models require datasets of sufficient scale, diversity and quality.
However, datasets in the field of automatic misinformation detection are
predominantly monolingual, include a limited amount of modalities and are not
of sufficient scale and quality. Addressing this, we develop a data collection
and linking system (MuMiN-trawl), to build a public misinformation graph
dataset (MuMiN), containing rich social media data (tweets, replies, users,
images, articles, hashtags) spanning 21 million tweets belonging to 26 thousand
Twitter threads, each of which have been semantically linked to 13 thousand
fact-checked claims across dozens of topics, events and domains, in 41
different languages, spanning more than a decade. The dataset is made available
as a heterogeneous graph via a Python package (mumin). We provide baseline
results for two node classification tasks related to the veracity of a claim
involving social media, and demonstrate that these are challenging tasks, with
the highest macro-average F1-score being 62.55% and 61.45% for the two tasks,
respectively. The MuMiN ecosystem is available at
https://mumin-dataset.github.io/, including the data, documentation, tutorials
and leaderboards.
</summary>
    <author>
      <name>Dan Saattrup Nielsen</name>
    </author>
    <author>
      <name>Ryan McConville</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9+3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.11684v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.11684v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.12413v1</id>
    <updated>2022-02-24T23:10:29Z</updated>
    <published>2022-02-24T23:10:29Z</published>
    <title>Construction of Large-Scale Misinformation Labeled Datasets from Social
  Media Discourse using Label Refinement</title>
    <summary>  Malicious accounts spreading misinformation has led to widespread false and
misleading narratives in recent times, especially during the COVID-19 pandemic,
and social media platforms struggle to eliminate these contents rapidly. This
is because adapting to new domains requires human intensive fact-checking that
is slow and difficult to scale. To address this challenge, we propose to
leverage news-source credibility labels as weak labels for social media posts
and propose model-guided refinement of labels to construct large-scale, diverse
misinformation labeled datasets in new domains. The weak labels can be
inaccurate at the article or social media post level where the stance of the
user does not align with the news source or article credibility. We propose a
framework to use a detection model self-trained on the initial weak labels with
uncertainty sampling based on entropy in predictions of the model to identify
potentially inaccurate labels and correct for them using self-supervision or
relabeling. The framework will incorporate social context of the post in terms
of the community of its associated user for surfacing inaccurate labels towards
building a large-scale dataset with minimum human effort. To provide labeled
datasets with distinction of misleading narratives where information might be
missing significant context or has inaccurate ancillary details, the proposed
framework will use the few labeled samples as class prototypes to separate high
confidence samples into false, unproven, mixture, mostly false, mostly true,
true, and debunk information. The approach is demonstrated for providing a
large-scale misinformation dataset on COVID-19 vaccines.
</summary>
    <author>
      <name>Karishma Sharma</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WWW (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2202.12413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.12413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.11724v2</id>
    <updated>2022-09-19T18:52:50Z</updated>
    <published>2022-03-20T11:22:59Z</published>
    <title>Explainable Misinformation Detection Across Multiple Social Media
  Platforms</title>
    <summary>  In this work, the integration of two machine learning approaches, namely
domain adaptation and explainable AI, is proposed to address these two issues
of generalized detection and explainability. Firstly the Domain Adversarial
Neural Network (DANN) develops a generalized misinformation detector across
multiple social media platforms DANN is employed to generate the classification
results for test domains with relevant but unseen data. The DANN-based model, a
traditional black-box model, cannot justify its outcome, i.e., the labels for
the target domain. Hence a Local Interpretable Model-Agnostic Explanations
(LIME) explainable AI model is applied to explain the outcome of the DANN mode.
To demonstrate these two approaches and their integration for effective
explainable generalized detection, COVID-19 misinformation is considered a case
study. We experimented with two datasets, namely CoAID and MiSoVac, and
compared results with and without DANN implementation. DANN significantly
improves the accuracy measure F1 classification score and increases the
accuracy and AUC performance. The results obtained show that the proposed
framework performs well in the case of domain shift and can learn
domain-invariant features while explaining the target labels with LIME
implementation enabling trustworthy information processing and extraction to
combat misinformation effectively.
</summary>
    <author>
      <name>Gargi Joshi</name>
    </author>
    <author>
      <name>Ananya Srivastava</name>
    </author>
    <author>
      <name>Bhargav Yagnik</name>
    </author>
    <author>
      <name>Mohammed Hasan</name>
    </author>
    <author>
      <name>Zainuddin Saiyed</name>
    </author>
    <author>
      <name>Lubna A Gabralla</name>
    </author>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <author>
      <name>Rahee Walambe</name>
    </author>
    <author>
      <name>Ketan Kotecha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages,4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.11724v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.11724v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.06654v3</id>
    <updated>2023-10-14T05:53:02Z</updated>
    <published>2022-10-13T01:24:19Z</published>
    <title>The Inventory is Dark and Full of Misinformation: Understanding the
  Abuse of Ad Inventory Pooling in the Ad-Tech Supply Chain</title>
    <summary>  Ad-tech enables publishers to programmatically sell their ad inventory to
millions of demand partners through a complex supply chain. Bogus or low
quality publishers can exploit the opaque nature of the ad-tech to deceptively
monetize their ad inventory. In this paper, we investigate for the first time
how misinformation sites subvert the ad-tech transparency standards and pool
their ad inventory with unrelated sites to circumvent brand safety protections.
We find that a few major ad exchanges are disproportionately responsible for
the dark pools that are exploited by misinformation websites. We further find
evidence that dark pooling allows misinformation sites to deceptively sell
their ad inventory to reputable brands. We conclude with a discussion of
potential countermeasures such as better vetting of ad exchange partners,
adoption of new ad-tech transparency standards that enable end-to-end
validation of the ad-tech supply chain, as well as widespread deployment of
independent audits like ours.
</summary>
    <author>
      <name>Yash Vekaria</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of California, Davis</arxiv:affiliation>
    </author>
    <author>
      <name>Rishab Nithyanand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Iowa</arxiv:affiliation>
    </author>
    <author>
      <name>Zubair Shafiq</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of California, Davis</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at IEEE Symposium on Security &amp; Privacy (Oakland) 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.06654v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.06654v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.17166v1</id>
    <updated>2022-10-31T09:21:01Z</updated>
    <published>2022-10-31T09:21:01Z</published>
    <title>Listen to what they say: Better understand and detect online
  misinformation with user feedback</title>
    <summary>  Social media users who report content are key allies in the management of
online misinformation, however, no research has been conducted yet to
understand their role and the different trends underlying their reporting
activity. We suggest an original approach to studying misinformation: examining
it from the reporting users perspective at the content-level and comparatively
across regions and platforms. We propose the first classification of reported
content pieces, resulting from a review of c. 9,000 items reported on Facebook
and Instagram in France, the UK, and the US in June 2020. This allows us to
observe meaningful distinctions regarding reporting content between countries
and platforms as it significantly varies in volume, type, topic, and
manipulation technique. Examining six of these techniques, we identify a novel
one that is specific to Instagram US and significantly more sophisticated than
others, potentially presenting a concrete challenge for algorithmic detection
and human moderation. We also identify four reporting behaviours, from which we
derive four types of noise capable of explaining half of the inaccuracy found
in content reported as misinformation. We finally show that breaking down the
user reporting signal into a plurality of behaviours allows to train a simple,
although competitive, classifier on a small dataset with a combination of basic
users-reports to classify the different types of reported content pieces.
</summary>
    <author>
      <name>Hubert Etienne</name>
    </author>
    <author>
      <name>Onur √áelebi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.54501/jots.v1i5.106</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.54501/jots.v1i5.106" rel="related"/>
    <link href="http://arxiv.org/abs/2210.17166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.17166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.11495v3</id>
    <updated>2022-12-19T13:29:45Z</updated>
    <published>2022-11-21T14:32:37Z</published>
    <title>Global misinformation spillovers in the online vaccination debate before
  and during COVID-19</title>
    <summary>  Anti-vaccination views pervade online social media, fueling distrust in
scientific expertise and increasing vaccine-hesitant individuals. While
previous studies focused on specific countries, the COVID-19 pandemic brought
the vaccination discourse worldwide, underpinning the need to tackle
low-credible information flows on a global scale to design effective
countermeasures. Here, we leverage 316 million vaccine-related Twitter messages
in 18 languages, from October 2019 to March 2021, to quantify misinformation
flows between users exposed to anti-vaccination (no-vax) content. We find that,
during the pandemic, no-vax communities became more central in the
country-specific debates and their cross-border connections strengthened,
revealing a global Twitter anti-vaccination network. U.S. users are central in
this network, while Russian users also become net exporters of misinformation
during vaccination roll-out. Interestingly, we find that Twitter's content
moderation efforts, and in particular the suspension of users following the
January 6th U.S. Capitol attack, had a worldwide impact in reducing
misinformation spread about vaccines. These findings may help public health
institutions and social media platforms to mitigate the spread of
health-related, low-credible information by revealing vulnerable online
communities.
</summary>
    <author>
      <name>Jacopo Lenti</name>
    </author>
    <author>
      <name>Kyriaki Kalimeri</name>
    </author>
    <author>
      <name>Andr√© Panisson</name>
    </author>
    <author>
      <name>Daniela Paolotti</name>
    </author>
    <author>
      <name>Michele Tizzani</name>
    </author>
    <author>
      <name>Yelena Mejova</name>
    </author>
    <author>
      <name>Michele Starnini</name>
    </author>
    <link href="http://arxiv.org/abs/2211.11495v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.11495v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.05443v3</id>
    <updated>2023-03-08T00:54:10Z</updated>
    <published>2023-02-10T18:58:34Z</published>
    <title>Believability and Harmfulness Shape the Virality of Misleading Social
  Media Posts</title>
    <summary>  Misinformation on social media presents a major threat to modern societies.
While previous research has analyzed the virality across true and false social
media posts, not every misleading post is necessarily equally viral. Rather,
misinformation has different characteristics and varies in terms of its
believability and harmfulness - which might influence its spread. In this work,
we study how the perceived believability and harmfulness of misleading posts
are associated with their virality on social media. Specifically, we analyze
(and validate) a large sample of crowd-annotated social media posts from
Twitter's Birdwatch platform, on which users can rate the believability and
harmfulness of misleading tweets. To address our research questions, we
implement an explanatory regression model and link the crowd ratings for
believability and harmfulness to the virality of misleading posts on Twitter.
Our findings imply that misinformation that is (i) easily believable and (ii)
not particularly harmful is associated with more viral resharing cascades.
These results offer insights into how different kinds of crowd fact-checked
misinformation spreads and suggest that the most viral misleading posts are
often not the ones that are particularly concerning from the perspective of
public safety. From a practical view, our findings may help platforms to
develop more effective strategies to curb the proliferation of misleading posts
on social media.
</summary>
    <author>
      <name>Chiara Drolsbach</name>
    </author>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3543507.3583857</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3543507.3583857" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at WWW 23</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.05443v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05443v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.06858v3</id>
    <updated>2023-06-11T22:11:10Z</updated>
    <published>2023-04-13T23:04:30Z</published>
    <title>Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter</title>
    <summary>  Vaccine hesitancy continues to be a main challenge for public health
officials during the COVID-19 pandemic. As this hesitancy undermines vaccine
campaigns, many researchers have sought to identify its root causes, finding
that the increasing volume of anti-vaccine misinformation on social media
platforms is a key element of this problem. We explored Twitter as a source of
misleading content with the goal of extracting overlapping cultural and
political beliefs that motivate the spread of vaccine misinformation. To do
this, we have collected a data set of vaccine-related Tweets and annotated them
with the help of a team of annotators with a background in communications and
journalism. Ultimately we hope this can lead to effective and targeted public
health communication strategies for reaching individuals with anti-vaccine
beliefs. Moreover, this information helps with developing Machine Learning
models to automatically detect vaccine misinformation posts and combat their
negative impacts. In this paper, we present Vax-Culture, a novel Twitter
COVID-19 dataset consisting of 6373 vaccine-related tweets accompanied by an
extensive set of human-provided annotations including vaccine-hesitancy stance,
indication of any misinformation in tweets, the entities criticized and
supported in each tweet and the communicated message of each tweet. Moreover,
we define five baseline tasks including four classification and one sequence
generation tasks, and report the results of a set of recent transformer-based
models for them. The dataset and code are publicly available at
https://github.com/mrzarei5/Vax-Culture.
</summary>
    <author>
      <name>Mohammad Reza Zarei</name>
    </author>
    <author>
      <name>Michael Christensen</name>
    </author>
    <author>
      <name>Sarah Everts</name>
    </author>
    <author>
      <name>Majid Komeili</name>
    </author>
    <link href="http://arxiv.org/abs/2304.06858v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.06858v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.14133v3</id>
    <updated>2023-10-18T13:19:52Z</updated>
    <published>2023-04-27T12:28:29Z</published>
    <title>VERITE: A Robust Benchmark for Multimodal Misinformation Detection
  Accounting for Unimodal Bias</title>
    <summary>  Multimedia content has become ubiquitous on social media platforms, leading
to the rise of multimodal misinformation (MM) and the urgent need for effective
strategies to detect and prevent its spread. In recent years, the challenge of
multimodal misinformation detection (MMD) has garnered significant attention by
researchers and has mainly involved the creation of annotated, weakly
annotated, or synthetically generated training datasets, along with the
development of various deep learning MMD models. However, the problem of
unimodal bias has been overlooked, where specific patterns and biases in MMD
benchmarks can result in biased or unimodal models outperforming their
multimodal counterparts on an inherently multimodal task; making it difficult
to assess progress. In this study, we systematically investigate and identify
the presence of unimodal bias in widely-used MMD benchmarks, namely VMU-Twitter
and COSMOS. To address this issue, we introduce the "VERification of Image-TExt
pairs" (VERITE) benchmark for MMD which incorporates real-world data, excludes
"asymmetric multimodal misinformation" and utilizes "modality balancing". We
conduct an extensive comparative study with a Transformer-based architecture
that shows the ability of VERITE to effectively address unimodal bias,
rendering it a robust evaluation framework for MMD. Furthermore, we introduce a
new method -- termed Crossmodal HArd Synthetic MisAlignment (CHASMA) -- for
generating realistic synthetic training data that preserve crossmodal relations
between legitimate images and false human-written captions. By leveraging
CHASMA in the training process, we observe consistent and notable improvements
in predictive performance on VERITE; with a 9.2% increase in accuracy. We
release our code at: https://github.com/stevejpapad/image-text-verification
</summary>
    <author>
      <name>Stefanos-Iordanis Papadopoulos</name>
    </author>
    <author>
      <name>Christos Koutlis</name>
    </author>
    <author>
      <name>Symeon Papadopoulos</name>
    </author>
    <author>
      <name>Panagiotis C. Petrantonakis</name>
    </author>
    <link href="http://arxiv.org/abs/2304.14133v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.14133v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.12155v1</id>
    <updated>2023-07-22T19:59:16Z</updated>
    <published>2023-07-22T19:59:16Z</published>
    <title>Identifying Misinformation on YouTube through Transcript Contextual
  Analysis with Transformer Models</title>
    <summary>  Misinformation on YouTube is a significant concern, necessitating robust
detection strategies. In this paper, we introduce a novel methodology for video
classification, focusing on the veracity of the content. We convert the
conventional video classification task into a text classification task by
leveraging the textual content derived from the video transcripts. We employ
advanced machine learning techniques like transfer learning to solve the
classification challenge. Our approach incorporates two forms of transfer
learning: (a) fine-tuning base transformer models such as BERT, RoBERTa, and
ELECTRA, and (b) few-shot learning using sentence-transformers MPNet and
RoBERTa-large. We apply the trained models to three datasets: (a) YouTube
Vaccine-misinformation related videos, (b) YouTube Pseudoscience videos, and
(c) Fake-News dataset (a collection of articles). Including the Fake-News
dataset extended the evaluation of our approach beyond YouTube videos. Using
these datasets, we evaluated the models distinguishing valid information from
misinformation. The fine-tuned models yielded Matthews Correlation
Coefficient&gt;0.81, accuracy&gt;0.90, and F1 score&gt;0.90 in two of three datasets.
Interestingly, the few-shot models outperformed the fine-tuned ones by 20% in
both Accuracy and F1 score for the YouTube Pseudoscience dataset, highlighting
the potential utility of this approach -- especially in the context of limited
training data.
</summary>
    <author>
      <name>Christos Christodoulou</name>
    </author>
    <author>
      <name>Nikos Salamanos</name>
    </author>
    <author>
      <name>Pantelitsa Leonidou</name>
    </author>
    <author>
      <name>Michail Papadakis</name>
    </author>
    <author>
      <name>Michael Sirivianos</name>
    </author>
    <link href="http://arxiv.org/abs/2307.12155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.12155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10587v1</id>
    <updated>2023-11-17T15:37:18Z</updated>
    <published>2023-11-17T15:37:18Z</published>
    <title>Countering Misinformation via Emotional Response Generation</title>
    <summary>  The proliferation of misinformation on social media platforms (SMPs) poses a
significant danger to public health, social cohesion and ultimately democracy.
Previous research has shown how social correction can be an effective way to
curb misinformation, by engaging directly in a constructive dialogue with users
who spread -- often in good faith -- misleading messages. Although professional
fact-checkers are crucial to debunking viral claims, they usually do not engage
in conversations on social media. Thereby, significant effort has been made to
automate the use of fact-checker material in social correction; however, no
previous work has tried to integrate it with the style and pragmatics that are
commonly employed in social media communication. To fill this gap, we present
VerMouth, the first large-scale dataset comprising roughly 12 thousand
claim-response pairs (linked to debunking articles), accounting for both
SMP-style and basic emotions, two factors which have a significant role in
misinformation credibility and spreading. To collect this dataset we used a
technique based on an author-reviewer pipeline, which efficiently combines LLMs
and human annotators to obtain high-quality data. We also provide comprehensive
experiments showing how models trained on our proposed dataset have significant
improvements in terms of output quality and generalization capabilities.
</summary>
    <author>
      <name>Daniel Russo</name>
    </author>
    <author>
      <name>Shane Peter Kaszefski-Yaschuk</name>
    </author>
    <author>
      <name>Jacopo Staiano</name>
    </author>
    <author>
      <name>Marco Guerini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2023 main conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.10587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.02395v1</id>
    <updated>2024-01-04T18:18:32Z</updated>
    <published>2024-01-04T18:18:32Z</published>
    <title>Analyzing Misinformation Claims During the 2022 Brazilian General
  Election on WhatsApp, Twitter, and Kwai</title>
    <summary>  This study analyzes misinformation from WhatsApp, Twitter, and Kwai during
the 2022 Brazilian general election. Given the democratic importance of
accurate information during elections, multiple fact-checking organizations
collaborated to identify and respond to misinformation via WhatsApp tiplines
and power a fact-checking feature within a chatbot operated by Brazil's
election authority, the TSE. WhatsApp is installed on over 99% of smartphones
in Brazil, and the TSE chatbot was used by millions of citizens in the run-up
to the elections. During the same period, we collected social media data from
Twitter (now X) and Kwai (a popular video-sharing app similar to TikTok). Using
the WhatsApp, Kwai, and Twitter data along with fact-checks from three
Brazilian fact-checking organizations, we find unique claims on each platform.
Even when the same claims are present on different platforms, they often differ
in format, detail, length, or other characteristics. Our research highlights
the limitations of current claim matching algorithms to match claims across
platforms with such differences and identifies areas for further algorithmic
development. Finally, we perform a descriptive analysis examining the formats
(image, video, audio, text) and content themes of popular misinformation
claims.
</summary>
    <author>
      <name>Scott A. Hale</name>
    </author>
    <author>
      <name>Adriano Belisario</name>
    </author>
    <author>
      <name>Ahmed Mostafa</name>
    </author>
    <author>
      <name>Chico Camargo</name>
    </author>
    <link href="http://arxiv.org/abs/2401.02395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.02395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.13554v3</id>
    <updated>2024-06-18T09:29:48Z</updated>
    <published>2024-05-22T11:40:22Z</published>
    <title>The Influencer Next Door: How Misinformation Creators Use GenAI</title>
    <summary>  Advances in generative AI (GenAI) have raised concerns about detecting and
discerning AI-generated content from human-generated content. Most existing
literature assumes a paradigm where 'expert' organized disinformation creators
and flawed AI models deceive 'ordinary' users. Based on longitudinal
ethnographic research with misinformation creators and consumers between
2022-2023, we instead find that GenAI supports bricolage work, where
non-experts increasingly use GenAI to remix, repackage, and (re)produce content
to meet their personal needs and desires. This research yielded four key
findings: First, participants primarily used GenAI for creation, rather than
truth-seeking. Second, a spreading 'influencer millionaire' narrative drove
participants to become content creators, using GenAI as a productivity tool to
generate a volume of (often misinformative) content. Third, GenAI lowered the
barrier to entry for content creation across modalities, enticing consumers to
become creators and significantly increasing existing creators' output.
Finally, participants used Gen AI to learn and deploy marketing tactics to
expand engagement and monetize their content. We argue for shifting analysis
from the public as consumers of AI content to bricoleurs who use GenAI
creatively, often without a detailed understanding of its underlying
technology. We analyze how these understudied emergent uses of GenAI produce
new or accelerated misinformation harms, and their implications for AI
products, platforms and policies.
</summary>
    <author>
      <name>Amelia Hassoun</name>
    </author>
    <author>
      <name>Ariel Abonizio</name>
    </author>
    <author>
      <name>Katy Osborn</name>
    </author>
    <author>
      <name>Cameron Wu</name>
    </author>
    <author>
      <name>Beth Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 25 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.13554v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.13554v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03181v1</id>
    <updated>2024-06-05T12:11:10Z</updated>
    <published>2024-06-05T12:11:10Z</published>
    <title>Missci: Reconstructing Fallacies in Misrepresented Science</title>
    <summary>  Health-related misinformation on social networks can lead to poor
decision-making and real-world dangers. Such misinformation often misrepresents
scientific publications and cites them as "proof" to gain perceived
credibility. To effectively counter such claims automatically, a system must
explain how the claim was falsely derived from the cited publication. Current
methods for automated fact-checking or fallacy detection neglect to assess the
(mis)used evidence in relation to misinformation claims, which is required to
detect the mismatch between them. To address this gap, we introduce Missci, a
novel argumentation theoretical model for fallacious reasoning together with a
new dataset for real-world misinformation detection that misrepresents
biomedical publications. Unlike previous fallacy detection datasets, Missci (i)
focuses on implicit fallacies between the relevant content of the cited
publication and the inaccurate claim, and (ii) requires models to verbalize the
fallacious reasoning in addition to classifying it. We present Missci as a
dataset to test the critical reasoning abilities of large language models
(LLMs), that are required to reconstruct real-world fallacious arguments, in a
zero-shot setting. We evaluate two representative LLMs and the impact of
different levels of detail about the fallacy classes provided to the LLM via
prompts. Our experiments and human evaluation show promising results for GPT 4,
while also demonstrating the difficulty of this task.
</summary>
    <author>
      <name>Max Glockner</name>
    </author>
    <author>
      <name>Yufang Hou</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2024 (main)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.00243v1</id>
    <updated>2024-08-30T20:28:21Z</updated>
    <published>2024-08-30T20:28:21Z</published>
    <title>PRADA: Proactive Risk Assessment and Mitigation of Misinformed Demand
  Attacks on Navigational Route Recommendations</title>
    <summary>  Leveraging recent advances in wireless communication, IoT, and AI,
intelligent transportation systems (ITS) played an important role in reducing
traffic congestion and enhancing user experience. Within ITS, navigational
recommendation systems (NRS) are essential for helping users simplify route
choices in urban environments. However, NRS are vulnerable to information-based
attacks that can manipulate both the NRS and users to achieve the objectives of
the malicious entities. This study aims to assess the risks of misinformed
demand attacks, where attackers use techniques like Sybil-based attacks to
manipulate the demands of certain origins and destinations considered by the
NRS. We propose a game-theoretic framework for proactive risk assessment of
demand attacks (PRADA) and treat the interaction between attackers and the NRS
as a Stackelberg game. The attacker is the leader who conveys misinformed
demands, while the NRS is the follower responding to the provided information.
Specifically, we consider the case of local-targeted attacks, in which the
attacker aims to make the NRS recommend the authentic users towards a specific
road that favors certain groups. Our analysis unveils the equivalence between
users' incentive compatibility and Wardrop equilibrium recommendations and
shows that the NRS and its users are at high risk when encountering intelligent
attackers who can significantly alter user routes by strategically fabricating
non-existent demands. To mitigate these risks, we introduce a trust mechanism
that leverages users' confidence in the integrity of the NRS, and show that it
can effectively reduce the impact of misinformed demand attacks. Numerical
experiments are used to corroborate the results and demonstrate a Resilience
Paradox, where locally targeted attacks can sometimes benefit the overall
traffic conditions.
</summary>
    <author>
      <name>Ya-Ting Yang</name>
    </author>
    <author>
      <name>Haozhe Lei</name>
    </author>
    <author>
      <name>Quanyan Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2409.00243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.00243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.22746v1</id>
    <updated>2025-03-26T23:28:21Z</updated>
    <published>2025-03-26T23:28:21Z</published>
    <title>Susceptibility of Large Language Models to User-Driven Factors in
  Medical Queries</title>
    <summary>  Large language models (LLMs) are increasingly used in healthcare, but their
reliability is heavily influenced by user-driven factors such as question
phrasing and the completeness of clinical information. In this study, we
examined how misinformation framing, source authority, model persona, and
omission of key clinical details affect the diagnostic accuracy and reliability
of LLM outputs. We conducted two experiments: one introducing misleading
external opinions with varying assertiveness (perturbation test), and another
removing specific categories of patient information (ablation test). Using
public datasets (MedQA and Medbullets), we evaluated proprietary models
(GPT-4o, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash)
and open-source models (LLaMA 3 8B, LLaMA 3 Med42 8B, DeepSeek R1 8B). All
models were vulnerable to user-driven misinformation, with proprietary models
especially affected by definitive and authoritative language. Assertive tone
had the greatest negative impact on accuracy. In the ablation test, omitting
physical exam findings and lab results caused the most significant performance
drop. Although proprietary models had higher baseline accuracy, their
performance declined sharply under misinformation. These results highlight the
need for well-structured prompts and complete clinical context. Users should
avoid authoritative framing of misinformation and provide full clinical
details, especially for complex cases.
</summary>
    <author>
      <name>Kyung Ho Lim</name>
    </author>
    <author>
      <name>Ujin Kang</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Jin Sung Kim</name>
    </author>
    <author>
      <name>Young-Chul Jung</name>
    </author>
    <author>
      <name>Sangjoon Park</name>
    </author>
    <author>
      <name>Byung-Hoon Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2503.22746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.22746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06010v2</id>
    <updated>2025-07-28T08:37:35Z</updated>
    <published>2025-04-08T13:16:48Z</published>
    <title>Latent Multimodal Reconstruction for Misinformation Detection</title>
    <summary>  Multimodal misinformation, such as miscaptioned images, where captions
misrepresent an image's origin, context, or meaning, poses a growing challenge
in the digital age. To support fact-checkers, researchers have focused on
developing datasets and methods for multimodal misinformation detection (MMD).
Due to the scarcity of large-scale annotated MMD datasets, recent approaches
rely on synthetic training data created via out-of-context pairings or named
entity manipulations (e.g., altering names, dates, or locations). However,
these often yield simplistic examples that lack real-world complexity, limiting
model robustness. Meanwhile, Large Vision-Language Models (LVLMs) remain
underexplored for generating diverse and realistic synthetic data for MMD. To
address, we introduce "Miscaption This!", a collection of LVLM-generated
miscaptioned image datasets. Additionally, we introduce "Latent Multimodal
Reconstruction" (LAMAR), a network trained to reconstruct the embeddings of
truthful captions, providing a strong auxiliary signal to guide detection. We
explore various training strategies (end-to-end vs. large-scale pre-training)
and integration mechanisms (direct, mask, gate, and attention). Extensive
experiments show that models trained on "MisCaption This!" generalize better to
real-world misinformation while LAMAR achieves new state-of-the-art on both
NewsCLIPpings and VERITE benchmarks; highlighting the value of LVLM-generated
data and reconstruction-based networks for advancing MMD. Our code is available
at https://github.com/stevejpapad/miscaptioned-image-reconstruction
</summary>
    <author>
      <name>Stefanos-Iordanis Papadopoulos</name>
    </author>
    <author>
      <name>Christos Koutlis</name>
    </author>
    <author>
      <name>Symeon Papadopoulos</name>
    </author>
    <author>
      <name>Panagiotis C. Petrantonakis</name>
    </author>
    <link href="http://arxiv.org/abs/2504.06010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00008v1</id>
    <updated>2025-04-16T22:27:10Z</updated>
    <published>2025-04-16T22:27:10Z</published>
    <title>A Scoping Review of Natural Language Processing in Addressing Medically
  Inaccurate Information: Errors, Misinformation, and Hallucination</title>
    <summary>  Objective: This review aims to explore the potential and challenges of using
Natural Language Processing (NLP) to detect, correct, and mitigate medically
inaccurate information, including errors, misinformation, and hallucination. By
unifying these concepts, the review emphasizes their shared methodological
foundations and their distinct implications for healthcare. Our goal is to
advance patient safety, improve public health communication, and support the
development of more reliable and transparent NLP applications in healthcare.
  Methods: A scoping review was conducted following PRISMA guidelines,
analyzing studies from 2020 to 2024 across five databases. Studies were
selected based on their use of NLP to address medically inaccurate information
and were categorized by topic, tasks, document types, datasets, models, and
evaluation metrics.
  Results: NLP has shown potential in addressing medically inaccurate
information on the following tasks: (1) error detection (2) error correction
(3) misinformation detection (4) misinformation correction (5) hallucination
detection (6) hallucination mitigation. However, challenges remain with data
privacy, context dependency, and evaluation standards.
  Conclusion: This review highlights the advancements in applying NLP to tackle
medically inaccurate information while underscoring the need to address
persistent challenges. Future efforts should focus on developing real-world
datasets, refining contextual methods, and improving hallucination management
to ensure reliable and transparent healthcare applications.
</summary>
    <author>
      <name>Zhaoyi Sun</name>
    </author>
    <author>
      <name>Wen-Wai Yim</name>
    </author>
    <author>
      <name>Ozlem Uzuner</name>
    </author>
    <author>
      <name>Fei Xia</name>
    </author>
    <author>
      <name>Meliha Yetisgen</name>
    </author>
    <link href="http://arxiv.org/abs/2505.00008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.01481v1</id>
    <updated>2025-07-02T08:44:09Z</updated>
    <published>2025-07-02T08:44:09Z</published>
    <title>Mapping the interaction between science and misinformation in COVID-19
  tweets</title>
    <summary>  During the COVID-19 pandemic, scientific understanding related to the topic
evolved rapidly. Along with scientific information being discussed widely, a
large circulation of false information, labelled an infodemic by the WHO,
emerged. Here, we study the interaction between misinformation and science on
Twitter (now X) during the COVID-19 pandemic. We built a comprehensive database
of $\sim$407M COVID-19 related tweets and classified the reliability of URLs in
the tweets based on Media Bias/Fact Check. In addition, we use Altmetric data
to see whether a tweet refers to a scientific publication. We find that many
users find that many users share both scientific and unreliable content; out of
the $\sim$1.2M users who share science, $45\%$ also share unreliable content.
Publications that are more frequently shared by users who also share unreliable
content are more likely to be preprints, slightly more often retracted, have
fewer citations, and are published in lower-impact journals on average. Our
findings suggest that misinformation is not related to a ``deficit'' of
science. In addition, our findings raise some critical questions about certain
open science practices and their potential for misuse. Given the fundamental
opposition between science and misinformation, our findings highlight the
necessity for proactive scientific engagement on social media platforms to
counter false narratives during global crises.
</summary>
    <author>
      <name>Lucila G. Alvarez-Zuzek</name>
    </author>
    <author>
      <name>Juan P. Bascur</name>
    </author>
    <author>
      <name>Anna Bertani</name>
    </author>
    <author>
      <name>Riccardo Gallotti</name>
    </author>
    <author>
      <name>Vincent A. Traag</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.01481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.01481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.05939v1</id>
    <updated>2025-07-08T12:38:19Z</updated>
    <published>2025-07-08T12:38:19Z</published>
    <title>Remember Past, Anticipate Future: Learning Continual Multimodal
  Misinformation Detectors</title>
    <summary>  Nowadays, misinformation articles, especially multimodal ones, are widely
spread on social media platforms and cause serious negative effects. To control
their propagation, Multimodal Misinformation Detection (MMD) becomes an active
topic in the community to automatically identify misinformation. Previous MMD
methods focus on supervising detectors by collecting offline data. However, in
real-world scenarios, new events always continually emerge, making MMD models
trained on offline data consistently outdated and ineffective. To address this
issue, training MMD models under online data streams is an alternative,
inducing an emerging task named continual MMD. Unfortunately, it is hindered by
two major challenges. First, training on new data consistently decreases the
detection performance on past data, named past knowledge forgetting. Second,
the social environment constantly evolves over time, affecting the
generalization on future data. To alleviate these challenges, we propose to
remember past knowledge by isolating interference between event-specific
parameters with a Dirichlet process-based mixture-of-expert structure, and
anticipate future environmental distributions by learning a continuous-time
dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.
Extensive experiments demonstrate that DAEDCMD can consistently and
significantly outperform the compared methods, including six MMD baselines and
three continual learning methods.
</summary>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Ximing Li</name>
    </author>
    <author>
      <name>Mengzhe Ye</name>
    </author>
    <author>
      <name>Changchun Li</name>
    </author>
    <author>
      <name>Bo Fu</name>
    </author>
    <author>
      <name>Jianfeng Qu</name>
    </author>
    <author>
      <name>Lin Yuanbo Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM MM 2025. 10 pages, 6 figures. Code:
  https://github.com/wangbing1416/DAEDCMD</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.05939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.05939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0336v1</id>
    <updated>2012-12-03T09:48:02Z</updated>
    <published>2012-12-03T09:48:02Z</published>
    <title>Analytical model of misinformation of a social network node</title>
    <summary>  This paper presents the research of the influence of cognitive, behavioral,
representational factors on the susceptibility of the participants in social
networks to misinformation, as well as on the activity of the nodes in this
regard. The importance of this research consists of method of blocking the
propaganda. This is very important because when people involuntarily acquire
information some of them experience an undesired change in their social
attitude. Such phenomena typically lead towards the information warfare. A
model was developed during this research for calculating the level of
misinformation of the social network participant (network node) based on the
model of iterative learning process.
</summary>
    <author>
      <name>Yuri Monakhov</name>
    </author>
    <author>
      <name>Maria Medvednikova</name>
    </author>
    <author>
      <name>Konstantin Abramov</name>
    </author>
    <author>
      <name>Natalia Kostina</name>
    </author>
    <author>
      <name>Roman Malyshev</name>
    </author>
    <author>
      <name>Makarov Oleg</name>
    </author>
    <author>
      <name>Irina Semenova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09435v1</id>
    <updated>2016-09-29T17:31:44Z</updated>
    <published>2016-09-29T17:31:44Z</published>
    <title>On the statistical properties of viral misinformation in online social
  media</title>
    <summary>  The massive diffusion of online social media allows for the rapid and
uncontrolled spreading of conspiracy theories, hoaxes, unsubstantiated claims,
and false news. Such an impressive amount of misinformation can influence
policy preferences and encourage behaviors strongly divergent from recommended
practices. In this paper, we study the statistical properties of viral
misinformation in online social media. By means of methods belonging to Extreme
Value Theory, we show that the number of extremely viral posts over time
follows a homogeneous Poisson process, and that the interarrival times between
such posts are independent and identically distributed, following an
exponential distribution. Moreover, we characterize the uncertainty around the
rate parameter of the Poisson process through Bayesian methods. Finally, we are
able to derive the predictive posterior probability distribution of the number
of posts exceeding a certain threshold of shares over a finite interval of
time.
</summary>
    <author>
      <name>Alessandro Bessi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2016.11.012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2016.11.012" rel="related"/>
    <link href="http://arxiv.org/abs/1609.09435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01213v1</id>
    <updated>2017-05-03T01:03:23Z</updated>
    <published>2017-05-03T01:03:23Z</published>
    <title>Informative and misinformative interactions in a school of fish</title>
    <summary>  It is generally accepted that, when moving in groups, animals process
information to coordinate their motion. Recent studies have begun to apply
rigorous methods based on Information Theory to quantify such distributed
computation. Following this perspective, we use transfer entropy to quantify
dynamic information flows locally in space and time across a school of fish
during directional changes around a circular tank, i.e. U-turns. This analysis
reveals peaks in information flows during collective U-turns and identifies two
different flows: an informative flow (positive transfer entropy) based on fish
that have already turned about fish that are turning, and a misinformative flow
(negative transfer entropy) based on fish that have not turned yet about fish
that are turning. We also reveal that the information flows are related to
relative position and alignment between fish, and identify spatial patterns of
information and misinformation cascades. This study offers several
methodological contributions and we expect further application of these
methodologies to reveal intricacies of self-organisation in other animal groups
and active matter in general.
</summary>
    <author>
      <name>Emanuele Crosato</name>
    </author>
    <author>
      <name>Li Jiang</name>
    </author>
    <author>
      <name>Valentin Lecheval</name>
    </author>
    <author>
      <name>Joseph T. Lizier</name>
    </author>
    <author>
      <name>X. Rosalind Wang</name>
    </author>
    <author>
      <name>Pierre Tichit</name>
    </author>
    <author>
      <name>Guy Theraulaz</name>
    </author>
    <author>
      <name>Mikhail Prokopenko</name>
    </author>
    <link href="http://arxiv.org/abs/1705.01213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09494v2</id>
    <updated>2017-07-08T13:28:55Z</updated>
    <published>2017-06-28T21:54:16Z</published>
    <title>Misinformation spreading on Facebook</title>
    <summary>  Social media are pervaded by unsubstantiated or untruthful rumors, that
contribute to the alarming phenomenon of misinformation. The widespread
presence of a heterogeneous mass of information sources may affect the
mechanisms behind the formation of public opinion. Such a scenario is a florid
environment for digital wildfires when combined with functional illiteracy,
information overload, and confirmation bias. In this essay, we focus on a
collection of works aiming at providing quantitative evidence about the
cognitive determinants behind misinformation and rumor spreading. We account
for users' behavior with respect to two distinct narratives: a) conspiracy and
b) scientific information sources. In particular, we analyze Facebook data on a
time span of five years in both the Italian and the US context, and measure
users' response to i) information consistent with one's narrative, ii) troll
contents, and iii) dissenting information e.g., debunking attempts. Our
findings suggest that users tend to a) join polarized communities sharing a
common narrative (echo chambers), b) acquire information confirming their
beliefs (confirmation bias) even if containing false claims, and c) ignore
dissenting information.
</summary>
    <author>
      <name>Fabiana Zollo</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09494v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09494v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00557v1</id>
    <updated>2018-09-03T11:27:28Z</updated>
    <published>2018-09-03T11:27:28Z</published>
    <title>Fake Cures: User-centric Modeling of Health Misinformation in Social
  Media</title>
    <summary>  Social media's unfettered access has made it an important venue for health
discussion and a resource for patients and their loved ones. However, the
quality of the information available, as well as the motivations of its
posters, has been questioned. This work examines the individuals on social
media that are posting questionable health-related information, and in
particular promoting cancer treatments which have been shown to be ineffective
(making it a kind of misinformation, willful or not). Using a multi-stage user
selection process, we study 4,212 Twitter users who have posted about one of
139 such "treatments", and compare them to a baseline of users generally
interested in cancer. Considering features capturing user attributes, writing
style, and sentiment, we build a classifier which is able to identify users
prone to propagate such misinformation at an accuracy of over 90%, providing a
potential tool for public health officials to identify such individuals for
preventive intervention.
</summary>
    <author>
      <name>Amira Ghenai</name>
    </author>
    <author>
      <name>Yelena Mejova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Conference on Computer Supported Cooperative Work and Social
  Computing (CSCW) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.05521v2</id>
    <updated>2018-11-21T00:22:24Z</updated>
    <published>2018-09-14T17:46:58Z</published>
    <title>Defending Elections Against Malicious Spread of Misinformation</title>
    <summary>  The integrity of democratic elections depends on voters' access to accurate
information. However, modern media environments, which are dominated by social
media, provide malicious actors with unprecedented ability to manipulate
elections via misinformation, such as fake news. We study a zero-sum game
between an attacker, who attempts to subvert an election by propagating a fake
new story or other misinformation over a set of advertising channels, and a
defender who attempts to limit the attacker's impact. Computing an equilibrium
in this game is challenging as even the pure strategy sets of players are
exponential. Nevertheless, we give provable polynomial-time approximation
algorithms for computing the defender's minimax optimal strategy across a range
of settings, encompassing different population structures as well as models of
the information available to each player. Experimental results confirm that our
algorithms provide near-optimal defender strategies and showcase variations in
the difficulty of defending elections depending on the resources and knowledge
available to the defender.
</summary>
    <author>
      <name>Bryan Wilder</name>
    </author>
    <author>
      <name>Yevgeniy Vorobeychik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of paper accepted to AAAI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.05521v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05521v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00435v2</id>
    <updated>2019-09-13T18:04:08Z</updated>
    <published>2019-06-30T19:14:17Z</published>
    <title>YouTube Chatter: Understanding Online Comments Discourse on
  Misinformative and Political YouTube Videos</title>
    <summary>  We conduct a preliminary analysis of comments on political YouTube content
containing misinformation in comparison to comments on trustworthy or
apolitical videos, labelling the bias and factual ratings of our channels
according to Media Bias Fact Check where applicable. One of our most
interesting discoveries is that especially-polarized or misinformative
political channels (Left-Bias, Right-Bias, PragerU, Conspiracy-Pseudoscience,
and Questionable Source) generate 7.5x more comments per view and 10.42x more
replies per view than apolitical or Pro-Science channels; in particular,
Conspiracy-Pseudoscience and Questionable Sources generate 8.3x more comments
per view and 11.0x more replies per view than apolitical and Pro-Science
channels. We also compared average thread lengths, average comment lengths, and
profanity rates across channels, and present simple machine learning
classifiers for predicting the bias category of a video based on these
statistics.
</summary>
    <author>
      <name>Aarash Heydari</name>
    </author>
    <author>
      <name>Janny Zhang</name>
    </author>
    <author>
      <name>Shaan Appel</name>
    </author>
    <author>
      <name>Xinyi Wu</name>
    </author>
    <author>
      <name>Gireeja Ranade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 23 figures. Primary contributors: Aarash Heydari and Janny
  Zhang. These authors contributed equally to the work</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.00435v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00435v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03654v1</id>
    <updated>2019-09-09T06:45:07Z</updated>
    <published>2019-09-09T06:45:07Z</published>
    <title>The Future of Misinformation Detection: New Perspectives and Trends</title>
    <summary>  The massive spread of misinformation in social networks has become a global
risk, implicitly influencing public opinion and threatening social/political
development. Misinformation detection (MID) has thus become a surging research
topic in recent years. As a promising and rapid developing research field, we
find that many efforts have been paid to new research problems and approaches
of MID. Therefore, it is necessary to give a comprehensive review of the new
research trends of MID. We first give a brief review of the literature history
of MID, based on which we present several new research challenges and
techniques of it, including early detection, detection by multimodal data
fusion, and explanatory detection. We further investigate the extraction and
usage of various crowd intelligence in MID, which paves a promising way to
tackle MID challenges. Finally, we give our own views on the open issues and
future research directions of MID, such as model adaptivity/generality to new
events, embracing of novel machine learning models, explanatory detection
models, and so on.
</summary>
    <author>
      <name>Bin Guo</name>
    </author>
    <author>
      <name>Yasan Ding</name>
    </author>
    <author>
      <name>Lina Yao</name>
    </author>
    <author>
      <name>Yunji Liang</name>
    </author>
    <author>
      <name>Zhiwen Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM Computing Surveys</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.03654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.03159v1</id>
    <updated>2020-10-07T04:55:34Z</updated>
    <published>2020-10-07T04:55:34Z</published>
    <title>Where Are the Facts? Searching for Fact-checked Information to Alleviate
  the Spread of Fake News</title>
    <summary>  Although many fact-checking systems have been developed in academia and
industry, fake news is still proliferating on social media. These systems
mostly focus on fact-checking but usually neglect online users who are the main
drivers of the spread of misinformation. How can we use fact-checked
information to improve users' consciousness of fake news to which they are
exposed? How can we stop users from spreading fake news? To tackle these
questions, we propose a novel framework to search for fact-checking articles,
which address the content of an original tweet (that may contain
misinformation) posted by online users. The search can directly warn fake news
posters and online users (e.g. the posters' followers) about misinformation,
discourage them from spreading fake news, and scale up verified content on
social media. Our framework uses both text and images to search for
fact-checking articles, and achieves promising results on real-world datasets.
Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.
</summary>
    <author>
      <name>Nguyen Vo</name>
    </author>
    <author>
      <name>Kyumin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full paper, EMNLP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.03159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.03159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.06019v2</id>
    <updated>2020-10-14T15:50:22Z</updated>
    <published>2020-10-12T20:43:41Z</published>
    <title>Probabilistic Social Learning Improves the Public's Detection of
  Misinformation</title>
    <summary>  The digital spread of misinformation is one of the leading threats to
democracy, public health, and the global economy. Popular strategies for
mitigating misinformation include crowdsourcing, machine learning, and media
literacy programs that require social media users to classify news in binary
terms as either true or false. However, research on peer influence suggests
that framing decisions in binary terms can amplify judgment errors and limit
social learning, whereas framing decisions in probabilistic terms can reliably
improve judgments. In this preregistered experiment, we compare online peer
networks that collaboratively evaluate the veracity of news by communicating
either binary or probabilistic judgments. Exchanging probabilistic estimates of
news veracity substantially improved individual and group judgments, with the
effect of eliminating polarization in news evaluation. By contrast, exchanging
binary classifications reduced social learning and entrenched polarization. The
benefits of probabilistic social learning are robust to participants'
education, gender, race, income, religion, and partisanship.
</summary>
    <author>
      <name>Douglas Guilbeault</name>
    </author>
    <author>
      <name>Samuel Woolley</name>
    </author>
    <author>
      <name>Joshua Becker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0247487</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0247487" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.06019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.06019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09029v2</id>
    <updated>2021-06-13T02:17:47Z</updated>
    <published>2020-10-18T16:52:27Z</published>
    <title>CHECKED: Chinese COVID-19 Fake News Dataset</title>
    <summary>  COVID-19 has impacted all lives. To maintain social distancing and avoiding
exposure, works and lives have gradually moved online. Under this trend, social
media usage to obtain COVID-19 news has increased. Also, misinformation on
COVID-19 is frequently spread on social media. In this work, we develop
CHECKED, the first Chinese dataset on COVID-19 misinformation. CHECKED provides
a total 2,104 verified microblogs related to COVID-19 from December 2019 to
August 2020, identified by using a specific list of keywords. Correspondingly,
CHECKED includes 1,868,175 reposts, 1,185,702 comments, and 56,852,736 likes
that reveal how these verified microblogs are spread and reacted on Weibo. The
dataset contains a rich set of multimedia information for each microblog
including ground-truth label, textual, visual, temporal, and network
information. Extensive experiments have been conducted to analyze CHECKED data
and to provide benchmark results for well-established methods when predicting
fake news using CHECKED. We hope that CHECKED can facilitate studies that
target misinformation on coronavirus. The dataset is available at
https://github.com/cyang03/CHECKED.
</summary>
    <author>
      <name>Chen Yang</name>
    </author>
    <author>
      <name>Xinyi Zhou</name>
    </author>
    <author>
      <name>Reza Zafarani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Social Network Analysis and Mining (SNAM)</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.09029v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09029v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13387v2</id>
    <updated>2021-07-10T06:08:44Z</updated>
    <published>2020-10-26T07:33:28Z</published>
    <title>Check Mate: Prioritizing User Generated Multi-Media Content for
  Fact-Checking</title>
    <summary>  Volume of content and misinformation on social media is rapidly increasing.
There is a need for systems that can support fact checkers by prioritizing
content that needs to be fact checked. Prior research on prioritizing content
for fact-checking has focused on news media articles, predominantly in English
language. Increasingly, misinformation is found in user-generated content. In
this paper we present a novel dataset that can be used to prioritize
check-worthy posts from multi-media content in Hindi. It is unique in its 1)
focus on user generated content, 2) language and 3) accommodation of
multi-modality in social media posts. In addition, we also provide metadata for
each post such as number of shares and likes of the post on ShareChat, a
popular Indian social media platform, that allows for correlative analysis
around virality and misinformation. The data is accessible on Zenodo
(https://zenodo.org/record/4032629) under Creative Commons Attribution License
(CC BY 4.0).
</summary>
    <author>
      <name>Tarunima Prabhakar</name>
    </author>
    <author>
      <name>Anushree Gupta</name>
    </author>
    <author>
      <name>Kruttika Nadig</name>
    </author>
    <author>
      <name>Denny George</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 13 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International AAAI Conference on Web and Social
  Media, Volume 15(1), 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.13387v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13387v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.09536v1</id>
    <updated>2020-12-17T12:17:55Z</updated>
    <published>2020-12-17T12:17:55Z</published>
    <title>Conspiracy Machines -- The Role of Social Bots during the COVID-19
  Infodemic</title>
    <summary>  The omnipresent COVID-19 pandemic gave rise to a parallel spreading of
misinformation, also referred to as an Infodemic. Consequently, social media
have become targets for the application of social bots, that is, algorithms
that mimic human behaviour. Their ability to exert influence on social media
can be exploited by amplifying misinformation, rumours, or conspiracy theories
which might be harmful to society and the mastery of the pandemic. By applying
social bot detection and content analysis techniques, this study aims to
determine the extent to which social bots interfere with COVID- 19 discussions
on Twitter. A total of 78 presumptive bots were detected within a sample of
542,345 users. The analysis revealed that bot-like users who disseminate
misinformation, at the same time, intersperse news from renowned sources. The
findings of this research provide implications for improved bot detection and
managing potential threats through social bots during ongoing and future
crises.
</summary>
    <author>
      <name>Julian Marx</name>
    </author>
    <author>
      <name>Felix Br√ºnker</name>
    </author>
    <author>
      <name>Milad Mirbabaie</name>
    </author>
    <author>
      <name>Eric Hochstrate</name>
    </author>
    <link href="http://arxiv.org/abs/2012.09536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.11920v1</id>
    <updated>2019-11-27T02:33:24Z</updated>
    <published>2019-11-27T02:33:24Z</published>
    <title>Warning Signs in Communicating the Machine Learning Detection Results of
  Misinformation with Individuals</title>
    <summary>  With the prevalence of misinformation online, researchers have focused on
developing various machine learning algorithms to detect fake news. However,
users' perception of machine learning outcomes and related behaviors have been
widely ignored. Hence, this paper proposed to bridge this gap by studying how
to pass the detection results of machine learning to the users, and aid their
decisions in handling misinformation. An online experiment was conducted, to
evaluate the effect of the proposed machine learning warning sign against a
control condition. We examined participants' detection and sharing of news. The
data showed that warning sign's effects on participants' trust toward the fake
news were not significant. However, we found that people's uncertainty about
the authenticity of the news dropped with the presence of the machine learning
warning sign. We also found that social media experience had effects on users'
trust toward the fake news, and age and social media experience had effects on
users' sharing decision. Therefore, the results indicate that there are many
factors worth studying that affect people's trust in the news. Moreover, the
warning sign in communicating machine learning detection results is different
from ordinary warnings and needs more detailed research and design. These
findings hold important implications for the design of machine learning
warnings.
</summary>
    <author>
      <name>Limeng Cui</name>
    </author>
    <link href="http://arxiv.org/abs/1911.11920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.09600v1</id>
    <updated>2020-04-20T19:56:48Z</updated>
    <published>2020-04-20T19:56:48Z</published>
    <title>Why do People Share Misinformation during the COVID-19 Pandemic?</title>
    <summary>  The World Health Organization have emphasised that misinformation - spreading
rapidly through social media - poses a serious threat to the COVID-19 response.
Drawing from theories of health perception and cognitive load, we develop and
test a research model hypothesizing why people share unverified COVID-19
information through social media. Our findings suggest a person's trust in
online information and perceived information overload are strong predictors of
unverified information sharing. Furthermore, these factors, along with a
person's perceived COVID-19 severity and vulnerability influence cyberchondria.
Females were significantly more likely to suffer from cyberchondria, however,
males were more likely to share news without fact checking their source. Our
findings suggest that to mitigate the spread of COVID-19 misinformation and
cyberchondria, measures should be taken to enhance a healthy skepticism of
health news while simultaneously guarding against information overload.
</summary>
    <author>
      <name>Samuli Laato</name>
    </author>
    <author>
      <name>A. K. M. Najmul Islam</name>
    </author>
    <author>
      <name>Muhammad Nazrul Islam</name>
    </author>
    <author>
      <name>Eoin Whelan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/0960085X.2020.1770632</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/0960085X.2020.1770632" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Journal of Information Systems (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.09600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04077v1</id>
    <updated>2021-04-01T22:37:34Z</updated>
    <published>2021-04-01T22:37:34Z</published>
    <title>Two Truths and a Lie: Exploring Soft Moderation of COVID-19
  Misinformation with Amazon Alexa</title>
    <summary>  In this paper, we analyzed the perceived accuracy of COVID-19 vaccine Tweets
when they were spoken back by a third-party Amazon Alexa skill. We mimicked the
soft moderation that Twitter applies to COVID-19 misinformation content in both
forms of warning covers and warning tags to investigate whether the third-party
skill could affect how and when users heed these warnings. The results from a
304-participant study suggest that the spoken back warning covers may not work
as intended, even when converted from text to speech. We controlled for
COVID-19 vaccination hesitancy and political leanings and found that the
vaccination hesitant Alexa users ignored any type of warning as long as the
Tweets align with their personal beliefs. The politically independent users
trusted Alexa less then their politically-laden counterparts and that helped
them accurately perceiving truthful COVID-19 information. We discuss soft
moderation adaptations for voice assistants to achieve the intended effect of
curbing COVID-19 misinformation.
</summary>
    <author>
      <name>Donald Gover</name>
    </author>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2104.00779</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.02775v1</id>
    <updated>2021-07-06T17:37:56Z</updated>
    <published>2021-07-06T17:37:56Z</published>
    <title>Countering Misinformation on Social Media Through Educational
  Interventions: Evidence from a Randomized Experiment in Pakistan</title>
    <summary>  Fake news is a growing problem in developing countries with potentially
far-reaching consequences. We conduct a randomized experiment in urban Pakistan
to evaluate the effectiveness of two educational interventions to counter
misinformation among low-digital literacy populations. We do not find a
significant effect of video-based general educational messages about
misinformation. However, when such messages are augmented with personalized
feedback based on individuals' past engagement with fake news, we find an
improvement of 0.14 standard deviations in identifying fake news. We also find
negative but insignificant effects on identifying true news, driven by female
respondents. Our results suggest that educational interventions can enable
information discernment but their effectiveness critically depends on how well
their features and delivery are customized for the population of interest.
</summary>
    <author>
      <name>Ayesha Ali</name>
    </author>
    <author>
      <name>Ihsan Ayyub Qazi</name>
    </author>
    <link href="http://arxiv.org/abs/2107.02775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.02775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.04494v2</id>
    <updated>2020-02-16T14:23:28Z</updated>
    <published>2020-02-11T15:49:32Z</published>
    <title>The Rumour Mill: Making the Spread of Misinformation Explicit and
  Tangible</title>
    <summary>  Misinformation spread presents a technological and social threat to society.
With the advance of AI-based language models, automatically generated texts
have become difficult to identify and easy to create at scale. We present "The
Rumour Mill", a playful art piece, designed as a commentary on the spread of
rumours and automatically-generated misinformation. The mill is a tabletop
interactive machine, which invites a user to experience the process of creating
believable text by interacting with different tangible controls on the mill.
The user manipulates visible parameters to adjust the genre and type of an
automatically generated text rumour. The Rumour Mill is a physical
demonstration of the state of current technology and its ability to generate
and manipulate natural language text, and of the act of starting and spreading
rumours.
</summary>
    <author>
      <name>Nanna Inie</name>
    </author>
    <author>
      <name>Jeanette Falk Olesen</name>
    </author>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3334480.3383159</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3334480.3383159" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CHI 2020 Interactivity</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.04494v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04494v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.14146v2</id>
    <updated>2021-02-15T02:26:25Z</updated>
    <published>2020-11-28T15:30:14Z</published>
    <title>Towards Combating Pandemic-related Misinformation in Social Media</title>
    <summary>  Conventional preventive measures during pandemic include social distancing
and lockdown. Such measures in the time of social media brought about a new set
of challenges - vulnerability to the toxic impact of online misinformation is
high. A case in point is the prevailing COVID-19; as the virus propagates, so
does the associated misinformation and fake news about it leading to infodemic.
Since the outbreak, there has been a surge of studies investigating various
aspects of the pandemic. Of interest to this chapter include studies centring
on datasets from online social media platforms where the bulk of the public
discourse happen. Consequently, the main goal is to support the fight against
negative infodemic by (1) contributing a diverse set of curated relevant
datasets (2) recommending relevant areas to study using the datasets (3)
discussion on how relevant datasets, strategies and state-of-the-art IT tools
can be leveraged in managing the pandemic.
</summary>
    <author>
      <name>Isa Inuwa-Dutse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.14146v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.14146v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.15703v1</id>
    <updated>2021-09-19T15:49:25Z</updated>
    <published>2021-09-19T15:49:25Z</published>
    <title>Navigating the Kaleidoscope of COVID-19 Misinformation Using Deep
  Learning</title>
    <summary>  Irrespective of the success of the deep learning-based mixed-domain transfer
learning approach for solving various Natural Language Processing tasks, it
does not lend a generalizable solution for detecting misinformation from
COVID-19 social media data. Due to the inherent complexity of this type of
data, caused by its dynamic (context evolves rapidly), nuanced (misinformation
types are often ambiguous), and diverse (skewed, fine-grained, and overlapping
categories) nature, it is imperative for an effective model to capture both the
local and global context of the target domain. By conducting a systematic
investigation, we show that: (i) the deep Transformer-based pre-trained models,
utilized via the mixed-domain transfer learning, are only good at capturing the
local context, thus exhibits poor generalization, and (ii) a combination of
shallow network-based domain-specific models and convolutional neural networks
can efficiently extract local as well as global context directly from the
target data in a hierarchical fashion, enabling it to offer a more
generalizable solution.
</summary>
    <author>
      <name>Yuanzhi Chen</name>
    </author>
    <author>
      <name>Mohammad Rashedul Hasan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 2021 Conference on Empirical Methods in Natural Language
  Processing (EMNLP)</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.15703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.15703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06080v1</id>
    <updated>2021-12-11T21:57:57Z</updated>
    <published>2021-12-11T21:57:57Z</published>
    <title>UPV at TREC Health Misinformation Track 2021 Ranking with SBERT and
  Quality Estimators</title>
    <summary>  Health misinformation on search engines is a significant problem that could
negatively affect individuals or public health. To mitigate the problem, TREC
organizes a health misinformation track. This paper presents our submissions to
this track. We use a BM25 and a domain-specific semantic search engine for
retrieving initial documents. Later, we examine a health news schema for
quality assessment and apply it to re-rank documents. We merge the scores from
the different components by using reciprocal rank fusion. Finally, we discuss
the results and conclude with future works.
</summary>
    <author>
      <name>Ipek Baris Schlicht</name>
    </author>
    <author>
      <name>Angel Felipe Magnoss√£o de Paula</name>
    </author>
    <author>
      <name>Paolo Rosso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; presented at the TREC 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.06080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.08594v2</id>
    <updated>2022-05-03T00:51:02Z</updated>
    <published>2021-12-16T03:37:20Z</published>
    <title>Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal
  Misinformation</title>
    <summary>  Detecting out-of-context media, such as "mis-captioned" images on Twitter, is
a relevant problem, especially in domains of high public significance. In this
work we aim to develop defenses against such misinformation for the topics of
Climate Change, COVID-19, and Military Vehicles. We first present a large-scale
multimodal dataset with over 884k tweets relevant to these topics. Next, we
propose a detection method, based on the state-of-the-art CLIP model, that
leverages automatically generated hard image-text mismatches. While this
approach works well on our automatically constructed out-of-context tweets, we
aim to validate its usefulness on data representative of the real world. Thus,
we test it on a set of human-generated fakes created by mimicking in-the-wild
misinformation. We achieve an 11% detection improvement in a high precision
regime over a strong baseline. Finally, we share insights about our best model
design and analyze the challenges of this emerging threat.
</summary>
    <author>
      <name>Giscard Biamby</name>
    </author>
    <author>
      <name>Grace Luo</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Anna Rohrbach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.08594v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.08594v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.00094v2</id>
    <updated>2024-01-24T23:43:49Z</updated>
    <published>2022-01-31T21:23:39Z</published>
    <title>Account credibility inference based on news-sharing networks</title>
    <summary>  The spread of misinformation poses a threat to the social media ecosystem.
Effective countermeasures to mitigate this threat require that social media
platforms be able to accurately detect low-credibility accounts even before the
content they share can be classified as misinformation. Here we present methods
to infer account credibility from information diffusion patterns, in particular
leveraging two networks: the reshare network, capturing an account's trust in
other accounts, and the bipartite account-source network, capturing an
account's trust in media sources. We extend network centrality measures and
graph embedding techniques, systematically comparing these algorithms on data
from diverse contexts and social media platforms. We demonstrate that both
kinds of trust networks provide useful signals for estimating account
credibility. Some of the proposed methods yield high accuracy, providing
promising solutions to promote the dissemination of reliable information in
online communities. Two kinds of homophily emerge from our results: accounts
tend to have similar credibility if they reshare each other's content or share
content from similar sources. Our methodology invites further investigation
into the relationship between accounts and news sources to better characterize
misinformation spreaders.
</summary>
    <author>
      <name>Bao Tran Truong</name>
    </author>
    <author>
      <name>Oliver Melbourne Allen</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <link href="http://arxiv.org/abs/2202.00094v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.00094v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.13000v1</id>
    <updated>2022-08-27T12:55:58Z</updated>
    <published>2022-08-27T12:55:58Z</published>
    <title>YouTube COVID-19 Vaccine Misinformation on Twitter: Platform
  Interactions and Moderation Blind Spots</title>
    <summary>  While most social media companies have attempted to address the challenge of
COVID-19 misinformation, the success of those policies is difficult to assess,
especially when focusing on individual platforms. This study explores the
relationship between Twitter and YouTube in spreading COVID-19 vaccine-related
misinformation through a mixed-methods approach to analyzing a collection of
tweets in 2021 sharing YouTube videos where those Twitter accounts had also
linked to deleted YouTube videos. Principal components, cluster and network
analyses are used to group the videos and tweets into interpretable groups by
shared tweet dates, terms and sharing patterns; content analysis is employed to
assess the orientation of tweets and videos to COVID-19 messages. From this we
observe that a preponderance of anti-vaccine messaging remains among users who
previously shared suspect information, in which a dissident political framing
dominates, and which suggests moderation policy inefficacy where the platforms
interact.
</summary>
    <author>
      <name>David S. Axelrod</name>
    </author>
    <author>
      <name>Brian P. Harper</name>
    </author>
    <author>
      <name>John C. Paolillo</name>
    </author>
    <link href="http://arxiv.org/abs/2208.13000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.13000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.12571v1</id>
    <updated>2022-11-22T20:37:32Z</updated>
    <published>2022-11-22T20:37:32Z</published>
    <title>"Coherent Mode" for the World's Public Square</title>
    <summary>  Systems for large scale deliberation have resolved polarized issues and
shifted agenda setting into the public's hands. These systems integrate
bridging-based ranking algorithms - including group informed consensus
implemented in Polis and the continuous matrix factorization approach
implemented by Twitter Birdwatch - making it possible to highlight statements
which enjoy broad support from a diversity of opinion groups.
  Polis has been productively employed to foster more constructive political
deliberation at nation scale in law making exercises. Twitter Birdwatch is
implemented with the intention of addressing misinformation in the global
public square. From one perspective, Twitter Birdwatch can be viewed as an
anti-misinformation system which has deliberative aspects. But it can also be
viewed as a first step towards a generalized deliberative system, using
Twitter's misinformation problem as a proving ground.
  In this paper, we propose that Twitter could adapt Birdwatch to produce maps
of public opinion. We describe a system in five parts for generalizing
Birdwatch: activation of a deliberative system and topic selection, population
sampling and the role of expert networks, deliberation, reporting interpretable
results and finally distribution of the results to the public and those in
power.
</summary>
    <author>
      <name>Colin Megill</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Computational Democracy Project</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth Barry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Computational Democracy Project</arxiv:affiliation>
    </author>
    <author>
      <name>Christopher Small</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Computational Democracy Project</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2211.12571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.12571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.06696v1</id>
    <updated>2022-12-13T16:09:19Z</updated>
    <published>2022-12-13T16:09:19Z</published>
    <title>A Comparative Evaluation of Interventions Against Misinformation:
  Augmenting the WHO Checklist</title>
    <summary>  During the COVID-19 pandemic, the World Health Organization provided a
checklist to help people distinguish between accurate and misinformation. In
controlled experiments in the United States and Germany, we investigated the
utility of this ordered checklist and designed an interactive version to lower
the cost of acting on checklist items. Across interventions, we observe
non-trivial differences in participants' performance in distinguishing accurate
and misinformation between the two countries and discuss some possible reasons
that may predict the future helpfulness of the checklist in different
environments. The checklist item that provides source labels was most
frequently followed and was considered most helpful. Based on our empirical
findings, we recommend practitioners focus on providing source labels rather
than interventions that support readers performing their own fact-checks, even
though this recommendation may be influenced by the WHO's chosen order. We
discuss the complexity of providing such source labels and provide design
recommendations.
</summary>
    <author>
      <name>Hendrik Heuer</name>
    </author>
    <author>
      <name>Elena Leah Glassman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3491102.3517717</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3491102.3517717" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at CHI '22, Conference on Human Factors in Computing
  Systems, April 29-May 5, 2022, New Orleans, LA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.06696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.03301v1</id>
    <updated>2023-01-09T12:43:58Z</updated>
    <published>2023-01-09T12:43:58Z</published>
    <title>Deep Breath: A Machine Learning Browser Extension to Tackle Online
  Misinformation</title>
    <summary>  Over the past decade, the media landscape has seen a radical shift. As more
of the public stay informed of current events via online sources, competition
has grown as outlets vie for attention. This competition has prompted some
online outlets to publish sensationalist and alarmist content to grab readers'
attention. Such practices may threaten democracy by distorting the truth and
misleading readers about the nature of events. This paper proposes a novel
system for detecting, processing, and warning users about misleading content
online to combat the threats posed by misinformation. By training a machine
learning model on an existing dataset of 32,000 clickbait news article
headlines, the model predicts how sensationalist a headline is and then
interfaces with a web browser extension which constructs a unique content
warning notification based on existing design principles and incorporates the
models' prediction. This research makes a novel contribution to machine
learning and human-centred security with promising findings for future
research. By warning users when they may be viewing misinformation, it is
possible to prevent spontaneous reactions, helping users to take a deep breath
and approach online media with a clear mind.
</summary>
    <author>
      <name>Marc Kydd</name>
    </author>
    <author>
      <name>Lynsay A. Shepherd</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.03301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.11318v1</id>
    <updated>2023-04-22T05:20:58Z</updated>
    <published>2023-04-22T05:20:58Z</published>
    <title>A Semi-Supervised Framework for Misinformation Detection</title>
    <summary>  The spread of misinformation in social media outlets has become a prevalent
societal problem and is the cause of many kinds of social unrest. Curtailing
its prevalence is of great importance and machine learning has shown
significant promise. However, there are two main challenges when applying
machine learning to this problem. First, while much too prevalent in one
respect, misinformation, actually, represents only a minor proportion of all
the postings seen on social media. Second, labeling the massive amount of data
necessary to train a useful classifier becomes impractical. Considering these
challenges, we propose a simple semi-supervised learning framework in order to
deal with extreme class imbalances that has the advantage, over other
approaches, of using actual rather than simulated data to inflate the minority
class. We tested our framework on two sets of Covid-related Twitter data and
obtained significant improvement in F1-measure on extremely imbalanced
scenarios, as compared to simple classical and deep-learning data generation
methods such as SMOTE, ADASYN, or GAN-based data generation.
</summary>
    <author>
      <name>Yueyang Liu</name>
    </author>
    <author>
      <name>Zois Boukouvalas</name>
    </author>
    <author>
      <name>Nathalie Japkowicz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-88942-5_5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-88942-5_5" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Soares, C., Torgo, L. (eds) Discovery Science. DS 2021.
  Lecture Notes in Computer Science(), vol 12986. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.11318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.13769v1</id>
    <updated>2023-04-26T18:23:16Z</updated>
    <published>2023-04-26T18:23:16Z</published>
    <title>Evaluating Code Metrics in GitHub Repositories Related to Fake News and
  Misinformation</title>
    <summary>  The surge of research on fake news and misinformation in the aftermath of the
2016 election has led to a significant increase in publicly available source
code repositories. Our study aims to systematically analyze and evaluate the
most relevant repositories and their Python source code in this area to improve
awareness, quality, and understanding of these resources within the research
community. Additionally, our work aims to measure the quality and complexity
metrics of these repositories and identify their fundamental features to aid
researchers in advancing the fields knowledge in understanding and preventing
the spread of misinformation on social media. As a result, we found that more
popular fake news repositories and associated papers with higher citation
counts tend to have more maintainable code measures, more complex code paths, a
larger number of lines of code, a higher Halstead effort, and fewer comments.
</summary>
    <author>
      <name>Jason Duran</name>
    </author>
    <author>
      <name>Mostofa Sakib</name>
    </author>
    <author>
      <name>Nasir Eisty</name>
    </author>
    <author>
      <name>Francesca Spezzano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Accepted SERA 2023 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.13769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.13769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.13507v3</id>
    <updated>2023-10-25T18:43:02Z</updated>
    <published>2023-05-22T21:52:24Z</published>
    <title>Multimodal Automated Fact-Checking: A Survey</title>
    <summary>  Misinformation is often conveyed in multiple modalities, e.g. a miscaptioned
image. Multimodal misinformation is perceived as more credible by humans, and
spreads faster than its text-only counterparts. While an increasing body of
research investigates automated fact-checking (AFC), previous surveys mostly
focus on text. In this survey, we conceptualise a framework for AFC including
subtasks unique to multimodal misinformation. Furthermore, we discuss related
terms used in different communities and map them to our framework. We focus on
four modalities prevalent in real-world fact-checking: text, image, audio, and
video. We survey benchmarks and models, and discuss limitations and promising
directions for future research
</summary>
    <author>
      <name>Mubashara Akhtar</name>
    </author>
    <author>
      <name>Michael Schlichtkrull</name>
    </author>
    <author>
      <name>Zhijiang Guo</name>
    </author>
    <author>
      <name>Oana Cocarascu</name>
    </author>
    <author>
      <name>Elena Simperl</name>
    </author>
    <author>
      <name>Andreas Vlachos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP): Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.13507v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.13507v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.02068v3</id>
    <updated>2024-02-03T01:14:57Z</updated>
    <published>2023-08-03T22:42:30Z</published>
    <title>Specious Sites: Tracking the Spread and Sway of Spurious News Stories at
  Scale</title>
    <summary>  Misinformation, propaganda, and outright lies proliferate on the web, with
some narratives having dangerous real-world consequences on public health,
elections, and individual safety. However, despite the impact of
misinformation, the research community largely lacks automated and programmatic
approaches for tracking news narratives across online platforms. In this work,
utilizing daily scrapes of 1,334 unreliable news websites, the large-language
model MPNet, and DP-Means clustering, we introduce a system to automatically
identify and track the narratives spread within online ecosystems. Identifying
52,036 narratives on these 1,334 websites, we describe the most prevalent
narratives spread in 2022 and identify the most influential websites that
originate and amplify narratives. Finally, we show how our system can be
utilized to detect new narratives originating from unreliable news websites and
to aid fact-checkers in more quickly addressing misinformation. We release code
and data at https://github.com/hanshanley/specious-sites.
</summary>
    <author>
      <name>Hans W. A. Hanley</name>
    </author>
    <author>
      <name>Deepak Kumar</name>
    </author>
    <author>
      <name>Zakir Durumeric</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE S&amp;P 2024. Updated Emails</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.02068v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.02068v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.09223v1</id>
    <updated>2023-10-13T16:21:07Z</updated>
    <published>2023-10-13T16:21:07Z</published>
    <title>Automated Claim Matching with Large Language Models: Empowering
  Fact-Checkers in the Fight Against Misinformation</title>
    <summary>  In today's digital era, the rapid spread of misinformation poses threats to
public well-being and societal trust. As online misinformation proliferates,
manual verification by fact checkers becomes increasingly challenging. We
introduce FACT-GPT (Fact-checking Augmentation with Claim matching
Task-oriented Generative Pre-trained Transformer), a framework designed to
automate the claim matching phase of fact-checking using Large Language Models
(LLMs). This framework identifies new social media content that either supports
or contradicts claims previously debunked by fact-checkers. Our approach
employs GPT-4 to generate a labeled dataset consisting of simulated social
media posts. This data set serves as a training ground for fine-tuning more
specialized LLMs. We evaluated FACT-GPT on an extensive dataset of social media
content related to public health. The results indicate that our fine-tuned LLMs
rival the performance of larger pre-trained LLMs in claim matching tasks,
aligning closely with human annotations. This study achieves three key
milestones: it provides an automated framework for enhanced fact-checking;
demonstrates the potential of LLMs to complement human expertise; offers public
resources, including datasets and models, to further research and applications
in the fact-checking domain.
</summary>
    <author>
      <name>Eun Cheol Choi</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <link href="http://arxiv.org/abs/2310.09223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.09223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.11969v3</id>
    <updated>2024-03-18T16:49:59Z</updated>
    <published>2024-01-22T14:17:03Z</published>
    <title>Claim Detection for Automated Fact-checking: A Survey on Monolingual,
  Multilingual and Cross-Lingual Research</title>
    <summary>  Automated fact-checking has drawn considerable attention over the past few
decades due to the increase in the diffusion of misinformation on online
platforms. This is often carried out as a sequence of tasks comprising (i) the
detection of sentences circulating in online platforms which constitute claims
needing verification, followed by (ii) the verification process of those
claims. This survey focuses on the former, by discussing existing efforts
towards detecting claims needing fact-checking, with a particular focus on
multilingual data and methods. This is a challenging and fertile direction
where existing methods are yet far from matching human performance due to the
profoundly challenging nature of the issue. Especially, the dissemination of
information across multiple social platforms, articulated in multiple languages
and modalities demands more generalized solutions for combating misinformation.
Focusing on multilingual misinformation, we present a comprehensive survey of
existing multilingual claim detection research. We present state-of-the-art
multilingual claim detection research categorized into three key factors of the
problem, verifiability, priority, and similarity. Further, we present a
detailed overview of the existing multilingual datasets along with the
challenges and suggest possible future advancements.
</summary>
    <author>
      <name>Rrubaa Panchendrarajan</name>
    </author>
    <author>
      <name>Arkaitz Zubiaga</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nlp.2024.100066</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nlp.2024.100066" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted revision</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.11969v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.11969v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.07338v4</id>
    <updated>2025-03-28T16:53:29Z</updated>
    <published>2024-02-12T00:08:51Z</published>
    <title>Exploring Saliency Bias in Manipulation Detection</title>
    <summary>  The social media-fuelled explosion of fake news and misinformation supported
by tampered images has led to growth in the development of models and datasets
for image manipulation detection. However, existing detection methods mostly
treat media objects in isolation, without considering the impact of specific
manipulations on viewer perception. Forensic datasets are usually analyzed
based on the manipulation operations and corresponding pixel-based masks, but
not on the semantics of the manipulation, i.e., type of scene, objects, and
viewers' attention to scene content. The semantics of the manipulation play an
important role in spreading misinformation through manipulated images. In an
attempt to encourage further development of semantic-aware forensic approaches
to understand visual misinformation, we propose a framework to analyze the
trends of visual and semantic saliency in popular image manipulation datasets
and their impact on detection.
</summary>
    <author>
      <name>Joshua Krinsky</name>
    </author>
    <author>
      <name>Alan Bettis</name>
    </author>
    <author>
      <name>Qiuyu Tang</name>
    </author>
    <author>
      <name>Daniel Moreira</name>
    </author>
    <author>
      <name>Aparna Bharati</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIP51287.2024.10648063</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIP51287.2024.10648063" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: 2024 IEEE International Conference on Image Processing
  (ICIP)</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.07338v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.07338v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.10426v2</id>
    <updated>2024-07-05T00:59:45Z</updated>
    <published>2024-02-16T03:24:56Z</published>
    <title>DELL: Generating Reactions and Explanations for LLM-Based Misinformation
  Detection</title>
    <summary>  Large language models are limited by challenges in factuality and
hallucinations to be directly employed off-the-shelf for judging the veracity
of news articles, where factual accuracy is paramount. In this work, we propose
DELL that identifies three key stages in misinformation detection where LLMs
could be incorporated as part of the pipeline: 1) LLMs could \emph{generate
news reactions} to represent diverse perspectives and simulate user-news
interaction networks; 2) LLMs could \emph{generate explanations} for proxy
tasks (e.g., sentiment, stance) to enrich the contexts of news articles and
produce experts specializing in various aspects of news understanding; 3) LLMs
could \emph{merge task-specific experts} and provide an overall prediction by
incorporating the predictions and confidence scores of varying experts.
Extensive experiments on seven datasets with three LLMs demonstrate that DELL
outperforms state-of-the-art baselines by up to 16.8\% in macro f1-score.
Further analysis reveals that the generated reactions and explanations are
greatly helpful in misinformation detection, while our proposed LLM-guided
expert merging helps produce better-calibrated predictions.
</summary>
    <author>
      <name>Herun Wan</name>
    </author>
    <author>
      <name>Shangbin Feng</name>
    </author>
    <author>
      <name>Zhaoxuan Tan</name>
    </author>
    <author>
      <name>Heng Wang</name>
    </author>
    <author>
      <name>Yulia Tsvetkov</name>
    </author>
    <author>
      <name>Minnan Luo</name>
    </author>
    <link href="http://arxiv.org/abs/2402.10426v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.10426v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.19280v2</id>
    <updated>2024-03-05T14:42:45Z</updated>
    <published>2024-02-29T15:48:11Z</published>
    <title>Mobile Health Text Misinformation Identification Using Mobile Data
  Mining</title>
    <summary>  More than six million people died of the COVID-19 by April 2022. The heavy
casualties have put people on great and urgent alert and people try to find all
kinds of information to keep them from being inflected by the coronavirus. This
research tries to find out whether the mobile health text information sent to
peoples devices is correct as smartphones becoming the major information source
for people. The proposed method uses various mobile information retrieval and
data mining technologies including lexical analysis, stopword elimination,
stemming, and decision trees to classify the mobile health text information to
one of the following classes: (i) true, (ii) fake, (iii) misinformative, (iv)
disinformative, and (v) neutral. Experiment results show the accuracy of the
proposed method is above the threshold value 50 percentage, but is not optimal.
It is because the problem, mobile text misinformation identification, is
intrinsically difficult.
</summary>
    <author>
      <name>Wen-Chen Hu</name>
    </author>
    <author>
      <name>Sanjaikanth E Vadakkethil Somanathan Pillai</name>
    </author>
    <author>
      <name>Abdelrahman Ahmed ElSaid</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4018/IJMDWTFE.311433</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4018/IJMDWTFE.311433" rel="related"/>
    <link href="http://arxiv.org/abs/2402.19280v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.19280v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.01646v1</id>
    <updated>2024-03-03T23:54:51Z</updated>
    <published>2024-03-03T23:54:51Z</published>
    <title>TweetInfo: An Interactive System to Mitigate Online Harm</title>
    <summary>  The increase in active users on social networking sites (SNSs) has also
observed an increase in harmful content on social media sites. Harmful content
is described as an inappropriate activity to harm or deceive an individual or a
group of users. Alongside existing methods to detect misinformation and hate
speech, users still need to be well-informed about the harmfulness of the
content on SNSs. This study proposes a user-interactive system TweetInfo for
mitigating the consumption of harmful content by providing metainformation
about the posts. It focuses on two types of harmful content: hate speech and
misinformation. TweetInfo provides insights into tweets by doing content
analysis. Based on previous research, we have selected a list of
metainformation. We offer the option to filter content based on metainformation
Bot, Hate Speech, Misinformation, Verified Account, Sentiment, Tweet Category,
Language. The proposed user interface allows customising the user's timeline to
mitigate harmful content. This study present the demo version of the propose
user interface of TweetInfo.
</summary>
    <author>
      <name>Gautam Kishore Shahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.01646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.01646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.08783v1</id>
    <updated>2024-01-29T11:55:14Z</updated>
    <published>2024-01-29T11:55:14Z</published>
    <title>Image-Text Out-Of-Context Detection Using Synthetic Multimodal
  Misinformation</title>
    <summary>  Misinformation has become a major challenge in the era of increasing digital
information, requiring the development of effective detection methods. We have
investigated a novel approach to Out-Of-Context detection (OOCD) that uses
synthetic data generation. We created a dataset specifically designed for OOCD
and developed an efficient detector for accurate classification. Our
experimental findings validate the use of synthetic data generation and
demonstrate its efficacy in addressing the data limitations associated with
OOCD. The dataset and detector should serve as valuable resources for future
research and the development of robust misinformation detection systems.
</summary>
    <author>
      <name>Fatma Shalabi</name>
    </author>
    <author>
      <name>Huy H. Nguyen</name>
    </author>
    <author>
      <name>Hichem Felouat</name>
    </author>
    <author>
      <name>Ching-Chun Chang</name>
    </author>
    <author>
      <name>Isao Echizen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/APSIPAASC58517.2023.10317336</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/APSIPAASC58517.2023.10317336" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.08783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.08783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.18602v1</id>
    <updated>2024-04-29T11:17:42Z</updated>
    <published>2024-04-29T11:17:42Z</published>
    <title>Unraveling the Italian and English Telegram Conspiracy Spheres through
  Message Forwarding</title>
    <summary>  Telegram has grown into a significant platform for news and information
sharing, favored for its anonymity and minimal moderation. This openness,
however, makes it vulnerable to misinformation and conspiracy theories. In this
study, we explore the dynamics of conspiratorial narrative dissemination within
Telegram, focusing on Italian and English landscapes. In particular, we
leverage the mechanism of message forwarding within Telegram and collect two
extensive datasets through snowball strategy. We adopt a network-based approach
and build the Italian and English Telegram networks to reveal their respective
communities. By employing topic modeling, we uncover distinct narratives and
dynamics of misinformation spread. Results highlight differences between
Italian and English conspiracy landscapes, with Italian discourse involving
assorted conspiracy theories and alternative news sources intertwined with
legitimate news sources, whereas English discourse is characterized by a more
focused approach on specific narratives such as QAnon and political
conspiracies. Finally, we show that our methodology exhibits robustness across
initial seed selections, suggesting broader applicability. This study
contributes to understanding information and misinformation spread on Italian
and English Telegram ecosystems through the mechanism of message forwarding
</summary>
    <author>
      <name>Lorenzo Alvisi</name>
    </author>
    <author>
      <name>Serena Tardelli</name>
    </author>
    <author>
      <name>Maurizio Tesconi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ASONAM 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.18602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.18602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2, I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.19563v1</id>
    <updated>2024-05-29T23:11:53Z</updated>
    <published>2024-05-29T23:11:53Z</published>
    <title>Unlearning Climate Misinformation in Large Language Models</title>
    <summary>  Misinformation regarding climate change is a key roadblock in addressing one
of the most serious threats to humanity. This paper investigates factual
accuracy in large language models (LLMs) regarding climate information. Using
true/false labeled Q&amp;A data for fine-tuning and evaluating LLMs on
climate-related claims, we compare open-source models, assessing their ability
to generate truthful responses to climate change questions. We investigate the
detectability of models intentionally poisoned with false climate information,
finding that such poisoning may not affect the accuracy of a model's responses
in other domains. Furthermore, we compare the effectiveness of unlearning
algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually
grounding LLMs on climate change topics. Our evaluation reveals that unlearning
algorithms can be effective for nuanced conceptual claims, despite previous
findings suggesting their inefficacy in privacy contexts. These insights aim to
guide the development of more factually reliable LLMs and highlight the need
for additional work to secure LLMs against misinformation attacks.
</summary>
    <author>
      <name>Michael Fore</name>
    </author>
    <author>
      <name>Simranjit Singh</name>
    </author>
    <author>
      <name>Chaehong Lee</name>
    </author>
    <author>
      <name>Amritanshu Pandey</name>
    </author>
    <author>
      <name>Antonios Anastasopoulos</name>
    </author>
    <author>
      <name>Dimitrios Stamoulis</name>
    </author>
    <link href="http://arxiv.org/abs/2405.19563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.19563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04756v1</id>
    <updated>2024-06-07T08:57:25Z</updated>
    <published>2024-06-07T08:57:25Z</published>
    <title>Interpretable Multimodal Out-of-context Detection with Soft Logic
  Regularization</title>
    <summary>  The rapid spread of information through mobile devices and media has led to
the widespread of false or deceptive news, causing significant concerns in
society. Among different types of misinformation, image repurposing, also known
as out-of-context misinformation, remains highly prevalent and effective.
However, current approaches for detecting out-of-context misinformation often
lack interpretability and offer limited explanations. In this study, we propose
a logic regularization approach for out-of-context detection called LOGRAN
(LOGic Regularization for out-of-context ANalysis). The primary objective of
LOGRAN is to decompose the out-of-context detection at the phrase level. By
employing latent variables for phrase-level predictions, the final prediction
of the image-caption pair can be aggregated using logical rules. The latent
variables also provide an explanation for how the final result is derived,
making this fine-grained detection method inherently explanatory. We evaluate
the performance of LOGRAN on the NewsCLIPpings dataset, showcasing competitive
overall results. Visualized examples also reveal faithful phrase-level
predictions of out-of-context images, accompanied by explanations. This
highlights the effectiveness of our approach in addressing out-of-context
detection and enhancing interpretability.
</summary>
    <author>
      <name>Huanhuan Ma</name>
    </author>
    <author>
      <name>Jinghao Zhang</name>
    </author>
    <author>
      <name>Qiang Liu</name>
    </author>
    <author>
      <name>Shu Wu</name>
    </author>
    <author>
      <name>Liang Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP48485.2024.10447706</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP48485.2024.10447706" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2024 lecture paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.04854v1</id>
    <updated>2024-09-07T15:25:03Z</updated>
    <published>2024-09-07T15:25:03Z</published>
    <title>Adaptation Procedure in Misinformation Games</title>
    <summary>  We study interactions between agents in multi-agent systems, in which the
agents are misinformed with regards to the game that they play, essentially
having a subjective and incorrect understanding of the setting, without being
aware of it. For that, we introduce a new game-theoretic concept, called
misinformation games, that provides the necessary toolkit to study this
situation. Subsequently, we enhance this framework by developing a
time-discrete procedure (called the Adaptation Procedure) that captures
iterative interactions in the above context. During the Adaptation Procedure,
the agents update their information and reassess their behaviour in each step.
We demonstrate our ideas through an implementation, which is used to study the
efficiency and characteristics of the Adaptation Procedure.
</summary>
    <author>
      <name>Konstantinos Varsos</name>
    </author>
    <author>
      <name>Merkouris Papamichail</name>
    </author>
    <author>
      <name>Giorgos Flouris</name>
    </author>
    <author>
      <name>Marina Bitsaki</name>
    </author>
    <link href="http://arxiv.org/abs/2409.04854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.04854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.08829v2</id>
    <updated>2025-01-26T05:44:25Z</updated>
    <published>2024-09-13T13:45:44Z</published>
    <title>Community Fact-Checks Trigger Moral Outrage in Replies to Misleading
  Posts on Social Media</title>
    <summary>  Displaying community fact-checks is a promising approach to reduce engagement
with misinformation on social media. However, how users respond to misleading
content emotionally after community fact-checks are displayed on posts is
unclear. Here, we employ quasi-experimental methods to causally analyze changes
in sentiments and (moral) emotions in replies to misleading posts following the
display of community fact-checks. Our evaluation is based on a large-scale
panel dataset comprising N=2,225,260 replies across 1841 source posts from X's
Community Notes platform. We find that informing users about falsehoods through
community fact-checks significantly increases negativity (by 7.3%), anger (by
13.2%), disgust (by 4.7%), and moral outrage (by 16.0%) in the corresponding
replies. These results indicate that users perceive spreading misinformation as
a violation of social norms and that those who spread misinformation should
expect negative reactions once their content is debunked. We derive important
implications for the design of community-based fact-checking systems.
</summary>
    <author>
      <name>Yuwei Chuai</name>
    </author>
    <author>
      <name>Anastasia Sergeeva</name>
    </author>
    <author>
      <name>Gabriele Lenzini</name>
    </author>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3706598.3713909</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3706598.3713909" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conditionally accepted at CHI2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.08829v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.08829v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.16452v2</id>
    <updated>2025-02-02T20:20:48Z</updated>
    <published>2024-09-24T20:44:30Z</published>
    <title>FMDLlama: Financial Misinformation Detection based on Large Language
  Models</title>
    <summary>  The emergence of social media has made the spread of misinformation easier.
In the financial domain, the accuracy of information is crucial for various
aspects of financial market, which has made financial misinformation detection
(FMD) an urgent problem that needs to be addressed. Large language models
(LLMs) have demonstrated outstanding performance in various fields. However,
current studies mostly rely on traditional methods and have not explored the
application of LLMs in the field of FMD. The main reason is the lack of FMD
instruction tuning datasets and evaluation benchmarks. In this paper, we
propose FMDLlama, the first open-sourced instruction-following LLMs for FMD
task based on fine-tuning Llama3.1 with instruction data, the first multi-task
FMD instruction dataset (FMDID) to support LLM instruction tuning, and a
comprehensive FMD evaluation benchmark (FMD-B) with classification and
explanation generation tasks to test the FMD ability of LLMs. We compare our
models with a variety of LLMs on FMD-B, where our model outperforms other
open-sourced LLMs as well as OpenAI's products. This project is available at
https://github.com/lzw108/FMD.
</summary>
    <author>
      <name>Zhiwei Liu</name>
    </author>
    <author>
      <name>Xin Zhang</name>
    </author>
    <author>
      <name>Kailai Yang</name>
    </author>
    <author>
      <name>Qianqian Xie</name>
    </author>
    <author>
      <name>Jimin Huang</name>
    </author>
    <author>
      <name>Sophia Ananiadou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3701716.3715599</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3701716.3715599" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by The Web Conference (WWW) 2025 Short Paper Track</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.16452v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.16452v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.17190v1</id>
    <updated>2024-09-25T06:30:06Z</updated>
    <published>2024-09-25T06:30:06Z</published>
    <title>Enhancing Guardrails for Safe and Secure Healthcare AI</title>
    <summary>  Generative AI holds immense promise in addressing global healthcare access
challenges, with numerous innovative applications now ready for use across
various healthcare domains. However, a significant barrier to the widespread
adoption of these domain-specific AI solutions is the lack of robust safety
mechanisms to effectively manage issues such as hallucination, misinformation,
and ensuring truthfulness. Left unchecked, these risks can compromise patient
safety and erode trust in healthcare AI systems. While general-purpose
frameworks like Llama Guard are useful for filtering toxicity and harmful
content, they do not fully address the stringent requirements for truthfulness
and safety in healthcare contexts. This paper examines the unique safety and
security challenges inherent to healthcare AI, particularly the risk of
hallucinations, the spread of misinformation, and the need for factual accuracy
in clinical settings. I propose enhancements to existing guardrails frameworks,
such as Nvidia NeMo Guardrails, to better suit healthcare-specific needs. By
strengthening these safeguards, I aim to ensure the secure, reliable, and
accurate use of AI in healthcare, mitigating misinformation risks and improving
patient safety.
</summary>
    <author>
      <name>Ananya Gangavarapu</name>
    </author>
    <link href="http://arxiv.org/abs/2409.17190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.17190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.19656v1</id>
    <updated>2024-09-29T11:01:14Z</updated>
    <published>2024-09-29T11:01:14Z</published>
    <title>Multimodal Misinformation Detection by Learning from Synthetic Data with
  Multimodal LLMs</title>
    <summary>  Detecting multimodal misinformation, especially in the form of image-text
pairs, is crucial. Obtaining large-scale, high-quality real-world fact-checking
datasets for training detectors is costly, leading researchers to use synthetic
datasets generated by AI technologies. However, the generalizability of
detectors trained on synthetic data to real-world scenarios remains unclear due
to the distribution gap. To address this, we propose learning from synthetic
data for detecting real-world multimodal misinformation through two
model-agnostic data selection methods that match synthetic and real-world data
distributions. Experiments show that our method enhances the performance of a
small MLLM (13B) on real-world fact-checking datasets, enabling it to even
surpass GPT-4V~\cite{GPT-4V}.
</summary>
    <author>
      <name>Fengzhu Zeng</name>
    </author>
    <author>
      <name>Wenqian Li</name>
    </author>
    <author>
      <name>Wei Gao</name>
    </author>
    <author>
      <name>Yan Pang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2024 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.19656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.19656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.00866v1</id>
    <updated>2024-10-01T17:01:09Z</updated>
    <published>2024-10-01T17:01:09Z</published>
    <title>"I don't trust them": Exploring Perceptions of Fact-checking Entities
  for Flagging Online Misinformation</title>
    <summary>  The spread of misinformation through online social media platforms has had
substantial societal consequences. As a result, platforms have introduced
measures to alert users of news content that may be misleading or contain
inaccuracies as a means to discourage them from sharing it. These interventions
sometimes cite external sources, such as fact-checking organizations and news
outlets, for providing assessments related to the accuracy of the content.
However, it is unclear whether users trust the assessments provided by these
entities and whether perceptions vary across different topics of news. We
conducted an online study with 655 US participants to explore user perceptions
of eight categories of fact-checking entities across two misinformation topics,
as well as factors that may impact users' perceptions. We found that
participants' opinions regarding the trustworthiness and bias of the entities
varied greatly, aligning largely with their political preference. However, just
the presence of a fact-checking label appeared to discourage participants from
sharing the headlines studied. Our results hint at the need for further
exploring fact-checking entities that may be perceived as neutral, as well as
the potential for incorporating multiple assessments in such labels.
</summary>
    <author>
      <name>Hana Habib</name>
    </author>
    <author>
      <name>Sara Elsharawy</name>
    </author>
    <author>
      <name>Rifat Rahman</name>
    </author>
    <link href="http://arxiv.org/abs/2410.00866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.00866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.20140v2</id>
    <updated>2025-01-31T20:55:12Z</updated>
    <published>2024-10-26T10:34:22Z</published>
    <title>LLM-Consensus: Multi-Agent Debate for Visual Misinformation Detection</title>
    <summary>  One of the most challenging forms of misinformation involves the
out-of-context (OOC) use of images paired with misleading text, creating false
narratives. Existing AI-driven detection systems lack explainability and
require expensive finetuning. We address these issues with LLM-Consensus, a
multi-agent debate system for OOC misinformation detection. LLM-Consensus
introduces a novel multi-agent debate framework where multimodal agents
collaborate to assess contextual consistency and request external information
to enhance cross-context reasoning and decision-making. Our framework enables
explainable detection with state-of-the-art accuracy even without
domain-specific fine-tuning. Extensive ablation studies confirm that external
retrieval significantly improves detection accuracy, and user studies
demonstrate that LLM-Consensus boosts performance for both experts and
non-experts. These results position LLM-Consensus as a powerful tool for
autonomous and citizen intelligence applications.
</summary>
    <author>
      <name>Kumud Lakara</name>
    </author>
    <author>
      <name>Georgia Channing</name>
    </author>
    <author>
      <name>Juil Sock</name>
    </author>
    <author>
      <name>Christian Rupprecht</name>
    </author>
    <author>
      <name>Philip Torr</name>
    </author>
    <author>
      <name>John Collomosse</name>
    </author>
    <author>
      <name>Christian Schroeder de Witt</name>
    </author>
    <link href="http://arxiv.org/abs/2410.20140v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.20140v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.00549v1</id>
    <updated>2024-11-30T18:03:04Z</updated>
    <published>2024-11-30T18:03:04Z</published>
    <title>SeQwen at the Financial Misinformation Detection Challenge Task:
  Sequential Learning for Claim Verification and Explanation Generation in
  Financial Domains</title>
    <summary>  This paper presents the system description of our entry for the COLING 2025
FMD challenge, focusing on misinformation detection in financial domains. We
experimented with a combination of large language models, including Qwen,
Mistral, and Gemma-2, and leveraged pre-processing and sequential learning for
not only identifying fraudulent financial content but also generating coherent,
and concise explanations that clarify the rationale behind the classifications.
Our approach achieved competitive results with an F1-score of 0.8283 for
classification, and ROUGE-1 of 0.7253 for explanations. This work highlights
the transformative potential of LLMs in financial applications, offering
insights into their capabilities for combating misinformation and enhancing
transparency while identifying areas for future improvement in robustness and
domain adaptation.
</summary>
    <author>
      <name>Jebish Purbey</name>
    </author>
    <author>
      <name>Siddhant Gupta</name>
    </author>
    <author>
      <name>Nikhil Manali</name>
    </author>
    <author>
      <name>Siddartha Pullakhandam</name>
    </author>
    <author>
      <name>Drishti Sharma</name>
    </author>
    <author>
      <name>Ashay Srivastava</name>
    </author>
    <author>
      <name>Ram Mohan Rao Kadiyala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures, Submitted to FinNLP-FNP-LLMFinLegal @ COLING 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.00549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.00549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13086v1</id>
    <updated>2025-01-22T18:45:34Z</updated>
    <published>2025-01-22T18:45:34Z</published>
    <title>Information Degradation and Misinformation in Gossip Networks</title>
    <summary>  We study networks of gossiping users where a source observing a process sends
updates to an underlying graph. Nodes in the graph update their neighbors
randomly and nodes always accept packets that have newer information, thus
attempting to minimize their age of information (AoI). We show that while
gossiping reduces AoI, information can rapidly degrade in such a network. We
model degradation by arbitrary discrete-time Markov chains on k states. As a
packet is transmitted through the network it modifies its state according to
the Markov chain. In the last section, we specialize the Markov chain to
represent misinformation spread, and show that the rate of misinformation
spread is proportional to the age of information in both the fully-connected
graph and ring graph.
</summary>
    <author>
      <name>Thomas Jacob Maranzatto</name>
    </author>
    <author>
      <name>Arunabh Srivastava</name>
    </author>
    <author>
      <name>Sennur Ulukus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures. Submitted to ISIT 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02016v1</id>
    <updated>2025-03-03T19:50:52Z</updated>
    <published>2025-03-03T19:50:52Z</published>
    <title>Mind the (Belief) Gap: Group Identity in the World of LLMs</title>
    <summary>  Social biases and belief-driven behaviors can significantly impact Large
Language Models (LLMs) decisions on several tasks. As LLMs are increasingly
used in multi-agent systems for societal simulations, their ability to model
fundamental group psychological characteristics remains critical yet
under-explored. In this study, we present a multi-agent framework that
simulates belief congruence, a classical group psychology theory that plays a
crucial role in shaping societal interactions and preferences. Our findings
reveal that LLMs exhibit amplified belief congruence compared to humans, across
diverse contexts. We further investigate the implications of this behavior on
two downstream tasks: (1) misinformation dissemination and (2) LLM learning,
finding that belief congruence in LLMs increases misinformation dissemination
and impedes learning. To mitigate these negative impacts, we propose strategies
inspired by: (1) contact hypothesis, (2) accuracy nudges, and (3) global
citizenship framework. Our results show that the best strategies reduce
misinformation dissemination by up to 37% and enhance learning by 11%. Bridging
social psychology and AI, our work provides insights to navigate real-world
interactions using LLMs while addressing belief-driven biases.
</summary>
    <author>
      <name>Angana Borah</name>
    </author>
    <author>
      <name>Marwa Houalla</name>
    </author>
    <author>
      <name>Rada Mihalcea</name>
    </author>
    <link href="http://arxiv.org/abs/2503.02016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.12882v1</id>
    <updated>2025-04-17T12:14:38Z</updated>
    <published>2025-04-17T12:14:38Z</published>
    <title>ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection
  in Videos</title>
    <summary>  The growing influence of video content as a medium for communication and
misinformation underscores the urgent need for effective tools to analyze
claims in multilingual and multi-topic settings. Existing efforts in
misinformation detection largely focus on written text, leaving a significant
gap in addressing the complexity of spoken text in video transcripts. We
introduce ViClaim, a dataset of 1,798 annotated video transcripts across three
languages (English, German, Spanish) and six topics. Each sentence in the
transcripts is labeled with three claim-related categories: fact-check-worthy,
fact-non-check-worthy, or opinion. We developed a custom annotation tool to
facilitate the highly complex annotation process. Experiments with
state-of-the-art multilingual language models demonstrate strong performance in
cross-validation (macro F1 up to 0.896) but reveal challenges in generalization
to unseen topics, particularly for distinct domains. Our findings highlight the
complexity of claim detection in video transcripts. ViClaim offers a robust
foundation for advancing misinformation detection in video-based communication,
addressing a critical gap in multimodal analysis.
</summary>
    <author>
      <name>Patrick Giedemann</name>
    </author>
    <author>
      <name>Pius von D√§niken</name>
    </author>
    <author>
      <name>Jan Deriu</name>
    </author>
    <author>
      <name>Alvaro Rodrigo</name>
    </author>
    <author>
      <name>Anselmo Pe√±as</name>
    </author>
    <author>
      <name>Mark Cieliebak</name>
    </author>
    <link href="http://arxiv.org/abs/2504.12882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.12882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.07486v1</id>
    <updated>2025-05-12T12:19:31Z</updated>
    <published>2025-05-12T12:19:31Z</published>
    <title>Shots and Boosters: Exploring the Use of Combined Prebunking
  Interventions to Raise Critical Thinking and Create Long-Term Protection
  Against Misinformation</title>
    <summary>  The problem of how to effectively mitigate the flow of misinformation remains
a significant challenge. The classical approach to this is public disapproval
of claims or "debunking." The approach is still widely used on social media,
but it has some severe limitations in terms of applicability and efficiency. An
alternative strategy is to enhance individuals' critical thinking through
educational interventions. Instead of merely disproving misinformation, these
approaches aim to strengthen users' reasoning skills, enabling them to evaluate
and reject false information independently. In this position paper, we explore
a combination of intervention methods designed to improve critical thinking in
the context of online media consumption. We highlight the role of AI in
supporting different stages of these interventions and present a design concept
that integrates AI-driven strategies to foster critical reasoning and media
literacy.
</summary>
    <author>
      <name>Huiyun Tang</name>
    </author>
    <author>
      <name>Anastasia Sergeeva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2025 ACM Workshop on Human-AI Interaction for
  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction
  for Augmented Reasoning</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2505.07486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.07486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16730v1</id>
    <updated>2025-05-22T14:40:10Z</updated>
    <published>2025-05-22T14:40:10Z</published>
    <title>Detecting Fake News Belief via Skin and Blood Flow Signals</title>
    <summary>  Misinformation poses significant risks to public opinion, health, and
security. While most fake news detection methods rely on text analysis, little
is known about how people physically respond to false information or repeated
exposure to the same statements. This study investigates whether wearable
sensors can detect belief in a statement or prior exposure to it. We conducted
a controlled experiment where participants evaluated statements while wearing
an EmotiBit sensor that measured their skin conductance (electrodermal
activity, EDA) and peripheral blood flow (photoplethysmography, PPG). From 28
participants, we collected a dataset of 672 trials, each labeled with whether
the participant believed the statement and whether they had seen it before.
This dataset introduces a new resource for studying physiological responses to
misinformation. Using machine learning models, including KNN, CNN, and
LightGBM, we analyzed these physiological patterns. The best-performing model
achieved 67.83\% accuracy, with skin conductance outperforming PPG. These
findings demonstrate the potential of wearable sensors as a minimally intrusive
tool for detecting belief and prior exposure, offering new directions for
real-time misinformation detection and adaptive, user-aware systems.
</summary>
    <author>
      <name>Gennie Nguyen</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Yangxueqing Jiang</name>
    </author>
    <author>
      <name>Tom Gedeon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Research Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.16730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.17870v1</id>
    <updated>2025-05-23T13:20:23Z</updated>
    <published>2025-05-23T13:20:23Z</published>
    <title>Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat
  Falsehoods</title>
    <summary>  Generative AI models often learn and reproduce false information present in
their training corpora. This position paper argues that, analogous to
biological immunization, where controlled exposure to a weakened pathogen
builds immunity, AI models should be fine tuned on small, quarantined sets of
explicitly labeled falsehoods as a "vaccine" against misinformation. These
curated false examples are periodically injected during finetuning,
strengthening the model ability to recognize and reject misleading claims while
preserving accuracy on truthful inputs. An illustrative case study shows that
immunized models generate substantially less misinformation than baselines. To
our knowledge, this is the first training framework that treats fact checked
falsehoods themselves as a supervised vaccine, rather than relying on input
perturbations or generic human feedback signals, to harden models against
future misinformation. We also outline ethical safeguards and governance
controls to ensure the safe use of false data. Model immunization offers a
proactive paradigm for aligning AI systems with factuality.
</summary>
    <author>
      <name>Shaina Raza</name>
    </author>
    <author>
      <name>Rizwan Qureshi</name>
    </author>
    <author>
      <name>Marcelo Lotif</name>
    </author>
    <author>
      <name>Aman Chadha</name>
    </author>
    <author>
      <name>Deval Pandya</name>
    </author>
    <author>
      <name>Christos Emmanouilidis</name>
    </author>
    <link href="http://arxiv.org/abs/2505.17870v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.17870v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20584v1</id>
    <updated>2025-05-26T23:34:41Z</updated>
    <published>2025-05-26T23:34:41Z</published>
    <title>A Dashboard Approach to Monitoring Mpox-Related Discourse and
  Misinformation on Social Media</title>
    <summary>  Mpox (formerly monkeypox) is a zoonotic disease caused by an orthopoxvirus
closely related to variola and remains a significant global public health
concern. During outbreaks, social media platforms like X (formerly Twitter) can
both inform and misinform the public, complicating efforts to convey accurate
health information. To support local response efforts, we developed a
researcher-focused dashboard for use by public health stakeholders and the
public that enables searching and visualizing mpox-related tweets through an
interactive interface. Following the CDC's designation of mpox as an emerging
virus in August 2024, our dashboard recorded a marked increase in tweet volume
compared to 2023, illustrating the rapid spread of health discourse across
digital platforms. These findings underscore the continued need for real-time
social media monitoring tools to support public health communication and track
evolving sentiment and misinformation trends at the local level.
</summary>
    <author>
      <name> Linfeng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Leon</arxiv:affiliation>
    </author>
    <author>
      <name> Zhao</name>
    </author>
    <author>
      <name>Rishul Bhuvanagiri</name>
    </author>
    <author>
      <name>Blake Gonzales</name>
    </author>
    <author>
      <name>Kellen Sharp</name>
    </author>
    <author>
      <name>Dhiraj Murthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; I.2; I.7; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22354v1</id>
    <updated>2025-05-28T13:35:07Z</updated>
    <published>2025-05-28T13:35:07Z</published>
    <title>LLMs Struggle to Reject False Presuppositions when Misinformation Stakes
  are High</title>
    <summary>  This paper examines how LLMs handle false presuppositions and whether certain
linguistic factors influence their responses to falsely presupposed content.
Presuppositions subtly introduce information as given, making them highly
effective at embedding disputable or false information. This raises concerns
about whether LLMs, like humans, may fail to detect and correct misleading
assumptions introduced as false presuppositions, even when the stakes of
misinformation are high. Using a systematic approach based on linguistic
presupposition analysis, we investigate the conditions under which LLMs are
more or less sensitive to adopt or reject false presuppositions. Focusing on
political contexts, we examine how factors like linguistic construction,
political party, and scenario probability impact the recognition of false
presuppositions. We conduct experiments with a newly created dataset and
examine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's
Mistral-7B-v03. Our results show that the models struggle to recognize false
presuppositions, with performance varying by condition. This study highlights
that linguistic presupposition analysis is a valuable tool for uncovering the
reinforcement of political misinformation in LLM responses.
</summary>
    <author>
      <name>Judith Sieker</name>
    </author>
    <author>
      <name>Clara Lachenmaier</name>
    </author>
    <author>
      <name>Sina Zarrie√ü</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages (including References). Accepted at CogSci 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.22354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.08952v2</id>
    <updated>2025-06-11T06:58:55Z</updated>
    <published>2025-06-10T16:20:09Z</published>
    <title>Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded
  Political Questions</title>
    <summary>  Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.
</summary>
    <author>
      <name>Clara Lachenmaier</name>
    </author>
    <author>
      <name>Judith Sieker</name>
    </author>
    <author>
      <name>Sina Zarrie√ü</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint accepted at ACL Main Conference 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.08952v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.08952v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.12357v1</id>
    <updated>2025-06-14T05:39:52Z</updated>
    <published>2025-06-14T05:39:52Z</published>
    <title>`Socheton': A Culturally Appropriate AI Tool to Support Reproductive
  Well-being</title>
    <summary>  Reproductive well-being education in the Global South is often challenged as
many communities perceive many of its contents as misinformation,
misconceptions, and language-inappropriate. Our ten-month-long ethnographic
study (n=41) investigated the impact of sociocultural landscape, cultural
beliefs, and healthcare infrastructure on Bangladeshi people's access to
quality reproductive healthcare and set four design goals: combating
misinformation, including culturally appropriate language, professionals'
accountable moderation, and promoting users' democratic participation. Building
on the model of `\textit{Distributive Justice,}' we designed and evaluated
\textit{`Socheton,'} a culturally appropriate AI-mediated tool for reproductive
well-being that includes healthcare professionals, AI-language teachers, and
community members to moderate and run the activity-based platform. Our user
study (n=28) revealed that only combating misinformation and language
inappropriateness may still leave the community with a conservative mob culture
and patronize reproductive care-seeking. This guides well-being HCI design
toward being culturally appropriate in the context of reproductive justice with
sensitive marginalized communities.
</summary>
    <author>
      <name>Sharifa Sultana</name>
    </author>
    <author>
      <name>Hafsah Mahzabin Chowdhury</name>
    </author>
    <author>
      <name>Zinnat Sultana</name>
    </author>
    <author>
      <name>Nervo Verdezoto</name>
    </author>
    <link href="http://arxiv.org/abs/2506.12357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.12357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.12814v1</id>
    <updated>2025-06-15T11:18:10Z</updated>
    <published>2025-06-15T11:18:10Z</published>
    <title>Governments Should Mandate Tiered Anonymity on Social-Media Platforms to
  Counter Deepfakes and LLM-Driven Mass Misinformation</title>
    <summary>  This position paper argues that governments should mandate a three-tier
anonymity framework on social-media platforms as a reactionary measure prompted
by the ease-of-production of deepfakes and large-language-model-driven
misinformation. The tiers are determined by a given user's $\textit{reach
score}$: Tier 1 permits full pseudonymity for smaller accounts, preserving
everyday privacy; Tier 2 requires private legal-identity linkage for accounts
with some influence, reinstating real-world accountability at moderate reach;
Tier 3 would require per-post, independent, ML-assisted fact-checking, review
for accounts that would traditionally be classed as
sources-of-mass-information.
  An analysis of Reddit shows volunteer moderators converge on comparable gates
as audience size increases -- karma thresholds, approval queues, and identity
proofs -- demonstrating operational feasibility and social legitimacy.
Acknowledging that existing engagement incentives deter voluntary adoption, we
outline a regulatory pathway that adapts existing US jurisprudence and recent
EU-UK safety statutes to embed reach-proportional identity checks into existing
platform tooling, thereby curbing large-scale misinformation while preserving
everyday privacy.
</summary>
    <author>
      <name>David Khachaturov</name>
    </author>
    <author>
      <name>Roxanne Schnyder</name>
    </author>
    <author>
      <name>Robert Mullins</name>
    </author>
    <link href="http://arxiv.org/abs/2506.12814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.12814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.15794v1</id>
    <updated>2025-06-18T18:24:59Z</updated>
    <published>2025-06-18T18:24:59Z</published>
    <title>Veracity: An Open-Source AI Fact-Checking System</title>
    <summary>  The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.
</summary>
    <author>
      <name>Taylor Lynn Curtis</name>
    </author>
    <author>
      <name>Maximilian Puelma Touzel</name>
    </author>
    <author>
      <name>William Garneau</name>
    </author>
    <author>
      <name>Manon Gruaz</name>
    </author>
    <author>
      <name>Mike Pinder</name>
    </author>
    <author>
      <name>Li Wei Wang</name>
    </author>
    <author>
      <name>Sukanya Krishna</name>
    </author>
    <author>
      <name>Luda Cohen</name>
    </author>
    <author>
      <name>Jean-Fran√ßois Godbout</name>
    </author>
    <author>
      <name>Reihaneh Rabbany</name>
    </author>
    <author>
      <name>Kellin Pelrine</name>
    </author>
    <link href="http://arxiv.org/abs/2506.15794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.15794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22940v1</id>
    <updated>2025-06-28T16:13:42Z</updated>
    <published>2025-06-28T16:13:42Z</published>
    <title>Context, Credibility, and Control: User Reflections on AI Assisted
  Misinformation Tools</title>
    <summary>  This paper investigates how collaborative AI systems can enhance user agency
in identifying and evaluating misinformation on social media platforms.
Traditional methods, such as personal judgment or basic fact-checking, often
fall short when faced with emotionally charged or context-deficient content. To
address this, we designed and evaluated an interactive interface that
integrates collaborative AI features, including real-time explanations, source
aggregation, and debate-style interaction. These elements aim to support
critical thinking by providing contextual cues and argumentative reasoning in a
transparent, user-centered format. In a user study with 14 participants, 79%
found the debate mode more effective than standard chatbot interfaces, and the
multiple-source view received an average usefulness rating of 4.6 out of 5. Our
findings highlight the potential of context-rich, dialogic AI systems to
improve media literacy and foster trust in digital information environments. We
argue that future tools for misinformation mitigation should prioritize ethical
design, explainability, and interactive engagement to empower users in a
post-truth era.
</summary>
    <author>
      <name>Varun Sangwan</name>
    </author>
    <author>
      <name>Heidi Makitalo</name>
    </author>
    <link href="http://arxiv.org/abs/2506.22940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.22940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.23610v1</id>
    <updated>2025-06-30T08:16:07Z</updated>
    <published>2025-06-30T08:16:07Z</published>
    <title>Evaluating the Simulation of Human Personality-Driven Susceptibility to
  Misinformation with LLMs</title>
    <summary>  Large language models (LLMs) make it possible to generate synthetic
behavioural data at scale, offering an ethical and low-cost alternative to
human experiments. Whether such data can faithfully capture psychological
differences driven by personality traits, however, remains an open question. We
evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to
reproduce personality-based variation in susceptibility to misinformation,
focusing on news discernment, the ability to judge true headlines as true and
false headlines as false. Leveraging published datasets in which human
participants with known personality profiles rated headline accuracy, we create
matching LLM agents and compare their responses to the original human patterns.
Certain trait-misinformation associations, notably those involving
Agreeableness and Conscientiousness, are reliably replicated, whereas others
diverge, revealing systematic biases in how LLMs internalize and express
personality. The results underscore both the promise and the limits of
personality-aligned LLMs for behavioral simulation, and offer new insight into
modeling cognitive diversity in artificial agents.
</summary>
    <author>
      <name>Manuel Pratelli</name>
    </author>
    <author>
      <name>Marinella Petrocchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">pre-print version - paper actually under submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.23610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.23610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.05179v2</id>
    <updated>2025-07-13T17:59:01Z</updated>
    <published>2025-07-07T16:34:28Z</published>
    <title>From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating
  Hindi News Veracity Explanations</title>
    <summary>  In an era of rampant misinformation, generating reliable news explanations is
vital, especially for under-represented languages like Hindi. Lacking robust
automated tools, Hindi faces challenges in scaling misinformation detection. To
bridge this gap, we propose a novel framework integrating Direct Preference
Optimization (DPO) with curriculum learning to align machine-generated
explanations with human reasoning. Fact-checked explanations from credible
sources serve as preferred responses, while LLM outputs highlight system
limitations and serve as non-preferred responses. To refine task-specific
alignment, we introduce two key parameters -- Actuality and Finesse -- into the
DPO loss function, enhancing explanation quality and consistency. Experiments
with LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's
effectiveness in generating coherent, contextually relevant explanations. This
scalable approach combats misinformation and extends automated explanation
generation to low-resource languages.
</summary>
    <author>
      <name>Pulkit Bansal</name>
    </author>
    <author>
      <name>Raghvendra Kumar</name>
    </author>
    <author>
      <name>Shakti Singh</name>
    </author>
    <author>
      <name>Sriparna Saha</name>
    </author>
    <author>
      <name>Adam Jatowt</name>
    </author>
    <link href="http://arxiv.org/abs/2507.05179v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.05179v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.09174v1</id>
    <updated>2025-07-12T07:46:51Z</updated>
    <published>2025-07-12T07:46:51Z</published>
    <title>RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation
  Detection in Multimodal Fact-Checking</title>
    <summary>  The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.
</summary>
    <author>
      <name>Shuo Yang</name>
    </author>
    <author>
      <name>Zijian Yu</name>
    </author>
    <author>
      <name>Zhenzhe Ying</name>
    </author>
    <author>
      <name>Yuqin Dai</name>
    </author>
    <author>
      <name>Guoqing Wang</name>
    </author>
    <author>
      <name>Jun Lan</name>
    </author>
    <author>
      <name>Jinfeng Xu</name>
    </author>
    <author>
      <name>Jinze Li</name>
    </author>
    <author>
      <name>Edith C. H. Ngai</name>
    </author>
    <link href="http://arxiv.org/abs/2507.09174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.09174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.12723v1</id>
    <updated>2025-07-17T02:02:39Z</updated>
    <published>2025-07-17T02:02:39Z</published>
    <title>Cross-Modal Watermarking for Authentic Audio Recovery and Tamper
  Localization in Synthesized Audiovisual Forgeries</title>
    <summary>  Recent advances in voice cloning and lip synchronization models have enabled
Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are
manipulated to mimic a target speaker. This significantly increases the risk of
misinformation by making fake content seem real. To address this issue,
existing methods detect or localize manipulations but cannot recover the
authentic audio that conveys the semantic content of the message. This
limitation reduces their effectiveness in combating audiovisual misinformation.
In this work, we introduce the task of Authentic Audio Recovery (AAR) and
Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal
watermarking framework to embed authentic audio into visuals before
manipulation. This enables AAR, TLA, and a robust defense against
misinformation. Extensive experiments demonstrate the strong performance of our
method in AAR and TLA against various manipulations, including voice cloning
and lip synchronization.
</summary>
    <author>
      <name>Minyoung Kim</name>
    </author>
    <author>
      <name>Sehwan Park</name>
    </author>
    <author>
      <name>Sungmin Cha</name>
    </author>
    <author>
      <name>Paul Hongsuck Seo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, Interspeech 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.12723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.12723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.07074v3</id>
    <updated>2020-10-30T15:49:04Z</updated>
    <published>2020-03-16T08:51:40Z</published>
    <title>A Machine Learning Application for Raising WASH Awareness in the Times
  of COVID-19 Pandemic</title>
    <summary>  Background: The COVID-19 pandemic has uncovered the potential of digital
misinformation in shaping the health of nations. The deluge of unverified
information that spreads faster than the epidemic itself is an unprecedented
phenomenon that has put millions of lives in danger. Mitigating this Infodemic
requires strong health messaging systems that are engaging, vernacular,
scalable, effective and continuously learn the new patterns of misinformation.
  Objective: We created WashKaro, a multi-pronged intervention for mitigating
misinformation through conversational AI, machine translation and natural
language processing. WashKaro provides the right information matched against
WHO guidelines through AI, and delivers it in the right format in local
languages.
  Methods: We theorize (i) an NLP based AI engine that could continuously
incorporate user feedback to improve relevance of information, (ii) bite sized
audio in the local language to improve penetrance in a country with skewed
gender literacy ratios, and (iii) conversational but interactive AI engagement
with users towards an increased health awareness in the community. Results: A
total of 5026 people who downloaded the app during the study window, among
those 1545 were active users. Our study shows that 3.4 times more females
engaged with the App in Hindi as compared to males, the relevance of
AI-filtered news content doubled within 45 days of continuous machine learning,
and the prudence of integrated AI chatbot Satya increased thus proving the
usefulness of an mHealth platform to mitigate health misinformation.
  Conclusion: We conclude that a multi-pronged machine learning application
delivering vernacular bite-sized audios and conversational AI is an effective
approach to mitigate health misinformation.
</summary>
    <author>
      <name>Rohan Pandey</name>
    </author>
    <author>
      <name>Vaibhav Gautam</name>
    </author>
    <author>
      <name>Ridam Pal</name>
    </author>
    <author>
      <name>Harsh Bandhey</name>
    </author>
    <author>
      <name>Lovedeep Singh Dhingra</name>
    </author>
    <author>
      <name>Himanshu Sharma</name>
    </author>
    <author>
      <name>Chirag Jain</name>
    </author>
    <author>
      <name>Kanav Bhagat</name>
    </author>
    <author>
      <name> Arushi</name>
    </author>
    <author>
      <name>Lajjaben Patel</name>
    </author>
    <author>
      <name>Mudit Agarwal</name>
    </author>
    <author>
      <name>Samprati Agrawal</name>
    </author>
    <author>
      <name>Rishabh Jalan</name>
    </author>
    <author>
      <name>Akshat Wadhwa</name>
    </author>
    <author>
      <name>Ayush Garg</name>
    </author>
    <author>
      <name>Vihaan Misra</name>
    </author>
    <author>
      <name>Yashwin Agrawal</name>
    </author>
    <author>
      <name>Bhavika Rana</name>
    </author>
    <author>
      <name>Ponnurangam Kumaraguru</name>
    </author>
    <author>
      <name>Tavpritesh Sethi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.07074v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07074v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.06351v2</id>
    <updated>2024-03-26T09:55:59Z</updated>
    <published>2023-11-10T19:00:16Z</published>
    <title>A geometric analysis of the SIRS compartmental model with fast
  information and misinformation spreading</title>
    <summary>  We propose a novel slow-fast SIRS compartmental model with demography, by
coupling a slow disease spreading model and a fast information and
misinformation spreading model. Beside the classes of susceptible, infected and
recovered individuals of a common SIRS model, here we define three new classes
related to the information spreading model, e.g. unaware individuals,
misinformed individuals and individuals who are skeptical to disease-related
misinformation. Under our assumptions, the system evolves on two time scales.
We completely characterize its asymptotic behaviour with techniques of
Geometric Singular Perturbation Theory (GSPT). We exploit the time scale
separation to analyse two lower dimensional subsystem separately. First, we
focus on the analysis of the fast dynamics and we find three equilibrium point
which are feasible and stable under specific conditions. We perform a
theoretical bifurcation analysis of the fast system to understand the relations
between these three equilibria when varying specific parameters of the fast
system. Secondly, we focus on the evolution of the slow variables and we
identify three branches of the critical manifold, which are described by the
three equilibria of the fast system. We fully characterize the slow dynamics on
each branch. Moreover, we show how the inclusion of (mis)information spread may
negatively or positively affect the evolution of the epidemic, depending on
whether the slow dynamics evolves on the second branch of the critical
manifold, related to the skeptical-free equilibrium or on the third one,
related to misinformed-free equilibrium, respectively. We conclude with
numerical simulations which showcase our analytical results.
</summary>
    <author>
      <name>Iulia Martina Bulai</name>
    </author>
    <author>
      <name>Mattia Sensi</name>
    </author>
    <author>
      <name>Sara Sottile</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.chaos.2024.115104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.chaos.2024.115104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 8 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Chaos, Solitons and Fractals, Volume 185, August 2024, 115104</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2311.06351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.06351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="34C23, 34C60, 34E13, 34E15, 37N25, 92D30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2893v1</id>
    <updated>2014-11-11T17:34:13Z</updated>
    <published>2014-11-11T17:34:13Z</published>
    <title>Viral Misinformation: The Role of Homophily and Polarization</title>
    <summary>  The spreading of unsubstantiated rumors on online social networks (OSN)
either unintentionally or intentionally (e.g., for political reasons or even
trolling) can have serious consequences such as in the recent case of rumors
about Ebola causing disruption to health-care workers. Here we show that
indicators aimed at quantifying information consumption patterns might provide
important insights about the virality of false claims. In particular, we
address the driving forces behind the popularity of contents by analyzing a
sample of 1.2M Facebook Italian users consuming different (and opposite) types
of information (science and conspiracy news). We show that users' engagement
across different contents correlates with the number of friends having similar
consumption patterns (homophily), indicating the area in the social network
where certain types of contents are more likely to spread. Then, we test
diffusion patterns on an external sample of $4,709$ intentional satirical false
claims showing that neither the presence of hubs (structural properties) nor
the most active users (influencers) are prevalent in viral phenomena. Instead,
we found out that in an environment where misinformation is pervasive, users'
aggregation around shared beliefs may make the usual exposure to conspiracy
stories (polarization) a determinant for the virality of false information.
</summary>
    <author>
      <name>Aris Anagnostopoulos</name>
    </author>
    <author>
      <name>Alessandro Bessi</name>
    </author>
    <author>
      <name>Guido Caldarelli</name>
    </author>
    <author>
      <name>Michela Del Vicario</name>
    </author>
    <author>
      <name>Fabio Petroni</name>
    </author>
    <author>
      <name>Antonio Scala</name>
    </author>
    <author>
      <name>Fabiana Zollo</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Misinformation, Virality, Attention Patterns</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06887v1</id>
    <updated>2018-01-23T17:50:37Z</updated>
    <published>2018-01-23T17:50:37Z</published>
    <title>A Multiclass Mean-Field Game for Thwarting Misinformation Spread in the
  Internet of Battlefield Things (IoBT)</title>
    <summary>  In this paper, the problem of misinformation propagation is studied for an
Internet of Battlefield Things (IoBT) system in which an attacker seeks to
inject false information in the IoBT nodes in order to compromise the IoBT
operations. In the considered model, each IoBT node seeks to counter the
misinformation attack by finding the optimal probability of accepting a given
information that minimizes its cost at each time instant. The cost is expressed
in terms of the quality of information received as well as the infection cost.
The problem is formulated as a mean-field game with multiclass agents which is
suitable to model a massive heterogeneous IoBT system. For this game, the
mean-field equilibrium is characterized, and an algorithm based on the forward
backward sweep method is proposed to find the mean-field equilibrium. Then, the
finite IoBT case is considered, and the conditions of convergence of the Nash
equilibria in the finite case to the mean-field equilibrium are presented.
Numerical results show that the proposed scheme can achieve a 1.2-fold increase
in the quality of information (QoI) compared to a baseline scheme in which the
IoBT nodes are always transmitting. The results also show that the proposed
scheme can reduce the proportion of infected nodes by 99% compared to the
baseline.
</summary>
    <author>
      <name>Nof Abuzainab</name>
    </author>
    <author>
      <name>Walid Saad</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07647v2</id>
    <updated>2021-07-06T09:16:25Z</updated>
    <published>2020-10-15T10:31:28Z</published>
    <title>Identifying Possible Rumor Spreaders on Twitter: A Weak Supervised
  Learning Approach</title>
    <summary>  Online Social Media (OSM) platforms such as Twitter, Facebook are extensively
exploited by the users of these platforms for spreading the (mis)information to
a large audience effortlessly at a rapid pace. It has been observed that the
misinformation can cause panic, fear, and financial loss to society. Thus, it
is important to detect and control the misinformation in such platforms before
it spreads to the masses. In this work, we focus on rumors, which is one type
of misinformation (other types are fake news, hoaxes, etc). One way to control
the spread of the rumors is by identifying users who are possibly the rumor
spreaders, that is, users who are often involved in spreading the rumors. Due
to the lack of availability of rumor spreaders labeled dataset (which is an
expensive task), we use publicly available PHEME dataset, which contains rumor
and non-rumor tweets information, and then apply a weak supervised learning
approach to transform the PHEME dataset into rumor spreaders dataset. We
utilize three types of features, that is, user, text, and ego-network features,
before applying various supervised learning approaches. In particular, to
exploit the inherent network property in this dataset (user-user reply graph),
we explore Graph Convolutional Network (GCN), a type of Graph Neural Network
(GNN) technique. We compare GCN results with the other approaches: SVM, RF, and
LSTM. Extensive experiments performed on the rumor spreaders dataset, where we
achieve up to 0.864 value for F1-Score and 0.720 value for AUC-ROC, shows the
effectiveness of our methodology for identifying possible rumor spreaders using
the GCN technique.
</summary>
    <author>
      <name>Shakshi Sharma</name>
    </author>
    <author>
      <name>Rajesh Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at The International Joint Conference on Neural Networks
  2021 (IJCNN2021). Please cite the IJCNN version</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.07647v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07647v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05134v2</id>
    <updated>2021-05-14T21:05:27Z</updated>
    <published>2021-05-11T15:43:41Z</published>
    <title>COVID-19 Vaccine Hesitancy on Social Media: Building a Public Twitter
  Dataset of Anti-vaccine Content, Vaccine Misinformation and Conspiracies</title>
    <summary>  False claims about COVID-19 vaccines can undermine public trust in ongoing
vaccination campaigns, thus posing a threat to global public health.
Misinformation originating from various sources has been spreading online since
the beginning of the COVID-19 pandemic. In this paper, we present a dataset of
Twitter posts that exhibit a strong anti-vaccine stance. The dataset consists
of two parts: a) a streaming keyword-centered data collection with more than
1.8 million tweets, and b) a historical account-level collection with more than
135 million tweets. The former leverages the Twitter streaming API to follow a
set of specific vaccine-related keywords starting from mid-October 2020. The
latter consists of all historical tweets of 70K accounts that were engaged in
the active spreading of anti-vaccine narratives. We present descriptive
analyses showing the volume of activity over time, geographical distributions,
topics, news sources, and inferred account political leaning. This dataset can
be used in studying anti-vaccine misinformation on social media and enable a
better understanding of vaccine hesitancy. In compliance with Twitter's Terms
of Service, our anonymized dataset is publicly available at:
https://github.com/gmuric/avax-tweets-dataset
</summary>
    <author>
      <name>Goran Muric</name>
    </author>
    <author>
      <name>Yusong Wu</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <link href="http://arxiv.org/abs/2105.05134v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05134v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07592v4</id>
    <updated>2018-05-24T23:18:12Z</updated>
    <published>2017-07-24T14:53:36Z</published>
    <title>The spread of low-credibility content by social bots</title>
    <summary>  The massive spread of digital misinformation has been identified as a major
global risk and has been alleged to influence elections and threaten
democracies. Communication, cognitive, social, and computer scientists are
engaged in efforts to study the complex causes for the viral diffusion of
misinformation online and to develop solutions, while search and social media
platforms are beginning to deploy countermeasures. With few exceptions, these
efforts have been mainly informed by anecdotal evidence rather than systematic
data. Here we analyze 14 million messages spreading 400 thousand articles on
Twitter during and following the 2016 U.S. presidential campaign and election.
We find evidence that social bots played a disproportionate role in amplifying
low-credibility content. Accounts that actively spread articles from
low-credibility sources are significantly more likely to be bots. Automated
accounts are particularly active in amplifying content in the very early
spreading moments, before an article goes viral. Bots also target users with
many followers through replies and mentions. Humans are vulnerable to this
manipulation, retweeting bots who post links to low-credibility content.
Successful low-credibility sources are heavily supported by social bots. These
results suggest that curbing social bots may be an effective strategy for
mitigating the spread of online misinformation.
</summary>
    <author>
      <name>Chengcheng Shao</name>
    </author>
    <author>
      <name>Giovanni Luca Ciampaglia</name>
    </author>
    <author>
      <name>Onur Varol</name>
    </author>
    <author>
      <name>Kaicheng Yang</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41467-018-06930-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41467-018-06930-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 20 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature Communications, 9: 4787, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.07592v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07592v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.00742v2</id>
    <updated>2020-11-19T00:01:44Z</updated>
    <published>2020-04-01T23:44:58Z</published>
    <title>#ArsonEmergency and Australia's "Black Summer": Polarisation and
  misinformation on social media</title>
    <summary>  During the summer of 2019-20, while Australia suffered unprecedented
bushfires across the country, false narratives regarding arson and limited
backburning spread quickly on Twitter, particularly using the hashtag
#ArsonEmergency. Misinformation and bot- and troll-like behaviour were detected
and reported by social media researchers and the news soon reached mainstream
media. This paper examines the communication and behaviour of two polarised
online communities before and after news of the misinformation became public
knowledge. Specifically, the Supporter community actively engaged with others
to spread the hashtag, using a variety of news sources pushing the arson
narrative, while the Opposer community engaged less, retweeted more, and
focused its use of URLs to link to mainstream sources, debunking the narratives
and exposing the anomalous behaviour. This influenced the content of the
broader discussion. Bot analysis revealed the active accounts were
predominantly human, but behavioural and content analysis suggests Supporters
engaged in trolling, though both communities used aggressive language.
</summary>
    <author>
      <name>Derek Weber</name>
    </author>
    <author>
      <name>Mehwish Nasim</name>
    </author>
    <author>
      <name>Lucia Falzon</name>
    </author>
    <author>
      <name>Lewis Mitchell</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-61841-4_11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-61841-4_11" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 images, presented at the 2nd Multidisciplinary
  International Symposium on Disinformation in Open Online Media (MISDOOM
  2020), Leiden, The Netherlands. Published in: van Duijn M., Preuss M.,
  Spaiser V., Takes F., Verberne S. (eds) Disinformation in Open Online Media.
  MISDOOM 2020. Lecture Notes in Computer Science, vol 12259. Springer, Cham.
  https://doi.org/10.1007/978-3-030-61841-4_11</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.00742v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.00742v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.05848v1</id>
    <updated>2020-07-11T20:37:14Z</updated>
    <published>2020-07-11T20:37:14Z</published>
    <title>Fighting Disaster Misinformation in Latin America: The #19S Mexican
  Earthquake Case Study</title>
    <summary>  Social media platforms have been extensively used during natural disasters.
However, most prior work has lacked focus on studying their usage during
disasters in the Global South, where Internet access and social media
utilization differs from developing countries. In this paper, we study how
social media was used in the aftermath of the 7.1-magnitude earthquake that hit
Mexico on September 19 of 2017 (known as the #19S earthquake). We conduct an
analysis of how participants utilized social media platforms in the #19S
aftermath. Our research extends investigations of crisis informatics by: 1)
examining how participants used different social media platforms in the
aftermath of a natural disaster in a Global South country; 2) uncovering how
individuals developed their own processes to verify news reports using an
on-the-ground citizen approach; 3) revealing how people developed their own
mechanisms to deal with outdated information. For this, we surveyed 356 people.
Additionally, we analyze one month of activity from: Facebook (12,606 posts),
Twitter (2,909,109 tweets), Slack (28,782 messages), and GitHub (2,602
commits). This work offers a multi-platform view on user behavior to coordinate
relief efforts, reduce the spread of misinformation and deal with obsolete
information which seems to have been essential to help in the coordination and
efficiency of relief efforts. Finally, based on our findings, we make
recommendations for technology design to improve the effectiveness of social
media use during crisis response efforts and mitigate the spread of
misinformation across social media platforms.
</summary>
    <author>
      <name>Claudia Flores-Saviaga</name>
    </author>
    <author>
      <name>Saiph Savage</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00779-020-01411-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00779-020-01411-5" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Springer - Personal and Ubiquitous Computing 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.05848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.05848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09682v3</id>
    <updated>2021-03-26T18:35:43Z</updated>
    <published>2020-07-19T14:52:34Z</published>
    <title>Twitter and Facebook posts about COVID-19 are less likely to spread
  false and low-credibility content compared to other health topics</title>
    <summary>  On February 2, 2020, the World Health Organization declared a COVID-19 social
media "infodemic", with special attention to misinformation -- frequently
understood as false claims. To understand the infodemic's scope and scale, we
analyzed over 500 million posts from Twitter and Facebook about COVID-19 and
other health topics, between March 8 and May 1, 2020. Following prior work, we
assumed URL source credibility is a proxy for false content, but we also tested
this assumption. Contrary to expectations, we found that messages about
COVID-19 were more likely to contain links to more credible sources.
Additionally, messages linking to government sources, and to news with
intermediate credibility, were shared more often, on average, than links to
non-credible sources. These results suggest that more ambiguous forms of
misinformation about COVID-19 may be more likely to be disseminated through
credible sources when compared to other health topics. Furthermore, the
assumption that credibility is an adequate proxy for false content may
overestimate the prevalence of false content online: less than 25% of posts
linking to the least credible sources contained false content. Our results
emphasize the importance of distinguishing between explicit falsehoods and more
ambiguous forms of misinformation due to the search for meaning in an
environment of scientific uncertainty.
</summary>
    <author>
      <name>David A. Broniatowski</name>
    </author>
    <author>
      <name>Daniel Kerchner</name>
    </author>
    <author>
      <name>Fouzia Farooq</name>
    </author>
    <author>
      <name>Xiaolei Huang</name>
    </author>
    <author>
      <name>Amelia M. Jamison</name>
    </author>
    <author>
      <name>Mark Dredze</name>
    </author>
    <author>
      <name>Sandra Crouse Quinn</name>
    </author>
    <link href="http://arxiv.org/abs/2007.09682v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09682v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09703v1</id>
    <updated>2020-07-19T16:14:58Z</updated>
    <published>2020-07-19T16:14:58Z</published>
    <title>A curated collection of COVID-19 online datasets</title>
    <summary>  One of the defining moments of the year 2020 is the outbreak of Coronavirus
Disease (Covid-19), a deadly virus affecting the body's respiratory system to
the point of needing a breathing aid via ventilators. As of June 21, 2020 there
are 12,929,306 confirmed cases and 569,738 confirmed deaths across 216
countries, areas or territories. The scale of spread and impact of the pandemic
left many nations grappling with preventive and curative approaches. The
infamous lockdown measure introduced to mitigate the virus spread has altered
many aspects of our social routines in which demand for online-based services
skyrocketed. As the virus propagate, so does misinformation and fake news
around it via online social media, which seems to favour virality over
veracity. With a majority of the populace confined to their homes for a long
period, vulnerability to the toxic impact of online misinformation is high. A
case in point is the various myths and disinformation associated with the
Covid-19, which, if left unchecked, could lead to a catastrophic outcome and
hamper the fight against the virus.
  While the scientific community is actively engaged in identifying the virus
treatment, there is a growing interest in combating the associated harmful
infodemic. To this end, researchers have been curating and documenting various
datasets about Covid-19. In line with existing studies, we provide an expansive
collection of curated datasets to support the fight against the pandemic,
especially concerning misinformation. The collection consists of 3 categories
of Twitter data, information about standard practices from credible sources and
a chronicle of global situation reports. We describe how to retrieve the
hydrated version of the data and proffer some research problems that could be
addressed using the data.
</summary>
    <author>
      <name>Isa Inuwa-Dutse</name>
    </author>
    <author>
      <name>Ioannis Korkontzelos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.09703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.05416v1</id>
    <updated>2020-11-09T04:15:44Z</updated>
    <published>2020-11-09T04:15:44Z</published>
    <title>Challenges and Opportunities in Rapid Epidemic Information Propagation
  with Live Knowledge Aggregation from Social Media</title>
    <summary>  A rapidly evolving situation such as the COVID-19 pandemic is a significant
challenge for AI/ML models because of its unpredictability. %The most reliable
indicator of the pandemic spreading has been the number of test positive cases.
However, the tests are both incomplete (due to untested asymptomatic cases) and
late (due the lag from the initial contact event, worsening symptoms, and test
results). Social media can complement physical test data due to faster and
higher coverage, but they present a different challenge: significant amounts of
noise, misinformation and disinformation. We believe that social media can
become good indicators of pandemic, provided two conditions are met. The first
(True Novelty) is the capture of new, previously unknown, information from
unpredictably evolving situations. The second (Fact vs. Fiction) is the
distinction of verifiable facts from misinformation and disinformation. Social
media information that satisfy those two conditions are called live knowledge.
We apply evidence-based knowledge acquisition (EBKA) approach to collect,
filter, and update live knowledge through the integration of social media
sources with authoritative sources. Although limited in quantity, the reliable
training data from authoritative sources enable the filtering of misinformation
as well as capturing truly new information. We describe the EDNA/LITMUS tools
that implement EBKA, integrating social media such as Twitter and Facebook with
authoritative sources such as WHO and CDC, creating and updating live knowledge
on the COVID-19 pandemic.
</summary>
    <author>
      <name>Calton Pu</name>
    </author>
    <author>
      <name>Abhijit Suprem</name>
    </author>
    <author>
      <name>Rodrigo Alves Lima</name>
    </author>
    <link href="http://arxiv.org/abs/2011.05416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.02382v1</id>
    <updated>2021-02-04T02:47:31Z</updated>
    <published>2021-02-04T02:47:31Z</published>
    <title>Mainstreaming of conspiracy theories and misinformation</title>
    <summary>  Parents - particularly moms - increasingly consult social media for support
when taking decisions about their young children, and likely also when advising
other family members such as elderly relatives. Minimizing malignant online
influences is therefore crucial to securing their assent for policies ranging
from vaccinations, masks and social distancing against the pandemic, to
household best practices against climate change, to acceptance of future 5G
towers nearby. Here we show how a strengthening of bonds across online
communities during the pandemic, has led to non-Covid-19 conspiracy theories
(e.g. fluoride, chemtrails, 5G) attaining heightened access to mainstream
parent communities. Alternative health communities act as the critical conduits
between conspiracy theorists and parents, and make the narratives more
palatable to the latter. We demonstrate experimentally that these
inter-community bonds can perpetually generate new misinformation, irrespective
of any changes in factual information. Our findings show explicitly why
Facebook's current policies have failed to stop the mainstreaming of
non-Covid-19 and Covid-19 conspiracy theories and misinformation, and why
targeting the largest communities will not work. A simple yet exactly solvable
and empirically grounded mathematical model, shows how modest tailoring of
mainstream communities' couplings could prevent them from tipping against
establishment guidance. Our conclusions should also apply to other social media
platforms and topics.
</summary>
    <author>
      <name>N. F. Johnson</name>
    </author>
    <author>
      <name>N. Velasquez</name>
    </author>
    <author>
      <name>N. Johnson Restrepo</name>
    </author>
    <author>
      <name>R. Leahy</name>
    </author>
    <author>
      <name>R. Sear</name>
    </author>
    <author>
      <name>N. Gabriel</name>
    </author>
    <author>
      <name>H. Larson</name>
    </author>
    <author>
      <name>Y. Lupu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper. Comments welcome to neiljohnson@gwu.edu</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.02382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.02382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.01627v1</id>
    <updated>2021-06-03T06:56:09Z</updated>
    <published>2021-06-03T06:56:09Z</published>
    <title>Piercing the Veil: Designs to Support Information Literacy on Social
  Platforms</title>
    <summary>  In this position paper we approach problems concerning critical digital and
information literacy with ideas to provide more digestible explanations of
abstract concepts through interface design. In particular, we focus on social
media platforms where we see the possibility of counteracting the spread of
misinformation by providing users with more proficiency through our approaches.
We argue that the omnipresent trend to abstract away and hide information from
users via UI/UX design opposes their ability to self-learn. This leads us to
propose a different framework in which we unify elegant and simple interfaces
with nudges that promote a look behind the curtain. Such designs serve to
foster a deeper understanding of employed technologies and aim to increase the
critical assessment of content encountered on social platforms. Furthermore, we
consider users with an intermediary skill level to be largely ignored in
current approaches, as they are given no tools to broaden their knowledge
without consultation of expert material. The resulting stagnation is
exemplified by the tactics of misinformation campaigns, which exploit the
ensuing lack of information literacy and critical thinking. We propose an
approach to design that sufficiently emancipates users in both aspects by
promoting a look behind the abstraction of UI/UX so that an autonomous learning
process is given the chance to occur. Furthermore, we name ideas for future
research within this area that take our considerations into account.
</summary>
    <author>
      <name>Jan Wolff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Originally submitted to and presented at CHI'21 Workshop on
  Technologies to Support Critical Thinking in an Age of Misinformation</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.01627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.05815v1</id>
    <updated>2021-06-10T15:30:42Z</updated>
    <published>2021-06-10T15:30:42Z</published>
    <title>Italian Twitter semantic network during the Covid-19 epidemic</title>
    <summary>  The Covid-19 pandemic has had a deep impact on the lives of the entire world
population, inducing a participated societal debate. As in other contexts, the
debate has been the subject of several d/misinformation campaigns; in a quite
unprecedented fashion, however, the presence of false information has seriously
put at risk the public health. In this sense, detecting the presence of
malicious narratives and identifying the kinds of users that are more prone to
spread them represent the first step to limit the persistence of the former
ones. In the present paper we analyse the semantic network observed on Twitter
during the first Italian lockdown (induced by the hashtags contained in
approximately 1.5 millions tweets published between the 23rd of March 2020 and
the 23rd of April 2020) and study the extent to which various discursive
communities are exposed to d/misinformation arguments. As observed in other
studies, the recovered discursive communities largely overlap with traditional
political parties, even if the debated topics concern different facets of the
management of the pandemic. Although the themes directly related to
d/misinformation are a minority of those discussed within our semantic
networks, their popularity is unevenly distributed among the various discursive
communities.
</summary>
    <author>
      <name>Mattia Mattei</name>
    </author>
    <author>
      <name>Guido Caldarelli</name>
    </author>
    <author>
      <name>Tiziano Squartini</name>
    </author>
    <author>
      <name>Fabio Saracco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjds/s13688-021-00301-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjds/s13688-021-00301-x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPJ Data Science 10 (47) (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.05815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.05815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.15454v1</id>
    <updated>2021-10-28T22:19:14Z</updated>
    <published>2021-10-28T22:19:14Z</published>
    <title>VigDet: Knowledge Informed Neural Temporal Point Process for
  Coordination Detection on Social Media</title>
    <summary>  Recent years have witnessed an increasing use of coordinated accounts on
social media, operated by misinformation campaigns to influence public opinion
and manipulate social outcomes. Consequently, there is an urgent need to
develop an effective methodology for coordinated group detection to combat the
misinformation on social media. However, existing works suffer from various
drawbacks, such as, either limited performance due to extreme reliance on
predefined signatures of coordination, or instead an inability to address the
natural sparsity of account activities on social media with useful prior domain
knowledge. Therefore, in this paper, we propose a coordination detection
framework incorporating neural temporal point process with prior knowledge such
as temporal logic or pre-defined filtering functions. Specifically, when
modeling the observed data from social media with neural temporal point
process, we jointly learn a Gibbs-like distribution of group assignment based
on how consistent an assignment is to (1) the account embedding space and (2)
the prior knowledge. To address the challenge that the distribution is hard to
be efficiently computed and sampled from, we design a theoretically guaranteed
variational inference approach to learn a mean-field approximation for it.
Experimental results on a real-world dataset show the effectiveness of our
proposed method compared to the SOTA model in both unsupervised and
semi-supervised settings. We further apply our model on a COVID-19 Vaccine
Tweets dataset. The detection result suggests the presence of suspicious
coordinated efforts on spreading misinformation about COVID-19 vaccines.
</summary>
    <author>
      <name>Yizhou Zhang</name>
    </author>
    <author>
      <name>Karishma Sharma</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by NeurIPS 2021. 17 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Neural Information Processing Systems (NeurIPS), 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.15454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.15454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.06771v1</id>
    <updated>2022-02-14T14:43:20Z</updated>
    <published>2022-02-14T14:43:20Z</published>
    <title>DS4DH at TREC Health Misinformation 2021: Multi-Dimensional Ranking
  Models with Transfer Learning and Rank Fusion</title>
    <summary>  This paper describes the work of the Data Science for Digital Health (DS4DH)
group at the TREC Health Misinformation Track 2021. The TREC Health
Misinformation track focused on the development of retrieval methods that
provide relevant, correct and credible information for health related searches
on the Web. In our methodology, we used a two-step ranking approach that
includes i) a standard retrieval phase, based on BM25 model, and ii) a
re-ranking phase, with a pipeline of models focused on the usefulness,
supportiveness and credibility dimensions of the retrieved documents. To
estimate the usefulness, we classified the initial rank list using pre-trained
language models based on the transformers architecture fine-tuned on the MS
MARCO corpus. To assess the supportiveness, we utilized BERT-based models
fine-tuned on scientific and Wikipedia corpora. Finally, to evaluate the
credibility of the documents, we employed a random forest model trained on the
Microsoft Credibility dataset combined with a list of credible sites. The
resulting ranked lists were then combined using the Reciprocal Rank Fusion
algorithm to obtain the final list of useful, supporting and credible
documents. Our approach achieved competitive results, being top-2 in the
compatibility measurement for the automatic runs. Our findings suggest that
integrating automatic ranking models created for each information quality
dimension with transfer learning can increase the effectiveness of
health-related information retrieval.
</summary>
    <author>
      <name>Boya Zhang</name>
    </author>
    <author>
      <name>Nona Naderi</name>
    </author>
    <author>
      <name>Fernando Jaume-Santero</name>
    </author>
    <author>
      <name>Douglas Teodoro</name>
    </author>
    <link href="http://arxiv.org/abs/2202.06771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.06771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.07731v1</id>
    <updated>2022-03-15T08:54:36Z</updated>
    <published>2022-03-15T08:54:36Z</published>
    <title>Evaluating BERT-based Pre-training Language Models for Detecting
  Misinformation</title>
    <summary>  It is challenging to control the quality of online information due to the
lack of supervision over all the information posted online. Manual checking is
almost impossible given the vast number of posts made on online media and how
quickly they spread. Therefore, there is a need for automated rumour detection
techniques to limit the adverse effects of spreading misinformation. Previous
studies mainly focused on finding and extracting the significant features of
text data. However, extracting features is time-consuming and not a highly
effective process. This study proposes the BERT- based pre-trained language
models to encode text data into vectors and utilise neural network models to
classify these vectors to detect misinformation. Furthermore, different
language models (LM) ' performance with different trainable parameters was
compared. The proposed technique is tested on different short and long text
datasets. The result of the proposed technique has been compared with the
state-of-the-art techniques on the same datasets. The results show that the
proposed technique performs better than the state-of-the-art techniques. We
also tested the proposed technique by combining the datasets. The results
demonstrated that the large data training and testing size considerably
improves the technique's performance.
</summary>
    <author>
      <name>Rini Anggrainingsih</name>
    </author>
    <author>
      <name>Ghulam Mubashar Hassan</name>
    </author>
    <author>
      <name>Amitava Datta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.07731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.07731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.04118v1</id>
    <updated>2022-06-08T18:33:48Z</updated>
    <published>2022-06-08T18:33:48Z</published>
    <title>Spatial Games of Fake News</title>
    <summary>  To curb the spread of fake news on social media platforms, recent studies
have considered an online crowdsourcing fact-checking approach as one possible
intervention method to reduce misinformation. However, it remains unclear under
what conditions crowdsourcing fact-checking efforts deter the spread of
misinformation. To address this issue, we model such distributed fact-checking
as `peer policing' that will reduce the perceived payoff to share or
disseminate false information (fake news) and also reward the spread of
trustworthy information (real news). By simulating our model on synthetic
square lattices and small-world networks, we show that the presence of social
network structure enables fake news spreaders to be self-organized into echo
chambers, thereby providing a boost to the efficacy of fake news and thus its
resistance to fact-checking efforts. Additionally, to study our model in a more
realistic setting, we utilize a Twitter network dataset and study the
effectiveness of deliberately choosing specific individuals to be
fact-checkers. We find that targeted fact-checking efforts can be highly
effective, seeing the same level of success with as little as a fifth of the
number of fact-checkers, but it depends on the structure of the network in
question. In the limit of weak selection, we obtain closed-form analytical
conditions for critical threshold of crowdsourced fact-checking in terms of the
payoff values in our fact-checker/fake news game. Our work has practical
implications for developing model-based mitigation strategies for controlling
the spread of misinformation that interferes with the political discourse.
</summary>
    <author>
      <name>Matthew I Jones</name>
    </author>
    <author>
      <name>Scott D. Pauls</name>
    </author>
    <author>
      <name>Feng Fu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.04118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.11419v1</id>
    <updated>2022-06-22T23:27:01Z</updated>
    <published>2022-06-22T23:27:01Z</published>
    <title>Misinformation Mitigation under Differential Propagation Rates and
  Temporal Penalties</title>
    <summary>  We propose an information propagation model that captures important temporal
aspects that have been well observed in the dynamics of fake news diffusion, in
contrast with the diffusion of truth. The model accounts for differential
propagation rates of truth and misinformation and for user reaction times. We
study a time-sensitive variant of the \textit{misinformation mitigation}
problem, where $k$ seeds are to be selected to activate a truth campaign so as
to minimize the number of users that adopt misinformation propagating through a
social network. We show that the resulting objective is non-submodular and
employ a sandwiching technique by defining submodular upper and lower bounding
functions, providing data-dependent guarantees. In order to enable the use of a
reverse sampling framework, we introduce a weighted version of reverse
reachability sets that captures the associated differential propagation rates
and establish a key equivalence between weighted set coverage probabilities and
mitigation with respect to the sandwiching functions. Further, we propose an
offline reverse sampling framework that provides $(1 - 1/e -
\epsilon)$-approximate solutions to our bounding functions and introduce an
importance sampling technique to reduce the sample complexity of our solution.
Finally, we show how our framework can provide an anytime solution to the
problem. Experiments over five datasets show that our approach outperforms
previous approaches and is robust to uncertainty in the model parameters.
</summary>
    <author>
      <name>Michael Simpson</name>
    </author>
    <author>
      <name>Farnoosh Hashemi</name>
    </author>
    <author>
      <name>Laks V. S. Lakshmanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, VLDB 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.11419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.11419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.01483v1</id>
    <updated>2022-06-12T19:41:01Z</updated>
    <published>2022-06-12T19:41:01Z</published>
    <title>"COVID-19 was a FIFA conspiracy #curropt": An Investigation into the
  Viral Spread of COVID-19 Misinformation</title>
    <summary>  The outbreak of the infectious and fatal disease COVID-19 has revealed that
pandemics assail public health in two waves: first, from the contagion itself
and second, from plagues of suspicion and stigma. Now, we have in our hands and
on our phones an outbreak of moral controversy. Modern dependency on social
medias has not only facilitated access to the locations of vaccine clinics and
testing sites but also-and more frequently-to the convoluted explanations of
how "COVID-19 was a FIFA conspiracy"[1]. The MIT Media Lab finds that false
news "diffuses significantly farther, faster, deeper, and more broadly than
truth, in all categories of information, and by an order of magnitude"[2]. The
question is, how does the spread of misinformation interact with a physical
epidemic disease? In this paper, we estimate the extent to which misinformation
has influenced the course of the COVID-19 pandemic using natural language
processing models and provide a strategy to combat social media posts that are
likely to cause widespread harm.
</summary>
    <author>
      <name>Alexander Wang</name>
    </author>
    <author>
      <name>Jerry Sun</name>
    </author>
    <author>
      <name>Kaitlyn Chen</name>
    </author>
    <author>
      <name>Kevin Zhou</name>
    </author>
    <author>
      <name>Edward Li Gu</name>
    </author>
    <author>
      <name>Chenxin Fang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Winner of the 2021 ProjectX undergraduate research competition hosted
  by the University of Toronto under the category of Epidemiology. Accepted by
  the University of Toronto AI Conference 2022. 8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.01483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.01483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.07562v1</id>
    <updated>2022-07-15T16:09:15Z</updated>
    <published>2022-07-15T16:09:15Z</published>
    <title>How many others have shared this? Experimentally investigating the
  effects of social cues on engagement, misinformation, and unpredictability on
  social media</title>
    <summary>  Unlike traditional media, social media typically provides quantified metrics
of how many users have engaged with each piece of content. Some have argued
that the presence of these cues promotes the spread of misinformation. Here we
investigate the causal effect of social cues on users' engagement with social
media posts. We conducted an experiment with N=628 Americans on a custom-built
newsfeed interface where we systematically varied the presence and strength of
social cues. We find that when cues are shown, indicating that a larger number
of others have engaged with a post, users were more likely to share and like
that post. Furthermore, relative to a control without social cues, the presence
of social cues increased the sharing of true relative to false news. The
presence of social cues also makes it more difficult to precisely predict how
popular any given post would be. Together, our results suggest that -- instead
of distracting users or causing them to share low-quality news -- social cues
may, in certain circumstances, actually boost truth discernment and reduce the
sharing of misinformation. Our work suggests that social cues play important
roles in shaping users' attention and engagement on social media, and platforms
should understand the effects of different cues before making changes to what
cues are displayed and how.
</summary>
    <author>
      <name>Ziv Epstein</name>
    </author>
    <author>
      <name>Hause Lin</name>
    </author>
    <author>
      <name>Gordon Pennycook</name>
    </author>
    <author>
      <name>David Rand</name>
    </author>
    <link href="http://arxiv.org/abs/2207.07562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.07562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.03907v3</id>
    <updated>2022-08-11T22:28:47Z</updated>
    <published>2022-08-08T04:04:47Z</published>
    <title>Bridging the Gap: Commonality and Differences between Online and Offline
  COVID-19 Data</title>
    <summary>  With the onset of the COVID-19 pandemic, news outlets and social media have
become central tools for disseminating and consuming information. Because of
their ease of access, users seek COVID-19-related information from online
social media (i.e., online news) and news outlets (i.e., offline news). Online
and offline news are often connected, sharing common topics while each has
unique, different topics. A gap between these two news sources can lead to
misinformation propagation. For instance, according to the Guardian, most
COVID-19 misinformation comes from users on social media. Without fact-checking
social media news, misinformation can lead to health threats. In this paper, we
focus on the novel problem of bridging the gap between online and offline data
by monitoring their common and distinct topics generated over time. We employ
Twitter (online) and local news (offline) data for a time span of two years.
Using online matrix factorization, we analyze and study online and offline
COVID-19-related data differences and commonalities. We design experiments to
show how online and offline data are linked together and what trends they
follow.
</summary>
    <author>
      <name>Nayoung Kim</name>
    </author>
    <author>
      <name>Ahmadreza Mosallanezhad</name>
    </author>
    <author>
      <name>Lu Cheng</name>
    </author>
    <author>
      <name>Baoxin Li</name>
    </author>
    <author>
      <name>Huan Li</name>
    </author>
    <link href="http://arxiv.org/abs/2208.03907v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03907v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.09467v1</id>
    <updated>2022-10-17T22:51:45Z</updated>
    <published>2022-10-17T22:51:45Z</published>
    <title>Adversarial and Safely Scaled Question Generation</title>
    <summary>  Question generation has recently gained a lot of research interest,
especially with the advent of large language models. In and of itself, question
generation can be considered 'AI-hard', as there is a lack of unanimously
agreed sense of what makes a question 'good' or 'bad'. In this paper, we tackle
two fundamental problems in parallel: on one hand, we try to solve the scaling
problem, where question-generation and answering applications have to be
applied to a massive amount of text without ground truth labeling. The usual
approach to solve this problem is to either downsample or summarize. However,
there are critical risks of misinformation with these approaches. On the other
hand, and related to the misinformation problem, we try to solve the 'safety'
problem, as many public institutions rely on a much higher level of accuracy
for the content they provide. We introduce an adversarial approach to tackle
the question generation safety problem with scale. Specifically, we designed a
question-answering system that specifically prunes out unanswerable questions
that may be generated, and further increases the quality of the answers that
are generated. We build a production-ready, easily-plugged pipeline that can be
used on any given body of text, that is scalable and immune from generating any
hate speech, profanity, or misinformation. Based on the results, we are able to
generate more than six times the number of quality questions generated by the
abstractive approach, with a perceived quality being 44% higher, according to a
survey of 168 participants.
</summary>
    <author>
      <name>Sreehari Sankar</name>
    </author>
    <author>
      <name>Zhihang Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 8 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.09467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.09467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.00419v2</id>
    <updated>2023-02-20T09:31:34Z</updated>
    <published>2022-12-01T10:49:18Z</published>
    <title>Propaganda and Misinformation on Facebook and Twitter during the Russian
  Invasion of Ukraine</title>
    <summary>  Online social media represent an oftentimes unique source of information, and
having access to reliable and unbiased content is crucial, especially during
crises and contentious events. We study the spread of propaganda and
misinformation that circulated on Facebook and Twitter during the first few
months of the Russia-Ukraine conflict. By leveraging two large datasets of
millions of social media posts, we estimate the prevalence of Russian
propaganda and low-credibility content on the two platforms, describing
temporal patterns and highlighting the disproportionate role played by
superspreaders in amplifying unreliable content. We infer the political leaning
of Facebook pages and Twitter users sharing propaganda and misinformation, and
observe they tend to be more right-leaning than the average. By estimating the
amount of content moderated by the two platforms, we show that only about 8-15%
of the posts and tweets sharing links to Russian propaganda or untrustworthy
sources were removed. Overall, our findings show that Facebook and Twitter are
still vulnerable to abuse, especially during crises: we highlight the need to
urgently address this issue to preserve the integrity of online conversations.
</summary>
    <author>
      <name>Francesco Pierri</name>
    </author>
    <author>
      <name>Luca Luceri</name>
    </author>
    <author>
      <name>Nikhil Jindal</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3578503.3583597</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3578503.3583597" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at WebSci'2023</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WebSci '23: Proceedings of the 15th ACM Web Science Conference
  2023, April 2023, Pages 65-74</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2212.00419v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.00419v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.12984v1</id>
    <updated>2023-01-30T15:28:47Z</updated>
    <published>2023-01-30T15:28:47Z</published>
    <title>ContCommRTD: A Distributed Content-based Misinformation-aware Community
  Detection System for Real-Time Disaster Reporting</title>
    <summary>  Real-time social media data can provide useful information on evolving
hazards. Alongside traditional methods of disaster detection, the integration
of social media data can considerably enhance disaster management. In this
paper, we investigate the problem of detecting geolocation-content communities
on Twitter and propose a novel distributed system that provides in near
real-time information on hazard-related events and their evolution. We show
that content-based community analysis leads to better and faster dissemination
of reports on hazards. Our distributed disaster reporting system analyzes the
social relationship among worldwide geolocated tweets, and applies topic
modeling to group tweets by topics. Considering for each tweet the following
information: user, timestamp, geolocation, retweets, and replies, we create a
publisher-subscriber distribution model for topics. We use content similarity
and the proximity of nodes to create a new model for geolocation-content based
communities. Users can subscribe to different topics in specific geographical
areas or worldwide and receive real-time reports regarding these topics. As
misinformation can lead to increase damage if propagated in hazards related
tweets, we propose a new deep learning model to detect fake news. The
misinformed tweets are then removed from display. We also show empirically the
scalability capabilities of the proposed system.
</summary>
    <author>
      <name>Elena-Simona Apostol</name>
    </author>
    <author>
      <name>Ciprian-Octavian TruicƒÉ</name>
    </author>
    <author>
      <name>Adrian Paschke</name>
    </author>
    <link href="http://arxiv.org/abs/2301.12984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.12984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91D30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.08597v1</id>
    <updated>2023-02-16T21:47:59Z</updated>
    <published>2023-02-16T21:47:59Z</published>
    <title>True or false? Cognitive load when reading COVID-19 news headlines: an
  eye-tracking study</title>
    <summary>  Misinformation is an important topic in the Information Retrieval (IR)
context and has implications for both system-centered and user-centered IR.
While it has been established that the performance in discerning misinformation
is affected by a person's cognitive load, the variation in cognitive load in
judging the veracity of news is less understood. To understand the variation in
cognitive load imposed by reading news headlines related to COVID-19 claims,
within the context of a fact-checking system, we conducted a within-subject,
lab-based, quasi-experiment (N=40) with eye-tracking. Our results suggest that
examining true claims imposed a higher cognitive load on participants when news
headlines provided incorrect evidence for a claim and were inconsistent with
the person's prior beliefs. In contrast, checking false claims imposed a higher
cognitive load when the news headlines provided correct evidence for a claim
and were consistent with the participants' prior beliefs. However, changing
beliefs after examining a claim did not have a significant relationship with
cognitive load while reading the news headlines. The results illustrate that
reading news headlines related to true and false claims in the fact-checking
context impose different levels of cognitive load. Our findings suggest that
user engagement with tools for discerning misinformation needs to account for
the possible variation in the mental effort involved in different information
contexts.
</summary>
    <author>
      <name>Li Shi</name>
    </author>
    <author>
      <name>Nilavra Bhattacharya</name>
    </author>
    <author>
      <name>Anubrata Das</name>
    </author>
    <author>
      <name>Jacek Gwizdka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3576840.3578290</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3576840.3578290" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In ACM SIGIR Conference on Human Information Interaction and
  Retrieval (CHIIR 23), March 19-23, 2023, Austin, TX, USA. ACM, New York, NY,
  USA, 10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.08597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.03697v1</id>
    <updated>2023-03-07T07:26:09Z</updated>
    <published>2023-03-07T07:26:09Z</published>
    <title>Stylometric Detection of AI-Generated Text in Twitter Timelines</title>
    <summary>  Recent advancements in pre-trained language models have enabled convenient
methods for generating human-like text at a large scale. Though these
generation capabilities hold great potential for breakthrough applications, it
can also be a tool for an adversary to generate misinformation. In particular,
social media platforms like Twitter are highly susceptible to AI-generated
misinformation. A potential threat scenario is when an adversary hijacks a
credible user account and incorporates a natural language generator to generate
misinformation. Such threats necessitate automated detectors for AI-generated
tweets in a given user's Twitter timeline. However, tweets are inherently
short, thus making it difficult for current state-of-the-art pre-trained
language model-based detectors to accurately detect at what point the AI starts
to generate tweets in a given Twitter timeline. In this paper, we present a
novel algorithm using stylometric signals to aid detecting AI-generated tweets.
We propose models corresponding to quantifying stylistic changes in human and
AI tweets in two related tasks: Task 1 - discriminate between human and
AI-generated tweets, and Task 2 - detect if and when an AI starts to generate
tweets in a given Twitter timeline. Our extensive experiments demonstrate that
the stylometric features are effective in augmenting the state-of-the-art
AI-generated text detectors.
</summary>
    <author>
      <name>Tharindu Kumarage</name>
    </author>
    <author>
      <name>Joshua Garland</name>
    </author>
    <author>
      <name>Amrita Bhattacharjee</name>
    </author>
    <author>
      <name>Kirill Trapeznikov</name>
    </author>
    <author>
      <name>Scott Ruston</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2303.03697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.03697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.12683v1</id>
    <updated>2023-03-22T16:30:02Z</updated>
    <published>2023-03-22T16:30:02Z</published>
    <title>Knowing what to know: Implications of the choice of prior distribution
  on the behavior of adaptive design optimization</title>
    <summary>  Adaptive design optimization (ADO) is a state-of-the-art technique for
experimental design (Cavagnaro, Myung, Pitt, &amp; Kujala, 2010). ADO dynamically
identifies stimuli that, in expectation, yield the most information about a
hypothetical construct of interest (e.g., parameters of a cognitive model). To
calculate this expectation, ADO leverages the modeler's existing knowledge,
specified in the form of a prior distribution. Informative priors align with
the distribution of the focal construct in the participant population. This
alignment is assumed by ADO's internal assessment of expected information gain.
If the prior is instead misinformative, i.e., does not align with the
participant population, ADO's estimates of expected information gain could be
inaccurate. In many cases, the true distribution that characterizes the
participant population is unknown, and experimenters rely on heuristics in
their choice of prior and without an understanding of how this choice affects
ADO's behavior.
  Our work introduces a mathematical framework that facilitates investigation
of the consequences of the choice of prior distribution on the efficiency of
experiments designed using ADO. Through theoretical and empirical results, we
show that, in the context of prior misinformation, measures of expected
information gain are distinct from the correctness of the corresponding
inference. Through a series of simulation experiments, we show that, in the
case of parameter estimation, ADO nevertheless outperforms other design
methods. Conversely, in the case of model selection, misinformative priors can
lead inference to favor the wrong model, and rather than mitigating this
pitfall, ADO exacerbates it.
</summary>
    <author>
      <name>Sabina J. Sloman</name>
    </author>
    <author>
      <name>Daniel Cavagnaro</name>
    </author>
    <author>
      <name>Stephen B. Broomell</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3758/s13428-024-02410-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3758/s13428-024-02410-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for journal publication</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Behavior Research Methods (2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.12683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.12683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.13524v1</id>
    <updated>2023-02-23T17:35:27Z</updated>
    <published>2023-02-23T17:35:27Z</published>
    <title>Talking Abortion (Mis)information with ChatGPT on TikTok</title>
    <summary>  In this study, we tested users' perception of accuracy and engagement with
TikTok videos in which ChatGPT responded to prompts about "at-home" abortion
remedies. The chatbot's responses, though somewhat vague and confusing,
nonetheless recommended consulting with health professionals before attempting
an "at-home" abortion. We used ChatGPT to create two TikTok video variants -
one where users can see ChatGPT explicitly typing back a response, and one
where the text response is presented without any notion to the chatbot. We
randomly exposed 100 participants to each variant and found that the group of
participants unaware of ChatGPT's text synthetization was more inclined to
believe the responses were misinformation. Under the same impression, TikTok
itself attached misinformation warning labels ("Get the facts about abortion")
to all videos after we collected our initial results. We then decided to test
the videos again with another set of 50 participants and found that the labels
did not affect the perceptions of abortion misinformation except in the case
where ChatGPT explicitly responded to a prompt for a lyrical output. We also
found that more than 60% of the participants expressed negative or hesitant
opinions about chatbots as sources of credible health information.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Jennifer Vander Loop</name>
    </author>
    <author>
      <name>Peter Jachim</name>
    </author>
    <author>
      <name>Amy Devine</name>
    </author>
    <author>
      <name>Emma Pieroni</name>
    </author>
    <link href="http://arxiv.org/abs/2303.13524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.13524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.10249v1</id>
    <updated>2023-04-19T02:53:59Z</updated>
    <published>2023-04-19T02:53:59Z</published>
    <title>Harnessing the Power of Text-image Contrastive Models for Automatic
  Detection of Online Misinformation</title>
    <summary>  As growing usage of social media websites in the recent decades, the amount
of news articles spreading online rapidly, resulting in an unprecedented scale
of potentially fraudulent information. Although a plenty of studies have
applied the supervised machine learning approaches to detect such content, the
lack of gold standard training data has hindered the development. Analysing the
single data format, either fake text description or fake image, is the
mainstream direction for the current research. However, the misinformation in
real-world scenario is commonly formed as a text-image pair where the news
article/news title is described as text content, and usually followed by the
related image. Given the strong ability of learning features without labelled
data, contrastive learning, as a self-learning approach, has emerged and
achieved success on the computer vision. In this paper, our goal is to explore
the constrastive learning in the domain of misinformation identification. We
developed a self-learning model and carried out the comprehensive experiments
on a public data set named COSMOS. Comparing to the baseline classifier, our
model shows the superior performance of non-matched image-text pair detection
(approximately 10%) when the training data is insufficient. In addition, we
observed the stability for contrsative learning and suggested the use of it
offers large reductions in the number of training data, whilst maintaining
comparable classification results.
</summary>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Peng Zheng</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Shu Hu</name>
    </author>
    <author>
      <name>Bin Zhu</name>
    </author>
    <author>
      <name>Jinrong Hu</name>
    </author>
    <author>
      <name>Xi Wu</name>
    </author>
    <author>
      <name>Siwei Lyu</name>
    </author>
    <link href="http://arxiv.org/abs/2304.10249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.10249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.06459v3</id>
    <updated>2025-06-10T14:34:02Z</updated>
    <published>2023-08-12T03:55:26Z</published>
    <title>Using co-sharing to identify use of mainstream news for promoting
  potentially misleading narratives</title>
    <summary>  Much of the research quantifying volume and spread of online misinformation
measures the construct at the source level, identifying a set of specific
unreliable domains that account for a relatively small share of news
consumption. This source-level dichotomy obscures the potential for users to
repurpose factually true information from reliable sources to advance
misleading narratives. We demonstrate this potentially far more prevalent form
of misinformation by identifying articles from reliable sources that are
frequently co-shared with (shared by users who also shared) "fake" news on
social media, and concurrently extracting narratives present in fake news
content and claims fact-checked as false. Specifically in this study, we use
Twitter/X data from May 2018 to November 2021 matched to a U.S. voter file. We
find that narratives present in misinformation content are significantly more
likely to occur in co-shared articles than in articles from the same reliable
sources that are not co-shared, consistent with users using information from
mainstream sources to enhance the credibility and reach of potentially
misleading claims.
</summary>
    <author>
      <name>Pranav Goel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Network Science Institute, Northeastern University</arxiv:affiliation>
    </author>
    <author>
      <name>Jon Green</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Political Science, Duke University</arxiv:affiliation>
    </author>
    <author>
      <name>David Lazer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Network Science Institute, Northeastern University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute for Quantitative Social Science, Harvard University</arxiv:affiliation>
    </author>
    <author>
      <name>Philip Resnik</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Linguistics, University of Maryland</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute for Advanced Computer Studies, University of Maryland</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41562-025-02223-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41562-025-02223-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the Nature Human Behavior journal:
  https://www.nature.com/articles/s41562-025-02223-4</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature Human Behavior (2025)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2308.06459v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.06459v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.05542v1</id>
    <updated>2023-10-09T09:08:11Z</updated>
    <published>2023-10-09T09:08:11Z</published>
    <title>Harmful Conspiracies in Temporal Interaction Networks: Understanding the
  Dynamics of Digital Wildfires through Phase Transitions</title>
    <summary>  Shortly after the first COVID-19 cases became apparent in December 2020,
rumors spread on social media suggesting a connection between the virus and the
5G radiation emanating from the recently deployed telecommunications network.
In the course of the following weeks, this idea gained increasing popularity,
and various alleged explanations for how such a connection manifests emerged.
Ultimately, after being amplified by prominent conspiracy theorists, a series
of arson attacks on telecommunication equipment follows, concluding with the
kidnapping of telecommunication technicians in Peru. In this paper, we study
the spread of content related to a conspiracy theory with harmful consequences,
a so-called digital wildfire. In particular, we investigate the 5G and COVID-19
misinformation event on Twitter before, during, and after its peak in April and
May 2020. For this purpose, we examine the community dynamics in complex
temporal interaction networks underlying Twitter user activity. We assess the
evolution of such digital wildfires by appropriately defining the temporal
dynamics of communication in communities within social networks. We show that,
for this specific misinformation event, the number of interactions of the users
participating in a digital wildfire, as well as the size of the engaged
communities, both follow a power-law distribution. Moreover, our research
elucidates the possibility of quantifying the phases of a digital wildfire, as
per established literature. We identify one such phase as a critical
transition, marked by a shift from sporadic tweets to a global spread event,
highlighting the dramatic scaling of misinformation propagation.
</summary>
    <author>
      <name>Kaspara Skovli G√•sv√¶r</name>
    </author>
    <author>
      <name>Pedro G. Lind</name>
    </author>
    <author>
      <name>Johannes Langguth</name>
    </author>
    <author>
      <name>Morten Hjorth-Jensen</name>
    </author>
    <author>
      <name>Michael Kreil</name>
    </author>
    <author>
      <name>Daniel Thilo Schroeder</name>
    </author>
    <link href="http://arxiv.org/abs/2310.05542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.05542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.08821v2</id>
    <updated>2024-09-13T13:23:51Z</updated>
    <published>2023-10-13T02:28:54Z</published>
    <title>Is Fact-Checking Politically Neutral? Asymmetries in How U.S.
  Fact-Checking Organizations Pick Up False Statements Mentioning Political
  Elites</title>
    <summary>  Political elites play an important role in the proliferation of online
misinformation. However, an understanding of how fact-checking platforms pick
up politicized misinformation for fact-checking is still in its infancy. Here,
we conduct an empirical analysis of mentions of U.S. political elites within
fact-checked statements. For this purpose, we collect a comprehensive dataset
consisting of 35,014 true and false statements that have been fact-checked by
two major fact-checking organizations (Snopes, PolitiFact) in the U.S. between
2008 and 2023, i.e., within an observation period of 15 years. Subsequently, we
perform content analysis and explanatory regression modeling to analyze how
veracity is linked to mentions of U.S. political elites in fact-checked
statements. Our analysis yields the following main findings: (i) Fact-checked
false statements are, on average, 20% more likely to mention political elites
than true fact-checked statements. (ii) There is a partisan asymmetry such that
fact-checked false statements are 88.1% more likely to mention Democrats, but
26.5% less likely to mention Republicans, compared to fact-checked true
statements. (iii) Mentions of political elites in fact-checked false statements
reach the highest level during the months preceding elections. (iv)
Fact-checked false statements that mention political elites carry stronger
other-condemning emotions and are more likely to be pro-Republican, compared to
fact-checked true statements. In sum, our study offers new insights into
understanding mentions of political elites in false statements on U.S.
fact-checking platforms, and bridges important findings at the intersection
between misinformation and politicization.
</summary>
    <author>
      <name>Yuwei Chuai</name>
    </author>
    <author>
      <name>Jichang Zhao</name>
    </author>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <author>
      <name>Gabriele Lenzini</name>
    </author>
    <link href="http://arxiv.org/abs/2310.08821v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.08821v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.05724v2</id>
    <updated>2025-01-29T05:03:32Z</updated>
    <published>2023-11-09T20:16:06Z</published>
    <title>Susceptibility to Unreliable Information Sources: Swift Adoption with
  Minimal Exposure</title>
    <summary>  Misinformation proliferation on social media platforms is a pervasive threat
to the integrity of online public discourse. Genuine users, susceptible to
others' influence, often unknowingly engage with, endorse, and re-share
questionable pieces of information, collectively amplifying the spread of
misinformation. In this study, we introduce an empirical framework to
investigate users' susceptibility to influence when exposed to unreliable and
reliable information sources. Leveraging two datasets on political and public
health discussions on Twitter, we analyze the impact of exposure on the
adoption of information sources, examining how the reliability of the source
modulates this relationship. Our findings provide evidence that increased
exposure augments the likelihood of adoption. Users tend to adopt
low-credibility sources with fewer exposures than high-credibility sources, a
trend that persists even among non-partisan users. Furthermore, the number of
exposures needed for adoption varies based on the source credibility, with
extreme ends of the spectrum (very high or low credibility) requiring fewer
exposures for adoption. Additionally, we reveal that the adoption of
information sources often mirrors users' prior exposure to sources with
comparable credibility levels. Our research offers critical insights for
mitigating the endorsement of misinformation by vulnerable users, offering a
framework to study the dynamics of content exposure and adoption on social
media platforms.
</summary>
    <author>
      <name>Jinyi Ye</name>
    </author>
    <author>
      <name>Luca Luceri</name>
    </author>
    <author>
      <name>Julie Jiang</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3589334.3648154</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3589334.3648154" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the ACM Web Conference 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2311.05724v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.05724v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.09630v3</id>
    <updated>2024-10-13T19:42:46Z</updated>
    <published>2023-11-16T07:22:56Z</published>
    <title>Decoding Susceptibility: Modeling Misbelief to Misinformation Through a
  Computational Approach</title>
    <summary>  Susceptibility to misinformation describes the degree of belief in
unverifiable claims, a latent aspect of individuals' mental processes that is
not observable. Existing susceptibility studies heavily rely on self-reported
beliefs, which can be subject to bias, expensive to collect, and challenging to
scale for downstream applications. To address these limitations, in this work,
we propose a computational approach to model users' latent susceptibility
levels. As shown in previous research, susceptibility is influenced by various
factors (e.g., demographic factors, political ideology), and directly
influences people's reposting behavior on social media. To represent the
underlying mental process, our susceptibility modeling incorporates these
factors as inputs, guided by the supervision of people's sharing behavior.
Using COVID-19 as a testbed domain, our experiments demonstrate a significant
alignment between the susceptibility scores estimated by our computational
modeling and human judgments, confirming the effectiveness of this latent
modeling approach. Furthermore, we apply our model to annotate susceptibility
scores on a large-scale dataset and analyze the relationships between
susceptibility with various factors. Our analysis reveals that political
leanings and psychological factors exhibit varying degrees of association with
susceptibility to COVID-19 misinformation.
</summary>
    <author>
      <name>Yanchen Liu</name>
    </author>
    <author>
      <name>Mingyu Derek Ma</name>
    </author>
    <author>
      <name>Wenna Qin</name>
    </author>
    <author>
      <name>Azure Zhou</name>
    </author>
    <author>
      <name>Jiaao Chen</name>
    </author>
    <author>
      <name>Weiyan Shi</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Diyi Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.09630v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.09630v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.11678v4</id>
    <updated>2024-03-18T15:19:23Z</updated>
    <published>2023-12-18T19:44:32Z</published>
    <title>Misinformation as a harm: structured approaches for fact-checking
  prioritization</title>
    <summary>  In this work, we examine how fact-checkers prioritize which claims to
fact-check and what tools may assist them in their efforts. Through a series of
interviews with 23 professional fact-checkers from around the world, we
validate that harm assessment is a central component of how fact-checkers
triage their work. We also clarify the processes behind fact-checking
prioritization, finding that they are typically ad hoc, and gather suggestions
for tools that could help with these processes.
  To address the needs articulated by fact-checkers, we present a structured
framework of questions to help fact-checkers negotiate the priority of claims
through assessing potential harms. Our FABLE Framework of Misinformation Harms
incorporates five dimensions of magnitude -- (social) Fragmentation,
Actionability, Believability, Likelihood of spread, and Exploitativeness --
that can help determine the potential urgency of a specific message or claim
when considering misinformation as harm. The result is a practical and
conceptual tool to support fact-checkers and others as they make strategic
decisions to prioritize their efforts. We conclude with a discussion of
computational approaches to support structured prioritization, as well as
applications beyond fact-checking to content moderation and curation.
</summary>
    <author>
      <name>Connie Moon Sehat</name>
    </author>
    <author>
      <name>Ryan Li</name>
    </author>
    <author>
      <name>Peipei Nie</name>
    </author>
    <author>
      <name>Tarunima Prabhakar</name>
    </author>
    <author>
      <name>Amy X. Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CSCW 2024, with clean up for typos and figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.11678v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.11678v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.16558v1</id>
    <updated>2024-01-29T20:50:28Z</updated>
    <published>2024-01-29T20:50:28Z</published>
    <title>Diverse, but Divisive: LLMs Can Exaggerate Gender Differences in Opinion
  Related to Harms of Misinformation</title>
    <summary>  The pervasive spread of misinformation and disinformation poses a significant
threat to society. Professional fact-checkers play a key role in addressing
this threat, but the vast scale of the problem forces them to prioritize their
limited resources. This prioritization may consider a range of factors, such as
varying risks of harm posed to specific groups of people. In this work, we
investigate potential implications of using a large language model (LLM) to
facilitate such prioritization. Because fact-checking impacts a wide range of
diverse segments of society, it is important that diverse views are represented
in the claim prioritization process. This paper examines whether a LLM can
reflect the views of various groups when assessing the harms of misinformation,
focusing on gender as a primary variable. We pose two central questions: (1) To
what extent do prompts with explicit gender references reflect gender
differences in opinion in the United States on topics of social relevance? and
(2) To what extent do gender-neutral prompts align with gendered viewpoints on
those topics? To analyze these questions, we present the TopicMisinfo dataset,
containing 160 fact-checked claims from diverse topics, supplemented by nearly
1600 human annotations with subjective perceptions and annotator demographics.
Analyzing responses to gender-specific and neutral prompts, we find that GPT
3.5-Turbo reflects empirically observed gender differences in opinion but
amplifies the extent of these differences. These findings illuminate AI's
complex role in moderating online communication, with implications for
fact-checkers, algorithm designers, and the use of crowd-workers as annotators.
We also release the TopicMisinfo dataset to support continuing research in the
community.
</summary>
    <author>
      <name>Terrence Neumann</name>
    </author>
    <author>
      <name>Sooyong Lee</name>
    </author>
    <author>
      <name>Maria De-Arteaga</name>
    </author>
    <author>
      <name>Sina Fazelpour</name>
    </author>
    <author>
      <name>Matthew Lease</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Review</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.16558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.16558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.09874v1</id>
    <updated>2024-02-15T10:58:22Z</updated>
    <published>2024-02-15T10:58:22Z</published>
    <title>Camouflage is all you need: Evaluating and Enhancing Language Model
  Robustness Against Camouflage Adversarial Attacks</title>
    <summary>  Adversarial attacks represent a substantial challenge in Natural Language
Processing (NLP). This study undertakes a systematic exploration of this
challenge in two distinct phases: vulnerability evaluation and resilience
enhancement of Transformer-based models under adversarial attacks.
  In the evaluation phase, we assess the susceptibility of three Transformer
configurations, encoder-decoder, encoder-only, and decoder-only setups, to
adversarial attacks of escalating complexity across datasets containing
offensive language and misinformation. Encoder-only models manifest a 14% and
21% performance drop in offensive language detection and misinformation
detection tasks, respectively. Decoder-only models register a 16% decrease in
both tasks, while encoder-decoder models exhibit a maximum performance drop of
14% and 26% in the respective tasks.
  The resilience-enhancement phase employs adversarial training, integrating
pre-camouflaged and dynamically altered data. This approach effectively reduces
the performance drop in encoder-only models to an average of 5% in offensive
language detection and 2% in misinformation detection tasks. Decoder-only
models, occasionally exceeding original performance, limit the performance drop
to 7% and 2% in the respective tasks. Although not surpassing the original
performance, Encoder-decoder models can reduce the drop to an average of 6% and
2% respectively.
  Results suggest a trade-off between performance and robustness, with some
models maintaining similar performance while gaining robustness. Our study and
adversarial training techniques have been incorporated into an open-source tool
for generating camouflaged datasets. However, methodology effectiveness depends
on the specific camouflage technique and data encountered, emphasizing the need
for continued exploration.
</summary>
    <author>
      <name>√Ålvaro Huertas-Garc√≠a</name>
    </author>
    <author>
      <name>Alejandro Mart√≠n</name>
    </author>
    <author>
      <name>Javier Huertas-Tato</name>
    </author>
    <author>
      <name>David Camacho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.09874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.09874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.09956v1</id>
    <updated>2024-02-15T14:05:14Z</updated>
    <published>2024-02-15T14:05:14Z</published>
    <title>A Quantum Approach to News Verification from the Perspective of a News
  Aggregator</title>
    <summary>  In the dynamic landscape of digital information, the rise of misinformation
and fake news presents a pressing challenge. This paper takes a completely new
approach to verifying news, inspired by how quantum actors can reach agreement
even when they are spatially spread out. We propose a radically new, to the
best of our knowledge, algorithm that uses quantum ``entanglement'' (think of
it as a special connection) to help news aggregators sniff out bad actors,
whether they be other news sources or even fact-checkers trying to spread
misinformation. This algorithm doesn't rely on quantum signatures, it just uses
basic quantum technology we already have, in particular, special pairs of
particles called ``EPR pairs'' that are much easier to create than other
options. More complex entangled states are like juggling too many balls -
they're hard to make and slow things down, especially when many players are
involved. For instance, bigger, more complex states like ``GHZ states'' work
for small groups, but they become messy with larger numbers. So, we stick with
Bell states, the simplest form of entanglement, which are easy to generate no
matter how many players are in the game. This means our algorithm is faster to
set up, works for any number of participants, and is more practical for
real-world use. Bonus points: it finishes in a fixed number of steps,
regardless of how many players are involved, making it even more scalable. This
new approach may lead to a powerful and efficient way to fight misinformation
in the digital age, using the weird and wonderful world of quantum mechanics.
</summary>
    <author>
      <name>Theodore Andronikos</name>
    </author>
    <author>
      <name>Alla Sirokofskich</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/info15040207</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/info15040207" rel="related"/>
    <link href="http://arxiv.org/abs/2402.09956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.09956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.06765v3</id>
    <updated>2024-08-12T10:55:38Z</updated>
    <published>2024-03-11T14:35:45Z</published>
    <title>ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large
  Language Model</title>
    <summary>  The internet has brought both benefits and harms to society. A prime example
of the latter is misinformation, including conspiracy theories, which flood the
web. Recent advances in natural language processing, particularly the emergence
of large language models (LLMs), have improved the prospects of accurate
misinformation detection. However, most LLM-based approaches to conspiracy
theory detection focus only on binary classification and fail to account for
the important relationship between misinformation and affective features (i.e.,
sentiment and emotions). Driven by a comprehensive analysis of conspiracy text
that reveals its distinctive affective features, we propose ConspEmoLLM, the
first open-source LLM that integrates affective information and is able to
perform diverse tasks relating to conspiracy theories. These tasks include not
only conspiracy theory detection, but also classification of theory type and
detection of related discussion (e.g., opinions towards theories). ConspEmoLLM
is fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,
which includes five tasks to support LLM instruction tuning and evaluation. We
demonstrate that when applied to these tasks, ConspEmoLLM largely outperforms
several open-source general domain LLMs and ChatGPT, as well as an LLM that has
been fine-tuned using ConDID, but which does not use affective features. This
project will be released on https://github.com/lzw108/ConspEmoLLM/.
</summary>
    <author>
      <name>Zhiwei Liu</name>
    </author>
    <author>
      <name>Boyang Liu</name>
    </author>
    <author>
      <name>Paul Thompson</name>
    </author>
    <author>
      <name>Kailai Yang</name>
    </author>
    <author>
      <name>Sophia Ananiadou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/FAIA241060</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/FAIA241060" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.06765v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.06765v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09343v1</id>
    <updated>2024-06-13T17:25:12Z</updated>
    <published>2024-06-13T17:25:12Z</published>
    <title>Frameworks, Modeling and Simulations of Misinformation and
  Disinformation: A Systematic Literature Review</title>
    <summary>  The prevalence of misinformation and disinformation poses a significant
challenge in today's digital landscape. That is why several methods and tools
are proposed to analyze and understand these phenomena from a scientific
perspective. To assess how the mis/disinformation is being conceptualized and
evaluated in the literature, this paper surveys the existing frameworks, models
and simulations of mis/disinformation dynamics by performing a systematic
literature review up to 2023. After applying the PRISMA methodology, 57
research papers are inspected to determine (1) the terminology and definitions
of mis/disinformation, (2) the methods used to represent mis/disinformation,
(3) the primary purpose beyond modeling and simulating mis/disinformation, (4)
the context where the mis/disinformation is studied, and (5) the validation of
the proposed methods for understanding mis/disinformation.
  The main findings reveal a consistent essence definition of misinformation
and disinformation across studies, with intent as the key distinguishing
factor. Research predominantly uses social frameworks, epidemiological models,
and belief updating simulations. These studies aim to estimate the
effectiveness of mis/disinformation, primarily in health and politics. The
preferred validation strategy is to compare methods with real-world data and
statistics. Finally, this paper identifies current trends and open challenges
in the mis/disinformation research field, providing recommendations for future
work agenda.
</summary>
    <author>
      <name>Alejandro Buitrago L√≥pez</name>
    </author>
    <author>
      <name>Javier Pastor-Galindo</name>
    </author>
    <author>
      <name>Jos√© A. Ruip√©rez-Valiente</name>
    </author>
    <link href="http://arxiv.org/abs/2406.09343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.02662v1</id>
    <updated>2024-07-02T20:51:06Z</updated>
    <published>2024-07-02T20:51:06Z</published>
    <title>Supporters and Skeptics: LLM-based Analysis of Engagement with Mental
  Health (Mis)Information Content on Video-sharing Platforms</title>
    <summary>  Over one in five adults in the US lives with a mental illness. In the face of
a shortage of mental health professionals and offline resources, online
short-form video content has grown to serve as a crucial conduit for
disseminating mental health help and resources. However, the ease of content
creation and access also contributes to the spread of misinformation, posing
risks to accurate diagnosis and treatment. Detecting and understanding
engagement with such content is crucial to mitigating their harmful effects on
public health. We perform the first quantitative study of the phenomenon using
YouTube Shorts and Bitchute as the sites of study. We contribute MentalMisinfo,
a novel labeled mental health misinformation (MHMisinfo) dataset of 739 videos
(639 from Youtube and 100 from Bitchute) and 135372 comments in total, using an
expert-driven annotation schema. We first found that few-shot in-context
learning with large language models (LLMs) are effective in detecting MHMisinfo
videos. Next, we discover distinct and potentially alarming linguistic patterns
in how audiences engage with MHMisinfo videos through commentary on both
video-sharing platforms. Across the two platforms, comments could exacerbate
prevailing stigma with some groups showing heightened susceptibility to and
alignment with MHMisinfo. We discuss technical and public health-driven
adaptive solutions to tackling the "epidemic" of mental health misinformation
online.
</summary>
    <author>
      <name>Viet Cuong Nguyen</name>
    </author>
    <author>
      <name>Mini Jain</name>
    </author>
    <author>
      <name>Abhijat Chauhan</name>
    </author>
    <author>
      <name>Heather Jaime Soled</name>
    </author>
    <author>
      <name>Santiago Alvarez Lesmes</name>
    </author>
    <author>
      <name>Zihang Li</name>
    </author>
    <author>
      <name>Michael L. Birnbaum</name>
    </author>
    <author>
      <name>Sunny X. Tang</name>
    </author>
    <author>
      <name>Srijan Kumar</name>
    </author>
    <author>
      <name>Munmun De Choudhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, in submission to ICWSM</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.02662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.02662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.10644v1</id>
    <updated>2024-07-15T12:03:23Z</updated>
    <published>2024-07-15T12:03:23Z</published>
    <title>TripletViNet: Mitigating Misinformation Video Spread Across Platforms</title>
    <summary>  There has been rampant propagation of fake news and misinformation videos on
many platforms lately, and moderation of such content faces many challenges
that must be overcome. Recent research has shown the feasibility of identifying
video titles from encrypted network traffic within a single platform, for
example, within YouTube or Facebook. However, there are no existing methods for
cross-platform video recognition, a crucial gap that this works aims to
address. Encrypted video traffic classification within a single platform, that
is, classifying the video title of a traffic trace of a video on one platform
by training on traffic traces of videos on the same platform, has significant
limitations due to the large number of video platforms available to users to
upload harmful content to. To attempt to address this limitation, we conduct a
feasibility analysis into and attempt to solve the challenge of recognizing
videos across multiple platforms by using the traffic traces of videos on one
platform only. We propose TripletViNet, a framework that encompasses i)
platform-wise pre-processing, ii) an encoder trained utilizing triplet learning
for improved accuracy and iii) multiclass classifier for classifying the video
title of a traffic trace. To evaluate the performance of TripletViNet, a
comprehensive dataset with traffic traces for 100 videos on six major platforms
with the potential for spreading misinformation such as YouTube, X, Instagram,
Facebook, Rumble, and Tumblr was collected and used to test TripletViNet in
both closed-set and open-set scenarios. TripletViNet achieves significant
improvements in accuracy due to the correlation between video traffic and the
video's VBR, with impressive final accuracies exceeding 90% in certain
scenarios.
</summary>
    <author>
      <name>Petar Smolovic</name>
    </author>
    <author>
      <name>Thilini Dahanayaka</name>
    </author>
    <author>
      <name>Kanchana Thilakarathna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3660512.3665519</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3660512.3665519" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has already been published in ACM SCID '24</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SCID '24, ACM, 1-12 (2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2407.10644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.10644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.00024v1</id>
    <updated>2024-07-31T05:39:07Z</updated>
    <published>2024-07-31T05:39:07Z</published>
    <title>Deceptive AI systems that give explanations are more convincing than
  honest AI systems and can amplify belief in misinformation</title>
    <summary>  Advanced Artificial Intelligence (AI) systems, specifically large language
models (LLMs), have the capability to generate not just misinformation, but
also deceptive explanations that can justify and propagate false information
and erode trust in the truth. We examined the impact of deceptive AI generated
explanations on individuals' beliefs in a pre-registered online experiment with
23,840 observations from 1,192 participants. We found that in addition to being
more persuasive than accurate and honest explanations, AI-generated deceptive
explanations can significantly amplify belief in false news headlines and
undermine true ones as compared to AI systems that simply classify the headline
incorrectly as being true/false. Moreover, our results show that personal
factors such as cognitive reflection and trust in AI do not necessarily protect
individuals from these effects caused by deceptive AI generated explanations.
Instead, our results show that the logical validity of AI generated deceptive
explanations, that is whether the explanation has a causal effect on the
truthfulness of the AI's classification, plays a critical role in countering
their persuasiveness - with logically invalid explanations being deemed less
credible. This underscores the importance of teaching logical reasoning and
critical thinking skills to identify logically invalid arguments, fostering
greater resilience against advanced AI-driven misinformation.
</summary>
    <author>
      <name>Valdemar Danry</name>
    </author>
    <author>
      <name>Pat Pataranutaporn</name>
    </author>
    <author>
      <name>Matthew Groh</name>
    </author>
    <author>
      <name>Ziv Epstein</name>
    </author>
    <author>
      <name>Pattie Maes</name>
    </author>
    <link href="http://arxiv.org/abs/2408.00024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.00024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.13091v1</id>
    <updated>2024-08-23T14:16:54Z</updated>
    <published>2024-08-23T14:16:54Z</published>
    <title>Analysis of child development facts and myths using text mining
  techniques and classification models</title>
    <summary>  The rapid dissemination of misinformation on the internet complicates the
decision-making process for individuals seeking reliable information,
particularly parents researching child development topics. This misinformation
can lead to adverse consequences, such as inappropriate treatment of children
based on myths. While previous research has utilized text-mining techniques to
predict child abuse cases, there has been a gap in the analysis of child
development myths and facts. This study addresses this gap by applying text
mining techniques and classification models to distinguish between myths and
facts about child development, leveraging newly gathered data from publicly
available websites. The research methodology involved several stages. First,
text mining techniques were employed to pre-process the data, ensuring enhanced
accuracy. Subsequently, the structured data was analysed using six robust
Machine Learning (ML) classifiers and one Deep Learning (DL) model, with two
feature extraction techniques applied to assess their performance across three
different training-testing splits. To ensure the reliability of the results,
cross-validation was performed using both k-fold and leave-one-out methods.
Among the classification models tested, Logistic Regression (LR) demonstrated
the highest accuracy, achieving a 90% accuracy with the Bag-of-Words (BoW)
feature extraction technique. LR stands out for its exceptional speed and
efficiency, maintaining low testing time per statement (0.97 microseconds).
These findings suggest that LR, when combined with BoW, is effective in
accurately classifying child development information, thus providing a valuable
tool for combating misinformation and assisting parents in making informed
decisions.
</summary>
    <author>
      <name>Mehedi Tajrian</name>
    </author>
    <author>
      <name>Azizur Rahman</name>
    </author>
    <author>
      <name>Muhammad Ashad Kabir</name>
    </author>
    <author>
      <name>Md Rafiqul Islam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.heliyon.2024.e36652</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.heliyon.2024.e36652" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Heliyon, 2024, 10 (17)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2408.13091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.13091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.03067v1</id>
    <updated>2024-09-04T20:38:14Z</updated>
    <published>2024-09-04T20:38:14Z</published>
    <title>A Comparative Study of Offline Models and Online LLMs in Fake News
  Detection</title>
    <summary>  Fake news detection remains a critical challenge in today's rapidly evolving
digital landscape, where misinformation can spread faster than ever before.
Traditional fake news detection models often rely on static datasets and
auxiliary information, such as metadata or social media interactions, which
limits their adaptability to real-time scenarios. Recent advancements in Large
Language Models (LLMs) have demonstrated significant potential in addressing
these challenges due to their extensive pre-trained knowledge and ability to
analyze textual content without relying on auxiliary data. However, many of
these LLM-based approaches are still rooted in static datasets, with limited
exploration into their real-time processing capabilities. This paper presents a
systematic evaluation of both traditional offline models and state-of-the-art
LLMs for real-time fake news detection. We demonstrate the limitations of
existing offline models, including their inability to adapt to dynamic
misinformation patterns. Furthermore, we show that newer LLM models with online
capabilities, such as GPT-4, Claude, and Gemini, are better suited for
detecting emerging fake news in real-time contexts. Our findings emphasize the
importance of transitioning from offline to online LLM models for real-time
fake news detection. Additionally, the public accessibility of LLMs enhances
their scalability and democratizes the tools needed to combat misinformation.
By leveraging real-time data, our work marks a significant step toward more
adaptive, effective, and scalable fake news detection systems.
</summary>
    <author>
      <name>Ruoyu Xu</name>
    </author>
    <author>
      <name>Gaoxiang Li</name>
    </author>
    <link href="http://arxiv.org/abs/2409.03067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.03067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.04860v1</id>
    <updated>2024-09-07T15:43:19Z</updated>
    <published>2024-09-07T15:43:19Z</published>
    <title>Sequential Classification of Misinformation</title>
    <summary>  In recent years there have been a growing interest in online auditing of
information flow over social networks with the goal of monitoring undesirable
effects, such as, misinformation and fake news. Most previous work on the
subject, focus on the binary classification problem of classifying information
as fake or genuine. Nonetheless, in many practical scenarios, the
multi-class/label setting is of particular importance. For example, it could be
the case that a social media platform may want to distinguish between ``true",
``partly-true", and ``false" information. Accordingly, in this paper, we
consider the problem of online multiclass classification of information flow.
To that end, driven by empirical studies on information flow over real-world
social media networks, we propose a probabilistic information flow model over
graphs. Then, the learning task is to detect the label of the information flow,
with the goal of minimizing a combination of the classification error and the
detection time. For this problem, we propose two detection algorithms; the
first is based on the well-known multiple sequential probability ratio test,
while the second is a novel graph neural network based sequential decision
algorithm. For both algorithms, we prove several strong statistical guarantees.
We also construct a data driven algorithm for learning the proposed
probabilistic model. Finally, we test our algorithms over two real-world
datasets, and show that they outperform other state-of-the-art misinformation
detection algorithms, in terms of detection time and classification error.
</summary>
    <author>
      <name>Daniel Toma</name>
    </author>
    <author>
      <name>Wasim Huleihel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.04860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.04860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.10850v2</id>
    <updated>2024-10-17T11:52:38Z</updated>
    <published>2024-10-06T07:40:11Z</published>
    <title>On the Reliability of Large Language Models to Misinformed and
  Demographically-Informed Prompts</title>
    <summary>  We investigate and observe the behaviour and performance of Large Language
Model (LLM)-backed chatbots in addressing misinformed prompts and questions
with demographic information within the domains of Climate Change and Mental
Health. Through a combination of quantitative and qualitative methods, we
assess the chatbots' ability to discern the veracity of statements, their
adherence to facts, and the presence of bias or misinformation in their
responses. Our quantitative analysis using True/False questions reveals that
these chatbots can be relied on to give the right answers to these close-ended
questions. However, the qualitative insights, gathered from domain experts,
shows that there are still concerns regarding privacy, ethical implications,
and the necessity for chatbots to direct users to professional services. We
conclude that while these chatbots hold significant promise, their deployment
in sensitive areas necessitates careful consideration, ethical oversight, and
rigorous refinement to ensure they serve as a beneficial augmentation to human
expertise rather than an autonomous solution.
</summary>
    <author>
      <name>Toluwani Aremu</name>
    </author>
    <author>
      <name>Oluwakemi Akinwehinmi</name>
    </author>
    <author>
      <name>Chukwuemeka Nwagu</name>
    </author>
    <author>
      <name>Syed Ishtiaque Ahmed</name>
    </author>
    <author>
      <name>Rita Orji</name>
    </author>
    <author>
      <name>Pedro Arnau Del Amo</name>
    </author>
    <author>
      <name>Abdulmotaleb El Saddik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Study conducted between August and December 2023. Under review at
  AAAI-AI Magazine. Submitted for archival purposes only</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.10850v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.10850v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.05060v4</id>
    <updated>2025-06-18T16:56:37Z</updated>
    <published>2024-11-07T18:47:39Z</published>
    <title>A Guide to Misinformation Detection Data and Evaluation</title>
    <summary>  Misinformation is a complex societal issue, and mitigating solutions are
difficult to create due to data deficiencies. To address this, we have curated
the largest collection of (mis)information datasets in the literature, totaling
75. From these, we evaluated the quality of 36 datasets that consist of
statements or claims, as well as the 9 datasets that consist of data in purely
paragraph form. We assess these datasets to identify those with solid
foundations for empirical work and those with flaws that could result in
misleading and non-generalizable results, such as spurious correlations, or
examples that are ambiguous or otherwise impossible to assess for veracity. We
find the latter issue is particularly severe and affects most datasets in the
literature. We further provide state-of-the-art baselines on all these
datasets, but show that regardless of label quality, categorical labels may no
longer give an accurate evaluation of detection model performance. Finally, we
propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the
field toward systemic solutions rather than inadvertently propagating issues in
evaluation. Overall, this guide aims to provide a roadmap for higher quality
data and better grounded evaluations, ultimately improving research in
misinformation detection. All datasets and other artifacts are available at
misinfo-datasets.complexdatalab.com.
</summary>
    <author>
      <name>Camille Thibault</name>
    </author>
    <author>
      <name>Jacob-Junqi Tian</name>
    </author>
    <author>
      <name>Gabrielle Peloquin-Skulski</name>
    </author>
    <author>
      <name>Taylor Lynn Curtis</name>
    </author>
    <author>
      <name>James Zhou</name>
    </author>
    <author>
      <name>Florence Laflamme</name>
    </author>
    <author>
      <name>Yuxiang Guan</name>
    </author>
    <author>
      <name>Reihaneh Rabbany</name>
    </author>
    <author>
      <name>Jean-Fran√ßois Godbout</name>
    </author>
    <author>
      <name>Kellin Pelrine</name>
    </author>
    <link href="http://arxiv.org/abs/2411.05060v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.05060v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.10032v1</id>
    <updated>2024-11-15T08:20:26Z</updated>
    <published>2024-11-15T08:20:26Z</published>
    <title>VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying
  Misinformation of Short Videos</title>
    <summary>  Short video platforms have become important channels for news dissemination,
offering a highly engaging and immediate way for users to access current events
and share information. However, these platforms have also emerged as
significant conduits for the rapid spread of misinformation, as fake news and
rumors can leverage the visual appeal and wide reach of short videos to
circulate extensively among audiences. Existing fake news detection methods
mainly rely on single-modal information, such as text or images, or apply only
basic fusion techniques, limiting their ability to handle the complex,
multi-layered information inherent in short videos. To address these
limitations, this paper presents a novel fake news detection method based on
multimodal information, designed to identify misinformation through a
multi-level analysis of video content. This approach effectively utilizes
different modal representations to generate a unified textual description,
which is then fed into a large language model for comprehensive evaluation. The
proposed framework successfully integrates multimodal features within videos,
significantly enhancing the accuracy and reliability of fake news detection.
Experimental results demonstrate that the proposed approach outperforms
existing models in terms of accuracy, robustness, and utilization of multimodal
information, achieving an accuracy of 90.93%, which is significantly higher
than the best baseline model (SV-FEND) at 81.05%. Furthermore, case studies
provide additional evidence of the effectiveness of the approach in accurately
distinguishing between fake news, debunking content, and real incidents,
highlighting its reliability and robustness in real-world applications.
</summary>
    <author>
      <name>Weihao Zhong</name>
    </author>
    <author>
      <name>Yinhao Xiao</name>
    </author>
    <author>
      <name>Minghui Xu</name>
    </author>
    <author>
      <name>Xiuzhen Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2211.10973 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.10032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.10032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.15396v1</id>
    <updated>2024-11-23T00:43:27Z</updated>
    <published>2024-11-23T00:43:27Z</published>
    <title>The Decoy Dilemma in Online Medical Information Evaluation: A
  Comparative Study of Credibility Assessments by LLM and Human Judges</title>
    <summary>  Can AI be cognitively biased in automated information judgment tasks? Despite
recent progresses in measuring and mitigating social and algorithmic biases in
AI and large language models (LLMs), it is not clear to what extent LLMs behave
"rationally", or if they are also vulnerable to human cognitive bias triggers.
To address this open problem, our study, consisting of a crowdsourcing user
experiment and a LLM-enabled simulation experiment, compared the credibility
assessments by LLM and human judges under potential decoy effects in an
information retrieval (IR) setting, and empirically examined the extent to
which LLMs are cognitively biased in COVID-19 medical (mis)information
assessment tasks compared to traditional human assessors as a baseline. The
results, collected from a between-subject user experiment and a LLM-enabled
replicate experiment, demonstrate that 1) Larger and more recent LLMs tend to
show a higher level of consistency and accuracy in distinguishing credible
information from misinformation. However, they are more likely to give higher
ratings for misinformation due to the presence of a more salient, decoy
misinformation result; 2) While decoy effect occurred in both human and LLM
assessments, the effect is more prevalent across different conditions and
topics in LLM judgments compared to human credibility ratings. In contrast to
the generally assumed "rationality" of AI tools, our study empirically confirms
the cognitive bias risks embedded in LLM agents, evaluates the decoy impact on
LLMs against human credibility assessments, and thereby highlights the
complexity and importance of debiasing AI agents and developing
psychology-informed AI audit techniques and policies for automated judgment
tasks and beyond.
</summary>
    <author>
      <name>Jiqun Liu</name>
    </author>
    <author>
      <name>Jiangen He</name>
    </author>
    <link href="http://arxiv.org/abs/2411.15396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.15396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.06274v2</id>
    <updated>2025-01-31T16:41:17Z</updated>
    <published>2025-01-10T08:00:58Z</published>
    <title>Polarized Patterns of Language Toxicity and Sentiment of Debunking Posts
  on Social Media</title>
    <summary>  The rise of misinformation and fake news in online political discourse poses
significant challenges to democratic processes and public engagement. While
debunking efforts aim to counteract misinformation and foster fact-based
dialogue, these discussions often involve language toxicity and emotional
polarization. We examined over 86 million debunking tweets and more than 4
million Reddit debunking comments to investigate the relationship between
language toxicity, pessimism, and social polarization in debunking efforts.
Focusing on discussions of the 2016 and 2020 U.S. presidential elections and
the QAnon conspiracy theory, our analysis reveals three key findings: (1)
peripheral participants (1-degree users) play a disproportionate role in
shaping toxic discourse, driven by lower community accountability and emotional
expression; (2) platform mechanisms significantly influence polarization, with
Twitter amplifying partisan differences and Reddit fostering higher overall
toxicity due to its structured, community-driven interactions; and (3) a
negative correlation exists between language toxicity and pessimism, with
increased interaction reducing toxicity, especially on Reddit. We show that
platform architecture affects informational complexity of user interactions,
with Twitter promoting concentrated, uniform discourse and Reddit encouraging
diverse, complex communication. Our findings highlight the importance of user
engagement patterns, platform dynamics, and emotional expressions in shaping
polarization in debunking discourse. This study offers insights for
policymakers and platform designers to mitigate harmful effects and promote
healthier online discussions, with implications for understanding
misinformation, hate speech, and political polarization in digital
environments.
</summary>
    <author>
      <name>Wentao Xu</name>
    </author>
    <author>
      <name>Wenlu Fan</name>
    </author>
    <author>
      <name>Shiqian Lu</name>
    </author>
    <author>
      <name>Tenghao Li</name>
    </author>
    <author>
      <name>Bin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.06274v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.06274v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.10455v1</id>
    <updated>2025-02-12T04:25:14Z</updated>
    <published>2025-02-12T04:25:14Z</published>
    <title>E2LVLM:Evidence-Enhanced Large Vision-Language Model for Multimodal
  Out-of-Context Misinformation Detection</title>
    <summary>  Recent studies in Large Vision-Language Models (LVLMs) have demonstrated
impressive advancements in multimodal Out-of-Context (OOC) misinformation
detection, discerning whether an authentic image is wrongly used in a claim.
Despite their success, the textual evidence of authentic images retrieved from
the inverse search is directly transmitted to LVLMs, leading to inaccurate or
false information in the decision-making phase. To this end, we present E2LVLM,
a novel evidence-enhanced large vision-language model by adapting textual
evidence in two levels. First, motivated by the fact that textual evidence
provided by external tools struggles to align with LVLMs inputs, we devise a
reranking and rewriting strategy for generating coherent and contextually
attuned content, thereby driving the aligned and effective behavior of LVLMs
pertinent to authentic images. Second, to address the scarcity of news domain
datasets with both judgment and explanation, we generate a novel OOC multimodal
instruction-following dataset by prompting LVLMs with informative content to
acquire plausible explanations. Further, we develop a multimodal
instruction-tuning strategy with convincing explanations for beyond detection.
This scheme contributes to E2LVLM for multimodal OOC misinformation detection
and explanation. A multitude of experiments demonstrate that E2LVLM achieves
superior performance than state-of-the-art methods, and also provides
compelling rationales for judgments.
</summary>
    <author>
      <name>Junjie Wu</name>
    </author>
    <author>
      <name>Yumeng Fu</name>
    </author>
    <author>
      <name>Nan Yu</name>
    </author>
    <author>
      <name>Guohong Fu</name>
    </author>
    <link href="http://arxiv.org/abs/2502.10455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03417v3</id>
    <updated>2025-06-05T10:17:20Z</updated>
    <published>2025-03-05T11:47:32Z</published>
    <title>When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding
  Models Against Misinformation Edits</title>
    <summary>  Online misinformation remains a critical challenge, and fact-checkers
increasingly rely on claim matching systems that use sentence embedding models
to retrieve relevant fact-checks. However, as users interact with claims
online, they often introduce edits, and it remains unclear whether current
embedding models used in retrieval are robust to such edits. To investigate
this, we introduce a perturbation framework that generates valid and natural
claim variations, enabling us to assess the robustness of a wide-range of
sentence embedding models in a multi-stage retrieval pipeline and evaluate the
effectiveness of various mitigation approaches. Our evaluation reveals that
standard embedding models exhibit notable performance drops on edited claims,
while LLM-distilled embedding models offer improved robustness at a higher
computational cost. Although a strong reranker helps to reduce the performance
drop, it cannot fully compensate for first-stage retrieval gaps. To address
these retrieval gaps, we evaluate train- and inference-time mitigation
approaches, demonstrating that they can improve in-domain robustness by up to
17 percentage points and boost out-of-domain generalization by 10 percentage
points. Overall, our findings provide practical improvements to claim-matching
systems, enabling more reliable fact-checking of evolving misinformation. Code
and data are available at
https://github.com/JabezNzomo99/claim-matching-robustness.
</summary>
    <author>
      <name>Jabez Magomere</name>
    </author>
    <author>
      <name>Emanuele La Malfa</name>
    </author>
    <author>
      <name>Manuel Tonneau</name>
    </author>
    <author>
      <name>Ashkan Kazemi</name>
    </author>
    <author>
      <name>Scott Hale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2025 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.03417v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03417v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01900v1</id>
    <updated>2025-05-03T19:14:24Z</updated>
    <published>2025-05-03T19:14:24Z</published>
    <title>CAMOUFLAGE: Exploiting Misinformation Detection Systems Through
  LLM-driven Adversarial Claim Transformation</title>
    <summary>  Automated evidence-based misinformation detection systems, which evaluate the
veracity of short claims against evidence, lack comprehensive analysis of their
adversarial vulnerabilities. Existing black-box text-based adversarial attacks
are ill-suited for evidence-based misinformation detection systems, as these
attacks primarily focus on token-level substitutions involving gradient or
logit-based optimization strategies, which are incapable of fooling the
multi-component nature of these detection systems. These systems incorporate
both retrieval and claim-evidence comparison modules, which requires attacks to
break the retrieval of evidence and/or the comparison module so that it draws
incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach
that employs a two-agent system, a Prompt Optimization Agent and an Attacker
Agent, to create adversarial claim rewritings that manipulate evidence
retrieval and mislead claim-evidence comparison, effectively bypassing the
system without altering the meaning of the claim. The Attacker Agent produces
semantically equivalent rewrites that attempt to mislead detectors, while the
Prompt Optimization Agent analyzes failed attack attempts and refines the
prompt of the Attacker to guide subsequent rewrites. This enables larger
structural and stylistic transformations of the text rather than token-level
substitutions, adapting the magnitude of changes based on previous outcomes.
Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on
binary model decisions to guide its rewriting process, eliminating the need for
classifier logits or extensive querying. We evaluate CAMOUFLAGE on four
systems, including two recent academic systems and two real-world APIs, with an
average attack success rate of 46.92\% while preserving textual coherence and
semantic equivalence to the original claims.
</summary>
    <author>
      <name>Mazal Bethany</name>
    </author>
    <author>
      <name>Nishant Vishwamitra</name>
    </author>
    <author>
      <name>Cho-Yu Jason Chiang</name>
    </author>
    <author>
      <name>Peyman Najafirad</name>
    </author>
    <link href="http://arxiv.org/abs/2505.01900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.13302v1</id>
    <updated>2025-05-19T16:20:54Z</updated>
    <published>2025-05-19T16:20:54Z</published>
    <title>I'll believe it when I see it: Images increase misinformation sharing in
  Vision-Language Models</title>
    <summary>  Large language models are increasingly integrated into news recommendation
systems, raising concerns about their role in spreading misinformation. In
humans, visual content is known to boost credibility and shareability of
information, yet its effect on vision-language models (VLMs) remains unclear.
We present the first study examining how images influence VLMs' propensity to
reshare news content, whether this effect varies across model families, and how
persona conditioning and content attributes modulate this behavior. To support
this analysis, we introduce two methodological contributions: a
jailbreaking-inspired prompting strategy that elicits resharing decisions from
VLMs while simulating users with antisocial traits and political alignments;
and a multimodal dataset of fact-checked political news from PolitiFact, paired
with corresponding images and ground-truth veracity labels. Experiments across
model families reveal that image presence increases resharing rates by 4.8% for
true news and 15.0% for false news. Persona conditioning further modulates this
effect: Dark Triad traits amplify resharing of false news, whereas
Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the
tested models, only Claude-3-Haiku demonstrates robustness to visual
misinformation. These findings highlight emerging risks in multimodal model
behavior and motivate the development of tailored evaluation frameworks and
mitigation strategies for personalized AI systems. Code and dataset are
available at: https://github.com/3lis/misinfo_vlm
</summary>
    <author>
      <name>Alice Plebe</name>
    </author>
    <author>
      <name>Timothy Douglas</name>
    </author>
    <author>
      <name>Diana Riazi</name>
    </author>
    <author>
      <name>R. Maria del Rio-Chanona</name>
    </author>
    <link href="http://arxiv.org/abs/2505.13302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.13302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15489v2</id>
    <updated>2025-05-26T17:51:52Z</updated>
    <published>2025-05-21T13:14:32Z</published>
    <title>Seeing Through Deception: Uncovering Misleading Creator Intent in
  Multimodal News with Vision-Language Models</title>
    <summary>  The real-world impact of misinformation stems from the underlying misleading
narratives that creators seek to convey. As such, interpreting misleading
creator intent is essential for multimodal misinformation detection (MMD)
systems aimed at effective information governance. In this paper, we introduce
an automated framework that simulates real-world multimodal news creation by
explicitly modeling creator intent through two components: the desired
influence and the execution plan. Using this framework, we construct
DeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs
aligned with trustworthy reference articles. The dataset captures both
misleading and non-misleading intents and spans manipulations across visual and
textual modalities. We conduct a comprehensive evaluation of 14
state-of-the-art vision-language models (VLMs) on three intent-centric tasks:
(1) misleading intent detection, (2) misleading source attribution, and (3)
creator desire inference. Despite recent advances, we observe that current VLMs
fall short in recognizing misleading intent, often relying on spurious cues
such as superficial cross-modal consistency, stylistic signals, and heuristic
authenticity hints. Our findings highlight the pressing need for intent-aware
modeling in MMD and open new directions for developing systems capable of
deeper reasoning about multimodal misinformation.
</summary>
    <author>
      <name>Jiaying Wu</name>
    </author>
    <author>
      <name>Fanxiao Li</name>
    </author>
    <author>
      <name>Min-Yen Kan</name>
    </author>
    <author>
      <name>Bryan Hooi</name>
    </author>
    <link href="http://arxiv.org/abs/2505.15489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20067v1</id>
    <updated>2025-05-26T14:50:18Z</updated>
    <published>2025-05-26T14:50:18Z</published>
    <title>Community Moderation and the New Epistemology of Fact Checking on Social
  Media</title>
    <summary>  Social media platforms have traditionally relied on internal moderation teams
and partnerships with independent fact-checking organizations to identify and
flag misleading content. Recently, however, platforms including X (formerly
Twitter) and Meta have shifted towards community-driven content moderation by
launching their own versions of crowd-sourced fact-checking -- Community Notes.
If effectively scaled and governed, such crowd-checking initiatives have the
potential to combat misinformation with increased scale and speed as
successfully as community-driven efforts once did with spam. Nevertheless,
general content moderation, especially for misinformation, is inherently more
complex. Public perceptions of truth are often shaped by personal biases,
political leanings, and cultural contexts, complicating consensus on what
constitutes misleading content. This suggests that community efforts, while
valuable, cannot replace the indispensable role of professional fact-checkers.
Here we systemically examine the current approaches to misinformation detection
across major platforms, explore the emerging role of community-driven
moderation, and critically evaluate both the promises and challenges of
crowd-checking at scale.
</summary>
    <author>
      <name>Isabelle Augenstein</name>
    </author>
    <author>
      <name>Michiel Bakker</name>
    </author>
    <author>
      <name>Tanmoy Chakraborty</name>
    </author>
    <author>
      <name>David Corney</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <author>
      <name>Scott Hale</name>
    </author>
    <author>
      <name>Eduard Hovy</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <author>
      <name>Irene Larraz</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Paolo Papotti</name>
    </author>
    <author>
      <name>Dhruv Sahnan</name>
    </author>
    <author>
      <name>Greta Warren</name>
    </author>
    <author>
      <name>Giovanni Zagni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 Figure, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.22845v1</id>
    <updated>2025-05-28T20:24:45Z</updated>
    <published>2025-05-28T20:24:45Z</published>
    <title>Security Benefits and Side Effects of Labeling AI-Generated Images</title>
    <summary>  Generative artificial intelligence is developing rapidly, impacting humans'
interaction with information and digital media. It is increasingly used to
create deceptively realistic misinformation, so lawmakers have imposed
regulations requiring the disclosure of AI-generated content. However, only
little is known about whether these labels reduce the risks of AI-generated
misinformation.
  Our work addresses this research gap. Focusing on AI-generated images, we
study the implications of labels, including the possibility of mislabeling.
Assuming that simplicity, transparency, and trust are likely to impact the
successful adoption of such labels, we first qualitatively explore users'
opinions and expectations of AI labeling using five focus groups. Second, we
conduct a pre-registered online survey with over 1300 U.S. and EU participants
to quantitatively assess the effect of AI labels on users' ability to recognize
misinformation containing either human-made or AI-generated images. Our focus
groups illustrate that, while participants have concerns about the practical
implementation of labeling, they consider it helpful in identifying
AI-generated images and avoiding deception. However, considering security
benefits, our survey revealed an ambiguous picture, suggesting that users might
over-rely on labels. While inaccurate claims supported by labeled AI-generated
images were rated less credible than those with unlabeled AI-images, the belief
in accurate claims also decreased when accompanied by a labeled AI-generated
image. Moreover, we find the undesired side effect that human-made images
conveying inaccurate claims were perceived as more credible in the presence of
labels.
</summary>
    <author>
      <name>Sandra H√∂ltervennhoff</name>
    </author>
    <author>
      <name>Jonas Ricker</name>
    </author>
    <author>
      <name>Maike M. Raphael</name>
    </author>
    <author>
      <name>Charlotte Schwedes</name>
    </author>
    <author>
      <name>Rebecca Weil</name>
    </author>
    <author>
      <name>Asja Fischer</name>
    </author>
    <author>
      <name>Thorsten Holz</name>
    </author>
    <author>
      <name>Lea Sch√∂nherr</name>
    </author>
    <author>
      <name>Sascha Fahl</name>
    </author>
    <link href="http://arxiv.org/abs/2505.22845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.22845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10577v2</id>
    <updated>2025-07-16T13:25:34Z</updated>
    <published>2025-07-11T10:08:05Z</published>
    <title>Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos
  and influence opinions</title>
    <summary>  Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.
</summary>
    <author>
      <name>C√©cile Log√©</name>
    </author>
    <author>
      <name>Rehan Ghori</name>
    </author>
    <link href="http://arxiv.org/abs/2507.10577v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.10577v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.11408v1</id>
    <updated>2019-01-29T22:02:24Z</updated>
    <published>2019-01-29T22:02:24Z</published>
    <title>Building Knowledge Graphs About Political Agents in the Age of
  Misinformation</title>
    <summary>  This paper presents the construction of a Knowledge Graph about relations
between agents in a political system. It discusses the main modeling
challenges, with emphasis on the issue of trust and provenance. Implementation
decisions are also presented
</summary>
    <author>
      <name>Daniel Schwabe</name>
    </author>
    <author>
      <name>Carlos Laufer</name>
    </author>
    <author>
      <name>Antonio Busson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1804.06015</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.11408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.11408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11; H.2.1; I.2.4; K.4.1; K.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01523v3</id>
    <updated>2022-03-10T08:10:38Z</updated>
    <published>2019-03-04T20:18:32Z</published>
    <title>Well-posedness of the viscous equations with strong stratification</title>
    <summary>  This is the old version of this project. Please find the new version at
1906.12233.
</summary>
    <author>
      <name>Xin Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been updated and resubmitted as arXiv:1906.12233. This
  original version contains mistake and misinformation</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.01523v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01523v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35Q30, 35Q86, 76D03, 76D05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01913v2</id>
    <updated>2021-02-23T12:00:09Z</updated>
    <published>2020-10-05T10:49:32Z</published>
    <title>Flow of online misinformation during the peak of the COVID-19 pandemic
  in Italy</title>
    <summary>  The COVID-19 pandemic has impacted on every human activity and, because of
the urgency of finding the proper responses to such an unprecedented emergency,
it generated a diffused societal debate. The online version of this discussion
was not exempted by the presence of d/misinformation campaigns, but differently
from what already witnessed in other debates, the COVID-19 -- intentional or
not -- flow of false information put at severe risk the public health, reducing
the effectiveness of governments' countermeasures. In the present manuscript,
we study the effective impact of misinformation in the Italian societal debate
on Twitter during the pandemic, focusing on the various discursive communities.
In order to extract the discursive communities, we focus on verified users,
i.e. accounts whose identity is officially certified by Twitter. We thus infer
the various discursive communities based on how verified users are perceived by
standard ones: if two verified accounts are considered as similar by non
unverified ones, we link them in the network of certified accounts. We first
observe that, beside being a mostly scientific subject, the COVID-19 discussion
show a clear division in what results to be different political groups. At this
point, by using a commonly available fact-checking software (NewsGuard), we
assess the reputation of the pieces of news exchanged. We filter the network of
retweets (i.e. users re-broadcasting the same elementary piece of information,
or tweet) from random noise and check the presence of messages displaying an
url. The impact of misinformation posts reaches the 22.1% in the right and
center-right wing community and its contribution is even stronger in absolute
numbers, due to the activity of this group: 96% of all non reputable urls
shared by political groups come from this community.
</summary>
    <author>
      <name>Guido Caldarelli</name>
    </author>
    <author>
      <name>Rocco de Nicola</name>
    </author>
    <author>
      <name>Marinella Petrocchi</name>
    </author>
    <author>
      <name>Manuel Pratelli</name>
    </author>
    <author>
      <name>Fabio Saracco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjds/s13688-021-00289-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjds/s13688-021-00289-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 4 figures. The Abstract, the Introduction, the Results, the
  Conclusions and the Methods were substantially rewritten. The plot of the
  network have been changed, as well as tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPJ Data Sci. 10, 34 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.01913v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01913v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05745v1</id>
    <updated>2021-04-12T18:13:40Z</updated>
    <published>2021-04-12T18:13:40Z</published>
    <title>Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble</title>
    <summary>  This paper describes the TOKOFOU system, an ensemble model for misinformation
detection tasks based on six different transformer-based pre-trained encoders,
implemented in the context of the COVID-19 Infodemic Shared Task for English.
We fine tune each model on each of the task's questions and aggregate their
prediction scores using a majority voting approach. TOKOFOU obtains an overall
F1 score of 89.7%, ranking first.
</summary>
    <author>
      <name>Giorgos Tziafas</name>
    </author>
    <author>
      <name>Konstantinos Kogkalidis</name>
    </author>
    <author>
      <name>Tommaso Caselli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, NLP4IF 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.05745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.12841v3</id>
    <updated>2020-08-27T04:01:13Z</updated>
    <published>2020-07-25T03:03:20Z</published>
    <title>Combating Misinformation in Bangladesh: Roles and Responsibilities as
  Perceived by Journalists, Fact-checkers, and Users</title>
    <summary>  There has been a growing interest within CSCW community in understanding the
characteristics of misinformation propagated through computational media, and
the devising techniques to address the associated challenges. However, most
work in this area has been concentrated on the cases in the western world
leaving a major portion of this problem unaddressed that is situated in the
Global South. This paper aims to broaden the scope of this discourse by
focusing on this problem in the context of Bangladesh, a country in the Global
South. The spread of misinformation on Facebook in Bangladesh, a country with a
population over 163 million, has resulted in chaos, hate attacks, and killings.
By interviewing journalists, fact-checkers, in addition to surveying the
general public, we analyzed the current state of verifying misinformation in
Bangladesh. Our findings show that most people in the `news audience' want the
news media to verify the authenticity of online information that they see
online. However, the newspaper journalists say that fact-checking online
information is not a part of their job, and it is also beyond their capacity
given the amount of information being published online everyday. We further
find that the voluntary fact-checkers in Bangladesh are not equipped with
sufficient infrastructural support to fill in this gap. We show how our
findings are connected to some of the core concerns of CSCW community around
social media, collaboration, infrastructural politics, and information
inequality. From our analysis, we also suggest several pathways to increase the
impact of fact-checking efforts through collaboration, technology design, and
infrastructure development.
</summary>
    <author>
      <name>Md Mahfuzul Haque</name>
    </author>
    <author>
      <name>Mohammad Yousuf</name>
    </author>
    <author>
      <name>Ahmed Shatil Alam</name>
    </author>
    <author>
      <name>Pratyasha Saha</name>
    </author>
    <author>
      <name>Syed Ishtiaque Ahmed</name>
    </author>
    <author>
      <name>Naeemul Hassan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3415201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3415201" rel="related"/>
    <link href="http://arxiv.org/abs/2007.12841v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12841v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.13832v1</id>
    <updated>2024-01-24T22:15:38Z</updated>
    <published>2024-01-24T22:15:38Z</published>
    <title>Algorithmically Curated Lies: How Search Engines Handle Misinformation
  about US Biolabs in Ukraine</title>
    <summary>  The growing volume of online content prompts the need for adopting
algorithmic systems of information curation. These systems range from web
search engines to recommender systems and are integral for helping users stay
informed about important societal developments. However, unlike journalistic
editing the algorithmic information curation systems (AICSs) are known to be
subject to different forms of malperformance which make them vulnerable to
possible manipulation. The risk of manipulation is particularly prominent in
the case when AICSs have to deal with information about false claims that
underpin propaganda campaigns of authoritarian regimes. Using as a case study
of the Russian disinformation campaign concerning the US biolabs in Ukraine, we
investigate how one of the most commonly used forms of AICSs - i.e. web search
engines - curate misinformation-related content. For this aim, we conduct
virtual agent-based algorithm audits of Google, Bing, and Yandex search outputs
in June 2022. Our findings highlight the troubling performance of search
engines. Even though some search engines, like Google, were less likely to
return misinformation results, across all languages and locations, the three
search engines still mentioned or promoted a considerable share of false
content (33% on Google; 44% on Bing, and 70% on Yandex). We also find
significant disparities in misinformation exposure based on the language of
search, with all search engines presenting a higher number of false stories in
Russian. Location matters as well with users from Germany being more likely to
be exposed to search results promoting false information. These observations
stress the possibility of AICSs being vulnerable to manipulation, in particular
in the case of the unfolding propaganda campaigns, and underline the importance
of monitoring performance of these systems to prevent it.
</summary>
    <author>
      <name>Elizaveta Kuznetsova</name>
    </author>
    <author>
      <name>Mykola Makhortykh</name>
    </author>
    <author>
      <name>Maryna Sydorova</name>
    </author>
    <author>
      <name>Aleksandra Urman</name>
    </author>
    <author>
      <name>Ilaria Vitulano</name>
    </author>
    <author>
      <name>Martha Stolze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.13832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.13832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.14268v1</id>
    <updated>2024-02-22T04:07:00Z</updated>
    <published>2024-02-22T04:07:00Z</published>
    <title>Can Large Language Models Detect Misinformation in Scientific News
  Reporting?</title>
    <summary>  Scientific facts are often spun in the popular press with the intent to
influence public opinion and action, as was evidenced during the COVID-19
pandemic. Automatic detection of misinformation in the scientific domain is
challenging because of the distinct styles of writing in these two media types
and is still in its nascence. Most research on the validity of scientific
reporting treats this problem as a claim verification challenge. In doing so,
significant expert human effort is required to generate appropriate claims. Our
solution bypasses this step and addresses a more real-world scenario where such
explicit, labeled claims may not be available. The central research question of
this paper is whether it is possible to use large language models (LLMs) to
detect misinformation in scientific reporting. To this end, we first present a
new labeled dataset SciNews, containing 2.4k scientific news stories drawn from
trusted and untrustworthy sources, paired with related abstracts from the
CORD-19 database. Our dataset includes both human-written and LLM-generated
news articles, making it more comprehensive in terms of capturing the growing
trend of using LLMs to generate popular press articles. Then, we identify
dimensions of scientific validity in science news articles and explore how this
can be integrated into the automated detection of scientific misinformation. We
propose several baseline architectures using LLMs to automatically detect false
representations of scientific findings in the popular press. For each of these
architectures, we use several prompt engineering strategies including
zero-shot, few-shot, and chain-of-thought prompting. We also test these
architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B,
Llama2-13B.
</summary>
    <author>
      <name>Yupeng Cao</name>
    </author>
    <author>
      <name>Aishwarya Muralidharan Nair</name>
    </author>
    <author>
      <name>Elyon Eyimife</name>
    </author>
    <author>
      <name>Nastaran Jamalipour Soofi</name>
    </author>
    <author>
      <name>K. P. Subbalakshmi</name>
    </author>
    <author>
      <name>John R. Wullert II</name>
    </author>
    <author>
      <name>Chumki Basu</name>
    </author>
    <author>
      <name>David Shallcross</name>
    </author>
    <link href="http://arxiv.org/abs/2402.14268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.14268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.07490v1</id>
    <updated>2017-01-17T18:52:22Z</updated>
    <published>2017-01-17T18:52:22Z</published>
    <title>What Are People Tweeting about Zika? An Exploratory Study Concerning
  Symptoms, Treatment, Transmission, and Prevention</title>
    <summary>  The purpose of this study was to do a dataset distribution analysis, a
classification performance analysis, and a topical analysis concerning what
people are tweeting about four disease characteristics: symptoms, transmission,
prevention, and treatment. A combination of natural language processing and
machine learning techniques were used to determine what people are tweeting
about Zika. Specifically, a two-stage classifier system was built to find
relevant tweets on Zika, and then categorize these into the four disease
categories. Tweets in each disease category were then examined using latent
dirichlet allocation (LDA) to determine the five main tweet topics for each
disease characteristic. Results 1,234,605 tweets were collected. Tweets by
males and females were similar (28% and 23% respectively). The classifier
performed well on the training and test data for relevancy (F=0.87 and 0.99
respectively) and disease characteristics (F=0.79 and 0.90 respectively). Five
topics for each category were found and discussed with a focus on the symptoms
category. Through this process, we demonstrate how misinformation can be
discovered so that public health officials can respond to the tweets with
misinformation.
</summary>
    <author>
      <name>Michele Miller</name>
    </author>
    <author>
      <name>Dr. Tanvi Banerjee</name>
    </author>
    <author>
      <name>RoopTeja Muppalla</name>
    </author>
    <author>
      <name>Dr. William Romine</name>
    </author>
    <author>
      <name>Dr. Amit Sheth</name>
    </author>
    <link href="http://arxiv.org/abs/1701.07490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.07490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03573v1</id>
    <updated>2018-02-10T12:22:59Z</updated>
    <published>2018-02-10T12:22:59Z</published>
    <title>Social Media, News and Political Information during the US Election: Was
  Polarizing Content Concentrated in Swing States?</title>
    <summary>  US voters shared large volumes of polarizing political news and information
in the form of links to content from Russian, WikiLeaks and junk news sources.
Was this low quality political information distributed evenly around the
country, or concentrated in swing states and particular parts of the country?
In this data memo we apply a tested dictionary of sources about political news
and information being shared over Twitter over a ten day period around the 2016
Presidential Election. Using self-reported location information, we place a
third of users by state and create a simple index for the distribution of
polarizing content around the country. We find that (1) nationally, Twitter
users got more misinformation, polarizing and conspiratorial content than
professionally produced news. (2) Users in some states, however, shared more
polarizing political news and information than users in other states. (3)
Average levels of misinformation were higher in swing states than in
uncontested states, even when weighted for the relative size of the user
population in each state. We conclude with some observations about the impact
of strategically disseminated polarizing information on public life.
</summary>
    <author>
      <name>Philip N. Howard</name>
    </author>
    <author>
      <name>Bence Kollanyi</name>
    </author>
    <author>
      <name>Samantha Bradshaw</name>
    </author>
    <author>
      <name>Lisa-Maria Neudert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Data Memo</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05999v2</id>
    <updated>2018-05-17T09:33:35Z</updated>
    <published>2018-05-15T19:09:41Z</published>
    <title>Agent Based Rumor Spreading in a scale-free network</title>
    <summary>  In the last years, the study of rumor spreading on social networks produced a
lot of interest among the scientific community, expecially due to the role of
social networks in the last political events. The goal of this work is to
reproduce real-like diffusions of information and misinformation in a
scale-free network using a multi-agent-based model. The data concerning the
virtual spreading are easily obtainable, in particular the diffusion of
information during the announcement for the discovery of the Higgs Boson on
Twitter was recorded and investigated in detail. We made some assumptions on
the micro behavior of our agents and registered the effects in a statistical
analysis replying the real data diffusion. Then, we studied an hypotetical
response to a misinformation diffusion adding debunking agents and trying to
model a critic response from the agents using real data from a hoax regarding
the Occupy Wall Street movement. After tuning our model to reproduce these
results, we measured some network properties and proved the emergence of
substantially separated structures like echochambers, independently from the
network size scale, i.e. with one hundred, one thousand and ten thousand
agents.
</summary>
    <author>
      <name>Mattia Mazzoli</name>
    </author>
    <author>
      <name>Tullio Re</name>
    </author>
    <author>
      <name>Roberto Bertilone</name>
    </author>
    <author>
      <name>Marco Maggiora</name>
    </author>
    <author>
      <name>Jacopo Pellegrino</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05999v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05999v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T42" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.01806v1</id>
    <updated>2018-11-05T15:43:45Z</updated>
    <published>2018-11-05T15:43:45Z</published>
    <title>Fact-checking Initiatives in Bangladesh, India, and Nepal: A Study of
  User Engagement and Challenges</title>
    <summary>  Fake news and misinformation spread in developing countries as fast as they
do in developed countries with increasing penetration of the internet and
social media. However, fighting misinformation is more difficult in developing
countries where resources and necessary technologies are scarce. This study
provides an understanding of the challenges various fact-checking initiatives
face in three South Asian countries--Bangladesh, India, and Nepal. In-depth
interviews were conducted with senior editors of six fact-checking initiatives.
Challenges identified include lack of resources, technologies, and political
pressure. An analysis of Facebook pages of these initiatives shows increasing
user engagement with their posts.
</summary>
    <author>
      <name>Md Mahfuzul Haque</name>
    </author>
    <author>
      <name>Mohammad Yousuf</name>
    </author>
    <author>
      <name>Zahedur Arman</name>
    </author>
    <author>
      <name>Md Main Uddin Rony</name>
    </author>
    <author>
      <name>Ahmed Shatil Alam</name>
    </author>
    <author>
      <name>Kazi Mehedi Hasan</name>
    </author>
    <author>
      <name>Md Khadimul Islam</name>
    </author>
    <author>
      <name>Naeemul Hassan</name>
    </author>
    <link href="http://arxiv.org/abs/1811.01806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.03957v1</id>
    <updated>2019-08-11T19:51:48Z</updated>
    <published>2019-08-11T19:51:48Z</published>
    <title>Tensor Factorization with Label Information for Fake News Detection</title>
    <summary>  The buzz over the so-called "fake news" has created concerns about a
degenerated media environment and led to the need for technological solutions.
As the detection of fake news is increasingly considered a technological
problem, it has attracted considerable research. Most of these studies
primarily focus on utilizing information extracted from textual news content.
In contrast, we focus on detecting fake news solely based on structural
information of social networks. We suggest that the underlying network
connections of users that share fake news are discriminative enough to support
the detection of fake news. Thereupon, we model each post as a network of
friendship interactions and represent a collection of posts as a
multidimensional tensor. Taking into account the available labeled data, we
propose a tensor factorization method which associates the class labels of data
samples with their latent representations. Specifically, we combine a
classification error term with the standard factorization in a unified
optimization process. Results on real-world datasets demonstrate that our
proposed method is competitive against state-of-the-art methods by implementing
an arguably simpler approach.
</summary>
    <author>
      <name>Frosso Papanastasiou</name>
    </author>
    <author>
      <name>Georgios Katsimpras</name>
    </author>
    <author>
      <name>Georgios Paliouras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Workshop on Reducing Online Misinformation Exposure
  ROME 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(SIGIR 2019) Proceedings of Workshop on Reducing Online
  Misinformation Exposure (ROME)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.03957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08926v1</id>
    <updated>2019-11-25T03:55:55Z</updated>
    <published>2019-11-25T03:55:55Z</published>
    <title>Rumor Detection and Classification for Twitter Data</title>
    <summary>  With the pervasiveness of online media data as a source of information
verifying the validity of this information is becoming even more important yet
quite challenging. Rumors spread a large quantity of misinformation on
microblogs. In this study we address two common issues within the context of
microblog social media. First we detect rumors as a type of misinformation
propagation and next we go beyond detection to perform the task of rumor
classification. WE explore the problem using a standard data set. We devise
novel features and study their impact on the task. We experiment with various
levels of preprocessing as a precursor of the classification as well as
grouping of features. We achieve and f-measure of over 0.82 in RDC task in
mixed rumors data set and 84 percent in a single rumor data set using a
two-step classification approach.
</summary>
    <author>
      <name>Sardar Hamidian</name>
    </author>
    <author>
      <name>Mona T Diab</name>
    </author>
    <link href="http://arxiv.org/abs/1912.08926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.16357v1</id>
    <updated>2020-10-30T16:26:35Z</updated>
    <published>2020-10-30T16:26:35Z</published>
    <title>A Cross-lingual Natural Language Processing Framework for Infodemic
  Management</title>
    <summary>  The COVID-19 pandemic has put immense pressure on health systems which are
further strained due to the misinformation surrounding it. Under such a
situation, providing the right information at the right time is crucial. There
is a growing demand for the management of information spread using Artificial
Intelligence. Hence, we have exploited the potential of Natural Language
Processing for identifying relevant information that needs to be disseminated
amongst the masses. In this work, we present a novel Cross-lingual Natural
Language Processing framework to provide relevant information by matching daily
news with trusted guidelines from the World Health Organization. The proposed
pipeline deploys various techniques of NLP such as summarizers, word
embeddings, and similarity metrics to provide users with news articles along
with a corresponding healthcare guideline. A total of 36 models were evaluated
and a combination of LexRank based summarizer on Word2Vec embedding with Word
Mover distance metric outperformed all other models. This novel open-source
approach can be used as a template for proactive dissemination of relevant
healthcare information in the midst of misinformation spread associated with
epidemics.
</summary>
    <author>
      <name>Ridam Pal</name>
    </author>
    <author>
      <name>Rohan Pandey</name>
    </author>
    <author>
      <name>Vaibhav Gautam</name>
    </author>
    <author>
      <name>Kanav Bhagat</name>
    </author>
    <author>
      <name>Tavpritesh Sethi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 2 Figures, 3 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.16357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.16357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.14500v2</id>
    <updated>2021-01-25T02:29:18Z</updated>
    <published>2020-12-28T21:51:31Z</published>
    <title>A Paragraph-level Multi-task Learning Model for Scientific
  Fact-Verification</title>
    <summary>  Even for domain experts, it is a non-trivial task to verify a scientific
claim by providing supporting or refuting evidence rationales. The situation
worsens as misinformation is proliferated on social media or news websites,
manually or programmatically, at every moment. As a result, an automatic
fact-verification tool becomes crucial for combating the spread of
misinformation. In this work, we propose a novel, paragraph-level, multi-task
learning model for the SciFact task by directly computing a sequence of
contextualized sentence embeddings from a BERT model and jointly training the
model on rationale selection and stance prediction.
</summary>
    <author>
      <name>Xiangci Li</name>
    </author>
    <author>
      <name>Gully Burns</name>
    </author>
    <author>
      <name>Nanyun Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages; The AAAI-21 Workshop on Scientific Document Understanding</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.14500v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.14500v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.00957v1</id>
    <updated>2019-05-02T20:50:22Z</updated>
    <published>2019-05-02T20:50:22Z</published>
    <title>A Topic-Agnostic Approach for Identifying Fake News Pages</title>
    <summary>  Fake news and misinformation have been increasingly used to manipulate
popular opinion and influence political processes. To better understand fake
news, how they are propagated, and how to counter their effect, it is necessary
to first identify them. Recently, approaches have been proposed to
automatically classify articles as fake based on their content. An important
challenge for these approaches comes from the dynamic nature of news: as new
political events are covered, topics and discourse constantly change and thus,
a classifier trained using content from articles published at a given time is
likely to become ineffective in the future. To address this challenge, we
propose a topic-agnostic (TAG) classification strategy that uses linguistic and
web-markup features to identify fake news pages. We report experimental results
using multiple data sets which show that our approach attains high accuracy in
the identification of fake news, even as topics evolve over time.
</summary>
    <author>
      <name>Sonia Castelo</name>
    </author>
    <author>
      <name>Thais Almeida</name>
    </author>
    <author>
      <name>Anas Elghafari</name>
    </author>
    <author>
      <name>A√©cio Santos</name>
    </author>
    <author>
      <name>Kien Pham</name>
    </author>
    <author>
      <name>Eduardo Nakamura</name>
    </author>
    <author>
      <name>Juliana Freire</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3308560.3316739</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3308560.3316739" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the Companion Proceedings of the 2019
  World Wide Web Conference (WWW'19 Companion). Presented in the 2019
  International Workshop on Misinformation, Computational Fact-Checking and
  Credible Web (MisinfoWorkshop2019). 6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.00957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.04260v1</id>
    <updated>2019-05-10T17:00:40Z</updated>
    <published>2019-05-10T17:00:40Z</published>
    <title>Check-It: A Plugin for Detecting and Reducing the Spread of Fake News
  and Misinformation on the Web</title>
    <summary>  Over the past few years, we have been witnessing the rise of misinformation
on the Web. People fall victims of fake news during their daily lives and
assist their further propagation knowingly and inadvertently. There have been
many initiatives that are trying to mitigate the damage caused by fake news,
focusing on signals from either domain flag-lists, online social networks or
artificial intelligence. In this work, we present Check-It, a system that
combines, in an intelligent way, a variety of signals into a pipeline for fake
news identification. Check-It is developed as a web browser plugin with the
objective of efficient and timely fake news detection, respecting the user's
privacy. Experimental results show that Check-It is able to outperform the
state-of-the-art methods. On a dataset, consisting of 9 millions of articles
labeled as fake and real, Check-It obtains classification accuracies that
exceed 99%.
</summary>
    <author>
      <name>Demetris Paschalides</name>
    </author>
    <author>
      <name>Alexandros Kornilakis</name>
    </author>
    <author>
      <name>Chrysovalantis Christodoulou</name>
    </author>
    <author>
      <name>Rafael Andreou</name>
    </author>
    <author>
      <name>George Pallis</name>
    </author>
    <author>
      <name>Marios D. Dikaiakos</name>
    </author>
    <author>
      <name>Evangelos Markatos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures,</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.04260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02202v1</id>
    <updated>2019-10-05T03:23:45Z</updated>
    <published>2019-10-05T03:23:45Z</published>
    <title>Learning from Fact-checkers: Analysis and Generation of Fact-checking
  Language</title>
    <summary>  In fighting against fake news, many fact-checking systems comprised of
human-based fact-checking sites (e.g., snopes.com and politifact.com) and
automatic detection systems have been developed in recent years. However,
online users still keep sharing fake news even when it has been debunked. It
means that early fake news detection may be insufficient and we need another
complementary approach to mitigate the spread of misinformation. In this paper,
we introduce a novel application of text generation for combating fake news. In
particular, we (1) leverage online users named \emph{fact-checkers}, who cite
fact-checking sites as credible evidences to fact-check information in public
discourse; (2) analyze linguistic characteristics of fact-checking tweets; and
(3) propose and build a deep learning framework to generate responses with
fact-checking intention to increase the fact-checkers' engagement in
fact-checking activities. Our analysis reveals that the fact-checkers tend to
refute misinformation and use formal language (e.g. few swear words and
Internet slangs). Our framework successfully generates relevant responses, and
outperforms competing models by achieving up to 30\% improvements. Our
qualitative study also confirms that the superiority of our generated responses
compared with responses generated from the existing models.
</summary>
    <author>
      <name>Nguyen Vo</name>
    </author>
    <author>
      <name>Kyumin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07100v1</id>
    <updated>2019-11-16T21:13:33Z</updated>
    <published>2019-11-16T21:13:33Z</published>
    <title>Defending Against Model Stealing Attacks with Adaptive Misinformation</title>
    <summary>  Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which
allows a data-limited adversary with no knowledge of the training dataset to
clone the functionality of a target model, just by using black-box query
access. Such attacks are typically carried out by querying the target model
using inputs that are synthetically generated or sampled from a surrogate
dataset to construct a labeled dataset. The adversary can use this labeled
dataset to train a clone model, which achieves a classification accuracy
comparable to that of the target model. We propose "Adaptive Misinformation" to
defend against such model stealing attacks. We identify that all existing model
stealing attacks invariably query the target model with Out-Of-Distribution
(OOD) inputs. By selectively sending incorrect predictions for OOD queries, our
defense substantially degrades the accuracy of the attacker's clone model (by
up to 40%), while minimally impacting the accuracy (&lt;0.5%) for benign users.
Compared to existing defenses, our defense has a significantly better security
vs accuracy trade-off and incurs minimal computational overhead.
</summary>
    <author>
      <name>Sanjay Kariyappa</name>
    </author>
    <author>
      <name>Moinuddin K Qureshi</name>
    </author>
    <link href="http://arxiv.org/abs/1911.07100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.08166v2</id>
    <updated>2021-02-14T20:33:58Z</updated>
    <published>2020-04-17T10:55:07Z</published>
    <title>Too Many Claims to Fact-Check: Prioritizing Political Claims Based on
  Check-Worthiness</title>
    <summary>  The massive amount of misinformation spreading on the Internet on a daily
basis has enormous negative impacts on societies. Therefore, we need automated
systems helping fact-checkers in the combat against misinformation. In this
paper, we propose a model prioritizing the claims based on their
check-worthiness. We use BERT model with additional features including
domain-specific controversial topics, word embeddings, and others. In our
experiments, we show that our proposed model outperforms all state-of-the-art
models in both test collections of CLEF Check That! Lab in 2018 and 2019. We
also conduct a qualitative analysis to shed light-detecting check-worthy
claims. We suggest requesting rationales behind judgments are needed to
understand subjective nature of the task and problematic labels.
</summary>
    <author>
      <name>Yavuz Selim Kartal</name>
    </author>
    <author>
      <name>Busra Guvenen</name>
    </author>
    <author>
      <name>Mucahid Kutlu</name>
    </author>
    <link href="http://arxiv.org/abs/2004.08166v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08166v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01215v1</id>
    <updated>2021-04-02T19:27:53Z</updated>
    <published>2021-04-02T19:27:53Z</published>
    <title>The Coronavirus is a Bioweapon: Analysing Coronavirus Fact-Checked
  Stories</title>
    <summary>  The 2020 coronavirus pandemic has heightened the need to flag
coronavirus-related misinformation, and fact-checking groups have taken to
verifying misinformation on the Internet. We explore stories reported by
fact-checking groups PolitiFact, Poynter and Snopes from January to June 2020,
characterising them into six story clusters before then analyse time-series and
story validity trends and the level of agreement across sites. We further break
down the story clusters into more granular story types by proposing a unique
automated method with a BERT classifier, which can be used to classify diverse
story sources, in both fact-checked stories and tweets.
</summary>
    <author>
      <name>Lynnette Hui Xian Ng</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SBP-Brims 2020 COVID Special Track</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.01215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.06952v1</id>
    <updated>2021-04-14T16:25:22Z</updated>
    <published>2021-04-14T16:25:22Z</published>
    <title>The Surprising Performance of Simple Baselines for Misinformation
  Detection</title>
    <summary>  As social media becomes increasingly prominent in our day to day lives, it is
increasingly important to detect informative content and prevent the spread of
disinformation and unverified rumours. While many sophisticated and successful
models have been proposed in the literature, they are often compared with older
NLP baselines such as SVMs, CNNs, and LSTMs. In this paper, we examine the
performance of a broad set of modern transformer-based language models and show
that with basic fine-tuning, these models are competitive with and can even
significantly outperform recently proposed state-of-the-art methods. We present
our framework as a baseline for creating and evaluating new methods for
misinformation detection. We further study a comprehensive set of benchmark
datasets, and discuss potential data leakage and the need for careful design of
the experiments and understanding of datasets to account for confounding
variables. As an extreme case example, we show that classifying only based on
the first three digits of tweet ids, which contain information on the date,
gives state-of-the-art performance on a commonly used benchmark dataset for
fake news detection --Twitter16. We provide a simple tool to detect this
problem and suggest steps to mitigate it in future datasets.
</summary>
    <author>
      <name>Kellin Pelrine</name>
    </author>
    <author>
      <name>Jacob Danovitch</name>
    </author>
    <author>
      <name>Reihaneh Rabbany</name>
    </author>
    <link href="http://arxiv.org/abs/2104.06952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13352v2</id>
    <updated>2021-06-13T12:27:10Z</updated>
    <published>2021-04-24T08:54:02Z</published>
    <title>Tracking Peaceful Tractors on Social Media -- XAI-enabled analysis of
  Red Fort Riots 2021</title>
    <summary>  On 26 January 2021, India witnessed a national embarrassment from the
demographic least expected from - farmers. People across the nation watched in
horror as a pseudo-patriotic mob of farmers stormed capital Delhi and
vandalized the national pride- Red Fort. Investigations that followed the event
revealed the existence of a social media trail that led to the likes of such an
event. Consequently, it became essential and necessary to archive this trail
for social media analysis - not only to understand the bread-crumbs that are
dispersed across the trail but also to visualize the role played by
misinformation and fake news in this event. In this paper, we propose the
tractor2twitter dataset which contains around 0.05 million tweets that were
posted before, during, and after this event. Also, we benchmark our dataset
with an Explainable AI ML model for classification of each tweet into either of
the three categories - disinformation, misinformation, and opinion.
</summary>
    <author>
      <name>Ajay Agarwal</name>
    </author>
    <author>
      <name>Basant Agarwal</name>
    </author>
    <link href="http://arxiv.org/abs/2104.13352v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13352v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06440v1</id>
    <updated>2021-11-11T19:40:51Z</updated>
    <published>2021-11-11T19:40:51Z</published>
    <title>Personalized multi-faceted trust modeling to determine trust links in
  social media and its potential for misinformation management</title>
    <summary>  In this paper, we present an approach for predicting trust links between
peers in social media, one that is grounded in the artificial intelligence area
of multiagent trust modeling. In particular, we propose a data-driven
multi-faceted trust modeling which incorporates many distinct features for a
comprehensive analysis. We focus on demonstrating how clustering of similar
users enables a critical new functionality: supporting more personalized, and
thus more accurate predictions for users. Illustrated in a trust-aware item
recommendation task, we evaluate the proposed framework in the context of a
large Yelp dataset. We then discuss how improving the detection of trusted
relationships in social media can assist in supporting online users in their
battle against the spread of misinformation and rumours, within a social
networking environment which has recently exploded in popularity. We conclude
with a reflection on a particularly vulnerable user base, older adults, in
order to illustrate the value of reasoning about groups of users, looking to
some future directions for integrating known preferences with insights gained
through data analysis.
</summary>
    <author>
      <name>Alexandre Parmentier</name>
    </author>
    <author>
      <name>Robin Cohen</name>
    </author>
    <author>
      <name>Xueguang Ma</name>
    </author>
    <author>
      <name>Gaurav Sahu</name>
    </author>
    <author>
      <name>Queenie Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.06440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.08791v1</id>
    <updated>2021-11-16T21:42:23Z</updated>
    <published>2021-11-16T21:42:23Z</published>
    <title>PROVENANCE: An Intermediary-Free Solution for Digital Content
  Verification</title>
    <summary>  The threat posed by misinformation and disinformation is one of the defining
challenges of the 21st century. Provenance is designed to help combat this
threat by warning users when the content they are looking at may be
misinformation or disinformation. It is also designed to improve media literacy
among its users and ultimately reduce susceptibility to the threat among
vulnerable groups within society. The Provenance browser plugin checks the
content that users see on the Internet and social media and provides warnings
in their browser or social media feed. Unlike similar plugins, which require
human experts to provide evaluations and can only provide simple binary
warnings, Provenance's state of the art technology does not require human input
and it analyses seven aspects of the content users see and provides warnings
where necessary.
</summary>
    <author>
      <name>Bilal Yousuf</name>
    </author>
    <author>
      <name>M. Atif Qureshi</name>
    </author>
    <author>
      <name>Brendan Spillane</name>
    </author>
    <author>
      <name>Gary Munnelly</name>
    </author>
    <author>
      <name>Oisin Carroll</name>
    </author>
    <author>
      <name>Matthew Runswick</name>
    </author>
    <author>
      <name>Kirsty Park</name>
    </author>
    <author>
      <name>Eileen Culloty</name>
    </author>
    <author>
      <name>Owen Conlan</name>
    </author>
    <author>
      <name>Jane Suiter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.08791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.02736v1</id>
    <updated>2022-05-05T16:16:03Z</updated>
    <published>2022-05-05T16:16:03Z</published>
    <title>A Structured Analysis of Journalistic Evaluations for News Source
  Reliability</title>
    <summary>  In today's era of information disorder, many organizations are moving to
verify the veracity of news published on the web and social media. In
particular, some agencies are exploring the world of online media and, through
a largely manual process, ranking the credibility and transparency of news
sources around the world. In this paper, we evaluate two procedures for
assessing the risk of online media exposing their readers to m/disinformation.
The procedures have been dictated by NewsGuard and The Global Disinformation
Index, two well-known organizations combating d/misinformation via practices of
good journalism. Specifically, considering a fixed set of media outlets, we
examine how many of them were rated equally by the two procedures, and which
aspects led to disagreement in the assessment. The result of our analysis shows
a good degree of agreement, which in our opinion has a double value: it
fortifies the correctness of the procedures and lays the groundwork for their
automation.
</summary>
    <author>
      <name>Manuel Pratelli</name>
    </author>
    <author>
      <name>Marinella Petrocchi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.36190/2022.51</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.36190/2022.51" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at MEDIATE 2022. `Misinformation: new directions in
  automation, real-world applications, and interventions', a workshop @ICWSM
  2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.02736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.02736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.03153v1</id>
    <updated>2022-05-06T11:44:35Z</updated>
    <published>2022-05-06T11:44:35Z</published>
    <title>Bridging the Domain Gap for Stance Detection for the Zulu language</title>
    <summary>  Misinformation has become a major concern in recent last years given its
spread across our information sources. In the past years, many NLP tasks have
been introduced in this area, with some systems reaching good results on
English language datasets. Existing AI based approaches for fighting
misinformation in literature suggest automatic stance detection as an integral
first step to success. Our paper aims at utilizing this progress made for
English to transfers that knowledge into other languages, which is a
non-trivial task due to the domain gap between English and the target
languages. We propose a black-box non-intrusive method that utilizes techniques
from Domain Adaptation to reduce the domain gap, without requiring any human
expertise in the target language, by leveraging low-quality data in both a
supervised and unsupervised manner. This allows us to rapidly achieve similar
results for stance detection for the Zulu language, the target language in this
work, as are found for English. We also provide a stance detection dataset in
the Zulu language. Our experimental results show that by leveraging English
datasets and machine translation we can increase performances on both English
data along with other languages.
</summary>
    <author>
      <name>Gcinizwe Dlamini</name>
    </author>
    <author>
      <name>Imad Eddine Ibrahim Bekkouch</name>
    </author>
    <author>
      <name>Adil Khan</name>
    </author>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to Intellisys</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.03153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.03153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04404v1</id>
    <updated>2022-05-09T16:19:28Z</updated>
    <published>2022-05-09T16:19:28Z</published>
    <title>TeamX@DravidianLangTech-ACL2022: A Comparative Analysis for Troll-Based
  Meme Classification</title>
    <summary>  The spread of fake news, propaganda, misinformation, disinformation, and
harmful content online raised concerns among social media platforms, government
agencies, policymakers, and society as a whole. This is because such harmful or
abusive content leads to several consequences to people such as physical,
emotional, relational, and financial. Among different harmful content
\textit{trolling-based} online content is one of them, where the idea is to
post a message that is provocative, offensive, or menacing with an intent to
mislead the audience. The content can be textual, visual, a combination of
both, or a meme. In this study, we provide a comparative analysis of
troll-based memes classification using the textual, visual, and multimodal
content. We report several interesting findings in terms of code-mixed text,
multimodal setting, and combining an additional dataset, which shows
improvements over the majority baseline.
</summary>
    <author>
      <name>Rabindra Nath Nandi</name>
    </author>
    <author>
      <name>Firoj Alam</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at DravidianLangTech-ACL2022 (Colocated with ACL-2022).
  disinformation, misinformation, factuality, harmfulness, fake news,
  propaganda, multimodality, text, images, videos, network structure,
  temporality</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.04404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.07154v2</id>
    <updated>2022-05-23T04:24:01Z</updated>
    <published>2022-05-15T00:00:49Z</published>
    <title>Evaluating Generalizability of Fine-Tuned Models for Fake News Detection</title>
    <summary>  The Covid-19 pandemic has caused a dramatic and parallel rise in dangerous
misinformation, denoted an `infodemic' by the CDC and WHO. Misinformation tied
to the Covid-19 infodemic changes continuously; this can lead to performance
degradation of fine-tuned models due to concept drift. Degredation can be
mitigated if models generalize well-enough to capture some cyclical aspects of
drifted data. In this paper, we explore generalizability of pre-trained and
fine-tuned fake news detectors across 9 fake news datasets. We show that
existing models often overfit on their training dataset and have poor
performance on unseen data. However, on some subsets of unseen data that
overlap with training data, models have higher accuracy. Based on this
observation, we also present KMeans-Proxy, a fast and effective method based on
K-Means clustering for quickly identifying these overlapping subsets of unseen
data. KMeans-Proxy improves generalizability on unseen fake news datasets by
0.1-0.2 f1-points across datasets. We present both our generalizability
experiments as well as KMeans-Proxy to further research in tackling the fake
news problem.
</summary>
    <author>
      <name>Abhijit Suprem</name>
    </author>
    <author>
      <name>Calton Pu</name>
    </author>
    <link href="http://arxiv.org/abs/2205.07154v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07154v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.09817v1</id>
    <updated>2022-05-19T19:36:08Z</updated>
    <published>2022-05-19T19:36:08Z</published>
    <title>MiDAS: Multi-integrated Domain Adaptive Supervision for Fake News
  Detection</title>
    <summary>  COVID-19 related misinformation and fake news, coined an 'infodemic', has
dramatically increased over the past few years. This misinformation exhibits
concept drift, where the distribution of fake news changes over time, reducing
effectiveness of previously trained models for fake news detection. Given a set
of fake news models trained on multiple domains, we propose an adaptive
decision module to select the best-fit model for a new sample. We propose
MiDAS, a multi-domain adaptative approach for fake news detection that ranks
relevancy of existing models to new samples. MiDAS contains 2 components: a
doman-invariant encoder, and an adaptive model selector. MiDAS integrates
multiple pre-trained and fine-tuned models with their training data to create a
domain-invariant representation. Then, MiDAS uses local Lipschitz smoothness of
the invariant embedding space to estimate each model's relevance to a new
sample. Higher ranked models provide predictions, and lower ranked models
abstain. We evaluate MiDAS on generalization to drifted data with 9 fake news
datasets, each obtained from different domains and modalities. MiDAS achieves
new state-of-the-art performance on multi-domain adaptation for
out-of-distribution fake news classification.
</summary>
    <author>
      <name>Abhijit Suprem</name>
    </author>
    <author>
      <name>Calton Pu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We use Lipschitz smoothness and probabilistic Lipschitzness to build
  a theoretical foundation for effective multi-domain adaptation using
  randomized perturbations on unseen data</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.09817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.09817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09223v2</id>
    <updated>2018-04-10T05:22:23Z</updated>
    <published>2018-01-28T13:08:43Z</published>
    <title>Probability Mass Exclusions and the Directed Components of Pointwise
  Mutual Information</title>
    <summary>  This paper examines how an event from one random variable provides pointwise
mutual information about an event from another variable via probability mass
exclusions. We start by introducing probability mass diagrams, which provide a
visual representation of how a prior distribution is transformed to a posterior
distribution through exclusions. With the aid of these diagrams, we identify
two distinct types of probability mass exclusions---namely informative and
misinformative exclusions. Then, motivated by Fano's derivation of the
pointwise mutual information, we propose four postulates which aim to decompose
the pointwise mutual information into two separate informational components: a
non-negative term associated with the informative exclusion and a non-positive
term associated with the misinformative exclusions. This yields a novel
derivation of a familiar decomposition of the pointwise mutual information into
entropic components. We conclude by discussing the relevance of considering
information in terms of probability mass exclusions to the ongoing effort to
decompose multivariate information.
</summary>
    <author>
      <name>Conor Finn</name>
    </author>
    <author>
      <name>Joseph T Lizier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/e20110826</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/e20110826" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09223v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09223v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A15, 94A17" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08444v2</id>
    <updated>2020-03-26T21:29:51Z</updated>
    <published>2020-03-18T19:18:21Z</published>
    <title>NELA-GT-2019: A Large Multi-Labelled News Dataset for The Study of
  Misinformation in News Articles</title>
    <summary>  In this paper, we present an updated version of the NELA-GT-2018 dataset
(N{\o}rregaard, Horne, and Adal{\i} 2019), entitled NELA-GT-2019. NELA-GT-2019
contains 1.12M news articles from 260 sources collected between January 1st
2019 and December 31st 2019. Just as with NELA-GT-2018, these sources come from
a wide range of mainstream news sources and alternative news sources. Included
with the dataset are source-level ground truth labels from 7 different
assessment sites covering multiple dimensions of veracity. The NELA-GT-2019
dataset can be found at: https://doi.org/10.7910/DVN/O7FWPO
</summary>
    <author>
      <name>Maur√≠cio Gruppi</name>
    </author>
    <author>
      <name>Benjamin D. Horne</name>
    </author>
    <author>
      <name>Sibel Adalƒ±</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated dataset for paper NELA-GT-2018: A Large Multi-Labelled News
  Dataset for The Study of Misinformation in News Articles, originally
  published at ICWSM in 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.08444v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08444v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.06915v3</id>
    <updated>2020-06-25T01:54:21Z</updated>
    <published>2020-05-14T12:37:48Z</published>
    <title>Can The Crowd Identify Misinformation Objectively? The Effects of
  Judgment Scale and Assessor's Background</title>
    <summary>  Truthfulness judgments are a fundamental step in the process of fighting
misinformation, as they are crucial to train and evaluate classifiers that
automatically distinguish true and false statements. Usually such judgments are
made by experts, like journalists for political statements or medical doctors
for medical statements. In this paper, we follow a different approach and rely
on (non-expert) crowd workers. This of course leads to the following research
question: Can crowdsourcing be reliably used to assess the truthfulness of
information and to create large-scale labeled collections for information
credibility systems? To address this issue, we present the results of an
extensive study based on crowdsourcing: we collect thousands of truthfulness
assessments over two datasets, and we compare expert judgments with crowd
judgments, expressed on scales with various granularity levels. We also measure
the political bias and the cognitive background of the workers, and quantify
their effect on the reliability of the data provided by the crowd.
</summary>
    <author>
      <name>Kevin Roitero</name>
    </author>
    <author>
      <name>Michael Soprano</name>
    </author>
    <author>
      <name>Shaoyang Fan</name>
    </author>
    <author>
      <name>Damiano Spina</name>
    </author>
    <author>
      <name>Stefano Mizzaro</name>
    </author>
    <author>
      <name>Gianluca Demartini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3397271.3401112</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3397271.3401112" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of the full paper accepted at SIGIR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.06915v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06915v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13270v1</id>
    <updated>2020-05-27T10:29:14Z</updated>
    <published>2020-05-27T10:29:14Z</published>
    <title>BRENDA: Browser Extension for Fake News Detection</title>
    <summary>  Misinformation such as fake news has drawn a lot of attention in recent
years. It has serious consequences on society, politics and economy. This has
lead to a rise of manually fact-checking websites such as Snopes and
Politifact. However, the scale of misinformation limits their ability for
verification. In this demonstration, we propose BRENDA a browser extension
which can be used to automate the entire process of credibility assessments of
false claims. Behind the scenes BRENDA uses a tested deep neural network
architecture to automatically identify fact check worthy claims and classifies
as well as presents the result along with evidence to the user. Since BRENDA is
a browser extension, it facilities fast automated fact checking for the end
user without having to leave the Webpage.
</summary>
    <author>
      <name>Bjarte Botnevik</name>
    </author>
    <author>
      <name>Eirik Sakariassen</name>
    </author>
    <author>
      <name>Vinay Setty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3397271.3401396</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3397271.3401396" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as SIGIR demo</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 43rd International ACM SIGIR Conference on
  Research and Development in Information Retrieval (SIGIR 2020), July 25 to
  30, 2020, Virtual Event, China. ACM, New York, NY, USA, 4 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.13270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.02620v2</id>
    <updated>2020-09-11T11:34:22Z</updated>
    <published>2020-07-06T10:20:12Z</published>
    <title>Reducing Misinformation in Query Autocompletions</title>
    <summary>  Query autocompletions help users of search engines to speed up their searches
by recommending completions of partially typed queries in a drop down box.
These recommended query autocompletions are usually based on large logs of
queries that were previously entered by the search engine's users. Therefore,
misinformation entered -- either accidentally or purposely to manipulate the
search engine -- might end up in the search engine's recommendations,
potentially harming organizations, individuals, and groups of people. This
paper proposes an alternative approach for generating query autocompletions by
extracting anchor texts from a large web crawl, without the need to use query
logs. Our evaluation shows that even though query log autocompletions perform
better for shorter queries, anchor text autocompletions outperform query log
autocompletions for queries of 2 words or more.
</summary>
    <author>
      <name>Djoerd Hiemstra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at the 2nd International Symposium on Open Search
  Technology, 12-14 October 2020, CERN, Geneva, Switzerland</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.02620v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.02620v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.08078v2</id>
    <updated>2021-03-06T15:11:31Z</updated>
    <published>2020-07-16T02:13:55Z</published>
    <title>Political audience diversity and news reliability in algorithmic ranking</title>
    <summary>  Newsfeed algorithms frequently amplify misinformation and other low-quality
content. How can social media platforms more effectively promote reliable
information? Existing approaches are difficult to scale and vulnerable to
manipulation. In this paper, we propose using the political diversity of a
website's audience as a quality signal. Using news source reliability ratings
from domain experts and web browsing data from a diverse sample of 6,890 U.S.
citizens, we first show that websites with more extreme and less politically
diverse audiences have lower journalistic standards. We then incorporate
audience diversity into a standard collaborative filtering framework and show
that our improved algorithm increases the trustworthiness of websites suggested
to users -- especially those who most frequently consume misinformation --
while keeping recommendations relevant. These findings suggest that partisan
audience diversity is a valuable signal of higher journalistic standards that
should be incorporated into algorithmic ranking decisions.
</summary>
    <author>
      <name>Saumya Bhadani</name>
    </author>
    <author>
      <name>Shun Yamaya</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <author>
      <name>Giovanni Luca Ciampaglia</name>
    </author>
    <author>
      <name>Brendan Nyhan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41562-021-01276-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41562-021-01276-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages, 23 figures, 5 tables (including supplementary materials).
  Nat Hum Behav (2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.08078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.08413v1</id>
    <updated>2020-09-17T16:42:08Z</updated>
    <published>2020-09-17T16:42:08Z</published>
    <title>Not sure? Handling hesitancy of COVID-19 vaccines</title>
    <summary>  From the moment the first COVID-19 vaccines are rolled out, there will need
to be a large fraction of the global population ready in line. It is therefore
crucial to start managing the growing global hesitancy to any such COVID-19
vaccine. The current approach of trying to convince the "no"s cannot work
quickly enough, nor can the current policy of trying to find, remove and/or
rebut all the individual pieces of COVID and vaccine misinformation. Instead,
we show how this can be done in a simpler way by moving away from chasing
misinformation content and focusing instead on managing the "yes--no--not-sure"
hesitancy ecosystem.
</summary>
    <author>
      <name>N. F. Johnson</name>
    </author>
    <author>
      <name>N. Velasquez</name>
    </author>
    <author>
      <name>R. Leahy</name>
    </author>
    <author>
      <name>N. Johnson Restrepo</name>
    </author>
    <author>
      <name>O. Jha</name>
    </author>
    <author>
      <name>Y. Lupu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper. Comments welcome to neiljohnson@gwu.edu</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.08413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.08413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.12905v1</id>
    <updated>2020-09-27T17:38:54Z</updated>
    <published>2020-09-27T17:38:54Z</published>
    <title>COVID-19's (mis)information ecosystem on Twitter: How partisanship
  boosts the spread of conspiracy narratives on German speaking Twitter</title>
    <summary>  In late 2019, the gravest pandemic in a century began spreading across the
world. A state of uncertainty related to what has become known as SARS-CoV-2
has since fueled conspiracy narratives on social media about the origin,
transmission and medical treatment of and vaccination against the resulting
disease, COVID-19. Using social media intelligence to monitor and understand
the proliferation of conspiracy narratives is one way to analyze the
distribution of misinformation on the pandemic. We analyzed more than 9.5M
German language tweets about COVID-19. The results show that only about 0.6% of
all those tweets deal with conspiracy theory narratives. We also found that the
political orientation of users correlates with the volume of content users
contribute to the dissemination of conspiracy narratives, implying that
partisan communicators have a higher motivation to take part in conspiratorial
discussions on Twitter. Finally, we showed that contrary to other studies,
automated accounts do not significantly influence the spread of misinformation
in the German speaking Twitter sphere. They only represent about 1.31% of all
conspiracy-related activities in our database.
</summary>
    <author>
      <name>Morteza Shahrezaye</name>
    </author>
    <author>
      <name>Miriam Meckel</name>
    </author>
    <author>
      <name>L√©a Steinacker</name>
    </author>
    <author>
      <name>Viktor Suter</name>
    </author>
    <link href="http://arxiv.org/abs/2009.12905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.12905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.14337v1</id>
    <updated>2020-09-29T22:58:33Z</updated>
    <published>2020-09-29T22:58:33Z</published>
    <title>StratLearner: Learning a Strategy for Misinformation Prevention in
  Social Networks</title>
    <summary>  Given a combinatorial optimization problem taking an input, can we learn a
strategy to solve it from the examples of input-solution pairs without knowing
its objective function? In this paper, we consider such a setting and study the
misinformation prevention problem. Given the examples of attacker-protector
pairs, our goal is to learn a strategy to compute protectors against future
attackers, without the need of knowing the underlying diffusion model. To this
end, we design a structured prediction framework, where the main idea is to
parameterize the scoring function using random features constructed through
distance functions on randomly sampled subgraphs, which leads to a kernelized
scoring function with weights learnable via the large margin method. Evidenced
by experiments, our method can produce near-optimal protectors without using
any information of the diffusion model, and it outperforms other possible
graph-based and learning-based methods by an evident margin.
</summary>
    <author>
      <name>Guangmo Tong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS'20</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.14337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.14337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.03529v1</id>
    <updated>2021-01-10T11:52:17Z</updated>
    <published>2021-01-10T11:52:17Z</published>
    <title>TIB's Visual Analytics Group at MediaEval '20: Detecting Fake News on
  Corona Virus and 5G Conspiracy</title>
    <summary>  Fake news on social media has become a hot topic of research as it negatively
impacts the discourse of real news in the public. Specifically, the ongoing
COVID-19 pandemic has seen a rise of inaccurate and misleading information due
to the surrounding controversies and unknown details at the beginning of the
pandemic. The FakeNews task at MediaEval 2020 tackles this problem by creating
a challenge to automatically detect tweets containing misinformation based on
text and structure from Twitter follower network. In this paper, we present a
simple approach that uses BERT embeddings and a shallow neural network for
classifying tweets using only text, and discuss our findings and limitations of
the approach in text-based misinformation detection.
</summary>
    <author>
      <name>Gullal S. Cheema</name>
    </author>
    <author>
      <name>Sherzod Hakimov</name>
    </author>
    <author>
      <name>Ralph Ewerth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MediaEval 2020 Fake News Task</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.03529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.03529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.04031v1</id>
    <updated>2021-02-08T07:08:36Z</updated>
    <published>2021-02-08T07:08:36Z</published>
    <title>Rihanna versus Bollywood: Twitter Influencers and the Indian Farmers'
  Protest</title>
    <summary>  A tweet from popular entertainer and businesswoman, Rihanna, bringing
attention to farmers' protests around Delhi set off heightened activity on
Indian social media. An immediate consequence was the weighing in by Indian
politicians, entertainers, media and other influencers on the issue. In this
paper, we use data from Twitter and an archive of debunked misinformation
stories to understand some of the patterns around influencer engagement with a
political issue. We found that more followed influencers were less likely to
come out in support of the tweet. We also find that the later engagement of
major influencers on the side of the government's position shows suggestion's
of collusion. Irrespective of their position on the issue, influencers who
engaged saw a significant rise in their following after their tweets. While a
number of tweets thanked Rihanna for raising awareness on the issue, she was
systematically trolled on the grounds of her gender, race, nationality and
religion. Finally, we observed how misinformation existing prior to the tweet
set up the grounds for alternative narratives that emerged.
</summary>
    <author>
      <name>Dibyendu Mishra</name>
    </author>
    <author>
      <name>Syeda Zainab Akbar</name>
    </author>
    <author>
      <name>Arshia Arya</name>
    </author>
    <author>
      <name>Saloni Dash</name>
    </author>
    <author>
      <name>Rynaa Grover</name>
    </author>
    <author>
      <name>Joyojeet Pal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.04031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.04031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.06109v1</id>
    <updated>2021-02-11T16:44:09Z</updated>
    <published>2021-02-11T16:44:09Z</published>
    <title>The Deepfake Detection Dilemma: A Multistakeholder Exploration of
  Adversarial Dynamics in Synthetic Media</title>
    <summary>  Synthetic media detection technologies label media as either synthetic or
non-synthetic and are increasingly used by journalists, web platforms, and the
general public to identify misinformation and other forms of problematic
content. As both well-resourced organizations and the non-technical general
public generate more sophisticated synthetic media, the capacity for purveyors
of problematic content to adapt induces a \newterm{detection dilemma}: as
detection practices become more accessible, they become more easily
circumvented. This paper describes how a multistakeholder cohort from academia,
technology platforms, media entities, and civil society organizations active in
synthetic media detection and its socio-technical implications evaluates the
detection dilemma. Specifically, we offer an assessment of detection contexts
and adversary capacities sourced from the broader, global AI and media
integrity community concerned with mitigating the spread of harmful synthetic
media. A collection of personas illustrates the intersection between
unsophisticated and highly-resourced sponsors of misinformation in the context
of their technical capacities. This work concludes that there is no "best"
approach to navigating the detector dilemma, but derives a set of implications
from multistakeholder input to better inform detection process decisions and
policies, in practice.
</summary>
    <author>
      <name>Claire Leibowicz</name>
    </author>
    <author>
      <name>Sean McGregor</name>
    </author>
    <author>
      <name>Aviv Ovadya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.06109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.07857v1</id>
    <updated>2021-02-15T21:41:12Z</updated>
    <published>2021-02-15T21:41:12Z</published>
    <title>KNH: Multi-View Modeling with K-Nearest Hyperplanes Graph for
  Misinformation Detection</title>
    <summary>  Graphs are one of the most efficacious structures for representing datapoints
and their relations, and they have been largely exploited for different
applications. Previously, the higher-order relations between the nodes have
been modeled by a generalization of graphs known as hypergraphs. In
hypergraphs, the edges are defined by a set of nodes i.e., hyperedges to
demonstrate the higher order relationships between the data. However, there is
no explicit higher-order generalization for nodes themselves. In this work, we
introduce a novel generalization of graphs i.e., K-Nearest Hyperplanes graph
(KNH) where the nodes are defined by higher order Euclidean subspaces for
multi-view modeling of the nodes. In fact, in KNH, nodes are hyperplanes or
more precisely m-flats instead of datapoints. We experimentally evaluate the
KNH graph on two multi-aspect datasets for misinformation detection. The
experimental results suggest that multi-view modeling of articles using KNH
graph outperforms the classic KNN graph in terms of classification performance.
</summary>
    <author>
      <name>Sara Abdali</name>
    </author>
    <author>
      <name>Neil Shah</name>
    </author>
    <author>
      <name>Evangelos E. Papalexakis</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Second International TrueFact Workshop 2020: Making a Credible Web
  for Tomorrow</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.07857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.07857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.03794v1</id>
    <updated>2021-06-07T16:59:46Z</updated>
    <published>2021-06-07T16:59:46Z</published>
    <title>COVID-Fact: Fact Extraction and Verification of Real-World Claims on
  COVID-19 Pandemic</title>
    <summary>  We introduce a FEVER-like dataset COVID-Fact of $4,086$ claims concerning the
COVID-19 pandemic. The dataset contains claims, evidence for the claims, and
contradictory claims refuted by the evidence. Unlike previous approaches, we
automatically detect true claims and their source articles and then generate
counter-claims using automatic methods rather than employing human annotators.
Along with our constructed resource, we formally present the task of
identifying relevant evidence for the claims and verifying whether the evidence
refutes or supports a given claim. In addition to scientific claims, our data
contains simplified general claims from media sources, making it better suited
for detecting general misinformation regarding COVID-19. Our experiments
indicate that COVID-Fact will provide a challenging testbed for the development
of new systems and our approach will reduce the costs of building
domain-specific datasets for detecting misinformation.
</summary>
    <author>
      <name>Arkadiy Saakyan</name>
    </author>
    <author>
      <name>Tuhin Chakrabarty</name>
    </author>
    <author>
      <name>Smaranda Muresan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2021 Camera Ready</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.03794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.03794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.04899v1</id>
    <updated>2021-10-10T20:40:11Z</updated>
    <published>2021-10-10T20:40:11Z</published>
    <title>Influencing the Influencers: Evaluating Person-to-Person Influence on
  Social Networks Using Granger Causality</title>
    <summary>  We introduce a novel method for analyzing person-to-person content influence
on Twitter. Using an Ego-Alter framework and Granger Causality, we examine
President Donald Trump (the Ego) and the people he retweets (Alters) as a case
study. We find that each Alter has a different scope of influence across
multiple topics, different magnitude of influence on a given topic, and the
magnitude of a single Alter's influence can vary across topics. This work is
novel in its focus on person-to-person influence and content-based influence.
Its impact is two-fold: (1) identifying "canaries in the coal mine" who could
be observed by misinformation researchers or platforms to identify
misinformation narratives before super-influencers spread them to large
audiences, and (2) enabling digital marketing targeted toward upstream Alters
of super-influencers.
</summary>
    <author>
      <name>Richard Kuzma</name>
    </author>
    <author>
      <name>Iain J. Cruickshank</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <link href="http://arxiv.org/abs/2110.04899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.04899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06507v1</id>
    <updated>2021-12-13T09:38:41Z</updated>
    <published>2021-12-13T09:38:41Z</published>
    <title>Automated Evidence Collection for Fake News Detection</title>
    <summary>  Fake news, misinformation, and unverifiable facts on social media platforms
propagate disharmony and affect society, especially when dealing with an
epidemic like COVID-19. The task of Fake News Detection aims to tackle the
effects of such misinformation by classifying news items as fake or real. In
this paper, we propose a novel approach that improves over the current
automatic fake news detection approaches by automatically gathering evidence
for each claim. Our approach extracts supporting evidence from the web articles
and then selects appropriate text to be treated as evidence sets. We use a
pre-trained summarizer on these evidence sets and then use the extracted
summary as supporting evidence to aid the classification task. Our experiments,
using both machine learning and deep learning-based methods, help perform an
extensive evaluation of our approach. The results show that our approach
outperforms the state-of-the-art methods in fake news detection to achieve an
F1-score of 99.25 over the dataset provided for the CONSTRAINT-2021 Shared
Task. We also release the augmented dataset, our code and models for any
further research.
</summary>
    <author>
      <name>Mrinal Rawat</name>
    </author>
    <author>
      <name>Diptesh Kanojia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICON 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.06507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.13910v1</id>
    <updated>2021-12-05T02:15:01Z</updated>
    <published>2021-12-05T02:15:01Z</published>
    <title>Visual Persuasion in COVID-19 Social Media Content: A Multi-Modal
  Characterization</title>
    <summary>  Social media content routinely incorporates multi-modal design to covey
information and shape meanings, and sway interpretations toward desirable
implications, but the choices and outcomes of using both texts and visual
images have not been sufficiently studied. This work proposes a computational
approach to analyze the outcome of persuasive information in multi-modal
content, focusing on two aspects, popularity and reliability, in
COVID-19-related news articles shared on Twitter. The two aspects are
intertwined in the spread of misinformation: for example, an unreliable article
that aims to misinform has to attain some popularity. This work has several
contributions. First, we propose a multi-modal (image and text) approach to
effectively identify popularity and reliability of information sources
simultaneously. Second, we identify textual and visual elements that are
predictive to information popularity and reliability. Third, by modeling
cross-modal relations and similarity, we are able to uncover how unreliable
articles construct multi-modal meaning in a distorted, biased fashion. Our work
demonstrates how to use multi-modal analysis for understanding influential
content and has implications to social media literacy and engagement.
</summary>
    <author>
      <name>Mesut Erhan Unal</name>
    </author>
    <author>
      <name>Adriana Kovashka</name>
    </author>
    <author>
      <name>Wen-Ting Chung</name>
    </author>
    <author>
      <name>Yu-Ru Lin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3487553.3524647</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3487553.3524647" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.13910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.13910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.11664v1</id>
    <updated>2022-01-26T16:04:37Z</updated>
    <published>2022-01-26T16:04:37Z</published>
    <title>Team Yao at Factify 2022: Utilizing Pre-trained Models and Co-attention
  Networks for Multi-Modal Fact Verification</title>
    <summary>  In recent years, social media has enabled users to get exposed to a myriad of
misinformation and disinformation; thus, misinformation has attracted a great
deal of attention in research fields and as a social issue. To address the
problem, we propose a framework, Pre-CoFact, composed of two pre-trained models
for extracting features from text and images, and multiple co-attention
networks for fusing the same modality but different sources and different
modalities. Besides, we adopt the ensemble method by using different
pre-trained models in Pre-CoFact to achieve better performance. We further
illustrate the effectiveness from the ablation study and examine different
pre-trained models for comparison. Our team, Yao, won the fifth prize
(F1-score: 74.585\%) in the Factify challenge hosted by De-Factify @ AAAI 2022,
which demonstrates that our model achieved competitive performance without
using auxiliary tasks or extra information. The source code of our work is
publicly available at
https://github.com/wywyWang/Multi-Modal-Fact-Verification-2021
</summary>
    <author>
      <name>Wei-Yao Wang</name>
    </author>
    <author>
      <name>Wen-Chih Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by AAAI 2022 De-Factify Workshop: First Workshop on
  Multimodal Fact-Checking and Hate Speech Detection</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.11664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.11664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.09381v1</id>
    <updated>2022-02-18T19:01:01Z</updated>
    <published>2022-02-18T19:01:01Z</published>
    <title>Synthetic Disinformation Attacks on Automated Fact Verification Systems</title>
    <summary>  Automated fact-checking is a needed technology to curtail the spread of
online misinformation. One current framework for such solutions proposes to
verify claims by retrieving supporting or refuting evidence from related
textual sources. However, the realistic use cases for fact-checkers will
require verifying claims against evidence sources that could be affected by the
same misinformation. Furthermore, the development of modern NLP tools that can
produce coherent, fabricated content would allow malicious actors to
systematically generate adversarial disinformation for fact-checkers.
  In this work, we explore the sensitivity of automated fact-checkers to
synthetic adversarial evidence in two simulated settings: AdversarialAddition,
where we fabricate documents and add them to the evidence repository available
to the fact-checking system, and AdversarialModification, where existing
evidence source documents in the repository are automatically altered. Our
study across multiple models on three benchmarks demonstrates that these
systems suffer significant performance drops against these attacks. Finally, we
discuss the growing threat of modern NLG systems as generators of
disinformation in the context of the challenges they pose to automated
fact-checkers.
</summary>
    <author>
      <name>Yibing Du</name>
    </author>
    <author>
      <name>Antoine Bosselut</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.09381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.09381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.12883v4</id>
    <updated>2024-01-15T22:14:36Z</updated>
    <published>2022-02-25T18:47:32Z</published>
    <title>Human Detection of Political Speech Deepfakes across Transcripts, Audio,
  and Video</title>
    <summary>  Recent advances in technology for hyper-realistic visual and audio effects
provoke the concern that deepfake videos of political speeches will soon be
indistinguishable from authentic video recordings. The conventional wisdom in
communication theory predicts people will fall for fake news more often when
the same version of a story is presented as a video versus text. We conduct 5
pre-registered randomized experiments with 2,215 participants to evaluate how
accurately humans distinguish real political speeches from fabrications across
base rates of misinformation, audio sources, question framings, and media
modalities. We find base rates of misinformation minimally influence
discernment and deepfakes with audio produced by the state-of-the-art
text-to-speech algorithms are harder to discern than the same deepfakes with
voice actor audio. Moreover across all experiments, we find audio and visual
information enables more accurate discernment than text alone: human
discernment relies more on how something is said, the audio-visual cues, than
what is said, the speech content.
</summary>
    <author>
      <name>Matthew Groh</name>
    </author>
    <author>
      <name>Aruna Sankaranarayanan</name>
    </author>
    <author>
      <name>Nikhil Singh</name>
    </author>
    <author>
      <name>Dong Young Kim</name>
    </author>
    <author>
      <name>Andrew Lippman</name>
    </author>
    <author>
      <name>Rosalind Picard</name>
    </author>
    <link href="http://arxiv.org/abs/2202.12883v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.12883v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.13758v3</id>
    <updated>2022-12-12T04:47:49Z</updated>
    <published>2022-02-28T13:18:26Z</published>
    <title>Logical Fallacy Detection</title>
    <summary>  Reasoning is central to human intelligence. However, fallacious arguments are
common, and some exacerbate problems such as spreading misinformation about
climate change. In this paper, we propose the task of logical fallacy
detection, and provide a new dataset (Logic) of logical fallacies generally
found in text, together with an additional challenge set for detecting logical
fallacies in climate change claims (LogicClimate). Detecting logical fallacies
is a hard problem as the model must understand the underlying logical structure
of the argument. We find that existing pretrained large language models perform
poorly on this task. In contrast, we show that a simple structure-aware
classifier outperforms the best language model by 5.46% on Logic and 4.51% on
LogicClimate. We encourage future work to explore this task as (a) it can serve
as a new reasoning challenge for language models, and (b) it can have potential
applications in tackling the spread of misinformation. Our dataset and code are
available at https://github.com/causalNLP/logical-fallacy
</summary>
    <author>
      <name>Zhijing Jin</name>
    </author>
    <author>
      <name>Abhinav Lalwani</name>
    </author>
    <author>
      <name>Tejas Vaidhya</name>
    </author>
    <author>
      <name>Xiaoyu Shen</name>
    </author>
    <author>
      <name>Yiwen Ding</name>
    </author>
    <author>
      <name>Zhiheng Lyu</name>
    </author>
    <author>
      <name>Mrinmaya Sachan</name>
    </author>
    <author>
      <name>Rada Mihalcea</name>
    </author>
    <author>
      <name>Bernhard Sch√∂lkopf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2021 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.13758v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13758v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.11863v1</id>
    <updated>2022-06-06T09:11:03Z</updated>
    <published>2022-06-06T09:11:03Z</published>
    <title>CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking</title>
    <summary>  The explosion of misinformation spreading in the media ecosystem urges for
automated fact-checking. While misinformation spans both geographic and
linguistic boundaries, most work in the field has focused on English. Datasets
and tools available in other languages, such as Chinese, are limited. In order
to bridge this gap, we construct CHEF, the first CHinese Evidence-based
Fact-checking dataset of 10K real-world claims. The dataset covers multiple
domains, ranging from politics to public health, and provides annotated
evidence retrieved from the Internet. Further, we develop established baselines
and a novel approach that is able to model the evidence retrieval as a latent
variable, allowing jointly training with the veracity prediction model in an
end-to-end fashion. Extensive experiments show that CHEF will provide a
challenging testbed for the development of fact-checking systems designed to
retrieve and reason over non-English claims.
</summary>
    <author>
      <name>Xuming Hu</name>
    </author>
    <author>
      <name>Zhijiang Guo</name>
    </author>
    <author>
      <name>Guanyu Wu</name>
    </author>
    <author>
      <name>Aiwei Liu</name>
    </author>
    <author>
      <name>Lijie Wen</name>
    </author>
    <author>
      <name>Philip S. Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In NAACL 2022 as a long paper. Code and data available at
  https://github.com/THU-BPM/CHEF</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.11863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.11863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.09524v4</id>
    <updated>2024-01-30T23:54:37Z</updated>
    <published>2022-07-19T19:41:24Z</published>
    <title>Identifying and characterizing superspreaders of low-credibility content
  on Twitter</title>
    <summary>  The world's digital information ecosystem continues to struggle with the
spread of misinformation. Prior work has suggested that users who consistently
disseminate a disproportionate amount of low-credibility content -- so-called
superspreaders -- are at the center of this problem. We quantitatively confirm
this hypothesis and introduce simple metrics to predict the top superspreaders
several months into the future. We then conduct a qualitative review to
characterize the most prolific superspreaders and analyze their sharing
behaviors. Superspreaders include pundits with large followings,
low-credibility media outlets, personal accounts affiliated with those media
outlets, and a range of influencers. They are primarily political in nature and
use more toxic language than the typical user sharing misinformation. We also
find concerning evidence that suggests Twitter may be overlooking prominent
superspreaders. We hope this work will further public understanding of bad
actors and promote steps to mitigate their negative impacts on healthy digital
discourse.
</summary>
    <author>
      <name>Matthew R. DeVerna</name>
    </author>
    <author>
      <name>Rachith Aiyappa</name>
    </author>
    <author>
      <name>Diogo Pacheco</name>
    </author>
    <author>
      <name>John Bryden</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <link href="http://arxiv.org/abs/2207.09524v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.09524v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.13644v1</id>
    <updated>2022-07-27T17:05:16Z</updated>
    <published>2022-07-27T17:05:16Z</published>
    <title>Using Deep Learning to Detecting Deepfakes</title>
    <summary>  In the recent years, social media has grown to become a major source of
information for many online users. This has given rise to the spread of
misinformation through deepfakes. Deepfakes are videos or images that replace
one persons face with another computer-generated face, often a more
recognizable person in society. With the recent advances in technology, a
person with little technological experience can generate these videos. This
enables them to mimic a power figure in society, such as a president or
celebrity, creating the potential danger of spreading misinformation and other
nefarious uses of deepfakes. To combat this online threat, researchers have
developed models that are designed to detect deepfakes. This study looks at
various deepfake detection models that use deep learning algorithms to combat
this looming threat. This survey focuses on providing a comprehensive overview
of the current state of deepfake detection models and the unique approaches
many researchers take to solving this problem. The benefits, limitations, and
suggestions for future work will be thoroughly discussed throughout this paper.
</summary>
    <author>
      <name>Jacob Mallet</name>
    </author>
    <author>
      <name>Rushit Dave</name>
    </author>
    <author>
      <name>Naeem Seliya</name>
    </author>
    <author>
      <name>Mounika Vanamala</name>
    </author>
    <link href="http://arxiv.org/abs/2207.13644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.13644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02007v1</id>
    <updated>2022-09-05T15:16:33Z</updated>
    <published>2022-09-05T15:16:33Z</published>
    <title>Stop the [Image] Steal: The Role and Dynamics of Visual Content in the
  2020 U.S. Election Misinformation Campaign</title>
    <summary>  Images are powerful. Visual information can attract attention, improve
persuasion, trigger stronger emotions, and is easy to share and spread. We
examine the characteristics of the popular images shared on Twitter as part of
"Stop the Steal", the widespread misinformation campaign during the 2020 U.S.
election. We analyze the spread of the forty most popular images shared on
Twitter as part of this campaign. Using a coding process, we categorize and
label the images according to their type, content, origin, and role, and
perform a mixed-method analysis of these images' spread on Twitter. Our results
show that popular images include both photographs and text rendered as image.
Only very few of these popular images included alleged photographic evidence of
fraud; and none of the popular photographs had been manipulated. Most images
reached a significant portion of their total spread within several hours from
their first appearance, and both popular- and less-popular accounts were
involved in various stages of their spread.
</summary>
    <author>
      <name>Hana Matatov</name>
    </author>
    <author>
      <name>Mor Naaman</name>
    </author>
    <author>
      <name>Ofra Amir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3555599</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3555599" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ACM Hum.-Comput. Interact. 6, CSCW2, Article 541 (November
  2022), 24 pages. https://doi.org/10.1145/3555599</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.02007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.05667v1</id>
    <updated>2022-09-13T00:43:44Z</updated>
    <published>2022-09-13T00:43:44Z</published>
    <title>CovidMis20: COVID-19 Misinformation Detection System on Twitter Tweets
  using Deep Learning Models</title>
    <summary>  Online news and information sources are convenient and accessible ways to
learn about current issues. For instance, more than 300 million people engage
with posts on Twitter globally, which provides the possibility to disseminate
misleading information. There are numerous cases where violent crimes have been
committed due to fake news. This research presents the CovidMis20 dataset
(COVID-19 Misinformation 2020 dataset), which consists of 1,375,592 tweets
collected from February to July 2020. CovidMis20 can be automatically updated
to fetch the latest news and is publicly available at:
https://github.com/everythingguy/CovidMis20. This research was conducted using
Bi-LSTM deep learning and an ensemble CNN+Bi-GRU for fake news detection. The
results showed that, with testing accuracy of 92.23% and 90.56%, respectively,
the ensemble CNN+Bi-GRU model consistently provided higher accuracy than the
Bi-LSTM model.
</summary>
    <author>
      <name>Aos Mulahuwaish</name>
    </author>
    <author>
      <name>Manish Osti</name>
    </author>
    <author>
      <name>Kevin Gyorick</name>
    </author>
    <author>
      <name>Majdi Maabreh</name>
    </author>
    <author>
      <name>Ajay Gupta</name>
    </author>
    <author>
      <name>Basheer Qolomany</name>
    </author>
    <link href="http://arxiv.org/abs/2209.05667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.05667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03250v1</id>
    <updated>2022-10-06T23:29:10Z</updated>
    <published>2022-10-06T23:29:10Z</published>
    <title>Unsupervised Domain Adaptation for COVID-19 Information Service with
  Contrastive Adversarial Domain Mixup</title>
    <summary>  In the real-world application of COVID-19 misinformation detection, a
fundamental challenge is the lack of the labeled COVID data to enable
supervised end-to-end training of the models, especially at the early stage of
the pandemic. To address this challenge, we propose an unsupervised domain
adaptation framework using contrastive learning and adversarial domain mixup to
transfer the knowledge from an existing source data domain to the target
COVID-19 data domain. In particular, to bridge the gap between the source
domain and the target domain, our method reduces a radial basis function (RBF)
based discrepancy between these two domains. Moreover, we leverage the power of
domain adversarial examples to establish an intermediate domain mixup, where
the latent representations of the input text from both domains could be mixed
during the training process. Extensive experiments on multiple real-world
datasets suggest that our method can effectively adapt misinformation detection
systems to the unseen COVID-19 target domain with significant improvements
compared to the state-of-the-art baselines.
</summary>
    <author>
      <name>Huimin Zeng</name>
    </author>
    <author>
      <name>Zhenrui Yue</name>
    </author>
    <author>
      <name>Ziyi Kou</name>
    </author>
    <author>
      <name>Lanyu Shang</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2210.03250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04612v1</id>
    <updated>2022-10-03T21:04:30Z</updated>
    <published>2022-10-03T21:04:30Z</published>
    <title>When Infodemic Meets Epidemic: a Systematic Literature Review</title>
    <summary>  Epidemics and outbreaks present arduous challenges requiring both individual
and communal efforts. Social media offer significant amounts of data that can
be leveraged for bio-surveillance. They also provide a platform to quickly and
efficiently reach a sizeable percentage of the population, hence their
potential impact on various aspects of epidemic mitigation. The general
objective of this systematic literature review is to provide a methodical
overview of the integration of social media in different epidemic-related
contexts. Three research questions were conceptualized for this review,
resulting in over 10000 publications collected in the first PRISMA stage, 129
of which were selected for inclusion. A thematic method-oriented synthesis was
undertaken and identified 5 main themes related to social media enabled
epidemic surveillance, misinformation management, and mental health. Findings
uncover a need for more robust applications of the lessons learned from
epidemic post-mortem documentation. A vast gap exists between retrospective
analysis of epidemic management and result integration in prospective studies.
Harnessing the full potential of social media in epidemic related tasks
requires streamlining the results of epidemic forecasting, public opinion
understanding and misinformation propagation, all while keeping abreast of
potential mental health implications. Pro-active prevention has thus become
vital for epidemic curtailment and containment.
</summary>
    <author>
      <name>Chaimae Asaad</name>
    </author>
    <author>
      <name>Imane Khaouja</name>
    </author>
    <author>
      <name>Mounir Ghogho</name>
    </author>
    <author>
      <name>Karim Ba√Øna</name>
    </author>
    <link href="http://arxiv.org/abs/2210.04612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.04612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.07467v2</id>
    <updated>2023-10-02T20:48:41Z</updated>
    <published>2022-10-14T02:34:12Z</published>
    <title>Query Rewriting for Effective Misinformation Discovery</title>
    <summary>  We propose a novel system to help fact-checkers formulate search queries for
known misinformation claims and effectively search across multiple social media
platforms. We introduce an adaptable rewriting strategy, where editing actions
for queries containing claims (e.g., swap a word with its synonym; change verb
tense into present simple) are automatically learned through offline
reinforcement learning. Our model uses a decision transformer to learn a
sequence of editing actions that maximizes query retrieval metrics such as mean
average precision. We conduct a series of experiments showing that our query
rewriting system achieves a relative increase in the effectiveness of the
queries of up to 42%, while producing editing action sequences that are human
interpretable.
</summary>
    <author>
      <name>Ashkan Kazemi</name>
    </author>
    <author>
      <name>Artem Abzaliev</name>
    </author>
    <author>
      <name>Naihao Deng</name>
    </author>
    <author>
      <name>Rui Hou</name>
    </author>
    <author>
      <name>Scott A. Hale</name>
    </author>
    <author>
      <name>Ver√≥nica P√©rez-Rosas</name>
    </author>
    <author>
      <name>Rada Mihalcea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AACL 2023 (long paper)</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.07467v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.07467v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.15723v1</id>
    <updated>2022-10-27T18:57:20Z</updated>
    <published>2022-10-27T18:57:20Z</published>
    <title>Birdwatch: Crowd Wisdom and Bridging Algorithms can Inform Understanding
  and Reduce the Spread of Misinformation</title>
    <summary>  We present an approach for selecting objectively informative and subjectively
helpful annotations to social media posts. We draw on data from on an online
environment where contributors annotate misinformation and simultaneously rate
the contributions of others. Our algorithm uses a matrix-factorization (MF)
based approach to identify annotations that appeal broadly across heterogeneous
user groups - sometimes referred to as "bridging-based ranking." We pair these
data with a survey experiment in which individuals are randomly assigned to see
annotations to posts. We find that annotations selected by the algorithm
improve key indicators compared with overall average and crowd-generated
baselines. Further, when deployed on Twitter, people who saw annotations
selected through this bridging-based approach were significantly less likely to
reshare social media posts than those who did not see the annotations.
</summary>
    <author>
      <name>Stefan Wojcik</name>
    </author>
    <author>
      <name>Sophie Hilgard</name>
    </author>
    <author>
      <name>Nick Judd</name>
    </author>
    <author>
      <name>Delia Mocanu</name>
    </author>
    <author>
      <name>Stephen Ragain</name>
    </author>
    <author>
      <name>M. B. Fallin Hunzaker</name>
    </author>
    <author>
      <name>Keith Coleman</name>
    </author>
    <author>
      <name>Jay Baxter</name>
    </author>
    <link href="http://arxiv.org/abs/2210.15723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.15723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.11947v2</id>
    <updated>2023-06-06T19:59:34Z</updated>
    <published>2022-11-22T01:58:26Z</published>
    <title>Measuring Belief Dynamics on Twitter</title>
    <summary>  There is growing concern about misinformation and the role online media plays
in social polarization. Analyzing belief dynamics is one way to enhance our
understanding of these problems. Existing analytical tools, such as survey
research or stance detection, lack the power to correlate contextual factors
with population-level changes in belief dynamics. In this exploratory study, I
present the Belief Landscape Framework, which uses data about people's
professed beliefs in an online setting to measure belief dynamics with high
resolution. I provide initial validation of the approach by comparing the
method's output to a set of hypotheses drawn from the literature and by
inspecting the "belief landscape" generated by the method. My analysis
indicates that the method is relatively robust to different parameter settings,
and results suggest that 1) there are many stable configurations of belief, or
attractors, on the polarizing issue of climate change and 2) that people move
in predictable ways around these attractors. The method paves the way for more
powerful tools that can be used to understand how the modern digital media
ecosystem impacts collective belief dynamics and what role misinformation plays
in that process.
</summary>
    <author>
      <name>Joshua Introne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figure, to appear in the Proceedings of ICWSM '23</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.11947v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.11947v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.01241v1</id>
    <updated>2023-02-28T21:53:48Z</updated>
    <published>2023-02-28T21:53:48Z</published>
    <title>PANACEA: An Automated Misinformation Detection System on COVID-19</title>
    <summary>  In this demo, we introduce a web-based misinformation detection system
PANACEA on COVID-19 related claims, which has two modules, fact-checking and
rumour detection. Our fact-checking module, which is supported by novel natural
language inference methods with a self-attention network, outperforms
state-of-the-art approaches. It is also able to give automated veracity
assessment and ranked supporting evidence with the stance towards the claim to
be checked. In addition, PANACEA adapts the bi-directional graph convolutional
networks model, which is able to detect rumours based on comment networks of
related tweets, instead of relying on the knowledge base. This rumour detection
module assists by warning the users in the early stages when a knowledge base
may not be available.
</summary>
    <author>
      <name>Runcong Zhao</name>
    </author>
    <author>
      <name>Miguel Arana-Catania</name>
    </author>
    <author>
      <name>Lixing Zhu</name>
    </author>
    <author>
      <name>Elena Kochkina</name>
    </author>
    <author>
      <name>Lin Gui</name>
    </author>
    <author>
      <name>Arkaitz Zubiaga</name>
    </author>
    <author>
      <name>Rob Procter</name>
    </author>
    <author>
      <name>Maria Liakata</name>
    </author>
    <author>
      <name>Yulan He</name>
    </author>
    <link href="http://arxiv.org/abs/2303.01241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.01241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.14309v1</id>
    <updated>2023-03-25T00:43:36Z</updated>
    <published>2023-03-25T00:43:36Z</published>
    <title>The Challenges of Studying Misinformation on Video-Sharing Platforms
  During Crises and Mass-Convergence Events</title>
    <summary>  Mis- and disinformation can spread rapidly on video-sharing platforms (VSPs).
Despite the growing use of VSPs, there has not been a proportional increase in
our ability to understand this medium and the messages conveyed through it. In
this work, we draw on our prior experiences to outline three core challenges
faced in studying VSPs in high-stakes and fast-paced settings: (1) navigating
the unique affordances of VSPs, (2) understanding VSP content and determining
its authenticity, and (3) novel user behaviors on VSPs for spreading
misinformation. By highlighting these challenges, we hope that researchers can
reflect on how to adapt existing research methods and tools to these new
contexts, or develop entirely new ones.
</summary>
    <author>
      <name>Sukrit Venkatagiri</name>
    </author>
    <author>
      <name>Joseph S. Schafer</name>
    </author>
    <author>
      <name>Stephen Prochaska</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the ACM CHI 2023 Workshop on Building Credibility, Trust,
  and Safety on Video-Sharing Platforms (https://safevsp.github.io/)</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.14309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.02983v1</id>
    <updated>2023-04-06T10:30:04Z</updated>
    <published>2023-04-06T10:30:04Z</published>
    <title>Leveraging Social Interactions to Detect Misinformation on Social Media</title>
    <summary>  Detecting misinformation threads is crucial to guarantee a healthy
environment on social media. We address the problem using the data set created
during the COVID-19 pandemic. It contains cascades of tweets discussing
information weakly labeled as reliable or unreliable, based on a previous
evaluation of the information source. The models identifying unreliable threads
usually rely on textual features. But reliability is not just what is said, but
by whom and to whom. We additionally leverage on network information. Following
the homophily principle, we hypothesize that users who interact are generally
interested in similar topics and spreading similar kind of news, which in turn
is generally reliable or not. We test several methods to learn representations
of the social interactions within the cascades, combining them with deep neural
language models in a Multi-Input (MI) framework. Keeping track of the sequence
of the interactions during the time, we improve over previous state-of-the-art
models.
</summary>
    <author>
      <name>Tommaso Fornaciari</name>
    </author>
    <author>
      <name>Luca Luceri</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <author>
      <name>Dirk Hovy</name>
    </author>
    <link href="http://arxiv.org/abs/2304.02983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.02983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.07417v1</id>
    <updated>2023-04-14T22:32:27Z</updated>
    <published>2023-04-14T22:32:27Z</published>
    <title>Understanding and Mitigating Mental Health Misinformation on Video
  Sharing Platforms</title>
    <summary>  Despite the ever-strong demand for mental health care globally, access to
traditional mental health services remains severely limited expensive, and
stifled by stigma and systemic barriers. Thus, over the last few years, young
people are increasingly turning to content on video-sharing platforms (VSPs)
like TikTok and YouTube to help them navigate their mental health journey.
However, navigating towards trustworthy information relating to mental health
on these platforms is challenging, given the uncontrollable and unregulated
growth of dedicated mental health content and content creators catering to a
wide array of mental health conditions on these platforms. In this paper, we
attempt to define what constitutes as "mental health misinformation" through
examples. In addition, we also suggest some open questions to answer and
challenges to tackle regarding this important and timely research topic
</summary>
    <author>
      <name>Viet Cuong Nguyen</name>
    </author>
    <author>
      <name>Michael Birnbaum</name>
    </author>
    <author>
      <name>Munmun De Choudhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.07417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.07417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.07926v1</id>
    <updated>2023-04-17T00:37:53Z</updated>
    <published>2023-04-17T00:37:53Z</published>
    <title>User Perceptions of Automatic Fake News Detection: Can Algorithms Fight
  Online Misinformation?</title>
    <summary>  Fake news detection algorithms apply machine learning to various news
attributes and their relationships. However, their success is usually evaluated
based on how the algorithm performs on a static benchmark, independent of real
users. On the other hand, studies of user trust in fake news has identified
relevant factors such as the user's previous beliefs, the article format, and
the source's reputation. We present a user study (n=40) evaluating how warnings
issued by fake news detection algorithms affect the user's ability to detect
misinformation. We find that such warnings strongly influence users' perception
of the truth, that even a moderately accurate classifier can improve overall
user accuracy, and that users tend to be biased towards agreeing with the
algorithm, even when it is incorrect.
</summary>
    <author>
      <name>Bruno Tafur</name>
    </author>
    <author>
      <name>Advait Sarkar</name>
    </author>
    <link href="http://arxiv.org/abs/2304.07926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.07926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.14504v1</id>
    <updated>2023-04-21T16:38:26Z</updated>
    <published>2023-04-21T16:38:26Z</published>
    <title>Hybrid Deepfake Detection Utilizing MLP and LSTM</title>
    <summary>  The growing reliance of society on social media for authentic information has
done nothing but increase over the past years. This has only raised the
potential consequences of the spread of misinformation. One of the growing
methods in popularity is to deceive users using a deepfake. A deepfake is an
invention that has come with the latest technological advancements, which
enables nefarious online users to replace their face with a computer generated,
synthetic face of numerous powerful members of society. Deepfake images and
videos now provide the means to mimic important political and cultural figures
to spread massive amounts of false information. Models that can detect these
deepfakes to prevent the spread of misinformation are now of tremendous
necessity. In this paper, we propose a new deepfake detection schema utilizing
two deep learning algorithms: long short term memory and multilayer perceptron.
We evaluate our model using a publicly available dataset named 140k Real and
Fake Faces to detect images altered by a deepfake with accuracies achieved as
high as 74.7%
</summary>
    <author>
      <name>Jacob Mallet</name>
    </author>
    <author>
      <name>Natalie Krueger</name>
    </author>
    <author>
      <name>Mounika Vanamala</name>
    </author>
    <author>
      <name>Rushit Dave</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.14504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.14504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.03336v1</id>
    <updated>2023-05-05T07:40:41Z</updated>
    <published>2023-05-05T07:40:41Z</published>
    <title>QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion
  Techniques Detection using Multilingual Models</title>
    <summary>  Misinformation spreading in mainstream and social media has been misleading
users in different ways. Manual detection and verification efforts by
journalists and fact-checkers can no longer cope with the great scale and quick
spread of misleading information. This motivated research and industry efforts
to develop systems for analyzing and verifying news spreading online. The
SemEval-2023 Task 3 is an attempt to address several subtasks under this
overarching problem, targeting writing techniques used in news articles to
affect readers' opinions. The task addressed three subtasks with six languages,
in addition to three ``surprise'' test languages, resulting in 27 different
test setups. This paper describes our participating system to this task. Our
team is one of the 6 teams that successfully submitted runs for all setups. The
official results show that our system is ranked among the top 3 systems for 10
out of the 27 setups.
</summary>
    <author>
      <name>Maram Hasanain</name>
    </author>
    <author>
      <name>Ahmed Oumar El-Shangiti</name>
    </author>
    <author>
      <name>Rabindra Nath Nandi</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Firoj Alam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at SemEval-23 (ACL-23, propaganda, disinformation,
  misinformation, fake news</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.03336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.03336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.08283v3</id>
    <updated>2023-07-06T00:40:53Z</updated>
    <published>2023-05-15T00:06:30Z</published>
    <title>From Pretraining Data to Language Models to Downstream Tasks: Tracking
  the Trails of Political Biases Leading to Unfair NLP Models</title>
    <summary>  Language models (LMs) are pretrained on diverse data sources, including news,
discussion forums, books, and online encyclopedias. A significant portion of
this data includes opinions and perspectives which, on one hand, celebrate
democracy and diversity of ideas, and on the other hand are inherently socially
biased. Our work develops new methods to (1) measure political biases in LMs
trained on such corpora, along social and economic axes, and (2) measure the
fairness of downstream NLP models trained on top of politically biased LMs. We
focus on hate speech and misinformation detection, aiming to empirically
quantify the effects of political (social, economic) biases in pretraining data
on the fairness of high-stakes social-oriented tasks. Our findings reveal that
pretrained LMs do have political leanings that reinforce the polarization
present in pretraining corpora, propagating social biases into hate speech
predictions and misinformation detectors. We discuss the implications of our
findings for NLP research and propose future directions to mitigate unfairness.
</summary>
    <author>
      <name>Shangbin Feng</name>
    </author>
    <author>
      <name>Chan Young Park</name>
    </author>
    <author>
      <name>Yuhan Liu</name>
    </author>
    <author>
      <name>Yulia Tsvetkov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.08283v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.08283v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.07875v1</id>
    <updated>2023-06-13T16:10:10Z</updated>
    <published>2023-06-13T16:10:10Z</published>
    <title>ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support
  Lateral Reading</title>
    <summary>  With the rapid growth and spread of online misinformation, people need tools
to help them evaluate the credibility and accuracy of online information.
Lateral reading, a strategy that involves cross-referencing information with
multiple sources, may be an effective approach to achieving this goal. In this
paper, we present ReadProbe, a tool to support lateral reading, powered by
generative large language models from OpenAI and the Bing search engine. Our
tool is able to generate useful questions for lateral reading, scour the web
for relevant documents, and generate well-attributed answers to help people
better evaluate online information. We made a web-based application to
demonstrate how ReadProbe can help reduce the risk of being misled by false
information. The code is available at
https://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won
the first prize in a national AI misinformation hackathon.
</summary>
    <author>
      <name>Dake Zhang</name>
    </author>
    <author>
      <name>Ronak Pradeep</name>
    </author>
    <link href="http://arxiv.org/abs/2306.07875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.07875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.12118v2</id>
    <updated>2023-07-06T15:11:20Z</updated>
    <published>2023-06-21T09:01:53Z</published>
    <title>Visualizing Relation Between (De)Motivating Topics and Public Stance
  toward COVID-19 Vaccine</title>
    <summary>  While social media plays a vital role in communication nowadays,
misinformation and trolls can easily take over the conversation and steer
public opinion on these platforms. We saw the effect of misinformation during
the COVID-19 pandemic when public health officials faced significant push-back
while trying to motivate the public to vaccinate. To tackle the current and any
future threats in emergencies and motivate the public towards a common goal, it
is essential to understand how public motivation shifts and which topics
resonate among the general population. In this study, we proposed an
interactive visualization tool to inspect and analyze the topics that resonated
among Twitter-sphere during the COVID-19 pandemic and understand the key
factors that shifted public stance for vaccination. This tool can easily be
generalized for any scenario for visual analysis and to increase the
transparency of social media data for researchers and the general population
alike.
</summary>
    <author>
      <name>Ashiqur Rahman</name>
    </author>
    <author>
      <name>Hamed Alhoori</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JCDL57899.2023.00067</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JCDL57899.2023.00067" rel="related"/>
    <link href="http://arxiv.org/abs/2306.12118v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.12118v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.12807v2</id>
    <updated>2023-06-26T15:14:32Z</updated>
    <published>2023-06-22T11:09:06Z</published>
    <title>AI could create a perfect storm of climate misinformation</title>
    <summary>  We are in the midst of a transformation of the digital news ecosystem. The
expansion of online social networks, the influence of recommender systems,
increased automation, and new generative artificial intelligence tools are
rapidly changing the speed and the way misinformation about climate change and
sustainability issues moves around the world. Policymakers, researchers and the
public need to combine forces to address the dangerous combination of opaque
social media algorithms, polarizing social bots, and a new generation of
AI-generated content. This synthesis brief is the result of a collaboration
between Stockholm Resilience Centre at Stockholm University, the Beijer
Institute of Ecological Economics at the Royal Swedish Academy of Sciences, the
Complexity Science Hub Vienna, and Karolinska Institutet. It has been put
together as an independent contribution to the Nobel Prize Summit 2023, Truth,
Trust and Hope, Washington D.C., 24th to 26th of May 2023.
</summary>
    <author>
      <name>Victor Galaz</name>
    </author>
    <author>
      <name>Hannah Metzler</name>
    </author>
    <author>
      <name>Stefan Daume</name>
    </author>
    <author>
      <name>Andreas Olsson</name>
    </author>
    <author>
      <name>Bj√∂rn Lindstr√∂m</name>
    </author>
    <author>
      <name>Arvid Marklund</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 1 table, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.12807v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.12807v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.11575v3</id>
    <updated>2023-12-20T15:44:48Z</updated>
    <published>2023-07-21T13:28:24Z</published>
    <title>The connection between the spread of misinformation, time of day, and
  individual user activity patterns</title>
    <summary>  Social media manipulation poses a significant threat to cognitive autonomy
and unbiased opinion formation. Prior literature explored the relationship
between online activity and emotional state, cognitive resources, sunlight and
weather. However, a limited understanding exists regarding the role of time of
day in content spread and the impact of user activity patterns on
susceptibility to mis- and disinformation. This work uncovers a strong
correlation between user activity patterns and the tendency to spread
manipulated content. Through quantitative analysis of Twitter data, we examine
how user activity throughout the day aligns with chronotypical archetypes.
Evening types exhibit a significantly higher inclination towards spreading
potentially manipulated content, which is generally more likely between 2:30 AM
and 4:15 AM. This knowledge can become crucial for developing targeted
interventions and strategies that mitigate misinformation spread by addressing
vulnerable periods and user groups more susceptible to manipulation.
</summary>
    <author>
      <name>Elisabeth Stockinger</name>
    </author>
    <author>
      <name>Riccardo Gallotti</name>
    </author>
    <author>
      <name>Carina I. Hausladen</name>
    </author>
    <link href="http://arxiv.org/abs/2307.11575v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.11575v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.12332v1</id>
    <updated>2023-07-23T13:58:00Z</updated>
    <published>2023-07-23T13:58:00Z</published>
    <title>X-CapsNet For Fake News Detection</title>
    <summary>  News consumption has significantly increased with the growing popularity and
use of web-based forums and social media. This sets the stage for misinforming
and confusing people. To help reduce the impact of misinformation on users'
potential health-related decisions and other intents, it is desired to have
machine learning models to detect and combat fake news automatically. This
paper proposes a novel transformer-based model using Capsule neural
Networks(CapsNet) called X-CapsNet. This model includes a CapsNet with dynamic
routing algorithm paralyzed with a size-based classifier for detecting short
and long fake news statements. We use two size-based classifiers, a Deep
Convolutional Neural Network (DCNN) for detecting long fake news statements and
a Multi-Layer Perceptron (MLP) for detecting short news statements. To resolve
the problem of representing short news statements, we use indirect features of
news created by concatenating the vector of news speaker profiles and a vector
of polarity, sentiment, and counting words of news statements. For evaluating
the proposed architecture, we use the Covid-19 and the Liar datasets. The
results in terms of the F1-score for the Covid-19 dataset and accuracy for the
Liar dataset show that models perform better than the state-of-the-art
baselines.
</summary>
    <author>
      <name>Mohammad Hadi Goldani</name>
    </author>
    <author>
      <name>Reza Safabakhsh</name>
    </author>
    <author>
      <name>Saeedeh Momtazi</name>
    </author>
    <link href="http://arxiv.org/abs/2307.12332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.12332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.07213v3</id>
    <updated>2024-10-14T18:04:55Z</updated>
    <published>2023-08-14T15:31:32Z</published>
    <title>Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using
  Matchmaking for AI</title>
    <summary>  While many Natural Language Processing (NLP) techniques have been proposed
for fact-checking, both academic research and fact-checking organizations
report limited adoption of such NLP work due to poor alignment with
fact-checker practices, values, and needs. To address this, we investigate a
co-design method, Matchmaking for AI, to enable fact-checkers, designers, and
NLP researchers to collaboratively identify what fact-checker needs should be
addressed by technology, and to brainstorm ideas for potential solutions.
Co-design sessions we conducted with 22 professional fact-checkers yielded a
set of 11 design ideas that offer a "north star", integrating fact-checker
criteria into novel NLP design concepts. These concepts range from pre-bunking
misinformation, efficient and personalized monitoring misinformation,
proactively reducing fact-checker potential biases, and collaborative writing
fact-check reports. Our work provides new insights into both human-centered
fact-checking research and practice and AI co-design research.
</summary>
    <author>
      <name>Houjiang Liu</name>
    </author>
    <author>
      <name>Anubrata Das</name>
    </author>
    <author>
      <name>Alexander Boltz</name>
    </author>
    <author>
      <name>Didi Zhou</name>
    </author>
    <author>
      <name>Daisy Pinaroc</name>
    </author>
    <author>
      <name>Matthew Lease</name>
    </author>
    <author>
      <name>Min Kyung Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3686962</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3686962" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CSCW 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.07213v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.07213v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.15202v1</id>
    <updated>2023-08-29T10:40:46Z</updated>
    <published>2023-08-29T10:40:46Z</published>
    <title>Benchmarking the Generation of Fact Checking Explanations</title>
    <summary>  Fighting misinformation is a challenging, yet crucial, task. Despite the
growing number of experts being involved in manual fact-checking, this activity
is time-consuming and cannot keep up with the ever-increasing amount of Fake
News produced daily. Hence, automating this process is necessary to help curb
misinformation. Thus far, researchers have mainly focused on claim veracity
classification. In this paper, instead, we address the generation of
justifications (textual explanation of why a claim is classified as either true
or false) and benchmark it with novel datasets and advanced baselines. In
particular, we focus on summarization approaches over unstructured knowledge
(i.e. news articles) and we experiment with several extractive and abstractive
strategies. We employed two datasets with different styles and structures, in
order to assess the generalizability of our findings. Results show that in
justification production summarization benefits from the claim information,
and, in particular, that a claim-driven extractive step improves abstractive
summarization performances. Finally, we show that although cross-dataset
experiments suffer from performance degradation, a unique model trained on a
combination of the two datasets is able to retain style information in an
efficient manner.
</summary>
    <author>
      <name>Daniel Russo</name>
    </author>
    <author>
      <name>Serra Sinem Tekiroglu</name>
    </author>
    <author>
      <name>Marco Guerini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to TACL. This arXiv version is a pre-MIT Press publication
  version</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.15202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.15202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.15491v1</id>
    <updated>2023-08-28T01:55:44Z</updated>
    <published>2023-08-28T01:55:44Z</published>
    <title>Detecting Inactive Cyberwarriors from Online Forums</title>
    <summary>  The proliferation of misinformation has emerged as a new form of warfare in
the information age. This type of warfare involves cyberwarriors, who
deliberately propagate messages aimed at defaming opponents or fostering unity
among allies. In this study, we investigate the level of activity exhibited by
cyberwarriors within a large online forum, and remarkably, we discover that
only a minute fraction of cyberwarriors are active users. Surprisingly, despite
their expected role of actively disseminating misinformation, cyberwarriors
remain predominantly silent during peacetime and only spring into action when
necessary. Moreover, we analyze the challenges associated with identifying
cyberwarriors and provide evidence that detecting inactive cyberwarriors is
considerably more challenging than identifying their active counterparts.
Finally, we discuss potential methodologies to more effectively identify
cyberwarriors during their inactive phases, offering insights into better
capturing their presence and actions. The experimental code is released for
reproducibility:
\url{https://github.com/Ryaninthegame/Detect-Inactive-Spammers-on-PTT}.
</summary>
    <author>
      <name>Ruei-Yuan Wang</name>
    </author>
    <author>
      <name>Hung-Hsuan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2308.15491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.15491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00912v1</id>
    <updated>2023-09-02T11:23:16Z</updated>
    <published>2023-09-02T11:23:16Z</published>
    <title>Enable people to identify science news based on retracted articles on
  social media</title>
    <summary>  For many people, social media is an important way to consume news on
important topics like health. Unfortunately, some influential health news is
misinformation because it is based on retracted scientific work. Ours is the
first work to explore how people can understand this form of misinformation and
how an augmented social media interface can enable them to make use of
information about retraction. We report a between subjects think-aloud study
with 44 participants, where the experimental group used our augmented
interface. Our results indicate that this helped them consider retraction when
judging the credibility of news. Our key contributions are foundational
insights for tackling the problem, revealing the interplay between people's
understanding of scientific retraction, their prior beliefs about a topic, and
the way they use a social media interface that provides access to retraction
information.
</summary>
    <author>
      <name>Waheeb Yaqub</name>
    </author>
    <author>
      <name>Judy Kay</name>
    </author>
    <author>
      <name>Micah Goldwater</name>
    </author>
    <link href="http://arxiv.org/abs/2309.00912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.14966v1</id>
    <updated>2023-09-26T14:36:19Z</updated>
    <published>2023-09-26T14:36:19Z</published>
    <title>Interactively Learning Social Media Representations Improves News Source
  Factuality Detection</title>
    <summary>  The rise of social media has enabled the widespread propagation of fake news,
text that is published with an intent to spread misinformation and sway
beliefs. Rapidly detecting fake news, especially as new events arise, is
important to prevent misinformation.
  While prior works have tackled this problem using supervised learning
systems, automatedly modeling the complexities of the social media landscape
that enables the spread of fake news is challenging. On the contrary, having
humans fact check all news is not scalable. Thus, in this paper, we propose to
approach this problem interactively, where humans can interact to help an
automated system learn a better social media representation quality. On real
world events, our experiments show performance improvements in detecting
factuality of news sources, even after few human interactions.
</summary>
    <author>
      <name>Nikhil Mehta</name>
    </author>
    <author>
      <name>Dan Goldwasser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Findings of IJCNLP-AACL 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.14966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.14966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.00305v1</id>
    <updated>2023-09-30T08:33:04Z</updated>
    <published>2023-09-30T08:33:04Z</published>
    <title>Towards LLM-based Fact Verification on News Claims with a Hierarchical
  Step-by-Step Prompting Method</title>
    <summary>  While large pre-trained language models (LLMs) have shown their impressive
capabilities in various NLP tasks, they are still under-explored in the
misinformation domain. In this paper, we examine LLMs with in-context learning
(ICL) for news claim verification, and find that only with 4-shot demonstration
examples, the performance of several prompting methods can be comparable with
previous supervised models. To further boost performance, we introduce a
Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to
separate a claim into several subclaims and then verify each of them via
multiple questions-answering steps progressively. Experiment results on two
public misinformation datasets show that HiSS prompting outperforms
state-of-the-art fully-supervised approach and strong few-shot ICL-enabled
baselines.
</summary>
    <author>
      <name>Xuan Zhang</name>
    </author>
    <author>
      <name>Wei Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by AACL 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.00305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.00305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.07078v1</id>
    <updated>2023-08-05T22:38:05Z</updated>
    <published>2023-08-05T22:38:05Z</published>
    <title>Auditing and Robustifying COVID-19 Misinformation Datasets via
  Anticontent Sampling</title>
    <summary>  This paper makes two key contributions. First, it argues that highly
specialized rare content classifiers trained on small data typically have
limited exposure to the richness and topical diversity of the negative class
(dubbed anticontent) as observed in the wild. As a result, these classifiers'
strong performance observed on the test set may not translate into real-world
settings. In the context of COVID-19 misinformation detection, we conduct an
in-the-wild audit of multiple datasets and demonstrate that models trained with
several prominently cited recent datasets are vulnerable to anticontent when
evaluated in the wild. Second, we present a novel active learning pipeline that
requires zero manual annotation and iteratively augments the training data with
challenging anticontent, robustifying these classifiers.
</summary>
    <author>
      <name>Clay H. Yoo</name>
    </author>
    <author>
      <name>Ashiqur R. KhudaBukhsh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted at AAAI 2023 (Robust and Safe AI track)</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.07078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.07078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.14595v1</id>
    <updated>2023-10-23T06:03:55Z</updated>
    <published>2023-10-23T06:03:55Z</published>
    <title>Online Auditing of Information Flow</title>
    <summary>  Modern social media platforms play an important role in facilitating rapid
dissemination of information through their massive user networks. Fake news,
misinformation, and unverifiable facts on social media platforms propagate
disharmony and affect society. In this paper, we consider the problem of online
auditing of information flow/propagation with the goal of classifying news
items as fake or genuine. Specifically, driven by experiential studies on
real-world social media platforms, we propose a probabilistic Markovian
information spread model over networks modeled by graphs. We then formulate our
inference task as a certain sequential detection problem with the goal of
minimizing the combination of the error probability and the time it takes to
achieve correct decision. For this model, we find the optimal detection
algorithm minimizing the aforementioned risk and prove several statistical
guarantees. We then test our algorithm over real-world datasets. To that end,
we first construct an offline algorithm for learning the probabilistic
information spreading model, and then apply our optimal detection algorithm.
Experimental study show that our algorithm outperforms state-of-the-art
misinformation detection algorithms in terms of accuracy and detection time.
</summary>
    <author>
      <name>Mor Oren-Loberman</name>
    </author>
    <author>
      <name>Vered Azar</name>
    </author>
    <author>
      <name>Wasim Huleihel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.14595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.14595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.19193v1</id>
    <updated>2023-10-29T23:16:20Z</updated>
    <published>2023-10-29T23:16:20Z</published>
    <title>A Survey on Watching Social Issue Videos among YouTube and TikTok Users</title>
    <summary>  The openness and influence of video-sharing platforms (VSPs) such as YouTube
and TikTok attracted creators to share videos on various social issues.
Although social issue videos (SIVs) affect public opinions and breed
misinformation, how VSP users obtain information and interact with SIVs is
under-explored. This work surveyed 659 YouTube and 127 TikTok users to
understand the motives for consuming SIVs on VSPs. We found that VSP users are
primarily motivated by the information and entertainment gratifications to use
the platform. VSP users use SIVs for information-seeking purposes and find
YouTube and TikTok convenient to interact with SIVs. VSP users moderately watch
SIVs for entertainment and inactively engage in social interactions. SIV
consumption is associated with information and socialization gratifications of
the platform. VSP users appreciate the diversity of information and opinions
but would also do their own research and are concerned about the misinformation
and echo chamber problems.
</summary>
    <author>
      <name>Shuo Niu</name>
    </author>
    <author>
      <name>Dilasha Shrestha</name>
    </author>
    <author>
      <name>Abhisan Ghimire</name>
    </author>
    <author>
      <name>Zhicong Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2310.19193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.19193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.07453v2</id>
    <updated>2024-02-16T12:14:05Z</updated>
    <published>2023-11-13T16:35:29Z</published>
    <title>ChartCheck: Explainable Fact-Checking over Real-World Chart Images</title>
    <summary>  Whilst fact verification has attracted substantial interest in the natural
language processing community, verifying misinforming statements against data
visualizations such as charts has so far been overlooked. Charts are commonly
used in the real-world to summarize and communicate key information, but they
can also be easily misused to spread misinformation and promote certain
agendas. In this paper, we introduce ChartCheck, a novel, large-scale dataset
for explainable fact-checking against real-world charts, consisting of 1.7k
charts and 10.5k human-written claims and explanations. We systematically
evaluate ChartCheck using vision-language and chart-to-table models, and
propose a baseline to the community. Finally, we study chart reasoning types
and visual attributes that pose a challenge to these models
</summary>
    <author>
      <name>Mubashara Akhtar</name>
    </author>
    <author>
      <name>Nikesh Subedi</name>
    </author>
    <author>
      <name>Vivek Gupta</name>
    </author>
    <author>
      <name>Sahar Tahmasebi</name>
    </author>
    <author>
      <name>Oana Cocarascu</name>
    </author>
    <author>
      <name>Elena Simperl</name>
    </author>
    <link href="http://arxiv.org/abs/2311.07453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.07453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.08001v1</id>
    <updated>2023-11-14T08:55:11Z</updated>
    <published>2023-11-14T08:55:11Z</published>
    <title>A Comparative Analysis of the COVID-19 Infodemic in English and Chinese:
  Insights from Social Media Textual Data</title>
    <summary>  The COVID-19 infodemic, characterized by the rapid spread of misinformation
and unverified claims related to the pandemic, presents a significant
challenge. This paper presents a comparative analysis of the COVID-19 infodemic
in the English and Chinese languages, utilizing textual data extracted from
social media platforms. To ensure a balanced representation, two infodemic
datasets were created by augmenting previously collected social media textual
data. Through word frequency analysis, the thirty-five most frequently
occurring infodemic words are identified, shedding light on prevalent
discussions surrounding the infodemic. Moreover, topic clustering analysis
uncovers thematic structures and provides a deeper understanding of primary
topics within each language context. Additionally, sentiment analysis enables
comprehension of the emotional tone associated with COVID-19 information on
social media platforms in English and Chinese. This research contributes to a
better understanding of the COVID-19 infodemic phenomenon and can guide the
development of strategies to combat misinformation during public health crises
across different languages.
</summary>
    <author>
      <name>Jia Luo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-SARA</arxiv:affiliation>
    </author>
    <author>
      <name>Daiyun Peng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-SARA</arxiv:affiliation>
    </author>
    <author>
      <name>Lei Shi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-SARA</arxiv:affiliation>
    </author>
    <author>
      <name>Didier El Baz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-SARA</arxiv:affiliation>
    </author>
    <author>
      <name>Xinran Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fpubh.2023.1281259</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fpubh.2023.1281259" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Frontiers in Public Health, 2023, 11</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2311.08001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.08001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.03730v2</id>
    <updated>2023-12-08T19:42:35Z</updated>
    <published>2023-11-27T21:01:21Z</published>
    <title>FakeWatch ElectionShield: A Benchmarking Framework to Detect Fake News
  for Credible US Elections</title>
    <summary>  In today's technologically driven world, the spread of fake news,
particularly during crucial events such as elections, presents an increasing
challenge to the integrity of information. To address this challenge, we
introduce FakeWatch ElectionShield, an innovative framework carefully designed
to detect fake news. We have created a novel dataset of North American
election-related news articles through a blend of advanced language models
(LMs) and thorough human verification, for precision and relevance. We propose
a model hub of LMs for identifying fake news. Our goal is to provide the
research community with adaptable and accurate classification models in
recognizing the dynamic nature of misinformation. Extensive evaluation of fake
news classifiers on our dataset and a benchmark dataset shows our that while
state-of-the-art LMs slightly outperform the traditional ML models, classical
models are still competitive with their balance of accuracy, explainability,
and computational efficiency. This research sets the foundation for future
studies to address misinformation related to elections.
</summary>
    <author>
      <name>Tahniat Khan</name>
    </author>
    <author>
      <name>Mizanur Rahman</name>
    </author>
    <author>
      <name>Veronica Chatrath</name>
    </author>
    <author>
      <name>Oluwanifemi Bamgbose</name>
    </author>
    <author>
      <name>Shaina Raza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.03730v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.03730v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.01769v1</id>
    <updated>2024-02-01T03:01:11Z</updated>
    <published>2024-02-01T03:01:11Z</published>
    <title>Redefining "Hallucination" in LLMs: Towards a psychology-informed
  framework for mitigating misinformation</title>
    <summary>  In recent years, large language models (LLMs) have become incredibly popular,
with ChatGPT for example being used by over a billion users. While these models
exhibit remarkable language understanding and logical prowess, a notable
challenge surfaces in the form of "hallucinations." This phenomenon results in
LLMs outputting misinformation in a confident manner, which can lead to
devastating consequences with such a large user base. However, we question the
appropriateness of the term "hallucination" in LLMs, proposing a psychological
taxonomy based on cognitive biases and other psychological phenomena. Our
approach offers a more fine-grained understanding of this phenomenon, allowing
for targeted solutions. By leveraging insights from how humans internally
resolve similar challenges, we aim to develop strategies to mitigate LLM
hallucinations. This interdisciplinary approach seeks to move beyond
conventional terminology, providing a nuanced understanding and actionable
pathways for improvement in LLM reliability.
</summary>
    <author>
      <name>Elijah Berberette</name>
    </author>
    <author>
      <name>Jack Hutchins</name>
    </author>
    <author>
      <name>Amir Sadovnik</name>
    </author>
    <link href="http://arxiv.org/abs/2402.01769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.01769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.00411v2</id>
    <updated>2024-03-22T15:54:03Z</updated>
    <published>2024-03-01T09:57:46Z</published>
    <title>Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with
  Fact-Checking in Turkish</title>
    <summary>  The rapid spread of misinformation through social media platforms has raised
concerns regarding its impact on public opinion. While misinformation is
prevalent in other languages, the majority of research in this field has
concentrated on the English language. Hence, there is a scarcity of datasets
for other languages, including Turkish. To address this concern, we have
introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset
spans multiple domains and incorporates evidence collected from three Turkish
fact-checking organizations. Additionally, we aim to assess the effectiveness
of cross-lingual transfer learning for low-resource languages, with a
particular focus on Turkish. We demonstrate in-context learning (zero-shot and
few-shot) performance of large language models in this context. The
experimental results indicate that the dataset has the potential to advance
research in the Turkish language.
</summary>
    <author>
      <name>Recep Firat Cekinel</name>
    </author>
    <author>
      <name>Pinar Karagoz</name>
    </author>
    <author>
      <name>Cagri Coltekin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LREC-COLING 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.00411v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.00411v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.12916v1</id>
    <updated>2024-03-19T17:16:34Z</updated>
    <published>2024-03-19T17:16:34Z</published>
    <title>Effects of Automated Misinformation Warning Labels on the Intents to
  Like, Comment and Share Posts</title>
    <summary>  With fact-checking by professionals being difficult to scale on social media,
algorithmic techniques have been considered. However, it is uncertain how the
public may react to labels by automated fact-checkers. In this study, we
investigate the use of automated warning labels derived from misinformation
detection literature and investigate their effects on three forms of post
engagement. Focusing on political posts, we also consider how partisanship
affects engagement. In a two-phases within-subjects experiment with 200
participants, we found that the generic warnings suppressed intents to comment
on and share posts, but not on the intent to like them. Furthermore, when
different reasons for the labels were provided, their effects on post
engagement were inconsistent, suggesting that the reasons could have
undesirably motivated engagement instead. Partisanship effects were observed
across the labels with higher engagement for politically congruent posts. We
discuss the implications on the design and use of automated warning labels.
</summary>
    <author>
      <name>Gionnieve Lim</name>
    </author>
    <author>
      <name>Simon T. Perrault</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3623809.3623856</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3623809.3623856" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">HAI 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.12916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.12916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.20204v1</id>
    <updated>2024-03-29T14:32:41Z</updated>
    <published>2024-03-29T14:32:41Z</published>
    <title>The Future of Combating Rumors? Retrieval, Discrimination, and
  Generation</title>
    <summary>  Artificial Intelligence Generated Content (AIGC) technology development has
facilitated the creation of rumors with misinformation, impacting societal,
economic, and political ecosystems, challenging democracy. Current rumor
detection efforts fall short by merely labeling potentially misinformation
(classification task), inadequately addressing the issue, and it is unrealistic
to have authoritative institutions debunk every piece of information on social
media. Our proposed comprehensive debunking process not only detects rumors but
also provides explanatory generated content to refute the authenticity of the
information. The Expert-Citizen Collective Wisdom (ECCW) module we designed
aensures high-precision assessment of the credibility of information and the
retrieval module is responsible for retrieving relevant knowledge from a
Real-time updated debunking database based on information keywords. By using
prompt engineering techniques, we feed results and knowledge into a LLM (Large
Language Model), achieving satisfactory discrimination and explanatory effects
while eliminating the need for fine-tuning, saving computational costs, and
contributing to debunking efforts.
</summary>
    <author>
      <name>Junhao Xu</name>
    </author>
    <author>
      <name>Longdi Xian</name>
    </author>
    <author>
      <name>Zening Liu</name>
    </author>
    <author>
      <name>Mingliang Chen</name>
    </author>
    <author>
      <name>Qiuyang Yin</name>
    </author>
    <author>
      <name>Fenghua Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.20204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.20204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.08869v1</id>
    <updated>2024-04-13T01:42:10Z</updated>
    <published>2024-04-13T01:42:10Z</published>
    <title>Misinformation Resilient Search Rankings with Webgraph-based
  Interventions</title>
    <summary>  The proliferation of unreliable news domains on the internet has had
wide-reaching negative impacts on society. We introduce and evaluate
interventions aimed at reducing traffic to unreliable news domains from search
engines while maintaining traffic to reliable domains. We build these
interventions on the principles of fairness (penalize sites for what is in
their control), generality (label/fact-check agnostic), targeted (increase the
cost of adversarial behavior), and scalability (works at webscale). We refine
our methods on small-scale webdata as a testbed and then generalize the
interventions to a large-scale webgraph containing 93.9M domains and 1.6B
edges. We demonstrate that our methods penalize unreliable domains far more
than reliable domains in both settings and we explore multiple avenues to
mitigate unintended effects on both the small-scale and large-scale webgraph
experiments. These results indicate the potential of our approach to reduce the
spread of misinformation and foster a more reliable online information
ecosystem. This research contributes to the development of targeted strategies
to enhance the trustworthiness and quality of search engine results, ultimately
benefiting users and the broader digital community.
</summary>
    <author>
      <name>Peter Carragher</name>
    </author>
    <author>
      <name>Evan M. Williams</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <link href="http://arxiv.org/abs/2404.08869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.08869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.16859v1</id>
    <updated>2024-04-11T19:38:22Z</updated>
    <published>2024-04-11T19:38:22Z</published>
    <title>Rumour Evaluation with Very Large Language Models</title>
    <summary>  Conversational prompt-engineering-based large language models (LLMs) have
enabled targeted control over the output creation, enhancing versatility,
adaptability and adhoc retrieval. From another perspective, digital
misinformation has reached alarming levels. The anonymity, availability and
reach of social media offer fertile ground for rumours to propagate. This work
proposes to leverage the advancement of prompting-dependent LLMs to combat
misinformation by extending the research efforts of the RumourEval task on its
Twitter dataset. To the end, we employ two prompting-based LLM variants
(GPT-3.5-turbo and GPT-4) to extend the two RumourEval subtasks: (1) veracity
prediction, and (2) stance classification. For veracity prediction, three
classifications schemes are experimented per GPT variant. Each scheme is tested
in zero-, one- and few-shot settings. Our best results outperform the precedent
ones by a substantial margin. For stance classification,
prompting-based-approaches show comparable performance to prior results, with
no improvement over finetuning methods. Rumour stance subtask is also extended
beyond the original setting to allow multiclass classification. All of the
generated predictions for both subtasks are equipped with confidence scores
determining their trustworthiness degree according to the LLM, and post-hoc
justifications for explainability and interpretability purposes. Our primary
aim is AI for social good.
</summary>
    <author>
      <name>Dahlia Shehata</name>
    </author>
    <author>
      <name>Robin Cohen</name>
    </author>
    <author>
      <name>Charles Clarke</name>
    </author>
    <link href="http://arxiv.org/abs/2404.16859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.16859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.18033v1</id>
    <updated>2024-04-28T00:29:24Z</updated>
    <published>2024-04-28T00:29:24Z</published>
    <title>Exposing Text-Image Inconsistency Using Diffusion Models</title>
    <summary>  In the battle against widespread online misinformation, a growing problem is
text-image inconsistency, where images are misleadingly paired with texts with
different intent or meaning. Existing classification-based methods for
text-image inconsistency can identify contextual inconsistencies but fail to
provide explainable justifications for their decisions that humans can
understand. Although more nuanced, human evaluation is impractical at scale and
susceptible to errors. To address these limitations, this study introduces
D-TIIL (Diffusion-based Text-Image Inconsistency Localization), which employs
text-to-image diffusion models to localize semantic inconsistencies in text and
image pairs. These models, trained on large-scale datasets act as ``omniscient"
agents that filter out irrelevant information and incorporate background
knowledge to identify inconsistencies. In addition, D-TIIL uses text embeddings
and modified image regions to visualize these inconsistencies. To evaluate
D-TIIL's efficacy, we introduce a new TIIL dataset containing 14K consistent
and inconsistent text-image pairs. Unlike existing datasets, TIIL enables
assessment at the level of individual words and image regions and is carefully
designed to represent various inconsistencies. D-TIIL offers a scalable and
evidence-based approach to identifying and localizing text-image inconsistency,
providing a robust framework for future research combating misinformation.
</summary>
    <author>
      <name>Mingzhen Huang</name>
    </author>
    <author>
      <name>Shan Jia</name>
    </author>
    <author>
      <name>Zhou Zhou</name>
    </author>
    <author>
      <name>Yan Ju</name>
    </author>
    <author>
      <name>Jialing Cai</name>
    </author>
    <author>
      <name>Siwei Lyu</name>
    </author>
    <link href="http://arxiv.org/abs/2404.18033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.18033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.05559v1</id>
    <updated>2024-06-08T19:24:17Z</updated>
    <published>2024-06-08T19:24:17Z</published>
    <title>ThatiAR: Subjectivity Detection in Arabic News Sentences</title>
    <summary>  Detecting subjectivity in news sentences is crucial for identifying media
bias, enhancing credibility, and combating misinformation by flagging
opinion-based content. It provides insights into public sentiment, empowers
readers to make informed decisions, and encourages critical thinking. While
research has developed methods and systems for this purpose, most efforts have
focused on English and other high-resourced languages. In this study, we
present the first large dataset for subjectivity detection in Arabic,
consisting of ~3.6K manually annotated sentences, and GPT-4o based explanation.
In addition, we included instructions (both in English and Arabic) to
facilitate LLM based fine-tuning. We provide an in-depth analysis of the
dataset, annotation process, and extensive benchmark results, including PLMs
and LLMs. Our analysis of the annotation process highlights that annotators
were strongly influenced by their political, cultural, and religious
backgrounds, especially at the beginning of the annotation process. The
experimental results suggest that LLMs with in-context learning provide better
performance. We aim to release the dataset and resources for the community.
</summary>
    <author>
      <name>Reem Suwaileh</name>
    </author>
    <author>
      <name>Maram Hasanain</name>
    </author>
    <author>
      <name>Fatema Hubail</name>
    </author>
    <author>
      <name>Wajdi Zaghouani</name>
    </author>
    <author>
      <name>Firoj Alam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Subjectivity, Sentiment, Disinformation, Misinformation, Fake news,
  LLMs, Transformers, Instruction Dataset</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.05559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.05559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09815v1</id>
    <updated>2024-06-14T08:13:34Z</updated>
    <published>2024-06-14T08:13:34Z</published>
    <title>Retrieval Augmented Fact Verification by Synthesizing Contrastive
  Arguments</title>
    <summary>  The rapid propagation of misinformation poses substantial risks to public
interest. To combat misinformation, large language models (LLMs) are adapted to
automatically verify claim credibility. Nevertheless, existing methods heavily
rely on the embedded knowledge within LLMs and / or black-box APIs for evidence
collection, leading to subpar performance with smaller LLMs or upon unreliable
context. In this paper, we propose retrieval augmented fact verification
through the synthesis of contrasting arguments (RAFTS). Upon input claims,
RAFTS starts with evidence retrieval, where we design a retrieval pipeline to
collect and re-rank relevant documents from verifiable sources. Then, RAFTS
forms contrastive arguments (i.e., supporting or refuting) conditioned on the
retrieved evidence. In addition, RAFTS leverages an embedding model to identify
informative demonstrations, followed by in-context prompting to generate the
prediction and explanation. Our method effectively retrieves relevant documents
as evidence and evaluates arguments from varying perspectives, incorporating
nuanced information for fine-grained decision-making. Combined with informative
in-context examples as prior, RAFTS achieves significant improvements to
supervised and LLM baselines without complex prompts. We demonstrate the
effectiveness of our method through extensive experiments, where RAFTS can
outperform GPT-based methods with a significantly smaller 7B LLM.
</summary>
    <author>
      <name>Zhenrui Yue</name>
    </author>
    <author>
      <name>Huimin Zeng</name>
    </author>
    <author>
      <name>Lanyu Shang</name>
    </author>
    <author>
      <name>Yifan Liu</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.09815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09967v1</id>
    <updated>2024-06-14T12:16:08Z</updated>
    <published>2024-06-14T12:16:08Z</published>
    <title>Bag of Lies: Robustness in Continuous Pre-training BERT</title>
    <summary>  This study aims to acquire more insights into the continuous pre-training
phase of BERT regarding entity knowledge, using the COVID-19 pandemic as a case
study. Since the pandemic emerged after the last update of BERT's pre-training
data, the model has little to no entity knowledge about COVID-19. Using
continuous pre-training, we control what entity knowledge is available to the
model. We compare the baseline BERT model with the further pre-trained variants
on the fact-checking benchmark Check-COVID. To test the robustness of
continuous pre-training, we experiment with several adversarial methods to
manipulate the input data, such as training on misinformation and shuffling the
word order until the input becomes nonsensical. Surprisingly, our findings
reveal that these methods do not degrade, and sometimes even improve, the
model's downstream performance. This suggests that continuous pre-training of
BERT is robust against misinformation. Furthermore, we are releasing a new
dataset, consisting of original texts from academic publications in the
LitCovid repository and their AI-generated false counterparts.
</summary>
    <author>
      <name>Ine Gevers</name>
    </author>
    <author>
      <name>Walter Daelemans</name>
    </author>
    <link href="http://arxiv.org/abs/2406.09967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.00369v1</id>
    <updated>2024-06-29T08:39:07Z</updated>
    <published>2024-06-29T08:39:07Z</published>
    <title>How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open
  Models</title>
    <summary>  Given the growing influx of misinformation across news and social media,
there is a critical need for systems that can provide effective real-time
verification of news claims. Large language or multimodal model based
verification has been proposed to scale up online policing mechanisms for
mitigating spread of false and harmful content. While these can potentially
reduce burden on human fact-checkers, such efforts may be hampered by
foundation model training data becoming outdated. In this work, we test the
limits of improving foundation model performance without continual updating
through an initial study of knowledge transfer using either existing intra- and
inter- domain benchmarks or explanations generated from large language models
(LLMs). We evaluate on 12 public benchmarks for fact-checking and
misinformation detection as well as two other tasks relevant to content
moderation -- toxicity and stance detection. Our results on two recent
multi-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that
knowledge transfer strategies can improve Fakeddit performance over the
state-of-the-art by up to 1.7% and Mocheg performance by up to 2.9%.
</summary>
    <author>
      <name>Jaeyoung Lee</name>
    </author>
    <author>
      <name>Ximing Lu</name>
    </author>
    <author>
      <name>Jack Hessel</name>
    </author>
    <author>
      <name>Faeze Brahman</name>
    </author>
    <author>
      <name>Youngjae Yu</name>
    </author>
    <author>
      <name>Yonatan Bisk</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <author>
      <name>Saadia Gabriel</name>
    </author>
    <link href="http://arxiv.org/abs/2407.00369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.03190v2</id>
    <updated>2024-07-26T21:51:19Z</updated>
    <published>2024-06-15T02:36:11Z</published>
    <title>Cutting through the noise to motivate people: A comprehensive analysis
  of COVID-19 social media posts de/motivating vaccination</title>
    <summary>  The COVID-19 pandemic exposed significant weaknesses in the healthcare
information system. The overwhelming volume of misinformation on social media
and other socioeconomic factors created extraordinary challenges to motivate
people to take proper precautions and get vaccinated. In this context, our work
explored a novel direction by analyzing an extensive dataset collected over two
years, identifying the topics de/motivating the public about COVID-19
vaccination. We analyzed these topics based on time, geographic location, and
political orientation. We noticed that while the motivating topics remain the
same over time and geographic location, the demotivating topics change rapidly.
We also identified that intrinsic motivation, rather than external mandate, is
more advantageous to inspire the public. This study addresses scientific
communication and public motivation in social media. It can help public health
officials, policymakers, and social media platforms develop more effective
messaging strategies to cut through the noise of misinformation and educate the
public about scientific findings.
</summary>
    <author>
      <name>Ashiqur Rahman</name>
    </author>
    <author>
      <name>Ehsan Mohammadi</name>
    </author>
    <author>
      <name>Hamed Alhoori</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nlp.2024.100085</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nlp.2024.100085" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">51 pages, 13 figures, 12 tables. Accepted at Natural Language
  Processing Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Language Processing Journal, Volume 8, 2024, 100085, ISSN
  2949-7191</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2407.03190v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.03190v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.03865v1</id>
    <updated>2024-07-04T11:54:09Z</updated>
    <published>2024-07-04T11:54:09Z</published>
    <title>Reliability Criteria for News Websites</title>
    <summary>  Misinformation poses a threat to democracy and to people's health.
Reliability criteria for news websites can help people identify misinformation.
But despite their importance, there has been no empirically substantiated list
of criteria for distinguishing reliable from unreliable news websites. We
identify reliability criteria, describe how they are applied in practice, and
compare them to prior work. Based on our analysis, we distinguish between
manipulable and less manipulable criteria and compare politically diverse
laypeople as end users and journalists as expert users. We discuss 11 widely
recognized criteria, including the following 6 criteria that are difficult to
manipulate: content, political alignment, authors, professional standards, what
sources are used, and a website's reputation. Finally, we describe how
technology may be able to support people in applying these criteria in practice
to assess the reliability of websites.
</summary>
    <author>
      <name>Hendrik Heuer</name>
    </author>
    <author>
      <name>Elena Leah Glassman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3635147</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3635147" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published by ACM Transactions on Computer-Human Interaction,
  presented at CHI 2024</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Comput.-Hum. Interact. 31, 2, Article 21 (April 2024),
  33 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2407.03865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.03865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.05464v1</id>
    <updated>2024-07-07T18:31:09Z</updated>
    <published>2024-07-07T18:31:09Z</published>
    <title>Experiments with truth using Machine Learning: Spectral analysis and
  explainable classification of synthetic, false, and genuine information</title>
    <summary>  Misinformation is still a major societal problem and the arrival of Large
Language Models (LLMs) only added to it. This paper analyzes synthetic, false,
and genuine information in the form of text from spectral analysis,
visualization, and explainability perspectives to find the answer to why the
problem is still unsolved despite multiple years of research and a plethora of
solutions in the literature. Various embedding techniques on multiple datasets
are used to represent information for the purpose. The diverse spectral and
non-spectral methods used on these embeddings include t-distributed Stochastic
Neighbor Embedding (t-SNE), Principal Component Analysis (PCA), and Variational
Autoencoders (VAEs). Classification is done using multiple machine learning
algorithms. Local Interpretable Model-Agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), and Integrated Gradients are used for the
explanation of the classification. The analysis and the explanations generated
show that misinformation is quite closely intertwined with genuine information
and the machine learning algorithms are not as effective in separating the two
despite the claims in the literature.
</summary>
    <author>
      <name>Vishnu S. Pendyala</name>
    </author>
    <author>
      <name>Madhulika Dutta</name>
    </author>
    <link href="http://arxiv.org/abs/2407.05464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.05464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.08172v2</id>
    <updated>2025-04-15T20:01:58Z</updated>
    <published>2024-06-28T20:31:06Z</published>
    <title>Global Patterns of Viral Content on WhatsApp</title>
    <summary>  This paper explores the nature and spread of viral WhatsApp content among
everyday users in three diverse countries: India, Indonesia, and Colombia. By
analyzing hundreds of viral messages collected with participants' consent from
private WhatsApp groups, we provide one of the first cross-cultural
categorizations of viral content on WhatsApp. Despite the differences in
cultural and geographic settings, our findings reveal striking similarities in
the types of groups users engage with and the viral content they receive,
particularly in the prevalence of misinformation. Our comparative analysis
shows that viral content often includes political and religious narratives,
with misinformation frequently recirculated despite prior debunking by
fact-checking organizations. These parallels suggest that closed messaging
platforms like WhatsApp facilitate similar patterns of information
dissemination across different cultural contexts. This work contributes to the
broader understanding of global digital communication ecosystems and provides a
foundation for future research on information flow and moderation strategies in
private messaging platforms.
</summary>
    <author>
      <name>Kiran Garimella</name>
    </author>
    <author>
      <name>Princessa Cintaqia</name>
    </author>
    <author>
      <name>Juan Jose Rojas Constain</name>
    </author>
    <author>
      <name>Bharat Nayak</name>
    </author>
    <author>
      <name>Aditya Vashistha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at ICWSM. please cite the ICWSM version</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.08172v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.08172v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.13488v1</id>
    <updated>2024-07-18T13:08:55Z</updated>
    <published>2024-07-18T13:08:55Z</published>
    <title>Similarity over Factuality: Are we making progress on multimodal
  out-of-context misinformation detection?</title>
    <summary>  Out-of-context (OOC) misinformation poses a significant challenge in
multimodal fact-checking, where images are paired with texts that misrepresent
their original context to support false narratives. Recent research in
evidence-based OOC detection has seen a trend towards increasingly complex
architectures, incorporating Transformers, foundation models, and large
language models. In this study, we introduce a simple yet robust baseline,
which assesses MUltimodal SimilaritiEs (MUSE), specifically the similarity
between image-text pairs and external image and text evidence. Our results
demonstrate that MUSE, when used with conventional classifiers like Decision
Tree, Random Forest, and Multilayer Perceptron, can compete with and even
surpass the state-of-the-art on the NewsCLIPpings and VERITE datasets.
Furthermore, integrating MUSE in our proposed "Attentive Intermediate
Transformer Representations" (AITR) significantly improved performance, by 3.3%
and 7.5% on NewsCLIPpings and VERITE, respectively. Nevertheless, the success
of MUSE, relying on surface-level patterns and shortcuts, without examining
factuality and logical inconsistencies, raises critical questions about how we
define the task, construct datasets, collect external evidence and overall, how
we assess progress in the field. We release our code at:
https://github.com/stevejpapad/outcontext-misinfo-progress
</summary>
    <author>
      <name>Stefanos-Iordanis Papadopoulos</name>
    </author>
    <author>
      <name>Christos Koutlis</name>
    </author>
    <author>
      <name>Symeon Papadopoulos</name>
    </author>
    <author>
      <name>Panagiotis C. Petrantonakis</name>
    </author>
    <link href="http://arxiv.org/abs/2407.13488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.13488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.07448v2</id>
    <updated>2024-09-02T11:45:41Z</updated>
    <published>2024-08-14T10:36:17Z</published>
    <title>LiveFC: A System for Live Fact-Checking of Audio Streams</title>
    <summary>  The advances in the digital era have led to rapid dissemination of
information. This has also aggravated the spread of misinformation and
disinformation. This has potentially serious consequences, such as civil
unrest. While fact-checking aims to combat this, manual fact-checking is
cumbersome and not scalable. While automated fact-checking approaches exist,
they do not operate in real-time and do not always account for spread of
misinformation through different modalities. This is particularly important as
proactive fact-checking on live streams in real-time can help people be
informed of false narratives and prevent catastrophic consequences that may
cause civil unrest. This is particularly relevant with the rapid dissemination
of information through video on social media platforms or other streams like
political rallies and debates. Hence, in this work we develop a platform named
LiveFC, that can aid in fact-checking live audio streams in real-time. LiveFC
has a user-friendly interface that displays the claims detected along with
their veracity and evidence for live streams with associated speakers for
claims from respective segments. The app can be accessed at
http://livefc.factiverse.ai and a screen recording of the demo can be found at
https://bit.ly/3WVAoIw.
</summary>
    <author>
      <name>Venktesh V</name>
    </author>
    <author>
      <name>Vinay Setty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Review, 11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.07448v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.07448v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.07718v1</id>
    <updated>2024-08-14T08:49:41Z</updated>
    <published>2024-08-14T08:49:41Z</published>
    <title>Impact of Inaccurate Contamination Ratio on Robust Unsupervised Anomaly
  Detection</title>
    <summary>  Training data sets intended for unsupervised anomaly detection, typically
presumed to be anomaly-free, often contain anomalies (or contamination), a
challenge that significantly undermines model performance. Most robust
unsupervised anomaly detection models rely on contamination ratio information
to tackle contamination. However, in reality, contamination ratio may be
inaccurate. We investigate on the impact of inaccurate contamination ratio
information in robust unsupervised anomaly detection. We verify whether they
are resilient to misinformed contamination ratios. Our investigation on 6
benchmark data sets reveals that such models are not adversely affected by
exposure to misinformation. In fact, they can exhibit improved performance when
provided with such inaccurate contamination ratios.
</summary>
    <author>
      <name>Jordan F. Masakuna</name>
    </author>
    <author>
      <name>DJeff Kanda Nkashama</name>
    </author>
    <author>
      <name>Arian Soltani</name>
    </author>
    <author>
      <name>Marc Frappier</name>
    </author>
    <author>
      <name>Pierre-Martin Tardif</name>
    </author>
    <author>
      <name>Froduald Kabanza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an accepted extended abstract at Black in AI Workshop which
  will be co-located with NeurIPS 2024 in Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.07718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.07718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.07085v1</id>
    <updated>2024-09-11T08:11:16Z</updated>
    <published>2024-09-11T08:11:16Z</published>
    <title>Understanding Knowledge Drift in LLMs through Misinformation</title>
    <summary>  Large Language Models (LLMs) have revolutionized numerous applications,
making them an integral part of our digital ecosystem. However, their
reliability becomes critical, especially when these models are exposed to
misinformation. We primarily analyze the susceptibility of state-of-the-art
LLMs to factual inaccuracies when they encounter false information in a QnA
scenario, an issue that can lead to a phenomenon we refer to as *knowledge
drift*, which significantly undermines the trustworthiness of these models. We
evaluate the factuality and the uncertainty of the models' responses relying on
Entropy, Perplexity, and Token Probability metrics. Our experiments reveal that
an LLM's uncertainty can increase up to 56.6% when the question is answered
incorrectly due to the exposure to false information. At the same time,
repeated exposure to the same false information can decrease the models
uncertainty again (-52.8% w.r.t. the answers on the untainted prompts),
potentially manipulating the underlying model's beliefs and introducing a drift
from its original knowledge. These findings provide insights into LLMs'
robustness and vulnerability to adversarial inputs, paving the way for
developing more reliable LLM applications across various domains. The code is
available at https://github.com/afastowski/knowledge_drift.
</summary>
    <author>
      <name>Alina Fastowski</name>
    </author>
    <author>
      <name>Gjergji Kasneci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures. Accepted at DELTA workshop at KDD 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.07085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.07085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.12809v2</id>
    <updated>2025-01-08T13:59:28Z</updated>
    <published>2024-09-19T14:34:20Z</published>
    <title>Don't be Fooled: The Misinformation Effect of Explanations in Human-AI
  Collaboration</title>
    <summary>  Across various applications, humans increasingly use black-box artificial
intelligence (AI) systems without insight into these systems' reasoning. To
counter this opacity, explainable AI (XAI) methods promise enhanced
transparency and interpretability. While recent studies have explored how XAI
affects human-AI collaboration, few have examined the potential pitfalls caused
by incorrect explanations. The implications for humans can be far-reaching but
have not been explored extensively. To investigate this, we ran a study (n=160)
on AI-assisted decision-making in which humans were supported by XAI. Our
findings reveal a misinformation effect when incorrect explanations accompany
correct AI advice with implications post-collaboration. This effect causes
humans to infer flawed reasoning strategies, hindering task execution and
demonstrating impaired procedural knowledge. Additionally, incorrect
explanations compromise human-AI team-performance during collaboration. With
our work, we contribute to HCI by providing empirical evidence for the negative
consequences of incorrect explanations on humans post-collaboration and
outlining guidelines for designers of AI.
</summary>
    <author>
      <name>Philipp Spitzer</name>
    </author>
    <author>
      <name>Joshua Holstein</name>
    </author>
    <author>
      <name>Katelyn Morrison</name>
    </author>
    <author>
      <name>Kenneth Holstein</name>
    </author>
    <author>
      <name>Gerhard Satzger</name>
    </author>
    <author>
      <name>Niklas K√ºhl</name>
    </author>
    <link href="http://arxiv.org/abs/2409.12809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.12809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.15319v1</id>
    <updated>2024-09-05T19:24:38Z</updated>
    <published>2024-09-05T19:24:38Z</published>
    <title>Merging AI Incidents Research with Political Misinformation Research:
  Introducing the Political Deepfakes Incidents Database</title>
    <summary>  This article presents the Political Deepfakes Incidents Database (PDID), a
collection of politically-salient deepfakes, encompassing synthetically-created
videos, images, and less-sophisticated `cheapfakes.' The project is driven by
the rise of generative AI in politics, ongoing policy efforts to address harms,
and the need to connect AI incidents and political communication research. The
database contains political deepfake content, metadata, and researcher-coded
descriptors drawn from political science, public policy, communication, and
misinformation studies. It aims to help reveal the prevalence, trends, and
impact of political deepfakes, such as those featuring major political figures
or events. The PDID can benefit policymakers, researchers, journalists,
fact-checkers, and the public by providing insights into deepfake usage, aiding
in regulation, enabling in-depth analyses, supporting fact-checking and
trust-building efforts, and raising awareness of political deepfakes. It is
suitable for research and application on media effects, political discourse, AI
ethics, technology governance, media literacy, and countermeasures.
</summary>
    <author>
      <name>Christina P. Walker</name>
    </author>
    <author>
      <name>Daniel S. Schiff</name>
    </author>
    <author>
      <name>Kaylyn Jackson Schiff</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1609/aaai.v38i21.30349</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1609/aaai.v38i21.30349" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the AAAI Conference on Artificial Intelligence.
  Vol. 38. No. 21. 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2409.15319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.15319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.17484v2</id>
    <updated>2025-03-14T05:38:35Z</updated>
    <published>2024-09-26T02:46:43Z</published>
    <title>Crafting Synthetic Realities: Examining Visual Realism and
  Misinformation Potential of Photorealistic AI-Generated Images</title>
    <summary>  Advances in generative models have created Artificial Intelligence-Generated
Images (AIGIs) nearly indistinguishable from real photographs. Leveraging a
large corpus of 30,824 AIGIs collected from Instagram and Twitter, and
combining quantitative content analysis with qualitative analysis, this study
unpacks AI photorealism of AIGIs from four key dimensions, content, human,
aesthetic, and production features. We find that photorealistic AIGIs often
depict human figures, especially celebrities and politicians, with a high
degree of surrealism and aesthetic professionalism, alongside a low degree of
overt signals of AI production. This study is the first to empirically
investigate photorealistic AIGIs across multiple platforms using a
mixed-methods approach. Our findings provide important implications and
insights for understanding visual misinformation and mitigating potential risks
associated with photorealistic AIGIs. We also propose design recommendations to
enhance the responsible use of AIGIs.
</summary>
    <author>
      <name>Qiyao Peng</name>
    </author>
    <author>
      <name>Yingdan Lu</name>
    </author>
    <author>
      <name>Yilang Peng</name>
    </author>
    <author>
      <name>Sijia Qian</name>
    </author>
    <author>
      <name>Xinyi Liu</name>
    </author>
    <author>
      <name>Cuihua Shen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3706599.3719834</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3706599.3719834" rel="related"/>
    <link href="http://arxiv.org/abs/2409.17484v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.17484v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.04068v1</id>
    <updated>2024-10-05T07:41:17Z</updated>
    <published>2024-10-05T07:41:17Z</published>
    <title>ECon: On the Detection and Resolution of Evidence Conflicts</title>
    <summary>  The rise of large language models (LLMs) has significantly influenced the
quality of information in decision-making systems, leading to the prevalence of
AI-generated content and challenges in detecting misinformation and managing
conflicting information, or "inter-evidence conflicts." This study introduces a
method for generating diverse, validated evidence conflicts to simulate
real-world misinformation scenarios. We evaluate conflict detection methods,
including Natural Language Inference (NLI) models, factual consistency (FC)
models, and LLMs, on these conflicts (RQ1) and analyze LLMs' conflict
resolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models
exhibit high precision in detecting answer conflicts, though weaker models
suffer from low recall; (2) FC models struggle with lexically similar answer
conflicts, while NLI and LLM models handle these better; and (3) stronger
models like GPT-4 show robust performance, especially with nuanced conflicts.
For conflict resolution, LLMs often favor one piece of conflicting evidence
without justification and rely on internal knowledge if they have prior
beliefs.
</summary>
    <author>
      <name>Cheng Jiayang</name>
    </author>
    <author>
      <name>Chunkit Chan</name>
    </author>
    <author>
      <name>Qianqian Zhuang</name>
    </author>
    <author>
      <name>Lin Qiu</name>
    </author>
    <author>
      <name>Tianhang Zhang</name>
    </author>
    <author>
      <name>Tengxiao Liu</name>
    </author>
    <author>
      <name>Yangqiu Song</name>
    </author>
    <author>
      <name>Yue Zhang</name>
    </author>
    <author>
      <name>Pengfei Liu</name>
    </author>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by EMNLP 2024 main conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.04068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.04068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.08753v2</id>
    <updated>2024-10-14T13:15:22Z</updated>
    <published>2024-10-11T12:12:39Z</published>
    <title>Who should fight the spread of fake news?</title>
    <summary>  This study investigates who should bear the responsibility of combating the
spread of misinformation in social networks. Should that be the online
platforms or their users? Should that be done by debunking the "fake news"
already in circulation or by investing in preemptive efforts to prevent their
diffusion altogether? We seek to answer such questions in a stylized opinion
dynamics framework, where agents in a network aggregate the information they
receive from peers and/or from influential external sources, with the aim of
learning a ground truth among a set of competing hypotheses. In most cases, we
find centralized sources to be more effective at combating misinformation than
distributed ones, suggesting that online platforms should play an active role
in the fight against fake news. In line with literature on the "backfire
effect", we find that debunking in certain circumstances can be a
counterproductive strategy, whereas some targeted strategies (akin to
"deplatforming") and/or preemptive campaigns turn out to be quite effective.
Despite its simplicity, our model provides useful guidelines that could inform
the ongoing debate on online disinformation and the best ways to limit its
damaging effects.
</summary>
    <author>
      <name>Diana Riazi</name>
    </author>
    <author>
      <name>Giacomo Livan</name>
    </author>
    <link href="http://arxiv.org/abs/2410.08753v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.08753v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.10303v1</id>
    <updated>2024-10-14T09:02:42Z</updated>
    <published>2024-10-14T09:02:42Z</published>
    <title>A Comparative Study of Translation Bias and Accuracy in Multilingual
  Large Language Models for Cross-Language Claim Verification</title>
    <summary>  The rise of digital misinformation has heightened interest in using
multilingual Large Language Models (LLMs) for fact-checking. This study
systematically evaluates translation bias and the effectiveness of LLMs for
cross-lingual claim verification across 15 languages from five language
families: Romance, Slavic, Turkic, Indo-Aryan, and Kartvelian. Using the XFACT
dataset to assess their impact on accuracy and bias, we investigate two
distinct translation methods: pre-translation and self-translation. We use
mBERT's performance on the English dataset as a baseline to compare
language-specific accuracies. Our findings reveal that low-resource languages
exhibit significantly lower accuracy in direct inference due to
underrepresentation in the training data. Furthermore, larger models
demonstrate superior performance in self-translation, improving translation
accuracy and reducing bias. These results highlight the need for balanced
multilingual training, especially in low-resource languages, to promote
equitable access to reliable fact-checking tools and minimize the risk of
spreading misinformation in different linguistic contexts.
</summary>
    <author>
      <name>Aryan Singhal</name>
    </author>
    <author>
      <name>Veronica Shao</name>
    </author>
    <author>
      <name>Gary Sun</name>
    </author>
    <author>
      <name>Ryan Ding</name>
    </author>
    <author>
      <name>Jonathan Lu</name>
    </author>
    <author>
      <name>Kevin Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ATTRIB @ NeurIPS 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.10303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.10303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.11856v1</id>
    <updated>2024-10-04T03:54:59Z</updated>
    <published>2024-10-04T03:54:59Z</published>
    <title>Malak: AI-based multilingual personal assistant to combat misinformation
  and generative AI safety issues</title>
    <summary>  The widespread use of AI technologies to generate digital content has led to
increased misinformation and online harm. Deep fake technologies, a type of AI,
make it easier to create convincing but fake content on social media, leading
to various cyber threats. Malicious actors exploit AI capabilities, posing
digital, physical, and psychological harm to individuals. While social media
platforms have safety measures such as content rating and feedback systems,
these are often used by people with higher digital literacy. There is a lack of
preventive measures and a need for user-friendly tools that can be used by
people with lower digital literacy. Our goal is to create a user-friendly
multilingual AI-based personal assistant, Malak, to reduce online harm and
promote safe online interactions, benefiting users with lower literacy levels.
</summary>
    <author>
      <name>Farnaz Farid</name>
    </author>
    <author>
      <name>Farhad Ahamed</name>
    </author>
    <link href="http://arxiv.org/abs/2410.11856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.11856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.14964v2</id>
    <updated>2025-05-15T01:51:31Z</updated>
    <published>2024-10-19T03:44:19Z</published>
    <title>ChronoFact: Timeline-based Temporal Fact Verification</title>
    <summary>  Temporal claims, often riddled with inaccuracies, are a significant challenge
in the digital misinformation landscape. Fact-checking systems that can
accurately verify such claims are crucial for combating misinformation. Current
systems struggle with the complexities of evaluating the accuracy of these
claims, especially when they include multiple, overlapping, or recurring
events. We introduce a novel timeline-based fact verification framework that
identify events from both claim and evidence and organize them into their
respective chronological timelines. The framework systematically examines the
relationships between the events in both claim and evidence to predict the
veracity of each claim event and their chronological accuracy. This allows us
to accurately determine the overall veracity of the claim. We also introduce a
new dataset of complex temporal claims involving timeline-based reasoning for
the training and evaluation of our proposed framework. Experimental results
demonstrate the effectiveness of our approach in handling the intricacies of
temporal claim verification.
</summary>
    <author>
      <name>Anab Maulana Barik</name>
    </author>
    <author>
      <name>Wynne Hsu</name>
    </author>
    <author>
      <name>Mong Li Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2410.14964v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.14964v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.05922v1</id>
    <updated>2024-11-08T19:10:42Z</updated>
    <published>2024-11-08T19:10:42Z</published>
    <title>Bridging Nodes and Narrative Flows: Identifying Intervention Targets for
  Disinformation on Telegram</title>
    <summary>  In recent years, mass-broadcast messaging platforms like Telegram have gained
prominence for both, serving as a harbor for private communication and enabling
large-scale disinformation campaigns. The encrypted and networked nature of
these platforms makes it challenging to identify intervention targets since
most channels that promote misleading information are not originators of the
message. In this work, we examine the structural mechanisms that facilitate the
propagation of debunked misinformation on Telegram, focusing on the role of
cross-community hubs-nodes that bridge otherwise isolated groups in amplifying
misinformation. We introduce a multi-dimensional 'bridging' metric to quantify
the influence of nodal Telegram channels, exploring their role in reshaping
network topology during key geopolitical events. By analyzing over 1740
Telegram channels and applying network analysis we uncover the small subset of
nodes, and identify patterns that are emblematic of information 'flows' on this
platform. Our findings provide insights into the structural vulnerabilities of
distributed platforms, offering practical suggestions for interventions to
mitigate networked disinformation flows.
</summary>
    <author>
      <name>Devang Shah</name>
    </author>
    <author>
      <name>Hriday Ranka</name>
    </author>
    <author>
      <name>Lynnette Hui Xian NG</name>
    </author>
    <author>
      <name>Swapneel Mehta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">*Both Authors contributed equally to this work. 22 pages, 11 figures,
  3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.05922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.05922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.07791v1</id>
    <updated>2024-11-26T10:20:53Z</updated>
    <published>2024-11-26T10:20:53Z</published>
    <title>Digital Democracy in the Age of Artificial Intelligence</title>
    <summary>  This chapter explores the influence of Artificial Intelligence (AI) on
digital democracy, focusing on four main areas: citizenship, participation,
representation, and the public sphere. It traces the evolution from electronic
to virtual and network democracy, underscoring how each stage has broadened
democratic engagement through technology. Focusing on digital citizenship, the
chapter examines how AI can improve online engagement and promote ethical
behaviour while posing privacy risks and fostering identity stereotyping.
Regarding political participation, it highlights AI's dual role in mobilising
civic actions and spreading misinformation. Regarding representation, AI's
involvement in electoral processes can enhance voter registration, e-voting,
and the efficiency of result tabulation but raises concerns regarding privacy
and public trust. Also, AI's predictive capabilities shift the dynamics of
political competition, posing ethical questions about manipulation and the
legitimacy of democracy. Finally, the chapter examines how integrating AI and
digital technologies can facilitate democratic political advocacy and
personalised communication. However, this also comes with higher risks of
misinformation and targeted propaganda.
</summary>
    <author>
      <name>Claudio Novelli</name>
    </author>
    <author>
      <name>Giulia Sandri</name>
    </author>
    <link href="http://arxiv.org/abs/2412.07791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.07791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.12385v1</id>
    <updated>2024-12-16T22:43:41Z</updated>
    <published>2024-12-16T22:43:41Z</published>
    <title>Enhancing Temporal Link Prediction with HierTKG: A Hierarchical Temporal
  Knowledge Graph Framework</title>
    <summary>  The rapid spread of misinformation on social media, especially during crises,
challenges public decision-making. To address this, we propose HierTKG, a
framework combining Temporal Graph Networks (TGN) and hierarchical pooling
(DiffPool) to model rumor dynamics across temporal and structural scales.
HierTKG captures key propagation phases, enabling improved temporal link
prediction and actionable insights for misinformation control. Experiments
demonstrate its effectiveness, achieving an MRR of 0.9845 on ICEWS14 and 0.9312
on WikiData, with competitive performance on noisy datasets like PHEME (MRR:
0.8802). By modeling structured event sequences and dynamic social
interactions, HierTKG adapts to diverse propagation patterns, offering a
scalable and robust solution for real-time analysis and prediction of rumor
spread, aiding proactive intervention strategies.
</summary>
    <author>
      <name>Mariam Almutairi</name>
    </author>
    <author>
      <name>Melike Yildiz Aktas</name>
    </author>
    <author>
      <name>Nawar Wali</name>
    </author>
    <author>
      <name>Shutonu Mitra</name>
    </author>
    <author>
      <name>Dawei Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.12385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.12385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.16269v1</id>
    <updated>2024-12-20T13:54:25Z</updated>
    <published>2024-12-20T13:54:25Z</published>
    <title>(Mis)information diffusion and the financial market</title>
    <summary>  This paper investigates the interplay between information diffusion in social
networks and its impact on financial markets with an Agent-Based Model (ABM).
Agents receive and exchange information about an observable stochastic
component of the dividend process of a risky asset \`a la Grossman and
Stiglitz. A small proportion of the network has access to a private signal
about the component, which can be clean (information) or distorted
(misinformation). Other agents are uninformed and can receive information only
from their peers. All agents are Bayesian, adjusting their beliefs according to
the confidence they have in the source of information. We examine, by means of
simulations, how information diffuses in the network and provide a framework to
account for delayed absorption of shocks, that are not immediately priced as
predicted by classical financial models. We investigate the effect of the
network topology on the resulting asset price and evaluate under which
condition misinformation diffusion can make the market more inefficient.
</summary>
    <author>
      <name>Tommaso Di Francesco</name>
    </author>
    <author>
      <name>Daniel Torren Peraire</name>
    </author>
    <link href="http://arxiv.org/abs/2412.16269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.16269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.16486v1</id>
    <updated>2024-12-21T05:02:26Z</updated>
    <published>2024-12-21T05:02:26Z</published>
    <title>Evaluating the Performance of Large Language Models in Scientific Claim
  Detection and Classification</title>
    <summary>  The pervasive influence of social media during the COVID-19 pandemic has been
a double-edged sword, enhancing communication while simultaneously propagating
misinformation. This \textit{Digital Infodemic} has highlighted the urgent need
for automated tools capable of discerning and disseminating factual content.
This study evaluates the efficacy of Large Language Models (LLMs) as innovative
solutions for mitigating misinformation on platforms like Twitter. LLMs, such
as OpenAI's GPT and Meta's LLaMA, offer a pre-trained, adaptable approach that
bypasses the extensive training and overfitting issues associated with
traditional machine learning models. We assess the performance of LLMs in
detecting and classifying COVID-19-related scientific claims, thus facilitating
informed decision-making. Our findings indicate that LLMs have significant
potential as automated fact-checking tools, though research in this domain is
nascent and further exploration is required. We present a comparative analysis
of LLMs' performance using a specialized dataset and propose a framework for
their application in public health communication.
</summary>
    <author>
      <name>Tanjim Bin Faruk</name>
    </author>
    <link href="http://arxiv.org/abs/2412.16486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.16486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.14728v1</id>
    <updated>2025-01-24T18:59:31Z</updated>
    <published>2025-01-24T18:59:31Z</published>
    <title>Mitigating GenAI-powered Evidence Pollution for Out-of-Context
  Multimodal Misinformation Detection</title>
    <summary>  While large generative artificial intelligence (GenAI) models have achieved
significant success, they also raise growing concerns about online information
security due to their potential misuse for generating deceptive content.
Out-of-context (OOC) multimodal misinformation detection, which often retrieves
Web evidence to identify the repurposing of images in false contexts, faces the
issue of reasoning over GenAI-polluted evidence to derive accurate predictions.
Existing works simulate GenAI-powered pollution at the claim level with
stylistic rewriting to conceal linguistic cues, and ignore evidence-level
pollution for such information-seeking applications. In this work, we
investigate how polluted evidence affects the performance of existing OOC
detectors, revealing a performance degradation of more than 9 percentage
points. We propose two strategies, cross-modal evidence reranking and
cross-modal claim-evidence reasoning, to address the challenges posed by
polluted evidence. Extensive experiments on two benchmark datasets show that
these strategies can effectively enhance the robustness of existing
out-of-context detectors amidst polluted evidence.
</summary>
    <author>
      <name>Zehong Yan</name>
    </author>
    <author>
      <name>Peng Qi</name>
    </author>
    <author>
      <name>Wynne Hsu</name>
    </author>
    <author>
      <name>Mong Li Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.14728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.14728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.00339v1</id>
    <updated>2025-02-01T06:56:17Z</updated>
    <published>2025-02-01T06:56:17Z</published>
    <title>Challenges and Innovations in LLM-Powered Fake News Detection: A
  Synthesis of Approaches and Future Directions</title>
    <summary>  The pervasiveness of the dissemination of fake news through social media
platforms poses critical risks to the trust of the general public, societal
stability, and democratic institutions. This challenge calls for novel
methodologies in detection, which can keep pace with the dynamic and
multi-modal nature of misinformation. Recent works include powering the
detection using large language model advances in multimodal frameworks,
methodologies using graphs, and adversarial training in the literature of fake
news. Based on the different approaches which can bring success, some key
highlights will be underlined: enhanced LLM-improves accuracy through more
advanced semantics and cross-modality fusion for robust detections. The review
further identifies critical gaps in adaptability to dynamic social media
trends, real-time, and cross-platform detection capabilities, as well as the
ethical challenges thrown up by the misuse of LLMs. Future directions underline
the development of style-agnostic models, cross-lingual detection frameworks,
and robust policies with a view to mitigating LLM-driven misinformation. This
synthesis thus lays a concrete foundation for those researchers and
practitioners committed to reinforcing fake news detection systems with
complications that keep on growing in the digital landscape.
</summary>
    <author>
      <name>Jingyuan Yi</name>
    </author>
    <author>
      <name>Zeqiu Xu</name>
    </author>
    <author>
      <name>Tianyi Huang</name>
    </author>
    <author>
      <name>Peiyang Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2502.00339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.00339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.12624v2</id>
    <updated>2025-02-24T17:23:04Z</updated>
    <published>2025-02-18T08:09:53Z</published>
    <title>Implicit Repair with Reinforcement Learning in Emergent Communication</title>
    <summary>  Conversational repair is a mechanism used to detect and resolve
miscommunication and misinformation problems when two or more agents interact.
One particular and underexplored form of repair in emergent communication is
the implicit repair mechanism, where the interlocutor purposely conveys the
desired information in such a way as to prevent misinformation from any other
interlocutor. This work explores how redundancy can modify the emergent
communication protocol to continue conveying the necessary information to
complete the underlying task, even with additional external environmental
pressures such as noise. We focus on extending the signaling game, called the
Lewis Game, by adding noise in the communication channel and inputs received by
the agents. Our analysis shows that agents add redundancy to the transmitted
messages as an outcome to prevent the negative impact of noise on the task
success. Additionally, we observe that the emerging communication protocol's
generalization capabilities remain equivalent to architectures employed in
simpler games that are entirely deterministic. Additionally, our method is the
only one suitable for producing robust communication protocols that can handle
cases with and without noise while maintaining increased generalization
performance levels.
</summary>
    <author>
      <name>F√°bio Vital</name>
    </author>
    <author>
      <name>Alberto Sardinha</name>
    </author>
    <author>
      <name>Francisco S. Melo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAMAS 2025 - full paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.12624v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.12624v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.14132v2</id>
    <updated>2025-05-27T10:54:58Z</updated>
    <published>2025-02-19T22:26:39Z</published>
    <title>Can Community Notes Replace Professional Fact-Checkers?</title>
    <summary>  Two commonly employed strategies to combat the rise of misinformation on
social media are (i) fact-checking by professional organisations and (ii)
community moderation by platform users. Policy changes by Twitter/X and, more
recently, Meta, signal a shift away from partnerships with fact-checking
organisations and towards an increased reliance on crowdsourced community
notes. However, the extent and nature of dependencies between fact-checking and
helpful community notes remain unclear. To address these questions, we use
language models to annotate a large corpus of Twitter/X community notes with
attributes such as topic, cited sources, and whether they refute claims tied to
broader misinformation narratives. Our analysis reveals that community notes
cite fact-checking sources up to five times more than previously reported.
Fact-checking is especially crucial for notes on posts linked to broader
narratives, which are twice as likely to reference fact-checking sources
compared to other sources. Our results show that successful community
moderation relies on professional fact-checking and highlight how citizen and
professional fact-checking are deeply intertwined.
</summary>
    <author>
      <name>Nadav Borenstein</name>
    </author>
    <author>
      <name>Greta Warren</name>
    </author>
    <author>
      <name>Desmond Elliott</name>
    </author>
    <author>
      <name>Isabelle Augenstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the main proceedings of ACL 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.14132v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.14132v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.16612v1</id>
    <updated>2025-02-23T15:35:48Z</updated>
    <published>2025-02-23T15:35:48Z</published>
    <title>MemeIntel: Explainable Detection of Propagandistic and Hateful Memes</title>
    <summary>  The proliferation of multimodal content on social media presents significant
challenges in understanding and moderating complex, context-dependent issues
such as misinformation, hate speech, and propaganda. While efforts have been
made to develop resources and propose new methods for automatic detection,
limited attention has been given to label detection and the generation of
explanation-based rationales for predicted labels. To address this challenge,
we introduce MemeIntel, an explanation-enhanced dataset for propaganda memes in
Arabic and hateful memes in English, making it the first large-scale resource
for these tasks. To solve these tasks, we propose a multi-stage optimization
approach and train Vision-Language Models (VLMs). Our results demonstrate that
this approach significantly improves performance over the base model for both
\textbf{label detection} and explanation generation, outperforming the current
state-of-the-art with an absolute improvement of ~3% on ArMeme and ~7% on
Hateful Memes. For reproducibility and future research, we aim to make the
MemeIntel dataset and experimental resources publicly available.
</summary>
    <author>
      <name>Mohamed Bayan Kmainasi</name>
    </author>
    <author>
      <name>Abul Hasnat</name>
    </author>
    <author>
      <name>Md Arid Hasan</name>
    </author>
    <author>
      <name>Ali Ezzat Shahroor</name>
    </author>
    <author>
      <name>Firoj Alam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">disinformation, misinformation, factuality, harmfulness, fake news,
  propaganda, hateful meme, multimodality, text, images</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.16612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.16612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.00955v2</id>
    <updated>2025-05-12T03:35:52Z</updated>
    <published>2025-03-02T16:22:46Z</published>
    <title>SemViQA: A Semantic Question Answering System for Vietnamese Information
  Fact-Checking</title>
    <summary>  The rise of misinformation, exacerbated by Large Language Models (LLMs) like
GPT and Gemini, demands robust fact-checking solutions, especially for
low-resource languages like Vietnamese. Existing methods struggle with semantic
ambiguity, homonyms, and complex linguistic structures, often trading accuracy
for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking
framework integrating Semantic-based Evidence Retrieval (SER) and Two-step
Verdict Classification (TVC). Our approach balances precision and speed,
achieving state-of-the-art results with 78.97\% strict accuracy on ISE-DSC01
and 80.82\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge.
Additionally, SemViQA Faster improves inference speed 7x while maintaining
competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact
verification, advancing the fight against misinformation. The source code is
available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.
</summary>
    <author>
      <name>Dien X. Tran</name>
    </author>
    <author>
      <name>Nam V. Nguyen</name>
    </author>
    <author>
      <name>Thanh T. Tran</name>
    </author>
    <author>
      <name>Anh T. Hoang</name>
    </author>
    <author>
      <name>Tai V. Duong</name>
    </author>
    <author>
      <name>Di T. Le</name>
    </author>
    <author>
      <name>Phuc-Lu Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.00955v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.00955v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02887v1</id>
    <updated>2025-02-12T05:52:10Z</updated>
    <published>2025-02-12T05:52:10Z</published>
    <title>Dynamics and Inequalities in Digital Social Networks: A Computational
  and Sociological Review</title>
    <summary>  Digital networks have profoundly transformed the ways in which individuals
interact, exchange information, and establish connections, leading to the
emergence of phenomena such as virality, misinformation cascades, and online
polarization. This review conducts a thorough examination of the micro-macro
linkages within digital social networks, analyzing how individual actions like
liking, sharing, and commenting coalesce into broader systemic patterns and how
these interactions are influenced by algorithmic mediation. Utilizing a
multidisciplinary literature base, this study explores the interaction among
user behaviors, network structures, and platform algorithms that intensify
biases, strengthen homophily, and foster echo chambers. We delve into crucial
dynamics including the scalability's impact on weak tie propagation, the
amplification effects on influencers, and the rise of digital inequalities,
employing both theoretical and empirical approaches. By synthesizing insights
from sociology, network theory, and computational social science, this paper
underscores the necessity for novel frameworks that integrate algorithmic
processes into established micro-macro models. The conclusion presents
practical strategies aimed at promoting fairer digital networks through
decentralized architectures, algorithmic fairness, and improved digital
inclusion, tackling significant challenges such as polarization and
misinformation within networked societies.
</summary>
    <author>
      <name>Pengjia Cui</name>
    </author>
    <link href="http://arxiv.org/abs/2503.02887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10445v1</id>
    <updated>2025-03-13T15:10:55Z</updated>
    <published>2025-03-13T15:10:55Z</published>
    <title>More Than Just Warnings:Exploring the Ways of Communicating Credibility
  Assessment on Social Media</title>
    <summary>  Reducing the spread of misinformation is challenging. AI-based fact
verification systems offer a promising solution by addressing the high costs
and slow pace of traditional fact-checking. However, the problem of how to
effectively communicate the results to users remains unsolved. Warning labels
may seem an easy solution, but they fail to account for fuzzy misinformation
that is not entirely fake. Additionally, users' limited attention spans and
social media information should be taken into account while designing the
presentation. The online experiment (n = 537) investigates the impact of
sources and granularity on users' perception of information veracity and the
system's usefulness and trustworthiness. Findings show that fine-grained
indicators enhance nuanced opinions, information awareness, and the intention
to use fact-checking systems. Source differences had minimal impact on opinions
and perceptions, except for informativeness. Qualitative findings suggest the
proposed indicators promote critical thinking. We discuss implications for
designing concise, user-friendly AI fact-checking feedback.
</summary>
    <author>
      <name>Huiyun Tang</name>
    </author>
    <author>
      <name>Bj√∂rn Rohles</name>
    </author>
    <author>
      <name>Yuwei Chuai</name>
    </author>
    <author>
      <name>Gabriele Lenzini</name>
    </author>
    <author>
      <name>Anastasia Sergeeva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.10445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10690v1</id>
    <updated>2025-03-12T01:53:49Z</updated>
    <published>2025-03-12T01:53:49Z</published>
    <title>Battling Misinformation: An Empirical Study on Adversarial Factuality in
  Open-Source Large Language Models</title>
    <summary>  Adversarial factuality refers to the deliberate insertion of misinformation
into input prompts by an adversary, characterized by varying levels of
expressed confidence. In this study, we systematically evaluate the performance
of several open-source large language models (LLMs) when exposed to such
adversarial inputs. Three tiers of adversarial confidence are considered:
strongly confident, moderately confident, and limited confidence. Our analysis
encompasses eight LLMs: LLaMA 3.1 (8B), Phi 3 (3.8B), Qwen 2.5 (7B),
Deepseek-v2 (16B), Gemma2 (9B), Falcon (7B), Mistrallite (7B), and LLaVA (7B).
Empirical results indicate that LLaMA 3.1 (8B) exhibits a robust capability in
detecting adversarial inputs, whereas Falcon (7B) shows comparatively lower
performance. Notably, for the majority of the models, detection success
improves as the adversary's confidence decreases; however, this trend is
reversed for LLaMA 3.1 (8B) and Phi 3 (3.8B), where a reduction in adversarial
confidence corresponds with diminished detection performance. Further analysis
of the queries that elicited the highest and lowest rates of successful attacks
reveals that adversarial attacks are more effective when targeting less
commonly referenced or obscure information.
</summary>
    <author>
      <name>Shahnewaz Karim Sakib</name>
    </author>
    <author>
      <name>Anindya Bijoy Das</name>
    </author>
    <author>
      <name>Shibbir Ahmed</name>
    </author>
    <link href="http://arxiv.org/abs/2503.10690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15205v1</id>
    <updated>2025-03-19T13:47:28Z</updated>
    <published>2025-03-19T13:47:28Z</published>
    <title>A Peek Behind the Curtain: Using Step-Around Prompt Engineering to
  Identify Bias and Misinformation in GenAI Models</title>
    <summary>  This research examines the emerging technique of step-around prompt
engineering in GenAI research, a method that deliberately bypasses AI safety
measures to expose underlying biases and vulnerabilities in GenAI models. We
discuss how Internet-sourced training data introduces unintended biases and
misinformation into AI systems, which can be revealed through the careful
application of step-around techniques.
  Drawing parallels with red teaming in cybersecurity, we argue that
step-around prompting serves a vital role in identifying and addressing
potential vulnerabilities while acknowledging its dual nature as both a
research tool and a potential security threat. Our findings highlight three key
implications: (1) the persistence of Internet-derived biases in AI training
data despite content filtering, (2) the effectiveness of step-around techniques
in exposing these biases when used responsibly, and (3) the need for robust
safeguards against malicious applications of these methods.
  We conclude by proposing an ethical framework for using step-around prompting
in AI research and development, emphasizing the importance of balancing system
improvements with security considerations.
</summary>
    <author>
      <name>Don Hickerson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">British University Vietnam</arxiv:affiliation>
    </author>
    <author>
      <name>Mike Perkins</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">British University Vietnam</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2503.15205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16031v3</id>
    <updated>2025-06-19T06:50:49Z</updated>
    <published>2025-03-20T10:58:02Z</published>
    <title>Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging
  Fabricated Claims with Humorous Content</title>
    <summary>  In the evolving landscape of online discourse, misinformation increasingly
adopts humorous tones to evade detection and gain traction. This work
introduces Deceptive Humor as a novel research direction, emphasizing how false
narratives, when coated in humor, can become more difficult to detect and more
likely to spread. To support research in this space, we present the Deceptive
Humor Dataset (DHD) a collection of humor-infused comments derived from
fabricated claims using the ChatGPT-4o model. Each entry is labeled with a
Satire Level (from 1 for subtle satire to 3 for overt satire) and categorized
into five humor types: Dark Humor, Irony, Social Commentary, Wordplay, and
Absurdity. The dataset spans English, Telugu, Hindi, Kannada, Tamil, and their
code-mixed forms, making it a valuable resource for multilingual analysis. DHD
offers a structured foundation for understanding how humor can serve as a
vehicle for the propagation of misinformation, subtly enhancing its reach and
impact. Strong baselines are established to encourage further research and
model development in this emerging area.
</summary>
    <author>
      <name>Sai Kartheek Reddy Kasu</name>
    </author>
    <author>
      <name>Shankar Biradar</name>
    </author>
    <author>
      <name>Sunil Saumya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, 2 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16031v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16031v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.02900v1</id>
    <updated>2025-04-03T02:10:27Z</updated>
    <published>2025-04-03T02:10:27Z</published>
    <title>Comparative Analysis of Deepfake Detection Models: New Approaches and
  Perspectives</title>
    <summary>  The growing threat posed by deepfake videos, capable of manipulating
realities and disseminating misinformation, drives the urgent need for
effective detection methods. This work investigates and compares different
approaches for identifying deepfakes, focusing on the GenConViT model and its
performance relative to other architectures present in the DeepfakeBenchmark.
To contextualize the research, the social and legal impacts of deepfakes are
addressed, as well as the technical fundamentals of their creation and
detection, including digital image processing, machine learning, and artificial
neural networks, with emphasis on Convolutional Neural Networks (CNNs),
Generative Adversarial Networks (GANs), and Transformers. The performance
evaluation of the models was conducted using relevant metrics and new datasets
established in the literature, such as WildDeep-fake and DeepSpeak, aiming to
identify the most effective tools in the battle against misinformation and
media manipulation. The obtained results indicated that GenConViT, after
fine-tuning, exhibited superior performance in terms of accuracy (93.82%) and
generalization capacity, surpassing other architectures in the
DeepfakeBenchmark on the DeepSpeak dataset. This study contributes to the
advancement of deepfake detection techniques, offering contributions to the
development of more robust and effective solutions against the dissemination of
false information.
</summary>
    <author>
      <name>Matheus Martins Batista</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Bachelor's thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.02900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06517v1</id>
    <updated>2025-04-09T01:33:08Z</updated>
    <published>2025-04-09T01:33:08Z</published>
    <title>Can dialogues with AI systems help humans better discern visual
  misinformation?</title>
    <summary>  The widespread emergence of manipulated news media content poses significant
challenges to online information integrity. This study investigates whether
dialogues with AI about AI-generated images and associated news statements can
increase human discernment abilities and foster short-term learning in
detecting misinformation. We conducted a study with 80 participants who engaged
in structured dialogues with an AI system about news headline-image pairs,
generating 1,310 human-AI dialogue exchanges. Results show that AI interaction
significantly boosts participants' accuracy in identifying real versus fake
news content from approximately 60\% to 90\% (p$&lt;$0.001). However, these
improvements do not persist when participants are presented with new, unseen
image-statement pairs without AI assistance, with accuracy returning to
baseline levels (~60\%, p=0.88). These findings suggest that while AI systems
can effectively change immediate beliefs about specific content through
persuasive dialogue, they may not produce lasting improvements that transfer to
novel examples, highlighting the need for developing more effective
interventions that promote durable learning outcomes.
</summary>
    <author>
      <name>Anku Rani</name>
    </author>
    <author>
      <name>Valdemar Danry</name>
    </author>
    <author>
      <name>Andy Lippman</name>
    </author>
    <author>
      <name>Pattie Maes</name>
    </author>
    <link href="http://arxiv.org/abs/2504.06517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10878v1</id>
    <updated>2025-04-15T05:11:40Z</updated>
    <published>2025-04-15T05:11:40Z</published>
    <title>Large Language Model-Informed Feature Discovery Improves Prediction and
  Interpretation of Credibility Perceptions of Visual Content</title>
    <summary>  In today's visually dominated social media landscape, predicting the
perceived credibility of visual content and understanding what drives human
judgment are crucial for countering misinformation. However, these tasks are
challenging due to the diversity and richness of visual features. We introduce
a Large Language Model (LLM)-informed feature discovery framework that
leverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and
explain its reasoning. We extract and quantify interpretable features using
targeted prompts and integrate them into machine learning models to improve
credibility predictions. We tested this approach on 4,191 visual social media
posts across eight topics in science, health, and politics, using credibility
ratings from 5,355 crowdsourced workers. Our method outperformed zero-shot
GPT-based predictions by 13 percent in R2, and revealed key features like
information concreteness and image format. We discuss the implications for
misinformation mitigation, visual credibility, and the role of LLMs in social
science.
</summary>
    <author>
      <name>Yilang Peng</name>
    </author>
    <author>
      <name>Sijia Qian</name>
    </author>
    <author>
      <name>Yingdan Lu</name>
    </author>
    <author>
      <name>Cuihua Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.9; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.10254v1</id>
    <updated>2025-05-15T13:08:11Z</updated>
    <published>2025-05-15T13:08:11Z</published>
    <title>Community Fact-Checks Do Not Break Follower Loyalty</title>
    <summary>  Major social media platforms increasingly adopt community-based fact-checking
to address misinformation on their platforms. While previous research has
largely focused on its effect on engagement (e.g., reposts, likes), an
understanding of how fact-checking affects a user's follower base is missing.
In this study, we employ quasi-experimental methods to causally assess whether
users lose followers after their posts are corrected via community fact-checks.
Based on time-series data on follower counts for N=3516 community fact-checked
posts from X, we find that community fact-checks do not lead to meaningful
declines in the follower counts of users who post misleading content. This
suggests that followers of spreaders of misleading posts tend to remain loyal
and do not view community fact-checks as a sufficient reason to disengage. Our
findings underscore the need for complementary interventions to more
effectively disincentivize the production of misinformation on social media.
</summary>
    <author>
      <name>Michelle Bobek</name>
    </author>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <link href="http://arxiv.org/abs/2505.10254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.10254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11189v1</id>
    <updated>2025-05-16T12:48:44Z</updated>
    <published>2025-05-16T12:48:44Z</published>
    <title>Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule
  Extraction vs RuleSHAP</title>
    <summary>  Generative AI systems can help spread information but also misinformation and
biases, potentially undermining the UN Sustainable Development Goals (SDGs).
Explainable AI (XAI) aims to reveal the inner workings of AI systems and expose
misbehaviours or biases. However, current XAI tools, built for simpler models,
struggle to handle the non-numerical nature of large language models (LLMs).
This paper examines the effectiveness of global XAI methods, such as
rule-extraction algorithms and SHAP, in detecting bias in LLMs. To do so, we
first show a text-to-ordinal mapping strategy to convert non-numerical
inputs/outputs into numerical features, enabling these tools to identify (some)
misinformation-related biases in LLM-generated content. Then, we inject
non-linear biases of varying complexity (univariate, conjunctive, and
non-convex) into widespread LLMs like ChatGPT and Llama via system
instructions, using global XAI methods to detect them. This way, we found that
RuleFit struggles with conjunctive and non-convex biases, while SHAP can
approximate conjunctive biases but cannot express them as actionable rules.
Hence, we introduce RuleSHAP, a global rule extraction algorithm combining SHAP
and RuleFit to detect more non-univariate biases, improving injected bias
detection over RuleFit by +94% (MRR@1) on average.
</summary>
    <author>
      <name>Francesco Sovrano</name>
    </author>
    <link href="http://arxiv.org/abs/2505.11189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.19355v1</id>
    <updated>2025-05-25T23:03:24Z</updated>
    <published>2025-05-25T23:03:24Z</published>
    <title>Estimating Online Influence Needs Causal Modeling! Counterfactual
  Analysis of Social Media Engagement</title>
    <summary>  Understanding true influence in social media requires distinguishing
correlation from causation--particularly when analyzing misinformation spread.
While existing approaches focus on exposure metrics and network structures,
they often fail to capture the causal mechanisms by which external temporal
signals trigger engagement. We introduce a novel joint treatment-outcome
framework that leverages existing sequential models to simultaneously adapt to
both policy timing and engagement effects. Our approach adapts causal inference
techniques from healthcare to estimate Average Treatment Effects (ATE) within
the sequential nature of social media interactions, tackling challenges from
external confounding signals. Through our experiments on real-world
misinformation and disinformation datasets, we show that our models outperform
existing benchmarks by 15--22% in predicting engagement across diverse
counterfactual scenarios, including exposure adjustment, timing shifts, and
varied intervention durations. Case studies on 492 social media users show our
causal effect measure aligns strongly with the gold standard in influence
estimation, the expert-based empirical influence.
</summary>
    <author>
      <name>Lin Tian</name>
    </author>
    <author>
      <name>Marian-Andrei Rizoiu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.19355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.19355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.00308v1</id>
    <updated>2025-05-30T23:37:10Z</updated>
    <published>2025-05-30T23:37:10Z</published>
    <title>MythTriage: Scalable Detection of Opioid Use Disorder Myths on a
  Video-Sharing Platform</title>
    <summary>  Understanding the prevalence of misinformation in health topics online can
inform public health policies and interventions. However, measuring such
misinformation at scale remains a challenge, particularly for high-stakes but
understudied topics like opioid-use disorder (OUD)--a leading cause of death in
the U.S. We present the first large-scale study of OUD-related myths on
YouTube, a widely-used platform for health information. With clinical experts,
we validate 8 pervasive myths and release an expert-labeled video dataset. To
scale labeling, we introduce MythTriage, an efficient triage pipeline that uses
a lightweight model for routine cases and defers harder ones to a
high-performing, but costlier, large language model (LLM). MythTriage achieves
up to 0.86 macro F1-score while estimated to reduce annotation time and
financial cost by over 76% compared to experts and full LLM labeling. We
analyze 2.9K search results and 343K recommendations, uncovering how myths
persist on YouTube and offering actionable insights for public health and
platform moderation.
</summary>
    <author>
      <name>Hayoung Jung</name>
    </author>
    <author>
      <name>Shravika Mittal</name>
    </author>
    <author>
      <name>Ananya Aatreya</name>
    </author>
    <author>
      <name>Navreet Kaur</name>
    </author>
    <author>
      <name>Munmun De Choudhury</name>
    </author>
    <author>
      <name>Tanushree Mitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 14 figures, 21 tables. In submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.00308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.00308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.15572v1</id>
    <updated>2025-06-18T15:49:22Z</updated>
    <published>2025-06-18T15:49:22Z</published>
    <title>Misinformation by Omission: The Need for More Environmental Transparency
  in AI</title>
    <summary>  In recent years, Artificial Intelligence (AI) models have grown in size and
complexity, driving greater demand for computational power and natural
resources. In parallel to this trend, transparency around the costs and impacts
of these models has decreased, meaning that the users of these technologies
have little to no information about their resource demands and subsequent
impacts on the environment. Despite this dearth of adequate data, escalating
demand for figures quantifying AI's environmental impacts has led to numerous
instances of misinformation evolving from inaccurate or de-contextualized
best-effort estimates of greenhouse gas emissions. In this article, we explore
pervasive myths and misconceptions shaping public understanding of AI's
environmental impacts, tracing their origins and their spread in both the media
and scientific publications. We discuss the importance of data transparency in
clarifying misconceptions and mitigating these harms, and conclude with a set
of recommendations for how AI developers and policymakers can leverage this
information to mitigate negative impacts in the future.
</summary>
    <author>
      <name>Sasha Luccioni</name>
    </author>
    <author>
      <name>Boris Gamazaychikov</name>
    </author>
    <author>
      <name>Theo Alves da Costa</name>
    </author>
    <author>
      <name>Emma Strubell</name>
    </author>
    <link href="http://arxiv.org/abs/2506.15572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.15572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.16202v1</id>
    <updated>2025-06-19T10:32:52Z</updated>
    <published>2025-06-19T10:32:52Z</published>
    <title>AI labeling reduces the perceived accuracy of online content but has
  limited broader effects</title>
    <summary>  Explicit labeling of online content produced by artificial intelligence (AI)
is a widely mooted policy for ensuring transparency and promoting public
confidence. Yet little is known about the scope of AI labeling effects on
public assessments of labeled content. We contribute new evidence on this
question from a survey experiment using a high-quality nationally
representative probability sample (n = 3,861). First, we demonstrate that
explicit AI labeling of a news article about a proposed public policy reduces
its perceived accuracy. Second, we test whether there are spillover effects in
terms of policy interest, policy support, and general concerns about online
misinformation. We find that AI labeling reduces interest in the policy, but
neither influences support for the policy nor triggers general concerns about
online misinformation. We further find that increasing the salience of AI use
reduces the negative impact of AI labeling on perceived accuracy, while
one-sided versus two-sided framing of the policy has no moderating effect.
Overall, our findings suggest that the effects of algorithm aversion induced by
AI labeling of online content are limited in scope.
</summary>
    <author>
      <name>Chuyao Wang</name>
    </author>
    <author>
      <name>Patrick Sturgis</name>
    </author>
    <author>
      <name>Daniel de Kadt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 5 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.16202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.16202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P25, 91C99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.20944v1</id>
    <updated>2025-06-26T02:20:45Z</updated>
    <published>2025-06-26T02:20:45Z</published>
    <title>E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News
  Verification via MLLMs</title>
    <summary>  The rapid spread of misinformation in mobile and wireless networks presents
critical security challenges. This study introduces a training-free,
retrieval-based multimodal fact verification system that leverages pretrained
vision-language models and large language models for credibility assessment. By
dynamically retrieving and cross-referencing trusted data sources, our approach
mitigates vulnerabilities of traditional training-based models, such as
adversarial attacks and data poisoning. Additionally, its lightweight design
enables seamless edge device integration without extensive on-device
processing. Experiments on two fact-checking benchmarks achieve SOTA results,
confirming its effectiveness in misinformation detection and its robustness
against various attack vectors, highlighting its potential to enhance security
in mobile and wireless communication environments.
</summary>
    <author>
      <name>Van-Hoang Phan</name>
    </author>
    <author>
      <name>Long-Khanh Pham</name>
    </author>
    <author>
      <name>Dang Vu</name>
    </author>
    <author>
      <name>Anh-Duy Tran</name>
    </author>
    <author>
      <name>Minh-Son Dao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to AsiaCCS 2025 @ SCID</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.20944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.20944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.09972v1</id>
    <updated>2025-07-14T06:41:55Z</updated>
    <published>2025-07-14T06:41:55Z</published>
    <title>A New Incentive Model For Content Trust</title>
    <summary>  This paper outlines an incentive-driven and decentralized approach to
verifying the veracity of digital content at scale. Widespread misinformation,
an explosion in AI-generated content and reduced reliance on traditional news
sources demands a new approach for content authenticity and truth-seeking that
is fit for a modern, digital world. By using smart contracts and digital
identity to incorporate 'trust' into the reward function for published content,
not just engagement, we believe that it could be possible to foster a
self-propelling paradigm shift to combat misinformation through a
community-based governance model. The approach described in this paper requires
that content creators stake financial collateral on factual claims for an
impartial jury to vet with a financial reward for contribution. We hypothesize
that with the right financial and social incentive model users will be
motivated to participate in crowdsourced fact-checking and content creators
will place more care in their attestations. This is an exploratory paper and
there are a number of open issues and questions that warrant further analysis
and exploration.
</summary>
    <author>
      <name>Lucas Barbosa</name>
    </author>
    <author>
      <name>Sam Kirshner</name>
    </author>
    <author>
      <name>Rob Kopel</name>
    </author>
    <author>
      <name>Eric Tze Kuan Lim</name>
    </author>
    <author>
      <name>Tom Pagram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures and 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.09972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.09972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.07920v3</id>
    <updated>2019-04-17T14:32:32Z</updated>
    <published>2019-01-23T14:42:47Z</published>
    <title>The Junk News Aggregator: Examining junk news posted on Facebook,
  starting with the 2018 US Midterm Elections</title>
    <summary>  In recent years, the phenomenon of online misinformation and junk news
circulating on social media has come to constitute an important and widespread
problem affecting public life online across the globe, particularly around
important political events such as elections. At the same time, there have been
calls for more transparency around misinformation on social media platforms, as
many of the most popular social media platforms function as "walled gardens,"
where it is impossible for researchers and the public to readily examine the
scale and nature of misinformation activity as it unfolds on the platforms. In
order to help address this, we present the Junk News Aggregator, a publicly
available interactive web tool, which allows anyone to examine, in near
real-time, all of the public content posted to Facebook by important junk news
sources in the US. It allows the public to gain access to and examine the
latest articles posted on Facebook (the most popular social media platform in
the US and one where content is not readily accessible at scale from the open
Web), as well as organise them by time, news publisher, and keywords of
interest, and sort them based on all eight engagement metrics available on
Facebook. Therefore, the Aggregator allows the public to gain insights on the
volume, content, key themes, and types and volumes of engagement received by
content posted by junk news publishers, in near real-time, hence opening up and
offering transparency in these activities as they unfold, at scale across the
top most popular junk news publishers. In this way, the Aggregator can help
increase transparency around the nature, volume, and engagement with junk news
on social media, and serve as a media literacy tool for the public.
</summary>
    <author>
      <name>Dimitra Liotsiou</name>
    </author>
    <author>
      <name>Bence Kollanyi</name>
    </author>
    <author>
      <name>Philip N. Howard</name>
    </author>
    <link href="http://arxiv.org/abs/1901.07920v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07920v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.05701v1</id>
    <updated>2020-08-13T05:53:24Z</updated>
    <published>2020-08-13T05:53:24Z</published>
    <title>The COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation
  Objectively?</title>
    <summary>  Misinformation is an ever increasing problem that is difficult to solve for
the research community and has a negative impact on the society at large. Very
recently, the problem has been addressed with a crowdsourcing-based approach to
scale up labeling efforts: to assess the truthfulness of a statement, instead
of relying on a few experts, a crowd of (non-expert) judges is exploited. We
follow the same approach to study whether crowdsourcing is an effective and
reliable method to assess statements truthfulness during a pandemic. We
specifically target statements related to the COVID-19 health emergency, that
is still ongoing at the time of the study and has arguably caused an increase
of the amount of misinformation that is spreading online (a phenomenon for
which the term "infodemic" has been used). By doing so, we are able to address
(mis)information that is both related to a sensitive and personal issue like
health and very recent as compared to when the judgment is done: two issues
that have not been analyzed in related work. In our experiment, crowd workers
are asked to assess the truthfulness of statements, as well as to provide
evidence for the assessments as a URL and a text justification. Besides showing
that the crowd is able to accurately judge the truthfulness of the statements,
we also report results on many different aspects, including: agreement among
workers, the effect of different aggregation functions, of scales
transformations, and of workers background / bias. We also analyze workers
behavior, in terms of queries submitted, URLs found / selected, text
justifications, and other behavioral data like clicks and mouse actions
collected by means of an ad hoc logger.
</summary>
    <author>
      <name>Kevin Roitero</name>
    </author>
    <author>
      <name>Michael Soprano</name>
    </author>
    <author>
      <name>Beatrice Portelli</name>
    </author>
    <author>
      <name>Damiano Spina</name>
    </author>
    <author>
      <name>Vincenzo Della Mea</name>
    </author>
    <author>
      <name>Giuseppe Serra</name>
    </author>
    <author>
      <name>Stefano Mizzaro</name>
    </author>
    <author>
      <name>Gianluca Demartini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3340531.3412048</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3340531.3412048" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; Preprint of the full paper accepted at CIKM 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.05701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.05701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05513v2</id>
    <updated>2020-05-13T16:47:44Z</updated>
    <published>2020-05-12T01:51:07Z</published>
    <title>Psychometric Analysis and Coupling of Emotions Between State Bulletins
  and Twitter in India during COVID-19 Infodemic</title>
    <summary>  COVID-19 infodemic has been spreading faster than the pandemic itself. The
misinformation riding upon the infodemic wave poses a major threat to people's
health and governance systems. Since social media is the largest source of
information, managing the infodemic not only requires mitigating of
misinformation but also an early understanding of psychological patterns
resulting from it. During the COVID-19 crisis, Twitter alone has seen a sharp
45% increase in the usage of its curated events page, and a 30% increase in its
direct messaging usage, since March 6th 2020. In this study, we analyze the
psychometric impact and coupling of the COVID-19 infodemic with the official
bulletins related to COVID-19 at the national and state level in India. We look
at these two sources with a psycho-linguistic lens of emotions and quantified
the extent and coupling between the two. We modified path, a deep skip-gram
based open-sourced lexicon builder for effective capture of health-related
emotions. We were then able to capture the time-evolution of health-related
emotions in social media and official bulletins. An analysis of lead-lag
relationships between the time series of extracted emotions from official
bulletins and social media using Granger's causality showed that state
bulletins were leading the social media for some emotions such as Medical
Emergency. Further insights that are potentially relevant for the policymaker
and the communicators actively engaged in mitigating misinformation are also
discussed. Our paper also introduces CoronaIndiaDataset2, the first social
media based COVID-19 dataset at national and state levels from India with over
5.6 million national and 2.6 million state-level tweets. Finally, we present
our findings as COVibes, an interactive web application capturing psychometric
insights captured upon the CoronaIndiaDataset, both at a national and state
level.
</summary>
    <author>
      <name>Baani Leen Kaur Jolly</name>
    </author>
    <author>
      <name>Palash Aggrawal</name>
    </author>
    <author>
      <name>Amogh Gulati</name>
    </author>
    <author>
      <name>Amarjit Singh Sethi</name>
    </author>
    <author>
      <name>Ponnurangam Kumaraguru</name>
    </author>
    <author>
      <name>Tavpritesh Sethi</name>
    </author>
    <link href="http://arxiv.org/abs/2005.05513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.11824v5</id>
    <updated>2021-05-23T04:56:44Z</updated>
    <published>2021-01-28T05:45:01Z</published>
    <title>Exploring Lightweight Interventions at Posting Time to Reduce the
  Sharing of Misinformation on Social Media</title>
    <summary>  When users on social media share content without considering its veracity,
they may unwittingly be spreading misinformation. In this work, we investigate
the design of lightweight interventions that nudge users to assess the accuracy
of information as they share it. Such assessment may deter users from posting
misinformation in the first place, and their assessments may also provide
useful guidance to friends aiming to assess those posts themselves. In support
of lightweight assessment, we first develop a taxonomy of the reasons why
people believe a news claim is or is not true; this taxonomy yields a checklist
that can be used at posting time. We conduct evaluations to demonstrate that
the checklist is an accurate and comprehensive encapsulation of people's
free-response rationales. In a second experiment, we study the effects of three
behavioral nudges -- 1) checkboxes indicating whether headings are accurate, 2)
tagging reasons (from our taxonomy) that a post is accurate via a checklist and
3) providing free-text rationales for why a headline is or is not accurate --
on people's intention of sharing the headline on social media. From an
experiment with 1668 participants, we find that both providing accuracy
assessment and rationale reduce the sharing of false content. They also reduce
the sharing of true content, but to a lesser degree that yields an overall
decrease in the fraction of shared content that is false. Our findings have
implications for designing social media and news sharing platforms that draw
from richer signals of content credibility contributed by users. In addition,
our validated taxonomy can be used by platforms and researchers as a way to
gather rationales in an easier fashion than free-response.
</summary>
    <author>
      <name>Farnaz Jahanbakhsh</name>
    </author>
    <author>
      <name>Amy X. Zhang</name>
    </author>
    <author>
      <name>Adam J. Berinsky</name>
    </author>
    <author>
      <name>Gordon Pennycook</name>
    </author>
    <author>
      <name>David G. Rand</name>
    </author>
    <author>
      <name>David R. Karger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449092</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449092" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In CSCW'21</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW1, Article 18.
  Publication date: April 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.11824v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11824v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.3; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07726v1</id>
    <updated>2022-01-19T17:05:23Z</updated>
    <published>2022-01-19T17:05:23Z</published>
    <title>"Learn the Facts About COVID-19": Analyzing the Use of Warning Labels on
  TikTok Videos</title>
    <summary>  During the COVID-19 pandemic, health-related misinformation and harmful
content shared online had a significant adverse effect on society. To mitigate
this adverse effect, mainstream social media platforms employed soft moderation
interventions (i.e., warning labels) on potentially harmful posts. Despite the
recent popularity of these moderation interventions, we lack empirical analyses
aiming to uncover how these warning labels are used in the wild, particularly
during challenging times like the COVID-19 pandemic. In this work, we analyze
the use of warning labels on TikTok, focusing on COVID-19 videos. First, we
construct a set of 26 COVID-19 related hashtags, then we collect 41K videos
that include those hashtags in their description. Second, we perform a
quantitative analysis on the entire dataset to understand the use of warning
labels on TikTok. Then, we perform an in-depth qualitative study, using
thematic analysis, on 222 COVID-19 related videos to assess the content and the
connection between the content and the warning labels. Our analysis shows that
TikTok broadly applies warning labels on TikTok videos, likely based on
hashtags included in the description. More worrying is the addition of COVID-19
warning labels on videos where their actual content is not related to COVID-19
(23% of the cases in a sample of 143 English videos that are not related to
COVID-19). Finally, our qualitative analysis on a sample of 222 videos shows
that 7.7% of the videos share misinformation/harmful content and do not include
warning labels, 37.3% share benign information and include warning labels, and
that 35% of the videos that share misinformation/harmful content (and need a
warning label) are made for fun. Our study demonstrates the need to develop
more accurate and precise soft moderation systems, especially on a platform
like TikTok that is extremely popular among people of younger age.
</summary>
    <author>
      <name>Chen Ling</name>
    </author>
    <author>
      <name>Krishna P. Gummadi</name>
    </author>
    <author>
      <name>Savvas Zannettou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages (include reference), 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.07726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.01509v1</id>
    <updated>2022-08-02T14:54:21Z</updated>
    <published>2022-08-02T14:54:21Z</published>
    <title>Characterizing Vaccination Movements on YouTube in the United States and
  Brazil</title>
    <summary>  In the context of COVID-19 pandemic, social networks such as Twitter and
YouTube stand out as important sources of information. YouTube, as the largest
and most engaging online media consumption platform, has a large influence in
the spread of information and misinformation, which makes it important to study
how it deals with the problems that arise from disinformation, as well as how
its users interact with different types of content. Considering that United
States (USA) and Brazil (BR) are two countries with the highest COVID-19 death
tolls, we asked the following question: What are the nuances of vaccination
campaigns in the two countries? With that in mind, we engage in a comparative
analysis of pro and anti-vaccine movements on YouTube. We also investigate the
role of YouTube in countering online vaccine misinformation in USA and BR. For
this means, we monitored the removal of vaccine related content on the platform
and also applied various techniques to analyze the differences in discourse and
engagement in pro and anti-vaccine "comment sections". We found that American
anti-vaccine content tend to lead to considerably more toxic and negative
discussion than their pro-vaccine counterparts while also leading to 18% higher
user-user engagement, while Brazilian anti-vaccine content was significantly
less engaging. We also found that pro-vaccine and anti-vaccine discourses are
considerably different as the former is associated with conspiracy theories
(e.g. ccp), misinformation and alternative medicine (e.g. hydroxychloroquine),
while the latter is associated with protective measures. Finally, it was
observed that YouTube content removals are still insufficient, with only
approximately 16% of the anti-vaccine content being removed by the end of the
studied period, with the USA registering the highest percentage of removed
anti-vaccine content(34%) and BR registering the lowest(9.8%).
</summary>
    <author>
      <name>Marcelo Sartori Locatelli</name>
    </author>
    <author>
      <name>Josemar Caetano</name>
    </author>
    <author>
      <name>Wagner Meira Jr.</name>
    </author>
    <author>
      <name>Virgilio Almeida</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3511095.3531283</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3511095.3531283" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM HT 2022, 15 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 33rd ACM Conference on Hypertext and Social
  Media. 2022. p. 80-90</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.01509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.01509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15407v1</id>
    <updated>2022-10-17T16:22:18Z</updated>
    <published>2022-10-17T16:22:18Z</published>
    <title>Fine-tuned Sentiment Analysis of COVID-19 Vaccine-Related Social Media
  Data: Comparative Study</title>
    <summary>  This study investigated and compared public sentiment related to COVID-19
vaccines expressed on two popular social media platforms, Reddit and Twitter,
harvested from January 1, 2020, to March 1, 2022. To accomplish this task, we
created a fine-tuned DistilRoBERTa model to predict sentiments of approximately
9.5 million Tweets and 70 thousand Reddit comments. To fine-tune our model, our
team manually labeled the sentiment of 3600 Tweets and then augmented our
dataset by the method of back-translation. Text sentiment for each social media
platform was then classified with our fine-tuned model using Python and the
Huggingface sentiment analysis pipeline. Our results determined that the
average sentiment expressed on Twitter was more negative (52% positive) than
positive and the sentiment expressed on Reddit was more positive than negative
(53% positive). Though average sentiment was found to vary between these social
media platforms, both displayed similar behavior related to sentiment shared at
key vaccine-related developments during the pandemic. Considering this similar
trend in shared sentiment demonstrated across social media platforms, Twitter
and Reddit continue to be valuable data sources that public health officials
can utilize to strengthen vaccine confidence and combat misinformation. As the
spread of misinformation poses a range of psychological and psychosocial risks
(anxiety, fear, etc.), there is an urgency in understanding the public
perspective and attitude toward shared falsities. Comprehensive educational
delivery systems tailored to the population's expressed sentiments that
facilitate digital literacy, health information-seeking behavior, and precision
health promotion could aid in clarifying such misinformation.
</summary>
    <author>
      <name>Chad A Melton</name>
    </author>
    <author>
      <name>Brianna M White</name>
    </author>
    <author>
      <name>Robert L Davis</name>
    </author>
    <author>
      <name>Robert A Bednarczyk</name>
    </author>
    <author>
      <name>Arash Shaban-Nejad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2196/40408</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2196/40408" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages, 5 Figures, and 1 Table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Medical Internet Research (JMIR) 2022;24(10):e40408</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2211.15407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92-11" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.04910v2</id>
    <updated>2025-03-28T18:34:35Z</updated>
    <published>2023-07-10T21:28:26Z</published>
    <title>Medical Misinformation in AI-Assisted Self-Diagnosis: Development of a
  Method (EvalPrompt) for Analyzing Large Language Models</title>
    <summary>  Rapid integration of large language models (LLMs) in health care is sparking
global discussion about their potential to revolutionize health care quality
and accessibility. At a time when improving health care quality and access
remains a critical concern for countries worldwide, the ability of these models
to pass medical examinations is often cited as a reason to use them for medical
training and diagnosis. However, the impact of their inevitable use as a
self-diagnostic tool and their role in spreading healthcare misinformation has
not been evaluated. This study aims to assess the effectiveness of LLMs,
particularly ChatGPT, from the perspective of an individual self-diagnosing to
better understand the clarity, correctness, and robustness of the models. We
propose the comprehensive testing methodology evaluation of LLM prompts
(EvalPrompt). This evaluation methodology uses multiple-choice medical
licensing examination questions to evaluate LLM responses. We use open-ended
questions to mimic real-world self-diagnosis use cases, and perform sentence
dropout to mimic realistic self-diagnosis with missing information. Human
evaluators then assess the responses returned by ChatGPT for both experiments
for clarity, correctness, and robustness. The results highlight the modest
capabilities of LLMs, as their responses are often unclear and inaccurate. As a
result, medical advice by LLMs should be cautiously approached. However,
evidence suggests that LLMs are steadily improving and could potentially play a
role in healthcare systems in the future. To address the issue of medical
misinformation, there is a pressing need for the development of a comprehensive
self-diagnosis dataset. This dataset could enhance the reliability of LLMs in
medical applications by featuring more realistic prompt styles with minimal
information across a broader range of medical fields.
</summary>
    <author>
      <name>Troy Zada</name>
    </author>
    <author>
      <name>Natalie Tam</name>
    </author>
    <author>
      <name>Francois Barnard</name>
    </author>
    <author>
      <name>Marlize Van Sittert</name>
    </author>
    <author>
      <name>Venkat Bhat</name>
    </author>
    <author>
      <name>Sirisha Rambhatla</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2196/66207</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2196/66207" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, Journal of Medical Internet Research: Formative
  Research</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JMIR Form Res 2025;9:e66207</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2307.04910v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.04910v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0603218v1</id>
    <updated>2006-03-26T08:07:32Z</updated>
    <published>2006-03-26T08:07:32Z</published>
    <title>Self-Assembly of Information in Networks</title>
    <summary>  We model self-assembly of information in networks to investigate necessary
conditions for building a global perception of a system by local communication.
Our approach is to let agents chat in a model system to self-organize distant
communication-pathways. We demonstrate that simple local rules allow agents to
build a perception of the system, that is robust to dynamical changes and
mistakes. We find that messages are most effectively forwarded in the presence
of hubs, while transmission in hub-free networks is more robust against
misinformation and failures.
</summary>
    <author>
      <name>M. Rosvall</name>
    </author>
    <author>
      <name>K. Sneppen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/epl/i2006-10064-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/epl/i2006-10064-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages and 4 figures, Java simulation available at
  http://cmol.nbi.dk/models/infoflow/infoflow.html</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Europhys. Lett., 74, 1109-1115 (2006).</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/physics/0603218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0603218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00715v1</id>
    <updated>2017-10-31T18:52:28Z</updated>
    <published>2017-10-31T18:52:28Z</published>
    <title>Related Fact Checks: a tool for combating fake news</title>
    <summary>  The emergence of "Fake News" and misinformation via online news and social
media has spurred an interest in computational tools to combat this phenomenon.
In this paper we present a new "Related Fact Checks" service, which can help a
reader critically evaluate an article and make a judgment on its veracity by
bringing up fact checks that are relevant to the article. We describe the core
technical problems that need to be solved in building a "Related Fact Checks"
service, and present results from an evaluation of an implementation.
</summary>
    <author>
      <name>Sreya Guha</name>
    </author>
    <link href="http://arxiv.org/abs/1711.00715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01780v1</id>
    <updated>2019-03-05T12:10:11Z</updated>
    <published>2019-03-05T12:10:11Z</published>
    <title>Trust and Trustworthiness in Social Recommender Systems</title>
    <summary>  The prevalence of misinformation on online social media has tangible
empirical connections to increasing political polarization and partisan
antipathy in the United States. Ranking algorithms for social recommendation
often encode broad assumptions about network structure (like homophily) and
group cognition (like, social action is largely imitative). Assumptions like
these can be na\"ive and exclusionary in the era of fake news and ideological
uniformity towards the political poles. We examine these assumptions with aid
from the user-centric framework of trustworthiness in social recommendation.
The constituent dimensions of trustworthiness (diversity, transparency,
explainability, disruption) highlight new opportunities for discouraging
dogmatization and building decision-aware, transparent news recommender
systems.
</summary>
    <author>
      <name>Taha Hassan</name>
    </author>
    <author>
      <name>D. Scott McCrickard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WWW '19 FATES</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.01780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.05838v1</id>
    <updated>2019-09-12T17:44:15Z</updated>
    <published>2019-09-12T17:44:15Z</published>
    <title>Multilingual Multimodal Digital Deception Detection and Disinformation
  Spread across Social Platforms</title>
    <summary>  Our main contribution in this work is novel results of multilingual models
that go beyond typical applications of rumor or misinformation detection in
English social news content to identify fine-grained classes of digital
deception across multiple languages (e.g. Russian, Spanish, etc.). In addition,
we present models for multimodal deception detection from images and text and
discuss the limitations of image only and text only models. Finally, we
elaborate on the ongoing work on measuring deceptive content (in particular
disinformation) spread across social platforms.
</summary>
    <author>
      <name>Maria Glenski</name>
    </author>
    <author>
      <name>Ellyn Ayton</name>
    </author>
    <author>
      <name>Josh Mendoza</name>
    </author>
    <author>
      <name>Svitlana Volkova</name>
    </author>
    <link href="http://arxiv.org/abs/1909.05838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.05838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07989v1</id>
    <updated>2020-12-14T22:40:49Z</updated>
    <published>2020-12-14T22:40:49Z</published>
    <title>The Emerging Threats of Deepfake Attacks and Countermeasures</title>
    <summary>  Deepfake technology (DT) has taken a new level of sophistication.
Cybercriminals now can manipulate sounds, images, and videos to defraud and
misinform individuals and businesses. This represents a growing threat to
international institutions and individuals which needs to be addressed. This
paper provides an overview of deepfakes, their benefits to society, and how DT
works. Highlights the threats that are presented by deepfakes to businesses,
politics, and judicial systems worldwide. Additionally, the paper will explore
potential solutions to deepfakes and conclude with future research direction.
</summary>
    <author>
      <name>Shadrack Awah Buo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.23089.81762</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.23089.81762" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.07989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03508v1</id>
    <updated>2018-03-31T09:48:20Z</updated>
    <published>2018-03-31T09:48:20Z</published>
    <title>Seeing Through Misinformation: A Framework for Identifying Fake Online
  News</title>
    <summary>  The fake news epidemic makes it imperative to develop a diagnostic framework
that is both parsimonious and valid to guide present and future efforts in fake
news detection. This paper represents one of the very first attempts to fill a
void in the research on this topic. The LeSiE (Lexical Structure, Simplicity,
Emotion) framework we created and validated allows lay people to identify
potential fake news without the use of calculators or complex statistics by
looking out for three simple cues.
</summary>
    <author>
      <name>Murphy Choy</name>
    </author>
    <author>
      <name>Mark Chong</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06196v1</id>
    <updated>2018-04-17T12:25:39Z</updated>
    <published>2018-04-17T12:25:39Z</published>
    <title>Demystifying Deception Technology:A Survey</title>
    <summary>  Deception boosts security for systems and components by denial, deceit,
misinformation, camouflage and obfuscation. In this work an extensive overview
of the deception technology environment is presented. Taxonomies, theoretical
backgrounds, psychological aspects as well as concepts, implementations, legal
aspects and ethics are discussed and compared.
</summary>
    <author>
      <name>Daniel Fraunholz</name>
    </author>
    <author>
      <name>Simon Duque Anton</name>
    </author>
    <author>
      <name>Christoph Lipps</name>
    </author>
    <author>
      <name>Daniel Reti</name>
    </author>
    <author>
      <name>Daniel Krohmer</name>
    </author>
    <author>
      <name>Frederic Pohl</name>
    </author>
    <author>
      <name>Matthias Tammen</name>
    </author>
    <author>
      <name>Hans Dieter Schotten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 169 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12918v1</id>
    <updated>2021-04-27T00:21:55Z</updated>
    <published>2021-04-27T00:21:55Z</published>
    <title>Extractive and Abstractive Explanations for Fact-Checking and Evaluation
  of News</title>
    <summary>  In this paper, we explore the construction of natural language explanations
for news claims, with the goal of assisting fact-checking and news evaluation
applications. We experiment with two methods: (1) an extractive method based on
Biased TextRank -- a resource-effective unsupervised graph-based algorithm for
content extraction; and (2) an abstractive method based on the GPT-2 language
model. We perform comparative evaluations on two misinformation datasets in the
political and health news domains, and find that the extractive method shows
the most promise.
</summary>
    <author>
      <name>Ashkan Kazemi</name>
    </author>
    <author>
      <name>Zehua Li</name>
    </author>
    <author>
      <name>Ver√≥nica P√©rez-Rosas</name>
    </author>
    <author>
      <name>Rada Mihalcea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NLP for Internet Freedom Workshop at NAACL 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.12918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.01546v1</id>
    <updated>2019-04-02T17:01:25Z</updated>
    <published>2019-04-02T17:01:25Z</published>
    <title>NELA-GT-2018: A Large Multi-Labelled News Dataset for The Study of
  Misinformation in News Articles</title>
    <summary>  In this paper, we present a dataset of 713k articles collected between
02/2018-11/2018. These articles are collected directly from 194 news and media
outlets including mainstream, hyper-partisan, and conspiracy sources. We
incorporate ground truth ratings of the sources from 8 different assessment
sites covering multiple dimensions of veracity, including reliability, bias,
transparency, adherence to journalistic standards, and consumer trust. The
NELA-GT-2018 dataset can be found at https://doi.org/10.7910/DVN/ULHLCB.
</summary>
    <author>
      <name>Jeppe Norregaard</name>
    </author>
    <author>
      <name>Benjamin D. Horne</name>
    </author>
    <author>
      <name>Sibel Adali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ICWSM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.01546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.01546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06634v1</id>
    <updated>2020-03-14T14:02:27Z</updated>
    <published>2020-03-14T14:02:27Z</published>
    <title>Text Similarity Using Word Embeddings to Classify Misinformation</title>
    <summary>  Fake news is a growing problem in the last years, especially during
elections. It's hard work to identify what is true and what is false among all
the user generated content that circulates every day. Technology can help with
that work and optimize the fact-checking process. In this work, we address the
challenge of finding similar content in order to be able to suggest to a
fact-checker articles that could have been verified before and thus avoid that
the same information is verified more than once. This is especially important
in collaborative approaches to fact-checking where members of large teams will
not know what content others have already fact-checked.
</summary>
    <author>
      <name>Caio Almeida</name>
    </author>
    <author>
      <name>D√©bora Santos</name>
    </author>
    <link href="http://arxiv.org/abs/2003.06634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.03611v1</id>
    <updated>2020-07-06T03:18:54Z</updated>
    <published>2020-07-06T03:18:54Z</published>
    <title>P-Values in a Post-Truth World</title>
    <summary>  The role of statisticians in society is to provide tools, techniques, and
guidance with regards to how much to trust data. This role is increasingly more
important with more data and more misinformation than ever before. The American
Statistical Association recently released two statements on p-values, and
provided four guiding principles. We evaluate their claims using these
principles and find that they failed to adhere to them. In this age of
distrust, we have an opportunity to be role models of trustworthiness, and
responsibility to take it.
</summary>
    <author>
      <name>Joshua T. Vogelstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.03611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.07198v1</id>
    <updated>2021-02-14T17:09:08Z</updated>
    <published>2021-02-14T17:09:08Z</published>
    <title>How Misuse of Statistics Can Spread Misinformation: A Study of
  Misrepresentation of COVID-19 Data</title>
    <summary>  This paper investigates various ways in which a pandemic such as the novel
coronavirus, could be predicted using different mathematical models. It also
studies the various ways in which these models could be depicted using various
visualization techniques. This paper aims to present various statistical
techniques suggested by the Centres for Disease Control and Prevention in order
to represent the epidemiological data. The main focus of this paper is to
analyse how epidemiological data or contagious diseases are theorized using any
available information and later may be presented wrongly by not following the
guidelines, leading to inaccurate representation and interpretations of the
current scenario of the pandemic; with a special reference to the Indian
Subcontinent.
</summary>
    <author>
      <name>Shailesh Bharati</name>
    </author>
    <author>
      <name>Rahul Batra</name>
    </author>
    <link href="http://arxiv.org/abs/2102.07198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.07198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11433v4</id>
    <updated>2025-02-28T08:04:17Z</updated>
    <published>2021-06-21T22:24:23Z</published>
    <title>Retractions: Updating from Complex Information</title>
    <summary>  We modify a canonical experimental design to identify the effectiveness of
retractions. Comparing beliefs after retractions to beliefs (a) without the
retracted information and (b) after equivalent new information, we find that
retractions result in diminished belief updating in both cases. We propose this
reflects updating from retractions being more complex, and our analysis
supports this: we find longer response times, lower accuracy, and higher
variability. The results -- robust across diverse subject groups and design
variations -- enhance our understanding of belief updating and offer insights
into addressing misinformation.
</summary>
    <author>
      <name>Duarte Gon√ßalves</name>
    </author>
    <author>
      <name>Jonathan Libgober</name>
    </author>
    <author>
      <name>Jack Willis</name>
    </author>
    <link href="http://arxiv.org/abs/2106.11433v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11433v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08684v3</id>
    <updated>2023-04-14T09:53:01Z</updated>
    <published>2022-01-21T13:19:30Z</published>
    <title>VisQualdex -- the comprehensive guide to good data visualization</title>
    <summary>  The rapid influx of low-quality data visualisations is one of the main
challenges in today's communication. Misleading, unreadable, or confusing
visualisations spread misinformation, failing to fulfill their purpose. The
lack of proper tooling further heightens the problem of the quality assessment
process. Therefore, we propose VisQualdex, a systematic set of guidelines
isnpired by the Grammar of Graphics for evaluating the quality of data
visualisations. To increase the practical impact of VisQualdex, we make these
guidelines available in the form of the web server, visqual.info.
</summary>
    <author>
      <name>Jan Sawicki</name>
    </author>
    <author>
      <name>Micha≈Ç Burdukiewicz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.26583/sv.15.1.11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.26583/sv.15.1.11" rel="related"/>
    <link href="http://arxiv.org/abs/2201.08684v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08684v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02955v1</id>
    <updated>2022-03-06T12:49:40Z</updated>
    <published>2022-03-06T12:49:40Z</published>
    <title>Twitter Dataset for 2022 Russo-Ukrainian Crisis</title>
    <summary>  Online Social Networks (OSNs) play a significant role in information sharing
during a crisis. The data collected during such a crisis can reflect the large
scale public opinions and sentiment. In addition, OSN data can also be used to
study different campaigns that are employed by various entities to engineer
public opinions. Such information sharing campaigns can range from spreading
factual information to propaganda and misinformation. We provide a Twitter
dataset of the 2022 Russo-Ukrainian conflict. In the first release, we share
over 1.6 million tweets shared during the 1st week of the crisis.
</summary>
    <author>
      <name>Ehsan-Ul Haq</name>
    </author>
    <author>
      <name>Gareth Tyson</name>
    </author>
    <author>
      <name>Lik-Hang Lee</name>
    </author>
    <author>
      <name>Tristan Braud</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <link href="http://arxiv.org/abs/2203.02955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.05659v2</id>
    <updated>2023-03-17T22:21:50Z</updated>
    <published>2022-03-10T21:58:33Z</published>
    <title>NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of
  Misinformation in News Articles</title>
    <summary>  In this paper, we present the fifth installment of the NELA-GT datasets,
NELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between
January 1st, 2022 and December 31st, 2022. Just as in past releases of the
dataset, NELA-GT-2022 includes outlet-level veracity labels from Media
Bias/Fact Check and tweets embedded in collected news articles. The
NELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H
</summary>
    <author>
      <name>Maur√≠cio Gruppi</name>
    </author>
    <author>
      <name>Benjamin D. Horne</name>
    </author>
    <author>
      <name>Sibel Adalƒ±</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report documenting the NELA-GT recent update
  (NELA-GT-2022). arXiv admin note: substantial text overlap with
  arXiv:2102.04567</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.05659v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.05659v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.05967v1</id>
    <updated>2022-03-09T19:06:04Z</updated>
    <published>2022-03-09T19:06:04Z</published>
    <title>A Weibo Dataset for the 2022 Russo-Ukrainian Crisis</title>
    <summary>  Online social networks such as Twitter and Weibo play an important role in
how people stay informed and exchange reactions. Each crisis encompasses a new
opportunity to study the portability of models for various tasks (e.g.,
information extraction, complex event understanding, misinformation detection,
etc.), due to differences in domain, entities, and event types. We present the
Russia-Ukraine Crisis Weibo (RUW) dataset, with over 3.5M user posts and
comments in the first release. Our data is available at
https://github.com/yrf1/RussiaUkraine_weibo_dataset.
</summary>
    <author>
      <name>Yi R. Fung</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Russia-Ukraine Crisis, Weibo Dataset</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.05967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.05967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.07990v1</id>
    <updated>2022-01-28T18:13:03Z</updated>
    <published>2022-01-28T18:13:03Z</published>
    <title>UofA-Truth at Factify 2022 : Transformer And Transfer Learning Based
  Multi-Modal Fact-Checking</title>
    <summary>  Identifying fake news is a very difficult task, especially when considering
the multiple modes of conveying information through text, image, video and/or
audio. We attempted to tackle the problem of automated
misinformation/disinformation detection in multi-modal news sources (including
text and images) through our simple, yet effective, approach in the FACTIFY
shared task at De-Factify@AAAI2022. Our model produced an F1-weighted score of
74.807%, which was the fourth best out of all the submissions. In this paper we
will explain our approach to undertake the shared task.
</summary>
    <author>
      <name>Abhishek Dhankar</name>
    </author>
    <author>
      <name>Osmar R. Za√Øane</name>
    </author>
    <author>
      <name>Francois Bolduc</name>
    </author>
    <link href="http://arxiv.org/abs/2203.07990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.07990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03810v1</id>
    <updated>2022-12-07T17:46:22Z</updated>
    <published>2022-12-07T17:46:22Z</published>
    <title>The Social Emotional Web</title>
    <summary>  The social web has linked people on a global scale, transforming how we
communicate and interact. The massive interconnectedness has created new
vulnerabilities in the form of social manipulation and misinformation. As the
social web matures, we are entering a new phase, where people share their
private feelings and emotions. This so-called social emotional web creates new
opportunities for human flourishing, but also exposes new vulnerabilities. To
reap the benefits of the social emotional web, and reduce potential harms, we
must anticipate how it will evolve and create policies that minimize risks.
</summary>
    <author>
      <name>Kristina Lerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 8th IEEE International Conference on Collaboration and Internet
  Computing (IEEE CIC 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.03810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.03475v1</id>
    <updated>2023-02-07T14:00:40Z</updated>
    <published>2023-02-07T14:00:40Z</published>
    <title>Entity-Aware Dual Co-Attention Network for Fake News Detection</title>
    <summary>  Fake news and misinformation spread rapidly on the Internet. How to identify
it and how to interpret the identification results have become important
issues. In this paper, we propose a Dual Co-Attention Network (Dual-CAN) for
fake news detection, which takes news content, social media replies, and
external knowledge into consideration. Our experimental results support that
the proposed Dual-CAN outperforms current representative models in two
benchmark datasets. We further make in-depth discussions by comparing how
models work in both datasets with empirical analysis of attention weights.
</summary>
    <author>
      <name>Sin-Han Yang</name>
    </author>
    <author>
      <name>Chung-Chi Chen</name>
    </author>
    <author>
      <name>Hen-Hsen Huang</name>
    </author>
    <author>
      <name>Hsin-Hsi Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EACL 2023 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.03475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.03475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09866v1</id>
    <updated>2023-04-18T17:24:14Z</updated>
    <published>2023-04-18T17:24:14Z</published>
    <title>Towards Designing a ChatGPT Conversational Companion for Elderly People</title>
    <summary>  Loneliness and social isolation are serious and widespread problems among
older people, affecting their physical and mental health, quality of life, and
longevity. In this paper, we propose a ChatGPT-based conversational companion
system for elderly people. The system is designed to provide companionship and
help reduce feelings of loneliness and social isolation. The system was
evaluated with a preliminary study. The results showed that the system was able
to generate responses that were relevant to the created elderly personas.
However, it is essential to acknowledge the limitations of ChatGPT, such as
potential biases and misinformation, and to consider the ethical implications
of using AI-based companionship for the elderly, including privacy concerns.
</summary>
    <author>
      <name>Abeer Alessa</name>
    </author>
    <author>
      <name>Hend Al-Khalifa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 Figures, Workshop paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.09866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.01241v1</id>
    <updated>2023-06-02T02:33:12Z</updated>
    <published>2023-06-02T02:33:12Z</published>
    <title>Committee Moderation on Encrypted Messaging Platforms</title>
    <summary>  Encrypted messaging services like WhatsApp, Facebook Messenger, and Signal
provide secure and deniable communication for billions across the world, but
these exact properties prevent holding users accountable for sending messages
that are abusive, misinformative, or otherwise harmful to society. Previous
works have addressed this concern by allowing a moderator to verify the
identity of a message's sender if a message is reported; if not reported,
messages maintain all security guarantees. Using primitives from threshold
cryptography, this work extends the message-reporting protocol Hecate from
Issa, Alhaddad, and Varia to a setting in which consensus among a group of
moderators is required to reveal and verify the identity of a message's sender.
</summary>
    <author>
      <name>Alistair Pattison</name>
    </author>
    <author>
      <name>Nicholas Hopper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 44th IEEE Symposium on Security and Privacy</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.01241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.01241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.08236v1</id>
    <updated>2023-06-14T04:19:06Z</updated>
    <published>2023-06-14T04:19:06Z</published>
    <title>Extracting Information from Twitter Screenshots</title>
    <summary>  Screenshots are prevalent on social media as a common approach for
information sharing. Users rarely verify before sharing a screenshot whether
the post it contains is fake or real. Information sharing through fake
screenshots can be highly responsible for misinformation and disinformation
spread on social media. Our ultimate goal is to develop a tool that could take
a screenshot of a tweet and provide a probability that the tweet is real, using
resources found on the live web and in web archives. This paper provides
methods for extracting the tweet text, timestamp, and Twitter handle from a
screenshot of a tweet.
</summary>
    <author>
      <name>Tarannum Zaki</name>
    </author>
    <author>
      <name>Michael L. Nelson</name>
    </author>
    <author>
      <name>Michele C. Weigle</name>
    </author>
    <link href="http://arxiv.org/abs/2306.08236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.08236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.15716v1</id>
    <updated>2023-11-27T10:59:16Z</updated>
    <published>2023-11-27T10:59:16Z</published>
    <title>Justifiable Artificial Intelligence: Engineering Large Language Models
  for Legal Applications</title>
    <summary>  In this work, I discuss how Large Language Models can be applied in the legal
domain, circumventing their current drawbacks. Despite their large success and
acceptance, their lack of explainability hinders legal experts to trust in
their output, and this happens rightfully so. However, in this paper, I argue
in favor of a new view, Justifiable Artificial Intelligence, instead of
focusing on Explainable Artificial Intelligence. I discuss in this paper how
gaining evidence for and against a Large Language Model's output may make their
generated texts more trustworthy - or hold them accountable for misinformation.
</summary>
    <author>
      <name>Sabine Wehnert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.15716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.15716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.2; H.3.3; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.18297v1</id>
    <updated>2023-11-30T07:03:36Z</updated>
    <published>2023-11-30T07:03:36Z</published>
    <title>TrustMark: Universal Watermarking for Arbitrary Resolution Images</title>
    <summary>  Imperceptible digital watermarking is important in copyright protection,
misinformation prevention, and responsible generative AI. We propose TrustMark
- a GAN-based watermarking method with novel design in architecture and
spatio-spectra losses to balance the trade-off between watermarked image
quality with the watermark recovery accuracy. Our model is trained with
robustness in mind, withstanding various in- and out-place perturbations on the
encoded image. Additionally, we introduce TrustMark-RM - a watermark remover
method useful for re-watermarking. Our methods achieve state-of-art performance
on 3 benchmarks comprising arbitrary resolution images.
</summary>
    <author>
      <name>Tu Bui</name>
    </author>
    <author>
      <name>Shruti Agarwal</name>
    </author>
    <author>
      <name>John Collomosse</name>
    </author>
    <link href="http://arxiv.org/abs/2311.18297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.18297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.04373v1</id>
    <updated>2024-02-06T20:18:32Z</updated>
    <published>2024-02-06T20:18:32Z</published>
    <title>The World of Generative AI: Deepfakes and Large Language Models</title>
    <summary>  We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes
and Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in
particular, pose an alarming threat to society as they are capable of spreading
misinformation and changing the truth. LLMs are powerful language models that
generate general-purpose language. However due to its generative aspect, it can
also be a risk for people if used with ill intentions. The ethical use of these
technologies is a big concern. This short article tries to find out the
interrelationship between them.
</summary>
    <author>
      <name>Alakananda Mitra</name>
    </author>
    <author>
      <name>Saraju P. Mohanty</name>
    </author>
    <author>
      <name>Elias Kougianos</name>
    </author>
    <link href="http://arxiv.org/abs/2402.04373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.04373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.05904v1</id>
    <updated>2024-02-08T18:43:05Z</updated>
    <published>2024-02-08T18:43:05Z</published>
    <title>FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs</title>
    <summary>  Our society is facing rampant misinformation harming public health and trust.
To address the societal challenge, we introduce FACT-GPT, a system leveraging
Large Language Models (LLMs) to automate the claim matching stage of
fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social
media content that aligns with, contradicts, or is irrelevant to previously
debunked claims. Our evaluation shows that our specialized LLMs can match the
accuracy of larger models in identifying related claims, closely mirroring
human judgment. This research provides an automated solution for efficient
claim matching, demonstrates the potential of LLMs in supporting fact-checkers,
and offers valuable resources for further research in the field.
</summary>
    <author>
      <name>Eun Cheol Choi</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <link href="http://arxiv.org/abs/2402.05904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.05904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.14825v1</id>
    <updated>2024-02-08T11:04:34Z</updated>
    <published>2024-02-08T11:04:34Z</published>
    <title>Deepfake Detection and the Impact of Limited Computing Capabilities</title>
    <summary>  The rapid development of technologies and artificial intelligence makes
deepfakes an increasingly sophisticated and challenging-to-identify technique.
To ensure the accuracy of information and control misinformation and mass
manipulation, it is of paramount importance to discover and develop artificial
intelligence models that enable the generic detection of forged videos. This
work aims to address the detection of deepfakes across various existing
datasets in a scenario with limited computing resources. The goal is to analyze
the applicability of different deep learning techniques under these
restrictions and explore possible approaches to enhance their efficiency.
</summary>
    <author>
      <name>Paloma Cantero-Arjona</name>
    </author>
    <author>
      <name>Alfonso S√°nchez-Maci√°n</name>
    </author>
    <link href="http://arxiv.org/abs/2402.14825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.14825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.18621v1</id>
    <updated>2024-02-28T18:59:13Z</updated>
    <published>2024-02-28T18:59:13Z</published>
    <title>Unveiling News Publishers Trustworthiness Through Social Interactions</title>
    <summary>  With the primary goal of raising readers' awareness of misinformation
phenomena, extensive efforts have been made by both academic institutions and
independent organizations to develop methodologies for assessing the
trustworthiness of online news publishers. Unfortunately, existing approaches
are costly and face critical scalability challenges. This study presents a
novel framework for assessing the trustworthiness of online news publishers
using user interactions on social media platforms. The proposed methodology
provides a versatile solution that serves the dual purpose of i) identifying
verifiable online publishers and ii) automatically performing an initial
estimation of the trustworthiness of previously unclassified online news
outlets.
</summary>
    <author>
      <name>Manuel Pratelli</name>
    </author>
    <author>
      <name>Fabio Saracco</name>
    </author>
    <author>
      <name>Marinella Petrocchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A pre-final version of the paper accepted at WebSci'24</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.18621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.18621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.07896v1</id>
    <updated>2024-04-11T16:31:35Z</updated>
    <published>2024-04-11T16:31:35Z</published>
    <title>Auditing health-related recommendations in social media: A Case Study of
  Abortion on YouTube</title>
    <summary>  Recommendation algorithms (RS) used by social media, like YouTube,
significantly shape our information consumption across various domains,
especially in healthcare. Hence, algorithmic auditing becomes crucial to
uncover their potential bias and misinformation, particularly in the context of
controversial topics like abortion. We introduce a simple yet effective sock
puppet auditing approach to investigate how YouTube recommends abortion-related
videos to individuals with different backgrounds. Our framework allows for
efficient auditing of RS, regardless of the complexity of the underlying
algorithms
</summary>
    <author>
      <name>Mohammed Lahsaini</name>
    </author>
    <author>
      <name>Mohamed Lechiakh</name>
    </author>
    <author>
      <name>Alexandre Maurer</name>
    </author>
    <link href="http://arxiv.org/abs/2404.07896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.07896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.05923v1</id>
    <updated>2024-04-23T17:37:33Z</updated>
    <published>2024-04-23T17:37:33Z</published>
    <title>Darkverse -- A New DarkWeb?</title>
    <summary>  The "Darkverse" could be the negative harmful area of the Metaverse; a new
virtual immersive environment for the facilitation of illicit activity such as
misinformation, fraud, harassment, and illegal marketplaces. This paper
explores the potential for inappropriate activities within the Metaverse, and
the similarities between the Darkverse and the Dark Web. Challenges and future
directions for investigation are also discussed, including user identification,
creation of privacy-preserving frameworks and other data monitoring methods.
</summary>
    <author>
      <name>Raymond Chan</name>
    </author>
    <author>
      <name>Benjamin W. J. Kwok</name>
    </author>
    <author>
      <name>Adriel Yeo</name>
    </author>
    <author>
      <name>Kan Chen</name>
    </author>
    <author>
      <name>Jeannie S. Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an accepted position statement of CHI 2024 Workshop (Novel
  Approaches for Understanding and Mitigating Emerging New Harms in Immersive
  and Embodied Virtual Spaces: A Workshop at CHI 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.05923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.05923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.12853v1</id>
    <updated>2024-07-09T01:54:13Z</updated>
    <published>2024-07-09T01:54:13Z</published>
    <title>Automated Justification Production for Claim Veracity in Fact Checking:
  A Survey on Architectures and Approaches</title>
    <summary>  Automated Fact-Checking (AFC) is the automated verification of claim
accuracy. AFC is crucial in discerning truth from misinformation, especially
given the huge amounts of content are generated online daily. Current research
focuses on predicting claim veracity through metadata analysis and language
scrutiny, with an emphasis on justifying verdicts. This paper surveys recent
methodologies, proposing a comprehensive taxonomy and presenting the evolution
of research in that landscape. A comparative analysis of methodologies and
future directions for improving fact-checking explainability are also
discussed.
</summary>
    <author>
      <name>Islam Eldifrawi</name>
    </author>
    <author>
      <name>Shengrui Wang</name>
    </author>
    <author>
      <name>Amine Trabelsi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18653/v1/2024.acl-long.361</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18653/v1/2024.acl-long.361" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024 Main Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.12853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.12853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.08473v1</id>
    <updated>2024-09-13T01:58:42Z</updated>
    <published>2024-09-13T01:58:42Z</published>
    <title>Stark Decline in Journalists' Use of Preprints Post-pandemic</title>
    <summary>  The COVID-19 pandemic accelerated the use of preprints, aiding rapid research
dissemination but also facilitating the spread of misinformation. This study
analyzes media coverage of preprints from 2014 to 2023, revealing a significant
post-pandemic decline. Our findings suggest that heightened awareness of the
risks associated with preprints has led to more cautious media practices. While
the decline in preprint coverage may mitigate concerns about premature media
exposure, it also raises questions about the future role of preprints in
science communication, especially during emergencies. Balanced policies based
on up-to-date evidence are needed to address this shift.
</summary>
    <author>
      <name>Juan Pablo Alperin</name>
    </author>
    <author>
      <name>Kenneth Shores</name>
    </author>
    <author>
      <name>Alice Fleerackers</name>
    </author>
    <author>
      <name>Natascha Chtena</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1177/10755470241285405</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1177/10755470241285405" rel="related"/>
    <link href="http://arxiv.org/abs/2409.08473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.08473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.05507v1</id>
    <updated>2024-10-07T21:25:49Z</updated>
    <published>2024-10-07T21:25:49Z</published>
    <title>Structural Constraints for Physics-augmented Learning</title>
    <summary>  When the physics is wrong, physics-informed machine learning becomes
physics-misinformed machine learning. A powerful black-box model should not be
able to conceal misconceived physics. We propose two criteria that can be used
to assert integrity that a hybrid (physics plus black-box) model: 0) the
black-box model should be unable to replicate the physical model, and 1) any
best-fit hybrid model has the same physical parameter as a best-fit standalone
physics model. We demonstrate them for a sample nonlinear mechanical system
approximated by its small-signal linearization.
</summary>
    <author>
      <name>Simon Kuang</name>
    </author>
    <author>
      <name>Xinfan Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2410.05507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.05507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.20940v1</id>
    <updated>2024-10-28T11:46:30Z</updated>
    <published>2024-10-28T11:46:30Z</published>
    <title>Attacking Misinformation Detection Using Adversarial Examples Generated
  by Language Models</title>
    <summary>  We investigate the challenge of generating adversarial examples to test the
robustness of text classification algorithms detecting low-credibility content,
including propaganda, false claims, rumours and hyperpartisan news. We focus on
simulation of content moderation by setting realistic limits on the number of
queries an attacker is allowed to attempt. Within our solution (TREPAT),
initial rephrasings are generated by large language models with prompts
inspired by meaning-preserving NLP tasks, e.g. text simplification and style
transfer. Subsequently, these modifications are decomposed into small changes,
applied through beam search procedure until the victim classifier changes its
decision. The evaluation confirms the superiority of our approach in the
constrained scenario, especially in case of long input text (news articles),
where exhaustive search is not feasible.
</summary>
    <author>
      <name>Piotr Przyby≈Ça</name>
    </author>
    <link href="http://arxiv.org/abs/2410.20940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.20940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.00073v1</id>
    <updated>2024-11-26T06:35:26Z</updated>
    <published>2024-11-26T06:35:26Z</published>
    <title>Addressing Vulnerabilities in AI-Image Detection: Challenges and
  Proposed Solutions</title>
    <summary>  The rise of advanced AI models like Generative Adversarial Networks (GANs)
and diffusion models such as Stable Diffusion has made the creation of highly
realistic images accessible, posing risks of misuse in misinformation and
manipulation. This study evaluates the effectiveness of convolutional neural
networks (CNNs), as well as DenseNet architectures, for detecting AI-generated
images. Using variations of the CIFAKE dataset, including images generated by
different versions of Stable Diffusion, we analyze the impact of updates and
modifications such as Gaussian blurring, prompt text changes, and Low-Rank
Adaptation (LoRA) on detection accuracy. The findings highlight vulnerabilities
in current detection methods and propose strategies to enhance the robustness
and reliability of AI-image detection systems.
</summary>
    <author>
      <name>Justin Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2412.00073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.00073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.04046v1</id>
    <updated>2025-01-05T22:37:19Z</updated>
    <published>2025-01-05T22:37:19Z</published>
    <title>Traits of a Leader: User Influence Level Prediction through
  Sociolinguistic Modeling</title>
    <summary>  Recognition of a user's influence level has attracted much attention as human
interactions move online. Influential users have the ability to sway others'
opinions to achieve some goals. As a result, predicting users' level of
influence can help to understand social networks, forecast trends, prevent
misinformation, etc. However, predicting user influence is a challenging
problem because the concept of influence is specific to a situation or a
domain, and user communications are limited to text. In this work, we define
user influence level as a function of community endorsement and develop a model
that significantly outperforms the baseline by leveraging demographic and
personality data. This approach consistently improves RankDCG scores across
eight different domains.
</summary>
    <author>
      <name>Denys Katerenchuk</name>
    </author>
    <author>
      <name>Rivka Levitan</name>
    </author>
    <link href="http://arxiv.org/abs/2501.04046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.04046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08709v1</id>
    <updated>2025-03-10T03:05:21Z</updated>
    <published>2025-03-10T03:05:21Z</published>
    <title>Simulating Influence Dynamics with LLM Agents</title>
    <summary>  This paper introduces a simulator designed for opinion dynamics researchers
to model competing influences within social networks in the presence of
LLM-based agents. By integrating established opinion dynamics principles with
state-of-the-art LLMs, this tool enables the study of influence propagation and
counter-misinformation strategies. The simulator is particularly valuable for
researchers in social science, psychology, and operations research, allowing
them to analyse societal phenomena without requiring extensive coding
expertise. Additionally, the simulator will be openly available on GitHub,
ensuring accessibility and adaptability for those who wish to extend its
capabilities for their own research.
</summary>
    <author>
      <name>Mehwish Nasim</name>
    </author>
    <author>
      <name>Syed Muslim Gilani</name>
    </author>
    <author>
      <name>Amin Qasmi</name>
    </author>
    <author>
      <name>Usman Naseem</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.6.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10029v1</id>
    <updated>2025-06-10T09:24:05Z</updated>
    <published>2025-06-10T09:24:05Z</published>
    <title>Evaluation empirique de la s√©curisation et de l'alignement de ChatGPT
  et Gemini: analyse comparative des vuln√©rabilit√©s par exp√©rimentations
  de jailbreaks</title>
    <summary>  Large Language models (LLMs) are transforming digital usage, particularly in
text generation, image creation, information retrieval and code development.
ChatGPT, launched by OpenAI in November 2022, quickly became a reference,
prompting the emergence of competitors such as Google's Gemini. However, these
technological advances raise new cybersecurity challenges, including prompt
injection attacks, the circumvention of regulatory measures (jailbreaking), the
spread of misinformation (hallucinations) and risks associated with deep fakes.
This paper presents a comparative analysis of the security and alignment levels
of ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated
with experiments.
</summary>
    <author>
      <name>Rafa√´l Nouailles</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GdR</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French language</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.10029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.03533v1</id>
    <updated>2018-12-09T17:53:08Z</updated>
    <published>2018-12-09T17:53:08Z</published>
    <title>Propagation from Deceptive News Sources: Who Shares, How Much, How
  Evenly, and How Quickly?</title>
    <summary>  As people rely on social media as their primary sources of news, the spread
of misinformation has become a significant concern. In this large-scale study
of news in social media we analyze eleven million posts and investigate
propagation behavior of users that directly interact with news accounts
identified as spreading trusted versus malicious content. Unlike previous work,
which looks at specific rumors, topics, or events, we consider all content
propagated by various news sources. Moreover, we analyze and contrast
population versus sub-population behaviour (by demographics) when spreading
misinformation, and distinguish between two types of propagation, i.e., direct
retweets and mentions. Our evaluation examines how evenly, how many, how
quickly, and which users propagate content from various types of news sources
on Twitter.
  Our analysis has identified several key differences in propagation behavior
from trusted versus suspicious news sources. These include high inequity in the
diffusion rate based on the source of disinformation, with a small group of
highly active users responsible for the majority of disinformation spread
overall and within each demographic. Analysis by demographics showed that users
with lower annual income and education share more from disinformation sources
compared to their counterparts. News content is shared significantly more
quickly from trusted, conspiracy, and disinformation sources compared to
clickbait and propaganda. Older users propagate news from trusted sources more
quickly than younger users, but they share from suspicious sources after longer
delays. Finally, users who interact with clickbait and conspiracy sources are
likely to share from propaganda accounts, but not the other way around.
</summary>
    <author>
      <name>Maria Glenski</name>
    </author>
    <author>
      <name>Tim Weninger</name>
    </author>
    <author>
      <name>Svitlana Volkova</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSS.2018.2881071</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSS.2018.2881071" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 7 tables, published in IEEE TCSS December 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Computational Social Systems ( Volume: 5 ,
  Issue: 4 , Dec. 2018 )</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.03533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.03533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.10508v1</id>
    <updated>2018-12-26T19:31:32Z</updated>
    <published>2018-12-26T19:31:32Z</published>
    <title>A blockchain based Secure and Trusted framework for Information
  Propagation on Online Social Networks</title>
    <summary>  The online social networks facilitate naturally for the users to share
information. On these platforms, each user shares information based on his or
her interests. The particular information being shared by a user may be
legitimate or fake. Sometimes a misinformation, propagated by users and group
can create chaos or in some cases, might leads to cases of riots. Nowadays the
third party like ALT news and Cobrapost check the information authenticity, but
it takes too much time to validate the news. Therefore, a robust and new system
is required to check the information authenticity within the network, to stop
the propagation of misinformation. In this paper, we propose a blockchain based
framework for sharing the information securely at the peer level. In the
blockchain model, a chain is created by combining blocks of information. Each
node of network propagates the information based on its credibility to its peer
nodes. The credibility of a node will vary according to the respective
information. Trust is calculated between sender and receiver in two ways:(i)
Local trust used for sharing information at the peer level and (ii) global
trust is used for a credibility check of each user in the network. We evaluate
our framework using real dataset derived from Facebook. Our approach achieves
an accuracy of 83% which shows the effectiveness of our proposed framework.
</summary>
    <author>
      <name>Md Arquam</name>
    </author>
    <author>
      <name>Anurag Singh</name>
    </author>
    <author>
      <name>Rajesh Sharma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13278-021-00754-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13278-021-00754-y" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Social Network Analysis and Mining volume 11, Article number: 49
  (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.10508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04624v1</id>
    <updated>2020-01-14T04:50:47Z</updated>
    <published>2020-01-14T04:50:47Z</published>
    <title>A Feature-Driven Approach for Identifying Pathogenic Social Media
  Accounts</title>
    <summary>  Over the past few years, we have observed different media outlets' attempts
to shift public opinion by framing information to support a narrative that
facilitate their goals. Malicious users referred to as "pathogenic social
media" (PSM) accounts are more likely to amplify this phenomena by spreading
misinformation to viral proportions. Understanding the spread of misinformation
from account-level perspective is thus a pressing problem. In this work, we aim
to present a feature-driven approach to detect PSM accounts in social media.
Inspired by the literature, we set out to assess PSMs from three broad
perspectives: (1) user-related information (e.g., user activity, profile
characteristics), (2) source-related information (i.e., information linked via
URLs shared by users) and (3) content-related information (e.g., tweets
characteristics). For the user-related information, we investigate malicious
signals using causality analysis (i.e., if user is frequently a cause of viral
cascades) and profile characteristics (e.g., number of followers, etc.). For
the source-related information, we explore various malicious properties linked
to URLs (e.g., URL address, content of the associated website, etc.). Finally,
for the content-related information, we examine attributes (e.g., number of
hashtags, suspicious hashtags, etc.) from tweets posted by users. Experiments
on real-world Twitter data from different countries demonstrate the
effectiveness of the proposed approach in identifying PSM users.
</summary>
    <author>
      <name>Hamidreza Alvari</name>
    </author>
    <author>
      <name>Ghazaleh Beigi</name>
    </author>
    <author>
      <name>Soumajyoti Sarkar</name>
    </author>
    <author>
      <name>Scott W. Ruston</name>
    </author>
    <author>
      <name>Steven R. Corman</name>
    </author>
    <author>
      <name>Hasan Davulcu</name>
    </author>
    <author>
      <name>Paulo Shakarian</name>
    </author>
    <link href="http://arxiv.org/abs/2001.04624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09473v1</id>
    <updated>2020-01-26T15:42:43Z</updated>
    <published>2020-01-26T15:42:43Z</published>
    <title>Information Credibility in the Social Web: Contexts, Approaches, and
  Open Issues</title>
    <summary>  In the Social Web scenario, large amounts of User-Generated Content (UGC) are
diffused through social media often without almost any form of traditional
trusted intermediaries. Therefore, the risk of running into misinformation is
not negligible. For this reason, assessing and mining the credibility of online
information constitutes nowadays a fundamental research issue. Credibility,
also referred as believability, is a quality perceived by individuals, who are
not always able to discern, with their own cognitive capacities, genuine
information from fake one. Hence, in the last years, several approaches have
been proposed to automatically assess credibility in social media. Many of them
are based on data-driven models, i.e., they employ machine learning techniques
to identify misinformation, but recently also model-driven approaches are
emerging, as well as graph-based approaches focusing on credibility
propagation, and knowledge-based ones exploiting Semantic Web technologies.
Three of the main contexts in which the assessment of information credibility
has been investigated concern: (i) the detection of opinion spam in review
sites, (ii) the detection of fake news in microblogging, and (iii) the
credibility assessment of online health-related information. In this article,
the main issues connected to the evaluation of information credibility in the
Social Web, which are shared by the above-mentioned contexts, are discussed. A
concise survey of the approaches and methodologies that have been proposed in
recent years to address these issues is also presented.
</summary>
    <author>
      <name>Gabriella Pasi</name>
    </author>
    <author>
      <name>Marco Viviani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Article accepted and presented at ITASEC 2020: Italian Conference on
  Cybersecurity. February 4-7, 2020, Ancona, Italy. https://itasec.it/</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.09473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.11638v5</id>
    <updated>2021-10-12T23:03:38Z</updated>
    <published>2020-10-22T12:20:01Z</published>
    <title>"It is just a flu": Assessing the Effect of Watch History on YouTube's
  Pseudoscientific Video Recommendations</title>
    <summary>  The role played by YouTube's recommendation algorithm in unwittingly
promoting misinformation and conspiracy theories is not entirely understood.
Yet, this can have dire real-world consequences, especially when
pseudoscientific content is promoted to users at critical times, such as the
COVID-19 pandemic. In this paper, we set out to characterize and detect
pseudoscientific misinformation on YouTube. We collect 6.6K videos related to
COVID-19, the Flat Earth theory, as well as the anti-vaccination and anti-mask
movements. Using crowdsourcing, we annotate them as pseudoscience, legitimate
science, or irrelevant and train a deep learning classifier to detect
pseudoscientific videos with an accuracy of 0.79.
  We quantify user exposure to this content on various parts of the platform
and how this exposure changes based on the user's watch history. We find that
YouTube suggests more pseudoscientific content regarding traditional
pseudoscientific topics (e.g., flat earth, anti-vaccination) than for emerging
ones (like COVID-19). At the same time, these recommendations are more common
on the search results page than on a user's homepage or in the recommendation
section when actively watching videos. Finally, we shed light on how a user's
watch history substantially affects the type of recommended videos.
</summary>
    <author>
      <name>Kostantinos Papadamou</name>
    </author>
    <author>
      <name>Savvas Zannettou</name>
    </author>
    <author>
      <name>Jeremy Blackburn</name>
    </author>
    <author>
      <name>Emiliano De Cristofaro</name>
    </author>
    <author>
      <name>Gianluca Stringhini</name>
    </author>
    <author>
      <name>Michael Sirivianos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the 16th International Conference on Web and Social
  Media (ICWSM 2022). Please cite the ICWSM version</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.11638v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.11638v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.09353v2</id>
    <updated>2021-04-03T00:22:38Z</updated>
    <published>2020-12-17T02:00:43Z</published>
    <title>The COVID-19 Infodemic: Twitter versus Facebook</title>
    <summary>  The global spread of the novel coronavirus is affected by the spread of
related misinformation -- the so-called COVID-19 Infodemic -- that makes
populations more vulnerable to the disease through resistance to mitigation
efforts. Here we analyze the prevalence and diffusion of links to
low-credibility content about the pandemic across two major social media
platforms, Twitter and Facebook. We characterize cross-platform similarities
and differences in popular sources, diffusion patterns, influencers,
coordination, and automation. Comparing the two platforms, we find divergence
among the prevalence of popular low-credibility sources and suspicious videos.
A minority of accounts and pages exert a strong influence on each platform.
These misinformation "superspreaders" are often associated with the
low-credibility sources and tend to be verified by the platforms. On both
platforms, there is evidence of coordinated sharing of Infodemic content. The
overt nature of this manipulation points to the need for societal-level
solutions in addition to mitigation strategies within the platforms. However,
we highlight limits imposed by inconsistent data-access policies on our
capability to study harmful manipulations of information ecosystems.
</summary>
    <author>
      <name>Kai-Cheng Yang</name>
    </author>
    <author>
      <name>Francesco Pierri</name>
    </author>
    <author>
      <name>Pik-Mai Hui</name>
    </author>
    <author>
      <name>David Axelrod</name>
    </author>
    <author>
      <name>Christopher Torres-Lugo</name>
    </author>
    <author>
      <name>John Bryden</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.09353v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09353v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.13968v1</id>
    <updated>2020-12-27T16:03:32Z</updated>
    <published>2020-12-27T16:03:32Z</published>
    <title>Detecting Medical Misinformation on Social Media Using Multimodal Deep
  Learning</title>
    <summary>  In 2019, outbreaks of vaccine-preventable diseases reached the highest number
in the US since 1992. Medical misinformation, such as antivaccine content
propagating through social media, is associated with increases in vaccine delay
and refusal. Our overall goal is to develop an automatic detector for
antivaccine messages to counteract the negative impact that antivaccine
messages have on the public health. Very few extant detection systems have
considered multimodality of social media posts (images, texts, and hashtags),
and instead focus on textual components, despite the rapid growth of
photo-sharing applications (e.g., Instagram). As a result, existing systems are
not sufficient for detecting antivaccine messages with heavy visual components
(e.g., images) posted on these newer platforms. To solve this problem, we
propose a deep learning network that leverages both visual and textual
information. A new semantic- and task-level attention mechanism was created to
help our model to focus on the essential contents of a post that signal
antivaccine messages. The proposed model, which consists of three branches, can
generate comprehensive fused features for predictions. Moreover, an ensemble
method is proposed to further improve the final prediction accuracy. To
evaluate the proposed model's performance, a real-world social media dataset
that consists of more than 30,000 samples was collected from Instagram between
January 2016 and October 2019. Our 30 experiment results demonstrate that the
final network achieves above 97% testing accuracy and outperforms other
relevant models, demonstrating that it can detect a large amount of antivaccine
messages posted daily. The implementation code is available at
https://github.com/wzhings/antivaccine_detection.
</summary>
    <author>
      <name>Zuhui Wang</name>
    </author>
    <author>
      <name>Zhaozheng Yin</name>
    </author>
    <author>
      <name>Young Anna Argyris</name>
    </author>
    <link href="http://arxiv.org/abs/2012.13968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.10272v1</id>
    <updated>2021-05-21T10:46:43Z</updated>
    <published>2021-05-21T10:46:43Z</published>
    <title>Stance Detection with BERT Embeddings for Credibility Analysis of
  Information on Social Media</title>
    <summary>  The evolution of electronic media is a mixed blessing. Due to the easy
access, low cost, and faster reach of the information, people search out and
devour news from online social networks. In contrast, the increasing acceptance
of social media reporting leads to the spread of fake news. This is a minacious
problem that causes disputes and endangers societal stability and harmony. Fake
news spread has gained attention from researchers due to its vicious nature.
proliferation of misinformation in all media, from the internet to cable news,
paid advertising and local news outlets, has made it essential for people to
identify the misinformation and sort through the facts. Researchers are trying
to analyze the credibility of information and curtail false information on such
platforms. Credibility is the believability of the piece of information at
hand. Analyzing the credibility of fake news is challenging due to the intent
of its creation and the polychromatic nature of the news. In this work, we
propose a model for detecting fake news. Our method investigates the content of
the news at the early stage i.e. when the news is published but is yet to be
disseminated through social media. Our work interprets the content with
automatic feature extraction and the relevance of the text pieces. In summary,
we introduce stance as one of the features along with the content of the
article and employ the pre-trained contextualized word embeddings BERT to
obtain the state-of-art results for fake news detection. The experiment
conducted on the real-world dataset indicates that our model outperforms the
previous work and enables fake news detection with an accuracy of 95.32%.
</summary>
    <author>
      <name>Hema Karande</name>
    </author>
    <author>
      <name>Rahee Walambe</name>
    </author>
    <author>
      <name>Victor Benjamin</name>
    </author>
    <author>
      <name>Ketan Kotecha</name>
    </author>
    <author>
      <name>T. S. Raghu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7717/peerj-cs.467</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7717/peerj-cs.467" rel="related"/>
    <link href="http://arxiv.org/abs/2105.10272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03778v1</id>
    <updated>2017-07-12T15:55:20Z</updated>
    <published>2017-07-12T15:55:20Z</published>
    <title>Catching Zika Fever: Application of Crowdsourcing and Machine Learning
  for Tracking Health Misinformation on Twitter</title>
    <summary>  In February 2016, World Health Organization declared the Zika outbreak a
Public Health Emergency of International Concern. With developing evidence it
can cause birth defects, and the Summer Olympics coming up in the worst
affected country, Brazil, the virus caught fire on social media. In this work,
use Zika as a case study in building a tool for tracking the misinformation
around health concerns on Twitter. We collect more than 13 million tweets --
spanning the initial reports in February 2016 and the Summer Olympics --
regarding the Zika outbreak and track rumors outlined by the World Health
Organization and Snopes fact checking website. The tool pipeline, which
incorporates health professionals, crowdsourcing, and machine learning, allows
us to capture health-related rumors around the world, as well as clarification
campaigns by reputable health organizations. In the case of Zika, we discover
an extremely bursty behavior of rumor-related topics, and show that, once the
questionable topic is detected, it is possible to identify rumor-bearing tweets
using automated techniques. Thus, we illustrate insights the proposed tools
provide into potentially harmful information on social media, allowing public
health researchers and practitioners to respond with a targeted and timely
action.
</summary>
    <author>
      <name>Amira Ghenai</name>
    </author>
    <author>
      <name>Yelena Mejova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures, short version to be published in the Fifth IEEE
  International Conference on Healthcare Informatics (ICHI 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.03778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.3.3; I.2.7; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05927v1</id>
    <updated>2018-08-17T16:47:52Z</updated>
    <published>2018-08-17T16:47:52Z</published>
    <title>Characterizing the public perception of WhatsApp through the lens of
  media</title>
    <summary>  WhatsApp is, as of 2018, a significant component of the global information
and communication infrastructure, especially in developing countries. However,
probably due to its strong end-to-end encryption, WhatsApp became an attractive
place for the dissemination of misinformation, extremism and other forms of
undesirable behavior. In this paper, we investigate the public perception of
WhatsApp through the lens of media. We analyze two large datasets of news and
show the kind of content that is being associated with WhatsApp in different
regions of the world and over time. Our analyses include the examination of
named entities, general vocabulary, and topics addressed in news articles that
mention WhatsApp, as well as the polarity of these texts. Among other results,
we demonstrate that the vocabulary and topics around the term "whatsapp" in the
media have been changing over the years and in 2018 concentrate on matters
related to misinformation, politics and criminal scams. More generally, our
findings are useful to understand the impact that tools like WhatsApp play in
the contemporary society and how they are seen by the communities themselves.
</summary>
    <author>
      <name>Josemar Alves Caetano</name>
    </author>
    <author>
      <name>Gabriel Magno</name>
    </author>
    <author>
      <name>Evandro Cunha</name>
    </author>
    <author>
      <name>Wagner Meira Jr.</name>
    </author>
    <author>
      <name>Humberto T. Marques-Neto</name>
    </author>
    <author>
      <name>Virgilio Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a full paper at the 2nd International Workshop on Rumours
  and Deception in Social Media (RDSM 2018), co-located with CIKM 2018 in
  Turin. Please cite the RDSM version</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05305v1</id>
    <updated>2019-05-13T22:27:59Z</updated>
    <published>2019-05-13T22:27:59Z</published>
    <title>Consequential Ranking Algorithms and Long-term Welfare</title>
    <summary>  Ranking models are typically designed to provide rankings that optimize some
measure of immediate utility to the users. As a result, they have been unable
to anticipate an increasing number of undesirable long-term consequences of
their proposed rankings, from fueling the spread of misinformation and
increasing polarization to degrading social discourse. Can we design ranking
models that understand the consequences of their proposed rankings and, more
importantly, are able to avoid the undesirable ones? In this paper, we first
introduce a joint representation of rankings and user dynamics using Markov
decision processes. Then, we show that this representation greatly simplifies
the construction of consequential ranking models that trade off the immediate
utility and the long-term welfare. In particular, we can obtain optimal
consequential rankings just by applying weighted sampling on the rankings
provided by models that maximize measures of immediate utility. However, in
practice, such a strategy may be inefficient and impractical, specially in high
dimensional scenarios. To overcome this, we introduce an efficient
gradient-based algorithm to learn parameterized consequential ranking models
that effectively approximate optimal ones. We showcase our methodology using
synthetic and real data gathered from Reddit and show that ranking models
derived using our methodology provide ranks that may mitigate the spread of
misinformation and improve the civility of online discussions.
</summary>
    <author>
      <name>Behzad Tabibian</name>
    </author>
    <author>
      <name>Vicen√ß G√≥mez</name>
    </author>
    <author>
      <name>Abir De</name>
    </author>
    <author>
      <name>Bernhard Sch√∂lkopf</name>
    </author>
    <author>
      <name>Manuel Gomez Rodriguez</name>
    </author>
    <link href="http://arxiv.org/abs/1905.05305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00531v2</id>
    <updated>2019-10-03T19:43:53Z</updated>
    <published>2019-10-01T16:32:59Z</published>
    <title>On the Influence of Twitter Trolls during the 2016 US Presidential
  Election</title>
    <summary>  It is a widely accepted fact that state-sponsored Twitter accounts operated
during the 2016 US presidential election spreading millions of tweets with
misinformation and inflammatory political content. Whether these social media
campaigns of the so-called "troll" accounts were able to manipulate public
opinion is still in question. Here we aim to quantify the influence of troll
accounts and the impact they had on Twitter by analyzing 152.5 million tweets
from 9.9 million users, including 822 troll accounts. The data collected during
the US election campaign, contain original troll tweets before they were
deleted by Twitter. From these data, we constructed a very large interaction
graph; a directed graph of 9.3 million nodes and 169.9 million edges. Recently,
Twitter released datasets on the misinformation campaigns of 8,275
state-sponsored accounts linked to Russia, Iran and Venezuela as part of the
investigation on the foreign interference in the 2016 US election. These data
serve as ground-truth identifier of troll users in our dataset. Using graph
analysis techniques we qualify the diffusion cascades of web and media context
that have been shared by the troll accounts. We present strong evidence that
authentic users were the source of the viral cascades. Although the trolls were
participating in the viral cascades, they did not have a leading role in them
and only four troll accounts were truly influential.
</summary>
    <author>
      <name>Nikos Salamanos</name>
    </author>
    <author>
      <name>Michael J. Jensen</name>
    </author>
    <author>
      <name>Xinlei He</name>
    </author>
    <author>
      <name>Yang Chen</name>
    </author>
    <author>
      <name>Michael Sirivianos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">With this version, we are correcting an error in the Acknowledgments
  regarding the research funding that supports this work. The correct one is
  the European Union's Horizon 2020 Research and Innovation program under the
  Cybersecurity CONCORDIA project (Grant Agreement No. 830927)</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00531v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00531v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02223v1</id>
    <updated>2019-10-05T06:48:23Z</updated>
    <published>2019-10-05T06:48:23Z</published>
    <title>A Machine Learning Analysis of the Features in Deceptive and Credible
  News</title>
    <summary>  Fake news is a type of pervasive propaganda that spreads misinformation
online, taking advantage of social media's extensive reach to manipulate public
perception. Over the past three years, fake news has become a focal discussion
point in the media due to its impact on the 2016 U.S. presidential election.
Fake news can have severe real-world implications: in 2016, a man walked into a
pizzeria carrying a rifle because he read that Hillary Clinton was harboring
children as sex slaves. This project presents a high accuracy (87%) machine
learning classifier that determines the validity of news based on the word
distributions and specific linguistic and stylistic differences in the first
few sentences of an article. This can help readers identify the validity of an
article by looking for specific features in the opening lines aiding them in
making informed decisions. Using a dataset of 2,107 articles from 30 different
websites, this project establishes an understanding of the variations between
fake and credible news by examining the model, dataset, and features. This
classifier appears to use the differences in word distribution, levels of tone
authenticity, and frequency of adverbs, adjectives, and nouns. The
differentiation in the features of these articles can be used to improve future
classifiers. This classifier can also be further applied directly to browsers
as a Google Chrome extension or as a filter for social media outlets or news
websites to reduce the spread of misinformation.
</summary>
    <author>
      <name>Qi Jia Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10130v1</id>
    <updated>2019-11-22T16:35:37Z</updated>
    <published>2019-11-22T16:35:37Z</published>
    <title>A Data Set of Internet Claims and Comparison of their Sentiments with
  Credibility</title>
    <summary>  In this modern era, communication has become faster and easier. This means
fallacious information can spread as fast as reality. Considering the damage
that fake news kindles on the psychology of people and the fact that such news
proliferates faster than truth, we need to study the phenomenon that helps
spread fake news. An unbiased data set that depends on reality for rating news
is necessary to construct predictive models for its classification. This paper
describes the methodology to create such a data set. We collect our data from
snopes.com which is a fact-checking organization. Furthermore, we intend to
create this data set not only for classification of the news but also to find
patterns that reason the intent behind misinformation. We also formally define
an Internet Claim, its credibility, and the sentiment behind such a claim. We
try to realize the relationship between the sentiment of a claim with its
credibility. This relationship pours light on the bigger picture behind the
propagation of misinformation. We pave the way for further research based on
the methodology described in this paper to create the data set and usage of
predictive modeling along with research-based on psychology/mentality of people
to understand why fake news spreads much faster than reality.
</summary>
    <author>
      <name>Amey Parundekar</name>
    </author>
    <author>
      <name>Susan Elias</name>
    </author>
    <author>
      <name>Ashwin Ashok</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, A paper accepted at the Truth Discovery and Fact
  Checking: Theory and Practice SIGKDD 2019 Workshop, August 5th, Anchorage,
  Alaska</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.10130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3, I.2.7" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.04334v3</id>
    <updated>2020-10-04T13:28:50Z</updated>
    <published>2020-06-08T03:05:28Z</published>
    <title>Characterizing Sociolinguistic Variation in the Competing Vaccination
  Communities</title>
    <summary>  Public health practitioners and policy makers grapple with the challenge of
devising effective message-based interventions for debunking public health
misinformation in cyber communities. "Framing" and "personalization" of the
message is one of the key features for devising a persuasive messaging
strategy. For an effective health communication, it is imperative to focus on
"preference-based framing" where the preferences of the target sub-community
are taken into consideration. To achieve that, it is important to understand
and hence characterize the target sub-communities in terms of their social
interactions. In the context of health-related misinformation, vaccination
remains to be the most prevalent topic of discord. Hence, in this paper, we
conduct a sociolinguistic analysis of the two competing vaccination communities
on Twitter: "pro-vaxxers" or individuals who believe in the effectiveness of
vaccinations, and "anti-vaxxers" or individuals who are opposed to
vaccinations. Our data analysis show significant linguistic variation between
the two communities in terms of their usage of linguistic intensifiers,
pronouns, and uncertainty words. Our network-level analysis show significant
differences between the two communities in terms of their network density,
echo-chamberness, and the EI index. We hypothesize that these sociolinguistic
differences can be used as proxies to characterize and understand these
communities to devise better message interventions.
</summary>
    <author>
      <name>Shahan Ali Memon</name>
    </author>
    <author>
      <name>Aman Tyagi</name>
    </author>
    <author>
      <name>David R. Mortensen</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 tables, 1 figure, 1 algorithm, accepted to SBP-BRiMS 2020
  -- International Conference on Social Computing, Behavioral-Cultural Modeling
  &amp; Prediction and Behavior Representation in Modeling and Simulation</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.04334v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04334v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.06854v1</id>
    <updated>2020-08-16T08:06:52Z</updated>
    <published>2020-08-16T08:06:52Z</published>
    <title>SGG: Spinbot, Grammarly and GloVe based Fake News Detection</title>
    <summary>  Recently, news consumption using online news portals has increased
exponentially due to several reasons, such as low cost and easy accessibility.
However, such online platforms inadvertently also become the cause of spreading
false information across the web. They are being misused quite frequently as a
medium to disseminate misinformation and hoaxes. Such malpractices call for a
robust automatic fake news detection system that can keep us at bay from such
misinformation and hoaxes. We propose a robust yet simple fake news detection
system, leveraging the tools for paraphrasing, grammar-checking, and
word-embedding. In this paper, we try to the potential of these tools in
jointly unearthing the authenticity of a news article. Notably, we leverage
Spinbot (for paraphrasing), Grammarly (for grammar-checking), and GloVe (for
word-embedding) tools for this purpose. Using these tools, we were able to
extract novel features that could yield state-of-the-art results on the Fake
News AMT dataset and comparable results on Celebrity datasets when combined
with some of the essential features. More importantly, the proposed method is
found to be more robust empirically than the existing ones, as revealed in our
cross-domain analysis and multi-domain analysis.
</summary>
    <author>
      <name>Akansha Gautam</name>
    </author>
    <author>
      <name>Koteswar Rao Jerripothula</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, Accepted by IEEE International Conference on
  Multimedia Big Data (BigMM), 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.06854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.06854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.09533v1</id>
    <updated>2020-08-21T15:19:18Z</updated>
    <published>2020-08-21T15:19:18Z</published>
    <title>Investigating Differences in Crowdsourced News Credibility Assessment:
  Raters, Tasks, and Expert Criteria</title>
    <summary>  Misinformation about critical issues such as climate change and vaccine
safety is oftentimes amplified on online social and search platforms. The
crowdsourcing of content credibility assessment by laypeople has been proposed
as one strategy to combat misinformation by attempting to replicate the
assessments of experts at scale. In this work, we investigate news credibility
assessments by crowds versus experts to understand when and how ratings between
them differ. We gather a dataset of over 4,000 credibility assessments taken
from 2 crowd groups---journalism students and Upwork workers---as well as 2
expert groups---journalists and scientists---on a varied set of 50 news
articles related to climate science, a topic with widespread disconnect between
public opinion and expert consensus. Examining the ratings, we find differences
in performance due to the makeup of the crowd, such as rater demographics and
political leaning, as well as the scope of the tasks that the crowd is assigned
to rate, such as the genre of the article and partisanship of the publication.
Finally, we find differences between expert assessments due to differing expert
criteria that journalism versus science experts use---differences that may
contribute to crowd discrepancies, but that also suggest a way to reduce the
gap by designing crowd tasks tailored to specific expert criteria. From these
findings, we outline future research directions to better design crowd
processes that are tailored to specific crowds and types of content.
</summary>
    <author>
      <name>Md Momen Bhuiyan</name>
    </author>
    <author>
      <name>Amy X. Zhang</name>
    </author>
    <author>
      <name>Connie Moon Sehat</name>
    </author>
    <author>
      <name>Tanushree Mitra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3415164</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3415164" rel="related"/>
    <link href="http://arxiv.org/abs/2008.09533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.09533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08790v4</id>
    <updated>2022-03-22T04:42:13Z</updated>
    <published>2021-04-18T09:50:11Z</published>
    <title>Misinfo Reaction Frames: Reasoning about Readers' Reactions to News
  Headlines</title>
    <summary>  Even to a simple and short news headline, readers react in a multitude of
ways: cognitively (e.g. inferring the writer's intent), emotionally (e.g.
feeling distrust), and behaviorally (e.g. sharing the news with their friends).
Such reactions are instantaneous and yet complex, as they rely on factors that
go beyond interpreting factual content of news. We propose Misinfo Reaction
Frames (MRF), a pragmatic formalism for modeling how readers might react to a
news headline. In contrast to categorical schema, our free-text dimensions
provide a more nuanced way of understanding intent beyond being benign or
malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced
dataset of reactions to over 25k news headlines focusing on global crises: the
Covid-19 pandemic, climate change, and cancer. Empirical results confirm that
it is indeed possible for neural models to predict the prominent patterns of
readers' reactions to previously unseen news headlines. Additionally, our user
study shows that displaying machine-generated MRF implications alongside news
headlines to readers can increase their trust in real news while decreasing
their trust in misinformation. Our work demonstrates the feasibility and
importance of pragmatic inferences on news headlines to help enhance AI-guided
misinformation detection and mitigation.
</summary>
    <author>
      <name>Saadia Gabriel</name>
    </author>
    <author>
      <name>Skyler Hallinan</name>
    </author>
    <author>
      <name>Maarten Sap</name>
    </author>
    <author>
      <name>Pemi Nguyen</name>
    </author>
    <author>
      <name>Franziska Roesner</name>
    </author>
    <author>
      <name>Eunsol Choi</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2022 camera-ready</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.08790v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08790v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04416v1</id>
    <updated>2021-10-11T11:08:38Z</updated>
    <published>2021-10-11T11:08:38Z</published>
    <title>Topic Modeling, Clade-assisted Sentiment Analysis, and Vaccine Brand
  Reputation Analysis of COVID-19 Vaccine-related Facebook Comments in the
  Philippines</title>
    <summary>  Vaccine hesitancy and other COVID-19-related concerns and complaints in the
Philippines are evident on social media. It is important to identify these
different topics and sentiments in order to gauge public opinion, use the
insights to develop policies, and make necessary adjustments or actions to
improve public image and reputation of the administering agency and the
COVID-19 vaccines themselves. This paper proposes a semi-supervised machine
learning pipeline to perform topic modeling, sentiment analysis, and an
analysis of vaccine brand reputation to obtain an in-depth understanding of
national public opinion of Filipinos on Facebook. The methodology makes use of
a multilingual version of Bidirectional Encoder Representations from
Transformers or BERT for topic modeling, hierarchical clustering, five
different classifiers for sentiment analysis, and cosine similarity of BERT
topic embeddings for vaccine brand reputation analysis. Results suggest that
any type of COVID-19 misinformation is an emergent property of COVID-19 public
opinion, and that the detection of COVID-19 misinformation can be an
unsupervised task. Sentiment analysis aided by hierarchical clustering reveal
that 21 of the 25 topics extrapolated by topic modeling are negative topics.
Such negative comments spike in count whenever the Department of Health in the
Philippines posts about the COVID-19 situation in other countries.
Additionally, the high numbers of laugh reactions on the Facebook posts by the
same agency -- without any humorous content -- suggest that the reactors of
these posts tend to react the way they do, not because of what the posts are
about but because of who posted them.
</summary>
    <author>
      <name>Jasper Kyle Catapang</name>
    </author>
    <author>
      <name>Jerome V. Cleofas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Applied Soft Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.04416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.11272v1</id>
    <updated>2021-11-22T15:21:30Z</updated>
    <published>2021-11-22T15:21:30Z</published>
    <title>SOMPS-Net : Attention based social graph framework for early detection
  of fake health news</title>
    <summary>  Fake news is fabricated information that is presented as genuine, with
intention to deceive the reader. Recently, the magnitude of people relying on
social media for news consumption has increased significantly. Owing to this
rapid increase, the adverse effects of misinformation affect a wider audience.
On account of the increased vulnerability of people to such deceptive fake
news, a reliable technique to detect misinformation at its early stages is
imperative. Hence, the authors propose a novel graph-based framework SOcial
graph with Multi-head attention and Publisher information and news Statistics
Network (SOMPS-Net) comprising of two components - Social Interaction Graph
(SIG) and Publisher and News Statistics (PNS). The posited model is
experimented on the HealthStory dataset and generalizes across diverse medical
topics including Cancer, Alzheimer's, Obstetrics, and Nutrition. SOMPS-Net
significantly outperformed other state-of-the-art graph-based models
experimented on HealthStory by 17.1%. Further, experiments on early detection
demonstrated that SOMPS-Net predicted fake news articles with 79% certainty
within just 8 hours of its broadcast. Thus the contributions of this work lay
down the foundation for capturing fake health news across multiple medical
topics at its early stages.
</summary>
    <author>
      <name>Prasannakumaran D</name>
    </author>
    <author>
      <name>Harish Srinivasan</name>
    </author>
    <author>
      <name>Sowmiya Sree S</name>
    </author>
    <author>
      <name>Sri Gayathri Devi I</name>
    </author>
    <author>
      <name>Saikrishnan S</name>
    </author>
    <author>
      <name>Vineeth Vijayaraghavan</name>
    </author>
    <link href="http://arxiv.org/abs/2111.11272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.11272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07111v1</id>
    <updated>2020-02-17T18:13:09Z</updated>
    <published>2020-02-17T18:13:09Z</published>
    <title>Targeted Forgetting and False Memory Formation in Continual Learners
  through Adversarial Backdoor Attacks</title>
    <summary>  Artificial neural networks are well-known to be susceptible to catastrophic
forgetting when continually learning from sequences of tasks. Various continual
(or "incremental") learning approaches have been proposed to avoid catastrophic
forgetting, but they are typically adversary agnostic, i.e., they do not
consider the possibility of a malicious attack. In this effort, we explore the
vulnerability of Elastic Weight Consolidation (EWC), a popular continual
learning algorithm for avoiding catastrophic forgetting. We show that an
intelligent adversary can bypass the EWC's defenses, and instead cause gradual
and deliberate forgetting by introducing small amounts of misinformation to the
model during training. We demonstrate such an adversary's ability to assume
control of the model via injection of "backdoor" attack samples on both
permuted and split benchmark variants of the MNIST dataset. Importantly, once
the model has learned the adversarial misinformation, the adversary can then
control the amount of forgetting of any task. Equivalently, the malicious actor
can create a "false memory" about any task by inserting carefully-designed
backdoor samples to any fraction of the test instances of that task. Perhaps
most damaging, we show this vulnerability to be very acute; neural network
memory can be easily compromised with the addition of backdoor samples into as
little as 1% of the training data of even a single task.
</summary>
    <author>
      <name>Muhammad Umer</name>
    </author>
    <author>
      <name>Glenn Dawson</name>
    </author>
    <author>
      <name>Robi Polikar</name>
    </author>
    <link href="http://arxiv.org/abs/2002.07111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05096v1</id>
    <updated>2020-03-11T03:16:04Z</updated>
    <published>2020-03-11T03:16:04Z</published>
    <title>Exploring the Role of Visual Content in Fake News Detection</title>
    <summary>  The increasing popularity of social media promotes the proliferation of fake
news, which has caused significant negative societal effects. Therefore, fake
news detection on social media has recently become an emerging research area of
great concern. With the development of multimedia technology, fake news
attempts to utilize multimedia content with images or videos to attract and
mislead consumers for rapid dissemination, which makes visual content an
important part of fake news. Despite the importance of visual content, our
understanding of the role of visual content in fake news detection is still
limited. This chapter presents a comprehensive review of the visual content in
fake news, including the basic concepts, effective visual features,
representative detection methods and challenging issues of multimedia fake news
detection. This chapter can help readers to understand the role of visual
content in fake news detection, and effectively utilize visual content to
assist in detecting multimedia fake news.
</summary>
    <author>
      <name>Juan Cao</name>
    </author>
    <author>
      <name>Peng Qi</name>
    </author>
    <author>
      <name>Qiang Sheng</name>
    </author>
    <author>
      <name>Tianyun Yang</name>
    </author>
    <author>
      <name>Junbo Guo</name>
    </author>
    <author>
      <name>Jintao Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-42699-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-42699-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprint of a chapter published in Disinformation,
  Misinformation, and Fake News in Social Media: Emerging Research Challenges
  and Opportunities, edited by Kai, S., Suhang, W., Dongwon, L., Huan, L, 2020,
  Springer reproduced with permission of Springer Nature Switzerland AG. The
  final authenticated version is available online at:
  https://www.springer.com/gp/book/9783030426989. arXiv admin note: text
  overlap with arXiv:2001.00623, arXiv:1808.06686, arXiv:1903.00788 by other
  authors</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Disinformation, Misinformation, and Fake News in Social Media.
  2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.05096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08377v2</id>
    <updated>2020-04-09T13:24:29Z</updated>
    <published>2020-03-18T17:54:46Z</published>
    <title>Network disruption: maximizing disagreement and polarization in social
  networks</title>
    <summary>  Recent years have seen a marked increase in the spread of misinformation, a
phenomenon which has been accelerated and amplified by social media such as
Facebook and Twitter. While some actors spread misinformation to push a
specific agenda, it has also been widely documented that others aim to simply
disrupt the network by increasing disagreement and polarization across the
network and thereby destabilizing society. Popular social networks are also
vulnerable to large-scale attacks. Motivated by this reality, we introduce a
simple model of network disruption where an adversary can take over a limited
number of user profiles in a social network with the aim of maximizing
disagreement and/or polarization in the network.
  We investigate this model both theoretically and empirically. We show that
the adversary will always change the opinion of a taken-over profile to an
extreme in order to maximize disruption. We also prove that an adversary can
increase disagreement / polarization at most linearly in the number of user
profiles it takes over. Furthermore, we present a detailed empirical study of
several natural algorithms for the adversary on both synthetic networks and
real world (Reddit and Twitter) data sets. These show that even simple,
unsophisticated heuristics, such as targeting centrists, can disrupt a network
effectively, causing a large increase in disagreement / polarization. Studying
the problem of network disruption through the lens of an adversary thus
highlights the seriousness of the problem.
</summary>
    <author>
      <name>Mayee F. Chen</name>
    </author>
    <author>
      <name>Miklos Z. Racz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.08377v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08377v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11459v1</id>
    <updated>2020-03-23T23:43:02Z</updated>
    <published>2020-03-23T23:43:02Z</published>
    <title>BaitWatcher: A lightweight web interface for the detection of
  incongruent news headlines</title>
    <summary>  In digital environments where substantial amounts of information are shared
online, news headlines play essential roles in the selection and diffusion of
news articles. Some news articles attract audience attention by showing
exaggerated or misleading headlines. This study addresses the \textit{headline
incongruity} problem, in which a news headline makes claims that are either
unrelated or opposite to the contents of the corresponding article. We present
\textit{BaitWatcher}, which is a lightweight web interface that guides readers
in estimating the likelihood of incongruence in news articles before clicking
on the headlines. BaitWatcher utilizes a hierarchical recurrent encoder that
efficiently learns complex textual representations of a news headline and its
associated body text. For training the model, we construct a million scale
dataset of news articles, which we also release for broader research use. Based
on the results of a focus group interview, we discuss the importance of
developing an interpretable AI agent for the design of a better interface for
mitigating the effects of online misinformation.
</summary>
    <author>
      <name>Kunwoo Park</name>
    </author>
    <author>
      <name>Taegyun Kim</name>
    </author>
    <author>
      <name>Seunghyun Yoon</name>
    </author>
    <author>
      <name>Meeyoung Cha</name>
    </author>
    <author>
      <name>Kyomin Jung</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-42699-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-42699-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages (single column), 7 figures. This research article is
  published as a book chapter of \textit{Fake News, Disinformation, and
  Misinformation in Social Media-Emerging Research Challenges and
  Opportunities}. Springer, 2020. arXiv admin note: text overlap with
  arXiv:1811.07066</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.11459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.07996v2</id>
    <updated>2021-04-09T08:52:10Z</updated>
    <published>2020-07-15T21:18:30Z</published>
    <title>Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective
  and a Call to Arms</title>
    <summary>  With the outbreak of the COVID-19 pandemic, people turned to social media to
read and to share timely information including statistics, warnings, advice,
and inspirational stories. Unfortunately, alongside all this useful
information, there was also a new blending of medical and political
misinformation and disinformation, which gave rise to the first global
infodemic. While fighting this infodemic is typically thought of in terms of
factuality, the problem is much broader as malicious content includes not only
fake news, rumors, and conspiracy theories, but also promotion of fake cures,
panic, racism, xenophobia, and mistrust in the authorities, among others. This
is a complex problem that needs a holistic approach combining the perspectives
of journalists, fact-checkers, policymakers, government entities, social media
platforms, and society as a whole. Taking them into account we define an
annotation schema and detailed annotation instructions, which reflect these
perspectives. We performed initial annotations using this schema, and our
initial experiments demonstrated sizable improvements over the baselines. Now,
we issue a call to arms to the research community and beyond to join the fight
by supporting our crowdsourcing annotation efforts.
</summary>
    <author>
      <name>Firoj Alam</name>
    </author>
    <author>
      <name>Fahim Dalvi</name>
    </author>
    <author>
      <name>Shaden Shaar</name>
    </author>
    <author>
      <name>Nadir Durrani</name>
    </author>
    <author>
      <name>Hamdy Mubarak</name>
    </author>
    <author>
      <name>Alex Nikolov</name>
    </author>
    <author>
      <name>Giovanni Da San Martino</name>
    </author>
    <author>
      <name>Ahmed Abdelali</name>
    </author>
    <author>
      <name>Hassan Sajjad</name>
    </author>
    <author>
      <name>Kareem Darwish</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COVID-19, Infodemic, Disinformation, Misinformation, Fake News, Call
  to Arms, Crowdsourcing Annotations</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.07996v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.07996v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.02931v1</id>
    <updated>2020-09-07T08:03:21Z</updated>
    <published>2020-09-07T08:03:21Z</published>
    <title>Team Alex at CLEF CheckThat! 2020: Identifying Check-Worthy Tweets With
  Transformer Models</title>
    <summary>  While misinformation and disinformation have been thriving in social media
for years, with the emergence of the COVID-19 pandemic, the political and the
health misinformation merged, thus elevating the problem to a whole new level
and giving rise to the first global infodemic. The fight against this infodemic
has many aspects, with fact-checking and debunking false and misleading claims
being among the most important ones. Unfortunately, manual fact-checking is
time-consuming and automatic fact-checking is resource-intense, which means
that we need to pre-filter the input social media posts and to throw out those
that do not appear to be check-worthy. With this in mind, here we propose a
model for detecting check-worthy tweets about COVID-19, which combines deep
contextualized text representations with modeling the social context of the
tweet. We further describe a number of additional experiments and comparisons,
which we believe should be useful for future research as they provide some
indication about what techniques are effective for the task. Our official
submission to the English version of CLEF-2020 CheckThat! Task 1, system
Team_Alex, was ranked second with a MAP score of 0.8034, which is almost tied
with the wining system, lagging behind by just 0.003 MAP points absolute.
</summary>
    <author>
      <name>Alex Nikolov</name>
    </author>
    <author>
      <name>Giovanni Da San Martino</name>
    </author>
    <author>
      <name>Ivan Koychev</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Check-worthiness; Fact-Checking; Veracity</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CLEF-2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.02931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.08924v3</id>
    <updated>2021-04-13T08:38:02Z</updated>
    <published>2021-02-17T18:30:43Z</published>
    <title>Cross-SEAN: A Cross-Stitch Semi-Supervised Neural Attention Model for
  COVID-19 Fake News Detection</title>
    <summary>  As the COVID-19 pandemic sweeps across the world, it has been accompanied by
a tsunami of fake news and misinformation on social media. At the time when
reliable information is vital for public health and safety, COVID-19 related
fake news has been spreading even faster than the facts. During times such as
the COVID-19 pandemic, fake news can not only cause intellectual confusion but
can also place lives of people at risk. This calls for an immediate need to
contain the spread of such misinformation on social media. We introduce CTF,
the first COVID-19 Twitter fake news dataset with labeled genuine and fake
tweets. Additionally, we propose Cross-SEAN, a cross-stitch based
semi-supervised end-to-end neural attention model, which leverages the large
amount of unlabelled data. Cross-SEAN partially generalises to emerging fake
news as it learns from relevant external knowledge. We compare Cross-SEAN with
seven state-of-the-art fake news detection methods. We observe that it achieves
$0.95$ F1 Score on CTF, outperforming the best baseline by $9\%$. We also
develop Chrome-SEAN, a Cross-SEAN based chrome extension for real-time
detection of fake tweets.
</summary>
    <author>
      <name>William Scott Paka</name>
    </author>
    <author>
      <name>Rachit Bansal</name>
    </author>
    <author>
      <name>Abhay Kaushik</name>
    </author>
    <author>
      <name>Shubhashis Sengupta</name>
    </author>
    <author>
      <name>Tanmoy Chakraborty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The Journal of Applied Soft Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.08924v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08924v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.12541v2</id>
    <updated>2022-09-28T18:41:47Z</updated>
    <published>2021-03-13T18:04:17Z</published>
    <title>A Survey on Multimodal Disinformation Detection</title>
    <summary>  Recent years have witnessed the proliferation of offensive content online
such as fake news, propaganda, misinformation, and disinformation. While
initially this was mostly about textual content, over time images and videos
gained popularity, as they are much easier to consume, attract more attention,
and spread further than text. As a result, researchers started leveraging
different modalities and combinations thereof to tackle online multimodal
offensive content. In this study, we offer a survey on the state-of-the-art on
multimodal disinformation detection covering various combinations of
modalities: text, images, speech, video, social media network structure, and
temporal information. Moreover, while some studies focused on factuality,
others investigated how harmful the content is. While these two components in
the definition of disinformation (i) factuality, and (ii) harmfulness, are
equally important, they are typically studied in isolation. Thus, we argue for
the need to tackle disinformation detection by taking into account multiple
modalities as well as both factuality and harmfulness, in the same framework.
Finally, we discuss current challenges and future research directions
</summary>
    <author>
      <name>Firoj Alam</name>
    </author>
    <author>
      <name>Stefano Cresci</name>
    </author>
    <author>
      <name>Tanmoy Chakraborty</name>
    </author>
    <author>
      <name>Fabrizio Silvestri</name>
    </author>
    <author>
      <name>Dimiter Dimitrov</name>
    </author>
    <author>
      <name>Giovanni Da San Martino</name>
    </author>
    <author>
      <name>Shaden Shaar</name>
    </author>
    <author>
      <name>Hamed Firooz</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at COLING-2022, disinformation, misinformation, factuality,
  harmfulness, fake news, propaganda, multimodality, text, images, videos,
  network structure, temporality</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.12541v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.12541v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.13385v1</id>
    <updated>2021-06-25T01:49:45Z</updated>
    <published>2021-06-25T01:49:45Z</published>
    <title>Trends, Politics, Sentiments, and Misinformation: Understanding People's
  Reactions to COVID-19 During its Early Stages</title>
    <summary>  The sudden outbreak of COVID-19 resulted in large volumes of data shared on
different social media platforms. Analyzing and visualizing these data is
doubtlessly essential to having a deep understanding of the pandemic's impacts
on people's lives and their reactions to them. In this work, we conduct a
large-scale spatiotemporal data analytic study to understand peoples' reactions
to the COVID-19 pandemic during its early stages. In particular, we analyze a
JSON-based dataset that is collected from news/messages/boards/blogs in English
about COVID-19 over a period of 4 months, for a total of 5.2M posts. The data
are collected from December 2019 to March 2020 from several social media
platforms such as Facebook, LinkedIn, Pinterest, StumbleUpon and VK. Our study
aims mainly to understand which implications of COVID-19 have interested social
media users the most and how did they vary over time, the spatiotemporal
distribution of misinformation, and the public opinion toward public figures
during the pandemic. Our results can be used by many parties (e.g.,
governments, psychologists, etc.) to make more informative decisions, taking
into account the actual interests and opinions of the people.
</summary>
    <author>
      <name>Omar Abdel Wahab</name>
    </author>
    <author>
      <name>Ali Mustafa</name>
    </author>
    <author>
      <name>Andr√© Bertrand Abisseck Bamatakina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.13385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.00230v1</id>
    <updated>2021-10-01T06:38:19Z</updated>
    <published>2021-10-01T06:38:19Z</published>
    <title>Users' ability to perceive misinformation: An information quality
  assessment approach</title>
    <summary>  Digital information exchange enables quick creation and sharing of
information and thus changes existing habits. Social media is becoming the main
source of news for end-users replacing traditional media. This also enables the
proliferation of fake news, which misinforms readers and is used to serve the
interests of the creators. As a result, automated fake news detection systems
are attracting attention. However, automatic fake news detection presents a
major challenge; content evaluation is increasingly becoming the responsibility
of the end-user. Thus, in the present study we used information quality (IQ) as
an instrument to investigate how users can detect fake news. Specifically, we
examined how users perceive fake news in the form of shorter paragraphs on
individual IQ dimensions. We also investigated which user characteristics might
affect fake news detection. We performed an empirical study with 1123 users,
who evaluated randomly generated stories with statements of various level of
correctness by individual IQ dimensions. The results reveal that IQ can be used
as a tool for fake news detection. Our findings show that (1) domain knowledge
has a positive impact on fake news detection; (2) education in combination with
domain knowledge improves fake news detection; and (3) personality trait
conscientiousness contributes significantly to fake news detection in all
dimensions.
</summary>
    <author>
      <name>Alja≈æ Zrnec</name>
    </author>
    <author>
      <name>Marko Po≈æenel</name>
    </author>
    <author>
      <name>Dejan Lavbiƒç</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipm.2021.102739</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipm.2021.102739" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Processing &amp; Management 59 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.00230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09234v1</id>
    <updated>2021-10-07T16:05:10Z</updated>
    <published>2021-10-07T16:05:10Z</published>
    <title>Impact of COVID-19 Policies and Misinformation on Social Unrest</title>
    <summary>  The novel coronavirus disease (COVID-19) pandemic has impacted every corner
of earth, disrupting governments and leading to socioeconomic instability. This
crisis has prompted questions surrounding how different sectors of society
interact and influence each other during times of change and stress. Given the
unprecedented economic and societal impacts of this pandemic, many new data
sources have become available, allowing us to quantitatively explore these
associations. Understanding these relationships can help us better prepare for
future disasters and mitigate the impacts. Here, we focus on the interplay
between social unrest (protests), health outcomes, public health orders, and
misinformation in eight countries of Western Europe and four regions of the
United States. We created 1-3 week forecasts of both a binary protest metric
for identifying times of high protest activity and the overall protest counts
over time. We found that for all regions, except Belgium, at least one feature
from our various data streams was predictive of protests. However, the accuracy
of the protest forecasts varied by country, that is, for roughly half of the
countries analyzed, our forecasts outperform a na\"ive model. These mixed
results demonstrate the potential of diverse data streams to predict a topic as
volatile as protests as well as the difficulties of predicting a situation that
is as rapidly evolving as a pandemic.
</summary>
    <author>
      <name>Martha Barnard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">A-1 Information Systems and Modeling, Los Alamos National Lab, Los Alamos, NM, USA</arxiv:affiliation>
    </author>
    <author>
      <name>Radhika Iyer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">A-1 Information Systems and Modeling, Los Alamos National Lab, Los Alamos, NM, USA</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Political Science and Department of Computing, Data Science, and Society, University of California, Berkeley, Berkeley, CA, USA</arxiv:affiliation>
    </author>
    <author>
      <name>Sara Y. Del Valle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">A-1 Information Systems and Modeling, Los Alamos National Lab, Los Alamos, NM, USA</arxiv:affiliation>
    </author>
    <author>
      <name>Ashlynn R. Daughton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">A-1 Information Systems and Modeling, Los Alamos National Lab, Los Alamos, NM, USA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.09234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.13090v1</id>
    <updated>2021-10-25T16:35:58Z</updated>
    <published>2021-10-25T16:35:58Z</published>
    <title>SciClops: Detecting and Contextualizing Scientific Claims for Assisting
  Manual Fact-Checking</title>
    <summary>  This paper describes SciClops, a method to help combat online scientific
misinformation. Although automated fact-checking methods have gained
significant attention recently, they require pre-existing ground-truth
evidence, which, in the scientific context, is sparse and scattered across a
constantly-evolving scientific literature. Existing methods do not exploit this
literature, which can effectively contextualize and combat science-related
fallacies. Furthermore, these methods rarely require human intervention, which
is essential for the convoluted and critical domain of scientific
misinformation. SciClops involves three main steps to process scientific claims
found in online news articles and social media postings: extraction,
clustering, and contextualization. First, the extraction of scientific claims
takes place using a domain-specific, fine-tuned transformer model. Second,
similar claims extracted from heterogeneous sources are clustered together with
related scientific literature using a method that exploits their content and
the connections among them. Third, check-worthy claims, broadcasted by popular
yet unreliable sources, are highlighted together with an enhanced fact-checking
context that includes related verified claims, news articles, and scientific
papers. Extensive experiments show that SciClops tackles sufficiently these
three steps, and effectively assists non-expert fact-checkers in the
verification of complex scientific claims, outperforming commercial
fact-checking systems.
</summary>
    <author>
      <name>Panayiotis Smeros</name>
    </author>
    <author>
      <name>Carlos Castillo</name>
    </author>
    <author>
      <name>Karl Aberer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3459637.3482475</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3459637.3482475" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 30th ACM International Conference on Information
  and Knowledge Management (CIKM '21). November 1-5, 2021. QLD, Australia</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.13090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.13090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00061v3</id>
    <updated>2022-03-20T12:55:44Z</updated>
    <published>2021-11-30T19:36:20Z</published>
    <title>Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context
  Images via Online Resources</title>
    <summary>  Misinformation is now a major problem due to its potential high risks to our
core democratic and societal values and orders. Out-of-context misinformation
is one of the easiest and effective ways used by adversaries to spread viral
false stories. In this threat, a real image is re-purposed to support other
narratives by misrepresenting its context and/or elements. The internet is
being used as the go-to way to verify information using different sources and
modalities. Our goal is an inspectable method that automates this
time-consuming and reasoning-intensive process by fact-checking the
image-caption pairing using Web evidence. To integrate evidence and cues from
both modalities, we introduce the concept of 'multi-modal cycle-consistency
check'; starting from the image/caption, we gather textual/visual evidence,
which will be compared against the other paired caption/image, respectively.
Moreover, we propose a novel architecture, Consistency-Checking Network (CCN),
that mimics the layered human reasoning across the same and different
modalities: the caption vs. textual evidence, the image vs. visual evidence,
and the image vs. caption. Our work offers the first step and benchmark for
open-domain, content-based, multi-modal fact-checking, and significantly
outperforms previous baselines that did not leverage external evidence.
</summary>
    <author>
      <name>Sahar Abdelnabi</name>
    </author>
    <author>
      <name>Rakibul Hasan</name>
    </author>
    <author>
      <name>Mario Fritz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.00061v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00061v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07230v1</id>
    <updated>2021-12-14T08:36:47Z</updated>
    <published>2021-12-14T08:36:47Z</published>
    <title>Do you trust experts on Twitter?: Successful correction of
  COVID-19-related misinformation</title>
    <summary>  This study focuses on how scientifically-correct information is disseminated
through social media, and how misinformation can be corrected. We have
identified examples on Twitter where scientific terms that have been misused
have been rectified and replaced by scientifically-correct terms through the
interaction of users. The results show that the percentage of correct terms
("variant" or "COVID-19 variant") being used instead of the incorrect terms
("strain") on Twitter has already increased since the end of December 2020.
This was about a month before the release of an official statement by the
Japanese Association for Infectious Diseases regarding the correct terminology,
and the use of terms on social media was faster than it was in television. Some
Twitter users who quickly started using the correct term were more likely to
retweet messages sent by leading influencers on Twitter, rather than messages
sent by traditional media or portal sites. However, a few Twitter users
continued to use wrong terms even after March 2021, even though the use of the
correct terms was widespread. Further analysis of their tweets revealed that
they were quoting sources that differed from that of other users. This study
empirically verified that self-correction occurs even on Twitter, which is
often known as a "hotbed for spreading rumors." The results of this study also
suggest that influencers with expertise can influence the direction of public
opinion on social media and that the media that users usually cite can also
affect the possibility of behavioral changes.
</summary>
    <author>
      <name>Dongwoo Lim</name>
    </author>
    <author>
      <name>Fujio Toriumi</name>
    </author>
    <author>
      <name>Mitsuo Yoshida</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3486622.3493982</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3486622.3493982" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 20th IEEE/WIC/ACM International Joint Conference on Web
  Intelligence and Intelligent Agent Technology (WI-IAT '21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.07230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.11261v2</id>
    <updated>2022-05-12T13:45:34Z</updated>
    <published>2022-03-21T18:43:40Z</published>
    <title>Healthy Twitter discussions? Time will tell</title>
    <summary>  Studying misinformation and how to deal with unhealthy behaviours within
online discussions has recently become an important field of research within
social studies. With the rapid development of social media, and the increasing
amount of available information and sources, rigorous manual analysis of such
discourses has become unfeasible. Many approaches tackle the issue by studying
the semantic and syntactic properties of discussions following a supervised
approach, for example using natural language processing on a dataset labeled
for abusive, fake or bot-generated content. Solutions based on the existence of
a ground truth are limited to those domains which may have ground truth.
However, within the context of misinformation, it may be difficult or even
impossible to assign labels to instances. In this context, we consider the use
of temporal dynamic patterns as an indicator of discussion health. Working in a
domain for which ground truth was unavailable at the time (early COVID-19
pandemic discussions) we explore the characterization of discussions based on
the the volume and time of contributions. First we explore the types of
discussions in an unsupervised manner, and then characterize these types using
the concept of ephemerality, which we formalize. In the end, we discuss the
potential use of our ephemerality definition for labeling online discourses
based on how desirable, healthy and constructive they are.
</summary>
    <author>
      <name>Dmitry Gnatyshak</name>
    </author>
    <author>
      <name>Dario Garcia-Gasulla</name>
    </author>
    <author>
      <name>Sergio Alvarez-Napagao</name>
    </author>
    <author>
      <name>Jamie Arjona</name>
    </author>
    <author>
      <name>Tommaso Venturini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. Related to the SoBigData++ project:
  https://plusplus.sobigdata.eu/</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.11261v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.11261v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10814v3</id>
    <updated>2023-06-14T15:51:37Z</updated>
    <published>2022-08-23T08:59:32Z</published>
    <title>From alternative conceptions of honesty to alternative facts in
  communications by U.S. politicians</title>
    <summary>  The spread of online misinformation on social media is increasingly perceived
as a problem for societal cohesion and democracy. The role of political leaders
in this process has attracted less research attention, even though politicians
who "speak their mind" are perceived by segments of the public as authentic and
honest even if their statements are unsupported by evidence. Analyzing
communications by members of the U.S. Congress on Twitter between 2011 and
2022, we show that politicians' conception of honesty has undergone a distinct
shift, with authentic belief-speaking that may be decoupled from evidence
becoming more prominent and more differentiated from explicitly evidence-based
truth seeking. We show that for Republicans - but not Democrats - an increase
of belief-speaking of 10% is associated with a decrease of 12.8 points of
quality (NewsGuard scoring system) in the sources shared in a tweet.
Conversely, an increase in truth-seeking language is associated with an
increase in quality of sources for both parties. The results support the
hypothesis that the current dissemination of misinformation in political
discourse is in part driven by an alternative understanding of truth and
honesty that emphasizes invocation of subjective belief at the expense of
reliance on evidence.
</summary>
    <author>
      <name>Jana Lasser</name>
    </author>
    <author>
      <name>Segun Taofeek Aroyehun</name>
    </author>
    <author>
      <name>Fabio Carrella</name>
    </author>
    <author>
      <name>Almog Simchon</name>
    </author>
    <author>
      <name>David Garcia</name>
    </author>
    <author>
      <name>Stephan Lewandowsky</name>
    </author>
    <link href="http://arxiv.org/abs/2208.10814v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10814v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.07245v1</id>
    <updated>2022-09-15T12:32:15Z</updated>
    <published>2022-09-15T12:32:15Z</published>
    <title>Efficient first-order predictor-corrector multiple objective
  optimization for fair misinformation detection</title>
    <summary>  Multiple-objective optimization (MOO) aims to simultaneously optimize
multiple conflicting objectives and has found important applications in machine
learning, such as minimizing classification loss and discrepancy in treating
different populations for fairness. At optimality, further optimizing one
objective will necessarily harm at least another objective, and decision-makers
need to comprehensively explore multiple optima (called Pareto front) to
pinpoint one final solution. We address the efficiency of finding the Pareto
front. First, finding the front from scratch using stochastic multi-gradient
descent (SMGD) is expensive with large neural networks and datasets. We propose
to explore the Pareto front as a manifold from a few initial optima, based on a
predictor-corrector method. Second, for each exploration step, the predictor
solves a large-scale linear system that scales quadratically in the number of
model parameters and requires one backpropagation to evaluate a second-order
Hessian-vector product per iteration of the solver. We propose a Gauss-Newton
approximation that only scales linearly, and that requires only first-order
inner-product per iteration. This also allows for a choice between the MINRES
and conjugate gradient methods when approximately solving the linear system.
The innovations make predictor-corrector possible for large networks.
Experiments on multi-objective (fairness and accuracy) misinformation detection
tasks show that 1) the predictor-corrector method can find Pareto fronts better
than or similar to SMGD with less time; and 2) the proposed first-order method
does not harm the quality of the Pareto front identified by the second-order
method, while further reduce running time.
</summary>
    <author>
      <name>Eric Enouen</name>
    </author>
    <author>
      <name>Katja Mathesius</name>
    </author>
    <author>
      <name>Sean Wang</name>
    </author>
    <author>
      <name>Arielle Carr</name>
    </author>
    <author>
      <name>Sihong Xie</name>
    </author>
    <link href="http://arxiv.org/abs/2209.07245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.07245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.00160v1</id>
    <updated>2022-10-01T01:39:08Z</updated>
    <published>2022-10-01T01:39:08Z</published>
    <title>Explaining Website Reliability by Visualizing Hyperlink Connectivity</title>
    <summary>  As the information on the Internet continues growing exponentially,
understanding and assessing the reliability of a website is becoming
increasingly important. Misinformation has far-ranging repercussions, from
sowing mistrust in media to undermining democratic elections. While some
research investigates how to alert people to misinformation on the web, much
less research has been conducted on explaining how websites engage in spreading
false information. To fill the research gap, we present MisVis, a web-based
interactive visualization tool that helps users assess a website's reliability
by understanding how it engages in spreading false information on the World
Wide Web. MisVis visualizes the hyperlink connectivity of the website and
summarizes key characteristics of the Twitter accounts that mention the site. A
large-scale user study with 139 participants demonstrates that MisVis
facilitates users to assess and understand false information on the web and
node-link diagrams can be used to communicate with non-experts. MisVis is
available at the public demo link: https://poloclub.github.io/MisVis.
</summary>
    <author>
      <name>Seongmin Lee</name>
    </author>
    <author>
      <name>Sadia Afroz</name>
    </author>
    <author>
      <name>Haekyu Park</name>
    </author>
    <author>
      <name>Zijie J. Wang</name>
    </author>
    <author>
      <name>Omar Shaikh</name>
    </author>
    <author>
      <name>Vibhor Sehgal</name>
    </author>
    <author>
      <name>Ankit Peshin</name>
    </author>
    <author>
      <name>Duen Horng Chau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IEEE VIS 2022, 5 pages, 4 figures, For a live demo, visit
  https://poloclub.github.io/MisVis</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.00160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.00160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04710v2</id>
    <updated>2022-10-11T12:00:06Z</updated>
    <published>2022-10-10T14:08:46Z</published>
    <title>Empowering the Fact-checkers! Automatic Identification of Claim Spans on
  Twitter</title>
    <summary>  The widespread diffusion of medical and political claims in the wake of
COVID-19 has led to a voluminous rise in misinformation and fake news. The
current vogue is to employ manual fact-checkers to efficiently classify and
verify such data to combat this avalanche of claim-ridden misinformation.
However, the rate of information dissemination is such that it vastly outpaces
the fact-checkers' strength. Therefore, to aid manual fact-checkers in
eliminating the superfluous content, it becomes imperative to automatically
identify and extract the snippets of claim-worthy (mis)information present in a
post. In this work, we introduce the novel task of Claim Span Identification
(CSI). We propose CURT, a large-scale Twitter corpus with token-level claim
spans on more than 7.5k tweets. Furthermore, along with the standard token
classification baselines, we benchmark our dataset with DABERTa, an
adapter-based variation of RoBERTa. The experimental results attest that
DABERTa outperforms the baseline systems across several evaluation metrics,
improving by about 1.5 points. We also report detailed error analysis to
validate the model's performance along with the ablation studies. Lastly, we
release our comprehensive span annotation guidelines for public use.
</summary>
    <author>
      <name>Megha Sundriyal</name>
    </author>
    <author>
      <name>Atharva Kulkarni</name>
    </author>
    <author>
      <name>Vaibhav Pulastya</name>
    </author>
    <author>
      <name>Md Shad Akhtar</name>
    </author>
    <author>
      <name>Tanmoy Chakraborty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EMNLP22. 16 pages including Appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.04710v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.04710v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.09014v2</id>
    <updated>2023-04-13T13:07:26Z</updated>
    <published>2022-10-05T17:34:51Z</published>
    <title>Addressing contingency in algorithmic (mis)information classification:
  Toward a responsible machine learning agenda</title>
    <summary>  Machine learning (ML) enabled classification models are becoming increasingly
popular for tackling the sheer volume and speed of online misinformation and
other content that could be identified as harmful. In building these models,
data scientists need to take a stance on the legitimacy, authoritativeness and
objectivity of the sources of ``truth" used for model training and testing.
This has political, ethical and epistemic implications which are rarely
addressed in technical papers. Despite (and due to) their reported high
accuracy and performance, ML-driven moderation systems have the potential to
shape online public debate and create downstream negative impacts such as undue
censorship and the reinforcing of false beliefs. Using collaborative
ethnography and theoretical insights from social studies of science and
expertise, we offer a critical analysis of the process of building ML models
for (mis)information classification: we identify a series of algorithmic
contingencies--key moments during model development that could lead to
different future outcomes, uncertainty and harmful effects as these tools are
deployed by social media platforms. We conclude by offering a tentative path
toward reflexive and responsible development of ML tools for moderating
misinformation and other harmful content online.
</summary>
    <author>
      <name>Andr√©s Dom√≠nguez Hern√°ndez</name>
    </author>
    <author>
      <name>Richard Owen</name>
    </author>
    <author>
      <name>Dan Saattrup Nielsen</name>
    </author>
    <author>
      <name>Ryan McConville</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/23299460.2023.2222514</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/23299460.2023.2222514" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Andr\'es Dom\'inguez Hern\'andez, Richard Owen, Dan Saattrup Nielsen
  and Ryan McConville. 2023. Addressing contingency in algorithmic
  (mis)information classification: Toward a responsible machine learning
  agenda. Accepted in 2023 ACM Conference on Fairness, Accountability, and
  Transparency (FAccT '23), June 12-15, 2023, Chicago, United States of
  America. ACM, New York, NY, USA, 16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.09014v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.09014v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.13638v6</id>
    <updated>2023-09-15T23:45:21Z</updated>
    <published>2022-12-27T23:09:44Z</published>
    <title>Battling the Coronavirus Infodemic Among Social Media Users in Kenya and
  Nigeria</title>
    <summary>  How can we induce social media users to be discerning when sharing
information during a pandemic? An experiment on Facebook Messenger with users
from Kenya (n = 7,498) and Nigeria (n = 7,794) tested interventions designed to
decrease intentions to share COVID-19 misinformation without decreasing
intentions to share factual posts. The initial stage of the study incorporated:
(i) a factorial design with 40 intervention combinations; and (ii) a contextual
adaptive design, increasing the probability of assignment to treatments that
worked better for previous subjects with similar characteristics. The second
stage evaluated the best-performing treatments and a targeted treatment
assignment policy estimated from the data. We precisely estimate null effects
from warning flags and related article suggestions, tactics used by social
media platforms. However, nudges to consider information's accuracy reduced
misinformation sharing relative to control by 4.9% (estimate = -2.3 pp, s.e. =
1.0 , Z = -2.31, p = 0.021, 95% CI = [-4.2 , -0.35]). Such low-cost scalable
interventions may improve the quality of information circulating online.
</summary>
    <author>
      <name>Molly Offer-Westort</name>
    </author>
    <author>
      <name>Leah R. Rosenzweig</name>
    </author>
    <author>
      <name>Susan Athey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">52 pages including appendix, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.13638v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.13638v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00453v1</id>
    <updated>2023-01-01T18:06:26Z</updated>
    <published>2023-01-01T18:06:26Z</published>
    <title>Investigating the Dynamics of Social Norm Emergence within Online
  Communities</title>
    <summary>  Although the effects of the social norm on mitigating misinformation are
identified, scant knowledge exists about the patterns of social norm emergence,
such as the patterns and variations of social tipping in online communities
with diverse characteristics. Accordingly, this study investigates the features
of social tipping in online communities and examines the correlations between
the tipping features and characteristics of online communities. Taking the side
effects of COVID-19 vaccination as the case topic, we first track the patterns
of tipping features in 100 online communities, which are detected using Louvain
Algorithm from the aggregated communication network on Twitter between May 2020
and April 2021. Then, we use multi-variant linear regression to explore the
correlations between tipping features and community characteristics. We find
that social tipping in online communities can sustain for two to four months
and lead to a 50% increase in populations who accept the normative belief in
online communities. The regression indicates that the duration of social
tipping is positively related to the community populations and original
acceptance of social norms, while the correlation between the tipping duration
and the degrees among community members is negative. Additionally, the network
modularity and original acceptance of social norms have negative relationships
with the extent of social tipping, while the degree and betweenness centrality
can have significant positive relationships with the extent of tipping. Our
findings shed light on more precise normative interventions on misinformation
in digital environments as it offers preliminary evidence about the timing and
mechanism of social norm emergence.
</summary>
    <author>
      <name>Shangde Gao</name>
    </author>
    <author>
      <name>Yan Wang</name>
    </author>
    <author>
      <name>My T. Thai</name>
    </author>
    <link href="http://arxiv.org/abs/2301.00453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.10710v1</id>
    <updated>2023-01-25T17:22:08Z</updated>
    <published>2023-01-25T17:22:08Z</published>
    <title>Powering an AI Chatbot with Expert Sourcing to Support Credible Health
  Information Access</title>
    <summary>  During a public health crisis like the COVID-19 pandemic, a credible and
easy-to-access information portal is highly desirable. It helps with disease
prevention, public health planning, and misinformation mitigation. However,
creating such an information portal is challenging because 1) domain expertise
is required to identify and curate credible and intelligible content, 2) the
information needs to be updated promptly in response to the fast-changing
environment, and 3) the information should be easily accessible by the general
public; which is particularly difficult when most people do not have the domain
expertise about the crisis. In this paper, we presented an expert-sourcing
framework and created Jennifer, an AI chatbot, which serves as a credible and
easy-to-access information portal for individuals during the COVID-19 pandemic.
Jennifer was created by a team of over 150 scientists and health professionals
around the world, deployed in the real world and answered thousands of user
questions about COVID-19. We evaluated Jennifer from two key stakeholders'
perspectives, expert volunteers and information seekers. We first interviewed
experts who contributed to the collaborative creation of Jennifer to learn
about the challenges in the process and opportunities for future improvement.
We then conducted an online experiment that examined Jennifer's effectiveness
in supporting information seekers in locating COVID-19 information and gaining
their trust. We share the key lessons learned and discuss design implications
for building expert-sourced and AI-powered information portals, along with the
risks and opportunities of misinformation mitigation and beyond.
</summary>
    <author>
      <name>Ziang Xiao</name>
    </author>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Michelle X. Zhou</name>
    </author>
    <author>
      <name>Tyrone Grandison</name>
    </author>
    <author>
      <name>Yunyao Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Forthcoming for IUI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.10710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.03782v2</id>
    <updated>2023-02-09T17:18:31Z</updated>
    <published>2023-02-07T22:41:37Z</published>
    <title>Does AI-Assisted Fact-Checking Disproportionately Benefit Majority
  Groups Online?</title>
    <summary>  In recent years, algorithms have been incorporated into fact-checking
pipelines. They are used not only to flag previously fact-checked
misinformation, but also to provide suggestions about which trending claims
should be prioritized for fact-checking - a paradigm called `check-worthiness.'
While several studies have examined the accuracy of these algorithms, none have
investigated how the benefits from these algorithms (via reduction in exposure
to misinformation) are distributed amongst various online communities. In this
paper, we investigate how diverse representation across multiple stages of the
AI development pipeline affects the distribution of benefits from AI-assisted
fact-checking for different online communities. We simulate information
propagation through the network using our novel Topic-Aware, Community-Impacted
Twitter (TACIT) simulator on a large Twitter followers network, tuned to
produce realistic cascades of true and false information across multiple
topics. Finally, using simulated data as a test bed, we implement numerous
algorithmic fact-checking interventions that explicitly account for notions of
diversity. We find that both representative and egalitarian methods for
sampling and labeling check-worthiness model training data can lead to
network-wide benefit concentrated in majority communities, while incorporating
diversity into how fact-checkers use algorithmic recommendations can actively
reduce inequalities in benefits between majority and minority communities.
These findings contribute to an important conversation around the responsible
implementation of AI-assisted fact-checking by social media platforms and
fact-checking organizations.
</summary>
    <author>
      <name>Terrence Neumann</name>
    </author>
    <author>
      <name>Nicholas Wolczynski</name>
    </author>
    <link href="http://arxiv.org/abs/2302.03782v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.03782v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.07363v3</id>
    <updated>2023-04-27T19:39:43Z</updated>
    <published>2023-02-14T21:51:56Z</published>
    <title>Attacking Fake News Detectors via Manipulating News Social Engagement</title>
    <summary>  Social media is one of the main sources for news consumption, especially
among the younger generation. With the increasing popularity of news
consumption on various social media platforms, there has been a surge of
misinformation which includes false information or unfounded claims. As various
text- and social context-based fake news detectors are proposed to detect
misinformation on social media, recent works start to focus on the
vulnerabilities of fake news detectors. In this paper, we present the first
adversarial attack framework against Graph Neural Network (GNN)-based fake news
detectors to probe their robustness. Specifically, we leverage a multi-agent
reinforcement learning (MARL) framework to simulate the adversarial behavior of
fraudsters on social media. Research has shown that in real-world settings,
fraudsters coordinate with each other to share different news in order to evade
the detection of fake news detectors. Therefore, we modeled our MARL framework
as a Markov Game with bot, cyborg, and crowd worker agents, which have their
own distinctive cost, budget, and influence. We then use deep Q-learning to
search for the optimal policy that maximizes the rewards. Extensive
experimental results on two real-world fake news propagation datasets
demonstrate that our proposed framework can effectively sabotage the GNN-based
fake news detector performance. We hope this paper can provide insights for
future research on fake news detection.
</summary>
    <author>
      <name>Haoran Wang</name>
    </author>
    <author>
      <name>Yingtong Dou</name>
    </author>
    <author>
      <name>Canyu Chen</name>
    </author>
    <author>
      <name>Lichao Sun</name>
    </author>
    <author>
      <name>Philip S. Yu</name>
    </author>
    <author>
      <name>Kai Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Web Conference 2023 (WWW'23)</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.07363v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.07363v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.13529v1</id>
    <updated>2023-02-27T05:49:30Z</updated>
    <published>2023-02-27T05:49:30Z</published>
    <title>Minimizing the Influence of Misinformation via Vertex Blocking</title>
    <summary>  Information cascade in online social networks can be rather negative, e.g.,
the spread of rumors may trigger panic. To limit the influence of
misinformation in an effective and efficient manner, the influence minimization
(IMIN) problem is studied in the literature: given a graph G and a seed set S,
blocking at most b vertices such that the influence spread of the seed set is
minimized. In this paper, we are the first to prove the IMIN problem is NP-hard
and hard to approximate. Due to the hardness of the problem, existing works
resort to greedy solutions and use Monte-Carlo Simulations to solve the
problem. However, they are cost-prohibitive on large graphs since they have to
enumerate all the candidate blockers and compute the decrease of expected
spread when blocking each of them. To improve the efficiency, we propose the
AdvancedGreedy algorithm (AG) based on a new graph sampling technique that
applies the dominator tree structure, which can compute the decrease of the
expected spread of all candidate blockers at once. Besides, we further propose
the GreedyReplace algorithm (GR) by considering the relationships among
candidate blockers. Extensive experiments on 8 real-life graphs demonstrate
that our AG and GR algorithms are significantly faster than the
state-of-the-art by up to 6 orders of magnitude, and GR can achieve better
effectiveness with its time cost close to AG.
</summary>
    <author>
      <name>Jiadong Xie</name>
    </author>
    <author>
      <name>Fan Zhang</name>
    </author>
    <author>
      <name>Kai Wang</name>
    </author>
    <author>
      <name>Xuemin Lin</name>
    </author>
    <author>
      <name>Wenjie Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper will appear in ICDE 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.13529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.16733v1</id>
    <updated>2023-03-29T14:36:17Z</updated>
    <published>2023-03-29T14:36:17Z</published>
    <title>Emotional Framing in the Spreading of False and True Claims</title>
    <summary>  The explosive growth of online misinformation, such as false claims, has
affected the social behavior of online users. In order to be persuasive and
mislead the audience, false claims are made to trigger emotions in their
audience. This paper contributes to understanding how misinformation in social
media is shaped by investigating the emotional framing that authors of the
claims try to create for their audience. We investigate how, firstly, the
existence of emotional framing in the claims depends on the topic and
credibility of the claims. Secondly, we explore how emotionally framed content
triggers emotional response posts by social media users, and how emotions
expressed in claims and corresponding users' response posts affect their
sharing behavior on social media. Analysis of four data sets covering different
topics (politics, health, Syrian war, and COVID-19) reveals that authors shape
their claims depending on the topic area to pass targeted emotions to their
audience. By analysing responses to claims, we show that the credibility of the
claim influences the distribution of emotions that the claim incites in its
audience. Moreover, our analysis shows that emotions expressed in the claims
are repeated in the users' responses. Finally, the analysis of users' sharing
behavior shows that negative emotional framing such as anger, fear, and sadness
of false claims leads to more interaction among users than positive emotions.
This analysis also reveals that in the claims that trigger happy responses,
true claims result in more sharing compared to false claims.
</summary>
    <author>
      <name>Akram Sadat Hosseini</name>
    </author>
    <author>
      <name>Steffen Staab</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3578503.3583611</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3578503.3583611" rel="related"/>
    <link href="http://arxiv.org/abs/2303.16733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09548v4</id>
    <updated>2024-09-18T14:26:21Z</updated>
    <published>2023-05-16T15:45:59Z</published>
    <title>Measuring Dimensions of Self-Presentation in Twitter Bios and their
  Links to Misinformation Sharing</title>
    <summary>  Social media platforms provide users with a profile description field,
commonly known as a ``bio," where they can present themselves to the world. A
growing literature shows that text in these bios can improve our understanding
of online self-presentation and behavior, but existing work relies exclusively
on keyword-based approaches to do so. We here propose and evaluate a suite of
\hl{simple, effective, and theoretically motivated} approaches to embed bios in
spaces that capture salient dimensions of social meaning, such as age and
partisanship. We \hl{evaluate our methods on four tasks, showing that the
strongest one out-performs several practical baselines.} We then show the
utility of our method in helping understand associations between
self-presentation and the sharing of URLs from low-quality news sites on
Twitter\hl{, with a particular focus on explore the interactions between age
and partisanship, and exploring the effects of self-presentations of
religiosity}. Our work provides new tools to help computational social
scientists make use of information in bios, and provides new insights into how
misinformation sharing may be perceived on Twitter.
</summary>
    <author>
      <name>Navid Madani</name>
    </author>
    <author>
      <name>Rabiraj Bandyopadhyay</name>
    </author>
    <author>
      <name>Briony Swire-Thompson</name>
    </author>
    <author>
      <name>Michael Miller Yoder</name>
    </author>
    <author>
      <name>Kenneth Joseph</name>
    </author>
    <link href="http://arxiv.org/abs/2305.09548v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09548v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.14378v1</id>
    <updated>2023-06-26T01:52:41Z</updated>
    <published>2023-06-26T01:52:41Z</published>
    <title>A study on information disorders on social networks during the Chilean
  social outbreak and COVID-19 pandemic</title>
    <summary>  Information disorders on social media can have a significant impact on
citizens' participation in democratic processes. To better understand the
spread of false and inaccurate information online, this research analyzed data
from Twitter, Facebook, and Instagram. The data was collected and verified by
professional fact-checkers in Chile between October 2019 and October 2021, a
period marked by political and health crises. The study found that false
information spreads faster and reaches more users than true information on
Twitter and Facebook. Instagram, on the other hand, seemed to be less affected
by this phenomenon. False information was also more likely to be shared by
users with lower reading comprehension skills. True information, on the other
hand, tended to be less verbose and generate less interest among audiences.
This research provides valuable insights into the characteristics of
misinformation and how it spreads online. By recognizing the patterns of how
false information diffuses and how users interact with it, we can identify the
circumstances in which false and inaccurate messages are prone to become
widespread. This knowledge can help us develop strategies to counter the spread
of misinformation and protect the integrity of democratic processes.
</summary>
    <author>
      <name>Marcelo Mendoza</name>
    </author>
    <author>
      <name>Sebasti√°n Valenzuela</name>
    </author>
    <author>
      <name>Enrique N√∫√±ez-Mussa</name>
    </author>
    <author>
      <name>Fabi√°n Padilla</name>
    </author>
    <author>
      <name>Eliana Providel</name>
    </author>
    <author>
      <name>Sebasti√°n Campos</name>
    </author>
    <author>
      <name>Renato Bassi</name>
    </author>
    <author>
      <name>Andrea Riquelme</name>
    </author>
    <author>
      <name>Valeria Aldana</name>
    </author>
    <author>
      <name>Claudia L√≥pez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/app13095347</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/app13095347" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Sciences 2023, 13(9), 5347</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2306.14378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.14378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.07610v1</id>
    <updated>2023-07-14T20:14:12Z</updated>
    <published>2023-07-14T20:14:12Z</published>
    <title>Assessing and Exploiting Domain Name Misinformation</title>
    <summary>  Cloud providers' support for network evasion techniques that misrepresent the
server's domain name is more prevalent than previously believed, which has
serious implications for security and privacy due to the reliance on domain
names in common security architectures. Domain fronting is one such evasive
technique used by privacy enhancing technologies and malware to hide the
domains they visit, and it uses shared hosting and HTTPS to present a benign
domain to observers while signaling the target domain in the encrypted HTTP
request. In this paper, we construct an ontology of domain name misinformation
and detail a novel measurement methodology to identify support among cloud
infrastructure providers. Despite several of the largest cloud providers having
publicly stated that they no longer support domain fronting, our findings
demonstrate a more complex environment with many exceptions.
  We also present a novel and straightforward attack that allows an adversary
to man-in-the-middle all the victim's encrypted traffic bound to a content
delivery network that supports domain fronting, breaking the authenticity,
confidentiality, and integrity guarantees expected by the victim when using
HTTPS. By using dynamic linker hijacking to rewrite the HTTP Host field, our
attack does not generate any artifacts that are visible to the victim or
passive network monitoring solutions, and the attacker does not need a separate
channel to exfiltrate data or perform command-and-control, which can be
achieved by rewriting HTTP headers.
</summary>
    <author>
      <name>Blake Anderson</name>
    </author>
    <author>
      <name>David McGrew</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 8th International Workshop on Traffic Measurements
  for Cybersecurity (WTMC 2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.07610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.07610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.11848v1</id>
    <updated>2023-07-21T18:35:24Z</updated>
    <published>2023-07-21T18:35:24Z</published>
    <title>MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through
  Multi-Answer Open-Domain Question Answering</title>
    <summary>  Check-worthy claim detection aims at providing plausible misinformation to
downstream fact-checking systems or human experts to check. This is a crucial
step toward accelerating the fact-checking process. Many efforts have been put
into how to identify check-worthy claims from a small scale of pre-collected
claims, but how to efficiently detect check-worthy claims directly from a
large-scale information source, such as Twitter, remains underexplored. To fill
this gap, we introduce MythQA, a new multi-answer open-domain question
answering(QA) task that involves contradictory stance mining for query-based
large-scale check-worthy claim detection. The idea behind this is that
contradictory claims are a strong indicator of misinformation that merits
scrutiny by the appropriate authorities. To study this task, we construct
TweetMythQA, an evaluation dataset containing 522 factoid multi-answer
questions based on controversial topics. Each question is annotated with
multiple answers. Moreover, we collect relevant tweets for each distinct
answer, then classify them into three categories: "Supporting", "Refuting", and
"Neutral". In total, we annotated 5.3K tweets. Contradictory evidence is
collected for all answers in the dataset. Finally, we present a baseline system
for MythQA and evaluate existing NLP models for each system component using the
TweetMythQA dataset. We provide initial benchmarks and identify key challenges
for future models to improve upon. Code and data are available at:
https://github.com/TonyBY/Myth-QA
</summary>
    <author>
      <name>Yang Bai</name>
    </author>
    <author>
      <name>Anthony Colas</name>
    </author>
    <author>
      <name>Daisy Zhe Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3539618.3591907</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3539618.3591907" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by SIGIR 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.11848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.11848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.12402v1</id>
    <updated>2023-07-13T14:45:47Z</updated>
    <published>2023-07-13T14:45:47Z</published>
    <title>ChatGPT and Bard Responses to Polarizing Questions</title>
    <summary>  Recent developments in natural language processing have demonstrated the
potential of large language models (LLMs) to improve a range of educational and
learning outcomes. Of recent chatbots based on LLMs, ChatGPT and Bard have made
it clear that artificial intelligence (AI) technology will have significant
implications on the way we obtain and search for information. However, these
tools sometimes produce text that is convincing, but often incorrect, known as
hallucinations. As such, their use can distort scientific facts and spread
misinformation. To counter polarizing responses on these tools, it is critical
to provide an overview of such responses so stakeholders can determine which
topics tend to produce more contentious responses -- key to developing targeted
regulatory policy and interventions. In addition, there currently exists no
annotated dataset of ChatGPT and Bard responses around possibly polarizing
topics, central to the above aims. We address the indicated issues through the
following contribution: Focusing on highly polarizing topics in the US, we
created and described a dataset of ChatGPT and Bard responses. Broadly, our
results indicated a left-leaning bias for both ChatGPT and Bard, with Bard more
likely to provide responses around polarizing topics. Bard seemed to have fewer
guardrails around controversial topics, and appeared more willing to provide
comprehensive, and somewhat human-like responses. Bard may thus be more likely
abused by malicious actors. Stakeholders may utilize our findings to mitigate
misinformative and/or polarizing responses from LLMs
</summary>
    <author>
      <name>Abhay Goyal</name>
    </author>
    <author>
      <name>Muhammad Siddique</name>
    </author>
    <author>
      <name>Nimay Parekh</name>
    </author>
    <author>
      <name>Zach Schwitzky</name>
    </author>
    <author>
      <name>Clara Broekaert</name>
    </author>
    <author>
      <name>Connor Michelotti</name>
    </author>
    <author>
      <name>Allie Wong</name>
    </author>
    <author>
      <name>Lam Yin Cheung</name>
    </author>
    <author>
      <name>Robin O Hanlon</name>
    </author>
    <author>
      <name>Lam Yin Cheung</name>
    </author>
    <author>
      <name>Munmun De Choudhury</name>
    </author>
    <author>
      <name>Roy Ka-Wei Lee</name>
    </author>
    <author>
      <name>Navin Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/2307.12402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.12402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.02312v4</id>
    <updated>2024-02-07T22:28:28Z</updated>
    <published>2023-08-04T13:23:20Z</published>
    <title>Is Stack Overflow Obsolete? An Empirical Study of the Characteristics of
  ChatGPT Answers to Stack Overflow Questions</title>
    <summary>  Q&amp;A platforms have been crucial for the online help-seeking behavior of
programmers. However, the recent popularity of ChatGPT is altering this trend.
Despite this popularity, no comprehensive study has been conducted to evaluate
the characteristics of ChatGPT's answers to programming questions. To bridge
the gap, we conducted the first in-depth analysis of ChatGPT answers to 517
programming questions on Stack Overflow and examined the correctness,
consistency, comprehensiveness, and conciseness of ChatGPT answers.
Furthermore, we conducted a large-scale linguistic analysis, as well as a user
study, to understand the characteristics of ChatGPT answers from linguistic and
human aspects. Our analysis shows that 52% of ChatGPT answers contain incorrect
information and 77% are verbose. Nonetheless, our user study participants still
preferred ChatGPT answers 35% of the time due to their comprehensiveness and
well-articulated language style. However, they also overlooked the
misinformation in the ChatGPT answers 39% of the time. This implies the need to
counter misinformation in ChatGPT answers to programming questions and raise
awareness of the risks associated with seemingly correct answers.
</summary>
    <author>
      <name>Samia Kabir</name>
    </author>
    <author>
      <name>David N. Udo-Imeh</name>
    </author>
    <author>
      <name>Bonan Kou</name>
    </author>
    <author>
      <name>Tianyi Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3613904.3642596</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3613904.3642596" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been conditionally accepted for the CHI Conference on
  Human Factors in Computing Systems (CHI'24). The new version of this paper
  has been modified with updated discussions to address more stakeholders</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.02312v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.02312v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.10220v2</id>
    <updated>2023-12-24T01:50:05Z</updated>
    <published>2023-08-20T10:08:55Z</published>
    <title>Designing and Evaluating Presentation Strategies for Fact-Checked
  Content</title>
    <summary>  With the rapid growth of online misinformation, it is crucial to have
reliable fact-checking methods. Recent research on finding check-worthy claims
and automated fact-checking have made significant advancements. However,
limited guidance exists regarding the presentation of fact-checked content to
effectively convey verified information to users. We address this research gap
by exploring the critical design elements in fact-checking reports and
investigating whether credibility and presentation-based design improvements
can enhance users' ability to interpret the report accurately. We co-developed
potential content presentation strategies through a workshop involving
fact-checking professionals, communication experts, and researchers. The
workshop examined the significance and utility of elements such as veracity
indicators and explored the feasibility of incorporating interactive components
for enhanced information disclosure. Building on the workshop outcomes, we
conducted an online experiment involving 76 crowd workers to assess the
efficacy of different design strategies. The results indicate that proposed
strategies significantly improve users' ability to accurately interpret the
verdict of fact-checking articles. Our findings underscore the critical role of
effective presentation of fact reports in addressing the spread of
misinformation. By adopting appropriate design enhancements, the effectiveness
of fact-checking reports can be maximized, enabling users to make informed
judgments.
</summary>
    <author>
      <name>Danula Hettiachchi</name>
    </author>
    <author>
      <name>Kaixin Ji</name>
    </author>
    <author>
      <name>Jenny Kennedy</name>
    </author>
    <author>
      <name>Anthony McCosker</name>
    </author>
    <author>
      <name>Flora D. Salim</name>
    </author>
    <author>
      <name>Mark Sanderson</name>
    </author>
    <author>
      <name>Falk Scholer</name>
    </author>
    <author>
      <name>Damiano Spina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3583780.3614841</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3583780.3614841" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 32nd ACM International Conference on Information and
  Knowledge Management (CIKM '23)</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.10220v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.10220v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.13927v1</id>
    <updated>2023-08-26T18:01:47Z</updated>
    <published>2023-08-26T18:01:47Z</published>
    <title>Quantifying the Influence of User Behaviors on the Dissemination of Fake
  News on Twitter with Multivariate Hawkes Processes</title>
    <summary>  Fake news has emerged as a pervasive problem within Online Social Networks,
leading to a surge of research interest in this area. Understanding the
dissemination mechanisms of fake news is crucial in comprehending the
propagation of disinformation/misinformation and its impact on users in Online
Social Networks. This knowledge can facilitate the development of interventions
to curtail the spread of false information and inform affected users to remain
vigilant against fraudulent/malicious content. In this paper, we specifically
target the Twitter platform and propose a Multivariate Hawkes Point Processes
model that incorporates essential factors such as user networks, response tweet
types, and user stances as model parameters. Our objective is to investigate
and quantify their influence on the dissemination process of fake news. We
derive parameter estimation expressions using an Expectation Maximization
algorithm and validate them on a simulated dataset. Furthermore, we conduct a
case study using a real dataset of fake news collected from Twitter to explore
the impact of user stances and tweet types on dissemination patterns. This
analysis provides valuable insights into how users are influenced by or
influence the dissemination process of disinformation/misinformation, and
demonstrates how our model can aid in intervening in this process.
</summary>
    <author>
      <name>Yichen Jiang</name>
    </author>
    <author>
      <name>Michael D. Porter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures, journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.13927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.13927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00240v1</id>
    <updated>2023-09-01T04:14:39Z</updated>
    <published>2023-09-01T04:14:39Z</published>
    <title>FactLLaMA: Optimizing Instruction-Following Language Models with
  External Knowledge for Automated Fact-Checking</title>
    <summary>  Automatic fact-checking plays a crucial role in combating the spread of
misinformation. Large Language Models (LLMs) and Instruction-Following
variants, such as InstructGPT and Alpaca, have shown remarkable performance in
various natural language processing tasks. However, their knowledge may not
always be up-to-date or sufficient, potentially leading to inaccuracies in
fact-checking. To address this limitation, we propose combining the power of
instruction-following language models with external evidence retrieval to
enhance fact-checking performance. Our approach involves leveraging search
engines to retrieve relevant evidence for a given input claim. This external
evidence serves as valuable supplementary information to augment the knowledge
of the pretrained language model. Then, we instruct-tune an open-sourced
language model, called LLaMA, using this evidence, enabling it to predict the
veracity of the input claim more accurately. To evaluate our method, we
conducted experiments on two widely used fact-checking datasets: RAWFC and
LIAR. The results demonstrate that our approach achieves state-of-the-art
performance in fact-checking tasks. By integrating external evidence, we bridge
the gap between the model's knowledge and the most up-to-date and sufficient
context available, leading to improved fact-checking outcomes. Our findings
have implications for combating misinformation and promoting the dissemination
of accurate information on online platforms. Our released materials are
accessible at: https://thcheung.github.io/factllama.
</summary>
    <author>
      <name>Tsun-Hin Cheung</name>
    </author>
    <author>
      <name>Kin-Man Lam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in APSIPA ASC 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.00240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.00737v3</id>
    <updated>2024-01-22T22:12:05Z</updated>
    <published>2023-10-01T17:25:56Z</published>
    <title>GenAI Against Humanity: Nefarious Applications of Generative Artificial
  Intelligence and Large Language Models</title>
    <summary>  Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
are marvels of technology; celebrated for their prowess in natural language
processing and multimodal content generation, they promise a transformative
future. But as with all powerful tools, they come with their shadows. Picture
living in a world where deepfakes are indistinguishable from reality, where
synthetic identities orchestrate malicious campaigns, and where targeted
misinformation or scams are crafted with unparalleled precision. Welcome to the
darker side of GenAI applications. This article is not just a journey through
the meanders of potential misuse of GenAI and LLMs, but also a call to
recognize the urgency of the challenges ahead. As we navigate the seas of
misinformation campaigns, malicious content generation, and the eerie creation
of sophisticated malware, we'll uncover the societal implications that ripple
through the GenAI revolution we are witnessing. From AI-powered botnets on
social media platforms to the unnerving potential of AI to generate fabricated
identities, or alibis made of synthetic realities, the stakes have never been
higher. The lines between the virtual and the real worlds are blurring, and the
consequences of potential GenAI's nefarious applications impact us all. This
article serves both as a synthesis of rigorous research presented on the risks
of GenAI and misuse of LLMs and as a thought-provoking vision of the different
types of harmful GenAI applications we might encounter in the near future, and
some ways we can prepare for them.
</summary>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s42001-024-00250-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s42001-024-00250-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in: Journal of Computational Social Science</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J Comput Soc Sc (2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2310.00737v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.00737v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.18545v1</id>
    <updated>2023-10-28T00:27:21Z</updated>
    <published>2023-10-28T00:27:21Z</published>
    <title>Identifying Conspiracy Theories News based on Event Relation Graph</title>
    <summary>  Conspiracy theories, as a type of misinformation, are narratives that
explains an event or situation in an irrational or malicious manner. While most
previous work examined conspiracy theory in social media short texts, limited
attention was put on such misinformation in long news documents. In this paper,
we aim to identify whether a news article contains conspiracy theories. We
observe that a conspiracy story can be made up by mixing uncorrelated events
together, or by presenting an unusual distribution of relations between events.
Achieving a contextualized understanding of events in a story is essential for
detecting conspiracy theories. Thus, we propose to incorporate an event
relation graph for each article, in which events are nodes, and four common
types of event relations, coreference, temporal, causal, and subevent
relations, are considered as edges. Then, we integrate the event relation graph
into conspiracy theory identification in two ways: an event-aware language
model is developed to augment the basic language model with the knowledge of
events and event relations via soft labels; further, a heterogeneous graph
attention network is designed to derive a graph embedding based on hard labels.
Experiments on a large benchmark dataset show that our approach based on event
relation graph improves both precision and recall of conspiracy theory
identification, and generalizes well for new unseen media sources.
</summary>
    <author>
      <name>Yuanyuan Lei</name>
    </author>
    <author>
      <name>Ruihong Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2023 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.18545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.18545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.15720v1</id>
    <updated>2024-01-28T17:59:55Z</updated>
    <published>2024-01-28T17:59:55Z</published>
    <title>The Impact of Snippet Reliability on Misinformation in Online Health
  Search</title>
    <summary>  Search result snippets are crucial in modern search engines, providing users
with a quick overview of a website's content. Snippets help users determine the
relevance of a document to their information needs, and in certain scenarios
even enable them to satisfy those needs without visiting web documents. Hence,
it is crucial for snippets to reliably represent the content of their
corresponding documents. While this may be a straightforward requirement for
some queries, it can become challenging in the complex domain of healthcare,
and can lead to misinformation. This paper aims to examine snippets'
reliability in representing their corresponding documents, specifically in the
health domain. To achieve this, we conduct a series of user studies using
Google's search results, where participants are asked to infer viewpoints of
search results pertaining to queries about the effectiveness of a medical
intervention for a medical condition, based solely on their titles and
snippets. Our findings reveal that a considerable portion of Google's snippets
(28%) failed to present any viewpoint on the intervention's effectiveness, and
that 35% were interpreted by participants as having a different viewpoint
compared to their corresponding documents. To address this issue, we propose a
snippet extraction solution tailored directly to users' information needs,
i.e., extracting snippets that summarize documents' viewpoints regarding the
intervention and condition that appear in the query. User study demonstrates
that our information need-focused solution outperforms the mainstream
query-based approach. With only 19.67% of snippets generated by our solution
reported as not presenting a viewpoint and a mere 20.33% misinterpreted by
participants. These results strongly suggest that an information need-focused
approach can significantly improve the reliability of extracted snippets in
online health search.
</summary>
    <author>
      <name>Anat Hashavit</name>
    </author>
    <author>
      <name>Tamar Stern</name>
    </author>
    <author>
      <name>Hongning Wang</name>
    </author>
    <author>
      <name>Sarit Kraus</name>
    </author>
    <link href="http://arxiv.org/abs/2401.15720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.15720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.06659v2</id>
    <updated>2024-10-14T16:17:34Z</updated>
    <published>2024-02-05T18:55:53Z</published>
    <title>Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language
  Models</title>
    <summary>  Vision-Language Models (VLMs) excel in generating textual responses from
visual inputs, but their versatility raises security concerns. This study takes
the first step in exposing VLMs' susceptibility to data poisoning attacks that
can manipulate responses to innocuous, everyday prompts. We introduce
Shadowcast, a stealthy data poisoning attack where poison samples are visually
indistinguishable from benign images with matching texts. Shadowcast
demonstrates effectiveness in two attack types. The first is a traditional
Label Attack, tricking VLMs into misidentifying class labels, such as confusing
Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging
VLMs' text generation capabilities to craft persuasive and seemingly rational
narratives for misinformation, such as portraying junk food as healthy. We show
that Shadowcast effectively achieves the attacker's intentions using as few as
50 poison samples. Crucially, the poisoned samples demonstrate transferability
across different VLM architectures, posing a significant concern in black-box
settings. Moreover, Shadowcast remains potent under realistic conditions
involving various text prompts, training data augmentation, and image
compression techniques. This work reveals how poisoned VLMs can disseminate
convincing yet deceptive misinformation to everyday, benign users, emphasizing
the importance of data integrity for responsible VLM deployments. Our code is
available at: https://github.com/umd-huang-lab/VLM-Poisoning.
</summary>
    <author>
      <name>Yuancheng Xu</name>
    </author>
    <author>
      <name>Jiarui Yao</name>
    </author>
    <author>
      <name>Manli Shu</name>
    </author>
    <author>
      <name>Yanchao Sun</name>
    </author>
    <author>
      <name>Zichu Wu</name>
    </author>
    <author>
      <name>Ning Yu</name>
    </author>
    <author>
      <name>Tom Goldstein</name>
    </author>
    <author>
      <name>Furong Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Thirty-Eighth Annual Conference on Neural Information
  Processing Systems (Neurips 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.06659v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.06659v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.14154v3</id>
    <updated>2024-09-02T02:41:26Z</updated>
    <published>2024-02-21T22:27:40Z</published>
    <title>MM-Soc: Benchmarking Multimodal Large Language Models in Social Media
  Platforms</title>
    <summary>  Social media platforms are hubs for multimodal information exchange,
encompassing text, images, and videos, making it challenging for machines to
comprehend the information or emotions associated with interactions in online
spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising
solution to these challenges, yet they struggle to accurately interpret human
emotions and complex content such as misinformation. This paper introduces
MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of
multimodal social media content. MM-Soc compiles prominent multimodal datasets
and incorporates a novel large-scale YouTube tagging dataset, targeting a range
of tasks from misinformation detection, hate speech detection, and social
context generation. Through our exhaustive evaluation on ten size-variants of
four open-source MLLMs, we have identified significant performance disparities,
highlighting the need for advancements in models' social understanding
capabilities. Our analysis reveals that, in a zero-shot setting, various types
of MLLMs generally exhibit difficulties in handling social media tasks.
However, MLLMs demonstrate performance improvements post fine-tuning,
suggesting potential pathways for improvement. Our code and data are available
at https://github.com/claws-lab/MMSoc.git.
</summary>
    <author>
      <name>Yiqiao Jin</name>
    </author>
    <author>
      <name>Minje Choi</name>
    </author>
    <author>
      <name>Gaurav Verma</name>
    </author>
    <author>
      <name>Jindong Wang</name>
    </author>
    <author>
      <name>Srijan Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.14154v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.14154v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.05593v2</id>
    <updated>2024-04-19T14:59:28Z</updated>
    <published>2024-03-06T15:00:11Z</published>
    <title>Introducing First-Principles Calculations: New Approach to Group
  Dynamics and Bridging Social Phenomena in TeNP-Chain Based Social Dynamics
  Simulations</title>
    <summary>  This note considers an innovative interdisciplinary methodology that bridges
the gap between the fundamental principles of quantum mechanics applied to the
study of materials such as tellurium nanoparticles (TeNPs) and graphene and the
complex dynamics of social systems. The basis for this approach lies in the
metaphorical parallels drawn between the structural features of TeNPs and
graphene and the behavioral patterns of social groups in the face of
misinformation. TeNPs exhibit unique properties such as the strengthening of
covalent bonds within telluric chains and the disruption of secondary structure
leading to the separation of these chains. This is analogous to increased
cohesion within social groups and disruption of information flow between
different subgroups, respectively. . Similarly, the outstanding properties of
graphene, such as high electrical conductivity, strength, and flexibility,
provide additional aspects for understanding the resilience and adaptability of
social structures in response to external stimuli such as fake news. This
research note proposes a novel metaphorical framework for analyzing the spread
of fake news within social groups, analogous to the structural features of
telluric nanoparticles (TeNPs). We investigate how the strengthening of
covalent bonds within TeNPs reflects the strengthening of social cohesion in
groups that share common beliefs and values. This paper is partially an attempt
to utilize "Generative AI" and was written with educational intent. There are
currently no plans for it to become a peer-reviewed paper.
</summary>
    <author>
      <name>Yasuko Kawahata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TeNP Chains, First-principles calculations, Tellurium nanoparticles
  (TeNPs), Graphene, Fake news dissemination, Social cohesion, Information Flow
  Disruption, Quantum Mechanics, Interdisciplinary approach, Misinformation
  mitigation</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.05593v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.05593v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.08771v2</id>
    <updated>2024-04-19T14:44:47Z</updated>
    <published>2024-01-13T13:33:06Z</published>
    <title>Pathfinding of Digital False Information Diffusion by Conformal Field
  Theory and Feynman's Green's Function: Multiple Analysis Considering Stress
  Energy Tensor and Symmetry of Virasoro Operators</title>
    <summary>  This study delves into the application of Conformal Field Theory (CFT) to
understand information diffusion within digital media and its broader social
implications. Focusing on the digital native generation, vulnerable to
misinformation and data privacy issues, the research highlights the urgency of
addressing psychological health in the digital realm. By leveraging CFT's
analytical power, particularly in tracing information flows and assessing the
impact of misinformation, the study offers novel insights into the dynamics of
digital communication. It proposes a model for predicting and analyzing the
spread of biased content, emphasizing the need for improved information
literacy and personal data protection. This paper explores the concept of
infinitesimal conformal transformations using the Virasoro operator, forming a
conformal transformation group. In the examination of multiple analytical
methods, the approach to quantitatively analyze cases that are likely to expose
the risk of spreading filter bubbles such as negative information, rumors, and
fake news is to incorporate a two-dimensional conformal field theory approach
using the Ward-Takahashi identity and path integral with trace anomalies and
metric are introduced. Trace anomalies and metrics were introduced. A trace
anomaly refers to the spread of information in an unexpected way, usually
indicating anomalous behavior or hidden dynamics in the system. Metric
variation hypothesized and analyzed how diffusion characteristics, such as the
rate and pattern of information diffusion, vary over time.
</summary>
    <author>
      <name>Yasuko Kawahata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Short Discussion Paper, Various hypotheses and inferences with
  applications to social physics This paper is partially an attempt to utilize
  "Generative AI" and was written with educational intent. There are currently
  no plans for it to become a peer-reviewed paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.08771v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.08771v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.16116v2</id>
    <updated>2024-04-26T01:51:51Z</updated>
    <published>2024-04-24T18:13:29Z</published>
    <title>Classifying Human-Generated and AI-Generated Election Claims in Social
  Media</title>
    <summary>  Politics is one of the most prevalent topics discussed on social media
platforms, particularly during major election cycles, where users engage in
conversations about candidates and electoral processes. Malicious actors may
use this opportunity to disseminate misinformation to undermine trust in the
electoral process. The emergence of Large Language Models (LLMs) exacerbates
this issue by enabling malicious actors to generate misinformation at an
unprecedented scale. Artificial intelligence (AI)-generated content is often
indistinguishable from authentic user content, raising concerns about the
integrity of information on social networks. In this paper, we present a novel
taxonomy for characterizing election-related claims. This taxonomy provides an
instrument for analyzing election-related claims, with granular categories
related to jurisdiction, equipment, processes, and the nature of claims. We
introduce ElectAI, a novel benchmark dataset that consists of 9,900 tweets,
each labeled as human- or AI-generated. For AI-generated tweets, the specific
LLM variant that produced them is specified. We annotated a subset of 1,550
tweets using the proposed taxonomy to capture the characteristics of
election-related claims. We explored the capabilities of LLMs in extracting the
taxonomy attributes and trained various machine learning models using ElectAI
to distinguish between human- and AI-generated posts and identify the specific
LLM variant.
</summary>
    <author>
      <name>Alphaeus Dmonte</name>
    </author>
    <author>
      <name>Marcos Zampieri</name>
    </author>
    <author>
      <name>Kevin Lybarger</name>
    </author>
    <author>
      <name>Massimiliano Albanese</name>
    </author>
    <author>
      <name>Genya Coulter</name>
    </author>
    <link href="http://arxiv.org/abs/2404.16116v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.16116v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.18971v1</id>
    <updated>2024-04-29T13:47:04Z</updated>
    <published>2024-04-29T13:47:04Z</published>
    <title>Credible, Unreliable or Leaked?: Evidence Verification for Enhanced
  Automated Fact-checking</title>
    <summary>  Automated fact-checking (AFC) is garnering increasing attention by
researchers aiming to help fact-checkers combat the increasing spread of
misinformation online. While many existing AFC methods incorporate external
information from the Web to help examine the veracity of claims, they often
overlook the importance of verifying the source and quality of collected
"evidence". One overlooked challenge involves the reliance on "leaked
evidence", information gathered directly from fact-checking websites and used
to train AFC systems, resulting in an unrealistic setting for early
misinformation detection. Similarly, the inclusion of information from
unreliable sources can undermine the effectiveness of AFC systems. To address
these challenges, we present a comprehensive approach to evidence verification
and filtering. We create the "CREDible, Unreliable or LEaked" (CREDULE)
dataset, which consists of 91,632 articles classified as Credible, Unreliable
and Fact checked (Leaked). Additionally, we introduce the EVidence VERification
Network (EVVER-Net), trained on CREDULE to detect leaked and unreliable
evidence in both short and long texts. EVVER-Net can be used to filter evidence
collected from the Web, thus enhancing the robustness of end-to-end AFC
systems. We experiment with various language models and show that EVVER-Net can
demonstrate impressive performance of up to 91.5% and 94.4% accuracy, while
leveraging domain credibility scores along with short or long texts,
respectively. Finally, we assess the evidence provided by widely-used
fact-checking datasets including LIAR-PLUS, MOCHEG, FACTIFY, NewsCLIPpings+ and
VERITE, some of which exhibit concerning rates of leaked and unreliable
evidence.
</summary>
    <author>
      <name>Zacharias Chrysidis</name>
    </author>
    <author>
      <name>Stefanos-Iordanis Papadopoulos</name>
    </author>
    <author>
      <name>Symeon Papadopoulos</name>
    </author>
    <author>
      <name>Panagiotis C. Petrantonakis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3643491.3660278</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3643491.3660278" rel="related"/>
    <link href="http://arxiv.org/abs/2404.18971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.18971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.12871v1</id>
    <updated>2024-05-21T15:39:11Z</updated>
    <published>2024-05-21T15:39:11Z</published>
    <title>Efficient Influence Minimization via Node Blocking</title>
    <summary>  Given a graph G, a budget k and a misinformation seed set S, Influence
Minimization (IMIN) via node blocking aims to find a set of k nodes to be
blocked such that the expected spread of S is minimized. This problem finds
important applications in suppressing the spread of misinformation and has been
extensively studied in the literature. However, existing solutions for IMIN
still incur significant computation overhead, especially when k becomes large.
In addition, there is still no approximation solution with non-trivial
theoretical guarantee for IMIN via node blocking prior to our work. In this
paper, we conduct the first attempt to propose algorithms that yield
data-dependent approximation guarantees. Based on the Sandwich framework, we
first develop submodular and monotonic lower and upper bounds for our
non-submodular objective function and prove the computation of proposed bounds
is \#P-hard. In addition, two advanced sampling methods are proposed to
estimate the value of bounding functions. Moreover, we develop two novel
martingale-based concentration bounds to reduce the sample complexity and
design two non-trivial algorithms that provide (1-1/e-\epsilon)-approximate
solutions to our bounding functions. Comprehensive experiments on 9 real-world
datasets are conducted to validate the efficiency and effectiveness of the
proposed techniques. Compared with the state-of-the-art methods, our solutions
can achieve up to two orders of magnitude speedup and provide theoretical
guarantees for the quality of returned results.
</summary>
    <author>
      <name>Jinghao Wang</name>
    </author>
    <author>
      <name>Yanping Wu</name>
    </author>
    <author>
      <name>Xiaoyang Wang</name>
    </author>
    <author>
      <name>Ying Zhang</name>
    </author>
    <author>
      <name>Lu Qin</name>
    </author>
    <author>
      <name>Wenjie Zhang</name>
    </author>
    <author>
      <name>Xuemin Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2405.12871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.12871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.20254v2</id>
    <updated>2024-08-16T17:55:41Z</updated>
    <published>2024-05-30T17:07:07Z</published>
    <title>Conversational Agents to Facilitate Deliberation on Harmful Content in
  WhatsApp Groups</title>
    <summary>  WhatsApp groups have become a hotbed for the propagation of harmful content
including misinformation, hate speech, polarizing content, and rumors,
especially in Global South countries. Given the platform's end-to-end
encryption, moderation responsibilities lie on group admins and members, who
rarely contest such content. Another approach is fact-checking, which is
unscalable, and can only contest factual content (e.g., misinformation) but not
subjective content (e.g., hate speech). Drawing on recent literature, we
explore deliberation -- open and inclusive discussion -- as an alternative. We
investigate the role of a conversational agent in facilitating deliberation on
harmful content in WhatsApp groups. We conducted semi-structured interviews
with 21 Indian WhatsApp users, employing a design probe to showcase an example
agent. Participants expressed the need for anonymity and recommended AI
assistance to reduce the effort required in deliberation. They appreciated the
agent's neutrality but pointed out the futility of deliberation in echo chamber
groups. Our findings highlight design tensions for such an agent, including
privacy versus group dynamics and freedom of speech in private spaces. We
discuss the efficacy of deliberation using deliberative theory as a lens,
compare deliberation with moderation and fact-checking, and provide design
recommendations for future such systems. Ultimately, this work advances CSCW by
offering insights into designing deliberative systems for combating harmful
content in private group chats on social media.
</summary>
    <author>
      <name>Dhruv Agarwal</name>
    </author>
    <author>
      <name>Farhana Shahid</name>
    </author>
    <author>
      <name>Aditya Vashistha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3687030</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3687030" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CSCW 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.20254v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.20254v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.06400v2</id>
    <updated>2024-06-12T09:25:36Z</updated>
    <published>2024-06-10T15:53:50Z</published>
    <title>An Empirical Design Justice Approach to Identifying Ethical
  Considerations in the Intersection of Large Language Models and Social
  Robotics</title>
    <summary>  The integration of Large Language Models (LLMs) in social robotics presents a
unique set of ethical challenges and social impacts. This research is set out
to identify ethical considerations that arise in the design and development of
these two technologies in combination. Using LLMs for social robotics may
provide benefits, such as enabling natural language open-domain dialogues.
However, the intersection of these two technologies also gives rise to ethical
concerns related to misinformation, non-verbal cues, emotional disruption, and
biases. The robot's physical social embodiment adds complexity, as ethical
hazards associated with LLM-based Social AI, such as hallucinations and
misinformation, can be exacerbated due to the effects of physical embodiment on
social perception and communication. To address these challenges, this study
employs an empirical design justice-based methodology, focusing on identifying
socio-technical ethical considerations through a qualitative co-design and
interaction study. The purpose of the study is to identify ethical
considerations relevant to the process of co-design of, and interaction with a
humanoid social robot as the interface of a LLM, and to evaluate how a design
justice methodology can be used in the context of designing LLMs-based social
robotics. The findings reveal a mapping of ethical considerations arising in
four conceptual dimensions: interaction, co-design, terms of service and
relationship and evaluates how a design justice approach can be used
empirically in the intersection of LLMs and social robotics.
</summary>
    <author>
      <name>Alva Markelius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.06400v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.06400v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; K.4; H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.12444v1</id>
    <updated>2024-06-18T09:47:58Z</updated>
    <published>2024-06-18T09:47:58Z</published>
    <title>Who Checks the Checkers? Exploring Source Credibility in Twitter's
  Community Notes</title>
    <summary>  In recent years, the proliferation of misinformation on social media
platforms has become a significant concern. Initially designed for sharing
information and fostering social connections, platforms like Twitter (now
rebranded as X) have also unfortunately become conduits for spreading
misinformation. To mitigate this, these platforms have implemented various
mechanisms, including the recent suggestion to use crowd-sourced non-expert
fact-checkers to enhance the scalability and efficiency of content vetting. An
example of this is the introduction of Community Notes on Twitter.
  While previous research has extensively explored various aspects of Twitter
tweets, such as information diffusion, sentiment analytics and opinion
summarization, there has been a limited focus on the specific feature of
Twitter Community Notes, despite its potential role in crowd-sourced
fact-checking. Prior research on Twitter Community Notes has involved empirical
analysis of the feature's dataset and comparative studies that also include
other methods like expert fact-checking. Distinguishing itself from prior
works, our study covers a multi-faceted analysis of sources and audience
perception within Community Notes. We find that the majority of cited sources
are news outlets that are left-leaning and are of high factuality, pointing to
a potential bias in the platform's community fact-checking. Left biased and low
factuality sources validate tweets more, while Center sources are used more
often to refute tweet content. Additionally, source factuality significantly
influences public agreement and helpfulness of the notes, highlighting the
effectiveness of the Community Notes Ranking algorithm. These findings showcase
the impact and biases inherent in community-based fact-checking initiatives.
</summary>
    <author>
      <name>Uku Kangur</name>
    </author>
    <author>
      <name>Roshni Chakraborty</name>
    </author>
    <author>
      <name>Rajesh Sharma</name>
    </author>
    <link href="http://arxiv.org/abs/2406.12444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.12444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.07914v1</id>
    <updated>2024-07-05T10:33:15Z</updated>
    <published>2024-07-05T10:33:15Z</published>
    <title>Health Misinformation Detection in Web Content via Web2Vec: A
  Structural-, Content-based, and Context-aware Approach based on Web2Vec</title>
    <summary>  In recent years, we have witnessed the proliferation of large amounts of
online content generated directly by users with virtually no form of external
control, leading to the possible spread of misinformation. The search for
effective solutions to this problem is still ongoing, and covers different
areas of application, from opinion spam to fake news detection. A more recently
investigated scenario, despite the serious risks that incurring disinformation
could entail, is that of the online dissemination of health information. Early
approaches in this area focused primarily on user-based studies applied to Web
page content. More recently, automated approaches have been developed for both
Web pages and social media content, particularly with the advent of the
COVID-19 pandemic. These approaches are primarily based on handcrafted features
extracted from online content in association with Machine Learning. In this
scenario, we focus on Web page content, where there is still room for research
to study structural-, content- and context-based features to assess the
credibility of Web pages. Therefore, this work aims to study the effectiveness
of such features in association with a deep learning model, starting from an
embedded representation of Web pages that has been recently proposed in the
context of phishing Web page detection, i.e., Web2Vec.
</summary>
    <author>
      <name>Rishabh Upadhyay</name>
    </author>
    <author>
      <name>Gabriella Pasi</name>
    </author>
    <author>
      <name>Marco Viviani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3462203.3475898</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3462203.3475898" rel="related"/>
    <link href="http://arxiv.org/abs/2407.07914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.07914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.12812v2</id>
    <updated>2025-02-07T08:18:00Z</updated>
    <published>2024-08-23T03:16:26Z</published>
    <title>Grounding Fallacies Misrepresenting Scientific Publications in Evidence</title>
    <summary>  Health-related misinformation claims often falsely cite a credible biomedical
publication as evidence. These publications only superficially seem to support
the false claim, when logical fallacies are applied. In this work, we aim to
detect and to highlight such fallacies, which requires assessing the exact
content of the misrepresented publications. To achieve this, we introduce
MissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus
extends Missci by grounding the applied fallacies in real-world passages from
misrepresented studies. This creates a realistic test-bed for detecting and
verbalizing fallacies under real-world input conditions, and enables new and
realistic passage-retrieval tasks. MissciPlus is the first logical fallacy
dataset which pairs the real-world misrepresented evidence with incorrect
claims, identical to the input to evidence-based fact-checking models. With
MissciPlus, we i) benchmark retrieval models in identifying passages that
support claims only with fallacious reasoning, ii) evaluate how well LLMs
verbalize fallacious reasoning based on misrepresented scientific passages, and
iii) assess the effectiveness of fact-checking models in refuting claims that
misrepresent biomedical research. Our findings show that current fact-checking
models struggle to use misrepresented scientific passages to refute
misinformation. Moreover, these passages can mislead LLMs into accepting false
claims as true.
</summary>
    <author>
      <name>Max Glockner</name>
    </author>
    <author>
      <name>Yufang Hou</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NAACL 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.12812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.12812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.13461v2</id>
    <updated>2025-02-18T16:21:50Z</updated>
    <published>2024-09-20T12:50:17Z</published>
    <title>Analyzing News Engagement on Facebook: Tracking Ideological Segregation
  and News Quality in the Facebook URL Dataset</title>
    <summary>  The Facebook Privacy-Protected Full URLs Dataset was released to enable
independent, academic research on the impact of Facebook's platform on society
while ensuring user privacy. The dataset has been used in several studies to
analyze the relationship between social media engagement and societal issues
such as misinformation, polarization, and the quality of consumed news. In this
paper, we conduct a comprehensive analysis of the engagement with popular news
domains, covering four years from January 2017 to December 2020, with a focus
on user engagement metrics related to news URLs in the U.S. By incorporating
the ideological alignment and quality of news sources, along with users'
political preferences, we construct weighted averages of ideology and quality
of news consumption for liberal, conservative, and moderate audiences. This
allows us to track the evolution of (i) the ideological gap in news consumption
between liberals and conservatives and (ii) the average quality of each group's
news consumption. We identify two major shifts in trends, each tied to
engagement changes. In both, the ideological gap widens and news quality
declines. However, engagement rises in the first shift but falls in the second.
Finally, we contextualize these trends by linking them to two major Facebook
News Feed updates. Our findings provide empirical evidence to better understand
user behavior, polarization, and misinformation during the period covered by
the dataset.
</summary>
    <author>
      <name>Emma Fraxanet</name>
    </author>
    <author>
      <name>Andreas Kaltenbrunner</name>
    </author>
    <author>
      <name>Fabrizio Germano</name>
    </author>
    <author>
      <name>Vicen√ß G√≥mez</name>
    </author>
    <link href="http://arxiv.org/abs/2409.13461v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.13461v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.17352v1</id>
    <updated>2024-09-25T20:58:07Z</updated>
    <published>2024-09-25T20:58:07Z</published>
    <title>On the Interplay of Clustering and Evolution in the Emergence of
  Epidemic Outbreaks</title>
    <summary>  In an increasingly interconnected world, a key scientific challenge is to
examine mechanisms that lead to the widespread propagation of contagions, such
as misinformation and pathogens, and identify risk factors that can trigger
large-scale outbreaks. Underlying both the spread of disease and misinformation
epidemics is the evolution of the contagion as it propagates, leading to the
emergence of different strains, e.g., through genetic mutations in pathogens
and alterations in the information content. Recent studies have revealed that
models that do not account for heterogeneity in transmission risks associated
with different strains of the circulating contagion can lead to inaccurate
predictions. However, existing results on multi-strain spreading assume that
the network has a vanishingly small clustering coefficient, whereas clustering
is widely known to be a fundamental property of real-world social networks. In
this work, we investigate spreading processes that entail evolutionary
adaptations on random graphs with tunable clustering and arbitrary degree
distributions. We derive a mathematical framework to quantify the epidemic
characteristics of a contagion that evolves as it spreads, with the structure
of the underlying network as given via arbitrary {\em joint} degree
distributions of single-edges and triangles. To the best of our knowledge, our
work is the first to jointly analyze the impact of clustering and evolution on
the emergence of epidemic outbreaks. We supplement our theoretical finding with
numerical simulations and case studies, shedding light on the impact of
clustering on contagion spread.
</summary>
    <author>
      <name>Mansi Sood</name>
    </author>
    <author>
      <name>Hejin Gu</name>
    </author>
    <author>
      <name>Rashad Eletreby</name>
    </author>
    <author>
      <name>Swarun Kumar</name>
    </author>
    <author>
      <name>Chai Wah Wu</name>
    </author>
    <author>
      <name>Osman Yagan</name>
    </author>
    <link href="http://arxiv.org/abs/2409.17352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.17352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18866v1</id>
    <updated>2024-10-24T15:51:04Z</updated>
    <published>2024-10-24T15:51:04Z</published>
    <title>The Cat and Mouse Game: The Ongoing Arms Race Between Diffusion Models
  and Detection Methods</title>
    <summary>  The emergence of diffusion models has transformed synthetic media generation,
offering unmatched realism and control over content creation. These
advancements have driven innovation across fields such as art, design, and
scientific visualization. However, they also introduce significant ethical and
societal challenges, particularly through the creation of hyper-realistic
images that can facilitate deepfakes, misinformation, and unauthorized
reproduction of copyrighted material. In response, the need for effective
detection mechanisms has become increasingly urgent. This review examines the
evolving adversarial relationship between diffusion model development and the
advancement of detection methods. We present a thorough analysis of
contemporary detection strategies, including frequency and spatial domain
techniques, deep learning-based approaches, and hybrid models that combine
multiple methodologies. We also highlight the importance of diverse datasets
and standardized evaluation metrics in improving detection accuracy and
generalizability. Our discussion explores the practical applications of these
detection systems in copyright protection, misinformation prevention, and
forensic analysis, while also addressing the ethical implications of synthetic
media. Finally, we identify key research gaps and propose future directions to
enhance the robustness and adaptability of detection methods in line with the
rapid advancements of diffusion models. This review emphasizes the necessity of
a comprehensive approach to mitigating the risks associated with AI-generated
content in an increasingly digital world.
</summary>
    <author>
      <name>Linda Laurier</name>
    </author>
    <author>
      <name>Ave Giulietta</name>
    </author>
    <author>
      <name>Arlo Octavia</name>
    </author>
    <author>
      <name>Meade Cleti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.20293v4</id>
    <updated>2025-03-09T07:42:04Z</updated>
    <published>2024-10-26T23:55:50Z</published>
    <title>A Systematic Review of Machine Learning Approaches for Detecting
  Deceptive Activities on Social Media: Methods, Challenges, and Biases</title>
    <summary>  Social media platforms like Twitter, Facebook, and Instagram have facilitated
the spread of misinformation, necessitating automated detection systems. This
systematic review evaluates 36 studies that apply machine learning (ML) and
deep learning (DL) models to detect fake news, spam, and fake accounts on
social media. Using the Prediction model Risk Of Bias ASsessment Tool
(PROBAST), the review identified key biases across the ML lifecycle: selection
bias due to non-representative sampling, inadequate handling of class
imbalance, insufficient linguistic preprocessing (e.g., negations), and
inconsistent hyperparameter tuning. Although models such as Support Vector
Machines (SVM), Random Forests, and Long Short-Term Memory (LSTM) networks
showed strong potential, over-reliance on accuracy as an evaluation metric in
imbalanced data settings was a common flaw. The review highlights the need for
improved data preprocessing (e.g., resampling techniques), consistent
hyperparameter tuning, and the use of appropriate metrics like precision,
recall, F1 score, and AUROC. Addressing these limitations can lead to more
reliable and generalizable ML/DL models for detecting deceptive content,
ultimately contributing to the reduction of misinformation on social media.
</summary>
    <author>
      <name>Yunchong Liu</name>
    </author>
    <author>
      <name>Xiaorui Shen</name>
    </author>
    <author>
      <name>Yeyubei Zhang</name>
    </author>
    <author>
      <name>Zhongyan Wang</name>
    </author>
    <author>
      <name>Yexin Tian</name>
    </author>
    <author>
      <name>Jianglai Dai</name>
    </author>
    <author>
      <name>Yuchen Cao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s41060-025-00850-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s41060-025-00850-8" rel="related"/>
    <link href="http://arxiv.org/abs/2410.20293v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.20293v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.21330v1</id>
    <updated>2024-10-27T16:23:26Z</updated>
    <published>2024-10-27T16:23:26Z</published>
    <title>LLM Robustness Against Misinformation in Biomedical Question Answering</title>
    <summary>  The retrieval-augmented generation (RAG) approach is used to reduce the
confabulation of large language models (LLMs) for question answering by
retrieving and providing additional context coming from external knowledge
sources (e.g., by adding the context to the prompt). However, injecting
incorrect information can mislead the LLM to generate an incorrect answer.
  In this paper, we evaluate the effectiveness and robustness of four LLMs
against misinformation - Gemma 2, GPT-4o-mini, Llama~3.1, and Mixtral - in
answering biomedical questions. We assess the answer accuracy on yes-no and
free-form questions in three scenarios: vanilla LLM answers (no context is
provided), "perfect" augmented generation (correct context is provided), and
prompt-injection attacks (incorrect context is provided). Our results show that
Llama 3.1 (70B parameters) achieves the highest accuracy in both vanilla
(0.651) and "perfect" RAG (0.802) scenarios. However, the accuracy gap between
the models almost disappears with "perfect" RAG, suggesting its potential to
mitigate the LLM's size-related effectiveness differences.
  We further evaluate the ability of the LLMs to generate malicious context on
one hand and the LLM's robustness against prompt-injection attacks on the other
hand, using metrics such as attack success rate (ASR), accuracy under attack,
and accuracy drop. As adversaries, we use the same four LLMs (Gemma 2,
GPT-4o-mini, Llama 3.1, and Mixtral) to generate incorrect context that is
injected in the target model's prompt. Interestingly, Llama is shown to be the
most effective adversary, causing accuracy drops of up to 0.48 for vanilla
answers and 0.63 for "perfect" RAG across target models. Our analysis reveals
that robustness rankings vary depending on the evaluation measure, highlighting
the complexity of assessing LLM resilience to adversarial attacks.
</summary>
    <author>
      <name>Alexander Bondarenko</name>
    </author>
    <author>
      <name>Adrian Viehweger</name>
    </author>
    <link href="http://arxiv.org/abs/2410.21330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.21330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.10713v1</id>
    <updated>2024-11-16T05:54:36Z</updated>
    <published>2024-11-16T05:54:36Z</published>
    <title>A Regularized LSTM Method for Detecting Fake News Articles</title>
    <summary>  Nowadays, the rapid diffusion of fake news poses a significant problem, as it
can spread misinformation and confusion. This paper aims to develop an advanced
machine learning solution for detecting fake news articles. Leveraging a
comprehensive dataset of news articles, including 23,502 fake news articles and
21,417 accurate news articles, we implemented and evaluated three
machine-learning models. Our dataset, curated from diverse sources, provides
rich textual content categorized into title, text, subject, and Date features.
These features are essential for training robust classification models to
distinguish between fake and authentic news articles. The initial model
employed a Long Short-Term Memory (LSTM) network, achieving an accuracy of 94%.
The second model improved upon this by incorporating additional regularization
techniques and fine-tuning hyperparameters, resulting in a 97% accuracy. The
final model combined the strengths of previous architectures with advanced
optimization strategies, achieving a peak accuracy of 98%. These results
demonstrate the effectiveness of our approach in identifying fake news with
high precision. Implementing these models showcases significant advancements in
natural language processing and machine learning techniques, contributing
valuable tools for combating misinformation. Our work highlights the potential
for deploying such models in real-world applications, providing a reliable
method for automated fake news detection and enhancing the credibility of news
dissemination.
</summary>
    <author>
      <name>Tanjina Sultana Camelia</name>
    </author>
    <author>
      <name>Faizur Rahman Fahim</name>
    </author>
    <author>
      <name>Md. Musfique Anwar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 7 figures, 2024 IEEE International Conference on Signal
  Processing, Information, Communication and Systems (SPICSCON)</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.10713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.10713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.05292v1</id>
    <updated>2025-01-09T14:55:25Z</updated>
    <published>2025-01-09T14:55:25Z</published>
    <title>Detection of Rumors and Their Sources in Social Networks: A
  Comprehensive Survey</title>
    <summary>  With the recent advancements in social network platform technology, an
overwhelming amount of information is spreading rapidly. In this situation, it
can become increasingly difficult to discern what information is false or true.
If false information proliferates significantly, it can lead to undesirable
outcomes. Hence, when we receive some information, we can pose the following
two questions: $(i)$ Is the information true? $(ii)$ If not, who initially
spread that information? % The first problem is the rumor detection issue,
while the second is the rumor source detection problem. A rumor-detection
problem involves identifying and mitigating false or misleading information
spread via various communication channels, particularly online platforms and
social media. Rumors can range from harmless ones to deliberately misleading
content aimed at deceiving or manipulating audiences. Detecting misinformation
is crucial for maintaining the integrity of information ecosystems and
preventing harmful effects such as the spread of false beliefs, polarization,
and even societal harm. Therefore, it is very important to quickly distinguish
such misinformation while simultaneously finding its source to block it from
spreading on the network. However, most of the existing surveys have analyzed
these two issues separately. In this work, we first survey the existing
research on the rumor-detection and rumor source detection problems with joint
detection approaches, simultaneously. % This survey deals with these two issues
together so that their relationship can be observed and it provides how the two
problems are similar and different. The limitations arising from the rumor
detection, rumor source detection, and their combination problems are also
explained, and some challenges to be addressed in future works are presented.
</summary>
    <author>
      <name>Otabek Sattarov</name>
    </author>
    <author>
      <name>Jaeyoung Choi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TBDATA.2024.3522801</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TBDATA.2024.3522801" rel="related"/>
    <link href="http://arxiv.org/abs/2501.05292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.05292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.04666v1</id>
    <updated>2025-02-07T05:19:13Z</updated>
    <published>2025-02-07T05:19:13Z</published>
    <title>Enhancing Health Information Retrieval with RAG by Prioritizing Topical
  Relevance and Factual Accuracy</title>
    <summary>  The exponential surge in online health information, coupled with its
increasing use by non-experts, highlights the pressing need for advanced Health
Information Retrieval models that consider not only topical relevance but also
the factual accuracy of the retrieved information, given the potential risks
associated with health misinformation. To this aim, this paper introduces a
solution driven by Retrieval-Augmented Generation (RAG), which leverages the
capabilities of generative Large Language Models (LLMs) to enhance the
retrieval of health-related documents grounded in scientific evidence. In
particular, we propose a three-stage model: in the first stage, the user's
query is employed to retrieve topically relevant passages with associated
references from a knowledge base constituted by scientific literature. In the
second stage, these passages, alongside the initial query, are processed by
LLMs to generate a contextually relevant rich text (GenText). In the last
stage, the documents to be retrieved are evaluated and ranked both from the
point of view of topical relevance and factual accuracy by means of their
comparison with GenText, either through stance detection or semantic
similarity. In addition to calculating factual accuracy, GenText can offer a
layer of explainability for it, aiding users in understanding the reasoning
behind the retrieval. Experimental evaluation of our model on benchmark
datasets and against baseline models demonstrates its effectiveness in
enhancing the retrieval of both topically relevant and factually accurate
health information, thus presenting a significant step forward in the health
misinformation mitigation problem.
</summary>
    <author>
      <name>Rishabh Uapadhyay</name>
    </author>
    <author>
      <name>Marco Viviani</name>
    </author>
    <link href="http://arxiv.org/abs/2502.04666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.04666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.05803v2</id>
    <updated>2025-02-16T22:05:21Z</updated>
    <published>2025-02-09T08:14:11Z</published>
    <title>FlashCheck: Exploration of Efficient Evidence Retrieval for Fast
  Fact-Checking</title>
    <summary>  The advances in digital tools have led to the rampant spread of
misinformation. While fact-checking aims to combat this, manual fact-checking
is cumbersome and not scalable. It is essential for automated fact-checking to
be efficient for aiding in combating misinformation in real-time and at the
source. Fact-checking pipelines primarily comprise a knowledge retrieval
component which extracts relevant knowledge to fact-check a claim from large
knowledge sources like Wikipedia and a verification component. The existing
works primarily focus on the fact-verification part rather than evidence
retrieval from large data collections, which often face scalability issues for
practical applications such as live fact-checking. In this study, we address
this gap by exploring various methods for indexing a succinct set of factual
statements from large collections like Wikipedia to enhance the retrieval phase
of the fact-checking pipeline. We also explore the impact of vector
quantization to further improve the efficiency of pipelines that employ dense
retrieval approaches for first-stage retrieval. We study the efficiency and
effectiveness of the approaches on fact-checking datasets such as HoVer and
WiCE, leveraging Wikipedia as the knowledge source. We also evaluate the
real-world utility of the efficient retrieval approaches by fact-checking 2024
presidential debate and also open source the collection of claims with
corresponding labels identified in the debate. Through a combination of indexed
facts together with Dense retrieval and Index compression, we achieve up to a
10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the
classical fact-checking pipelines over large collections.
</summary>
    <author>
      <name>Kevin Nanekhan</name>
    </author>
    <author>
      <name>Venktesh V</name>
    </author>
    <author>
      <name>Erik Martin</name>
    </author>
    <author>
      <name>Henrik Vatndal</name>
    </author>
    <author>
      <name>Vinay Setty</name>
    </author>
    <author>
      <name>Avishek Anand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECIR 2025, 15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.05803v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.05803v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.16101v2</id>
    <updated>2025-05-21T21:12:31Z</updated>
    <published>2025-02-22T05:50:15Z</published>
    <title>Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the
  Robustness of RAG Against Misleading Retrievals</title>
    <summary>  Retrieval-augmented generation (RAG) has shown impressive capabilities in
mitigating hallucinations in large language models (LLMs). However, LLMs
struggle to handle misleading retrievals and often fail to maintain their own
reasoning when exposed to conflicting or selectively-framed evidence, making
them vulnerable to real-world misinformation. In such real-world retrieval
scenarios, misleading and conflicting information is rampant, particularly in
the political domain, where evidence is often selectively framed, incomplete,
or polarized. However, existing RAG benchmarks largely assume a clean retrieval
setting, where models succeed by accurately retrieving and generating answers
from gold-standard documents. This assumption fails to align with real-world
conditions, leading to an overestimation of RAG system performance. To bridge
this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate
the robustness of RAG systems against misleading retrievals. Unlike prior
benchmarks that rely on synthetic noise, our dataset constructs its retrieval
corpus from Reddit discussions, capturing naturally occurring misinformation.
It categorizes retrieved evidence into three types: supporting, misleading, and
irrelevant, providing a realistic and challenging testbed for assessing how
well RAG systems navigate different retrieval information. Our benchmark
experiments reveal that when exposed to misleading retrievals, all tested
LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no
retrieval at all), highlighting their susceptibility to noisy environments. To
the best of our knowledge, RAGuard is the first benchmark to systematically
assess RAG robustness against misleading evidence. We expect this benchmark
will drive future research toward improving RAG systems beyond idealized
datasets, making them more reliable for real-world applications.
</summary>
    <author>
      <name>Linda Zeng</name>
    </author>
    <author>
      <name>Rithwik Gupta</name>
    </author>
    <author>
      <name>Divij Motwani</name>
    </author>
    <author>
      <name>Diji Yang</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2502.16101v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.16101v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.16573v1</id>
    <updated>2025-02-23T13:45:47Z</updated>
    <published>2025-02-23T13:45:47Z</published>
    <title>LawPal : A Retrieval Augmented Generation Based System for Enhanced
  Legal Accessibility in India</title>
    <summary>  Access to legal knowledge in India is often hindered by a lack of awareness,
misinformation and limited accessibility to judicial resources. Many
individuals struggle to navigate complex legal frameworks, leading to the
frequent misuse of laws and inadequate legal protection. To address these
issues, we propose a Retrieval-Augmented Generation (RAG)-based legal chatbot
powered by vectorstore oriented FAISS for efficient and accurate legal
information retrieval. Unlike traditional chatbots, our model is trained using
an extensive dataset comprising legal books, official documentation and the
Indian Constitution, ensuring accurate responses to even the most complex or
misleading legal queries. The chatbot leverages FAISS for rapid vector-based
search, significantly improving retrieval speed and accuracy. It is also
prompt-engineered to handle twisted or ambiguous legal questions, reducing the
chances of incorrect interpretations. Apart from its core functionality of
answering legal queries, the platform includes additional features such as
real-time legal news updates, legal blogs, and access to law-related books,
making it a comprehensive resource for users. By integrating advanced AI
techniques with an optimized retrieval system, our chatbot aims to democratize
legal knowledge, enhance legal literacy, and prevent the spread of
misinformation. The study demonstrates that our approach effectively improves
legal accessibility while maintaining high accuracy and efficiency, thereby
contributing to a more informed and empowered society.
</summary>
    <author>
      <name>Dnyanesh Panchal</name>
    </author>
    <author>
      <name>Aaryan Gole</name>
    </author>
    <author>
      <name>Vaibhav Narute</name>
    </author>
    <author>
      <name>Raunak Joshi</name>
    </author>
    <link href="http://arxiv.org/abs/2502.16573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.16573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17832v2</id>
    <updated>2025-03-09T02:52:43Z</updated>
    <published>2025-02-25T04:23:59Z</published>
    <title>MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning
  Attacks</title>
    <summary>  Multimodal large language models (MLLMs) equipped with Retrieval Augmented
Generation (RAG) leverage both their rich parametric knowledge and the dynamic,
external knowledge to excel in tasks such as Question Answering. While RAG
enhances MLLMs by grounding responses in query-relevant external knowledge,
this reliance poses a critical yet underexplored safety risk: knowledge
poisoning attacks, where misinformation or irrelevant knowledge is
intentionally injected into external knowledge bases to manipulate model
outputs to be incorrect and even harmful. To expose such vulnerabilities in
multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack
framework with two attack strategies: Localized Poisoning Attack (LPA), which
injects query-specific misinformation in both text and images for targeted
manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance
during MLLM generation to elicit nonsensical responses across all queries. We
evaluate our attacks across multiple tasks, models, and access settings,
demonstrating that LPA successfully manipulates the MLLM to generate
attacker-controlled answers, with a success rate of up to 56% on MultiModalQA.
Moreover, GPA completely disrupts model generation to 0% accuracy with just a
single irrelevant knowledge injection. Our results highlight the urgent need
for robust defenses against knowledge poisoning to safeguard multimodal RAG
frameworks.
</summary>
    <author>
      <name>Hyeonjeong Ha</name>
    </author>
    <author>
      <name>Qiusi Zhan</name>
    </author>
    <author>
      <name>Jeonghwan Kim</name>
    </author>
    <author>
      <name>Dimitrios Bralios</name>
    </author>
    <author>
      <name>Saikrishna Sanniboina</name>
    </author>
    <author>
      <name>Nanyun Peng</name>
    </author>
    <author>
      <name>Kai-Wei Chang</name>
    </author>
    <author>
      <name>Daniel Kang</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available at https://github.com/HyeonjeongHa/MM-PoisonRAG</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.17832v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17832v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.01829v2</id>
    <updated>2025-03-06T22:13:20Z</updated>
    <published>2025-03-03T18:53:21Z</published>
    <title>Persuade Me if You Can: A Framework for Evaluating Persuasion
  Effectiveness and Susceptibility Among Large Language Models</title>
    <summary>  Large Language Models (LLMs) demonstrate persuasive capabilities that rival
human-level persuasion. While these capabilities can be used for social good,
they also present risks of potential misuse. Moreover, LLMs' susceptibility to
persuasion raises concerns about alignment with ethical principles. To study
these dynamics, we introduce Persuade Me If You Can (PMIYC), an automated
framework for evaluating persuasion through multi-agent interactions. Here,
Persuader agents engage in multi-turn conversations with the Persuadee agents,
allowing us to measure LLMs' persuasive effectiveness and their susceptibility
to persuasion. We conduct comprehensive evaluations across diverse LLMs,
ensuring each model is assessed against others in both subjective and
misinformation contexts. We validate the efficacy of our framework through
human evaluations and show alignment with prior work. PMIYC offers a scalable
alternative to human annotation for studying persuasion in LLMs. Through PMIYC,
we find that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness,
outperforming Claude 3 Haiku by 30%. However, GPT-4o demonstrates over 50%
greater resistance to persuasion for misinformation compared to Llama-3.3-70B.
These findings provide empirical insights into the persuasive dynamics of LLMs
and contribute to the development of safer AI systems.
</summary>
    <author>
      <name>Nimet Beyza Bozdag</name>
    </author>
    <author>
      <name>Shuhaib Mehri</name>
    </author>
    <author>
      <name>Gokhan Tur</name>
    </author>
    <author>
      <name>Dilek Hakkani-T√ºr</name>
    </author>
    <link href="http://arxiv.org/abs/2503.01829v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.01829v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.04957v1</id>
    <updated>2025-03-06T20:43:14Z</updated>
    <published>2025-03-06T20:43:14Z</published>
    <title>SafeArena: Evaluating the Safety of Autonomous Web Agents</title>
    <summary>  LLM-based agents are becoming increasingly proficient at solving web-based
tasks. With this capability comes a greater risk of misuse for malicious
purposes, such as posting misinformation in an online forum or selling illicit
substances on a website. To evaluate these risks, we propose SafeArena, the
first benchmark to focus on the deliberate misuse of web agents. SafeArena
comprises 250 safe and 250 harmful tasks across four websites. We classify the
harmful tasks into five harm categories -- misinformation, illegal activity,
harassment, cybercrime, and social bias, designed to assess realistic misuses
of web agents. We evaluate leading LLM-based web agents, including GPT-4o,
Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To
systematically assess their susceptibility to harmful tasks, we introduce the
Agent Risk Assessment framework that categorizes agent behavior across four
risk levels. We find agents are surprisingly compliant with malicious requests,
with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests,
respectively. Our findings highlight the urgent need for safety alignment
procedures for web agents. Our benchmark is available here:
https://safearena.github.io
</summary>
    <author>
      <name>Ada Defne Tur</name>
    </author>
    <author>
      <name>Nicholas Meade</name>
    </author>
    <author>
      <name>Xing Han L√π</name>
    </author>
    <author>
      <name>Alejandra Zambrano</name>
    </author>
    <author>
      <name>Arkil Patel</name>
    </author>
    <author>
      <name>Esin Durmus</name>
    </author>
    <author>
      <name>Spandana Gella</name>
    </author>
    <author>
      <name>Karolina Sta≈Ñczak</name>
    </author>
    <author>
      <name>Siva Reddy</name>
    </author>
    <link href="http://arxiv.org/abs/2503.04957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.04957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11000v1</id>
    <updated>2025-04-15T09:16:17Z</updated>
    <published>2025-04-15T09:16:17Z</published>
    <title>Why am I seeing this? Towards recognizing social media recommender
  systems with missing recommendations</title>
    <summary>  Social media plays a crucial role in shaping society, often amplifying
polarization and spreading misinformation. These effects stem from complex
dynamics involving user interactions, individual traits, and recommender
algorithms driving content selection. Recommender systems, which significantly
shape the content users see and decisions they make, offer an opportunity for
intervention and regulation. However, assessing their impact is challenging due
to algorithmic opacity and limited data availability. To effectively model user
decision-making, it is crucial to recognize the recommender system adopted by
the platform.
  This work introduces a method for Automatic Recommender Recognition using
Graph Neural Networks (GNNs), based solely on network structure and observed
behavior. To infer the hidden recommender, we first train a Recommender Neutral
User model (RNU) using a GNN and an adapted hindsight academic network
recommender, aiming to reduce reliance on the actual recommender in the data.
We then generate several Recommender Hypothesis-specific Synthetic Datasets
(RHSD) by combining the RNU with different known recommenders, producing ground
truths for testing. Finally, we train Recommender Hypothesis-specific User
models (RHU) under various hypotheses and compare each candidate with the
original used to generate the RHSD.
  Our approach enables accurate detection of hidden recommenders and their
influence on user behavior. Unlike audit-based methods, it captures system
behavior directly, without ad hoc experiments that often fail to reflect real
platforms. This study provides insights into how recommenders shape behavior,
aiding efforts to reduce polarization and misinformation.
</summary>
    <author>
      <name>Sabrina Guidotti</name>
    </author>
    <author>
      <name>Sabrina Patania</name>
    </author>
    <author>
      <name>Giuseppe Vizzari</name>
    </author>
    <author>
      <name>Dimitri Ognibene</name>
    </author>
    <author>
      <name>Gregor Donabauer</name>
    </author>
    <author>
      <name>Udo Kruschwitz</name>
    </author>
    <author>
      <name>Davide Taibi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at RLDM 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.14171v1</id>
    <updated>2025-04-19T04:18:32Z</updated>
    <published>2025-04-19T04:18:32Z</published>
    <title>Adaptation Method for Misinformation Identification</title>
    <summary>  Multimodal fake news detection plays a crucial role in combating online
misinformation. Unfortunately, effective detection methods rely on annotated
labels and encounter significant performance degradation when domain shifts
exist between training (source) and test (target) data. To address the
problems, we propose ADOSE, an Active Domain Adaptation (ADA) framework for
multimodal fake news detection which actively annotates a small subset of
target samples to improve detection performance. To identify various deceptive
patterns in cross-domain settings, we design multiple expert classifiers to
learn dependencies across different modalities. These classifiers specifically
target the distinct deception patterns exhibited in fake news, where two
unimodal classifiers capture knowledge errors within individual modalities
while one cross-modal classifier identifies semantic inconsistencies between
text and images. To reduce annotation costs from the target domain, we propose
a least-disagree uncertainty selector with a diversity calculator for selecting
the most informative samples. The selector leverages prediction disagreement
before and after perturbations by multiple classifiers as an indicator of
uncertain samples, whose deceptive patterns deviate most from source domains.
It further incorporates diversity scores derived from multi-view features to
ensure the chosen samples achieve maximal coverage of target domain features.
The extensive experiments on multiple datasets show that ADOSE outperforms
existing ADA methods by 2.72\% $\sim$ 14.02\%, indicating the superiority of
our model.
</summary>
    <author>
      <name>Yangping Chen</name>
    </author>
    <author>
      <name>Weijie Shi</name>
    </author>
    <author>
      <name>Mengze Li</name>
    </author>
    <author>
      <name>Yue Cui</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Jia Zhu</name>
    </author>
    <author>
      <name>Jiajie Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.14171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.14171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17204v1</id>
    <updated>2025-04-24T02:29:50Z</updated>
    <published>2025-04-24T02:29:50Z</published>
    <title>Factually: Exploring Wearable Fact-Checking for Augmented Truth
  Discernment</title>
    <summary>  Wearable devices are transforming human capabilities by seamlessly augmenting
cognitive functions. In this position paper, we propose a voice-based,
interactive learning companion designed to amplify and extend cognitive
abilities through informal learning. Our vision is threefold: (1) to enable
users to discover new knowledge on-the-go through contextual interactive
quizzes, fostering critical thinking and mindfulness, (2) to proactively detect
misinformation, empowering users to critically assess information in real time,
and (3) to provide spoken language correction and prompting hints for second
language learning and effective communication. As an initial step toward this
vision, we present Factually - a proactive, wearable fact-checking system
integrated into devices like smartwatches or rings. Factually discreetly alerts
users to potential falsehoods via vibrotactile feedback, helping them assess
information critically. We demonstrate its utility through three illustrative
scenarios, highlighting its potential to extend cognitive abilities for
real-time misinformation detection. Early qualitative feedback suggests that
Factually can enhance users' fact-checking capabilities, offering both
practical and experiential benefits.
</summary>
    <author>
      <name>Chitralekha Gupta</name>
    </author>
    <author>
      <name>Hanjun Wu</name>
    </author>
    <author>
      <name>Praveen Sasikumar</name>
    </author>
    <author>
      <name>Shreyas Sridhar</name>
    </author>
    <author>
      <name>Priambudi Bagaskara</name>
    </author>
    <author>
      <name>Suranga Nanayakkara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2025 ACM Workshop on Human-AI Interaction for
  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction
  for Augmented Reasoning</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2504.17204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02009v2</id>
    <updated>2025-05-21T06:50:54Z</updated>
    <published>2025-05-04T06:37:20Z</published>
    <title>Towards Safer Pretraining: Analyzing and Filtering Harmful Content in
  Webscale datasets for Responsible LLMs</title>
    <summary>  Large language models (LLMs) have become integral to various real-world
applications, leveraging massive, web-sourced datasets like Common Crawl, C4,
and FineWeb for pretraining. While these datasets provide linguistic data
essential for high-quality natural language generation, they often contain
harmful content, such as hate speech, misinformation, and biased narratives.
Training LLMs on such unfiltered data risks perpetuating toxic behaviors,
spreading misinformation, and amplifying societal biases which can undermine
trust in LLM-driven applications and raise ethical concerns about their use.
This paper presents a large-scale analysis of inappropriate content across
these datasets, offering a comprehensive taxonomy that categorizes harmful
webpages into Topical and Toxic based on their intent. We also introduce a
prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and
a transformer-based model (HarmFormer) for harmful content filtering.
Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC)
and provide crucial insights into how models respond to adversarial toxic
inputs. We share TTP, TTP-Eval, HAVOC and a sample of C4 inferenced on
HarmFormer. Our work offers insights into ensuring safer LLM pretraining and
serves as a resource for Responsible AI (RAI) compliance.
</summary>
    <author>
      <name>Sai Krishna Mendu</name>
    </author>
    <author>
      <name>Harish Yenala</name>
    </author>
    <author>
      <name>Aditi Gulati</name>
    </author>
    <author>
      <name>Shanu Kumar</name>
    </author>
    <author>
      <name>Parag Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures. Accepted at the International Joint Conferences
  on Artificial Intelligence IJCAI 2025 (main track)</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.02009v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02009v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02317v1</id>
    <updated>2025-05-05T02:21:44Z</updated>
    <published>2025-05-05T02:21:44Z</published>
    <title>A longitudinal analysis of misinformation, polarization and toxicity on
  Bluesky after its public launch</title>
    <summary>  Bluesky is a decentralized, Twitter-like social media platform that has
rapidly gained popularity. Following an invite-only phase, it officially opened
to the public on February 6th, 2024, leading to a significant expansion of its
user base. In this paper, we present a longitudinal analysis of user activity
in the two months surrounding its public launch, examining how the platform
evolved due to this rapid growth. Our analysis reveals that Bluesky exhibits an
activity distribution comparable to more established social platforms, yet it
features a higher volume of original content relative to reshared posts and
maintains low toxicity levels. We further investigate the political leanings of
its user base, misinformation dynamics, and engagement in harmful
conversations. Our findings indicate that Bluesky users predominantly lean left
politically and tend to share high-credibility sources. After the platform's
public launch, an influx of new users, particularly those posting in English
and Japanese, contributed to a surge in activity. Among them, several accounts
displayed suspicious behaviors, such as mass-following users and sharing
content from low-credibility news sources. Some of these accounts have already
been flagged as spam or suspended, suggesting that Bluesky's moderation efforts
have been effective.
</summary>
    <author>
      <name>Gianluca Nogara</name>
    </author>
    <author>
      <name>Erfan Samieyan Sahneh</name>
    </author>
    <author>
      <name>Matthew R. DeVerna</name>
    </author>
    <author>
      <name>Nick Liu</name>
    </author>
    <author>
      <name>Luca Luceri</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <author>
      <name>Francesco Pierri</name>
    </author>
    <author>
      <name>Silvia Giordano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 11 figures, 3 tables, submitted to special issue
  "Disinformation, toxicity, harms in Online Social Networks and Media" OSNEM</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.02317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P25, 91D30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5; K.4.1; K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04673v1</id>
    <updated>2025-05-07T10:09:55Z</updated>
    <published>2025-05-07T10:09:55Z</published>
    <title>REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM</title>
    <summary>  Vision Large Language Models (VLLMs) represent a significant advancement in
artificial intelligence by integrating image-processing capabilities with
textual understanding, thereby enhancing user interactions and expanding
application domains. However, their increased complexity introduces novel
safety and ethical challenges, particularly in multi-modal and multi-turn
conversations. Traditional safety evaluation frameworks, designed for
text-based, single-turn interactions, are inadequate for addressing these
complexities. To bridge this gap, we introduce the REVEAL (Responsible
Evaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated
pipeline for evaluating image-input harms in VLLMs. REVEAL includes automated
image mining, synthetic adversarial data generation, multi-turn conversational
expansion using crescendo attack strategies, and comprehensive harm assessment
through evaluators like GPT-4o.
  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2,
Qwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual
harm, violence, and misinformation. Our findings reveal that multi-turn
interactions result in significantly higher defect rates compared to
single-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably,
GPT-4o demonstrated the most balanced performance as measured by our
Safety-Usability Index (SUI) followed closely by Pixtral. Additionally,
misinformation emerged as a critical area requiring enhanced contextual
defenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \%$) while
Qwen2-VL showed the highest MT refusal rate ($19.1 \%$).
</summary>
    <author>
      <name>Madhur Jindal</name>
    </author>
    <author>
      <name>Saurabh Deshpande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages (8 main), to be published in IJCAI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.04673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.07891v2</id>
    <updated>2025-06-22T15:39:02Z</updated>
    <published>2025-05-11T17:00:21Z</published>
    <title>TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for
  Fact-Checking</title>
    <summary>  In the age of social media, the rapid spread of misinformation and rumors has
led to the emergence of infodemics, where false information poses a significant
threat to society. To combat this issue, we introduce TrumorGPT, a novel
generative artificial intelligence solution designed for fact-checking in the
health domain. TrumorGPT aims to distinguish "trumors", which are
health-related rumors that turn out to be true, providing a crucial tool in
differentiating between mere speculation and verified facts. This framework
leverages a large language model (LLM) with few-shot learning for semantic
health knowledge graph construction and semantic reasoning. TrumorGPT
incorporates graph-based retrieval-augmented generation (GraphRAG) to address
the hallucination issue common in LLMs and the limitations of static training
data. GraphRAG involves accessing and utilizing information from regularly
updated semantic health knowledge graphs that consist of the latest medical
news and health information, ensuring that fact-checking by TrumorGPT is based
on the most recent data. Evaluating with extensive healthcare datasets,
TrumorGPT demonstrates superior performance in fact-checking for public health
claims. Its ability to effectively conduct fact-checking across various
platforms marks a critical step forward in the fight against health-related
misinformation, enhancing trust and accuracy in the digital information age.
</summary>
    <author>
      <name>Ching Nam Hang</name>
    </author>
    <author>
      <name>Pei-Duo Yu</name>
    </author>
    <author>
      <name>Chee Wei Tan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TAI.2025.3567369</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TAI.2025.3567369" rel="related"/>
    <link href="http://arxiv.org/abs/2505.07891v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.07891v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15067v1</id>
    <updated>2025-05-21T03:33:13Z</updated>
    <published>2025-05-21T03:33:13Z</published>
    <title>Lawful but Awful: Evolving Legislative Responses to Address Online
  Misinformation, Disinformation, and Mal-Information in the Age of Generative
  AI</title>
    <summary>  "Fake news" is an old problem. In recent years, however, increasing usage of
social media as a source of information, the spread of unverified medical
advice during the Covid-19 pandemic, and the rise of generative artificial
intelligence have seen a rush of legislative proposals seeking to minimize or
mitigate the impact of false information spread online. Drawing on a novel
dataset of statutes and other instruments, this article analyses changing
perceptions about the potential harms caused by misinformation, disinformation,
and "mal-information". The turn to legislation began in countries that were
less free, in terms of civil liberties, and poorer, as measured by GDP per
capita. Internet penetration does not seem to have been a driving factor. The
focus of such laws is most frequently on national security broadly construed,
though 2020 saw a spike in laws addressing public health. Unsurprisingly,
governments with fewer legal constraints on government action have generally
adopted more robust positions in dealing with false information. Despite early
reservations, however, growth in such laws is now steepest in Western states.
Though there are diverse views on the appropriate response to false information
online, the need for legislation of some kind appears now to be global. The
question is no longer whether to regulate "lawful but awful" speech online, but
how.
</summary>
    <author>
      <name>Simon Chesterman</name>
    </author>
    <link href="http://arxiv.org/abs/2505.15067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16702v1</id>
    <updated>2025-05-22T14:03:20Z</updated>
    <published>2025-05-22T14:03:20Z</published>
    <title>Truth and Trust: Fake News Detection via Biosignals</title>
    <summary>  Understanding how individuals physiologically respond to false information is
crucial for advancing misinformation detection systems. This study explores the
potential of using physiological signals, specifically electrodermal activity
(EDA) and photoplethysmography (PPG), to classify both the veracity of
information and its interaction with user belief. In a controlled laboratory
experiment, we collected EDA and PPG signals while participants evaluated the
truthfulness of climate-related claims. Each trial was labeled based on the
objective truth of the claim and the participant's belief, enabling two
classification tasks: binary veracity detection and a novel four-class joint
belief-veracity classification. We extracted handcrafted features from the raw
signals and trained several machine learning models to benchmark the dataset.
Our results show that EDA outperforms PPG, indicating its greater sensitivity
to physiological responses related to truth perception. However, performance
significantly drops in the joint belief-veracity classification task,
highlighting the complexity of modeling the interaction between belief and
truth. These findings suggest that while physiological signals can reflect
basic truth perception, accurately modeling the intricate relationships between
belief and veracity remains a significant challenge. This study emphasizes the
importance of multimodal approaches that incorporate psychological,
physiological, and cognitive factors to improve fake news detection systems.
Our work provides a foundation for future research aimed at enhancing
misinformation detection via addressing the complexities of human belief and
truth processing.
</summary>
    <author>
      <name>Gennie Nguyen</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Yangxueqing Jiang</name>
    </author>
    <author>
      <name>Tom Gedeon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Research report</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.16702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.19191v1</id>
    <updated>2025-05-25T15:35:24Z</updated>
    <published>2025-05-25T15:35:24Z</published>
    <title>Misleading through Inconsistency: A Benchmark for Political
  Inconsistencies Detection</title>
    <summary>  Inconsistent political statements represent a form of misinformation. They
erode public trust and pose challenges to accountability, when left unnoticed.
Detecting inconsistencies automatically could support journalists in asking
clarification questions, thereby helping to keep politicians accountable. We
propose the Inconsistency detection task and develop a scale of inconsistency
types to prompt NLP-research in this direction. To provide a resource for
detecting inconsistencies in a political domain, we present a dataset of 698
human-annotated pairs of political statements with explanations of the
annotators' reasoning for 237 samples. The statements mainly come from voting
assistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland,
reflecting real-world political issues. We benchmark Large Language Models
(LLMs) on our dataset and show that in general, they are as good as humans at
detecting inconsistencies, and might be even better than individual humans at
predicting the crowd-annotated ground-truth. However, when it comes to
identifying fine-grained inconsistency types, none of the model have reached
the upper bound of performance (due to natural labeling variation), thus
leaving room for improvement. We make our dataset and code publicly available.
</summary>
    <author>
      <name>Nursulu Sagimbayeva</name>
    </author>
    <author>
      <name>Ruveyda Bet√ºl Bah√ßeci</name>
    </author>
    <author>
      <name>Ingmar Weber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures. Accepted for publication in the Proceedings of
  1st Workshop on Misinformation Detection in the Era of LLMs (MisD) at
  ICWSM-2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.19191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.19191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.01587v1</id>
    <updated>2025-06-02T12:19:28Z</updated>
    <published>2025-06-02T12:19:28Z</published>
    <title>Unified Large Language Models for Misinformation Detection in
  Low-Resource Linguistic Settings</title>
    <summary>  The rapid expansion of social media platforms has significantly increased the
dissemination of forged content and misinformation, making the detection of
fake news a critical area of research. Although fact-checking efforts
predominantly focus on English-language news, there is a noticeable gap in
resources and strategies to detect news in regional languages, such as Urdu.
Advanced Fake News Detection (FND) techniques rely heavily on large, accurately
labeled datasets. However, FND in under-resourced languages like Urdu faces
substantial challenges due to the scarcity of extensive corpora and the lack of
validated lexical resources. Current Urdu fake news datasets are often
domain-specific and inaccessible to the public. They also lack human
verification, relying mainly on unverified English-to-Urdu translations, which
compromises their reliability in practical applications. This study highlights
the necessity of developing reliable, expert-verified, and domain-independent
Urdu-enhanced FND datasets to improve fake news detection in Urdu and other
resource-constrained languages. This paper presents the first benchmark large
FND dataset for Urdu news, which is publicly available for validation and deep
analysis. We also evaluate this dataset using multiple state-of-the-art
pre-trained large language models (LLMs), such as XLNet, mBERT, XLM-RoBERTa,
RoBERTa, DistilBERT, and DeBERTa. Additionally, we propose a unified LLM model
that outperforms the others with different embedding and feature extraction
techniques. The performance of these models is compared based on accuracy, F1
score, precision, recall, and human judgment for vetting the sample results of
news.
</summary>
    <author>
      <name>Muhammad Islam</name>
    </author>
    <author>
      <name>Javed Ali Khan</name>
    </author>
    <author>
      <name>Mohammed Abaker</name>
    </author>
    <author>
      <name>Ali Daud</name>
    </author>
    <author>
      <name>Azeem Irshad</name>
    </author>
    <link href="http://arxiv.org/abs/2506.01587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.01587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.04222v1</id>
    <updated>2025-07-06T03:25:38Z</updated>
    <published>2025-07-06T03:25:38Z</published>
    <title>VaxPulse: Active Global Vaccine Infodemic Risk Assessment</title>
    <summary>  Vaccine infodemics, driven by misinformation, disinformation, and inauthentic
online behaviours, pose significant threats to global public health. This paper
presents our response to this challenge, demonstrating how we developed
VaxPulse Vaccine Infodemic Risk Assessment Lifecycle (VIRAL), an AI-powered
social listening platform designed to monitor and assess vaccine-related
infodemic risks. Leveraging interdisciplinary expertise and international
collaborations, VaxPulse VIRAL integrates machine learning methods, including
deep learning, active learning, and data augmentation, to provide real-time
insights into public sentiments, misinformation trends, and social bot
activity. Iterative feedback from domain experts and stakeholders has guided
the development of dynamic dashboards that offer tailored, actionable insights
to support immunisation programs and address information disorder. Ongoing
improvements to VaxPulse will continue through collaboration with our
international network and community leaders.
</summary>
    <author>
      <name>Gerardo Luis Dimaguila</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Epidemiology Informatics, Centre for Health Analytics, Melbourne Children's Campus, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAEFVIC, Murdoch Children's Research Institute, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Paediatrics, The University of Melbourne, Australia</arxiv:affiliation>
    </author>
    <author>
      <name>Muhammad Javed</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Epidemiology Informatics, Centre for Health Analytics, Melbourne Children's Campus, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAEFVIC, Murdoch Children's Research Institute, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Paediatrics, The University of Melbourne, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Global Vaccine Data Network, University of Auckland, New Zealand</arxiv:affiliation>
    </author>
    <author>
      <name>Jeremiah Munakabayo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Epidemiology Informatics, Centre for Health Analytics, Melbourne Children's Campus, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAEFVIC, Murdoch Children's Research Institute, Australia</arxiv:affiliation>
    </author>
    <author>
      <name>Sedigh Khademi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAEFVIC, Murdoch Children's Research Institute, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Paediatrics, The University of Melbourne, Australia</arxiv:affiliation>
    </author>
    <author>
      <name>Hazel Clothier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Epidemiology Informatics, Centre for Health Analytics, Melbourne Children's Campus, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAEFVIC, Murdoch Children's Research Institute, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Paediatrics, The University of Melbourne, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Global Vaccine Data Network, University of Auckland, New Zealand</arxiv:affiliation>
    </author>
    <author>
      <name>Joanne Hickman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Epidemiology Informatics, Centre for Health Analytics, Melbourne Children's Campus, Australia</arxiv:affiliation>
    </author>
    <author>
      <name>Jim Buttery</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Epidemiology Informatics, Centre for Health Analytics, Melbourne Children's Campus, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAEFVIC, Murdoch Children's Research Institute, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Paediatrics, The University of Melbourne, Australia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Global Vaccine Data Network, University of Auckland, New Zealand</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Infectious Diseases, Department of General Medicine, Royal Children's Hospital, Australia</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 images, Full Paper MedInfo 2025 conference, to be
  published in Studies in Health Technology and Informatics</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.04222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.04222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.14242v1</id>
    <updated>2025-07-17T21:19:47Z</updated>
    <published>2025-07-17T21:19:47Z</published>
    <title>Culling Misinformation from Gen AI: Toward Ethical Curation and
  Refinement</title>
    <summary>  While Artificial Intelligence (AI) is not a new field, recent developments,
especially with the release of generative tools like ChatGPT, have brought it
to the forefront of the minds of industry workers and academic folk alike.
There is currently much talk about AI and its ability to reshape many everyday
processes as we know them through automation. It also allows users to expand
their ideas by suggesting things they may not have thought of on their own and
provides easier access to information. However, not all of the changes this
technology will bring or has brought so far are positive; this is why it is
extremely important for all modern people to recognize and understand the risks
before using these tools and allowing them to cause harm. This work takes a
position on better understanding many equity concerns and the spread of
misinformation that result from new AI, in this case, specifically ChatGPT and
deepfakes, and encouraging collaboration with law enforcement, developers, and
users to reduce harm. Considering many academic sources, it warns against these
issues, analyzing their cause and impact in fields including healthcare,
education, science, academia, retail, and finance. Lastly, we propose a set of
future-facing guidelines and policy considerations to solve these issues while
still enabling innovation in these fields, this responsibility falling upon
users, developers, and government entities.
</summary>
    <author>
      <name>Prerana Khatiwada</name>
    </author>
    <author>
      <name>Grace Donaher</name>
    </author>
    <author>
      <name>Jasymyn Navarro</name>
    </author>
    <author>
      <name>Lokesh Bhatta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2507.14242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2507.14242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.06906v1</id>
    <updated>2020-10-14T09:37:51Z</updated>
    <published>2020-10-14T09:37:51Z</published>
    <title>No Rumours Please! A Multi-Indic-Lingual Approach for COVID Fake-Tweet
  Detection</title>
    <summary>  The sudden widespread menace created by the present global pandemic COVID-19
has had an unprecedented effect on our lives. Man-kind is going through
humongous fear and dependence on social media like never before. Fear
inevitably leads to panic, speculations, and the spread of misinformation. Many
governments have taken measures to curb the spread of such misinformation for
public well being. Besides global measures, to have effective outreach, systems
for demographically local languages have an important role to play in this
effort. Towards this, we propose an approach to detect fake news about COVID-19
early on from social media, such as tweets, for multiple Indic-Languages
besides English. In addition, we also create an annotated dataset of Hindi and
Bengali tweet for fake news detection. We propose a BERT based model augmented
with additional relevant features extracted from Twitter to identify fake
tweets. To expand our approach to multiple Indic languages, we resort to mBERT
based model which is fine-tuned over created dataset in Hindi and Bengali. We
also propose a zero-shot learning approach to alleviate the data scarcity issue
for such low resource languages. Through rigorous experiments, we show that our
approach reaches around 89% F-Score in fake tweet detection which supercedes
the state-of-the-art (SOTA) results. Moreover, we establish the first benchmark
for two Indic-Languages, Hindi and Bengali. Using our annotated data, our model
achieves about 79% F-Score in Hindi and 81% F-Score for Bengali Tweets. Our
zero-shot model achieves about 81% F-Score in Hindi and 78% F-Score for Bengali
Tweets without any annotated data, which clearly indicates the efficacy of our
approach.
</summary>
    <author>
      <name>Debanjana Kar</name>
    </author>
    <author>
      <name>Mohit Bhardwaj</name>
    </author>
    <author>
      <name>Suranjana Samanta</name>
    </author>
    <author>
      <name>Amar Prakash Azad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.06906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.06906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.05940v1</id>
    <updated>2020-08-13T14:48:03Z</updated>
    <published>2020-08-13T14:48:03Z</published>
    <title>Impossible by Conventional Means: Ten Years on from the DARPA Red
  Balloon Challenge</title>
    <summary>  Ten years ago, DARPA launched the 'Network Challenge', more commonly known as
the 'DARPA Red Balloon Challenge'. Ten red weather balloons were fixed at
unknown locations in the US. An open challenge was launched to locate all ten,
the first to do so would be declared the winner receiving a cash prize. A team
from MIT Media Lab was able to locate them all within 9 hours using social
media and a novel reward scheme that rewarded viral recruitment. This
achievement was rightly seen as proof of the remarkable ability of social
media, then relatively nascent, to solve real world problems such as
large-scale spatial search. Upon reflection, however, the challenge was also
remarkable as it succeeded despite many efforts to provide false information on
the location of the balloons. At the time the false reports were filtered based
on manual inspection of visual proof and comparing the IP addresses of those
reporting with the purported coordinates of the balloons. In the ten years
since, misinformation on social media has grown in prevalence and
sophistication to be one of the defining social issues of our time. Seen
differently we can cast the misinformation observed in the Red Balloon
Challenge, and unexpected adverse effects in other social mobilisation
challenges subsequently, not as bugs but as essential features. We further
investigate the role of the increasing levels of political polarisation in
modulating social mobilisation. We confirm that polarisation not only impedes
the overall success of mobilisation, but also leads to a low reachability to
oppositely polarised states, significantly hampering recruitment. We find that
diversifying geographic pathways of social influence are key to circumvent
barriers of political mobilisation and can boost the success of new open
challenges.
</summary>
    <author>
      <name>Alex Rutherford</name>
    </author>
    <author>
      <name>Manuel Cebrian</name>
    </author>
    <author>
      <name>Inho Hong</name>
    </author>
    <author>
      <name>Iyad Rahwan</name>
    </author>
    <link href="http://arxiv.org/abs/2008.05940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.05940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.07175v3</id>
    <updated>2021-12-14T04:37:59Z</updated>
    <published>2021-04-15T00:25:52Z</published>
    <title>Community-Based Fact-Checking on Twitter's Birdwatch Platform</title>
    <summary>  Misinformation undermines the credibility of social media and poses
significant threats to modern societies. As a countermeasure, Twitter has
recently introduced "Birdwatch," a community-driven approach to address
misinformation on Twitter. On Birdwatch, users can identify tweets they believe
are misleading, write notes that provide context to the tweet and rate the
quality of other users' notes. In this work, we empirically analyze how users
interact with this new feature. For this purpose, we collect {all} Birdwatch
notes and ratings between the introduction of the feature in early 2021 and end
of July 2021. We then map each Birdwatch note to the fact-checked tweet using
Twitter's historical API. In addition, we use text mining methods to extract
content characteristics from the text explanations in the Birdwatch notes
(e.g., sentiment). Our empirical analysis yields the following main findings:
(i) users more frequently file Birdwatch notes for misleading than not
misleading tweets. These misleading tweets are primarily reported because of
factual errors, lack of important context, or because they treat unverified
claims as facts. (ii) Birdwatch notes are more helpful to other users if they
link to trustworthy sources and if they embed a more positive sentiment. (iii)
The social influence of the author of the source tweet is associated with
differences in the level of user consensus. For influential users with many
followers, Birdwatch notes yield a lower level of consensus among users and
community-created fact checks are more likely to be seen as being incorrect and
argumentative. Altogether, our findings can help social media platforms to
formulate guidelines for users on how to write more helpful fact checks. At the
same time, our analysis suggests that community-based fact-checking faces
challenges regarding opinion speculation and polarization among the user base.
</summary>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <link href="http://arxiv.org/abs/2104.07175v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07175v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.11755v2</id>
    <updated>2021-09-19T10:14:15Z</updated>
    <published>2021-07-25T08:37:09Z</published>
    <title>Can the Crowd Judge Truthfulness? A Longitudinal Study on Recent
  Misinformation about COVID-19</title>
    <summary>  Recently, the misinformation problem has been addressed with a
crowdsourcing-based approach: to assess the truthfulness of a statement,
instead of relying on a few experts, a crowd of non-expert is exploited. We
study whether crowdsourcing is an effective and reliable method to assess
truthfulness during a pandemic, targeting statements related to COVID-19, thus
addressing (mis)information that is both related to a sensitive and personal
issue and very recent as compared to when the judgment is done. In our
experiments, crowd workers are asked to assess the truthfulness of statements,
and to provide evidence for the assessments. Besides showing that the crowd is
able to accurately judge the truthfulness of the statements, we report results
on workers behavior, agreement among workers, effect of aggregation functions,
of scales transformations, and of workers background and bias. We perform a
longitudinal study by re-launching the task multiple times with both novice and
experienced workers, deriving important insights on how the behavior and
quality change over time. Our results show that: workers are able to detect and
objectively categorize online (mis)information related to COVID-19; both
crowdsourced and expert judgments can be transformed and aggregated to improve
quality; worker background and other signals (e.g., source of information,
behavior) impact the quality of the data. The longitudinal study demonstrates
that the time-span has a major effect on the quality of the judgments, for both
novice and experienced workers. Finally, we provide an extensive failure
analysis of the statements misjudged by the crowd-workers.
</summary>
    <author>
      <name>Kevin Roitero</name>
    </author>
    <author>
      <name>Michael Soprano</name>
    </author>
    <author>
      <name>Beatrice Portelli</name>
    </author>
    <author>
      <name>Massimiliano De Luise</name>
    </author>
    <author>
      <name>Damiano Spina</name>
    </author>
    <author>
      <name>Vincenzo Della Mea</name>
    </author>
    <author>
      <name>Giuseppe Serra</name>
    </author>
    <author>
      <name>Stefano Mizzaro</name>
    </author>
    <author>
      <name>Gianluca Demartini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00779-021-01604-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00779-021-01604-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages; Preprint of an article accepted in Personal and Ubiquitous
  Computing (Special Issue on Intelligent Systems for Tackling Online Harms,
  2021). arXiv admin note: substantial text overlap with arXiv:2008.05701</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.11755v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.11755v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.07970v1</id>
    <updated>2022-05-16T20:20:43Z</updated>
    <published>2022-05-16T20:20:43Z</published>
    <title>SciLander: Mapping the Scientific News Landscape</title>
    <summary>  The COVID-19 pandemic has fueled the spread of misinformation on social media
and the Web as a whole. The phenomenon dubbed `infodemic' has taken the
challenges of information veracity and trust to new heights by massively
introducing seemingly scientific and technical elements into misleading
content. Despite the existing body of work on modeling and predicting
misinformation, the coverage of very complex scientific topics with inherent
uncertainty and an evolving set of findings, such as COVID-19, provides many
new challenges that are not easily solved by existing tools. To address these
issues, we introduce SciLander, a method for learning representations of news
sources reporting on science-based topics. SciLander extracts four
heterogeneous indicators for the news sources; two generic indicators that
capture (1) the copying of news stories between sources, and (2) the use of the
same terms to mean different things (i.e., the semantic shift of terms), and
two scientific indicators that capture (1) the usage of jargon and (2) the
stance towards specific citations. We use these indicators as signals of source
agreement, sampling pairs of positive (similar) and negative (dissimilar)
samples, and combine them in a unified framework to train unsupervised news
source embeddings with a triplet margin loss objective. We evaluate our method
on a novel COVID-19 dataset containing nearly 1M news articles from 500 sources
spanning a period of 18 months since the beginning of the pandemic in 2020. Our
results show that the features learned by our model outperform state-of-the-art
baseline methods on the task of news veracity classification. Furthermore, a
clustering analysis suggests that the learned representations encode
information about the reliability, political leaning, and partisanship bias of
these sources.
</summary>
    <author>
      <name>Maur√≠cio Gruppi</name>
    </author>
    <author>
      <name>Panayiotis Smeros</name>
    </author>
    <author>
      <name>Sibel Adalƒ±</name>
    </author>
    <author>
      <name>Carlos Castillo</name>
    </author>
    <author>
      <name>Karl Aberer</name>
    </author>
    <link href="http://arxiv.org/abs/2205.07970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.08039v3</id>
    <updated>2021-05-10T11:02:30Z</updated>
    <published>2020-05-16T16:52:12Z</published>
    <title>Improved x-space Algorithm for Min-Max Bilevel Integer Programming with
  an Application to Misinformation Spread in Social Networks</title>
    <summary>  In this work we propose an improvement of the $x$-space algorithm developed
for solving a class of min--max bilevel optimization problems (Tang Y., Richard
J.P.P., Smith J.C. (2016), A class of algorithms for mixed-integer bilevel
min--max optimization. Journal of Global Optimization, 66(2), 225--262). In
this setting, the leader of the upper level problem aims at restricting the
follower's decisions by minimizing an objective function, which the follower
intends to maximize in the lower level problem by making decisions still
available to her. The $x$-space algorithm solves upper and lower bound problems
consecutively until convergence, and requires the dualization of an
approximation of the follower's problem in formulating the lower bound problem.
We first reformulate the lower bound problem using the properties of an optimal
solution to the original formulation, which makes the dualization step
unnecessary. The reformulation makes possible the integration of a greedy
covering heuristic into the solution scheme, which results in a considerable
increase in the efficiency. The new algorithm referred to as the improved
$x$-space algorithm is implemented and applied to a recent min--max bilevel
optimization problem that arises in the context of reducing the misinformation
spread in social networks. It is also assessed on the benchmark instances of
two other bilevel problems: zero-one knapsack problem with interdiction and
maximum clique problem with interdiction. Numerical results indicate that the
performance of the new algorithm is superior to that of the original algorithm,
and also compares favorably with a recent algorithm developed for mixed-integer
bilevel linear programs.
</summary>
    <author>
      <name>K√ºbra Tanƒ±nmƒ±≈ü</name>
    </author>
    <author>
      <name>Necati Aras</name>
    </author>
    <author>
      <name>ƒ∞. Kuban Altƒ±nel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ejor.2021.05.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ejor.2021.05.008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 7 tables, 4 figures. To be published in EJOR</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.08039v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08039v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.12226v2</id>
    <updated>2020-10-01T16:44:25Z</updated>
    <published>2020-07-23T19:43:27Z</published>
    <title>Understanding the dynamics emerging from infodemics: A call to action
  for interdisciplinary research</title>
    <summary>  Research on infodemics, i.e., the rapid spread of (mis)information related to
a hazardous event, such as the COVID-19 pandemic, requires the integration of a
multiplicity of scientific disciplines. The dynamics emerging from infodemics
have the potential to generate complex behavioral patterns. In order to react
appropriately, it is of ultimate importance for the fields of Business and
Economics to understand the dynamics emerging from it. In the short run,
dynamics might lead to an adaptation in household spending or to a shift in
buying behavior towards online providers. In the long run, changes in
investments, consumer behavior, and markets are to be expected. We argue that
the dynamics emerge from complex interactions among multiple factors, such as
information and misinformation accessible for individuals and the formation and
revision of beliefs. (Mis)information accessible to individuals is, amongst
others, affected by algorithms specifically designed to provide personalized
information, while automated fact-checking algorithms can help reduce the
amount of circulating misinformation. The formation and revision of individual
(and probably false) beliefs and individual fact-checking and interpretation of
information are heavily affected by linguistic patterns inherent to information
during pandemics and infodemics and further factors, such as affect, intuition
and motives. We argue that, in order to get a deep(er) understanding of the
dynamics emerging from infodemics, the fields of Business and Economics should
integrate the perspectives of Computer Science and Information Systems,
(Computational) Linguistics, and Cognitive Science into the wider context of
economic systems (e.g., organizations, markets or industries) and propose a way
to do so.
</summary>
    <author>
      <name>Stephan Leitner</name>
    </author>
    <author>
      <name>Bartosz Gula</name>
    </author>
    <author>
      <name>Dietmar Jannach</name>
    </author>
    <author>
      <name>Ulrike Krieg-Holz</name>
    </author>
    <author>
      <name>Friederike Wall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.12226v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12226v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q11, 68U35, 91E10, 68T50, 91F20, 91B44, 91B70" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4; J.4; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.08537v5</id>
    <updated>2022-05-10T01:03:57Z</updated>
    <published>2021-02-17T02:35:13Z</published>
    <title>Political Bias and Factualness in News Sharing across more than 100,000
  Online Communities</title>
    <summary>  As civil discourse increasingly takes place online, misinformation and the
polarization of news shared in online communities have become ever more
relevant concerns with real world harms across our society. Studying online
news sharing at scale is challenging due to the massive volume of content which
is shared by millions of users across thousands of communities. Therefore,
existing research has largely focused on specific communities or specific
interventions, such as bans. However, understanding the prevalence and spread
of misinformation and polarization more broadly, across thousands of online
communities, is critical for the development of governance strategies,
interventions, and community design. Here, we conduct the largest study of news
sharing on reddit to date, analyzing more than 550 million links spanning 4
years. We use non-partisan news source ratings from Media Bias/Fact Check to
annotate links to news sources with their political bias and factualness. We
find that, compared to left-leaning communities, right-leaning communities have
105% more variance in the political bias of their news sources, and more links
to relatively-more biased sources, on average. We observe that reddit users'
voting and re-sharing behaviors generally decrease the visibility of extremely
biased and low factual content, which receives 20% fewer upvotes and 30% fewer
exposures from crossposts than more neutral or more factual content. This
suggests that reddit is more resilient to low factual content than Twitter. We
show that extremely biased and low factual content is very concentrated, with
99% of such content being shared in only 0.5% of communities, giving credence
to the recent strategy of community-wide bans and quarantines.
</summary>
    <author>
      <name>Galen Weld</name>
    </author>
    <author>
      <name>Maria Glenski</name>
    </author>
    <author>
      <name>Tim Althoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures. Published at ICWSM 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.08537v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08537v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.01222v2</id>
    <updated>2021-08-23T04:21:58Z</updated>
    <published>2021-08-03T00:44:55Z</published>
    <title>The Many Dimensions of Truthfulness: Crowdsourcing Misinformation
  Assessments on a Multidimensional Scale</title>
    <summary>  Recent work has demonstrated the viability of using crowdsourcing as a tool
for evaluating the truthfulness of public statements. Under certain conditions
such as: (1) having a balanced set of workers with different backgrounds and
cognitive abilities; (2) using an adequate set of mechanisms to control the
quality of the collected data; and (3) using a coarse grained assessment scale,
the crowd can provide reliable identification of fake news. However, fake news
are a subtle matter: statements can be just biased ("cherrypicked"), imprecise,
wrong, etc. and the unidimensional truth scale used in existing work cannot
account for such differences. In this paper we propose a multidimensional
notion of truthfulness and we ask the crowd workers to assess seven different
dimensions of truthfulness selected based on existing literature: Correctness,
Neutrality, Comprehensibility, Precision, Completeness, Speaker's
Trustworthiness, and Informativeness. We deploy a set of quality control
mechanisms to ensure that the thousands of assessments collected on 180
publicly available fact-checked statements distributed over two datasets are of
adequate quality, including a custom search engine used by the crowd workers to
find web pages supporting their truthfulness assessments. A comprehensive
analysis of crowdsourced judgments shows that: (1) the crowdsourced assessments
are reliable when compared to an expert-provided gold standard; (2) the
proposed dimensions of truthfulness capture independent pieces of information;
(3) the crowdsourcing task can be easily learned by the workers; and (4) the
resulting assessments provide a useful basis for a more complete estimation of
statement truthfulness.
</summary>
    <author>
      <name>Michael Soprano</name>
    </author>
    <author>
      <name>Kevin Roitero</name>
    </author>
    <author>
      <name>David La Barbera</name>
    </author>
    <author>
      <name>Davide Ceolin</name>
    </author>
    <author>
      <name>Damiano Spina</name>
    </author>
    <author>
      <name>Stefano Mizzaro</name>
    </author>
    <author>
      <name>Gianluca Demartini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipm.2021.102710</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipm.2021.102710" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages; Paper accepted at Information Processing &amp; Management on
  July 28, 2021; IP&amp;M Special Issue on Dis/Misinformation Mining from Social
  Media</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Processing &amp; Management Information Processing &amp;
  Management, Volume 58, Issue 6, November 2021, 102710</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.01222v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.01222v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03465v2</id>
    <updated>2022-06-13T11:29:34Z</updated>
    <published>2022-04-07T14:28:51Z</published>
    <title>BERTuit: Understanding Spanish language in Twitter through a native
  transformer</title>
    <summary>  The appearance of complex attention-based language models such as BERT,
Roberta or GPT-3 has allowed to address highly complex tasks in a plethora of
scenarios. However, when applied to specific domains, these models encounter
considerable difficulties. This is the case of Social Networks such as Twitter,
an ever-changing stream of information written with informal and complex
language, where each message requires careful evaluation to be understood even
by humans given the important role that context plays. Addressing tasks in this
domain through Natural Language Processing involves severe challenges. When
powerful state-of-the-art multilingual language models are applied to this
scenario, language specific nuances use to get lost in translation. To face
these challenges we present \textbf{BERTuit}, the larger transformer proposed
so far for Spanish language, pre-trained on a massive dataset of 230M Spanish
tweets using RoBERTa optimization. Our motivation is to provide a powerful
resource to better understand Spanish Twitter and to be used on applications
focused on this social network, with special emphasis on solutions devoted to
tackle the spreading of misinformation in this platform. BERTuit is evaluated
on several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very
competitive multilingual transformers. The utility of our approach is shown
with applications, in this case: a zero-shot methodology to visualize groups of
hoaxes and profiling authors spreading disinformation.
  Misinformation spreads wildly on platforms such as Twitter in languages other
than English, meaning performance of transformers may suffer when transferred
outside English speaking communities.
</summary>
    <author>
      <name>Javier Huertas-Tato</name>
    </author>
    <author>
      <name>Alejandro Martin</name>
    </author>
    <author>
      <name>David Camacho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Support: 1) BBVA FOUNDATION - CIVIC, 2) Spanish Ministry of Science
  and Innovation - FightDIS (PID2020-117263GB-100) and XAI-Disinfodemics
  (PLEC2021-007681), 3) Comunidad Autonoma de Madrid - S2018/TCS-4566, 4)
  European Comission - IBERIFIER (2020-EU-IA-0252), 5) Digital Future Society
  (Mobile World Capital Barcelona) - DisTrack, 6) UPM - Programa de Excelencia
  para el Profesorado Universitario</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.03465v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03465v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.06456v1</id>
    <updated>2022-06-13T20:27:24Z</updated>
    <published>2022-06-13T20:27:24Z</published>
    <title>A comparison of partial information decompositions using data from real
  and simulated layer 5b pyramidal cells</title>
    <summary>  Partial information decomposition allows the joint mutual information between
an output and a set of inputs to be divided into components that are
synergistic or shared or unique to each input. We consider five different
decompositions and compare their results on data from layer 5b pyramidal cells
in two different studies. The first study was of the amplification of somatic
action potential output by apical dendritic input and its regulation by
dendritic inhibition. We find that two of the decompositions produce much
larger estimates of synergy and shared information than the others, as well as
large levels of unique misinformation. When within-neuron differences in the
components are examined, the five methods produce more similar results for all
but the shared information component, for which two methods produce a different
statistical conclusion from the others. There are some differences in the
expression of unique information asymmetry among the methods. It is
significantly larger, on average, under dendritic inhibition. Three of the
methods support a previous conclusion that apical amplification is reduced by
dendritic inhibition. The second study used a detailed compartmental model to
produce action potentials for many combinations of the numbers of basal and
apical synaptic inputs. Two analyses of decompositions are conducted on subsets
of the data. In the first, the decompositions reveal a bifurcation in unique
information asymmetry. For three of the methods this suggests that apical drive
switches to basal drive as the strength of the basal input increases, while the
other two show changing mixtures of information and misinformation.
Decompositions produced using the second set of subsets show that all five
decompositions provide support for properties of cooperative
context-sensitivity - to varying extents.
</summary>
    <author>
      <name>Jim W. Kay</name>
    </author>
    <author>
      <name>Jan M. Schulz</name>
    </author>
    <author>
      <name>W. A. Phillips</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/e24081021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/e24081021" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Entropy, 24th July, 2022, 24(8), 1021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.06456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.06456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.14358v2</id>
    <updated>2024-01-22T00:38:08Z</updated>
    <published>2022-06-29T01:57:44Z</published>
    <title>Using Twitter Data to Understand Public Perceptions of Approved versus
  Off-label Use for COVID-19-related Medications</title>
    <summary>  Understanding public discourse on emergency use of unproven therapeutics is
crucial for monitoring safe use and combating misinformation. We developed a
natural language processing-based pipeline to comprehend public perceptions of
and stances on coronavirus disease 2019 (COVID-19)-related drugs on Twitter
over time. This retrospective study included 609,189 US-based tweets from
January 29, 2020, to November 30, 2021, about four drugs that garnered
significant public attention during the COVID-19 pandemic: (1)
Hydroxychloroquine and Ivermectin, therapies with anecdotal evidence; and (2)
Molnupiravir and Remdesivir, FDA-approved treatments for eligible patients.
Time-trend analysis was employed to understand popularity trends and related
events. Content and demographic analyses were conducted to explore potential
rationales behind people's stances on each drug. Time-trend analysis indicated
that Hydroxychloroquine and Ivermectin were discussed more than Molnupiravir
and Remdesivir, particularly during COVID-19 surges. Hydroxychloroquine and
Ivermectin discussions were highly politicized, related to conspiracy theories,
hearsay, and celebrity influences. The distribution of stances between the two
major US political parties was significantly different (P &lt; .001); Republicans
were more likely to support Hydroxychloroquine (55%) and Ivermectin (30%) than
Democrats. People with healthcare backgrounds tended to oppose
Hydroxychloroquine (7%) more than the general population, while the general
population was more likely to support Ivermectin (14%). Our study found that
social media users have varying perceptions and stances on off-label versus
FDA-authorized drug use at different stages of COVID-19. This indicates that
health systems, regulatory agencies, and policymakers should design tailored
strategies to monitor and reduce misinformation to promote safe drug use.
</summary>
    <author>
      <name>Yining Hua</name>
    </author>
    <author>
      <name>Hang Jiang</name>
    </author>
    <author>
      <name>Shixu Lin</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Joseph M. Plasek</name>
    </author>
    <author>
      <name>David W. Bates</name>
    </author>
    <author>
      <name>Li Zhou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/jamia/ocac114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/jamia/ocac114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full paper published in JAMIA</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">amiajnl-2022-012337.R1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.14358v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.14358v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.03056v1</id>
    <updated>2023-01-08T15:13:13Z</updated>
    <published>2023-01-08T15:13:13Z</published>
    <title>The State of Human-centered NLP Technology for Fact-checking</title>
    <summary>  Misinformation threatens modern society by promoting distrust in science,
changing narratives in public health, heightening social polarization, and
disrupting democratic elections and financial markets, among a myriad of other
societal harms. To address this, a growing cadre of professional fact-checkers
and journalists provide high-quality investigations into purported facts.
However, these largely manual efforts have struggled to match the enormous
scale of the problem. In response, a growing body of Natural Language
Processing (NLP) technologies have been proposed for more scalable
fact-checking. Despite tremendous growth in such research, however, practical
adoption of NLP technologies for fact-checking still remains in its infancy
today.
  In this work, we review the capabilities and limitations of the current NLP
technologies for fact-checking. Our particular focus is to further chart the
design space for how these technologies can be harnessed and refined in order
to better meet the needs of human fact-checkers. To do so, we review key
aspects of NLP-based fact-checking: task formulation, dataset construction,
modeling, and human-centered strategies, such as explainable models and
human-in-the-loop approaches. Next, we review the efficacy of applying
NLP-based fact-checking tools to assist human fact-checkers. We recommend that
future research include collaboration with fact-checker stakeholders early on
in NLP research, as well as incorporation of human-centered design practices in
model development, in order to further guide technology development for human
use and practical adoption. Finally, we advocate for more research on benchmark
development supporting extrinsic evaluation of human-centered fact-checking
technologies.
</summary>
    <author>
      <name>Anubrata Das</name>
    </author>
    <author>
      <name>Houjiang Liu</name>
    </author>
    <author>
      <name>Venelin Kovatchev</name>
    </author>
    <author>
      <name>Matthew Lease</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipm.2022.103219</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipm.2022.103219" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Machine and Human Factors in Misinformation Management
  -- a special issue of Information Processing and Management</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Processing &amp; Management, 60(2), 103219 (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.03056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09519v2</id>
    <updated>2023-09-16T12:38:45Z</updated>
    <published>2023-05-16T15:16:07Z</published>
    <title>Community Notes vs. Snoping: How the Crowd Selects Fact-Checking Targets
  on Social Media</title>
    <summary>  Deploying links to fact-checking websites (so-called "snoping") is a common
intervention that can be used by social media users to refute misleading
claims. However, its real-world effect may be limited as it suffers from low
visibility and distrust towards professional fact-checkers. As a remedy,
Twitter launched its community-based fact-checking system Community Notes on
which fact-checks are carried out by actual Twitter users and directly shown on
the fact-checked tweets. Yet, an understanding of how fact-checking via
Community Notes differs from snoping is absent. In this study, we analyze
differences in how contributors to Community Notes and Snopers select their
targets when fact-checking social media posts. For this purpose, we analyze two
unique datasets from Twitter: (a) 25,912 community-created fact-checks from
Twitter's Community Notes platform; and (b) 52,505 "snopes" that debunk tweets
via fact-checking replies linking to professional fact-checking websites. We
find that Notes contributors and Snopers focus on different targets when
fact-checking social media content. For instance, Notes contributors tend to
fact-check posts from larger accounts with higher social influence and are
relatively less likely to endorse/emphasize the accuracy of not misleading
posts. Fact-checking targets of Notes contributors and Snopers rarely overlap;
however, those overlapping exhibit a high level of agreement in the
fact-checking assessment. Moreover, we demonstrate that Snopers fact-check
social media posts at a higher speed. Altogether, our findings imply that
different fact-checking approaches -- carried out on the same social media
platform -- can result in vastly different social media posts getting
fact-checked. This has important implications for future research on
misinformation, which should not rely on a single fact-checking approach when
compiling misinformation datasets.
</summary>
    <author>
      <name>Moritz Pilarski</name>
    </author>
    <author>
      <name>Kirill Solovev</name>
    </author>
    <author>
      <name>Nicolas Pr√∂llochs</name>
    </author>
    <link href="http://arxiv.org/abs/2305.09519v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09519v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.06683v2</id>
    <updated>2024-07-08T12:47:35Z</updated>
    <published>2023-06-11T13:57:58Z</published>
    <title>To be a pro-vax or not, the COVID-19 vaccine conundrum on Twitter</title>
    <summary>  The most surprising observation reported by the study in (arXiv:2208.13523),
involving stance detection of COVID-19 vaccine related tweets during the first
year of pandemic, is the presence of a significant number of users (~2 million)
who posted tweets with both anti-vax and pro-vax stances. This is a sizable
cohort even when the stance detection noise is considered. In this paper, we
tried to get deeper understanding of this 'dual-stance' group. Out of this
group, 60% of users have more pro-vax tweets than anti-vax tweets and 17% have
the same number of tweets in both classes. The rest have more anti-vax tweets,
and they were highly active in expressing concerns about mandate and safety of
a fast-tracked vaccine, while also tweeted some updates about vaccine
development. The leaning pro-vax group have opposite composition: more vaccine
updates and some posts about concerns. It is important to note that vaccine
concerns were not always genuine and had a large dose of misinformation. 43% of
the balanced group have only tweeted one tweet of each type during our study
period and are the less active participants in the vaccine discourse. Our
temporal study also shows that the change-of-stance behaviour became really
significant once the trial results of COVID-19 vaccine were announced to the
public, and it appears as the change of stance towards pro-vax is a reaction to
people changing their opinion towards anti-vax. Our study finished at Mar 23,
2021 when the conundrum was still going strong. The dilemma might be a
reflection of the uncertain and stressful times, but it also highlights the
importance of building public trust to combat prevalent misinformation.
</summary>
    <author>
      <name>Zainab Zaidi</name>
    </author>
    <author>
      <name>Mengbin Ye</name>
    </author>
    <author>
      <name>Shanika Karunasekera</name>
    </author>
    <author>
      <name>Yoshihisa Kashima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.06683v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.06683v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.17366v3</id>
    <updated>2025-06-16T14:20:42Z</updated>
    <published>2024-02-27T10:01:01Z</published>
    <title>The risks of risk assessment: causal blind spots when using prediction
  models for treatment decisions</title>
    <summary>  Clinicians increasingly rely on prediction models to guide treatment choices.
Most prediction models, however, are developed using observational data that
include some patients who have already received the treatment the prediction
model is meant to inform. Special attention to the causal role of those earlier
treatments is required when interpreting the resulting predictions. We identify
'causal blind spots' in three common approaches to handling treatment when
developing a prediction model: including treatment as a predictor, restricting
to individuals taking a certain treatment, and ignoring treatment. Through
several real examples, we illustrate how the risks obtained from models
developed using such approaches may be misinterpreted and can lead to
misinformed decision-making. Our discussion covers issues attributable to
confounding, selection, mediation and changes in treatment protocols over time.
We advocate for an extension of guidelines for the development, reporting and
evaluation of prediction models to avoid such misinterpretations. Developers
must ensure that the intended target population for the model, and the
treatment conditions under which predictions hold, are clearly communicated.
When prediction models are intended to inform treatment decisions, they need to
provide estimates of risk under the specific treatment (or intervention)
options being considered, known as 'prediction under interventions'. Next to
suitable data, this requires causal reasoning and causal inference techniques
during model development and evaluation. Being clear about what a given
prediction model can and cannot be used for prevents misinformed treatment
decisions and thereby prevents potential harm to patients.
</summary>
    <author>
      <name>Nan van Geloven</name>
    </author>
    <author>
      <name>Ruth H Keogh</name>
    </author>
    <author>
      <name>Wouter van Amsterdam</name>
    </author>
    <author>
      <name>Giovanni Cin√†</name>
    </author>
    <author>
      <name>Jesse H. Krijthe</name>
    </author>
    <author>
      <name>Niels Peek</name>
    </author>
    <author>
      <name>Kim Luijken</name>
    </author>
    <author>
      <name>Sara Magliacane</name>
    </author>
    <author>
      <name>Pawe≈Ç Morzywo≈Çek</name>
    </author>
    <author>
      <name>Thijs van Ommen</name>
    </author>
    <author>
      <name>Hein Putter</name>
    </author>
    <author>
      <name>Matthew Sperrin</name>
    </author>
    <author>
      <name>Junfeng Wang</name>
    </author>
    <author>
      <name>Daniala L. Weir</name>
    </author>
    <author>
      <name>Vanessa Didelez</name>
    </author>
    <link href="http://arxiv.org/abs/2402.17366v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.17366v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.09674v1</id>
    <updated>2024-02-04T13:21:19Z</updated>
    <published>2024-02-04T13:21:19Z</published>
    <title>Navigating the Peril of Generated Alternative Facts: A ChatGPT-4
  Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation</title>
    <summary>  In an era where artificial intelligence (AI) intertwines with medical
research, the delineation of truth becomes increasingly complex. This study
ostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega
variant, showcasing 31 unique mutations in the S gene region. However, the real
undercurrent of this narrative is a demonstration of the ease with which AI,
specifically ChatGPT-4, can fabricate convincing yet entirely fictional
scientific data. The so-called Omega variant was identified in a fully
vaccinated, previously infected 35-year-old male presenting with severe
COVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and
contact tracing, this study mirrors the rigorous methodology of genuine case
reports, thereby setting the stage for a compelling but entirely constructed
narrative. The entire case study was generated by ChatGPT-4, a large language
model by OpenAI. The fabricated Omega variant features an ensemble of
mutations, including N501Y and E484K, known for enhancing ACE2 receptor
affinity, alongside L452R and P681H, ostensibly indicative of immune evasion.
This variant's contrived interaction dynamics - severe symptoms in a vaccinated
individual versus mild ones in unvaccinated contacts - were designed to mimic
real-world complexities, including suggestions of antibody-dependent
enhancement (ADE). While the Omega variant is a product of AI-generated
fiction, the implications of this exercise are real and profound. The ease with
which AI can generate believable but false scientific information, as
illustrated in this case, raises significant concerns about the potential for
misinformation in medicine. This study, therefore, serves as a cautionary tale,
emphasizing the necessity for critical evaluation of sources, especially in an
age where AI tools like ChatGPT are becoming increasingly sophisticated and
widespread in their use.
</summary>
    <author>
      <name>Malik Sallam</name>
    </author>
    <author>
      <name>Jan Egger</name>
    </author>
    <author>
      <name>Rainer Roehrig</name>
    </author>
    <author>
      <name>Behrus Puladi</name>
    </author>
    <link href="http://arxiv.org/abs/2403.09674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.09674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.20651v2</id>
    <updated>2025-01-23T18:56:21Z</updated>
    <published>2024-10-28T01:17:34Z</published>
    <title>SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts' QA
  Through Six-Dimensional Feature Analysis</title>
    <summary>  Fact-checking is extensively studied in the context of misinformation and
disinformation, addressing objective inaccuracies. However, a softer form of
misinformation involves responses that are factually correct but lack certain
features such as clarity and relevance. This challenge is prevalent in formal
Question-Answer (QA) settings such as press conferences in finance, politics,
sports, and other domains, where subjective answers can obscure transparency.
Despite this, there is a lack of manually annotated datasets for subjective
features across multiple dimensions. To address this gap, we introduce
SubjECTive-QA, a human annotated dataset on Earnings Call Transcripts' (ECTs)
QA sessions as the answers given by company representatives are often open to
subjective interpretations and scrutiny. The dataset includes 49,446
annotations for long-form QA pairs across six features: Assertive, Cautious,
Optimistic, Specific, Clear, and Relevant. These features are carefully
selected to encompass the key attributes that reflect the tone of the answers
provided during QA sessions across different domain. Our findings are that the
best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar
weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity,
such as Relevant and Clear, with a mean difference of 2.17% in their weighted
F1 scores. The models perform significantly better on features with higher
subjectivity, such as Specific and Assertive, with a mean difference of 10.01%
in their weighted F1 scores. Furthermore, testing SubjECTive-QA's
generalizability using QAs from White House Press Briefings and Gaggles yields
an average weighted F1 score of 65.97% using our best models for each feature,
demonstrating broader applicability beyond the financial domain. SubjECTive-QA
is publicly available under the CC BY 4.0 license
</summary>
    <author>
      <name>Huzaifa Pardawala</name>
    </author>
    <author>
      <name>Siddhant Sukhani</name>
    </author>
    <author>
      <name>Agam Shah</name>
    </author>
    <author>
      <name>Veer Kejriwal</name>
    </author>
    <author>
      <name>Abhishek Pillai</name>
    </author>
    <author>
      <name>Rohan Bhasin</name>
    </author>
    <author>
      <name>Andrew DiBiasio</name>
    </author>
    <author>
      <name>Tarun Mandapati</name>
    </author>
    <author>
      <name>Dhruv Adha</name>
    </author>
    <author>
      <name>Sudheer Chava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NeurIPS 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.20651v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.20651v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.05146v1</id>
    <updated>2025-04-07T14:50:13Z</updated>
    <published>2025-04-07T14:50:13Z</published>
    <title>Query Smarter, Trust Better? Exploring Search Behaviours for Verifying
  News Accuracy</title>
    <summary>  While it is often assumed that searching for information to evaluate
misinformation will help identify false claims, recent work suggests that
search behaviours can instead reinforce belief in misleading news, particularly
when users generate queries using vocabulary from the source articles. Our
research explores how different query generation strategies affect news
verification and whether the way people search influences the accuracy of their
information evaluation. A mixed-methods approach was used, consisting of three
parts: (1) an analysis of existing data to understand how search behaviour
influences trust in fake news, (2) a simulation of query generation strategies
using a Large Language Model (LLM) to assess the impact of different query
formulations on search result quality, and (3) a user study to examine how
'Boost' interventions in interface design can guide users to adopt more
effective query strategies. The results show that search behaviour
significantly affects trust in news, with successful searches involving
multiple queries and yielding higher-quality results. Queries inspired by
different parts of a news article produced search results of varying quality,
and weak initial queries improved when reformulated using full SERP
information. Although 'Boost' interventions had limited impact, the study
suggests that interface design encouraging users to thoroughly review search
results can enhance query formulation. This study highlights the importance of
query strategies in evaluating news and proposes that interface design can play
a key role in promoting more effective search practices, serving as one
component of a broader set of interventions to combat misinformation.
</summary>
    <author>
      <name>David Elsweiler</name>
    </author>
    <author>
      <name>Samy Ateia</name>
    </author>
    <author>
      <name>Markus Bink</name>
    </author>
    <author>
      <name>Gregor Donabauer</name>
    </author>
    <author>
      <name>Marcos Fern√°ndez Pichel</name>
    </author>
    <author>
      <name>Alexander Frummet</name>
    </author>
    <author>
      <name>Udo Kruschwitz</name>
    </author>
    <author>
      <name>David Losada</name>
    </author>
    <author>
      <name>Bernd Ludwig</name>
    </author>
    <author>
      <name>Selina Meyer</name>
    </author>
    <author>
      <name>Noel Pascual Presa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, Pre-Print SIGIR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.05146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.05146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0702542v3</id>
    <updated>2009-01-24T18:59:33Z</updated>
    <published>2007-02-20T21:06:10Z</published>
    <title>Tainted Evidence: Cosmological Model Selection vs. Fitting</title>
    <summary>  Interpretation of cosmological data to determine the number and values of
parameters describing the universe must not rely solely on statistics but
involve physical insight. When statistical techniques such as "model selection"
or "integrated survey optimization" blindly apply Occam's Razor, this can lead
to painful results. We emphasize that the sensitivity to prior probabilities
and to the number of models compared can lead to "prior selection" rather than
robust model selection. A concrete example demonstrates that Information
Criteria can in fact misinform over a large region of parameter space.
</summary>
    <author>
      <name>Eric V. Linder</name>
    </author>
    <author>
      <name>Ramon Miquel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0218271808013881</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0218271808013881" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure; v2 minor rephrasings, clarifications; v3 minor
  changes to match published article under title "Cosmological Model Selection:
  Statistics and Physics"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Int.J.Mod.Phys.D17:2315,2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/astro-ph/0702542v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0702542v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.4357v1</id>
    <updated>2008-03-31T00:24:19Z</updated>
    <published>2008-03-31T00:24:19Z</published>
    <title>Paradoxical popups: Why are they hard to catch?</title>
    <summary>  Even professional baseball players occasionally find it difficult to
gracefully approach seemingly routine pop-ups. This paper describes a set of
towering pop-ups with trajectories that exhibit cusps and loops near the apex.
For a normal fly ball, the horizontal velocity is continuously decreasing due
to drag caused by air resistance. But for pop-ups, the Magnus force (the force
due to the ball spinning in a moving airflow) is larger than the drag force. In
these cases the horizontal velocity decreases in the beginning, like a normal
fly ball, but after the apex, the Magnus force accelerates the horizontal
motion. We refer to this class of pop-ups as paradoxical because they appear to
misinform the typically robust optical control strategies used by fielders and
lead to systematic vacillation in running paths, especially when a trajectory
terminates near the fielder. In short, some of the dancing around when
infielders pursue pop-ups can be well explained as a combination of bizarre
trajectories and misguidance by the normally reliable optical control strategy,
rather than apparent fielder error. Former major league infielders confirm that
our model agrees with their experiences.
</summary>
    <author>
      <name>Michael K. McBeath</name>
    </author>
    <author>
      <name>Alan M. Nathan</name>
    </author>
    <author>
      <name>A. Terry Bahill</name>
    </author>
    <author>
      <name>David G. Baldwin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1119/1.2937899</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1119/1.2937899" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 10 figures, sumitted to American Journal of Physics</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.4357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.4357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2448v3</id>
    <updated>2009-03-23T18:42:39Z</updated>
    <published>2009-03-13T18:30:55Z</published>
    <title>Positive Logic with Adjoint Modalities: Proof Theory, Semantics and
  Reasoning about Information</title>
    <summary>  We consider a simple modal logic whose non-modal part has conjunction and
disjunction as connectives and whose modalities come in adjoint pairs, but are
not in general closure operators. Despite absence of negation and implication,
and of axioms corresponding to the characteristic axioms of (e.g.) T, S4 and
S5, such logics are useful, as shown in previous work by Baltag, Coecke and the
first author, for encoding and reasoning about information and misinformation
in multi-agent systems. For such a logic we present an algebraic semantics,
using lattices with agent-indexed families of adjoint pairs of operators, and a
cut-free sequent calculus. The calculus exploits operators on sequents, in the
style of "nested" or "tree-sequent" calculi; cut-admissibility is shown by
constructive syntactic methods. The applicability of the logic is illustrated
by reasoning about the muddy children puzzle, for which the calculus is
augmented with extra rules to express the facts of the muddy children scenario.
</summary>
    <author>
      <name>Mehrnoosh Sadrzadeh</name>
    </author>
    <author>
      <name>Roy Dyckhoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is the full version of the article that is to appear in
  the ENTCS proceedings of the 25th conference on the Mathematical Foundations
  of Programming Semantics (MFPS), April 2009, University of Oxford</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.2448v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2448v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.3768v1</id>
    <updated>2010-11-16T17:46:23Z</updated>
    <published>2010-11-16T17:46:23Z</published>
    <title>Detecting and Tracking the Spread of Astroturf Memes in Microblog
  Streams</title>
    <summary>  Online social media are complementing and in some cases replacing
person-to-person social interaction and redefining the diffusion of
information. In particular, microblogs have become crucial grounds on which
public relations, marketing, and political battles are fought. We introduce an
extensible framework that will enable the real-time analysis of meme diffusion
in social media by mining, visualizing, mapping, classifying, and modeling
massive streams of public microblogging events. We describe a Web service that
leverages this framework to track political memes in Twitter and help detect
astroturfing, smear campaigns, and other misinformation in the context of U.S.
political elections. We present some cases of abusive behaviors uncovered by
our service. Finally, we discuss promising preliminary results on the detection
of suspicious memes via supervised learning based on features extracted from
the topology of the diffusion networks, sentiment analysis, and crowdsourced
annotations.
</summary>
    <author>
      <name>Jacob Ratkiewicz</name>
    </author>
    <author>
      <name>Michael Conover</name>
    </author>
    <author>
      <name>Mark Meiss</name>
    </author>
    <author>
      <name>Bruno Gon√ßalves</name>
    </author>
    <author>
      <name>Snehal Patil</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1963192.1963301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1963192.1963301" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 20th international conference companion on
  World wide web, 249-252 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1011.3768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.3768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.3620v2</id>
    <updated>2011-12-19T18:28:16Z</updated>
    <published>2011-12-15T19:35:01Z</published>
    <title>Probably a discovery: Bad mathematics means rough scientific
  communication</title>
    <summary>  According to the media, in spring of this year the experiment CDF at Fermilab
has made most likely ("this result has a 99.7 percent chance of being correct",
Discovery News) a great discovery ("the most significant in physics in half a
century", NYT). However, since the very beginning, practically all particle
physics experts did not believe that was the case. This is the last of a quite
long series of fake claims based on trivial mistakes in the probabilistic
reasoning. The main purpose of this note is to invite everybody, but especially
journalists and general public, most times innocent victims of misinformation
of this kind, to mistrust claims not explicitly reported in terms of how much
we should believe something, under well stated conditions and assumptions. (A
last minute appendix has been added, with comments on the recent news
concerning the Higgs at LHC.)
</summary>
    <author>
      <name>G. D'Agostini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 2 figures, note based on lectures at the University of
  Perugia, 15-16 April 2011 and at MAPSES School in Lecce, 23-25 November 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.3620v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.3620v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.7404v1</id>
    <updated>2013-03-28T10:29:04Z</updated>
    <published>2013-03-28T10:29:04Z</published>
    <title>Megaprojects and Risk: An Anatomy of Ambition</title>
    <summary>  Back cover text: Megaprojects and Risk provides the first detailed
examination of the phenomenon of megaprojects. It is a fascinating account of
how the promoters of multibillion-dollar megaprojects systematically and
self-servingly misinform parliaments, the public and the media in order to get
projects approved and built. It shows, in unusual depth, how the formula for
approval is an unhealthy cocktail of underestimated costs, overestimated
revenues, undervalued environmental impacts and overvalued economic development
effects. This results in projects that are extremely risky, but where the risk
is concealed from MPs, taxpayers and investors. The authors not only explore
the problems but also suggest practical solutions drawing on theory and hard,
scientific evidence from the several hundred projects in twenty nations that
illustrate the book. Accessibly written, it will be essential reading in its
field for students, scholars, planners, economists, auditors, politicians,
journalists and interested citizens.
</summary>
    <author>
      <name>Bent Flyvbjerg</name>
    </author>
    <author>
      <name>Nils Bruzelius</name>
    </author>
    <author>
      <name>Werner Rothengatter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Cambridge University Press, 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.7404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.7404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.2176v1</id>
    <updated>2013-07-08T17:19:02Z</updated>
    <published>2013-07-08T17:19:02Z</published>
    <title>Cost overruns in Large-Scale Transportation Infrastructure Projects:
  Explanations and Their Theoretical Embeddedness</title>
    <summary>  Managing large-scale transportation infrastructure projects is difficult due
to frequent misinformation about the costs which results in large cost overruns
that often threaten the overall project viability. This paper investigates the
explanations for cost overruns that are given in the literature. Overall, four
categories of explanations can be distinguished: technical, economic,
psychological, and political. Political explanations have been seen to be the
most dominant explanations for cost overruns. Agency theory is considered the
most interesting for political explanations and an eclectic theory is also
considered possible. Nonpolitical explanations are diverse in character,
therefore a range of different theories (including rational choice theory and
prospect theory), depending on the kind of explanation is considered more
appropriate than one all-embracing theory.
</summary>
    <author>
      <name>Chantal C. Cantarelli</name>
    </author>
    <author>
      <name>Bent Flybjerg</name>
    </author>
    <author>
      <name>Eric J. E. Molin</name>
    </author>
    <author>
      <name>Bert van Wee</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Journal of Transport and Infrastructure Research, vol.
  10, no. 1, March 2010, 5-18</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.2176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.2176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1942v1</id>
    <updated>2013-10-07T20:19:36Z</updated>
    <published>2013-10-07T20:19:36Z</published>
    <title>Containing Viral Spread on Sparse Random Graphs: Bounds, Algorithms, and
  Experiments</title>
    <summary>  Viral spread on large graphs has many real-life applications such as malware
propagation in computer networks and rumor (or misinformation) spread in
Twitter-like online social networks. Although viral spread on large graphs has
been intensively analyzed on classical models such as
Susceptible-Infectious-Recovered, there still exits a deficit of effective
methods in practice to contain epidemic spread once it passes a critical
threshold. Against this backdrop, we explore methods of containing viral spread
in large networks with the focus on sparse random networks. The viral
containment strategy is to partition a large network into small components and
then to ensure the sanity of all messages delivered across different
components. With such a defense mechanism in place, an epidemic spread starting
from any node is limited to only those nodes belonging to the same component as
the initial infection node. We establish both lower and upper bounds on the
costs of inspecting inter-component messages. We further propose
heuristic-based approaches to partition large input graphs into small
components. Finally, we study the performance of our proposed algorithms under
different network topologies and different edge weight models.
</summary>
    <author>
      <name>Milan Bradonjiƒá</name>
    </author>
    <author>
      <name>Michael Molloy</name>
    </author>
    <author>
      <name>Guanhua Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.1942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0693v2</id>
    <updated>2014-01-20T12:58:12Z</updated>
    <published>2014-01-03T18:10:14Z</published>
    <title>Non-existence of greedy bases in direct sums of mixed $\ell_{p}$ spaces</title>
    <summary>  The fact that finite direct sums of two or more mutually different spaces
from the family $\{\ell_{p} : 1\le p&lt;\infty\}\cup c_{0}$ fail to have greedy
bases is stated in [Dilworth et al., Greedy bases for Besov spaces, Constr.
Approx. 34 (2011), no. 2, 281-296]. However, the concise proof that the authors
give of this fundamental result in greedy approximation relies on a fallacious
argument, namely the alleged uniqueness of unconditional basis up to
permutation of the spaces involved. The main goal of this note is to settle the
problem by providing a correct proof. For that we first show that all greedy
bases in an $\ell_{p}$ space have fundamental functions of the same order. As a
by-product of our work we obtain that every almost greedy basis of a Banach
space with unconditional basis and nontrivial type contains a greedy subbasis.
</summary>
    <author>
      <name>Fernando Albiac</name>
    </author>
    <author>
      <name>Jos√© L. Ansorena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The manuscript needs to be revised in accordance with some feedback
  we received from Dilworth et al. The submitted version contained an
  inaccurate account of a statement in [Dilworth et al., Greedy Basis for Besov
  Spaces, Constr. Approx. 34 (2011), 281-296]. We apologize for the
  misinformation</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.0693v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0693v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="46A35, 46A45, 46B15, 46B25, 46B45, 46E30, 46T99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3344v1</id>
    <updated>2014-03-13T17:57:05Z</updated>
    <published>2014-03-13T17:57:05Z</published>
    <title>Collective attention in the age of (mis)information</title>
    <summary>  In this work we study, on a sample of 2.3 million individuals, how Facebook
users consumed different information at the edge of political discussion and
news during the last Italian electoral competition. Pages are categorized,
according to their topics and the communities of interests they pertain to, in
a) alternative information sources (diffusing topics that are neglected by
science and main stream media); b) online political activism; and c) main
stream media. We show that attention patterns are similar despite the different
qualitative nature of the information, meaning that unsubstantiated claims
(mainly conspiracy theories) reverberate for as long as other information.
Finally, we categorize users according to their interaction patterns among the
different topics and measure how a sample of this social ecosystem (1279 users)
responded to the injection of 2788 false information posts. Our analysis
reveals that users which are prominently interacting with alternative
information sources (i.e. more exposed to unsubstantiated claims) are more
prone to interact with false claims.
</summary>
    <author>
      <name>Delia Mocanu</name>
    </author>
    <author>
      <name>Luca Rossi</name>
    </author>
    <author>
      <name>Qian Zhang</name>
    </author>
    <author>
      <name>M√†rton Karsai</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">misinformation, attention patterns, false information, social
  response</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.3344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5378v1</id>
    <updated>2014-08-19T18:00:36Z</updated>
    <published>2014-08-19T18:00:36Z</published>
    <title>Social Learning in a Human Society: An Experimental Study</title>
    <summary>  This paper presents an experimental study to investigate the learning and
decision making behavior of individuals in a human society. Social learning is
used as the mathematical basis for modelling interaction of individuals that
aim to perform a perceptual task interactively. A psychology experiment was
conducted on a group of undergraduate students at the University of British
Columbia to examine whether the decision (action) of one individual affects the
decision of the subsequent individuals. The major experimental observation that
stands out here is that the participants of the experiment (agents) were
affected by decisions of their partners in a relatively large fraction (60%) of
trials. We fit a social learning model that mimics the interactions between
participants of the psychology experiment. Misinformation propagation (also
known as data incest) within the society under study is further investigated in
this paper.
</summary>
    <author>
      <name>Maziyar Hamdi</name>
    </author>
    <author>
      <name>Grayden Solman</name>
    </author>
    <author>
      <name>Alan Kingstone</name>
    </author>
    <author>
      <name>Vikram Krishnamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/1408.5378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2651v1</id>
    <updated>2014-09-09T09:43:44Z</updated>
    <published>2014-09-09T09:43:44Z</published>
    <title>Social determinants of content selection in the age of (mis)information</title>
    <summary>  Despite the enthusiastic rhetoric about the so called \emph{collective
intelligence}, conspiracy theories -- e.g. global warming induced by chemtrails
or the link between vaccines and autism -- find on the Web a natural medium for
their dissemination. Users preferentially consume information according to
their system of beliefs and the strife within users of opposite narratives may
result in heated debates. In this work we provide a genuine example of
information consumption from a sample of 1.2 million of Facebook Italian users.
We show by means of a thorough quantitative analysis that information
supporting different worldviews -- i.e. scientific and conspiracist news -- are
consumed in a comparable way by their respective users. Moreover, we measure
the effect of the exposure to 4709 evidently false information (satirical
version of conspiracy theses) and to 4502 debunking memes (information aiming
at contrasting unsubstantiated rumors) of the most polarized users of
conspiracy claims. We find that either contrasting or teasing consumers of
conspiracy narratives increases their probability to interact again with
unsubstantiated rumors.
</summary>
    <author>
      <name>Alessandro Bessi</name>
    </author>
    <author>
      <name>Guido Caldarelli</name>
    </author>
    <author>
      <name>Michela Del Vicario</name>
    </author>
    <author>
      <name>Antonio Scala</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">misinformation, collective narratives, crowd dynamics, information
  spreading</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.2651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00994v1</id>
    <updated>2015-01-05T21:00:51Z</updated>
    <published>2015-01-05T21:00:51Z</published>
    <title>Online Reputation and Polling Systems: Data Incest, Social Learning and
  Revealed Preferences</title>
    <summary>  This paper considers online reputation and polling systems where individuals
make recommendations based on their private observations and recommendations of
friends. Such interaction of individuals and their social influence is modelled
as social learning on a directed acyclic graph. Data incest (misinformation
propagation) occurs due to unintentional re-use of identical actions in the
for- mation of public belief in social learning; the information gathered by
each agent is mistakenly considered to be independent. This results in
overconfidence and bias in estimates of the state. Necessary and sufficient
conditions are given on the structure of information exchange graph to mitigate
data incest. Incest removal algorithms are presented. Experimental results on
human subjects are presented to illustrate the effect of social influence and
data incest on decision making. These experimental results indicate that social
learning protocols require careful design to handle and mitigate data incest.
The incest removal algorithms are illustrated in an expectation polling system
where participants in a poll respond with a summary of their friends' beliefs.
Finally, the principle of revealed preferences arising in micro-economics
theory is used to parse Twitter datasets to determine if social sensors are
utility maximizers and then determine their utility functions.
</summary>
    <author>
      <name>Vikram Krishnamurthy</name>
    </author>
    <author>
      <name>William Hoiles</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1412.4171</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.00994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.03471v1</id>
    <updated>2015-01-14T20:18:21Z</updated>
    <published>2015-01-14T20:18:21Z</published>
    <title>Computational fact checking from knowledge networks</title>
    <summary>  Traditional fact checking by expert journalists cannot keep up with the
enormous volume of information that is now generated online. Computational fact
checking may significantly enhance our ability to evaluate the veracity of
dubious information. Here we show that the complexities of human fact checking
can be approximated quite well by finding the shortest path between concept
nodes under properly defined semantic proximity metrics on knowledge graphs.
Framed as a network problem this approach is feasible with efficient
computational techniques. We evaluate this approach by examining tens of
thousands of claims related to history, entertainment, geography, and
biographical information using a public knowledge graph extracted from
Wikipedia. Statements independently known to be true consistently receive
higher support via our method than do false ones. These findings represent a
significant step toward scalable computational fact-checking methods that may
one day mitigate the spread of harmful misinformation.
</summary>
    <author>
      <name>Giovanni Luca Ciampaglia</name>
    </author>
    <author>
      <name>Prashant Shiralkar</name>
    </author>
    <author>
      <name>Luis M. Rocha</name>
    </author>
    <author>
      <name>Johan Bollen</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0128193</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0128193" rel="related"/>
    <link href="http://arxiv.org/abs/1501.03471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.03471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.07162v3</id>
    <updated>2015-10-28T20:36:49Z</updated>
    <published>2015-02-25T13:29:17Z</published>
    <title>Measuring Online Social Bubbles</title>
    <summary>  Social media have quickly become a prevalent channel to access information,
spread ideas, and influence opinions. However, it has been suggested that
social and algorithmic filtering may cause exposure to less diverse points of
view, and even foster polarization and misinformation. Here we explore and
validate this hypothesis quantitatively for the first time, at the collective
and individual levels, by mining three massive datasets of web traffic, search
logs, and Twitter posts. Our analysis shows that collectively, people access
information from a significantly narrower spectrum of sources through social
media and email, compared to search. The significance of this finding for
individual exposure is revealed by investigating the relationship between the
diversity of information sources experienced by users at the collective and
individual level. There is a strong correlation between collective and
individual diversity, supporting the notion that when we use social media we
find ourselves inside "social bubbles". Our results could lead to a deeper
understanding of how technology biases our exposure to new information.
</summary>
    <author>
      <name>Dimitar Nikolov</name>
    </author>
    <author>
      <name>Diego F. M. Oliveira</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <link href="http://arxiv.org/abs/1502.07162v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.07162v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07109v1</id>
    <updated>2015-07-25T15:13:34Z</updated>
    <published>2015-07-25T15:13:34Z</published>
    <title>Political Bots and the Manipulation of Public Opinion in Venezuela</title>
    <summary>  Social and political bots have a small but strategic role in Venezuelan
political conversations. These automated scripts generate content through
social media platforms and then interact with people. In this preliminary study
on the use of political bots in Venezuela, we analyze the tweeting, following
and retweeting patterns for the accounts of prominent Venezuelan politicians
and prominent Venezuelan bots. We find that bots generate a very small
proportion of all the traffic about political life in Venezuela. Bots are used
to retweet content from Venezuelan politicians but the effect is subtle in that
less than 10 percent of all retweets come from bot-related platforms.
Nonetheless, we find that the most active bots are those used by Venezuela's
radical opposition. Bots are pretending to be political leaders, government
agencies and political parties more than citizens. Finally, bots are promoting
innocuous political events more than attacking opponents or spreading
misinformation.
</summary>
    <author>
      <name>Michelle Forelle</name>
    </author>
    <author>
      <name>Phil Howard</name>
    </author>
    <author>
      <name>Andr√©s Monroy-Hern√°ndez</name>
    </author>
    <author>
      <name>Saiph Savage</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.07109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00189v2</id>
    <updated>2015-12-21T15:03:12Z</updated>
    <published>2015-09-01T09:24:21Z</published>
    <title>Echo chambers in the age of misinformation</title>
    <summary>  The wide availability of user-provided content in online social media
facilitates the aggregation of people around common interests, worldviews, and
narratives. Despite the enthusiastic rhetoric on the part of some that this
process generates "collective intelligence", the WWW also allows the rapid
dissemination of unsubstantiated conspiracy theories that often elicite rapid,
large, but naive social responses such as the recent case of Jade Helm 15 --
where a simple military exercise turned out to be perceived as the beginning of
the civil war in the US. We study how Facebook users consume information
related to two different kinds of narrative: scientific and conspiracy news. We
find that although consumers of scientific and conspiracy stories present
similar consumption patterns with respect to content, the sizes of the
spreading cascades differ. Homogeneity appears to be the primary driver for the
diffusion of contents, but each echo chamber has its own cascade dynamics. To
mimic these dynamics, we introduce a data-driven percolation model on signed
networks.
</summary>
    <author>
      <name>Michela Del Vicario</name>
    </author>
    <author>
      <name>Alessandro Bessi</name>
    </author>
    <author>
      <name>Fabiana Zollo</name>
    </author>
    <author>
      <name>Fabio Petroni</name>
    </author>
    <author>
      <name>Antonio Scala</name>
    </author>
    <author>
      <name>Guido Caldarelli</name>
    </author>
    <author>
      <name>H. Eugene Stanley</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <link href="http://arxiv.org/abs/1509.00189v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00189v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00975v1</id>
    <updated>2016-02-02T15:29:42Z</updated>
    <published>2016-02-02T15:29:42Z</published>
    <title>BotOrNot: A System to Evaluate Social Bots</title>
    <summary>  While most online social media accounts are controlled by humans, these
platforms also host automated agents called social bots or sybil accounts.
Recent literature reported on cases of social bots imitating humans to
manipulate discussions, alter the popularity of users, pollute content and
spread misinformation, and even perform terrorist propaganda and recruitment
actions. Here we present BotOrNot, a publicly-available service that leverages
more than one thousand features to evaluate the extent to which a Twitter
account exhibits similarity to the known characteristics of social bots. Since
its release in May 2014, BotOrNot has served over one million requests via our
website and APIs.
</summary>
    <author>
      <name>Clayton A. Davis</name>
    </author>
    <author>
      <name>Onur Varol</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <author>
      <name>Alessandro Flammini</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2872518.2889302</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2872518.2889302" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, WWW Developers Day</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 25th International Conference Companion on
  World Wide Web (pp. 273-274). 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.00975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04721v1</id>
    <updated>2016-06-15T11:08:24Z</updated>
    <published>2016-06-15T11:08:24Z</published>
    <title>Personality Traits and Echo Chambers on Facebook</title>
    <summary>  In online social networks, users tend to select information that adhere to
their system of beliefs and to form polarized groups of like minded people.
Polarization as well as its effects on online social interactions have been
extensively investigated. Still, the relation between group formation and
personality traits remains unclear. A better understanding of the cognitive and
psychological determinants of online social dynamics might help to design more
efficient communication strategies and to challenge the digital misinformation
threat. In this work, we focus on users commenting posts published by US
Facebook pages supporting scientific and conspiracy-like narratives, and we
classify the personality traits of those users according to their online
behavior. We show that different and conflicting communities are populated by
users showing similar psychological profiles, and that the dominant personality
model is the same in both scientific and conspiracy echo chambers. Moreover, we
observe that the permanence within echo chambers slightly shapes users'
psychological profiles. Our results suggest that the presence of specific
personality traits in individuals lead to their considerable involvement in
supporting narratives inside virtual echo chambers.
</summary>
    <author>
      <name>Alessandro Bessi</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04579v1</id>
    <updated>2017-04-15T04:47:25Z</updated>
    <published>2017-04-15T04:47:25Z</published>
    <title>Evaluating Quality of Chatbots and Intelligent Conversational Agents</title>
    <summary>  Chatbots are one class of intelligent, conversational software agents
activated by natural language input (which can be in the form of text, voice,
or both). They provide conversational output in response, and if commanded, can
sometimes also execute tasks. Although chatbot technologies have existed since
the 1960s and have influenced user interface development in games since the
early 1980s, chatbots are now easier to train and implement. This is due to
plentiful open source code, widely available development platforms, and
implementation options via Software as a Service (SaaS). In addition to
enhancing customer experiences and supporting learning, chatbots can also be
used to engineer social harm - that is, to spread rumors and misinformation, or
attack people for posting their thoughts and opinions online. This paper
presents a literature review of quality issues and attributes as they relate to
the contemporary issue of chatbot development and implementation. Finally,
quality assessment approaches are reviewed, and a quality assessment method
based on these attributes and the Analytic Hierarchy Process (AHP) is proposed
and examined.
</summary>
    <author>
      <name>Nicole M. Radziwill</name>
    </author>
    <author>
      <name>Morgan C. Benton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software Quality Professional, June 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02522v1</id>
    <updated>2017-05-06T19:38:33Z</updated>
    <published>2017-05-06T19:38:33Z</published>
    <title>People on Drugs: Credibility of User Statements in Health Communities</title>
    <summary>  Online health communities are a valuable source of information for patients
and physicians. However, such user-generated resources are often plagued by
inaccuracies and misinformation. In this work we propose a method for
automatically establishing the credibility of user-generated medical statements
and the trustworthiness of their authors by exploiting linguistic cues and
distant supervision from expert sources. To this end we introduce a
probabilistic graphical model that jointly learns user trustworthiness,
statement credibility, and language objectivity. We apply this methodology to
the task of extracting rare or unknown side-effects of medical drugs --- this
being one of the problems where large scale non-expert data has the potential
to complement expert medical knowledge. We show that our method can reliably
extract side-effects and filter out false statements, while identifying
trustworthy users that are likely to contribute valuable medical information.
</summary>
    <author>
      <name>Subhabrata Mukherjee</name>
    </author>
    <author>
      <name>Gerhard Weikum</name>
    </author>
    <author>
      <name>Cristian Danescu-Niculescu-Mizil</name>
    </author>
    <link href="http://arxiv.org/abs/1705.02522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.11187v1</id>
    <updated>2017-05-31T17:33:31Z</updated>
    <published>2017-05-31T17:33:31Z</published>
    <title>U-Phylogeny: Undirected Provenance Graph Construction in the Wild</title>
    <summary>  Deriving relationships between images and tracing back their history of
modifications are at the core of Multimedia Phylogeny solutions, which aim to
combat misinformation through doctored visual media. Nonetheless, most recent
image phylogeny solutions cannot properly address cases of forged composite
images with multiple donors, an area known as multiple parenting phylogeny
(MPP). This paper presents a preliminary undirected graph construction solution
for MPP, without any strict assumptions. The algorithm is underpinned by robust
image representative keypoints and different geometric consistency checks among
matching regions in both images to provide regions of interest for direct
comparison. The paper introduces a novel technique to geometrically filter the
most promising matches as well as to aid in the shared region localization
task. The strength of the approach is corroborated by experiments with
real-world cases, with and without image distractors (unrelated cases).
</summary>
    <author>
      <name>Aparna Bharati</name>
    </author>
    <author>
      <name>Daniel Moreira</name>
    </author>
    <author>
      <name>Allan Pinto</name>
    </author>
    <author>
      <name>Joel Brogan</name>
    </author>
    <author>
      <name>Kevin Bowyer</name>
    </author>
    <author>
      <name>Patrick Flynn</name>
    </author>
    <author>
      <name>Walter Scheirer</name>
    </author>
    <author>
      <name>Anderson Rocha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Accepted in International Conference on Image Processing,
  2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.11187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.11187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03019v1</id>
    <updated>2017-06-09T16:21:46Z</updated>
    <published>2017-06-09T16:21:46Z</published>
    <title>Understanding Information Spreading in Social Media during Hurricane
  Sandy: User Activity and Network Properties</title>
    <summary>  Many people use social media to seek information during disasters while
lacking access to traditional information sources. In this study, we analyze
Twitter data to understand information spreading activities of social media
users during hurricane Sandy. We create multiple subgraphs of Twitter users
based on activity levels and analyze network properties of the subgraphs. We
observe that user information sharing activity follows a power-law distribution
suggesting the existence of few highly active nodes in disseminating
information and many other nodes being less active. We also observe close
enough connected components and isolates at all levels of activity, and
networks become less transitive, but more assortative for larger subgraphs. We
also analyze the association between user activities and characteristics that
may influence user behavior to spread information during a crisis. Users become
more active in spreading information if they are centrally placed in the
network, less eccentric, and have higher degrees. Our analysis provides
insights on how to exploit user characteristics and network properties to
spread information or limit the spreading of misinformation during a crisis
event.
</summary>
    <author>
      <name>Arif Mohaimin Sadri</name>
    </author>
    <author>
      <name>Samiul Hasan</name>
    </author>
    <author>
      <name>Satish V. Ukkusuri</name>
    </author>
    <author>
      <name>Manuel Cebrian</name>
    </author>
    <link href="http://arxiv.org/abs/1706.03019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02763v2</id>
    <updated>2018-01-04T07:51:52Z</updated>
    <published>2017-08-09T08:54:54Z</published>
    <title>Has the Online Discussion Been Manipulated? Quantifying Online
  Discussion Authenticity within Online Social Media</title>
    <summary>  Online social media (OSM) has a enormous influence in today's world. Some
individuals view OSM as fertile ground for abuse and use it to disseminate
misinformation and political propaganda, slander competitors, and spread spam.
The crowdturfing industry employs large numbers of bots and human workers to
manipulate OSM and misrepresent public opinion. The detection of online
discussion topics manipulated by OSM \emph{abusers} is an emerging issue
attracting significant attention. In this paper, we propose an approach for
quantifying the authenticity of online discussions based on the similarity of
OSM accounts participating in the discussion to known abusers and legitimate
accounts. Our method uses several similarity functions for the analysis and
classification of OSM accounts. The proposed methods are demonstrated using
Twitter data collected for this study and previously published \emph{Arabic
honeypot dataset}. The former includes manually labeled accounts and abusers
who participated in crowdturfing platforms. Evaluation of the topic's
authenticity, derived from account similarity functions, shows that the
suggested approach is effective for discriminating between topics that were
strongly promoted by abusers and topics that attracted authentic public
interest.
</summary>
    <author>
      <name>Aviad Elyashar</name>
    </author>
    <author>
      <name>Jorge Bendahan</name>
    </author>
    <author>
      <name>Rami Puzis</name>
    </author>
    <link href="http://arxiv.org/abs/1708.02763v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02763v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07412v2</id>
    <updated>2017-12-01T20:06:56Z</updated>
    <published>2017-11-20T17:01:20Z</published>
    <title>Distributed Rumor Blocking with Multiple Positive Cascades</title>
    <summary>  Misinformation and rumor can spread rapidly and widely through online social
networks and therefore rumor controlling has become a critical issue. It is
often assumed that there is a single authority whose goal is to minimize the
spread of rumor by generating a positive cascade. In this paper, we study a
more realistic scenario when there are multiple positive cascades generated by
different agents. For the multiple-cascade diffusion, we propose the P2P
independent cascade (PIC) model for private social communications. The main
part of this paper is an analysis of the rumor blocking effect (i.e. the number
of the users activated by rumor) when the agents non-cooperatively generate the
positive cascades. We show that the rumor blocking effect provided by the Nash
equilibrium will not be arbitrarily worse even if the positive cascades are
generated non-cooperatively. In addition, we give a discussion on how the
cascade priority and activation order affect the rumor blocking problem. We
experimentally examine the Nash equilibrium of the proposed games by
simulations done on real social network structures.
</summary>
    <author>
      <name>Guangmo Amo Tong</name>
    </author>
    <author>
      <name>Weili Wu</name>
    </author>
    <author>
      <name>Ding-Zhu Du</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.07412v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07412v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08615v1</id>
    <updated>2017-11-23T08:36:17Z</updated>
    <published>2017-11-23T08:36:17Z</published>
    <title>Controlling Elections through Social Influence</title>
    <summary>  Election control considers the problem of an adversary who attempts to tamper
with a voting process, in order to either ensure that their favored candidate
wins (constructive control) or another candidate loses (destructive control).
As online social networks have become significant sources of information for
potential voters, a new tool in an attacker's arsenal is to effect control by
harnessing social influence, for example, by spreading fake news and other
forms of misinformation through online social media.
  We consider the computational problem of election control via social
influence, studying the conditions under which finding good adversarial
strategies is computationally feasible. We consider two objectives for the
adversary in both the constructive and destructive control settings:
probability and margin of victory (POV and MOV, respectively). We present
several strong negative results, showing, for example, that the problem of
maximizing POV is inapproximable for any constant factor. On the other hand, we
present approximation algorithms which provide somewhat weaker approximation
guarantees, such as bicriteria approximations for the POV objective and
constant-factor approximations for MOV. Finally, we present mixed integer
programming formulations for these problems. Experimental results show that our
approximation algorithms often find near-optimal control strategies, indicating
that election control through social influence is a salient threat to election
integrity.
</summary>
    <author>
      <name>Bryan Wilder</name>
    </author>
    <author>
      <name>Yevgeniy Vorobeychik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.08615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03572v1</id>
    <updated>2018-02-10T12:16:12Z</updated>
    <published>2018-02-10T12:16:12Z</published>
    <title>Junk News on Military Affairs and National Security: Social Media
  Disinformation Campaigns Against US Military Personnel and Veterans</title>
    <summary>  Social media provides political news and information for both active duty
military personnel and veterans. We analyze the subgroups of Twitter and
Facebook users who spend time consuming junk news from websites that target US
military personnel and veterans with conspiracy theories, misinformation, and
other forms of junk news about military affairs and national security issues.
(1) Over Twitter we find that there are significant and persistent interactions
between current and former military personnel and a broad network of extremist,
Russia-focused, and international conspiracy subgroups. (2) Over Facebook, we
find significant and persistent interactions between public pages for military
and veterans and subgroups dedicated to political conspiracy, and both sides of
the political spectrum. (3) Over Facebook, the users who are most interested in
conspiracy theories and the political right seem to be distributing the most
junk news, whereas users who are either in the military or are veterans are
among the most sophisticated news consumers, and share very little junk news
through the network.
</summary>
    <author>
      <name>John D. Gallacher</name>
    </author>
    <author>
      <name>Vlad Barash</name>
    </author>
    <author>
      <name>Philip N. Howard</name>
    </author>
    <author>
      <name>John Kelly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Data Memo</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04698v1</id>
    <updated>2018-05-12T11:05:11Z</updated>
    <published>2018-05-12T11:05:11Z</published>
    <title>Bitcoin Risk Modeling with Blockchain Graphs</title>
    <summary>  A key challenge for Bitcoin cryptocurrency holders, such as startups using
ICOs to raise funding, is managing their FX risk. Specifically, a misinformed
decision to convert Bitcoin to fiat currency could, by itself, cost USD
millions.
  In contrast to financial exchanges, Blockchain based crypto-currencies expose
the entire transaction history to the public. By processing all transactions,
we model the network with a high fidelity graph so that it is possible to
characterize how the flow of information in the network evolves over time. We
demonstrate how this data representation permits a new form of microstructure
modeling - with the emphasis on the topological network structures to study the
role of users, entities and their interactions in formation and dynamics of
crypto-currency investment risk. In particular, we identify certain sub-graphs
('chainlets') that exhibit predictive influence on Bitcoin price and
volatility, and characterize the types of chainlets that signify extreme
losses.
</summary>
    <author>
      <name>Cuneyt Akcora</name>
    </author>
    <author>
      <name>Matthew Dixon</name>
    </author>
    <author>
      <name>Yulia Gel</name>
    </author>
    <author>
      <name>Murat Kantarcioglu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">JEL Classification: C58, C63, G18</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11303v1</id>
    <updated>2018-05-29T08:34:53Z</updated>
    <published>2018-05-29T08:34:53Z</published>
    <title>Trust-based dynamic linear threshold models for non-competitive and
  competitive influence propagation</title>
    <summary>  What are the key-features that enable an information diffusion model to
explain the inherent dynamic, and often competitive, nature of real-world
propagation phenomena? In this paper we aim to answer this question by
proposing a novel class of diffusion models, inspired by the classic Linear
Threshold model, and built around the following aspects: trust/distrust in the
user relationships, which is leveraged to model different effects of social
influence on the decisions taken by an individual; changes in adopting one or
alternative information items; hesitation towards adopting an information item
over time; latency in the propagation; time horizon for the unfolding of the
diffusion process; and multiple cascades of information that might occur
competitively. To the best of our knowledge, the above aspects have never been
unified into the same LT-based diffusion model. We also define different
strategies for the selection of the initial influencers to simulate
non-competitive and competitive diffusion scenarios, particularly related to
the problem of limitation of misinformation spread. Results on publicly
available networks have shown the meaningfulness and uniqueness of our models.
</summary>
    <author>
      <name>Antonio Cali√≤</name>
    </author>
    <author>
      <name>Andrea Tagarelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted (May 5, 2018) at the IEEE TrustCom/BigDataSE 2018 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.06416v1</id>
    <updated>2018-09-17T19:51:18Z</updated>
    <published>2018-09-17T19:51:18Z</published>
    <title>DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep
  Learning</title>
    <summary>  Misinformation such as fake news is one of the big challenges of our society.
Research on automated fact-checking has proposed methods based on supervised
learning, but these approaches do not consider external evidence apart from
labeled training instances. Recent approaches counter this deficit by
considering external sources related to a claim. However, these methods require
substantial feature modeling and rich lexicons. This paper overcomes these
limitations of prior work with an end-to-end model for evidence-aware
credibility assessment of arbitrary textual claims, without any human
intervention. It presents a neural network model that judiciously aggregates
signals from external evidence articles, the language of these articles and the
trustworthiness of their sources. It also derives informative features for
generating user-comprehensible explanations that makes the neural network
predictions transparent to the end-user. Experiments with four datasets and
ablation studies show the strength of our method.
</summary>
    <author>
      <name>Kashyap Popat</name>
    </author>
    <author>
      <name>Subhabrata Mukherjee</name>
    </author>
    <author>
      <name>Andrew Yates</name>
    </author>
    <author>
      <name>Gerhard Weikum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.06416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.06416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04670v1</id>
    <updated>2018-11-12T11:40:09Z</updated>
    <published>2018-11-12T11:40:09Z</published>
    <title>A Deep Ensemble Framework for Fake News Detection and Classification</title>
    <summary>  Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.
</summary>
    <author>
      <name>Arjun Roy</name>
    </author>
    <author>
      <name>Kingshuk Basak</name>
    </author>
    <author>
      <name>Asif Ekbal</name>
    </author>
    <author>
      <name>Pushpak Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, accepted as a short paper in Web Intelligence 2018
  (https://webintelligence2018.com/accepted-papers.html), title changed from
  {"Going Deep to Detect Liars" Detecting Fake News using Deep Learning} to {A
  Deep Ensemble Framework for Fake News Detection and Classification} as per
  reviewers suggestion</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07031v1</id>
    <updated>2018-11-16T20:57:37Z</updated>
    <published>2018-11-16T20:57:37Z</published>
    <title>Improving Rotated Text Detection with Rotation Region Proposal Networks</title>
    <summary>  A significant number of images shared on social media platforms such as
Facebook and Instagram contain text in various forms. It's increasingly
becoming commonplace for bad actors to share misinformation, hate speech or
other kinds of harmful content as text overlaid on images on such platforms. A
scene-text understanding system should hence be able to handle text in various
orientations that the adversary might use. Moreover, such a system can be
incorporated into screen readers used to aid the visually impaired. In this
work, we extend the scene-text extraction system at Facebook, Rosetta, to
efficiently handle text in various orientations. Specifically, we incorporate
the Rotation Region Proposal Networks (RRPN) in our text extraction pipeline
and offer practical suggestions for building and deploying a model for
detecting and recognizing text in arbitrary orientations efficiently.
Experimental results show a significant improvement on detecting rotated text.
</summary>
    <author>
      <name>Jing Huang</name>
    </author>
    <author>
      <name>Viswanath Sivakumar</name>
    </author>
    <author>
      <name>Mher Mnatsakanyan</name>
    </author>
    <author>
      <name>Guan Pang</name>
    </author>
    <link href="http://arxiv.org/abs/1811.07031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.09729v3</id>
    <updated>2019-09-11T18:29:06Z</updated>
    <published>2018-11-24T00:06:10Z</published>
    <title>Generate, Segment and Refine: Towards Generic Manipulation Segmentation</title>
    <summary>  Detecting manipulated images has become a significant emerging challenge. The
advent of image sharing platforms and the easy availability of advanced photo
editing software have resulted in a large quantities of manipulated images
being shared on the internet. While the intent behind such manipulations varies
widely, concerns on the spread of fake news and misinformation is growing.
Current state of the art methods for detecting these manipulated images suffers
from the lack of training data due to the laborious labeling process. We
address this problem in this paper, for which we introduce a manipulated image
generation process that creates true positives using currently available
datasets. Drawing from traditional work on image blending, we propose a novel
generator for creating such examples. In addition, we also propose to further
create examples that force the algorithm to focus on boundary artifacts during
training. Strong experimental results validate our proposal.
</summary>
    <author>
      <name>Peng Zhou</name>
    </author>
    <author>
      <name>Bor-Chun Chen</name>
    </author>
    <author>
      <name>Xintong Han</name>
    </author>
    <author>
      <name>Mahyar Najibi</name>
    </author>
    <author>
      <name>Abhinav Shrivastava</name>
    </author>
    <author>
      <name>Ser Nam Lim</name>
    </author>
    <author>
      <name>Larry S. Davis</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI-2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.09729v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.09729v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.12349v2</id>
    <updated>2018-12-04T16:15:25Z</updated>
    <published>2018-11-29T17:54:49Z</published>
    <title>Combating Fake News with Interpretable News Feed Algorithms</title>
    <summary>  Nowadays, artificial intelligence algorithms are used for targeted and
personalized content distribution in the large scale as part of the intense
competition for attention in the digital media environment. Unfortunately,
targeted information dissemination may result in intellectual isolation and
discrimination. Further, as demonstrated in recent political events in the US
and EU, malicious bots and social media users can create and propagate targeted
`fake news' content in different forms for political gains. From the other
direction, fake news detection algorithms attempt to combat such problems by
identifying misinformation and fraudulent user profiles. This paper reviews
common news feed algorithms as well as methods for fake news detection, and we
discuss how news feed algorithms could be misused to promote falsified content,
affect news diversity, or impact credibility. We review how news feed
algorithms and recommender engines can enable confirmation bias to isolate
users to certain news sources and affecting the perception of reality. As a
potential solution for increasing user awareness of how content is selected or
sorted, we argue for the use of interpretable and explainable news feed
algorithms. We discuss how improved user awareness and system transparency
could mitigate unwanted outcomes of echo chambers and bubble filters in social
media.
</summary>
    <author>
      <name>Sina Mohseni</name>
    </author>
    <author>
      <name>Eric Ragan</name>
    </author>
    <link href="http://arxiv.org/abs/1811.12349v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12349v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.09383v2</id>
    <updated>2019-01-03T14:35:55Z</updated>
    <published>2018-12-21T21:46:34Z</published>
    <title>Technology-Enabled Disinformation: Summary, Lessons, and Recommendations</title>
    <summary>  Technology is increasingly used -- unintentionally (misinformation) or
intentionally (disinformation) -- to spread false information at scale, with
potentially broad-reaching societal effects. For example, technology enables
increasingly realistic false images and videos, and hyper-personal targeting
means different people may see different versions of reality. This report is
the culmination of a PhD-level special topics course
(https://courses.cs.washington.edu/courses/cse599b/18au/) in Computer Science &amp;
Engineering at the University of Washington's Paul G. Allen School in the fall
of 2018. The goals of this course were to study (1) how technologies and
today's technical platforms enable and support the creation and spread of such
mis- and disinformation, as well as (2) how technical approaches could be used
to mitigate these issues. In this report, we summarize the space of
technology-enabled mis- and disinformation based on our investigations, and
then surface our lessons and recommendations for technologists, researchers,
platform designers, policymakers, and users.
</summary>
    <author>
      <name>John Akers</name>
    </author>
    <author>
      <name>Gagan Bansal</name>
    </author>
    <author>
      <name>Gabriel Cadamuro</name>
    </author>
    <author>
      <name>Christine Chen</name>
    </author>
    <author>
      <name>Quanze Chen</name>
    </author>
    <author>
      <name>Lucy Lin</name>
    </author>
    <author>
      <name>Phoebe Mulcaire</name>
    </author>
    <author>
      <name>Rajalakshmi Nandakumar</name>
    </author>
    <author>
      <name>Matthew Rockett</name>
    </author>
    <author>
      <name>Lucy Simko</name>
    </author>
    <author>
      <name>John Toman</name>
    </author>
    <author>
      <name>Tongshuang Wu</name>
    </author>
    <author>
      <name>Eric Zeng</name>
    </author>
    <author>
      <name>Bill Zorn</name>
    </author>
    <author>
      <name>Franziska Roesner</name>
    </author>
    <link href="http://arxiv.org/abs/1812.09383v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09383v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.11535v1</id>
    <updated>2018-12-30T13:53:44Z</updated>
    <published>2018-12-30T13:53:44Z</published>
    <title>Group based Centrality for Immunization of Complex Networks</title>
    <summary>  Network immunization is an extensively recognized issue in several domains
like virtual network security, public health and social media, to deal with the
problem of node inoculation so as to minimize the transmission through the
links existed in these networks. We aim to identify top ranked nodes to
immunize networks, leading to control the outbreak of epidemics or
misinformation. We consider group based centrality and define a heuristic
objective criteria to establish the target of key nodes finding in network
which if immunized result in essential network vulnerability. We propose a
group based game theoretic payoff division approach, by employing Shapley value
to assign the surplus acquired by participating nodes in different groups
through the positional power and functional influence over other nodes. We tag
these key nodes as Shapley Value based Information Delimiters (SVID).
Experiments on empirical data sets and model networks establish the efficacy of
our proposed approach and acknowledge performance of node inoculation to
delimit contagion outbreak.
</summary>
    <author>
      <name>Chandni Saxena</name>
    </author>
    <author>
      <name>M. N. Doja</name>
    </author>
    <author>
      <name>Tanvir Ahmad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2018.05.107</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2018.05.107" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A: Statistical Mechanics and its Applications Volume 508,
  15 October 2018, Pages 35-47</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.11535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.11535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.06437v1</id>
    <updated>2019-01-18T22:57:09Z</updated>
    <published>2019-01-18T22:57:09Z</published>
    <title>Combating Fake News: A Survey on Identification and Mitigation
  Techniques</title>
    <summary>  The proliferation of fake news on social media has opened up new directions
of research for timely identification and containment of fake news, and
mitigation of its widespread impact on public opinion. While much of the
earlier research was focused on identification of fake news based on its
contents or by exploiting users' engagements with the news on social media,
there has been a rising interest in proactive intervention strategies to
counter the spread of misinformation and its impact on society. In this survey,
we describe the modern-day problem of fake news and, in particular, highlight
the technical challenges associated with it. We discuss existing methods and
techniques applicable to both identification and mitigation, with a focus on
the significant advances in each method and their advantages and limitations.
In addition, research has often been limited by the quality of existing
datasets and their specific application contexts. To alleviate this problem, we
comprehensively compile and summarize characteristic features of available
datasets. Furthermore, we outline new directions of research to facilitate
future development of effective and interdisciplinary solutions.
</summary>
    <author>
      <name>Karishma Sharma</name>
    </author>
    <author>
      <name>Feng Qian</name>
    </author>
    <author>
      <name>He Jiang</name>
    </author>
    <author>
      <name>Natali Ruchansky</name>
    </author>
    <author>
      <name>Ming Zhang</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Intelligent Systems and Technology, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.06437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.06437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02580v2</id>
    <updated>2019-06-14T09:51:50Z</updated>
    <published>2019-02-07T12:00:35Z</published>
    <title>The few-get-richer: a surprising consequence of popularity-based
  rankings</title>
    <summary>  Ranking algorithms play a crucial role in online platforms ranging from
search engines to recommender systems. In this paper, we identify a surprising
consequence of popularity-based rankings: the fewer the items reporting a given
signal, the higher the share of the overall traffic they collectively attract.
This few-get-richer effect emerges in settings where there are few distinct
classes of items (e.g., left-leaning news sources versus right-leaning news
sources), and items are ranked based on their popularity. We demonstrate
analytically that the few-get-richer effect emerges when people tend to click
on top-ranked items and have heterogeneous preferences for the classes of
items. Using simulations, we analyze how the strength of the effect changes
with assumptions about the setting and human behavior. We also test our
predictions experimentally in an online experiment with human participants. Our
findings have important implications to understand the spread of
misinformation.
</summary>
    <author>
      <name>Fabrizio Germano</name>
    </author>
    <author>
      <name>Vicen√ß G√≥mez</name>
    </author>
    <author>
      <name>Ga√´l Le Mens</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3308558.3313693</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3308558.3313693" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 1 table, Proceedings of The Web Conference (WWW
  2019)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WWW '19: The World Wide Web ConferenceMay 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.02580v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02580v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1; H.3.3; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.00788v3</id>
    <updated>2019-04-09T21:17:49Z</updated>
    <published>2019-03-02T23:14:58Z</published>
    <title>AIRD: Adversarial Learning Framework for Image Repurposing Detection</title>
    <summary>  Image repurposing is a commonly used method for spreading misinformation on
social media and online forums, which involves publishing untampered images
with modified metadata to create rumors and further propaganda. While manual
verification is possible, given vast amounts of verified knowledge available on
the internet, the increasing prevalence and ease of this form of semantic
manipulation call for the development of robust automatic ways of assessing the
semantic integrity of multimedia data. In this paper, we present a novel method
for image repurposing detection that is based on the real-world adversarial
interplay between a bad actor who repurposes images with counterfeit metadata
and a watchdog who verifies the semantic consistency between images and their
accompanying metadata, where both players have access to a reference dataset of
verified content, which they can use to achieve their goals. The proposed
method exhibits state-of-the-art performance on location-identity,
subject-identity and painting-artist verification, showing its efficacy across
a diverse set of scenarios.
</summary>
    <author>
      <name>Ayush Jaiswal</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Wael AbdAlmageed</name>
    </author>
    <author>
      <name>Iacopo Masi</name>
    </author>
    <author>
      <name>Premkumar Natarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera-ready version for the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR), 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.00788v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.00788v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.11452v3</id>
    <updated>2020-08-20T20:19:03Z</updated>
    <published>2019-03-27T14:28:17Z</published>
    <title>Lexical convergence and collective identities on Facebook</title>
    <summary>  Recent studies, targeting Facebook, showed the tendency of users to interact
with information adhering to their preferred narrative and to ignore dissenting
information. Primarily driven by confirmation bias, users tend to join
polarized clusters where they cooperate to reinforce a like-minded system of
beliefs, thus facilitating fake news and misinformation cascades. To gain a
deeper understanding of these phenomena, in this work we analyze the lexicons
used by the communities of users emerging on Facebook around verified and
unverified contents. We show how the lexical approach provides important
insights about the kind of information processed by the two communities of
users and about their overall sentiment. Furthermore, by focusing on comment
threads, we observe a strong positive correlation between the lexical
convergence of co-commenters and their number of interactions, which in turns
suggests that such a trend could be a proxy for the emergence of collective
identities and polarization in opinion dynamics.
</summary>
    <author>
      <name>Emanuele Brugnoli</name>
    </author>
    <author>
      <name>Matteo Cinelli</name>
    </author>
    <author>
      <name>Fabiana Zollo</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <author>
      <name>Antonio Scala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.11452v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.11452v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91F20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.03588v1</id>
    <updated>2019-07-05T17:16:14Z</updated>
    <published>2019-07-05T17:16:14Z</published>
    <title>A New Approach to Distributed Hypothesis Testing and Non-Bayesian
  Learning: Improved Learning Rate and Byzantine-Resilience</title>
    <summary>  We study a setting where a group of agents, each receiving partially
informative private signals, seek to collaboratively learn the true underlying
state of the world (from a finite set of hypotheses) that generates their joint
observation profiles. To solve this problem, we propose a distributed learning
rule that differs fundamentally from existing approaches, in that it does not
employ any form of "belief-averaging". Instead, agents update their beliefs
based on a min-rule. Under standard assumptions on the observation model and
the network structure, we establish that each agent learns the truth
asymptotically almost surely. As our main contribution, we prove that with
probability 1, each false hypothesis is ruled out by every agent exponentially
fast at a network-independent rate that is strictly larger than existing rates.
We then develop a computationally-efficient variant of our learning rule that
is provably resilient to agents who do not behave as expected (as represented
by a Byzantine adversary model) and deliberately try to spread misinformation.
</summary>
    <author>
      <name>Aritra Mitra</name>
    </author>
    <author>
      <name>John A. Richards</name>
    </author>
    <author>
      <name>Shreyas Sundaram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1903.05817</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.03588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11543v2</id>
    <updated>2019-07-29T03:14:27Z</updated>
    <published>2019-07-26T12:52:57Z</published>
    <title>Entropy-Regularized Stochastic Games</title>
    <summary>  In two-player zero-sum stochastic games, where two competing players make
decisions under uncertainty, a pair of optimal strategies is traditionally
described by Nash equilibrium and computed under the assumption that the
players have perfect information about the stochastic transition model of the
environment. However, implementing such strategies may make the players
vulnerable to unforeseen changes in the environment. In this paper, we
introduce entropy-regularized stochastic games where each player aims to
maximize the causal entropy of its strategy in addition to its expected payoff.
The regularization term balances each player's rationality with its belief
about the level of misinformation about the transition model. We consider both
entropy-regularized $N$-stage and entropy-regularized discounted stochastic
games, and establish the existence of a value in both games. Moreover, we prove
the sufficiency of Markovian and stationary mixed strategies to attain the
value, respectively, in $N$-stage and discounted games. Finally, we present
algorithms, which are based on convex optimization problems, to compute the
optimal strategies. In a numerical example, we demonstrate the proposed method
on a motion planning scenario and illustrate the effect of the regularization
term on the expected payoff.
</summary>
    <author>
      <name>Yagiz Savas</name>
    </author>
    <author>
      <name>Mohamadreza Ahmadi</name>
    </author>
    <author>
      <name>Takashi Tanaka</name>
    </author>
    <author>
      <name>Ufuk Topcu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.11543v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11543v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.00153v2</id>
    <updated>2019-08-29T00:46:25Z</updated>
    <published>2019-08-01T00:28:57Z</published>
    <title>Hateful People or Hateful Bots? Detection and Characterization of Bots
  Spreading Religious Hatred in Arabic Social Media</title>
    <summary>  Arabic Twitter space is crawling with bots that fuel political feuds, spread
misinformation, and proliferate sectarian rhetoric. While efforts have long
existed to analyze and detect English bots, Arabic bot detection and
characterization remains largely understudied. In this work, we contribute new
insights into the role of bots in spreading religious hatred on Arabic Twitter
and introduce a novel regression model that can accurately identify Arabic
language bots. Our assessment shows that existing tools that are highly
accurate in detecting English bots don't perform as well on Arabic bots. We
identify the possible reasons for this poor performance, perform a thorough
analysis of linguistic, content, behavioral and network features, and report on
the most informative features that distinguish Arabic bots from humans as well
as the differences between Arabic and English bots. Our results mark an
important step toward understanding the behavior of malicious bots on Arabic
Twitter and pave the way for a more effective Arabic bot detection tools.
</summary>
    <author>
      <name>Nuha Albadi</name>
    </author>
    <author>
      <name>Maram Kurdi</name>
    </author>
    <author>
      <name>Shivakant Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3359163</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3359163" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ACM Hum.-Comput. Interact. 3, CSCW: Article 61 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.00153v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00153v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01760v1</id>
    <updated>2019-08-05T17:59:44Z</updated>
    <published>2019-08-05T17:59:44Z</published>
    <title>The Myths of Our Time: Fake News</title>
    <summary>  While the purpose of most fake news is misinformation and political
propaganda, our team sees it as a new type of myth that is created by people in
the age of internet identities and artificial intelligence. Seeking insights on
the fear and desire hidden underneath these modified or generated stories, we
use machine learning methods to generate fake articles and present them in the
form of an online news blog. This paper aims to share the details of our
pipeline and the techniques used for full generation of fake news, from dataset
collection to presentation as a media art project on the internet.
</summary>
    <author>
      <name>V√≠t R≈Ø≈æiƒçka</name>
    </author>
    <author>
      <name>Eunsu Kang</name>
    </author>
    <author>
      <name>David Gordon</name>
    </author>
    <author>
      <name>Ankita Patel</name>
    </author>
    <author>
      <name>Jacqui Fashimpaur</name>
    </author>
    <author>
      <name>Manzil Zaheer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, in proceedings of International Symposium on
  Electronic Art 2019 (ISEA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of International Symposium on Electronic Art 2019
  (ISEA), pages 494-498</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.01760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.01708v1</id>
    <updated>2019-11-20T23:32:59Z</updated>
    <published>2019-11-20T23:32:59Z</published>
    <title>Celebrating Three Decades of Worldwide Stock Market Manipulation</title>
    <summary>  As the decade turns, we reflect on nearly thirty years of successful
manipulation of the world's public equity markets. This reflection highlights a
few of the key enabling ingredients and lessons learned along the way. A
quantitative understanding of market impact and its decay, which we cover
briefly, lets you move long-term market prices to your advantage at acceptable
cost. Hiding your footprints turns out to be less important than moving prices
in the direction most people want them to move. Widespread (if misplaced) trust
of market prices -- buttressed by overestimates of the cost of manipulation and
underestimates of the benefits to certain market participants -- makes price
manipulation a particularly valuable and profitable tool. Of the many recent
stories heralding the dawn of the present golden age of misinformation, the
manipulation leading to the remarkable increase in the market capitalization of
the world's publicly traded companies over the past three decades is among the
best.
</summary>
    <author>
      <name>Bruce Knuteson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.01708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06745v1</id>
    <updated>2019-12-13T23:32:38Z</updated>
    <published>2019-12-13T23:32:38Z</published>
    <title>An Unsupervised Domain-Independent Framework for Automated Detection of
  Persuasion Tactics in Text</title>
    <summary>  With the increasing growth of social media, people have started relying
heavily on the information shared therein to form opinions and make decisions.
While such a reliance is motivation for a variety of parties to promote
information, it also makes people vulnerable to exploitation by slander,
misinformation, terroristic and predatorial advances. In this work, we aim to
understand and detect such attempts at persuasion. Existing works on detecting
persuasion in text make use of lexical features for detecting persuasive
tactics, without taking advantage of the possible structures inherent in the
tactics used. We formulate the task as a multi-class classification problem and
propose an unsupervised, domain-independent machine learning framework for
detecting the type of persuasion used in text, which exploits the inherent
sentence structure present in the different persuasion tactics. Our work shows
promising results as compared to existing work.
</summary>
    <author>
      <name>Rahul Radhakrishnan Iyer</name>
    </author>
    <author>
      <name>Katia Sycara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.06745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.03231v2</id>
    <updated>2020-01-31T01:50:53Z</updated>
    <published>2020-01-09T21:45:25Z</published>
    <title>Four Years in Review: Statistical Practices of Likert Scales in
  Human-Robot Interaction Studies</title>
    <summary>  As robots become more prevalent, the importance of the field of human-robot
interaction (HRI) grows accordingly. As such, we should endeavor to employ the
best statistical practices. Likert scales are commonly used metrics in HRI to
measure perceptions and attitudes. Due to misinformation or honest mistakes,
most HRI researchers do not adopt best practices when analyzing Likert data. We
conduct a review of psychometric literature to determine the current standard
for Likert scale design and analysis. Next, we conduct a survey of four years
of the International Conference on Human-Robot Interaction (2016 through 2019)
and report on incorrect statistical practices and design of Likert scales.
During these years, only 3 of the 110 papers applied proper statistical testing
to correctly-designed Likert scales. Our analysis suggests there are areas for
meaningful improvement in the design and testing of Likert scales. Lastly, we
provide recommendations to improve the accuracy of conclusions drawn from
Likert data.
</summary>
    <author>
      <name>Mariah L. Schrum</name>
    </author>
    <author>
      <name>Michael Johnson</name>
    </author>
    <author>
      <name>Muyleng Ghuy</name>
    </author>
    <author>
      <name>Matthew C. Gombolay</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3319502.3378178</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3319502.3378178" rel="related"/>
    <link href="http://arxiv.org/abs/2001.03231v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03231v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09545v1</id>
    <updated>2020-01-27T00:19:41Z</updated>
    <published>2020-01-27T00:19:41Z</published>
    <title>aiTPR: Attribute Interaction-Tensor Product Representation for Image
  Caption</title>
    <summary>  Region visual features enhance the generative capability of the machines
based on features, however they lack proper interaction attentional perceptions
and thus ends up with biased or uncorrelated sentences or pieces of
misinformation. In this work, we propose Attribute Interaction-Tensor Product
Representation (aiTPR) which is a convenient way of gathering more information
through orthogonal combination and learning the interactions as physical
entities (tensors) and improving the captions. Compared to previous works,
where features are added up to undefined feature spaces, TPR helps in
maintaining sanity in combinations and orthogonality helps in defining familiar
spaces. We have introduced a new concept layer that defines the objects and
also their interactions that can play a crucial role in determination of
different descriptions. The interaction portions have contributed heavily for
better caption quality and has out-performed different previous works on this
domain and MSCOCO dataset. We introduced, for the first time, the notion of
combining regional image features and abstracted interaction likelihood
embedding for image captioning.
</summary>
    <author>
      <name>Chiranjib Sur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.09545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00502v2</id>
    <updated>2021-08-10T11:41:16Z</updated>
    <published>2020-10-01T15:50:41Z</published>
    <title>AMUSED: An Annotation Framework of Multi-modal Social Media Data</title>
    <summary>  In this paper, we present a semi-automated framework called AMUSED for
gathering multi-modal annotated data from the multiple social media platforms.
The framework is designed to mitigate the issues of collecting and annotating
social media data by cohesively combining machine and human in the data
collection process. From a given list of the articles from professional news
media or blog, AMUSED detects links to the social media posts from news
articles and then downloads contents of the same post from the respective
social media platform to gather details about that specific post. The framework
is capable of fetching the annotated data from multiple platforms like Twitter,
YouTube, Reddit. The framework aims to reduce the workload and problems behind
the data annotation from the social media platforms. AMUSED can be applied in
multiple application domains, as a use case, we have implemented the framework
for collecting COVID-19 misinformation data from different social media
platforms.
</summary>
    <author>
      <name>Gautam Kishore Shahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.00502v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00502v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09078v1</id>
    <updated>2020-10-18T19:37:24Z</updated>
    <published>2020-10-18T19:37:24Z</published>
    <title>Incorporating Count-Based Features into Pre-Trained Models for Improved
  Stance Detection</title>
    <summary>  The explosive growth and popularity of Social Media has revolutionised the
way we communicate and collaborate. Unfortunately, this same ease of accessing
and sharing information has led to an explosion of misinformation and
propaganda. Given that stance detection can significantly aid in veracity
prediction, this work focuses on boosting automated stance detection, a task on
which pre-trained models have been extremely successful on, as on several other
tasks. This work shows that the task of stance detection can benefit from
feature based information, especially on certain under performing classes,
however, integrating such features into pre-trained models using ensembling is
challenging. We propose a novel architecture for integrating features with
pre-trained models that address these challenges and test our method on the
RumourEval 2019 dataset. This method achieves state-of-the-art results with an
F1-score of 63.94 on the test set.
</summary>
    <author>
      <name>Anushka Prakash</name>
    </author>
    <author>
      <name>Harish Tayyar Madabushi</name>
    </author>
    <link href="http://arxiv.org/abs/2010.09078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09113v1</id>
    <updated>2020-10-18T21:44:23Z</updated>
    <published>2020-10-18T21:44:23Z</published>
    <title>Disinformation in the Online Information Ecosystem: Detection,
  Mitigation and Challenges</title>
    <summary>  With the rapid increase in access to internet and the subsequent growth in
the population of online social media users, the quality of information posted,
disseminated and consumed via these platforms is an issue of growing concern. A
large fraction of the common public turn to social media platforms and in
general the internet for news and even information regarding highly concerning
issues such as COVID-19 symptoms. Given that the online information ecosystem
is extremely noisy, fraught with misinformation and disinformation, and often
contaminated by malicious agents spreading propaganda, identifying genuine and
good quality information from disinformation is a challenging task for humans.
In this regard, there is a significant amount of ongoing research in the
directions of disinformation detection and mitigation. In this survey, we
discuss the online disinformation problem, focusing on the recent 'infodemic'
in the wake of the coronavirus pandemic. We then proceed to discuss the
inherent challenges in disinformation research, and then elaborate on the
computational and interdisciplinary approaches towards mitigation of
disinformation, after a short overview of the various directions explored in
detection efforts.
</summary>
    <author>
      <name>Amrita Bhattacharjee</name>
    </author>
    <author>
      <name>Kai Shu</name>
    </author>
    <author>
      <name>Min Gao</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A Chinese version of this manuscript has been submitted to the
  Journal of Computer Research and Development</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.09113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13691v2</id>
    <updated>2021-03-17T02:21:05Z</updated>
    <published>2020-10-26T16:11:56Z</published>
    <title>The Manufacture of Partisan Echo Chambers by Follow Train Abuse on
  Twitter</title>
    <summary>  A growing body of evidence points to critical vulnerabilities of social
media, such as the emergence of partisan echo chambers and the viral spread of
misinformation. We show that these vulnerabilities are amplified by abusive
behaviors associated with so-called "follow trains" on Twitter, in which long
lists of like-minded accounts are mentioned for others to follow. We present
the first systematic analysis of a large U.S. hyper-partisan train network. We
observe an artificial inflation of influence: accounts heavily promoted by
follow trains profit from a median six-fold increase in daily follower growth.
This catalyzes the formation of highly clustered echo chambers, hierarchically
organized around a dense core of active accounts. Train accounts also engage in
other behaviors that violate platform policies: we find evidence of activity by
inauthentic automated accounts and abnormal content deletion, as well as
amplification of toxic content from low-credibility and conspiratorial sources.
Some train accounts have been active for years, suggesting that platforms need
to pay greater attention to this kind of abuse.
</summary>
    <author>
      <name>Christopher Torres-Lugo</name>
    </author>
    <author>
      <name>Kai-Cheng Yang</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. Intl. AAAI Conf. on Web and Social Media (ICWSM), 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.13691v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13691v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.14190v1</id>
    <updated>2020-10-27T10:52:23Z</updated>
    <published>2020-10-27T10:52:23Z</published>
    <title>Collective Movement with Signaling</title>
    <summary>  We consider a population of mobile agents able to make noisy observation of
the environment and communicate their observation by production and
comprehension of signals. Individuals try to align their movement direction
with their neighbors. Besides, they try to collectively find and travel towards
an environmental direction. We show that, when the fraction of informed
individuals is small, by increasing the noise in communication, similarly to
the Viscek model, the model shows a discontinuous order-disorder transition
with strong finite size effects. In contrast, for large fraction of informed
individuals, it is possible to go from the ordered phase to the disordered
phase without passing any phase transition. The ordered phase is composed of
two phases separated by a discontinuous transition. Informed collective motion,
in which the population collectively infers the correct environmental
direction, occurs for high fraction of informed individuals. When the fraction
of informed individuals is low, misinformed collective motion, where the
population fails to find the environmental direction becomes stable as well.
Besides, we show that an amount of noise in the production of signals is more
detrimental for the inference capability of the population, and increases the
density fluctuations and the probability of group fragmentation, compared to
the same amount of noise in the comprehension.
</summary>
    <author>
      <name>Mohammad Salahshour</name>
    </author>
    <author>
      <name>Shahin Rouhani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages; 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.14190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.14190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.00614v2</id>
    <updated>2021-01-02T16:07:48Z</updated>
    <published>2020-12-01T16:32:54Z</published>
    <title>CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims</title>
    <summary>  We introduce CLIMATE-FEVER, a new publicly available dataset for verification
of climate change-related claims. By providing a dataset for the research
community, we aim to facilitate and encourage work on improving algorithms for
retrieving evidential support for climate-specific claims, addressing the
underlying language understanding challenges, and ultimately help alleviate the
impact of misinformation on climate change. We adapt the methodology of FEVER
[1], the largest dataset of artificially designed claims, to real-life claims
collected from the Internet. While during this process, we could rely on the
expertise of renowned climate scientists, it turned out to be no easy task. We
discuss the surprising, subtle complexity of modeling real-world
climate-related claims within the \textsc{fever} framework, which we believe
provides a valuable challenge for general natural language understanding. We
hope that our work will mark the beginning of a new exciting long-term joint
effort by the climate science and AI community.
</summary>
    <author>
      <name>Thomas Diggelmann</name>
    </author>
    <author>
      <name>Jordan Boyd-Graber</name>
    </author>
    <author>
      <name>Jannis Bulian</name>
    </author>
    <author>
      <name>Massimiliano Ciaramita</name>
    </author>
    <author>
      <name>Markus Leippold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for the Tackling Climate Change with Machine Learning
  Workshop at NeurIPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.00614v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.00614v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.01462v3</id>
    <updated>2021-03-01T12:24:15Z</updated>
    <published>2020-12-02T19:05:25Z</published>
    <title>ArCorona: Analyzing Arabic Tweets in the Early Days of Coronavirus
  (COVID-19) Pandemic</title>
    <summary>  Over the past few months, there were huge numbers of circulating tweets and
discussions about Coronavirus (COVID-19) in the Arab region. It is important
for policy makers and many people to identify types of shared tweets to better
understand public behavior, topics of interest, requests from governments,
sources of tweets, etc. It is also crucial to prevent spreading of rumors and
misinformation about the virus or bad cures. To this end, we present the
largest manually annotated dataset of Arabic tweets related to COVID-19. We
describe annotation guidelines, analyze our dataset and build effective machine
learning and transformer based models for classification.
</summary>
    <author>
      <name>Hamdy Mubarak</name>
    </author>
    <author>
      <name>Sabit Hassan</name>
    </author>
    <link href="http://arxiv.org/abs/2012.01462v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.01462v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07517v1</id>
    <updated>2020-11-30T16:41:04Z</updated>
    <published>2020-11-30T16:41:04Z</published>
    <title>Fake News Detection in Social Media using Graph Neural Networks and NLP
  Techniques: A COVID-19 Use-case</title>
    <summary>  The paper presents our solutions for the MediaEval 2020 task namely FakeNews:
Corona Virus and 5G Conspiracy Multimedia Twitter-Data-Based Analysis. The task
aims to analyze tweets related to COVID-19 and 5G conspiracy theories to detect
misinformation spreaders. The task is composed of two sub-tasks namely (i)
text-based, and (ii) structure-based fake news detection. For the first task,
we propose six different solutions relying on Bag of Words (BoW) and BERT
embedding. Three of the methods aim at binary classification task by
differentiating in 5G conspiracy and the rest of the COVID-19 related tweets
while the rest of them treat the task as ternary classification problem. In the
ternary classification task, our BoW and BERT based methods obtained an
F1-score of .606% and .566% on the development set, respectively. On the binary
classification, the BoW and BERT based solutions obtained an average F1-score
of .666% and .693%, respectively. On the other hand, for structure-based fake
news detection, we rely on Graph Neural Networks (GNNs) achieving an average
ROC of .95% on the development set.
</summary>
    <author>
      <name>Abdullah Hamid</name>
    </author>
    <author>
      <name>Nasrullah Shiekh</name>
    </author>
    <author>
      <name>Naina Said</name>
    </author>
    <author>
      <name>Kashif Ahmad</name>
    </author>
    <author>
      <name>Asma Gul</name>
    </author>
    <author>
      <name>Laiq Hassan</name>
    </author>
    <author>
      <name>Ala Al-Fuqaha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.07517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.10185v1</id>
    <updated>2020-12-18T12:10:13Z</updated>
    <published>2020-12-18T12:10:13Z</published>
    <title>Recommenders with a mission: assessing diversity in newsrecommendations</title>
    <summary>  News recommenders help users to find relevant online content and have the
potential to fulfill a crucial role in a democratic society, directing the
scarce attention of citizens towards the information that is most important to
them. Simultaneously, recent concerns about so-called filter bubbles,
misinformation and selective exposure are symptomatic of the disruptive
potential of these digital news recommenders. Recommender systems can make or
break filter bubbles, and as such can be instrumental in creating either a more
closed or a more open internet. Current approaches to evaluating recommender
systems are often focused on measuring an increase in user clicks and
short-term engagement, rather than measuring the user's longer term interest in
diverse and important information.
  This paper aims to bridge the gap between normative notions of diversity,
rooted in democratic theory, and quantitative metrics necessary for evaluating
the recommender system. We propose a set of metrics grounded in social science
interpretations of diversity and suggest ways for practical implementations.
</summary>
    <author>
      <name>Sanne Vrijenhoek</name>
    </author>
    <author>
      <name>Mesut Kaya</name>
    </author>
    <author>
      <name>Nadia Metoui</name>
    </author>
    <author>
      <name>Judith M√∂ller</name>
    </author>
    <author>
      <name>Daan Odijk</name>
    </author>
    <author>
      <name>Natali Helberger</name>
    </author>
    <link href="http://arxiv.org/abs/2012.10185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.10185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.11967v3</id>
    <updated>2021-01-13T11:36:32Z</updated>
    <published>2020-12-22T12:43:12Z</published>
    <title>g2tmn at Constraint@AAAI2021: Exploiting CT-BERT and Ensembling Learning
  for COVID-19 Fake News Detection</title>
    <summary>  The COVID-19 pandemic has had a huge impact on various areas of human life.
Hence, the coronavirus pandemic and its consequences are being actively
discussed on social media. However, not all social media posts are truthful.
Many of them spread fake news that cause panic among readers, misinform people
and thus exacerbate the effect of the pandemic. In this paper, we present our
results at the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in
English. In particular, we propose our approach using the transformer-based
ensemble of COVID-Twitter-BERT (CT-BERT) models. We describe the models used,
the ways of text preprocessing and adding extra data. As a result, our best
model achieved the weighted F1-score of 98.69 on the test set (the first place
in the leaderboard) of this shared task that attracted 166 submitted teams in
total.
</summary>
    <author>
      <name>Anna Glazkova</name>
    </author>
    <author>
      <name>Maksim Glazkov</name>
    </author>
    <author>
      <name>Timofey Trifonov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-73696-5_12</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-73696-5_12" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The winning solution at the Constraint shared task (AAAI-2021)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Combating Online Hostile Posts in Regional Languages during
  Emergency Situation, 116-127, 2021. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.11967v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.11967v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.7.m; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.00192v2</id>
    <updated>2023-01-08T01:04:42Z</updated>
    <published>2021-05-01T08:25:43Z</published>
    <title>Deep Insights of Deepfake Technology : A Review</title>
    <summary>  Under the aegis of computer vision and deep learning technology, a new
emerging techniques has introduced that anyone can make highly realistic but
fake videos, images even can manipulates the voices. This technology is widely
known as Deepfake Technology. Although it seems interesting techniques to make
fake videos or image of something or some individuals but it could spread as
misinformation via internet. Deepfake contents could be dangerous for
individuals as well as for our communities, organizations, countries religions
etc. As Deepfake content creation involve a high level expertise with
combination of several algorithms of deep learning, it seems almost real and
genuine and difficult to differentiate. In this paper, a wide range of articles
have been examined to understand Deepfake technology more extensively. We have
examined several articles to find some insights such as what is Deepfake, who
are responsible for this, is there any benefits of Deepfake and what are the
challenges of this technology. We have also examined several creation and
detection techniques. Our study revealed that although Deepfake is a threat to
our societies, proper measures and strict regulations could prevent this.
</summary>
    <author>
      <name>Bahar Uddin Mahmud</name>
    </author>
    <author>
      <name>Afsana Sharmin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DUJASE Vol. 5(1 &amp; 2) 13-23, 2020 (January &amp; July)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.00192v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00192v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.02306v1</id>
    <updated>2021-05-05T20:08:36Z</updated>
    <published>2021-05-05T20:08:36Z</published>
    <title>Primary and Secondary Social Media Source Identification</title>
    <summary>  Social networks like Facebook and WhatsApp have enabled users to share images
with other users around the world. Along with this has come the rapid spread of
misinformation. One step towards verifying the authenticity of an image is
understanding its origin, including it distribution history through social
media. In this paper, we present a method for tracing the posting history of an
image across different social networks. To do this, we propose a two-stage
deep-learning-based approach, which takes advantage of cascaded fingerprints in
images left by social networks during uploading. Our proposed system is not
reliant upon metadata or similar easily falsifiable information. Through a
series of experiments, we show that we are able to outperform existing social
media source identification algorithms. and identify chains of social networks
up to length two with over over 84% accuracy.
</summary>
    <author>
      <name>Brian C Hosler</name>
    </author>
    <author>
      <name>Matthew C Stamm</name>
    </author>
    <link href="http://arxiv.org/abs/2105.02306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07454v2</id>
    <updated>2021-06-05T15:10:56Z</updated>
    <published>2021-05-16T15:38:49Z</published>
    <title>A Synchronized Action Framework for Responsible Detection of
  Coordination on Social Media</title>
    <summary>  The study of coordinated manipulation of conversations on social media has
become more prevalent as social media's role in amplifying misinformation,
hate, and polarization has come under scrutiny. We discuss the implications of
successful coordination detection algorithms based on shifts of power, and
consider how responsible coordination detection may be carried out through
synchronized action. We then propose a Synchronized Action Framework for
detection of automated coordination through construction and analysis of
multi-view networks. We validate our framework by examining the Reopen America
conversation on Twitter, discovering three coordinated campaigns. We further
investigate covert coordination surrounding the protests and find the task to
be far more complex than examples seen in prior work, demonstrating the need
for our multi-view approach. A cluster of suspicious users is identified and
the activity of three members is detailed. These users amplify protest messages
using the same hashtags at very similar times, though they all focus on
different states. Through this analysis, we emphasize both the potential
usefulness of coordination detection algorithms in investigating amplification,
and the need for careful and responsible deployment of such tools.
</summary>
    <author>
      <name>Thomas Magelinski</name>
    </author>
    <author>
      <name>Lynnette Hui Xian Ng</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <link href="http://arxiv.org/abs/2105.07454v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07454v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07854v1</id>
    <updated>2021-05-06T15:26:48Z</updated>
    <published>2021-05-06T15:26:48Z</published>
    <title>"Hey Alexa, What do You Know About the COVID-19 Vaccine?" --
  (Mis)perceptions of Mass Immunization Among Voice Assistant Users</title>
    <summary>  In this paper, we analyzed the perceived accuracy of COVID-19 vaccine
information spoken back by Amazon Alexa. Unlike social media, Amazon Alexa
doesn't apply soft moderation to unverified content, allowing for use of
third-party malicious skills to arbitrarily phrase COVID-19 vaccine
information. The results from a 210-participant study suggest that a
third-party malicious skill could successful reduce the perceived accuracy
among the users of information as to who gets the vaccine first, vaccine
testing, and the side effects of the vaccine. We also found that the
vaccine-hesitant participants are drawn to pessimistically rephrased Alexa
responses focused on the downsides of the mass immunization. We discuss
solutions for soft moderation against misperception-inducing or altogether
COVID-19 misinformation malicious third-party skills.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Anna Slowinski</name>
    </author>
    <author>
      <name>Peter Jachim</name>
    </author>
    <author>
      <name>Emma Pieroni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2104.04077</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.07854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09284v1</id>
    <updated>2021-04-25T05:00:53Z</updated>
    <published>2021-04-25T05:00:53Z</published>
    <title>SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and
  Images</title>
    <summary>  We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in
Texts and Images: the data, the annotation guidelines, the evaluation setup,
the results, and the participating systems. The task focused on memes and had
three subtasks: (i) detecting the techniques in the text, (ii) detecting the
text spans where the techniques are used, and (iii) detecting techniques in the
entire meme, i.e., both in the text and in the image. It was a popular task,
attracting 71 registrations, and 22 teams that eventually made an official
submission on the test set. The evaluation results for the third subtask
confirmed the importance of both modalities, the text and the image. Moreover,
some teams reported benefits when not just combining the two modalities, e.g.,
by using early or late fusion, but rather modeling the interaction between them
in a joint model.
</summary>
    <author>
      <name>Dimitar Dimitrov</name>
    </author>
    <author>
      <name>Bishr Bin Ali</name>
    </author>
    <author>
      <name>Shaden Shaar</name>
    </author>
    <author>
      <name>Firoj Alam</name>
    </author>
    <author>
      <name>Fabrizio Silvestri</name>
    </author>
    <author>
      <name>Hamed Firooz</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Giovanni Da San Martino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">propaganda, disinformation, misinformation, fake news, memes,
  multimodality</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SemEval-2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.09284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.10671v1</id>
    <updated>2021-05-22T09:26:13Z</updated>
    <published>2021-05-22T09:26:13Z</published>
    <title>SOK: Fake News Outbreak 2021: Can We Stop the Viral Spread?</title>
    <summary>  Social Networks' omnipresence and ease of use has revolutionized the
generation and distribution of information in today's world. However, easy
access to information does not equal an increased level of public knowledge.
Unlike traditional media channels, social networks also facilitate faster and
wider spread of disinformation and misinformation. Viral spread of false
information has serious implications on the behaviors, attitudes and beliefs of
the public, and ultimately can seriously endanger the democratic processes.
Limiting false information's negative impact through early detection and
control of extensive spread presents the main challenge facing researchers
today. In this survey paper, we extensively analyze a wide range of different
solutions for the early detection of fake news in the existing literature. More
precisely, we examine Machine Learning (ML) models for the identification and
classification of fake news, online fake news detection competitions,
statistical outputs as well as the advantages and disadvantages of some of the
available data sets. Finally, we evaluate the online web browsing tools
available for detecting and mitigating fake news and present some open research
challenges.
</summary>
    <author>
      <name>Tanveer Khan</name>
    </author>
    <author>
      <name>Antonis Michalas</name>
    </author>
    <author>
      <name>Adnan Akhunzada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.10671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.14005v1</id>
    <updated>2021-05-28T17:30:51Z</updated>
    <published>2021-05-28T17:30:51Z</published>
    <title>Online Hate: Behavioural Dynamics and Relationship with Misinformation</title>
    <summary>  Online debates are often characterised by extreme polarisation and heated
discussions among users. The presence of hate speech online is becoming
increasingly problematic, making necessary the development of appropriate
countermeasures. In this work, we perform hate speech detection on a corpus of
more than one million comments on YouTube videos through a machine learning
model fine-tuned on a large set of hand-annotated data. Our analysis shows that
there is no evidence of the presence of "serial haters", intended as active
users posting exclusively hateful comments. Moreover, coherently with the echo
chamber hypothesis, we find that users skewed towards one of the two categories
of video channels (questionable, reliable) are more prone to use inappropriate,
violent, or hateful language within their opponents community. Interestingly,
users loyal to reliable sources use on average a more toxic language than their
counterpart. Finally, we find that the overall toxicity of the discussion
increases with its length, measured both in terms of number of comments and
time. Our results show that, coherently with Godwin's law, online debates tend
to degenerate towards increasingly toxic exchanges of views.
</summary>
    <author>
      <name>Matteo Cinelli</name>
    </author>
    <author>
      <name>Andra≈æ Pelicon</name>
    </author>
    <author>
      <name>Igor Mozetiƒç</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <author>
      <name>Petra Kralj Novak</name>
    </author>
    <author>
      <name>Fabiana Zollo</name>
    </author>
    <link href="http://arxiv.org/abs/2105.14005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.14376v1</id>
    <updated>2021-05-29T21:22:24Z</updated>
    <published>2021-05-29T21:22:24Z</published>
    <title>Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis</title>
    <summary>  The rapid advances in deep generative models over the past years have led to
highly {realistic media, known as deepfakes,} that are commonly
indistinguishable from real to human eyes. These advances make assessing the
authenticity of visual data increasingly difficult and pose a misinformation
threat to the trustworthiness of visual content in general. Although recent
work has shown strong detection accuracy of such deepfakes, the success largely
relies on identifying frequency artifacts in the generated images, which will
not yield a sustainable detection approach as generative models continue
evolving and closing the gap to real images. In order to overcome this issue,
we propose a novel fake detection that is designed to re-synthesize testing
images and extract visual cues for detection. The re-synthesis procedure is
flexible, allowing us to incorporate a series of visual tasks - we adopt
super-resolution, denoising and colorization as the re-synthesis. We
demonstrate the improved effectiveness, cross-GAN generalization, and
robustness against perturbations of our approach in a variety of detection
scenarios involving multiple generators over CelebA-HQ, FFHQ, and LSUN
datasets. Source code is available at
https://github.com/SSAW14/BeyondtheSpectrum.
</summary>
    <author>
      <name>Yang He</name>
    </author>
    <author>
      <name>Ning Yu</name>
    </author>
    <author>
      <name>Margret Keuper</name>
    </author>
    <author>
      <name>Mario Fritz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IJCAI2021. Source code at
  https://github.com/SSAW14/BeyondtheSpectrum</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.14376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03264v2</id>
    <updated>2018-05-21T10:31:40Z</updated>
    <published>2017-07-11T13:44:51Z</published>
    <title>A simple but tough-to-beat baseline for the Fake News Challenge stance
  detection task</title>
    <summary>  Identifying public misinformation is a complicated and challenging task. An
important part of checking the veracity of a specific claim is to evaluate the
stance different news sources take towards the assertion. Automatic stance
evaluation, i.e. stance detection, would arguably facilitate the process of
fact checking. In this paper, we present our stance detection system which
claimed third place in Stage 1 of the Fake News Challenge. Despite our
straightforward approach, our system performs at a competitive level with the
complex ensembles of the top two winning teams. We therefore propose our system
as the 'simple but tough-to-beat baseline' for the Fake News Challenge stance
detection task.
</summary>
    <author>
      <name>Benjamin Riedel</name>
    </author>
    <author>
      <name>Isabelle Augenstein</name>
    </author>
    <author>
      <name>Georgios P. Spithourakis</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 3 tables; additional reference and details added,
  typos and wording corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.03264v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03264v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09939v1</id>
    <updated>2017-07-31T16:20:31Z</updated>
    <published>2017-07-31T16:20:31Z</published>
    <title>An Analysis of the Twitter Discussion on the 2016 Austrian Presidential
  Elections</title>
    <summary>  In this paper, we provide a systematic analysis of the Twitter discussion on
the 2016 Austrian presidential elections. In particular, we extracted and
analyzed a data-set consisting of 343645 Twitter messages related to the 2016
Austrian presidential elections. Our analysis combines methods from network
science, sentiment analysis, as well as bot detection. Among other things, we
found that: a) the winner of the election (Alexander Van der Bellen) was
considerably more popular and influential on Twitter than his opponent, b) the
Twitter followers of Van der Bellen substantially participated in the spread of
misinformation about him, c) there was a clear polarization in terms of the
sentiments spread by Twitter followers of the two presidential candidates, d)
the in-degree and out-degree distributions of the underlying communication
network are heavy-tailed, and e) compared to other recent events, such as the
2016 Brexit referendum or the 2016 US presidential elections, only a very small
number of bots participated in the Twitter discussion on the 2016 Austrian
presidential election.
</summary>
    <author>
      <name>Ema Ku≈°en</name>
    </author>
    <author>
      <name>Mark Strembeck</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.2; I.2.7; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02233v2</id>
    <updated>2018-04-16T11:53:56Z</updated>
    <published>2018-04-06T12:36:28Z</published>
    <title>Forex trading and Twitter: Spam, bots, and reputation manipulation</title>
    <summary>  Currency trading (Forex) is the largest world market in terms of volume. We
analyze trading and tweeting about the EUR-USD currency pair over a period of
three years. First, a large number of tweets were manually labeled, and a
Twitter stance classification model is constructed. The model then classifies
all the tweets by the trading stance signal: buy, hold, or sell (EUR vs. USD).
The Twitter stance is compared to the actual currency rates by applying the
event study methodology, well-known in financial economics. It turns out that
there are large differences in Twitter stance distribution and potential
trading returns between the four groups of Twitter users: trading robots,
spammers, trading companies, and individual traders. Additionally, we observe
attempts of reputation manipulation by post festum removal of tweets with poor
predictions, and deleting/reposting of identical tweets to increase the
visibility without tainting one's Twitter timeline.
</summary>
    <author>
      <name>Igor Mozetiƒç</name>
    </author>
    <author>
      <name>Peter Gabrov≈°ek</name>
    </author>
    <author>
      <name>Petra Kralj Novak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MIS2: Misinformation and Misbehavior Mining on the Web, Workshop at
  WSDM-18, Marina Del Rey, CA, USA, Feb. 9, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.02233v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02233v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01997v1</id>
    <updated>2018-06-06T03:47:39Z</updated>
    <published>2018-06-06T03:47:39Z</published>
    <title>TrollSpot: Detecting misbehavior in commenting platforms</title>
    <summary>  Commenting platforms, such as Disqus, have emerged as a major online
communication platform with millions of users and posts. Their popularity has
also attracted parasitic and malicious behav- iors, such as trolling and
spamming. There has been relatively little research on modeling and
safeguarding these platforms. As our key contribution, we develop a systematic
approach to detect malicious users on commenting platforms focusing on having:
(a) interpretable, and (b) fine-grained classification of malice. Our work has
two key novelties: (a) we propose two classifications methods, with one
following a two stage approach, which first maps observ- able features to
behaviors and then maps these behaviors to user roles, and (b) we use a
comprehensive set of 73 features that span four dimensions of information. We
use 7 million comments during a 9 month period, and we show that our
classification methods can distinguish between benign, and malicious roles
(spammers, trollers, and fanatics) with a 0.904 AUC. Our work is a solid step
to- wards ensuring that commenting platforms are a safe and pleasant medium for
the exchange of ideas.
</summary>
    <author>
      <name>Tai Ching Li</name>
    </author>
    <author>
      <name>Joobin Gharibshah</name>
    </author>
    <author>
      <name>Evangelos E. Papalexakis</name>
    </author>
    <author>
      <name>Michalis Faloutsos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in WSDM workshop on Misinformation and Misbehavior Mining on
  the Web, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02720v1</id>
    <updated>2018-06-07T15:09:57Z</updated>
    <published>2018-06-07T15:09:57Z</published>
    <title>Anchored in a Data Storm: How Anchoring Bias Can Affect User Strategy,
  Confidence, and Decisions in Visual Analytics</title>
    <summary>  Cognitive biases have been shown to lead to faulty decision-making. Recent
research has demonstrated that the effect of cognitive biases, anchoring bias
in particular, transfers to information visualization and visual analytics.
However, it is still unclear how users of visual interfaces can be anchored and
the impact of anchoring on user performance and decision-making process. To
investigate, we performed two rounds of between-subjects, in-laboratory
experiments with 94 participants to analyze the effect of visual anchors and
strategy cues in decision-making with a visual analytic system that employs
coordinated multiple view design. The decision-making task is identifying
misinformation from Twitter news accounts. Participants were randomly assigned
one of three treatment groups (including control) in which participant training
processes were modified. Our findings reveal that strategy cues and visual
anchors (scenario videos) can significantly affect user activity, speed,
confidence, and, under certain circumstances, accuracy. We discuss the
implications of our experiment results on training users how to use a newly
developed visual interface. We call for more careful consideration into how
visualization designers and researchers train users to avoid unintentionally
anchoring users and thus affecting the end result.
</summary>
    <author>
      <name>Ryan Wesslen</name>
    </author>
    <author>
      <name>Sashank Santhanam</name>
    </author>
    <author>
      <name>Alireza Karduni</name>
    </author>
    <author>
      <name>Isaac Cho</name>
    </author>
    <author>
      <name>Samira Shaikh</name>
    </author>
    <author>
      <name>Wenwen Dou</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07516v2</id>
    <updated>2019-10-09T22:04:46Z</updated>
    <published>2018-06-20T01:26:21Z</published>
    <title>The Rise of Guardians: Fact-checking URL Recommendation to Combat Fake
  News</title>
    <summary>  A large body of research work and efforts have been focused on detecting fake
news and building online fact-check systems in order to debunk fake news as
soon as possible. Despite the existence of these systems, fake news is still
wildly shared by online users. It indicates that these systems may not be fully
utilized. After detecting fake news, what is the next step to stop people from
sharing it? How can we improve the utilization of these fact-check systems? To
fill this gap, in this paper, we (i) collect and analyze online users called
guardians, who correct misinformation and fake news in online discussions by
referring fact-checking URLs; and (ii) propose a novel fact-checking URL
recommendation model to encourage the guardians to engage more in fact-checking
activities. We found that the guardians usually took less than one day to reply
to claims in online conversations and took another day to spread verified
information to hundreds of millions of followers. Our proposed recommendation
model outperformed four state-of-the-art models by 11%~33%. Our source code and
dataset are available at https://github.com/nguyenvo09/CombatingFakeNews.
</summary>
    <author>
      <name>Nguyen Vo</name>
    </author>
    <author>
      <name>Kyumin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07516v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07516v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07687v2</id>
    <updated>2018-09-05T12:47:39Z</updated>
    <published>2018-06-20T12:13:53Z</published>
    <title>Automated Fact Checking: Task formulations, methods and future
  directions</title>
    <summary>  The recently increased focus on misinformation has stimulated research in
fact checking, the task of assessing the truthfulness of a claim. Research in
automating this task has been conducted in a variety of disciplines including
natural language processing, machine learning, knowledge representation,
databases, and journalism. While there has been substantial progress, relevant
papers and articles have been published in research communities that are often
unaware of each other and use inconsistent terminology, thus impeding
understanding and further progress. In this paper we survey automated fact
checking research stemming from natural language processing and related
disciplines, unifying the task formulations and methodologies across papers and
authors. Furthermore, we highlight the use of evidence as an important
distinguishing factor among them cutting across task formulations and methods.
We conclude with proposing avenues for future NLP research on automated fact
checking.
</summary>
    <author>
      <name>James Thorne</name>
    </author>
    <author>
      <name>Andreas Vlachos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at the 27th International Conference on Computational
  Linguistics (COLING 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07687v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07687v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09541v1</id>
    <updated>2018-06-06T10:47:20Z</updated>
    <published>2018-06-06T10:47:20Z</published>
    <title>Technology, Propaganda, and the Limits of Human Intellect</title>
    <summary>  "Fake news" is a recent phenomenon, but misinformation and propaganda are
not. Our new communication technologies make it easy for us to be exposed to
high volumes of true, false, irrelevant, and unprovable information. Future AI
is expected to amplify the problem even more. At the same time, our brains are
reaching their limits in handling information. How should we respond to
propaganda? Technology can help, but relying on it alone will not suffice in
the long term. We also need ethical policies, laws, regulations, and trusted
authorities, including fact-checkers. However, we will not solve the problem
without the active engagement of the educated citizen. Epistemological
education, recognition of self biases and protection of our channels of
communication and trusted networks are all needed to overcome the problem and
continue our progress as democratic societies.
</summary>
    <author>
      <name>Panagiotis Metaxas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06809v1</id>
    <updated>2018-08-21T09:08:56Z</updated>
    <published>2018-08-21T09:08:56Z</published>
    <title>Are You Tampering With My Data?</title>
    <summary>  We propose a novel approach towards adversarial attacks on neural networks
(NN), focusing on tampering the data used for training instead of generating
attacks on trained models. Our network-agnostic method creates a backdoor
during training which can be exploited at test time to force a neural network
to exhibit abnormal behaviour. We demonstrate on two widely used datasets
(CIFAR-10 and SVHN) that a universal modification of just one pixel per image
for all the images of a class in the training set is enough to corrupt the
training procedure of several state-of-the-art deep neural networks causing the
networks to misclassify any images to which the modification is applied. Our
aim is to bring to the attention of the machine learning community, the
possibility that even learning-based methods that are personally trained on
public datasets can be subject to attacks by a skillful adversary.
</summary>
    <author>
      <name>Michele Alberti</name>
    </author>
    <author>
      <name>Vinaychandran Pondenkandath</name>
    </author>
    <author>
      <name>Marcel W√ºrsch</name>
    </author>
    <author>
      <name>Manuel Bouillon</name>
    </author>
    <author>
      <name>Mathias Seuret</name>
    </author>
    <author>
      <name>Rolf Ingold</name>
    </author>
    <author>
      <name>Marcus Liwicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Conference on Computer Vision (ECCV 2018), Workshop on
  Objectionable Content and Misinformation</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.06809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08524v2</id>
    <updated>2020-01-21T17:41:41Z</updated>
    <published>2018-08-26T09:46:47Z</published>
    <title>When facts fail: Bias, polarisation and truth in social networks</title>
    <summary>  Online social networks provide users with unprecedented opportunities to
engage with diverse opinions. At the same time, they enable confirmation bias
on large scales by empowering individuals to self-select narratives they want
to be exposed to. A precise understanding of such tradeoffs is still largely
missing. We introduce a social learning model where most participants in a
network update their beliefs unbiasedly based on new information, while a
minority of participants reject information that is incongruent with their
preexisting beliefs. This simple mechanism generates permanent opinion
polarization and cascade dynamics, and accounts for the aforementioned tradeoff
between confirmation bias and social connectivity through analytic results. We
investigate the model's predictions empirically using US county-level data on
the impact of Internet access on the formation of beliefs about global warming.
We conclude by discussing policy implications of our model, highlighting the
downsides of debunking and suggesting alternative strategies to contrast
misinformation.
</summary>
    <author>
      <name>Orowa Sikder</name>
    </author>
    <author>
      <name>Robert E. Smith</name>
    </author>
    <author>
      <name>Pierpaolo Vivo</name>
    </author>
    <author>
      <name>Giacomo Livan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08524v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08524v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.06973v2</id>
    <updated>2018-10-17T09:09:06Z</updated>
    <published>2018-10-16T13:18:22Z</published>
    <title>Opinion Dynamics via Search Engines (and other Algorithmic Gatekeepers)</title>
    <summary>  Ranking algorithms are the information gatekeepers of the Internet era. We
develop a stylized model to study the effects of ranking algorithms on opinion
dynamics. We consider a search engine that uses an algorithm based on
popularity and on personalization. We find that popularity-based rankings
generate an advantage of the fewer effect: fewer websites reporting a given
signal attract relatively more traffic overall. This highlights a novel,
ranking-driven channel that explains the diffusion of misinformation, as
websites reporting incorrect information may attract an amplified amount of
traffic precisely because they are few. Furthermore, when individuals provide
sufficiently positive feedback to the ranking algorithm, popularity-based
rankings tend to aggregate information while personalization acts in the
opposite direction.
</summary>
    <author>
      <name>Fabrizio Germano</name>
    </author>
    <author>
      <name>Francesco Sobbrio</name>
    </author>
    <link href="http://arxiv.org/abs/1810.06973v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.06973v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.00582v3</id>
    <updated>2019-05-16T06:56:55Z</updated>
    <published>2019-05-02T06:06:25Z</published>
    <title>Recurrent Convolutional Strategies for Face Manipulation Detection in
  Videos</title>
    <summary>  The spread of misinformation through synthetically generated yet realistic
images and videos has become a significant problem, calling for robust
manipulation detection methods. Despite the predominant effort of detecting
face manipulation in still images, less attention has been paid to the
identification of tampered faces in videos by taking advantage of the temporal
information present in the stream. Recurrent convolutional models are a class
of deep learning models which have proven effective at exploiting the temporal
information from image streams across domains. We thereby distill the best
strategy for combining variations in these models along with domain specific
face preprocessing techniques through extensive experimentation to obtain
state-of-the-art performance on publicly available video-based facial
manipulation benchmarks. Specifically, we attempt to detect Deepfake, Face2Face
and FaceSwap tampered faces in video streams. Evaluation is performed on the
recently introduced FaceForensics++ dataset, improving the previous
state-of-the-art by up to 4.55% in accuracy.
</summary>
    <author>
      <name>Ekraam Sabir</name>
    </author>
    <author>
      <name>Jiaxin Cheng</name>
    </author>
    <author>
      <name>Ayush Jaiswal</name>
    </author>
    <author>
      <name>Wael AbdAlmageed</name>
    </author>
    <author>
      <name>Iacopo Masi</name>
    </author>
    <author>
      <name>Prem Natarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at Workshop on Applications of Computer Vision and Pattern
  Recognition to Media Forensics at CVPR 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.00582v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00582v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.12325v2</id>
    <updated>2020-02-27T16:05:21Z</updated>
    <published>2019-06-28T17:27:35Z</published>
    <title>Modeling echo chambers and polarization dynamics in social networks</title>
    <summary>  Echo chambers and opinion polarization recently quantified in several
sociopolitical contexts and across different social media, raise concerns on
their potential impact on the spread of misinformation and on openness of
debates. Despite increasing efforts, the dynamics leading to the emergence of
these phenomena stay unclear. We propose a model that introduces the dynamics
of radicalization, as a reinforcing mechanism driving the evolution to extreme
opinions from moderate initial conditions. Inspired by empirical findings on
social interaction dynamics, we consider agents characterized by heterogeneous
activities and homophily. We show that the transition between a global
consensus and emerging radicalized states is mostly governed by social
influence and by the controversialness of the topic discussed. Compared with
empirical data of polarized debates on Twitter, the model qualitatively
reproduces the observed relation between users' engagement and opinions, as
well as opinion segregation in the interaction network. Our findings shed light
on the mechanisms that may lie at the core of the emergence of echo chambers
and polarization in social media.
</summary>
    <author>
      <name>Fabian Baumann</name>
    </author>
    <author>
      <name>Philipp Lorenz-Spreen</name>
    </author>
    <author>
      <name>Igor M. Sokolov</name>
    </author>
    <author>
      <name>Michele Starnini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.124.048301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.124.048301" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 124, 048301 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.12325v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.12325v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.01458v1</id>
    <updated>2019-09-19T21:48:03Z</updated>
    <published>2019-09-19T21:48:03Z</published>
    <title>Attention Based Neural Architecture for Rumor Detection with Author
  Context Awareness</title>
    <summary>  The prevalence of social media has made information sharing possible across
the globe. The downside, unfortunately, is the wide spread of misinformation.
Methods applied in most previous rumor classifiers give an equal weight, or
attention, to words in the microblog, and do not take the context beyond
microblog contents into account; therefore, the accuracy becomes plateaued. In
this research, we propose an ensemble neural architecture to detect rumor on
Twitter. The architecture incorporates word attention and context from the
author to enhance the classification performance. In particular, the word-level
attention mechanism enables the architecture to put more emphasis on important
words when constructing the text representation. To derive further context,
microblog posts composed by individual authors are exploited since they can
reflect style and characteristics in spreading information, which are
significant cues to help classify whether the shared content is rumor or
legitimate news. The experiment on the real-world Twitter dataset collected
from two well-known rumor tracking websites demonstrates promising results.
</summary>
    <author>
      <name>Sansiri Tarnpradab</name>
    </author>
    <author>
      <name>Kien A. Hua</name>
    </author>
    <link href="http://arxiv.org/abs/1910.01458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02103v1</id>
    <updated>2019-10-04T18:45:09Z</updated>
    <published>2019-10-04T18:45:09Z</published>
    <title>Health Wars and Beyond: The Rapidly Expanding and Efficient Network
  Insurgency Interlinking Local and Global Online Crowds of Distrust</title>
    <summary>  We present preliminary results on the online war surrounding distrust of
expertise in medical science -- specifically, the issue of vaccinations. While
distrust and misinformation in politics can damage democratic elections, in the
medical context it may also endanger lives through missed vaccinations and DIY
cancer cures. We find that this online health war has evolved into a highly
efficient network insurgency with direct inter-crowd links across countries,
continents and cultures. The online anti-vax crowds (referred to as Red) now
appear better positioned to groom new recruits (Green) than those supporting
established expertise (Blue). We also present preliminary results from a
mathematically-grounded, crowd-based analysis of the war's evolution, which
offers an explanation for how Red seems to be turning the tide on Blue.
</summary>
    <author>
      <name>N. F. Johnson</name>
    </author>
    <author>
      <name>N. Velasquez</name>
    </author>
    <author>
      <name>N. Johnson Restrepo</name>
    </author>
    <author>
      <name>R. Leahy</name>
    </author>
    <author>
      <name>N. Gabriel</name>
    </author>
    <author>
      <name>S. Wuchty</name>
    </author>
    <author>
      <name>D. Broniatowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper. Comments welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00643v1</id>
    <updated>2019-11-02T04:06:30Z</updated>
    <published>2019-11-02T04:06:30Z</published>
    <title>Credibility-based Fake News Detection</title>
    <summary>  Fake news can significantly misinform people who often rely on online sources
and social media for their information. Current research on fake news detection
has mostly focused on analyzing fake news content and how it propagates on a
network of users. In this paper, we emphasize the detection of fake news by
assessing its credibility. By analyzing public fake news data, we show that
information on news sources (and authors) can be a strong indicator of
credibility. Our findings suggest that an author's history of association with
fake news, and the number of authors of a news article, can play a significant
role in detecting fake news. Our approach can help improve traditional fake
news detection methods, wherein content features are often used to detect fake
news.
</summary>
    <author>
      <name>Niraj Sitaula</name>
    </author>
    <author>
      <name>Chilukuri K. Mohan</name>
    </author>
    <author>
      <name>Jennifer Grygiel</name>
    </author>
    <author>
      <name>Xinyi Zhou</name>
    </author>
    <author>
      <name>Reza Zafarani</name>
    </author>
    <link href="http://arxiv.org/abs/1911.00643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.00673v2</id>
    <updated>2020-04-21T20:50:30Z</updated>
    <published>2020-04-01T19:32:25Z</published>
    <title>Hate multiverse spreads malicious COVID-19 content online beyond
  individual platform control</title>
    <summary>  We show that malicious COVID-19 content, including hate speech,
disinformation, and misinformation, exploits the multiverse of online hate to
spread quickly beyond the control of any individual social media platform.
Machine learning topic analysis shows quantitatively how online hate
communities are weaponizing COVID-19, with topics evolving rapidly and content
becoming increasingly coherent. Our mathematical analysis provides a
generalized form of the public health R0 predicting the tipping point for
multiverse-wide viral spreading, which suggests new policy options to mitigate
the global spread of malicious COVID-19 content without relying on future
coordination between all online platforms.
</summary>
    <author>
      <name>N. Vel√°squez</name>
    </author>
    <author>
      <name>R. Leahy</name>
    </author>
    <author>
      <name>N. Johnson Restrepo</name>
    </author>
    <author>
      <name>Y. Lupu</name>
    </author>
    <author>
      <name>R. Sear</name>
    </author>
    <author>
      <name>N. Gabriel</name>
    </author>
    <author>
      <name>O. Jha</name>
    </author>
    <author>
      <name>B. Goldberg</name>
    </author>
    <author>
      <name>N. F. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper. Feedback welcomed from the community to
  neiljohnson@gwu.edu</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.00673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.00673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.00777v1</id>
    <updated>2020-04-02T02:22:45Z</updated>
    <published>2020-04-02T02:22:45Z</published>
    <title>Skepticism and rumor spreading: the role of spatial correlations</title>
    <summary>  Critical thinking and skepticism are fundamental mechanisms that one may use
to prevent the spreading of rumors, fake-news and misinformation. We consider a
simple model in which agents without previous contact with the rumor, being
skeptically oriented, may convince spreaders to stop their activity or, once
exposed to the rumor, decide not to propagate it as a consequence, for example,
of fact-checking. We extend a previous, mean-field analysis of the combined
effect of these two mechanisms, active and passive skepticism, to include
spatial correlations. This can be done either analytically, through the pair
approximation, or simulating an agent-based version on diverse networks. Our
results show that while in mean-field there is no coexistence between spreaders
and susceptibles (although, depending on the parameters, there may be
bistability depending on the initial conditions), when spatial correlations are
included, because of the protective effect of the isolation provided by removed
agents, coexistence is possible.
</summary>
    <author>
      <name>Marco Antonio Amaral</name>
    </author>
    <author>
      <name>W. G. Dantas</name>
    </author>
    <author>
      <name>Jeferson J. Arenzon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.101.062418</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.101.062418" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 101, 062418 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.00777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.00777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.01967v1</id>
    <updated>2020-04-04T16:21:13Z</updated>
    <published>2020-04-04T16:21:13Z</published>
    <title>The Paradox of Information Access: Growing Isolation in the Age of
  Sharing</title>
    <summary>  Modern online media, such as Twitter, Instagram, and YouTube, enable anyone
to become an information producer and to offer online content for potentially
global consumption. By increasing the amount of globally accessible real-time
information, today's ubiquitous producers contribute to a world, where an
individual consumes vanishingly smaller fractions of all produced content. In
general, consumers preferentially select information that closely matches their
individual views and values. The bias inherent in such selection is further
magnified by today's information curation services that maximize user
engagement (and thus service revenue) by filtering new content in accordance
with observed consumer preferences. Consequently, individuals get exposed to
increasingly narrower bands of the ideology spectrum. Societies get fragmented
into increasingly ideologically isolated enclaves. These enclaves (or
echo-chambers) then become vulnerable to misinformation spread, which in turn
further magnifies polarization and bias. We call this dynamic the paradox of
information access; a growing ideological fragmentation in the age of sharing.
This article describes the technical, economic, and socio-cognitive
contributors to this paradox, and explores research directions towards its
mitigation.
</summary>
    <author>
      <name>Tarek Abdelzaher</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <author>
      <name>Jinyang Li</name>
    </author>
    <author>
      <name>Chaoqi Yang</name>
    </author>
    <author>
      <name>John Dellaverson</name>
    </author>
    <author>
      <name>Lixia Zhang</name>
    </author>
    <author>
      <name>Chao Xu</name>
    </author>
    <author>
      <name>Boleslaw K. Szymanski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.03788v1</id>
    <updated>2020-04-08T03:22:21Z</updated>
    <published>2020-04-08T03:22:21Z</published>
    <title>Satirical News Detection with Semantic Feature Extraction and
  Game-theoretic Rough Sets</title>
    <summary>  Satirical news detection is an important yet challenging task to prevent
spread of misinformation. Many feature based and end-to-end neural nets based
satirical news detection systems have been proposed and delivered promising
results. Existing approaches explore comprehensive word features from satirical
news articles, but lack semantic metrics using word vectors for tweet form
satirical news. Moreover, the vagueness of satire and news parody determines
that a news tweet can hardly be classified with a binary decision, that is,
satirical or legitimate. To address these issues, we collect satirical and
legitimate news tweets, and propose a semantic feature based approach. Features
are extracted by exploring inconsistencies in phrases, entities, and between
main and relative clauses. We apply game-theoretic rough set model to detect
satirical news, in which probabilistic thresholds are derived by game
equilibrium and repetition learning mechanism. Experimental results on the
collected dataset show the robustness and improvement of the proposed approach
compared with Pawlak rough set model and SVM.
</summary>
    <author>
      <name>Yue Zhou</name>
    </author>
    <author>
      <name>Yan Zhang</name>
    </author>
    <author>
      <name>JingTao Yao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-59491-6_12</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-59491-6_12" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.03788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.04315v2</id>
    <updated>2020-04-22T22:38:15Z</updated>
    <published>2020-04-09T01:07:12Z</published>
    <title>Large Arabic Twitter Dataset on COVID-19</title>
    <summary>  The 2019 coronavirus disease (COVID-19), emerged late December 2019 in China,
is now rapidly spreading across the globe. At the time of writing this paper,
the number of global confirmed cases has passed two millions and half with over
180,000 fatalities. Many countries have enforced strict social distancing
policies to contain the spread of the virus. This have changed the daily life
of tens of millions of people, and urged people to turn their discussions
online, e.g., via online social media sites like Twitter. In this work, we
describe the first Arabic tweets dataset on COVID-19 that we have been
collecting since January 1st, 2020. The dataset would help researchers and
policy makers in studying different societal issues related to the pandemic.
Many other tasks related to behavioral change, information sharing,
misinformation and rumors spreading can also be analyzed.
</summary>
    <author>
      <name>Sarah Alqurashi</name>
    </author>
    <author>
      <name>Ahmad Alhindi</name>
    </author>
    <author>
      <name>Eisa Alanazi</name>
    </author>
    <link href="http://arxiv.org/abs/2004.04315v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04315v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.05113v1</id>
    <updated>2020-04-07T02:57:35Z</updated>
    <published>2020-04-07T02:57:35Z</published>
    <title>Automatically Assessing Quality of Online Health Articles</title>
    <summary>  The information ecosystem today is overwhelmed by an unprecedented quantity
of data on versatile topics are with varied quality. However, the quality of
information disseminated in the field of medicine has been questioned as the
negative health consequences of health misinformation can be life-threatening.
There is currently no generic automated tool for evaluating the quality of
online health information spanned over a broad range. To address this gap, in
this paper, we applied a data mining approach to automatically assess the
quality of online health articles based on 10 quality criteria. We have
prepared a labeled dataset with 53012 features and applied different feature
selection methods to identify the best feature subset with which our trained
classifier achieved an accuracy of 84%-90% varied over 10 criteria. Our
semantic analysis of features shows the underpinning associations between the
selected features &amp; assessment criteria and further rationalize our assessment
approach. Our findings will help in identifying high-quality health articles
and thus aiding users in shaping their opinion to make the right choice while
picking health-related help from online.
</summary>
    <author>
      <name>Fariha Afsana</name>
    </author>
    <author>
      <name>Muhammad Ashad Kabir</name>
    </author>
    <author>
      <name>Naeemul Hassan</name>
    </author>
    <author>
      <name>Manoranjan Paul</name>
    </author>
    <link href="http://arxiv.org/abs/2004.05113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11138v3</id>
    <updated>2020-09-13T22:44:33Z</updated>
    <published>2020-04-23T13:35:49Z</published>
    <title>The Creation and Detection of Deepfakes: A Survey</title>
    <summary>  Generative deep learning algorithms have progressed to a point where it is
difficult to tell the difference between what is real and what is fake. In
2018, it was discovered how easy it is to use this technology for unethical and
malicious applications, such as the spread of misinformation, impersonation of
political leaders, and the defamation of innocent individuals. Since then,
these `deepfakes' have advanced significantly.
  In this paper, we explore the creation and detection of deepfakes and provide
an in-depth view of how these architectures work. The purpose of this survey is
to provide the reader with a deeper understanding of (1) how deepfakes are
created and detected, (2) the current trends and advancements in this domain,
(3) the shortcomings of the current defense solutions, and (4) the areas which
require further research and attention.
</summary>
    <author>
      <name>Yisroel Mirsky</name>
    </author>
    <author>
      <name>Wenke Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3425780</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3425780" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Computing Surveys (CSUR), 2020, preprint</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.11138v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11138v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.12226v1</id>
    <updated>2020-04-25T20:07:54Z</updated>
    <published>2020-04-25T20:07:54Z</published>
    <title>A First Instagram Dataset on COVID-19</title>
    <summary>  The novel coronavirus (COVID-19) pandemic outbreak is drastically shaping and
reshaping many aspects of our life, with a huge impact on our social life. In
this era of lockdown policies in most of the major cities around the world, we
see a huge increase in people and professional engagement in social media.
Social media is playing an important role in news propagation as well as
keeping people in contact. At the same time, this source is both a blessing and
a curse as the coronavirus infodemic has become a major concern, and is already
a topic that needs special attention and further research. In this paper, we
provide a multilingual coronavirus (COVID-19) Instagram dataset that we have
been continuously collected since March 30, 2020. We are making our dataset
available to the research community at Github. We believe that this
contribution will help the community to better understand the dynamics behind
this phenomenon in Instagram, as one of the major social media. This dataset
could also help study the propagation of misinformation related to this
outbreak.
</summary>
    <author>
      <name>Koosha Zarei</name>
    </author>
    <author>
      <name>Reza Farahbakhsh</name>
    </author>
    <author>
      <name>Noel Crespi</name>
    </author>
    <author>
      <name>Gareth Tyson</name>
    </author>
    <link href="http://arxiv.org/abs/2004.12226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.12864v1</id>
    <updated>2020-04-27T15:18:49Z</updated>
    <published>2020-04-27T15:18:49Z</published>
    <title>DeSePtion: Dual Sequence Prediction and Adversarial Examples for
  Improved Fact-Checking</title>
    <summary>  The increased focus on misinformation has spurred development of data and
systems for detecting the veracity of a claim as well as retrieving
authoritative evidence. The Fact Extraction and VERification (FEVER) dataset
provides such a resource for evaluating end-to-end fact-checking, requiring
retrieval of evidence from Wikipedia to validate a veracity prediction. We show
that current systems for FEVER are vulnerable to three categories of realistic
challenges for fact-checking -- multiple propositions, temporal reasoning, and
ambiguity and lexical variation -- and introduce a resource with these types of
claims. Then we present a system designed to be resilient to these "attacks"
using multiple pointer networks for document selection and jointly modeling a
sequence of evidence sentences and veracity relation predictions. We find that
in handling these attacks we obtain state-of-the-art results on FEVER, largely
due to improved evidence retrieval.
</summary>
    <author>
      <name>Christopher Hidey</name>
    </author>
    <author>
      <name>Tuhin Chakrabarty</name>
    </author>
    <author>
      <name>Tariq Alhindi</name>
    </author>
    <author>
      <name>Siddharth Varia</name>
    </author>
    <author>
      <name>Kriste Krstovski</name>
    </author>
    <author>
      <name>Mona Diab</name>
    </author>
    <author>
      <name>Smaranda Muresan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.12864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.14484v2</id>
    <updated>2020-06-08T06:18:40Z</updated>
    <published>2020-04-29T21:08:44Z</published>
    <title>Prevalence of Low-Credibility Information on Twitter During the COVID-19
  Outbreak</title>
    <summary>  As the novel coronavirus spreads across the world, concerns regarding the
spreading of misinformation about it are also growing. Here we estimate the
prevalence of links to low-credibility information on Twitter during the
outbreak, and the role of bots in spreading these links. We find that the
combined volume of tweets linking to low-credibility information is comparable
to the volume of New York Times articles and CDC links. Content analysis
reveals a politicization of the pandemic. The majority of this content spreads
via retweets. Social bots are involved in both posting and amplifying
low-credibility information, although the majority of volume is generated by
likely humans. Some of these accounts appear to amplify low-credibility sources
in a coordinated fashion.
</summary>
    <author>
      <name>Kai-Cheng Yang</name>
    </author>
    <author>
      <name>Christopher Torres-Lugo</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.36190/2020.16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.36190/2020.16" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ICWSM Intl. Workshop on Cyber Social Threats (CySoc), 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.14484v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.14484v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.04278v1</id>
    <updated>2020-06-07T22:00:43Z</updated>
    <published>2020-06-07T22:00:43Z</published>
    <title>Disinformation and Misinformation on Twitter during the Novel
  Coronavirus Outbreak</title>
    <summary>  As the novel coronavirus spread globally, a growing public panic was
expressed over the internet. We examine the public discussion concerning
COVID-19 on Twitter. We use a dataset of 67 million tweets from 12 million
users collected between January 29, 2020 and March 4, 2020. We categorize users
based on their home countries, social identities, and political orientation. We
find that news media, government officials, and individual news reporters
posted a majority of influential tweets, while the most influential ones are
still written by regular users. Tweets mentioning "fake news" URLs and
disinformation story-lines are also more likely to be spread by regular users.
Unlike real news and normal tweets, tweets containing URLs pointing to "fake
news" sites are most likely to be retweeted within the source country and so
are less likely to spread internationally.
</summary>
    <author>
      <name>Binxuan Huang</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <link href="http://arxiv.org/abs/2006.04278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.01273v2</id>
    <updated>2021-02-15T22:03:49Z</updated>
    <published>2020-08-04T02:04:17Z</published>
    <title>Analyzing Twitter Users' Behavior Before and After Contact by the
  Internet Research Agency</title>
    <summary>  Social media platforms have been exploited to conduct election interference
in recent years. In particular, the Russian-backed Internet Research Agency
(IRA) has been identified as a key source of misinformation spread on Twitter
prior to the 2016 U.S. presidential election. The goal of this research is to
understand whether general Twitter users changed their behavior in the year
following first contact from an IRA account. We compare the before and after
behavior of contacted users to determine whether there were differences in
their mean tweet count, the sentiment of their tweets, and the frequency and
sentiment of tweets mentioning @realDonaldTrump or @HillaryClinton. Our results
indicate that users overall exhibited statistically significant changes in
behavior across most of these metrics, and that those users that engaged with
the IRA generally showed greater changes in behavior.
</summary>
    <author>
      <name>Upasana Dutta</name>
    </author>
    <author>
      <name>Rhett Hanscom</name>
    </author>
    <author>
      <name>Jason Shuo Zhang</name>
    </author>
    <author>
      <name>Richard Han</name>
    </author>
    <author>
      <name>Tamara Lehman</name>
    </author>
    <author>
      <name>Qin Lv</name>
    </author>
    <author>
      <name>Shivakant Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449164</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449164" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CSCW 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.01273v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.01273v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.00088v2</id>
    <updated>2021-05-03T15:42:38Z</updated>
    <published>2021-03-31T20:09:09Z</published>
    <title>Transfer Learning for Node Regression Applied to Spreading Prediction</title>
    <summary>  Understanding how information propagates in real-life complex networks yields
a better understanding of dynamic processes such as misinformation or epidemic
spreading. The recently introduced branch of machine learning methods for
learning node representations offers many novel applications, one of them being
the task of spreading prediction addressed in this paper. We explore the
utility of the state-of-the-art node representation learners when used to
assess the effects of spreading from a given node, estimated via extensive
simulations. Further, as many real-life networks are topologically similar, we
systematically investigate whether the learned models generalize to previously
unseen networks, showing that in some cases very good model transfer can be
obtained. This work is one of the first to explore transferability of the
learned representations for the task of node regression; we show there exist
pairs of networks with similar structure between which the trained models can
be transferred (zero-shot), and demonstrate their competitive performance. To
our knowledge, this is one of the first attempts to evaluate the utility of
zero-shot transfer for the task of node regression.
</summary>
    <author>
      <name>Sebastian Me≈ænar</name>
    </author>
    <author>
      <name>Nada Lavraƒç</name>
    </author>
    <author>
      <name>Bla≈æ ≈†krlj</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.25088/ComplexSystems.30.4.457</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.25088/ComplexSystems.30.4.457" rel="related"/>
    <link href="http://arxiv.org/abs/2104.00088v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00088v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01140v2</id>
    <updated>2023-03-30T21:39:24Z</updated>
    <published>2021-04-02T16:39:23Z</published>
    <title>Ideology-driven polarisation in online ratings: the review bombing of
  The Last of Us Part II</title>
    <summary>  A review bomb is a large and quick surge in online reviews about a product,
service, or business, coordinated by a group of people willing to manipulate
public opinion about that entity. This study challenges the assumption that
review bombing is solely a phenomenon of misinformation and connects
motivations and substantial content of online reviews with the broader theory
of judgement of facts and of value. These theories are verified in a
quantitative analysis of the most prominent case of review bombing, which
involves the video game The Last of Us Part II. It is discovered that
ideology-driven ratings are followed by a grassroots counter-bombing, aimed at
mitigating the effects of the negative ratings. The two factions of bombers,
despite being politically polar opposites, are very similar in terms of other
metrics. Evidence suggests the theoretical framework of political
disinformation is insufficient to explain this case of review bombing. In light
of the need to re-frame review bombing, recommendations are proposed for the
preventive management of future cases.
</summary>
    <author>
      <name>Giulio Giacomo Cantone</name>
    </author>
    <author>
      <name>Venera Tomaselli</name>
    </author>
    <author>
      <name>Valeria Mazzeo</name>
    </author>
    <link href="http://arxiv.org/abs/2104.01140v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01140v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04389v1</id>
    <updated>2021-04-09T14:22:52Z</updated>
    <published>2021-04-09T14:22:52Z</published>
    <title>A Few Observations About State-Centric Online Propaganda</title>
    <summary>  This paper presents a few observations about pro-Kremlin propaganda between
2015 and early 2021 with a dataset from the East Stratcom Task Force (ESTF),
which is affiliated with the European Union (EU) but working independently from
it. Instead of focusing on misinformation and disinformation, the observations
are motivated by classical propaganda research and the ongoing transformation
of media systems. According to the tentative results, (i) the propaganda can be
assumed to target both domestic and foreign audiences. Of the countries and
regions discussed, (ii) Russia, Ukraine, the United States, and within Europe,
Germany, Poland, and the EU have been the most frequently discussed. Also other
conflict regions such as Syria have often appeared in the propaganda. In terms
of longitudinal trends, however, (iii) most of these discussions have decreased
in volume after the digital tsunami in 2016, although the conflict in Ukraine
seems to have again increased the intensity of pro-Kremlin propaganda. Finally,
(iv) the themes discussed align with state-centric war propaganda and conflict
zones, although also post-truth themes frequently appear; from conspiracy
theories via COVID-19 to fascism -- anything goes, as is typical to propaganda.
</summary>
    <author>
      <name>Jukka Ruohonen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05866v1</id>
    <updated>2021-04-12T23:46:54Z</updated>
    <published>2021-04-12T23:46:54Z</published>
    <title>On Representation Learning for Scientific News Articles Using
  Heterogeneous Knowledge Graphs</title>
    <summary>  In the era of misinformation and information inflation, the credibility
assessment of the produced news is of the essence. However, fact-checking can
be challenging considering the limited references presented in the news. This
challenge can be transcended by utilizing the knowledge graph that is related
to the news articles. In this work, we present a methodology for creating
scientific news article representations by modeling the directed graph between
the scientific news articles and the cited scientific publications. The network
used for the experiments is comprised of the scientific news articles, their
topic, the cited research literature, and their corresponding authors. We
implement and present three different approaches: 1) a baseline Relational
Graph Convolutional Network (R-GCN), 2) a Heterogeneous Graph Neural Network
(HetGNN) and 3) a Heterogeneous Graph Transformer (HGT). We test these models
in the downstream task of link prediction on the: a) news article - paper links
and b) news article - article topic links. The results show promising
applications of graph neural network approaches in the domains of knowledge
tracing and scientific news credibility assessment.
</summary>
    <author>
      <name>Angelika Romanou</name>
    </author>
    <author>
      <name>Panayiotis Smeros</name>
    </author>
    <author>
      <name>Karl Aberer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3442442.3451362</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3442442.3451362" rel="related"/>
    <link href="http://arxiv.org/abs/2104.05866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05893v2</id>
    <updated>2021-09-21T20:38:45Z</updated>
    <published>2021-04-13T01:53:26Z</published>
    <title>NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media</title>
    <summary>  Online misinformation is a prevalent societal issue, with adversaries relying
on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated
by the threat scenario where an image is used out of context to support a
certain narrative. While some prior datasets for detecting image-text
inconsistency generate samples via text manipulation, we propose a dataset
where both image and text are unmanipulated but mismatched. We introduce
several strategies for automatically retrieving convincing images for a given
caption, capturing cases with inconsistent entities or semantic context. Our
large-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates
that machine-driven image repurposing is now a realistic threat, and (2)
provides samples that represent challenging instances of mismatch between text
and image in news that are able to mislead humans. We benchmark several
state-of-the-art multimodal models on our dataset and analyze their performance
across different pretraining domains and visual backbones.
</summary>
    <author>
      <name>Grace Luo</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Anna Rohrbach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.05893v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05893v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11639v2</id>
    <updated>2021-05-01T18:22:39Z</updated>
    <published>2021-04-23T14:45:31Z</published>
    <title>Claim Detection in Biomedical Twitter Posts</title>
    <summary>  Social media contains unfiltered and unique information, which is potentially
of great value, but, in the case of misinformation, can also do great harm.
With regards to biomedical topics, false information can be particularly
dangerous. Methods of automatic fact-checking and fake news detection address
this problem, but have not been applied to the biomedical domain in social
media yet. We aim to fill this research gap and annotate a corpus of 1200
tweets for implicit and explicit biomedical claims (the latter also with span
annotations for the claim phrase). With this corpus, which we sample to be
related to COVID-19, measles, cystic fibrosis, and depression, we develop
baseline models which detect tweets that contain a claim automatically. Our
analyses reveal that biomedical tweets are densely populated with claims (45 %
in a corpus sampled to contain 1200 tweets focused on the domains mentioned
above). Baseline classification experiments with embedding-based classifiers
and BERT-based transfer learning demonstrate that the detection is challenging,
however, shows acceptable performance for the identification of explicit
expressions of claims. Implicit claim tweets are more challenging to detect.
</summary>
    <author>
      <name>Amelie W√ºhrl</name>
    </author>
    <author>
      <name>Roman Klinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the BioNLP Workshop at NAACL 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.11639v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11639v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11729v1</id>
    <updated>2021-04-23T17:25:38Z</updated>
    <published>2021-04-23T17:25:38Z</published>
    <title>Evaluating Deception Detection Model Robustness To Linguistic Variation</title>
    <summary>  With the increasing use of machine-learning driven algorithmic judgements, it
is critical to develop models that are robust to evolving or manipulated
inputs. We propose an extensive analysis of model robustness against linguistic
variation in the setting of deceptive news detection, an important task in the
context of misinformation spread online. We consider two prediction tasks and
compare three state-of-the-art embeddings to highlight consistent trends in
model performance, high confidence misclassifications, and high impact
failures. By measuring the effectiveness of adversarial defense strategies and
evaluating model susceptibility to adversarial attacks using character- and
word-perturbed text, we find that character or mixed ensemble models are the
most effective defenses and that character perturbation-based attack tactics
are more successful.
</summary>
    <author>
      <name>Maria Glenski</name>
    </author>
    <author>
      <name>Ellyn Ayton</name>
    </author>
    <author>
      <name>Robin Cosbey</name>
    </author>
    <author>
      <name>Dustin Arendt</name>
    </author>
    <author>
      <name>Svitlana Volkova</name>
    </author>
    <link href="http://arxiv.org/abs/2104.11729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12069v2</id>
    <updated>2022-06-22T17:11:40Z</updated>
    <published>2021-04-25T05:56:57Z</published>
    <title>Making Generated Images Hard To Spot: A Transferable Attack On Synthetic
  Image Detectors</title>
    <summary>  Visually realistic GAN-generated images have recently emerged as an important
misinformation threat. Research has shown that these synthetic images contain
forensic traces that are readily identifiable by forensic detectors.
Unfortunately, these detectors are built upon neural networks, which are
vulnerable to recently developed adversarial attacks. In this paper, we propose
a new anti-forensic attack capable of fooling GAN-generated image detectors.
Our attack uses an adversarially trained generator to synthesize traces that
these detectors associate with real images. Furthermore, we propose a technique
to train our attack so that it can achieve transferability, i.e. it can fool
unknown CNNs that it was not explicitly trained against. We evaluate our attack
through an extensive set of experiments, where we show that our attack can fool
eight state-of-the-art detection CNNs with synthetic images created using seven
different GANs, and outperform other alternative attacks.
</summary>
    <author>
      <name>Xinwei Zhao</name>
    </author>
    <author>
      <name>Matthew C. Stamm</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Pattern Recognition, August 2022,
  Montr\'eal Qu\'ebec</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.12069v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12069v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12556v3</id>
    <updated>2021-08-04T10:37:23Z</updated>
    <published>2021-04-16T12:41:15Z</published>
    <title>COVID-19 Modeling: A Review</title>
    <summary>  The SARS-CoV-2 virus and COVID-19 disease have posed unprecedented and
overwhelming demand, challenges and opportunities to domain, model and data
driven modeling. This paper provides a comprehensive review of the challenges,
tasks, methods, progress, gaps and opportunities in relation to modeling
COVID-19 problems, data and objectives. It constructs a research landscape of
COVID-19 modeling tasks and methods, and further categorizes, summarizes,
compares and discusses the related methods and progress of modeling COVID-19
epidemic transmission processes and dynamics, case identification and tracing,
infection diagnosis and medical treatments, non-pharmaceutical interventions
and their effects, drug and vaccine development, psychological, economic and
social influence and impact, and misinformation, etc. The modeling methods
involve mathematical and statistical models, domain-driven modeling by
epidemiological compartmental models, medical and biomedical analysis, AI and
data science in particular shallow and deep machine learning, simulation
modeling, social science methods, and hybrid modeling.
</summary>
    <author>
      <name>Longbing Cao</name>
    </author>
    <author>
      <name>Qing Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">73 pages, 3 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.12556v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12556v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13559v2</id>
    <updated>2021-05-18T05:41:05Z</updated>
    <published>2021-04-28T03:38:24Z</published>
    <title>AraStance: A Multi-Country and Multi-Domain Dataset of Arabic Stance
  Detection for Fact Checking</title>
    <summary>  With the continuing spread of misinformation and disinformation online, it is
of increasing importance to develop combating mechanisms at scale in the form
of automated systems that support multiple languages. One task of interest is
claim veracity prediction, which can be addressed using stance detection with
respect to relevant documents retrieved online. To this end, we present our new
Arabic Stance Detection dataset (AraStance) of 4,063 claim--article pairs from
a diverse set of sources comprising three fact-checking websites and one news
website. AraStance covers false and true claims from multiple domains (e.g.,
politics, sports, health) and several Arab countries, and it is well-balanced
between related and unrelated documents with respect to the claims. We
benchmark AraStance, along with two other stance detection datasets, using a
number of BERT-based models. Our best model achieves an accuracy of 85\% and a
macro F1 score of 78\%, which leaves room for improvement and reflects the
challenging nature of AraStance and the task of stance detection in general.
</summary>
    <author>
      <name>Tariq Alhindi</name>
    </author>
    <author>
      <name>Amal Alabdulkarim</name>
    </author>
    <author>
      <name>Ali Alshehri</name>
    </author>
    <author>
      <name>Muhammad Abdul-Mageed</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 2021 Workshop on NLP4IF: Censorship, Disinformation,
  and Propaganda</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.13559v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13559v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13748v1</id>
    <updated>2021-04-28T13:28:27Z</updated>
    <published>2021-04-28T13:28:27Z</published>
    <title>QuTI! Quantifying Text-Image Consistency in Multimodal Documents</title>
    <summary>  The World Wide Web and social media platforms have become popular sources for
news and information. Typically, multimodal information, e.g., image and text
is used to convey information more effectively and to attract attention. While
in most cases image content is decorative or depicts additional information, it
has also been leveraged to spread misinformation and rumors in recent years. In
this paper, we present a Web-based demo application that automatically
quantifies the cross-modal relations of entities (persons, locations, and
events) in image and text. The applications are manifold. For example, the
system can help users to explore multimodal articles more efficiently, or can
assist human assessors and fact-checking efforts in the verification of the
credibility of news stories, tweets, or other multimodal documents.
</summary>
    <author>
      <name>Matthias Springstein</name>
    </author>
    <author>
      <name>Eric M√ºller-Budack</name>
    </author>
    <author>
      <name>Ralph Ewerth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in: International ACM SIGIR Conference on
  Research and Development in Information Retrieval 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.13748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13754v5</id>
    <updated>2022-12-19T19:37:14Z</updated>
    <published>2021-04-28T13:35:28Z</published>
    <title>Can crowdsourcing rescue the social marketplace of ideas?</title>
    <summary>  Facebook and Twitter recently announced community-based review platforms to
address misinformation. We provide an overview of the potential affordances of
such community-based approaches to content moderation based on past research
and preliminary analysis of Twitter's Birdwatch data. While our analysis
generally supports a community-based approach to content moderation, it also
warns against potential pitfalls, particularly when the implementation of the
new infrastructure focuses on crowd-based "validation" rather than
"collaboration." We call for multidisciplinary research utilizing methods from
complex systems studies, behavioural sociology, and computational social
science to advance the research on crowd-based content moderation.
</summary>
    <author>
      <name>Taha Yasseri</name>
    </author>
    <author>
      <name>Filippo Menczer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3578645</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3578645" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Press in Communications of the ACM (CACM)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications of the ACM (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.13754v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13754v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13816v1</id>
    <updated>2021-04-28T15:04:22Z</updated>
    <published>2021-04-28T15:04:22Z</published>
    <title>The Evolution of Rumors on a Closed Platform during COVID-19</title>
    <summary>  In this work we looked into a dataset of 114 thousands of suspicious messages
collected from the most popular closed messaging platform in Taiwan between
January and July, 2020. We proposed an hybrid algorithm that could efficiently
cluster a large number of text messages according their topics and narratives.
That is, we obtained groups of messages that are within a limited content
alterations within each other. By employing the algorithm to the dataset, we
were able to look at the content alterations and the temporal dynamics of each
particular rumor over time. With qualitative case studies of three COVID-19
related rumors, we have found that key authoritative figures were often
misquoted in false information. It was an effective measure to increase the
popularity of one false information. In addition, fact-check was not effective
in stopping misinformation from getting attention. In fact, the popularity of
one false information was often more influenced by major societal events and
effective content alterations.
</summary>
    <author>
      <name>Andrea W Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Information Operations Research Group</arxiv:affiliation>
    </author>
    <author>
      <name>Jo-Yu Lan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Information Engineering and Computer Science, Feng Chia University</arxiv:affiliation>
    </author>
    <author>
      <name>Chihhao Yu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Information Operations Research Group</arxiv:affiliation>
    </author>
    <author>
      <name>Ming-Hung Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Information Engineering and Computer Science, Feng Chia University</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2104.13816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.02012v1</id>
    <updated>2021-07-01T11:07:47Z</updated>
    <published>2021-07-01T11:07:47Z</published>
    <title>Tackling COVID-19 Infodemic using Deep Learning</title>
    <summary>  Humanity is battling one of the most deleterious virus in modern history, the
COVID-19 pandemic, but along with the pandemic there's an infodemic permeating
the pupil and society with misinformation which exacerbates the current malady.
We try to detect and classify fake news on online media to detect fake
information relating to COVID-19 and coronavirus. The dataset contained fake
posts, articles and news gathered from fact checking websites like politifact
whereas real tweets were taken from verified twitter handles. We incorporated
multiple conventional classification techniques like Naive Bayes, KNN, Gradient
Boost and Random Forest along with Deep learning approaches, specifically CNN,
RNN, DNN and the ensemble model RMDL. We analyzed these approaches with two
feature extraction techniques, TF-IDF and GloVe Word Embeddings which would
provide deeper insights into the dataset containing COVID-19 info on online
media.
</summary>
    <author>
      <name>Prathmesh Pathwar</name>
    </author>
    <author>
      <name>Simran Gill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, Accepted in 4th International Conference on
  Computational Intelligence and Data Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.02012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.02012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.05243v1</id>
    <updated>2021-07-12T08:07:09Z</updated>
    <published>2021-07-12T08:07:09Z</published>
    <title>Putting words into the system's mouth: A targeted attack on neural
  machine translation using monolingual data poisoning</title>
    <summary>  Neural machine translation systems are known to be vulnerable to adversarial
test inputs, however, as we show in this paper, these systems are also
vulnerable to training attacks. Specifically, we propose a poisoning attack in
which a malicious adversary inserts a small poisoned sample of monolingual text
into the training set of a system trained using back-translation. This sample
is designed to induce a specific, targeted translation behaviour, such as
peddling misinformation. We present two methods for crafting poisoned examples,
and show that only a tiny handful of instances, amounting to only 0.02% of the
training set, is sufficient to enact a successful attack. We outline a defence
method against said attacks, which partly ameliorates the problem. However, we
stress that this is a blind-spot in modern NMT, demanding immediate attention.
</summary>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Chang Xu</name>
    </author>
    <author>
      <name>Francisco Guzman</name>
    </author>
    <author>
      <name>Ahmed El-Kishky</name>
    </author>
    <author>
      <name>Yuqing Tang</name>
    </author>
    <author>
      <name>Benjamin I. P. Rubinstein</name>
    </author>
    <author>
      <name>Trevor Cohn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Findings of ACL, to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.05243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.05243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08357v1</id>
    <updated>2021-07-18T04:09:47Z</updated>
    <published>2021-07-18T04:09:47Z</published>
    <title>As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical
  Translation</title>
    <summary>  Mistranslated numbers have the potential to cause serious effects, such as
financial loss or medical misinformation. In this work we develop comprehensive
assessments of the robustness of neural machine translation systems to
numerical text via behavioural testing. We explore a variety of numerical
translation capabilities a system is expected to exhibit and design effective
test examples to expose system underperformance. We find that numerical
mistranslation is a general issue: major commercial systems and
state-of-the-art research models fail on many of our test examples, for high-
and low-resource languages. Our tests reveal novel errors that have not
previously been reported in NMT systems, to the best of our knowledge. Lastly,
we discuss strategies to mitigate numerical mistranslation.
</summary>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Chang Xu</name>
    </author>
    <author>
      <name>Francisco Guzman</name>
    </author>
    <author>
      <name>Ahmed El-Kishky</name>
    </author>
    <author>
      <name>Benjamin I. P. Rubinstein</name>
    </author>
    <author>
      <name>Trevor Cohn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Findings of ACL, to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.08357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.09183v2</id>
    <updated>2021-09-03T19:53:03Z</updated>
    <published>2021-07-19T22:47:17Z</published>
    <title>Analysis of External Content in the Vaccination Discussion on Twitter</title>
    <summary>  The spread of coronavirus and anti-vaccine conspiracies online hindered
public health responses to the pandemic. We examined the content of external
articles shared on Twitter from February to June 2020 to understand how
conspiracy theories and fake news competed with legitimate sources of
information. Examining external content--articles, rather than social media
posts--is a novel methodology that allows for non-social media specific
analysis of misinformation, tracking of changing narratives over time, and
determining which types of resources (government, news, scientific, or dubious)
dominate the pandemic vaccine conversation. We find that distinct narratives
emerge, those narratives change over time, and lack of government and
scientific messaging on coronavirus created an information vacuum filled by
both traditional news and conspiracy theories.
</summary>
    <author>
      <name>Richard Kuzma</name>
    </author>
    <author>
      <name>Iain J. Cruickshank</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Data Ownership Issues</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.09183v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09183v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.02283v1</id>
    <updated>2021-09-06T08:28:43Z</updated>
    <published>2021-09-06T08:28:43Z</published>
    <title>Does Melania Trump have a body double from the perspective of automatic
  face recognition?</title>
    <summary>  In this paper, we explore whether automatic face recognition can help in
verifying widespread misinformation on social media, particularly conspiracy
theories that are based on the existence of body doubles. The conspiracy theory
addressed in this paper is the case of the Melania Trump body double. We
employed four different state-of-the-art descriptors for face recognition to
verify the integrity of the claim of the studied conspiracy theory. In
addition, we assessed the impact of different image quality metrics on the
variation of face recognition results. Two sets of image quality metrics were
considered: acquisition-related metrics and subject-related metrics.
</summary>
    <author>
      <name>Khawla Mallat</name>
    </author>
    <author>
      <name>Fabiola Becerra-Riera</name>
    </author>
    <author>
      <name>Annette Morales-Gonz√°lez</name>
    </author>
    <author>
      <name>Heydi M√©ndez-V√°zquez</name>
    </author>
    <author>
      <name>Jean-Luc Dugelay</name>
    </author>
    <link href="http://arxiv.org/abs/2109.02283v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02283v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.02927v3</id>
    <updated>2021-12-11T14:30:34Z</updated>
    <published>2021-09-07T08:28:57Z</published>
    <title>Heterogeneity-aware Twitter Bot Detection with Relational Graph
  Transformers</title>
    <summary>  Twitter bot detection has become an important and challenging task to combat
misinformation and protect the integrity of the online discourse.
State-of-the-art approaches generally leverage the topological structure of the
Twittersphere, while they neglect the heterogeneity of relations and influence
among users. In this paper, we propose a novel bot detection framework to
alleviate this problem, which leverages the topological structure of
user-formed heterogeneous graphs and models varying influence intensity between
users. Specifically, we construct a heterogeneous information network with
users as nodes and diversified relations as edges. We then propose relational
graph transformers to model heterogeneous influence between users and learn
node representations. Finally, we use semantic attention networks to aggregate
messages across users and relations and conduct heterogeneity-aware Twitter bot
detection. Extensive experiments demonstrate that our proposal outperforms
state-of-the-art methods on a comprehensive Twitter bot detection benchmark.
Additional studies also bear out the effectiveness of our proposed relational
graph transformers, semantic attention networks and the graph-based approach in
general.
</summary>
    <author>
      <name>Shangbin Feng</name>
    </author>
    <author>
      <name>Zhaoxuan Tan</name>
    </author>
    <author>
      <name>Rui Li</name>
    </author>
    <author>
      <name>Minnan Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at AAAI 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.02927v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02927v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11372v1</id>
    <updated>2021-09-22T14:26:08Z</updated>
    <published>2021-09-22T14:26:08Z</published>
    <title>A Second Pandemic? Analysis of Fake News About COVID-19 Vaccines in
  Qatar</title>
    <summary>  While COVID-19 vaccines are finally becoming widely available, a second
pandemic that revolves around the circulation of anti-vaxxer fake news may
hinder efforts to recover from the first one. With this in mind, we performed
an extensive analysis of Arabic and English tweets about COVID-19 vaccines,
with focus on messages originating from Qatar. We found that Arabic tweets
contain a lot of false information and rumors, while English tweets are mostly
factual. However, English tweets are much more propagandistic than Arabic ones.
In terms of propaganda techniques, about half of the Arabic tweets express
doubt, and 1/5 use loaded language, while English tweets are abundant in loaded
language, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in
terms of framing, Arabic tweets adopt a health and safety perspective, while in
English economic concerns dominate.
</summary>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Firoj Alam</name>
    </author>
    <author>
      <name>Shaden Shaar</name>
    </author>
    <author>
      <name>Giovanni Da San Martino</name>
    </author>
    <author>
      <name>Yifan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COVID-19, disinformation, misinformation, factuality, fact-checking,
  fact-checkers, check-worthiness, framing, harmfulness, social media
  platforms, social media</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">RANLP-2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.11372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12777v1</id>
    <updated>2021-09-27T03:40:28Z</updated>
    <published>2021-09-27T03:40:28Z</published>
    <title>ReINTEL Challenge 2020: A Comparative Study of Hybrid Deep Neural
  Network for Reliable Intelligence Identification on Vietnamese SNSs</title>
    <summary>  The overwhelming abundance of data has created a misinformation crisis.
Unverified sensationalism that is designed to grab the readers' short attention
span, when crafted with malice, has caused irreparable damage to our society's
structure. As a result, determining the reliability of an article has become a
crucial task. After various ablation studies, we propose a multi-input model
that can effectively leverage both tabular metadata and post content for the
task. Applying state-of-the-art finetuning techniques for the pretrained
component and training strategies for our complete model, we have achieved a
0.9462 ROC-score on the VLSP private test set.
</summary>
    <author>
      <name>Hoang Viet Trinh</name>
    </author>
    <author>
      <name>Tung Tien Bui</name>
    </author>
    <author>
      <name>Tam Minh Nguyen</name>
    </author>
    <author>
      <name>Huy Quang Dao</name>
    </author>
    <author>
      <name>Quang Huu Pham</name>
    </author>
    <author>
      <name>Ngoc N. Tran</name>
    </author>
    <author>
      <name>Ta Minh Thanh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 7th International Workshop on Vietnamese
  Language and Speech Processing (VLSP), Hanoi, Vietnam, 2020, pp. 6-12</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.12777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13589v1</id>
    <updated>2021-09-28T09:58:02Z</updated>
    <published>2021-09-28T09:58:02Z</published>
    <title>Learning Ideological Embeddings from Information Cascades</title>
    <summary>  Modeling information cascades in a social network through the lenses of the
ideological leaning of its users can help understanding phenomena such as
misinformation propagation and confirmation bias, and devising techniques for
mitigating their toxic effects.
  In this paper we propose a stochastic model to learn the ideological leaning
of each user in a multidimensional ideological space, by analyzing the way
politically salient content propagates. In particular, our model assumes that
information propagates from one user to another if both users are interested in
the topic and ideologically aligned with each other. To infer the parameters of
our model, we devise a gradient-based optimization procedure maximizing the
likelihood of an observed set of information cascades. Our experiments on
real-world political discussions on Twitter and Reddit confirm that our model
is able to learn the political stance of the social media users in a
multidimensional ideological space.
</summary>
    <author>
      <name>Corrado Monti</name>
    </author>
    <author>
      <name>Giuseppe Manco</name>
    </author>
    <author>
      <name>Cigdem Aslay</name>
    </author>
    <author>
      <name>Francesco Bonchi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3459637.3482444</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3459637.3482444" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in CIKM 2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 30th ACM International Conference on
  Information and Knowledge Management (CIKM 2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.13589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.05815v1</id>
    <updated>2021-11-10T17:18:07Z</updated>
    <published>2021-11-10T17:18:07Z</published>
    <title>(Mis)perceptions and Engagement on Twitter: COVID-19 Vaccine Rumors on
  Efficacy and Mass Immunization Effort</title>
    <summary>  This paper reports the findings of a 606-participant study where we analyzed
the perception and engagement effects of COVID-19 vaccine rumours on Twitter
pertaining to (a) vaccine efficacy; and (b) mass immunization efforts in the
United States. Misperceptions regarding vaccine efficacy were successfully
induced through simple content alterations and the addition of popular anti
COVID-19 hashtags to otherwise valid Twitter content. Twitter's misinformation
contextual tags caused a "backfire effect" for the skeptic, vaccine-hesitant
reinforcing their opposition stance. While the majority of the participants
staunchly refrain from engaging with the COVID-19 rumours, the skeptic,
vaccine-hesitant ones were open to comment, re-tweet, like and share the
vaccine efficacy rumors. We discuss the implications of our results in the
context of broadening the effort for dispelling rumors about COVID-19 on social
media.
</summary>
    <author>
      <name>Filipo Sharevski</name>
    </author>
    <author>
      <name>Allice Huff</name>
    </author>
    <author>
      <name>Peter Jachim</name>
    </author>
    <author>
      <name>Emma Pieroni</name>
    </author>
    <link href="http://arxiv.org/abs/2111.05815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.05815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.11955v1</id>
    <updated>2021-11-23T15:48:54Z</updated>
    <published>2021-11-23T15:48:54Z</published>
    <title>People Lie, Actions Don't! Modeling Infodemic Proliferation Predictors
  among Social Media Users</title>
    <summary>  Social media is interactive, and interaction brings misinformation. With the
growing amount of user-generated data, fake news on online platforms has become
much frequent since the arrival of social networks. Now and then, an event
occurs and becomes the topic of discussion, generating and propagating false
information. Existing literature studying fake news primarily elaborates on
fake news classification models. Approaches exploring fake news characteristics
and ways to distinguish it from real news are minimal. Not many researches have
focused on statistical testing and generating new factor discoveries. This
study assumes fourteen hypotheses to identify factors exhibiting a relationship
with fake news. We perform the experiments on two real-world COVID-19 datasets
using qualitative and quantitative testing methods. This study concludes that
sentiment polarity and gender can significantly identify fake news. Dependence
on the presence of visual media is, however, inconclusive. Additionally,
Twitter-specific factors like followers count, friends count, and retweet count
significantly differ in fake and real news. Though, the contribution of status
count and favorites count is disputed. This study identifies practical factors
to be conjunctly utilized in the development of fake news detection algorithms.
</summary>
    <author>
      <name>Chahat Raj</name>
    </author>
    <author>
      <name>Priyanka Meel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.11955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.11955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.13735v1</id>
    <updated>2021-11-26T20:14:27Z</updated>
    <published>2021-11-26T20:14:27Z</published>
    <title>Resilient Nash Equilibrium Seeking in the Partial Information Setting</title>
    <summary>  Current research in distributed Nash equilibrium (NE) seeking in the partial
information setting assumes that information is exchanged between agents that
are "truthful". However, in general noncooperative games agents may consider
sending misinformation to neighboring agents with the goal of further reducing
their cost. Additionally, communication networks are vulnerable to attacks from
agents outside the game as well as communication failures. In this paper, we
propose a distributed NE seeking algorithm that is robust against adversarial
agents that transmit noise, random signals, constant singles, deceitful
messages, as well as being resilient to external factors such as dropped
communication, jammed signals, and man in the middle attacks. The core issue
that makes the problem challenging is that agents have no means of verifying if
the information they receive is correct, i.e. there is no "ground truth". To
address this problem, we use an observation graph, that gives truthful action
information, in conjunction with a communication graph, that gives (potentially
incorrect) information. By filtering information obtained from these two
graphs, we show that our algorithm is resilient against adversarial agents and
converges to the Nash equilibrium.
</summary>
    <author>
      <name>Dian Gadjov</name>
    </author>
    <author>
      <name>Lacra Pavel</name>
    </author>
    <link href="http://arxiv.org/abs/2111.13735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.13735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00102v2</id>
    <updated>2022-06-17T19:55:57Z</updated>
    <published>2022-04-29T23:23:37Z</published>
    <title>Manipulating Elections by Changing Voter Perceptions</title>
    <summary>  The integrity of elections is central to democratic systems. However, a
myriad of malicious actors aspire to influence election outcomes for financial
or political benefit. A common means to such ends is by manipulating
perceptions of the voting public about select candidates, for example, through
misinformation. We present a formal model of the impact of perception
manipulation on election outcomes in the framework of spatial voting theory, in
which the preferences of voters over candidates are generated based on their
relative distance in the space of issues. We show that controlling elections in
this model is, in general, NP-hard, whether issues are binary or real-valued.
However, we demonstrate that critical to intractability is the diversity of
opinions on issues exhibited by the voting public. When voter views lack
diversity, and we can instead group them into a small number of categories --
for example, as a result of political polarization -- the election control
problem can be solved in polynomial time in the number of issues and candidates
for arbitrary scoring rules.
</summary>
    <author>
      <name>Junlin Wu</name>
    </author>
    <author>
      <name>Andrew Estornell</name>
    </author>
    <author>
      <name>Lecheng Kong</name>
    </author>
    <author>
      <name>Yevgeniy Vorobeychik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00102v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00102v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00538v1</id>
    <updated>2022-05-01T19:00:52Z</updated>
    <published>2022-05-01T19:00:52Z</published>
    <title>Can Information Behaviour Inform Machine Learning?</title>
    <summary>  The objective of this paper is to explore the opportunities for human
information behaviour research to inform and influence the field of machine
learning and the resulting machine information behaviour. Using the development
of foundation models in machine learning as an example, the paper illustrates
how human information behaviour research can bring to machine learning a more
nuanced view of information and informing, a better understanding of
information need and how that affects the communication among people and
systems, guidance on the nature of context and how to operationalize that in
models and systems, and insights into bias, misinformation, and
marginalization. Despite their clear differences, the fields of information
behaviour and machine learning share many common objectives, paradigms, and key
research questions. The example of foundation models illustrates that human
information behaviour research has much to offer in addressing some of the
challenges emerging in the nascent area of machine information behaviour.
</summary>
    <author>
      <name>Michael Ridley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01989v1</id>
    <updated>2022-05-04T10:43:58Z</updated>
    <published>2022-05-04T10:43:58Z</published>
    <title>MM-Claims: A Dataset for Multimodal Claim Detection in Social Media</title>
    <summary>  In recent years, the problem of misinformation on the web has become
widespread across languages, countries, and various social media platforms.
Although there has been much work on automated fake news detection, the role of
images and their variety are not well explored. In this paper, we investigate
the roles of image and text at an earlier stage of the fake news detection
pipeline, called claim detection. For this purpose, we introduce a novel
dataset, MM-Claims, which consists of tweets and corresponding images over
three topics: COVID-19, Climate Change and broadly Technology. The dataset
contains roughly 86000 tweets, out of which 3400 are labeled manually by
multiple annotators for the training and evaluation of multimodal models. We
describe the dataset in detail, evaluate strong unimodal and multimodal
baselines, and analyze the potential and drawbacks of current models.
</summary>
    <author>
      <name>Gullal S. Cheema</name>
    </author>
    <author>
      <name>Sherzod Hakimov</name>
    </author>
    <author>
      <name>Abdul Sittar</name>
    </author>
    <author>
      <name>Eric M√ºller-Budack</name>
    </author>
    <author>
      <name>Christian Otto</name>
    </author>
    <author>
      <name>Ralph Ewerth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Findings of NAACL 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.01989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.02596v1</id>
    <updated>2022-05-05T12:11:31Z</updated>
    <published>2022-05-05T12:11:31Z</published>
    <title>Natural Language Inference with Self-Attention for Veracity Assessment
  of Pandemic Claims</title>
    <summary>  We present a comprehensive work on automated veracity assessment from dataset
creation to developing novel methods based on Natural Language Inference (NLI),
focusing on misinformation related to the COVID-19 pandemic. We first describe
the construction of the novel PANACEA dataset consisting of heterogeneous
claims on COVID-19 and their respective information sources. The dataset
construction includes work on retrieval techniques and similarity measurements
to ensure a unique set of claims. We then propose novel techniques for
automated veracity assessment based on Natural Language Inference including
graph convolutional networks and attention based approaches. We have carried
out experiments on evidence retrieval and veracity assessment on the dataset
using the proposed techniques and found them competitive with SOTA methods, and
provided a detailed discussion.
</summary>
    <author>
      <name>M. Arana-Catania</name>
    </author>
    <author>
      <name>Elena Kochkina</name>
    </author>
    <author>
      <name>Arkaitz Zubiaga</name>
    </author>
    <author>
      <name>Maria Liakata</name>
    </author>
    <author>
      <name>Rob Procter</name>
    </author>
    <author>
      <name>Yulan He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 1 figure, 8 tables, presented in NAACL 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.02596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.02596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04402v1</id>
    <updated>2022-05-09T16:11:04Z</updated>
    <published>2022-05-09T16:11:04Z</published>
    <title>Detecting the Role of an Entity in Harmful Memes: Techniques and Their
  Limitations</title>
    <summary>  Harmful or abusive online content has been increasing over time, raising
concerns for social media platforms, government agencies, and policymakers.
Such harmful or abusive content can have major negative impact on society,
e.g., cyberbullying can lead to suicides, rumors about COVID-19 can cause
vaccine hesitance, promotion of fake cures for COVID-19 can cause health harms
and deaths. The content that is posted and shared online can be textual,
visual, or a combination of both, e.g., in a meme. Here, we describe our
experiments in detecting the roles of the entities (hero, villain, victim) in
harmful memes, which is part of the CONSTRAINT-2022 shared task, as well as our
system for the task. We further provide a comparative analysis of different
experimental settings (i.e., unimodal, multimodal, attention, and
augmentation). For reproducibility, we make our experimental code publicly
available. \url{https://github.com/robi56/harmful_memes_block_fusion}
</summary>
    <author>
      <name>Rabindra Nath Nandi</name>
    </author>
    <author>
      <name>Firoj Alam</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CONSTRAINT 2022 (Colocated with ACL-2022),
  disinformation, misinformation, factuality, harmfulness, fake news,
  propaganda, multimodality, text, images, videos, network structure,
  temporality</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.04402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.05783v2</id>
    <updated>2022-05-13T00:37:18Z</updated>
    <published>2022-05-11T21:44:26Z</published>
    <title>MEWS: Real-time Social Media Manipulation Detection and Analysis</title>
    <summary>  This article presents a beta-version of MEWS (Misinformation Early Warning
System). It describes the various aspects of the ingestion, manipulation
detection, and graphing algorithms employed to determine--in near
real-time--the relationships between social media images as they emerge and
spread on social media platforms. By combining these various technologies into
a single processing pipeline, MEWS can identify manipulated media items as they
arise and identify when these particular items begin trending on individual
social media platforms or even across multiple platforms. The emergence of a
novel manipulation followed by rapid diffusion of the manipulated content
suggests a disinformation campaign.
</summary>
    <author>
      <name>Trenton W. Ford</name>
    </author>
    <author>
      <name>William Theisen</name>
    </author>
    <author>
      <name>Michael Yankoski</name>
    </author>
    <author>
      <name>Tom Henry</name>
    </author>
    <author>
      <name>Farah Khashman</name>
    </author>
    <author>
      <name>Katherine R. Dearstyne</name>
    </author>
    <author>
      <name>Tim Weninger</name>
    </author>
    <link href="http://arxiv.org/abs/2205.05783v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.05783v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06241v2</id>
    <updated>2022-12-12T14:49:22Z</updated>
    <published>2022-05-12T17:39:54Z</published>
    <title>Can counterfactual explanations of AI systems' predictions skew lay
  users' causal intuitions about the world? If so, can we correct for that?</title>
    <summary>  Counterfactual (CF) explanations have been employed as one of the modes of
explainability in explainable AI-both to increase the transparency of AI
systems and to provide recourse. Cognitive science and psychology, however,
have pointed out that people regularly use CFs to express causal relationships.
Most AI systems are only able to capture associations or correlations in data
so interpreting them as casual would not be justified. In this paper, we
present two experiment (total N = 364) exploring the effects of CF explanations
of AI system's predictions on lay people's causal beliefs about the real world.
In Experiment 1 we found that providing CF explanations of an AI system's
predictions does indeed (unjustifiably) affect people's causal beliefs
regarding factors/features the AI uses and that people are more likely to view
them as causal factors in the real world. Inspired by the literature on
misinformation and health warning messaging, Experiment 2 tested whether we can
correct for the unjustified change in causal beliefs. We found that pointing
out that AI systems capture correlations and not necessarily causal
relationships can attenuate the effects of CF explanations on people's causal
beliefs.
</summary>
    <author>
      <name>Marko Tesic</name>
    </author>
    <author>
      <name>Ulrike Hahn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patter.2022.100635</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patter.2022.100635" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Patterns, 3(12), 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.06241v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06241v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06684v1</id>
    <updated>2022-05-13T14:39:25Z</updated>
    <published>2022-05-13T14:39:25Z</published>
    <title>The Effectiveness of Temporal Dependency in Deepfake Video Detection</title>
    <summary>  Deepfakes are a form of synthetic image generation used to generate fake
videos of individuals for malicious purposes. The resulting videos may be used
to spread misinformation, reduce trust in media, or as a form of blackmail.
These threats necessitate automated methods of deepfake video detection. This
paper investigates whether temporal information can improve the deepfake
detection performance of deep learning models.
  To investigate this, we propose a framework that classifies new and existing
approaches by their defining characteristics. These are the types of feature
extraction: automatic or manual, and the temporal relationship between frames:
dependent or independent. We apply this framework to investigate the effect of
temporal dependency on a model's deepfake detection performance.
  We find that temporal dependency produces a statistically significant (p &lt;
0.05) increase in performance in classifying real images for the model using
automatic feature selection, demonstrating that spatio-temporal information can
increase the performance of deepfake video detection models.
</summary>
    <author>
      <name>Will Rowan</name>
    </author>
    <author>
      <name>Nick Pears</name>
    </author>
    <link href="http://arxiv.org/abs/2205.06684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.12221v2</id>
    <updated>2023-06-11T05:30:31Z</updated>
    <published>2022-05-24T17:29:13Z</published>
    <title>ClaimDiff: Comparing and Contrasting Claims on Contentious Issues</title>
    <summary>  With the growing importance of detecting misinformation, many studies have
focused on verifying factual claims by retrieving evidence. However, canonical
fact verification tasks do not apply to catching subtle differences in
factually consistent claims, which might still bias the readers, especially on
contentious political or economic issues. Our underlying assumption is that
among the trusted sources, one's argument is not necessarily more true than the
other, requiring comparison rather than verification. In this study, we propose
ClaimDiff, a novel dataset that primarily focuses on comparing the nuance
between claim pairs. In ClaimDiff, we provide 2,941 annotated claim pairs from
268 news articles. We observe that while humans are capable of detecting the
nuances between claims, strong baselines struggle to detect them, showing over
a 19% absolute gap with the humans. We hope this initial study could help
readers to gain an unbiased grasp of contentious issues through machine-aided
comparison.
</summary>
    <author>
      <name>Miyoung Ko</name>
    </author>
    <author>
      <name>Ingyu Seong</name>
    </author>
    <author>
      <name>Hwaran Lee</name>
    </author>
    <author>
      <name>Joonsuk Park</name>
    </author>
    <author>
      <name>Minsuk Chang</name>
    </author>
    <author>
      <name>Minjoon Seo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published at Findings of ACL 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.12221v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.12221v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.14060v4</id>
    <updated>2023-12-20T17:52:38Z</updated>
    <published>2022-05-27T15:47:27Z</published>
    <title>Content Filtering with Inattentive Information Consumers</title>
    <summary>  We develop a model of content filtering as a game between the filter and the
content consumer, where the latter incurs information costs for examining the
content. Motivating examples include censoring misinformation, spam/phish
filtering, and recommender systems. When the attacker is exogenous, we show
that improving the filter's quality is weakly Pareto improving, but has no
impact on equilibrium payoffs until the filter becomes sufficiently accurate.
Further, if the filter does not internalize the information costs, its lack of
commitment power may render it useless and lead to inefficient outcomes. When
the attacker is also strategic, improvements to filter quality may sometimes
decrease equilibrium payoffs.
</summary>
    <author>
      <name>Ian Ball</name>
    </author>
    <author>
      <name>James Bono</name>
    </author>
    <author>
      <name>Justin Grana</name>
    </author>
    <author>
      <name>Nicole Immorlica</name>
    </author>
    <author>
      <name>Brendan Lucier</name>
    </author>
    <author>
      <name>Aleksandrs Slivkins</name>
    </author>
    <link href="http://arxiv.org/abs/2205.14060v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.14060v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06919v1</id>
    <updated>2017-12-19T13:39:37Z</updated>
    <published>2017-12-19T13:39:37Z</published>
    <title>A Production Oriented Approach for Vandalism Detection in Wikidata - The
  Buffaloberry Vandalism Detector at WSDM Cup 2017</title>
    <summary>  Wikidata is a free and open knowledge base from the Wikimedia Foundation,
that not only acts as a central storage of structured data for other projects
of the organization, but also for a growing array of information systems,
including search engines. Like Wikipedia, Wikidata's content can be created and
edited by anyone; which is the main source of its strength, but also allows for
malicious users to vandalize it, risking the spreading of misinformation
through all the systems that rely on it as a source of structured facts. Our
task at the WSDM Cup 2017 was to come up with a fast and reliable prediction
system that narrows down suspicious edits for human revision. Elaborating on
previous works by Heindorf et al. we were able to outperform all other
contestants, while incorporating new interesting features, unifying the
programming language used to only Python and refactoring the feature extractor
into a simpler and more compact code base.
</summary>
    <author>
      <name>Rafael Crescenzi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Austral University</arxiv:affiliation>
    </author>
    <author>
      <name>Marcelo Fernandez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Austral University</arxiv:affiliation>
    </author>
    <author>
      <name>Federico A. Garcia Calabria</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Austral University</arxiv:affiliation>
    </author>
    <author>
      <name>Pablo Albani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Austral University</arxiv:affiliation>
    </author>
    <author>
      <name>Diego Tauziet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Austral University</arxiv:affiliation>
    </author>
    <author>
      <name>Adriana Baravalle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Austral University</arxiv:affiliation>
    </author>
    <author>
      <name>Andr√©s Sebasti√°n D'Ambrosio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Austral University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Vandalism Detector at WSDM Cup 2017, see arXiv:1712.05956</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.06919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05327v1</id>
    <updated>2018-07-14T03:33:36Z</updated>
    <published>2018-07-14T03:33:36Z</published>
    <title>How Humans versus Bots React to Deceptive and Trusted News Sources: A
  Case Study of Active Users</title>
    <summary>  Society's reliance on social media as a primary source of news has spawned a
renewed focus on the spread of misinformation. In this work, we identify the
differences in how social media accounts identified as bots react to news
sources of varying credibility, regardless of the veracity of the content those
sources have shared. We analyze bot and human responses annotated using a
fine-grained model that labels responses as being an answer, appreciation,
agreement, disagreement, an elaboration, humor, or a negative reaction. We
present key findings of our analysis into the prevalence of bots, the variety
and speed of bot and human reactions, and the disparity in authorship of
reaction tweets between these two sub-populations. We observe that bots are
responsible for 9-15% of the reactions to sources of any given type but
comprise only 7-10% of accounts responsible for reaction-tweets; trusted news
sources have the highest proportion of humans who reacted; bots respond with
significantly shorter delays than humans when posting answer-reactions in
response to sources identified as propaganda. Finally, we report significantly
different inequality levels in reaction rates for accounts identified as bots
vs not.
</summary>
    <author>
      <name>Maria Glenski</name>
    </author>
    <author>
      <name>Tim Weninger</name>
    </author>
    <author>
      <name>Svitlana Volkova</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06926v1</id>
    <updated>2018-07-18T13:41:57Z</updated>
    <published>2018-07-18T13:41:57Z</published>
    <title>Fake news as we feel it: perception and conceptualization of the term
  "fake news" in the media</title>
    <summary>  In this article, we quantitatively analyze how the term "fake news" is being
shaped in news media in recent years. We study the perception and the
conceptualization of this term in the traditional media using eight years of
data collected from news outlets based in 20 countries. Our results not only
corroborate previous indications of a high increase in the usage of the
expression "fake news", but also show contextual changes around this expression
after the United States presidential election of 2016. Among other results, we
found changes in the related vocabulary, in the mentioned entities, in the
surrounding topics and in the contextual polarity around the term "fake news",
suggesting that this expression underwent a change in perception and
conceptualization after 2016. These outcomes expand the understandings on the
usage of the term "fake news", helping to comprehend and more accurately
characterize this relevant social phenomenon linked to misinformation and
manipulation.
</summary>
    <author>
      <name>Evandro Cunha</name>
    </author>
    <author>
      <name>Gabriel Magno</name>
    </author>
    <author>
      <name>Josemar Caetano</name>
    </author>
    <author>
      <name>Douglas Teixeira</name>
    </author>
    <author>
      <name>Virgilio Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a full paper at the 10th International Conference on
  Social Informatics (SocInfo 2018). Please cite the SocInfo version</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.02037v1</id>
    <updated>2019-04-03T14:46:44Z</updated>
    <published>2019-04-03T14:46:44Z</published>
    <title>Automated Fact Checking in the News Room</title>
    <summary>  Fact checking is an essential task in journalism; its importance has been
highlighted due to recently increased concerns and efforts in combating
misinformation. In this paper, we present an automated fact-checking platform
which given a claim, it retrieves relevant textual evidence from a document
collection, predicts whether each piece of evidence supports or refutes the
claim, and returns a final verdict. We describe the architecture of the system
and the user interface, focusing on the choices made to improve its
user-friendliness and transparency. We conduct a user study of the
fact-checking platform in a journalistic setting: we integrated it with a
collection of news articles and provide an evaluation of the platform using
feedback from journalists in their workflow. We found that the predictions of
our platform were correct 58\% of the time, and 59\% of the returned evidence
was relevant.
</summary>
    <author>
      <name>Sebasti√£o Miranda</name>
    </author>
    <author>
      <name>David Nogueira</name>
    </author>
    <author>
      <name>Afonso Mendes</name>
    </author>
    <author>
      <name>Andreas Vlachos</name>
    </author>
    <author>
      <name>Andrew Secker</name>
    </author>
    <author>
      <name>Rebecca Garrett</name>
    </author>
    <author>
      <name>Jeff Mitchel</name>
    </author>
    <author>
      <name>Zita Marinho</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WEBCONF 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.02037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00850v2</id>
    <updated>2020-02-06T14:30:49Z</updated>
    <published>2020-01-28T19:56:03Z</published>
    <title>A Kernel of Truth: Determining Rumor Veracity on Twitter by Diffusion
  Pattern Alone</title>
    <summary>  Recent work in the domain of misinformation detection has leveraged rich
signals in the text and user identities associated with content on social
media. But text can be strategically manipulated and accounts reopened under
different aliases, suggesting that these approaches are inherently brittle. In
this work, we investigate an alternative modality that is naturally robust: the
pattern in which information propagates. Can the veracity of an unverified
rumor spreading online be discerned solely on the basis of its pattern of
diffusion through the social network?
  Using graph kernels to extract complex topological information from Twitter
cascade structures, we train accurate predictive models that are blind to
language, user identities, and time, demonstrating for the first time that such
"sanitized" diffusion patterns are highly informative of veracity. Our results
indicate that, with proper aggregation, the collective sharing pattern of the
crowd may reveal powerful signals of rumor truth or falsehood, even in the
early stages of propagation.
</summary>
    <author>
      <name>Nir Rosenfeld</name>
    </author>
    <author>
      <name>Aron Szanto</name>
    </author>
    <author>
      <name>David C. Parkes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3366423.3380180</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3366423.3380180" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at The Web Conference (WWW) 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.00850v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00850v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03438v1</id>
    <updated>2020-02-09T19:53:23Z</updated>
    <published>2020-02-09T19:53:23Z</published>
    <title>Limits of Detecting Text Generated by Large-Scale Language Models</title>
    <summary>  Some consider large-scale language models that can generate long and coherent
pieces of text as dangerous, since they may be used in misinformation
campaigns. Here we formulate large-scale language model output detection as a
hypothesis testing problem to classify text as genuine or generated. We show
that error exponents for particular language models are bounded in terms of
their perplexity, a standard measure of language generation performance. Under
the assumption that human language is stationary and ergodic, the formulation
is extended from considering specific language models to considering maximum
likelihood language models, among the class of k-order Markov approximations;
error probabilities are characterized. Some discussion of incorporating
semantic side information is also given.
</summary>
    <author>
      <name>Lav R. Varshney</name>
    </author>
    <author>
      <name>Nitish Shirish Keskar</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ITA 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07917v1</id>
    <updated>2020-02-18T22:56:40Z</updated>
    <published>2020-02-18T22:56:40Z</published>
    <title>TIES: Temporal Interaction Embeddings For Enhancing Social Media
  Integrity At Facebook</title>
    <summary>  Since its inception, Facebook has become an integral part of the online
social community. People rely on Facebook to make connections with others and
build communities. As a result, it is paramount to protect the integrity of
such a rapidly growing network in a fast and scalable manner. In this paper, we
present our efforts to protect various social media entities at Facebook from
people who try to abuse our platform. We present a novel Temporal Interaction
EmbeddingS (TIES) model that is designed to capture rogue social interactions
and flag them for further suitable actions. TIES is a supervised, deep
learning, production ready model at Facebook-scale networks. Prior works on
integrity problems are mostly focused on capturing either only static or
certain dynamic features of social entities. In contrast, TIES can capture both
these variant behaviors in a unified model owing to the recent strides made in
the domains of graph embedding and deep sequential pattern learning. To show
the real-world impact of TIES, we present a few applications especially for
preventing spread of misinformation, fake account detection, and reducing ads
payment risks in order to enhance the platform's integrity.
</summary>
    <author>
      <name>Nima Noorshams</name>
    </author>
    <author>
      <name>Saurabh Verma</name>
    </author>
    <author>
      <name>Aude Hofleitner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to KDD 2020 applied DS track</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.07917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11104v1</id>
    <updated>2020-02-24T20:04:54Z</updated>
    <published>2020-02-24T20:04:54Z</published>
    <title>An Information Diffusion Approach to Rumor Propagation and
  Identification on Twitter</title>
    <summary>  With the increasing use of online social networks as a source of news and
information, the propensity for a rumor to disseminate widely and quickly poses
a great concern, especially in disaster situations where users do not have
enough time to fact-check posts before making the informed decision to react to
a post that appears to be credible. In this study, we explore the propagation
pattern of rumors on Twitter by exploring the dynamics of microscopic-level
misinformation spread, based on the latent message and user interaction
attributes. We perform supervised learning for feature selection and
prediction. Experimental results with real-world data sets give the models'
prediction accuracy at about 90\% for the diffusion of both True and False
topics. Our findings confirm that rumor cascades run deeper and that rumor
masked as news, and messages that incite fear, will diffuse faster than other
messages. We show that the models for True and False message propagation differ
significantly, both in the prediction parameters and in the message features
that govern the diffusion. Finally, we show that the diffusion pattern is an
important metric in identifying the credibility of a tweet.
</summary>
    <author>
      <name>Abiola Osho</name>
    </author>
    <author>
      <name>Caden Waters</name>
    </author>
    <author>
      <name>George Amariucai</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11768v4</id>
    <updated>2022-01-19T09:28:08Z</updated>
    <published>2020-02-19T04:18:45Z</published>
    <title>Attacking Neural Text Detectors</title>
    <summary>  Machine learning based language models have recently made significant
progress, which introduces a danger to spread misinformation. To combat this
potential danger, several methods have been proposed for detecting text written
by these language models. This paper presents two classes of black-box attacks
on these detectors, one which randomly replaces characters with homoglyphs, and
the other a simple scheme to purposefully misspell words. The homoglyph and
misspelling attacks decrease a popular neural text detector's recall on neural
text from 97.44% to 0.26% and 22.68%, respectively. Results also indicate that
the attacks are transferable to other neural text detectors.
</summary>
    <author>
      <name>Max Wolff</name>
    </author>
    <author>
      <name>Stuart Wolff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the ICLR 2020 workshop "Towards Trustworthy ML:
  Rethinking Security and Privacy for ML."</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.11768v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11768v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00923v1</id>
    <updated>2020-03-02T14:07:56Z</updated>
    <published>2020-03-02T14:07:56Z</published>
    <title>Advertisers Jump on Coronavirus Bandwagon: Politics, News, and Business</title>
    <summary>  In the age of social media, disasters and epidemics usher not only a
devastation and affliction in the physical world, but also prompt a deluge of
information, opinions, prognoses and advice to billions of internet users. The
coronavirus epidemic of 2019-2020, or COVID-19, is no exception, with the World
Health Organization warning of a possible "infodemic" of fake news. In this
study, we examine the alternative narratives around the coronavirus outbreak
through advertisements promoted on Facebook, the largest social media platform
in the US. Using the new Facebook Ads Library, we discover advertisers from
public health and non-profit sectors, alongside those from news media,
politics, and business, incorporating coronavirus into their messaging and
agenda. We find the virus used in political attacks, donation solicitations,
business promotion, stock market advice, and animal rights campaigning. Among
these, we find several instances of possible misinformation, ranging from
bioweapons conspiracy theories to unverifiable claims by politicians. As we
make the dataset available to the community, we hope the advertising domain
will become an important part of quality control for public health
communication and public discourse in general.
</summary>
    <author>
      <name>Yelena Mejova</name>
    </author>
    <author>
      <name>Kyriaki Kalimeri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Under Review</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.00923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01797v1</id>
    <updated>2020-03-03T21:13:12Z</updated>
    <published>2020-03-03T21:13:12Z</published>
    <title>Discover Your Social Identity from What You Tweet: a Content Based
  Approach</title>
    <summary>  An identity denotes the role an individual or a group plays in highly
differentiated contemporary societies. In this paper, our goal is to classify
Twitter users based on their role identities. We first collect a coarse-grained
public figure dataset automatically, then manually label a more fine-grained
identity dataset. We propose a hierarchical self-attention neural network for
Twitter user role identity classification. Our experiments demonstrate that the
proposed model significantly outperforms multiple baselines. We further propose
a transfer learning scheme that improves our model's performance by a large
margin. Such transfer learning also greatly reduces the need for a large amount
of human labeled data.
</summary>
    <author>
      <name>Binxuan Huang</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-42699-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-42699-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprint of a chapter published in Disinformation,
  Misinformation, and Fake News in Social Media: Emerging Research Challenges
  and Opportunities, edited by Kai, S., Suhang, W., Dongwon, L., Huan, L, 2020,
  Springer reproduced with permission of Springer Nature Switzerland AG. The
  final authenticated version is available online at:
  http://dx.doi.org/10.1007/978-3-030-42699-6</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.01797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05004v1</id>
    <updated>2020-03-10T21:14:17Z</updated>
    <published>2020-03-10T21:14:17Z</published>
    <title>The COVID-19 Social Media Infodemic</title>
    <summary>  We address the diffusion of information about the COVID-19 with a massive
data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze
engagement and interest in the COVID-19 topic and provide a differential
assessment on the evolution of the discourse on a global scale for each
platform and their users. We fit information spreading with epidemic models
characterizing the basic reproduction numbers $R_0$ for each social media
platform. Moreover, we characterize information spreading from questionable
sources, finding different volumes of misinformation in each platform. However,
information from both reliable and questionable sources do not present
different spreading patterns. Finally, we provide platform-dependent numerical
estimates of rumors' amplification.
</summary>
    <author>
      <name>Matteo Cinelli</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <author>
      <name>Alessandro Galeazzi</name>
    </author>
    <author>
      <name>Carlo Michele Valensise</name>
    </author>
    <author>
      <name>Emanuele Brugnoli</name>
    </author>
    <author>
      <name>Ana Lucia Schmidt</name>
    </author>
    <author>
      <name>Paola Zola</name>
    </author>
    <author>
      <name>Fabiana Zollo</name>
    </author>
    <author>
      <name>Antonio Scala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41598-020-73510-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41598-020-73510-5" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sci Rep 10, 16598 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.05004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06857v2</id>
    <updated>2020-03-17T01:03:04Z</updated>
    <published>2020-03-15T15:53:27Z</published>
    <title>Can Celebrities Burst Your Bubble?</title>
    <summary>  Polarization is a growing, global problem. As such, many social media based
solutions have been proposed in order to reduce it. In this study, we propose a
new solution that recommends topics to celebrities to encourage them to join a
polarized debate and increase exposure to contrarian content - bursting the
filter bubble. Using a state-of-the art model that quantifies the degree of
polarization, this paper makes a first attempt to empirically answer the
question: Can celebrities burst filter bubbles? We use a case study to analyze
how people react when celebrities are involved in a controversial topic and
conclude with a list possible research directions.
</summary>
    <author>
      <name>Tuƒürulcan Elmas</name>
    </author>
    <author>
      <name>Kristina Hardi</name>
    </author>
    <author>
      <name>Rebekah Overdorf</name>
    </author>
    <author>
      <name>Karl Aberer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, accepted for non-archival track of IID2020,
  workshop in WWW2020</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop on Misinformation Integrity in Social
  Networks 2021 (MISINFO 2021) Vol-2890</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.06857v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06857v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.07192v4</id>
    <updated>2021-01-08T03:43:46Z</updated>
    <published>2020-03-12T18:37:19Z</published>
    <title>Social Media and Misleading Information in a Democracy: A Mechanism
  Design Approach</title>
    <summary>  In this paper, we present a resource allocation mechanism for the problem of
incentivizing filtering among a finite number of strategic social media
platforms. We consider the presence of a strategic government and private
knowledge of how misinformation affects the users of the social media
platforms. Our proposed mechanism incentivizes social media platforms to filter
misleading information efficiently, and thus indirectly prevents the spread of
fake news. In particular, we design an economically inspired mechanism that
strongly implements all generalized Nash equilibria for efficient filtering of
misleading information in the induced game. We show that our mechanism is
individually rational, budget balanced, while it has at least one equilibrium.
Finally, we show that for quasi-concave utilities and constraints, our
mechanism admits a generalized Nash equilibrium and implements a Pareto
efficient solution.
</summary>
    <author>
      <name>Aditya Dave</name>
    </author>
    <author>
      <name>Ioannis Vasileios Chremos</name>
    </author>
    <author>
      <name>Andreas A. Malikopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TAC.2021.3087466</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TAC.2021.3087466" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Automatic Control, Vol. 67, 5, pp. 2633-2639,
  2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.07192v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07192v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.07372v2</id>
    <updated>2020-06-02T18:35:00Z</updated>
    <published>2020-03-16T18:00:04Z</published>
    <title>Tracking Social Media Discourse About the COVID-19 Pandemic: Development
  of a Public Coronavirus Twitter Data Set</title>
    <summary>  At the time of this writing, the novel coronavirus (COVID-19) pandemic
outbreak has already put tremendous strain on many countries' citizens,
resources and economies around the world. Social distancing measures, travel
bans, self-quarantines, and business closures are changing the very fabric of
societies worldwide. With people forced out of public spaces, much conversation
about these phenomena now occurs online, e.g., on social media platforms like
Twitter. In this paper, we describe a multilingual coronavirus (COVID-19)
Twitter dataset that we have been continuously collecting since January 22,
2020. We are making our dataset available to the research community
(https://github.com/echen102/COVID-19-TweetIDs). It is our hope that our
contribution will enable the study of online conversation dynamics in the
context of a planetary-scale epidemic outbreak of unprecedented proportions and
implications. This dataset could also help track scientific coronavirus
misinformation and unverified rumors, or enable the understanding of fear and
panic -- and undoubtedly more. Ultimately, this dataset may contribute towards
enabling informed solutions and prescribing targeted policy interventions to
fight this global crisis.
</summary>
    <author>
      <name>Emily Chen</name>
    </author>
    <author>
      <name>Kristina Lerman</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2196/19273</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2196/19273" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JMIR Public Health Surveill 2020;6(2):e19273</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.07372v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07372v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.10359v1</id>
    <updated>2020-03-23T16:25:55Z</updated>
    <published>2020-03-23T16:25:55Z</published>
    <title>Understanding the perception of COVID-19 policies by mining a
  multilanguage Twitter dataset</title>
    <summary>  The objective of this work is to explore popular discourse about the COVID-19
pandemic and policies implemented to manage it. Using Natural Language
Processing, Text Mining, and Network Analysis to analyze corpus of tweets that
relate to the COVID-19 pandemic, we identify common responses to the pandemic
and how these responses differ across time. Moreover, insights as to how
information and misinformation were transmitted via Twitter, starting at the
early stages of this pandemic, are presented. Finally, this work introduces a
dataset of tweets collected from all over the world, in multiple languages,
dating back to January 22nd, when the total cases of reported COVID-19 were
below 600 worldwide. The insights presented in this work could help inform
decision makers in the face of future pandemics, and the dataset introduced can
be used to acquire valuable knowledge to help mitigate the COVID-19 pandemic.
</summary>
    <author>
      <name>Christian E. Lopez</name>
    </author>
    <author>
      <name>Malolan Vasu</name>
    </author>
    <author>
      <name>Caleb Gallemore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://github.com/lopezbec/COVID19_Tweets_Dataset</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.10359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.10359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.13907v1</id>
    <updated>2020-03-31T01:51:35Z</updated>
    <published>2020-03-31T01:51:35Z</published>
    <title>A first look at COVID-19 information and misinformation sharing on
  Twitter</title>
    <summary>  Since December 2019, COVID-19 has been spreading rapidly across the world.
Not surprisingly, conversation about COVID-19 is also increasing. This article
is a first look at the amount of conversation taking place on social media,
specifically Twitter, with respect to COVID-19, the themes of discussion, where
the discussion is emerging from, myths shared about the virus, and how much of
it is connected to other high and low quality information on the Internet
through shared URL links. Our preliminary findings suggest that a meaningful
spatio-temporal relationship exists between information flow and new cases of
COVID-19, and while discussions about myths and links to poor quality
information exist, their presence is less dominant than other crisis specific
themes. This research is a first step toward understanding social media
conversation about COVID-19.
</summary>
    <author>
      <name>Lisa Singh</name>
    </author>
    <author>
      <name>Shweta Bansal</name>
    </author>
    <author>
      <name>Leticia Bode</name>
    </author>
    <author>
      <name>Ceren Budak</name>
    </author>
    <author>
      <name>Guangqing Chi</name>
    </author>
    <author>
      <name>Kornraphop Kawintiranon</name>
    </author>
    <author>
      <name>Colton Padden</name>
    </author>
    <author>
      <name>Rebecca Vanarsdall</name>
    </author>
    <author>
      <name>Emily Vraga</name>
    </author>
    <author>
      <name>Yanchen Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.13907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.02443v1</id>
    <updated>2020-05-05T19:08:26Z</updated>
    <published>2020-05-05T19:08:26Z</published>
    <title>A Dataset of Fact-Checked Images Shared on WhatsApp During the Brazilian
  and Indian Elections</title>
    <summary>  Recently, messaging applications, such as WhatsApp, have been reportedly
abused by misinformation campaigns, especially in Brazil and India. A notable
form of abuse in WhatsApp relies on several manipulated images and memes
containing all kinds of fake stories. In this work, we performed an extensive
data collection from a large set of WhatsApp publicly accessible groups and
fact-checking agency websites. This paper opens a novel dataset to the research
community containing fact-checked fake images shared through WhatsApp for two
distinct scenarios known for the spread of fake news on the platform: the 2018
Brazilian elections and the 2019 Indian elections.
</summary>
    <author>
      <name>Julio C. S. Reis</name>
    </author>
    <author>
      <name>Philipe de Freitas Melo</name>
    </author>
    <author>
      <name>Kiran Garimella</name>
    </author>
    <author>
      <name>Jussara M. Almeida</name>
    </author>
    <author>
      <name>Dean Eckles</name>
    </author>
    <author>
      <name>Fabr√≠cio Benevenuto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. This is a preprint version of an accepted paper on ICWSM'20.
  Please, consider to cite the conference version instead of this one</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.02443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.06012v4</id>
    <updated>2021-02-05T22:19:06Z</updated>
    <published>2020-05-02T10:23:27Z</published>
    <title>Mega-COV: A Billion-Scale Dataset of 100+ Languages for COVID-19</title>
    <summary>  We describe Mega-COV, a billion-scale dataset from Twitter for studying
COVID-19. The dataset is diverse (covers 268 countries), longitudinal (goes as
back as 2007), multilingual (comes in 100+ languages), and has a significant
number of location-tagged tweets (~169M tweets). We release tweet IDs from the
dataset. We also develop and release two powerful models, one for identifying
whether or not a tweet is related to the pandemic (best F1=97%) and another for
detecting misinformation about COVID-19 (best F1=92%). A human annotation study
reveals the utility of our models on a subset of Mega-COV. Our data and models
can be useful for studying a wide host of phenomena related to the pandemic.
Mega-COV and our models are publicly available.
</summary>
    <author>
      <name>Muhammad Abdul-Mageed</name>
    </author>
    <author>
      <name>AbdelRahim Elmadany</name>
    </author>
    <author>
      <name>El Moatez Billah Nagoudi</name>
    </author>
    <author>
      <name>Dinesh Pabbi</name>
    </author>
    <author>
      <name>Kunal Verma</name>
    </author>
    <author>
      <name>Rannie Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2005.06012v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06012v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
