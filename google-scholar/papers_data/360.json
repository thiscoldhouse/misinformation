[{"title": "The role of science in the climate change discussions on Reddit", "year": "2025", "pdf_data": "ID: pclm.0000541 \u2014 2025/5/28 \u2014 page 1 \u2014 #1\nPLOS CLIMATE\nOPEN ACCESS\nCitation:CornaleP,TizzaniM,CiullaF,Kalimeri\nK,OmodeiE,PaolottiD,MejovaY(2025)The\nroleofscienceintheclimatechange\ndiscussionsonReddit.PLoSClim4(5):\ne0000541.\nhttps://doi.org/10.1371/journal.pclm.0000541\nEditor:DiogoGuedesVidal,Universityof\nCoimbra,PORTUGAL\nReceived: November06,2024\nAccepted: March21,2025\nPublished: May7,2025\nCopyright: \u00a92025 Cornaleetal.Thisisan\nopenaccessarticledistributedundertheterms\noftheCreativeCommonsAttributionLicense ,\nwhichpermitsunrestricteduse,distribution,\nandreproductioninanymedium,providedthe\noriginalauthorandsourcearecredited.\nData Availability Statement: Thedatausedin\nthisstudycanbeaccessedhere: https://github.\ncom/ymejova/climate_sources_on_reddit .RESEARCH ARTICLE\nThe role of science in the climate change\ndiscussions on Reddit\nPaolo Cornale\n  \n1, Michele Tizzani\n  \n1, Fabio Ciulla\n  \n1, Kyriaki Kalimeri\n  \n1,\nElisa Omodei\n  \n2, Daniela Paolotti\n  \n1, Yelena Mejova\n  \n1\u2217\u2021\n1ISI Foundation, Turin, Italy, 2Department of Network and Data Science, Central European University,\nVienna, Austria\n\u2021 Current Address: Via Chisola 5, ISI Foundation, Turin, Italy 10126\n\u2217yelenamejova@acm.org\nAbstract\nWell-informed collective and individual action necessary to address climate change\nhinges on the public\u2019s understanding of the relevant scientific findings. Social media\nhas been a popular platform for the deliberation around climate change and the policies\naimed at addressing it. Whether such deliberation is informed by scientific findings is an\nimportant step in gauging the public\u2019s awareness of scientific resources and their latest\nfindings. In this study, we examine the use of scientific sources in the course of 14 years\nof public deliberation around climate change on one of the largest social media platforms,\nReddit. We find that only 4.0% of the links in the Reddit posts, and 6.5% in the com-\nments, point to domains of scientific sources, although these rates have been increasing\nin the past decades. These links are dwarfed, however, by the citations of mass media,\nnewspapers, and social media, the latter of which peaked especially during 2019\u20132020.\nFurther, scientific sources are more likely to be posted by users who also post links to\nsources having central-left political leaning, and less so by those posting more polarized\nsources. Scientific sources are not often used in response to links to unreliable sources,\ninstead, other such sources are likely to appear in their comments. This study provides\nthe quantitative evidence of the dearth of scientific basis of the online public debate and\nputs it in the context of other, potentially unreliable, sources of information.\nIntroduction\nClimate change poses a critical threat that requires urgent global action. Despite a broad\nscientific agreement around a strong anthropogenic component of climate change [ 1], as of\n2023, only 56% of US respondents to the Yale Climate Opinion survey thought that \u201cmost\nscientists think global warming is happening\u201d [ 2,3]. Given the importance of public under-\nstanding of the latest scientific findings necessary for informed decision-making, in this study,\nwe examine to what degree scientific resources are used to drive or substantiate the online\ndiscussions around climate change, in comparison to other sources, such as news and social\nmedia, including sources known to be unreliable.\nDespite its privileged status in academia and industry, scientific communication competes\nfor clicks in a cutthroat attention economy of the Web, contending with the fickle proprietary\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 1/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 2 \u2014 #2\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nrecommendation systems and shortening attention spans of their users [ 4]. Further, the per-\nception, understanding, and citation of scientific literature by non-experts depend on a myr-\niad of factors, including numerical literacy [ 5], religious beliefs [ 6] and spirituality [ 7], social\ncontext [ 8], as well as moral rhetoric [ 9] that enforces climate denialism [ 10]. The direct cita-\ntion of science on the Web and social media may then suffer from decreased information\nretention and interest over time [ 11], and may also be replaced with mainstream news media\nreporting as a mediator in the access to scientific news [ 12].Funding:Theauthorsacknowledgesupport\nfromtheLagrangeProjectoftheInstitutefor\nScientificInterchangeFoundation(ISI\nFoundation,toPC,MT,KK,DP,YM)fundedby\nFondazioneCassadiRisparmiodiTorino\n(FondazioneCRT),Italy.Thefundershadno\nroleinstudydesign,datacollectionand\nanalysis,decisiontopublish,orpreparationof\nthemanuscript.\nCompeting interests: Theauthorshave\ndeclaredthatnocompetinginterestsexist.Beyond the traditional mainstream media, social media is becoming an increasingly\nimportant source of information, with Pew Research Center concluding in 2023 that half of\nU.S. adults get news at least sometimes from social media [ 13]. Climate change debate has\nbeen extensively studied on Twitter [ 14,15], yielding observations of homophilous segrega-\ntion of users into like-minded camps of \u201cskeptics\u201d and \u201cactivists\u201d , which can be detected via\nthe posted content [ 16] or network analysis [ 17], and which intensify during events such as\nthe COP Climate Change conference [ 18]. However, Reddit\u2014the fifth most visited website\nin the US [ 19], which is much less studied\u2014has been shown to display much less polariza-\ntion than Twitter [ 20] and may foster more deliberative interactions. The literature is lacking\nin the broad, longitudinal examination of scientific discourse on this platform, instead focus-\ning on particular subreddits such as r/science [ 21,22], or those relevant to the climate change\ndebate, i.e. r/climate or r/climateskeptics [ 23\u201326]. A broad view of all the Reddit communities\nis necessary to capture the diversity and reach of this topic in online discourse.\nIn the US, the debate is further complicated by the politicization of stances around climate\nchange: in 2023, 23% of Republicans considered climate change a major threat, compared\nto 78% of Democrats [ 27]. Indeed, the stated policies of the two parties concerning climate\nchange differ substantially: whereas the Democratic party elites have been consistently sup-\nportive of the climate consensus [ 28], the Republican party, and especially its neoliberal\nchampions, argue that environmentalists in the government \u201cintrude\u201d on society by curtailing\nconsumer choice and property rights [ 29,30]. In fact, the attitudes towards science, in general,\nare different between the two parties: 47% of Republicans view science as benefiting society,\ncompared to 69% of Democrats [ 31]. In this study, we use 14 years of climate change-related\nReddit posts and comments spanning thousands of subreddits to gauge the use of scientific\nresources in this deliberative space, including in the context of political interest. Its results\npoint to a scant, but increasing, utilization of scientific sourcing, more frequently used by\nthose showing center-left political interests. Alas, we find little evidence of it cited in response\nto information from unreliable sources.\nMaterials and methods\nReddit dataset collection\nWe choose a high-precision keyword-based approach to collect a dataset related to Climate\nChange. This method has been used extensively in the literature [ 26,32,33], as it has been\nshown that conversations relevant to climate change happen in many subreddits, most of\nthem not devoted exclusively to this topic. We use the data collected by Pushshift via the\nReddit API [ 34] in the 168 months between January 2009 and December 2022. Using man-\nual examination of the dataset, we compose a set of 64 word bigrams (e.g., \u201cglobal warm-\ning\u201d) that maximize the coverage and minimize false positives (see S1 Table). We first collect\nthe posts and comments that contain at least one of these bigrams in the title (for posts) or\nthe content (posts and comments). We then add to our dataset all comments to the selected\nposts regardless of their match to the keywords list. The matching resulted in 1,301,970 posts\nand 6,428,051 comments, and an additional 15,273,754 comments in response to the posts.\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 2/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 3 \u2014 #3\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nAfter removing duplicates, a total of 20,279,912 comments remain in our dataset. The volume\nof posting increases over time, and peaks around 2019-2020 (see S1 Fig).\nTo assess the relevance of the resulting dataset, all six authors, who are fluent in English,\nmanually labeled a random sample of the submissions and comments that matched keywords\nby using three labels: \u201crelevant, \u201d \u201cpartially relevant\u201d and \u201cnon-relevant. \u201d After labeling 324\nposts, we found that 84.9% were labeled as relevant, 10.5% as partially relevant, and 4.6% as\nnon-relevant. The Cohen\u2019s kappa, computed on a sample of 60 posts, is \ud835\udf05=0.55. After label-\ning 384 comments that matched keywords, 79% were judged to be relevant, 16% as partially\nrelevant, and 5% as non-relevant. The Cohen\u2019s kappa, computed on a sample of 66 comments,\nis\ud835\udf05=0.50. The comments in response to relevant posts were often too short and uninforma-\ntive to be accurately labeled. We assume they are relevant in the discussion, because they are\nanswers to posts mostly considered pertinent.\nSources of information\nAs the focus of this study is the citation of different kinds of information, and specifically\nscience, we consider the URLs shared in the posts and comments that talk about climate\nchange. We disregard URLs pointing to Reddit itself and resolve URLs to web.archive.org\nor archive.is\u2014the two most popular archive services for web pages in our dataset. In\nour study, we consider only the subreddits that have shared at least 10 URLs in the 14\nyears of the dataset. The remaining 7837 subreddits (12.19% of the total) allow us to\nkeep 778,728 (96.83%) URLs shared in the posts, and 2,929,061 (99.00%) URLs of the\ncomments.\nWe then focus on the domains of the extracted URLs and define six categories characteriz-\ning them as sources of information: social media, newspapers, mass media, WikiMedia, gov-\nernmental sources, and scientific sources (see S2 Table for a summary of the categories). We\nuse both external sources to create the lists of domains of interest, as well as examine the top\n100 domains used in our dataset that are not part of any list. By social media, we consider the\nsix most popular ones in our dataset: Twitter, YouTube, Facebook, Instagram, LinkedIn, and\nDiscord (\u201ctwitter.com, \u201d \u201cyoutube.com, \u201d \u201cfacebook.com, \u201d \u201cinstagram.com, \u201d \u201clinkedin.com, \u201d\n\u201cdiscord.gg, \u201d \u201cdiscord.com\u201d). We obtain the list of 4898 newspapers (with their domains) from\nthe media portal Scimago , with the addition of Financial Times \u201cft.com\u201d from the manual\ndomain examination. The list of mass media is taken from the media ranking website All-\nSides , in particular by looking at four types of sources: News Media, Reference, Fact Check,\nand Think Tank/Policy Groups. We do not consider the individual authors. After removing\ndomains already in other categories, the mass media list has 1623 domains. We cleaned this\nlist by removing the sources added to other categories, and added 9 domains from manual\nexamination (\u201chuffingtonpost.com, \u201d \u201cbusinessinsider.com, \u201d \u201cabc.net.au, \u201d \u201cpbs.twimg.com, \u201d\n\u201cmsn.com, \u201d \u201cnews.gallup.com, \u201d \u201cnationalobserver.com, \u201d \u201cctvnews.ca, \u201d \u201coann.com\u201d). The Wiki-\nMedia category contains all the domains from the official webpage of the Wikimedia Founda-\ntion Project ( https://wikimediafoundation.org/our-work/wikimedia-projects/#a2-collections\nhttps://foundation.wikimedia.org/wiki/Home ), adding also \u201cupload.wikimedia.org, \u201d used\nfor access to the media files. In total, there is an amount of 28 domains in this category.\nFor the governmental sources, we consider the domains ending with \u201c.gov, \u201d dropping\nthe 4 that appear in the scientific journal list (\u201ccdc.gov, \u201d \u201cehp.niehs.nih.gov, \u201d \u201cnist.gov, \u201d\n\u201cwwwnc.cdc.gov\u201d) and \u201ceric.ed.gov\u201d that is a preprint domain. In total, this category has 3194\n.gov domains. To these ones, we added 39 domains collected from the official UN website\n(https://www.un.org/en/about-us/un-system ).\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 3/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 4 \u2014 #4\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nFor the scientific sources, we make a distinction between four subcategories: magazines,\njournals, scientific news aggregators, and preprints.\n\u2022Magazines\u2014the mainstream scientific source of information written for the general\nnon-expert public. We obtain the list of the most popular English-language maga-\nzines, with their websites, from Wikipedia ( https://en.wikipedia.org/wiki/Category:\nScience_and_technology_magazines_by_country ) and manual research on the Web.\nAfter removing the few peer-reviewed ones (because they are considered journals), we\ncollected 185 magazine URLs.\n\u2022Journals\u2014a peer-reviewed publication, written by and for experts. We obtain a list\nof journals by scraping the platform Web of Science (https://wosjournal.com/ ). After\nremoving the journals without a URL (generally, they have only the link to the pub-\nlisher, which could be misleading), we manually added different variations of domains,\nresulting in 1943 different domains.\n\u2022Scientific news aggregators\u2014web applications that aggregate scientific or technological\ncontent from different sources, which are not necessarily peer-reviewed. After manual\nresearch on the Web, we collect five: \u201csciencedaily.com, \u201d \u201cphys.org, \u201d \u201ceurekalert.org, \u201d\n\u201cesciencenews.com\u201d and \u201cresearchgate.net. \u201d\n\u2022Preprints\u2014scientific papers published before the peer-review process. We scrape their\ndirectories from the Directory of Open Access Preprint Repositories webpage ( https://\ndoapr.coar-repositories.org/repositories/ ), collecting 83 preprint domains.\nWe make the full list of domains and their categories available to the research\ncommunity ( https://github.com/ymejova/climate_sources_on_reddit ).\nTo supplement our understanding of the quality of the domains, we use previous lit-\nerature and reputable sources to create a list of unreliable domains. For this purpose, we\nuse the Wikipedia Lists of fake news websites ( https://en.wikipedia.org/wiki/List_of_fake_\nnews_websites#Lists ,https://en.wikipedia.org/wiki/List_of_miscellaneous_fake_news_\nwebsites ), Media Bias Fact-Check (MBFC) lists of conspiracy and fake news webistes ( https://\nmediabiasfactcheck.com/conspiracy/ ,https://mediabiasfactcheck.com/fake-news/ ) and pre-\nvious literature on climate change on Reddit [ 23]. Additionally, we use the media ranking\nwebsite All Sides political bias domain labels, merging \u201cright\u201d and \u201ccenter right\u201d into \u201cright\u201d\nlabels and similarly for \u201cleft\u201d .\nFinally, we enrich the domain list with (US-centric) political leaning information from All-\nSides, which provides five labels: left, left-center, center, right-center, and right. For comput-\ning statistics, we merge \u201cright\u201d and \u201ccenter right\u201d into \u201cright\u201d labels and similarly for \u201cleft. \u201d\nOn the other hand, when we compute a political bias score for each subreddit based on the\nURLs appearing in its posts and comments, we assign numerical values to these labels from\n\u20132 to 2 (from \u201cleft\u201d to \u201cright\u201d) and average these scores for each subreddit. We perform the\nsame computation for the users to summarize the political leaning of the URLs they have\nshared in our dataset. To avoid noise due to sparsity (wherein not enough URLs were posted\nby each user), we examine the distribution of these scores. To gauge the sparsity of the dataset,\nwe plot the distribution of the user scores for users having at least tlinks with bias informa-\ntion, where tis a threshold from 1 to 10 (see S2 Fig). We then compute the Jensen-Shannon\nDistance between the pairs of consecutive distributions. We choose as the threshold 5, as the\nfirst time the Jensen-Shannon Distance between the distributions is lower than 0.2 is between\nthresholds 5 and 6. Thus, for further analysis, we consider only the users that shared at least\n5 URLs with a known bias score, resulting in 24,453, which is 14.6% of the users in the whole\ndataset.\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 4/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 5 \u2014 #5\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nModeling scientific URL use\nWe compute the political bias score for the users in a similar way as we have for the subreddits\nand we keep only the users that shared at least 5 biased links. Therefore, we consider 26,620\nusers. In order to find the most relevant attributes related to the sharing of scientific links, we\ndecided to build an explanatory model by focusing on the different number of categories of\ndomains and on the top 100 subreddits (by the number of links) in our dataset. The remain-\ning subreddits are placed in the \u201cother subreddits\u201d variable. We remove every scientific refer-\nence in the design matrix, both in the politically biased links (some scientific sources have a\n\u201ccentral\u201d bias) and in the number of URLs shared on the subreddits. After shuffling the data,\nwe take the logarithm of each of these numeric values (to which 1 was previously added [ 35],\nto compute the logarithm of the zeros) because the data is highly skewed/asymmetrical and\nrun a Random Forest model having the number of scientific links as the dependent variable.\nWe use 3-fold cross-validation to find the best values of the hyperparameters (number of trees\nand their depth) obtaining an average score (i.e., mean accuracy) of 0.63. Finally, we explain\nthe model with SHAP (SHapley Additive exPlanations), a method that uses the Shapley values\nfrom cooperative game theory to explain how the coefficients of the model interact with the\noutput [ 36].\nConditional probability of URL in response\nTo better understand how the different URL categories are used in response to potentially\npolitically biased content, we compute the conditional probabilities as follows. Given a post\nwith a URL of a particular category, we compute the conditional probability that a URL of\nanother category is used in a first-level comment to that post. Note that, for this computa-\ntion, we consider all posts that have at least one URL, and all first-level comments to them\nthat have at least one URL.\nResults\nDomain citation and engagement\nUsing a dataset of 1.3M posts and 20.3M comments on Reddit around the topic of Climate\nChange, we examine the categories of URLs used in this discussion. We were able to catego-\nrize 69.5% of URLs in posts and 55.2% in comments into 6 source types (see the legend of\nFig 1A ). The largest category of URLs cited in the past 14 years is mass media (30.2% in posts\nand 15.9% in comments), followed by newspapers and social media. The latter (social media)\nhas especially peaked during 2019 and 2020 (see Fig 1C and1D). Science-related URLs appear\nin only 4.0% of URLs in posts and 6.5% in comments, though the proportion of science-\nrelated URLs has been increasing in the last decade. These URLs to scientific domains, along\nwith Wikimedia projects (0.1% in posts, 8.8% in comments) and governmental domains\n(0.5%, 5.7%) appear mostly in comments, rather than posts, pointing to their importance in\nthe substantiation of discussion. Among the scientific URLs, journal domains are more likely\nto be cited in comments, in comparison to scientific magazines and aggregators (which may\nhave less rigorous inclusion criteria). Interestingly, preprints, which may contain cutting-\nedge reporting that has not passed peer review, are the least cited scientific domain cate-\ngory, at 0.1% and 0.2% in posts and comments, respectively. The last three statistics shown\ninFig 1A are characteristics of URLs that may overlap with those above: domains known to\npublish unreliable content, and those having a right or left political leaning. Science-related\ndomains are cited at a similar rate to domains known to be unreliable, though these are less\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 5/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 6 \u2014 #6\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nFig 1. Statistics of URL usage in the dataset: (a) proportion of URLs in a particular category, separately for posts (preceded with \u201cP\u201d) and comments (preceded\nwith \u201cC\u201d and dashed), (b) engagement with the posts containing a URL of a particular category in terms of the percentage of posts having at least one comment,\nthe average number of comments for posts having at least one comments, and the average length of the comments in terms of words, (c\u2013d) proportion of URLs in a\nparticular category in posts and comments, over time.\nhttps://doi.org/10.1371/journal.pclm.0000541.g001\nlikely to be used in the comments (5.4% of URLs in posts and 2.0% in comments). Further-\nmore, when the political leaning of a URL is known, it is more likely to be left-leaning than\nright-leaning, pointing to an unequal coverage of the topic. The quality of these URLs is also\ndifferent between the two sides: 55% of right-leaning URLs in posts and 33% in comments\nare listed as unreliable, compared to only 0.01% of left-leaning URLs in posts and 0.09% in\ncomments (echoing findings in [ 37]).\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 6/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 7 \u2014 #7\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nIn terms of engagement, the categories of domains that receive at least one comment are\nthose from Wikimedia, scientific journals, and magazines, followed by the governmental ones\n(seeFig 1B ). Interestingly, despite being a popular domain category, social media receives\ncomparatively fewer comments. However, the average length of the comments, which pro-\nvides another way to measure engagement, remains remarkably stable across the domain cat-\negories, ranging around 47 - 56 words per comment. Whereas left-leaning domains tend to\nreceive more comments compared to right-leaning ones, it is the domains listed as unreliable\nthat receive the longer comments.\nThe distribution of the scientific links among the subreddits is extremely concentrated,\nwith the top 10 subreddits (by the number of scientific links) accounting for 40.2% of\nall science-related URLs in our dataset. These include r/worldnews (contributing 20,451\nscientific URLs, which make up 7.2% of all URLs in that community in our data), r/science\n(11,235, 15.4%), r/environment (9665, 7.2%), r/politics (8995, 3.5%), r/collapse (7646,\n9.0%), r/climate (7140, 9.8%), r/climateskeptics (6764, 8.0%), r/Futurology (6422, 8.3%),\nr/climatechange (5741, 14.4%), and r/AskReddit (4600, 8.4%). Among several communities\naround Climate Change specifically, we also find more general ones, such as r/worldnews\nand r/politics, as well as r/AskReddit, attesting to the mainstream interest in the topic\nand the prevalence of scientific referencing even in non-specialized circles. Interestingly,\nr/climateskeptics, a community dedicated to \u201cQuestioning climate related environmen-\ntalism\u201d (in the description of the subreddit), contributes over 6K links to science-related\ndomains (which are 8.0% of all domains on that subreddit in our dataset, a similar rate\nto r/worldnews and r/climate). Turning to users who have contributed the highest num-\nber of scientific links, at the top we find ILikeNeurons (15,685), worldnews (20,451), Burn-\nerAcc2020 (3254), AutoModerator (2962), ZephirA WT (2926), Jaagsiekte (2412), EcoIn-\nternetNewsfeed (3382), MmmBaconBot (3382), kamjaxx (1093), and avogadros_number\n(1684). These top 10 users contribute 16.6% of all scientific URLs in our data. We find sev-\neral accounts (3/10) that are explicitly bots: AutoModerator ,EcoInternetNewsfeed , and\nMmmBaconBot , echoing previous findings on the prevalence of bots on the platform [ 38].\nHowever, from the behavior of the other accounts, it is likely that they are also at least par-\ntially automated, such as the ILikeNeurons . This finding suggests that there is a high num-\nber of bots on Reddit, something that was already noted in previous literature [ 38]. These\nbots, however, are not necessarily malicious, as they are explicit about their nature, and\nfollow the \u201cbottiquette\u201d ( https://www.reddit.com/wiki/bottiquette/ ) [39,40]. Given the\nimportance of (semi-)automated accounts, we do not attempt to remove them from our\ndataset.\nPolitical leaning and scientific citation\nNext, we consider users whose link posting activity puts them into the left, center-left, center-\nright, or right political leaning (as defined by the media bias ratings website AllSides.com).\nWe find that those in the center (especially center-left) are more likely to cite science than\nthose in the extremes (see leftmost panel of Fig 2 , and S3 Table). In particular, out of all\nURLs posted by the center-left users, on average, 7.4% are science-related, whereas 3.3% of\nthe ones posted by center-right users (compared to 2.2% and 0.8% for left and right, respec-\ntively). Due to the large dataset sizes, we find all of the within-category differences between\nthe consecutive groups statistically significant at p<0.001, despite the small effect size, except\nin the case of social media (which all groups of users use at roughly indistinguishable rates).\nAmong the other categories, we find right-leaning users to favor mass media, whereas\nleft, and center-left users \u2013 newspapers. The most striking difference between the groups,\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 7/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 8 \u2014 #8\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nFig 2. Distributions of the proportion of URLs posted by a user that are from a particular category, grouped by users whose URLs have a particular polit-\nical leaning. Under each distribution, the mean proportion is shown, and a * is shown between two consecutive groups if their distributions differ using the\nKolmogorov-Smirnov two-sample test at p<0.001.\nhttps://doi.org/10.1371/journal.pclm.0000541.g002\nhowever, is the posting of unreliable sources, which are more likely to be on the right political\nspectrum.\nTo understand the importance of political leaning in comparison with other climate\nchange-related interests of the users, we train a model to predict how many (log-normalized)\nscientific URLs a user posts in our dataset. Fig 3 shows the SHapley Additive exPlanations\n(SHAP) graph, a game theoretical approach developed to explain the contribution of each fea-\nture to the final output of a ML model [ 36]. We find that posting such URLs is associated with\nposting on popular scientific subreddits such as r/science, r/collapse, and r/environment, and,\nechoing previous results, with posting links with a center-left political leaning. Further, post-\ning scientific URLs is more likely by users who post many kinds of URLs in comments, this\nrelationship is reversed for social media \u2013 those posting social media links in their posts are\nlesslikely to also post science.\nFinally, we explore the relationship between citations in posts and comments (see Fig 4 ).\nWe find that posts with scientific URLs are much more likely to be responded to with other\nscientific URLs (26% of the time). Unfortunately, this is not the case for the posts sharing a\nlink to an unreliable source: only 5% of URLs in reply to these are to science sources, instead,\nlinks from social media, mass media, and\u2014more likely\u2014other unreliable sources are posted\nin response. Similarly, posts having right or left-leaning URLs have about the same chance,\naround 5%, of having a scientific URL in response.\nIn summary, when considering individual users and the political leaning of other sources\nthey cite, we do not find a strong polarization in terms of using scientific sources, with those\non center-left being more likely to post science than on center-right. However, the defining\nfeature of Reddit are the community, many of which can revolve around a topic or political\ntheme, and which may have their own cultures beyond individual users. We explore these\nnext.\nCase study\nTo delve further into the nature of the Reddit communities (subreddits), we select sev-\neral subreddits out of those contributing the most posts to our dataset such that they span\na variety of points of view: r/climate and r/climateskeptics (largest climate-related ones);\nr/science, r/worldnews, and r/politics (more general ones); and r/The_Donald (dedicated\nto the Republican politician Donald Trump), and r/SandersForPresident (supporting the\nIndependent/Democrat politician Bernie Sanders). Note that, besides having the largest\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 8/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 9 \u2014 #9\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nFig 3. SHapley Additive exPlanations (SHAP), a game theoretical approach for explaining the contribution of each\nfeature to the final output of a ML model. The Random Forest model predicts how many (log-normalized) science-related\nURLs a user has posted in our dataset (here, \u201chigh\u201d means more URLs were posted), using behavioral features including the\ncategories of the other URLs they shared, as well as the political leaning of those URLs. Top 100 most popular subreddits are\nused as features, and all others are summed in \u201cother subreddits. \u201d\nhttps://doi.org/10.1371/journal.pclm.0000541.g003\nshare of posts in our data among politically-oriented subreddits, Donald Trump and\nBernie Sanders represent political extremes of the right and left, respectively. See Supple-\nmentary Table S4 for statistics on the sizes of these subreddits. Fig 5 shows (a) the per-\ncentage of URLs having a particular domain category and (b) the percentage of URLs\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 9/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 10 \u2014 #10\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nFig 4. Conditional probability of a first-level comment with a URL containing a certain URL category (columns),\ngiven it is in response to a post having a scientific, unreliable, right- or left-leaning URL (rows).\nhttps://doi.org/10.1371/journal.pclm.0000541.g004\nFig 5. Case study of select subreddits , (a) the percentage of URLs having a particular domain category and ( b) the percentage of URLs having a particular political\nleaning. Statistics are shown separately for posts (P) and comments (C).\nhttps://doi.org/10.1371/journal.pclm.0000541.g005\nhaving a particular political leaning. We find a greater variety of URLs in the comments\nthan in posts, such that we are able to identify the category of fewer URLs in comments.\nAgain, we find science, Wikipedia, and governmental links to often appear in the com-\nments more than in posts. Also, the posts more often contain newspapers, mass, and social\nmedia links than the comments. Instead, the comments more often cite Wikipedia, gov-\nernmental and science links, suggesting these are important sources of argumentation.\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 10/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 11 \u2014 #11\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nUnsurprisingly, science links are mentioned the most in the r/science subreddit, but also\nin the r/climate and r/worldnews. Neither of the political subreddits do not share ref-\nerences to science, suggesting that the political discussion of the subject is not explic-\nitly supported by direct scientific references. In the case of r/climateskeptics, few sci-\nence URLs are included in posts, but many more are cited in the comments. The sub-\nreddit with the most right-leaning URLs is r/The_Donald, with the posts having many\nmore right-leaning URLs than comments (see Fig 5B ). We see a similar behavior in\nr/climateskeptics. The URLs in the rest of the selected subreddits are leaning to the left,\nwith r/SandersForPresident and r/climate ones being the most left-leaning, which is sur-\nprising, since r/climate is ostensibly not a political subreddit. In summary, our case study\nsuggests that, despite the Climate Change debate being highly politicized, the use of\nscientific evidence is lacking in the communities centered around politics, and instead is\nmore prevalent in scientific and even in science-skeptic communities (especially in their\ncomments).\nDiscussion\nThomas Jefferson is often attributed the (likely apocryphal) quote \u201c An educated citizenry is\na vital requisite for our survival as a free people\u201d [ 41]. Since then, the connection between\ndemocratic deliberation and scientific education has been promoted by educators and reform-\ners, such as John Dewey in How We Think [42], and more recently by the U.S. National\nResearch Council, positing that \u201cknowledge of science and engineering is required to engage\nwith the major public policy issues of today\u201d [ 43]. Our findings show that, in the Reddit dis-\ncussions of climate change, scientific sources have been dwarfed by links to news and social\nmedia, although the share of links to scientific resources has increased in the past decade.\nWhen they do appear, they are more likely to be in the comments (along with the links to\ngovernmental sources and Wikipedia), pointing to the importance of these resources to the\ndeliberative process around this topic. Unfortunately, we find that scientific links are much\nmore likely to be posted in response to posts with other scientific links, whereas posts having\nlinks to unreliable sources do not often receive scientific links in their replies. Instead, other\nunreliable sources are more likely to be cited.\nMeanwhile, surveys show that between 2009 and 2019 (roughly in the duration of the\nexamined data), the share of US respondents who acknowledge an increase in average\nglobal temperature rose by 8 percentage points, and the share who believe that humans\nhave contributed to this rose by 11 percentage points [ 44]. Whether the use of scientific\nresources contributed to this change of opinion is questionable. Experimental results sug-\ngest articles linking to scientific papers promote greater trust, however linking to any\nmainstream media may have the same effect [ 45]. We find that scientific links appear not\nonly in communities asserting the existence of anthropogenic climate change, but also in\nthose \u201cskeptical\u201d of it. According to the 2024 report by the Center for Countering Digi-\ntal Health, climate change denialism has evolved in the past few years, as global tempera-\ntures rose dramatically [ 46]. Instead of opposing the concept of climate change itself, the\n\u201cNew Denial\u201d themes include \u201cthe impacts of global warming are beneficial or harmless\u201d ,\n\u201cclimate solutions won\u2019t work\u201d and \u201cclimate science and the climate movement are unreli-\nable\u201d . The report points specifically to social media companies (including Instagram, Face-\nbook, TikTok, and X (previously Twitter)) as potentially benefiting from the popularity\nof such content, and allowing for the monetization and direct profit for its creators. What\nrole the latest scientific evidence may play in tackling these New Denial themes is an open\nquestion.\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 11/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 12 \u2014 #12\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nHowever, the mere presence of scientific evidence may not necessarily correspond\nto a cross-partisan conversation. Psychology literature suggests that individuals more\nknowledgeable on an issue are more susceptible to selection bias and motivated reasoning\n[47\u201349]. Our discovery that 9.4% of the URLs in r/climateskeptics community\u2019s comments are\nscientific links points to the active use of science in the community. Previous studies found\nthat, for instance, vaccine skeptics often express respect for the scientific method and are\ninterested in the rigorous scientific examination of matters affecting them personally [ 50].\nFurther, the level of one\u2019s education may affect the trust in climate science via the perception\nof having a lower or higher social status [ 51]. To some, attitudes towards climate science may\nbe \u201cnot just an opinion on an issue, but [an] aspect of self that defines who they are, what they\nstand for, and who they stand with (and against)\u201d [ 52]. Thus, the citation of science may be a\npart of the construction of self-evaluation as \u201ceco-habitus\u201d , a concept favoring environmental\nactions and engagement in a \u201cgreen\u201d lifestyle [ 53]. [51] found that such a subjective view of\none\u2019s social status may contribute to the distrust in climate science. What role the citation of\nscientific literature plays in the individuals\u2019 formation of a self-image is an interesting future\nresearch direction.\nIn the U.S., a major aspect of such self-image may be one\u2019s political affiliation. The stated\npolicies of the two parties concerning climate change differ substantially: whereas the\nDemocratic party elites have been consistently supportive of the climate consensus [ 28], the\nRepublican party, and especially its neoliberal champions, argue that environmentalists in the\ngovernment \u201cintrude\u201d on society by curtailing consumer choice and property rights [ 29,30].\nFurthermore, concerning science and academia, in the past decade, there has been a sharp\ndecline among Republicans of those who \u201cbelieve that colleges and universities have a \u201cpos-\nitive effect\u201d on the country\u201d [ 54]. However, in our case study, we find that both the Republi-\ncan (r/The_Donald) and Democrat/Independent (r/SandersForPresident) community had a\nnegligible number of links to scientific sources. The little scientific citation that does circulate\nin politically-oriented discussions may be influenced by the communiqu\u00e9s of NGOs, think\ntanks, and government reports (papers cited by such reports are more likely to be highly cited\n[55]), each bringing its own agenda. Conversely, the perception of a scientific source may be\naffected by the political stances of its editors [ 56]. Instead, social media dominates these com-\nmunities\u2019 links (21% in posts on r/The_Donald and 40% on r/SandersForPresident). As the\nmajor social media platforms have been thoroughly documented for spreading scientific mis-\ninformation [ 57,58], and some smaller ones boasting even more permissive policies [ 59], the\nextensive use of these as a resource for policy and science discussions is highly concerning\nand should be further investigated.\nConclusion\nIn this study, we quantify the sources of information that are used on one of the largest\nsocial media platforms, Reddit, when discussing climate change, with a particular focus\non direct links to scientific resources. We find that, although over the past 10 years such\nlinks have been used at an increasing rate, on average they constitute 4% of all the links\nfound in relevant posts. These findings point to the lack of scientific communication reach\ninto the public discourse around this topic, despite the importance of the public\u2019s under-\nstanding of the scientific findings and consensus around climate. We hope this study moti-\nvates further research into the characteristics of successful scientific communication, and\ninto the role of mainstream and social media as intermediaries of scientific findings to the\npublic.\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 12/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 13 \u2014 #13\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nSupporting information\nS1 Table. Keywords regarding climate change. A set of 64 key phrases used to retrieve\nReddit posts and comments relevant to the topic of climate change.\n(PDF)\nS2 Table. URL domain categories and their sources. Summary of the domain categories and\ntheir sources. Those above double-line are mutually exclusive, whereas those below (unreli-\nable and political leanings) are not.\n(PDF)\nS1 Fig. Volume of Reddit posts and comments around climate change. Temporal statistics\nof the dataset (both posts and comments), aggregated by month. The left figure shows the raw\nnumber of posts and comments, and the right one shows the proportion of these posts and\ncomments of the whole Reddit posting volume.\n(TIFF)\nS2 Fig. Link-based political leaning of users. Distribution of link-based political leaning\nscore for users having at least tlinks with political bias information, where tis a threshold\nfrom 1 to 10.\n(TIFF)\nS3 Table. URL sharing behavior of users with political leaning. Mean, median, and stan-\ndard deviation of the proportion of URLs of a certain category posted by users having a spe-\ncific political leaning (identified via other URLs they have shared).\n(PDF)\nS4 Table. Detailed description of the subreddits in the case study. For each subreddit in the\ncase study, the table shows its description (from Reddit), its membership in terms of users,\nnumber of posts and comments in our dataset, and the number of URLs present in these. All\nstatistics were collected in April 2024, except those for The_Donald, which was banned at the\ntime, for which its Wikipedia page was used: https://en.wikipedia.org/wiki/R/The_Donald .\n(PDF)\nAuthor contributions\nConceptualization: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri,\nElisa Omodei, Daniela Paolotti, Yelena Mejova.\nData curation: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei,\nDaniela Paolotti.\nFormal analysis: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei,\nDaniela Paolotti, Yelena Mejova.\nFunding acquisition: Paolo Cornale, Michele Tizzani, Kyriaki Kalimeri, Elisa Omodei,\nDaniela Paolotti, Yelena Mejova.\nInvestigation: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei,\nDaniela Paolotti, Yelena Mejova.\nMethodology: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei,\nDaniela Paolotti, Yelena Mejova.\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 13/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 14 \u2014 #14\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\nProject administration: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri,\nElisa Omodei, Daniela Paolotti, Yelena Mejova.\nResources: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei,\nDaniela Paolotti, Yelena Mejova.\nSoftware: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei,\nDaniela Paolotti.\nSupervision: Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei, Daniela Paolotti,\nYelena Mejova.\nValidation: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei,\nDaniela Paolotti, Yelena Mejova.\nVisualization: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei,\nDaniela Paolotti.\nWriting \u2013 original draft: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri,\nElisa Omodei, Daniela Paolotti, Yelena Mejova.\nWriting \u2013 review & editing: Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri,\nElisa Omodei, Daniela Paolotti, Yelena Mejova.\nReferences\n1.Abbass K, Qasim MZ, Song H, Murshed M, Mahmood H, Younis I. A review of the global climate\nchange impacts, adaptation, and sustainable mitigation measures. Environ Sci Pollut Res Int.\n2022;29(28):42539\u201359. http://dx.doi.org/10.1007/s11356-022-19718-6 PMID: 35378646\n2.Marlon JR, Wang X, Bergquist P, Howe PD, Leiserowitz A, Maibach E, et al. Change in US\nstate-level public opinion about climate change: 2008\u20132020. Environ Res Lett. 2022;17(12):124046.\nhttp://dx.doi.org/10.1088/1748-9326/aca702\n3.Marlon J, Goddard E, Howe P, Mildenberger M, Jefferson M, Fine E, et al. Yale Climate Opinion\nMaps 2023; 2023. Available from:\nhttps://climatecommunication.yale.edu/visualizations-data/ycom-us/\n4.Hyland K. Academic publishing and the attention economy. J Engl Acad Purp. 2023;64:101253.\nhttps://doi.org/10.1016/j.jeap.2023.101253\n5.Roozenbeek J, Schneider CR, Dryhurst S, Kerr J, Freeman AL, Recchia G, et al. Susceptibility to\nmisinformation about COVID-19 around the world. R Soc Open Sci. 2020;7(10):201199.\nhttps://doi.org/10.1098/rsos.201199. PMID: 33204475\n6.Schwarz J, Sikkink D. Blinded by religion? Religious school graduates and perceptions of science in\nyoung adulthood. Cardus Religious Schools Initiative; 2016.\n7.Browne M, Thomson P, Rockloff MJ, Pennycook G. Going against the herd: psychological and\ncultural factors underlying the \u2018vaccination confidence gap\u2019. PLoS One. 2015;10(9):e0132562.\nhttps://doi.org/10.1371/journal.pone.0132562 PMID: 26325522\n8.Lewandowsky S, Cook J, Fay N, Gignac GE. Science by social media: attitudes towards climate\nchange are mediated by perceived social consensus. Mem Cognit. 2019;47(8):1445\u201356.\nhttp://dx.doi.org/10.3758/s13421-019-00948-y PMID: 31228014\n9.D\u2019Ignazi J, Kalimeri K, Beir\u00f3 MG. Beyond sentiment: examining the role of moral foundations in user\nengagement with news on Twitter. arXiv. preprint. arXiv:250212009, 2025.\n10.Ramos R, Vaz P, Rodrigues MJ. Climate Denialism on social media: qualitative analysis of\ncomments on Portuguese newspaper Facebook pages. Psychol Int. 2025;7(1):6.\nhttps://doi.org/10.3390/psycholint7010006\n11.Hwang S, Horv\u00e1t E, Romero DM. Information retention in the multi-platform sharing of science. In:\nProceedings of the International AAAI Conference on Web and Social Media. vol. 17; 2023.\npp. 375\u201386. https://doi.org/10.1609/icwsm.v17i1.22153\n12.Takahashi B, Tandoc Jr EC. Media sources, credibility, and perceptions of science: learning about\nhow people learn about science. Public Underst Sci. 2016;25(6):674\u201390.\nhttps://doi.org/10.1177/0963662515574986. PMID: 25792288\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 14/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 15 \u2014 #15\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\n13.Pew Research. Social media and news fact sheet; 2023. Available from:\nhttps://www.pewresearch.org/journalism/fact-sheet/social-media-and-news-fact-sheet/\n14.Effrosynidis D, Karasakalidis AI, Sylaios G, Arampatzis A. The climate change Twitter dataset.\nExpert Syst Appl. 2022;204:117541. https://doi.org/10.1016/j.eswa.2022.117541\n15.Karimiziarani M, Shao W, Mirzaei M, Moradkhani H. Toward reduction of detrimental effects of\nhurricanes using a social media data analytic approach: how climate change is perceived? Clim\nRisk Manage. 2023;39:100480. https://doi.org/10.1016/j.crm.2023.100480\n16.Upadhyaya A, Fisichella M, Nejdl W. A multi-task model for sentiment aided stance detection of\nclimate change tweets. In: Proceedings of the International AAAI Conference on Web and Social\nmedia. vol. 17, 2023. pp. 854\u201365.\n17.Williams HT, McMurray JR, Kurz T, Lambert FH. Network analysis reveals open forums and echo\nchambers in social media discussions of climate change. Global Environ Change. 2015;32:126\u201338.\nhttps://doi.org/10.1016/j.gloenvcha.2015.03.006\n18.Falkenberg M, Galeazzi A, Torricelli M, Di Marco N, Larosa F, Sas M, et al. Growing polarization\naround climate change on social media. Nat Clim Chang. 2022;12(12):1114\u201321.\nhttps://doi.org/10.1038/s41558-022-01527-x\n19.Similarweb. Top websites ranking; 2024. Available from:\nhttps://www.similarweb.com/top-websites/united-states/\n20.De Francisci Morales G, Monti C, Starnini M. No echo in the chambers of political interactions on\nReddit. Sci Rep. 2021;11:2818. https://doi.org/10.1038/s41598-021-81531-x\n21.Moriarty D, Mehlenbacher AR. Reddit and engaged science communication online: An examination\nof reddit\u2019s R/science ask-me-anythings and science discussion series. Tech Commun Q.\n2024;33(1):54\u201370. http://dx.doi.org/10.1080/10572252.2023.2194676\n22.Hara N, Abbazio J, Perkins K. An emerging form of public engagement with science: ask me\nanything (AMA) sessions on Reddit r/science. PLoS One. 2019;14(5):e0216789.\nhttp://dx.doi.org/10.1371/journal.pone.0216789. PMID: 31091264\n23.Gadanidis T. The discourse of climate change on Reddit. Toronto Working Papers in Linguistics.\n2020. Available from: https://twpl.library.utoronto.ca/index.php/twpl/article/view/41010\n24.Oswald L, Bright J. How do climate change skeptics engage with opposing views online? Evidence\nfrom a major climate change skeptic forum on Reddit. Environ Commun. 2022;16(6):805\u201321.\nhttp://dx.doi.org/10.1080/17524032.2022.2071314\n25.Villanueva II. Climate change frames and emotional responses on Reddit. University of Arkansas;\n2021.\n26.Parsa MS, Shi H, Xu Y, Yim A, Yin Y, Golab L. Analyzing climate change discussions on Reddit. In:\n2022 International Conference on Computational Science and Computational Intelligence (CSCI).\nIEEE; 2022. pp. 826\u201332.\n27.Pew Research. What the data says about Americans\u2019 views of climate change; 2023. Available\nfrom: https://www.pewresearch.org/short-reads/2023/08/09/\nwhat-the-data-says-about-americans-views-of-climate-change/\n28.Merkley E, Stecula DA. Party cues in the news: democratic elites, Republican backlash, and the\ndynamics of climate skepticism. Br J Polit Sci. 2021;51(4):1439\u201356.\nhttp://dx.doi.org/10.1017/S0007123420000113\n29.Antonio RJ, Brulle RJ. The unbearable lightness of politics: climate change denial and political\npolarization. Sociol Q. 2011;52(2):195\u2013202. https://doi.org/10.1111/j.1533-8525.2011.01199.x\n30.Smith EK, Bognar MJ, Mayer AP. Polarisation of climate and environmental attitudes in the United\nStates, 1973-2022. npj Clim Action. 2024;3(1):2. https://doi.org/10.1038/s44168-023-00074-1\n31.Viswanathan G. Americans\u2019 trust in science declining, Pew survey says; 2023. Available from:\nhttps://edition.cnn.com/2023/11/14/health/trust-in-science-pew-survey/index.html\n32.Treen K, Williams H, O\u2019Neill S, Coan TG. Discussion of climate change on Reddit: polarized\ndiscourse or deliberative debate? Environ Commun. 2022;16(5):680\u201398.\nhttps://doi.org/10.1080/17524032.2022.2050776\n33.Shah M, Seraj S, Pennebaker JW. Climate denial fuels climate change discussions more than local\nclimate-related disasters. Front Psychol. 2021;12:682057.\nhttps://doi.org/10.3389/fpsyg.2021.682057 PMID: 34512442\n34.Baumgartner J, Zannettou S, Keegan B, Squire M, Blackburn J. The pushshift reddit dataset. In:\nProceedings of the International AAAI Conference on Web and Social Media. vol. 14; 2020. pp.\n830\u20139.\n35.Pither J. Introduction to biostatistics; 2023. Available from: https://ubco-biology.github.io/BIOL202/\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 15/ 16\nID: pclm.0000541 \u2014 2025/5/28 \u2014 page 16 \u2014 #16\nPLOS CLIMATE The role of science in the climate change discussions on Reddit\n36.Lundberg SM, Lee SI. A unified approach to interpreting model predictions. In: Guyon I, Luxburg\nUV, Bengio S, Wallach H, Fergus R, Vishwanathan S, et al., editors. Advances in Neural\nInformation Processing Systems 30; 2017. pp. 4765\u201374.\n37.Robertson RE, Green J, Ruck DJ, Ognyanova K, Wilson C, Lazer D. Users choose to engage with\nmore partisan news than they are exposed to on Google search. Nature. 2023;618(7964):342\u20138.\nhttps://doi.org/10.1038/s41586-023-06078-5 PMID: 37225979\n38.Ferrara E, Varol O, Davis C, Menczer F, Flammini A. The rise of social bots. Commun ACM.\n2016;59(7):96\u2013104. https://doi.org/10.1145/2818717\n39.Hurtado S, Ray P, Marculescu R. Bot detection in reddit political discussion. In: Proceedings of the\nFourth International Workshop on Social Sensing; 2019. pp. 30\u201335.\n40.Ma MC, Lalor JP. An empirical analysis of human-bot interaction on reddit. In: Proceedings of the\nSixth Workshop on Noisy User-generated Text (w-NUT 2020); 2020. pp. 101\u20136.\n41.Bowman J. Thomas Jefferson & American education; 2019. Available from:\nhttps://www.proedtn.org/news/339466/Thomas-Jefferson--American-Education.htm\n42.Dewey J. How we think. Heath & Co Publishers; 1933.\n43.National Research Council. A framework for K-12 science education: practices, crosscutting\nconcepts, and core ideas. Washington, DC: National Academies Press; 2012.\n44.Carlsson F, Kataria M, Krupnick A, Lampi E, L\u00f6fgren \u00c5, Qin P, et al. The climate decade: Changing\nattitudes on three continents. J Environ Econ Manage. 2021;107:102426.\nhttps://doi.org/10.1016/j.jeem.2021.102426\n45.Verma N, Fleischmann KR, Koltai KS. Human values and trust in scientific journals, the mainstream\nmedia and fake news. Proc Assoc Inf Sci Technol. 2017;54(1):426\u201335.\nhttps://doi.org/10.1002/pra2.2017.14505401046\n46.Center for Countering Digital Health. The new climate denial. Center for Countering Digital Health;\n2024. Available from:\nhttps://counterhate.com/wp-content/uploads/2024/01/CCDH-The-New-Climate-Denial_FINAL.pdf\n47.Bushey B, Farady M, Perkins D. Everyday reasoning and the roots of intelligence. In: Voss JF,\nPerkins DN, Segal JW, editors, Informal reasoning and education. Lawrence Erlbaum Associates,\nInc.; 1991, pp. 83\u2013-105.\n48.Hannon M. Are knowledgeable voters better voters? Polit Philos Econ. 2022;21(1):29\u201354.\nhttps://doi.org/10.1177/1470594X211065080\n49.Kahan DM. Ideology, motivated reasoning, and cognitive reflection. Judgm Decis Mak.\n2013;8(4):407\u201324. https://doi.org/10.1017/S1930297500005271\n50.Koltai KS, Fleischmann KR. Questioning science with science: the evolution of the vaccine safety\nmovement. Proc Assoc Inf Sci Technol. 2017;54(1):232\u201340.\nhttps://doi.org/10.1002/pra2.2017.14505401026\n51.Hoekstra AG, Noordzij K, de Koster W, van der Waal J. The educational divide in climate change\nattitudes: understanding the role of scientific knowledge and subjective social status. Glob Environ\nChange. 2024;86:102851. https://doi.org/10.1016/j.gloenvcha.2024.102851\n52.Bliuc AM, McGarty C, Thomas EF, Lala G, Berndsen M, Misajon R. Public division about climate\nchange rooted in conflicting socio-political identities. Nat Clim Change. 2015;5(3):226\u20139.\nhttps://doi.org/10.1038/nclimate2507\n53.Kennedy EH, Givens JE. Eco-habitus or eco-powerlessness? Examining environmental concern\nacross social class. Sociol Perspect. 2019;62(5):646\u201367.\nhttps://doi.org/10.1177/0731121419836966\n54.Atkeson LR, Taylor AJ. Partisan affiliation in political science: insights from Florida and North\nCarolina. PS Polit Sci Polit. 2019;52(4):706\u201310. https://doi.org/10.1017/S1049096519000647\n55.Bornmann L, Haunschild R, Boyack K, Marx W, Minx JC. How relevant is climate change research\nfor climate change policy? An empirical analysis based on Overton data. PLoS One.\n2022;17(9):e0274693. https://doi.org/10.1371/journal.pone.0274693 PMID: 36137101\n56.Zhang FJ. Political endorsement by Nature and trust in scientific expertise during COVID-19. Nat\nHum Behav. 2023;7(5):696\u2013706. https://doi.org/10.1038/s41562-023-01537-5 PMID: 36941467\n57.Osman W, Mohamed F, Elhassan M, Shoufan A. Is YouTube a reliable source of health-related\ninformation? A systematic review. BMC Med Educ. 2022;22(1):382.\nhttps://doi.org/10.1186/s12909-022-03446-z PMID: 35590410\n58.Do Nascimento IJB, Pizarro AB, Almeida JM, Azzopardi-Muscat N, Gon\u00e7alves MA, Bj\u00f6rklund M,\net al. Infodemics and health misinformation: a systematic review of reviews. Bull World Health\nOrgan. 2022;100(9):544. https://doi.org/10.2471/BLT.21.287654 PMID: 36062247\n59.B\u00e4r D, Pr\u00f6llochs N, Feuerriegel S. New threats to society from free-speech social media platforms.\nCommun ACM. 2023;66(10):37\u201340. https://doi.org/10.1145/3587094\nPLOS Climate https://doi.org/10.1371/journal.pclm.0000541 May 7, 2025 16/ 16", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "The role of science in the climate change discussions on Reddit", "author": ["P Cornale", "M Tizzani", "F Ciulla", "K Kalimeri"], "pub_year": "2025", "venue": "PLoS \u2026", "abstract": "Well-informed collective and individual action necessary to address climate change hinges  on the public\u2019s understanding of the relevant scientific findings. Social media has been a"}, "filled": false, "gsrank": 561, "pub_url": "https://journals.plos.org/climate/article?id=10.1371/journal.pclm.0000541", "author_id": ["", "iXfwdp4AAAAJ", "whAXlGoAAAAJ", "3RnBcXAAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:fuC9T16eh8YJ:scholar.google.com/&output=cite&scirp=560&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D560%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=fuC9T16eh8YJ&ei=bLWsaO2lAY6IieoP0sKRuAk&json=", "num_citations": 3, "citedby_url": "/scholar?cites=14305576869267628158&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:fuC9T16eh8YJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://journals.plos.org/climate/article/file?id=10.1371/journal.pclm.0000541&type=printable"}}, {"title": "Understanding the (in) effectiveness of content moderation: A case study of facebook in the context of the us capitol riot", "year": "2023", "pdf_data": "Understanding the (In)Effectiveness of Content Moderation:\nA Case Study of Facebook in the Context of the U.S. Capitol Riot\u2217\nIan Goldstein\nNew York University\nUSALaura Edelson\nNew York University\nUSAMinh-Kha Nguyen\nUniversit\u00e9 Grenoble Alpes, CNRS,\nInria, Grenoble INP\nFrance\nOana Goga\nCNRS, Inria, Institut Polytechnique de\nParis\nFranceDamon McCoy\nNew York University\nUSATobias Lauinger\nNew York University\nUSA\nABSTRACT\nSocial media networks commonly employ content moderation as\na tool to limit the spread of harmful content. However, the effi-\ncacy of this strategy in limiting the delivery of harmful content to\nusers is not well understood. In this paper, we create a framework\nto quantify the efficacy of content moderation and use our met-\nrics to analyze content removal on Facebook within the U.S. news\necosystem. In a data set of over 2\ud835\udc40posts with 1.6\ud835\udc35user engage-\nments collected from 2,551 U.S. news sources before and during\nthe Capitol Riot on January 6, 2021, we identify 10,811 removed\nposts. We find that the active engagement life cycle of Facebook\nposts is very short, with 90 % of all engagement occurring within\nthe first 30 hours after posting. Thus, even relatively quick inter-\nvention allowed significant accrual of engagement before removal,\nand prevented only 21 % of the predicted engagement potential\nduring a baseline period before the U.S. Capitol attack. Nearly a\nweek after the attack, Facebook began removing older content, but\nthese removals occurred so late in these posts\u2019 engagement life\ncycles that they disrupted less than 1 % of predicted future engage-\nment, highlighting the limited impact of this intervention. Content\nmoderation likely has limits in its ability to prevent engagement,\nespecially in a crisis, and we recommend that other approaches\nsuch as slowing down the rate of content diffusion be investigated.\n1 INTRODUCTION\nSocial media companies such as Google (YouTube) and Meta (Face-\nbook) have become inextricably linked to contemporary civil soci-\nety [ 34] through their communication and content sharing \u201cplat-\nforms.\u201d Billions of users rely on these services to read or comment\non the news, entertain themselves, and conduct business. Despite\nstated intentions [ 19] and the commonly used term \u201cplatform,\u201d most\nsocial media companies are not impartial channels for users\u2019 com-\nmunications. On the one hand, social media companies set the\noperating parameters of algorithms that promote \u201cinteresting\u201d con-\ntent to users. On the other hand, when it is deemed necessary to\nretain users [ 30] or to survive public scrutiny [ 67], social media\ncompanies may intervene against extreme or offensive material.\nContent moderation is currently the primary strategy that so-\ncial media companies employ to counter the spread of harmful\n\u2217Version 1 (January 2023): Non-peer-reviewed technical report.\nVersion 2 (February 2023): Added related work, acknowledgments, and authors.content [ 5,25]. Nearly all services, including venues for adult (On-\nlyFans [ 49]) or extreme (8kun [ 1]) content, have established policies\nthat dictate what they are willing to host; content deemed incon-\nsistent with those policies may be removed [ 24]. Public debate has\nquestioned the adequacy of these policies [ 68] (i.e., which content\nshould be deleted) or surfaced instances of objectionable content\nthat was not removed [60]. Prior research studied potential biases\nin moderation of YouTube comments [ 42,43], the impact of content\nmoderation on user behavior on Reddit [ 41,59], and the magnitude\nof content moderation regarding third-party links to conspiracy\ntheory stories [ 50]. It is still an open question how quickly content\nis removed, independent of the specific policy. We argue that to\njudge the effectiveness of content moderation, it is important to\nunderstand the spread of content until the time of deletion, i.e.,\nwhether content is removed before it reaches a large audience.\nIn this paper, we present what we believe to be the first mea-\nsurement study of the speed and impact of content moderation on\nFacebook, both under normal times and during a crisis. We were\nlucky to obtain from Edelson et al. [ 18] a data set of public Facebook\nposts from 2,551 U.S. news publishers and \u201cinfluencers\u201d incidentally\ncollected around the time of the U.S. Capitol Riot on January 6,\n2021. Because Facebook currently does not make any post-level\ndata transparent about content moderation, we develop a novel\nmethodology to infer content removals and their approximate tim-\ning from daily observations of active posts. For our analysis, we\ndevelop novel metrics to estimate the impact of content removal\non user engagement, that is, the combined number of comments,\nshares, and reactions such as \u201clike.\u201d We do so both in terms of\npast engagement that was allowed to occur because of the time\nit took to remove a post, and in terms of future engagement that\nwas prevented . We estimate the latter based on predictions of the\nengagement potential of posts, which on aggregate have a net error\nof 4.5 % for viral posts, and 3.2 % for normal posts.\nFor a baseline period selected to represent \u201cnormal\u201d times, we\nfound that the active engagement life cycle of Facebook posts was\nvery short, with 90 % of all engagement occurring within the first\n30 hours after posting. An implication of this is that even rela-\ntively quick removal of content (we observed a median of 21 hours)\nallowed significant accrual of engagement before removal ( 3.8\ud835\udc40en-\ngagements with 3,843 posts), and prevented only 21.2 % of the pre-\ndicted engagement potential. By the time a removal decision was\n1arXiv:2301.02737v2  [cs.SI]  21 Feb 2023\nIan Goldstein, Laura Edelson, Minh-Kha Nguyen, Oana Goga, Damon McCoy, and Tobias Lauinger\nmade, the content had already reached the vast majority of its typi-\ncal audience, and the removal had only a minor impact in terms of\npreventing engagement or exposure to users.\nDuring the crisis surrounding January 6, we initially observed\nsimilar levels of content removals and prevented engagement rates,\nbut with generally higher engagement across both removed and\nnon-removed posts. It took 6 days, and two announcements of\nchanges to content moderation by Facebook, before we observed\na meaningful change in our metrics. Our results show that on\nJanuary 12, Facebook began removing older content, but because\nthese 1,416 removals occurred so late in these posts\u2019 engagement\nlife cycles, we estimate that they disrupted less than 1 %of future\nengagement, and made hardly any difference in practice. In this\nregard, Facebook did not appear to be prepared to contain the fallout\nof the crisis in a timely manner.\nOur results suggest that Facebook\u2019s content moderation of U.S.\nnews publishers and \u201cinfluencers\u201d could not keep up with the speed\nat which content spread on their social network, neither during\nnormal times nor in times of crisis. They highlight that content\nmoderation on a social network such as Facebook operates within\nthe constraints of content recommendation, and its efficacy cannot\nbe assessed in isolation. Merely tallying numbers of removed posts,\nwithout taking into account enforcement delays or the rates at\nwhich deleted posts reached their projected audiences, does not\nadequately characterize the outcomes of content moderation. Un-\nfortunately, this is not reflected in the transparency metrics that\nFacebook is currently reporting.\nOur work makes the following contributions:\n(1)We propose a methodology for inferring content moderation\nof public posts from current transparency data.\n(2)We introduce more insightful metrics for quantifying the im-\npact of delayed content removals on accrued and prevented\nuser engagement.\n(3)We show that moderation of public posts by U.S. news sources\nhappened late in their engagement life cycle, resulting in\nonly 21.2 % prevented engagement.\n2 BACKGROUND & RELATED WORK\nFacebook and other social media networks reach billions of users\nby promoting content generated by large content producers such\nas news organizations or \u201cinfluencers,\u201d and also by individual users.\nUnfortunately, social media has been utilized for harmful purposes,\nsuch as spreading disinformation [ 8] or planning and publicizing\nviolent activities [ 26]. This has caused these networks to come\nunder increasing societal, regulatory and legal pressures to prevent\nthe promotion of dangerous and harmful content on their systems.\nContent Moderation Processes. Virtually all social media networks\nhave established rules of conduct and acceptable content for their\nservices. They have a range of techniques at their disposal to enforce\nthese standards, such as deleting [ 41,59], downranking [ 32], quaran-\ntining [ 12,57], labeling [ 70], or demonetizing [ 16] undesirable con-\ntent, or temporarily or permanently banning the accounts of users\nwho repeatedly post violating content (\u201cdeplatforming\u201d) [ 20,40,52].\nIn this paper, we study content moderation [5,23,25,29] from the\nangle of removal of violating content.The technical and human challenges of implementing reliable\nmechanisms for content moderation at scale have been topics of\nrepeated study [ 38,61]. Social media networks often use automated\nsystems to monitor user content and communications, and ide-\nally interdict violating content before it is published. Such proac-\ntive policy enforcement is typically built as sophisticated pattern\nmatching systems, comparing content to \u201cblocklists\u201d of known\nexamples [ 31,35], and is thereby limited to predefined classes of\nviolations (graphic violence, sexual content, child abuse, spam) [ 36].\nTo identify new patterns of violations, and address more ambiguous\nor context-sensitive cases that cannot be handled well by automated\nsystems, social media networks also commonly have content exam-\nined by human reviewers after publication, for example when the\ncontent reaches a certain popularity threshold, exhibits suspicious\ninteraction patterns, or causes complaints from other users [ 15,56].\nHuman review of the often violent content has been documented to\nhave a detrimental effect to the mental well-being of those engaged\nin the process [53, 56].\nOutcomes of Content Moderation. Prior research on content mod-\neration (specifically, content removals) has often focused on legal\nand socio-political issues (e.g., [ 13,36,45,54]), or on qualitative\nwork aiming to understand users\u2019 experiences and perceptions of\ncontent moderation (e.g., [ 37,39,61,69]). Quantitative research\nmeasuring content removals is more scarce. Srinivasan et al. [ 59]\nstudied the impact of content removals on subsequent user behavior\nwithin a single subreddit, whereas Jhaver et al. [ 41] measured the\nimpact that explanations accompanying content removals had on\nfuture user behavior across the entire platform. Jiang et al. [ 42,43]\nevaluated partisan bias in content and comment moderation on\nYouTube and found no evidence of left or right bias in removals.\nPapakyriakopoulos et al. [ 50] quantified the sharing of conspiracy\ntheory-related URLs on various social media and modeled the im-\npact of content moderation; in contrast to our work, they did not\ndelve into the timing of engagement accrual or deletion delays. To\nthe best of our knowledge, we are the first to study the interplay\nbetween the timeliness of content removals and the accrual (or\nprevention) of user engagement with the removed content.\nCommunity Standards on Facebook. The rules for user behavior\nand acceptable content on Facebook are called \u201cCommunity Stan-\ndards\u201d [ 24]. They are divided into six broad areas ranging from\nviolence and incitement to intellectual property concerns. These\npolicies have evolved over time, often in response to crises.\nOur study period around the U.S. Capitol Riot on January 6, 2021\nencompasses several such policy changes. On December 14, 2020,\nBiden was declared the winner of the Electoral College vote [ 14]\nfollowing the 2020 U.S. presidential election that had taken place\non November 3. However, President Trump refused to concede and\nannounced a rally in Washington, D.C. for January 6. After that\n\u201cStop the Steal\u201d rally, a group of armed people breached security\nand stormed the U.S. Capitol building [ 7]. Social media platforms\nwere at the center of the spread of false and misleading information\nabout the election, including the use of Facebook to promote and\nlivestream the Capitol attack [ 6,11,63]. In reaction to the attack,\nFacebook publicly updated their Community Standards to explicitly\nprohibit \u2018praise and support\u2019 of the storming of the U.S. Capitol,\n2\nUnderstanding the (In)Effectiveness of Content Moderation\ncalls to bring weapons to locations in the U.S., video posts and pho-\ntos from Capitol insurrectionists, violations of the D.C. curfew, and\nfuture calls to violence. In addition, Facebook committed to enforce-\nment actions targeting militarized social movements, specifically\nnaming the Oathkeepers and QAnon [ 22]. On January 11, Facebook\nannounced that they would remove content containing \u201cStop the\nSteal\u201d [ 21]. In Section 4.2, we analyze how these announced policy\nchanges were reflected in observable content moderation activity.\nOther Related Work. More loosely related to our work are prior\nstudies of deleted content (e.g., [ 4,9,71]), which however were\nfocused on characterizing users who voluntarily deleted their own\ncontent after \u201cregretting\u201d its publication rather than measuring\ncontent moderation imposed by the social network. Similarly, there\nhas been ample prior work on the spread of (and engagement\nwith) misinformation on social media (e.g., [ 3,18,58]), but it has\nlargely left open the question of how fast such misinformation\nmay be removed by the social network. More generally, researchers\nhave also studied the diffusion of content on social media and the\nconcept of \u201cviral\u201d content, for example from the perspectives of\nuser intention [ 2] and the structure of diffusion [ 33]. Studies of\nviral content typically take a set number of the \u2018most popular\u2019 or\nwidely spreading content. Examples of this include Pressgrove et\nal. [51] with Twitter content, and Vallet et al. [ 65] for YouTube\ncontent spreading via Twitter. Past research has also tackled the\nproblem of predicting the popularity of content (e.g., for Facebook\nand YouTube videos based on visual features [ 64]). Unfortunately,\nour data set does not contain the multimedia content from removed\nposts, thus we cannot use approaches based on content features to\nestimate the engagement potential of deleted posts.\n3 DATA SET\nAfter the events of January 6, 2021, we sought to study the changes\nFacebook made to their content moderation efforts. Unfortunately,\nCrowdTangle, Facebook\u2019s major transparency tool, does not reveal\nanything about posts blocked before publication, and states that\nonce a published post is deleted from Facebook, it is also removed\nfrom CrowdTangle (with a delay).1Thus, in order to measure post-\npublication content moderation, it is necessary to discover newly\npublished posts in near real time, before they may be deleted, but\nwe had not set up a measurement process designed to detect content\nmoderation in advance of the Capitol Riot. However, we were able\nto obtain a data set of public Facebook posts by U.S. news publishers\ncollected by the authors of another study [ 18], which happened\nto include the time period of interest. Below, we summarize the\noriginal data collection methodology, providing additional detail\nwhere it pertains to the timeliness of detecting new posts. We\nthen describe our methodology for detecting deleted posts and\nestimating content moderation delays in a data set that was not\nspecifically designed for that purpose. We derive three post data\nsets for our study, as shown in Table 1: The Removed Set, which we\nfurther split into smaller sets for specific analyses; all non-removed\nposts from Facebook pages with at least one deleted post (Impacted\nPublisher Set); and all non-removed posts across all pages.\n1https://help.crowdtangle.com/en/articles/3323105-academics-researchers-faq3.1 Monitoring News Publishers on Facebook\nThe data set we use in this paper is comprised of the public Facebook\nposts of 2,551 U.S. news publishers during the 2020 U.S. presidential\nelection, provided to us by the authors of a study on user engage-\nment with misinformation [ 18]. Their selection of Facebook pages\ndefines the scope of our study. The authors of that study based their\nselection of Facebook pages on third-party data provided by the\nnews rating organizations NewsGuard [ 48] and Media Bias/Fact\nCheck [ 46]. Based on the assessments of these organizations, the\nauthors derived the political leaning of each news publisher (Far\nLeft to Far Right) as well as a binary \u201cmisinformation\u201d attribute\nindicating whether a news publisher had a known history of spread-\ning misinformation [ 18]. To exclude largely inactive news publish-\ners, the authors of that study also removed Facebook pages with\nfewer than 100 followers or 100 total engagements during their\nfive-month study period; our study inherits this filter. Due to the\nselection of Facebook pages in the original data set, we can study\npost-publication moderation of public posts (not comments) on\nthe Facebook pages of U.S. news sources. These news sources are\na mix of publishers known for reliable reporting as well as less\nreputable sources with a history of spreading misinformation, both\nmainstream and niche, from across the political spectrum.\n3.2 Original Data Collection Methodology\nThe original study authors collected all public Facebook posts and\ncorresponding engagement metadata from the 2,551 U.S. news pub-\nlishers using two types of crawls of the CrowdTangle API: (1) A\ndaily crawl to discover all new posts since the last crawl, and (2)\na second, separate daily history crawl to update the engagement\nmetadata such as the number of likes, shares, and top-level com-\nments of existing posts . This latter crawl went back in history as far\nas the remaining daily time permitted; in practice, existing posts\nfell into the observable history window for at least two weeks (or\nlonger) after publication. Both crawls were running daily from\nAugust 10, 2020 until January 18, 2021.\n3.3 Detecting Deleted Posts & Post Lifetime\nIn September 2021, we performed a follow-up crawl of the same\nnews publisher pages to determine whether each post from the\noriginal data set was still available on CrowdTangle or had been\ndeleted. This follow-up crawl, however, does not reveal when a\npost was deleted. To detect deletions with daily granularity, we\nleverage the daily history crawls. In the original data set, these\nhistory crawls ran until January 18, thus we limit our analysis\nto posts published between December 14, 2020 and January 15,\n2021 so that we have sufficient buffer to detect possible deletions.\nWe conservatively estimate post lifetimes as the time difference\nbetween the publication date of the post (provided by CrowdTangle)\nand the last time the post was observed in a daily history crawl. This\nis a lower bound on actual post lifetimes, i.e., we underestimate\nrather than overestimating post lifetimes, as a post could have been\ndeleted any time in the 24 h between the last time the post was\nobserved, and the first time the post was missing in a history crawl.\nWe do not know why any particular post was removed or whether\nit was deleted voluntarily or forcibly moderated by Facebook; we\nonly know that a post is not publicly accessible any more when it\n3\nIan Goldstein, Laura Edelson, Minh-Kha Nguyen, Oana Goga, Damon McCoy, and Tobias Lauinger\nfails to show up in the daily history crawl or our September follow-\nup crawl. We also note that very short-lived posts (published and\nthen quickly deleted within less than 24 h) may not be visible to us,\ndepending on how these events fall between consecutive crawls.\n3.4 Data Sets & Time Periods\nTable 1 shows a summary of the data sets used in this paper. Our\nmethodology identified 10,811 posts from 878 U.S. news publishers\nthat are no longer accessible via CrowdTangle (i.e., no longer pub-\nlicly visible on Facebook). We refer to these posts and associated\nmetadata as the removed set . We divide the removed set into four\nsubsets. First, we set aside the subset of posts from deleted pages .\nWhen a page is deleted, the deletion cascades to all of its posts.\nSince the deletion times of posts deleted with their page are identi-\ncal (likely independent from their original posting times), we study\nsuch pages and their posts separately. We subdivide the remaining\ndeleted posts into three time periods for more targeted analysis:\n(1)The baseline period contains posts removed prior to Janu-\nary 4, 2021, and is used to assess \u201cnormal\u201d behavior prior to\nthe events around January 6.\n(2)The January 6 period contains posts removed between Janu-\nary 4\u201311, 2021, and is used to analyze the immediate reaction\nin the days leading up to January 6 and just afterwards.\n(3)TheJanuary 12 period contains posts removed on January 12\nor later. We analyze this period separately because we no-\nticed increased deletion delays after January 12 (deleted posts\nwere older than usual); this different deletion behavior may\ncorrespond to changes on Facebook\u2019s side.\nWe create two additional data sets of posts that were not deleted.\nTheimpacted publisher set comprises the 297,774 remaining\nposts from the Facebook pages that had likely experienced content\nmoderation (at least one post deleted). This data set serves to inves-\ntigate the non-moderated content produced by these pages. To give\nan overview of the overall ecosystem, the non-removed set con-\ntains all non-deleted posts collected during the observation period.\nThis data set serves as a reference for comparison and consists of\n2.76 M posts from 2,315 news publishers (over 2.5 B engagements).\n3.5 Post Engagement and Virality\nIn the context of Facebook posts, we define engagement as the\nsum of all types of interactions that users can have with a post\n(and that are made transparent through CrowdTangle). Specifically,\nengagement consists of the number of \u201clikes\u201d (and other reaction\ntypes available on Facebook, such as \u201cangry\u201d or \u201csad\u201d), the number\nof times a post is shared, and the number of top-level comments\nbelow the post. Unfortunately, other types of popularity metrics\nare not available in CrowdTangle (e.g., the number of times a post\nhas been shown to users, or how many unique users have viewed\nthe post), thus we cannot include them in our study.\nFor posts that were still available in September 2021, we obtain\nlong-term engagement numbers from our follow-up crawl (Sec-\ntion 3.3); for deleted posts, we use the engagement numbers from\nthe last time they were observed on CrowdTangle. Similar to our\nestimates of post lifetime, engagement numbers for deleted postsare a lower bound. Additional engagement could have accrued be-\ntween the last observation and the time the post was removed, but\nas it is unobserved, it does not factor into our analysis.\nTo study the speed of engagement accrual of (non-deleted) posts,\nwe leverage time series data extracted from CrowdTangle in Sep-\ntember 2021. These show engagement values in increasing time\nsteps relative to the publication time of the post.\nVirality. In the non-removed data set, posts receive a mean total\nengagement of 896user interactions. However, the uneven distribu-\ntion of engagement with content on social media is well known: a\nsmall number of posts go viral, and the vast majority do not [ 62]. We\nare not aware of a commonly accepted engagement-based threshold\nfor virality. We define as viral any post that reaches 14.6 K engage-\nments (three standard deviations above the mean in the set of all\nnon-removed posts). This definition of virality is based purely on\npopularity, independent of the speed of engagement accrual.\n3.6 Estimating Prevented Engagement for\nRemoved Posts\nTo understand the impact of content removals, it is useful to esti-\nmate how much more user engagement a post might have accrued\nhad it not been deleted. We do so by first estimating a post\u2019s en-\ngagement potential , defined as the total engagement a post would\ntypically receive in the long run (i.e., if it is not deleted). Because\naccrual of engagement is different for each post, but potentially\nmore similar among posts from the same publisher, we estimate a\npost\u2019s engagement potential based on non-deleted posts from the\nsame Facebook page. From the engagement time series obtained\nin September 2021 for the surviving posts published during the\nbaseline period (which are old enough that we expect them to have\nexhausted their engagement potential), we calculate the mean en-\ngagement at each time step over all non-deleted posts from the same\npage. Since our observations show that there is a distinct engage-\nment curve for viral posts, we estimate engagement separately for\nviral posts. For pages with more than 10 non-removed viral posts,\nwe estimate viral mean engagement per time step individually; for\nall other pages, we fall back to using global viral engagement es-\ntimates. We define prevented engagement for a deleted post as\nthe difference between its engagement potential and engagement\nactually accrued before the deletion. It is estimated by summing all\nengagement on the applicable mean time series after the last obser-\nvation of the post. We likely overestimate prevented engagement\nbecause posts may have remained active for up to 24 h after the\nlast observation in the crawl, thus they might have accrued more\nreal engagement (and less engagement was prevented) compared\nto the estimate. In content moderation, it is desirable to remove\npolicy-violating posts quickly and prevent as much engagement as\npossible, thus our overestimation of prevented engagement paints\ncontent moderation efforts as more effective than they may be.\nTo validate the quality of these engagement predictions, we\ntested our methodology on a sample of 10,000 non-deleted posts\npublished during the baseline period. We simulated removals by\nrandomly selecting \u201cdeletion\u201d timestamps with selection weights\nbased on the typical lifetimes of deleted posts in the baseline period\n(Section 4.1). The prediction accuracy for engagement potential was\n89.7 % for non-viral posts, and 84.4 % for viral posts. The net error\n4\nUnderstanding the (In)Effectiveness of Content Moderation\nData Set Post Count Total Engagement Delayed Removals Delayed Engagement\nFull Data Set (posts published Dec 14\u2013Jan 15) 2.02 M 1.61 B\nRemoved Set (last observation of removed posts) 10,811 10,889,679 1,416 2,875,064\nBaseline Period (Dec 14\u2013Jan 3) 3,843 3,785,464 281 301,741\nJanuary 6 Period (Jan 4\u2013Jan 11) 1,171 3,302,379 60 374,066\nJanuary 12 Period (Jan 12\u2013Jan 15) 2,300 3,257,523 1,075 2,496,708\nRemoved Pages (Dec 14\u2013Jan 15) 3,497 544,313 NA NA\nImpacted Publisher Set (posts pub. Dec 14\u2013Jan 15) 297,774 784,842,977\nNon-Removed Set (posts pub. Dec 14\u2013Jan 15) 2.02 M 1.61 B\nTable 1: Overview of data sets. Posts created December 14, 2020\u2013January 15, 2021. Engagement is the total accrued by posts at\nthe time of final observation. We define delayed removals as those occurring more than 30 hours after post creation.\nacross all 9,929 non-viral sample post predictions was within 3.2 % of\nthe total ground truth engagement potential (lifetime engagement)\nfor these posts (4.5 % for the 71 viral posts). As our intention is to\napply our prediction methodology to a large number of posts and\nnot to draw inferences about the performance of individual posts,\nthese results suggest that our methodology is sufficiently robust.\n4 RESULTS\nIn this work, we aim to characterize how Facebook moderated\npublic content published on the Facebook pages of U.S. news pub-\nlishers. When a post is rendered unavailable, we do not know who\nremoved the content. However, as we discuss more thoroughly\nin Section 5.2, the patterns we observe across publishers suggest\nthat the vast majority of post removals were not voluntary, but\ninstances of content moderation, that is, posts deleted by Facebook\nfor violation of their platform policies. Both user engagement with\ncontent and Facebook\u2019s apparent content moderation changed in\nthe days during and after the events of January 6, 2021, thus we\nanalyze these time periods separately.\n4.1 Impacts of Content Removals\nTo understand Facebook\u2019s content moderation performance during\n\u201cnormal\u201d times, we begin with a baseline period from December 14,\n2020 to January 3, 2021. During this time, the 2,551 U.S. news pages\npublished 1.35 M posts. Facebook users engaged with these posts\n(i.e., \u201cliked,\u201d commented, or shared) a total of 948 M times. In terms\nof likely content moderation, 3,843 posts (0.28 %) were removed\nfrom 640 pages during this period, on average 183 removals per\nday. However, at the last time they were observed active in our data\nset, these posts had already accumulated 3.8 M user engagements\n(0.32 % of total engagement during the baseline period). If Facebook\ndeemed these posts inappropriate to remain on their platform, they\nstill allowed a considerable number of users to see and interact with\nthe offending content before reaching the decision to take it down.\nWe turn to the lifetimes of removed posts to understand how\nthese presumably policy-violating posts could accrue so much en-\ngagement before being deleted. As Figure 1 shows, the average time\nbetween publication of a post to its last observation by the crawler\nis 23.7 hours (median: 21 hours). (Note that the actual time of re-\nmoval could be up to 24 hours later because the crawler checked a\npost\u2019s status only once every 24 hours, thus we likely underestimate\n0 1 2 3 4 5 6 7\nTime since publication (days)0.00.20.40.60.81.0Proportion of removed\nposts (cumulative)30 hoursTime Period\nBaseline\nJanuary 6\nJanuary 12Figure 1: Post lifetime: Days between posting and last obser-\nvation. In the baseline and January 6 periods, nearly all last\nobservations were at most 30 h after posting, whereas dur-\ning the January 12 period, much older posts were removed.\nThe sharp pivot at 30 h suggests different removal processes.\npost lifetimes.) The distribution of post lifetimes pivots 30 hours\nafter post creation; approximately 90 % of removed posts have had\ntheir last observation at this point. The remaining 10 % are deleted\nat a noticeably slower pace over a much longer time span, which\nsuggests that a different process might be at play for those deletions.\nIn summary, most decisions to take down posts appear to be rela-\ntively fast in absolute terms, but they do take time, and significant\namounts of engagement are allowed to accrue during that time.\nThe situation of viral posts is slightly different from the overall\npost distribution. The baseline period saw 9,768 posts accumulate\nsufficient engagement to be considered viral (425 M combined); 61\nof them were removed during this period (more than twice the\nrate of all content removals). Until their last observation, these 61\nremoved viral posts had been able to accrue 1.7 M engagements,\n44.8 % of all engagement with posts removed during this period.\nThis share illustrates the importance of handling viral posts well;\nquick intervention on a relatively small number of violating viral\nposts could disproportionately reduce the odds of Facebook users\nseeing and engaging with inappropriate content. However, our data\nsuggest that viral posts may be more difficult for Facebook to handle,\nas viral posts tended to be active for longer before being removed\n5\nIan Goldstein, Laura Edelson, Minh-Kha Nguyen, Oana Goga, Damon McCoy, and Tobias Lauinger\n0 1 2 3 4 5 6 7\nTime since publication (days)0.00.20.40.60.81.0Proportion of posts\n(cumulative)Post Type\nviral\nnon-viral\nFigure 2: Days until non-removed baseline posts reach 80 %\nof their lifetime engagement. Around 78 % of non-viral posts\nreach 80 % of their lifetime engagement within one day,\nwhereas it takes two days for viral posts.\n(mean time to last observation: 31 hours, more than 7 hours longer\nthan the overall mean; median: 24 hours, or 3 hours longer).\nTo put the speed of deletions into context, we compare it to the\nrate of accrual of engagement. Most engagement with posts in our\ndata set happens relatively soon after the content is created. This is\ntrue both for viral and non-viral posts, as shown in Figure 2. Among\nnon-removed content created during our baseline period, 78.1 % of\nnormal posts reached at least 80 % of their total engagement within\none day, while 93.3 % of normal posts achieved it within two days. In\nother words, while the typical content moderation delays observed\nin our data set may seem fast in absolute terms, in fact they do\ncome late in a post\u2019s life cycle because most of a post\u2019s engagement\nusually occurs before the typical content moderation delays. As a\nresult, these instances of content moderation may not make a big\ndifference in practice because most of a post\u2019s audience will already\nhave seen and interacted with the post before it is deleted.\nViral posts have a longer period of active engagement accrual\ncompared to non-viral posts, with 55.2 % reaching 80 % of their\ntotal engagement in one day, and 80.6 % reaching it within two\ndays. However, this does not mean that slower content moderation\nwould be appropriate due to the slower relative rate of engagement\naccrual. In fact, total engagement with viral posts is so much higher\nthat despite the slower relative increase, the absolute increase is\nsubstantial. In the hour before their respective median removal\ntime, viral posts accrued an average of 1,583 additional engage-\nments, whereas it was only 7.5 for non-viral posts. With the goal\nof reducing total exposure to harmful content, it is important to\nmoderate viral posts with the same (or higher) speed than non-viral\nposts, yet our data suggest the opposite is occurring in practice.\nWe assess the benefit of typical content moderation delays by\nestimating how much engagement was prevented by removing\nthe post, that is, how much of the typical audience did notsee\nand interact with the offending post. We estimate a post\u2019s typi-\ncal engagement on a per-page basis and separately for viral and\nnon-viral posts, as described in Section 3.6, and calculate prevented\nengagement as the difference between that estimate and the last\nobserved engagement count before deletion. During the baseline\nperiod, we estimate that post removals disrupted 21.2 % of potentialengagement (314 or fewer prevented engagements each for 90 % of\nremoved posts). For the 3,782 removed non-viral posts, this breaks\ndown to a 20.5 % prevention rate, or a mean of 151 prevented en-\ngagements per non-viral post. The removal of each of the 61 viral\nposts had a much larger impact in absolute terms, with a mean of\n7,289 prevented engagements. Viral posts accrue engagement for a\nlonger time (median 1.4 times longer) than non-viral posts, offering\nthe opportunity to disrupt a larger fraction of their engagement\npotential, but because they were also removed more slowly (median\n1.2 times slower), the engagement prevention rate for viral posts\nwas only slightly higher at 22.0 %. Timelier enforcement of the\ncomparatively few removed viral posts could have made an outsize\ndifference in preventing Facebook users from engaging with sub-\nsequently moderated posts. Overall, we estimate that the content\nremovals we observed during the baseline period prevented 996 K\nengagements from occurring, which is a consequential number\nin absolute terms. Unfortunately though, these removals came so\nlate that the posts had already reached over three quarters of their\nengagement potential, and only a small minority of their potential\nengagement was actually prevented by the removal.\n4.2 Content Removals After the Capitol Riot\nTo understand how Facebook carried out content moderation dur-\ning and after the Capitol Riot, we repeat the analyses of the baseline\nperiod above for the time just before and after the event of Janu-\nary 6, 2021. In the eight days from January 4\u201311, which we refer to\nas the January 6 period, we observed comparable levels of content\nremovals in absolute terms, but higher engagement with content\nby the Facebook users. In detail, the U.S. news pages published\n393 K posts that received a total of 409 M engagements. The pe-\nriod saw 1,171 posts removed from 246 pages, a mean of 146 per\nday, which is roughly comparable to the baseline period ( \ud835\udc61=1.83,\n\ud835\udc5d=0.079). However, by the time they were last observed, these\nposts had received 3.3 M engagements, 0.81 % of all user engage-\nment, significantly ( \ud835\udc61=4.06,\ud835\udc5d<0.001) more than during the\nbaseline period. The median post lifetime in the January 6 period\nwas consistent with the baseline period for non-viral ( \ud835\udf122=1.24,\n\ud835\udc5d=0.266) and viral posts ( \ud835\udf122=.043,\ud835\udc5d=0.835), also evidenced by\nthe closely overlapping curves seen in Figure 1. Thus, the increased\nengagement accrual did not stem from slower removals, but higher\nuser engagement. Although we saw no increase in the proportion\nof posts moderated during the January 6 period, and no significant\nincrease of non-removed viral posts ( \ud835\udc61=0.62,\ud835\udc5d=0.536), there\nwas a significant increase in the removal of viral posts as compared\nwith the baseline period ( \ud835\udc61=2.38,\ud835\udc5d=0.024), which explains the in-\ncrease in engagement-weighted removals. 80% of engagement with\nremoved viral posts in this period was associated with extremely\npartisan sources, and the remainder originated from a single page.\nWe estimate that post removals during the January 6 period pre-\nvented a similar share of potential engagement (25.8 %) as in the\nbaseline period (a total of 306 K engagements with 1,117 non-viral\nposts, and 521 K engagements with 54 viral posts). In summary,\nwhile we observed increased engagement from users, any changes\nFacebook may have made to content moderation in the immediate\naftermath of January 6 do not stand out in our metrics.\n6\nUnderstanding the (In)Effectiveness of Content Moderation\nWe do observe increased levels of content removals during the\nfour days from January 12\u201315 (the January 12 period), after Face-\nbook announced another content moderation policy change [ 22]\non January 11th. U.S. news pages published 186 K posts during\nthese four days, reaching 161 M total engagements. Facebook re-\nmoved 12 entire pages with all their 3,463 posts created between\nDecember 14, 2020 and January 15, 2021. We discuss these removed\npages in Section 4.5. In addition, we observed 2,300 removals of\nindividual posts from 377 pages, a mean of 575 per day, which was\na significant increase ( \ud835\udc61=8.63,\ud835\udc5d<0.001) from the baseline period.\nThese posts had received a total of 3.3 M engagements at the time\nof their final observation. This last observation came considerably\nlater in a post\u2019s lifetime than in the baseline and January 6 periods.\nIn the January 12 period, the mean time between post creation and\nlast observation was 152 hours (median: 24 hours), significantly\nlonger than the baseline period for both non-viral ( \ud835\udf122=126.7,\n\ud835\udc5d<0.001) and viral posts ( \ud835\udf122=36.8,\ud835\udc5d<0.001). As shown in\nFigure 1, 48 % of last observations occurred more than 30 hours\nafter publication, whereas it was only approximately 6.8 % in the\nbaseline and January 6 periods. This sharp increase in delayed con-\ntent removals suggests that Facebook may have applied changes\nin content moderation announced on January 11 retroactively to\nolder posts, which were originally published in the time around\nJanuary 6, but deleted only after January 11. (We note that all of the\nposts removed after more than 30 hours were from \u201crepeat offender\u201d\npages, i.e., we had already observed removed posts from these pages\nin the past.) This further suggests that a policy change, which took\nnearly a week to implement, was necessary for Facebook to deal\nwith the fallout of January 6 on their platform.\nThe unusually late removals are reflected in our estimates of\nprevented engagement. For all posts removed during the Janu-\nary 12 period, we estimate that 569 K engagements were disrupted,\nrepresenting only 14.9 % of removed posts\u2019 predicted engagement\npotential. In particular, for the subset of posts removed with a long\ndelay (of more than 30 hours), the deletions appear almost incon-\nsequential because they prevented only 0.60 % of the engagement\nwe predicted these posts to achieve without being deleted. In other\nwords, if these deletions were indeed instances of \u201cretroactive\u201d con-\ntent moderation, Facebook allowed these posts to reach virtually\ntheir entire potential audience before adjudicating that they were\nin violation of platform policy and had to be taken down.\n4.3 Impact of Removals by Misinformation and\nPartisanship Reputation\nOver the last several years, there has been intense public scrutiny\nof the partisan impact of content moderation policies on Face-\nbook [ 27,44,66]. We now analyze content removals taking into\naccount the partisanship and factualness classifications in our data\nset. These labels reflect the reputation of the pages in question\nrather than characterizing individual posts, but they still allow us\nto compare the effects of content removal along partisan lines. We\ncalculate the engagement-weighted rate of removal as the ob-\nserved engagement from removed posts in a category of pages over\nthe total observed engagement from all posts in that category.Partisanship Misinformation Non-misinformation\nFar Left 1.02 % 0.57 %\nSlightly Left 2.67 % 0.19 %\nCenter 3.34 % 0.29 %\nSlightly Right 6.97 % 0.07 %\nFar Right 1.19 % 0.53 %\nTable 2: Post removal rates by partisanship and factualness\nof the source (weighted by engagement, entire data set).\nAcross the political spectrum, news sources known to spread\nmisinformation saw a higher fraction of their accumulated\nuser engagement affected by content removals.\nOver the full time range of our data set, posts made by pages\nwith a reputation for misinformation had a higher engagement-\nweighted rate of removal than posts from pages not classified as\nmisinformation providers. This finding of higher misinformation\nremoval rates held true within each of the five partisanship cat-\negories, as shown in Table 2. At the same time, within the same\nfactualness category, engagement-weighted rates of removal varied\nconsiderably according to the partisanship of pages; in the misin-\nformation category, they ranged from 1.02 % for posts made by Far\nLeft pages to 6.97 % for posts from Slightly Right pages. We note\nthat misinformation pages of extreme partisanship (Far Left and\nFar Right) had the lowest post removal rates among misinformation\npages, whereas non-misinformation pages of extreme partisanship\nhad the highest post removal rates of all non-misinformation pages.\nWe do not currently have an explanation for this effect.\nSo far, our analysis of removal rates covered the entire duration\nof our data set, about a month roughly split in half by the events\nof January 6, 2021. These events were of a deeply partisan nature;\nwe now investigate whether any corresponding effects based on\npartisanship are observable among the removed posts. To better\nisolate the impact of these events, we again split the data set into\nthe baseline, January 6, and January 12 periods. (To streamline the\nanalysis, we no longer differentiate pages by their misinformation\nreputation.) The upper two rows of Figure 3 show the engagement-\nweighted partisan breakdown of posts removed during the baseline\nperiod (above), and non-removed posts published during the same\nperiod (below). While not perfectly balanced, there does not appear\nto be any major partisan bias in the posts removed during the\nbaseline period compared to the posts that were not removed.\nThe picture looks very different in the January 6 period (mid-\ndle two rows of Figure 3). The majority of engagement with posts\nremoved during these eight days corresponded to Far Right pages\n(50.8 %), while Slightly Right pages accounted for the next largest\nshare (21.9 %). The partisan breakdown of engagement with posts\nthat were not removed does not exhibit this effect, and more closely\nresembles the baseline period (albeit not exactly). During the four\ndays of the January 12 period, the proportion of engagement attrib-\nutable to removed posts from Slightly Right pages receded to a level\ncomparable to the baseline period. However, the corresponding\nproportion from Far Right pages remained more than double the\nbaseline share of engagement-weighted post removals. (This does\nnot necessarily correspond to \u201cnew\u201d engagement accrued during\n7\nIan Goldstein, Laura Edelson, Minh-Kha Nguyen, Oana Goga, Damon McCoy, and Tobias Lauinger\n01\nRemovedFar Left S. Left Center S. Right Far Right\n01\nNon-removed\n01\nRemoved\n01\nNon-removed\n01\nRemoved\n01\nNon-removed Baseline Period\n January 6 Period\nJanuary 12 Period\nPolitical Leaning (% Engagement)\nFigure 3: Engagement with removed and non-removed posts\nfrom U.S. news pages by partisanship for the baseline, Jan-\nuary 6, and January 12 periods, respectively. Around Janu-\nary 6, the vast majority of engagement with posts that were\nultimately removed went to posts from sources classified as\nSlightly Right and Far Right. The rate of engagement with\nmisinformation sources across all time periods was 66.1 %\nfor removed content and 28.2 % for non-removed content.\nthis period, but may be partially due to delayed enforcement; 65.7 %\nof these removals were posts originally published in the days around\nJanuary 6, and deleted only on or after January 12.) In conclusion,\nthe partisan breakdown of engagement-weighted removal rates\nwas somewhat balanced along partisan lines before the events of\nJanuary 6, and remained at comparable levels across all three time\nperiods for non-removed posts. Likely content moderation around\nJanuary 6 disproportionately affected posts from Slightly Right and\nFar Right pages. This is in line with the policy changes that Face-\nbook announced on January 6 and January 11, which specifically\ntargeted so-called \u201cStop the Steal\u201d [21, 22] misinformation.\n4.4 Most Impacted Publishers\nFor illustrative purposes to complement our quantitative findings,\nwe identified the U.S. news pages with most user engagement that\nlikely experienced content moderation during our entire observa-\ntion period (the Impacted Publisher Set). The two most popular\npages with significant post removals were \u2018Dan Bongino\u2019 and \u2018Oc-\ncupy Democrats, \u2019 both classified as sources of misinformation with\nFar Right2and Far Left3partisanship, respectively. Taken together,\nboth pages generated 835 K engagements with content that was\nlater removed, though this represented only a fraction of the tens\nof millions of engagements each of these pages generated in our\nobservation window. \u2018The Blaze,\u2019 a conservative4media company\nwith a reputation for misinformation, received the greatest total\nengagement (1.37 M) with 310 ultimately removed posts, account-\ning for 28.3 % of all engagement that the page received between\nDecember 14, 2020 and January 15, 2021. At 91.5 %, the share of\n2https://mediabiasfactcheck.com/dan-bongino/\n3https://mediabiasfactcheck.com/occupy-democrats/\n4https://mediabiasfactcheck.com/the-blaze/engagement attributed to deleted posts was highest for \u2018I Love\nMy Freedom,\u2019 a page associated with a conservatively oriented\nonline apparel retailer.5Three quarters of engagement-weighted\nremovals from this page occurred after Facebook\u2019s January 11 pol-\nicy announcement, with a mean delay of 8.5 days, suggesting that\nthe affected content might not have been in clear violation of the\npolicies in effect prior to that date. In line with the unusually long\nremoval delays, we estimate that only 6.2 % of potential engage-\nment was prevented. In summary, the pages most heavily impacted\nby removed content tended to be pages categorized as extremely\npartisan, misinformation providers, or both.\n4.5 Page Deletions\nSo far, we have only considered removals of individual posts from\npages that themselves remained active. In addition, we observed the\nremoval of 14 entire pages (the Removed Pages Set), which included\ntheir 3,497 posts created between December 14, 2020 and Janu-\nary 15, 2021. These posts had accumulated an engagement of 544 K.\nDeleted pages fall into three categories: known sources of health\nand vaccine misinformation (\u2018Erin at Health Nut News,\u2019 \u2018LifeSite-\nNews.com\u2019), extreme Far Right media with poor records for factual-\nness and newsworthiness (\u2018TruNews,\u2019 \u2018ConservativeOpinion.com\u2019),\nand pages frequently linked to conspiracy sources (\u2018The Drudge\nReport,\u2019 \u2018Zero Hedge\u2019). Many of these were widely known as spread-\ners of inaccurate information, and their forced deplatforming by\nFacebook and the related debate were widely reported [ 17,28,47].\n5 DISCUSSION\nContent moderation aims to limit the exposure of users to harmful\ncontent by way of deleting user-generated content that is deemed\nunacceptable. In this framework, content moderation needs to com-\npete with content recommendation systems, in that the speed of\nenforcement needs to keep up with the potentially viral spread of\ntoxic content in order to effectively contain undesirable content.\nOur results suggest that Facebook\u2019s content moderation was not\neffective at meaningfully limiting the spread of harmful content\nfrom U.S. news publishers. In the most common case, content mod-\neration was relatively quick in absolute terms; half of removed posts\nin the baseline period were last observed 21 hours after publica-\ntion. Yet, the speed of removal pales in comparison to how quickly\ncontent accrues engagement, and how quickly interest in content\nfades. By the time of removal, the posts in our data set had already\nbeen shown to millions of Facebook users, accumulating a total of\n10.9 million engagements. We estimate that the removals prevented\nonly 21.2 % of engagement from occurring because the vast majority\nof the posts\u2019 potential audience had already seen them. Further-\nmore, it appears that Facebook needed to change their policy in\norder to moderate some content after the Capitol riot of January 6,\n2021. This policy change took nearly a week to implement, likely\ndelaying the removal of 995 posts by multiple days, with the result\nthat they received over 2 M engagements in the meantime, and\ntheir removal prevented virtually none ( <1 %).\nWhether or not content moderation can be effective depends\non factors that are under Facebook\u2019s control. For one, Facebook\ncould aim to delete content more quickly by decreasing delays in\n5https://www.facebook.com/ILMFOrg/about\n8\nUnderstanding the (In)Effectiveness of Content Moderation\nhuman review and implementing a content policy that does not\nrequire lengthy adjustments to allow reaction to major real-world\nevents. It is unclear, however, how much potential there is for such\nimprovements. As we have shown, the currently very short ac-\ntive engagement periods of posts, and the speed of viral accrual\nof engagement create a limited window during which content re-\nmoval can be effective as a strategy. Therefore, the second avenue\nto making content moderation more effective would be changing\nthe parameters of the content recommendation algorithm to en-\nlarge this time window and give Facebook more time to intervene\neffectively. This could take various forms. To increase the time for\nintervention, new content could be recommended more slowly. To\nlimit the impact of harmful content, the relatively small number\nof posts predicted to \u201cgo viral\u201d could be reviewed manually before\nthey accrue significant engagement.\nAdmittedly, the scenario for content moderation that we stud-\nied in this paper may be more challenging than the ecosystem of\nFacebook as a whole. Public posts from U.S. news publishers, due\nto their newsworthiness, may require a more thorough review than\nother types of policy violations. Furthermore, due to their larger\naudience, delayed removals have a much larger aggregate impact\nthan moderation of posts in smaller groups, or private messages.\nThese challenges highlight, however, the limits of what can be\nachieved with the tool of content moderation under the constraints\nof a recommendation system that has been designed and optimized\nto spread engaging content as quickly as possible.\n5.1 Limitations\nAt the time of this writing, Facebook does not provide fine-grained\ntransparency for content moderation. Our efforts to study content\nmoderation rely on what is observable indirectly through Crowd-\nTangle. As a result, we can only study posts deleted after publication.\nOther types of content moderation, such as rejecting posts before\npublication, or deliberately downranking posts in the recommen-\ndation algorithm (without deleting them) are outside the scope of\nour study. Facebook claims that up to 90 % of content violations\n(including hate speech, nudity, violence, and bullying) are deleted\nbefore they are likely to have been seen [ 23]. Our study can only\ninclude posts that slip through pre-publication content filters. Yet,\ncollectively Facebook users engaged with the deleted posts in our\ndata set more than 10.8 million times. While we do not have exact\nnumbers about how this relates to impressions and unique users,\nit is reasonable to estimate that millions of Facebook users were\nexposed to these posts that Facebook later deemed inappropriate.\nBecause we rely on a repurposed data set that was not originally\ncollected to study content moderation, our analysis is subject to\nadditional limitations. The authors of the data set did not archive\nmultimedia content such as images or videos associated with posts.\nPosts may have been deleted for offending image or video content,\nthus we cannot conduct a meaningful study of the contents of\ndeleted posts. Furthermore, the timing of post discovery in the\noriginal crawl causes a bias towards longer-lived posts, that is,\nwhen posts are deleted quickly after publication, they are more\nlikely to be missing in the data set. Similarly, the daily status checks\nof older posts in the original crawl mean that the inferred post\nlifetimes are only a rough estimate.Our analysis is based on public posts published by the Face-\nbook pages of 2,551 U.S. news organizations. The content produced\nby these pages is not representative of the Facebook ecosystem\nas a whole. Furthermore, the selection of these pages and their\ncategorization with regard to their political leaning and history\nof spreading misinformation is subject to the methodology and\nlimitations described by Edelson et al. [18].\n5.2 Content Moderation vs. Voluntary\nRemovals\nBecause we cannot observe post deletions directly, we do not know\nfor certain why content was removed; it could have been moder-\nated by Facebook, deleted by the page owner, or the visibility could\nhave been changed to private. However, taken together, several\nindependent observations from our analysis suggest that a majority\nof removed posts were indeed subject to content moderation by\nFacebook. First, the pattern of deleted post lifetimes with a sharp\npivot at 30 hours was consistent across pages, which suggests inter-\nvention by a common process as opposed to independent voluntary\ndeletions by page owners. We were able to confirm one instance of\nvoluntary deletions by contacting the page administrators of the\nNational Center for Missing & Exploited Children,6a non-partisan,\ncharitable organization who confirmed to us that they routinely\ntake down posts about missing children when those children are\nfound. Statistically, their 185 voluntary removals in our data set\n(with a total of 344 K engagements) looked very different from ag-\ngregate removal patterns, suggesting that the bulk of removals are\nnot voluntary. In terms of post lifetimes, we last observed their vol-\nuntarily deleted posts after a mean of 7.3 days (median: 2.4 days), as\nopposed to 2.7 days (0.9 days) across all removed posts in our data\nset. This supports our hypothesis based on patterns in post lifetimes\nthat most observed removals were likely deletions by Facebook.\nSecond, the unusually long removal delays only observed in the Jan-\nuary 12 period and the coinciding, prior announcement by Facebook\nof a content moderation policy change suggest that these removals\nwere instances of content moderation. Third, the higher prevalence\nof removals among pages with a reputation for spreading misinfor-\nmation makes it less likely these are voluntary deletions, unless we\nhypothesize that these misinformation providers are more prone to\nself-censorship. While only improved transparency from Facebook\ncould provide clarity, we are confident that the removal patterns\nwe observed are predominantly a reflection of content moderation.\n5.3 Improved Measurement Design\nIf we were to design a measurement of content moderation of public\nposts from scratch, and assuming that a transparency tool such\nas CrowdTangle continues to impose low rate limits and does not\nprovide specific transparency around content moderation, what\nwould our measurement design look like?\n\u2022Archive multimedia content such as images and video. If space\nis an issue, keep only the content of deleted posts, or of all\nposts from pages that have deleted posts.\n6https://www.missingkids.org/\n9\nIan Goldstein, Laura Edelson, Minh-Kha Nguyen, Oana Goga, Damon McCoy, and Tobias Lauinger\n\u2022Detect new posts more frequently (as opposed to daily) to\nobserve short-lived posts. Prioritize the time of day when\nmost posts are published.\n\u2022Check the status of existing posts more frequently than daily\nto get more fine-grained post lifetimes, at least during the\nfirst 2-3 days, which see the vast majority of deletions.\nFor example, hourly daytime crawls to detect publication or deletion\nof recent posts (0-3 days old) could be combined with nightly history\ncrawls to detect deletion of older posts with daily granularity.\n5.4 Transparency Metrics & Reporting\nMeta\u2019s primary mechanism for making data relating to content\nmoderation transparent is their Community Standards Enforce-\nment Report [ 23]. In these quarterly reports, the company shares\naggregate, global data about eleven categories of removals, reported\nas the absolute number of posts removed, and the relative share\nof global user impressions attributable to moderated content. The\ndiscrepancy between absolute and relative numbers makes them\ndifficult to compare, and they are not broken down at a country\nlevel. Furthermore, the raw number of removed posts is not per se\ninformative for understanding the efficacy of content moderation.\n(A more detailed discussion of Facebook content moderation trans-\nparency metrics can be found in the 2019 report of the Facebook\nData Transparency Advisory Group [ 10].) Based on the metrics we\nused in our analysis, we recommend reporting how long content\nremains active before it is taken down, how many people are ex-\nposed to content that is later removed, and how much of a removed\npost\u2019s potential audience did notsee the post as a result of the in-\ntervention, that is, whether removal of the post made a meaningful\ndifference in safeguarding users.\n6 CONCLUSION\nIn this work, we repurposed a data set of over 2 M Facebook posts\nfrom 2,551 U.S. news sources In the absence of explicit transparency\nabout content moderation, we developed a methodology to identify\nremoved posts within an existing data set, and infer when removals\noccurred. We also developed a method for predicting lifetime en-\ngagement of posts based on the engagement of other posts from the\nsame Facebook page, and proposed novel metrics to quantify the\nimpact of content moderation based on accrued and prevented user\nengagement. We found that content moderation during \u201cnormal\u201d\ntimes was relatively quick in absolute terms (median post lifetime\nof 21 hours), yet posts tended to exhaust their engagement poten-\ntial at an even faster rate, and we estimate that at most 21.3 % of\npredicted future engagement was prevented over the entire data set.\nIn summary moderation of public content from U.S. news sources\nand influencers on Facebook was too slow to keep up with the\nspread of content and ensuing engagement; it is unclear whether\nthis race to limit user exposure to harmful content can be won\npurely by speeding up content moderation. We recommend that\nother strategies be explored further, such as changes to content rec-\nommendation algorithms that slow the diffusion of content through\nsocial networks overall.ACKNOWLEDGMENTS\nThis research was supported in part by the Democracy Fund, the\nFrench National Research Agency (ANR) under ANR-17-CE23-0014,\nANR-21-CE23-0031-02, by the MIAI@Grenoble Alpes ANR-19-P3IA-\n0003, and by the EU 101041223, 101021377 and 952215 grants.\n7 ETHICS STATEMENT\nIn conducting this research and creating this paper, we honored\nthe guidelines and principles established for the conduct of ethical\nresearch. We have laid out the limitations of our data and methods\nin Section 5.1.\nIn our research, we relied on an existing data set created us-\ning CrowdTangle, an official tool from Facebook, with API access\ngranted to both us and the original authors [ 18]. CrowdTangle con-\ntains only public posts of Facebook pages and aggregate, purely\nquantitative engagement data, but no personally identifiable infor-\nmation. We did not have access to information about individual\nnews consumers, and expose no personally identifiable information.\nThe purpose of our paper is to measure the circumstances and\nimpact of content moderation on social media, particularly during\na crisis. We are cognizant that our work could cast a poor light\non Facebook, or on the authors of moderated posts, who might\nobject to a work that emphasizes the classification of some of their\ncontent as objectionable. We argue that the benefit of better un-\nderstanding content moderation outweighs the potential impact to\nreputation, particularly when Facebook\u2019s conduct has been thor-\noughly reported [6, 55].\nIn identifying the strong connection between removed content\nand sources\u2019 reputations for mis- and disinformation (Section 4.3),\nour intention is not to adjudge the merits of any publisher, merely\nto record engagement based on existing reputations for factualness,\nfor which we rely on multiple third-party sources for classification.\nUltimately, we reason that it is a better outcome for society in\ngeneral that Facebook not become a haven for harmful content than\nthat any of these stakeholders maintain a sterling reputation. To this\nend, we contribute recommendations for improving moderation\ntransparency and efficacy for the benefit of Facebook and its users.\nREFERENCES\n[1] 8kun 2022. Guidance . 8kun. https://8kun.top/guidance.html\n[2]Saleem Alhabash and Anna R McAlister. 2015. Redefining virality in less broad\nstrokes: Predicting viral behavioral intentions from motivations and uses of\nFacebook and Twitter. New Media & Society 17, 8 (2015), 1317\u20131339. https:\n//doi.org/10.1177/1461444814523726\n[3]Hunt Allcott, Matthew Gentzkow, and Chuan Yu. 2019. Trends in the diffusion\nof misinformation on social media. Research & Politics 6, 2 (2019). https://doi.\norg/10.1177/2053168019848554\n[4]Hazim Almuhimedi, Shomir Wilson, Bin Liu, Norman Sadeh, and Alessandro\nAcquisti. 2013. Tweets are forever: A large-scale quantitative analysis of deleted\ntweets. In Conference on Computer-Supported Cooperative Work (CSCW) . ACM.\nhttps://doi.org/10.1145/2441776.2441878\n[5]Stephanie Alice Baker, Matthew Wade, and Michael James Walsh. 2020. The chal-\nlenges of responding to misinformation during a pandemic: Content moderation\nand the limitations of the concept of harm. Media International Australia 177, 1\n(2020), 103\u2013107. https://doi.org/10.1177/1329878X20951301\n[6]Michael Baldassaro, Katie Harbath, and Michael Scholtens. 2021. The Big Lie and\nBig Tech . The Carter Center. https://www.cartercenter.org/resources/pdfs/news/\npeace_publications/democracy/the-big-lie-and-big-tech.pdf\n[7]Dan Barry and Sheera Frenkel. 2021. \u2018Be There. Will Be Wild!\u2019: Trump all but\ncircled the date . The New York Times. https://www.nytimes.com/2021/01/06/us/\npolitics/capitol-mob-trump-supporters.html\n10\nUnderstanding the (In)Effectiveness of Content Moderation\n[8]Yochai Benkler, Robert Faris, and Hal Roberts. 2018. Network propaganda: Manip-\nulation, disinformation, and radicalization in American politics . Oxford University\nPress, New York. 472 pages.\n[9]Parantapa Bhattacharya and Niloy Ganguly. 2016. Characterizing deleted tweets\nand their authors. In International Conference on Web and Social Media (ICWSM) .\nAAAI. https://doi.org/10.1609/icwsm.v10i1.14803\n[10] Ben Bradford, Florian Grisel, Tracey L Meares, Emily Owens, Baron L Pineda,\nJacob N Shapiro, Tom R Tyler, and Danieli Evans Peterman. 2019. Report of\nthe Facebook Data Transparency Advisory Group. https://law.yale.edu/sites/\ndefault/files/area/center/justice/document/dtag_report_5.22.2019.pdf\n[11] Center for an Informed Public, Digital Forensic Research Lab, Graphika, and\nStanford Internet Observatory. 2021. The long fuse: Misinformation and the 2020\nelection . Stanford Digital Repository: Election Integrity Partnership. https:\n//purl.stanford.edu/tr171zs0069\n[12] Eshwar Chandrasekharan, Shagun Jhaver, Amy Bruckman, and Eric Gilbert.\n2022. Quarantined! Examining the effects of a community-wide moderation\nintervention on Reddit. ACM Transactions on Computer-Human Interaction 29, 4,\nArticle 29 (March 2022), 26 pages. https://doi.org/10.1145/3490499\n[13] MacKenzie F. Common. 2020. Fear the Reaper: How content moderation rules are\nenforced on social media. International Review of Law, Computers & Technology\n34, 2 (2020), 126\u2013152. https://doi.org/10.1080/13600869.2020.1733762\n[14] Nick Corasaniti and Jim Rutenberg. 2020. Electoral College vote officially affirms\nBiden\u2019s victory . The New York Times. https://www.nytimes.com/2020/12/14/us/\npolitics/biden-electoral-college.html\n[15] Kate Crawford and Tarleton Gillespie. 2016. What is a flag for? Social media\nreporting tools and the vocabulary of complaint. New Media & Society 18, 3\n(2016), 410\u2013428. https://doi.org/10.1177/1461444814543163\n[16] Arun Dunna, Katherine A. Keith, Ethan Zuckerman, Narseo Vallina-Rodriguez,\nBrendan O\u2019Connor, and Rishab Nithyanand. 2022. Paying attention to the algo-\nrithm behind the curtain: Bringing transparency to YouTube\u2019s demonetization\nalgorithms. In Conference on Computer-Supported Cooperative Work (CSCW) . ACM.\nhttps://doi.org/10.1145/3555209\n[17] Alison Durkee. 2020. Google and Facebook are cracking down on the Far Right .\nVanity Fair. https://www.vanityfair.com/news/2020/06/google-ads-bans-zero-\nhedge-federalist-facebook-far-right\n[18] Laura Edelson, Minh-Kha Nguyen, Ian Goldstein, Oana Goga, Damon McCoy, and\nTobias Lauinger. 2021. Understanding engagement with U.S. (Mis)information\nnews sources on Facebook. In Internet Measurement Conference (IMC) . ACM.\nhttps://doi.org/10.1145/3487552.3487859\n[19] Facebook 2019. Global feedback and input on the Facebook Oversight Board for\ncontent decisions . Facebook. https://about.fb.com/news/2019/06/global-feedback-\non-oversight-board/\n[20] Facebook 2021. In response to Oversight Board, Trump suspended for two years;\nwill only be reinstated if conditions permit . Facebook. https://about.fb.com/news/\n2021/06/facebook-response-to-oversight-board-recommendations-trump/\n[21] Facebook 2021. Our preparations ahead of Inauguration Day . Facebook. https:\n//about.fb.com/news/2021/01/preparing-for-inauguration-day/\n[22] Facebook 2021. Our response to the violence in Washington . Facebook. https:\n//about.fb.com/news/2021/01/responding-to-the-violence-in-washington-dc/\n[23] Facebook 2022. Community standards enforcement report . Facebook. https:\n//transparency.fb.com/data/community-standards-enforcement/\n[24] Facebook 2022. Facebook community standards . Facebook. https://transparency.\nfb.com/policies/community-standards/\n[25] Facebook 2022. Taking action . Facebook. https://transparency.fb.com/\nenforcement/taking-action/\n[26] Filippo Fagnoni. 2019. Terrorism in the digital era: The dark side of\nlivestreaming, online content and internet subcultures . Masters of Me-\ndia. http://mastersofmedia.hum.uva.nl/blog/2019/09/22/terrorism-in-the-digital-\nera-the-dark-side-of-livestreaming-online-content-and-internet-subcultures/\n[27] Jason A Gallo and Clare Y Cho. 2021. Social media: Misinformation and content\nmoderation issues for Congress. Congressional Research Service Report 46662\n(2021). https://crsreports.congress.gov/product/details?prodcode=R46662\n[28] David Gilbert. 2021. Facebook Is finally doing something about the biggest spreaders\nof anti-vax lies . Vice News. https://www.vice.com/en/article/m7evpn/bill-gates-\nanti-vax-covid-lies-facebook-banned\n[29] Tarleton Gillespie. 2018. Custodians of the Internet: Platforms, content moderation,\nand the hidden decisions that shape social media . Yale University Press, New\nHaven. 288 pages. https://doi.org/10.12987/9780300235029\n[30] Tarleton Gillespie. 2018. Platforms are not intermediaries. Georgetown Law Tech-\nnology Review 198 (2018). https://georgetownlawtechreview.org/wp-content/\nuploads/2018/07/2.2-Gilespie-pp-198-216.pdf\n[31] Tarleton Gillespie. 2020. Content moderation, AI, and the question of scale. Big\nData & Society 7 (2020). https://doi.org/10.1177/2053951720943234\n[32] Tarleton Gillespie. 2022. Do not recommend? Reduction as a form of con-\ntent moderation. Social Media + Society 8, 3 (2022). https://doi.org/10.1177/\n20563051221117552\n[33] Sharad Goel, Ashton Anderson, Jake Hofman, and Duncan J. Watts. 2015. The\nstructural virality of online diffusion. Management Science 62, 1 (2015), 180\u2013196.https://doi.org/10.1287/mnsc.2015.2158\n[34] Robert Gorwa. 2019. What is platform governance? Information, Communication\n& Society 22 (2019). https://doi.org/10.1080/1369118X.2019.1573914\n[35] Robert Gorwa. 2020. Algorithmic content moderation: Technical and political\nchallenges in the automation of platform governance. Big Data & Society 7 (2020).\nhttps://doi.org/10.1177/2053951719897945\n[36] James Grimmelmann. 2015. The virtues of moderation. Yale Journal of Law &\nTechnology 42 (2015). https://doi.org/10.31228/osf.io/qwxf5\n[37] Oliver L. Haimson, Daniel Delmonaco, Peipei Nie, and Andrea Wegner. 2021.\nDisproportionate removals and differing content moderation experiences for\nconservative, transgender, and black social media users: Marginalization and\nmoderation gray areas. In Conference on Computer-Supported Cooperative Work\n(CSCW) . ACM. https://doi.org/10.1145/3479610\n[38] Natali Helberger. 2020. The political power of platforms: How current attempts\nto regulate misinformation amplify opinion power. Digital Journalism 8 (2020).\nhttps://doi.org/10.1080/21670811.2020.1773888\n[39] Shagun Jhaver, Darren Scott Appling, Eric Gilbert, and Amy Bruckman. 2019.\n\u201cDid you suspect the post would be removed?\": Understanding user reactions to\ncontent removals on Reddit. In Conference on Computer-Supported Cooperative\nWork (CSCW) . ACM. https://doi.org/10.1145/3359294\n[40] Shagun Jhaver, Christian Boylston, Diyi Yang, and Amy Bruckman. 2021. Eval-\nuating the effectiveness of deplatforming as a moderation strategy on Twit-\nter. In Conference on Computer-Supported Cooperative Work (CSCW) . ACM.\nhttps://doi.org/10.1145/3479525\n[41] Shagun Jhaver, Amy Bruckman, and Eric Gilbert. 2019. Does transparency in\nmoderation really matter? User behavior after content removal explanations on\nReddit. In Conference on Computer-Supported Cooperative Work (CSCW) . ACM.\nhttps://doi.org/10.1145/3359252\n[42] Shan Jiang, Ronald E. Robertson, and Christo Wilson. 2019. Bias misperceived:\nThe role of partisanship and misinformation in YouTube comment moderation.\nInInternational Conference on Web and Social Media (ICWSM) . AAAI. https:\n//doi.org/10.1609/icwsm.v13i01.3229\n[43] Shan Jiang, Ronald E. Robertson, and Christo Wilson. 2020. Reasoning about\npolitical bias in content moderation. In AAAI Conference on Artificial Intelligence\n(AAAI) . AAAI. https://doi.org/10.1609/aaai.v34i09.7117\n[44] David Kemp and Emily Ekins. 2021. Poll: 75% don\u2019t trust social media to make fair\ncontent moderation decisions, 60% want more control over posts they see . Pew Re-\nsearch Center. https://www.cato.org/survey-reports/poll-75-dont-trust-social-\nmedia-make-fair-content-moderation-decisions-60-want-more\n[45] Kate Klonick. 2018. The new governors: The people, rules, and processes gov-\nerning online speech. Harvard Law Review 131, 6 (2018), 1598\u20131670. https:\n//www.jstor.org/stable/44865879\n[46] Media Bias/Fact Check 2021. Media Bias/Fact Check . https://mediabiasfactcheck.\ncom/\n[47] Nomann Merchant. 2022. US accuses financial website of spreading\nRussian propaganda . Associated Press News. https://apnews.com/\narticle/russia-ukraine-coronavirus-pandemic-health-moscow-media-\nff4a56b7b08bcdc6adaf02313a85edd9\n[48] NewsGuard 2021. NewsGuard . https://www.newsguardtech.com/\n[49] OnlyFans 2022. OnlyFans terms of service . OnlyFans. https://onlyfans.com/terms\n[50] Orestis Papakyriakopoulos, Juan Carlos Medina Serrano, and Simon Hegelich.\n2020. The spread of COVID-19 conspiracy theories on social media and the effect\nof content moderation. Harvard Kennedy School (HKS) Misinformation Review\n(2020). https://doi.org/10.37016/mr-2020-034\n[51] Geah Pressgrove, Brooke Weberling McKeever, and S Mo Jang. 2018. What is\ncontagious? Exploring why content goes viral on Twitter: A case study of the\nALS Ice Bucket Challenge. International Journal of Nonprofit and Voluntary Sector\nMarketing 23, 1 (2018). https://doi.org/10.1002/nvsm.1586\n[52] Manoel Ribeiro, Pedro Calais, Yuri Santos, Virg\u00edlio Almeida, and Wagner Meira\nJr. 2018. Characterizing and detecting hateful users on Twitter. In International\nConference on Web and Social Media (ICWSM) . AAAI. https://doi.org/10.1609/\nicwsm.v12i1.15057\n[53] Sarah Roberts. 2016. Commercial content moderation: Digital laborers\u2019 dirty work.\nInThe intersectional Internet: Race, sex, class and culture online , Safiya Noble and\nBrendesha Tynes (Eds.). Peter Lang Publishing. https://ir.lib.uwo.ca/commpub/12\n[54] Minna Ruckenstein and Linda Lisa Maria Turunen. 2020. Re-humanizing the\nplatform: Content moderators and the logic of care. New Media & Society 22, 6\n(2020), 1026\u20131042. https://doi.org/10.1177/1461444819875990\n[55] Jim Rutenberg, Nick Corasaniti, and Alan Feuer. 2020. Trump\u2019s fraud claims\ndied in court, but the myth of stolen elections lives on . The New York Times.\nhttps://www.nytimes.com/2020/12/26/us/politics/republicans-voter-fraud.html\n[56] Adam Satariano and Mike Isaac. 2021. The silent partner cleaning up Facebook for\n$500 million a year . The New York Times. https://www.nytimes.com/2021/08/\n31/technology/facebook-accenture-content-moderation.html\n[57] Qinlan Shen and Carolyn P. Ros\u00e9. 2022. A tale of two subreddits: Measuring\nthe impacts of quarantines on political engagement on Reddit. In International\nConference on Web and Social Media (ICWSM) . AAAI. https://doi.org/10.1609/\nicwsm.v16i1.19347\n11\nIan Goldstein, Laura Edelson, Minh-Kha Nguyen, Oana Goga, Damon McCoy, and Tobias Lauinger\n[58] Jieun Shin, Lian Jian, Kevin Driscoll, and Fran\u00e7ois Bar. 2018. The diffusion of mis-\ninformation on social media: Temporal pattern, message, and source. Computers\nin Human Behavior 83 (2018), 278\u2013287. https://doi.org/10.1016/j.chb.2018.02.008\n[59] Kumar Bhargav Srinivasan, Cristian Danescu-Niculescu-Mizil, Lillian Lee, and\nChenhao Tan. 2019. Content removal as a moderation strategy: Compliance and\nother outcomes in the ChangeMyView community. In Conference on Computer-\nSupported Cooperative Work (CSCW) . ACM. https://doi.org/10.1145/3359265\n[60] Wall Street Journal Staff. 2021. The Facebook files . The Wall Street Journal.\nhttps://www.wsj.com/articles/the-facebook-files-11631713039\n[61] Nicolas Suzor, Sarah West, Andrew Quodling, and Jillian York. 2019. What do\nwe mean when we talk about transparency? Toward meaningful transparency\nin commercial content moderation. International Journal of Communication 13\n(2019). https://ijoc.org/index.php/ijoc/article/view/9736\n[62] Iman Tahamtan and Javad Seif. 2019. The online resources shared on Twitter\nabout the #MeToo movement: The Pareto principle. (2019). arXiv:1906.12321\n[63] Craig Timberg, Elizabeth Dwoskin, and Reed Albergotti. 2021. Inside Face-\nbook, Jan. 6 violence fueled anger, regret over missed warning signs . Washington\nPost. https://www.washingtonpost.com/technology/2021/10/22/jan-6-capitol-\nriot-facebook/\n[64] Tomasz Trzci\u0144ski and Przemys\u0142aw Rokita. 2017. Predicting popularity of online\nvideos using support vector regression. IEEE Transactions on Multimedia 19, 11\n(2017), 2561\u20132570. https://doi.org/10.1109/TMM.2017.2695439[65] David Vallet, Shlomo Berkovsky, Sebastien Ardon, Anirban Mahanti, and Mo-\nhamed Ali Kafaar. 2015. Characterizing and predicting viral-and-popular video\ncontent. In Conference on Information and Knowledge Management (CIKM) . ACM.\nhttps://doi.org/10.1145/2806416.2806556\n[66] Emily Vogels, Andrew Perrin, and Monica Anderson. 2020. Most Ameri-\ncans think social media sites censor political viewpoints . Pew Research Cen-\nter. https://www.pewresearch.org/internet/2020/08/19/most-americans-think-\nsocial-media-sites-censor-political-viewpoints/\n[67] Jane Wakefield. 2019. Christchurch shootings: Social media races to stop attack\nfootage . BBC. https://www.bbc.com/news/technology-47583393\n[68] Charlie Warzel. 2020. Facebook can\u2019t be reformed . The New York Times. https:\n//www.nytimes.com/2020/07/01/opinion/facebook-zuckerberg.html\n[69] Sarah Myers West. 2018. Censored, suspended, shadowbanned: User interpreta-\ntions of content moderation on social media platforms. New Media & Society 20,\n11 (2018), 4366\u20134383. https://doi.org/10.1177/1461444818773059\n[70] Savvas Zannettou. 2021. \u201cI won the election!\u201d: An empirical analysis of soft\nmoderation interventions on Twitter. In International Conference on Web and\nSocial Media (ICWSM) . AAAI. https://doi.org/10.1609/icwsm.v15i1.18110\n[71] Lu Zhou, Wenbo Wang, and Keke Chen. 2016. Tweet properly: Analyzing deleted\ntweets to understand and identify regrettable ones. In International Conference\non World Wide Web (WWW) . ACM. https://doi.org/10.1145/2872427.2883052\n12", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Understanding the (in) effectiveness of content moderation: A case study of facebook in the context of the us capitol riot", "author": ["I Goldstein", "L Edelson", "MK Nguyen", "O Goga"], "pub_year": "2023", "venue": "arXiv preprint arXiv \u2026", "abstract": "Social media networks commonly employ content moderation as a tool to limit the spread of  harmful content. However, the efficacy of this strategy in limiting the delivery of harmful"}, "filled": false, "gsrank": 564, "pub_url": "https://arxiv.org/abs/2301.02737", "author_id": ["", "5-5-AqcAAAAJ", "w4cKb8EAAAAJ", "re_squoAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:e-z4QALqJzAJ:scholar.google.com/&output=cite&scirp=563&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D560%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=e-z4QALqJzAJ&ei=bLWsaO2lAY6IieoP0sKRuAk&json=", "num_citations": 8, "citedby_url": "/scholar?cites=3469999333313145979&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:e-z4QALqJzAJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2301.02737"}}, {"title": "Global Perspectives of AI Risks and Harms: Analyzing the Negative Impacts of AI Technologies as Prioritized by News Media", "year": "2025", "pdf_data": "Global Perspectives of AI Risks and Harms: Analyzing the\nNegative Impacts of AI Technologies as Prioritized by News Media\nMowafak Allaham\nmowafakallaham2021@u.northwestern.edu\nNorthwestern University\nEvanston, IL, USAKimon Kieslich\nInstitute for Information Law,\nUniversity of Amsterdam\nAmsterdam, The Netherlands\nk.kieslich@uva.nlNicholas Diakopoulos\nNorthwestern University\nEvanston, IL, USA\nnad@northwestern.edu\nABSTRACT\nEmerging AI technologies have the potential to drive economic\ngrowth and innovation but can also pose significant risks to so-\nciety. To mitigate these risks, governments, companies, and re-\nsearchers have contributed regulatory frameworks, risk assessment\napproaches, and safety benchmarks, but these can lack nuance\nwhen considered in global deployment contexts. One way to un-\nderstand these nuances is by looking at how the media reports on\nAI, as news media has a substantial influence on what negative\nimpacts of AI are discussed in the public sphere and which impacts\nare deemed important. In this work, we analyze a broad and di-\nverse sample of global news media spanning 27 countries across\nAsia, Africa, Europe, Middle East, North America, and Oceania to\ngain valuable insights into the risks and harms of AI technologies\nas reported and prioritized across media outlets in different coun-\ntries. This approach reveals a skewed prioritization of Societal Risks\nfollowed by Legal & Rights-related Risks ,Content Safety Risks ,Cog-\nnitive Risks ,Existential Risks , and Environmental Risks , as reflected\nin the prevalence of these risk categories in the news coverage of\ndifferent nations. Furthermore, it highlights how the distribution\nof such concerns varies based on the political bias of news outlets,\nunderscoring the political nature of AI risk assessment processes\nand public opinion. By incorporating views from various regions\nand political orientations for assessing the risks and harms of AI,\nthis work presents stakeholders, such as AI developers and policy\nmakers, with insights into the AI risks categories prioritized in the\npublic sphere. These insights may guide the development of more\ninclusive, safe, and responsible AI technologies that address the\ndiverse concerns and needs across the world.\nCCS CONCEPTS\n\u2022Do Not Use This Code \u2192Generate the Correct Terms for\nYour Paper ;Generate the Correct Terms for Your Paper ; Generate\nthe Correct Terms for Your Paper; Generate the Correct Terms for\nYour Paper.\nKEYWORDS\nAI governance, risk assessment, AI safety, news media, Global South\nACM Reference Format:\nMowafak Allaham, Kimon Kieslich, and Nicholas Diakopoulos. 2025. Global\nPerspectives of AI Risks and Harms: Analyzing the Negative Impacts of\nUnder review, Global Perspectives of AI Risks and Harms, 2025\n2025. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnAI Technologies as Prioritized by News Media. In Proceedings of 2025 (Un-\nder review). ACM, New York, NY, USA, 23 pages. https://doi.org/10.1145/\nnnnnnnn.nnnnnnn\n1 INTRODUCTION\nAI is becoming more integrated into various systems and applica-\ntions that are serving millions of user globally. Despite its potential\nto drive innovation, economic growth, and increases in productivity\n[41,64,91], it poses significant risks to society, especially if mis-\nused [ 13,21,33,85,109]. In response, governments, companies, and\nresearchers have contributed regulatory frameworks [ 11,63], risk\nassessments [ 1,71,73,100], and safety benchmarks [ 3,110,111] to\ngovern [ 90], anticipate [ 6,8,22,47,55,58], and potentially mitigate\nsuch risks. However, identifying risks of emerging AI technologies\nremains difficult, for a variety of reasons including the complex\nentanglements and interaction effects with the social worlds of\nhuman behavior and policy, particularly across different nations\nand societies, and especially during the pre-deployment phases of\ndesign and development [ 2,14,48]. Without such information, AI\ndevelopers cannot fully assess the potential safety concerns and\nrisks of technologies they are developing for a global user base. In\nfact, prior research suggests that if technology developers had been\naware of the impacts caused by similar technologies they are devel-\noping, many of these impacts could have been prevented through\ncareful evaluations early in the design process of these technologies\n[19, 37, 84].\nIn response, a few initiatives have emerged to organize, re-\nport, and monitor the observed risks and harms of AI technolo-\ngies post-deployment in the form of incident databases [ 67,78].\nThese databases document incidents based on the undesirable con-\nsequences of AI systems and technologies as articulated in news\nmedia (e.g, AI Incident Database, and OECD AI Incidents Monitor).\nWhile these databases are publicly accessible, they fall short in\nterms of (1) synthesizing global and region-specific risks associated\nwith the deployment of AI technologies across different nations\nand societies, (2) articulating which risks are considered more crit-\nical by each nation, and (3) expressing how these risks may vary\nwhen considering the political biases surrounding risk perceptions\n[83]. Addressing these gaps contributes towards more inclusive risk\nassessments of AI that incorporate perceptions of these risks from\nregions that are usually under-represented, such as the Middle East\nand Africa, in the risk and safety evaluations of AI. Moreover, ana-\nlyzing the interplay between the political bias of news outlets that\nare shaping the perceptions of these risks and the prioritization of\nthese risks by different nations, may help mitigate future polarizing\ndebates around AI risks, such as around economic harms [ 51], thatarXiv:2501.14040v2  [cs.CY]  6 Feb 2025\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nmay influence the public perception of AI during the current and\nfuture development of AI regulations and policies. To address these\nissues, this research pursues the following research questions: RQ1:\nWhat are the categories of AI risks covered in global news media?\nRQ2 :What is the prevalence of these risks across global regions? and\nRQ3 :How does the prevalence of these risks vary across regions when\nconsidering the political bias of the outlets reporting on them?\nAlthough news media reflects its own set of normative biases\nabout what is prioritized for coverage and how AI is covered [ 27,76],\nit plays a crucial role in highlighting risks and harms relevant to the\ngeneral public and shaping their perception of AI\u2013a key stakeholder\nin shaping the current and future development of AI regulations\nand public policy in democratic countries. Accordingly, by analyz-\ning a broad and diverse sample of global news media spanning 27\ncountries spread across six regions (Asia, Africa, Europe, Middle\nEast, North America, and Oceania), our research identified novel\nrisks that are not as prominently covered by risk frameworks and\nsafety benchmarks. In addition, it provides insights into the cover-\nage of AI risks and harms reported across media outlets globally\nand at the regional level. Specifically, our findings reveal the global\nprevalence of 6 AI risks in our sample from news media coverage\nof AI: Societal Risks ,Legal & Rights-related Risks ,Cognitive Risks ,\nContent Safety Risks ,Existential Risks , and Environmental Risks , re-\nflecting how those risks are prioritized by media and thus help set\nthe agenda for public perception. Furthermore, our findings high-\nlight how the coverage of such risks varies based on the political\nbias of news outlets, which is crucial to account for to better shape\nmore inclusive and globally informed AI regulations.\nBy leveraging news media, our research provides some guid-\nance to stakeholders and entities involved in the development of\nAI technologies within a country, but with potential plans to de-\nploy them regionally or globally in a safe and responsible manner.\nIn addition, this work offers a bottom-up approach that AI safety\nexperts, and regulators can use to (1) understand and prioritize\nevaluations of certain risks and harms associated with AI tech-\nnologies across different nations, (2) provide insights into potential\npartisan (mis)alignment with respect to various risks and harms of\nAI technologies which may be useful for politically-aware gover-\nnance initiatives, and (3) expand existing impact assessment and\nAI safety frameworks to incorporate global and broader risks and\nharms associated with the use of AI technologies across different\nsocieties.\n2 RELATED WORK\nHere we first critique current risk assessment practices, focusing on\nthe strong emphasis on expert voices from the Global North, and a\nproposal of the analysis of news media coverage as a promising way\nto enrich the current assessment landscape. Thus, in subsection\ntwo, we discuss the role of media coverage in risk assessment.\n2.1 AI Risk Assessments\nIn order to safeguard the development and implementation of AI\ntechnology, risk assessment of AI has received increasing attention\nin recent years. Governments, companies, and researchers alike\nhave published an enormous number of risk assessment reports\nand safety benchmarks (e.g., [ 12,39,99,107,108,110]. While theseframeworks uncover a variety of different risks, scholars have also\ncritically interrogated current risk assessment practices [ 46,56]. A\nmajor structural concern with current risk assessment practices is\nthe allocation of power. Who defines risks in the first place and with\nwhat intentions. In particular, scholars have criticized first or second\nparty risk assessments, i.e., technology developers and external\nparties hired by developing companies to conduct risk assessments\nor audits [ 28,42,46,86]. Since the main goal of companies is to\nbe economically successful, researchers fear that assessments by\ncompanies or their research teams tend to focus on those risks\nthat are relatively easy to address and leave out other issues (e.g.,\nsocio-technical issues) [44].\nRelated to this, scholars criticize a lack of inclusiveness in risk\nassessment practices. Often, risk assessment is based on the expert\njudgment of researchers or developers. However, while this exper-\ntise is certainly necessary to identify a plethora of risks, it is still\nlimited to the professional view of these experts and fails to recog-\nnize risks outside their experience [ 73]. But some risks materialize\nonly for users or affected groups and communities, especially for\ntechnologies that are highly shaped by user input, such as LLMs.\nScholars have thus emphasized the importance of non-expert knowl-\nedge as an additional resource for risk detection [ 71,72]. The lived\nexperiences of a wider array of stakeholders can help to detect\nnew harms that are not anticipated by experts, as well as to get\na sense of the importance and materialization of these harms for\naffected people [ 87]. There is also a strong focus on countries in the\nGlobal North. However, people in the Global South have different\nchallenges with AI, and consequently different risks may emerge\nthat are not covered by current risk assessments [ 93,105]. While\nscholars recognize the need to increase the inclusion of indepen-\ndent voices (including non-experts) [ 30,60,71,72], few assessments\nactually do so (with some exceptions such as [2, 7, 55, 58]). In this\nstudy, we focus on a resource that is 1) structurally more indepen-\ndent from corporate practices, and 2) should (by quality standards,\nand when appropriately sampled) ensure a broader inclusion of\nsocietal perspectives: media coverage.\n2.2 The Role of Media Coverage in Risk\nAssessments\nThe media play an influential role in shaping the national and public\ndiscourse on AI, including by helping to set the standards and expec-\ntations for AI accountability [ 36]. In the traditional understanding\nof communication science, the media function as agenda setters\n[66]. A key task of the media is to inform a broad public about\npolitically and socially relevant issues [ 97] and thereby ensure a\nplurality of voices, i.e. the inclusion of all societal stakeholders. How\nthe media portray these technologies is critical, as media coverage\nhas been shown to influence public opinion [ 77,96], especially for\nnovel technologies like AI with which ordinary citizens have little\npersonal experience [ 50]. Public opinion thus plays a crucial role in\ntechnology adoption. On the one hand, citizens act as consumers\nof AI technology, and the media\u2019s portrayal of AI can influence\nwhether or not people are willing to use the technology [ 82]. On the\nother hand, citizens can act as voters and thus influence regulatory\naspects [54, 59, 82].\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\nRecognizing the importance of the news media in relation to AI,\na significant number of scholars have focused on analyzing how\nthe news discusses AI. Most studies of media coverage focus on\nmedia coverage from countries in the Global North, such as the\nUS [27,40], the UK [ 16,18,94], Germany [ 57,70], the Netherlands\n[104], or take a comparative approach between the US and the UK\n[20,76] or the US and China [ 75]. Only a few studies focus on\nnon-Western countries, with the exception of China [ 110] and a\ncomparative study of 12 countries, including countries of the Global\nNorth and the Global South [ 52]. Thematically, studies on media\ncoverage mostly focus on mapping the general discourse on AI, for\nexample by analyzing the thematic structure or sentiment of media\ndiscourse (e.g. [ 16,52]), while rarely focusing explicitly on risks or\nnegative impacts (with the exception of [27, 76, 82]).\nOne of the main principles of journalistic news quality is to rep-\nresent a plurality of voices that are relevant to the discourse, and the\ninclusion of these voices (e.g., activists, academics, civilians, NGOs)\ncan enrich the discourse on AI [ 16]. In particular, when it comes to\nreporting on AI risks, the efforts of investigative journalists have\nhelped shed light on pressing issues such as the child benefit scan-\ndal in the Netherlands [ 29] or the COMPAS recidivism algorithm in\nthe US [ 4]. Indeed, scholars first articulated the idea of \u201calgorithmic\naccountability\u201d as stemming from investigations published in the\nnews media [ 35], and more recently have argued for the inclusion\nof journalists in third-party audits of AI systems, as they \u201cwere\nresponsible for uncovering deeply-rooted socio-technical harms in\nalgorithmic systems related mainly to representational harms due\nto discriminatory design choices. \u201d [ 46]. This is supported by the fact\nthat many of the sources of the AI Incident Database [ 67] are news-\npaper articles. As a result, news coverage of AI risks plays a key role\nin exposing the risks of AI systems. Unlike self-reporting by com-\npanies (including their research teams), journalists are structurally\nindependent and can uncover novel impacts that may conflict with\ncorporate goals. Their inclusion \u201censure[s] social accountability\nthrough domain knowledge and special access to affected commu-\nnities\u201d [46].\nAnother important factor when considering the impact of dis-\ncourse on public perception is its politicization. Scholarly research\nin this area states that politicization of an issue requires three con-\nditions [ 34,95]: (1) polarization of the issue, i.e., whether and how\nprevalent different (political) positions are on the issue. This could\nalso be achieved by different framing or agenda setting of topics\nrelated to the issue (e.g. different prevalence of AI risks). (2) The\nintensity of media coverage. This refers to the visibility of an issue.\nThe more it is covered, the more relevance is attributed to the issue.\nAnd (3) The resonance of the issue, i.e. how relevant the issue is in\nthe eyes of the public. Media coverage plays a key role in this regard,\nas it provides an important arena in which AI is discussed. Several\nstudies of media coverage have found a sharp increase in news\ncoverage of AI in recent years [ 27,40,52,82,104], which satisfies\nthe condition of intensity of coverage. Several scholars have also\nanalyzed the influence of the political leaning of the news outlet\non the framing of AI \u2013 with mixed results [ 16,94,104]. However,\nthe politicization of the AI risk debate in particular has not been\nexplored. Analyzing the politicization of AI risks in terms of politi-\ncal positions is important because it reflects political strategies in\nterms of regulation or policy enforcement. Furthermore, it showshow citizens who consume politically biased news are informed\nand perceive the AI risk discourse.\nOverall, we find that there is a gap in the use of news sources\nin risk assessment practices and that literature on news analysis\ndoesn\u2019t engage with AI risks. Therefore, in this paper, we address\nthese gaps by (1) using news as a source to inform risk assessment,\n(2) explicitly focusing on AI risks in the study of news content, (3)\ntaking a global perspective in analyzing news from countries in\nthe Global North and Global South, thus contributing to a more\ndiverse analysis of AI risks, and (4) analyzing the effect of political\npositioning of news coverage on the prevalence of AI related risks.\n3 DATA\nCurating AI-related articles from news media . To establish a\ndataset of online news articles related to AI from around the world,\nwe used GDELT [ 61] (Global Data on Events, Location and Tone\nProject) to collect online news articles published in English between\nJanuary 2022 and October 2024 based on a previously published\nlist of 41 AI-relevant keywords in English (listed in Appendix A.1)\nsourced from news media [ 2]. GDELT captures extensive cover-\nage of what is reported on in news media across different news\noutlets, languages, and offers an accessible way of querying such\ncoverage via an API [ 61,106]. In addition, it presents an alternative\nto scraping news portals and aggregators, such as Google News,\nthat display or rank news content that is recently published by\npopular news outlets, politically biased (i.e., slight leftward bias), or\nlimited in exposure to global perspective [ 49,74,102]. To retrieve\nAI-related articles from GDELT, we constructed multiple API re-\nquests in Python using GDELT\u2019s v2 API endpoint1. For each of\nthe 41 AI-related keywords, an API request was sent to GDELT\nto retrieve the URLs and metadata of the daily published news\narticles mentioning that keyword. In total, we retrieved all URLs\ncorresponding to 1,218,058 online articles published between Janu-\nary 2022 and October 2024 by 21,383 unique news outlets from 30\ncountries around the world.\nFiltering for national news domains . To analyze the risks\nof AI prioritized in news coverage across different countries, we\nrefined our data by filtering URLs to include only those from na-\ntional news domains in each country. This step is necessary to\nensure that our sample is free from content published by corporate\nor technical blogs. To this end, we leveraged the Global English\nLanguage Sources [ 69] from Media Cloud (MC) [ 92]. MC\u2019s list of\n1,064 national news outlets that publish articles in English across\n177 countries was used to filter the domains of articles we collected\nfrom the previous step. After applying the filter, we are left with\n235,636 URLs with 600 domains from 30 countries.\nDomain bias rating Although the global representation of news\nmedia in our sample is useful for understanding patterns of media\ncoverage of negative impacts of AI, prior research has shown that\nthe political bias of news domains tend to influence the discourse\naround scientific topics such as climate change [ 5,26] or emerging\ntechnologies (e.g., nuclear energy), including AI [ 16,94,104]. To\nrate the domains in our sample, we used Media Bias Fact Check\n(MBFC) domain-level ratings [ 25]. MBFC is an independent website\nmaintained by researchers and journalists that relies on human\n1https://api.gdeltproject.org/api/v2/doc/\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nfact-checkers affiliated with the International Fact-Checking Net-\nwork to evaluate media sources along different dimensions such as\nfactual reporting and bias [ 62]. MBFC categorizes news sources in\none of nine bias categories: least biased, left bias, left-center bias,\nright-center bias, right bias, conspiracy-pseudoscience, question-\nable sources, pro-science, and satire [ 25]. After aligning domains in\nour sample with MBFC ratings and excluding those categorized as\nquestionable, conspiracy-pseudoscience, pro-science, or satire\u2013as\nthese domains focus more on factuality and type of content than\npolitical bias\u2013our refined sample includes 277 domains, constitut-\ning 163,314 URLs to articles ( \u223c69.3% of the filtered articles per the\nprevious step).\nScraping news articles . For each URL, we attempted to scrape\nthe article text and title using a custom web scraper in Python\nthat leverages the newspaper library [ 81]. We could not scrape\na majority of the articles due to pay- or sign up walls or missing\ncontent (i.e., 404 errors). As a result, the final sample containing\nthe scraped articles includes 42,853 ( \u223c26.2% of 163,314) articles\npublished by 168 domains spanning 27 countries (see distribution\nof articles per country and global region in Table 1).\n4 METHODS\n4.1 Summarizing negative impacts of AI from\nnews media using an LLM\nFiltering articles by content . For the filtering step, we follow a\nsimilar zero-shot prompting approach to the one reported in pre-\nvious research with a similar objective of detecting the negative\nconsequences of AI in news media [ 84]. To develop our prompt, we\nreferred to prior research on social impact assessments [ 9,73] to\nsynthesize the following conceptual definition of an impact of an\nAI technology that we used to steer the LLM: An impact refers to an\neffect, consequence, or outcome of an AI system (i.e., model or appli-\ncation) that positively or negatively affects individuals, organizations,\ncommunities, or society . We did not limit the conceptual definition\nto negative impacts per se, so as to provide opportunity for future\nwork that may want to focus on positive impacts of AI technologies\n[58].\nTo assess the ability of the LLM for classifying articles reporting\nimpacts of AI, we randomly sampled 300 articles from our corpus.\nOne of the authors annotated each article to determine whether it\ncontained at least one impact of AI, based on our definition and\nfound that most articles (77%) had an impact. Using this annotated\nsample and prompt A.4, we used GPT-4o to classify each article\nas either containing an impact or not. The model performed well\non this task achieving an F1-macro score of 0.82 (macro-averaged\nprecision=0.81 and macro-averaged recall=0.83). We then applied\nthis prompt using the OpenAI batch API to the rest of the dataset.\nOut of the 42,853 articles in our sample, as reported in section 3,\nGPT-4o classified 32,439 (75.69%) as containing impacts.\nSummarizing negative impacts After identifying 32,439 arti-\ncles as having impacts of AI, we instructed GPT-4o with prompt\nA.5, including the entire article text as context to the model, to\nsummarize all the negative impacts reported in each article. Each\narticle could contain multiple impacts, and each was represented\nseparately in a list. This process resulted in 47,731 negative impactsof AI that were summarized from 16,312 articles sourced from 157\ndomains spanning 27 countries.\n4.2 Annotating negative impacts from news\nmedia\nHuman annotations of negative impacts . Due to the large num-\nber of negative impacts summarized from articles as outlined in the\nprevious section, we explored clustering these impacts and annotat-\ning the resulting clusters by sampling and annotating impacts from\neach cluster (as described in details in Appendix A.8). However, we\nobserved that the impact statements within each cluster refer to\ndifferent types of impacts. Accordingly, we modified the annotation\nprocess so each summary of impact statement is annotated based\non the type of impact, rather than the contextual use of AI per\nthe outcome of the clustering approach, as an AI technology could\nhave more than one negative impact within the same contextual\nuse which could provide us with the range of impacts necessary\nfor subsequent analysis. To do so, we 1) involved two authors to\nindependently annotate a randomly selected sample of negative\nimpacts, and then 2) calculate the inter-coder reliability to confirm\nthe validity of the annotated impact categories.\nBased on the AI Risk Categorization taxonomy (AIR-taxonomy)\n[110], two authors independently annotated a total of 1,060 negative\nimpacts. Although other expert-driven taxonomies of AI risks exist\n[98\u2013100,108], prior research found that these taxonomies may\nsuffer from inadvertent expert or selection biases and may not\nbe as representative of global perspectives of AI risks and harms\n[2,14,31,45,53]. Alternatively, the AIR-taxonomy presents a global,\nyet limited, perspective into the risks and harms of AI that is derived\nfrom eight government policies, including the European Union,\nUnited States, and China, and 16 company policies worldwide. This\nglobal representation of risks, at least by governments and industry,\nis more aligned with our research objective of incorporating global\nperspectives of AI risks and harms into the impact assessment\nprocess of AI.\nFor each impact summary in the sampled impacts, annotators\ncategorized the impact summary into one of the 16 Level-2 cate-\ngories of risks presented in the AIR-taxonomy [ 110]. We decided\nto code summaries of impacts at Level-2, because it offers a bal-\nanced level of granularity of the risk categories that are grouped\nbased on societal impact [ 110] and enable us to re-organize the\nannotated impacts into Level-1 for subsequent analyses and com-\nparisons between regions. If an impact summary aligns with one of\nthe AIR-taxonomy\u2019s Level-2 subcategories of risks, it is annotated\nwith that category\u2019s label. Alternatively, an impact summary is\nassigned an \u201cother\u201d label. Next, per the constant comparison pro-\ncess in qualitative thematic analysis [ 15], each impact summary\nin the \u201cother\u201d category was assigned to an emerging category of\nrisks. The definitions corresponding to the emerging categories\nwere continuously revised and assessed throughout the coding pro-\ncess to ensure that these categories made conceptual sense, and\nthen decide whether they needed to be reorganized, merged, or\nfurther broken into more categories as suggested by qualitative\nresearch methods [ 15]. After coding all 1,060 impact summaries,\nthe categories were largely saturated and stable showing no new\nemerging categories of risks.\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\nTo evaluate the reliability of the annotated impacts by human\ncoders based on the AIR-taxonomy, as well as on the emerging cate-\ngories, we calculated Kripendorph\u2019s alpha [ 65]. The metric showed\na high inter-coder reliability (K-alpha=0.97) between the two an-\nnotators, indicating that the definitions of the impact categories\nare consistent and stable. The distribution and prevalence of each\nimpact category in the sample can be found in Table 2 in the Appen-\ndix. The list of the risk categories and their definitions is provided\nin Appendix A.7, which is also utilized to annotate the remaining\nnegative impacts in our corpus using an LLM.\nLLM annotations of negative impacts . To scale up the anno-\ntation process using an LLM, we had to first validate the efficacy of\nthe LLM in annotating impacts to their corresponding categories\nbased on the sample of 1,060 annotated impacts. Accordingly, using\na zero-shot prompting approach, we sent requests via the OpenAI\nAPI instructing GPT-4o to annotate each impact summary for only\none of the defined categories by the annotators, as described in\nthe previous section and outlined in Prompt A.6. In addition, we\nincluded in the prompt an instruction for the LLM to assign an\n\u201cother\u201d label for impacts that do not fit any of the categories defined\nin the prompt, mirroring the best practices of thematic analysis\n[15]. We also included a \u201cno_impact\u201d category to capture potential\nfalse positives in the data\u2013summaries from articles that do not fit\nour definition of a negative impact.\nUsing the human annotated categories of the 1,060 impacts as\na baseline, we evaluated the efficacy of GPT-4o on this task after\naligning the annotated categories back to Level-1 of AIR-taxonomy,\nfocusing our analysis at the category rather than sub-category level,\nto facilitate a more interpretable comparison of risk categories\nacross regions. While the model achieved a suboptimal macro aver-\naged F1-score of 0.60, it is comparable to results observed in other\nmulti-class classification tasks [ 84,88]. Despite that, and based on\nthe classification report presented in Table 3, we observed a few\ncategories having a decent recall score of 0.70 or higher but are also\nassociated with low precision scores (i.e., low to mid sixties). We\nfocused our evaluation on recall scores, rather than F1-scores, to as-\nsess the LLM\u2019s ability in capturing the majority of relevant instances\nwithin each category. This decision was influenced by the LLM\u2019s\npoor performance on certain categories, such as Structure/Power,\nwhich impacts the overall F1-score. By manually reviewing in-\nstances of categories that had a recall score of 0.7 or higher, such\nas Cognitive Risks or Content Safety Risks (as shown in Table 3),\nwe observe that when the LLM misclassifies an impact, it predomi-\nnately classifies it into other plausible impact categories, from the\n31 categories of risks articulated in the previous section, rather\nthan the category assigned by the two annotators. This has also\nbeen observed in a similar tasks focusing on extracting concerns\nregarding AI and algorithmic platform decisions from discussion\nforums [ 88]. Furthermore, our manual review also shows around\n14.6% (155 out of 1,060) of impact summaries were misclassified by\nGPT-4o into a wrong and irrelevant category, affirming that the\nimpacts with the recall scores of 0.70 or higher but low precision\n(i.e., low to mid sixties) can still contribute to the analysis.\nBased on the classification report presented in Table 3, and the\nsensitivity score of the LLM on each category, we selected six impact\ncategories that had at least a recall score of 0.7 or higher for further\nanalysis: Societal Risks, Legal & Rights-related Risks, Cognitive Risks,Content Safety Risks, Existential Risks, Environmental Risks . The\nprevalence of these categories of risks across geographical regions\nand political ideologies is analyzed in the results section.\n5 RESULTS\n5.1 Emerging AI risk categories from news\nmedia\nThe global representation of news media coverage of AI in our\nsample captures a broad range of risks and concerns about AI across\ndifferent nations, enabling the expansion of the AIR-taxonomy\nwith the addition of new categories. By annotating these risks and\nconcerns in a sample, as described in Section 4.2, we contribute\n16 risk categories, marked with asterisk in Table 2 and defined\nin A.7, that are not present at any level of the AIR- taxonomy\n[110]. These 16 risk categories, which emerged from annotating\nour sample, include: AI Governance Risks, Authoritarian Use of AI\nRisks, Disruption of Service Risks, Environmental Risks, Ethical Risks,\nExistential Risks, Humanness Risks, Information Risks, Media Risks,\nMental & Emotional Risks, Over-reliance Risks, Performance Risks,\nSafety Risks, Structure & Power Risks, Technology Adoption Risks,\nand User Experience Risks . All emerging categories were aligned\nback at Level-1 of the AIR-taxonomy to enable an analysis at the\ncategory rather than sub-category level, which facilitates a more\ninterpretable comparison of risk categories across regions.\nTo align these emerging risk categories with the AIR-taxonomy\nof AI risks, one possible way is to organize the categories based\non their contextual independence and their potential in addressing\ngaps in the taxonomy through elaborating on existing categories.\nFor instance, Authoritarian Use of AI andMedia Risks (i.e., risks of\nAI on the news and media industries) can possibly be integrated\nas Level-2 categories under Societal Risks .Performance Risk ,Safety\nRisks , and Disruption of Service Risks could be grouped under the\nexisting category of System and Operational Risks at Level-1. In\ncontrast, Mental & Emotional Health ,Humanness Risks , and Over-\nreliance Risks did not fit any of the existing categories in the AIR-\ntaxonomy and all share cognitive implications to using or interact-\ning with AI, therefore one possibility is to aggregate them under a\nnew category at Level-1 named Cognitive Risks . In addition, Usabil-\nity Risks could encompass the two emerging categories related to\nTechnology Adoption Risks andUser Experience Risks . All remaining\nemerging risk categories\u2013 Information Risks, Ethical Risks, Existen-\ntial Risks, AI Governance Risks, and Structure/Power Risks \u2013were left\nas independent risk categories at Level-1, as we did not find them\nfit in any of the existing categories of the AIR-taxonomy. While\nour grouping of these categories introduce an expert bias to the\nanalysis, due to our interpretation of these risks in our sample, we\nacknowledge that these risks can be grouped in various ways. Thus,\nwe encourage future research in adopting a more theory-driven\napproach to grouping these categories by drawing from the impact\nassessment literature to understand their fit across a wider array of\ntypologies.\nAs a result of organizing and aligning these emergent categories\nof risks, we suggest 8 potential AI risks categories to the AIR-\ntaxonomy at Level-1, bringing the total number of risk categories to\n12, as listed in Table 4 in the Appendix. However, as outlined in the\nannotation step in Section 4.2, and based on the classification report\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nFigure 1: Figure (A) provides an overview of the media structure of AI coverage across regions stratified by political bias.\nNews coverage of AI in our sample is predominantly driven by news outlets with left center bias. Figure (B) shows the global\nprevalence of AI Risks for each category. To calculate the global prevalence, we counted the number of articles reporting risks\nrelated to each category and divided each number by the total number of articles across regions and categories (i.e., 16,312).\nin Table 3, we choose to only include 6 of the 12 risk categories in\nsubsequent analyses due to the model\u2019s satisfactory classification\nperformance on these categories: Societal Risks, Legal and Rights-\nRelated Risks, Cognitive Risks, Content Safety Risks, Existential Risks,\nand Environmental Risks . Next, we elaborate on these six main risks\ncategories.\nSocietal Risks - features risks of AI that have negative societal\nimplications on the public through AI-generated deception or ma-\nnipulation. Other risks include the implications of AI on politics\nsuch as influencing elections by \u201cproducing election misinformation\nabout ballot deadlines\u201d or \u201cdisseminate[ing] fake news\". Other soci-\netal risks of AI have implications on the economy, which involves\ndis-empowering workers by potentially \u201creplace[ing] or take[ing]\nover jobs\u201d or may cause instability to financial systems if AI starts\nto develop \u201ccomplex financial mechanisms that may become too\nsophisticated for any human to understand, potentially leading to\na loss of control over the financial system\u201d or \u201cdistorting market\nsignals in high-frequency trading\u201d. Lastly, AI also has implications\non news media such as \u201cweakening journalism\u201d and \u201cundermining\nthe [public] confidence in the media\u201d due to the ease of \u201ccreating\nfake content with AI\u201d.\nLegal and Rights-Related Risks - reflect risks of AI resulting from\nprivacy violations, due to \u201cAI voice cloning technology exploit-\ning personal data from social media to create voice replicas\u201d, or\nthe embedded bias in AI causing LLMs to \u201csexualize women in\nAI-generated images\u201d. Furthermore, other risks in this category\nraise concerns related to AI committing copyright infringement\n\u201cto articles from news media\u201d, or engaging in criminal activities\nto influence \u201clegal proceedings through tampering with evidence\nusing deepfakes\u201d.\nCognitive Risks \u2013 in this category, risks covered some cognitive\nimplications of interacting or using AI systems, such as mental andemotional risks resulting from AI generating images that \u201cdistort\nbody images presenting an unrealistic aesthetic ideal for both men\nand women\u201d, negatively impacting \u201cone\u2019s self-worth\u201d. In addition,\nthis category includes risks pertaining to the over-reliance on AI\nlike the potential \u201creliance on AI for decision making\u201d may result in\n\u201cover-reliance and trust in AI over human judgment\u201d. Another key\nrisk is the potential loss of human creativity and authenticity, as\ncovered by the humanness risks, due to AI \u201cundermining the unique\nhuman elements of creativity and imagination\u201d. Furthermore, there\nwere concerns of losing human touch because of \u201c[AI-generated]\ncontent that lacks the human element and nuances of real human\nrelationships, leading to less fulfilling interactions\u201d.\nContent Safety Risks - risks in this category are primarily associ-\nated with the potential of AI for generating \u201cchild abuse material\u201d\nthat causes child harm, or toxic speech such as generating \u201csexually\nharassing messages\u201d, or even explicit and non-consensual sexual\ncontent similar to the \u201cAI-generated nudity of Taylor Swift\u201d. Also,\nthis category includes risks related to the potential dissemination\nof sensitive content by LLMs related to \u201cbuilding chemical or bio-\nlogical weapons\", or violent and extreme content similar to Sarai,\nthe chat bot that encouraged Jaswant Singh Chail to \u201ckill the queen\nof England\u201d.\nExistential Risks - risks in this category predominantly express\nthe potential existential threats and harms posed by AI on human-\nity such as \u201cdemising of the human race\u201d. Other risks include the\npotential loss of control over AI systems resulting in \u201cwar, cyber\nconflicts, and deployment of nuclear weapons\u201d. Moreover, risks of\nAI misalignment were also covered in this category due to con-\ncerns of AI \u201ccounter[ing] human interests\u201d posing \u201ca significant\nexistential threat\u201d to humanity.\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\nEnvironmental Risks - encompass environmental risks resulting\nfrom the considerable energy and water resources for the infras-\ntructure and compute required to train LLMs and manufacture\nmicrochips (e.g., GPUs). This includes \u201ca rise in carbon emissions\u201d\nas AI use is \u201cincreased across all sectors\u201d, environmental and ecolog-\nical degradation such the loss of biodiversity as a result of \u201cextract-\ning rare earth metals\u201d for manufacturing AI hardware components\n\u201clead[ing] to environmental degradation and biodiversity loss\u201d.\n5.2 Global & Regional Prevalence of AI Risk in\nNews Media\n5.2.1 Global Prevalence of AI Risks. To measure the global preva-\nlence of AI risks across all six categories described in the previous\nsection, we counted the number of articles reporting risks related\nto each category, and divided this number by the total number of\narticles across regions and categories (i.e., 16,312). Results show\nthat prevalence of risks varies and is substantially different between\ncategories, as shown in Figure 1B.\nGlobal news coverage of AI in our sample prioritizes the cover-\nage of societal risks, constituting approximately half of the articles\nin our sample (50.6%). The reporting on Societal Risks is primarily\ndriven by economic risks and risks associated with potential misuse\nof AI for deception and manipulation, especially in politics, amount-\ning to\u223c25.3% and 18.5% of the overall articles in our sample. Legal\nand rights-related Risks is the second most prevalent risk category in\nthe global coverage of AI, accounting for approximately one-third\n(32.9%) of the articles discussing this risk and its sub-categories. The\ndiscourse around this risk category is centered around covering\nprivacy issues and discrimination incidents and biases caused by\nAI and its applications, which represent 13.6% and 13.0% of the\ntotal articles analyzed in our sample, respectively. The remaining\nfour risks categories- Cognitive Risks (14.20%), Content Safety Risks\n(8.29%), Existential Risks (7.24%), and Environmental Risks (1.86%)-\nreceived substantially less attention in news coverage in our sample\ncompared to Societal Risks andLegal and Rights-related Risks .\nOverall, the prevalence of the different risk categories irrespec-\ntive of their political bias mirror the global prevalence of risks\n(shown in Figure 1B). However, an analysis of news coverage for\neach risk category across the spectrum of political bias reveals\nthat political fringe media (both left and right) report more on AI\nrisks than center, or least-biased media outlets, with a relatively\nsimilar prioritization of Societal Risks ,Legal & Rights-related Risks ,\nCognitive Risks , and Content Safety Risks , as illustrated in Figure\n4. Furthermore, we find that right-biased media outlets lead the\nreporting on Existential Risks (9.1% vs. 6.8% for left-biased sources),\nbut also substantively disregard Environmental Risks (0.7%) as com-\npared to left bias sources (2.4%). In contrast, least-biased media\noutlets consistently under report on risks compared to fringe media\nin nearly every risk category.\n5.2.2 Regional Prevalence of AI Risks. Regional news coverage\nof AI risk categories follows the same overlying global trend of\nreporting on AI risks. Mainly, Societal Risks are the most prevalent\nin every region, followed by Legal & Rights-related Risks . In contrast,\nthe prevalence of the remaining risk categories did not exceed 20%\nper region. This shows an imbalance in the nature of coverage\nof AI risks by news media with a clear prioritization of SocietalandLegal & Rights-related risks. That said, we do observe changes\nin the rank order of risk categories across regions (see Figure 2\nfor details). We also map the political bias onto the prevalence of\nrisks in our sample for each region. This will enable us to compare\nthe prevalence of risk categories for each political bias group for\neach region, as shown in Figure 3 in the Appendix. In addition,\nthis provides early insights into the potential polarization of the\ndiscourse around the risk categories within each region.\nAfrica .Societal Risks dominate media coverage of AI in Africa in\nour sample, with a prevalence of 55.6%, surpassing that of any other\nregion for this category. In contrast, Existential Risks received the\nlowest prevalence in the news reporting compared to that in other\nregions. Other categories followed the global trend of risk preva-\nlence showing Legal & Rights-related Risks taking the second most\nprevalent risk category after Societal Risks amounting to (34.6%) of\nthe media coverage in Africa, followed by Cognitive Risks (15.1%),\nandContent Safety Risks (7.6%). Unlike other regions in our sample,\nmedia coverage of risks in Africa by least-biased sources dominates\nthe reporting on Societal Risks andCognitive Risks . However, report-\ning trends of other risk categories is mainly driven by left-biased\nsources. For instance, in comparison to left-biased media coverage,\nleft-center media outlets show much less coverage of Cognitive\nRisks (6.3%) and Content Safety Risks (3.1%) in their reporting.\nAsia . Asia\u2019s media coverage of AI was dominated by Societal\nRisks (48.6%). The most notable diversion of the coverage in Asia\ncompared to other regions is the considerably lower focus on Legal\n& Rights-related Risks (29.5%). Additionally, Content Safety Risks\n(6.2%) receive slightly less coverage than in other regions. Our\nsample of analyzed articles in Asia is politically centered for the\nmost part, reflecting a more balanced reporting on AI and its impacts\nrelative to other regions. However, we find media coverage in Asia\nby least-biased sources to report less on Societal risks, as compared\nto politically-centered sources, but more on Legal & Rights-related\nrisks. In addition, despite having only 25 articles from right-biased\nsources in our sample in Asia, these sources lead the coverage of\nEnvironmental and Existential Risks.\nEurope . The European media coverage of AI risks is mostly\naligned with the global trends shown in Figure 1B. Societal Risks\nare prevalent in over half of the analyzed articles (52.2%), and the\nLegal & Rights-related Risks were present in roughly one third of the\narticles (34.3%). In comparison to other regions, European media\ncoverage has a slightly stronger focus on Cognitive Risks (16.0%)\nand Content Safety Risks (11.5%). Also, European media, as well\nas news coverage of AI from sources in the middle east, show the\nhighest emphasis on existential threats (9.0%) compared to that\nof all other regions. The European sample consists primarily of\nleft-center media outlets (62.5%). In addition, there is a roughly\nequal share of articles by least-biased (12.4%), right-center (10.4%),\nand right-biased (12.1%) sources, with very small representation of\narticles by left-biased sources (2.5%). The European discourse shows\nthe highest prevalence differences between political leanings of all\nregions. Especially left media show a strong emphasis on Legal &\nRights-related Risks (52.1%) and Societal Risks (57.6%), indicating\na strong prioritization of these risks by left media in Europe. We\nalso find stark differences in other categories like Cognitive Risks ,\nwhere left media outlets cover those issues in 20.6% of articles,\nwhereas least-biased media do that only in approximately one out\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nFigure 2: Prevalence of AI risks in our sample from news media coverage across six different regions. The dashed red line\nrepresents the global average prevalence per risk category.\nof 10 articles (10.8%). As for content safety risks, we find that right-\nbiased media outlets have the highest share (13.5%), whereas left\nmedia outlets report on those issues at a much lower rate (6.9%).\nBy contrast, the center-left and center-right media coverage show\na comparable prevalence in each risk category.\nMiddle East . In the Middle East, news coverage shows an em-\nphasis on Societal Risks . Although it is still the most prioritized and\nprevalent category in the news coverage for that region, it is sub-\nstantially less pronounced (43.0%) compared to that of other regions.\nBy far, Content Safety Risks experience the highest prevalence in\ncoverage of AI in comparison to that of other regions, reaching\n15.6%. In contrast, Cognitive Risks had the lowest prevalence across\nall regions amounting to 11.6% of the articles. Media coverage of\nAI risks in our sample of articles from news outlets in the Middle\nEast consists of left-center, right-center, and right-biased media out-\nlets with clear dominance of right-center sources on the discourse\n(72%).In addition, least-biased and left-biased coverage of AI risks\nare not as prevalent as the coverage of other politically-biased do-\nmains in the region, which could be attributed to a sample bias due\nto the presence of a single article in the left-bias category. Also, the\ndiscourse shows signs of bipartisan emphasis across news domains\non covering AI risks specifically around societal risks related issues,\nwith left-center and right-biased media outlets showing stronger\nprevalence of these issues by 21.4 percentage points difference in\ncomparison to coverage in center-right outlets. Another critical\nfinding from this region, is the minimal concern of right-biased\nmedia coverage in our sample to cognitive and content safety risks,as reflected by the prevalence of articles on these issues, as to cen-\ntered media outlets which show a considerable attention on these\nissues. Lastly, and consistent with other regions, Environmental\nRisks are not as prioritized in media coverage as other risks. Also,\nissues related to these risks do not receiving much attention from\nnews domains except left-center and right-center biased domains,\namounting to 0.9% of the coverage in that region.\nNorth America . Media in North America shows consistent\npatterns to the global prevalence trends of AI risk categories. Again,\nSocietal Risks dominate the media coverage (50.9%), Legal & Rights-\nrelated Risks receive the second most attention (32.9%), followed by\nCognitive Risks (14.1%), Content Safety Risks (7.6%), and Existential\nRisks (6.7%). News coverage in our sample from domains in North\nAmerica is dominated by left-center media outlets, followed by\nright-centered, left, least biased, and right-biased outlets.\nTopically, we find differences in the prevalence around Societal\nRisks (58.8%) and Legal & Right-related Risks (42.9%) as they are\nprimarily led by right-biased news media. Left-biased media, on\nthe other hand, show the strongest emphasis on Cognitive Risks\n(17.1%) in their coverage for this category compared to media out-\nlets of other political bias groups. Also, we generally found that\nleast-biased media outlets have a lower prevalence of risks in their\ncoverage, especially in Legal & Rights-related Risks (29.8%) and Con-\ntent Safety Risks (5.3%) when compared to the prevalence of these\nrisks in left-biased and right-biased outlets.\nOceania . Consistent with the news coverage of AI risks in other\nregions, the majority of the news coverage in Oceania shows an\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\nemphasis on Societal Risks (50.2%). However, Legal & Rights-related\nRisks received more attention than other regions amounting to\n38.3% of the news coverage of AI in our sample. This is the hightest\nprevalence for this category across all regions. Likewise, Cognitive\nRisks received the hightest share of media attention in Oceania\ncompared to all other regions (16.4%). For Oceania, our sample\nconsists mostly of left-center outlets, followed by least biased, and\nright center outlets. Right-biased and left-biased news outlets are\nonly marginally present. Similar to North America, we find signs\nof potential bipartisan emphasis in the discourse around AI risks\nin Oceania. Specifically, right-biased news outlets show a strong\nemphasis on Societal Risks , which represents 61.5% of all articles\npublished in the region. As for the legal & rights-related risks,\nleast-biased media outlets, show a strong prevalence of these risks\nas compared to the coverage by other politically biased outlets.\nNotably, we also found that right-biased news sources in this region\noverly emphasize existential risks. Specifically, 23.1% of the articles\npublished by right-biased sources in this region cover issues related\ntoExistential Risks , making it the highest share among all regions\nand biases in this category.\n6 DISCUSSION\nIn this research, we analyze a broad and diverse sample of global\nnews media spanning 27 countries to unlock insights about AI risks\nand harms reported in media outlets globally across six regions:\nAfrica, Asia, Europe, Middle East, North America, and Oceania.\nAlthough our sample represents media coverage from six regions\nworldwide, it is subject to certain limitations and biases. First, our\nsample relies primarily on news domains that are \u201cfreely available\"\n(i.e., not behind paywalls) and report on AI risks in the English\nlanguage, which does not capture the full discourse around AI in\nlanguages native to several of the regions we are analyzing. In\naddition, it should be recognized that the reliance on English as a\nlanguage in our sample for news reporting, especially for countries\nin the Global South, is influenced by historical and cultural ties\nto the British Commonwealth. This colonial legacy may influence\nhow these nations view and interpret the economic, political, and\ncultural risks associated with AI, potentially biasing how AI is\nreported on in these countries. Thus, future work should explore\nconducting evaluations of AI risks in languages beyond English.\nAccordingly, the results of this work and our interpretations of\nthem are limited to publicly accessible news domains, the quality\nof information in these domains, and the limited representation of\nthese sources to the views of the regions they are publishing from,\nparticularly in non-English speaking regions. Second, although our\nsample includes AI risks from media coverage in 27 countries, it\ndoes not include AI risks reported in Latin America. This highlights\na gap in our efforts to achieve a more comprehensive and global\nunderstanding of the prevalence of the various types of AI risks in\nnews media.\nThe analysis of the news coverage of AI in our sample shows a\nglobal prioritization of news media, as reflected in the prevalence\nof these risks in our sample, in covering societal risks, followed by\nlegal and rights-related risks, cognitive risks, content safety risks,\nexistential risks, and environmental risks. One possible explanationfor the large degree of global convergence on covering these cate-\ngories of risks could be that there may be external forces driving\nthe anglophone media system towards isomorphism [ 10], including\naspects such as tech-industry influence of media coverage [ 17], or\nmedia norms such as newsworthiness which could serve to ho-\nmogenize coverage. We also observe that regional coverage of AI\nin the news, per the prevalence of AI risks in each region in our\nsample, seem to align with the overall global trends of AI risks,\nwith some variance between regions. For instance, in the Middle\nEast content safety risks were the third most prevalent whereas\nother regions had it as the fourth most prevalent impact reported,\nand Europe having existential risks be the fourth most prevalent\nwhereas others had it as the fifth (see Figure 2).\nThe emphasis of news media on societal issues in our sample,\nespecially those focusing on economic and political risks, could be\nreflecting the ongoing global narrative of investing in AI to avoid\nmissing out on opportunities for economic growth (i.e., the global\nAI race) [ 24,32]. We also speculate that the emergence of legal\n& rights-related risks as the second most prevalent risk in media\ncoverage of AI could be partly due to the growing academic and reg-\nulatory discourse around rights-related question related to privacy\nand copyright, but also the protection of fundamental rights [ 79,80].\nMoreover, we find it surprising that cognitive risks and content-\nsafety risks receive substantially less attention in media reporting,\ndespite involving severe risks to individuals [ 8]. For example, cog-\nnitive risks include mental & emotional risks, while content safety\nrisks include issues such as hate speech and pornographic deep-\nfakes, which cause tremendous harm to those affected by them.\nWe also observe that existential risks are a small but not entirely\ninsignificant part of the reporting of AI in news media. News media\ncoverage of AI appears to be more concerned about present tangi-\nble risks that are being realized or manifested in society, more so\nthan on what public figures may emphasize regarding the potential\ndangers of AI and super-intelligence on the human race. This is\nalso consistent with prior research providing little evidence that\nnews media has succumbed to the rhetoric of a doomsday scenario\nof AI [ 1,23,43]. Lastly, the nearly non-existent media coverage on\nenvironmental risks is perplexing in light of the ongoing politicized\nand polarized discussion around climate change policies [ 26]. This\nsuggests that climate debates have not gained substantial traction in\nthe discourse on AI, despite the significant resources involved and\nthe potential environmental impacts highlighted by our research.\nMore strikingly, a majority of the present risk assessments tend to\noverlook the environmental risks of AI [ 32,101,110], except a few\n[1,100,103], as if the infrastructure required for AI systems is only\nmarginally connected to environmental issues.\nWhen inspecting the potential influence of political bias of the\nnews domains reporting on AI, we mainly find that fringe media\n(left- and right-biased sources) tend to report more on risks - es-\npecially in comparison to the least-biased outlets. This perhaps\nmakes sense given that underlying every risk is a value judgment\nabout what matters and to whom. Also, we observe instances of\npotential politicization of the discourse around AI such as the one\nsurrounding legal and rights-related risks in left-biased news media\nin Europe, echoing results of some scholars [ 16,104]. This could\nbe attributed to the discussion around the introduction of the EU\nAI Act, where fundamental rights and societal values played an\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nimportant role [ 80]. We find similar patterns in other regions such\nas in Oceania where right-biased media focus more on societal and\nexistential risks, and North America where the prevalence of soci-\netal risks and legal and rights-related risks are more emphasized\nby right-wing media outlets. As for Africa, we found higher differ-\nences in political bias in reporting about individual risks (cognitive\nimpacts and content safety risks) indicating a potential polarized\ndebate for these dimensions. Explaining these differences is beyond\nthe scope of this descriptive paper, as it requires expertise in each of\nthe six regions to interpret the structural and contextual differences\nin AI reporting especially if the discourse is being shaped by local\nsources to these regions that are politically-biased. Therefore, we\nencourage future research to look into explaining the influence of\npolitical bias of news outlets on the reporting and framing of AI\nrisks, as this may contribute to mitigating future polarizing debates\naround AI risks, such as economic risks [ 51], that could influence\n(or weaponize) the public perception of AI during the current and\nfuture development of AI regulations and policies.\n7 CONCLUSION\nThis work highlights the importance of incorporating global per-\nspectives, both regionally and politically, when assessing the broader\nimpacts of AI risks and harms. One source rich with such diverse\nperspectives, is news media. By analyzing news coverage of AI\nacross six regions around the world, we find a global skew towards\nprioritizing societal risks and legal and rights-related risks, in com-\nparison to other risk categories, in the reporting of AI in our sample\nfrom news media. Our research presents stakeholders, such as AI\ndevelopers and policymakers, with insights into the categories of AI\nrisks prioritized globally and within each of the six regions. Further,\nby evaluating the political bias of the news sources contributing to\nthis coverage, we highlight the potential influence of the political\nbias of news outlets on the coverage of AI risks. If such bias is left\nunaccounted for in future evaluations of AI and its impacts, it may\ninfluence the public perception of AI\u2013who is key stakeholder in\nshaping the current and future development of AI regulations and\npolicies. Thus, hindering any progress towards shaping more inclu-\nsive and globally informed AI governance policies and regulations.\nREFERENCES\n[1]Mowafak Allaham and Nicholas Diakopoulos. 2024. Evaluating the Capabilities\nof LLMs for Supporting Anticipatory Impact Assessment. https://doi.org/10.\n48550/ARXIV.2401.18028 Version Number: 2.\n[2]Mowafak Allaham, Kimon Kieslich, and Nicholas Diakopoulos. 2024. Towards\nLeveraging News Media to Support Impact Assessment of AI Technologies.\narXiv preprint arXiv:2411.02536 (2024).\n[3]Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Due-\nnas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt\nFredrikson, et al .[n. d.]. Agentharm: A benchmark for measuring harmfulness\nof llm agents, 2024. URL https://api. semanticscholar. org/CorpusID 273323256\n([n. d.]).\n[4]Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. Machine\nbias. In Ethics of data and analytics . Auerbach Publications, 254\u2013264.\n[5]Anonymous. 2024. Enhancing LLMs for Governance with Human Oversight:\nEvaluating and Aligning LLMs on Expert Classification of Climate Misinforma-\ntion for Detecting False or Misleading Claims about Climate Change. Under\nreview.\n[6]Shahar Avin, Ross Gruetzemacher, and James Fox. 2020. Exploring AI Futures\nThrough Role Play. In Proceedings of the AAAI/ACM Conference on AI, Ethics,\nand Society . ACM, New York NY USA, 8\u201314. https://doi.org/10.1145/3375627.\n3375817[7]Julia Barnett and Nicholas Diakopoulos. 2022. Crowdsourcing Impacts: Ex-\nploring the Utility of Crowds for Anticipating Societal Impacts of Algorithmic\nDecision Making. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics,\nand Society . ACM, Oxford United Kingdom, 56\u201367. https://doi.org/10.1145/\n3514094.3534145\n[8]Julia Barnett, Kimon Kieslich, and Nicholas Diakopoulos. 2024. Simulating\nPolicy Impacts: Developing a Generative Scenario Writing Method to Evaluate\nthe Perceived Effects of Regulation. In Proceedings of the AAAI/ACM Conference\non AI, Ethics, and Society , Vol. 7. 82\u201393.\n[9]Henk A. Becker. 2001. Social impact assessment. European Journal of Operational\nResearch 128, 2 (Jan. 2001), 311\u2013321. https://doi.org/10.1016/S0377-2217(00)\n00074-6\n[10] Kim Bj\u00f6rn Becker, Felix M Simon, and Christopher Crum. 2024. Policies in\nparallel? A comparative study of journalistic AI policies in 52 global news\norganisations. Digital Journalism (2024), 1\u201321.\n[11] Joseph R Biden. 2023. Executive order on the safe, secure, and trustworthy\ndevelopment and use of artificial intelligence. (2023).\n[12] Charlotte Bird, Eddie L. Ungless, and Atoosa Kasirzadeh. 2023. Typology of\nRisks of Generative Text-to-Image Models. http://arxiv.org/abs/2307.05543\narXiv:2307.05543 [cs].\n[13] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,\nSydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al .2021. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258 (2021).\n[14] Andrea Bonaccorsi, Riccardo Apreda, and Gualtiero Fantoni. 2020. Expert biases\nin technology foresight. Why they are a problem and how to mitigate them.\nTechnological Forecasting and Social Change 151 (2020), 119855.\n[15] Virginia Braun and Victoria Clarke. 2012. Thematic analysis. American Psycho-\nlogical Association.\n[16] J. Brennen. 2018. An industry-led debate: How UK media cover artificial intelli-\ngence. (2018). Publisher: Reuters Institute for the Study of Journalism.\n[17] J Scott Brennen, Philip N Howard, and Rasmus K Nielsen. 2021. Balancing prod-\nuct reviews, traffic targets, and industry criticism: UK technology journalism in\npractice. Journalism Practice 15, 10 (2021), 1479\u20131496.\n[18] J Scott Brennen, Philip N Howard, and Rasmus K Nielsen. 2022. What\nto expect when you\u2019re expecting robots: Futures, expectations, and pseudo-\nartificial general intelligence in UK news. Journalism 23, 1 (Jan. 2022), 22\u201338.\nhttps://doi.org/10.1177/1464884920947535\n[19] Amy Bruckman. 2020. \u2019Have you thought about. . . \u2019 talking about ethical impli-\ncations of research. Commun. ACM 63, 9 (2020), 38\u201340.\n[20] Mercedes Bunz and Marco Braghieri. 2022. The AI doctor will see you now:\nassessing the framing of AI in news coverage. AI & SOCIETY 37, 1 (March 2022),\n9\u201322. https://doi.org/10.1007/s00146-021-01145-9\n[21] Matthew Burtell and Thomas Woodside. 2023. Artificial influence: An analysis\nof AI-driven persuasion. arXiv preprint arXiv:2303.08721 (2023).\n[22] Zana Bu\u00e7inca, Chau Minh Pham, Maurice Jakesch, Marco Tulio Ribeiro, Alexan-\ndra Olteanu, and Saleema Amershi. 2023. AHA!: Facilitating AI Impact As-\nsessment by Generating Examples of Harms. http://arxiv.org/abs/2306.03280\narXiv:2306.03280 [cs].\n[23] Stephen Cave, Claire Craig, Kanta Dihal, Sarah Dillon, Jessica Montgomery,\nBeth Singler, and Lindsay Taylor. 2018. Portrayals and perceptions of AI and why\nthey matter . Technical Report. Apollo - University of Cambridge Repository.\nhttps://doi.org/10.17863/cam.34502\n[24] Stephen Cave and Se\u00e1n S. \u00d3h\u00c9igeartaigh. 2018. An AI Race for Strategic Ad-\nvantage: Rhetoric and Risks. In Proceedings of the 2018 AAAI/ACM Confer-\nence on AI, Ethics, and Society . ACM, New Orleans LA USA, 36\u201340. https:\n//doi.org/10.1145/3278721.3278780\n[25] Media Bias/Fact Check. 2025. Media Bias/Fact Check: The Most Comprehensive\nMedia Bias Resource. https://mediabiasfactcheck.com/ Accessed on January\n17, 2025.\n[26] Sedona Chinn, P Sol Hart, and Stuart Soroka. 2020. Politicization and polarization\nin climate change news content, 1985-2017. Science Communication 42, 1 (2020),\n112\u2013129.\n[27] Ching-Hua Chuan, Wan-Hsiu Sunny Tsai, and Su Yeon Cho. 2019. Framing arti-\nficial intelligence in American newspapers. In Proceedings of the 2019 AAAI/ACM\nConference on AI, Ethics, and Society . 339\u2013344.\n[28] Julie E Cohen and Ari Azra Waldman. 2023. Introduction: Framing Regula-\ntory Managerialism as an Object of Study and Strategic Displacement. Law &\nContemp. Probs. 86 (2023), i. Publisher: HeinOnline.\n[29] Eva Constantaras, Gabriel Geiger, Justin-Casimir Braun, Dhruv Mehrotra, and\nHiet Aung. 2023. Inside the suspicion machine. Technical Report. https:\n//www.wired.com/story/welfare-state-algorithms/\n[30] Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. 2022. Who\nAudits the Auditors? Recommendations from a field scan of the algorithmic\nauditing ecosystem. In 2022 ACM Conference on Fairness, Accountability, and\nTransparency . ACM, Seoul Republic of Korea, 1571\u20131583. https://doi.org/10.\n1145/3531146.3533213\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\n[31] Kate Crawford. 2016. Artificial intelligence\u2019s white guy problem. The New York\nTimes 25, 06 (2016), 5.\n[32] Kate Crawford. 2021. The atlas of AI: Power, politics, and the planetary costs of\nartificial intelligence . Yale University Press.\n[33] Luigi De Angelis, Francesco Baglivo, Guglielmo Arzilli, Gaetano Pierpaolo\nPrivitera, Paolo Ferragina, Alberto Eugenio Tozzi, and Caterina Rizzo. 2023.\nChatGPT and the rise of large language models: the new AI-driven infodemic\nthreat in public health. Frontiers in public health 11 (2023), 1166120.\n[34] Pieter De Wilde. 2011. No polity for old politics? A framework for analyzing\nthe politicization of European integration. Journal of European integration 33, 5\n(2011), 559\u2013575.\n[35] Nicholas Diakopoulos. 2015. Algorithmic Accountability: Journalistic investi-\ngation of computational power structures. Digital Journalism 3, 3 (2015), 398 \u2013\n415. https://doi.org/10.1080/21670811.2014.976411\n[36] Nicholas Diakopoulos. 2025. Prospective Algorithmic Accountability and the\nRole of the News Media. In Computer Ethics Across Disciplines: Deborah G.\nJohnson and Algorithmic Accountability , M. Moorman and M. Verdicchio (Eds.).\n[37] Kimberly Do, Rock Yuren Pang, Jiachen Jiang, and Katharina Reinecke. 2023.\n\u201cThat\u2019s important, but...\u201d: How Computer Science Researchers Anticipate Unin-\ntended Consequences of Their Research Innovations. In Proceedings of the 2023\nCHI Conference on Human Factors in Computing Systems . 1\u201316.\n[38] HDBSCAN Documentation. 2025. HDBSCAN: Hierarchical Density-Based\nSpatial Clustering. https://hdbscan.readthedocs.io/ Accessed on January 17,\n2025.\n[39] Ravit Dotan, Borhane Blili-Hamelin, Ravi Madhavan, Jeanna Matthews, and\nJoshua Scarpino. 2024. Evolving AI Risk Management: A Maturity Model based\non the NIST AI Risk Management Framework. http://arxiv.org/abs/2401.15229\narXiv:2401.15229 [cs].\n[40] Ethan Fast and Eric Horvitz. 2017. Long-term trends in the public percep-\ntion of artificial intelligence. In Proceedings of the AAAI conference on artificial\nintelligence , Vol. 31. Issue: 1.\n[41] Francesco Filippucci, Peter Gal, Cecilia Jona-Lasinio, Alvaro Leandro, and\nGiuseppe Nicoletti. 2024. The impact of Artificial Intelligence on productivity,\ndistribution and growth: Key mechanisms, initial evidence and policy challenges.\n(2024).\n[42] Luciano Floridi and Andrew Strait. 2020. Ethical Foresight Analysis: What\nit is and Why it is Needed? Minds and Machines 30, 1 (March 2020), 77\u201397.\nhttps://doi.org/10.1007/s11023-020-09521-y\n[43] Fabrizio Gilardi, Atoosa Kasirzadeh, Abraham Bernstein, Steffen Staab, and Anita\nGohdes. 2024. We need to understand the effect of narratives about generative\nAI.Nature Human Behaviour (Oct. 2024). https://doi.org/10.1038/s41562-024-\n02026-z\n[44] Rachel Griffin. 2024. What do we talk about when we talk about\nrisk? Risk politics in the EU\u2019s Digital Services Act. https://dsa-\nobservatory.eu/2024/07/31/what-do-we-talk-about-when-we-talk-about-\nrisk-risk-politics-in-the-eus-digital-services-act/\n[45] Alexa Hagerty and Igor Rubinov. 2019. Global AI ethics: a review of the so-\ncial impacts and ethical implications of artificial intelligence. arXiv preprint\narXiv:1907.07892 (2019).\n[46] David Hartmann, Jos\u00e9 Renato Laranjeira De Pereira, Chiara Streitb\u00f6rger, and\nBettina Berendt. 2024. Addressing the regulatory gap: moving towards an EU\nAI audit ecosystem beyond the AI Act by including civil society. AI and Ethics\n(Nov. 2024). https://doi.org/10.1007/s43681-024-00595-3\n[47] Johanna Hautala and Hanna Heino. 2023. Spectrum of AI futures imaginaries by\nAI practitioners in Finland and Singapore: The unimagined speed of AI progress.\nFutures 153 (Oct. 2023), 103247. https://doi.org/10.1016/j.futures.2023.103247\n[48] Viviane Herdel, Sanja \u0160\u0107epanovi\u0107, Edyta Bogucka, and Daniele Quercia. 2024.\nExploreGen: Large language models for envisioning the uses and risks of AI\ntechnologies. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety , Vol. 7. 584\u2013596.\n[49] Raphael Hernandes and Giulio Corsi. 2024. Auditing Google\u2019s Search Algorithm:\nMeasuring News Diversity Across Brazil, the UK, and the US. arXiv preprint\narXiv:2410.23842 (2024).\n[50] Joseph Hilgard and Nan Li. 2017. A Recap . Vol. 1. Oxford University Press.\nhttps://doi.org/10.1093/oxfordhb/9780190497620.013.8\n[51] Hong Huang, Hua Zhu, Wenshi Liu, Hua Gao, Hai Jin, and Bang Liu. 2024.\nUncovering the essence of diverse media biases from the semantic embedding\nspace. Humanities and Social Sciences Communications 11, 1 (2024), 1\u201312.\n[52] Muhammad Ittefaq, Ali Zain, Rauf Arif, Mohammad Ala-Uddin, Taufiq Ahmad,\nand Azhar Iqbal. 2025. Global news media coverage of artificial intelligence (AI):\nA comparative analysis of frames, sentiments, and trends across 12 countries.\nTelematics and Informatics 96 (Jan. 2025), 102223. https://doi.org/10.1016/j.tele.\n2024.102223\n[53] Anna Jobin, Marcello Ienca, and Effy Vayena. 2019. The global landscape of\nAI ethics guidelines. Nature Machine Intelligence 1, 9 (Sept. 2019), 389\u2013399.\nhttps://doi.org/10.1038/s42256-019-0088-2[54] Kimon Kieslich. 2024. The role of public opinion on ethical AI principles and its\nimplication for a common good-oriented implementation. Ph. D. Dissertation. Uni-\nversit\u00e4t Hohenheim. https://hohpublica.uni-hohenheim.de/handle/123456789/\n15939\n[55] Kimon Kieslich, Nicholas Diakopoulos, and Natali Helberger. 2024. Anticipating\nimpacts: using large-scale scenario-writing to explore diverse implications of\ngenerative AI in the news environment. AI and Ethics (May 2024). https:\n//doi.org/10.1007/s43681-024-00497-4\n[56] Kimon Kieslich, Nicholas Diakopoulos, and Natali Helberger. 2024. Using\nScenario-Writing for Identifying and Mitigating Impacts of Generative AI. https:\n//doi.org/10.48550/ARXIV.2410.23704 Version Number: 1.\n[57] Kimon Kieslich, Pero Do\u0161enovi\u0107, and Frank Marcinkowski. 2022. Everything,\nbut hardly any science fiction . Technical Report 7. Meinungsmonitor K\u00fcn-\nstliche Intelligenz. https://www.researchgate.net/profile/Kimon-Kieslich/\npublication/365033703_Everything_but_hardly_any_science_fiction/links/\n63638442431b1f5300685b2d/Everything-but-hardly-any-science-fiction.pdf\n[58] Kimon Kieslich, Natali Helberger, and Nicholas Diakopoulos. 2024. My Future\nwith My Chatbot: A Scenario-Driven, User-Centric Approach to Anticipat-\ning AI Impacts. In The 2024 ACM Conference on Fairness, Accountability, and\nTransparency . ACM, Rio de Janeiro Brazil, 2071\u20132085. https://doi.org/10.1145/\n3630106.3659026\n[59] Kimon Kieslich, Marco L\u00fcnich, and Pero Do\u0161enovi\u0107. 2023. Ever Heard of Ethical\nAI? Investigating the Salience of Ethical AI Issues among the German Population.\nInternational Journal of Human\u2013Computer Interaction (Feb. 2023), 1\u201314. https:\n//doi.org/10.1080/10447318.2023.2178612\n[60] Tobias D. Krafft, Katharina A. Zweig, and Pascal D. K\u00f6nig. 2022. How to\nregulate algorithmic decision-making: A framework of regulatory requirements\nfor different applications. Regulation & Governance 16, 1 (Jan. 2022), 119\u2013136.\nhttps://doi.org/10.1111/rego.12369\n[61] Kalev Leetaru and Philip A Schrodt. 2013. Gdelt: Global data on events, location,\nand tone, 1979\u20132012. In ISA annual convention , Vol. 2. Citeseer, 1\u201349.\n[62] Hause Lin, Jana Lasser, Stephan Lewandowsky, Rocky Cole, Andrew Gully,\nDavid G Rand, and Gordon Pennycook. 2023. High level of correspondence\nacross different news domain quality rating sets. PNAS nexus 2, 9 (2023),\npgad286.\n[63] Tambiama Madiega. 2021. Artificial intelligence act. European Parliament:\nEuropean Parliamentary Research Service (2021).\n[64] Saidbek Mamasoliev. 2024. IMPACT OF ARTIFICIAL INTELLIGENCE ON\nUS ECONOMIC GROWTH AND GLOBAL COMPETITIVENESS. AMERICAN\nJOURNAL OF BUSINESS MANAGEMENT 2, 3 (2024), 82\u201391.\n[65] Giacomo Marzi, Marco Balzano, and Davide Marchiori. 2024. K-Alpha\nCalculator\u2013Krippendorff\u2019s Alpha Calculator: A user-friendly tool for computing\nKrippendorff\u2019s Alpha inter-rater reliability coefficient. MethodsX 12 (2024),\n102545.\n[66] Maxwell E McCombs and Donald L Shaw. 1972. The agenda-setting function of\nmass media. Public opinion quarterly 36, 2 (1972), 176\u2013187.\n[67] Sean McGregor. 2021. Preventing repeated real world AI failures by cataloging\nincidents: The AI incident database. In Proceedings of the AAAI Conference on\nArtificial Intelligence , Vol. 35. 15458\u201315463.\n[68] Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform man-\nifold approximation and projection for dimension reduction. arXiv preprint\narXiv:1802.03426 (2018).\n[69] Media Cloud. 2025. Collection 9272347. https://search.mediacloud.org/\ncollections/9272347 Accessed on January 17, 2025.\n[70] Florian Mei\u00dfner. 2024. Risks and opportunities of \u2018generative A.I.\u2019: How do news\nmedia cover ChatGPT?. In International Crisis and Risk Communication Con-\nference Proceedings . International Crisis and Risk Communication Association.\nhttps://doi.org/10.69931/WFEY9011\n[71] Jacob Metcalf, Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, and\nMadeleine Clare Elish. 2021. Algorithmic impact assessments and account-\nability: the co-construction of impacts. Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency (2021). https://doi.org/10.1145/\n3442188.3445935\n[72] Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and\nJacob Metcalf. 2021. Assembling Accountability: Algorithmic Impact Assessment\nfor the Public Interest. SSRN Electronic Journal (2021). https://doi.org/10.2139/\nssrn.3877437\n[73] Priyanka Nanayakkara, Jessica Hullman, and Nicholas Diakopoulos. 2021. Un-\npacking the expressed consequences of AI research in broader impact statements.\nInProceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society . 795\u2013\n806.\n[74] Efrat Nechushtai, Rodrigo Zamith, and Seth C Lewis. 2024. More of the same? Ho-\nmogenization in news recommendations when users search on Google, YouTube,\nFacebook, and Twitter. Mass Communication and Society 27, 6 (2024), 1309\u20131335.\n[75] Dennis Nguyen and Erik Hekman. 2022. A \u2018New Arms Race\u2019? Framing China and\nthe U.S.A. in A.I. News Reporting: A Comparative Analysis of the Washington\nPost and South China Morning Post. Global Media and China 7, 1 (March 2022),\n58\u201377. https://doi.org/10.1177/20594364221078626\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\n[76] Dennis Nguyen and Erik Hekman. 2024. The news framing of artificial intelli-\ngence: a critical exploration of how media discourses make sense of automation.\nAI & SOCIETY 39, 2 (April 2024), 437\u2013451. https://doi.org/10.1007/s00146-022-\n01511-1\n[77] Matthew C. Nisbet, Dietram A. Scheufele, James Shanahan, Patricia Moy,\nDominique Brossard, and Bruce V. Lewenstein. 2002. Knowledge, Reser-\nvations, or Promise?: A Media Effects Model for Public Perceptions of Sci-\nence and Technology. Communication Research 29, 5 (Oct. 2002), 584\u2013608.\nhttps://doi.org/10.1177/009365002236196\n[78] OECD AI Policy Observatory. n.d.. AI Incidents Database. https://oecd.ai/en/\nincidents Accessed: 2025-01-17.\n[79] Office of the United Nations High Commissioner for Human Rights (OHCHR).\n2023. A Taxonomy of Generative AI and Human Rights Harms.\nhttps://www.ohchr.org/sites/default/files/documents/issues/business/b-\ntech/taxonomy-GenAI-Human-Rights-Harms.pdf Accessed: 2025-01-17.\n[80] Carsten Orwat, Jascha Bareis, Anja Folberth, Jutta Jahnel, and Christian Wade-\nphul. 2024. Normative Challenges of Risk Regulation of Artificial Intelligence.\nNanoEthics 18, 2 (Aug. 2024), 11. https://doi.org/10.1007/s11569-024-00454-9\n[81] Lucas Ou-Yang. 2025. newspaper: Simplified Python Article Scraping & Curation.\nhttps://github.com/codelucas/newspaper Accessed on January 17, 2025.\n[82] Leila Ouchchy, Allen Coin, and Veljko Dubljevi\u0107. 2020. AI in the headlines: the\nportrayal of the ethical issues of artificial intelligence in the media. AI & SOCIETY\n35, 4 (Dec. 2020), 927\u2013936. https://doi.org/10.1007/s00146-020-00965-5\n[83] Kevin Paeth, Daniel Atherton, Nikiforos Pittaras, Heather Frase, and Sean Mc-\nGregor. 2024. Lessons for Editors of AI Incidents from the AI Incident Database.\narXiv preprint arXiv:2409.16425 (2024).\n[84] Rock Yuren Pang, Sebastin Santy, Ren\u00e9 Just, and Katharina Reinecke. 2024.\nBLIP: Facilitating the Exploration of Undesirable Consequences of Digital Tech-\nnologies. In Proceedings of the CHI Conference on Human Factors in Computing\nSystems . 1\u201318.\n[85] Peter S Park, Simon Goldstein, Aidan O\u2019Gara, Michael Chen, and Dan Hendrycks.\n2024. AI deception: A survey of examples, risks, and potential solutions. Patterns\n5, 5 (2024).\n[86] Frank Pasquale. 2023. Power and Knowledge in Policy Evaluation: From Man-\naging Budgets to Analyzing Scenarios. Law and Contemporary Problems 86, 3\n(2023).\n[87] P Marijn Poortvliet, Martijn Duineveld, and Kai Purnhagen. 2016. Performativity\nin action: How risk communication interacts in risk regulation. European Journal\nof Risk Regulation 7, 1 (2016), 213\u2013217. Publisher: Cambridge University Press.\n[88] Varun Nagaraj Rao, Eesha Agarwal, Samantha Dalal, Dan Calacci, and Andr\u00e9s\nMonroy-Hern\u00e1ndez. 2024. QuaLLM: An LLM-based Framework to Extract\nQuantitative Insights from Online Forums. arXiv preprint arXiv:2405.05345\n(2024).\n[89] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing . Association for Computational\nLinguistics. https://arxiv.org/abs/1908.10084\n[90] Anka Reuel, Ben Bucknall, Stephen Casper, Tim Fist, Lisa Soder, Onni Aarne,\nLewis Hammond, Lujain Ibrahim, Alan Chan, Peter Wills, et al .2024. Open\nproblems in technical ai governance. arXiv preprint arXiv:2407.14981 (2024).\n[91] MIT Technology Review. 2023. The Great Acceleration: CIO Perspectives on\nGenerative AI. https://www.technologyreview.com/2023/07/18/1076423/the-\ngreat-acceleration-cio-perspectives-on-generative-ai/ Accessed: 2025-01-17.\n[92] Hal Roberts, Rahul Bhargava, Linas Valiukas, Dennis Jen, Momin M Malik,\nCindy Sherman Bishop, Emily B Ndulue, Aashka Dave, Justin Clark, Bruce\nEtling, et al .2021. Media cloud: Massive open source collection of global news\non the open web. In Proceedings of the International AAAI Conference on Web\nand Social Media , Vol. 15. 1034\u20131045.\n[93] Cathy Roche, P. J. Wall, and Dave Lewis. 2022. Ethics and diversity in artificial\nintelligence policies, strategies and initiatives. AI and Ethics (Oct. 2022). https:\n//doi.org/10.1007/s43681-022-00218-9\n[94] Jasper Roe and Mike Perkins. 2023. \u2018What they\u2019re not telling you about\nChatGPT\u2019: exploring the discourse of AI in UK news media headlines. Hu-\nmanities and Social Sciences Communications 10, 1 (Oct. 2023), 753. https:\n//doi.org/10.1057/s41599-023-02282-w\n[95] Elmer E Schattschneider. 1957. Intensity, visibility, direction and scope. American\nPolitical Science Review 51, 4 (1957), 933\u2013942.\n[96] Dietram A. Scheufele and Bruce V. Lewenstein. 2005. The Public and Nan-\notechnology: How Citizens Make Sense of Emerging Technologies. Journal of\nNanoparticle Research 7, 6 (Dec. 2005), 659\u2013667. https://doi.org/10.1007/s11051-\n005-7526-2\n[97] Mike S. Sch\u00e4fer. 2017. How Changing Media Structures Are Affecting Science News\nCoverage . Vol. 1. Oxford University Press. https://doi.org/10.1093/oxfordhb/\n9780190497620.013.5\n[98] Renee Shelby, Shalaleh Rismani, Kathryn Henne, AJung Moon, Negar Ros-\ntamzadeh, Paul Nicholas, N\u2019Mah Yilla-Akbari, Jess Gallegos, Andrew Smart,\nEmilio Garcia, et al .2023. Sociotechnical harms of algorithmic systems: Scoping\na taxonomy for harm reduction. In Proceedings of the 2023 AAAI/ACM Conferenceon AI, Ethics, and Society . 723\u2013741.\n[99] Peter Slattery, Alexander K Saeri, Emily AC Grundy, Jess Graham, Michael\nNoetel, Risto Uuk, James Dao, Soroush Pour, Stephen Casper, and Neil Thompson.\n2024. The ai risk repository: A comprehensive meta-review, database, and\ntaxonomy of risks from artificial intelligence. arXiv preprint arXiv:2408.12622\n(2024).\n[100] Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin\nBlodgett, Hal Daum\u00e9 III, Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite,\nAlexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman,\nMarie-Therese Png, Andrew Strait, and Apostol Vassilev. 2023. Evaluating\nthe Social Impact of Generative AI Systems in Systems and Society. http:\n//arxiv.org/abs/2306.05949 arXiv:2306.05949 [cs].\n[101] Bernd Carsten Stahl, Josephina Antoniou, Nitika Bhalla, Laurence Brooks, Philip\nJansen, Blerta Lindqvist, Alexey Kirichenko, Samuel Marchal, Rowena Rodrigues,\nNicole Santiago, Zuzanna Warso, and David Wright. 2023. A systematic review\nof artificial intelligence impact assessments. Artificial Intelligence Review (March\n2023). https://doi.org/10.1007/s10462-023-10420-8\n[102] A Ulken. 2005. Question of Balance: Are Google News search results politically\nbiased.\n[103] Risto Uuk, Carlos Ignacio Gutierrez, Daniel Guppy, Lode Lauwaert, Lucia Ve-\nlasco, Peter Slattery, and Carina Prunkl. [n. d.]. A Taxonomy of Systemic Risks\nfrom General-Purpose AI. ([n. d.]).\n[104] Maurice Vergeer. 2020. Artificial Intelligence in the Dutch Press: An Analysis\nof Topics and Trends. Communication Studies 71, 3 (May 2020), 373\u2013392. https:\n//doi.org/10.1080/10510974.2020.1733038\n[105] Kutoma Wakunuma and Damian Eke. 2024. Africa, ChatGPT, and Generative AI\nSystems: Ethical Benefits, Concerns, and the Need for Governance. Philosophies\n9, 3 (June 2024), 80. https://doi.org/10.3390/philosophies9030080\n[106] Michael D Ward, Andreas Beger, Josh Cutler, Matthew Dickenson, Cassy Dorff,\nand Ben Radford. 2013. Comparing GDELT and ICEWS event data. Analysis 21,\n1 (2013), 267\u2013297.\n[107] Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne\nHendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben\nBariach, et al .2023. Sociotechnical safety evaluation of generative ai systems.\narXiv preprint arXiv:2310.11986 (2023).\n[108] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang,\nJohn Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Court-\nney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba\nBirhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean\nLegassick, Geoffrey Irving, and Iason Gabriel. [n. d.]. Taxonomy of Risks posed\nby Language Models. https://doi.org/10.1145/3531146.3533088 Publication\nTitle: 2022 ACM Conference on Fairness, Accountability, and Transparency\nVolume: 22.\n[109] Kai-Cheng Yang and Filippo Menczer. 2023. Anatomy of an AI-powered mali-\ncious social botnet. arXiv preprint arXiv:2307.16336 (2023).\n[110] Yi Zeng, Kevin Klyman, Andy Zhou, Yu Yang, Minzhou Pan, Ruoxi Jia, Dawn\nSong, Percy Liang, and Bo Li. 2024. AI Risk Categorization Decoded (AIR 2024):\nFrom Government Regulations to Corporate Policies. https://doi.org/10.48550/\nARXIV.2406.17864 Version Number: 1.\n[111] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long,\nXiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. 2023. Safetybench: Evaluating\nthe safety of large language models with multiple choice questions. arXiv\npreprint arXiv:2309.07045 (2023).\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\nA APPENDIX\nA.1 AI-relevant Keywords\nThe set of keywords used to probe the news media for articles on\nAI sourced from [2]:\nA.I., Artificial Intelligence, Automated Decision Making, Automated\nSystem, Autonomous Driving System, Autonomous Vehicles, Au-\ntonomous Weapon, Chat Bot, Chatbot, ChatGPT, Computer Vision,\nDeep Learning, Deepfake, Driverless Car, Facial Recognition, Gen-\neral Artificial Intelligence, Generative AI, GPT, Image Generator,\nIntelligence Software, Intelligent Machine, Intelligent System, Lan-\nguage Model, Large Language Model, LLMs, Machine Intelligence,\nMachine Learning, Machine Translation, Natural Language API,\nNatural Language Processing, Neural Net, Neural Network, Pre-\ndictive Policing, Reinforcement Learning, Self-Driving Car, Speech\nRecognition, Stable Diffusion, Synthetic Media, Virtual Reality,\nWeapons System.\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nA.2 Distribution of news articles in our sample\nRegion Country Articles Proportion (%)\nAfrica Nigeria 936 2.184\nSouth Africa 484 1.129\nKenya 235 0.548\nUganda 38 0.089\nAsia India 4957 11.567\nJapan 1560 3.640\nPhilippines 1230 2.870\nPakistan 1245 2.905\nChina 549 1.281\nSingapore 468 1.092\nMalaysia 166 0.387\nRussia 196 0.457\nTaiwan 42 0.098\nEurope United Kingdom 4846 11.308\nIreland 1320 3.080\nMalta 359 0.838\nGermany 253 0.590\nFrance 217 0.506\nBulgaria 54 0.126\nItaly 26 0.061\nIceland 6 0.014\nMiddle East Israel 1373 3.204\nTurkey 115 0.268\nNorth America United States 18399 42.935\nCanada 1923 4.487\nOceania Australia 1193 2.784\nNew Zealand 649 1.51\nTable 1: Distribution of the number of scraped articles and their prevalence (out of 42,853 articles) in our corpus per region and\ncountry.\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\nA.3 Distribution of AI Risks categories annotated by two authors\nClass Count % Prevalence\nEconomic Risks 140 13.21\nDeception/Manipulation Risks 100 9.43\nNo Impact 97 9.15\nPrivacy Risks 72 6.79\nAI Governance Risks\u221765 6.13\nOperational Misuses Risks 57 5.38\nInformation Risks\u221754 5.09\nFundamental Rights 50 4.72\nExistential Risks\u221738 3.58\nDiscrimination/Bias Risks 37 3.49\nPolitical Usage Risks 37 3.49\nUser Experience Risks\u221736 3.40\nSecurity Risks 29 2.74\nEthical Risks\u221728 2.64\nHumanness Risks\u221727 2.55\nStructure/Power Risks\u221722 2.08\nMental & Emotional Risks\u221720 1.89\nPerformance Risks\u221720 1.89\nEnvironmental Risk\u221718 1.70\nSafety Risks\u221717 1.60\nHate/Toxicity Risks 16 1.51\nAuthoritarian Use of AI Risks\u221715 1.42\nViolence & Extremism Risks 12 1.13\nCriminal Activities Risks 11 1.04\nOver-reliance Risks\u221710 0.94\nTechnology Adoption Risks\u221710 0.94\nSexual Content Risks 7 0.66\nDisruption of Service Risks\u22176 0.57\nMedia Risks\u22175 0.47\nDefamation Risks 2 0.19\nChild Harm Risks 2 0.19\nTable 2: Distribution of risk categories in a sample of 1,060 summarized impacts that were annotated by two authors as described\nin section 4.2. All risk categories appended with an asterisk (\u2217) are novel risk categories that are not part of the AIR-taxonomy\n[110] at any level.\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nCategory Precision Recall F1-Score Support\nAI Governance Risks* 0.46 0.54 0.50 65\nCognitive Risks* 0.65 0.77 0.70 57\nContent Safety Risks 0.58 0.70 0.63 37\nEnvironmental Risks* 1.00 1.00 1.00 18\nEthical Risk* 0.47 0.57 0.52 28\nExistential Risks* 0.75 0.95 0.84 38\nInformation Risks* 0.60 0.56 0.58 54\nLegal & Rights-related Risks 0.67 0.77 0.72 170\nNo Impact 0.83 0.05 0.10 97\nSocietal Risks 0.69 0.82 0.75 299\nStructure/Power* 0.43 0.27 0.33 22\nSytem & Operational Risks 0.54 0.49 0.51 129\nUsability Risks* 0.69 0.48 0.56 46\nAccuracy 0.64 1060\nMacro Avg 0.64 0.61 0.60 1060\nWeighted Avg 0.65 0.64 0.61 1060\nTable 3: Classification report showing the performance of GPT-4o in annotating negative impacts in a sample of\n1,060 annotated impacts by two annotators, as described in section 4.2. Risk categories marked with an asterisk (*)\nrepresent the emerging categories in our sample that we attempted to align with the AIR-taxonomy, as outlined in\nSection 5.1.\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\nFigure 3: Prevalence of AI risks in our sample from news media coverage across six different regions stratified by the political\nbias of news domains as rated by Media Bias Fact Check (MBFC) and elaborated on in Section 3. We marked the political bias\nleading the discourse for risks in each region by a diamond.\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nA.4 Prompt for filtering articles by content\n### Context\n{ article_text }\n### Definition\nAn impact refers to an effect , consequence , or outcome\nof an AI system (i.e., model or application ) that\npositively or negatively affects individuals ,\norganizations , communities , or society .\n### Task\nBased on the definition of an impact of an AI system\nprovided to you , does the article above cover or\ndescribe at least one impact of an AI system ?\nAnswer Yes or No.\nDO NOT explain yourself .\nListing 1: Prompt to classify whether an article describes an\nimpact of an AI system based on the conceptual definition\nprovided in the prompt.\nA.5 Prompt for summarizing negative impacts\nfrom articles\nSummarize ALL negative impacts that are ONLY relevant to\nthe AI system or model described in the context\nprovided to you. A negative impact refers to the\nset of risks or harms that have or may affect\nindividuals , organizations , or communities in\nsociety , as a result of an AI system , or its use.\nEach summarized impacts must be 1 sentence long\nand must have sufficient details that are grounded\nin the context provided to you .\nFormat your response in a jsonl format {\"\nnegative_impacts \": [ list of impacts ]}. If not\nimpacts are present , output {\" negative_impacts \"\n:[]}.\nDO NOT make up details for impacts .\nDO NOT interpret any details , share any highlights , or\ndraw conclusions .\nONLY stick to the context provided to you .\nDO NOT provide any other details in the answer besides\nthe jsonl content .\n### Context :\n{ article_text }\nListing 2: Prompt to summarize the negative impacts of AI\nin an article based on the provided operational definition of\na negative impact.\nA.6 Prompt for annotating negative impacts in\ncorpus of articles\nTask : Analyze the negative impact statements of AI\ntechnologies . A negative impact refers to the set\nof risks or harms that have or may affect\nindividuals , organizations , or communities in\nsociety , as a result of an AI system , or its use.\nEach impact statement should be evaluated and\ncategorized into ONLY one of the following 32\ncategories . For each impact statement listed ,\nassign exactly one label that corresponds to its\nmost appropriate classification in the same format\nas the original list .\nCategories :\n1. ai_governance : negative impacts associated with\ngovernance policies and regulations related to the\ndevelopment , deployment , licensing , or moderation\nof AI technologies .2. ai_incompetence : risks resulting from limitations and\nmalfunctions of AI technologies that impact their\nperformance .\n3. authoritative_use_of_ai : risks associated with the\npotential misuse of artificial intelligence by\ngovernments in ways that may support authoritarian\npractices that violate human rights and civil\nliberties .\n4. criminal_activities : risks associated with the misuse\nof AI technologies for online crimes such as\ncybercrimes or cyberattacks .\n5. deception / manipulation : risks associated with the use\nof AI technologies for fraud , dishonest\nactivities , sowing divisions , or misrepresenting\nindividuals to influence or alter perceptions ,\nbehaviors , or mislead individuals or society .\n6. discrimination / bias : risks of AI technologies\ngenerating outputs based on protected\ncharacteristics that result in unequal treatment\nor representation of individuals or social groups .\n7. disruption_of_service : risks related to disruptions\nor reductions in the accessibility , availability ,\nand functionality of AI systems .\n8. economic_harm : risks posed by AI technologies to\nfinancial systems , labor market , and trading\ndynamics .\n9. environmental : environmental and ecological risks\narising from the energy consumption and resource\nintensive process required for the development ,\ndeployment , and operation of AI technologies .\n10. ethical_impact : challenges related to inequalities\nin access to AI technologies , or in their\ndevelopment , deployment , and use , with a\nparticular focus on issues of fairness ,\naccountability , and transparency .\n11. existential_threats : risks related to potential\ninequalities in accessing AI technologies , their\ndevelopment , deployment , and use , with a\nparticular focus on issues of fairness ,\naccountability , and transparency .\n12. fundamental_rights : risks posed by AI technologies\nrelated to violating individual freedoms and\nrights , including freedom of expression and\nintellectual property .\n13. information_risks : risks associated with AI\nhallucinations , including the generation of\ninaccurate information , low - quality AI - generated\ncontent , and fabrication of information by AI\ntechnologies .\n14. hate / toxicity : risks associated with AI - generated\ncontent amplifying or spreading hateful , abusive ,\nor offensive content .\n15. humanness : risks related to the loss or diminishment\nof human qualities , such as creativity , emotional\ndepth , and authentic interpersonal connections ,\ndue to AI technologies .\n16. media_impacts : risks of AI technologies on the\nindependence , integrity , and reliability of media\nand journalism .\n17. mental_ & _emotional : risks related to the\npsychological well - being and emotional health of\nindividuals using or interacting with AI\ntechnologies .\n18. operational_misuses : risks associated with the\nmisuse of AI technologies in critical and highly\nregulated applications , such as unsafe autonomous\noperations , unreliable legal or military advice ,\nor automated decision - making .\n19. over - reliance : risks arising from over - relying on AI\ntechnologies in contexts that results in\nundermining human judgment , critical thinking , and\ndecision - making .\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\n20. political_useage : risks associated with the use of\nAI technologies to spread misinformation or\ndisinformation , influence elections or politics ,\nundermine democratic integrity , or disrupt social\norder .\n21. privacy : risks related to unauthorized access , use ,\nor disclosure of users 'data and personal\ninformation .\n22. safety_risks : risks of harming or endangering\nindividuals 'lives or safety arising from the\nmalfunction or misuse of AI technologies .\n23. security_risks : risks related to the threats and\nexploitation of vulnerabilities that compromise\nthe confidentiality , integrity , or availability of\nAI technologies .\n24. sexual_content : risks related to the non - consensual\ncreation , distribution , or misuse of sexually\nexplicit material or pornography using AI\ntechnologies .\n25. structure / power : risks related to the concentration\nof power and AI resources or technologies among a\nfew entities or governments , and its consequences\non competition , collaboration , innovation , and\nsafety of AI technologies .\n26. technology_adoption : risks related to the adoption\nof AI technologies due to integration and\nusability challenges , or due to barriers faced by\norganizations and individuals in adopting AI\ntechnologies into their work .\n27. user_experience : risks and issues that undermine the\nsatisfaction satisfaction , trust , and interaction\nof the end - user with AI technologies .\n28. violence_ & _extremism : risks pertaining to the use of\nAI technologies or AI - generated content to\nincites violence , promotes extremist ideologies ,\nor enables harmful activities such as weapon\ndevelopment or using AI in warfare .\n29. defamation : risks involving reputational harms to\nindividuals or organizations through AI - generated\nfalse or misleading statements , images , or\nrepresentations .\n30. child_harm : risks related to the misuse of AI\ntechnologies to harm or exploit children .\n31. no_impact : refers to general statements that do not\nhighlight potential or direct negative\nconsequences , risks , or harms of AI systems .\n32. other : Any risks or harms that do not fit into the\nabove categories .\nOutput Format : Present the classified categories\nwithout any numbers and clean from whitespace . The\ncategories should be selected from one of the\nabove 33 categories .\nNote : Ensure that each impact statement is classified\nunder only one of the aforementioned 32 categories\nand that there is one\nclassification corresponding to each impact statement in\nthe input . Do not generate any other text or\ninclude any additional details .\nDo not make up categories .\nImpact :\n{ summarized_impact }\nListing 3: Prompt to scale up the annotation process of\nnegative impacts of AI using GPT-4o as described in section\n4.2.\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nA.7 Definitions of Impact Categories\nBelow are the definitions of the impact categories, grouped at Level-1 of the AIR-taxonomy, resulted from annotating a sample of 1,060\nsummaries of negative impacts by two authors as outlined in Section 4.2. Novel categories that are not part of the AIR-taxonomy are\nformatted in bold.\n1. Societal Risks\n\u2022Economic Risks : risks posed by AI technologies to financial systems, labor market (incl. job safety), and trading dynamics.\n\u2022Authoritarian Use of AI Risks : risks associated with the potential misuse of artificial intelligence by governments in ways that may\nsupport authoritarian practices that violate human rights and civil liberties.\n\u2022Deception/Manipulation Risks : risks associated with the use of AI technologies for fraud, dishonest activities, sowing divisions, or\nmisrepresenting individuals to influence or alter perceptions, behaviors, or mislead individuals or society.\n\u2022Political Usage Risks : risks associated with the use of AI technologies to spread misinformation or disinformation, influence elections\nor politics, undermine democratic integrity, or disrupt social order.\n\u2022Defamation Risks : risks involving reputational harms to individuals or organizations through AI-generated false or misleading\nstatements, images, or representations.\n\u2022Media Risks : risks of AI technologies on the independence, integrity, and reliability of media and journalism.\n2. Legal and Rights-related Risks\n\u2022Privacy Risks : risks related to unauthorized access, use, or disclosure of users\u2019 data and personal information.\n\u2022Fundamental Rights Risks : risks posed by AI technologies related to violating individual freedoms and rights, including freedom of\nexpression and intellectual property.\n\u2022Discrimination/Bias Risks : risks of AI technologies generating outputs based on protected characteristics that result in unequal\ntreatment or representation of individuals or social groups.\n\u2022Criminal Activities Risks : risks associated with the misuse of AI technologies for online crimes such as cybercrimes or cyberattacks.\n3. Content Safety Risks\n\u2022Child Harm Risks : risks related to the misuse of AI technologies to harm or exploit children.\n\u2022Hate/Toxicity Risks : risks associated with AI-generated content amplifying or spreading hateful, abusive, or offensive content.\n\u2022Sexual Content Risks : risks related to the non-consensual creation, distribution, or misuse of sexually explicit material or pornography\nusing AI technologies.\n\u2022Violence & Extremism Risks : risks pertaining to the use of AI technologies or AI-generated content to incite violence, promote extremist\nideologies, or enable harmful activities such as weapon development or using AI in warfare.\n\u2022Hate/Toxicity Risks : risks associated with AI-generated content amplifying or spreading hateful, abusive, or offensive content.\n4. Cognitive Risks\n\u2022Mental & Emotional Risks : risks related to the psychological well-being and emotional health of individuals using or interacting\nwith AI technologies.\n\u2022Humanness Risks : risks related to the loss or diminishment of human qualities, such as creativity, emotional depth, and authentic\ninterpersonal connections, due to AI technologies.\n\u2022Over-reliance Risks : risks arising from over-relying on AI technologies in contexts that result in undermining human judgment,\ncritical thinking, and decision-making.\n5. Existential Risks\n\u2022Existential Risks : risks concerning the potential extinction of humanity or collapse of human civilization due to artificial intelligence.\n6. Environmental Risks\n\u2022Environmental Risks : environmental and ecological risks arising from the energy consumption and resource intensive processes\nrequired for the development, deployment, and operation of AI technologies.\nThe remaining categories that emerged from the annotation, but were excluded from further analysis (novel categories that are not in the\nAIR-taxonomy are formatted in bold):\n\u2022AI Governance Risks : negative impacts resulting from governance policies and regulations related to the development, deployment,\nlicensing, or moderation of AI technologies.\n\u2022Operational Misuse Risks : risks associated with the misuse of AI technologies in critical and highly regulated applications, such as\nunsafe autonomous operations (e.g., transportation, energy supply, military), unreliable advice in heavily regulated areas (e.g., legal,\nmedical), or automated decision-making (e.g., social scoring, profiling).\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\n\u2022Information Risks : risks associated with AI hallucinations, including the generation of inaccurate information, low-quality AI-\ngenerated content, and fabrication of information by AI technologies.\n\u2022User Experience Risks : risks and issues that undermine the satisfaction, trust, and interaction of the end-user with AI technologies.\n\u2022Security Risks : risks related to the threats and exploitation of vulnerabilities that compromise the confidentiality, integrity, or availability\nof AI technologies.\n\u2022Ethical Risks : risks related to potential inequalities in accessing AI technologies, their development, deployment, and use, with a\nparticular focus on issues of fairness, accountability, and transparency.\n\u2022Structure/Power Risks : risks related to the concentration of power and AI resources or technologies among a few entities or\ngovernments, and its consequences on competition, collaboration, innovation, and safety of AI technologies.\n\u2022Performance Risks : risks resulting from limitations and malfunctions of AI technologies that impact their performance.\n\u2022Safety Risks risks of harming or endangering individuals\u2019 lives or safety arising from the malfunction or misuse of AI technologies.\n\u2022Criminal Activities Risks : risks associated with the misuse of AI technologies for online crimes such as cybercrimes or cyberattacks.\n\u2022Technology Adoption Risks : risks related to the adoption of AI technologies due to integration and usability challenges, or due to\nbarriers faced by organizations and individuals in adopting AI technologies into their work.\n\u2022Disruption of Service Risks : risks related to disruptions or reductions in the accessibility, availability, and functionality of AI systems.\n\u2022Defamation Risks : risks involving reputational harms to individuals or organizations through AI-generated false or misleading\nstatements, images, or representations.\nUnder review, Global Perspectives of AI Risks and Harms, 2025 Allaham et al.\nCategory Number of Impacts Proportion (%)\nSocietal Risks 14,824 31.057\nLegal and Rights-Related Risks 9,169 19.210\nSystem and Operational Risks 6,766 14.175\nCognitive Risks* 3,345 7.008\nAI Governance Risks* 2,926 6.130\nInformation Risks* 2,464 5.162\nContent Safety Risks 2,353 4.930\nUsability Risks* 1,720 3.604\nExistential Risks* 1,647 3.451\nEthical Risks* 1,107 2.319\nEnvironmental Risks* 560 1.173\nStructure/Power Risks* 466 0.976\nTable 4: Distribution of risk categories annotated by the LLM over our corpus. Categories marked with an asterisk (*) represent\nthe eight categories we suggested to include in the AIR-taxonomy, as described in Section 5.1.\nGlobal Perspectives of AI Risks and Harms Under review, Global Perspectives of AI Risks and Harms, 2025\nFigure 4: Global prevalence of AI risks in our sample from news media coverage across the categories of political bias.\nA.8 Clustering negative impacts from news media\nAs described in Section 4.2, we tried clustering the negative impacts by mapping each summary of a single negative impact to a 384-dimensional\ndense vector (i.e., embeddings) using all-MiniLM-L6-v2 sentence-transformer [ 89]. Then, we applied the UMAP dimensionality reduction\nalgorithm to have a low dimensional representation of the embeddings so the clustering of a large number of negative impacts would\nbe computationally more efficient [ 68]. Finally, we clustered the negative impacts using HDBSCAN [ 38] resulting in 565 clusters. The\nunexpected large number of clusters led us to focus on selecting the largest 212 clusters based on the number of clustered impacts that\ncollectively encompass 80% of the 47,731 negative impacts. To ensure the intra-cluster consistency with respect to the types of impacts\nclustered, we sampled two impacts from each of the 212 clusters and qualitatively checked whether they consistently describe a similar\nimpact. At this stage, we observed that the impact statements within each cluster refer to different types of impacts. However, the contextual\nuse or application of the AI in these clusters is somewhat consistent. This prompted us to switch our annotation process of grouping negative\nimpacts from clustering the negative impacts to manually annotating a sample of these impacts, and then scaling the annotation process\nusing an LLM, as described in Section 4.2.\nReceived 22 January 2025", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Global Perspectives of AI Risks and Harms: Analyzing the Negative Impacts of AI Technologies as Prioritized by News Media", "author": ["M Allaham", "K Kieslich", "N Diakopoulos"], "pub_year": "2025", "venue": "arXiv preprint arXiv:2501.14040", "abstract": "Emerging AI technologies have the potential to drive economic growth and innovation but  can also pose significant risks to society. To mitigate these risks, governments, companies,"}, "filled": false, "gsrank": 565, "pub_url": "https://arxiv.org/abs/2501.14040", "author_id": ["aWCyBRcAAAAJ", "8EDg7-YAAAAJ", "7ip1Ih8AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:G6x8gKdw5jgJ:scholar.google.com/&output=cite&scirp=564&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D560%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=G6x8gKdw5jgJ&ei=bLWsaO2lAY6IieoP0sKRuAk&json=", "num_citations": 2, "citedby_url": "/scholar?cites=4100088375484918811&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:G6x8gKdw5jgJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2501.14040"}}, {"title": "Automatic fake news detection in political platforms-a transformer-based approach", "year": "2021", "pdf_data": "Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE) , pages 68\u201378\nAugust 5\u20136, 2021, \u00a92021 Association for Computational Linguistics68 \n \n Abstract  \nThe dynamics and influence of fake news \non Twitter during the 2020 US presidential \nelection remains to be clarified. Here, we \nuse a dataset related to 2020 U.S Election \nthat consists of news articles and tweets on \nthose articles. Therefore, it is extremely  \nimportant to stop the spread of fake news \nbefore it reaches a mass level, which is a \nbig challenge. We propose a novel fake \nnews detection framework that can address \nthis challenge. Our proposed framework \nexploits the information from news articles \nand so cial contexts to detect fake news. The \nproposed model is based on a Transformer \narchitecture, which can learn useful \nrepresentations from fake news data and \npredicts the probability of a news as being \nfake or real. Experimental results on real -\nworld data s how that our model can detect \nfake news with higher accuracy and much \nearlier, compared to the baselines . \n1 Introduction  \nFake news refers to false or misleading information \nthat appears as real news (Zhou & Zafarani, 2020) . \nFake news can be broadly categorized as either \nmisinformation (unintentional false information) \nor disinformation (deliberate false information) . \nRecent social and political events, such as 2020 \nUnited States presidential election, have seen an \nincrease in fake news (E. Chen et al., 2021) . \nAccording to a report by First Draft News1 , \nAmerica\u2019s current disinformation crisis is the result \nof more than two decades of corruption in \ncountry\u2019s information ecosystem. There are many \n \n1 https://firstdraftnews.org/latest/fake -news -complicated/  factors to blame for this social and political \nmisinformation. For example, the role of social \nmedia that is u nregulated, lack of investment in \npublic media, downfall  of local news outlets, and \nemergence of hyper -partisan online outlets.  \nAn information (news) ecosystem consists of \npublishers (news media that publish the news \narticle), information (news content) an d users \n(Anderson, 2016) . Initially, the news comes from \nthe publishers. Then, it goes to the news websites, \nfrom where it goes to the users who share news on \ndifferent platforms (blogs, social media, etc.). If the \nnews is fake, some users may find it more \nsensational and interesting to comment on and \nshare over their networks. The existence of the bots \nin social media makes it even worse , who  spread \nmisinformation through multiple  channels  to urge \npeople believe the fake news. Therefore, it \nbecomes cru cial to stop the fake news before it \nreaches to a broad audience.  In this paper, we aim \nto effectively  detect the fake news.  \nGenerally, the content of fake news is vague and \nmisleading (C. Liu et al., 2019) . According to a \nresearch (Horne & Adal\u0131, 2017) , the content of fake \nnews consists of certain patterns, such as excessive \nuse of capital letters, punctuations, or emotion -\nbearing words, which gives us clues about a news \nbeing  fake or real . However, if the content of news \nis not sufficient, then the social contexts may be \nuseful to assess the veracity (truthfulness) of news.  \nThe social contexts (Shu et al., 2019)  refers to  \nusers\u2019 interactions , such as , comments, shares, \nlikes, followers -followees relations  etc., that are \nhelpful to determine if a news fake or real.  \nSometimes, even the ve rified accounts in social \nmedia are involved in the propagation of fake news  Automatic Fake News Detection in Political Platforms \u2013 \n A Transformer -based Approach   \n \n \n \nShaina Raza  \nDepartment of Computer Science, Ryerson University  \nshaina.raza@ryerson.ca  \n \n \n \n \n \n69 \n \n (Shahi et al., 2021) . In this work, we plan to \nconsider both news content and social contexts to \ndetect fake news.  \nGenerally, a news item is represented by a news \nID or news title, which is not sufficient to capture \nthe patterns of fake news. There are many \nimportant pieces of inf ormation that may be  more  \nuseful. For example, a news body or news source \ncould be  (at times ) more convincing in persuading \nreaders to believe something, so, we need to pay \ncloser attention to such information . We refer to \nsuch auxiliary information as side (metadata) \ninformation. The side information associated with \na news  article  can be news body, source, time of \npublication, topics  etc. In this work, we pla n to \nconsider different  side information related to news . \nWe also consider embedded tweets on news \narticles, which provide us additional information to \ndetermine the veracity of news.  \nAccording to a research, the fake  news spreads \nwithin minutes once plan ted (V osoughi et al., \n2018) . For example, the fake news that Elon \nMusk\u2019s Tesla team is inviting people to give any \namount ranging from 0.1 to 20 bitcoins in \nexchange for double the amount, resulted in a loss \nof millions of dollars within the first few minutes2. \nSo, it is critical to detect fake news early on before \nit spreads. In this work, we plan to early detect the \nfake news within few minutes after its propagation.  \nIn recent years, the Transformer -based models \n(Vaswani et al., 2017)  have gained significant \npopularity in different NLP tasks, such as text \nclassification , detection methods . These models \nusually input  whole lexical da ta as one piece of \ninformation or document, without considering any \nside information (Wu et al., 2020) . In addition, the \ntemporal information is not considered (by default ) \nin these models. To better utilize the strengths of \nTransformer -based models for fake news \ndetection, it is important to  include  heterogenous \ninformation (main, side and temporal information) \nto build a classification mo del. In this work, we \nbuild a novel  Transformer model that considers \nheterogenous information for the task of fake news \ndetection . Throughout this paper, we refer to the \nmain information as the news headline , and we \nrefer to side information as consist ing of news -\nrelated features, social co ntexts (tweets), and \ntemporal information.  \nWe summarize our contributions as:  \n \n2 https://www.bbc.com/news/technology -56402378  \u2022 We propose a novel Transformer model that \nconsiders news content and associated side \ninformation for the fake news detection  task.  \n\u2022 We incorporate heterogenous side information \nin our model. In addition to only lexical data \n(as in typical Transformer s), we also consider \nthe non -lexical  (numeric, categorical)  data. We \nuse the multi -head attention mechanism to \nattend to different parts of such information.  \n\u2022 We propose to detect fake  new early within \nfew minutes after it is planted.  For that, we \nutilize the position encoding (Devlin et al., \n2018)  in the Transformer  model  that helps us \nto achieve our goal of early detection . The \nposition encoding represents the words\u2019 order \nin a model, i.e., the value of a word (content) \nand its temporal posi tion in a sentence.  \nWe evaluate our system by running experiments \non real -world data, which consists of news articles \nfrom various sources and social contexts from \nTwitter. Using an ablation study, we find that \nincluding both news content and social conte xts is \nbeneficial in detecting fake news patterns.  The \ninclusion of more side information proves very \nuseful as indicated in the results.  We also show  that \nour proposed model can detect fake news earlier \nand with greater accuracy than baselines.  \nThe rest o f the paper is organized as follows. \nSection 2 is about Methodology. Section 3 is for  \nExperiment , and Section 4  is for Experimental \nResults and Analysis. The Related Work is covered \nin Section 5. Section 6 is the Conclusion and \nrecommendations for the future work.  \n2  Methodology  \nProblem Definition : Given news and associated \nside information (news -related, social contexts and \ntemporal information) , the task is to determine if a \nnews item is fake or real.  \nWe consider  the fake news de tection  task as a \nbinary classification problem (news as fake or \nreal). We also consider a  multi class  classification \n(news as fake, real or mixed) in the experiments.   \nProposed Model:  In our work, we modify the \nstructure of pre -trained Bidirectional Encode r \nRepresentations from Transformers (BERT)  \n(Devlin et al., 20 18) to add side information  (in \naddition to main information) . The same \nmethodology can also be applied on other \nTransformers (RobertA, XLNet, BART,  T5 etc.).  \n70 \n \n  \n  \n \nFigure 1: Proposed Model Faker  \nWe represent each news item N by its title (main \ninformation) and side information. ( Temporal , \nnews -related, and social contexts). We believe that \nhaving more information is always beneficial. For \ninstance, the author and source provide us with \npartisan information (political party as belonging \nto right or left wing). The temporal information is \nuseful for determining whether a fake news is \nalready spread or just released. Similarly, social \ncontexts (tweets) give us additional information \nabout users\u2019 reactions on news.  \nWe present a novel  Transformer -based model , \nFaker , as shown  in Figure 1. The input to model is \nnews  items  and associated  side information. Each \nnews item N has a sequence of words, i.e., \ud835\udc41=\n{\ud835\udc5b1,\ud835\udc5b2,\u2026,\ud835\udc5b\ud835\udc59}  where l is length. For each news , \nwe have the accompanying side information , i.e.,  \n\ud835\udc46\ud835\udc3c={\ud835\udc60\ud835\udc561,\ud835\udc60\ud835\udc562,\u2026,\ud835\udc60\ud835\udc56\ud835\udc59} . In our work, we consider \ndifferent types  (lexical and non -lexical)  of side \ninformation , whereas our main information is \ntextual . We use \u2018word\u2019 as a general term to \nrepresent any word from N or feature from SI. The first layer in Faker  is the embedding layer. \nThe input to the embedding layer is the sequence \nof words from each input N or SI. The [CLS] token \nis added at the start of the sequence and is later used \nfor the class label prediction. We utilize the token \nand segment embedding from the BERT model  to \nrepresent the syntax and semantics of each word.  \nSimilar to (Q. Chen et al., 2019) , we also assume \nthat temporal order exists in sequences. So, w e use \nposition encoding (Vaswani et al., 2017)  to cap ture \nthe chronological information in the sequences . In \nour case, the position value of each word is decided \nby the timestamp of news publication.  \nThe output from the embedding layer is then fed \ninto the next twelve layers in the first Encoder \nblock. After  the encoding process, we get the \noutput vector for each word from  news. The \ncontextualized representation after the first \nEncoder block is \ud835\udc41\u0303={\ud835\udc5b\u03031,\ud835\udc5b\u03032 \u2026,\ud835\udc5b\u0303\ud835\udc59} for the news  \nand \ud835\udc46\ud835\udc3c\u0303={\ud835\udc60\ud835\udc56\u03031,\ud835\udc60\ud835\udc56\u03032,\u2026,\ud835\udc60\ud835\udc56\u0303\ud835\udc59} for the side information  \n(\ud835\udc60\ud835\udc56\u0303  comes from \ud835\udc60\ud835\udc56 , the dot above i under \ud835\udc60\ud835\udc56\u0303  is \n\n71 \n \n hidden under the  tilde ~). Each word vector from \n\ud835\udc41\u0303 and \ud835\udc46\ud835\udc3c\u0303 is then passed to a Fusion Block.  \nFusion Block: Inside the Fusion Block, we \nrepresent each piece of information (lexical or non -\nlexical)  from  \ud835\udc41\u0303 and \ud835\udc46\ud835\udc3c\u0303 with a token (word) . The x \nis a textual word , nu is numeric word (feature)  and \nc is a categorical word . \nInspired by the gating mechanism introduced in \n(Wang et al., 2018) , we first take each feature from \nthe non -lexical data ( nu and c) and combine them \nusing a gating mechanism  to produce a new non -\nlexical ve ctor h, as shown in Equation (1):  \n\u210e=\ud835\udc54\ud835\udc50\u2299(\ud835\udc4a\ud835\udc50\ud835\udc50)+\ud835\udc54\ud835\udc5b\ud835\udc62\u2299(\ud835\udc4a\ud835\udc5b\ud835\udc62\ud835\udc5b\ud835\udc62)+\ud835\udc4f\u210e (1) \nwhere  c is categorical feature, nu is numerical \nfeature , W denotes a weight matrix, \ud835\udc4f  denotes a \nscalar bias , and \ud835\udc54\ud835\udc50  and \ud835\udc54\ud835\udc5b\ud835\udc62  are the gating vector \nfor c and nu respectively . We m ay refer to  \ud835\udc54\ud835\udc56 as a \ngating vector for a non-lexical  feature i. The \ud835\udc54\ud835\udc56 is \nfused with x using  an activation function R. Then it \ngoes into h. The \ud835\udc54\ud835\udc56 is defined in Equation ( 2): \n\ud835\udc54\ud835\udc56=\ud835\udc45(\ud835\udc4a\ud835\udc54\ud835\udc56[\ud835\udc56 || \ud835\udc65]+\ud835\udc4f\ud835\udc56 ) (2) \nOnce, we get the h, we use a weighted \nsummation between the lexical vector  x and the \ncombined non-lexical vector  h to produce a fused \nsequence  m, as shown in Equation ( 3): \n\ud835\udc5a=\ud835\udc65+\ud835\udefc\u210e (3) \nwhere x is text feature, \u03b1 is a normalizing factor to \ndampen the magnitude of h representation within a \nrange . The \u03b1 is shown in Equation ( 4): \n\ud835\udefc=min (||\ud835\udc65||2\n||\u210e||2\u2217\ud835\udefd,1) (4) \nwhere the ||\ud835\udc65||2 and ||\u210e||2 denote the \ud835\udc592-norms of \n\ud835\udc65 and \u210e, and hyperparameter \ud835\udefd is selected  during \nthe validation process. Subsequently, an attention \nis applied over the lexical and non -lexical vectors \nto produce the final fused representation \ud835\udc5b\u0305 . The \noutput from each Fusion Block is \ud835\udc5b\u0305\ud835\udc56  and is \ncalculated for each word from the input sequence. \nThe new sequence \ud835\udc41\u0305={\ud835\udc5b\u0305\ud835\udc36\ud835\udc3f\ud835\udc46,\ud835\udc5b\u03051,\ud835\udc5b\u03052,\u2026,\ud835\udc5b\u0305\ud835\udc59}  is \nthen fed as input to the next Encoder block. We \napply the Encoder layers of our model on this \nsequence \ud835\udc5b\u0305 .  At the end of the second Encoder \nblock, we get the sequence \ud835\udc5b\u033f=\n{\ud835\udc5b\u033f\ud835\udc36\ud835\udc3f\ud835\udc46,\ud835\udc5b\u033f1,\ud835\udc5b\u033f2,\u2026,\ud835\udc5b\u033f\ud835\udc59}. The first element in \ud835\udc5b\u033f is the \n[CLS] token that has the necessary information to \npredict the class {real, fake} label. Therefore, the \n\ud835\udc5b\u033f\ud835\udc36\ud835\udc3f\ud835\udc46  goes through a final transformation to \nproduce a value  which can be used to predict a \nclass label.  \n \n3 https://mediabiasfactcheck.com/  3 Experiment  \nFake news data : We use the NELA -GT-2020 \ndataset (Horne, Benjamin; Gruppi, 2021) , which \ncovers a broad set of events, i ncluding the COVID - \n19 pandemic and the 2020 U.S. Presidential \nElection. In this work, we only consider the 2020 \nU.S. Election event -based data, which consists of \n294,504 related news articles across 403 sources  \nbetween January 1st, 2020 and December 31st,  \n2020. The source -level ground truth labels are \ncollected from the Media Bias/Fact Check \n(MBFC)3 website.  \nThe dataset also includes over 400,000 \nembedded Tweets found in news articles, which we \nalso employ in our research. Table 1 shows the \nfeatures of U S Elections data that we use.  \nWe use article IDs to create sequences based on \navailable features (in Table 1). The embedded \ntweet text is also included in the sequence. Each \nsequence record is grouped by article ID and  is \nsorted according to publication timestamp . The \nactual news articles are not labeled.  \nThe dataset only provides us the ground truth \nlabels (0 - reliable, 1 - mixed, 2 - unreliable) at \nsource -level. These source -level labels are \nobtained from MBFC , which c onsiders the \ndimensions of veracity based on a factuality \n(credibility) and on conspiracy sources. We use the \ndistant supervision (Mintz et al., 2009)  to assign a \nlabel to each news story. In that, first we take the \ndistant (weak) labels provided to each news source \nand use a weighted scheme to label each news \narticle. The intuition of distant labeling is that the \ntraining labels at source -level may be imprecise \nand partial but can be used to create a strong Feature  Description  Format  \nArticle ID*  Article identifier  Integer  \nNews title   Headline of news  Text  \nNews source *  News Source (e.g., \nCNN, theonion)  Categorical  \nNews content *  News Body  Text  \nAuthor *  Author of article  Categorical  \nURL *  URL of the article  Text  \nPublication \ntimestamp*  Publication time as \nunix timestamp  Integer  \nTweet ID *  ID of tweet  Integer  \nEmbedded \ntweet*  Raw data from \ntweets (on news)  Text  \nTable 1:  Dataset features, * is side information  \n72 \n \n predictive model. This approach is also suggested \nin the NELA -GT-18 paper (N\u00f8rregaard et al., \n2019)  and has shown promising results in a recent  \nwork (Horne et al., 2019) . \nAfter doing the labeling, we get around 37k \nlabels as \u2018fake\u2019, 12.5k labels as \u2018real\u2019 and 32k \nlabels as \u2018mixed\u2019. To handle the data imbalance \nproblem in in the dataset,  we use the under -\nsampling technique (Drummond et al., 2003) , in \nwhich the majority class is made closer to the \nminority class, by removing records from the \nmajority class.  Initially, we tried the SMOTE \ntechnique, in which the distribution of minority \nclass is increased by replicating some records, but \ndue to limited memory, we opt for under -sampling.  \nEvaluation Metrics:  To assess model perform , we \nuse accuracy ACC , precisio n Prec , recall Rec and \nF1-score F1, and area under curve AUC . \nCompared to ACC, AUC is usually better at \nranking predictions  because AUC evaluates model \nperformance across all possible thresholds. We \ntreat the fake news detection as a binary \nclassification problem using labels {\u2018Real\u2019, \n\u2018Fake\u2019}, and as multiclass  classification using \nlabels {\u2018Real\u2019, Fake\u2019 and \u2018Mixed\u2019}.  \nComparison Methods : For the baselines, we use : \nFake -news detection methods  \n\u2022 TriFN (Shu et al., 2019) : A matrix factorization \nmethods that exploits user, news and publisher \nrelationships for fake news detection.   \n\u2022 Declare (Popat et al., 2020) : A neural network  \nthat asse sses the credibility of claims on news.  \n\u2022 Grover (Zellers et al., 2019) : a neural \nframework to detect fake news . \nTransformer -based methods  \n\u2022 BERT  (Devlin et al., 2018) : Bidirectional \nEncoder Representations from Transformers . \n\u2022 GPT-2 (Radford et al., 2019) : Generative Pre -\ntrained Transformer model . \n\u2022 VGCN -BERT  (Lu et al., 2020) : Transformer -\nbased model that uses BERT with Graph \nConvolutional Network for text classification  \nOther methods  \n\u2022 SVM (Chang & Lin, 2011) : Support Vector \nMachine model for text classificat ion.  \n\u2022 DeepWalk (Perozzi et al., 2014) : Embedding -\nbased deep neural model for text classification.   \nExperimental Settings and Hyperparameters:  \nAll the experiments are conducted on the GPUs \nprovided  by Google Colab Pro. We implemented \nour model using TensorFlow. The sequences are \ncreated in a chronological order of a news publication timestamp . We temporally split the \ntime-ordered data (by timestamps) for model \ntraining. We use the last 15% of the \nchronologically sorted data as the test set, the \nsecond to last 15% of the data as the validation set \nand the initial 75% of the data as the train set. The \nknown labels are used as the ground truth for \nmodel training and evaluation.  \nIn the final settings, we  choose the following \nhyperparameters : the news stories and tweets are \non average 500 words , so we choose a sequence \nlength of 500  token . We use padding if the length \nis shorter  and truncation if it is greater . The \ndimensionality is set to be 768. Larger b atch sizes \ndid not work at our end due to memory limit ation . \nSo, we choose the batch size to be 8. The dropout \nrate is set be 0.25, epochs 10, learning rate 1e -3 and \nAdam optimizer is chosen  for optimization . \n4 Experimental Results and Analysis  \nWe present the results of binary and multi class  \nclassification  using ACC, F1 -score (harmonic \nmean precision and recall) and AUC  in Table 2. \n4.1 Binary Classification Results  \nAccording to the results shown  in Table 2, our \nproposed method Faker consistently outperforms \nall other methods in inferring binary classification \nlabels (for the evaluation metrics ACC, F1 -score, \nand AUC).  For example, our proposed model  \nFaker \u2019s accuracy score in inferring news articles is \n20-30% higher than that of the st ate-of-the-art fake \nnews detection models (TriFN, Declare, and \nGrover ), as well as Transformer -based models \n(BERT, GPT -2, and VGCN -BERT ), and other \nmethods (SVM and DeepWalk ).  \nTriFN outperforms other fake news detection \nbaselines (Declare, Grover) in term s of overall \nperformance. This is most likely because when we \nuse both social contexts and news content, we get \nbetter patterns for detecting fake news.  \nAmong the Transformer based models, the \ngeneral performance of BERT and VGCN -BERT \nis better than GPT -2. The BERT model is more \nsuited to generative (text generation) tasks, \nwhereas the GPT -2 model is better suited to \nautoregressive (time -series) tasks. The fake news \nand Transformer -based baselines have \noutperformed the simple machine learning (SVM) \nand neural baseline (DeepWalk).  \n73 \n \n 4.2 Multi -label Classification Results  \nIn addition to the simplified binary classification, \nwe infer instance labels using the or iginal 3 -class \nlabel space, as shown in Table 2.  \nThe results show that our proposed model Faker  \nconsistently outperforms all the models on \nmulticlass  classification  on the quality metrics:  \nACC, F1 -score and AUC . Similar to the results of \nbinary classification, the general performance of \nTriFN is better than other fake news baselines. The \nBERT -based models (in general) performs better \nthan GPT -2, which outperform simple baselines . \nIn terms of efficiency, the benefits of Faker are \nfar more pronounced in the binary classification \nsetting.  This is most likely due to the fact that when \nthe \u2018mixed ' label  is removed, the models are better \nable to identify the instances as real or false.  \n4.3 Sampling Ratio  \n We sample the training set, which is controlled by \na sampling ratio parameter \u03b8 \u2208 {0.2, 0.4, 0.6, 0.8 , \n1.0}. Here, \u03b8 = 0.2 denotes 20% and \u03b8 = 1.0 means  \n100% of training instances used. We have shown \nthe results with sample ratio of 1.0 in Table 2. For \nthe o ther ratios, we show  results in Appendices.  \nThe results in Figure 4 in Appendix A show that \nour proposed model Faker consistently \noutperforms baselines in inferring binary labels by  \n5-30%.   Figure 5 in Appendix B results also show \nthat Faker \u2019s score s during multiclass  classification \nis consistently higher than other baselines for all \nvalues of \u03b8. Overall, the F1 -score and AUC of \nFaker is significantly higher in the multi -label \nclassification compared to other approaches.   \n4.4 Precision -Recall  of Binary Cl assification  \n We also test model perform on a small subset of \n4000 instances for binary classification in Table 3 . \n Actual Fake  Actual Real  \nPredicted Fake  2008  110 \nPredicted Real  37 1845  \nTable 3:  Confusion Matrix of Sample data   \nThe results in  Table 3  show that Faker accuracy is \n96.3%. We get the precision 94.8%, which means \nthat we have a few false positives (news is real but \npredicted as fake) and we can correctly predict a \nlarge portion of true positives (i.e., news is fake and \npredicted as fake). We also get the recall value of \n98.81%, which shows that we have much more true \npositives than false negatives. Generally, a false \nnegative (news is fake but predicted as real) is a \nworse error than a false positive in fake news \ndetection. In our ex periment, we get less false \nnegatives than the false positives (which are also \nfewer). Our F1 -score is 96.46%, which is also high.  \n4.5 Effectiveness of Early Detection  \nIn this experiment, w e compare the performance of \nour model and  baselines on early fake news \ndetection. We follow the methodology of Liu and \nWu (Y . Liu & Wu, 2018)  to define a propagation \npath for each news story. The idea is that any \nobservation data after the detection deadline cannot \nbe used for training. According to the research in \nfake news de tection, the fake news usually takes \nless than an hour to spread. Therefore, we choose \nminutes as the unit for the detection deadlines . \n \nFigure 2: AUC of models  on detection deadlines  \nThe results in Figure 2 shows  that, in general,  the \nmodels perform bett er when the detection deadline \ndelayed. This is shown with the overall better \nperformance of those methods in later detection \ndeadlines  (except for SVM) . This probably shows \nthat more data obviously helps us to better classify \nthe truth. Our proposed Faker  model consistently 0.10.30.50.70.9\n15 min 30 min 60 min 120 minAUC\nFaker TriFN Grover\nDeclare BERT VGCN-BERT\nGPT-2 SVM DeepWalkModel/  \nMetric  TriFN  Grover  Declare  BERT  VGCN -\nBERT  GPT2  SVM  Deep \nWalk  Faker  \nBinary Classification  \nACC  0.695  0.602  0.579  0.690  0.652  0.602  0.459  0.620  0.824  \nF1 0.660  0.598  0.552  0.612  0.635  0.609  0.468  0.610  0.768  \nAUC  0.698  0.678  0.577  0.619  0.632  0.648  0.430  0.542  0.804  \nMulticlass Classification  \nACC  0.675  0.582  0.559  0.660  0.650  0.582  0.400  0.519  0.810  \nF1 0.640  0.580  0.540  0.591  0.605  0.589  0.456  0.598  0.750  \nAUC  0.680  0.660  0.563  0.601  0.632  0.636  0.420  0.529  0.780  \nTable 2: Results of all models using Binary and Multiclass  classification  \n74 \n \n achieves the best AUC for all the detection \ndeadlines.  Faker also achieve good performance \neven in the early stage after the news is released.  \nThe ability of Faker to detect early is attributed \nto its position -aware mechanism, which lea rns the \nhidden patterns from the sequences of news data \nand tweets, and then classify the news articles. \nUsing position encoding, the ranking position of \neach data point in a time -ordered sequence is \nconsidered. The model, then, pays more attention \nto thos e data points that reflect the truthfulness of \nthe news article with respect to a temporal pattern. \nFor example, the ranking position of a data point \nmight give us an important clue as to whether a \nconcerned news article is fake in the recent time . \n4.6 Ablatio n Study  \nIn ablation study, we remove a key feature \ncomponent from a model one a time and \ninvestigate its impact on performance. Due to \nlimited space, we just show the AUC performance \nof reduced variants with binary classification. In \nour experiments, we te sted many variants of Faker \nbut mention the important ones below:  \nFaker : Original model with news, tweets and side \ninformation.  \nFaker(n):  Faker with only news -related \ninformation - removing social contexts (tweets).  \nFaker(s)  Faker with only social contexts.  \nFaker(h -): Original Faker with headline removed \nfrom news content  \nFaker(b -): Original Faker with body removed  \nFaker(so -): Faker with ne ws source removed.  \nThe results are shown as in Figure 3: \n \nFigure 3: AUC of Faker\u2019s variants  \nFrom the results in Figure 3, we see that when we \nremove the social contexts as in Faker(n), the \nmodel performance is impacted but the model \nperformance is impacte d more when we remove \nthe news content as in Faker (s). This probably \nshows that news related information is very \nimportant to learn the patterns of fake news. \nHowever, together both news and social contexts gives us more accurate results, as demonstrated by \nhighest AUC of Faker. The Faker(n) variant also \nappears to indicate that the body text is not entirely \nresponsible for the overall performance, but it is \npretty close to the default system with all features.  \nThe results also show that model performance is \nimpacted more when we remove the news body, \ncompared to the removal of the headline or the \nsource of the news. This is seen with the lower \naccuracy of Faker(b -) compared to both the \nFaker(h -) and Faker(so -). This shows that headline \nand sour ce are important, but news body alone \ncarries more information. Between source and \nheadline, the source seems to be more informative, \nthis is perhaps related to the partisan information.  \nWe also test different setting, for example, \nnumber of layers, dropou t rate, number of heads, \nbatch size, and removing certain embedding, such \nas positional embeddings. With all these \nexperiments, we find that our current setup is the \nbest for achieving our goals.  \n5 Related Work  \nFollowing the 2016 election, Google, Twitter, a nd \nFacebook all took steps to combat fake news. \nFacebook and  Twitter also allow users to mark \nnews stories as fake. A marked news story usually  \nthen goes through a  manual  fact-checking process.  \nManual fact -checking is inefficient for detecting \nfake news ea rly because it is a time -consuming \nprocess , and it is also not scalable to handle a large \nvolume of fake news online . In this paper, we look \nat automated methods for detecting fake news.  \nThe automatic fake news detection methods can \nbe broadly categorized as: content -based and social \ncontexts -based methods. Most of the existing \ncontent -based detection methods (Horne & Adal\u0131, \n2017; Przybyla, 2020;  Zellers et al., 2019)  use \nstyle-based features (e.g., sentence segmentation, \ntokenization, bag -of-words, latent topics, and POS \ntagging) or linguistic features (e.g., frequencies of \nwords, case schemes, context -free grammar and \nsyntax etc.,) from news articles to detect fake news.  \nOne challenge of content -based techniques is \nthat fake news style, platform, and topics are \nchanging constantly. Models trained on one dataset \nmay perform poorly on a new dataset with a \ndifferent content, style, or language. Furthermore, \nbecause the target variables in fa ke news change \nover time, certain labels become obsolete, while \nothers must be re -labeled. These algorithms also \nnecessitate a massive amount of training data to 0.00.20.40.60.81.0\nFaker Faker(n) Faker(s) Faker (h-) Faker (b-) Faker (so-)AUC\n75 \n \n detect fake news. By the time these methods gath er \nenough data, fake news has spread far enough.  \nTo solve the issues of content -based methods, \nthe researchers begin focusing on social contexts to \ndetect fake news. The existing social contexts -\nbased approaches are categorized into two types: \n(i) stance -based methods, and (ii) propagation -\nbased methods. The stance -based approaches \nexploit the users\u2019 viewpoints from social media \nposts to determine the truth (De Maio et al., 2020; \nY . Liu & Wu, 2020; Nakamura et al., 2020; Shu et \nal., 2019) . The propagation -based methods (Huang \net al., 202 0; Jiang et al., 2019; Y . Liu & Wu, 2018; \nQian et al., 2018)  utilize the information related to \nthe dissemination of fake news, e.g., how users \nspread it. These methods use techniques such as \ngraphs and multi -dimensional points for fake news \ndetection (Huang et al., 2020; Y . Liu & Wu, 2018) .  \nWhile social context methods are useful when \nthere is a lack of news content, they also introduce \nadditional challenges. Gathering social contexts, \nfor example, is a broad topic. The data for social \ncontexts is not only large, but also incomplete, \nnoisy, and unstructured, which may render existing \ndetection a lgorithms ineffective.  \nFake news detection is a subtask of text \nclassification (C. Liu et al., 2019) , which is solved \nby various machine learning and deep learning \nmethods. Some work (Y . Liu & Wu, 2018)  uses \nRNN and CNN networks to build propagation \npaths for detecting the fake n ews. Some other work \n(Shu et al., 2019)  uses matrix factorization \nmethods to detect fake news . A few works (Zellers \net al., 2019)  use LSTM networks on users\u2019 \ncomments to explain if a news is real or fake. A few \nworks (Nguyen et al., 2020)  also uses graph \nnetworks  to propose an explainable fake news \ndetection system.  \nIn recent years, there has been a greater focus in \nNLP research using  the Transformer models, such \nas BERT (Devlin et al., 2018) . BERT is used in \nsome fake news detection mode ls (Jwa et al., 2019; \nC. Liu et al., 2019; Vijjali et al., 2020)  to classify \nthe news as real or fake. Despite the robust design \nproposed in these models, a few limitations are \nnoted. F irst, these models do not consider a richer \nset of features from the news items and social \ncontexts. Second, the focus in these methods is not \non early fake news detection.   \nThe inclusion of temporal information is \nimportant to early detect fake news (Y . Liu & Wu, \n2020) . Also the inclusion of  side (meta -data) information  related to news or social contexts  is \nimportant to understand the nature of fake news \ndata (Shu et al., 2019) . Recently, an exploratory \nstudy (Shahi e t al., 2021)  on fake news  gives us  \nmore  new insights about the timeline of \nmisinformation . In our work, we consider both the \ntemporal and side information to detect fake news.  \nThe existing works on fake news focus either on \nnews content  or social contexts to detect  fake news , \nwe consider both in our wo rk. Compared to some \nprevious works  (Nguyen et al., 2020; Popat et al., \n2020; Shu et al., 2019)  that consider both these \naspects, we include a wider set of news -related as \nwell as social context (tweets). A few works (Y . Liu \n& Wu, 2020 ; Shu et al., 2019)  propose early \ndetection of fake news. Compared to these \nmethods, we  can detect fake detect the fake news \nmuch earlier  (i.e., after a few minutes of news \npropagation). Compared to the previous works, we \nconsider the latest state -of-the-art neural \narchitectures (Transformers) . \n6 Conclusion  and Recommendation s \nIn our work, we propose a Transformer -based \narchitecture  for fake news detection . We utilize the \nnews content and social contexts to detect the \npatterns of fake news.  We also early detect the fake \nnews through a position -aware encoding.  We \nachieve higher  performance compared to the \nbaselines, which shows the usefulness o f our \nproposed approach. In addition to fake news \ndetection, this model can also serve for general \nclassification tasks.  \nTo further improve the proposed method , a \nrecommendation is to consider more  social \ncontexts, such as friends\u2019  network s, propagation \npaths and implicit users\u2019 feedbacks . It would also \nbe very useful to consider malicious social media \nusers\u2019 profiles and their activities. Another \nrecommendation is to combat data and concept \ndrifts . It would also be very useful to understand \nthe tactics of fake news producers in real -time \nscenarios.  Furthermore, data labelling  scheme  can \nbe investigated because of the possibility of \nincorrectly labelled data, which may lead to data \nbiases (Kishore Shahi, 2020) . A possible extension \nof this work is to mitigate those biases.  We also \nwant to break filter bubbles and burst echo \nchambers created due to the spread of fake news.  \n76 \n \n References  \nAnderson, C. W. (2016). News ecosystems. The SAGE \nHandbook of Digital Journalism , 410 \u2013423. \nChang, C. -C., & Lin, C. -J. (2011). LIBSVM: a library \nfor support vector machines. ACM Transactions on \nIntelligent System s and Technology (TIST) , 2(3), 1 \u2013\n27. \nChen, E., Chang, H., Rao, A., Lerman, K., Cowan, G., \n& Ferrara, E. (2021). COVID -19 misinformation \nand the 2020 US presidential election. The Harvard \nKennedy School Misinformation Review . \nChen, Q., Zhao, H., Li, W., Hua ng, P., & Ou, W. \n(2019). Behavior sequence transformer for E -\ncommerce recommendation in Alibaba. \nProceedings of the ACM SIGKDD International \nConference on Knowledge Discovery and Data \nMining .  \nDe Maio, C., Fenza, G., Gallo, M., Loia, V ., & V olpe, \nA. (2020) . Cross -relating heterogeneous Text \nStreams for Credibility Assessment. IEEE \nConference on Evolving and Adaptive Intelligent \nSystems , 2020 -May.  \nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. \n(2018). BERT: Pre -training of deep bidirectional \ntransformers for language understanding. ArXiv \nPreprint ArXiv:1810.04805 . \nDrummond, C., Holte, R. C., & others. (2003). C4. 5, \nclass imbalance, and cost sen sitivity: why under -\nsampling beats over -sampling. Workshop on \nLearning from Imbalanced Datasets II , 11, 1\u20138. \nHorne, Benjamin; Gruppi, M. (2021). NELA -GT-\n2020: A Large Multi -Labelled News Dataset for The \nStudy of Misinformation in News Articles. ArXiv \nPrepr int ArXiv:2102.04567 .  \nHorne, B. D., & Adal\u0131, S. (2017). This just in: Fake \nnews packs a lot in title, uses simpler, repetitive \ncontent in text body, more similar to satire than real \nnews. ArXiv Preprint ArXiv: 1703.09398  \nHorne, B. D., N\u00d8rregaard, J., & Ada li, S. (2019). \nRobust fake news detection over time and attack. \nACM Transactions on Intelligent Systems and \nTechnology , 11(1).  \nHuang, Q., Zhou, C., Wu, J., Liu, L., & Wang, B. \n(2020). Deep spatial \u2013temporal structure learning for \nrumor detection on Twitter . Neural Computing and \nApplications , August .  \nJiang, S., Chen, X., Zhang, L., Chen, S., & Liu, H. \n(2019). User -characteristic enhanced model for fake \nnews detection in social media. CCF International \nConference on Natural Language Processing and \nChinese Co mputing , 634 \u2013646. Jwa, H., Oh, D., Park, K., Kang, J. M., & Lim, H. \n(2019). exBAKE: Automatic fake news detection \nmodel based on Bidirectional Encoder \nRepresentations from Transformers (BERT). \nApplied Sciences (Switzerland) , 9(19), 4062.  \nKaliyar, R. K., G oswami, A., Narang, P., & Sinha, S. \n(2020). FNDNet \u2013 A deep convolutional neural \nnetwork for fake news detection. Cognitive Systems \nResearch , 61, 32\u201344.  \nKishore Shahi, G. (2020). AMUSED: An Annotation \nFramework of Multi -modal Social Media Data. \narXiv prep rint arXiv:2010.00502.  \nLiu, C., Wu, X., Yu, M., Li, G., Jiang, J., Huang, W., & \nLu, X. (2019). A Two -Stage Model Based on BERT \nfor Short Fake News Detection. Lecture Notes in \nComputer Science (Including Subseries Lecture \nNotes in Artificial Intelligence and Lecture Notes in \nBioinformatics) , 172 \u2013183.  \nLiu, Y ., & Wu, Y . F. B. (2018). Early detection of fake \nnews on social media through propagation path \nclassification with recurrent and convolutional \nnetworks. 32nd AAAI Conference on Artificial \nIntelligence, AAAI 2018 , 354 \u2013361. \nLiu, Y ., & Wu, Y . F. B. (2020). FNED: A Deep Network \nfor Fake News Early Detection on Social Media. \nACM Transactions on Information Systems , 38(3).  \nLu, Z., Du, P., & Nie, J. Y . (2020). VGCN -BERT: \nAugmenting BERT with Grap h Embedding for Text \nClassification. In Lecture Notes in Computer \nScience (including subseries Lecture Notes in \nArtificial Intelligence and Lecture Notes in \nBioinformatics): Vol. 12035 LNCS .  \nMintz, M., Bills, S., Snow, R., & Jurafsky, D. (2009). \nDistant s upervision for relation extraction without \nlabeled data. Proceedings of the Joint Conference of \nthe 47th Annual Meeting of the ACL and the 4th \nInternational Joint Conference on Natural \nLanguage Processing of the AFNLP , 1003 \u20131011.  \nMohammadrezaei, M., Shiri,  M. E., & Rahmani, A. M. \n(2018). Identifying Fake Accounts on Social \nNetworks Based on Graph Analysis and \nClassification Algorithms. Security and \nCommunication Networks , 2018 .  \nNakamura, K., Levy, S., & Wang, W. Y . (2020). \nr/Fakeddit: A new multimodal benc hmark dataset \nfor fine -grained fake news detection. In LREC 2020 \n- 12th International Conference on Language \nResources and Evaluation, Conference \nProceedings .  \nNguyen, V . -H., Nakov, P., & Kan, M. -Y . (2020). \nFANG: Leveraging Social Context for Fake News \nDetection Using Graph Representation. \nProceedings of the 29th ACM International \n77 \n \n Conference on Information & Knowledge \nManagemen t, 1165 -1174  \nN\u00f8rregaard, J., Horne, B. D., & Adal\u0131, S. (2019). \nNELA -GT-2018: A large multi -labelled news \ndataset for the study of mi sinformation in news \narticles. Proceedings of the 13th International \nConference on Web and Social Media, ICWSM \n2019 , Icwsm , 630 \u2013638.  \nPerozzi, B., Al -Rfou, R., & Skiena, S. (2014). \nDeepwalk: Online learning of social \nrepresentations. Proceedings of the 20t h ACM \nSIGKDD International Conference on Knowledge \nDiscovery and Data Mining , 701 \u2013710. \nPopat, K., Mukherjee, S., Yates, A., & Weikum, G. \n(2020). Declare: Debunking fake news and false \nclaims using evidence -aware deep learning. In \nProceedings of the 2018 Co nference on Empirical \nMethods in Natural Language Processing, EMNLP \n2018 .  \nPrzybyla, P. (2020). Capturing the Style of Fake News. \nProceedings of the AAAI Conference on Artificial \nIntelligence , 34(01), 490 \u2013497.  \nQian, F., Gong, C., Sharma, K., & Liu, Y . (20 18). \nNeural user response generator: Fake news \ndetection with collective user intelligence. IJCAI \nInternational Joint Conference on Artificial \nIntelligence , 2018 -July, 3834 \u20133840.  \nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., \n& Sutskever, I. (2019) . Language models are \nunsupervised multitask learners. OpenAI Blog , 1(8), \n9. \nShahi, G. K., Dirkson, A., & Majchrzak, T. A. (2021). \nAn exploratory study of COVID -19 misinformation \non Twitter. Online Social Networks and Media,  \n100104  \nShu, K., Wang, S., & Liu , H. (2019). Beyond news \ncontents: The role of social context for fake news \ndetection. WSDM 2019 - Proceedings of the 12th \nACM International Conference on Web Search and \nData Mining , 9, 312 \u2013320.  \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., \nJones, L., Gomez, A. N., Kaiser, \u0141., & Polosukhin, \nI. (2017). Attention is all you need. Advances in \nNeural Information Processing Systems , 5998 \u2013\n6008.  \nVijjali, R., Potluri, P., Kumar, S., & Teki, S. (2020). \nTwo stage transformer model for covid -19 fake \nnews detection and fact checking. ArXiv Preprint \nArXiv:2011.13253 . \nV osoughi, S., Roy, D., & Aral, S. (2018). The spread of \ntrue and false news online. Science, 359(6380), \n1146 \u20131151.  Wang, Y ., Shen, Y ., Liu, Z., Liang, P. P., Zadeh, A., & \nMorency, L. P. (2018). Words Can Shift: \nDynamically Adjusting Word Representations \nUsing Nonverbal Behaviors. In Proceedings of the \nAAAI Conference on Artificial Intelligence, 7216 -\n7223.  \nWu, F., Qiao,  Y ., Chen, J. -H., Wu, C., Qi, T., Lian, J., \nLiu, D., Xie, X., Gao, J., Wu, W., & Zhou, M. \n(2020). MIND: A Large -scale Dataset for News \nRecommendation. Proceedings of the 58th Annual \nMeeting of the Association for Computational \nLinguistics , 3597 \u20133606.  \nYang , S., Shu, K., Wang, S., Gu, R., Wu, F., & Liu, H. \n(2019). Unsupervised fake news detection on social \nmedia: A generative approach. Proceedings of the \nAAAI Conference on Artificial Intelligence , 33(01), \n5644 \u20135651.  \nZellers, R., Holtzman, A., Rashkin, H., Bi sk, Y ., \nFarhadi, A., Roesner, F., & Choi, Y . (2019). \nDefending against neural fake news. In \narXiv preprint arXiv:1905.12616.  \nZhou, X., & Zafarani, R. (2020). A Survey of Fake \nNews: Fundamental Theories, Detection Methods, \nand Opportunities. ACM Computing Su rveys , 53(5).  \n \n  \n78 \n \n Appendix A. Binary Classification  Sampling \nRatio s \n \nFigure 4 (a): Binary Classification Accuracy  \n \n \nFigure 4 (b) : Binary Classification F1 -score  \n \n \nFigure 4  (c): Binary Classification AUC  Appendix B. Multiclass  Classification  \nSampling Ratio s \n \nFigure 5 (a): Multiclass  Classification ACC  \n \n \nFigure 5(b): Binary Classification F1 -score  \n \n \nFigure 5(c): Multi -label Classification AUC  \n 0.30.40.50.60.70.8\n0.2 0.4 0.6 0.8ACCURACY\nSAMPLING RATIOFaker TriFN Grover\nDeclare BERT VGCN-BERT\nGPT-2 SVM DeepWalk\n0.30.40.50.60.70.8\n0.2 0.4 0.6 0.8F1-SVORE\nSAMPLING RATIOFaker TriFN Grover\nDeclare BERT VGCN-BERT\nGPT-2 SVM DeepWalk\n0.30.40.50.60.70.8\n0.2 0.4 0.6 0.8F1-SVORE\nSAMPLING RATIOFaker TriFN Grover\nDeclare BERT VGCN-BERT\nGPT-2 SVM DeepWalk0.30.40.50.60.70.8\n0.2 0.4 0.6 0.8ACCURACY\nSAMPLING RATIOFaker TriFN\nGrover Declare\nBERT VGCN-BERT\nGPT-2 SVM\n0.30.40.50.60.70.8\n0.2 0.4 0.6 0.8F1-SCORE\nSAMPLING RATIOFaker TriFN\nGrover Declare\nBERT VGCN-BERT\nGPT-2 SVM\n0.30.40.50.60.70.8\n0.2 0.4 0.6 0.8AUC\nSAMPLING RATIOFaker TriFN\nGrover Declare\nBERT VGCN-BERT\nGPT-2 SVM", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Automatic fake news detection in political platforms-a transformer-based approach", "author": ["S Raza"], "pub_year": "2021", "venue": "Proceedings of the 4th Workshop on Challenges and \u2026", "abstract": "The dynamics and influence of fake news on Twitter during the 2020 US presidential election  remains to be clarified. Here, we use a dataset related to 2020 US Election that consists of"}, "filled": false, "gsrank": 566, "pub_url": "https://aclanthology.org/2021.case-1.10/", "author_id": ["chcz7RMAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:QTXe6A7gOGkJ:scholar.google.com/&output=cite&scirp=565&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D560%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=QTXe6A7gOGkJ&ei=bLWsaO2lAY6IieoP0sKRuAk&json=", "num_citations": 27, "citedby_url": "/scholar?cites=7582056327319270721&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:QTXe6A7gOGkJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aclanthology.org/2021.case-1.10.pdf"}}, {"title": "Whose language counts as high quality? Measuring language ideologies in text data selection", "year": "2022", "pdf_data": "Whose Language Counts as High Quality?\nMeasuring Language Ideologies in Text Data Selection\nSuchin GururanganyDallas Card}Sarah K. Dreier~Emily K. Gade|\nLeroy Z. WangyZeyu WangyLuke ZettlemoyeryNoah A. Smithy\u007f\nyUniversity of Washington}University of Michigan~University of New Mexico\n|Emory University\u007fAllen Institute for AI\n{sg01, zwan4, lsz, nasmith}@cs.washington.edu dalc@umich.edu\nskdreier@unm.edu emily.gade@emory.edu lryw@uw.edu\nAbstract\nLanguage models increasingly rely on massive\nweb dumps for diverse text data. However,\nthese sources are rife with undesirable content.\nAs such, resources like Wikipedia, books, and\nnewswire often serve as anchors for automati-\ncally selecting web text most suitable for lan-\nguage modeling, a process typically referred\nto as quality \ufb01ltering . Using a new dataset of\nU.S. high school newspaper articles\u2014written\nby students from across the country\u2014we in-\nvestigate whose language is preferred by the\nquality \ufb01lter used for GPT-3. We \ufb01nd that\nnewspapers from larger schools, located in\nwealthier, educated, and urban ZIP codes are\nmore likely to be classi\ufb01ed as high quality. We\nthen demonstrate that the \ufb01lter\u2019s measurement\nof quality is unaligned with other sensible met-\nrics, such as factuality or literary acclaim. We\nargue that privileging any corpus as high qual-\nity entails a language ideology, and more care\nis needed to construct training corpora for lan-\nguage models, with better transparency and\njusti\ufb01cation for the inclusion or exclusion of\nvarious texts.\n1 Introduction\nThe language models central to modern NLP are\ntrained on large Internet corpora, typically gathered\nfrom community resources (e.g., Wikipedia; Liu\net al. 2019) or web dumps (e.g., WebText, Common\nCrawl; Radford et al. 2019, Brown et al. 2020).\nThe selection of texts impacts every research or\ndeployed NLP system that builds on these models.\nYet there is rarely any explicit justi\ufb01cation for why\nvarious texts were included.\nWeb dumps like Common Crawl offer the\npromise of more diverse text than what is avail-\nable in curated resources. However, much of the\nweb consists of frequently replicated boilerplate\n(e.g., privacy policies), code (e.g., HTML and\nJavascript), pornography, hate speech, and more.\nAutomated approaches, typically referred to asquality \ufb01lters ,1are often applied in an effort to\nremove this undesirable content from training data.\nThese \ufb01lters include code removers (Gao et al.,\n2020), heuristics (Rae et al., 2021), stopwords (Raf-\nfel et al., 2020), and classi\ufb01ers (Brown et al., 2020;\nWenzek et al., 2020).\nAlthough quality \ufb01ltering is often treated as a\nrelatively neutral preprocessing step, it necessarily\nimplies a value judgment: which data is assumed to\nbe of suf\ufb01ciently high quality to be included in the\ntraining corpus? More concretely, when a quality\n\ufb01lter is a classi\ufb01er trained on instances assumed to\nbe of high (and low) quality, the selection of those\nexamples will impact the language model and any\ndownstream technology that uses it. Many \ufb01lters\nuse Wikipedia, books, and newswire to represent\nhigh quality text. But what texts are excluded as a\nresult? Because natural language varies with social\nand demographic variables (Rickford, 1985; Eckert,\n1989; Labov, 2006; Blodgett et al., 2016; Hovy and\nYang, 2021; Lucy and Bamman, 2021, inter alia ),\nwe can also ask whose language will be excluded.\nWe begin with a summary of the handful of data\nsources used to construct training corpora for many\nlanguage models and assumed to be of high qual-\nity (\u00a72). The systematic authorship biases in these\ndatasets motivate the study that follows, in which\nwe replicate the quality \ufb01lter from Brown et al.\n(2020). We apply this \ufb01lter to a new dataset of\nU.S. high school newspapers, augmented (via ZIP\ncodes and counties) with demographic data from\nthe U.S. Census and the National Center for Ed-\nucation Statistics (\u00a73). We demonstrate that the\n\ufb01lter has strong topical and stylistic preferences,\nand favors text from authors who originate from\nregions with better educational attainment, urban\ncenters, larger schools, and higher valued homes.\n1We note that the term quality is often ill-de\ufb01ned in the\nNLP literature. For example, Brown et al. (2020) and Wen-\nzek et al. (2020) refer to \u201chigh-quality text\u201d or \u201chigh-quality\nsources\u201d\u2014both citing Wikipedia as an example\u2014but without\nexplaining precisely what is meant.arXiv:2201.10474v2  [cs.CL]  26 Jan 2022\nIn sociolinguistics, the term language ideology\nrefers to common (but often unspoken) presupposi-\ntions, beliefs, or re\ufb02ections about language that\njustify its social use and structure (Craft et al.,\n2020). Our analysis begins to characterize the lan-\nguage ideology encoded in the quality \ufb01lter used\nby Brown et al. (2020), a representative of a wider\nset of \ufb01ltering methods. We also observe in \u00a74 that\nthe \ufb01lter is unaligned with other notions of quality\nfamiliar from human endeavors: factuality ratings\nfor news sources, standardized test scores, and liter-\nary awards. Of course, these institutions hold their\nown language ideologies. We argue that when con-\nstructing a corpus, one cannot avoid adopting some\nlanguage ideology; the language ideology which is\nappropriate depends on the goals of the work, and\none language ideology may con\ufb02ict with another.\nIn short, there is no truly general-purpose corpus.\nOur code and analysis is publicly available.2\n2 Motivation: Data Sources\nAcross the many language models recently reported\nin the literature, the same small group of datasets\nhave been routinely used as training corpora\u2014\nWikipedia, collections of books, and popular online\narticles (\u00a7A.1). These data are often treated as ex-\nemplars of high quality text (Devlin et al., 2019;\nLiu et al., 2019; Radford et al., 2019; Raffel et al.,\n2020; Brown et al., 2020). Although these datasets\ninclude text from many sources, extensive research\nsuggests that the voices they represent are drawn\nfrom a relatively small, biased sample of the popu-\nlation, over-representing authors from hegemonic\nsocial positions.\nWikipedia Wikipedia serves as a backbone for\nlanguage models because of its scale, ease of use,\npermissive license, and goal of providing compre-\nhensive coverage of human knowledge. However,\nalthough anyone can edit Wikipedia content, not\neveryone does. In practice, there are signi\ufb01cant\nbiases in Wikipedia authorship, content, and per-\nspectives. For instance, despite efforts by Wikime-\ndia, the site has been unable to resolve a persistent\ngender imbalance among its editors (Huang, 2013;\nMeta-wiki, 2018). This imbalance is re\ufb02ected in\nwho gets written about, and how (Bamman and\nSmith, 2014; Graells-Garrido et al., 2015; Wagner\net al., 2015). There is also a pervasive urban bias;\neditors are less likely to come from rural areas, and\n2https://github.com/kernelmachine/\nquality-filtercoverage of these areas in Wikipedia tends to be\nmore limited (Mandiberg, 2020). Although cov-\nerage in English Wikipedia is not limited to those\nplaces where English is a majority language, an\nAnglo-American perspective dominates coverage.3\nLastly, a relatively small number of people are re-\nsponsible for most of the content (Panciera et al.,\n2009; Matei and Britt, 2017). Wikipedia is thus\nless representative of language of the population\nthan one might expect given its size and design.\nBooks Language models are also frequently\ntrained on book corpora. BERT (Devlin et al.,\n2019) used the Toronto BookCorpus (Zhu et al.,\n2015), which consists of 7,185 self-published nov-\nels, a dataset criticized for copyright violation, poor\nquality control, imbalanced representation, and\nlack of documentation (Bandy and Vincent, 2021).\nGPT-3 (Brown et al., 2020) and The Pile (Gao\net al., 2020) both use much larger corpora of books\n(although the former do not identify the source of\nthis data). However, the Pile\u2019s books (also called\nBooks3) are not a random selection. Rather, they\nappear to be drawn from a torrent \ufb01le containing\nhundreds of thousands of copyrighted eBooks.\nBooks3 is deserving of a more thorough inves-\ntigation, but preliminary analyses reveal that the\nmost prevalent authors in the corpus are American\nand British writers, especially of romance, mys-\ntery, and children\u2019s books (e.g., L. Ron Hubbard,\nDanielle Steel, etc.). This pattern should be consid-\nered against the background of the American book\npublishing industry, which has been widely criti-\ncized as homogeneous (Lee & Low Books, 2020).4\nNews and Other Popular Internet Content\nRadford et al. (2019) scrape text from the websites\nfeatured in popular Reddit submissions (i.e., those\nthat received at least three upvotes) to construct the\ntraining data for GPT-2 . As the original corpus\nis unavailable, we analyze its open-source replica,\nOpenWebText (Gokaslan and Cohen, 2019).\nWe do not expect the corpus to represent a wide\nrange of language variation. Reddit users are\nmostly male, younger, and lean liberal, which in\ufb02u-\nences the types of content shared on the platform.5\n3For example, of the ten most frequently mentioned peo-\nple in English Wikipedia, seven are U.S. Presidents, two are\nprominent \ufb01gures in Christianity, and the only woman is the\nBritish monarch, Queen Victoria.\n4This 2020 study found that Black people comprise only\n5% of the industry, and books by men tend to generate dispro-\nportionately more attention than those by women.\n5As of 2016, 71% of Reddit users are male, 59% are be-\nURL Domain # Docs % of Total Docs\nbbc.co.uk 116K 1.50%\ntheguardian.com 115K 1.50%\nwashingtonpost.com 89K 1.20%\nnytimes.com 88K 1.10%\nreuters.com 79K 1.10%\nhuf\ufb01ngtonpost.com 72K 0.96%\ncnn.com 70K 0.93%\ncbc.ca 67K 0.89%\ndailymail.co.uk 58K 0.77%\ngo.com 48K 0.63%\nTable 1: The most popular top-level URL domains\nin OpenWebText. Mainstream news forms the over-\nwhelming majority of content in the dataset. Overall,\njust 1% of the top-level URL domains in OpenWebText\ncontribute 75% of the total documents in the corpus.\nViral media on the Internet assume similar charac-\nteristics; they tend to elicit awe, anger, or anxiety\n(Berger and Milkman, 2012), validate group identi-\nties (Gaudette et al., 2021), and disseminate from\nusers with authority (Weismueller et al., 2022).\nIndeed, we \ufb01nd that 1% of the 311K unique\ntop-level domains in OpenWebText contribute 75%\nof documents in the corpus (Table 1). The most\ncommon websites in OpenWebText are internation-\nally circulating British and American news out-\nlets (e.g., BBC ,The New York Times ,The Wash-\nington Post ,The Guardian ), blogging platforms\n(e.g., Tumblr ,Blogspot , orMedium ), sports con-\ntent (e.g., ESPN ,SBNation ), and tech news (e.g.,\nTechCrunch ,Wired ). As expected, these links tend\nto appear on the most highly traf\ufb01cked subreddits\n(e.g., /r/politics ,/r/worldnews ,/r/news ).\nThese data are likely dominated by formal writ-\ning styles. Among news organizations, the adher-\nence to slowly evolving style guides expresses spe-\nci\ufb01c linguistic standards (Froke et al., 2020) and\neven geopolitical interests (Vultee, 2012), which\nencourage rules about language use that can rein-\nforce gender norms and racial hierarchies (DiNi-\ncola, 1994; Bien-Aim\u00e9, 2016).\nIn general, a relatively homogeneous set of au-\nthors writes the majority of newswire (Grieco,\n2018). Researchers \ufb01nd a striking lack of diversity\nin newsrooms and newspaper leadership.6This\nmay be compounded by the economic hardships as-\npiring journalists must incur,7which act as a \ufb01lter\ntween ages 18\u201329, and 43% identify as liberal (vs. 19% con-\nservative): https://pewrsr.ch/3FLbNL7\n6As of 2018, racial minorities make up 37% of the U.S.\npopulation, but only 17% of staff and 13% of leadership in\nU.S. newsrooms (Arana, 2018).\n7In 2020, median salary for U.S. news analysis, reporters,for who can afford to be employed in newsrooms.\nSummary Authors from speci\ufb01c, relatively pow-\nerful social positions produce a disproportionate\namount of text in the core data sources of existing\nlanguage models. These text sources favor privi-\nleged segments of the English-speaking population,\nincluding men, white populations, communities of\nhigher socio-economic status, and those harboring\nAmerican and Western European historical, geopo-\nlitical, and cultural perspectives. By contrast, these\ncorpora tend to be less inclusive of the voices of\nwomen and members of marginalized groups. Al-\nternative perspectives, including those of people\nfrom rural areas, non-dominant gender, sexual, or\nracial identities, and counter-hegemonic vantage\npoints, are less likely to be included, and thus less\nlikely to in\ufb02uence models trained on this data.\nAlthough formal, streamlined content like news\nor Wikipedia articles may seem like desirable\nsources for high quality content, not all writing\nstyles or substantive topics that might be relevant\nto language technologies and their user communi-\nties are represented in the resulting corpora. When\ndeployed, many of the technologies using language\nmodels trained on these data will face language\nthat\u2014despite being less formal, professional, or\ncarefully edited\u2014is no less high quality and is es-\nsential to the communicative lives of the people\nwho use it.\n3 Measuring the Language Ideology of\nthe GPT-3 Quality Filter\nEmpirically evaluating the full distribution of au-\nthors in the data sources from \u00a72 is dif\ufb01cult, due to\ntheir size, as well as their lack of metadata about\neach document\u2019s authors. We instead curate a new\ndataset of U.S. high school newspaper articles that\nvaries both topically and along demographic vari-\nables that can be resolved using ZIP codes. Al-\nthough we do not directly consider individual au-\nthors of these articles, this dataset is useful, in that\nit can be associated with extensive metadata at the\nlevel of individual newspapers. We then analyze\nthe behavior of a (replicated) quality \ufb01lter on text\nfrom this dataset and discuss its implications.\n3.1 U.S. S CHOOL NEWS\nBackground Many U.S. schools produce a news-\npaper to give students journalism experience, to\nand journalists was $35,950, a slight decrease from 2012 after\nadjusting for in\ufb02ation: https://pewrsr.ch/3qCO75v\nreport on local news, to comment on national or\nglobal events, and to publish school-related mate-\nrial (e.g., announcements, campus life, student in-\nterviews, sports or honor rolls; Gibson, 1961). The\nsubstantive content of school newspapers varies\nconsiderably, possibly due to their local audiences.\nBecause a school\u2019s access to resources is shaped\nby local income levels (Betts et al., 2000) and tied\nto student achievement (Greenwald et al., 1996),\nwe expect schools in wealthier areas (relative to\npoorer areas) to produce newspaper content that is\nmore similar to the formal, professional texts that a\nquality \ufb01lter is likely to classify as high quality.\nCollection We collect articles from English-\nlanguage U.S. school newspapers that used a com-\nmon Wordpress template.8After retrieving 2483\nschools who use this template, we scrape 1.95M\narticles from their respective newspaper sites (more\ndetails in \u00a7A.2). We retrieve article categories by\nextracting them from the article URL slugs. We\nthen resolve each school to its ZIP code using the\nGoogle Maps Place API.9We restrict our dataset to\narticles from U.S. high schools. We only consider\narticles from 2010\u20132019, remove pages under the\nvideo ,photo , ormultimedia categories, and remove\nschools that have less than 100 articles (which tend\nto contain scraping errors). The \ufb01nal corpus in-\ncludes 910K articles, from 1410 schools, located\nin 1329 ZIP codes (552 counties) dispersed across\nall U.S. states (plus D.C.).\nLimitations Our corpus is neither a random nor\na representative sample of U.S. school newspapers.\nInstead, it represents schools that had suf\ufb01cient\nInternet access, that elected to use a particular\nwebsite template, and that maintain websites with\nretrievable archived content. The lack of repre-\nsentation in school newspaper leadership positions\nmay in\ufb02uence which students contribute content\nto school newspapers (Chen et al., 2021). Educa-\ntors also likely shape some articles, at least in part\n(though we expect them to be similarly affected by\nresource constraints). Finally, much of the content\nin these articles is speci\ufb01c to student concerns (e.g.,\nsports, school events, campus culture, etc.), and\nthe writing is, by de\ufb01nition, amateur. Nevertheless,\nbecause the corpus captures a wide range of con-\ntent and geographical areas, it allows us to evaluate\n8SNOsites.com\n9https://developers.google.com/maps/\ndocumentation/places/web-service/\nsearch-find-place?hl=enhow a quality \ufb01lter handles real-world language\nvariation, within a particular domain.\nUsing text from school newspapers introduces\nprivacy concerns, especially since authors and sub-\njects are minors. We therefore use this data only for\nevaluation purposes, and do nottrain (or release)\nany models on this data, or any raw text from the\ncorpus. We do, however, release a Datasheet (Ge-\nbru et al., 2021) which documents the dataset\u2019s gen-\neral characteristics and curation procedure (\u00a7A.2).\n3.2 The GPT-3 Quality Filter\nTo investigate how quality correlates with vari-\nous attributes of a newspaper, we re-implement\nthe Brown et al., 2020 quality \ufb01lter based on the\ndescription provided in the paper. The \ufb01lter is a\nbinary logistic regression classi\ufb01er trained (using\nn-gram features) to distinguish between reference\ncorpora (Books3, Wikipedia, and OpenWebText)\nand a random sample of Common Crawl.\nWe replicate the \ufb01lter as closely as possible us-\ningscikit-learn (Pedregosa et al., 2011). To\ncreate the training data for the classi\ufb01er, we sam-\nple 80M whitespace-separated tokens of OpenWeb-\nText, Wikipedia, and Books3 each for the positive\nclass, and 240M whitespace-separated tokens of\na September 2019 Common Crawl snapshot for\nthe negative class. We download the Common\nCrawl snapshot using code provided by Wenzek\net al. (2020). We perform a 100-trial random hyper-\nparameter search, \ufb01xing only the hashing vectorizer\nand basic whitespace tokenization, following the\nimplementation in Brown et al. (2020). See the\nsearch space and \ufb01nal hyperparameters of our repli-\ncated \ufb01lter in \u00a7A.3. Our \ufb01nal classi\ufb01er gets 90.4%\nF1(91.7% accuracy) on a set of 60M test tokens\n(30M held-out tokens from each class, or 72K doc-\numents from the negative class, and 33K from the\npositive class). We release code for training the\nquality \ufb01lter and a demo of the trained \ufb01lter.10;11\nWe apply the quality \ufb01lter to the U.S. S CHOOL\nNEWS data, computing a quality score per docu-\nment, which we denote P(high quality ).\n3.3 Document-Level Analysis\nWe \ufb01rst explore document-level preferences of the\n\ufb01lter. The GPT-3 quality \ufb01lter is more likely to\nclassify high school newspaper articles as low qual-\n10https://github.com/kernelmachine/\nquality-filter\n11https://huggingface.co/spaces/ssgrn/\ngpt3-quality-filter\n0.0 0.2 0.4 0.6 0.8 1.0\nP(high quality)01234DensityNewswire\nHigh School\n0.0 0.2 0.4 0.6 0.8 1.0\nP(high quality)announcements\ncampus-life\nclubs\nsports\nop-ed\npoliticsArticle Category\nFigure 1: Scraped school articles tend to be consid-\nered lower quality by the GPT-3 quality \ufb01lter than gen-\neral newswire (histogram built from 10K random doc-\numents from each domain). This \ufb01nding is consistent\nacross a variety of categories, and more signi\ufb01cant for\ncertain ones (e.g., school announcements).\nity, compared to general newswire (Figure 1).12\nThis is unsurprising, since the training data for the\nGPT-3 quality \ufb01lter included texts by professional\njournalists. \u00a7A.4 shows a random sample of text\nfrom the dataset with high and low quality scores,\nillustrating differences in style and formality.\nMore notably, controlling for article category\n(e.g., opinion pieces), we \ufb01nd that the GPT-3 qual-\nity \ufb01lter has topical and stylistic preferences (dis-\ncovered through exploratory data analysis). For\ntopical features, we train an topic model (via Latent\nDirichlet Allocation; Blei et al. 2003) over opin-\nion pieces with 10 topics using scikit-learn .\nWe also consider whether documents contain \ufb01rst,\nsecond, or third person pronouns, and the length of\nthe document. We then combine these features in a\nregression model to assess the effect of particular\nattributes on the quality score of a document, while\ncontrolling for others.\nThe results of our regression are displayed in\nTable 2. We \ufb01nd that certain topics have quite\nlarge effect sizes (see \u00a7A.5 for the distribution of\nquality scores per topic). For example, documents\n12Here, the general newswire are articles from popular on-\nline news sources; see \u00a74 for data details.Dependent variable: P(high quality )\nNumber of observations: 10K opinion articles\nFeature Coef\ufb01cient\nIntercept 0:471\u0003\u0003\u0003\nTopic 5 ( christmas, dress, holiday ) \u00000.056\u0003\u0003\u0003\nTopic 2 ( school, college, year ) \u00000.037\u0003\u0003\u0003\nTopic 6 ( student, school, class ) \u00000.004\nTopic 1 ( people, just, like ) 0.003\nTopic 7 ( movie, \ufb01lm, movies ) 0.062\u0003\u0003\u0003\nTopic 3 ( music, album, song ) 0.113\u0003\u0003\u0003\nTopic 4 ( people, women, media ) 0.197\u0003\u0003\u0003\nTopic 9 ( game, team, players ) 0.246\u0003\u0003\u0003\nTopic 8 ( Trump, president, election ) 0.346\u0003\u0003\u0003\nPresence of \ufb01rst/second person pronoun \u00000.054\u0003\u0003\u0003\nPresence of third person pronoun 0.024\nlog2(Number of tokens) 0.088\u0003\u0003\u0003\nR20.336\nadj.R20.336\nTable 2: Regression of the quality score of an opinion\npiece in the U.S. S CHOOL NEWS dataset, on document\nfeatures. We observe that political and sports-related\ntopics, the lack of \ufb01rst and second person pronouns,\nand longer document lengths are associated with higher\nquality scores. We omit Topic 0 ( food,restaurant ,eat)\nto avoid a saturated model. See \u00a7A.5 for the distribu-\ntion of quality scores per topic.\u0003p <0.05,\u0003\u0003p <0.01,\n\u0003\u0003\u0003p <0.001.\nentirely about Trump and the presidential election\nhave quality scores 35 percentage points higher, on\naverage, whereas documents about sports are 25\npercentage points higher (relative to the omitted\ntopic about food). Stylistically, the presence of \ufb01rst\nor second pronouns in a document decreases qual-\nity score by 5 percentage points, while a doubling\nof the number of tokens in a document increases\nthe quality score by 9 percentage points.\n3.4 Demographic Analysis\nNext, we examine whether the GPT-3 quality \ufb01l-\nter prefers language from certain demographic\ngroups over others. We \ufb01rst check raw correla-\ntions between average quality scores (per newspa-\nper) and features of interest. As in \u00a73.3, we then\ncombine the features in a regression model.\nDemographic Features As we note in \u00a73.1, we\nexpect a priori that content from schools located\nin wealthier, more educated, and urban areas of the\nU.S. will tend to have higher quality scores, relative\nto poorer, less educated, rural areas. Therefore, we\nconsider demographic features that correspond to\nclass, rural/urban divides, and school resources.\nFor each school, we retrieve 2017\u20132018 school-\nlevel demographic data from the National Center\nfor Education Statistics (NCES).13These include\nthe number of students, student:teacher ratio, and\nindicators for charter, private, and magnet schools.\nWe also retrieve the latest ZIP code- and county-\nlevel demographic data from the 2020 U.S. Cen-\nsus.14To measure the wealth of the correspond-\ning ZIP code, we use median home values, and\nfor educational attainment we use the percentage\nof college-educated adults. We also use Census\ndata on the percent of rural population by county.\nFinally, we consider local political leanings, oper-\nationalized by GOP vote share in the 2016 Presi-\ndential election, using county-level data from the\nMIT election lab.15We display full descriptions of\nfeatures in our demographic analysis in \u00a7A.6.\nCorrelation Analysis To inform the variables\nwe include in our regressions, we explore correla-\ntions between variables of interest and the average\nquality score of a school newspaper. Our analyses\nin Figure 2 suggest that our initial hypotheses hold:\nschools in wealthier, urban, and more educated\nZIP codes, as well as those in Democrat-leaning\ncounties, tend to have higher quality scores.\nData Preprocessing Here, we use schools as\nthe unit of analysis, and consider average qual-\nity score assigned to the school\u2019s articles as the\ndependent variable. We only include those schools\nthat could be matched to the NCES database, drop-\nping schools which are missing school size, as\nwell as those located in ZIP codes with $1M or\ngreater median home value, due to a census arti-\nfact.16Missing values for other features are im-\nputed with the median value of that feature for the\ncorresponding ZIP code, or (if necessary) county\nor state. For regressions, we log-transform school\nsize, student:teacher ratio, and home values, us-\ning raw values for other features, to preserve in-\nterpretability. Our regression dataset includes 968\nhigh schools, in 926 ZIP codes across 354 coun-\nties. All linear regressions are implemented with\nthestatsmodels API.17We release this anony-\nmous dataset to support reproducibility.18\n13https://nces.ed.gov/ccd/elsi/\ntablegenerator.aspx\n14https://data.census.gov/cedsci/\n15https://electionlab.mit.edu/data\n16The census data we use imposes an arti\ufb01cial upper bound\non housing prices over $1M.\n17https://www.statsmodels.org/stable/\nindex.html\n18https://github.com/kernelmachine/\nquality-filter\n50K 100K 250K 500K 1M\nMedian Home Value0.10.20.30.40.50.60.7P(high quality)\nr: 0.27\n20 40 60 80\n% Adults  Bachelor Degrees\n0.10.20.30.40.50.60.7P(high quality)\nr: 0.30\n0 20 40 60 80\n% 2016 GOP Vote0.10.20.30.40.50.60.7P(high quality)\nr: -0.33\n0 20 40 60 80 100\n% Rural0.10.20.30.40.50.60.7P(high quality)\nr: -0.30Figure 2: Scatter plots displaying correlations of select\ndemographic features of a school\u2019s ZIP code or county\nwith its average P(high quality ).\nRegression Analysis Because the variables iden-\nti\ufb01ed above are correlated with each other, we use\nregression to estimate the effect of certain factors\nwhile controlling for others, with results shown\nin Table 3. Overall, home values, parental educa-\ntion, school size, public school status, and urban\nlocations all show signi\ufb01cant positive associations\nwith quality scores. Thus, even controlling for \ufb01-\nnancial resources, parental education, and other\nfactors, articles from rural schools are still scored\nas signi\ufb01cantly lower quality than those from urban\nschools.\nNevertheless, the effects, considered individu-\nally, are relatively modest. A 14 percentage point\nincrease in percent urban population or a 17 per-\ncentage point increase in parental education (per-\ncent of adults with college degrees) correspond to\na 1 percentage point increase in average quality\nscore, as does a doubling of home values, or a qua-\ndrupling of school size. Average quality scores\nassociated with public schools are 1.5 percentage\npoints higher than private schools, controlling for\nother factors. Coef\ufb01cients for charter schools, mag-\nnet schools, and student:teacher ratio are all sen-\nsible, though none are signi\ufb01cant. Altogether, the\ncombined effects of all these factors account for\nlarge differences in quality scores between wealthy,\nurban, educated locations, and poorer, rural, and\nless educated parts of the country.\nDependent variable: P(high quality )\nObservations: 968 schools\nFeature Coef\ufb01cient\nIntercept 0:076\n% Rural \u00000.069\u0003\u0003\u0003\n% Adults \u0015Bachelor Deg. 0.059\u0003\u0003\nlog2(Median Home Value) 0.010\u0003\nlog2(Number of students) 0.006\u0003\nlog2(Student:Teacher ratio) \u00000.007\nIs Public 0.015\u0003\nIs Magnet 0.013\nIs Charter 0.033\nR20.140\nadj.R20.133\nTable 3: Regression of the average P(high quality )of\na school in the U.S. S CHOOL NEWS dataset, on demo-\ngraphic variables. We observe that larger schools in ed-\nucated, urban, and wealthy areas of the U.S tend to be\nscored higher by the GPT-3 quality \ufb01lter. See \u00a7A.6 for\nmore information on these features.\u0003p < 0.05,\u0003\u0003p <\n0.01,\u0003\u0003\u0003p <0.001.\nSummary and Limitations This analysis re-\nveals an unintended consequence of the GPT-3\nquality \ufb01lter: by attempting to exclude text that is\nless like mainstream news and Wikipedia, the \ufb01lter\nreinforces a language ideology that text from au-\nthors of wealthy, urban, and educated backgrounds\nare more valuable for inclusion in language model\ntraining data. These implicit preferences align with\nthe attributes of authors that dominate the corpora\nfrom \u00a72, which the \ufb01lter considers to be high qual-\nity.\nAlthough most of the above \ufb01ndings are robust\nto alternate model speci\ufb01cations, the model ulti-\nmately only accounts for a relatively small amount\nof variance in quality scores. In addition, most of\nour features are taken from a single a point in time,\nand do not account for changing demographics over\nthe period 2010\u20132019. Data errors could also arise\ndue to how datasets were aligned (based on school\nname and ZIP code). These \ufb01ndings may not gen-\neralize to other domains (e.g., social media), and\ninclusion of additional features could affect these\n\ufb01ndings. For additional models which include vote\nshare and racial demographics taken from NCES\ndata, see \u00a7A.7.\n4 Alignment with Other Notions of\nQuality\nTheGPT-3 quality \ufb01lter purports to judge the qual-\nity of text. Humans, on the other hand, frequently\njudge the quality of text without the use of auto-mated systems. In this section, we consider three\nforms of human evaluations: institutional awards\nto select books, fact-checkers\u2019 designated factual-\nity of news outlets, and standardized test essays\nevaluated by human graders. How well does the\nbehavior of the GPT-3 quality \ufb01lter map onto these\nother notions of quality?\n4.1 Data\nFactually (Un)reliable News To analyze the cor-\nrespondence between the GPT-3 quality \ufb01lter and\nnews factuality, we use the list provided by Baly\net al. (2018) to identify a set of popular news\nsources from a broad range of factuality ratings\nand political leanings.19Using Newspaper3k ,20\nwe scrape and score 9.9K and 7.7K articles from\nhigh and low factuality news outlets, respectively.\nEssay Exams Next, to analyze the correspon-\ndence between the GPT-3 quality \ufb01lter and es-\nsay scores, we collect and score 12.1K partici-\npant essays from the Test Of English as a Foreign\nLanguage (TOEFL) exam, a widely used English\nlanguage pro\ufb01ciency test (Blanchard et al., 2013).\nThe TOEFL exam responses include of\ufb01cial scores\nfrom exam readers, as well as each essay\u2019s prompt.\nAward-Winning Literature Finally, to analyze\nthe correspondence between the GPT-3 quality \ufb01l-\nter and literary awards, we select and score books\nfrom Books3 and the Gutenberg corpus (Brooke\net al., 2015) that have won a Pulitzer Prize in vari-\nous categories. We collected these data by scraping\nthe publicly available list of recipients.21\n4.2 Results\nIf the \ufb01lter aligns with news factuality, we would\nexpect that articles from factually reliable sources\nwould be rated as higher quality than those from\nfactually unreliable ones. However, we \ufb01nd no\ndifference in the quality distribution between ar-\nticles from high and low factuality news sources\n(p=0.085, two-way Kolmogorov-Smirnov test;\nFigure 3). Many factually unreliable news articles\nare considered high quality by the \ufb01lter (\u00a7A.8).\nTurning to the TOEFL exam responses, we\nwould expect that if the \ufb01lter agrees with essay\n19Baly et al. (2018) release a dataset of factual reliabil-\nity and political leanings across news sources by scraping\nNewsMediaBiasFactCheck.org .\n20https://newspaper.readthedocs.io/en/\nlatest/\n21https://www.pulitzer.org/\nprize-winners-categories\n0.0 0.2 0.4 0.6 0.8 1.0\nP(high quality)0.00.20.40.60.81.01.21.4DensityHigh Factuality News\nLow Factuality NewsFigure 3: There is no difference in quality scores be-\ntween articles written by news sources of high and low\nfactual reliability.\nnonfiction fiction poetry drama\nPulitzer Prize Category0.10.20.30.40.50.60.70.8P(high quality)median quality\nof BooksCorpus\nFigure 4: Among works that have won a Pulitzer Prize,\nthe quality \ufb01lter tends to favor non\ufb01ction and longer\n\ufb01ctional forms, disfavoring poetry and dramatic plays.\nscores, higher scoring essays would receive higher\nquality scores. While essay scores are weakly\ncorrelated with quality scores (Pearson r=0.12,\np <0.001), Table 4 demonstrates that the essay\u2019s\nprompt is far more predictive of the essay\u2019s qual-\nity designation. For example, essays responding\nto a prompt ( P4) which asks participants to de-\nscribe \"...whether advertisements make products\nseem much better than they really are\" are much\nless likely to be \ufb01ltered than all other prompts, in-\ncluding P6, which asks participants to describe\n\"...whether it is best to travel in a group\" (see \u00a7A.9\nfor more details). The latter prompt tends to invoke\npersonal experiences in the responses.\nFinally, if the \ufb01lter aligns with literary awards,\nwe would expect that most Pulitzer-Prize winning\nbooks would achieve high quality scores. On the\ncontrary, quality scores vary heavily based on the\ngenre (Figure 4). Poetry and drama are less favored\nby the \ufb01lter relative to non-\ufb01ction, \ufb01ction, and even\nfan \ufb01ction (from the BookCorpus; Zhu et al. 2015).Dependent variable: P(high quality )\nObservations: 12.1K TOEFL exams\nFeature Coef\ufb01cient\nIntercept 0.0631\u0003\u0003\u0003\nLow score \u00000.0414\nHigh score 0.0339\nPrompt 7 \u00000.0283\u0003\u0003\u0003\nPrompt 6 \u00000.0204\u0003\u0003\u0003\nPrompt 2 0.0068\u0003\u0003\u0003\nPrompt 8 0.0346\u0003\u0003\u0003\nPrompt 3 0.0880\u0003\u0003\u0003\nPrompt 5 0.1470\u0003\u0003\u0003\nPrompt 4 0.6745\u0003\u0003\u0003\nR20.712\nadj.R20.711\nTable 4: Regression of the quality of a TOEFL exam\nessay on its assigned score and prompt. While we ob-\nserve some relationship between the score an essay re-\nceives and its quality score, the essay prompts them-\nselves have signi\ufb01cantly higher effect sizes. The high-\nest quality essays come from Prompt 4, which asks par-\nticipants to discuss products and advertisements. See\n\u00a7A.9 for visualizations of distributions of quality across\nprompts and scores.\u0003p < 0.05,\u0003\u0003p < 0.01,\u0003\u0003\u0003p <\n0.001.\nSummary Our analysis demonstrates that the\nGPT-3 quality \ufb01lter con\ufb02icts with other standards\nof text quality. Of course, even the alternative stan-\ndards we compare here are subject to their own lan-\nguage ideologies. Readers are more likely to trust\nnews as factual if its political position aligns with\ntheir own (Mitchell et al., 2018). English-language\nteaching pedagogies are rooted in ideologies about\nwell-spokenness (Vanegas et al., 2016). Literary\nawards favor white and male authors.22In general,\nany designation of text as high quality is subjective\nand in\ufb02uenced by sociopolitical context.\n5 Discussion\nThe above sections have demonstrated that auto-\nmated \ufb01ltering of text to build language modeling\ncorpora may lead to counterintuitive or undesirable\nexclusion of sources. Because of the variety of use\ncases for language models and the broad range of\ntext that could be appropriate for certain tasks, we\nsuggest that there is no simple, universal standard\nfor what should be considered high quality text.\nIndeed, there is a long history of privileging some\npeople\u2019s spoken language as better or more \u201ccor-\n22A 2016 study by the Columbia Journalism Review found\nthat since 1918, 84% of Pulitzer Prizes had been awarded\nto white authors, and 84% to male authors: https://www.\ncjr.org/analysis/100_years_of_data.php .\nrect\u201d than others. Researchers and practitioners of\nNLP who are aware of this history have the option\nto be intentional in their design of systems that,\nhowever implicitly, risk excluding the language of\nunderprivileged identities or communities.\nSome amount of selection in building corpora is\ninevitable. It is not possible to collect a uniform\nrandom sample of all written utterances. However,\nour \ufb01ndings suggest that current selection methods\nare, for many purposes, \ufb02awed. Future work into\nalternative \ufb01ltering criteria could be paired with\ninvestigations into the unintended consequences of\ntheir assumptions.\nWe do not believe that there is likely to be a\nsingle solution to this challenge. Indeed, the text\nthat is best suited for training a model may depend\non the application of that model. At a minimum,\nhowever, the NLP community could more care-\nfully consider and clearly document the criteria by\nwhich text is being selected for inclusion. NLP\npractitioners could also be explicit about the rea-\nsons for using certain sources, even if those reasons\nare related to availability or empirical performance.\nA collection of tests could also be deployed (and\nimproved over time), to give a clear understanding\nof the implications of different choices of \ufb01lters.\nMore generally, we echo calls in the literature\nfor more thoughtful and inclusive data collection\n(Jo and Gebru, 2020; Bender et al., 2021; Tanweer\net al., 2021). This could include, but is not lim-\nited to a) intentionally curating data from people\nand viewpoints that are not otherwise well repre-\nsented; b) including a greater diversity of genres; c)\nmore nuanced or intentional exclusion criteria; d)\nmore thorough interrogation of what text is being\nexcluded; e) developing standard checks for promi-\nnent biases in inclusion; f) abandoning the notion\nof a general-purpose corpus.\n6 Ethical Considerations & Limitations\nOurU.S. S CHOOL NEWS dataset comes with many\nlimitations, as described in \u00a73.1. For example, the\ndataset contains sampling biases (e.g., it depends\non use of a speci\ufb01c publication template), and the\nZIP codes and counties are not uniformly spread\nacross U.S. states. In general, our dataset likely\ncaptures neither the least resourced schools (which\nmay not have access to online resources) in the\nUnited States, nor the wealthiest ones (who may\nhave their own publication platforms). However,\nwe speculate that an expanded corpus, which in-cluded writings from these schools, would demon-\nstrate a continuation of trends we report in this\npaper.\nWhile the text in our dataset varies considerably\nalong topical, stylistic, and demographic variables,\nit is a niche domain; the text is a speci\ufb01c genre\nmeant for local student consumption, its authors\nare U.S. students, and it thus primarily represents\nU.S.-centric cultural and political perspectives. We\nacknowledge that we also perpetuate some of the\nbiases we identify, especially by working with En-\nglish language text from the United States. We\nhope future work will extend this study of language\nideologies to multilingual settings, other textual do-\nmains, and different sets of authors.\nWith respect to demographic variables, we\nmerge census demographics with school-level data\nvia ZIP codes or counties, which are imperfect\nidenti\ufb01ers of a school, since ZIP codes (and coun-\nties) may include multiple schools of varying re-\nsource levels. Moreover, tracking demographic\nvariables and other author metadata, if deployed\nat scale, implies a certain level of invasive surveil-\nlance (Brayne, 2017). Future work may explore\nhow to maintain the rights of authors as data sub-\njects and producers while mapping demographic\nrepresentation in large corpora.\nFinally, we did not seek consent from authors to\nscrape their articles. The ethical and legal norms\naround scraping public-facing web data, especially\nthose produced by minors, are still in \ufb02ux (Fiesler\net al., 2020), and may not align with user percep-\ntions of what constitutes fair use of online commu-\nnications (Williams et al., 2017). For these reasons,\nwe do not release the corpus of school newspaper\narticles, and only use it for analysis and evaluation.\nWe only make available a dataset of demographic\nvariables and quality scores per school, to support\nreproducibility.\n7 Related Work\nLanguage Ideologies Language ideologies have\nbeen widely explored in the sociolinguistics lit-\nerature (Gal and Irvine, 1995; Rosa and Flores,\n2017; Craft et al., 2020, inter alia ). An ideology\nthat promotes the inherent correctness, clarity, and\nobjectivity of certain language varieties over oth-\ners is a mechanism for linguistic discrimination\n(Craft et al., 2020; Gal, 2016; MacSwan, 2020;\nRickford and King, 2016). A salient example of\nsuch discrimination is the stigmatization of second-\nlanguage speakers of English (Lindemann, 2005).\nLanguage ideologies have an important, but of-\nten unacknowledged, in\ufb02uence on the development\nof NLP technologies (Blodgett et al., 2020). For\nexample, an ideology that distinguishes between\nstandard andnon-standard language variations sur-\nfaces in text normalization tasks (van der Goot\net al., 2021), which tend to strip documents of\npragmatic nuance (Baldwin and Chai, 2011) and\nsocial signals (Nguyen et al., 2021). Language\non the Internet has been historically treated as a\nnoisy variant of English, even though lexical vari-\nation on the Internet is highly communicative of\nsocial signals (Eisenstein, 2013), and varies con-\nsiderably along demographic variables (Eisenstein\net al., 2014) and community membership (Lucy\nand Bamman, 2021). Language ideologies also sur-\nface in tools for toxicity detection; for example, the\nclassi\ufb01cation behavior of the PERSPECTIVE API\n(a popular hate speech detector) aligns with the at-\ntitudes of conservative, white, female annotators,\nwho tend to perceive African-American dialects as\nmore toxic (Sap et al., 2021). In this work, we ex-\namine the language ideology encoded in a widely\nused quality \ufb01lter for text data selection.\nCritiques of Laissez-Faire Data Collection We\nprovide empirical evidence that laissez-faire data\ncollection (i.e., \ufb01ltering large web data sources)\nleads to data homogeneity (Bender et al., 2021).\nAs an alternative to laissez-faire collection, Jo and\nGebru (2020) recommend drawing on institutional\narchival practices. However, we note that language\nideologies are also prevalent (and may not be ex-\nplicit) in institutional archives, which, for exam-\nple, have preferred colonial voices over colonized\nones when documenting historical events (Trouil-\nlot, 1995; Decker, 2013).\nOther Quality Filters Other de\ufb01nitions of text\nquality are used to create pretraining datasets, some\nof which do not rely on the datasets from \u00a72. How-\never, all techniques adopt language ideologies of\nwhat constitutes high quality text. Bad-word \ufb01lter-\ning, which removes documents that contain certain\nstop-words, disproportionately excludes language\nabout and by minority groups (Dodge et al., 2021).\nFiltering Internet content for popularity (Radford\net al., 2019) leads to data homogeneity based on\nthe characteristics of viral media and the compo-\nsition of userbases in online forums (\u00a72). Even\nlightweight \ufb01lters (Aghajanyan et al., 2021; Raeet al., 2021) put more emphasis on features like\ndocument length over factuality when determining\nwhat makes a document high quality. Any \ufb01lter-\ning method requires transparent justi\ufb01cation and\nrecognition of tradeoffs.\nDownstream Behavior The behavior of lan-\nguage systems aligns with what we would expect\nfrom a language ideology that favors training data\nwritten by a narrow, powerful sector of society.\nFor example, dialogue agents perform signi\ufb01cantly\nworse when engaging in conversations about race\n(Schlesinger et al., 2018) and with minority dialects\nof English (Mengesha et al., 2021). GPT-3 fre-\nquently resorts to use of stereotypes when minority\ngroups are mentioned in its prompt (Abid et al.,\n2021; Blodgett, 2021). GPT-3 is also prone to\nproducing hate speech (Gehman et al., 2020) and\nmisinformation (McGuf\ufb01e and Newhouse, 2020),\nwhich we would expect if its quality \ufb01lter fails to\ndistinguish the factual reliability of news sources in\nits training data (\u00a74). Concurrent to this work, Gao\n(2021) show that aggressive data \ufb01ltering with the\nGPT-3 quality \ufb01lter degrades downstream task per-\nformance. A closer analysis of how the language\nideologies in data selection lead to certain model\nbehaviors is a rich area for future work.\n8 Conclusion\nUsing a new dataset of U.S. school newspapers,\nwe \ufb01nd that the conventional, automated valuation\nof Wikipedia, newswire, books, and popular In-\nternet content as reference for high quality text\nimplicitly favors content written by authors from\nlarger schools in wealthier, educated, urban areas\nof the United States. Adopting this language ide-\nology for text data selection leads to implicit, yet\nsystematic and as-yet undocumented inequalities\nin terms of whose language is more likely to be\nincluded in training corpora. Although no single\naction will solve this complicated issue, data cu-\nrators and researchers could be more intentional\nabout curating text from underrepresented authors\nand groups, gathering sources from multiple genres\nand writing styles, and documenting their curation\nprocedures and possible sources of exclusion.\nAcknowledgments\nThis paper bene\ufb01ted from thoughtful feedback\nfrom a number of people: Emily M. Bender, Amber\nBoydstun, Timnit Gebru, Eun Seo Jo, Kelvin Luu,\nLucy Li, Julian Michael, Amandalynne Paullada,\nKatharina Reinecke, Swabha Swayamdipta, Kelly\nWright, and Kaitlyn Zhou.\nReferences\nAbubakar Abid, Maheen Farooqi, and James Zou.\n2021. Persistent anti-Muslim bias in large language\nmodels. In Proceedings of the 2021 AAAI/ACM Con-\nference on AI, Ethics, and Society .\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke\nZettlemoyer. 2021. Htlm: Hyper-text pre-training\nand prompting of language models. arXiv ,\nabs/2107.06955.\nGabriel Arana. 2018. Decades of failure. Columbia\nJournalism Review .\nTyler Baldwin and Joyce Chai. 2011. Beyond normal-\nization: Pragmatics of word form in text messages.\nInProceedings of 5th International Joint Conference\non Natural Language Processing .\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of EMNLP .\nDavid Bamman and Noah A Smith. 2014. Unsuper-\nvised discovery of biographical structure from text.\nTransactions of the Association for Computational\nLinguistics , 2:363\u2013376.\nJack Bandy and Nicholas Vincent. 2021. Addressing\n\u201cdocumentation debt\u201d in machine learning: A retro-\nspective datasheet for BookCorpus. In NeurIPS .\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of FAccT .\nJonah Berger and Katherine L. Milkman. 2012. What\nmakes online content viral? Journal of Marketing\nResearch , 49(2):192\u2013205.\nJulian R. Betts, Kim S. Reuben, and Anne Danenberg.\n2000. Equal Resources, Equal Outcomes? The Dis-\ntribution of School Resources and Student Achieve-\nment in California . Public Policy Institute of Cali-\nfornia.\nSteve Bien-Aim\u00e9. 2016. AP stylebook normalizes\nsports as a male space. Newspaper Research Jour-\nnal, 37(1):44\u201357.Daniel Blanchard, Joel R. Tetreault, Derrick Higgins,\nA. Cahill, and Martin Chodorow. 2013. TOEFL11:\nA corpus of non-native English. ETS Research Re-\nport Series , 2013:15.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent Dirichlet allocation. J. Mach. Learn.\nRes., 3:993\u20131022.\nSu Lin Blodgett. 2021. Sociolinguistically Driven\nApproaches for Just Natural Language Processing .\nPh.D. thesis, University of Massachusetts Amherst.\nSu Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of \u201cbias\u201d in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5454\u2013\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Lisa Green, and Brendan O\u2019Connor.\n2016. Demographic dialectal variation in social me-\ndia: A case study of African-American English. In\nProceedings of EMNLP .\nSarah Brayne. 2017. Big data surveillance: The case of\npolicing. American Sociological Review , 82(5):977\u2013\n1008.\nJulian Brooke, Adam Hammond, and Graeme Hirst.\n2015. GutenTag: an NLP-driven tool for digital hu-\nmanities research in the Project Gutenberg corpus.\nInProceedings of the Fourth Workshop on Computa-\ntional Linguistics for Literature .\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.arXiv , abs/2005.14165.\nJanice Kai Chen, Ilena Peng, Jasen Lo, Trisha Ahmed,\nSimon J. Levien, and Devan Karp. 2021. V oices in-\nvestigation: Few black, latinx students are editors of\ntop college newspapers. AAJA Voices .\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR .\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nJustin T. Craft, Kelly E. Wright, Rachel Elizabeth\nWeissler, and Robin M. Queen. 2020. Language\nand discrimination: Generating meaning, perceiving\nidentities, and discriminating outcomes. Annual Re-\nview of Linguistics , 6(1):389\u2013407.\nStephanie Decker. 2013. The silence of the archives:\nbusiness history, post-colonialism and archival\nethnography. Management & Organizational His-\ntory, 8(2):155\u2013173.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL .\nRobert DiNicola. 1994. Teaching journalistic style\nwith the AP stylebook: Beyond fussy rules and\ndogma of \u2018correctness\u2019. The Journalism Educator ,\n49(2):64\u201370.\nJesse Dodge, Maarten Sap, Ana Marasovic, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, and Matt\nGardner. 2021. Documenting the english colossal\nclean crawled corpus. arXiv , abs/2104.08758.\nPenelope Eckert. 1989. Jocks and burnouts: Social cat-\negories and identity in the high school . Teachers col-\nlege press.\nJacob Eisenstein. 2013. What to do about bad language\non the internet. In Proceedings of NAACL , pages\n359\u2013369.\nJacob Eisenstein, Brendan O\u2019Connor, Noah A. Smith,\nand Eric P. Xing. 2014. Diffusion of lexical change\nin social media. PLoS ONE , 9.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and ef\ufb01cient sparsity.\nCasey Fiesler, Nathan Beard, and Brian Keegan. 2020.\nNo robots, spiders, or scrapers: Legal and ethical\nregulation of data collection methods in social media\nterms of service. In Proceedings of ICWSM .\nPaula Froke, Anna Jo Bratton, Jeff McMillan, Pia\nSarkar, Jerry Schwartz, and Raghuram Vadarevu.\n2020. The Associated Press stylebook 2020-2022 .\nThe Associated Press.\nSusan Gal. 2016. Sociolinguistic differentiation , page\n113\u2013136. Cambridge University Press.\nSusan Gal and Judith T. Irvine. 1995. The boundaries\nof languages and disciplines: How ideologies con-\nstruct difference. Social Research , 62(4):967\u20131001.\nLeo Gao. 2021. An empirical exploration in quality\n\ufb01ltering of text data. arXiv , abs/2109.00698.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800Gb dataset of diverse text for language modeling.\narXiv , abs/2101.00027.Tiana Gaudette, Ryan Scrivens, Garth Davies, and\nRichard Frank. 2021. Upvoting extremism: Collec-\ntive identity formation and the extreme right on red-\ndit.New Media & Society , 23(12):3491\u20133508.\nTimnit Gebru, Jamie Morgenstern, Briana Vec-\nchione, Jennifer Wortman Vaughan, Hanna Wal-\nlach, Hal Daum\u00e9 III, and Kate Crawford. 2021.\nDatasheets for datasets. Communications of the\nACM , 64(12):86\u201392.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxic-\nityPrompts: Evaluating neural toxic degeneration in\nlanguage models. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 .\nJoyce Still Gibson. 1961. A study of the status of high\nschool newspapers in the virginia public schools.\nMaster\u2019s thesis, University of Richmond.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus.\nEduardo Graells-Garrido, Mounia Lalmas, and Filippo\nMenczer. 2015. First women, second sex: Gender\nbias in Wikipedia. In Proceedings of the 26th ACM\nconference on hypertext & social media .\nRob Greenwald, Larry V . Hedges, and Richard D.\nLaine. 1996. The effect of school resources on stu-\ndent achievement. Review of Educational Research ,\n66(3):361\u2013396.\nElizabeth Grieco. 2018. Newsroom employees are less\ndiverse than U.S. workers overall. Pew Research\nCenter . [online; accessed 2022-01-22].\nDirk Hovy and Diyi Yang. 2021. The importance of\nmodeling social factors of language: Theory and\npractice. In Proceedings of NAACL .\nKeira Huang. 2013. Wikipedia fails to bridge gender\ngap. South China Morning Post . [online; accessed\n2022-01-11].\nEun Seo Jo and Timnit Gebru. 2020. Lessons from\narchives: Strategies for collecting sociocultural data\nin machine learning. Proceedings of FAccT .\nParesh Kharya and Ali Alvi. 2021. Using deepspeed\nand megatron to train megatron-turing nlg 530b, the\nworld\u2019s largest and most powerful generative lan-\nguage model. [online; accessed 2022-01-20].\nWilliam Labov. 2006. The Social Strati\ufb01cation of En-\nglish in New York City , 2 edition. Cambridge Uni-\nversity Press.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020 . OpenReview.net.\nLee & Low Books. 2020. Where is the diversity in pub-\nlishing? The 2019 diversity baseline survey results.\n[online; accessed 2021-11-24].\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics , pages 7871\u20137880, Online. Association\nfor Computational Linguistics.\nStephanie Lindemann. 2005. Who speaks \u201cbroken\nEnglish\u201d? US undergraduates\u2019 perceptions of non-\nnative English. International Journal of Applied Lin-\nguistics , 15(2):187\u2013212.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv , abs/1907.11692.\nLi Lucy and David Bamman. 2021. Characterizing\nEnglish Variation across Social Media Communities\nwith BERT. Transactions of the Association for\nComputational Linguistics , 9:538\u2013556.\nJeff MacSwan. 2020. Academic English as standard\nlanguage ideology: A renewed research agenda for\nasset-based language education. Language Teach-\ning Research , 24(1):28\u201336.\nMichael Mandiberg. 2020. Mapping wikipedia. The\nAtlantic . [online; accessed 2021-11-24].\nSorin Adam Matei and Brian C. Britt. 2017. Structural\nDifferentiation in Social Media . Springer Interna-\ntional Publishing.\nKris McGuf\ufb01e and Alex Newhouse. 2020. The radical-\nization risks of GPT-3 and advanced neural language\nmodels. arXiv , abs/2009.06807.\nZion Mengesha, Courtney Heldreth, Michal Lahav,\nJuliana Sublewski, and Elyse Tuennerman. 2021.\n\u201cI don\u2019t think these devices are very culturally\nsensitive.\u201d\u2014Impact of automated speech recogni-\ntion errors on African Americans. Frontiers in Ar-\nti\ufb01cial Intelligence , 4:169.\nMeta-wiki. 2018. Community insights/2018 re-\nport/contributors. [online; accessed 2012-11-24].\nAmy Mitchell, Jeffrey Gottfried, Michael Barthel, and\nNami Sumida. 2018. Can Americans tell factual\nfrom opinion statements in the news? Pew Re-\nsearch Center\u2019s Journalism Project . [online; ac-\ncessed 2022-01-22].\nDong Nguyen, Laura Rosseel, and Jack Grieve. 2021.\nOn learning and representing social meaning in NLP:\nA sociolinguistic perspective. In Proceedings of\nNAACL .Katherine Panciera, Aaron Halfaker, and Loren Ter-\nveen. 2009. Wikipedians are born, not made: A\nstudy of power editors on wikipedia. In Proceed-\nings of the ACM 2009 International Conference on\nSupporting Group Work .\nFabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-\ndre Passos, David Cournapeau, Matthieu Brucher,\nMatthieu Perrot, and \u00c9douard Duchesnay. 2011.\nScikit-learn: Machine learning in python. Journal\nof Machine Learning Research , 12(85):2825\u20132830.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers) , pages\n2227\u20132237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. [online; ac-\ncessed 2022-01-22].\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. [online;\naccessed 2022-01-22].\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, An-\ntonia Creswell, Nat McAleese, Amy Wu, Erich\nElsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nMichela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nmatzadeh, Elena Gribovskaya, Domenic Donato,\nAngeliki Lazaridou, Arthur Mensch, Jean-Baptiste\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake Hecht-\nman, Laura Weidinger, Iason Gabriel, William Isaac,\nEd Lockhart, Simon Osindero, Laura Rimell, Chris\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-\nway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. 2021. Scal-\ning language models: Methods, analysis & insights\nfrom training gopher. arXiv , abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch , 21(140):1\u201367.\nSean F Reardon and Ann Owens. 2014. 60 years after\nBrown: Trends and consequences of school segrega-\ntion. Annual Review of Sociology , 40:199\u2013218.\nJohn R. Rickford. 1985. Ethnicity as a sociolinguistic\nboundary. American Speech , 60(2):99\u2013125.\nJohn R. Rickford and Sharese King. 2016. Language\nand linguistics on trial: Hearing rachel jeantel (and\nother vernacular speakers) in the courtroom and be-\nyond. Language , 92(4):948\u2013988.\nJonathan Rosa and Nelson Flores. 2017. Unsettling\nrace and language: Toward a raciolinguistic perspec-\ntive. Language in Society , 46(5):621\u2013647.\nMaarten Sap, Swabha Swayamdipta, Laura Vianna,\nXuhui Zhou, Yejin Choi, and Noah A. Smith. 2021.\nAnnotators with attitudes: How annotator beliefs\nand identities bias toxic language detection. arXiv ,\nabs/2111.07997.\nDante J. Scala and Kenneth M. Johnson. 2017. Polit-\nical polarization along the rural-urban continuum?\nthe geography of the presidential vote, 2000\u20132016.\nThe ANNALS of the American Academy of Political\nand Social Science , 672(1):162\u2013184.\nAri Schlesinger, Kenton P. O\u2019Hara, and Alex S. Taylor.\n2018. Let\u2019s talk about race: Identity, chatbots, and\nAI. In Proceedings of CHI .\nAnissa Tanweer, Emily Kalah Gade, PM Krafft, and\nSarah K Dreier. 2021. Why the data revolution\nneeds qualitative thinking. Harvard Data Science\nReview .\nMichel-Rolph Trouillot. 1995. Silencing the past:\nPower and the production of history . Beacon Press.\nRob van der Goot, Alan Ramponi, Arkaitz Zubiaga,\nBarbara Plank, Benjamin Muller, I\u00f1aki San Vi-\ncente Roncal, Nikola Ljube\u0161i \u00b4c, \u00d6zlem \u00c7etino \u02d8glu,\nRahmad Mahendra, Talha \u00c7olako \u02d8glu, Timothy Bald-\nwin, Tommaso Caselli, and Wladimir Sidorenko.\n2021. MultiLexNorm: A shared task on multilin-\ngual lexical normalization. In Proceedings of the\nSeventh Workshop on Noisy User-generated Text .\nMarlon Vanegas, Juan Restrepo, Yurley Zapata, Gio-\nvany Rodr\u00edguez, Luis Cardona, and Cristian Mu\u00f1oz.\n2016. Linguistic discrimination in an English lan-\nguage teaching program: Voices of the invisible oth-\ners.\u00cdkala, Revista de Lenguaje y Cultura , 21.\nFred Vultee. 2012. A paleontology of style. Journal-\nism Practice , 6(4):450\u2013464.Claudia Wagner, David Garcia, Mohsen Jadidi, and\nMarkus Strohmaier. 2015. It\u2019s a man\u2019s Wikipedia?\nAssessing gender inequality in an online encyclope-\ndia. In Proceedings of the AAAI conference on web\nand social media .\nJason Weismueller, Paul Harrigan, Kristof Coussement,\nand Tina Tessitore. 2022. What makes people share\npolitical content on social media? The role of emo-\ntion, authority and ideology. Computers in Human\nBehavior , 129:107150.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In Proceedings of LREC .\nMatthew L Williams, Pete Burnap, and Luke Sloan.\n2017. Towards an ethical framework for publishing\nTwitter data in social research: Taking into account\nusers\u2019 views, online context and algorithmic estima-\ntion. Sociology , 51(6):1149\u20131168.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems , volume 32. Curran\nAssociates, Inc.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of ICCV .\nA Appendix\nA.1 Language Model Training Corpora\nWe display a list of popular language modeling\ncorpora in Table 5.\nA.2 Datasheet\nOur datasheet for the U.S. S CHOOL NEWS dataset\ncan be found here: https://bit.ly/3rLrmwV .\nA.3 Quality Filter Hyperparameters\nWe display the hyperparameters of our logistic re-\ngression classi\ufb01er (reproduction of the \ufb01lter devel-\noped by Brown et al. 2020) in Table 6.\nA.4 Example Articles\nWe display example articles and their quality scores\nin the U.S. S CHOOL NEWS dataset in Table 11.\nA.5 Topic Modeling\nSee the quality distribution among topics for 10K\nopinion pieces in Figure 5.\nA.6 Demographic Features\nWe display a table of features we use in our demo-\ngraphic regression model in Table 7.\nA.7 Additional Regressions\nHere we include regressions results from two mod-\nels with additional covariates.\nWe \ufb01rst consider race as a possible omitted vari-\nable, given the extent of school segregation in the\nU.S. (Reardon and Owens, 2014). NCES data pro-\nvides the distribution of students by race for each\nschool, using a particular set of racial categories,\nwhich comes with obvious limitations. Neverthe-\nless, we use the raw percentage scores provided\nas additional covariates in this model as a validity\ncheck. We exclude the Native and Paci\ufb01c Islander\ncategories, due to imbalanced data and geographic\nconcentration, as well as the white category, to\navoid a saturated model.\nAs shown in Table 8, the \ufb01ndings are nearly iden-\ntical to the results in the main paper, with the ex-\nception that home values are no longer signi\ufb01cant.\nThe only racial category that shows a signi\ufb01cant\neffect is Asian. However, we note a positive cor-\nrelation between percentage of Asian students and\nmedian home values (Pearson r=0.32,p <0.001),\nsuggesting that the variable for percentage of Asian\nstudents may be partially absorbing the effect of\nour measure of wealth.Table 9 shows the results for an alternate model\nwhich includes % GOP vote share in the 2016 elec-\ntion. Once again, the results are very similar to the\nresults in the main paper, although there is a strong\n(and signi\ufb01cant) negative association between GOP\nvote share and quality scores, whereas the measures\nof home values and percent rural are no longer sig-\nni\ufb01cant.\nThe results for this model exemplify the dif\ufb01-\nculty of working with highly correlated variables.\nGiven the strong association between GOP voters\nand rural areas, GOP vote share serves as an effec-\ntive proxy for other variables of interest. However,\nbecause the results of the 2016 Presidential election\nwere likely somewhat idiosyncratic, and because\nwe \ufb01nd wealth and geography to be a more plausi-\nble explanation for differences in student writing\nthan political preferences among their parents, we\nopt for the model without GOP vote share in the\nmain paper.\nA.8 Low Factuality News Considered High\nQuality\nWe display example low factuality news articles\nthat are assigned high quality scores by the GPT-3\nquality \ufb01lter in Table 12.\nA.9 TOEFL Exam Responses\nWe display the distribution of quality scores against\nprompts and essay scores in the TOEFL exam\ndataset in Figure 6. We display the prompts of\nthis dataset in Table 10.\nModel Pretraining Data Sources Citation\nELMo 1B Word benchmark (Peters et al., 2018)\nGPT-1 BookCorpus (Radford et al., 2018)\nGPT-2 WebText (Radford et al., 2019)\nBERT BookCorpus + Wikipedia (Devlin et al., 2019)\nRoBERTa BookCorpus + Wikipedia + CC-news + OpenWebText + Stories (Liu et al., 2019)\nXL-Net BookCorpus + Wikipedia + Giga5 + ClueWeb 2012-B + Common Crawl (Yang et al., 2019)\nALBERT BERT, RoBERTa, and XL-net\u2019s data sources (Lan et al., 2020)\nT5 Common Crawl (\ufb01ltered) (Raffel et al., 2020)\nXLM-R Common Crawl (\ufb01ltered) (Conneau et al., 2020)\nBART BookCorpus + Wikipedia (Lewis et al., 2020)\nGPT-3 Wikipedia + Books + WebText (expanded) + Common Crawl (\ufb01ltered) (Brown et al., 2020)\nELECTRA BookCorpus + Wikipedia + Giga5 + ClueWeb 2012-B + Common Crawl (Clark et al., 2020)\nMegatron-Turing NLG The Pile + Common Crawl (\ufb01ltered) + RealNews + Stories (Kharya and Alvi, 2021)\nSwitch-C Common Crawl (\ufb01ltered) (Fedus et al., 2021)\nGopher MassiveWeb + Books + Common Crawl (\ufb01ltered) + News + GitHub + Wikipedia (Rae et al., 2021)\nTable 5: Overview of recent language models and their training corpora. All studies tend to draw from the same\ncore data sources: Wikipedia, Books, News, or \ufb01ltered web dumps.\nComputing Infrastructure 56 Intel Xeon CPU Cores\nNumber of search trials 100\nSearch strategy uniform sampling\nBest validation F1 90.4\nHyperparameter Search space Best assignment\nregularization choice [L1, L2] L1\nC uniform-\ufb02oat [0, 1] 0.977778\nsolver 64 liblinear\ntol loguniform-\ufb02oat [10e-5, 10e-3] 0.000816\nngram range choice [\"1 2\", \"1 3\", \"2 3\"] \"1 2\"\nrandom state uniform-int [0, 100000] 44555\ntokenization whitespace whitespace\nvectorization hashing hashing\nremove stopwords choice [Yes, No] No\nTable 6: Hyperparameter search space and best assignments for our re-implementation of the GPT-3 quality \ufb01lter.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP(high quality)5:christmas dress holiday day thanksgiving dance prom halloween year wear\n2:school college year high senior seniors students class time classes\n0:food restaurant eat pizza menu chicken coffee meal foods cheese\n6:students school student teachers class high classes time schools teacher\n1:people just like life time don know day things ve\n7:movie film movies characters story character plot films marvel book\n3:album music song songs band lyrics sound listen like artists\n4:people women media world new social states gun country like\n9:game team players games season sports football teams play athletes\n8:trump president election vote political clinton country obama people donald\nFigure 5: Considering 10K opinion pieces in U.S. S CHOOL NEWS, we observe that the GPT-3 quality \ufb01lter prefers\ntopics that are more prevalent in Wikipedia or newswire.\nFeature Description Level Source\nIs Charter Is the school a charter school? School NCES database\nIs Private Is the school a private school? School NCES database\nIs Magnet Is the school a magnet school? School NCES database\n% Black Students % students who identify as Black School NCES database\n% Asian Students % students who identify as Asian School NCES database\n% Mixed Students % students who identify as Mixed race School NCES database\n% Hispanic Students % students who identify as Hispanic School NCES database\nStudent:Teacher Student-teacher ratio School NCES database\nSchool Size Total number of students School NCES database\nMedian Home Value Median home value ZIP code Census\n% Adults \u0015Bachelor Deg. % adults ( \u001525 years old) with at least a bachelor\u2019s degree ZIP code Census\n% Rural Percent of a county population living in a rural area County Census\n% 2016 GOP V ote Republican vote share in the 2016 presidential election County MIT Election Lab\nTable 7: Description of features we include in our demographic analyses.\nDependent variable: P(high quality )\nObservations: 968 schools\nFeature Coef\ufb01cient\nIntercept 0.134\n% Rural \u00000.073\u0003\u0003\u0003\n% Adults \u0015Bachelor Deg. 0.049\u0003\nlog2(Median Home Value) 0.007\nlog2(Number of students) 0.005\u0003\nlog2(Student:Teacher ratio) \u00000.008\nIs Public 0.020\u0003\nIs Magnet 0.013\nIs Charter 0.035\u0003\n% Asian Students 0.081\u0003\u0003\n% Mixed Students 0.051\n% Black Students \u00000.009\n% Hispanic Students \u00000.020\nR20.152\nadj.R20.142\nTable 8: Regression of the average P(high quality )of\na school in the U.S. S CHOOL NEWS dataset, on de-\nmographic variables. As in the main paper, larger\nschools in educated and urban areas of the U.S tend\nto be scored higher by the GPT-3 quality \ufb01lter. Asian\nis the only categorical race variable which shows a sig-\nni\ufb01cant association (using data and categories taken di-\nrectly from NCES). The association with home values\nis no longer signi\ufb01cant, plausibly explained by a cor-\nrelation between a higher proportion of Asian students\nand higher median home values. See \u00a7A.6 for more in-\nformation on these features.\u0003p < 0.05,\u0003\u0003p < 0.01,\n\u0003\u0003\u0003p <0.001.\nDependent variable: P(high quality )\nObservations: 968 schools\nFeature Coef\ufb01cient\nIntercept 0.248\u0003\u0003\n% Rural \u00000.021\n% Adults \u0015Bachelor Deg. 0.067\u0003\u0003\nlog2(Median Home Value) 0.003\nlog2(Number of students) 0.006\u0003\u0003\nlog2(Student:Teacher ratio) \u00000.007\nIs Public 0.017\u0003\nIs Magnet 0.009\nIs Charter 0.027\n% GOP vote share \u00000.114\u0003\u0003\u0003\nR20.164\nadj.R20.157\nTable 9: Regression of the average P(high quality )of\na school in the U.S. S CHOOL NEWS dataset, on demo-\ngraphic variables, including % 2016 GOP V ote. We ob-\nserve that including the political leaning of the county\ntends to wash out other variables, likely because parti-\nsan voting correlates heavily with other effects, like the\nurban/rural divide (Scala and Johnson, 2017). The only\nother covariates that stay signi\ufb01cant are school size,\nparental education, and public (as opposed to private)\nschools.\u0003p <0.05,\u0003\u0003p <0.01,\u0003\u0003\u0003p <0.001.\nlow medium high\nEssay Score0.00.20.40.60.8P(high quality)\nP7P6P1P2P8P3P5P4\nEssay Prompt0.00.20.40.60.81.0P(high quality)\nFigure 6: TOEFL exam score is weakly correlated\nwith quality score across prompts (Pearson correlation;\nr=0.12\u00060.05,p\u00190; top), but the essay prompt seems\nto be a much stronger indicator of quality scores than\nthe exam scores are (bottom).\nID Text P(high quality )\nP7 It is more important for students to understand ideas and concepts than it is for them to learn facts. 0.04\nP6 The best way to travel is in a group led by a tour guide. 0.05\nP1 It is better to have broad knowledge of many academic subjects than to specialize in one speci\ufb01c subject. 0.07\nP2 Young people enjoy life more than older people do. 0.08\nP8 Successful people try new things and take risks rather than only doing what they already know how to do well. 0.10\nP3 Young people nowadays do not give enough time to helping their communities. 0.16\nP5 In twenty years, there will be fewer cars in use than there are today. 0.22\nP4 Most advertisements make products seem much better than they really are. 0.74\nTable 10: TOEFL prompt IDs and their text, ordered by their quality score by GPT-3 quality \ufb01lter.\nCategory: Student-Life\nP(high quality) = 0.001\nAs our seniors count down their \ufb01nal days until graduation, we\nwill be featuring them each day. [REDACTED], what are your\nplans after graduation? To attend [REDACTED] in the fall and\nget my basics. Then attend the [REDACTED] program. What\nis your favorite high school memory? My crazy, obnoxious and\nsilly 5th hour English with [REDACTED]. What advice do you\nhave for underclassmen? Pay attention, stay awake (I suggest\nlots of coffee), and turn in your dang work! You can do it, keep\nyour head up because you are almost there!\nCategory: News\nP(high quality) = 0.99\nOn Monday, September 3rd, Colin Kaepernick, the American\nfootball star who started the \u201ctake a knee\u201d national anthem\nprotest against police brutality and racial inequality, was named\nthe new face of Nike\u2019s \u201cJust Do It\u201d 30th-anniversary campaign.\nShortly after, social media exploded with both positive and nega-\ntive feedback from people all over the United States. As football\nseason ramps back up, this advertisement and the message be-\nhind it keeps the NFL Anthem kneeling protest in the spotlight.\nTable 11: Examples of high school news paper articles from U.S. S CHOOL NEWS. Many of the articles in student-\nlife category, and similar, rated lower quality have very different styles from documents rated high quality.\nArticle from http://en-volve.com\nP(high quality) = 0.93\nThe German government has effectively began the process of\neliminating the unvaccinated by starving them to death by push-\ning grocery stories to ban unvaccinated residents from buying\nessential food items...The pressure on the unvaccinated grows\nand grows!...\nArticle from http://www.censored.news\nP(high quality) = 0.98\nThe provisional number of births in the U.S. was 3,605,201 in\n2020. That is the lowest number of births in the United States\nsince 1979, according to the Centers for Disease Control. 2020\nalso had the lowest fertility rate since the government started\ntracking births in 1902. And don\u2019t blame the so-called \u201cpan-\ndemic. \u201d...we\u2019re learning in 2021 that intelligent people succumb\nto government psy-ops. But critical thinkers understood imme-\ndiately that something was very wrong with all the COVID-19\nstuff. Plus many among the global elite continually and openly\ngloat about their desire to cull the masses. Bill Gates isn\u2019t even\ncoy about his desires...\nTable 12: Examples of news from low factuality sources (as identi\ufb01ed by MediaBiasFactCheck.com ) rated high\nquality by GPT-3 quality \ufb01lter, but contain COVID disinformation.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Whose language counts as high quality? Measuring language ideologies in text data selection", "author": ["S Gururangan", "D Card", "SK Dreier", "EK Gade"], "pub_year": "2022", "venue": "arXiv preprint arXiv \u2026", "abstract": "Language models increasingly rely on massive web dumps for diverse text data. However,  these sources are rife with undesirable content. As such, resources like Wikipedia, books,"}, "filled": false, "gsrank": 568, "pub_url": "https://arxiv.org/abs/2201.10474", "author_id": ["CJIKhNIAAAAJ", "qH-rJV8AAAAJ", "ZA3EQwUAAAAJ", "BirIJX8AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:NlR-tlDnHSIJ:scholar.google.com/&output=cite&scirp=567&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D560%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=NlR-tlDnHSIJ&ei=bLWsaO2lAY6IieoP0sKRuAk&json=", "num_citations": 35, "citedby_url": "/scholar?cites=2458375305459291190&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:NlR-tlDnHSIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2201.10474"}}, {"title": "Assessing learning activities based on the ACRL Framework for Information Literacy", "year": "2022", "pdf_data": "  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 1 of 27  \n______________________________________________________________________________  TITLE OF PROJECT:  Assessing learning activities based on the ACRL Framework for Information Literacy   PROJECT PARTICIPANTS  Principal applicant   Holly Hendrigan  Librarian for the Faculty of Applied Sciences  Faculty: Library  Department/School: Fraser Library, SFU Surrey  Phone: 778-782-8023  Email: hah1@sfu.ca  Collaborator(s)   Diana Cukierman  University Lecturer  School of Computing Science, Faculty of Applied Sciences  Co-applicant   Sheena Tan  PhD Student  Faculty of Education  Research assistant     \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 2 of 27  \nPART I \u2013 PROJECT FINDINGS   INTRODUCTION This report describes the information literacy units the research team developed for Diana Cukierman\u2019s CMPT 320 and CMPT 115 students in 2019 and 2021, respectively. Cukierman and Hendrigan, the PI, have been working on information literacy instruction and assessment based on the ACRL Framework for Information Literacy since 2017 (Hendrigan, Mukunda, and Cukierman, 2020). The report is organised as follows: We begin with background information on the ACRL Framework, the two Frames we focused on, and the knowledge practices and dispositions that we were looking for in students. We then describe and analyse the 2019 assignment, and answer the research questions we asked in the TLDG proposal. We then describe why we substantially changed the unit in 2021 based on what we learned from the 2019 assignment. We feel that the 2021 unit comes closer to guiding students to learn higher level information literacy concepts which will serve them well during the rest of their studies at SFU and post graduation.   This research aligns with one of the goals in SFU\u2019s proposed Educational Goals Initiative, which posits that students should be able to \u201cEvaluate and use source materials effectively and ethically to support and advance ongoing and new knowledge.\u201d To achieve this goal, we believe it is crucial for librarians and instructors to collaborate to develop robust information literacy pedagogies. SFU currently lacks a comprehensive information literacy strategy. We feel we have developed a unit that could be adopted by instructors in other disciplines, and marked by Teaching Assistants with some support.   The ACRL Framework for Information Literacy  structures information literacy as a set of interconnected core ideas rather than discrete skills. These core ideas, called Frames, are as follows:   \u25cf Authority Is Constructed and Contextual \u25cf Information Creation as a Process \u25cf Information Has Value \u25cf Research as Inquiry \u25cf Scholarship as Conversation \u25cf Searching as Strategic Exploration  We designed the assignment to focus on two of the six frames: Authority is Constructed and Contextual, and Searching as Strategic Exploration. We chose these frames because they address students\u2019 difficulties with finding appropriate sources. The Framework does not provide specific learning outcomes or lesson plans; instead, it allows educators to create curriculum according to the individual instructional need. However, the Framework briefly outlines proficiencies of the concepts, described as Knowledge Practices and Dispositions.  Knowledge practices are  \u201cdemonstrations of ways in which learners can increase their understanding of these information literacy concepts\u201d; dispositions \u201cdescribe ways in which to address the affective, attitudinal, or valuing dimension of learning.\u201d (Association of College and Research Libraries, 2015).  We examined student data in light of the following knowledge practices and dispositions of the two frames:   Selected Knowledge Practices of Authority is Constructed and Contextual frame \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 3 of 27  \n\u25cf define different types of authority, such as subject expertise (e.g., scholarship), societal position (e.g., public office or title), or special experience (e.g., participating in a historic event); \u25cf use research tools and indicators of authority to determine the credibility of sources;  \u25cf recognize that authoritative content may be packaged formally or informally and may include sources of all media types;  Selected Dispositions of Authority is Constructed and Contextual frame \u25cf develop awareness of the importance of assessing content with a sceptical stance and with a self-awareness of their own biases and worldview; \u25cf question traditional notions of granting authority and recognize the value of diverse ideas and worldviews;  Selected Knowledge Practices of Searching as Strategic Exploration frame \u25cf design and refine needs and search strategies as necessary, based on search results; \u25cf use different types of searching language (e.g., controlled vocabulary, keywords, natural language) appropriately;  Selected Dispositions of Searching as Strategic Exploration frame \u25cf understand that first attempts at searching do not always produce adequate result; \u25cf persist in the face of search challenges  2019 UNIT  For the 2019 unit, students in CMPT 320 were required to complete a reading, a workshop, a written assignment, and a research paper. Analysis of the research paper is beyond the scope of this research. Students were required to read an article about banning electronic devices in university classrooms (Renstrom, 2017) then assess the authority of the author (an instructor at Boston College) and the publication (Aeon, an online magazine.) We then asked students to find an article on the same topic in a subject database, Education Source Complete.   We had two primary learning outcomes: one for the Authority frame and another for the Searching frame. LO1A examined students\u2019 proficiencies in assessing authority; LO1B examined their assessment of a publication\u2019s reputation. LO2 examined students\u2019 proficiencies in searching a database (see Table 1.)  We also asked students an overall reflection question about what they learned, and extracted themes from this data.    Frame Learning Outcome Research Questions Authority is Constructed and Contextual LO1A: Students will describe different ways authors demonstrated authority. What markers of authority did students recognize?     What was the students\u2019 achievement of learning outcome LO1A? \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 4 of 27  \n LO1B: Students will apply assessment criteria to the authority of publications. How did the students evaluate the quality/ reputation of the different publications?    What was the students\u2019 achievement of learning outcome LO1B? (Assessing Authority of a Publication) Searching as Strategic Exploration LO2: Students will refine search strategies and use different search strategies. Students will persist in performing searches iteratively. What patterns emerged in students\u2019 search behaviour?     What patterns emerged from students who demonstrated deeper engagement with searching (variety of search terms or strategies)   Table 1: Frames, Learning Outcomes, and Research Questions  FINDINGS - 2019 DATA We conducted a qualitative content analysis on the written assignment to look for patterns in students\u2019 assessment criteria of authority of author and publication, and their search strategies for finding an article.   Learning Outcome 1A (Assessing Authority of Author) We asked the students two questions. The first question involved the author of the article they had read, from the online magazine Aeon. We asked, \u201cHow does Joelle Renstrom establish her authority?\u201d (Q5) The second question involved the author(s) of the scholarly article they were tasked to find. This question was, \u201cHow do(es) the author(s) of the scholarly article which you found establish their authority?\u201d (Q13). Figure 1 reveals the students\u2019 responses to the author of the magazine article (Q5);  Figure 2 reveals how students assessed the authority of the scholarly article (Q13).   \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 5 of 27  \n Figure 1. Categories and frequency of students\u2019 responses to Q5 - Renstrom\u2019s authority  \n Figure 2. Categories and frequency of students\u2019 responses to Q13 - Scholarly author\u2019s authority  \n\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 6 of 27  \nStudents primarily recognized the author\u2019s academic position as the main attribute of authority in both the magazine and scholarly article. They also noted the presence of the scientific method in each article to a lesser extent. The Aeon article provided more biographical information on the author than did any of the scholarly articles, which accounts for increased authority attributes assigned to Renstrom. When assessing the authority of the scholarly author, students more frequently mentioned attributes of the journal or the article than the author, such as the literature review, the peer review status, and the reputation of the journal.    Learning Outcome 1B (Assessing Authority of Publication) We asked students two questions about the reputation of the two different publications they read. First, \u201cDo you think Aeon is a reputable publisher? Why or why not?\u201d (Q7), and \u201cIs the journal where you found the scholarly article reputable? How do you know?\u201d (Q14) Again, the students\u2019 responses were coded into the following main categories, with their frequency. \n Figure 3. Categories and frequency of students\u2019 responses to Q7 - Reputation of Aeon  \n\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 7 of 27  \n Figure 4. Categories and frequency of students\u2019 responses to Q14 - Reputation of scholarly article  Notably, many students applied the assessment criteria of peer review status for both types of publications. Interestingly, although authors\u2019 authority is a common criteria across both questions, it was of less priority when assessing the authority of scholarly journals. This suggests that students apply different assessment criterias depending on the nature of the source.  Moreover, significantly more students searched for additional information on Aeon\u2019s website to evaluate its reputation (62) as compared to the scholarly journals (22). By contrast, more students used an external search to gather additional information on the scholarly journals (13) as compared to Aeon (6). Aeon provides more information about itself on its website than a scholarly journal typically does, which can possibly account for how the students investigated the source\u2019s reputation.    Learning Outcome 2 (Searching as Strategic Exploration) We asked students the questions, \u201cWhen you looked for this article in the Education Source database, which search terms did you try?\u201d (Q10) and \u201cWhen you looked for this article in the Education Source database, which search terms were most successful? \u201d (Q11) Students selected the search terms from a given list.  Their responses were tallied and compared between the two questions.  \n\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 8 of 27  \n Figure 5. Frequency of search terms selected by students  Students\u2019 responses were consistent across the two questions, with cell phones and cell phones in university being the top two choices of search terms and also top two most successful search terms. Other search terms that students tried and were successful include classroom, phone, university, cell phone, class, mobile phone, cellphone, addiction, and school. They are mostly alternative words, synonyms of the key words suggested to the students.   We also asked students to \u201cList any other strategies you used to find the best article.\u201d (Q12), where their responses were coded into the following main categories, with their frequency. \n\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 9 of 27  \n Figure 6. List of strategies students used  Overview of Themes from Reflection Finally, we asked students a reflection question: \u201cWhat did you learn from this exercise, and how might your approach to future research projects change?\u201d Their responses were coded into the following main categories, with their frequency. \n\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 10 of 27  \n Figure 7. Categories and frequency of students\u2019 reflections   On first glance, these categories reflect our two learning outcomes on assessing authority (LO1) and search strategies (LO2).  Students reported a higher awareness of investigating the authority of authors and publications:   \u201cI will try and find out about both an author and a Journal, before I use a source\u2026(7)  \u201cI also gained a better understanding of the \u201csignposts\u201d of a writer\u2019s authority and how to use them to evaluate research sources.\u201d (74)   Students also indicated that they learned that subject databases existed and the power or using different search strategies    \u201cI learnt that you can find great and reliable sources from the Using the Education Source database! There are various filters such as the range of publication dates, which can be very useful. In future research projects I will use this to find good and reliable sources. (32)  \u201cThe main thing I learned from this exercise is that services for querying research databases can have very sophisticated search parameters that help immensely when it comes to finding relevant, reputable sources of information. In the past, I have underused these resources when writing papers, and will try to take advantage of them when I approach future research projects, now that I have a better appreciation for how useful their advanced search filters can be\u2026\u201d (74) \n\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 11 of 27  \n However, with regard to LO1 and assessing authority, we noticed a discrepancy between what students say they learned about interrogating the authority of authors and what we noticed when we examined their responses. This reveals a gap in our unit where we did not instruct students on how to assess authors or publications. We asked them to stop and think about authority, but we feel we could have done a better job providing them with an evaluation strategy.     Student Response Assessment Pilot    The assignment was graded for completion only. The research team subsequently examined student responses as a first step to assess future iterations of the assignment. We created a pilot rubric consisting of four levels - exceeding, meeting, or approaching expectations of the learning outcome (coded EE, ME, AE respectively), and one level coded as missed learning outcome (MLO). We applied MLO when it was clear that the student had not understood the question.  We based our criteria of exceeding, meeting, and approaching expectations on the number of evaluation categories the student listed. For example, a student who mentioned 3-5 authority attributes would have exceeded expectations, whereas a student who mentioned only 2 would have simply met expectations. Based on this model, Table 2 shows students\u2019 performance for LO1.     # students who Exceeded Expectations of LO # students who Met Expectations of LO # students who Approached Expectations of LO # students who Missed Learning Outcome Q5 (Authority of Renstrom) 6 45 16 22 Q13 (Authority of scholarly article author)  3 36 38 10 Q7 (Reputation of Aeon) 4 41 27 17 Q14 (Reputation of scholarly journal)  8 21 35 23  Table 2. Student Achievement of LO1 (Assessing authority of author and publication)   Unpacking Issues with our Assessment Pilot and Pedagogy  1. Assessing authority of authors (Q5, Q13)   Q5: Assessing authority of Renstrom  Most students met or exceeded the LO for this question, and provided at least two markers of authority. Renstrom\u2019s article was more personal than a scholarly article, and provided ample authority criteria for students to choose from, such as her teaching experience and publication record. However, many (22)  students missed the LO, many of whom misunderstood the term \u201cauthority.\u201d In the question, we used the same vocabulary as the Frame (\u201cAuthority is constructed and contextual\u201d), \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 12 of 27  \nbut some students mistook cognitive authority for organizational authority. Student 49 is one example of this:   \u201cIn my view, Joelle Renstrom established her authority when she asked her students to silence their phones and lock them in a pouch. She was worried about a plethora of deleterious effects of phones. She did not want her students to blindly follow some rules. She wanted them to understand their phone habits..\u201d (49)   Q13: Assessing the authority of a scholarly journal author In this question, an equal number of students met/exceeded or approached expectations of the LO.  The high number of students (38) who Approached Expectations provided just one criteria. For example, student 34 responded \u201cBy mentioning that they are associated with the Psychology Department at the University of Ohio.\u201d This student was marked AE but this is an example where the sole criteria (academic affiliation) was an adequate response, and probably should have been assigned ME (met expectation.)   We learned that both our assessment methodology and assignment question needed work. First, we believe that we should have clarified our assessment criteria in the question itself.  If we require students to list more than one criteria in order to meet or exceed expectations, we should say so. We also feel it is important to distinguish students who achieved AE (Approaching Expectations) due to the number of criteria listed (i.e. only listing one criteria), from those who provided incorrect answers. For example, student 62 provided just one criteria, but we feel that the criteria was misguided. They said, \u201cThe authors established their voice in their scholarly article by constantly using first-person plural \u201cwe\u201d. The ownership make it more authoritative, which helps build trust with the readers e.g. \u201cwe measured rates of disengagement during lectures related to media use.\u201d\u201d This answer requires feedback from the instructor on cognitive authority.   2. Assessing authority of publications (Q7, Q14)   Q7: Assessing reputation of Aeon Most students had never heard of Aeon, so they could not draw on their own knowledge to answer the question whether the source was reputable or not. Yet, this is another question where we feel our pedagogy and assessment method is problematic. We realized that we did not provide students with a strategy on how to evaluate publications, and we needed to clarify our expectations.   There was an even split between those who met or exceeded the LO, and those who did not. Some students were assigned  MLO by evading the answer. We had asked why Aeon was or was not a reputable publisher and one student responded :    \u201cI do not think Aeon is a reputable publisher. As it is just a digital magazine even though its mission is to publish some of the most profound and provocative thinking on the web, I cannot really tell every article is reliable enough.\u201d (83)   Here is an example of a student who provides more than one criteria, but some of the criteria is weak:   \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 13 of 27  \n\u201cI think Aeon is a reputable publisher because they don't have ads, publishes only one essay every weekday which means content that is chosen on the day of publishing is chosen carefully and article titles are simple and not trying to grab attention. (52)   62 students mentioned that they consulted Aeon\u2019s \u201cAbout\u201d page, and only 6 did an external search to confirm or verify what the publication said about itself. This provides evidence that, without instruction otherwise, most students check the source itself as an evaluation strategy. We provide an evaluation strategy in the 2021 assignment.     Q14: Assessing the reputation of scholarly journal In Q14, we noted a high number of students who attained MLO (23) or AE (35). We believe that this should cause the research team to reconsider our pedagogy and assessment method. In many cases, students were assigned the MLO because they assessed the article rather than the  journal. When asked if the journal was reputable, one said \u201cYes, because this article made a scientific experiment based on the arguments put forward by itself, and made a perfect analysis of the results.\u201d (63). This is another example of a student evaluating a source by reading the source itself rather than checking with external sources.   As in Q5 and Q13, we assigned AE (Approached Expectations) when students provided just one criteria: for example, the journal was reputable because \u201cits first publication dates back to 1992\u201d (82)  Interestingly, some students also associated the authority of the publication with those who published there: \u201cIt looks reputable to me because author of articles are academic instructors.\u201d (73)   We also would like to call attention to the fact that more students used an external search (ie, Google) to gather additional information on the scholarly journals (13) than they did when they investigated Aeon (6). This might be because Aeon provides more information about itself on its website, whereas students who found articles in a subject database often needed to search the journals\u2019 website separately.   Assessing search skills (LO2) On examining the data, we found ourselves unable to develop a method to assess students\u2019 mastery of the Searching as Strategic Exploration frame. We feel that the question we asked (\u201cwhich terms did you use?\u201d) did not provide us with a good indication of student learning on search strategies. This question could also be construed as a leading question, since we used radio buttons. In addition, only 10 students articulated their persistence in searching for the most appropriate article.   Conclusion: 2019 Assignment  We recognized problems with both our pedagogy and our potential assessment approach for each of the LOs. Our original intent was to develop a strategy for assessment of the two frames Authority is Constructed and Contextual and Searching as Strategic Exploration; we had hoped that the data itself would inform our approach. However, when we examined the data, we learned that we also needed to change our pedagogy. Once we improved our pedagogy, we could then develop a reasonable and fair assessment strategy.    \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 14 of 27  \nThe main problem with our pedagogy is that we did not provide students with an evaluation strategy for authors and publications; we simply asked them to do it. Thus, this assignment functions more as a pre-test that gauges students\u2019 comprehension before explicit instruction. We realized that explicit instruction on evaluating sources was required.   Secondly, we have reservations about applying a quantitative approach to assess understanding of a concept. Should a student who provided one good criteria in their response necessarily receive a lower mark than a student who provided three or four criteria? Caulfield (2021) cautions against complicated strategies for evaluating resources. We realize we cannot apply a cookie-cutter approach to assessing students\u2019 evaluation strategies. Some sources may require more consideration than others. Some students may have pre-existing knowledge of a source so they do not need to check its reputation elsewhere; others may not.  We should remind students of the following mantra: \u201cThe greatest enemy of fact checking is hubris.\u201d (Wineburg and McGrew, 2017, p. 45) and provide students with a strategy to quickly find the best sources.   We were also dissatisfied with how we assessed students\u2019 achievement of their search processes (LO2). We felt we asked leading questions on the wrong platform. Furthermore, only 10 students articulated their persistence when searching for the most appropriate article. We learned that we should, instead, focus our assessment on students\u2019 research and evaluation processes. We decided a better approach would entail an open ended reflection question with a minimum word count. This allows students to better explain their processes, which in turn provides the research team with sufficient data to understand and assess their responses.   Despite the shortcomings with our pedagogy for the 2019 assignment iteration, we feel that the students and the research team still benefited from this assignment. It required students to stop and think about their research strategies and how they evaluate sources. Metacognition is, after all, an important facet of the Framework for Information Literacy. We also learned that we needed to provide more guidance on search strategies and evaluating sources, and clarify on our own assessment methods. Finally, the 2019 responses provide us with a control group dataset on evaluating sources that we can compare with our 2021 unit.    CHANGES MADE BASED ON 2019 DATA In 2021, the course instructor was not assigned to teach CMPT 320 and had developed a new course, CMPT 115 - Exploring Computer Science. This course shares learning objectives with CMPT 320, but it is targeted at students who are not CS majors and do not have any exposure to CS courses previously. Many students in both courses were advanced in their studies. We continued using the ACRL Framework and focused on the same frames: Authority is Constructed and Contextual and Searching as Strategic Exploration.  Similar to previous years, the information literacy unit is aligned with the course content. In this course, students study algorithms and how they affect our daily lives. We decided on a section from Noah Yuval Harari\u2019s book 21 lessons for the 21st century (Harari, 2018). In this passage, he makes provocative statements on the impact of Google and how algorithms and big data increasingly influence our major decisions.   \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 15 of 27  \nThe changes we made in 2021 are as follows. First, we switched platforms, from SurveyMonkey to Canvas. We found the survey format to be a flawed data collection method for learning about students\u2019 search processes, for we had asked these questions using closed/pre-defined survey questions. The survey approach made us vulnerable to criticism of having asked leading questions. We decided instead to ask open questions about their search strategy, and provide students with a higher word count requirement to fully articulate their process. Finally, we developed and applied an assessment rubric using Canvas\u2019 SpeedGrader, where students were manually marked by a TA in consultation with the research team. In previous years, the information literacy assignments were graded for completion only.  We also made substantial changes to the assignment\u2019s design. In 2019, we asked students to assess the authority of a single author and single publication, then directed them to a particular subject database. This approach, however, does not mimic a research assignment where students make their own decisions on which search tools and sources to use. Therefore, we decided to assign a research situation that was more naturalistic. Through literature searches, the PI discovered two curricula that could be adapted to suit the revised design: Lane Wilkinson's search strategy, which he calls Information Needs, Types, and Qualities (Wilkinson, 2016) and Mike Caulfield\u2019s evaluation strategy, called SIFT (Caulfield, 2019.)   Wilkinson (2017) recommends a structured approach for students\u2019 literature searches that is intended to overcome both information overload and misinformation. This approach is especially useful for students who are at the beginning of their research journey who might otherwise become overwhelmed by the sheer volume of search tools available to them, and the millions of documents their searches might retrieve. Instead, he provides a framework that he calls the \u201c5 needs\u201d approach. These information needs are: background information, current events, statistics, opinions, and research. Students are tasked with creating a 5-item bibliography of sources, each resource that satisfies a specific \u201cinformation need.\u201d    In 2019, we simply told students to assess the authority of the authors and the publications. We did not, however, provide any guidance on evaluating sources. To address this, we introduced the students to Mike Caulfield\u2019s SIFT method. SIFT stands for Stop, Investigate the Source, Find Better Coverage, and Trace Claims to the Original Source. It is based on earlier research from Wineburg & McGrew (2017) which demonstrates that professional fact checkers, who employ a technique of lateral reading, are faster and more effective web content evaluators than those who employ a strategy of vertical or close reading. Lateral reading is a strategy where researchers leave the site or resource in question and query the Web for authoritative resources about the original source. Vertical reading, by contrast, is where researchers remain on the site and base their evaluation on the site\u2019s content itself: the About page, the number of ads, whether the URL includes .com or .gov, etc. Caulfield has established the SIFT technique and its lateral reading strategies with a wide array of open resources including a textbook and video series (Caulfield, 2017, CTRL-F, 2019) .   To introduce the SIFT and 5 Needs content, we also added a \u201cflipped instruction\u201d component to the unit. We created a separate Canvas course of 6 modules: a module covering the SIFT method videos, and a module for each of the 5 needs (background, current events, opinion, statistics, and research). After students completed the online course, Hendrigan visited the class and presented \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 16 of 27  \nhow to apply the SIFT method, and demonstrated methods to find various sources by information needs.   We combined SIFT and \u201c5 Information Needs\u201d in an assignment that included a literature search and an open ended reflection passage on their process. Students were first tasked with finding 5 different types of sources on their chosen topic. They were then required to write a 200-500 word paragraph describing how they found one or two of their sources from the bibliography. This was our main data source for both qualitative analysis as well as assessment. We prompted them with the following questions:   \u25cf Which database(s) did you consult and which terms did you use? \u25cf How did your search evolve (in other words, what sources/tools were effective and which ones weren\u2019t? It\u2019s important to document your failures too!) \u25cf What did you learn about the sources when you employed the SIFT method? Was the SIFT method useful? \u25cf Why do you feel that you can trust the sources that you chose? \u25cf Did you find this exercise useful? If so, why?  Summary of 2021 Unit To summarise, the 2021 information literacy unit contains multiple assignments:   1. Assignment 1 required the students to read a chapter in Yuval Noah Harari\u2019s book 21 lessons for the 21st century.   2. Assignment 2 required students to complete Canvas tutorials on evaluating information (the SIFT technique) and how to find information based on Wilkinson\u2019s 5 needs categories.  3. Assignment 3 required students to complete a bibliography of sources based on the Harari reading. Specifically, students conducted literature searches and compiled a bibliography related to one of Harari\u2019s following claims: i. Google has diminished our ability to search for information  ii. Google's top search results are \"the truth\"  iii. AI will eventually make better major life decisions (eg who you marry, what you study) than you would have made yourself  iv. Big Data algorithms: they have lots of glitches, but we have no better alternative.  4. Assignment 4 required students to write a 200-500 word reflective passage on their research process with the following guiding questions:   FINDINGS - 2021 DATA We conducted a qualitative content analysis to students\u2019 bibliography and reflective passage to look for patterns in students\u2019 research processes and strategies, in relation to the two ACRL frameworks, Authority is Constructed and Contextual and Searching as Strategic Exploration. We also studied how students used the SIFT method in assessing authority.   SIFT (Stop, Investigate the source, Find better coverage, Trace claims to original source) Most students (84.2%) in the study population stopped and investigated the source. To investigate the source, many students checked on the reputation of the authors, publications (including information about their history, reach, affiliations, etc) and publishers.  \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 17 of 27  \n \u201cI then looked at the writers and did some research on them before reading the document.\u201d (3)  \u201cThe Wall street Journal was the first source that came up. I looked into it and the website looked very professional. They had many articles on relevant topics. I went to go search it up on Wikipedia and saw that it was founded a long time ago, and had a very high number of print circulation as it is one of the largest newspapers in the US.\u201d (6)  \u201cI used the sift method and tried googling about the background of the website and would feel like it's lacking the support such as when the website was first started and its networks and its accomplishments.  At first, I would go through websites and then try to find general information about the company then found a magazine that had supporting points for my topic. When I did background searches for the magazine brand it showed to me that this magazine has been around for a long time and has a decent net worth it also has covered many topics and looked very professional.\u201d (12)  Some students also stopped and investigated the source by checking their impact scores or citation counts.   \u201cUpon finding an article from the results I checked the credibility of the journal and it's impact score.\u201d (11)  Fewer students (47.4%) went further to find better coverage or trace claims to the original source. Among those who tried, some were able to find the original source while others were not successful.   \u201cAs I read the article, I deduced that it was taken from another source, which I traced back to BNN Bloomberg. Again, I did an open web search on both the publisher (using Wikipedia and a website I am familiar with, mediabiasfactcheck.com) to determine that the publisher was reputable. Additionally, I open web searched for the author of the article, and found that she was a reputable and knowledgable journalist who has extensively written about the intersection between technology, privacy, and society, ...\u201d (7)  \u201cI found a very interesting article about people at facebook reworking their algorythums to stop filter bubbles. It was initially published in the toronto star. As I made the citation for it I realized there were multiple authors listed. So I googled them. And in doing so found that the article was orginaly published by the NY Times, with a completely different title.\u201d (1)  \u201cI felt that was interesting and want to find the original 2020 Google Survey to learn more; this is one method I learned from SIFT (trace back to original source). Unfortunately, I was unable to find the original survey.\u201d (4)  Overall, many of the student population seem to find the SIFT method useful in their research process. They reflected being more careful in trusting their sources and being better able to access the credibility of the sources. Moreover, they enjoyed the process of using SIFT and were likely to continue to use it in future.  \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 18 of 27  \n \u201cAll the SIFT methods proved useful. Investigating my sources made me more cautious of the sources I'm using, being able to eliminate questionable ones even if they make claims relevant to the topic. STOP made me think through how I can better improve my search and made me consider narrowing or broadening my search. Finally, finding better coverage meant I viewed various sources and compared their claims, and think about their biases and possible agendas.\u201d (6)   \u201cAs I mentioned earlier, I think SIFT exercises will be of great help to my academic career. SIFT method allows me to establish an effective reference source base before writing essays, and I have practiced how to check the credibility of a source. I think this exercise can prepare me well for future writing courses.\u201d (5)  \u201cIt was also quite fascinating, doing a more thorough look at my sources with the SIFT methods. In previous courses, I only went in-depth when it was a source I didn't immediately trust, but I'm glad it's a skill I have now. It's really useful, being able to check right away if something is peer-reviewed or being able to track down a biography for an article's author. While most of my sources I'd picked straight from their original publication, those where I didn't--my News and Research pieces--brought me a sort of excitement, tracing the claim back to their original publications.\u201d (4)  Authority Is Constructed and Contextual Frame We assessed this frame through their reflections following the SIFT method. The SIFT method requires students to do research on the publication and the author, away from the document in question itself. In most cases, they learned about the source or the author from Wikipedia.   Students were able to use a range of indicators of authority to determine how credible the sources were (see Figure 8). Some of the common indicators used include the reputation of the publication and/or publisher, authority of the authors and whether it is peer-reviewed or not. They were also able to use tools such as a fact-checking website, citation counts or impact scores and whether it was sourced from the library website or not. We also noticed students evaluating the credibility of the sources based on their familiarity with the source.  \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 19 of 27  \n Figure 8. Indicators used to assess credibility of sources  The most prevalent indicator used by many students (73.7%) was the reputation of the publication or publisher which includes its accolades, history, scale and affiliations.   \u201cI investigated this company and found that it was a well-known magazine company founded in 1994 that had won many awards for its journalism.\u201d (2)  \u201cThe Wall street Journal was the first source that came up. I looked into it and the website looked very professional. They had many articles on relevant topics. I went to go search it up on Wikipedia and saw that it was founded a long time ago, and had a very high number of print circulation as it is one of the largest newspapers in the US. This led me to believe that it was a reliable source, which is how I knew that I could trust it.\u201d (6)  More often than not, students were observed to use multiple indicators to assess and cross check the credibility of the sources.   \u201cI searched for the magazine on Wikipedia and determined that it is a respected monthly American business magazine that was named magazine of the year in 2014 by the American Society of Magazine Editors. Mark Sullivan the author is an award-win[n]ing journalist who covers topics such as emerging technology, large tech companies and misinformation among others. After this search, I was confident that the article was from a credible source.\u201d (10)  \u201cI used Access Science to find the background source, which was a recommended Wikipedia alternative in the information literacy module. Additionally, Access Science is run by McGraw \n\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 20 of 27  \nHill, a popular education provider that I have used before. To further support the credibility, the two authors, Dr. Gupta and Dr. Mata-Toledo are experienced in the field of computer science working and teaching.\u201d (3)  Students also seem to have developed greater awareness of the need to assess the credibility of the sources before using them and assessing them with a more critical stance.   \u201cThis exercise was useful because it gave me a reminder about the importance of verifying claims and not blindly agreeing with text. It was a good way to hone my skills to ensure that I could discern credible sources from non-credible ones.\u201d (7)  \u201cAlthough I know about the NYT [New York Times], I still did my research into its credibility. I learnt that many times, criticism has been raised towards the paper when covering political issues for misrepresentation, but, I didn't find much in the way of normal reporting.\u201d (5)  Student #5 demonstrates a nuanced understanding of a publication as a non binary good/bad and also indicates they are using their own judgement and reading critically.    Searching as Strategic Exploration Frame  We qualitatively analyzed the \u201cNotes\u201d field of the assignment worksheet and the reflection passage to assess students\u2019 understanding of the Searching as Strategic Exploration frame. This frame distinguishes novice from expert searchers in terms of the number of resources and strategies employed to satisfy the search need and their willingness to persevere when their initial search(es) fail. Search processes are iterative, as researchers change their search terms and strategies based on their results.  Students\u2019 reflection passages about their search strategies indicated an ability to use different resources and strategies to access and gather relevant sources to meet their information needs. The most common search tool that students used was Google, followed by the library catalogue, subject databases and Google Scholar (Refer to Figure 9). Although Google was still the top choice among students, it was often complemented with the use of other resources. Only 13.2% of the students used solely Google in their search process.  \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 21 of 27  \n Figure 9: Categories and frequency of search resources used  Students also deployed different search strategies and used them appropriately. It was evident from their reflections that the choice of keywords played a significant role in their search process. Some students also used filters, boolean operators and quotation marks as part of their search strategies.  \n\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 22 of 27  \n Figure 10: Categories and frequency of search strategies used  Students also demonstrated search perseverance by refining their search strategies when necessary. 78.9% of the students acknowledged their failed search attempts and 63.2% shared how they refined their search strategies. Some of the refinement strategies include adapting the terms used in the search, use of additional filters or using alternative resources.   \u201cFor my background information, I tried searching on Google and found web pages from Google which explained how they operate. I felt this was not sufficient, and looked through the Google Wikipedia page, which also proved unsuccessful due to a large amount of info and citations listed and was simply not effective. At this point, I knew I should broaden from learning more about Google itself, but rather a more general topic on search engines. After that, I searched through several online encyclopedias. Some of them were recommendations from the library research guide, including Gale Virtual Reference Library, and Ethical and Social Issues in the Information Age. However, the time consumed as I flipped through pages of irrelevant information got me nowhere. Finally, I found an article on AccessScience with the keyword \"search engine OR algorithm\" \u2026.\u201d (6)  \u201cInitially, I had trouble finding a specific article that was relevant, and thus used other filters such as time range to narrow down my results.\u201d (7)   \n\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 23 of 27  \nSentiments  Overall, students found that the 2021 information literacy unit provided them with useful search and evaluation strategies.   \u201cThis exercise was very useful and honestly, I wish I was taught it far in advance of my third year in university... It would have made much of my previous academic endeavors easier.\u201d (21)   \u201cOverall I would say I found this exercise helpful, it helped me get a feel for other databases that the library catalogue offers. I've done quite a lot of research papers for other courses but I only used the business and political science database so this exercise was great to helping me get a feel for the other resources the library has to offer.\u201d (15)   \u201cIt was also quite fascinating, doing a more thorough look at my sources with the SIFT methods. In previous courses, I only went in-depth when it was a source I didn't immediately trust, but I'm glad it's a skill I have now. It's really useful, being able to check right away if something is peer-reviewed or being able to track down a biography for an article's author. While most of my sources I'd picked straight from their original publication, those where I didn't--my News and Research pieces--brought me a sort of excitement, tracing the claim back to their original publications. I really didn't mind the extra digging!\u201d (4)   \u201cLooking at multiple articles on different websites helped me determine what was most credible due to the similarities in research studies or quotes. also using SIFT I found that it was very easy to determine which articles would be the most credible and have the most relevance to what I was looking for. SIFT is very useful as you can evaluate the information on a deeper level and determine if it is relevant information. I found that evaluating these articles on a deeper level made me trust the information more, especially after seeing the relationships between different articles. Ultimately I believe this exercise was a great way to learn to use techniques that will help determine the credibility in articles used for future research, so yes this was a great exercise.\u201d (18)   Conclusion: 2021 unit The research team continues to refine our methods to teach and learn Framework concepts. We are writing an article on our 2021 unit where we will fully describe our findings and their implications. We plan to deliver the unit in the Summer 2022 term and are considering adding a formal survey to gather quantitative data on students\u2019 reactions to the unit. We are also interested in exploring these search and evaluation strategies with students who are using the Dialectical Map (DMap) tool for argumentative essays.   REFERENCES   Association of College & Research Libraries. (2015). Framework for Information Literacy for Higher Education. Association of College & Research Libraries (ACRL). http://www.ala.org/acrl/standards/ilframework  \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 24 of 27  \nCaulfield, M. (2021). Information Literacy for Mortals (PIL Provocation Series). https://projectinfolit.org/pubs/provocation-series/essays/information-literacy-for-mortals.html  Caulfield, M. (2019). SIFT (The Four Moves). Hapgood. https://hapgood.us/2019/06/19/sift-the-four-moves/  Caulfield, M. (2017). Web Literacy for Student Fact Checkers. Self-published. https://webliteracy.pressbooks.com/front-matter/web-strategies-for-student-fact-checkers/  CTRL-F. (2018). Online Verification Skills \u2014 Video 1: Introductory Video. https://www.youtube.com/watch?v=yBU2sDlUbp8  Harari, Y. N. (2018). 21 lessons for the 21st century. McClelland & Stewart.  Hendrigan, H., Mukunda, K., & Cukierman, D. (2020). Are They There Yet? Determining Student Mastery of Learning Outcomes Based on the ACRL Framework. In The Information Literacy Framework: Case Studies of Successful Implementation (pp. 33\u201347). Rowman & Littlefield International. http://summit.sfu.ca/item/19727  Pavlounis, D., Johnston, J., Brodsky, D., & Brooks, P. (2021). The Digital Media Literacy Gap: How to build widespread resilience to false and misleading information using  evidence-based classroom tools. CIVIX Canada. https://ctrl-f.ca/en/wp-content/uploads/2021/11/The-Digital-Media-Literacy-Gap-Nov-7.pdf  Renstrom, J. (2017). What happened when I made my students turn off their phones. Aeon. Retrieved December 20, 2021, https://aeon.co/ideas/what-happened-when-i-made-my-students-turn-off-their-phones  Wilkinson, L. (2016, January 6). Information Needs, Types, and Qualities. Community of Online Research Assignments. https://www.projectcora.org/assignment/information-needs-types-and-qualities  Wilkinson, L. (2017). LOEX 2017: Teaching Popular Source Evaluation in an Era of Fake News, Post-Truth, and Confirmation Bias. Sense & Reference. https://senseandreference.wordpress.com/2017/06/02/loex2017/  Wineburg, S., & McGrew, S. (2017). Lateral Reading: Reading Less and Learning More When Evaluating Digital Information (SSRN Scholarly Paper ID 3048994). Social Science Research Network. https://doi.org/10.2139/ssrn.3048994      \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 25 of 27  \nPART II \u2013 CHANGES AND IMPLICATIONS   1. Changes to the project plan.   When I (the Principal Investigator, Holly Hendrigan,) proposed this research study, I thought the data would reveal where we should make minor adjustments to the assignment. I did not anticipate that we would completely redesign the unit.   2. Additional funding.   No   3. Changes in our teaching.  Holly Hendrigan: As Librarian for the Faculty of Applied Sciences, I work mainly with the 100-level W courses in FAS. Students in these courses are required to write a technical and an argumentative paper that incorporates social and/or environmental aspects of technology. In previous years, I focused on how to search for background information, books, and scholarly articles. I now see the importance of providing the SIFT assessment strategy, as well as finding and news sources, opinions,  and statistics. This is especially relevant for in their early undergraduate years, who lack domain knowledge.  As a result of this project,  I now offer a simplified version of the 2021 approach (SIFT + 5 Needs) to CMPT 105W, CMPT 376, ENSC 105W, MSE 101W, and SEE 110W classes. Before my class visit,  students complete online modules of the \u201c5 content needs\u201d  as well as watch videos of the SIFT technique. In class, I reinforce these evaluation and search strategies. While I believe that students benefit from the bibliography and reflection exercise we implemented in CMPT 115 as well, this simplified version is adaptable for instructors and librarians in other courses who are interested in incorporating concepts from the ACRL Framework in their learning goals.   Diana Cukierman: As the course instructor where the information literacy units were implemented, I have personally learned about the ACRL Framework, the SIFT evaluation method, and the 5 Information Needs search scaffolding technique. Most importantly, the courses that I taught, and in particular CMPT 115, a course that I newly designed, was enriched with a unit which fits perfectly with the course objectives, including providing strategies to students which they can apply to process the overabundance of information now available to them. This project, and the close collaboration with an expert librarian and researcher, allowed me to develop a strong unit providing exactly such guided strategies. Further, as a result of the project, we had the possibility to deeply analyse the responses from students. This allowed us to better understand how students can learn effective research strategies. We also appreciate having developed an assessment rubric that can be applied by a TA  in future courses.   4. Learning from the unexpected.  \u25cf The 2019 data can be considered a control group and used as a comparator for how students assess authority without the SIFT method.  \u25cb Students in the 2019 CMPT 320 class use the same approach as high school students, the study population in this research: https://ctrl-f.ca/en/wp-\n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 26 of 27  \ncontent/uploads/2021/11/The-Digital-Media-Literacy-Gap.pdf . Both groups used vertical reading techniques when evaluating sources.   \u25cf We have been working on teaching and assessing ACRL Framework concepts since 2017; students in the 2021 iteration provided the most positive feedback that we have ever received.  Several students indicated that they wished that they had been provided this instruction earlier in their careers. They appreciated a concrete methodology to improve their research and evaluation strategies, and not just abstract ideas about what information literacy may be. Many students clearly reflected that the methodology provided (SIFT method, the 5 needs, the exercise applying such) was useful for them.   5. Influence of the project on the teaching of others.  \u25cf The new CMPT 115 course has the module incorporated into the course design  6. Other influences, links, outcomes or \u201cspin-offs\u201d.  \u25cf Holly Hendrigan has joined the Dialectical Map research group. I am hoping to investigate the extent to which the SIFT + 5 Needs method can be adopted in classes where students write argumentative essays and construct DMaps.  \u25cf Diana Cukierman is planning to continue offering this information literacy unit for CMPT 115 and CMPT 320, and possibly connected with DMaps facilitating argumentative essays. \u25cf Some graduate students (in particular a student supervised by Diana) have also been offered these materials to support them in their research. Further conversations will hopefully clarify the usefulness of these strategies for graduate students research.  PART III \u2013 KNOWLEDGE SHARING  1. Sharing findings with my/our colleagues.   \u25cf Holly Hendrigan: I shared preliminary findings with library colleagues on July 15, 2021. This was during a meeting of SFU Library\u2019s Instruction Interest Group, a group of librarians whose portfolios include information literacy instruction.   2. Publications and conference presentations already done.   \u25cf Hendrigan, H. (2021, May 27) \u201cSIFT + 5 Needs, or why can\u2019t I leave well enough alone?\u201d [Lightning talk].  BCIT Library's 6th Annual Non-Con event.   3. Future knowledge sharing plans.   \u25cf Preparing manuscript of research paper, to be submitted to an Education journal \u25cf Will consider submitting proposals for future conferences as appropriate, such as SIGCSE and ITICSE (recognized international conferences in the Computing Science Education community)  \n  INSTITUTE FOR THE STUDY OF TEACHING AND LEARNING IN THE DISCIPLINES Teaching and Learning Development Grants Program, www.sfu.ca/istld  Final Report   \n The TLDG program is funded, administered, and facilitated by the Institute for the Study of Teaching and Learning in the Disciplines (ISTLD)  Page 27 of 27  \n\u25cf Will re-purpose Part 1 of this report for Summit, SFU\u2019s institutional repository. Summit is indexed by Google Scholar, which will increase its discoverability.  \u25cf CEE opportunities for sharing instruction techniques have been limited due to the sudden pivot to online learning during the COVID-19 pandemic. However, Holly will keep an eye out for such opportunities in the future.   PART IV \u2013 KEYWORDS FOR PROJECT AND STUDENTS INVOLVED  1. Keyword description of project.   Information Literacy, ACRL Framework, Librarian-Faculty collaboration, Research Skills, Assessment, Action Research, Undergraduates, Multidisciplinary Approaches   2. Students involved.  Course number & title Semester (Spring 2016, Summer 2020, etc.) Approximate number of students CMPT 320 Summer 2019 122 CMPT 115 Summer 2021 35 CMPT 115 Fall 2021 30  ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Assessing learning activities based on the ACRL Framework for Information Literacy", "author": ["H Hendrigan", "D Cukierman", "S Tan"], "pub_year": "2022", "venue": "NA", "abstract": "This report describes the information literacy units the research team developed for Diana  Cukierman\u2019s CMPT 320 and CMPT 115 students in 2019 and 2021, respectively. Cukierman"}, "filled": false, "gsrank": 571, "pub_url": "https://summit.sfu.ca/item/36519", "author_id": ["", "", ""], "url_scholarbib": "/scholar?hl=en&q=info:wYwmU1XDLoYJ:scholar.google.com/&output=cite&scirp=570&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D570%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=wYwmU1XDLoYJ&ei=brWsaODUBsDZieoPqdqh8QU&json=", "num_citations": 1, "citedby_url": "/scholar?cites=9668880221245705409&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:wYwmU1XDLoYJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://summit.sfu.ca/_flysystem/fedora/2023-09/G0417_Hendrigan_FinalReport.pdf"}}, {"title": "Detecting Stance on Covid-19 Vaccine in a Polarized Media", "year": "2021", "pdf_data": "City Univ ersity of New Y ork (CUN Y) City Univ ersity of New Y ork (CUN Y) \nCUN Y Academic W orks CUN Y Academic W orks \nDisser tations, Theses, and Capst one Pr ojects CUN Y Graduate Center \n9-2021 \nDetecting Stance on Co vid-19 V accine in a P olariz ed Media Detecting Stance on Co vid-19 V accine in a P olariz ed Media \nRodica Ceslo v \nCUN Y Graduate Center \nHow does access t o this work benefit y ou? Let us know! \nMore information about this work at: https:/ /academicworks.cuny .edu/gc_etds/4616 \nDisco ver additional works at: https:/ /academicworks.cuny .edu \nThis work is made publicly a vailable b y the City Univ ersity of New Y ork (CUN Y). \nContact: AcademicW orks@cuny .edu \n              DETECTING STANCE ON COVID-19 VACCINE IN A POLARIZED MEDIA  by   RODICA CESLOV               A master\u2019s capstone project submitted to the Graduate Faculty in Data Analysis and Visualization in partial fulfillment of the requirements for the degree of Master of Science, The City University of New York   2021   \n  ii                                                 \u00a9 2021  RODICA CESLOV  All Rights Reserved   \n  iii  Detecting Stance on Covid-19 Vaccine in a Polarized Media  by Rodica Ceslov     This manuscript has been read and accepted by the Graduate Faculty in Data Analysis and Visualization Studies in satisfaction with the thesis requirement for the degree of Master of Science.          Date  Sos Agaian Capstone Advisor Distinguished Professor Date  Matthew Gold Executive Officer              THE CITY UNIVERSITY OF NEW YORK  \n  iv  ABSTRACT   Detecting Stance on Covid-19 Vaccine in a Polarized Media   by Rodica Ceslov   Advisor: Sos Agaian, Distinguished Professor  The growing polarization in the United States has been widely reported. There are some benefits to individuals and society from political polarization and conflict between opposing viewpoints but recent research has primarily highlighted the negative consequences of polarization which reached an all-time high. Media coverage plays an important role in shaping public opinion and influences public debates on complex and unfamiliar topics. One such topic is the Covid-19 vaccine which was developed in record time, and the public learned about its safety and possible risks through the media coverage.   In this capstone, I examine U.S. news media coverage on the Covid-19 vaccine topic as an illustration of a debate in a polarized environment through the stance in the news media on vaccine safety. I analyze opinion-framing in the Covid-19 vaccine debate as a way of attributing a statement or belief to someone else. For example, a health expert would say that \u201cThe leading researchers agree that Covid-19 vaccines are safe and effective,\u201d while a vaccine skeptic would say that \u201cMistaken researchers claim that Covid-19 vaccines are safe and effective\u201d. I also analyze if Left-leaning and Right-leaning media engage in self-affirming or opponent-doubting discourse.   I introduce VacStance, a dataset of 2,000 stance-labeled Covid-19 vaccine sentences extracted from 169,432 opinions drawing from 15,750 news articles covering Left-leaning and Right-leaning media outlets. To the best of my knowledge, VacStance is the first data set of media Covid-19 vaccine stances. My dataset and model are made available via GitHub for future projects on Covid-19 vaccine opinion-framing and stance detection.   \n  v ACKNOWLEDGMENTS  I would like to thank my advisor, Professor Sos Agaian for his guidance and support. I am grateful to Matt Gold for making time to listen, to answer questions, and to offer advice. I am also grateful to Jason Nielsen for all his help throughout the program.  I wish to thank Kyle Gorman whose Methods in Computational Linguistics class as well as his suggestions for conferences and relevant resources led to the work of this capstone. Alla Rozovskaya\u2019s Advanced Natural Language Processing class provided me with a better understanding of the concepts and skills required to do this work. Walter Kaczetow from the Quantitative Research Consulting Center helped me calculate Krippendorff's alpha for the InterAnnotator Agreement.  A special thank you to Roxanne Shirazi, Dissertation Research Librarian and Assistant Professor for her tremendous help with the capstone submission process. Lastly, I want to thank my husband, Sanjay for his faith in me and for his support.    \n  vi TABLE OF CONTENTS  Abstract                                                                                                                                    iv Acknowledgements                                                                                                                   v Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1 1. Context and Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  4 2. Materials and Methods                 5 2.1 Data                6 2.1.1 Data Collection and Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    6 2.1.2 Cleaning Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  7 2.1.3 Extracting and filtering opinions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   8 2.2 Data Annotation                8 2.2.1 Annotator Demographics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . .   9 2.2.2 Annotation Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . 10 2.2.3 Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . .  11 2.2.4 InterAnnotator Agreement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  11 3. Stance Detection             12 \n  vii 3.1.1 The Pre-trained stance model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12 3.1.2 Apply Pre-trained stance model to new data . . . . . . . . . . . . . . . . . . . . . . . . . . .   12 4. Analysis               14 4.1.1 Self-Affirming and Opponent-doubting in media coverage . . . . . . . . . . . . . . . . .  14 4.1.2 Analyze how the media ascribes opinions to sources  . . . . . . . . . . . . . . . . . . . . . 14 5. Findings                16 6. Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  18 Appendices                19 Bibliography                22     \n  viii LIST OF FIGURES Target Opinion . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  9 Bias in annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   16    \n  ix LIST OF TABLES 1.  Framing Devices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  5 2. Opinion Framing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   6 3. Quote Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  7 4.  Annotator Demographic Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 5.  Distribution of labels in annotated dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 6.  BERT Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   12 7.  Opinion Attribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15           \n  x DIGITAL MANIFEST   I. Whitepaper (PDF).  II. Git Repository zip file containing the VacStance data set and the files in the git repository at submission date: (https://github.com/ThoughtfulMind/VacStance).  III. Project web site and stance annotation: https://thoughtfulmind.github.io/VacStance/                \n  xi A Note on Technical Specifications  This VacStance Git Hub repository contains the code and data for the capstone project.  It is structured in 4 sections, each containing a Readme file with instructions:   1. Data Scraping \u2013 this folder contains the instructions on scraping using SerpAPI and Media Cloud API but not the scraping code. 2. Data Processing \u2013 contains the scripts and helper files to extract (Source, Predicate, Opinion) tuples from the full text of articles, then filtering extracted tuples and preparing the Opinion spans for input into the Covid-19 vaccine stance classifier. 3. Stance Detection \u2013 contains scripts for doing label inference from the noisy annotator labels, the demographic models, applying the pre-trained covid-19 vaccine stance model to new data, and setting up the environment to run the BERT model search. 4. Analyses \u2013 contains the scripts and other files for opinion-framing analyses. It also includes the lexicons directory files for affirming and doubting framing devices. Since the capstone is based on the global warming stance detection work by Luo, Card, Jurafsky, their GitHub repository can also be referenced for any other instructions not contained here, such as stance annotation using Amazon Mechanical Turk.   Repository structure The VacStance dataset can be requested via GitHub. The dataset contains tab-separated fields for each of the following:    1. `sentence`: the sentence     2. `annotator_0`, ..., `annotator_3`: ratings from each of the four annotators for the stance of the sentence.    3. `disagree`: the probability that the sentence expresses disagreement with the target opinion (that Covid-19 vaccine is safe.), as estimated by the Bayesian model.    4. `agree`: the probability that the sentence expresses agreement with the target opinion (that Covid-19 vaccine is safe.)    5. `neutral`: the probability that the sentence is neutral to the target opinion (that Covid-19 vaccine is safe.)    6. `guid`: a unique ID for each sentence.    7. `in_held_out_test`: whether the sentence was used in my held-out-test set for model and baseline evaluation. \n  1 INTRODUCTION The COVID-19 pandemic has spread across the world. Vaccinations are the most critical public health instruments for decreasing the spread and harm caused by dangerous diseases, including  COVID-19. As the World Health Organization (WHO) has noted, \u201cwhile immunization is one of the most successful public health interventions, coverage has plateaued over the last decade\u201d even though vaccination may prevent 2-3 million deaths each year.1 The COVID-19 pandemic and associated disruptions have strained health systems. The uncertainty about health risks, life difficulties, and vaccines' effectiveness consequences led to greater vaccine hesitancy which is one of the top ten global threats2. Despite significant evidence showing that Covid-19 vaccines are safe3 and effective with some reporting efficacies as high as 95%4, there is increasing polarization toward vaccination. \u201cThe Covid-19 epidemic in the United States risks becoming a tale of \u201ctwo Americas,\u201d as Anthony Fauci warned5 in June.\u201d  This growing polarization in the United States has far-reaching impacts and it is reflected in people\u2019s perceptions, attitudes, and behaviors. The causes of polarization are many, but some cite the growing fragmentation of the news media and social media platforms as factors. The media has been producing a high volume of news articles related to the Covid-19 pandemic and Covid-19 vaccines. News coverage plays an essential role in shaping public opinion and influences public debates, often on complex and unfamiliar topics. One such topic is the Covid-19 vaccine, which was developed in record time, and the public learned about its safety and possible risks through media coverage. Most of the public gets information on science and health-related topics from the media to make decisions about their health, so the accuracy of the science about the vaccine is critical. Recently, it was shown that public debates around vaccine safety could lead to vaccine hesitancy, resulting in deaths from vaccine-preventable diseases.6 My work is informed by a study examining the levels of politicization and polarization on Covid-19 covering stance detection in  1 https://ourworldindata.org/vaccination 2 https://en.wikipedia.org/wiki/Vaccine_hesitancy 3 Center for Disease Control and Prevention. Vaccine Safety: https://www.cdc.gov/vaccinesafety/index.html 4 Yale Medicine: https://www.yalemedicine.org/news/covid-19-vaccine-comparison 5 https://www.cnn.com/2021/06/30/health/us-coronavirus-wednesday/index.html 6 Vaccine Hesitancy https://en.wikipedia.org/wiki/Vaccine_hesitancy \n  2 Twitter. I used the methodology from DeSMOG: Detecting Stance in Media On Global Warming (Luo et al., 2020). Challenges:  It is imperative to: a) understand how a polarized media helps shape the public debate on the covid-19 vaccine because it is a key determinant of the public\u2019s approval. b) analyze stance on the vaccine's safety, especially one that disagrees that the vaccine is safe because it can lead to different interpretations and potentially life-altering outcomes for many Americans.  The objectives of the capstone are: a) To illustrate the debate on the Covid-19 vaccine7 topic in a polarized environment by looking at the stance on vaccine safety in the media.  b) To analyze opinion-framing on discourse that affirms one\u2019s point of view and on discourse casting doubt on the other side\u2019s point of view.  c) To develop a dataset of 2,000 stance annotated sentences.  Using SerpApi and Media Cloud API8, I extracted and filtered 15,750 articles covering Left-leaning and Right-leaning media from January 2020 to July 2021 using a list of 71 keywords (see Appendix A). The articles come from all the content published in the mainstream media, including some newswires and op-ed articles from which I then extracted 169,432 sentences (also referred to as opinion spans or quotes). I further filtered these sentences using the main keywords Covid Vaccine and Coronavirus vaccine to 14,512 sentences. I then randomly selected and manually processed a total of 2,000 sentences (1,000 for Left-leaning and 1,000 for Right-leaning media) which became my final dataset annotated by volunteer annotators. A trained BERT classifier analyzed aspects of argumentation, how the different sides of the vaccine debate represent their own and each other\u2019s opinions to determine if Left-Leaning and  7 I use the term Covid-19 vaccine throughout to refer to the Pfizer, Moderna, and Johnson & Johnson Covid-19 vaccines since these are the vaccines available in the United States and are fully approved or have been granted emergency authorization by the Food and Drug Administration (FDA). Some exceptions include other vaccines such as the Astra Zeneca vaccine when news media covers countries outside the United States. 8 SerpAPI serpapi.com and Media Cloud API: mediacloud.org \n  3 Right-Leaning media9 use framing devices and opinion attribution. BERT is a language representation model and it stands for Bidirectional Encoder Representations from Transformers. It is designed to \u201cpre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\u201d (Devlin et al., 2018).  The primary contributions of the capstone are: 1. VacStance \u2013 a dataset of 2K stance annotated sentences from Covid-19 vaccine news. 2. Analysis of the media coverage of the Covid-19 vaccine.  The following presents the capstone project structure:  Chapter 1: Context and Related Work covers recent research by Yiwei Luo, Dallas Card, and Dan Jurafsky on the stance in media on global warming, as well as a study examining the levels of politicization and polarization in Covid-19. Chapter 2: Data Scraping and Processing describes data collection, preprocessing, and annotation. Chapter 3: Stance Detection covers data annotation and the stance detection models. Chapter 4: Analysis examines the coverage of the media outlets to find if both Left-leaning and Right-leaning media engage in self-affirming and opponent-doubting in their coverage as well as how the media ascribe opinions to sources. Chapter 5: Findings summarizes the findings, the limitations of the study, and future work.           9 Media Bias/Fact Check classification for media leaning: https://mediabiasfactcheck.com \n  4 CHAPTER 1: CONTEXT AND RELATED WORK Although research to date has not examined stance in media on the Covid-19 vaccine, recent research by Yiwei Luo, Dallas Card and Dan Jurafsky at Stamford University investigated stance in media on global warming and developed a framework for future research on opinion-framing and the automatic detection of GW (Global Warming) Stance. They created and made publicly available a dataset of OPINION spans extracted from GW news articles annotated with stance judgements using Amazon Mechanical Turk. They studied the impact of annotator characteristics on their perception of stance, then combined ratings to infer a distribution over stance labels for each span accounting for bias which they released with the raw annotations. This capstone builds on their work. The paper, \u201cDeSMOG: Detecting Stance in Media on Global Warming\u201d and the code is available on GitHub.  A study examining the levels of politicization and polarization in Covid-19 news focused on newspapers and televised network news (Hart et al., 2020), found that newspaper coverage is highly polarized while network coverage to a lesser extent and suggested that the high degree of polarization in the initial Covid-19 coverage from March through May, 2020 possibly contributed to the polarization in U.S. Covid-19 attitudes. Their work also builds on climate change news coverage research (Chinn et al., 2020) which investigated polarization by analyzing how the discussion varied based on the presence of politicians from both political parties in the media. The categorization by Justine Zhang, Ravi Kumar, Sujith Ravi, and Cristian Danescu-Niculescu-Mizil. 2016 Conversational Flow in Oxford-style Debates and their proposed methodology for tracking how ideas flow during a debate between participants is the categorization I use here.  A Stance Data Set on Polarized Conversations on Twitter about the Efficacy of Hydroxychloroquine as a Treatment for COVID-19 (Mutlu et al.,) covers stance detection on the topic of Covid-19 treatment on Twitter. Although, our analysis is focused on the mainstream media and not on social media, their investigation of the polarized debates about unconfirmed medicines and treatments in Twitter has been informative.  \n  5 CHAPTER 2: MATERIALS AND METHODS In this chapter, I describe the methodology, the data collection, and the annotation methods. I am using the Covid-19 vaccine topic as an illustration of a debate in a polarized environment by looking at the stance in the media on vaccine safety. I focus on opinion-framing on discourse that affirms one\u2019s point of view and on discourse casting doubt on the other side\u2019s point of view. (Luo et al., 2020). Citing opinions is a strategy in argumentation. For example, a health expert would say that \u201cThe leading researchers agree that Covid-19 vaccines are safe and effective,\u201d framing the clause affirming her stance that the vaccine is \u201csafe and effective\u201d and as an opinion that is endorsed by a reputable [\u201cleading\u201d] and trustworthy source [\u201cresearchers\u201d] who agree that the vaccines are safe and effective. However, an anti-vaxxer would say that \u201cMistaken researchers claim that Covid-19 vaccines are safe and effective\u201d, framing the same clause as an opinion of an untrustworthy source. The difference is the choice of predicate agree vs claim and the way the source [\u201cresearchers\u201d] is described. The two statements have different interpretations, even though the implied \u201cCovid-19 vaccines are safe and effective\u201d is the same in both sentences. I will refer to such sentences containing entity \u2013 expresses \u2013 statement as opinion-framing. The components of opinion-framing are Source, Predicate and Opinion (Table 1) along with examples of affirming and doubting framing devices within these components. (Luo et al., 2020).  Source Predicate Opinion Few vaccine researchers Believe the Covid-19 vaccinated population experienced severe or fatal side-effects.10 Vaccine experts Claim Covid-19 vaccines are safe despite being developed quickly because they are built on trusted work that goes back decades.  Most Americans Agree a Covid-19 vaccine approved by the FDA should be safe for the general population.  Table 1. Example of framing devices (affirming: expert, most, agree and doubting: few, claim).    10 The Safety of COVID-19 Vaccinations\u2014We Should Rethink the Policy (Retracted): https://www.mdpi.com/2076-393X/9/7/693/htm \n  6 Opinion-framing is a way of attributing a statement or belief to someone else, and here I analyze if both Left-leaning and Right-leaning media engage in self-affirming or opponent-doubting discourse and if the public stance of the opinion of a named entity matches the actual stance of the respective entity. This requires identifying the stance of a given opinion about the Covid-19 vaccine.  Self-Affirming Discourse Opponent-Doubting Discourse Discourse that affirms one\u2019s opinions.  Example: The use of \u201cagree\u201d to frame opinions that agree with your stance. Discourse casting doubt on the other side\u2019s opinions.  Example: The use of \u201cclaim\u201d to cause people to doubt the opinions of the opposing side.  Table 2. Opinion-framing: self-affirming and opponent-doubting discourse.   2.1 Data 2.1.1 Data Collection and Preprocessing Using Media Cloud API and SerpAPI, I collected articles from January 1, 2020 to July 1, 2021 using the keywords (covid-19 AND vaccine) OR (coronavirus AND vaccine) OR (vaccine AND side-effects) and filtering out articles that were not in English.   I created an intermediate data frame for the collected urls and applied filtering, regularization, and deduplication. I then created the dataframe that combines all the data structures with urls and dropped the duplicate urls. I set up Serp Api using the keywords \u201cCovid-19 vaccine\u201d and set the query to \u201cUnited States\u201d to read the list of domains and political leaning as Left-leaning (L_Domains) and Right leaning (R_Domains). I fetched and deduplicated the articles using the Media Cloud API. I passed each article through SpaCy11, an open-source software library for advanced natural language processing. Using several lexical resources (Luo et al., 2020), I filtered the extracted opinion spans (also refered to as quotes in the code and sentences after they are transformed for the annotated dataset). I kept only the opinion spans that contain a stem from a list of 71 Covid-related keywords such as coronavirus, mrna, vaccination (Appendix A).  11 spaCy: https://spacy.io \n  7 Sentence Examples  Left-leaning media \u201cTheir vaccines are more than 90% effective at preventing COVID-19.\u201d  \u201cThe coronavirus vaccine campaign has heightened tensions between rural and urban America.\u201d  \u201cAmericans already see the COVID-19 vaccine project as a rush-job\u201a part of a last-ditch effort by Trump to turn around his abysmal polling numbers before the election.\u201d Right-leaning media \u201cIt\u2019s unconscionable for AOC, who's 30 years old, to be smiling gleefully and getting the vaccine when you got 85-year-old people in nursing homes who haven't gotten it.\u201d  \u201cThe partisan divide over the country's pandemic response has reinvigorated the anti-vaccine movement nationwide, with lawmakers in nearly 40 states, mostly Republicans, backing bills to restrict Covid-19 vaccine mandates or vaccine passports.\u201d  \u201cCOVID vaccines are a game-changer that make future surges, like those seen last winter, unlikely to happen again.\u201d Table 3. Example of sentences extracted from articles and processed.  2.1.2 Cleaning data:  The sentences resulting from the extracted opinions were transformed by simple operations such as adding punctuation, deleting extra words such as \u201cthat\u201d occurring at the start of some sentences, removing extra spaces, and capitalizing the first word of the sentence (Table 3). This improved readability. I also removed irrelevant or inappropriate sentences.  Here are a few examples of sentences that were innapropriate or not about vaccine safety but about logistics and intellectual property: Inappropriate: \u201cIt is time to expedite development of a vaccine for the devastating COVID19 even if it means increased death of volunteers willing to be guinea pigs\u201d. Intellectual Property: \u201cIt will work with the World Trade Organization (WTO) to negotiate a deal to suspend intellectual property rights associated with the Covid-19 vaccines\u201d. Logistics and distribution: \u201cAbout 2 million of the 6 million COVID-19 vaccine doses delayed by last week\u2019s winter weather were delivered over the weekend.\u201d; \u201cThe government could reach \n  8 hundreds of millions of doses of a coronavirus vaccine by the end of the year by taking a big financial risk\u201c).  Anti-vaccine statements or conspiracies were present more frequently in Right-Leaning media such as: \u201cThe coronavirus vaccine in particular could alter people's DNA or even transform them into 5G wireless antennas\u201d. There were also anti-vaccine statements present in the Left-Leaning media such as \u201cFauci remains unfazed as scientists rely on unproven methods to create covid vaccines\u201d.   2.1.3 Extracting and filtering opinions I extracted opinions that were filtered using the following criteria: the extracted Predicate is a Householder verb, the extracted opinion is not an indirect question, and the extracted opinion contains one of 71 Covid-19 vaccine/coronavirus vaccine-related keywords (see Appendix A). I removed the duplication resulted from media distribution.  2.2 VacStance dataset I created VacStance, a new dataset of 2K stance labeled sentences that were annotated with stance judgements as described bellow.  2.3 Data Annotation I recruited volunteer annotators by asking peers to provide referrals. The criteria were that the volunteer annotator group is a diverse group representing different ideologies and age groups based in the United States and familiar with both the political and the media environment. I informed the annotators that I am collecting judgments using the target opinion: Covid-19 vaccine is safe and the following labels: agree, disagree, and neutral (Figure 1). The annotation began with a practice trial to ensure that each annotator understands the task. Then each of the 4 volunteers annotated the stance for the entire dataset of 2,000 sentences which generated 4 judgments per item for a total of 8,000 annotations.  \n  9  Fig 1. Each sentence is labeled against the target opinion.   2.2.1 Annotator Demographics I collected demographic information such as age, state of residence, level of education as well as political affiliation for each annotator to determine if there is stance annotation bias along party lines or gender (Table 4). For political affiliation by age and gender in the US, I consulted Pew Research Center\u2019s reports on trends in party affiliation among demographic groups12 and on the  12 Trends in party affiliation among demographic groups: http://pewrsr.ch/2FVWtww \n\n  10 changing U.S. electorate13. I also asked annotators about their stance on the vaccine and if they believe that the Covid-19 vaccine is safe (see Appendix B). In addition, I asked each annotator to complete a poll about the Covid-19 pandemic.  Demographic Information  Female 2 Male 2 Age over 34 3 College degree or higher 4 Democrat 1 Republican 1 Independent (no party affiliation) 2 Table 4. Demographic information of volunteer annotators.  2.2.2 Annotation Challenges The task of annotating the dataset began with a practice task asking volunteers to annotate 10 sentences to ensure that each annotator understands the task before moving on to annotating the entire 2K dataset.   The annotation task was made more difficult by the pandemic period with many unknowns during vaccine development that were reflected in the media coverage and the fact that the Covid-19 vaccine debate is a complex topic. This means that many sentences do not have a \u201ctrue\u201d label of agree or disagree and that many sentences lack the necessary context which makes them difficult to annotate. This was reported by each annotator during the practice annotation and in the feedback provided at the end of the annotation. It was also reflected in the annotation disagreement (see InterAnotator Agreement).   13 Pew Report: In Changing U.S. Electorate, Race and Education Remain Stark Dividing Lines: https://www.pewresearch.org/politics/2020/06/02/in-changing-u-s-electorate-race-and-education-remain-stark-dividing-lines/  \n  11 2.2.3 Aggregation  I aggregated the annotations in the 2K sentences dataset and the distribution is shown in Table 5.  Label  Agree 2,948 Disagree 2,307 Neutral 2,745 Table 5. The distribution of labels in the annotated dataset.  2.2.4  InterAnotator Agreement As I mentioned earlier, due to ambiguity, some items cannot be easily labeled as agree, disagree or neutral. To address this issue, I looked at the disagreement between labels to identify those with a high level of disagreement. I used the average inter-annotator agreement (a measure of how well annotators can make the same annotation decision for a certain category) measured as Krippendorff\u2019s alpha14, a reliability coefficient developed to measure the agreement among annotators. For 4 annotators annotating a total of 8,000 sentences, Krippendorff\u2019s alpha = 0.0832. The low alpha is partially due to the low number of annotators as well as the disagreement between the self-identified Republican and Democrat annotators whose annotation is influenced by their respective ideological views as well as their reported stance on vaccine safety. To further verify this, I looked at Krippendorff\u2019s alpha for the 2 self-identified Republican and Democrat annotators who annotated a total of 4,000 sentences. The Krippendorff\u2019s alpha = \u2013 0.1522. This negative value indicates inverse agreement and is in line with findings by Pew Research15 that Republicans and Democrats live in \u201cnearly inverse news media environments\u201d which shape their increasingly polarized views. This is also a reminder that it is critical to be aware of the biases and variability in judgment intrinsic to annotated datasets because this can lead to biased models.    14 Krippendorff\u2019s alpha: https://en.wikipedia.org/wiki/Krippendorff%27s_alpha 15 https://www.niemanlab.org/2020/01/republicans-and-democrats-live-in-nearly-inverse-news-media-environments-pew-finds/ \n  12 CHAPTER 3: STANCE DETECTION In this chapter, I describe the model trained on the set of 2K sentences, how the leaning of the media outlet is determined, and the stance classification model on the full dataset.  3.1 The pre-trained Covid-19 vaccine stance model  I start by training a model on the set of 2K annotated sentences to predict the stance of a sentence S toward the target opinion T (\u201cCovid-19 vaccine is safe.\u201d). I select a random test set of 200 annotated instances that include the political leaning of the media outlet and use 5-fold cross validation to train on the remaining 1,800 examples.  Agree Disagree Neutral Weighted Average Accuracy = 0.85 Precision = 0.89 Precision = 0.81 Precision = 0.83 Precision = 0.85 Precision = 0.84 Recall = 0.85 Recall = 0.83 Recall = 0.86 Recall = 0.85 Recall = 0.85 F1 Score = 0.87 F1 Score = 0.82 F1 Score = 0.84 F1 Score = 0.85 F1 Score = 0.85 Table 6. BERT performance reported as accuracy and macro-F1 score for each label agree, neutral, disagree.  3. 2 Applying the pre-trained Covid-19 vaccine stance model to new data  I then applied the stance classification model to the unlabeled dataset of 169K opinions from which the 2K sentences had been extracted, transformed, annotated, and used in the pre-trained model above.  I use the political leaning methodology from Media Bias/Fact Check Project for Left-Leaning and Right-leaning media outlets as a proxy for Covid-19 vaccine stance (Jurafsky et al., 2020) to find whether both sides are engaging in self-affirming or opponent-doubting in their coverage. The Media Bias/Fact Check Project is a comprehensive media bias resource where a viewer can check the bias of any source. The Right and Left biases are the media sources that are moderately to strongly biased toward conservative or liberal causes \u201cthrough story selection and/or political affiliation. They may utilize strong loaded words (wording that attempts to influence an audience \n  13 by using appeal to emotion or stereotypes) to favor\u201d conservative or liberal causes. The more extreme Right and Left biases would \u201cpublish misleading reports and omit reporting of information that may damage\u201d conservative or liberal causes and some such sources are deemed untrustworthy.  I excluded the neutral label to prevent misclassification of the sentences that agree that the Covid-19 vaccine is safe and the sentences that disagree that the Covid-19 vaccine is safe. However, the result is not satisfactory and further investigation is needed.                                      \n  14 CHAPTER 4: ANALYSIS In this chapter, I present my additional analyses on self-affirming and opponent-doubting discourse on the labeled dataset and how the media ascribe opinions to sources.  4.1 Self-affirming and opponent-doubting discourse on labeled dataset I used the 2,000 labeled dataset containing 1,000 Left-leaning and 1,000 Right-leaning sentences to analyze media outlets to find if both Left-leaning and Right-leaning media engage in self-affirming and opponent-doubting in their coverage. I find that both Left-Leaning and Right-Leaning media engage in self-affirmation and opponent-doubting but the latter engages more in both opponent-doubting coverage as well as in self-affirmation coverage. However, further investigation is necessary due to the language complexity around the Covid-19 vaccine coverage, the limitations of the Lexicons used, and the small size of the dataset.  4.2. Analyze how the media ascribe opinions to sources.  I used the same 2,000 labeled dataset containing 1,000 Left-leaning and 1,000 Right-leaning sentences to search and select the sources that were most commonly present in the media coverage about the vaccine as well as the Wikipedia for American anti-vaccination activists list.16   I manually labeled the stance of the sources selected. I then analyzed the faithful and unfaithful instances across Right-leaning and Left-leaning media using the Lexicons (Luo et al., 2020). I define an opinion faithfully ascribed when the stance of the opinion (Covid19 vaccine-agree) matches the stance of the source (President Biden, President Trump, Dr. Fauci, Robert Redfield, Governor Cuomo, Bill Gates, etc.).  While both Left-Leaning and Right-Leaning media attribute opinions unfaithfully to sources that have different public views that are well known, the Right-Leaning media tends to emphasize hypocrisy more (with 31 instances versus 20 instances identified) and often cast doubt on the science or the credibility of the sources: \u201cThey are going to such desperate lengths to silence vaccine skeptics, censor doctors and scientists, and coerce as many people as possible into getting the vaccine injection.\u201d This may have to do with the fact that for political purposes, casting doubt  16 Wikipedia: https://en.wikipedia.org/wiki/Category:American_anti-vaccination_activists \n  15 on the Covid-19 vaccine is likely to be beneficial for Republicans, who according to Pew Research Center surveys conducted before as well as after the start of the pandemic are less likely than Democrats to trust scientists17 and this gap has widened in the past year.    Left-leaning Right-leaning \u201cFauci remains unfazed as scientists rely on unproven methods to create covid vaccines.\u201d \u201cYou understand that Moderna is Fauci's favorite vaccine company, and his agency, NIAID, stands to rake in cash if Moderna's shot turns out to be the choice for COVID\u201a when, in fact, no vaccine is necessary.\u201d  \u201cOne GOP state representative knows of doctors who are warning people who receive the COVID-19 vaccine.\u201d \u201cIs Joe a vax doubter? Exactly how much confidence does the Biden White House have in the COVID vaccinations?\u201d   Table 7: Opinion attribution \u2013 examples of unfaithful opinion attribution from Left-leaning and Right-leaning media.   The verbs such as claim, understand, knows generally indicate that the sources display hypocrisy as determined in previous work (Luo et al., 2020) by holding contradictory beliefs in private. However, the examples in Table 7 show that there is more complexity in the media coverage and further investigation is required. It is also important to note that the small labeled dataset was used for this analysis. The larger dataset must be used for a more conclusive result.               17 Pew Research: https://www.pewresearch.org/science/2020/05/21/trust-in-medical-scientists-has-grown-in-u-s-but-mainly-among-democrats/ \n  16 CHAPTER 5: FINDINGS In this chapter, I summarize the findings, the limitations of this study, and future work. 1. Ambiguity and complexity \u2013 I found that there are many sentences that have a \u201ctrue\u201d stance such as agree or disagree, but there are more sentences that are ambiguous which makes them difficult to annotate. This confirms findings in previous work (Luo et al., 2020). 2. Bias by party affiliation in stance annotation \u2013 I tested for bias by party affiliation in the stance annotation and I found evidence of bias along party lines. I found that the annotator\u2019s party affiliation or non-affiliation, as is the case for non-affiliated \u201cIndependent\u201d annotators, often influence the way annotators respond (see Fig.2). \n         Fig. 2. Bias in annotation by party affiliation (target opinion: \u201cCovid-19 vaccine is safe.\u201d).   3. Stated vaccine stance influences the way annotators respond  As stated previously, I collected annotator\u2019s stance on the vaccine safety before data annotation began to help determine if the annotator\u2019s stance influences annotation. I found that annotators whose stated stance is that the vaccine is safe, annotated more sentences as \n\n  17 agree. Similarly, the annotator whose stated stance is that the vaccine is not safe, annotated more sentences as disagree.  4. The disagreement between ratings is reflective of polarization along ideological lines. Krippendorff\u2019s alpha was negative when calculated on party-affiliated annotators ratings for the 2 self-identified Republican and Democrat annotators who annotated a total of 4,000 sentences. The Krippendorff\u2019s alpha = \u2013 0.1522. This negative value indicates inverse agreement and is in line with findings by Pew Research18 that Republicans and Democrats live in \u201cnearly inverse news media environments\u201d which shape their increasingly polarized views. This is also a reminder that it is critical to be aware of the biases and variability in judgment intrinsic to annotated datasets because this can lead to biased models. 5. Self-affirming and opponent doubting \u2013 I found that both sides are engaging in self-affirming or opponent-doubting in their coverage but to different degrees and that further investigation on the larger dataset is required. 6. Assigning hypocrisy \u2013 I examined if both sides attribute opinions to name entities that differ from the known public stances of the respective entities and found that Right-Leaning media tends to emphasize hypocrisy more (with 31 instances versus 20 instances identified) and often cast doubt on the science or the credibility of the sources. I also found that there is complexity in the interactions covered in the media beyond the verbs that I used and further investigation on the larger dataset is required.  Study Limitations and future work I collected sentences from across the media spectrum and I recruited annotators that represent a small but ideologically diverse group. However, this study is limited by the small number of annotators of which only two represent polarized ideological views. Future work can build on this study by increasing the number of annotators affiliated with both parties and by analyzing opinion-framing not just in text but also in images and video that are included in the news media coverage.   18 https://www.niemanlab.org/2020/01/republicans-and-democrats-live-in-nearly-inverse-news-media-environments-pew-finds/ \n  18 CHAPTER 6: CONCLUSION In this capstone, I introduced VacStance, a dataset of 2,000 stance-labeled Covid-19 vaccine sentences from media outlets that allowed us to examine stance on Covid-19 vaccine. To the best of my knowledge, VacStance is the first data set of media Covid-19 vaccine stances.  The dataset and model are available on GitHub for those who want to pursue future projects on Covid-19 vaccine opinion-framing and stance detection.              \n  19 APPENDIX A Keywords  coronavirus, covid-19, vaccine, vaccination, Pfizer, moderna, astrazeneca, antivaxxer, pandemic, inoculated, doses, viral, virus, side-effects, immunity, antibodies, herd, sars-COV-19, fever, cough, congestion, spread, containment, outbreak, asymptomatic, cdc, infected, infectious, quarantine, self-isolation, disease, inflammation, hypoxemia, infection, cases, fatalities, deaths, hospitalizations, healthcare, immunocompromised, incubation, intubation, mask, mortality, morbidity, global, economic, mrna, air, scientist, epidemiologist, scientific, research, pipeline, ppe, doctors, nurses, ventilator, respirator, travel, tested, sick, hhs, cluster, transmission, injection, vector, betacoronavirus, anti-vaccine, skeptic, delta.    APPENDIX B Annotator Information  First name Last name:  Age:  Gender: Female Male  Level of education: No High School High School Bachelors Graduate (Masters) Terminal degree (e.g. Doctorate)  Political affiliation: Independent Democrat  Republican  State of residence (United States residents or citizens):  Do you believe that Covid-19 vaccine is safe? Yes No   \n  20 APPENDIX C VacStance Annotator Poll  Based on DeSmog Annotator Poll (Jurafsky el all, 2020) Poll 1:     Q: Do you think that the Covid-19 virus has been spreading globally over the past year, or do you think this has not been happening?    a. Probably has been happening    b. Probably has not been happening  Poll 2:     Q (if chose a.): How sure are you that the Covid-19 virus has indeed been spreading over the past year?    Q (if chose b.): How sure are you that the Covid-19 virus has not been spreading over the past year?    a. Extremely sure    b. Very sure    c. Moderately sure    d. Somewhat sure    e. Not at all sure  Poll 3:     Q: Do you think that the spread of the Covid-19 virus over the past year has been caused mostly by things people have done, mostly by natural processes, or about equally by things people have done and by natural processes?    a. Mostly by things people have done    b. Mostly by natural processes    c. About equally by people and natural processes  Poll 4:    Q: In your opinion, do you think the federal government should or should not require vaccination at government run vaccination sites and at private organizations (e.g. pharmacies, hospitals)?    a. Federal government should require    b. Federal government should not require  Poll 5:    Q: How concerned are you about Covid-19?    a. Extremely concerned    b. Very concerned    c. Moderately concerned    d. Somewhat concerned    e. Not at all concerned  Poll 6: \n  21    Q: During the past 6 months, how often did you hear about Covid-19 vaccine in the media?    a. Multiple times a week    b. About once a week    c. A couple times a month    d. A couple times a year    e. Never  Poll 7:     Q: How accurately do you think the news media reports on Covid-19 vaccine?    a. Extremely accurately    b. Very accurately    c. Moderately accurately    d. Somewhat accurately    e. Not at all accurately                                \n  22 REFERENCES Bowman, Samuel R., Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. \u201cA Large Annotated Corpus for Learning Natural Language Inference.\u201d In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 632\u201342. Lisbon, Portugal: Association for Computational Linguistics. https://doi.org/10.18653/v1/D15-1075.  Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.\u201d ArXiv:1810.04805 [Cs], May. http://arxiv.org/abs/1810.04805.  Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.\u201d In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171\u201386. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423.  Field, Anjalie, Doron Kliger, Shuly Wintner, Jennifer Pan, Dan Jurafsky, and Yulia Tsvetkov. 2018. \u201cFraming and Agenda-Setting in Russian News: A Computational Analysis of Intricate Political Strategies.\u201d ArXiv:1808.09386 [Cs], October. http://arxiv.org/abs/1808.09386.  Ghosh, Shalmoli, Prajwal Singhania, Siddharth Singh, Koustav Rudra, and Saptarshi Ghosh. 2019. \u201cStance Detection in Web and Social Media: A Comparative Study.\u201d In Experimental IR Meets Multilinguality, Multimodality, and Interaction, edited by Fabio Crestani, Martin Braschler, Jacques Savoy, Andreas Rauber, Henning M\u00fcller, David E. Losada, Gundula Heinatz B\u00fcrki, Linda Cappellato, and Nicola Ferro, 75\u201387. Lecture Notes in Computer Science. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-28577-7_4.  Ghosh, Shalmoli, Prajwal Singhania, Siddharth Singh, Koustav Rudra, and Saptarshi Ghosh. 2019.   \u201cStance Detection in Web and Social Media.\u201d GitHub repository. https://github.com/prajwal1210/Stance-Detection-in-Web-and-Social-Media  Habernal, Ivan, and Iryna Gurevych. 2016. \u201cWhich Argument Is More Convincing? Analyzing and Predicting Convincingness of Web Arguments Using Bidirectional LSTM.\u201d In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1589\u201399. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1150.  Hovy, Dirk, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. \u201cLearning Whom to Trust with MACE.\u201d In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1120\u201330. Atlanta, Georgia: Association for Computational Linguistics. https://aclanthology.org/N13-1132. \n  23  Jacobs, Tom. n.d. \u201cThe Complex Ethical Codes of Anti-Vaxxers.\u201d Pacific Standard. Accessed October 12, 2021. https://psmag.com/education/the-curious-morality-of-the-anti-vaxxer.   Lipsitch, Marc, and Natalie E. Dean. 2020. \u201cUnderstanding COVID-19 Vaccine Efficacy.\u201d Science 370 (6518): 763\u201365. https://doi.org/10.1126/science.abe5938.  Loshchilov, Ilya, and Frank Hutter. 2017. \u201cDecoupled Weight Decay Regularization,\u201d November. https://arxiv.org/abs/1711.05101v3.  Luo, Yiwei, Dallas Card, and Dan Jurafsky. 2020. \u201cDetecting Stance in Media on Global Warming.\u201d In Findings of the Association for Computational Linguistics: EMNLP 2020, 3296\u20133315. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.findings-emnlp.296.   Luo, Yiwei, Dallas Card, and Dan Jurafsky. 2020. \u201cDetecting Stance in Media on Global Warming.\u201d GitHub repository. https://github.com/yiweiluo/GWStance   Mohammad, Saif, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry. 2016. \u201cSemEval-2016 Task 6: Detecting Stance in Tweets.\u201d In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), 31\u201341. San Diego, California: Association for Computational Linguistics. https://doi.org/10.18653/v1/S16-1003.  Mutlu, Ece \u00c7i\u011fdem, Toktam A. Oghaz, Jasser Jasser, Ege T\u00fct\u00fcnc\u00fcler, Amirarsalan Rajabi, Aida Tayebi, Ozlem Ozmen, and Ivan Garibay. 2020. \u201cA Stance Data Set on Polarized Conversations on Twitter about the Efficacy of Hydroxychloroquine as a Treatment for COVID-19.\u201d ArXiv:2009.01188 [Cs], September. http://arxiv.org/abs/2009.01188.  Weatherall, Cailin O\u2019Connor & James Owen. 2020. \u201cWhy False Claims About COVID-19 Refuse to Die.\u201d Nautilus. April 16, 2020. http://nautil.us/issue/84/outbreak/why-false-claims-about-covid_19-refuse-to-die.  Pryzant, Reid, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020. \u201cAutomatically Neutralizing Subjective Bias in Text.\u201d Proceedings of the AAAI Conference on Artificial Intelligence 34 (01): 480\u201389. https://doi.org/10.1609/aaai.v34i01.5385.  Simpson, Edwin, and Iryna Gurevych. 2018. \u201cFinding Convincing Arguments Using Scalable Bayesian Preference Learning.\u201d Transactions of the Association for Computational Linguistics 6 (June): 357\u201371. https://doi.org/10.1162/tacl_a_00026.  Tsur, Oren, Dan Calacci, and David Lazer. 2015. \u201cA Frame of Mind: Using Statistical Models for Detection of Framing and Agenda Setting Campaigns.\u201d In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1629\u201338. Beijing, China: Association for Computational Linguistics. https://doi.org/10.3115/v1/P15-1157. ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Detecting Stance on Covid-19 Vaccine in a Polarized Media", "author": ["R Ceslov"], "pub_year": "2021", "venue": "NA", "abstract": "The growing polarization in the United States has been widely reported. Media coverage  plays an important role in shaping public opinion and influences public debates on complex"}, "filled": false, "gsrank": 574, "pub_url": "https://academicworks.cuny.edu/gc_etds/4616/", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:TIebn1q1hYMJ:scholar.google.com/&output=cite&scirp=573&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D570%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=TIebn1q1hYMJ&ei=brWsaODUBsDZieoPqdqh8QU&json=", "num_citations": 1, "citedby_url": "/scholar?cites=9477180391700531020&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:TIebn1q1hYMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=5693&context=gc_etds"}}, {"title": "From fake to hyperpartisan news detection using domain adaptation", "year": "2023", "pdf_data": "From Fake to Hyperpartisan News Detection Using Domain Adaptation\nR\u02d8azvan-Alexandru Sm \u02d8adu1, Sebastian-Vasile Echim1, Dumitru-Clementin Cercel1,\nIuliana Marin2, Florin Pop1,3\n1University Politehnica of Bucharest, Faculty of Automatic Control and Computers\n2University Politehnica of Bucharest, Faculty of Engineering in Foreign Languages\n3National Institute for Research & Development in Informatics - ICI Bucharest, Romania\nrazvan.smadu@stud.acs.upb.ro, sebastian.echim@stud.aero.upb.ro\n{dumitru.cercel,iuliana.marin,florin.pop }@upb.ro\nAbstract\nUnsupervised Domain Adaptation (UDA) is a\npopular technique that aims to reduce the do-\nmain shift between two data distributions. It\nwas successfully applied in computer vision\nand natural language processing. In the cur-\nrent work, we explore the effects of various\nunsupervised domain adaptation techniques be-\ntween two text classification tasks: fake and\nhyperpartisan news detection. We investigate\nthe knowledge transfer from fake to hyperpar-\ntisan news detection without involving target\nlabels during training. Thus, we evaluate UDA,\ncluster alignment with a teacher, and cross-\ndomain contrastive learning. Extensive experi-\nments show that these techniques improve per-\nformance, while including data augmentation\nfurther enhances the results. In addition, we\ncombine clustering and topic modeling algo-\nrithms with UDA, resulting in improved perfor-\nmances compared to the initial UDA setup.\n1 Introduction\nFake news detection is a challenging task in which\nthe goal is to detect whether the news content\ndoes not disseminate false information which may\nharm society. Recently, this problem has broad\nattention to the research community, especially\nwith the rising interaction with social media plat-\nforms, which have become one of the primary\nsources of information for many individuals (Shu\net al., 2020). Detecting fake news is challenging\nfor many of us, since some news can be written\nvery convincingly, thus spreading misleading in-\nformation without control (Ahmed et al., 2017).\nTherefore, new datasets (such as BuzzFeed-Webis\nFake News (BuzzFeed) (Potthast et al., 2018) and\nISOT (Ahmed et al., 2017)) and novel detection\ntechniques (Koloski et al., 2022; Mosallanezhad\net al., 2022) have emerged in recent years.\nEspecially since the 2016 United States presiden-\ntial election, a related task, namely hyperpartisannews detection, identifies whether the information\nspread by the news is in a political extreme (Rae,\n2021). Hyperpartisan articles aim to expose infor-\nmation related to only one perspective, ignoring\nand, in some cases, even attacking the perspectives\nfrom other opposing sides (Kiesel et al., 2019).\nThe consequences of this type of news range from\nmisinformation in the media to an increase in the\nnumber of supporters of extreme ideologies (Huang\nand Lee, 2019).\nSome works (Potthast et al., 2018; Ross et al.,\n2021) linked fake news with hyperpartisan news,\nsince their goal is to spread as much as possible\nand influence people. This phenomenon is related\nto clickbait (Potthast et al., 2016), as the authors\nuse different techniques to make the content more\naccessible and viral on the media (Kiesel et al.,\n2019).\nRecently, many architectures based on Bidirec-\ntional Encoder Representations from Transformers\n(BERT) (Devlin et al., 2019) have been developed\nand fine-tuned on various natural language pro-\ncessing (NLP) tasks. The current work aims to\nevaluate unsupervised deep learning techniques on\nthe fake news detection task and adapt them to the\nhyperpartisan news detection task. Specifically, we\nemploy the Robustly optimized BERT pretraining\napproach (RoBERTa) (Liu et al., 2019) and eval-\nuate it in three domain adaptation scenarios: un-\nsupervised domain adaptation (UDA) (Ganin and\nLempitsky, 2015), cluster alignment with a teacher\n(CAT) (Deng et al., 2019), and cross-domain con-\ntrastive learning (CDCL) (Chen et al., 2020). In\naddition, we analyze topic modeling and clustering\nalgorithms to generate domain labels and perform\nUDA to learn about topic-aware features which\nare specific to fake and hyperpartisan news detec-\ntion. More precisely, we evaluate various clustering\nalgorithms for generating domain labels, namely\nK-Means (Lloyd, 1982), K-Medoids (Kaufmann,arXiv:2308.02185v1  [cs.CL]  4 Aug 2023\n1987), Gaussian Mixture (Fraley and Raftery,\n2002), and HDBSCAN (Campello et al., 2013).\nAdditionally, we explore four topic modeling al-\ngorithms: Latent Dirichlet Allocation (LDA) (Blei\net al., 2003), Non-negative Matrix Factorization\n(NMF) (Lee and Seung, 1999), Latent Semantic\nAnalysis (LSA) (Deerwester et al., 1990), and prob-\nabilistic LSA (pLSA) (Hofmann, 1999).\nTherefore, the main contributions of this work\nare as follows:\n\u2022We evaluate the RoBERTa model on a domain\nadaptation from fake to hyperpartisan news\ndetection by comparing three techniques, as\nwell as several fine-tuning strategies.\n\u2022To our knowledge, we are the first to show that\ncross-domain contrastive learning proposed\nby Wang et al. (2022), initially employed on\ncomputer vision, which performs better than\nother unsupervised learning techniques on an\nNLP task.\n\u2022We propose the cluster and topic-based UDA\napproaches, which obtain better results when\ncompared with the original formulation for\nUDA.\n\u2022We perform extensive experiments to assess\nthe effectiveness of each employed method\nunder various hyperparameter configurations\nand data augmentation techniques based on\nthe term frequency-inverse document fre-\nquency (TF-IDF) scores (Salton et al., 1975)\nand the Generative Pre-trained Transformer 2\n(GPT-2) model (Radford et al., 2019).\n2 Related Work\n2.1 Fake News Detection\nMachine learning techniques for detecting fake\nnews include various feature-based methods, rang-\ning from text to visual features (Zhang and\nGhorbani, 2020). For example, linguistic fea-\ntures (Choudhary and Arora, 2021; P \u00b4erez-Rosas\net al., 2018) capture aspects related to conveyed in-\nformation, document organization, and vocabulary\nused in news. In contrast, style-based features (Pot-\nthast et al., 2018; Zhou and Zafarani, 2020) are re-\nlated to the writing style, such as redaction objectiv-\nity and deception (Shu et al., 2017). In recent years,\nTransformer-based models (Vaswani et al., 2017)\nemerged in the fake news detection literature (Jwaet al., 2019; Zhang et al., 2020; Kaliyar et al., 2021;\nSzczepa \u00b4nski et al., 2021). Other techniques for de-\ntecting fake news use social aspects, such as the\nprofiles of the users who spread the news on social\nmedia platforms (Shu et al., 2017; Onose et al.,\n2019; Zhou and Zafarani, 2020; Sahoo and Gupta,\n2021). Techniques successfully employed for these\nscenarios rely on custom embeddings and linear\nclassifiers (Shu et al., 2019), classic supervised ma-\nchine learning techniques (Reis et al., 2019), and\ndeep learning networks, such as recurrent (Wu and\nLiu, 2018) and graph neural networks (Monti et al.,\n2019; Hamid et al., 2020; Paraschiv et al., 2021).\n2.2 Hyperpartisan News Detection\nTask 4 of SemEval-2019 (Kiesel et al., 2019) intro-\nduced hyperpartisan detection from news articles\nas a binary classification task. The organizers cre-\nated two balanced datasets by crawling data from\nvarious online publishers. Participants were asked\nto detect whether the news articles were hyper-\npartisan or mainstream. The winning team (Jiang\net al., 2019) of the shared task proposed an architec-\nture based on multiple pre-trained ELMo embed-\ndings (Peters et al., 2019) averaged in the embed-\nding space, followed by convolutional layers (Kim,\n2014) and batch normalization (Ioffe and Szegedy,\n2015). They achieved 84.04% accuracy on the\ntraining set and 82.16% accuracy on the test set,\nsuggesting the challenging setting. Other works for\nthe SemEval-2019 Task 4 were based on lexical\nand semantic handcrafted features via Universal\nSentence Encoder (Cer et al., 2018) or BERT, and\na linear classifier (Srivastava et al., 2019; Hanawa\net al., 2019). Furthermore, Potthast et al. (2018)\nshowed that hyperpartisan news detection could be\nanalyzed using fake news approaches. They argued\nthat the writing style for hyperpartisan news is sim-\nilar to fake news, despite their political orientation.\n2.3 Unsupervised Domain Adaptation\nThe core objective of unsupervised domain adap-\ntation is to enforce a feature representation invari-\nant to the domain of the examples with the same\nlabels. One of the most effective techniques is\nthe work of Ganin and Lempitsky (2015), which\ntreated the problem as a minimax optimization.\nWang et al. (2018) utilized domain adaptation tech-\nniques via adversarial training for fake news detec-\ntion by employing an event discriminator to learn\nevent-invariant features in a multi-modal setting.\nDeng et al. (2019) relied on the similarity in the\nfeature space by enforcing a clustered structure\namong similar features. In this case, the training\nprocedure optimizes clustering loss alongside the\ndomain adaptation loss. For the target dataset, a\nteacher model consisting of an ensemble of stu-\ndents generates pseudo-labels (i.e., estimates of the\ntrue labels). Also, contrastive learning (Chen et al.,\n2020) was used to achieve unsupervised domain\nadaptation. It aims to have closer representations of\nthe examples from the same class, while represen-\ntations from different classes should stay far apart.\nIn addition, Wang et al. (2022) proposed the cross-\ndomain contrastive loss to minimize the l2-norm\ndistance between features from the same category,\nand employed K-Means to compute pseudo-labels.\n3 Method\n3.1 Base Model\nIn our current work, we utilize the pre-trained\nRoBERTa language model, which shares the same\narchitectural design as BERT, the only difference\nbeing the pre-training objectives. The RoBERTa\narchitecture stacks multiple Transformer encoders,\neach based on the multi-head self-attention mech-\nanism (Vaswani et al., 2017). On top of the\nRoBERTa model, we add a label predictor con-\ntaining fully connected layers. RoBERTA uses\nthe Byte-Pair Encoding (BPE) tokenizer (Sennrich\net al., 2015). In what follows, we present the set-\ntings in which RoBERTa is employed in our work\n(see Figure 1).\n3.2 Unsupervised Domain Adaptation\nGiven two datasets Ds={(xi\ns, yi\ns)}Ns\ni=1andDt=\n{xi\nt}Nt\ni=1from different domains, the UDA setting\nreduces the shift between them (Ganin and Lem-\npitsky, 2015; Ganin et al., 2016). This approach\ncomprises a feature encoder Gf, a label predictor\nGy, and a domain discriminator Gd. The feature\nencoder maps the input space into a latent space.\nThen, the label predictor computes the labels of the\nunderlying examples. Simultaneously, the domain\nclassifier uses the latent space to predict the domain\nof the features (i.e., the source or target domain).\nTo obtain domain-invariant features, the opti-\nmization is two-fold. First, we minimize the pre-\ndiction loss concerning Gf\u2019s parameters \u03b8fand\nGy\u2019s parameters \u03b8y. Second, we maximize the do-\nmain classification loss until Gdcannot distinguish\nthe domains of the features. Formally, the loss func-\ntionL(see Eq. 1) depends on the prediction lossLybetween Gy\u2019s outputs and source labels, and the\ndomain adaptation loss Ldbetween Gd\u2019s outputs\nand domains di(i.e., hyperpartisan and fake news).\nThe trade-off between LyandLdis controlled by\n\u03bb. Note that we omitted the model\u2019s parameters for\nclarity.\nL=NsX\ni=1Ly(Gy(Gf(xi\ns)), yi\ns)\n\u2212\u03bbNX\ni=1Ld(Gd(Gf(xi)), di)(1)\nThe optimization problem associated with this\nformulation is described below:\n\u02c6\u03b8f,\u02c6\u03b8y= arg min\n\u03b8f,\u03b8yL(\u03b8f, \u03b8y,\u02c6\u03b8d) (2)\n\u02c6\u03b8d= arg max\n\u03b8dL(\u02c6\u03b8f,\u02c6\u03b8y, \u03b8d) (3)\nwhere the parameters with hat are fixed during the\noptimization step. This problem can be solved with\nan implementation trick, namely gradient reversal\nlayer (GRL) (Ganin and Lempitsky, 2015), which\nacts as the identity function during feed-forward\nand negates the gradients during back-propagation.\nThe GRL layer is inserted between the feature en-\ncoder and the domain discriminator.\nIn our setting, we use the RoBERTa\u2019s encoders\nfor feature extraction and fully connected layers for\nboth the label predictor and domain discriminator.\n3.3 Cluster Alignment with a Teacher\nAs an extension to UDA, Deng et al. (2019) ex-\nploited the class-conditional structure of the feature\nspace by cluster alignment in the teacher-student\nparadigm. A teacher model trained on the labeled\nsource examples estimates pseudo-labels for the\nunlabeled target dataset. To reduce the error am-\nplification caused by label estimation, the teacher\nmodel is built as an ensemble of previous student\nclassifiers. In addition, a student classifier mini-\nmizes the prediction loss Lyon the source exam-\nples in the supervised setting. The optimization\ninvolves minimizing both the prediction loss Ly\nand the sum of clustering losses Lc(i.e., for both\nthe source and the target domains) and the cluster-\nbase alignment loss La:\nL=Ly+\u03b1(Lc+La) (4)\nwhere the hyperparameter \u03b1controls the trade-off\nbetween the supervised and semi-supervised losses.\nFigure 1: (Left) The RoBERTa model in the UDA setting includes a label predictor and a domain discriminator.\n(Center) In the CAT method, the student and teacher use the RoBERTa model. (Right) In the CDCL setting, the\ncontrastive loss is applied between the RoBERTa features of an anchor and the source (s) / target (t) example.\nConsidering the labeled samples Xs=\n{xi\ns, yi\ns}Ns\ni=1, the unlabeled samples Xt={xi\nt}Nt\ni=1,\nthe feature extractor f(\u00b7), and the distance metric\ndbetween features, the total clustering loss is:\nLc(Xs, Xt) =Lc(Xs) +Lc(Xt) (5)\nwhere Lcis as follows for each X\u2217:\nLc(X\u2217) =1\n|X\u2217|2|X\u2217|X\ni=1|X\u2217|X\nj=1[\u03b4ijd(f(xi), f(xj))\n+ (1\u2212\u03b4ij) max(0 , m\u2212d(f(xi), f(xj)))](6)\nThe intuition is to enforce class-conditional\nstructure at the feature representation by group-\ning the classes into clusters, i.e., by minimizing the\ndistance between features xiandxjthat have the\nsame label when the indicator function \u03b4ij= 1,\nwhereas pushing different clusters away from at\nleast a margin mby maximizing the feature dis-\ntance when \u03b4ij= 0. The classifier trained on the\nsource features may not be able to differentiate be-\ntween the same class from different domains, and\ntherefore, an alignment loss Lais imposed between\nthe domains as follows:\nLa(Xs, Xt) =1\nKKX\nk=1||\u03bbs,k\u2212\u03bbt,k||2\n2(7)\nIn this case, given the number Kof classes to be\npredicted, and the samples X\u2217,kfrom either sourceor target whose labels are equal to k, the cluster\ncentroids \u03bb\u2217,kare computed using:\n\u03bb\u2217,k=1\n|X\u2217,k|X\nxi\u2217\u2208X\u2217,kf(xi\n\u2217) (8)\nThe loss Latries to match the source and target\nstatistics by aligning the clusters for each class k\nin the feature space. Additionally, the performance\ncan be further improved by aligning the marginal\ndistributions, i.e., adding a confidence threshold\nthat ignores the data points likely to be included in\nthe wrong class.\n3.4 Cross-Domain Contrastive Learning\nSelf-supervised contrastive learning (Chen et al.,\n2020) aims to learn representations such that, given\na pair of examples, closely related examples should\nbehave similarly, while dissimilar examples should\nstay far apart from each other. This can be achieved\nby employing various techniques such as data aug-\nmentation and custom losses (e.g., NT-Xent (Chen\net al., 2020), InfoNCE (Oord et al., 2018)). Since\nthere is no clear way to construct positive and neg-\native pairs in an unsupervised domain adaptation\nframework, Wang et al. (2022) argued that sam-\nples from the same category should be similar. In\ncontrast, samples from different categories should\nhave other feature representations, regardless of\nthe domain from which they come. Based on this\nhypothesis, they proposed the cross-domain con-\ntrastive (CDC) loss to reduce the domain shift be-\ntween source and target labels. We assume za\ntand\nzp\nsare the l2-normalized features for the anchor\nsample from the target domain xa\ntand the positive\nsample from the source domain xp\ns, respectively. In\nthis case, the loss function is described by:\nLt,a\nCDC=\u22121\n|Ps(\u02c6ya\nt)|X\np\u2208Ps(\u02c6ya\nt)logexp(za\nt\u00b7zp\ns/\u03c4)P\nj\u2208Isexp(za\nt\u00b7zj\ns/\u03c4)\n(9)\nwhere Ps(\u02c6ya\nt)denotes the set of positive samples\nfrom the source domain having the same label as\nthe anchor point, and Isis the set of all source\nsamples from the mini-batch. Similar to Eq. 9, we\ncompute Ls,a\nCDC, for which we consider the positive\nsamples from the target domain instead. The CDC\nloss with alignment at the feature level is1:\nLCDC =1\nNsNsX\na=1Ls,a\nCDC+1\nNtNtX\na=1Lt,a\nCDC(10)\nThe objective function is given by the sum of the\nprediction loss Lyand the loss LCDC scaled by \u03b3:\nL=Ly+\u03b3LCDC (11)\nWe generate pseudo-labels using the K-Means\nalgorithm since we require them when creating\npositive pairs. We initialize K-Means with the cen-\ntroids of the source domain and predict on the target\ndomain. The pseudo-labels are chosen to minimize\nthe similarity distance between the feature repre-\nsentation and the centroid. K-Means is performed\nat the beginning of each epoch.\n3.5 Cluster and Topic-Based Unsupervised\nDomain Adaptation\nWe propose an addition to the UDA approach, con-\nsidering the supervised setting (i.e., we have ac-\ncess to the labeled source dataset). First, we rep-\nresent the input text using TF-IDF or a pre-trained\nRoBERTa model. We employ a clustering/topic\nmodeling algorithm in this feature space to iden-\ntifykclusters or topics, which will be assigned as\ndomain labels. For clustering, we employ four al-\ngorithms, namely K-Means, K-Medoids, Gaussian\nMixture, and HDBSCAN. Also, we use four topic\nmodeling algorithms, namely LDA, NMF, LSA,\nand pLSA. The motivation is to compact the latent\nrepresentation, given estimates of latent domains\n1Note that we included the normalization terms compared\nto the original formulation.under a topic model (i.e., a dataset split). During\ntraining, it is minimized the loss given by Eq. 1\nwhile using the proposed domain labels. For the\ntarget examples, we do not include labels during\ntraining. We choose the number of clusters using\nthe elbow method2. After training on each pair\nof domain labels, the best-performing model is se-\nlected for the inference stage.\n4 Experimental Setup\n4.1 Datasets\nWe perform experiments on three datasets related to\nfake (i.e., ISOT and BuzzFeed) and hyperpartisan\n(i.e., BuzzFeed and Hyperpartisan (Kiesel et al.,\n2019)) news detection.\nThe ISOT fake news dataset contains news arti-\ncles collected from reuters.com, and other websites,\nwhich were validated by Politifact3. The dataset\ncomprises 44,898 articles, of which 21,417 contain\ntruthful information, and 23,481 are fake news. All\ncollected articles are related to politics and have at\nleast 200 characters.\nThe BuzzFeed dataset contains 1,627 articles in\nthree categories: mainstream, left-wing, and right-\nwing. The mainstream and hyperpartisan data are\nevenly distributed, and the length of the articles\nranges between 400 and 800 words. This dataset\nis annotated for both fake and hyperpartisan news\ndetection.\nThe Hyperpartisan dataset which contains hy-\nperpartisan news was released under the SemEval-\n2019 Task 4 shared task (Kiesel et al., 2019). The\ndataset was crawled from news publishers listed\nbyBuzzFeed4and Media Bias Fact Check5. From\nthese sources, 754,000 news articles were extracted\nand semi-automated labeled using distant super-\nvision (Mintz et al., 2009) at the publisher level,\nprovided in the HTML format. It was split into\n600,000 articles for training, 150,000 articles for\nvalidation, and 4,000 articles for testing. Half\nof the dataset consists of non-hyperpartisan arti-\ncles, and the other half is split equally among left-\nwing and right-wing articles. Since the authors\nalso released a smaller version of the dataset (645\nexamples for training and 628 examples for test-\ning), in what follows, we will refer to the larger\n2https://www.scikit-yb.org/en/latest/\napi/cluster/elbow.html\n3An organization that checks the veracity of the news.\n4https://github.com/BuzzFeedNews/2017-\n08-partisan-sites-and-facebook-pages\n5https://mediabiasfactcheck.com\ndataset as Hyperpartisan-L and the smaller dataset\nas Hyperpartisan-S.\n4.2 Data Preprocessing\nWe perform data cleaning on all three corpora, ig-\nnoring non-ASCII characters and removing HTML-\nspecific symbols and constructions that do not pro-\nvide any information about the actual content, such\nas multiple chains of dots in a line. BPE was uti-\nlized for tokenization, setting to output a maximum\nof 128 tokens per text sample.\nSince the ISOT and BuzzFeed datasets are not\nprovided with separate splits for validation and test-\ning, we use the following split: 70% for training,\n10% for validation, and 20% for testing. In addi-\ntion, due to limited computational resources and\nthe large size of the Hyperpartisan dataset, we se-\nlect a random 5% of the data from the training set\n(i.e., 30,000 examples) and 5% of the data for the\nvalidation set (i.e., 7,500 examples). Also, we use\nthe entire Hyperpartisan test set since it contains\nonly 4,000 examples.\n4.3 Hyperparameters\nWe utilize the pre-trained RoBERTa base version\n(123M parameters), which consists of a stack of 12\nTransformer blocks. For all experiments, the Adam\noptimizer (Kingma and Ba, 2015) with a linear\nscheduler is used with a warm-up (it is set with\n5% of the gradient steps) for the learning rate. The\nlearning rate varies among experiments, between\n1e\u22124and1e\u22125. We employ a dropout set between\n0.1 and 0.5. We also set the optimizer\u2019s weight\ndecay parameter to 1e\u22123, and clip the gradients\nbetween -1 and 1 to increase training stability and\nreduce overfitting.\n5 Results\nThere were conducted multiple experiments to eval-\nuate the impact of using various fine-tuned models\nfor RoBERTa. We also investigate the effects of\nfine-tuning the RoBERTa model on the downstream\ntask. Then, we analyze the impact of using a data\naugmentation technique (Xie et al., 2020) based on\nthe TF-IDF scores. In Appendix A.1, we present\nthe results of the GPT-2 data augmentation. Finally,\nwe use clustering and topic modeling algorithms to\nextract clusters and topics from the training set and\nperform domain adaptation. We present the results\nin terms of accuracy (Acc) and F1-score (F1).Table 1: Results obtained after fine-tuning and evaluat-\ning RoBERTa on each dataset.\nDataset Acc(%) F1(%)\nBuzzFeed 96.9 96.7\nISOT 99.8 99.7\nHyperpartisan-S 83.7 83.0\nHyperpartisan-L 62.1 69.0\nTable 2: Results for different fine-tuning strategies on\nthe Hyperpartisan-L dataset.\nModel Acc(%) F1(%)\nRoBERTa 62.1 69.0\nRoBERTa frozen 53.7 65.4\nRoBERTa fine-tuned first on BuzzFeed 62.3 68.0\nRoBERTa fine-tuned first on ISOT 63.0 70.0\n5.1 Baselines\nWe start with the most straightforward approach\nfor training a neural network. That is, we take\na pre-trained model on similar tasks and transfer\nsome of the acquired knowledge to the downstream\ntask via fine-tuning. The baseline model consists\nof the RoBERTa model followed by a stack of fully\nconnected layers. We employ two fully connected\nlayers, with 256 hidden units and two output neu-\nrons. The models are trained for 3 epochs, with a\nlearning rate of 1e\u22124and batch size between 32\nand 64.\nFirst, we evaluate the model on all four datasets\nfor baseline results. Table 1 presents the final re-\nsults obtained during experiments. We observe\nthat ISOT achieves the highest scores, followed\nby BuzzFeed and Hyperpartisan-S. We note that\nhumans annotated these datasets, whereas the\nHyperpartisan-L dataset was annotated with a semi-\nsupervised approach.\nBy comparing three fine-tuning methods (see\nTable 2), we observe that freezing the model\u2019s en-\ncoders yields poor performance. This increases the\nnumber of false positives and decreases the num-\nber of true negatives because of the domain shift\nbetween the datasets and training with fewer param-\neters. On the other hand, fine-tuning improves the\nresults since the model\u2019s parameters are adapted to\nthe new domain.\n5.2 Results for UDA\nWe consider the encoders from the RoBERTa\nmodel as feature generators. We also use a stack\nof fully connected layers, with 256 hidden neurons\nand two outputs for both the label predictor and the\nTable 3: Unsupervised domain adaptation between\nHyperpartisan-L and BuzzFeed datasets.\n\u03bb Source TargetSource Target\nAcc(%) F1(%) Acc(%) F1(%)\n0.1 Hyperpartisan-L BuzzFeed 61.5 67.7 85.4 86.4\n1 Hyperpartisan-L BuzzFeed 58.1 68.4 60.8 38.2\n5 Hyperpartisan-L BuzzFeed 50.0 2.5 54.0 3.8\n0.1 BuzzFeed Hyperpartisan-L 95.3 94.9 64.3 62.7\n1 BuzzFeed Hyperpartisan-L 96.5 96.6 50.0 66.5\n5 BuzzFeed Hyperpartisan-L 51.5 7.1 50.8 7.7\n0.1 BuzzFeed Hyperpartisan-L 94.4 94.5 56.7 64.1\nTable 4: Various linking positions of the GRL layer\nto the encoders of RoBERTa, on BuzzFeed (source) to\nHyperpartisan-L (target) adaptation.\nGRL\npos.Source Target\nAcc(%) F1(%) Acc(%) F1(%)\n4 95.9 95.2 62.1 61.7\n6 95.0 94.4 62.1 67.1\n10 91.3 89.1 60.9 64.1\n12 95.3 94.9 64.3 62.7\ndomain discriminator. The domain discriminator is\nlinked to the output of the RoBERTa encoder via a\ngradient reversal layer. We tested three values for\n\u03bb\u2208 {0.1,1,5}.\nFurthermore, we perform larger-to-smaller\nand smaller-to-larger dataset adaptations between\nHyperpartisan-L and BuzzFeed. The model is\ntrained for 3 epochs (i.e., the steps required to\npass through all examples from the larger dataset).\nThe batch size is set to 64, from which half are\nlabeled and the other half are unlabeled examples.\nThe results are shown in Table 3. We observe\nthat if \u03bbis set too large, the model does not learn\nthe data distribution but predicts only one class.\nConversely, UDA performs better when \u03bb= 0.1,\nachieving higher accuracy on the Hyperpartisan-L\ntarget dataset. This adaptation may have helped be-\ncause of the inherent similarities between domains\nand improved performance on out-of-distribution\npoints.\nMoreover, we employ different ways of linking\nthe GRL layer with the RoBERTa encoders. Since\nthe RoBERTa-base model uses 12 encoders, we\nutilized the 4th, 6th, and 10th, besides the previous\nexperiments. While the encoder returns a feature\nrepresentation for each element in the sequence,\nwe take the representation of the [CLS] token.\nTable 4 shows the results. The 12th layer performs\nbest, while similar performances are achieved using\nthe 4th or 6th layer. The results are supported by\nthe fact that more layers for the encoder mean more\nrepresentational power for the feature encoder that\nneeds to be adapted among domains.Table 5: Results for the CAT framework on BuzzFeed\n(source) to Hyperpartisan-L (target) adaptation.\n\u03bb \u03b1Source Target\nAcc(%) F1(%) Acc(%) F1(%)\n1 1 92.5 91.2 51.3 66.4\n1 0.1 94.7 93.8 57.9 62.6\n0.1 0.1 95.9 95.7 59.9 61.5\n0.1 0 96.5 96.4 58.7 64.3\n0 0.1 95.6 95.4 59.8 64.1\n0 0 93.7 92.7 58.9 62.5\n5.3 Results for CAT\nIn addition to the previous experimental setup, we\nset the parameter \u03b1\u2208 {0.1,1}for the clustering\nloss in the CAT configuration. We also consider a\nlower learning rate (i.e., 1e\u22125) to improve con-\nvergence. We consider an epoch is a complete pass\nthrough the smaller dataset to update the pseudo-\nlabels for the entire target domain using the teacher\nmodel. As such, we trained the models for 10-30\nepochs. We set the margin m= 2, the ensemble\nsize to 3, and the ensemble accumulation to 0.8.\nWe performed domain adaptation from Buz-\nzFeed to Hyperpartisan-L. The results are shown in\nTable 5. The model obtains over 90% accuracy on\nthe source domain and is bounded by 66.4% on the\ntarget domain. This approach generally achieves\na smaller accuracy than previous techniques, the\nbest score being when \u03bb=\u03b1= 0.1. Also, we\ncan observe that the difference between \u03bband\u03b1\naffects the performances. Analyzing the model pre-\ndictions, we notice that using smaller values for\n\u03bband\u03b1yields a high number of false positives,\nwhile larger values increase the number of false\nnegatives. Using \u03bb= 1and\u03b1= 0.1resulted in a\nbiased model towards mainstream examples.\n5.4 Results for CDCL\nFor the CDCL method, the experimental setup is\nsimilar to the one used for the CAT. We varied the\ntemperature \u03c4\u2208 {0.1,0.5,1}and the coefficient\n\u03b3\u2208 {0,0.1,1,5}. Table 6 provides the results of\nour analysis. We observe that both \u03c4and\u03b3affect\nthe performance. The best results were attained\nwhen \u03c4= 1, and\u03b3= 5, achieving 63.9% accuracy\non the target domain, while \u03c4= 0.5generates\nthe best values on the source dataset. It proves\nthatLCDC performs some regularization on the\nsource domain. We noticed that the models often\nproduce a high false positive rate, affecting the\nrecall more than the precision. In addition, training\nfor more epochs, the model starts overfitting on\nTable 6: Results for the CDCL framework on BuzzFeed\n(source) to Hyperpartisan-L (target) adaptation.\n\u03c4 \u03b3Source Target\nAcc(%) F1(%) Acc(%) F1(%)\n0.1 0 95.6 95.2 59.9 62.2\n0.1 0.1 91.3 90.1 63.3 64.9\n0.1 1 96.2 96.0 61.9 68.8\n0.1 5 96.2 96.0 62.6 67.9\n0.5 0 95.0 95.2 60.4 64.3\n0.5 0.1 95.3 95.7 57.1 67.8\n0.5 1 89.4 89.6 60.8 63.9\n0.5 5 96.5 96.4 63.4 66.5\n1 0 95.9 95.8 63.3 65.2\n1 0.1 95.9 95.8 61.6 68.6\n1 1 92.2 92.6 61.9 67.3\n1 5 95.6 95.4 63.9 69.2\nboth source and target domains while degrading\nthe performance of the validation set.\n5.5 Results for Text Augmentation Based on\nTF-IDF\nWe explore a data augmentation technique based on\nTF-IDF as proposed by Oord et al. (2018) for con-\nsistency training. Thus, we compute the TF-IDF\nscore for every token from the corpus and associate\nit with the probability of it being changed. The\nwords with the higher probability are replaced with\nnon-keywords from the vocabulary to avoid chang-\ning the meaning of the text. The TF-IDF-based\nword replacement depends on a hyperparameter\npthat controls the level of augmentation enabled\non the dataset. We vary pfor our experiments to\naugment the BuzzFeed dataset with multiple aug-\nmentation levels. Table 7 shows the results for all\ntraining configurations, where two or three values\nper augmentation type indicate that we applied each\nvalue of pand concatenated the augmented exam-\nples over the original dataset. Also, zero suggests\nthat only the unaltered dataset was used. Using\nmore augmentations (e.g., p\u2208 {0.1,0.2,0.3}) on\nthe CDCL and CAT frameworks yields better over-\nall results, while on UDA, using a much stronger\naugmentation (i.e., p= 0.5) leads to better results.\nOne problem with this data augmentation tech-\nnique is that it may alter the text in a way that is not\ncoherent anymore, specifically when many tokens\nare changed. The most frequent words may not\nalways have the same meaning, so their contextu-\nalized representation is affected. Since the context\ndefines the meaning of a word in language mod-\nels, this augmentation changes the representation,\nespecially on unlabelled data. Table 7 illustrates\nthe issue on the target dataset. However, on theTable 7: Results for the TF-IDF-based data aug-\nmentation. The source is BuzzFeed and the target is\nHyperpartisan-L.\npSource Target\nAcc(%) F1(%) Acc(%) F1(%)\nUDA\n0 94.0 93.4 59.1 64.5\n0.5 95.8 95.5 63.2 62.7\n0.1/0.2 94.7 94.5 57.3 61.5\n0.1/0.2/0.3 98.4 98.3 61.3 46.9\nCAT\n0 95.9 95.7 59.9 61.5\n0.5 93.0 92.7 60.5 65.2\n0.1/0.2 98.8 98.8 62.7 64.0\n0.1/0.2/0.3 98.2 98.1 60.7 64.7\nCDCL\n0 94.0 93.4 60.8 69.4\n0.5 95.1 94.8 63.2 69.0\n0.1/0.2 97.3 97.3 63.6 68.9\n0.1/0.2/0.3 98.8 98.8 64.4 69.4\nsource dataset, the performance is not affected but\ngenerally improved.\n5.6 Results for Cluster- and Topic-Based UDA\nIn the topic-based UDA approach, we follow the\nsame experimental setup as in classical UDA. For\ntraining, the only difference is that we train all\nmodels for 10 epochs. We explore both, the clus-\ntering on RoBERTa features (i.e., K-Means with\nEuclidean or cosine distance, K-Medoids, Gaussian\nMixture, and HDBSCAN) and the topic modeling\nalgorithms on TF-IDF features (i.e., LDA, NMF,\nLSA, and pLSA) to split the representation. We\nevaluate the experiments on the Hyperpartisan-L\ntest set and present the results in Table 8. Using\nclustering algorithms for domain labels provides\nthe best overall results compared to Table 3. The\nbest-performing models outperform the UDA ap-\nproach by over 3% in accuracy and are obtained\nwhen we adapted from a larger to a smaller split.\nIt is noteworthy that for the HDBSCAN, the clus-\nter 2 contains very few annotated examples (i.e.,\n332) compared with the other two (i.e., 17,092 and\n12,576), resulting in adaptation failure. When us-\ning the topic modeling, we see a degradation in\nperformance, especially in the case of NMF. Com-\npared with the RoBERTa baseline (see Table 2), the\nmodel achieves similar F1-scores.\n5.7 Feature Visualization\nWe use t-SNE (Van der Maaten and Hinton, 2008)\nto visualize the feature representation learned by\nthe best models we obtained for each category. In\nFigure 2, we present the plots for the baseline, the\nTable 8: Results for the cluster- and topic-based UDA, where 0, 1, and 2 identify cluster/topic assignments given by\nthe algorithm. The best score for each line is underlined, while bold indicates the best overall metrics.\nMethod0\u21921 1\u21920 2\u21920 0\u21922 1\u21922 2\u21921\nAcc(%) F1(%) Acc(%) F1(%) Acc(%) F1(%) Acc(%) F1(%) Acc(%) F1(%) Acc(%) F1(%)\nK-Means-euclidean 67.2 68.2 66.1 68.6 64.1 65.6 67.9 69.3 61.9 68.0 65.4 69.2\nK-Means-cosine 64.2 69.0 63.5 70.0 66.0 63.6 66.3 67.8 64.1 68.5 62.4 67.3\nK-Medoids 66.0 64.2 62.7 57.8 66.3 68.0 64.2 57.5 61.7 52.1 63.5 60.9\nGaussian Mixture 67.1 70.6 59.5 67.7 57.9 64.0 64.9 69.6 59.7 68.0 65.3 64.2\nHDBSCAN 65.1 68.9 62.5 63.4 50 0.0 60.0 55.6 62.2 66.0 50.0 0.0\nLDA 61.8 52.2 59.0 43.5 66.1 61.9 62.6 66.2 49.4 61.9 59.8 46.2\nNMF 63.3 53.3 59.9 55.7 56.0 58.1 54.9 36.3 59.8 57.0 60.5 45.4\nLSA 62.1 70.3 50.0 66.4 51.5 8.6 51.6 65.6 53.1 64.6 61.4 70.0\npLSA 61.6 68.7 50.0 1.4 57.1 66.1 60.1 66.2 60.2 54.8 62.4 67.6\n(a) Baseline\n (b) UDA\n(c) CAT\n (d) CDCL\nFigure 2: t-SNE visualizations of the feature represen-\ntations for the BuzzFeed (source) and Hyperpartisan-L\n(target) datasets. Blue \u2013 source (src) mainstream, orange\n\u2013 target (trg) mainstreams, green \u2013 source hyperpartisan,\nand red \u2013 target hyperpartisan. Best viewed in color.\nUDA, the CAT, and the CDCL. Using different\napproaches to domain adaptation may reduce the\ndomain gap in the feature space between the two\ndomains. Still, many examples cluster together far\napart from their counterparts. UDA obtains better\nrepresentations than the other methods. When con-\nsidering the topic-based adaptation (see Figure 3),\nwe notice a better separation when employing topic\nmodels. Also, we achieve poor separation among\nclasses for K-Means and K-Medoids.\n6 Conclusions\nIn this work, we addressed the problem of transfer-\nring knowledge from fake to hyperpartisan news de-\ntection. We employed three types of architectures\nbased on unsupervised training. We conducted\nmultiple experiments, showing the effects of the\nhyperparameters in the given configuration. All\nemployed methods manage to perform some do-\n(a) K-Means\n (b) K-Medoids\n(c) LDA\n (d) NMF\nFigure 3: t-SNE visualizations of the feature represen-\ntations when employing topic/clustering methods on the\nvalidation sets. Blue \u2013 source (src) mainstream, orange\n\u2013 target (trg) mainstreams, green \u2013 source hyperpartisan,\nand red \u2013 target hyperpartisan. Best viewed in color.\nmain adaptation. In particular, we showed that\nCDCL obtains the best results after applying data\naugmentation based on TF-IDF word replacement.\nIn contrast, CAT managed the poorest results. By\nanalyzing the t-SNE visualization, this model did\nnot learn a good feature representation, with a min-\nimal domain gap between the source and target\ndatasets. The low accuracy we hypothesize is due\nto a lack of data from the source domain, as we\nhave seen that data augmentation helped. For fu-\nture work, we aim to investigate our approaches on\nother fake news datasets.\nAcknowledgments\nThis research has been funded by the University\nPolitehnica of Bucharest through the PubArt pro-\ngram.\nReferences\nHadeer Ahmed, Issa Traore, and Sherif Saad. 2017. De-\ntection of online fake news using n-gram analysis and\nmachine learning techniques. In Intelligent, Secure,\nand Dependable Systems in Distributed and Cloud\nEnvironments: First International Conference, IS-\nDDC 2017, Vancouver, BC, Canada, October 26-28,\n2017, Proceedings 1 , pages 127\u2013138. Springer.\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence , volume 34, pages 7383\u20137390.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of machine\nLearning research , 3(Jan):993\u20131022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nRicardo JGB Campello, Davoud Moulavi, and J \u00a8org\nSander. 2013. Density-based clustering based on\nhierarchical density estimates. In Pacific-Asia confer-\nence on knowledge discovery and data mining , pages\n160\u2013172. Springer.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175 .\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning , pages\n1597\u20131607. PMLR.\nAnshika Choudhary and Anuja Arora. 2021. Linguistic\nfeature based learning model for fake news detection\nand classification. Expert Systems with Applications ,\n169:114171.\nScott Deerwester, Susan T Dumais, George W Furnas,\nThomas K Landauer, and Richard Harshman. 1990.\nIndexing by latent semantic analysis. Journal of the\nAmerican society for information science , 41(6):391\u2013\n407.Zhijie Deng, Yucen Luo, and Jun Zhu. 2019. Cluster\nalignment with a teacher for unsupervised domain\nadaptation. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision , pages 9944\u2013\n9953.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pages 4171\u2013\n4186.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 889\u2013898, Melbourne, Australia. Association\nfor Computational Linguistics.\nChris Fraley and Adrian E Raftery. 2002. Model-based\nclustering, discriminant analysis, and density estima-\ntion. Journal of the American statistical Association ,\n97(458):611\u2013631.\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam\nsearch strategies for neural machine translation. In\nProceedings of the First Workshop on Neural Ma-\nchine Translation , pages 56\u201360, Vancouver. Associa-\ntion for Computational Linguistics.\nYaroslav Ganin and Victor Lempitsky. 2015. Unsu-\npervised domain adaptation by backpropagation. In\nInternational conference on machine learning , pages\n1180\u20131189. PMLR.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, Fran c \u00b8ois Lavi-\nolette, Mario Marchand, and Victor Lempitsky. 2016.\nDomain-adversarial training of neural networks. The\njournal of machine learning research , 17(1):2096\u2013\n2030.\nAbdullah Hamid, Nasrullah Shiekh, Naina Said, Kashif\nAhmad, Asma Gul, Laiq Hassan, and Ala Al-Fuqaha.\n2020. Fake news detection in social media using\ngraph neural networks and nlp techniques: A covid-\n19 use-case. arXiv preprint arXiv:2012.07517 .\nKazuaki Hanawa, Shota Sasaki, Hiroki Ouchi, Jun\nSuzuki, and Kentaro Inui. 2019. The sally smedley\nhyperpartisan news detector at SemEval-2019 task 4.\nInProceedings of the 13th International Workshop on\nSemantic Evaluation , pages 1057\u20131061, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nThomas Hofmann. 1999. Probabilistic latent semantic\nanalysis. In Proceedings of the Fifteenth Conference\non Uncertainty in Artificial Intelligence , UAI\u201999,\npage 289\u2013296, San Francisco, CA, USA. Morgan\nKaufmann Publishers Inc.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net.\nGerald Ki Wei Huang and Jun Choi Lee. 2019. Hyper-\npartisan news and articles detection using bert and\nelmo. In 2019 International Conference on Com-\nputer and Drone Applications (IConDA) , pages 29\u2013\n32. IEEE.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by re-\nducing internal covariate shift. In International con-\nference on machine learning , pages 448\u2013456. pmlr.\nYe Jiang, Johann Petrak, Xingyi Song, Kalina\nBontcheva, and Diana Maynard. 2019. Team bertha\nvon suttner at SemEval-2019 task 4: Hyperpartisan\nnews detection using ELMo sentence representation\nconvolutional network. In Proceedings of the 13th In-\nternational Workshop on Semantic Evaluation , pages\n840\u2013844, Minneapolis, Minnesota, USA. Associa-\ntion for Computational Linguistics.\nHeejung Jwa, Dongsuk Oh, Kinam Park, Jang Mook\nKang, and Heuiseok Lim. 2019. exbake: Automatic\nfake news detection model based on bidirectional\nencoder representations from transformers (bert). Ap-\nplied Sciences , 9(19):4062.\nRohit Kumar Kaliyar, Anurag Goswami, and Pratik\nNarang. 2021. Fakebert: Fake news detection in so-\ncial media with a bert-based deep learning approach.\nMultimedia tools and applications , 80(8):11765\u2013\n11788.\nLeonard Kaufmann. 1987. Clustering by means of\nmedoids. In Proc. Statistical Data Analysis Based\non the L1 Norm Conference, Neuchatel, 1987 , pages\n405\u2013416.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 task 4: Hyperpartisan news detection. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation , pages 829\u2013839, Minneapolis,\nMinnesota, USA. Association for Computational Lin-\nguistics.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , pages 1746\u20131751.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings .\nBoshko Koloski, Timen Stepi \u02c7snik Perdih, Marko\nRobnik- \u02c7Sikonja, Senja Pollak, and Bla \u02c7z\u02c7Skrlj. 2022.Knowledge graph informed fake news classification\nvia heterogeneous representation ensembles. Neuro-\ncomputing , 496:208\u2013226.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. In Proceedings of the 2nd Workshop\non Life-long Learning for Spoken Language Systems ,\npages 18\u201326.\nDaniel D Lee and H Sebastian Seung. 1999. Learning\nthe parts of objects by non-negative matrix factoriza-\ntion. Nature , 401(6755):788\u2013791.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys , 55(9):1\u201335.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nS. Lloyd. 1982. Least squares quantization in\npcm. IEEE Transactions on Information Theory ,\n28(2):129\u2013137.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research , 9(11).\nMike Mintz, Steven Bills, Rion Snow, and Daniel Ju-\nrafsky. 2009. Distant supervision for relation ex-\ntraction without labeled data. In Proceedings of the\nJoint Conference of the 47th Annual Meeting of the\nACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP , pages\n1003\u20131011, Suntec, Singapore. Association for Com-\nputational Linguistics.\nFederico Monti, Fabrizio Frasca, Davide Eynard, Da-\nmon Mannion, and Michael M Bronstein. 2019. Fake\nnews detection on social media using geometric deep\nlearning. arXiv preprint arXiv:1902.06673 .\nAhmadreza Mosallanezhad, Mansooreh Karami, Kai\nShu, Michelle V Mancenido, and Huan Liu. 2022.\nDomain adaptive fake news detection via reinforce-\nment learning. In Proceedings of the ACM Web Con-\nference 2022 , pages 3632\u20133640.\nMihai Alexandru Niculescu, Stefan Ruseti, and Mi-\nhai Dascalu. 2022. Rosummary: Control tokens\nfor romanian news summarization. Algorithms ,\n15(12):472.\nCristian Onose, Claudiu-Marcel Nedelcu, Dumitru-\nClementin Cercel, and Stefan Trausan-Matu. 2019.\nA hierarchical attention network for bots and gender\nprofiling. In CLEF (Working Notes) .\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748 .\nAndrei Paraschiv, George-Eduard Zaharia, Dumitru-\nClementin Cercel, and Mihai Dascalu. 2021. Graph\nconvolutional networks applied to fakenews: corona\nvirus and 5g conspiracy. UPB Scientific Bulletin,\nSeries C: Electrical Engineering , 83(2):71\u201382.\nVer\u00b4onica P \u00b4erez-Rosas, Bennett Kleinberg, Alexandra\nLefevre, and Rada Mihalcea. 2018. Automatic de-\ntection of fake news. In Proceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 3391\u20133401.\nMatthew E Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) ,\npages 43\u201354.\nMartin Potthast, Johannes Kiesel, Kevin Reinartz, Janek\nBevendorff, and Benno Stein. 2018. A stylometric\ninquiry into hyperpartisan and fake news. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 231\u2013240.\nMartin Potthast, Sebastian K \u00a8opsel, Benno Stein, and\nMatthias Hagen. 2016. Clickbait detection. In Ad-\nvances in Information Retrieval: 38th European Con-\nference on IR Research, ECIR 2016, Padua, Italy,\nMarch 20\u201323, 2016. Proceedings 38 , pages 810\u2013817.\nSpringer.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nMaria Rae. 2021. Hyperpartisan news: Rethinking the\nmedia for populist politics. New Media & Society ,\n23(5):1117\u20131132.\nJulio CS Reis, Andr \u00b4e Correia, Fabr \u00b4\u0131cio Murai, Adriano\nVeloso, and Fabr \u00b4\u0131cio Benevenuto. 2019. Supervised\nlearning for fake news detection. IEEE Intelligent\nSystems , 34(2):76\u201381.\nRobert M Ross, David G Rand, and Gordon Pennycook.\n2021. Beyond \u201cfake news\u201d: Analytic thinking and\nthe detection of false and hyperpartisan news head-\nlines. Judgment and Decision making , 16(2):484\u2013\n504.\nSomya Ranjan Sahoo and Brij B Gupta. 2021. Multi-\nple features based approach for automatic fake news\ndetection on social networks using deep learning. Ap-\nplied Soft Computing , 100:106983.\nGerard Salton, Anita Wong, and Chung-Shu Yang. 1975.\nA vector space model for automatic indexing. Com-\nmunications of the ACM , 18(11):613\u2013620.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909 .Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dong-\nwon Lee, and Huan Liu. 2020. Fakenewsnet: A data\nrepository with news content, social context, and spa-\ntiotemporal information for studying fake news on\nsocial media. Big Data , 8(3):171\u2013188.\nKai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and\nHuan Liu. 2017. Fake news detection on social me-\ndia: A data mining perspective. ACM SIGKDD ex-\nplorations newsletter , 19(1):22\u201336.\nKai Shu, Suhang Wang, and Huan Liu. 2019. Beyond\nnews contents: The role of social context for fake\nnews detection. In Proceedings of the twelfth ACM\ninternational conference on web search and data\nmining , pages 312\u2013320.\nVertika Srivastava, Ankita Gupta, Divya Prakash,\nSudeep Kumar Sahoo, RR Rohit, and Yeon Hyang\nKim. 2019. Vernon-fenwick at semeval-2019 task\n4: hyperpartisan news detection using lexical and\nsemantic features. In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation , pages\n1078\u20131082.\nMateusz Szczepa \u00b4nski, Marek Pawlicki, Rafa\u0142 Kozik,\nand Micha\u0142 Chora \u00b4s. 2021. New explainability\nmethod for bert-based model in fake news detection.\nScientific reports , 11(1):1\u201313.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems , 30.\nRui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen,\nGuo-Jun Qi, and Yu-Gang Jiang. 2022. Cross-\ndomain contrastive learning for unsupervised domain\nadaptation. IEEE Transactions on Multimedia .\nYaqing Wang, Fenglong Ma, Zhiwei Jin, Ye Yuan,\nGuangxu Xun, Kishlay Jha, Lu Su, and Jing Gao.\n2018. Eann: Event adversarial neural networks for\nmulti-modal fake news detection. In Proceedings\nof the 24th acm sigkdd international conference on\nknowledge discovery & data mining , pages 849\u2013857.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies , pages 2300\u20132344.\nLiang Wu and Huan Liu. 2018. Tracing fake-news\nfootprints: Characterizing social media messages by\nhow they propagate. In Proceedings of the eleventh\nACM international conference on Web Search and\nData Mining , pages 637\u2013645.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. Advances in Neural\nInformation Processing Systems , 33:6256\u20136268.\nTong Zhang, Di Wang, Huanhuan Chen, Zhiwei Zeng,\nWei Guo, Chunyan Miao, and Lizhen Cui. 2020.\nBdann: Bert-based domain adaptation neural net-\nwork for multi-modal fake news detection. In 2020\ninternational joint conference on neural networks\n(IJCNN) , pages 1\u20138. IEEE.\nXichen Zhang and Ali A Ghorbani. 2020. An overview\nof online fake news: Characterization, detection, and\ndiscussion. Information Processing & Management ,\n57(2):102025.\nXinyi Zhou and Reza Zafarani. 2020. A survey of fake\nnews: Fundamental theories, detection methods, and\nopportunities. ACM Computing Surveys (CSUR) ,\n53(5):1\u201340.\nA Appendix\nA.1 Results for Text Augmentation Based on\nGPT-2\nObserving the improvements obtained using TF-\nIDF augmentation, we consider text generation\nan alternative. Therefore, we employ the GPT-\n2 model (Radford et al., 2019) to conditionally\ngenerate new examples given the news types (i.e.,\nleft-wing, right-wing, and mainstream). We fol-\nlow an approach similar to the LAMBADA method\nproposed by Anaby-Tavor et al. (2020). Therefore,\nwe fine-tune the GPT-2 base model on the hyper-\npartisan Buzzfeed dataset to generate new samples.\nInspired by other works (Brown et al., 2020; Liu\net al., 2023; Niculescu et al., 2022), we build the\npre-training dataset using, for each sample, the fol-\nlowing prompt:\nNews type : <LABEL>\nText : <TEXT>\n<|endoftext|>\nwhere <LABEL> isleft,right , or main-\nstream ,<TEXT> is the news content, and\n<|endoftext|> is the end token of the text.\nSince we use a relatively small context during\nexperiments (i.e., 128 tokens), we do not require\nthe auto-regressive model to learn to generate\nlong samples, but rather more variation within the\ngenerated samples. To achieve this, we split each\ntext into sentences and group every three sentences\ninto one example under the same label.\nAs suggested by Kumar et al. (2020), dur-\ning data generation, we iterate over each sample\nfrom the training set and prompt the model with\nNews type: <LABEL> Text: followed by\nthe first Ttokens from each sample. Because the\nmodel may generate text that is not correlated with\nthe label (i.e., either the model ignores the prompt\nlabel (Webson and Pavlick, 2022), or there is not\nenough data for the model to learn a clear distinc-\ntion), we use the RoBERTa baseline model fine-\ntuned on the Buzzfeed dataset to filter the samples,\nignoring those that do not match the model\u2019s pre-\ndiction.\nText generation quality depends on the decoding\nstrategy; thus, we explore multiple approaches.\nGreedy decoding. The most trivial and fastest\nway of synthesizing text is to consider the token\nwith the highest probability. Albeit simple, it hasthe disadvantage of generating repetitive and miss-\ning higher probability words behind lower proba-\nbility ones.\nBeam search. Beam search (Freitag and Al-\nOnaizan, 2017) seeks to solve the low probability\nissue from the greedy decoding by choosing the\nhighest probability sequence within a number of\nbeams. This method generally yields to higher\nprobability sequence than greedy decoding. During\nexperiments, we set the number of beams to 5.\nTop-k. Using the top-k decoding (Fan et al.,\n2018), we consider only the highest knext tokens\nfrom the probability distribution over possible next\ntokens. This simple yet effective method produces\nmore human-like text than previous approaches. In\nour experiments, we consider k= 30 tokens.\nTop-p nucleus sampling. Introduced by Holtz-\nman et al. (2020), the top-p nucleus sampling is an\nextension over top-k. We choose the tokens from\nthe smallest subset whose cumulative probability\nis at least pinstead of choosing from the top k\nprobabilities. For experiments, we set p= 99% .\nTo generate more samples, we repeat the pro-\ncedure while setting T\u2208 {3,5,10}. The results\nare shown in Table 9. CDCL obtains the highest\nscores on the source and target datasets using top-p\nand greedy decoding, respectively. On the source\ndataset, the accuracy reaches 97.8% and the F1-\nscore tops at 97.7%, while on the target dataset,\nthe best accuracy is 64.4% and F1-score is 70.4%.\nCompared with the TF-IDF text augmentation, the\nGPT-2 augmentation produces a higher best F1-\nscore by 1% on the target test set, and achieves\nlower scores on the source test set by 1%. In ad-\ndition, we notice that the performance improves\nwhen adding more data, especially on the source\ndataset, where we see an average improvement of\n0.6% and 0.8% for accuracy and F1-score, respec-\ntively. On average, greedy decoding improves the\ntarget F1-score (i.e., 68.0 \u00b11.5%) while the low-\nest average is obtained by top-p (i.e., 65.7 \u00b13.5%).\nWe notice a small improvement in favor of top-p\ncompared with top-k on the source domain, but the\ntarget domain does not benefit from it in our case.\nTable 9: Results for the text augmentation using GPT-2. The source is BuzzFeed and the target is Hyperpartisan-L.\nDecoding\nStrategyTUDA CAT CDCL\nSource Target Source Target Source Target\nAcc(%) F1(%) Acc(%) F1(%) Acc(%) F1(%) Acc(%) F1(%) Acc(%) F1(%) Acc(%) F1(%)\nGreedy decoding3 96.0 95.6 62.5 68.3 96.6 95.9 63.7 65.9 97.2 97.2 64.4 70.4\n3/5 96.0 95.6 60.1 69.2 96.6 95.9 63.9 66.6 96.3 96.3 61.7 68.6\n3/5/10 96.3 96.1 55.4 67.3 95.0 94.1 63.2 66.5 97.2 97.2 62.6 68.8\nBeam search3 95.7 95.3 63.4 68.2 94.4 93.1 63.5 64.1 94.7 94.5 64.2 68.1\n3/5 95.7 95.3 57.1 68.4 94.4 93.1 64.2 63.4 96.6 96.6 61.5 68.0\n3/5/10 97.8 97.7 62.1 68.9 96.3 95.6 64.4 66.1 96.9 96.9 60.7 66.2\nTop-k3 94.7 94.2 62.9 65.4 96.3 96.2 62.6 66.7 96.0 95.9 61.6 68.3\n3/5 95.0 94.7 61.7 68.6 96.9 96.8 63.8 65.9 96.9 96.8 60.7 66.7\n3/5/10 96.3 96.0 60.0 68.5 97.2 97.2 63.6 68.4 96.6 96.5 61.7 69.0\nTop-p3 95.7 95.3 61.5 67.1 96.9 96.8 63.3 61.4 97.2 97.1 63.6 69.1\n3/5 95.0 94.7 62.1 67.1 96.3 96.1 62.6 62.9 97.8 97.7 62.3 68.1\n3/5/10 95.7 95.3 61.4 68.3 96.3 96.2 61.5 59.3 97.5 97.5 62.5 67.9", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "From fake to hyperpartisan news detection using domain adaptation", "author": ["RA Sm\u0103du", "SV Echim", "DC Cercel", "I Marin"], "pub_year": "2023", "venue": "arXiv preprint arXiv \u2026", "abstract": "Unsupervised Domain Adaptation (UDA) is a popular technique that aims to reduce the  domain shift between two data distributions. It was successfully applied in computer vision and"}, "filled": false, "gsrank": 575, "pub_url": "https://arxiv.org/abs/2308.02185", "author_id": ["YiICQBYAAAAJ", "9Vex0zcAAAAJ", "jdsY1HEAAAAJ", "AdBDtNUAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:9alnEXO4xx8J:scholar.google.com/&output=cite&scirp=574&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D570%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=9alnEXO4xx8J&ei=brWsaODUBsDZieoPqdqh8QU&json=", "num_citations": 4, "citedby_url": "/scholar?cites=2290001739893942773&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:9alnEXO4xx8J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2308.02185"}}, {"title": "Harnessing Personalization Methods to Identify and Predict Unreliable Information Spreader Behavior", "year": "2024", "pdf_data": "Proceedings of the The 8th Workshop on Online Abuse and Harms (WOAH) , pages 146\u2013158\nJune 20, 2024 \u00a92024 Association for Computational Linguistics\nHarnessing Personalization Methods to\nIdentify and Predict Unreliable Information Spreader Behavior\nShaina Ashraf, Fabio Gruschka, Lucie Flek, Charles Welch\nConversational AI and Social Analytics (CAISA) Lab, University of Bonn\n{sashraf,flek,cfwelch}@bit.uni-bonn.de\nAbstract\nStudies on detecting and understanding the\nspread of unreliable news on social media\nhave identified key characteristic differences\nbetween reliable and unreliable posts. These\ndifferences in language use also vary in expres-\nsion across individuals, making it important\nto consider personal factors in unreliable news\ndetection. The application of personalization\nmethods for this has been made possible by\nrecent publication of datasets with user histo-\nries, though this area is still largely unexplored.\nIn this paper we present approaches to repre-\nsent social media users in order to improve per-\nformance on three tasks: (1) classification of\nunreliable news posts, (2) classification of un-\nreliable news spreaders, and, (3) prediction of\nthe spread of unreliable news. We compare the\nUser2Vec method from previous work to two\nother approaches; a learnable user embedding\nlayer trained with the downstream task, and a\nrepresentation derived from an authorship at-\ntribution classifier. We demonstrate that the\nimplemented strategies substantially improve\nclassification performance over state-of-the-art\nand provide initial results on the task of unreli-\nable news prediction.\n1 Introduction\nThe distribution of information and news over the\ninternet has enabled the uncontrolled spread of un-\nreliable news and calls for the development of new\nsocial norms of careful information evaluation and\nsharing. Algorithms decide the newsfeed for their\nusers and the widespread propagation of unreliable\nnews has led to the need of automated means of\ndetecting such information. Much research has ad-\ndressed this issue with a variety of corpora contain-\ning different types of unreliable news, however few\ncorpora exist which contain a longitudinal com-\nponent of the individuals who spread unreliable\nnews.\nStudies have analyzed the language used whenunreliable news is spread, finding differences in so-\ncial and self-referencing words, denial, complaints,\ngeneralizing terms, lower cognitive complexity,\nless exclusive words, and more negative emotion\nand action words (Sharma et al., 2019; de Oliveira\net al., 2021). Naturally, the way these expressions\nare formed varies across individuals, making it im-\nportant to model users to improve detection. Initial\nwork has begun to apply such methods, though the\napplication of personalization methods for this task\nis still largely unexplored (Sakketou et al., 2022;\nMu and Aletras, 2020).\nIn this work, we show that unreliable news\ncan be more accurately detected when using per-\nsonalization. Personalization has different mean-\nings across literature in natural language process-\ning (Flek, 2020) but in this work it refers to the\nprocess of building personalized representations\nof users in order to better model their behaviors.\nOur contributions are (1) state-of-the-art results on\nthe FACTOID and Twitter datasets for detecting\nunreliable news spreaders by improving user em-\nbeddings, (2) an exploration of the task of predict-\ning when unreliable news will be spread, showing\nimprovements over the best model from previous\nwork, and (3) a comparison of the performance of\nrecent personalization methods for both tasks.\n2 Related Work\nPrevious work uses neural methods to combine text-\nbased features, such as those from statements re-\nlated to news data Karimi et al. (2018). Liu and Wu\n(2018) use RNN and CNN-based methods to build\npropagation paths for detecting misinformation at\nthe early stages of propagation. Shu et al. (2019)\npropose a tri-relationship embedding framework to\nmodel relationships among publishers, news sto-\nries, and social media users for fake news detection.\nKaradzhov et al. (2017) introduced a framework\nfor fully-automatic fact checking using external146\nsources. They use a deep neural network with\nLSTM text encoding, semantic kernels and task-\nspecific embeddings that are combined to encode\na claim together with portions of possibly relevant\ntext from the web. Cui et al. (2019) propose an\nexplainable fake news detection system, DEFEND,\nwhich considers users\u2019 comments to explain if news\nis fake or real. Nguyen et al. (2020) propose a fake\nnews detection method that uses a graph learning\nframework to represent social contexts. Ghanem\net al. (2021) propose FakeFlow model, to enhance\nfake news detection by analyzing the flow of affec-\ntive information, such as emotions, sentiment, and\nhyperbolic language, within texts. By segmenting\ninput texts into smaller units, FakeFlow effectively\nmodels the interactions between topical and affec-\ntive terms, thereby improving its ability to identify\nfake news articles. Duan et al. (2020) extracted\nlinguistic and sentiment features from users\u2019 tweet.\nAlso the presence of emojis, hashtags and politi-\ncal bias has been taken into account for prediction.\n(Khilji et al., 2023) captured contextual informa-\ntion of user by exploring personalization methods\nbased on user metadata and credibility features for\ndebunking misinformation\nResearchers are also examining cognitive fac-\ntors influencing people\u2019s ability to distinguish fake\nnews (Pennycook and Rand, 2019). Data-driven\nstudies analyzing bots\u2019 participation in social me-\ndia discussion (Howard and Kollanyi, 2016), user\nreactions to reliable/unreliable news posts (Glenski\net al., 2018a), and demographic characteristics of\nusers propagating unreliable news sources (Glenski\net al., 2018b), are also integral to our understanding\nof the problem space.\nIn the exploration of penalization techniques for\nthe identification and prediction of misinformation\nspreaders, the work of (Plepi et al., 2023; Plepi and\nFlek, 2021) presents the importance of incorpo-\nrating user-specific context alongside conversation\ntext and have achieved significant results in both\ntheir sarcasm detection and perception classifica-\ntion tasks. (Salemi et al., 2023) also showcases the\nsignificant benefits of integration personalization\ntechniques into large language models through ex-\ntensive experimentation, including zero-shot and\nfine-tuned setups. Similarly, Lian et al. (2022) pro-\nposes an innovative incremental user embedding\nmodel that dynamically integrates recent user inter-\nactions into accumulated history vectors, utilizing\na transformer encoder for personalized text classifi-\ncation.Sakketou et al. (2022) introduced the misinfor-\nmation spreader dataset, FACTOID, that captures\nlong-term context of users\u2019 historical posts. They\nprovide initial findings on the dataset, which serve\nas a baseline for our experiments. The user his-\ntories allow us to address a new temporal task of\npredicting when someone will spread misinforma-\ntion. These histories are categorized across several\ncontentious topics, offering a comprehensive view\nof misinformation spread on Reddit. These cate-\ngories include general political debate, SARS-CoV-\n2 (COVID-19), gender rights, climate change, vac-\ncinations, abortion, gun rights, and debates about\n5G technology. Each category encapsulates dis-\ncussions from multiple subreddits, encompassing a\nvariety of stances and biases. The dataset\u2019s breadth\nacross these topics allows for a broader understand-\ning of misinformation trends and the development\nof strategies to anticipate.\nMu and Aletras (2020) predict, using only lan-\nguage information, whether a social media user\nwill propagate news items from unreliable or re-\nliable sources before they share any news items.\nUnreliable users have a history of sharing content\nfrom unreliable sources at least three times, while\nreliable users only share content from trustworthy\nsources. They define a binary classification task\nand train a machine learning model on a dataset of\nuser histories leading up to their first news repost,\nlabeled as either reliable or unreliable. Compara-\ntively, our study expands on this approach. While\nthey use data up until the first news item is shared,\nour work includes news items within a user\u2019s his-\ntory. We compare their best performing method to\nours, as described in \u00a73.4.\n3 Methodology\nIn this section, we discuss the approaches for the\ndifferent setups for personalized representations\nin our work. We use static word representations\nfrom GloVe pretrained on the respective dataset as\ninput for the most of our methods. To facilitate\ncomparisons with previous work, we also explored\nWord2Vec representations that were pretrained us-\ning both datasets. This allowed us to investigate\nwhether our results benefit from leveraging global\nword-word co-occurrence statistics and the linear\nsubstructures within the word vector space. With\nthese word representations we are able to learn per-\nsonalized user embeddings. We further discuss the\ntask setup and definitions.147\n3.1 Definitions\nIn the Twitter dataset, users are classified as reli-\nable orunreliable based on their sharing habits.\nMu and Aletras (2020) define unreliable sources to\nbe propaganda, clickbait, conspiracy theories, or\nsatire. In the FACTOID dataset, misinformation is\ndefined to encompass various forms of politically\noriented false or misleading news. This includes\nunintentionally misleading news, deliberately de-\nceptive disinformation, politically skewed hyper-\npartisan news, and humorously false satirical news\n(Sakketou et al., 2022).\nRuffo et al. (2023) provide a detailed descrip-\ntion and taxonomy of information types. The two\ndatasets we study both cover misinformation, dis-\ninformation, as the news may be intentionally or\nunintentionally spread, as well as malinformation,\nwhich includes things like propaganda and is spread\nwith a malicious intent. We adopt the term unreli-\nable to refer to these types of information propa-\ngated by online users.\n3.2 Task Definitions\nWe address three tasks, the first of which classifies\nusers, and two that classify individual posts, as\nvisualized in Figure 1.\nUnreliable News Spreader Detection We clas-\nsify if a given user is a spreader of unreliable news\nor not. Each user uiis associated with a posting\nhistory Hi, as in (Sakketou et al., 2022).\nUnreliable News Post Classification For the\nclassification of unreliable news posts, we want\nto predict yi\nj\u2208 {unreliable ,information }with the\npretrained embeddings Ei\njand the post history.\nUnreliable News Post Prediction For the predic-\ntion of unreliable news posts, we want to predict\nyi\nj\u2208 {unreliable ,information }only with the pre-\ntrained or task embeddings Ei\nj.\n3.3 Splitting User Data\nWhen we are classifying users as unreliable news\nspreaders, we use all data for that user, as in previ-\nous work. However, when we are classifying posts,\nwe need to use only posts that precede a post that\nwe want to classify. To do this, we split users into\nartificial users at points in time delimited by the\nnumber of preceding posts and experiment with\ndifferent limits to the number of preceding posts.\nWe partition the post history of each user\nuiinto chunks of size Xand create an artifi-\npost 1 \n\u2026\npost n-1 \npost n Prediction Classification \nBERT Encoding SBERT GloVe \nWord2Vec User2Vec \nAA\nTask\nT-BERT \nFusion Layer \nand Detection Post History Personalized \nEncodings Figure 1: Visualization of task setup for prediction and\nclassification tasks. The fusion and detection box repre-\nsents a final layer of our neural model, which assigns a\nlabel corresponding to the task type.\ncial user ai\njfor each chunk. The j-th artificial\nuser for real user iis defined as ai\nj\u2208 A =\n{a1\n1, . . . , a1\nM1, . . . , aN\n1, . . . , aN\nMN}where Mi=\n\u2308Li\nX\u2309represents the number of artificial users cre-\nated, and each user ui, with a length of post history\ndenoted by L, is split into segments of size X.\nFor each post history chunk, hi\nj, we take the\nfirstX\u22121posts and reserve the label of the X-\nth post as classification target. After that we drop\nallai\njwith|hi\nj|<20to compute the initial user\nrepresentation for Ei\njbased on their corresponding\nhistorical posts.\n3.4 User Representations\nUser2Vec Amir et al. (2016) presented User2Vec,\nwhich computes user embeddings from a corpus\nof their text. For the unreliable news spreader\napproach we calculate the embeddings Ei\u2208Rd\nof user uibased on their corresponding historical\npostsHi. Computing the embeddings Ei\njrequires\npretrained word embeddings, which we compute\nboth with word2vec and GloVe (Pennington et al.,\n2014; Mikolov et al., 2013).\nTask Embeddings This approach uses an em-\nbedding layer initialized with Xavier initializa-\ntion (Glorot and Bengio, 2010), which takes in a\nuser ID and converts it into a vector representation\nin the forward pass. It is updated during training, so\nit is expected to encode signals of misinformation\nspreaders.\nAuthorship Attribution Much previous work\nhas addressed authorship attribution (AA), the task\nof classifying, from a predetermined set of authors,\nwhich author wrote a given text (Stamatatos, 2009).\nRecent personalization work has looked into de-\nriving user representations from authorship attri-\nbution classifiers (Plepi et al., 2022a; Welch et al.,\n2022). We use SBERT to encode all posts (Reimers148\nand Gurevych, 2019) and use the resulting vectors\nfor classification by passing them through a feed-\nforward layer with input size 768. We calculate\nperformance on the validation set with the embed-\ndings before the classification layer ( d= 400 ) for\neach post for a user and average these to get the\nresulting AA embedding. This is in contrast to pre-\nviously mentioned methods that use the distribution\nof predictions or probabilities, which have a dimen-\nsion size equal to the number of users. This model\nachieves an accuracy of 1.5% which is 170x better\nthan chance for the FACTOID dataset and 0.5% on\nthe Twitter dataset (175x better than chance).\nCombined We perform ablations using each\ncombination of two of the above methods, and for\nusing all three at the same time.\nT-BERT Mu and Aletras (2020) presented a trun-\ncated version of the BERT (T-BERT) which takes\ninitial 512 words pieces from the text of each user\nas input. We also followed the same approach in\nall three of our tasks. For post classification and\nprediction tasks, we computed user contextualized\nT-BERT embeddings by taking the recent 512 to-\nkens from each user and concatenate them with\neach post before passing to model.\n4 Datasets\nOur study leverages two pre-existing datasets,\nFACTOID (Sakketou et al., 2022) and a Twitter\ndataset (Mu and Aletras, 2020). Initially, we con-\nsidered other datasets, including CMU-MisCov19\n(Memon and Carley, 2020), and data from the PAN\nshared tasks (Rangel et al., 2020), however they\nwere not suitable for our experimentation as they\nonly provide Tweet IDs or labels for authors not\nfor tweets and some have missing information for\nusers, lacking content for the user personalization\ntechniques.\nFACTOID consists of 4,150 users with 3.4M\nposts. We use the balanced user split from their pa-\nper, which consists of 1,086 unreliable news spread-\ners and an equal amount of real information spread-\ners for 2,172 in total. A user is annotated as a\nunreliable news spreader if they have at least two\nposts with unreliable news links. We split the data\ninto train/test to balance the number of spreaders.\nWe consider posts unreliable news if they have\none or more unreliable news links. When splitting\nto create artificial users as described in \u00a73.3, we\nvary the number of context posts, using 50, 100,and 200 posts per user, resulting in 12.8k, 12.5k,\nand 11.6k artificial users respectively. We then bal-\nance the post-level data to have an equal number of\nreal and unreliable news posts, resulting in 19,654\ntotal. Posts contain 119 tokens on average ( \u03c3=206).\nOther datasets designed for identifying unreliable\nnews spreaders only include binary labels for the\nuser-level. To obtain pretrained embeddings with\nunsupervised learning algorithms we use data from\nusers history, most of which is unlabeled (see Ta-\nble 1).\nTwitter provides all necessary information in-\ncluding user labels and IDs, which enabled us to\nrecompile the posting history of each user. Un-\nfortunately, not all tweets were available for us to\ncrawl, resulting in only 3.5K users whereas the\noriginal dataset had 6.2K users. The dataset has\n2.6M posts, with an approximate distribution of\n40:60 between users circulating unreliable news\nand other information sharers. Posts contain 25 to-\nkens on average ( \u03c3=18). The corpus was recrawled\nin Plepi et al. (2022b) and further details on col-\nlection can be found in their paper. Given that\nthis dataset indicated negligible social interaction\namong its users, our focus was predominantly on\nthe personalization techniques (rather than the tem-\nporal graphs they explored).Users who shared at\nleast three unreliable links were labeled as mis-\ninformation spreaders. Note that this is different\nfrom the FACTOID dataset, as we wanted to be\nconsistent with both original works.\nFACTOID Twitter\nTotal Posts 3,354,450 2,626,176\nTotal Users 4,150 3,541\nUnreliable Spreaders 1,086 1,455\nReliable Spreaders 3,064 2,086\nUnreliable Posts 9,835 1,521,415\nReliable Posts 70,168 1,104,761\nTable 1: Comparison of datasets and label distributions.\n5 Experiments\nTo evaluate the performance of the unreliable news\nspreader detection models, we use 5-fold cross val-\nidation, for consistency with previous work. We\ncompare the proposed personalized embeddings\nwith several previous models for the unreliable\nnews detection methods. For post-level tasks we\nshow results after 10 iterations with 20 epochs each\nand learning rate of 1e\u22125. For post-level tasks\nwe encode posts with BERT (Devlin et al., 2019)149\nModel F1 Score\nSakketou et al. (2022) 0.61\nT-BERT 0.58\nT-BERT+U2V-GloVe 0.59\nU2V-GloVe U2V-W2V AA\nRF 0.71 0.60 0.74\nRidge 0.73 0.67 0.67\nLR 0.71 0.63 0.64\nSVM 0.75 0.63 0.69\nTable 2: Unreliable News Spreader Detection results\non the balanced FACTOID dataset using the logistic\nregression (LR), ridge regression (Ridge), support vec-\ntor machine (SVM) and random forest (RF) classifiers\ncompared to previous work and our combined model.\nReported values are the F1- scores over a 5-fold Cross\nValidation. Bold denotes the best overall performance\non the task.\nbefore concatenating user representations. We com-\npare to a Random method, which is a model with a\nrandom vector as input and concatenated to BERT.\nWe also compare to the best model from Mu and\nAletras (2020), T-BERT. We did not compare to the\ngraph-based methods used in Sakketou et al. (2022).\nThey found that the graph-based method on Reddit\nachieved 0.3% higher F1 than the User2Vec ran-\ndom forest method. We find that the construction\nof the Reddit graph also is unlikely to signify inter-\naction between users as many users reply to posts\nwithout responding to other comments and with-\nout knowing other users. Due to these reasons and\nthe high model complexity of the graph attention\nnetwork, we did not use this model for our tasks.\n5.1 Setup & Parameters\nTo obtain User2Vec features we use the parameters\nmentioned in Amir et al. (2016). For the vector size\nparameter we adjust GloVe and Word2Vec to the\nsame dimension d= 400 based on manual tuning.\n5.2 Results\nFor comparison with previous work, we provide\nresults for the unreliable news spreader detection\ntask in a similar format and using mostly the same\nclassifiers as previous work. For results at the post-\nlevel we report results as a distribution over 10\nruns.\nUnreliable News Spreader Detection The re-\nsults for the unreliable news spreader detection\non the Factoid and Twitter datasets are shown in\nTable 2 and Table 3 respectively. In Table 2, theModel F1 Score\nT-BERT 0.51\nT-BERT+U2V-GloVe 0.65\nU2V-GloVe AA\nRF 0.62 0.70\nRidge 0.70 0.76\nLR 0.75 0.82\nSVM 0.70 0.76\nTable 3: Unreliable News Spreader Detection results on\nthe balanced Twitter dataset using the logistic regression\n(LR), ridge regression (Ridge), support vector machine\n(SVM) and random forest (RF) classifiers compared\nto previous work and our combined model. Reported\nvalues are the F1- scores over a 5-fold Cross Validation.\nBold denotes the best overall performance on the task.\nbest model from Sakketou et al. (2022) is our base-\nline at 0.61 F1, which uses a User2Vec (U2V)\nmodel trained on the Google News Corpus using\nword2vec (W2V). We compared this setup to one\nwhere the word embeddings are pretrained on in-\ndomain data using their corpus with both word2vec\n(U2V-W2V) and GloVe (U2V-GloVe). Note that\nthe User2Vec method is initialized with static em-\nbeddings only so contextualized embeddings from\nlarge pretrained language models are incompati-\nble with this approach. We used the same classic\nmachine learning classifiers (i.e. random forest, lo-\ngistic regression, support vector machines) for the\nsake of comparison. We also compared to the best\nperforming method from (Mu and Aletras, 2020)\n(T-BERT).\nWe included one more model based on T-BERT\nbut with the U2V-GloVe vectors concatenated to\nthe input before being passed to a final classifica-\ntion layer. We found that this improved perfor-\nmance on the FACTOID dataset, but only slightly\nover the T-BERT baseline. Simpler classification\nmodels with high quality user embeddings learned\nthrough the authorship attribution and User2Vec\nmethods outperformed the language model ap-\nproach, which we attribute to their training method,\nwhich takes all of a users previous data into account\nwhen learning a representative vector, whereas\nBERT can only encode a limited history.\nIn Table 3, the results are evaluated on the Twit-\nter data by following the same models and em-\nbedding methods used in the FACTOID dataset to\nassess their performance in detecting unreliable\nnews spreaders. Here, we did not include the\nword2vec approaches, as they performed poorly150\nRandom AA U2V Task U2V+AA Task+AA Task+U2V Combined T-BERT0.650.7F1 ScoreFACTOID: Unreliable News Post Classification\nContext\n50\n100\n200\nFigure 2: Distributions of F1-scores for personalization methods and combinations while varying the number of\ncontext posts (p) or tokens (t) for the task of classifying unreliable news posts.\nRandom AA U2V Task U2V+AA Task+AA Task+U2V Combined T-BERT0.650.70.75F1 ScoreTwitter: Unreliable News Post Classification\nContext\n50\n100\n200\nFigure 3: Distributions of F1-scores on the Twitter dataset for personalization methods and combinations while\nvarying the number of context posts for the task of classifying unreliable news posts.\non the other task compared to GloVe (which in-\ncludes Sakketou et al. (2022)). Interestingly, the\nhighest performance with 82% F1is achieved by\nthe model trained on authorship attribution embed-\ndings. Here the T-BERT with U2V-GloVe embed-\ndings performed much higher than the T-BERT\nbaseline, but still lower than the best U2V-GloVe\nand authorship attribution embedding approaches.\nFor further experiments with the commonly used\nLIWC features, see Appendix A. Note that we do\nnot compare to the task embedding method because\nit requires data from a user for both training and\ntesting, while this task setup has separate users\nacross the splits.\nUnreliable News Post Classification Figure 2\nshows the F1measure for the unreliable news de-\ntection task using FACTOID Dataset. Task embed-\ndings in combination with the pretrained authorship\nattribution features achieve the best results with a\nmedian F1score of 72%. The worst score is ob-\ntained by the User2Vec approach with 65%. If we\ncompare the different input sizes, the AA features\nbenefit from having more data to train on. Other ap-\nproaches considered individually seem not to learn\nbetter features with higher input sizes. The combi-nations follow this trend from the AA embeddings.\nThe combination of all three seems negatively im-\npacted by User2Vec. However, the influence is not\nstatistically significant (Kruskal and Wallis, 1952).\nSimilarly, Figure 3 shows the results for unre-\nliable news detection on Twitter. The combined\napproach using all user representations had the best\nperformance with a median F1score 75%. It is in-\nteresting to note that, all approaches appear to learn\nbetter features with fewer users and bigger message\nchunks. Contrary to the FACTOID dataset, the au-\nthorship attribution approach performs better, as\nit did for the unreliable news spreader task, than\nthe User2Vec embeddings. T-BERT performs rel-\natively low on this task and not much higher than\nour random baseline. We believe that the lack of\nreproducibility of Twitter datasets in general could\nlead to such discrepancies.\nUnreliable News Post Prediction Figure 4\nshows results for unreliable news prediction for\nthe FACTOID dataset. In this comparison, we see\nthat authorship attribution features lose up to 16%\nF1with fewer users and more potentially irrelevant\ncontext. With a smaller context of 50, the differ-\nence is lower by 6% than in the classification task.151\nRandom AA U2V Task U2V+AA Task+AA Task+U2V Combined T-BERT0.50.550.60.650.7F1 ScoreFACTOID: Unreliable News Post Prediction\nContext\n50\n100\n200\nFigure 4: Distributions of F1-scores for personalization methods and combinations while varying the number of\ncontext posts (p) or tokens (t) for the task of predicting unreliable news posts.\nRandom AA U2V Task U2V+AA Task+AA Task+U2V Combined T-BERT0.50.60.70.8F1 ScoreTwitter: Unreliable News Post Prediction\nContext\n50\n100\n200\nFigure 5: Distributions of F1-scores on the Twitter dataset for personalization methods and combinations while\nvarying the number of context posts for the task of predicting unreliable news posts.\nUser2Vec performs similar to chance and task em-\nbeddings remain high performing, not differing in\nthe median ( p < 0.0003 ). Combinations of person-\nalization methods show similarly high performance.\nHere T-BERT shows competitive performance but\nstill underperforms all of our methods that use task\nembeddings.\nSimilarly, Figure 5 displays the results of the\nunreliable news prediction task using the Twitter\ndataset. Although these methods rely only on user\nembeddings and omit post text, we can observe that\nthe model is still learning high quality representa-\ntions as the results are encouraging. The best score\nis obtained by task embeddings with median F1\n85%, combining task and User2Vec embeddings\nperform second best. We see competitive perfor-\nmance from the User2Vec embeddings whereas\nthey performed randomly on the FACTOID dataset.\nThe truncated BERT encodings caused the model\nto perform poorly, likely due to the fact that it does\nnot seem to capture enough context for the predic-\ntion task. Interestingly, T-BERT performs better\nfor the FACTOID dataset, and all of our methods\noutperform it on the Twitter dataset, leading to a\nnew state-of-the-art for this task.Linguistic Analysis In addition to our primary\nfocus on comparing results of user personalization\nmethods across two datasets, we explored linguistic\ncharacteristics of the spreaders\u2019 posts. Specifically,\nwe looked at sentiment scores, which provide an in-\ndication of the emotional tone expressed in the con-\ntent. These sentiment scores were computed using\nV ADER (Valence Aware Dictionary and sEntiment\nReasoner, Hutto and Gilbert (2014)), a lexicon and\nrule-based sentiment analysis tool specifically de-\nsigned to gauge sentiments expressed in social me-\ndia. Our analysis revealed that unreliable news\nspreaders exhibit significantly different sentiment\nscores compared to reliable news spreaders. We\ntested this observation using a two-sample t-test,\nwhich yielded a p < 0.0001 . This provides strong\nstatistical support for our observation: unreliable\nnews spreaders indeed have a significantly differ-\nent sentiment score than reliable news spreaders.\nInterestingly, our analysis also identified a nega-\ntive correlation of -0.11 between the number of\nunreliable news posts and sentiment score. This\nsuggests that as individuals disseminate more unre-\nliable news, their sentiment score decreases, imply-\ning a less positive linguistic style among unreliable\nnews spreaders as they become more active in the152\npropagation of unreliable news. By examining se-\nlected instances, we observed a consistent pattern.\nSentiment scores experienced a downward shift as\nindividuals approached the posting of a unreliable\nnews item.\nWe also looked at the correlation between the\nlabels in the FACTOID dataset and the LIWC cate-\ngories, similarly to Mu and Aletras (2020). How-\never, we did not find significant correlations be-\ntween the groups. On the Twitter dataset, they\nfound correlations, for instance, between the use of\npower and analyitic words with unreliable news\nspreaders, and informal and netspeak language\nwith reliable news spreaders. These differences\ncould be due to the difference in writing styles\nbetween Reddit and Twitter users.\n6 Discussion\nThe task of unreliable news post prediction could\nprovide insight into the patterns of users who\nspread unreliable news which could help inform\nthe design of social media policies or interventions\nto prevent such cases. We compared to the best\nmethod from previous work, T-BERT, which we\nfound competitive with the embedding combina-\ntions for post prediction on the Twitter dataset but\nwith lower scores for post classification and pre-\ndiction on the FACTOID dataset. When a higher\nnumber of context posts were available, the em-\nbedding methods more consistently outperformed\nT-BERT. On the spreader detection task, we found\nthat when we had high-quality user representations\nderived from other deep learning models, simple\nclassifiers were able to achieve higher performance\nthan the T-BERT baselines, which may introduce\nmore noise and complexity than necessary.\nOur results indicated that embedding perfor-\nmance varied depending on the dataset and the spe-\ncific task at hand. For instance, User2Vec excelled\nat capturing long-term behavioral patterns, making\nit particularly effective for tasks where a user\u2019s his-\ntorical behavior is a key factor. However, it may\nnot have been as adept at capturing the nuances of\nindividual posts or the specific contexts in which\nthey were made. Authorship attribution focused\non the unique linguistic style of users, making it\neffective for identifying unreliable news spreaders\nwho have a consistent writing style, but less so for\nthose who vary their writing style. These embed-\ndings were particularly useful in post-classification,\nwhere they were concatenated with text to providea more comprehensive representation. Task embed-\ndings were updated during training, allowing them\nto adapt to the unique challenges posed by unreli-\nable news detection. This adaptability was a key\nreason why they often outperformed other methods\nin our experiments. On the other hand, the combi-\nnation of all user representations (U2V+AA+Task)\nshowed the best performance on the Twitter dataset,\nsuggesting that a multifaceted approach that lever-\nages various aspects of user behavior and post char-\nacteristics can provide a more robust solution for\nunreliable news detection.\nIn summary, the effectiveness of each user rep-\nresentation strategy is highly dependent on the\nspecific challenges posed by the task of unreli-\nable news detection and the nature of the dataset.\nThere\u2019s no one size fits all solution, and the optimal\nstrategy may involve a combination of different\nuser representations to capture the multifaceted na-\nture of user behavior and unreliable news spread.\nIn a linguistic analysis, we identified that unre-\nliable news spreaders tend to exhibit distinct sen-\ntiment scores that decrease as they circulate more\nunreliable news. However, no significant correla-\ntions were observed between LIWC categories and\nreliable/unreliable news spreaders as was found in\nprevious work.\n7 Conclusions\nIn this work, we systematically studied the appli-\ncation of recent personalization methods to three\ndistinct yet interrelated tasks. These tasks included\nuser-level detection of unreliable news spreaders,\npost-level classification of unreliable news, and\npredicting when unreliable news will be spread.\nWe found significant improvements in the task of\ndetecting unreliable news spreaders at a user level\nwhen applying User2Vec embeddings learned with\nGloVe pretrained on in-domain data. This result\nindicates that a closer alignment with the domain of\nthe data yields superior performance in identifying\nunreliable news agents. Moreover, for post-level\ntasks such as classifying unreliable news and pre-\ndicting its propagation, we discovered that task em-\nbeddings learned jointly with the downstream task\noutperformed other personalization methods and\nprevious work. Furthermore, our findings suggest\nthat combining different personalization methods\ncan further boost performance.\nIn addition to these primary findings, our ex-\nploration into linguistic characteristics yielded in-153\ntriguing insights. We observed a significant differ-\nence in sentiment scores between unreliable news\nspreaders and reliable news spreaders, with unreli-\nable news spreaders exhibiting a less emotive lin-\nguistic style. We also noticed a negative correlation\nbetween the number of unreliable news posts and\nsentiment scores, indicating a decline in sentiment\nas the frequency of these posts increased.\nFuture work could explore the integration of our\napproach with other forms of analysis, such as net-\nwork analysis or more nuanced linguistic analysis,\nfor a more comprehensive understanding of unre-\nliable news dynamics. We release our code1and\ndata split to facilitate further research in this vital\nfield and support shared scientific goals.\nLimitations\nPrevious work from Sheikh Ali et al. (2022);\nSakketou et al. (2022) characterizes a user as a\nunreliable news spreader based on whether at least\ntwo unreliable news links were detected in their\npost history, while Mu and Aletras (2020) requires\nat least three posts. If we look inside the results of\nour model, it seems to classify users as unreliable\nnews spreaders if at least one unreliable news link\nwas detected. For example this post of a randomly\nselected user:\n\u201chttps://www.dailymail.co.\nuk/news/article-4364984/\nIvanka-Trump-hit-claim-ripping-designs.\nhtml is well in keeping of the Trump family trend\nof stealing ideas and claiming them as one\u2019s own.\u201d\nThis post contains an unreliable news link.2\nThis user was classified as an unreliable news\nspreader but according to the definition of an\nunreliable news spreader, they are a reliable\nnews spreader. Which leads to the question how\nmany times a user should post about unreliable\nnews in order to be considered as a unreliable\nnews spreader? Although this threshold of two\nunreliable news posts is somewhat arbitrary and\nshould be adjusted for the desired application, it\nserves to show the effectiveness of our approach.\nOur methods look at the text of posts being\nshared on social media. The links shared by indi-\nviduals contain additional multi-modal information.\n1Github: https://github.com/caisa-lab/\nWOAH24-FakenewsSpreader\n2According to https://mediabiasfactcheck.com/Often these links contain images or video. Our\nmodel does not take the link content into account\nand future work could improve model performance\nby modeling this information.\nThe datasets that we use were both labeled us-\ning curated lists of reliable and unreliable news\nsources. As such, it is possible that labels con-\ntain some noise, as reliable sources may sometimes\nhave less reliable articles and vice versa. It is also\npossible that bias exists in the websites providing\nground truth labels. As such, there is a risk that\nthis could lead a trained model to incorrectly clas-\nsify certain topics or populations. Relatedly, the\nprevious work that created these datasets assumed\nthat the sharing of a source was inherently an act\nof spreading unreliable news. A dataset that also\ncontained the stance of the sharer toward the arti-\ncles would allow for more nuance regarding what\nis shared, one may wish to separate those wishing\nto inform others of the unreliability of news from\nthose who are promoting it.\nEthics Statement\nIf we develop language models for authorship at-\ntribution, they could be used to find other online\naccounts of a person, given posts on a single one\nof their accounts. This could potentially be used\nfor user profiling and surveillance of target popu-\nlations (Rangel Pardo et al., 2013). Furthermore,\nthe identification of unreliable news spreaders must\nbe carefully applied in practice, as people may be\nmisclassified, leading to the suppression of speech\nfor these individuals.\nUser-augmented classification efforts risk invok-\ning harmful stereotyping, as the algorithm labels\npeople as unreliable news spreaders or classifies\nusers posts as unreliable news. These can be em-\nphasized by the semblance of objectivity created\nby the use of a computer algorithm (Koolen and\nvan Cranenburgh, 2017).\nThere are forms of bias that apply specifically\nin natural language processing research. For exam-\nple, gender bias in a text such as the use of words\nor syntactic constructs that connote or imply an\ninclination or prejudice against one gender (Hitti\net al., 2019). Machine learning algorithms trained\nin natural language processing tasks have exhib-\nited various forms of systemic racial and gender\nbiases. For example hate speech detection (Boluk-\nbasi et al., 2016) or learned word embeddings (Park\net al., 2018).154\nAcknowledgements\nThis work has been supported by the Federal\nMinistry of Education and Research of Germany\n(BMBF) as a part of the Junior AI Scientists pro-\ngram under the reference 01-S20060, and the state\nof North Rhine-Westphalia as part of the Lamarr\nInstitute for Machine Learning. Any opinions, find-\nings, conclusions, or recommendations in this ma-\nterial are those of the authors and do not necessarily\nreflect the views of the BMBF or Lamarr Institute.\nReferences\nSilvio Amir, Byron C. Wallace, Hao Lyu, Paula Car-\nvalho, and M\u00e1rio J. Silva. 2016. Modelling context\nwith user embeddings for sarcasm detection in social\nmedia. In Proceedings of the 20th SIGNLL Confer-\nence on Computational Natural Language Learning ,\npages 167\u2013177, Berlin, Germany. Association for\nComputational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam Tauman Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Ad-\nvances in Neural Information Processing Systems 29:\nAnnual Conference on Neural Information Process-\ning Systems 2016, December 5-10, 2016, Barcelona,\nSpain , pages 4349\u20134357.\nLimeng Cui, Kai Shu, Suhang Wang, Dongwon Lee,\nand Huan Liu. 2019. defend: A system for ex-\nplainable fake news detection. In Proceedings of\nthe 28th ACM International Conference on Informa-\ntion and Knowledge Management, CIKM 2019, Bei-\njing, China, November 3-7, 2019 , pages 2961\u20132964.\nACM.\nNicollas R de Oliveira, Pedro S Pisa, Martin An-\ndreoni Lopez, Dianne Scherly V de Medeiros, and\nDiogo MF Mattos. 2021. Identifying fake news on\nsocial networks based on natural language process-\ning: trends and challenges. Information , 12(1):38.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nXinhuan Duan, Elham Naghizade, Damiano Spina, and\nXiuzhen Zhang. 2020. Rmit at pan-clef 2020: Pro-\nfiling fake news spreaders on twitter. In CLEF 2020\nLabs and Workshops, Notebook Papers . CEUR Work-\nshop Proceedings.\nLucie Flek. 2020. Returning the N to NLP: Towards\ncontextually personalized classification models. InProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7828\u2013\n7838, Online. Association for Computational Lin-\nguistics.\nBilal Ghanem, Simone Paolo Ponzetto, Paolo Rosso,\nand Francisco Rangel. 2021. FakeFlow: Fake news\ndetection by modeling the flow of affective infor-\nmation. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume , pages 679\u2013689,\nOnline. Association for Computational Linguistics.\nMaria Glenski, Tim Weninger, and Svitlana V olkova.\n2018a. Identifying and understanding user reactions\nto deceptive and trusted social news sources. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers) , pages 176\u2013181, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nMaria Glenski, Tim Weninger, and Svitlana V olkova.\n2018b. Propagation from deceptive news sources\nwho shares, how much, how evenly, and how\nquickly? IEEE Transactions on Computational So-\ncial Systems , 5(4):1071\u20131082.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difficulty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth interna-\ntional conference on artificial intelligence and statis-\ntics, volume 9 of Proceedings of Machine Learning\nResearch , pages 249\u2013256, Chia Laguna Resort, Sar-\ndinia, Italy. JMLR Workshop and Conference Pro-\nceedings, PMLR.\nYasmeen Hitti, Eunbee Jang, Ines Moreno, and Car-\nolyne Pelletier. 2019. Proposed taxonomy for gender\nbias in text; a filtering methodology for the gender\ngeneralization subtype. In Proceedings of the First\nWorkshop on Gender Bias in Natural Language Pro-\ncessing , pages 8\u201317, Florence, Italy. Association for\nComputational Linguistics.\nPhilip N. Howard and Bence Kollanyi. 2016. Bots,\n#StrongerIn, and #Brexit: Computational Propa-\nganda during the UK-EU Referendum. CoRR ,\nabs/1606.06356.\nClayton Hutto and Eric Gilbert. 2014. Vader: A parsi-\nmonious rule-based model for sentiment analysis of\nsocial media text. In Proceedings of the international\nAAAI conference on web and social media , volume 8,\npages 216\u2013225. The AAAI Press.\nGeorgi Karadzhov, Preslav Nakov, Llu\u00eds M\u00e0rquez, Al-\nberto Barr\u00f3n-Cede\u00f1o, and Ivan Koychev. 2017. Fully\nautomated fact checking using external sources. In\nProceedings of the International Conference Recent\nAdvances in Natural Language Processing, RANLP\n2017 , pages 344\u2013353, Varna, Bulgaria. INCOMA\nLtd.\nHamid Karimi, Proteek Roy, Sari Saba-Sadiya, and\nJiliang Tang. 2018. Multi-source multi-class fake155\nnews detection. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics ,\npages 1546\u20131557, Santa Fe, New Mexico, USA. As-\nsociation for Computational Linguistics.\nAbdullah Faiz Ur Rahman Khilji, Anubhav Sachan, Di-\nvyansha Lachi, and et al. 2023. Can we debunk\ndisinformation by leveraging speaker credibility and\nperplexity measures?\nCorina Koolen and Andreas van Cranenburgh. 2017.\nThese are not the stereotypes you are looking for:\nBias and fairness in authorial gender attribution. In\nProceedings of the First ACL Workshop on Ethics in\nNatural Language Processing , pages 12\u201322, Valen-\ncia, Spain. Association for Computational Linguis-\ntics.\nWilliam H Kruskal and W Allen Wallis. 1952. Use of\nranks in one-criterion variance analysis. Journal of\nthe American statistical Association , 47(260):583\u2013\n621.\nRuixue Lian, Che-Wei Huang, Yuqing Tang, Qilong Gu,\nChengyuan Ma, and Chenlei Guo. 2022. Incremen-\ntal user embedding modeling for personalized text\nclassification. In IEEE International Conference on\nAcoustics, Speech and Signal Processing, ICASSP\n2022, Virtual and Singapore, 23-27 May 2022 , pages\n7832\u20137836. IEEE.\nYang Liu and Yi-fang Brook Wu. 2018. Early detection\nof fake news on social media through propagation\npath classification with recurrent and convolutional\nnetworks. In Proceedings of the Thirty-Second AAAI\nConference on Artificial Intelligence, (AAAI-18), the\n30th innovative Applications of Artificial Intelligence\n(IAAI-18), and the 8th AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence (EAAI-18),\nNew Orleans, Louisiana, USA, February 2-7, 2018 ,\npages 354\u2013361. AAAI Press.\nShahan Ali Memon and Kathleen M Carley. 2020.\nCMU-MisCov19: a Novel Twitter dataset for charac-\nterizing COVID-19 misinformation. Zenodo .\nTom\u00e1s Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In Advances in Neural Information Process-\ning Systems 26: 27th Annual Conference on Neural\nInformation Processing Systems 2013. Proceedings\nof a meeting held December 5-8, 2013, Lake Tahoe,\nNevada, United States , pages 3111\u20133119.\nYida Mu and Nikolaos Aletras. 2020. Identifying twit-\nter users who repost unreliable news sources with\nlinguistic information. PeerJ Computer Science ,\n6:e325.\nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav\nNakov, and Min-Yen Kan. 2020. FANG. In Pro-\nceedings of the 29th ACM International Conference\non Information &; Knowledge Management . ACM.Ji Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-\nducing gender bias in abusive language detection.\nInProceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n2799\u20132804, Brussels, Belgium. Association for Com-\nputational Linguistics.\nJames W. Pennebaker, Ryan Boyd, Kayla Jordan, and\nKate Blackburn. 2015. The development and psycho-\nmetric properties of LIWC2015 . University of Texas\nat Austin.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP) , pages 1532\u20131543, Doha, Qatar.\nAssociation for Computational Linguistics.\nGordon Pennycook and David G. Rand. 2019. Lazy, not\nbiased: Susceptibility to partisan fake news is better\nexplained by lack of reasoning than by motivated\nreasoning. Cognition , 188:39\u201350. The Cognitive\nScience of Political Thought.\nJoan Plepi, Magdalena Buski, and Lucie Flek. 2023.\nPersonalized intended and perceived sarcasm detec-\ntion on Twitter. In Proceedings of the 3rd Workshop\non Computational Linguistics for the Political and\nSocial Sciences , pages 8\u201318, Ingolstadt, Germany.\nAssociation for Computational Lingustics.\nJoan Plepi and Lucie Flek. 2021. Perceived and in-\ntended sarcasm detection with graph attention net-\nworks. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021 , pages 4746\u20134753,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJoan Plepi, B\u00e9la Neuendorf, Lucie Flek, and Charles\nWelch. 2022a. Unifying data perspectivism and per-\nsonalization: An application to social norms. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 7391\u2013\n7402, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nJoan Plepi, Flora Sakketou, Henri-Jacques Geiss, and\nLucie Flek. 2022b. Temporal graph analysis of misin-\nformation spreaders in social media. In Proceedings\nof TextGraphs-16: Graph-based Methods for Natu-\nral Language Processing , pages 89\u2013104, Gyeongju,\nRepublic of Korea. Association for Computational\nLinguistics.\nFrancisco Rangel, Anastasia Giachanou, Bilal Ghanem,\nand Paolo Rosso. 2020. Overview of the 8th Author\nProfiling Task at PAN 2020: Profiling Fake News\nSpreaders on Twitter. In CLEF 2020 Labs and Work-\nshops, Notebook Papers . CEUR-WS.org.\nFrancisco Rangel Pardo, Paolo Rosso, Irina Chugur,\nMartin Potthast, Martin Trenkmann, Benno Stein,\nBen Verhoeven, and Walter Daelemans. 2013.\nOverview of the 2nd author profiling task at pan 2014.\nCEUR Workshop Proceedings , 1180.156\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nV olkova, and Yejin Choi. 2017. Truth of varying\nshades: Analyzing language in fake news and po-\nlitical fact-checking. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 2931\u20132937, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP) , pages\n3982\u20133992, Hong Kong, China. Association for Com-\nputational Linguistics.\nGiancarlo Ruffo, Alfonso Semeraro, Anastasia Gi-\nachanou, and Paolo Rosso. 2023. Studying fake news\nspreading, polarisation dynamics, and manipulation\nby bots: A tale of networks and language. Computer\nscience review , 47:100531.\nFlora Sakketou, Joan Plepi, Riccardo Cervero,\nHenri Jacques Geiss, Paolo Rosso, and Lucie Flek.\n2022. FACTOID: A new dataset for identifying\nmisinformation spreaders and political bias. In Pro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference , pages 3231\u20133241, Marseille,\nFrance. European Language Resources Association.\nAlireza Salemi, Sheshera Mysore, Michael Bendersky,\nand Hamed Zamani. 2023. Lamp: When large lan-\nguage models meet personalization. ArXiv preprint ,\nabs/2304.11406.\nKarishma Sharma, Feng Qian, He Jiang, Natali Ruchan-\nsky, Ming Zhang, and Yan Liu. 2019. Combating\nfake news: A survey on identification and mitigation\ntechniques. ACM Transactions on Intelligent Systems\nand Technology (TIST) , 10(3):1\u201342.\nZien Sheikh Ali, Abdulaziz Al-Ali, and Tamer Elsayed.\n2022. Detecting users prone to spread fake news\non Arabic Twitter. In Proceedinsg of the 5th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools with Shared Tasks on Qur\u2019an QA and Fine-\nGrained Hate Speech Detection , pages 12\u201322, Mar-\nseille, France. European Language Resources Asso-\nciation.\nKai Shu, Suhang Wang, and Huan Liu. 2018. Exploiting\ntri-relationship for fake news detection. In Proceed-\nings of the Thirty-Second AAAI Conference on Ar-\ntificial Intelligence , volume abs/1712.07709. AAAI\nPress.\nKai Shu, Suhang Wang, and Huan Liu. 2019. Beyond\nnews contents: The role of social context for fake\nnews detection. In Proceedings of the Twelfth ACM\nInternational Conference on Web Search and Data\nMining, WSDM 2019, Melbourne, VIC, Australia,\nFebruary 11-15, 2019 , pages 312\u2013320. ACM.Efstathios Stamatatos. 2009. A survey of modern au-\nthorship attribution methods. Journal of the Ameri-\ncan Society for information Science and Technology ,\n60(3):538\u2013556.\nYla R Tausczik and James W Pennebaker. 2010. The\npsychological meaning of words: Liwc and comput-\nerized text analysis methods. Journal of language\nand social psychology , 29(1).\nCharles Welch, Chenxi Gu, Jonathan K. Kummerfeld,\nVeronica Perez-Rosas, and Rada Mihalcea. 2022.\nLeveraging similar users for personalized language\nmodeling with limited data. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages\n1742\u20131752, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nA Psycholinguistic Features for\nMisinformation Spreader Detection\nSeveral previous papers have addressed the use of\npsycholinguistic features for the detection of mis-\ninformation spreaders (Rashkin et al., 2017; Shu\net al., 2018). We decided to compare our approach\nto the use of such features using the commonly used\nlexicon, Linguistic Inquiry and Word count (LIWC;\n(Tausczik and Pennebaker, 2010; Pennebaker et al.,\n2015)). The lexicon provides a set of word cate-\ngories for over 6k words, representing linguistic\nand psycholinguistic processes.\nWe construct a feature-vector using the lexicon\nby counting each word category and concatenating\nthese into a single vector. We also experimented\nwith a concatenation of the LIWC feature vector\nand the User2Vec representations. We provide re-\nsults in Table 5. The methods for results that do\nnot use LIWC are copied from \u00a75 for comparison.\nWe include only the GloVe results here, as they\nperformed better than Word2Vec. We find that the\nLIWC features underperform the personalization\nmethods, and even lower performance when com-\nbined with the User2Vec approach.\nB Additional Training Details\nWe use the transformers HuggingFace model\nbert-base-uncased . The model has 12 layers,\na hidden size of 768, 12 heads, and 110M param-\neters. It was trained on lower-cased English text.\nThe non-BERT models run in a few minutes on a\nsingle CPU. The BERT models for the post-level\ntasks take 9-10 hours to run for one context size for\n10 runs on an NVIDIA A100 GPU.157\nModel U2V LIWC LIWC+U2V AA Baseline\nRF 0.71 0.57 0.68 0.74 0.61\nRidge 0.73 0.64 0.71 0.67 -\nLR 0.71 0.58 0.71 0.64 0.60\nSVM 0.75 0.61 0.71 0.69 0.61\nTable 4: Psycholinguistic feature comparison for unreliable news spreader detection results on the balanced\nFACTOID dataset using the logistic regression (LR), ridge regression (Ridge), support vector machine (SVM)\nand random forest (RF) classifiers. Reported values are the F1- scores over a 5-fold Cross Validation. User2Vec\napproaches use GloVe embeddings for training.\nModel U2V LIWC LIWC+U2V AA Baseline\nRF 0.62 0.65 0.74 0.70 -\nRidge 0.70 0.65 0.73 0.76 -\nLR 0.75 0.65 0.74 0.82 -\nSVM 0.70 0.63 0.71 0.76 -\nTable 5: Psycholinguistic feature comparison for unreliable news spreader detection results on the balanced Twitter\ndataset using the logistic regression (LR), ridge regression (Ridge), support vector machine (SVM) and random\nforest (RF) classifiers. Reported values are the F1-scores over a 5-fold Cross Validation. User2Vec approaches use\nGloVe embeddings for training.158", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Harnessing Personalization Methods to Identify and Predict Unreliable Information Spreader Behavior", "author": ["S Ashraf", "F Gruschka", "L Flek"], "pub_year": "2024", "venue": "Proceedings of the 8th \u2026", "abstract": "Studies on detecting and understanding the spread of unreliable news on social media  have identified key characteristic differences between reliable and unreliable posts. These"}, "filled": false, "gsrank": 576, "pub_url": "https://aclanthology.org/2024.woah-1.11/", "author_id": ["WvgYvhkAAAAJ", "", "qZCZFp0AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:saV8WjkCliMJ:scholar.google.com/&output=cite&scirp=575&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D570%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=saV8WjkCliMJ&ei=brWsaODUBsDZieoPqdqh8QU&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:saV8WjkCliMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aclanthology.org/2024.woah-1.11.pdf"}}, {"title": "Multilingual multifaceted understanding of online news in terms of genre, framing, and persuasion techniques", "year": "2023", "pdf_data": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers , pages 3001\u20133022\nJuly 9-14, 2023 \u00a92023 Association for Computational Linguistics\nMultilingual Multifaceted Understanding of Online News\nin Terms of Genre, Framing and Persuasion Techniques\nJakub Piskorski1, Nicolas Stefanovitch2\u2217, Nikolaos Nikolaidis3,\nGiovanni Da San Martino4,Preslav Nakov5\n1Institute of Computer Science, Polish Academy of Science, Poland jpiskorski@gmail.com\n2European Commission Joint Research Centre, Italy nicolas.stefanovitch@ec.europa.eu\n3Dept. of Informatics, Athens University of Economics and Business, Greece nnikon@aueb.gr\n4Department of Mathematics, University of Padova, Italy dasan@math.unipd.it\n5Mohamed bin Zayed University of Artificial Intelligence, UAE preslav.nakov@mbzuai.ac.ae\nAbstract\nWe present a new multilingual multifacet\ndataset of news articles, each annotated for\ngenre (objective news reporting vs. opinion\nvs. satire), framing (what key aspects are high-\nlighted), and persuasion techniques (logical\nfallacies, emotional appeals, ad hominem at-\ntacks, etc.). The persuasion techniques are an-\nnotated at the span level, using a taxonomy\nof 23 fine-grained techniques grouped into 6\ncoarse categories. The dataset contains 1,612\nnews articles covering recent news on current\ntopics of public interest in six European lan-\nguages (English, French, German, Italian, Pol-\nish, and Russian), with more than 37k anno-\ntated spans of persuasion techniques. We de-\nscribe the dataset and the annotation process,\nand we report the evaluation results of mul-\ntilabel classification experiments using state-\nof-the-art multilingual transformers at differ-\nent levels of granularity: token-level, sentence-\nlevel, paragraph-level, and document-level.\n1 Introduction\nInternet has changed profoundly the information\nlandscape by creating direct channels of commu-\nnication between information producers and con-\nsumers. At the same time, it has also increased\nthe risk for readers to be exposed to disinformation\n(aka \u201cfake news\u201d), propaganda, manipulation, etc.,\nwhich has grown into an infodemic (Alam et al.,\n2021). The consequences of this are very concrete,\nas swaying the hearts and the minds of a population\nalso sways their choices, notably during elections.\nTherefore, online media analysis is important in\norder to understand the news ecosystem and the\npresented narratives around certain topics across\ncountries, and to identify manipulation attempts\nand deceptive content, in order to provide citizens\nwith a more transparent and comprehensible under-\nstanding of the online news.\n\u2217The first and the second author have equally contributed\nto the work reported in this paper.Given the scale of the media landscape, media\nanalysis needs automatic tools, which in turn need\ntraining data. With this in mind, we introduce a new\ndataset that covers several complementary aspects\nof the news: genre (objective news reporting vs.\nopinion vs. satire), framing (what key aspects are\nhighlighted), and persuasion techniques (logical\nfallacies, emotional appeals, personal attacks, etc.).\nWe collected news articles between 2020 and\nmid-2022, from sources ranging in the whole po-\nlitical spectrum and revolving around widely dis-\ncussed topics such as COVID-19, climate change,\nabortion, migration, the Russo-Ukrainian war, and\nlocal elections. Our dataset is multilingual (English,\nFrench, German, Italian, Polish, and Russian), mul-\ntilabel, and covers complementary dimensions for\nbetter news understanding. Our taxonomy of per-\nsuasion techniques is an improvement and also an\nextension compared to previous inventories, and\nit contains 23 labels organised in a 2-tier hierar-\nchy. We annotated a total of 1,612 articles with\n37K annotated snippets for persuasion techniques,\nwhich is a 3-fold increase in the number of articles\nand 4-fold in the number of spans compared to the\nlargest previous efforts, which focused on English\nonly (Da San Martino et al., 2019).\nOur contributions can be summarized as follows:\n\u2022We release a new multilingual dataset, the\nlargest of its kind, jointly annotated for genre,\nframing, and persuasion techniques; we also\nrelease our detailed annotation guidelines;\n\u2022We report on different dataset statistics, and\nnotably explore persuasion techniques and\nframing in more detail, exhibiting their char-\nacteristics for different topics and languages;\n\u2022We report the results of several multiclass and\nmultilabel classification experiments, explor-\ning different settings in terms of taxonomy\ngranularity and focus in the document, also\nassessing multi/cross-lingual transfer.3001\n2 Related Work\nBelow, we discuss previous work related to each of\nthe three types of annotation we consider.\n2.1 News Genre Categorization\nRashkin et al. (2017) developed a corpus with\nnews annotations using distant supervision into\nfour classes: trusted ,satire ,hoax , and propaganda .\nHorne and Adali (2017) and Levi et al. (2019) stud-\nied the relationship between fake news, real news,\nand satire with focus on style. Golbeck et al. (2018)\ndeveloped a dataset of fake news and satire stories\nand analyzed and compared their thematic content.\nHardalov et al. (2016) developed a dataset to reli-\nable vs. satirical news. Satire was also one of the\ncategories in the NELA-GT-2018 dataset (N\u00f8rre-\ngaard et al., 2019), as well as its extended version\nNELA-GT-2019 (Gruppi et al., 2020).\nOur inventory is a bit different: ( i) we aim to\ndistinguish objective news reporting vs. opinion\npiece vs. satire, and ( ii) in a multilingual setup.\n2.2 Framing Detection\nFraming is a strategic device and a central con-\ncept in political communication for representing\ndifferent salient aspects and perspectives for the\npurpose of conveying the latent meaning about an\nissue (Entman, 1993). It is important for news\nmedia as the same topics can be discussed from\ndifferent perspectives. There has been work on\nautomatically identifying media frames, including\nannotation schemes and datasets such as the Me-\ndia Frames Corpus (Card et al., 2015), systems to\ndetect media frames (Liu et al., 2019; Zhang et al.,\n2019; Cheeks et al., 2020), large-scale automatic\nanalysis of New York Times (Kwak et al., 2020), of\nRussian news (Field et al., 2018), or of the Syrian\nrefugees crisis in US media (Chen et al., 2023). See\n(Ali and Hassan, 2022) for a recent survey.\nHere, we adopt the frame inventory of the Media\nFrames Corpus, and we create a new multilingual\ndataset with frame annotations in six languages.\n2.3 Persuasion Techniques Detection\nWork on persuasion detection overlaps to a large\nextent with work on propaganda detection, as there\nare many commonalities between the two.\nEarly work on propaganda detection focused on\ndocument-level analysis. Rashkin et al. (2017) pre-\ndicted four classes ( trusted ,satire ,hoax , and pro-\npaganda ), labeled using distant supervision.Barr\u00f3n-Cedeno et al. (2019) developed a cor-\npus with two labels (i.e., propaganda vs. non-\npropaganda ) and further investigated writing style\nand readability level. Their findings confirmed\nthat using distant supervision, in conjunction with\nrich representations, might encourage the model\nto predict the source of the article, rather than to\ndiscriminate propaganda from non-propaganda.\nAn alternative line of research focused on detect-\ning the use of specific propaganda techniques in\ntext, e.g., Habernal et al. (2017, 2018) developed a\ncorpus with 1.3k arguments annotated with five fal-\nlacies that relate to persuasion techniques. A more\nfine-grained analysis was done by Da San Martino\net al. (2019), who developed a corpus of news ar-\nticles annotated with 18 propaganda techniques,\nconsidering the tasks of technique span detection\nand classification. They further tackled a sentence-\nlevel task, and proposed a multigranular gated neu-\nral network. Subsequently, the Prta system was re-\nleased (Da San Martino et al., 2020b), and models\nwere proposed addressing the limitations of trans-\nformers (Chernyavskiy et al., 2021), or looking\ninto interpretable propaganda detection (Yu et al.,\n2021). Other work studied propaganda techniques\nin memes (Dimitrov et al., 2021a) and in code-\nswitched text (Salman et al., 2023), the relation-\nship between propaganda and coordination (Hris-\ntakieva et al., 2022), propaganda and metaphor\n(Baleato Rodr\u00edguez et al., 2023), and propaganda\nand fake news (Huang et al., 2023), and COVID-19\npropaganda in social media (Nakov et al., 2021a,b).\nSee (Da San Martino et al., 2020a) for a survey on\ncomputational propaganda detection.\nSeveral shared tasks on detecting propa-\nganda/persuasion techniques in text were also or-\nganized. SemEval-2020 task 11 on Detection of\nPersuasion Techniques in News Articles (Da San\nMartino et al., 2020) focused on news articles, and\nasked to detect the text spans and the type of propa-\nganda techniques (14 techniques). NLP4IF-2019\ntask on Fine-Grained Propaganda Detection asked\nto detect the spans of 18 propaganda techniques in\nnews articles. The SemEval-2021 task 6 on Detec-\ntion of Persuasion Techniques in Texts and Images\nfocused on 22 propaganda techniques in memes\n(Dimitrov et al., 2021b), while a WANLP\u20192022\nshared task asked to detect 20 propaganda tech-\nniques in Arabic tweets (Alam et al., 2022).\nWe ( i) extend and redesign the above annotation\nschemes, and we do so ( ii) in a multilingual setup.3002\n3 Multifacet Annotation Scheme\nThis section offers an overview of the three differ-\nent facets considered in our annotation scheme.\n3.1 Genre\nGiven a news article, we want to characterize the\nintended nature of the reporting: whether it is an\nopinion piece, it aims at objective news reporting ,\nor it is satirical . This is a multiclass annotation\nscheme at the article level.\nA satirical piece is a factually incorrect article,\nwith the intent not to deceive, but rather to call out,\nridicule, or expose behaviours considered \u2018bad\u2019. It\ndeliberately exposes real-world individuals, organi-\nsations and events to ridicule.\nGiven that the borders between opinion and ob-\njective news reporting might sometimes not be\nfully clear, we provide in Appendix A.1 an excerpt\nfrom the annotation guidelines with some rules that\nwere used to resolve opinion vs.reporting cases.\n3.2 Framing\nGiven a news article, we are interested in iden-\ntifying the frames used in the article. For this\npurpose, we adopted the concept of framing in-\ntroduced in (Card et al., 2015) and the taxonomy\nof 14 generic framing dimensions, their acronym is\nspecified in parenthesis: Economic (E) ,Capacity\nand resources (CR) ,Morality (M) ,Fairness and\nequality (FE) ,Legality, constitutionality and ju-\nrisprudence (LCJ) ,Policy prescription and evalua-\ntion (PPE) ,Crime and punishment (CP) ,Security\nand defense (SD) ,Health and safety (HS) ,Quality\nof life (QOL) ,Cultural identity (CI) ,Public opin-\nion (PO) ,Political (P) , and External regulation and\nreputation (EER) .\nThis is a multiclass multilabel annotation at the\narticle level.\n3.3 Persuasion Techniques\nGiven a news article, we identify the uses of per-\nsuasion techniques in it. These techniques are char-\nacterized by a specific use of language in order to\ninfluence the readers. We use a 2-level persuasion\ntechniques taxonomy, which is an extended version\nof the flat taxonomy introduced in Da San Martino\net al. (2019). At the top level, there are 6 coarse-\ngrained types of persuasion techniques: Attack on\nReputation ,Justification ,Simplification ,Distrac-\ntion,Call, and Manipulative Wording . We describe\nthem in more detail below.Attack on reputation: The argument does not\naddress the topic, but rather targets the participant\n(personality, experience, deeds) in order to question\nand/or to undermine their credibility. The object of\nthe argumentation can also refer to a group of indi-\nviduals, an organization, an object, or an activity.\nJustification: The argument is made of two parts,\na statement and an explanation or an appeal, where\nthe latter is used to justify and/or to support the\nstatement.\nSimplification: The argument excessively simpli-\nfies a problem, usually regarding the cause, the\nconsequence, or the existence of choices.\nDistraction: The argument takes focus away from\nthe main topic or argument to distract the reader.\nCall: The text is not an argument, but an encour-\nagement to act or to think in a particular way.\nManipulative wording: the text is not an argument\nper se, but uses specific language, which contains\nwords or phrases that are either non-neutral, confus-\ning, exaggerating, loaded, etc., in order to impact\nthe reader emotionally.\nThese six types are further subdivided into 23\nfine-grained techniques, i.e., five more than in\n(Da San Martino et al., 2019). Figure 1 gives\nan overview of our 2-tier persuasion techniques\ntaxonomy. A more comprehensive definitions of\nthese techniques, accompanied with some exam-\nples, is given in Appendix B and in (Piskorski et al.,\n2023a). Note that our list of 23 techniques differs\nfrom (Da San Martino et al., 2019) not only be-\ncause new techniques were added. For example,\ntheir Whataboutism included two separate aspects:\naccusing of hypocrisy the opponent and distracting\nfrom the current topic. Here, we refer to the for-\nmer aspect as the technique Appeal to Hypocrisy ,\ni.e., in our work Whataboutism covers only the\ndistracting-from-the-current topic aspect.\nThe persuasion technique annotation is a multi-\nclass multilabel annotation at the span level.\n4 Dataset Description\nWe feature six languages: English, French, Ger-\nman, Italian, Polish, and Russian. The English\narticles are the ones from (Da San Martino et al.,\n2019), but we slightly modified their annotations\nfor persuasion techniques to match the guidelines\nof this work (see Section 3.3). As genre and fram-\ning annotations for English were not present in\n(Da San Martino et al., 2019), we added them fol-\nlowing the guidelines for the other languages.3003\nATTACK ON REPUTATION\nName Calling or Labelling [AR:NCL]: a form of argument in which\nloaded labels are directed at an individual, group, object or activity,\ntypically in an insulting or demeaning way, but also using labels the target\naudience finds desirable.\nGuilt by Association [AR:GA]: attacking the opponent or an activity by\nassociating it with a another group, activity or concept that has sharp\nnegative connotations for the target audience.\nCasting Doubt [AR:D]: questioning the character or personal attributes of\nsomeone or something in order to question their general credibility or\nquality.\nAppeal to Hypocrisy [AR:AH]: the target of the technique is attacked on\nits reputation by charging them with hypocrisy/inconsistency.\nQuestioning the Reputation [AR:QR]: the target is attacked by making\nstrong negative claims about it, focusing specially on undermining its\ncharacter and moral stature rather than relying on an argument about the\ntopic.\nJUSTIFICATION\nFlag Waving [J:FW]: justifying an idea by exhaling the pride of a group or\nhighlighting the benefits for that specific group.\nAppeal to Authority [J:AA]: a weight is given to an argument, an idea or\ninformation by simply stating that a particular entity considered as an\nauthority is the source of the information.\nAppeal to Popularity [J:AP]: a weight is given to an argument or idea by\njustifying it on the basis that allegedly \"everybody\" (or the large majority)\nagrees with it or \"nobody\" disagrees with it.\nAppeal to Values [J:A V]: a weight is given to an idea by linking it to values\nseen by the target audience as positive.\nAppeal to Fear, Prejudice [J:AF]: promotes or rejects an idea through the\nrepulsion or fear of the audience towards this idea.\nDISTRACTION\nStrawman [D:SM]: consists in making an impression of refuting an\nargument of the opponent\u2019s proposition, whereas the real subject of the\nargument was not addressed or refuted, but instead replaced with a false one.\nRed Herring [D:RH]: consists in diverting the attention of the audience\nfrom the main topic being discussed, by introducing another topic, which is\nirrelevant.\nWhataboutism [D:W]: a technique that attempts to discredit an opponent\u2019s\nposition by charging them with hypocrisy without directly disproving their\nargument.\nSIMPLIFICATION\nCausal Oversimplification [S:CaO]: assuming a single cause or reason\nwhen there are actually multiple causes for an issue.\nFalse Dilemma or No Choice [S:FDNC]: a logical fallacy that presents\nonly two options or sides when there are many options or sides. In extreme,\nthe author tells the audience exactly what actions to take, eliminating any\nother possible choices.\nConsequential Oversimplification [S:CoO]: is an assertion one is making\nof some \"first\" event/action leading to a domino-like chain of events that\nhave some significant negative (positive) effects and consequences that\nappear to be ludicrous or unwarranted or with each step in the chain more\nand more improbable.\nCALL\nSlogans [C:S]: a brief and striking phrase, often acting like emotional\nappeals, that may include labeling and stereotyping.\nConversation Killer [A:CK]: words or phrases that discourage critical\nthought and meaningful discussion about a given topic.\nAppeal to Time [C:AT]: the argument is centred around the idea that time\nhas come for a particular action.\nMANIPULATIVE WORDING\nLoaded Language [MW:LL]: use of specific words and phrases with\nstrong emotional implications (either positive or negative) to influence and\nconvince the audience that an argument is valid.\nObfuscation, Intentional Vagueness, Confusion [MW:OVC]: use of\nwords that are deliberately not clear, vague or ambiguous so that the\naudience may have its own interpretations.\nExaggeration or Minimisation [MW:EM]: consists of either representing\nsomething in an excessive manner or making something seem less\nimportant or smaller than it really is.\nRepetition [MW:R]: the speaker uses the same phrase repeatedly with the\nhopes that the repetition will lead to persuade the audience.\nFigure 1: Persuasion techniques in our 2-tier taxon-\nomy. The six coarse-grained techniques are subdivided\ninto 23 fine-grained ones. An acronym for each tech-\nnique is given in squared brackets.4.1 Article Selection\nWe collected articles in French, German, Italian,\nPolish, and Russian, published in the period be-\ntween 2020 and mid-2022, and revolving around\nvarious globally discussed topics, including the\nCOVID-19 pandemic, abortion-related legislation,\nmigration, Russo-Ukrainian war, some local events\nsuch as parliamentary elections, etc. We con-\nsidered both mainstream media and \u201calternative\u201d\nmedia sources that could potentially spread mis-\n/disinformation. For the former, we used various\nnews aggregation engines, e.g., Google News1, Eu-\nrope Media Monitor2, etc., which cover sources\nwith different political orientation, whereas for the\nlatter, we used online services such as MediaBi-\nasFactCheck3and NewsGuard.4We extracted the\ncontent of the articles either with Trafilatura (Bar-\nbaresi, 2021) or, in few cases, manually.\n4.2 Annotation Process\nWe annotated each text for genre, framing, and per-\nsuasion techniques using the taxonomy described\nin Section 3. The main drive behind these multi-\nlayer annotation is to cover various complemen-\ntary aspects of what makes a text persuasive, i.e.,\nthe genre, the framing (what key aspects are high-\nlighted), and the rhetoric (which persuasion tech-\nniques are used). While genre and framing were\nannotated at the document level, we annotated the\npersuasion techniques at the span level.\nThe pool of annotators consisted of circa 40 per-\nsons, all native or near-native speakers of the lan-\nguage they annotated for. The majority of the anno-\ntators could be divided into two main groups with\nrespect to their background: (a) media analysts,\nfact-checkers, and disinformation experts, and (b)\nresearchers and experts in linguistics and computa-\ntional linguistics. Note that 80% of our annotators\nhad prior experience in performing linguistic anno-\ntations of news-like texts.\nWe divided the annotation process into three\nphases: ( i) training phase, during which single an-\nnotators were tasked to read the annotation guide-\nlines (Piskorski et al., 2023a), participate in on-\nline multichoice question-like training, and carry\nout pilot annotations; ( ii) text annotation phase, in\nwhich each document was annotated by at least\n1https://news.google.com\n2https://emm.newsbrief.eu\n3https://mediabiasfactcheck.com\n4https://www.newsguardtech.com3004\ntwo annotators independently; and ( iii) curation\nphase, in which the independent annotations were\njointly discussed by the annotators and a curator\n(a more experienced annotator, whose role was to\nfacilitate making a decision about the final anno-\ntations). We used INCEpTION (Klie et al., 2018)\nas our annotation platform (see Appendix C). An\nexcerpt from the annotation guidelines is provided\nin Appendix A.\n4.2.1 Text Annotation\nEach document was annotated by at least two an-\nnotators.\nWhile the framing dimensions in the dataset\nwere labeled at the document level, the annota-\ntors were tasked to label, for each type of framing\npresent in a document, at least one corresponding\ntext span for the sake of keeping track of what\ntriggered the choice of that framing.\nOn a weekly basis: ( i) reports were sent to anno-\ntator pairs highlighting the complementary and the\npotentially conflicting annotations, which helped\nthe annotators converge to a common understand-\ning of the task, and ( ii) regular meetings were held\nwith all annotators to align and to discuss specific\nannotation cases.\n4.2.2 Annotation Curation\nOnce the individual annotations for a document\nhave been accomplished, a curator, with the help of\nannotators, ( i) merged the complementary annota-\ntions (tagged only by one annotator), ( ii) resolved\nthe identified potential label conflicts, and ( iii) car-\nried out global consistency analysis. In order to re-\nsolve global inconsistencies, various spreadsheets\nwere automatically generated, e.g., a spreadsheet\nwith all text snippets (together with the local con-\ntext) labelled with persuasion techniques sorted\nalphabetically, which was used by the curators to\nexplore: ( i) whether similar text snippets (dupli-\ncates or near duplicates) were tagged with the same\nor a similar label (which should be intuitively the\ncase in most situations), and ( ii) whether there were\nany recurring inconsistencies when labelling simi-\nlar text snippets, e.g., decide and propagate multil-\nabel annotations for certain text snippets for which\nonly a single annotation were done (complementar-\nity). The global consistency analysis step sketched\nabove proved to be essential to ensure the high\nquality of the annotations.4.3 Annotation Quality\nWe measured the Inter-Annotator Agreement (IAA)\nusing Krippendorf\u2019s \u03b1, achieving a value of .342.\nThis is lower than the recommended threshold of\n.667, but we should note that this value represents\nthe agreement level before curation, and as such,\nit is more representative of the curation difficulty\nrather than of the quality of the final cosolidated\nannotations. We used the IAA during the cam-\npaign to allocate curation roles and to remove low-\nperforming annotators.\nWe further studied the IAA by ranking the an-\nnotators by their performance with respect to the\nground truth on the subset of documents they anno-\ntated. We then split the annotators into two groups:\ntopandlowbased on the median micro- F1. Their\nrespective values of \u03b1were .415 and .250. Finally,\nwe considered the \u03b1of the group of curators, based\non Italian, which was the only language with two\ncurators, achieving a score of .588, which is lower\nbut close to the recommended value.\n4.4 Statistics\n4.4.1 Distribution\nTable 1 gives some high-level statistics about our\ndataset, organized per language, including average\nnumber of persuasion techniques, their length and\nthe number of frames per document. Tables 2 and\n3 show the distribution of articles per language,\ngenre, and topic. Table 4 presents the number of\nframing dimensions per language.\nFigure 2 shows the normalised probability dis-\ntribution of the fine-grained technique knowing\nthe topic, re-weighted with the inverse document\nfrequency of the technique: Pr(tech|topic )\u00b7\nid f(tech ), yielding a tfidf-like vectorization of the\ntopics. This figure highlights the key characteris-\ntics of the techniques used more frequently in a\ntopic compared to other topics. We can see that,\ne.g., the most used techniques for COVID-19 ,Cli-\nmate Change , and Abortion areCasting Doubt ,\nAppeal to Hypocrisy , and Appeal to Values , respec-\ntively. Comparing the proportional use of tech-\nniques across the topics, we can see that, e.g.,Ap-\npeal to Time andAppeal to Fear are most charac-\nteristic of Climate Change andMigration , respec-\ntively. Appendix C gives additional information\nregarding the frequency of the techniques and fram-\nings with across languages and topics.3005\nlanguage #DOC #WORD #CHAR #SPANS AV G cAV G pAV G frAV G ptAV G ac\nEN 536 469K 2,834K 9K 5.3K 26 4 17 .014\nFR 211 153K 959K 7.4K 4.5K 25 4 36 .018\nIT 303 186K 1,214K 7.9K 4.0K 21 6 26 .018\nPL 194 144K 1,028K 3.8K 5.3K 31 7 20 .027\nDE 177 104K 751K 5.1K 4.2K 21 4 29 .021\nRU 191 104K 753K 4.1K 3.9K 23 4 22 .035\nall 1,612 1,160K 8,339K 37.6K 4.6K 24 4 25 .022\nTable 1: Statistics about the data for each language: total number of documents (#DOC), total number of words\n(#WORD), total number of characters (#CHAR), total number of text spans annotated with persuasion techniques\n(#SPANS), average document length counted in characters ( AV G c), average document length counted in paragraphs\n(AV G p), average number of frames per document ( AV G fr), average number of persuasion techniques per document\n(AV G pt), and average number of annotated characters ( AV G ac).\nFigure 2: How characteristic of a given topic is the use of the given techniques. The number of techniques is\nnormalized per topic and multiplied by the inverse document frequency of the technique: Pr(tech|topic )\u00b7id f(tech ).\nGenre\nlanguage opinion report satire\nEN 402 95 19\nFR 138 58 15\nIT 233 59 11\nPL 139 34 21\nDE 115 36 26\nRU 125 55 11\nall 1152 337 103\nTable 2: Data statistics per genre.\n4.4.2 Persuasion Techniques Co-occurrence\nWe studied how persuasion techniques co-occur\nwhen an instance of a technique is a proper sub-\npart (fully covered as a span) of another one, as\nthis gives an insight on how techniques tend to be\ncombined and structured as well as an indication of\nwhich techniques are hard to discriminate between.\nWe consider that an annotated span is a subpart of\nanother one if its span is strictly within the other\nand if the length is maximum 2/3 of the other. Fig-\nure 3 shows the number of such co-occurrences and,\nin order to get a clearer picture, we remove tech-\nniques co-occurring only with Loaded Language or\nManipulative Wording , as our analysis showed that\nthey are the most prevalent and tend to co-occur\nwith almost all other techniques.Topic\nlanguage A CC C19 M O RU\nEN - - - - - -\nFR 6 22 23 13 67 80\nIT 0 27 36 43 95 102\nPL 19 17 26 4 62 66\nDE 1 24 29 13 28 82\nRU 11 6 12 4 73 84\nall 37 96 126 77 325 414\nTable 3: Number of documents from each topic: abor-\ntion(A), climate change (CC), COVID-19 (C19), Mi-\ngration (M), Other (O), and the Russia\u2013Ukraine war\n(RU). For English, we relied on a preexisting dataset,\nfor which we did not have annotations for topic.\nWe can see that only Attack on Reputation ,Jus-\ntification andSimplification tend to be combined\nwith another technique. Notably, we can remark\nthatConsequential Oversimplification often uses\nAppeal to Fear , while Causal Oversimplification\nuses Casting Doubt .Questioning the Reputation\nandCasting Doubt have a high co-occurrence, sug-\ngesting that they are hard to distinguish. Appeal\nto Fear andCasting Doubt are the most frequently\nappearing techniques as part of another technique.\nThese statistics suggest an underlying hierarchy of\ntechniques, which we plan to study in future work.3006\nlanguage CI CP CR E ERR FE HS LCJ M P PO PPE QOL SD\nEN 33 262 37 44 198 123 64 265 219 317 52 126 98 197\nFR 25 19 59 90 83 26 66 39 57 127 26 28 32 118\nIT 47 72 157 219 136 55 156 77 68 226 43 138 101 209\nPL 45 49 79 199 98 34 182 48 71 160 92 115 85 122\nDE 55 10 78 46 22 27 109 19 29 61 22 39 18 124\nRU 15 83 44 151 58 24 92 66 32 58 23 18 31 124\nTable 4: Statistics about the distribution of framings.\nFigure 3: Statistics about how frequently one persuasion\ntechnique (on the x-axis) is properly included as part\nof another technique (on the y-axis), with a minimum\ncount of 15. The most prevalent combination of properly\nincluded techniques, namely, Loaded Language within\nName Calling is not included for better visibility.\n5 Experiments\nThe aim of our experiments is to provide base-\nlines and to explore the impact of multilingual data\non three classification tasks: for genre, for fram-\ning, and for persuasions techniques (PT). Genre\nand framing were annotated at the document level\nand the classification is multiclass and multilabel,\nrespectively. We treated PT classification in two\nways: (a) as a multiclass classification problem as\nin (Da San Martino et al., 2019), where, given a\nspan as an input, we predict the persuasion tech-\nnique in that span, in order to compare to the pre-\nvious state of the art; (b) as a multilabel token\nclassification problem, where, contrary to the pre-\nvious state of the art, we predict simultaneously the\nlocation and the label of the PT, which allows for\noverlapping classes . We report micro-average pre-\ncision, recall and F1as well as macro-average F1.\nFor all tasks, we experimentally assess the quality\nof monolingual models vs. a multilingual model\ntrained on all languages.Additionally, for persuasion technique classifica-\ntion, we explored (a) the granularity of the taxon-\nomy used in the input data: fine-grained (23 labels)\nor binary (presence or absence of a technique); (b)\nthe granularity of the data after aggregating the\nresults of the classifier: fine-grained (23 labels),\ncoarse-grained (6 labels), binary; and (c) the fo-\ncus of the classification, i.e., at which level the\nlabels are aggregated: paragraph level (split at new\nlines), sentence level (ad-hoc language-aware sen-\ntence splitter), and token level (using the RoBERTa\ntokenizer).\n5.1 Models\nWe used a multilingual pre-trained transformer,\nxlm-roberta-large (Conneau et al., 2020),\nand we customized the last layers depending on\nthe task (sigmoid for multilabel, softmax for multi-\nclass) and at the relevant level (sequence or token).\nAs persuasion technique classification requires\npredicting multilabel spans over long documents,\nwe needed to overcome the pre-trained RoBERTa\u2019s\ninherent inability to process texts longer than 512\ntokens). Thus, we implemented chunking and pool-\ning, in pre- and post-processing, respectively. We\nperformed the chunking in a redundant way using\na sliding window of 256 tokens. After inference,\nwe aligned the 512 length token vectors, and max-\npooled the overlapping tokens to a resulting length\nequal to the original input vector. We also imple-\nmented multilabel support at the token level, by\nadding a sigmoid layer on top of the output and\nby changing the loss to Binary Cross Entropy. See\nAppendix E for more details.\n5.2 Results\nThe results of the evaluation on genre and framing\nclassification are shown in Table 5. For framing,\nthe performance of the multilingual classifier has\na significantly higher macro F1score than for any\nindividual language, but the micro- F1score is not\nalways higher, notably for English.3007\nGenre classification\nLang. P R microF1macroF1\nall .548 .833 .661 .592\nEN .813 .790 .800 .504\nFR .966 .875 .918 .602\nIT .808 .783 .795 .472\nPL .936 .900 .918 .811\nDE .693 .741 .716 .681\nRU .795 .759 .777 .814\nFraming classification\nLang. P R microF1macroF1\nall .697 .608 .649 .583\nEN .706 .651 .677 .504\nFR .653 .473 .549 .392\nIT .622 .580 .600 .530\nPL .665 .561 .609 .547\nDE .590 .387 .468 .298\nRU .630 .333 .436 .261\nTable 5: Genre (top) and framing (bottom) evaluation\nresults for different languages, using XLM-RoBERTa.\nFor genre, this is not the case, as monolingual\nmodels have better performance. In both cases, the\ntexts were truncated to the first 512 tokens. This\nis critical for the framing task, as it can appear\nanywhere in the text, while for the genre task the\nwriting style is, in general, uniform throughout text.\nFor the persuasion techniques task, Table 6 com-\npares training on a single language to training on\nall languages and then testing on a specific target\nlanguage. The micro- F1score of the multilingual\nmodel is comparable to the monolingual one, be-\ning on average .01 point lower, but macro- F1is\nconsistently superior and is on average .034 points\nhigher. Next, Table 7 compares to the state of\nthe art, reusing the English train and dev folds\nfrom (Da San Martino et al., 2020). When using\nonly EN data, the micro F1score is .565, which is\nabout .05 points lower than the best reported per-\nformance. We provide this as a point of reference,\ntaking into account that our system, is a vanilla\nmulticlass model without engineered features or\nthorough hyper-parameter tuning. When trained\nusing both the English train fold and our new multi-\nlingual data, the results improve by .018 micro- F1\nand by macro- F1.058 points. The transfer capa-\nbilities of the model are very good as in the case\nof training without English data (third row), the\nperformance is only .076 points lower on average\ncompared to using English data only. These results\nshow an overall positive impact of multilingual\ntransfer learning.Monolingual models\nLang. P R microF1macroF1\nEN .499 .313 .385 .173\nFR .401 .274 .325 .230\nIT .485 .359 .412 .214\nPL .352 .212 .265 .168\nDE .397 .342 .368 .213\nRU .340 .305 .322 .157\nmultilingual models\nLang. P R microF1macroF1\nall .423 .300 .351 .258\nEN .497 .329 .396 .187\nFR .416 .296 .346 .276\nIT .467 .323 .382 .229\nPL .358 .217 .270 .221\nDE .406 .304 .348 .246\nRU .336 .322 .329 .201\nTable 6: Persuasion techniques evaluation results for\neach language when trained on (a) monolingual data,\nand (b) multilingual data (all languages), using our mul-\ntilabel XLM-R OBERT Aclassifier, and predicting at the\nsentence level.\nTable 8 shows the results for several experiments\non the persuasion techniques task using a token-\nlevel multilabel model under various settings. We\nobserve that we can improve the performance by\nwidening the focus from the token to the sentence\nand then to the paragraph level. In a similar way,\nthe performance is improved by going from fine-\ngrained to coarse-grained or even to binary classifi-\ncation. In the coarse-grained setting, both micro- F1\nimproves by .126 and macro- F1improves by .101\npoints compared to the fine-grained setting. This\nsuggests that pinpointing the exact span of a per-\nsuasion technique correctly is comparatively more\ndifficult than classifying it.\nWe can further see in Table 8 that the perfor-\nmance of the binary classifier at the paragraph level\nand with fine-grained granularity achieves a micro-\nF1score of .827, which is the highest score we\nreport in this table. It makes the model suitable for\nreal-world use, e.g., to flag paragraphs for review\nby a human analyst or for further classification by\na more fine-grained model (we leave this for fu-\nture work). Moreover, we observe that the model\ntrained on fine-tuned labels outperforms the model\ntrained on binary labels when evaluated on binary\ndata. Even in the case of detecting only the pres-\nence of a persuasion technique, the extra informa-\ntion included when assigning a class does indeed\nhelp improve the performance of the system.3008\nTrain Test P R microF1macroF1\nEN EN .323 .284 .565 .302\nMulti+EN EN .363 .358 .583 .360\nMulti EN .245 .300 .489 .269\nTable 7: Persuasion techniques: comparison to the state\nof the art of an XLM RoBERTa multiclass classifier\nevaluated on the EN test data and trained on an EN\ncorpus, our multilingual corpus, and our multilingual\ncorpus without EN data. We report macro precision and\nrecall.\nMode Gran. Gran. Focus P R micro macro\nTrain Eval F1F1\nB B B P .895 .691 .780 -\nB B B S .753 .531 .623 -\nB B B T .614 .266 .371 -\nM F B P .890 .773 .827 -\nM F B S .757 .599 .669 -\nM F B T .664 .499 .570 -\nM F C P .664 .536 .593 .489\nM F C S .532 .387 .448 .345\nM F C T .405 .265 .320 .261\nM F F P .537 .297 .382 .332\nM F F S .423 .300 .351 .258\nM F F T .316 .206 .249 .202\nTable 8: Persuasion techniques evaluation in different\nsettings using our XLM-RoBERTa multilabel token-\nlevel classifiers on our full multilingual dataset. Shown\nare results for fine-grained (F) vs. binary (B) classifica-\ntion, as well as for different granularities of the taxon-\nomy after aggregating the output as binary (B) detection\nof persuasion vs. fine-grained (F) vs. coarse-grained\n(C), and evaluating at the token (T) vs. sentence (S) vs.\nparagraph (P) level.\n6 Conclusion and Future Work\nWe presented a new multilingual multifacet dataset\nfor understanding the news in terms of genre, fram-\ning, and persuasion techniques. The dataset covers\ncurrent topics of public interest in six European lan-\nguages, and contains 1,612 documents with more\nthan 37k annotated spans. We further performed a\nnumber of multilabel classification experiments us-\ning state-of-the-art multilingual transformer-based\nmodels, exploring different levels of granularity\nand focus. Our experiments showed the utility of\nmultilingual representations even when evaluated\non a specific language. We hope that our dataset\nwill foster the development of methods and tools\nto support the analysis of online media content.\nIn future work, we plan to do in-depth analysis\nof the data, extend it to more languages, including\nnon Indo-European ones with non-Latin scripts,\nand other genres of text, e.g., social media posts.Note An extended version of the dataset pre-\nsented in this paper was used in the context of\nSemEval-2023 Task 3 on Detecting the genre, the\nframing, and the persuasion techniques in online\nnews in a multilingual set-up (Piskorski et al.,\n2023b),5where it was augmented with a new\ntest set, including three new languages: Georgian,\nGreek, and Spanish.\nWe make both the present and SemEval-2023\ntask 3 versions of the dataset publicly acces-\nsible to the community for research purposes.\nFor further information on the dataset and fu-\nture releases please refer to https://joedsm.\ngithub.io/pt-corpora/ .\n7 Limitations\nDataset Representativeness Our dataset covers\na range of topics of public interest (COVID-19,\nclimate change, abortion, migration, the Russo-\nUkrainian war, and local elections) as well as media\nfrom all sides of the political spectrum. However,\nit should not be seen as representative of the media\nin any country, nor should it be seen as perfectly\nbalanced in any specific way.\nBiases Human data annotation involves some\ndegree of subjectivity. To mitigate this, we cre-\nated a comprehensive 60-page guidelines document\n(Piskorski et al., 2023a), which we updated from\ntime to time to clarify newly arising important cases\nduring the annotation process. We further had qual-\nity control steps in the data annotation process, and\nwe have been excluding low-performing annotators.\nDespite all this, we are aware that some degree of\nintrinsic subjectivity will inevitably be present in\nthe dataset and will eventually be learned by mod-\nels trained on it.\nBaseline Models The reported experiments can\nbe seen as strong baselines as they include fairly\nsmall encoder-only transformer architectures. We\nleave for future work the exploration of other archi-\ntectures and modeling techniques that are known\nto improve the efficiency and to reduce the compu-\ntational requirements of the used models, e.g., few-\nshot and zero-shot in-context learning, instruction-\nbased evaluation, multitask learning, etc.\nModel biases We did not explore whether and to\nwhat extent our dataset contains unwanted biases.\n5https://propaganda.math.unipd.it/\nsemeval2023task3/3009\n8 Ethics and Broader Impact\nBiases We sampled the news for our dataset in\norder to have a non-partisan view of the topics,\nstriving to the extent possible to have a balanced\nrepresentation of the points of view on the topics,\nbut this was best effort and was not strictly en-\nforced. This should be taken into account when\nusing this data for doing media analysis. The data\nwas annotated without taking into account the an-\nnotator\u2019s feeling about the particular topic; rather,\nthis was done objectively with focus on whether\nspecific frames of persuasion techniques were used.\nWe did not use crowdsourcing, and our annotators\nwere fairly paid as part of their job duties.\nIntended Use and Misuse Potential Our models\ncan be of interest to the general public and could\nalso save time to fact-checkers. However, they\ncould also be misused by malicious actors. We,\ntherefore, ask researchers to exercise caution.\nEnvironmental Impact We would like to warn\nthat the use of large language models requires a\nlot of computations and the use of GPUs/TPUs\nfor training, which contributes to global warming\n(Strubell et al., 2019). This is a bit less of an issue\nin our case, as we do not train such models from\nscratch, we just fine-tune them.\nAcknowledgments\nWe are greatly indebted to all the annotators from\ndifferent organizations, including, inter alia, the\nEuropean Commission, the European Parliament,\nthe University of Padova, the Qatar Computing\nResearch Institute, HBKU, and Mohamed bin Za-\nyed University of Artificial Intelligence, who took\npart in the annotations, and notably to the language\ncurators whose patience and diligence have been\nfundamental for ensuring the quality of the dataset.\nReferences\nFiroj Alam, Hamdy Mubarak, Wajdi Zaghouani, Gio-\nvanni Da San Martino, and Preslav Nakov. 2022.\nOverview of the WANLP 2022 shared task on propa-\nganda detection in Arabic. In Proceedings of the The\nSeventh Arabic Natural Language Processing Work-\nshop (WANLP) , pages 108\u2013118, Abu Dhabi, United\nArab Emirates (Hybrid). Association for Computa-\ntional Linguistics.\nFiroj Alam, Shaden Shaar, Fahim Dalvi, Hassan Saj-\njad, Alex Nikolov, Hamdy Mubarak, Giovanni\nDa San Martino, Ahmed Abdelali, Nadir Durrani,Kareem Darwish, Abdulaziz Al-Homaid, Wajdi Za-\nghouani, Tommaso Caselli, Gijs Danoe, Friso Stolk,\nBritt Bruntink, and Preslav Nakov. 2021. Fighting\nthe COVID-19 infodemic: Modeling the perspec-\ntive of journalists, fact-checkers, social media plat-\nforms, policy makers, and the society. In Findings\nof EMNLP , pages 611\u2013649, Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nMohammad Ali and Naeemul Hassan. 2022. A sur-\nvey of computational framing analysis approaches.\nInProceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n9335\u20139348, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nDaniel Baleato Rodr\u00edguez, Verna Dankers, Preslav\nNakov, and Ekaterina Shutova. 2023. Paper bullets:\nModeling propaganda with the help of metaphor. In\nFindings of the Association for Computational Lin-\nguistics: EACL 2023 , pages 472\u2013489, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nAdrien Barbaresi. 2021. Trafilatura: A web scraping\nlibrary and command-line tool for text discovery and\nextraction. In Proceedings of the Joint Conference\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing:\nSystem Demonstrations , pages 122\u2013131. Association\nfor Computational Linguistics.\nAlberto Barr\u00f3n-Cedeno, Israa Jaradat, Giovanni\nDa San Martino, and Preslav Nakov. 2019. Proppy:\nOrganizing the news based on their propagandistic\ncontent. Information Processing & Management ,\n56(5).\nDallas Card, Amber E. Boydstun, Justin H. Gross, Philip\nResnik, and Noah A. Smith. 2015. The media frames\ncorpus: Annotations of frames across issues. In Pro-\nceedings of the 53rd Annual Meeting of the Asso-\nciation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers) , pages 438\u2013\n444, Beijing, China. Association for Computational\nLinguistics.\nLoretta H Cheeks, Tracy L Stepien, Dara M Wald, and\nAshraf Gaffar. 2020. Discovering news frames: An\napproach for exploring text, content, and concepts in\nonline news sources. In Cognitive Analytics: Con-\ncepts, Methodologies, Tools, and Applications , pages\n702\u2013721. IGI Global.\nKeyu Chen, Marzieh Babaeianjelodar, Yiwen Shi,\nKamila Janmohamed, Rupak Sarkar, Ingmar We-\nber, Thomas Davidson, Munmun De Choudhury,\nJonathan Huang, Shweta Yadav, Ashiqur Khud-\naBukhsh, Chris T Bauch, Preslav Nakov, Orestis\nPapakyriakopoulos, Koustuv Saha, Kaveh Khosh-\nnood, and Navin Kumar. 2023. Partisan US news\nmedia representations of Syrian refugees. Proceed-\nings of the International AAAI Conference on Web\nand Social Media , 17(1):103\u2013113.3010\nAnton Chernyavskiy, Dmitry Ilvovsky, and Preslav\nNakov. 2021. Transformers: \u201cThe end of history\u201d\nfor NLP? In Proceedings of the European Confer-\nence on Machine Learning and Principles and Prac-\ntice of Knowledge Discovery in Databases , ECML-\nPKDD\u201921.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440\u2013\n8451, Online. Association for Computational Lin-\nguistics.\nGiovanni Da San Martino, Alberto Barr\u00f3n-Cede\u00f1o,\nHenning Wachsmuth, Rostislav Petrov, and Preslav\nNakov. 2020. SemEval-2020 task 11: Detection of\npropaganda techniques in news articles. In Proceed-\nings of the 14th International Workshop on Semantic\nEvaluation , SemEval \u201920, Barcelona, Spain.\nGiovanni Da San Martino, Stefano Cresci, Alberto\nBarr\u00f3n-Cede\u00f1o, Seunghak Yu, Roberto Di Pietro,\nand Preslav Nakov. 2020a. A survey on computa-\ntional propaganda detection. In Proceedings of the\nInternational Joint Conference on Artificial Intelli-\ngence , IJCAI-PRICAI \u201920, pages 4826\u20134832. Survey\ntrack.\nGiovanni Da San Martino, Shaden Shaar, Yifan Zhang,\nSeunghak Yu, Alberto Barr\u00f3n-Cedeno, and Preslav\nNakov. 2020b. Prta: A system to support the anal-\nysis of propaganda techniques in the news. In Pro-\nceedings of the Annual Meeting of Association for\nComputational Linguistics , ACL \u201920, pages 287\u2013293.\nAssociation for Computational Linguistics.\nGiovanni Da San Martino, Seunghak Yu, Alberto\nBarr\u00f3n-Cede\u00f1o, Rostislav Petrov, and Preslav Nakov.\n2019. Fine-grained analysis of propaganda in news\narticle. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP) , pages\n5636\u20135646, Hong Kong, China. Association for Com-\nputational Linguistics.\nDimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj\nAlam, Fabrizio Silvestri, Hamed Firooz, Preslav\nNakov, and Giovanni Da San Martino. 2021a. De-\ntecting propaganda techniques in memes. In Pro-\nceedings of the Joint Conference of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing , ACL-IJCNLP \u201921,\npages 6603\u20136617.\nDimiter Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj\nAlam, Fabrizio Silvestri, Hamed Firooz, Preslav\nNakov, and Giovanni Da San Martino. 2021b. Task\n6 at SemEval-2021: Detection of persuasion tech-\nniques in texts and images. In Proceedings of the15th International Workshop on Semantic Evalua-\ntion, SemEval \u201921, pages 70\u201398, Bangkok, Thailand.\nRobert M Entman. 1993. Framing: Towards clarifica-\ntion of a fractured paradigm. McQuail\u2019s reader in\nmass communication theory , pages 390\u2013397.\nAnjalie Field, Doron Kliger, Shuly Wintner, Jennifer\nPan, Dan Jurafsky, and Yulia Tsvetkov. 2018. Fram-\ning and agenda-setting in Russian news: a computa-\ntional analysis of intricate political strategies. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3570\u2013\n3580, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nJennifer Golbeck, Matthew Mauriello, Brooke Aux-\nier, Keval H. Bhanushali, Christopher Bonk, Mo-\nhamed Amine Bouzaghrane, Cody Buntain, Riya\nChanduka, Paul Cheakalos, Jennine B. Everett,\nWaleed Falak, Carl Gieringer, Jack Graney, Kelly M.\nHoffman, Lindsay Huth, Zhenya Ma, Mayanka Jha,\nMisbah Khan, Varsha Kori, Elo Lewis, George Mi-\nrano, William T. Mohn IV , Sean Mussenden, Tam-\nmie M. Nelson, Sean Mcwillie, Akshat Pant, Priya\nShetye, Rusha Shrestha, Alexandra Steinheimer,\nAditya Subramanian, and Gina Visnansky. 2018.\nFake news vs satire: A dataset and analysis. In Pro-\nceedings of the 10th ACM Conference on Web Sci-\nence, WebSci \u201918, page 17\u201321, Amsterdam, Nether-\nlands. Association for Computing Machinery.\nMaur\u00edcio Gruppi, Benjamin D. Horne, and Sibel Adali.\n2020. NELA-GT-2019: A large multi-labelled news\ndataset for the study of misinformation in news arti-\ncles. arXiv , 2003.08444.\nIvan Habernal, Raffael Hannemann, Christian Pol-\nlak, Christopher Klamm, Patrick Pauli, and Iryna\nGurevych. 2017. Argotario: Computational argu-\nmentation meets serious games. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations ,\nEMNLP \u201917, pages 7\u201312, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nIvan Habernal, Patrick Pauli, and Iryna Gurevych. 2018.\nAdapting serious game for fallacious argumentation\nto German: Pitfalls, insights, and best practices. In\nProceedings of the 11th International Conference\non Language Resources and Evaluation , LREC \u201918,\npages 3329\u20133335, Miyazaki, Japan. European Lan-\nguage Resources Association (ELRA).\nMomchil Hardalov, Ivan Koychev, and Preslav Nakov.\n2016. In search of credible news. In Proceedings\nof the 17th International Conference on Artificial\nIntelligence: Methodology, Systems, and Applica-\ntions , AIMSA \u201916, pages 172\u2013180, Varna, Bulgaria.\nSpringer International Publishing.\nBenjamin Horne and Sibel Adali. 2017. This just in:\nFake news packs a lot in title, uses simpler, repetitive\ncontent in text body, more similar to satire than real\nnews. arXiv , 1703.09398.3011\nKristina Hristakieva, Stefano Cresci, Giovanni\nDa San Martino, Mauro Conti, and Preslav Nakov.\n2022. The spread of propaganda by coordinated\ncommunities on social media. In Proceedings of the\n14th ACM Web Science Conference , WebSci \u201922,\npages 191\u2013201, Barcelona, Spain. Association for\nComputing Machinery.\nKung-Hsiang Huang, Kathleen McKeown, Preslav\nNakov, Yejin Choi, and Heng Ji. 2023. Faking\nfake news for real fake news detection: Propaganda-\nloaded training data generation. In Proceedings of\nthe 61st Annual Meeting of the Association for Com-\nputational Linguistics , ACL\u201923, Toronto, Canada. As-\nsociation for Computational Linguistics.\nJan-Christoph Klie, Michael Bugert, Beto Boullosa,\nRichard Eckart de Castilho, and Iryna Gurevych.\n2018. The INCEpTION platform: Machine-assisted\nand knowledge-oriented interactive annotation. In\nProceedings of the 27th International Conference on\nComputational Linguistics: System Demonstrations ,\npages 5\u20139. Association for Computational Linguis-\ntics. Event Title: The 27th International Conference\non Computational Linguistics (COLING 2018).\nHaewoon Kwak, Jisun An, and Yong-Yeol Ahn. 2020. A\nsystematic media frame analysis of 1.5 million New\nYork Times articles from 2000 to 2017. In Proceed-\nings of the 12th ACM Conference on Web Science ,\nWebSci \u201920, pages 305\u2013314, Southampton, United\nKingdom. Association for Computing Machinery.\nOr Levi, Pedram Hosseini, Mona Diab, and David Bro-\nniatowski. 2019. Identifying nuances in fake news\nvs. satire: Using semantic and linguistic cues. In\nProceedings of the Second Workshop on Natural\nLanguage Processing for Internet Freedom: Censor-\nship, Disinformation, and Propaganda , pages 31\u201335,\nHong Kong, China. Association for Computational\nLinguistics.\nSiyi Liu, Lei Guo, Kate Mays, Margrit Betke, and\nDerry Tanti Wijaya. 2019. Detecting frames in news\nheadlines and its application to analyzing news fram-\ning trends surrounding US gun violence. In Proceed-\nings of the 23rd Conference on Computational Natu-\nral Language Learning , CoNLL \u201919, pages 504\u2013514,\nHong Kong, China.\nPreslav Nakov, Firoj Alam, Shaden Shaar, Giovanni\nDa San Martino, and Yifan Zhang. 2021a. COVID-\n19 in Bulgarian social media: Factuality, harmfulness,\npropaganda, and framing. In Proceedings of the Inter-\nnational Conference on Recent Advances in Natural\nLanguage Processing , RANLP \u201921.\nPreslav Nakov, Firoj Alam, Shaden Shaar, Giovanni\nDa San Martino, and Yifan Zhang. 2021b. A second\npandemic? Analysis of fake news about COVID-19\nvaccines in Qatar. In Proceedings of the International\nConference on Recent Advances in Natural Language\nProcessing , RANLP \u201921.\nJeppe N\u00f8rregaard, Benjamin D. Horne, and Sibel Adali.\n2019. NELA-GT-2018: A large multi-labelled newsdataset for the study of misinformation in news arti-\ncles. In Proceedings of the Thirteenth International\nConference on Web and Social Media , ICWSM \u201919,\npages 630\u2013638, Munich, Germany. AAAI Press.\nJakub Piskorski, Nicolas Stefanovitch, Valerie-Anne\nBausier, Nicolo Faggiani, Jens Linge, Sopho Kharazi,\nNikolaos Nikolaidis, Giulia Teodori, Bertrand\nDe Longueville, Brian Doherty, Jason Gonin,\nCamelia Ignat, Bonka Kotseva, Eleonora Mantica,\nLorena Marcaletti, Enrico Rossi, Alessio Spadaro,\nMarco Verile, Giovanni Da San Martino, Firoj Alam,\nand Preslav Nakov. 2023a. News categorization,\nframing and persuasion techniques: Annotation\nguidelines. Technical report, European Commission\nJoint Research Centre, Ispra (Italy).\nJakub Piskorski, Nicolas Stefanovitch, Giovanni\nDa San Martino, and Preslav Nakov. 2023b.\nSemEval-2023 task 3: Detecting the category, the\nframing, and the persuasion techniques in online\nnews in a multi-lingual setup. In Proceedings of the\n17th International Workshop on Semantic Evaluation ,\nSemEval 2023, Toronto, Canada.\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nV olkova, and Yejin Choi. 2017. Truth of varying\nshades: Analyzing language in fake news and politi-\ncal fact-checking. In Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP \u201917, pages 2931\u20132937, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nMuhammad Umar Salman, Asif Hanif, Shady She-\nhata, and Preslav Nakov. 2023. Detecting propa-\nganda techniques in code-switched social media text.\narXiv:2305.14534 .\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics , pages 3645\u20133650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nSeunghak Yu, Giovanni Da San Martino, Mitra Mo-\nhtarami, James Glass, and Preslav Nakov. 2021. In-\nterpretable propaganda detection in news articles.\nInProceedings of the International Conference on\nRecent Advances in Natural Language Processing ,\nRANLP \u201921, pages 1597\u20131605. INCOMA Ltd.\nYifan Zhang, Giovanni Da San Martino, Alberto Barr\u00f3n-\nCede\u00f1o, Salvatore Romeo, Jisun An, Haewoon Kwak,\nTodor Staykovski, Israa Jaradat, Georgi Karadzhov,\nRamy Baly, Kareem Darwish, James Glass, and\nPreslav Nakov. 2019. Tanbih: Get to know what you\nare reading. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing: System Demonstrations ,\nEMNLP-IJCNLP \u201919, pages 223\u2013228, Hong Kong,\nChina. Association for Computational Linguistics.3012\nA Annotation Guidelines\nThis appendix provides an excerpt of the annotation\nguidelines (Piskorski et al., 2023a) related to news\ngenre and persuasion techniques.\nA.1 News Genre\n\u2022opinion versus reporting : in the case of news\narticles that contain citations and opinions of\nothers (i.e., not of the author), the decision\nwhether to label such article as opinion or\nreporting should in principle depend on what\nthe reader thinks the intent of the author of\nthe article was. In order to make this decision\nsimpler, the following rules were applied:\n\u2013articles that contain even a single sen-\ntence (could be even the title) that is an\nopinion of the author or suggests that the\nauthor has some opinion on the specific\nmatter should be labelled as opinion ,\n\u2013articles containing a speech or an inter-\nview with a single politician or expert,\nwho provides her/his opinions should be\nlabelled as opinion ,\n\u2013articles that \u201creport\u201d what a single politi-\ncian or expert said in an interview, con-\nference, debate, etc. should be labelled\nasopinion as well,\n\u2013articles that provide a comprehensive\noverview (spectrum) of what many dif-\nferent politicians and experts said on a\nspecific matter (e.g., in a debate), includ-\ning their opinions, and without any opin-\nion of the author, should be labelled as\nreporting ,\n\u2013articles that provide a comprehensive\noverview (spectrum) of what many differ-\nent politicians and experts said on a spe-\ncific matter (e.g., in a debate), including\ntheir opinions, and with some opinion or\nanalysis of the author (the author might\ntry to tell a story), should be labelled as\nopinion ,\n\u2013commentaries and analysis articles\nshould be labelled as opinion .\n\u2022satire : A news article that contains some small\ntext fragment, e.g., a sentence, which appears\nsatirical is not supposed to be annotated as\nsatire .A.2 Persuasion Techniques\nThe following general rules are applied when anno-\ntating persuasion techniques:\n\u2022if one has doubts whether a given text frag-\nment contains a persuasion technique, then\nthey do not annotate it, ( conservative ap-\nproach )\n\u2022select the minimal amount of text6to annotate\nin case of doubts whether to include a longer\ntext fragment or not,\n\u2022avoid personal bias (i.e., opinion and emo-\ntions) on the topic being discussed as this has\nnothing to do with the annotation of persua-\nsion techniques,\n\u2022do not exploit external knowledge to decide\nwhether given text fragment should be tagged\nas a persuasion technique,\n\u2022do not confuse persuasion technique detection\nwith fact-checking . A given text fragment\nmight contain a claim that is known to be\ntrue, but that does not imply that there are\nno persuasion techniques to annotate in this\nparticular text fragment,\n\u2022often, authors use irony (not being explicitly\npart of the taxonomy), which in most cases\nserves the purpose to persuade the reader,\nmost frequently to attack the reputation of\nsomeone or something. In such cases, the re-\nspective persuasion technique type should be\nused, or other if the use of irony does not fall\nunder any persuasion technique type in the\ntaxonomy,\n\u2022in case of quotations or reporting of what a\ngiven person has said, the annotation of the\npersuasion techniques within the boundaries\nof that quotation should be done from the per-\nspective of that person who is making some\nstatement or claim ( point of reference ) and not\nfrom the author perspective.\n6In our guidelines, we do have specific rules for each of the\npersuasion techniques of what the annotation should include,\ne.g., for the Justificaton technique, the annotation should in-\nclude certain appeal and the claim or idea it supports, if ex-\nplicitly expressed in the immediate context, or, in the case of\nLoaded Language , only the emotionally-loaded word/phrase\nshould be annotated, disregarding the context it appears in.3013\nFigure 4: Decision diagram to determine which high-level approach is used in a text. The fine-grained techniques\nare marked in color, in an attempt to reflect the rhetorical dimension: (a) ethos, i.e., appeal to authority (green), (b)\nlogos, i.e., appeal to logic (blue), and (c) pathos, e.e., appeal to emotions (yellow).\nB Definitions of the Persuasion\nTechniques\nB.1 Attack on Reputation\nName Calling or Labelling: a form of argument\nin which loaded labels are directed at an individual\nor a group, typically in an insulting or demean-\ning way. Labelling an object as either something\nthe target audience fears, hates, or on the contrary\nfinds desirable or loves. This technique calls for\na qualitative judgement that disregards facts and\nfocuses solely on the essence of the subject being\ncharacterized. This technique is in a way also a\nmanipulative wording, as it is used at the level of\nthe nominal group rather than being a full-fledged\nargument with a premise and a conclusion. For\nexample, in the political discourse, typically one\nis using adjectives and nouns as labels that refer to\npolitical orientation, opinions, personal characteris-\ntics, and association to some organisations, as well\nas insults. What distinguishes it from the Loaded\nLanguage technique (see B.6), is that it is only\nconcerned with the characterization of the subject.\nExample: \u2019Fascist\u2019 Anti-Vax Riot Sparks COVID\nOutbreak in Australia.\nGuilt by Association: Attacking the opponent or\nan activity by associating it with another group,\nactivity, or concept that has sharp negative conno-tations for the target audience. The most common\nexample, which has given its name in the literature\n(i.e. Reduction ad Hitlerum ) to that technique is\nmaking comparisons to Hitler and the Nazi regime.\nHowever, it is important to emphasize, that this\ntechnique is not restricted to comparisons to that\ngroup only. More precisely, this can be done by\nclaiming a link or an equivalence between the tar-\nget of the technique to any individual, group, or\nevent in the presence or in the past, which has or\nhad an unquestionable negative perception (e.g.,\nwas considered a failure), or is depicted in such\nway.\nExample: Manohar is a big supporter for equal\npay for equal work. This is the same policy that all\nthose extreme feminist groups support. Extremists\nlike Manohar should not be taken seriously.\nCasting Doubt: Casting doubt on the character or\nthe personal attributes of someone or something in\norder to question their general credibility or quality,\ninstead of using a proper argument related to the\ntopic. This can be done for instance, by speaking\nabout the target\u2019s professional background, as a\nway to discredit their argument. Casting doubt can\nalso be done by referring to some actions or events\ncarried out or planned by some entity that are/were\nnot successful or appear as (probably) resulting in\nnot achieving the planned goals.3014\nExample: This task is quite complex. Is his profes-\nsional background, experience and the time left\nsufficient to accomplish the task at hand?\nAppeal to Hypocrisy: The target of the technique\nis attacked on its reputation by charging them with\nhypocrisy or inconsistency. This can be done ex-\nplicitly by calling out hypocrisy directly, or more\nimplicitly by underlying the contradictions between\ndifferent positions that were held or actions that\nwere done in the past. A special way of calling out\nhypocrisy is by telling that someone who criticizes\nyou for something you did, also did it in the past.\nExample: How can you demand that I eat less\nmeat to reduce my carbon footprint if you yourself\ndrive a big SUV and fly for holidays to Bali?\nQuestioning the Reputation: This technique is\nused to attack the reputation of the target by making\nstrong negative claims about it, focusing specially\non undermining its character and moral stature\nrather than relying on an argument about the topic.\nWhether the claims are true or false is irrelevant for\nthe effective use of this technique. Smears can be\nused at any point in a discussion. One particular\nway of using this technique is to preemptively call\ninto question the reputation/credibility of an oppo-\nnent, before he had any chance to express himself,\ntherefore biasing the audience perception. Hence,\none of the name of that technique is \u201cpoisoning the\nwell.\u201d\nThe main difference between Casting Doubt (in-\ntroduced earlier) and Questioning the reputation\ntechnique is that the former focuses on questioning\nthe capacity, the capabilities, and the credibility of\nthe target, while the latter targets undermining the\noverall reputation, moral qualities, behaviour, etc.\nExample: I hope I presented my argument clearly.\nNow, my opponent will attempt to refute my argu-\nment by his own fallacious, incoherent, illogical\nversion of history\nB.2 Justification\nFlag Waving: Justifying or promoting an idea by\nexhaling the pride of a group or highlighting the\nbenefits for that specific group. The stereotypical\nexample would be national pride, and hence the\nname of the technique; however, the target group it\napplies to might be any group, e.g., related to race,\ngender, political preference, etc. The connection\nto nationalism, patriotism, or benefit for an idea,\ngroup, or country might be fully undue and is usu-\nally based on the presumption that the recipientsalready have certain beliefs, biases, and prejudices\nabout the given issue. It can be seen as an appeal\nto emotions instead to logic of the audience aiming\nto manipulate them to win an argument. As such,\nthis technique can also appear outside the form of\nwell constructed argument, by simply making men-\ntions that resonate with the feeling of a particular\ngroup and as such setting up a context for further\narguments.\nExample: We should make America great again,\nand restrict the immigration laws.\nAppeal to Authority: a weight is given to an argu-\nment, an idea or information by simply stating that\na particular entity considered as an authority is the\nsource of the information. The entity mentioned\nas an authority may, but does not need to be, an\nactual valid authority in the domain-specific field\nto discuss a particular topic or to be considered and\nserve as an expert. What is important, and makes it\ndifferent from simply sourcing information, is that\nthe tone of the text indicates that it capitalizes on\nthe weight of an alleged authority in order to justify\nsome information, claim, or conclusion. Referenc-\ning a valid authority is not a logical fallacy, while\nreferencing an invalid authority is a logical fallacy,\nand both are captured within this label. In particu-\nlar, a self-reference as an authority falls under this\ntechnique as well.\nExample: Since the Pope said that this aspect of\nthe doctrine is true we should add it to the creed.\nAppeal to Popularity: This technique gives weight\nto an argument or idea by justifying it on the basis\nthat allegedly \u201c everybody \u201d (or the vast majority)\nagrees with it or \u201c nobody \u201d disagrees with it. As\nsuch, the target audience is encouraged to gregari-\nously adopt the same idea by considering \u201c everyone\nelse\u201d as an authority, and to join in and take the\ncourse of the same action. Here, \u201c everyone else \u201d\nmight refer to the general public, key entities and\nactors in a certain domain, countries, etc. Analo-\ngously, an attempt to persuade the audience not to\ndo something because \u201c nobody else is taking the\nsame action \u201d falls under our definition of Appeal\nto Popularity.\nExample: Because everyone else goes away to col-\nlege, it must be the right thing to do.\nAppeal to Values: This technique gives weight to\nan idea by linking it to values seen by the target\naudience as positive. These values are presented\nas an authoritative reference in order to support or\nto reject an argument. Examples of such values3015\nare, for instance: tradition, religion, ethics, age,\nfairness, liberty, democracy, peace, transparency,\netc. When such values are mentioned outside the\ncontext of a proper argument by simply using cer-\ntain adjectives or nouns as a way of characterizing\nsomething or someone, such references fall under\nanother label, namely, Loaded Language , which is\na form of Manipulative Wording (see B.6).\nExample: It\u2019s standard practice to pay men more\nthan women so we\u2019ll continue adhering to the\nsame standards this company has always followed.\nAppeal to Fear, Prejudice: This technique aims\nat promoting or rejecting an idea through the repul-\nsion or fear of the audience towards this idea (e.g.,\nvia exploiting some preconceived judgements) or\ntowards its alternative. The alternative could be the\nstatus quo, in which case the current situation is\ndescribed in a scary way with Loaded Language .\nIf the fear is linked to the consequences of a deci-\nsion, it is often the case that this technique is used\nsimultaneously with Appeal to Consequences (see\nSimplification techniques in B.4), and if there are\nonly two alternatives that are stated explicitly, then\nit is used simultaneously with the False Dilemma\ntechnique (see B.4).\nExample: It is a great disservice to the Church to\nmaintain the pretense that there is nothing problem-\natical about Amoris laetitia. A moral catastrophe\nis self-evidently underway and it is not possible\nhonestly to deny its cause.\nB.3 Distraction\nStrawman: This technique consists in making an\nimpression of refuting the argument of the oppo-\nnent\u2019s proposition, whereas the real subject of the\nargument was not addressed or refuted, but instead\nreplaced with a false one. Often, this technique is\nreferred to as misrepresentation of the argument.\nFirst, a new argument is created via the covert re-\nplacement of the original argument with something\nthat appears somewhat related, but is actually a\ndifferent, a distorted, an exaggerated, or a misrep-\nresented version of the original proposition, which\nis referred to as \u201c standing up a straw man .\u201d Sub-\nsequently, the newly created \u2018 false argument (the\nstrawman) is refuted, which is referred to as \u201c knock-\ning down a straw man .\u201d Often, the strawman ar-\ngument is created in such a way that it is easier\nto refute, and thus, creating an illusion of having\ndefeated an opponent\u2019s real proposition. Fighting\na strawman is easier than fighting against a realperson, which explains the origin of the name of\nthis technique. In practice, it appears often as an\nabusive reformulation or explanation of what the\nopponent actually \u2019 means or wants.\nExample: Referring to your claim that providing\nmedicare for all citizens would be costly and a\ndanger to the free market, I infer that you don\u2019t\ncare if people die from not having healthcare, so\nwe are not going to support your endeavour .\nRed Herring: This technique consists in divert-\ning the attention of the audience from the main\ntopic being discussed, by introducing another topic.\nThe aim of attempting to redirect the argument to\nanother issue is to focus on something the person\ndoing the redirecting can better respond to or to\nleave the original topic unaddressed. The name of\nthat technique comes from the idea that a fish with\na strong smell (like a herring) can be used to divert\ndogs from the scent of someone they are following.\nA strawman (defined earlier) is also a specific type\nof a red herring in the way that it distracts from the\nmain issue by painting the opponent\u2019s argument in\nan inaccurate light.\nExample: Lately, there has been a lot of criticism\nregarding the quality of our product. We\u2019ve decided\nto have a new sale in response, so you can buy\nmore at a lower cost! .\nWhataboutism: A technique that attempts to dis-\ncredit an opponent\u2019s position by charging them\nwith hypocrisy without directly disproving their\nargument. Instead of answering a critical question\nor argument, an attempt is made to retort with a\ncritical counter-question that expresses a counter-\naccusation, e.g., mentioning double standards, etc.\nThe intent is to distract from the content of a topic\nand to switch the topic actually. There is a fine\ndistinction between this technique and Appeal to\nHypocrisy , introduced earlier, where the former is\nan attack on the argument and introduces irrelevant\ninformation to the main topic, while the latter is an\nattack on reputation and highlights the hypocrisy\nof double standards on the same or a very related\ntopic.\nExample: A nation deflects criticism of its recent\nhuman rights violations by pointing to the history\nof slavery in the United States.\nB.4 Simplification\nCausal Oversimplification: Assuming a single\ncause or reason when there are actually multiple\ncauses for an issue. This technique has the follow-3016\ning logical form(s): (a) Y occurred after X; there-\nfore, X was the only cause of Y , or (b) X caused Y;\ntherefore, X was the only cause of Y+ (although A,\nB, C...etc. also contributed to Y.)\nExample: School violence has gone up and aca-\ndemic performance has gone down since video\ngames featuring violence were introduced. There-\nfore, video games with violence should be banned,\nresulting in school improvement.\nFalse Dilemma or No Choice: Sometimes called\ntheeither-or fallacy, a false dilemma is a logical\nfallacy that presents only two options or sides when\nthere actually are many. One of the alternatives is\ndepicted as a no-go option, and hence the only\nchoice is the other option. In extreme cases, the\nauthor tells the audience exactly what actions to\ntake, eliminating any other possible choices (also\nreferred to as Dictatorship ).\nExample: There is no alternative to Pfizer Covid-\n19 vaccine. Either one takes it or one dies.\nConsequential Oversimplification: An argument\nor an idea is rejected and instead of discussing\nwhether it makes sense and/or is valid, the argu-\nment affirms, without proof, that accepting the\nproposition would imply accepting other propo-\nsitions that are considered negative. This technique\nhas the following logical form: if A will happen\nthen B, C, D, ... will happen . The core essence\nbehind this fallacy is an assertion one is making of\nsome \u2018 first\u2019 event/action leading to a domino-like\nchain of events that have some significant nega-\ntive effects and consequences that appear to be\nludicrous. This technique is characterized by ig-\nnoring and/or understating the likelihood of the\nsequence of events from the first event leading\nto the end point (last event). In order to take into\naccount symmetric cases, i.e., using Consequen-\ntial Oversimplification to promote or to support\ncertain action in a similar way, we also consider\ncases when the sequence of events leads to positive\noutcomes (i.e., encouraging people to undertake a\ncertain course of action(s), with the promise of a\nmajor positive event in the end).\nExample: If we begin to restrict freedom of speech,\nthis will encourage the government to infringe\nupon other fundamental rights, and eventually\nthis will result in a totalitarian state where citizens\nhave little to no control of their lives and decisions\nthey make .B.5 Call\nSlogans: A brief and striking phrase that may in-\nclude labeling and stereotyping. Slogans tend to\nact as emotional appeals.\nExample: Immigrants welcome, racist not!\nConversation Killer: This includes words or\nphrases that discourage critical thought and mean-\ningful discussion about a given topic. They are a\nform of Loaded Language , often passing as folk\nwisdom, intended to end an argument and quell\ncognitive dissonance.\nExample: I\u2019m not so na\u00efve or simplistic to believe\nwe can eliminate wars. You can\u2019t change human\nnature.\nAppeal to Time: The argument is centered around\nthe idea that time has come for a particular action.\nThe very timeliness of the idea is part of the argu-\nment.\nExample: This is no time to engage in the luxury\nof cooling off or to take the tranquilizing drug of\ngradualism. Now is the time to make real the\npromises of democracy. Now is the time to rise\nfrom the dark and desolate valley of segregation\nto the sunlit path of racial justice.\nB.6 Manipulative Wording\nLoaded Language: use of specific words and\nphrases with strong emotional implications (either\npositive or negative) to influence and to convince\nthe audience that an argument is valid. It is also\nknown as Appeal to Argument from Emotive Lan-\nguage .\nExample: They keep feeding these people with\ntrash . They should stop.\nObfuscation, Intentional Vagueness, Confusion:\nThis fallacy uses words that are deliberately not\nclear, so that the audience may have its own inter-\npretations. For example, an unclear phrase with\nmultiple or unclear definitions is used within the\nargument and, therefore, does not support the con-\nclusion. Statements that are imprecise and inten-\ntionally do not fully or vaguely answer the question\nposed fall under this category too.\nExample: Feathers cannot be dark, because all\nfeathers are light!\nExaggeration or Minimisation: This technique\nconsists of either representing something in an ex-\ncessive manner \u2013 by making things larger, better,\nworse (e.g., the best of the best ,quality guaranteed )\n\u2013 or by making something seem less important or3017\nsmaller than it really is (e.g., saying that an insult\nwas just a joke), downplaying the statements and\nignoring the arguments and the accusations made\nby an opponent.\nExample: From the seminaries, to the clergy, to the\nbishops, to the cardinals, homosexuals are present\nat all levels, by the thousand .\nRepetition: The speaker uses the same word,\nphrase, story, or imagery repeatedly with the hope\nthat the repetition will lead to persuade the audi-\nence.\nExample: Hurtlocker deserves an Oscar . Other\nfilms have potential, but they do not deserve an\nOscar like Hurtlocker does . The other movies may\ndeserve an honorable mention but Hurtlocker de-\nserves the Oscar .\nFigure 4 shows a decision diagram that can be used\nto determine the high-level persuasion approach.\nC Annotation Platform\nFigure 5 shows the interface of Inception , the an-\nnotation platform we used, with an example of\nmultilabel text annotation. We chose this platform\nas it offers the functionality to create multilayer\nand overlapping text annotations and visual tools\nto carry out merging and to consolidate conflicting\nannotations.\nD Supplementary Corpus Statistics\nBelow, we provide additional statistics about our\ndataset.\nD.1 Overall Annotation Size\nFirst, Figure 6 shows a histogram of the number\nof annotated characters for all languages and doc-\nument types in the dataset. We can see a skewed\ndistribution with a long tail.\nD.2 Persuasion Techniques\nTable 9 gives detailed statistics about the anno-\ntated persuasion techniques. It further reports per-\ntechnique evaluation results in terms of precision,\nrecall, and F1score for our token-level multilabel\nmodel trained on the full multilingual data and eval-\nuated at the sentence level. For coarse-grained tech-\nniques, we report the average of the performances\nof the model for the corresponding fine-grained\ntechniques. We also report the total number of in-\nstances of each technique as well as the proportion\nof each technique in the dataset.Then, Table 10 shows statistics about the fine-\ngrained techniques per language. We can observe\nthatLoaded Language andName Calling are the\nmost frequent persuasion techniques irrespective\nof the language, trumping by several order of mag-\nnitude the lower populated classes and represent-\ning 42.4 % of the dataset. Then, we have Cast-\ning Doubt ,Questioning the Reputation andEx-\nageration Minimisation are the next most popu-\nlated classes, representing another 24%. These five\nclasses together cover 66.8% of the entire dataset.\nOverall, Attack on Reputation andManipulative\nWording are the most populated classes.\nD.3 Framing\nFigure 7 shows the normalized probability of the\nfine-grained distribution per rows, re-weighted with\nthe inverse document frequency of the technique:\nP(framing |topic )\u00b7id f(framing ), yielding a\ntf.idf-like vectorization of the different framings\nand topics, highlighting the key characteristics of\nthe topics in terms of framing. We can see that\nthe most frequent framing for the topics COVID-\n19,Climate Change , and Abortion areHealth and\nSafety ,Capacity and Resources , and Legality , re-\nspectively.\nE Model\nFor hyper-parameters, we experimented with vari-\nous learning rates and batch sizes without looking\nto overly optimize and we ended up with 1,5and\n3times 10-5for Genre, Framing and persuasion\ntechniques, respectively, a batch size of 12, 6, and\n12 respectively, and we used a weight decay of 0.01\nand early stopping with a patience of 750 steps.\nTable 9 shows the performance of our token-\nlevel multilabel model when trained on full multi-\nlingual data and evaluated at the sentence-level, for\nboth fine-grained and coarse-grained techniques.3018\nFigure 5: Example of a multilabel annotation using Inception: news genre is annotated as document metadata (left),\nwhile the persuasion techniques and the framings are highlighted in blue and in green, respectively.\nFigure 6: Proportion of annotated characters for all languages and document types.\nFigure 7: Co-occurrence of topics and framings. The number of framing instances is normalized per topic and is\nthen multiplied by the inverse document frequency of the framing: P(framing |topic )\u00b7id f(framing ).3019\nTechnique Abbrev. Prec. Rec. F1 Support %\nAttack on Reputation .418 .316 .357 14,814 39.8\nName Calling-Labeling NCL .633 .444 .522 5,935 15.9\nGuilt by Association GA .449 .273 .339 679 1.8\nDoubt D .404 .308 .349 4,922 13.2\nAppeal to Hypocrisy AH .277 .316 .295 1,013 2.7\nQuestioning the Reputation QR .326 .241 .277 2,265 6.1\nJustification .389 .25 .298 4,461 12.0\nFlag Waving FW .41 .321 .36 772 2.1\nAppeal to Authority AA .336 .19 .242 796 2.1\nAppeal to Popularity AP .373 .145 .209 378 1.0\nAppeal to Values A V .443 .232 .305 728 2.0\nAppeal to Fear-Prejudice AF .384 .36 .371 1,787 4.8\nDistraction .106 .043 .046 837 2.2\nStraw Man SM .068 .095 .079 414 1.1\nRed Herring RH .0 .0 .0 253 0.7\nWhataboutism W .25 .034 .06 170 0.5\nSimplification .293 .176 .211 1,625 4.4\nCausal Oversimplification CaO .157 .179 .167 685 1.8\nFalse Dilemma-No Choice FDNC .317 .2 .245 543 1.5\nConsequential Oversimplification CoO .406 .15 .219 397 1.1\nCall .383 .243 .295 2,004 5.4\nSlogans S .43 .314 .363 794 2.1\nConversation Killer CK .271 .181 .217 1,040 2.8\nAppeal to Time AT .448 .232 .306 170 0.5\nManipulative Wording .302 .168 .204 13,502 36.3\nLoaded Language LL .596 .423 .495 9,857 26.5\nObfuscation-Vagueness-Confusion OVC .133 .015 .026 440 1.2\nExaggeration-Minimisation EM .246 .181 .209 1916 5.1\nRepetition R .233 .052 .085 1,289 3.5\nTotal 37,243 100\nTable 9: Statistics about the fine-grained persuasion techniques. We report precision, recall, and F1score for our\ntoken-level multilabel model trained on full multilingual data and evaluated at the sentence level. For coarse-grained\ntechniques, we report the average of the performances of the model for the corresponding fine-grained techniques.\nWe also report the total number of instances of each technique as well as the proportion of each technique in the\ndataset.\nLanguageAttack on Reputation Call Distraction Justification Manip. Wording Simplification\nAH D GA NCL QR AT CK S RH SM W AA AF AP A V FW EM LL OVC R CaO CoO FDNC\nGerman 221 471 145 1118 333 10 173 165 73 64 41 281 265 87 110 73 297 793 138 21 119 52 78\nEnglish 53 748 67 1538 0 0 119 197 64 25 20 179 471 50 0 411 655 3,016 30 922 247 0 190\nFrench 189 497 184 767 518 57 235 202 67 190 76 133 326 107 154 47 398 2,199 166 175 188 185 122\nItalian 123 1879 91 1175 638 45 293 85 27 78 9 98 471 65 230 50 212 2,138 28 33 68 38 91\nPolish 283 459 148 950 273 21 103 49 19 25 13 93 178 59 171 130 175 524 48 33 17 32 20\nRussian 144 868 44 387 503 37 117 96 3 32 11 12 76 10 63 61 179 1,187 30 105 46 90 42\nTable 10: Statistics about the fine-grained persuasion techniques per language. The acronyms are those shown in the\nsecond column of Table 9. The zero values for English are for the newly introduced labels, which were not part of\nthe original English annotations.3020\nACL 2023 Responsible NLP Checklist\nA For every submission:\n/square\u0013A1. Did you describe the limitations of your work?\n7\n/square\u0013A2. Did you discuss any potential risks of your work?\n8\n/square\u0013A3. Do the abstract and introduction summarize the paper\u2019s main claims?\n1\n/square\u0017A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB/square\u0013Did you use or create scienti\ufb01c artifacts?\ntra\ufb01latura (section 4.1), XLM Roberta (section 5.1), the corpus described in Da San Martino et al.\n(2019a) - section 4\n/square\u0013B1. Did you cite the creators of artifacts you used?\ntra\ufb01latura (section 4.1), XLM Roberta (section 5.1), the corpus described in Da San Martino et al.\n(2019a) - section 4\n/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. they are all open source\n/square\u0017B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speci\ufb01ed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nwe use all artifacts according to their intended use.\n/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identi\ufb01es individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. we collected public news articles\n/square\u0013B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nsection 4.4\n/square\u0013B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signi\ufb01cant, while on small test sets they may not be.\nsection 4.4\nC/square\u0013Did you run computational experiments?\nsection 5\n/squareC1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. We performed \ufb01ne tuning on a standard LLM (RoBERTa), experiments were rather\nquick\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.3021\n/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. we used default hyperparameter values\n/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. we did one run only\n/square\u0013C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nsection 5\nD/square\u0013Did you use human annotators (e.g., crowdworkers) or research with human participants?\n4\n/square\u0013D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nappendix A\n/square\u0013D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants\u2019 demographic\n(e.g., country of residence)?\n4\n/squareD3. Did you discuss whether and how consent was obtained from people whose data you\u2019re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. they all volunteered\n/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. an almost identical annotation protocol has been approved in a previous work\n/square\u0013D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\n43022", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Multilingual multifaceted understanding of online news in terms of genre, framing, and persuasion techniques", "author": ["J Piskorski", "N Stefanovitch", "N Nikolaidis"], "pub_year": "2023", "venue": "Proceedings of the \u2026", "abstract": "We present a new multilingual multifacet dataset of news articles, each annotated for genre (objective  news reporting vs. opinion vs. satire), framing (what key aspects are highlighted),"}, "filled": false, "gsrank": 577, "pub_url": "https://aclanthology.org/2023.acl-long.169/", "author_id": ["xDQ3yuQAAAAJ", "Tqg6X5cAAAAJ", "p7Kr3woAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:8abFDbxTJCYJ:scholar.google.com/&output=cite&scirp=576&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D570%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=8abFDbxTJCYJ&ei=brWsaODUBsDZieoPqdqh8QU&json=", "num_citations": 44, "citedby_url": "/scholar?cites=2748413739752859377&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:8abFDbxTJCYJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aclanthology.org/2023.acl-long.169.pdf"}}]