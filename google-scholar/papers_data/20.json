[{"title": "Overview of the CLEF-2023 CheckThat! Lab Task 3 on Political Bias of News Articles and News Media.", "year": "2023", "pdf_data": "Overview of the CLEF-2023 CheckThat! Lab Task 3 on\nPolitical Bias of News Articles and News Media\nGiovanni Da San Martino1,\u02da, Firoj Alam2,\u02da, Maram Hasanain1, Rabindra Nath Nandi3,\nDilshod Azizov4and Preslav Nakov4\n1University of Padova, Italy\n2Qatar Computing Research Institute, HBKU, Qatar\n3Hishab Singapore Pte. Ltd, Singapore\n4Mohamed bin Zayed University of Artificial Intelligence, UAE\nAbstract\nWe provide an overview of task 3 of the CheckThat! lab from the Cross-Language Evaluation Forum\n(CLEF) 2023, which focuses on predicting the political leaning of English-language news articles and\nnews media outlets. We describe the data collection, the task setup, the evaluation outcomes, and the\napproaches used by the participating teams. A total of six teams submitted runs for the two subtasks.\nThe top-performing system in Subtask 3A achieved a Mean Absolute Error (MAE) of 0.473, while the\nbest system in Subtask 3B yielded a MAE of 0.549. We make all datasets and evaluation scripts available\nto the public, aiming to boost further research on this problem.\nKeywords\nPolitical bias, news articles, news media.\n1. Introduction\nIn the era of widespread digital information, the impact of political bias in news media and\nnews articles has emerged as a significant concern for democratic societies [ 1]. Accusations\nof bias against news organizations, which could influence the public opinion and the policy\ndiscourse, have been longstanding [ 2]. Various research approaches have been used to detect the\npotential bias of news articles, e.g., by using a headline attention network [ 3] or by monitoring\nthe frequency of mentions and quotes of politicians from different political parties [ 4]. At the\nmedium level, a number of recent studies [ 5,6,7,8,9,8,10,11,12] have been conducted, making\nuse of variety of information sources ranging from news articles to tweets, YouTube channels,\nand user overlap.\nTo enhance social awareness and to counteract the spread of false information, the\nCheckThat! lab at CLEF [ 13,14,15,16,17,18,19] has developed tasks using high-quality\ndata and suitable evaluation measures. As part of this initiative, the CheckThat! lab organiz-\ners have offered five distinct tasks [20, 21, 22].\nCLEF 2023: Conference and Labs of the Evaluation Forum, September 18\u201321, 2023, Thessaloniki, Greece\n\u02daCorresponding author.\n/envel\u2322pe-\u2322pendasan@math.unipd.it (G. Da San Martino); fialam@hbku.edu.qa (F. Alam); mhasanain@hbku.edu.qa\n(M. Hasanain); rabindro.rath@gmail.com (R. N. Nandi); dilshod.azizov@mbzuai.ac.ae (D. Azizov);\npreslav.nakov@mbzuai.ac.ae (P. Nakov)\n\u00a92023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedingshttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\n\nIn this paper, we provide an overview of Task 3, which focuses on detecting the political\nleaning of news articles andmedia outlets . We release high-quality, manually annotated data for\nthese two subtask, based on a 3-point ordinal scale for modeling the bias at the article and at\nthe medium level.\nThe remainder of this paper is organized as follows: In Section 2, we outline the task at hand\nand describe the dataset we created and released. Section 3 offers a comprehensive overview of\nthe evaluation settings. We discuss the results and we delve into the details of the submitted\nsystems in Section 4. Section 5 brings to light previous and recent work that aligns with our\nstudy. Finally, Section 6 presents our concluding observations and suggests directions for future\nresearch.\n2. Task and Datasets\nBelow, we define the tasks, and then we discuss the datasets.\n2.1. Task definition\nThe goal of the task is to detect the political bias of news reporting at the article and at the\nmedia level. This is an ordinal classification task and it is offered in English. It includes two\nsubtasks, defined below.\nSubtask 3A: Political Bias of News Articles Given an article, classify its political leaning as\nleft, center, or right.\nSubtask 3B: Political Bias of News Media Given the news article(s) a news outlet (e.g.,\nwww.cnn.com), predict the overall political bias of that news outlet as left, center, or right.\n2.2. Datasets\nBelow, we describe the datasets for the two subtasks.\n2.2.1. Subtask 3A\nWe release a new dataset, which we crawled from AllSides1, a website that gathers news\narticles from a variety of reputable national and international news sources to ensure a balanced\nrepresentation across different political spectrums. The site offers meticulous annotations\nof the bias of news articles, including expert assessments, third-party analysis, independent\nevaluations, and community input. In addition, AllSides uses annotated articles to support\nits Balanced Search tool, which displays the news coverage of a specific issue from numerous\nmedia providers, each with a different political bias from all political perspectives, as depicted\nin Figure 1.\nTo ensure that the dataset remains relevant and reflects the current political environment, it\nincludes news articles published from late 2022 till early 2023. Each article has several attributes,\nas shown in Table 1. In total, we have just over 55k articles in the dataset. We provide statistics\nabout the dataset in Table 2.\n1www.allsides.com\nFigure 1: Examples of news articles with bias labels assigned by AllSides (Subtask 3A). Source:\nwww.allsides.com\nTable 1\nData attributes for the article (Subtask 3A).\nID Unique identifier\nTitle Headline\nContent Full text of the article\nLabel Political bias of the article: left, center, or right\nTable 2\nStatistics about the training, development, and test partitions (Subtask 3A).\nLeft Center Right Total\nTrain 12,073 15,449 17,544 45,066\nDev 1,342 1,717 1,949 5,008\nTest 2,589 1,959 650 5,198\nTotal 16,004 19,125 20,143 55,272\n2.2.2. Subtask 3B\nWe assess the political bias of English-language media, sourced from Media Bias/Fact Check\nwebsite.2On that website, experts conduct an in-depth analysis and annotate the political bias\nof entire news outlets: examples are shown in Table 3. We further include a certain number of\narticles, which we crawl from each media source: these are to be used by the participants to\nanalyze that source. The dataset has similar attributes to subtask 3A, plus the source (the name\nof the medium) as an additional attribute. We have over 8,000 articles (approximately 10 per\nsource) and over 1,000 news sources. Tables 4 and 5 show the label distribution and the number\nof articles and news media.\n2www.mediabiasfactcheck.com\nTable 3\nExamples of news outlets and their biases (Subtask 3B).\nLeft Center Right\nThe Guardian BBC News Fox News\nThe New York Times Reuters The Daily Caller\nThe Washington Post The Associated Press The National Review\nTable 4\nStatistics about the training, the development, and the test partitions for the news outlets (Subtask 3B).\nLeft Center Right Total\nTrain 216 296 305 817\nDev 31 34 39 104\nTest 25 29 48 102\nTotal 272 359 392 1,023\nTable 5\nStatistics about the training, the development, and the test partitions for news articles across all news\noutlets (Subtask 3B).\nLeft Center Right Total\nTrain 1,350 1,822 2,051 5,223\nDev 378 386 434 1,196\nTest 526 536 564 1,626\nTotal 2,254 2,744 3,049 8,047\n3. Evaluation Settings\nSettings The evaluation comprises development and test phases. During the development\nphase, we provided the participants with the training and the development sets. This enabled\nthem to internally validate their systems and to adjust the parameter values using the develop-\nment set. During the test phase, the participants submitted their system\u2019s predictions for the\nprovided test set, which did not include reference labels. They were allowed to submit as many\nruns as they wanted, but only the last submission was considered as the final one.\nEvaluation This is an ordinal classification task, and thus we used mean absolute error as\nthe official measure for both subtasks.\nTable 6\nResults on the leaderboard: political bias of news articles and news media (MAE score).\nSubtask 3A Subtask 3B\nRank Team MAE Rank Team MAE\n1 Accenture [24] 0.473 1 Accenture [24] 0.549\n2 TOBB ETU [25] 0.646 2 Awakened 0.765\n3 KUCST 0.736 3 Baseline 0.902\n4 Awakened 0.752\n5 Baseline 0.877\nFrank [23] 0.270 Frank [23] 0.320\n4. Results and Overview of the Systems\n4.1. Results\nTable 6, shows the results for Task 3, in which four official teams and one non-official team\nparticipated. All teams outperformed the baseline.\nFour teams participated in Subtask 3A, with Accenture taking the lead, having the lowest\nMAE of 0.473. They are followed by TOBB ETU, KUCST , and Awakened with MAE scores of\n0.646, 0.736, and 0.752, respectively.\nTwo teams took part in Subtask 3B. Once again, Accenture was first, with a MAE of 0.549,\nfollowed by Awakened, with a MAE value of 0.765.\nTeam Frank [ 23], which did not officially appear on the leaderboard, outperformed all other\nteams on both subtasks. They achieved a MAE of 0.270 for subtask 3A and 0.320 in subtask 3B.\nBoth scores are much lower than those of the participating teams.\n4.2. Overview of the Systems\nAccenture [ 24]used machine back-translation to augment the minority classes examples and\nthus to address the class imbalance. Then, they fine-tuned RoBERTa on this augmented data.\nTOBB ETU [ 25]used zero-shot and few-shot classification with ChatGPT exclusively for\nsubtask 3A.\nFrank [ 23]used CatBoost, TF.IDF, oversampling, and an ensemble. They had a file formatting\nissue, and thus they are not officially on the leaderboard.\n5. Related Work\nThe detection of political bias in news articles and media has been the subject of several studies\n[7,26,27,28,29] due to its significance in ensuring balanced information dissemination and\nsupporting media literacy [30].\nHistorically, bias was primarily understood as coverage inequality, as put forth by Stevenson\net al. [31]. However, later definitions expanded to include systematic favoring of particular\nideologies or candidates, as illustrated by Waldman and Devitt [32].\nThese broader interpretations of bias consider factors like visual favorability in news images.\nBased on a review of numerous studies, [ 33] proposed three types of media bias: gatekeeping\nbias, coverage bias, and statement bias. Groeling [34] influenced this classification concept of\nmedia bias, focusing on selection bias (what to cover) and presentation bias (how to cover it).\nSelection bias research typically involves collecting news articles or transcripts, analyzing their\ncontent, and identifying systematic biases. Meanwhile, presentation bias is often evaluated\nthrough framing, visuals, tone, and sources [ 35]. Multiple methods have been proposed to\nquantify news slant, including analyzing the language used by different political parties and\nmapping the distances between media sources based on their mutual followers on social media\nplatforms like Twitter [36, 37, 29].\nThere have been various approaches for detecting the political bias of news articles. Kulkarni\net al. [38] used an attention-based multi-view model. Baly et al. [11] used adversarial training\nto make sure that the model learns to predict the bias rather than the source of the news article.\nAnother related task is hyper-partisan news detection, e.g., [ 39] proposed a meta-learning\napproach to model the style similarities between text categories [ 40]. Systems using averaged\nword embeddings from pre-trained ELMo models succeeded for this task [41].\nEfforts to predict the political ideology of news media used multimodal deep-learningDinkov\net al. [9]. Fact-checking methods that assess a document\u2019s stance towards a claim considering\nthe source\u2019s credibility have also been explored [ 26,28]. For assessing entire news outlets,\nresearchers modeled tweets and Twitter users [ 10], information from social media, YouTube,\nand Wikipedia [ 8], and inter-media similarity based on audience overlap [ 12]. There have also\nbeen attempts to model bias and factuality jointly in a multitask setup [7].\nTheCheckThat! lab for CLEF has expanded its task offerings compared to the previous iter-\nations, particularly concentrating on check-worthiness [ 42], subjectivity [ 43], bias (this paper),\nfactuality [ 44], and authority [ 45]. Notably, only in this sixth edition of the CheckThat! lab a\ntask has aimed at predicting bias at both the article level and the medium level.\nOverall, various computational models and datasets have advanced the detection of political\nbias and factuality and contributed to media literacy efforts [46, 47, 48].\n6. Conclusion and Future Work\nWe presented a comprehensive analysis of Task 3 from the CheckThat! lab at CLEF 2023. This\nlab focused on detecting the political bias in news articles and media outlets. The submissions\nused transformer-based models (such as RoBERTa and ChatGPT) and gradient boosting on\ndecision trees (like CatBoost), achieving sizable improvements over the baselines.\nIn future work, we plan to explore more information sources, to add more languages, and to\nadopt a finer-grained scale.\nAcknowledgments\nThe work of F. Alam and M. Hasanain is partially supported by NPRP 13S-0206-200281 and\nNPRP 14C-0916-210015 from the Qatar National Research Fund, a member of Qatar Foundation.\nThe findings herein are solely the responsibility of the authors.\nReferences\n[1]H. Allcott, M. Gentzkow, Social media and fake news in the 2016 election, Journal of\neconomic perspectives 31 (2017) 211\u2013236.\n[2]T. Groseclose, J. Milyo, A measure of media bias, The quarterly journal of economics 120\n(2005) 1191\u20131237.\n[3]R. R. R. Gangula, S. R. Duggenpudi, R. Mamidi, Detecting political bias in news articles using\nheadline attention, in: Proceedings of the 2019 ACL workshop BlackboxNLP: analyzing\nand interpreting neural networks for NLP, 2019, pp. 77\u201384.\n[4]K. Lazaridou, R. Krestel, Identifying political bias in news articles, Bulletin of the IEEE\nTCDL 12 (2016).\n[5]R. Baly, G. Karadzhov, D. Alexandrov, J. Glass, P. Nakov, Predicting factuality of reporting\nand bias of news media sources, in: Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, 2018, pp. 3528\u20133539.\n[6]R. Baly, M. Mohtarami, J. Glass, L. M\u00e0rquez, A. Moschitti, P. Nakov, Integrating stance\ndetection and fact checking in a unified corpus, in: Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers), 2018, pp. 21\u201327.\n[7]R. Baly, G. Karadzhov, A. Saleh, J. Glass, P. Nakov, Multi-task ordinal regression for\njointly predicting the trustworthiness and the leading political ideology of news media, in:\nProceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, 2019, pp. 2109\u20132116.\n[8]R. Baly, G. Karadzhov, J. An, H. Kwak, Y. Dinkov, A. Ali, J. Glass, P. Nakov, What was written\nvs. who read it: News media profiling using text analysis and social media context, in:\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\n2020, pp. 3364\u20133374.\n[9]Y. Dinkov, A. Ali, I. Koychev, P. Nakov, Predicting the leading political ideology of\nYouTube channels using acoustic, textual, and metadata information, in: Proceedings\nof the 20th Annual Conference of the International Speech Communication Association,\nINTERSPEECH \u201919, 2019.\n[10] P. Stefanov, K. Darwish, A. Atanasov, P. Nakov, Predicting the topical stance and polit-\nical leaning of media using tweets, in: Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 2020, pp. 527\u2013537.\n[11] R. Baly, G. Da San Martino, J. Glass, P. Nakov, We can detect your bias: Predicting the\npolitical ideology of news articles, in: Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP \u201920, 2020, pp. 4982\u20134991.\n[12] P. Panayotov, U. Shukla, H. T. Sencar, M. Nabeel, P. Nakov, GREENER: Graph neural\nnetworks for news media profiling, in: Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, Association for Computational Linguistics, Abu\nDhabi, United Arab Emirates, 2022, pp. 7470\u20137480.\n[13] P. Nakov, A. Barr\u00f3n-Cede\u00f1o, T. Elsayed, R. Suwaileh, L. M\u00e0rquez, W. Zaghouani,\nP. Atanasova, S. Kyuchukov, G. Da San Martino, Overview of the CLEF-2018 Check-\nThat! lab on automatic identification and verification of political claims, in: Proceedings\nof the Ninth International Conference of the CLEF Association: Experimental IR Meets\nMultilinguality, Multimodality, and Interaction, Lecture Notes in Computer Science, 2018,\npp. 372\u2013387.\n[14] T. Elsayed, P. Nakov, A. Barr\u00f3n-Cede\u00f1o, M. Hasanain, R. Suwaileh, G. Da San Martino,\nP. Atanasova, Overview of the CLEF-2019 CheckThat!: Automatic identification and\nverification of claims, in: Experimental IR Meets Multilinguality, Multimodality, and\nInteraction, LNCS, 2019, pp. 301\u2013321.\n[15] A. Barr\u00f3n-Cede\u00f1o, T. Elsayed, P. Nakov, G. Da San Martino, M. Hasanain, R. Suwaileh,\nF. Haouari, N. Babulkov, B. Hamdan, A. Nikolov, S. Shaar, Z. Ali, Overview of CheckThat!\n2020 \u2014 automatic identification and verification of claims in social media, in: Proceedings\nof the 11th International Conference of the CLEF Association: Experimental IR Meets\nMultilinguality, Multimodality, and Interaction, CLEF \u20192020, 2020, pp. 215\u2013236.\n[16] P. Nakov, G. Da San Martino, T. Elsayed, A. Barr\u00f3n-Cede\u00f1o, R. M\u00edguez, S. Shaar, F. Alam,\nF. Haouari, M. Hasanain, N. Babulkov, et al., The CLEF-2021 CheckThat! lab on detecting\ncheck-worthy claims, previously fact-checked claims, and fake news, in: ECIR (2), 2021.\n[17] P. Nakov, A. Barr\u00f3n-Cede\u00f1o, G. Da San Martino, F. Alam, J. M. Stru\u00df, T. Mandl, R. M\u00edguez,\nT. Caselli, M. Kutlu, W. Zaghouani, C. Li, S. Shaar, G. K. Shahi, H. Mubarak, A. Nikolov,\nN. Babulkov, Y. S. Kartal, J. Beltr\u00e1n, M. Wiegand, M. Siegel, J. K\u00f6hler, Overview of the\nCLEF-2022 CheckThat! lab on fighting the COVID-19 infodemic and fake news detection,\nin: A. Barr\u00f3n-Cede\u00f1o, G. Da San Martino, M. Degli Esposti, F. Sebastiani, C. Macdonald,\nG. Pasi, A. Hanbury, M. Potthast, G. Faggioli, F. Nicola (Eds.), Proceedings of the 13th\nInternational Conference of the CLEF Association: Information Access Evaluation meets\nMultilinguality, Multimodality, and Visualization, CLEF \u20192022, Bologna, Italy, 2022.\n[18] P. Nakov, A. Barr\u00f3n-Cede\u00f1o, G. Da San Martino, F. Alam, R. M\u00edguez, T. Caselli, M. Kutlu,\nW. Zaghouani, C. Li, S. Shaar, H. Mubarak, A. Nikolov, Y. S. Kartal, J. Beltr\u00e1n, Overview\nof the CLEF-2022 CheckThat! lab task 1 on identifying relevant claims in tweets, in:\nN. Faggioli, Guglielmo andd Ferro, A. Hanbury, M. Potthast (Eds.), Working Notes of CLEF\n2022\u2014Conference and Labs of the Evaluation Forum, CLEF \u20192022, Bologna, Italy, 2022.\n[19] A. Barr\u00f3n-Cede\u00f1o, F. Alam, T. Caselli, G. Da San Martino, T. Elsayed, A. Galassi, F. Haouari,\nF. Ruggeri, J. M. Stru\u00df, R. N. Nandi, G. S. Cheema, D. Azizov, P. Nakov, The CLEF-2023\nCheckThat! Lab: Checkworthiness, subjectivity, political bias, factuality, and authority, in:\nJ. Kamps, L. Goeuriot, F. Crestani, M. Maistro, H. Joho, B. Davis, C. Gurrin, U. Kruschwitz,\nA. Caputo (Eds.), Advances in Information Retrieval, Springer Nature Switzerland, Cham,\n2023, pp. 506\u2013517.\n[20] A. Barr\u00f3n-Cede\u00f1o, T. Elsayed, P. Nakov, G. Da San Martino, M. Hasanain, R. Suwaileh,\nF. Haouari, Checkthat! at clef 2020: Enabling the automatic identification and verification\nof claims in social media, in: J. M. Jose, E. Yilmaz, J. Magalh\u00e3es, P. Castells, N. Ferro, M. J.\nSilva, F. Martins (Eds.), Advances in Information Retrieval, Cham, 2020, pp. 499\u2013507.\n[21] A. Barr\u00f3n-Cede\u00f1o, F. Alam, A. Galassi, G. Da San Martino, P. Nakov, , T. Elsayed, D. Azizov,\nT. Caselli, G. Cheema, F. Haouari, M. Hasanain, M. Kutlu, C. Li, F. Ruggeri, J. M. Stru\u00df,\nW. Zaghouani, Overview of the CLEF\u20132023 CheckThat! Lab checkworthiness, subjectivity,\npolitical bias, factuality, and authority of news articles and their source, in: A. Arampatzis,\nE. Kanoulas, T. Tsikrika, S. Vrochidis, A. Giachanou, D. Li, M. Aliannejadi, M. Vlachos,\nG. Faggioli, N. Ferro (Eds.), Experimental IR Meets Multilinguality, Multimodality, and In-\nteraction. Proceedings of the Fourteenth International Conference of the CLEF Association\n(CLEF 2023), 2023.\n[22] G. Da San Martino, F. Alam, M. Hasanain, R. N. Nandi, D. Azizov, P. Nakov, Overview of\nthe CLEF-2023 CheckThat! lab task 3 on political bias of news articles and news media, in:\nWorking Notes of CLEF 2023\u2013Conference and Labs of the Evaluation Forum, CLEF \u20192023,\nThessaloniki, Greece, 2023.\n[23] D. Azizov, S. Liang, P. Nakov, Frank at checkthat! 2023: Detecting the political bias of\nnews articles and news media, in: Working Notes of CLEF 2023\u2013Conference and Labs of\nthe Evaluation Forum, CLEF \u20192023, Thessaloniki, Greece, 2023.\n[24] S. Tran, P. Rodrigues, B. Strauss, E. Williams, Accenture at CheckThat! 2023: Learning to\ndetect Political Bias of News Articles and Sources, in: [49], 2023.\n[25] M. D. T\u00fcrkmen, G. Co\u015fgun, M. Kutlu, Tobb etu at checkthat! 2023: Utilizing chatgpt to\ndetect subjective statements and political bias, in: [49], 2023.\n[26] R. Baly, G. Karadzhov, D. Alexandrov, J. Glass, P. Nakov, Predicting factuality of reporting\nand bias of news media sources, in: Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, 2018, pp. 3528\u20133539.\n[27] R. Baly, G. Da San Martino, J. Glass, P. Nakov, We can detect your bias: Predicting the\npolitical ideology of news articles, in: Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2020, pp. 4982\u20134991.\n[28] R. Baly, M. Mohtarami, J. Glass, L. M\u00e0rquez, A. Moschitti, P. Nakov, Integrating stance\ndetection and fact checking in a unified corpus, in: Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers), New Orleans, Louisiana, 2018, pp. 21\u201327.\n[29] P. Nakov, H. T. Sencar, J. An, H. Kwak, A survey on predicting the factuality and the bias\nof news media, arXiv preprint arXiv:2103.12506 (2021).\n[30] M. Gentzkow, J. M. Shapiro, What drives media slant? Evidence from US daily newspapers,\nEconometrica 78 (2010) 35\u201371.\n[31] R. L. Stevenson, R. A. Eisinger, B. M. Feinberg, A. B. Kotok, Untwisting the news twisters:\nA replication of efron\u2019s study, Journalism Quarterly 50 (1973) 211\u2013219.\n[32] P. Waldman, J. Devitt, Newspaper photographs and the 1996 presidential election: The\nquestion of bias, Journalism & Mass Communication Quarterly 75 (1998) 302\u2013311.\n[33] D. D\u2019Alessio, M. Allen, Media bias in presidential elections: A meta-analysis, Journal of\ncommunication 50 (2000) 133\u2013156.\n[34] T. Groeling, Media bias by the numbers: Challenges and opportunities in the empirical\nstudy of partisan news, Annual Review of Political Science 16 (2013) 129\u2013151.\n[35] E. C. Tandoc Jr, Journalism is twerking? How web analytics is changing the process of\ngatekeeping, New media & society 16 (2014) 559\u2013575.\n[36] K. Darwish, P. Stefanov, M. Aupetit, P. Nakov, Unsupervised user stance detection on\nTwitter, in: Proceedings of the International AAAI Conference on Web and Social Media,\nvolume 14, 2020, pp. 141\u2013152.\n[37] P. Stefanov, K. Darwish, A. Atanasov, P. Nakov, Predicting the topical stance and polit-\nical leaning of media using tweets, in: Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 2020, pp. 527\u2013537.\n[38] V. Kulkarni, J. Ye, S. Skiena, W. Y. Wang, Multi-view models for political ideology detection\nof news articles, in: Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, 2018, pp. 3518\u20133527.\n[39] J. Kiesel, M. Mestre, R. Shukla, E. Vincent, P. Adineh, D. Corney, B. Stein, M. Potthast,\nSemeval-2019 task 4: Hyperpartisan news detection, in: Proceedings of the 13th Interna-\ntional Workshop on Semantic Evaluation, 2019, pp. 829\u2013839.\n[40] M. Potthast, M. Hagen, T. Gollub, M. Tippmann, J. Kiesel, P. Rosso, E. Stamatatos, B. Stein,\nOverview of the 5th international competition on plagiarism detection, in: CLEF Confer-\nence on Multilingual and Multimodal Information Access Evaluation, CELCT, 2013, pp.\n301\u2013331.\n[41] Y. Jiang, J. Petrak, X. Song, K. Bontcheva, D. Maynard, Team bertha von suttner at\nSemEval-2019 task 4: Hyperpartisan news detection using ELMo sentence representation\nconvolutional network, in: Proceedings of the 13th International Workshop on Semantic\nEvaluation, Minneapolis, Minnesota, USA, 2019, pp. 840\u2013844.\n[42] F. Alam, A. Barr\u00f3n-Cede\u00f1o, G. S. Cheema, S. Hakimov, M. Hasanain, C. Li, R. M\u00edguez,\nH. Mubarak, G. K. Shahi, W. Zaghouani, P. Nakov, Overview of the CLEF-2023 CheckThat!\nlab task 1 on check-worthiness in multimodal and multigenre content, in: Working Notes\nof CLEF 2023\u2013Conference and Labs of the Evaluation Forum, CLEF \u20192023, Thessaloniki,\nGreece, 2023.\n[43] A. Galassi, F. Ruggeri, A. B.-C. no, F. Alam, T. Caselli, M. Kutlu, J. M. Struss, F. Antici,\nM. Hasanain, J. K\u00f6hler, K. Korre, F. Leistra, A. Muti, M. Siegel, M. D. Turkmen, M. Wiegand,\nW. Zaghouani, Overview of the CLEF-2023 CheckThat! lab task 2 on subjectivity in news\narticles, in: Working Notes of CLEF 2023\u2013Conference and Labs of the Evaluation Forum,\nCLEF \u20192023, Thessaloniki, Greece, 2023.\n[44] P. Nakov, F. Alam, G. Da San Martino, M. Hasanain, R. N. Nandi, D. Azizov, P. Panayotov,\nOverview of the CLEF-2023 CheckThat! lab task 4 on factuality of reporting of news\nmedia, in: Working Notes of CLEF 2023\u2013Conference and Labs of the Evaluation Forum,\nCLEF \u20192023, Thessaloniki, Greece, 2023.\n[45] F. Haouari, Z. Sheikh Ali, T. Elsayed, Overview of the CLEF-2023 CheckThat! lab task 5\non authority finding in Twitter, in: Working Notes of CLEF 2023\u2013Conference and Labs of\nthe Evaluation Forum, CLEF \u20192023, Thessaloniki, Greece, 2023.\n[46] P. Nakov, D. Corney, M. Hasanain, F. Alam, T. Elsayed, A. Barr\u00f3n-Cede\u00f1o, P. Papotti,\nS. Shaar, G. D. S. Martino, Automated fact-checking for assisting human fact-checkers, in:\nProceedings of the 30th International Joint Conference on Artificial Intelligence, 2021.\n[47] F. Alam, S. Shaar, F. Dalvi, H. Sajjad, A. Nikolov, H. Mubarak, G. D. S. Martino, A. Abdelali,\nN. Durrani, K. Darwish, A. Al-Homaid, W. Zaghouani, T. Caselli, G. Danoe, F. Stolk,\nB. Bruntink, P. Nakov, Fighting the COVID-19 infodemic: Modeling the perspective of\njournalists, fact-checkers, social media platforms, policy makers, and the society, in:\nFindings of EMNLP 2021, 2021, pp. 611\u2013649.\n[48] P. Nakov, S. Rosenthal, Z. Kozareva, V. Stoyanov, A. Ritter, T. Wilson, SemEval-2013 task 2:\nSentiment analysis in Twitter, in: Second Joint Conference on Lexical and Computational\nSemantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on\nSemantic Evaluation (SemEval 2013), 2013, pp. 312\u2013320.\n[49] M. Aliannejadi, G. Faggioli, N. Ferro, Vlachos, Michalis (Eds.), Working Notes of CLEF\n2023 - Conference and Labs of the Evaluation Forum, CLEF 2023, Thessaloniki, Greece,\n2023.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Overview of the CLEF-2023 CheckThat! Lab Task 3 on Political Bias of News Articles and News Media.", "author": ["G Da San Martino", "F Alam", "M Hasanain"], "pub_year": "2023", "venue": "CLEF (Working \u2026", "abstract": "We provide an overview of task 3 of the CheckThat! lab from the Cross-Language  Evaluation Forum (CLEF) 2023, which focuses on predicting the political leaning of English-language"}, "filled": false, "gsrank": 15, "pub_url": "https://ceur-ws.org/Vol-3497/paper-021.pdf", "author_id": ["URABLy0AAAAJ", "j-RtwDQAAAAJ", "GtvNhM8AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:33-j-PtiNhsJ:scholar.google.com/&output=cite&scirp=14&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D10%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=33-j-PtiNhsJ&ei=BbWsaMyPOeHUieoP9LKZ6AI&json=", "num_citations": 12, "citedby_url": "/scholar?cites=1960863522114207711&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:33-j-PtiNhsJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ceur-ws.org/Vol-3497/paper-021.pdf"}}, {"title": "An information theory approach to detect media bias in news websites", "year": "2020", "pdf_data": "An Information Theory Approach to Detect\nMedia Bias in News Websites\nVictoria Patricia Aires\nNew York University\nFederal University of Amazonas\nvictoria.aires@icomp.ufam.edu.brJuliana Freire\nNew York University\njuliana.freire@nyu.eduFabiola G. Nakamura\nFederal University of Amazonas\nfabiola@icomp.ufam.edu.br\nAltigran Soares da Silva\nFederal University of Amazonas\nalti@icomp.ufam.edu.brEduardo F. Nakamura\nFederal University of Amazonas\nnakamura@icomp.ufam.edu.br\nABSTRACT\nNews websites and portals are, together with social media, major\nsources of information nowadays. However, such types of media\nmay be biased regarding, especially, political and ideological lean-\ning/orientation. Hence, the awareness of such bias, leaning, or\norientation is a key factor for the readers (content consumers) to de-\ncide how much content/opinion they accept or reject from a given\nsource. Over the years, especially nowadays, biased information\nhas been used as a tool to control and manipulate public opinion,\nultimately leading to the proliferation of fake news. Consequently,\nit is important to develop methods to automatically identify and\ninform the reader about the eventual political and ideological bias\nof the sources. The majority of current research focuses on polarity\ndetection or a bi-class problem, such as left vs. right-wing leaning\nor Democratic vs. Republican. In addition, most of them are based\non a large number of features (lexical or bag-of-words), resulting\nin computationally intensive methods. In this work, we introduce\nPoll (POLitical Leaning detector), a strategy based on Information\nTheory concepts to detect media bias in news websites/portals con-\nsidering bi-class and multi-class problems. Our strategy reduces the\nfeature space to as little as the number of classes being considered,\nsignificantly reducing the overall computational cost. Compared to\na representative baseline, our strategy yields a macro accuracy of\nup to 76% for a four-class problem compared to 22% for the baseline\nunder the same conditions. For some classes, we could reach an F1\nof 0.80 against 0.28 from the baseline.\nCCS CONCEPTS\n\u2022Computing methodologies \u2192Supervised learning by clas-\nsification ;\u2022Information systems \u2192Content analysis and feature\nselection .\nKEYWORDS\nmedia bias detection, news analysis, classification, online news\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nWISDOM \u201920, August 24th, 2020, San Diego, CA\n\u00a92020 Copyright held by the owner/author(s). Publication rights licensed to WIS-\nDOM\u201920. See http://sentic.net/wisdom for details.ACM Reference Format:\nVictoria Patricia Aires, Juliana Freire, Fabiola G. Nakamura, Altigran Soares\nda Silva, and Eduardo F. Nakamura. 2020. An Information Theory Approach\nto Detect Media Bias in News Websites. In WISDOM \u201920: Workshop on Issues\nof Sentiment Discovery and Opinion Mining, August 24th, 2020, San Diego,\nCA.ACM, New York, NY, USA, 9 pages.\n1 INTRODUCTION\nWith the popularization of the web and social media, news websites\nand portals have become major sources of information for the pop-\nulation. For the sake of simplification we will use the terms news\nwebsites and news portals indistinctly hereafter, given this simplifi-\ncation does not impact on the paper contributions. Compared to\ntraditional vehicles such as newspapers, television, and radio, news\nportals are fast, have a worldwide reach, and allow interactions\namong the readers [ 1,6,11]. However, as with traditional vehicles,\nnews from these portals may be biased [ 12,19]. This is a fundamen-\ntal problem that is currently of major importance to our society,\npartially because the readers tend to spend little time on reading\nand even less (or none) assessing the quality of the source [4, 5].\nA starting point to mitigate the effect of bias in news dissemina-\ntion is to identify the ideology of the content provider. A proper\nideological identification allows the reader to reason about it and\ndecide whether or not that ideology introduces bias (information\nthat is incomplete, incorrect, misleading, or fake).\nThis first step is often approached as a bi-class problem that\nconsists in classifying an information source as left/right (ideology),\nor as Democratic/Republican (partisan) [ 20]. However, in practice\nit might be useful to consider a multi-class problem [ 10], such as a\nfour-class scenario: left (extreme or moderate) and right (extreme or\nmoderate). The particularities of the moderate classes (vocabulary,\nstyle, citation patterns) make the boundaries of moderate left and\nmoderate right a little fuzzy. These characteristics make the multi-\nclass problem more challenging.\nIn this work, we propose a novel method that analyzes the con-\ntent of articles to determine the political orientation/leaning and\nintensity of ideology of news portals (up to four classes). The pro-\nposed method uses basic Information Theory concepts to identify\nkey content (Shannon Entropy [ 22]) and quantifying the differ-\nences (Jensen-Shannon divergence [ 9,15]) between a target portal\nand news portals of known orientation to guide the detection of\nthe political orientation of the target portal. Compared to more\nWISDOM \u201920, August 24th, 2020, San Diego, CA Aires et al.\nTable 1: Summary of related works.\nWork Strategy Classes Performance Target\nEfron [7] Hyperlink co-citations Liberal/Conservative 77.50% (accuracy) web documents\nKrestel et al. [13]TF-IDF vectors\nand Cosine similarityLeft/Right German\npolitical partiesnot reportedGerman news\noutlets\nRao and Spasojevic [20] Word Embeddings and LSTM Democratic/Republican 87.57% (accuracy) tweets\nElejalde et al. [8] Rank difference Liberal/Conservative not reportedChilean news\noutlets\nRibeiro et al. [21]Audience demographics\nfrom social mediaLiberal/Moderate/\nConservativenot reported news sources\nGordon et al. [10] Word Embeddings Democratic/Republican not reported tweets\nBaly et al. [3]A varied set of features\nincluding lexical featuresLeft/Center/Right 41.74% (accuracy) news sources\ntraditional solutions, our strategy represents a significant reduc-\ntion in the (dimension) feature space used to characterize the news\nwebsite, which aims at reducing the computational cost while keep-\ning/improving the detection efficacy. In some cases, we were able\nto reach an accuracy score of 86%, by using as few as four features,\nagainst 43% from the baseline, which uses 282 features, and an F1\nof 0.80, against 0.28 from the same baseline.\nThe key contributions of this paper are threefold: (i) we propose\na classification strategy using Information Theory concepts such\nas Shannon entropy to compute more reliable features to classify\nmedia bias in news websites; (ii) we quantify the performance of the\nmethod computing different features and dissimilarity measures\nin two distinct datasets, under different classification tasks; and\n(iii) we show that our approach is robust and outperforms a more\ntraditional baseline, accurately classifying media bias for binary\nscenarios and, more importantly, a multi-class scenario.\nThe rest of the paper is organized as follows: Section 2 summa-\nrizes the related work; Section 3 describes the steps that compose\nour method; Section 4 includes details about our experiments and\nresults; and Section 5 presents our conclusions and future work.\n2 RELATED WORK\nIdeological bias has been detected by using different strategies,\nmostly based on the analysis of text content. Krestel et al . [13] used\npolitical text samples, such as speeches and statements, webpages\nof political parties and articles from news websites to characterize\nhow the discourse of German news sources are similar to those of\nGerman parties.\nEfron [7]discovers the political orientation of a web document\nby using co-citation data within a probabilistic model. Efron\u2019s model\nassesses the probability of co-citation between a set of reference\ndocuments, whose political orientation is well-known, and a target\ndocument, whose political orientation must be discovered. The\ndecision is based on the premise that documents with stronger\nco-citation are more likely to be politically aligned.\nElejalde et al . [8] use tweets to automatically compute the politi-\ncal and socioeconomic orientation of Chilean media news portals,\nmapping the opinions expressed in tweets in a political survey to\nobtain the ideological bias of the portals. Ribeiro et al . [21] relied on\nads to infer the political bias of news sources on social media such\nas Facebook and Twitter. The authors show that the ideologicalorientation (liberal or conservative) of a news source is related to\nthe political preference of the audience.\nBaly et al . [3] developed a method to predict factuality and bias\nof news media. They experimented with a varied set of features\nincluding lexical attributes to model headline and content of news\narticles, and information extracted from Twitter and Wikipedia.\nThey showed that their approach is better suited to factuality (low,\nmixed and high) than to media bias, where they got a accuracy\nscore of 41.74%. In this specific case, they performed two tasks: 3-\nway (left, center and right) and 7-way (extreme left, left, left center,\ncenter, right center, right and extreme right).\nDirectly related to political orientation, we have the political\ndisaffection defined by Monti et al . [18] as \u201cthe lack of confidence\nin the political process, politicians, and democratic institutions, but\nwith no questioning of the political regime. \u201d The authors show that\nthe amount of tweets of disaffection along time is a strong indicator\nof political inefficacy. The detection of political disaffection, in this\ncase, can be augmented with a bias detector to, among other things,\nunderstand the disaffection directed to a specific political ideology.\nWord embeddings have also been used to detect political bias in\ntweets [ 10,20]. In particular, Rao and Spasojevic [20] could define\nif a tweet leans towards the Democratic or Republican party with\nan accuracy as high as 87.57%. Gordon et al . [10] do not assess the\nperformance of the classifier but use the word embedding to find\nthat because of Trump\u2019s tweets, the Republican candidates category\nreaches a bias score of 0.97 (an indicator of the bias intensity with\nmaximum value of 1.00).\nThe related work is summarized in Table 1. Most researches offer\na case study or a very specific characterization, analyzing only a\nlimited set of news sources. Methods with a more general approach\nlike those of Efron [7]and Baly et al . [3]also have limitations. In the\nfirst case, the method does not perform well when classifying web\npages with only a few hyperlinks. In the latter, the set of features is\nvery large, 282 in total. This can have implications for processing\ntime and explainability. Also, the majority of works focus on a\nbi-class problem, classifying only left and right-wing leanings.\nThus, the major difference of our work is that we focus on using\nInformation Theory as a dimension reduction strategy to detect\nthe political leaning of a news website (not social media) regarding\nthe intensity (extreme vs. moderate) and ideology (left vs. right).\nAs a result, we can have up to four classes: left, left center, right\ncenter, and right, which makes the task harder compared to bi-class\nAn Information Theory Approach to Detect Media Bias in News Websites WISDOM \u201920, August 24th, 2020, San Diego, CA\napproaches. Noteworthy, when dealing with a three-class problem\n(harder than bi-class, but simpler than four-class problems) Baly\net al. [3], reported an accuracy of 42% for their method.\n3 POLITICAL LEANING DETECTOR USING A\nINFORMATION THEORY APPROACH\nIn this section, we describe the POLitical Leaning Detector (Poll),\na novel method we propose to detect media bias in news websites\nby using Information Theory concepts, more specifically, Shannon\nentropy and statistical divergence. Our method is similar to TF-IDF,\nbut the key difference is that we have a strategy to select the most\nuseful terms to characterize the speech of each bias class, using\nentropy to quantify the importance of terms. Figure 1 summarizes\nthe steps that compose our approach which are discussed in the\nnext subsections.\nClassifying and\nevaluatingNews articles\nPre-processing Computing\nimportance of terms\nRepresenting news \nportals and bias \nclasses01 0203040506070\nComputing \ndissimilarities01 020304050607001 0203040506070\nFigure 1: Overview of Poll (POLitical Leaning Detector), a\ninformation theory-based method to detect media bias in\nnews websites.\n3.1 News Articles and Preparing the Data\nAs a starting point for our method, we need a collection of news ar-\nticles belonging to websites of known leaning/bias. We pre-process\nthe data by transforming the text of the articles (from both title\nand content) to lower case, remove numbers, special characters\nand punctuation, and words that are not in English. This step is\nnecessary because we identified some noise, like words similar to\nthe name of functions in programming languages and HTML tags.\nSince we will quantify the importance of the terms in the next step,\nwe do not remove stop words. If they are irrelevant to the context,\nthe method will filter them.\n3.2 Computing the Importance of Terms\nTo calculate the importance of terms in the vocabulary, we com-\nputed Shannon entropy [ 22], a quantifier from Information Theory\nthat measures the amount of information carried by a variable (or\nrandomness, from a statistical perspective). Given a probabilitymass function (pmf) \ud835\udc5d=(\ud835\udc5d1,\ud835\udc5d2,...,\ud835\udc5d\ud835\udc5b)over a sample space of size\n\ud835\udc5b, i.e.\u0000\u00cd\ud835\udc5b\n\ud835\udc56=1\ud835\udc5d\ud835\udc56\u0001=1, the Shannon entropy is given by [22]\n\ud835\udc3b(\ud835\udc5d)=\u2212\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc5d\ud835\udc56log\ud835\udc5d\ud835\udc56. (1)\nWe use the Shannon entropy to quantify how useful a term is\nto distinguish two or more classes of bias by running through the\nfollowing steps:\n(1)Compute the frequency of all the terms in our reference\ncorpus and discard the low-frequency ones (less than 10 in\nour datasets), as those terms might be noisy terms , which\nyields our vocabulary \ud835\udc49.\n(2)Given a problem of \ud835\udc41classes1, for each term \ud835\udc61\u2208\ud835\udc49compute\n\ud835\udc5d(\ud835\udc61)=(\ud835\udc5d(\ud835\udc61)\n1,\ud835\udc5d(\ud835\udc61)\n2,...,\ud835\udc5d(\ud835\udc61)\n\ud835\udc41), which is the pmf of the term \ud835\udc61\nover the sample space of our bias classes.\n(3)For each term \ud835\udc61\u2208\ud835\udc49, compute\ud835\udc3b\u0010\n\ud835\udc5d(\ud835\udc61)\u0011\n, which represents\nthe importance of the term for distinguishing among the\nbias classes.\n(4)Select a subset \ud835\udc49\ud835\udc45\u2286\ud835\udc49, called vocabulary of reference, of the\n\ud835\udc5amost relevant terms, based on the \ud835\udc3b\u0010\n\ud835\udc5d(\ud835\udc61)\u0011\nvalues com-\nputed in the previous step. The naive strategy is to keep the\n\ud835\udc5aterms of lowest entropy. This is the strategy we adopt in\nthis paper, and the value of \ud835\udc5ais further specified in Section 4.\nTo understand how we use entropy in this work, let us check the\ntwo extreme cases. A term that evenly occurs across all the classes\nof our problem will have the maximum entropy log\ud835\udc41, and that\nterm will be useless to distinguish among those classes (random\noccurrence). On the other extreme, a term that occurs only in a\nsingle class will have entropy zero (minimum), and that term will\ncorrectly identify the target class (assuming our sample corpus\nperfectly describes the reality).\nFigure 2 shows an example comparing the entropy of the terms\ntrump and soros in one dataset. In this example, we can see that\ntrump is more evenly cited than soros , which is mostly concentrated\nwithin the class right . Thus, the term soros should be better than\ntrump to distinguish among those four classes.\n3.3 Representing a News Portal\nFor every news portal/website \ud835\udc64, we obtained a collection of arti-\ncles/pages. In this work, we represent \ud835\udc64by a pmf, in which each\nterm\ud835\udc61\u2208\ud835\udc49\ud835\udc45is mapped onto a bin of the pmf, representing the ex-\npected probability (normalized frequency) of \ud835\udc61in an average article\npublished in \ud835\udc64.\nLet us say that|\ud835\udc49\ud835\udc45|=\ud835\udc5b. Given a website \ud835\udc64with a corpus \ud835\udc37of\ndocuments/articles collected from \ud835\udc64, for every\ud835\udc61\u2208\ud835\udc49\ud835\udc45and\ud835\udc51\u2208\ud835\udc37,\n\ud835\udc39(\ud835\udc64)\n\ud835\udc61=\u00d5\n\ud835\udc51\u2208\ud835\udc37,\ud835\udc61\u2208\ud835\udc51\ud835\udc53\ud835\udc61,\ud835\udc51, (2)\nin which\ud835\udc53\ud835\udc61,\ud835\udc51is the raw count (frequency) of \ud835\udc61in\ud835\udc51. Then, portal \ud835\udc64\nis represented by the pmf \ud835\udc5d(\ud835\udc64)=\u0010\n\ud835\udc5d(\ud835\udc64)\n1,\ud835\udc5d(\ud835\udc64)\n2,...,\ud835\udc5d(\ud835\udc64)\n\ud835\udc5b\u0011\n, in which\n1For instance, \ud835\udc41=4for the problem of classifying a news portal as having a left,left\ncenter ,right center , orright political orientation/bias.\nWISDOM \u201920, August 24th, 2020, San Diego, CA Aires et al.\nLeft Left Center Right Center Right0.00.10.20.30.40.5\n(a) Citation of the term trump , referring to Donald Trump. The en-\ntropy score of this term was 1.69.\nLeft Left Center Right Center Right0.00.20.40.60.8(b) Citation of the term soros , referring to George Soros. The entropy\nscore of this term was 0.55.\nFigure 2: Example of citation of two terms by sources belonging to each four bias classes in dataset News-July. The values are\nnormalized.\n\ud835\udc5d(\ud835\udc64)\n\ud835\udc61=\ud835\udc39(\ud835\udc64)\n\ud835\udc61\u00cd\n\ud835\udc61\u2032\u2208\ud835\udc49\ud835\udc45\ud835\udc39(\ud835\udc64)\n\ud835\udc61\u2032(3)\nand, consequently,\u00cd\n\ud835\udc61\u2208\ud835\udc49\ud835\udc45\ud835\udc5d(\ud835\udc64)\n\ud835\udc61=1.\n3.4 Representing a Bias/Leaning Class\nNow let us consider we have the bias/leaning classes represented\nby\ud835\udc35={\ud835\udc4f1,\ud835\udc4f2,...,\ud835\udc4f\ud835\udc41}in which every \ud835\udc4f\u2208\ud835\udc35is a class (e.g. left,left\ncenter ,right center , and right, so that \ud835\udc41=4). We represent every\nclass\ud835\udc4f\u2208\ud835\udc35by a pmf that is computed analogously to the pmf for\neach portal, but instead of using the documents of a target portal,\nwe consider the documents for a target class.\nLet us say that|\ud835\udc49\ud835\udc45|=\ud835\udc5b. Given a class \ud835\udc4fwith a corpus \ud835\udc37\ud835\udc4fof\ndocuments/articles collected from news portals of class \ud835\udc4f, for every\n\ud835\udc61\u2208\ud835\udc49\ud835\udc45and\ud835\udc51\u2208\ud835\udc37\ud835\udc4f,\n\ud835\udc39(\ud835\udc4f)\n\ud835\udc61=\u00d5\n\ud835\udc51\u2208\ud835\udc37\ud835\udc4f,\ud835\udc61\u2208\ud835\udc51\ud835\udc53\ud835\udc61,\ud835\udc51, (4)\nin which\ud835\udc53\ud835\udc61,\ud835\udc51is the raw count (frequency) of \ud835\udc61in\ud835\udc51. Then, class \ud835\udc4fis\nrepresented by the pmf \ud835\udc5d(\ud835\udc4f)=\u0010\n\ud835\udc5d(\ud835\udc4f)\n1,\ud835\udc5d(\ud835\udc4f)\n2,...,\ud835\udc5d(\ud835\udc4f)\n\ud835\udc5b\u0011\n, in which\n\ud835\udc5d(\ud835\udc4f)\n\ud835\udc61=\ud835\udc39(\ud835\udc4f)\n\ud835\udc61\u00cd\n\ud835\udc61\u2032\u2208\ud835\udc49\ud835\udc45\ud835\udc39(\ud835\udc4f)\n\ud835\udc61\u2032(5)\nand, consequently,\u00cd\n\ud835\udc61\u2208\ud835\udc49\ud835\udc45\ud835\udc5d(\ud835\udc4f)\n\ud835\udc61=1.\nNoteworthy, the classes represented by eq. (4) and (5) include\nonly the documents of reference. The target documents represented\nby eq. (2) and (3) are not included in the computation of the \ud835\udc5d(\ud835\udc4f)\n\ud835\udc61\npmfs that represent the bias classes.3.5 Computing Dissimilarities Between News\nPortals and Bias Classes\nAfter obtaining the pmfs for each news portal, we calculate a dis-\nsimilarity matrix that will model how different every news portal\nis with respect to every bias class.\nFor every news portal \ud835\udc58and bias class \ud835\udc4f, we compute \ud835\udc37(\ud835\udc58\u2225\ud835\udc4f),\nin which\ud835\udc37(\u00b7\u2225\u00b7) is a divergence, i.e., given a space of probability\ndistributions \ud835\udc46, with common support, \ud835\udc37(\u00b7\u2225\u00b7) :\ud835\udc46\u00d7\ud835\udc46\u2192Ris a\nfunction such that\n\u2022\ud835\udc37(\ud835\udc5d\u2225\ud835\udc5e)\u22650, for all\ud835\udc5d,\ud835\udc5e\u2208\ud835\udc46and\n\u2022\ud835\udc37(\ud835\udc5d\u2225\ud835\udc5e)=0if, and only if, \ud835\udc5d=\ud835\udc5e.\nThe objective of \ud835\udc37(\u00b7\u2225\u00b7) is to account the difference between\ntwo pmfs (shapewise). In this work, we consider three important\ndivergences: Cosine distance, Jaccard distance, and Jensen-Shannon\ndivergence.\nThe cosine distance is commonly used for Information Retrieval\nproblems [ 2]. It is equivalent to the Pearson correlation, being\nproportional to the angle between two points in a vector space\n(sample space, in our case). The cosine distance between \ud835\udc5dand\ud835\udc5eis\ngiven by\ncos(\ud835\udc5d,\ud835\udc5e)=1\u2212\ud835\udc5b\u00cd\n\ud835\udc56=1\ud835\udc5d\ud835\udc56\ud835\udc5e\ud835\udc56\ns\n\ud835\udc5b\u00cd\n\ud835\udc56=1\ud835\udc5d2\n\ud835\udc56s\n\ud835\udc5b\u00cd\n\ud835\udc56=1\ud835\udc5e2\n\ud835\udc56. (6)\nThe Jaccard distance [ 14,16] between\ud835\udc5dand\ud835\udc5eis widely used in\nBiology domains and also in Computer Science to measure differ-\nences between vectors in R\ud835\udc5bspaces, and it is given by\njac(\ud835\udc5d,\ud835\udc5e)=1\u2212\ud835\udc5b\u00cd\n\ud835\udc56=1min(\ud835\udc5d\ud835\udc56,\ud835\udc5e\ud835\udc56)\n\ud835\udc5b\u00cd\n\ud835\udc56=1max(\ud835\udc5d\ud835\udc56,\ud835\udc5e\ud835\udc56). (7)\nAn Information Theory Approach to Detect Media Bias in News Websites WISDOM \u201920, August 24th, 2020, San Diego, CA\nThe Jensen-Shannon divergence [ 15] between\ud835\udc5dand\ud835\udc5erelates to\nthe concept of entropy and can be defined as\njsd(\ud835\udc5d,\ud835\udc5e)=\ud835\udc3b(\ud835\udc5d)+\ud835\udc3b(\ud835\udc5e)\n2\u2212\ud835\udc3b\u0010\ud835\udc5d+\ud835\udc5e\n2\u0011\n(8)\nin which\ud835\udc3b(\u00b7)is the Shannon Entropy as defined in eq. (1). In\ngeneral, the Jensen-Shannon divergence is a strong measure to\naccount the difference between pmfs. In a simplistic way, the Jensen-\nShannon divergence accounts the amount of bits that differs the\npmfs being compared [ 15] and it is closely related to the concept of\nmutual information [9].\n3.6 Classifying and Evaluating\nAfter obtaining a dissimilarity matrix that accounts for the differ-\nences between the speech of each news portal and each class of\nbias, we feed a classifier with this matrix as features. This classifier\nwill use these dissimilarity scores to distinguish the classes among\neach other.\n4 EXPERIMENTS & RESULTS\nIn this section, we describe the results obtained when applying the\nmethod to two different datasets and detail the experimental setup\nused to evaluate the performance of the classifier.\n4.1 Datasets\nAs discussed in Section 3, we need two types of data: news articles\nbelonging to news websites with previously known political bias;\nand the assigned orientation (leaning or bias) of these sources. We\nused the labels obtained from Media Bias Fact Check (MBFC) [ 17], a\nfact-checking website that classifies news websites regarding ideo-\nlogical bias and credibility of factual reporting. Their methodology,\nalthough subjective, is based on a quantified system. They define\nfive labels of political orientation/bias: left,left center ,center ,right\ncenter , and right . Figure 3 shows an example of a website labeled\nby MBFC. In this work, we will not consider websites from the cen-\nter class, since we want to focus on a polarized field of discourse,\nspecifically, left (extreme and moderate) and right-wing (extreme\nand moderate).\nFigure 3: An example of a website labeled by Media Bias Fact\nCheck [17].\nBecause most of the available news datasets include numerous\narticles from a few websites, we considered that they would notbe beneficial to our application. Our aim is to classify political bias\nof news portals, so we maximized the number of websites in our\ntests. Thus, we decided to build datasets more appropriated for this\ntask. Using the websites labeled by MBFC as seeds in a crawling\nprocess, we created two different datasets, News-July andNews-\nFebruary , collecting news articles from these seeds in different\ntime spans. It is important to highlight that we did not restricted\nthe crawl to a specific topic, i.e., we crawled news articles about\narbitrary subjects. After crawling these two datasets, we sampled\nthe articles to balance the number of websites belonging to each\nbias class and the number of articles of each source. We list the\ndetails of each dataset in Table 2.\nTable 2: Datasets built by crawling seeds from Media Bias\nFact Check [17].\nDataset Time windowNumber\nof websitesArticles\nper websiteTotal\nof articles\nNews-July June 17-19, 2019 248 20 4960\nNews-February February 14-15, 2020 576 20 11520\n4.2 Experimental Setup\nOnce the data was gathered, we defined an experimental setup\nto tune and evaluate the performance of our method. There are\nsome aspects to consider, such as model of the vocabulary, the\ndissimilarity metric, and the features. Our choices are discussed in\nthe next paragraphs.\nModeling the vocabulary .Among the ways of modeling the\nterms in the text, we decided to test unigrams and bigrams. The\nidea is to analyze if a better representation of the context has a\npositive impact on the performance.\nSelecting terms .Like explained in Section 3, we consider only the\n\ud835\udc5aterms of lowest entropy. We empirically determined \ud835\udc5a=10,000\nas the best value for our scenarios, based on the number of terms\nin each vocabulary for both News-July and News-February. Other\ndatasets and domains might have a different value.\nDissimilarity measure .Given two pmfs in a sample space of\nsize\ud835\udc5b(number of terms in our vocabulary of reference \ud835\udc49\ud835\udc45),\ud835\udc5d=\n(\ud835\udc5d1,\ud835\udc5d2,...,\ud835\udc5d\ud835\udc5b)and\ud835\udc5e=(\ud835\udc5e1,\ud835\udc5e2,...,\ud835\udc5e\ud835\udc5b), we assess the performance\nof three dissimilarity measures as our divergence \ud835\udc37(\ud835\udc5d\u2225\ud835\udc5e): the co-\nsine distance, Jaccard distance, and Jensen-Shannon divergence,\npresented in Section 3.5.\nFeatures .After computing the frequency histograms and having\nthe dissimilarity measures, we define which sets of classes will com-\npose the dissimilarity matrix. We chose the following alternatives\nto perform our tests:\n\u2022Extreme/Moderate ( \ud835\udc37\ud835\udc38,\ud835\udc37\ud835\udc40): in this case, for every target\ndocument we compute two features: (1) \ud835\udc37\ud835\udc38, the divergence\nof the document\u2019s pmf to the extreme (left and right) class\u2019\npmf; and (2) \ud835\udc37\ud835\udc40, the divergence of the document\u2019s pmf to\nthemoderate (left and right) class\u2019 pmf.\n\u2022Left/Right (\ud835\udc37\ud835\udc3f,\ud835\udc37\ud835\udc45): in this case, for every target document\nwe compute two features: (1) \ud835\udc37\ud835\udc3f, the divergence of the doc-\nument\u2019s pmf to the left(extreme and moderate) class\u2019 pmf;\nWISDOM \u201920, August 24th, 2020, San Diego, CA Aires et al.\n0.10.20.30.40.50.60.70.80.9\nPrecision0.10.20.30.40.50.60.70.80.9RecallExtreme/Moderate\nCombination\nu-cos\nu-jac\nu-jsd\nb-cos\nb-jac\nb-jsd\nF1\n0.0\n0.3\n0.6\n0.9\nFeatures\n{DE,DM}\n{DL,DR}\n{DL,DLC,DRC,DR}\n0.10.20.30.40.50.60.70.80.9\nPrecision0.10.20.30.40.50.60.70.80.9Left/Right\n0.10.20.30.40.50.60.70.80.9\nPrecision0.10.20.30.40.50.60.70.80.9Left/Left Center/Right Center/Right\n(a) Dataset News-July.\n0.10.20.30.40.50.60.70.80.9\nPrecision0.10.20.30.40.50.60.70.80.9RecallExtreme/Moderate\nCombination\nu-cos\nu-jac\nu-jsd\nb-cos\nb-jac\nb-jsd\nF1\n0.0\n0.3\n0.6\n0.9\nFeatures\n{DE,DM}\n{DL,DR}\n{DL,DLC,DRC,DR}\n0.10.20.30.40.50.60.70.80.9\nPrecision0.10.20.30.40.50.60.70.80.9Left/Right\n0.10.20.30.40.50.60.70.80.9\nPrecision0.10.20.30.40.50.60.70.80.9Left/Left Center/Right Center/Right\n(b) Dataset News-February.\nFigure 4: Performance obtained by Poll using different strategies in terms of precision, recall and F1.\nand (2)\ud835\udc37\ud835\udc45, the divergence of the document\u2019s pmf to the\nright (extreme and moderate) class\u2019 pmf.\n\u2022Left/Left Center/Right Center/Right ( \ud835\udc37\ud835\udc3f,\ud835\udc37\ud835\udc3f\ud835\udc36,\ud835\udc37\ud835\udc45\ud835\udc36,\ud835\udc37\ud835\udc45): fin\nthis case, for every target document we compute four fea-\ntures: (1)\ud835\udc37\ud835\udc3f, the divergence of the document\u2019s pmf to the\nextreme left class\u2019 pmf; (2) \ud835\udc37\ud835\udc3f\ud835\udc36, the divergence of the doc-\nument\u2019s pmf to the moderate left class\u2019 pmf; (3) \ud835\udc37\ud835\udc45\ud835\udc36, the\ndivergence of the document\u2019s pmf to the moderate right\nclass\u2019 pmf; and (4) \ud835\udc37\ud835\udc45, the divergence of the document\u2019s pmf\nto the extreme right class\u2019 pmf.\nTasks .We are interested in comparing our method, Poll, with three\ndifferent classification tasks:\n\u2022Extreme/Moderate.\n\u2022Left/Right.\n\u2022Left/Left center/Right center/Right.\nWith these tasks, we can evaluate if the discourse of extreme\nsources is more similar than the ones of more moderate sources.\nWe can also evaluate if using the corresponding set of features leads\nto better results when performing each task.\nClassifier and evaluation .For the classifier, we chose the Sup-\nport Vector Machine (SVM) model with RBF kernel, \ud835\udc36=1.0and\nremaining parameters set to the default of the scikit-learn library2.\n2https://scikit-learn.org/stable/We conducted the experiments using leave-one-out cross valida-\ntion (LOOCV) and computed the metrics: precision, recall, F1, and\naccuracy.\nLast, we compared Poll with the method proposed by Baly et al .\n[3]described in Section 2 as a baseline. This method represent news\narticles by calculating a set of 141 features like POS tags, sentiment\nscores, bias, subjectivity, and morality. They compute these features\nfor both title and body, which leads to a set of 282 features in\ntotal, that are given as input to a supervised method, specifically,\na SVM classifier. To perform the experiments, we implemented\nthe method using the code shared by the authors3to model and\nclassify our two datasets. The performance was evaluated applying\nthe same setup described to evaluate Poll, i.e., leave-one-out cross\nvalidation (LOOCV) and using precision, recall, F1 and accuracy as\nperformance measures.\nWe selected this baseline because, like our approach, they focus\non automatic detecting media bias of news websites; they also do\nnot restrict articles to a single subject and period of time, and they\nuse several features that are more common in text classification,\nmore specifically, in fake news detection. In addition, this baseline\nwas designed to work with multi-class problems as well. Thus,\nwe can determine if our method can perform better than a more\ntraditional method by applying a smaller set of features independent\n3https://github.com/ramybaly/News-Media-Reliability/\nAn Information Theory Approach to Detect Media Bias in News Websites WISDOM \u201920, August 24th, 2020, San Diego, CA\nTable 3: Classification results for two datasets when performing three classification tasks. Best results for each task are bold.\n(a) Performance for Extreme/Moderate task.\nMethod DatasetPerformance\nPrecision Recall F1Accuracy\nExtreme Moderate Extreme Moderate Extreme Moderate\nBaly et al. [3]News-July 0.42 0.44 0.36 0.49 0.39 0.46 43%\nNews-February 0.43 0.43 0.44 0.42 0.43 0.42 43%\nPollNews-July 0.87 0.84 0.84 0.88 0.86 0.86 86%\nNews-February 0.86 0.83 0.83 0.87 0.84 0.85 85%\n(b) Performance for Left/Right task.\nMethod DatasetPerformance\nPrecision Recall F1Accuracy\nLeft Right Left Right Left Right\nBaly et al. [3]News-July 0.50 0.50 0.48 0.53 0.49 0.52 50%\nNews-February 0.42 0.44 0.37 0.50 0.39 0.47 43%\nPollNews-July 0.84 0.78 0.76 0.85 0.80 0.82 81%\nNews-February 0.86 0.87 0.87 0.86 0.86 0.86 86%\n(c) Performance for Left/Left Center/Right Center/Right task.\nMethod DatasetPerformance\nPrecision Recall F1Accuracy\nLeftLeft\nCenterRight\nCenterRight LeftLeft\nCenterRight\nCenterRight LeftLeft\nCenterRight\nCenterRight\nBaly et al. [3]News-July 0.29 0.26 0.34 0.25 0.26 0.31 0.34 0.23 0.27 0.28 0.34 0.24 28%\nNews-February 0.26 0.22 0.19 0.22 0.30 0.22 0.19 0.19 0.28 0.22 0.19 0.21 22%\nPollNews-July 0.77 0.75 0.63 0.79 0.79 0.66 0.73 0.74 0.78 0.70 0.68 0.77 73%\nNews-February 0.82 0.73 0.69 0.80 0.78 0.73 0.77 0.75 0.80 0.73 0.73 0.77 76%\nof context. Noteworthy, although the method proposed by Baly et al .\n[3]incorporates other sources of information and different tasks,\nthe authors also allow to execute the method using only features\nextracted from the articles (title and content), which is exactly our\ncontext. Also, it provides the possibility to choose other bias classes.\nSo, we can use these settings to compare the baseline directly to\nour proposed method, and verify how it compares to a traditional\nmethod while using information obtained from news articles only,\ni.e., without relying on external sources.\n4.3 Experiment 1: Variations of Poll\nIn the first experiment, we compare the results obtained by Poll\nwhen using different vocabulary models, dissimilarity measures,\nand sets of features for each classification task and dataset. Figure 4\nillustrates the performances in terms of precision, recall, and F1.\nIn this figure, each method refers to combining unigrams ( u) or\nbigrams ( b) with a divergence: cosine distance ( cos), Jaccard distance\n(jac) and Jensen-Shannon divergence ( jsd).\nComparing the performances obtained by unigrams and bigrams,\nwe see that bigrams outperformed unigrams. This result indicates\nthat a better representation of the context leads to more represen-\ntative probability mass functions and a better characterization of\nthe discourses of the bias classes.In terms of the experimented divergences, the results highlight\nsome key differences in performance. In general, the combinations\nthat used cosine distance got the lower balances between precision,\nrecall and F1. Computing Jaccard distance and Jensen-Shannon\ndivergence led to similar results, but Jensen-Shannon performed\nbetter in all cases. These results make sense, since we are working\nwith probability mass functions: Jensen-Shannon divergence is\nmore sensitive to differentiate between these distributions, thus\nbeing more suitable than more common metrics.\nIn terms of which task was easier or more challenging, we see\nthat classifying extreme/moderate sources was easier, followed by\nleft/right and left/left center/right center/right as the most chal-\nlenging. This is illustrated in each plot, where the green semi-circle\nhighlights that classifying extreme/moderate resulted in a better\nclassification, leading to similar values of precision, recall, and F1.\nIn comparison, the performances when classifying left/right and\nfour-classes were more nuanced. This confirms the intuition that\na multi-class problem is more challenging than the binary cases,\nand that the speeches of extreme sources are more similar between\nthem than to those of moderate sources.\nComparing the three sets of features, the results show that using\ndissimilarities based in four classes was the best strategy in all\ntasks for both datasets. Besides that, there seems to be a correlation\nWISDOM \u201920, August 24th, 2020, San Diego, CA Aires et al.\n(a) Confusion matrix of the baseline for News-July.\n (b) Confusion matrix of the baseline for News-February.\n(c) Confusion matrix of Poll for News-July.\n (d) Confusion matrix of Poll for News-February.\nFigure 5: Confusion matrices obtained by each method when performing the multi-class problem. Values are normalized by\nrows.\nbetween the task and the corresponding set of features. When\nperforming the extreme/moderate task, using extreme/moderate\ndivergences led to a good balance of precision, recall and F1. The\nsame occurred when performing the left/right task using left/right\nfeatures. In the third class, using the four features set was the best\nstrategy, especially in the News-February dataset.\nOn a side note, Figure 4 also shows that Poll, regardless the\nterm size and the divergence being used is well balanced regarding\nprecision and recall, because the points are close to a implicit 45\u25e6\nline between the precision and recall axes.\nFrom this first experiment, we conclude that: (i) using bigrams\nas the vocabulary model leads to better results; (ii) the best dissimi-\nlarity measure for this context was Jensen-Shannon divergence; (iii)\nthere seems to be a relation between the task and the set of features,\nbut the best choice for all tasks was to compute left/left center/right\ncenter/right dissimilarities and (iv) the best combination for Poll is:\nbigrams as terms, Jensen-Shannon divergence as the divergence,\nand\ud835\udc37\ud835\udc3f,\ud835\udc37\ud835\udc3f\ud835\udc36,\ud835\udc37\ud835\udc45\ud835\udc36,\ud835\udc37\ud835\udc45as features. For now on, when we refer to\nPoll, we are referring to this specific combination. This result is\nused to compare Poll to the baseline in the next experiment.\n4.4 Experiment 2: Poll vs. the Baseline\nAfter the first experiment, we verified how Poll, combined with\nbigrams, Jensen-Shannon divergence, and four-class features, per-\nforms against a more traditional baseline. Our tests included the\nthree classification tasks. Table 3 summarizes the results.In the first task (Table 3a), where we classify extreme and mod-\nerate sources, the results show that the baseline has low precision,\nrecall, F1, and accuracy. This means that the method is not able to\ndistinguish between the classes, especially in the case of dataset\nNews-July, where it had more confusion between extreme and mod-\nerate sources. Poll, in contrast, achieved high scores and balanced\nresults in precision, recall, and 0.86of F1 for both classes, indicating\nthat our features were able to discriminate well between classes.\nIn absolute terms, our method performed almost twice as better\nthan the baseline for all performance metrics, achieving a macro\naccuracy of 86%versus 43%obtained by the baseline in the same\nsituation.\nSimilarly, in the second task (Table 3b), in which we classify\nleft and right sources, the results show that the baseline was more\nsuccessful classifying the right class than the left class. But even so,\nprecision, recall, F1, and accuracy were low (close to 0.50). Again,\nPoll performed almost twice as better than the baseline, with bal-\nanced results for both classes (F1 equal to 0.86) and a macro accuracy\nscore of 86%against 43%achieved by the baseline in the same case.\nThe four-class problem (Table 3c), classifying all four classes of\nbias, is where our method really stands out. Figure 5 shows the\nconfusion matrices obtained by each method for each dataset. The\nbaseline had trouble to distinguish between the four bias classes,\nperforming poorly. Poll, on the other hand, was able to distinguish\nbetween the four bias classes. Our method performed better when\nclassifying both extreme classes (left and right), with F1 equal to\n0.80and 0.77for these classes in the best case. But even to the\nAn Information Theory Approach to Detect Media Bias in News Websites WISDOM \u201920, August 24th, 2020, San Diego, CA\nmoderate classes, that tend to be more similar between them, we\ngot good results, with F1 of 0.73for both left center and right center\nclasses in the same case. Compared to the baseline, Poll performed\nalmost 3.5 times better, achieving a macro accuracy score of 76%\nversus 22%in the same dataset.\nThese results are related to the strategy each method applies to\nrepresent news websites and bias classes. The baseline uses 282\ntextual features that are probably very similar for all four bias\nclasses. So, they are not very useful to characterize the discourse\nof each ideological bias/orientation. Our strategy, on the other\nhand, reduces the number of features by focusing on capturing\nparticularities of the discourses of each bias class. The results show\nthat this strategy leads to more representative features, allowing a\nclassifier to accurately distinguish the four bias classes.\nSo, with the second experiment, we conclude that: (i) Poll was\nsuccessful in accurately classifying two binary problems and a multi-\nclass problem; (ii) Poll outperformed the baseline with balanced\nscores of precision, recall, and F1, reaching accuracy scores above\n73%(multi-class) and 81%(binary), using only four features, against\n282 from the baseline whose accuracy was as low as 22%.\n5 CONCLUSIONS & FUTURE WORK\nIn this paper, we presented Poll (POLitical Leaning Detector), a\nnew approach to detect media bias in news websites. Our approach\napplies concepts from Information Theory to quantify the impor-\ntance of terms in news articles and better characterize the speech\nof websites with a particular ideological leaning.\nTo evaluate the effectiveness of the method, we performed ex-\nperiments to classify two datasets composed by news of different\nperiods and discussing several topics, without restriction to a single\nsubject. Thus, we showed that our approach accurately classifies\nthe bias of news websites in three different situations: separating\nmore extreme and more moderate sources; left and right sources;\nand a more detailed classification, separating four classes of bias\n(left, left center, right center, and right sources). We observed that\nour method outperformed a more traditional approach that uses\n282 textual features like sentiment scores and POS tags, achieving\naccuracy scores 2 to 3 times higher than the baseline. Furthermore,\nour approach obtains these results using a set of only 2\u20134 features.\nThis result shows that our proposed method effectively captures\nthe particularities of the discourse used by websites of each political\nbias/orientation.\nAs future work, we plan to explore other strategies to select\nterms. Instead of using a fixed number of terms, we can investigate\nmore sophisticated possibilities and analyze the impact of these\nchoices on the final classification. Also, we intend to check how\nthe method performs when classifying other collections of news,\nlike past news and more recent news, and also news about specific\ntopics.\nREFERENCES\n[1]Vict\u00f3ria Patr\u00edcia Aires, Fab\u00edola G. Nakamura, and Eduardo Freire Nakamura. 2019.\nA Link-based Approach to Detect Media Bias in News Websites. In Companion of\nThe 2019 World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May\n13-17, 2019 , Sihem Amer-Yahia, Mohammad Mahdian, Ashish Goel, Geert-Jan\nHouben, Kristina Lerman, Julian J. McAuley, Ricardo Baeza-Yates, and Leila Zia\n(Eds.). ACM, 742\u2013745. https://doi.org/10.1145/3308560.3316460[2]Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al .1999. Modern information\nretrieval . Vol. 463. ACM press New York.\n[3]Ramy Baly, Georgi Karadzhov, Dimitar Alexandrov, James Glass, and Preslav\nNakov. 2018. Predicting Factuality of Reporting and Bias of News Media Sources.\nInProceedings of the Conference on Empirical Methods in Natural Language Pro-\ncessing . Association for Computational Linguistics, Brussels, Belgium.\n[4]Jonas Nygaard Blom and Kenneth Reinecke Hansen. 2015. Click bait: Forward-\nreference as lure in online news headlines. Journal of Pragmatics 76 (Jan. 2015),\n87\u2013100. https://doi.org/10.1016/j.pragma.2014.11.010\n[5]Wei-Fan Chen, Henning Wachsmuth, Khalid Al-Khatib, and Benno Stein. 2018.\nLearning to Flip the Bias of News Headlines. In Proceedings of the 11th In-\nternational Conference on Natural Language Generation . Association for Com-\nputational Linguistics, Tilburg University, The Netherlands, 79\u201388. https:\n//doi.org/10.18653/v1/W18-6509\n[6]Alexander Dallmann, Florian Lemmerich, Daniel Zoller, and Andreas Hotho.\n2015. Media bias in german online newspapers. In Proceedings of the 26th ACM\nConference on Hypertext & Social Media . ACM, 133\u2013137.\n[7]Miles Efron. 2004. The liberal media and right-wing conspiracies: using cocitation\ninformation to estimate political orientation in web documents. In Proceedings\nof the Thirteenth ACM International Conference on Information and Knowledge\nManagement . ACM, 390\u2013398.\n[8]Erick Elejalde, Leo Ferres, and Eelco Herder. 2017. The nature of real and perceived\nbias in chilean media. In Proceedings of the 28th ACM Conference on Hypertext\nand Social Media . ACM, 95\u2013104.\n[9]D. M. Endres and J. E. Schindelin. 2003. A new metric for probability distributions.\nIEEE Transactions on Information Theory 49, 7 (July 2003), 1858\u20131860. https:\n//doi.org/10.1109/TIT.2003.813506\n[10] Joshua Gordon, Marzieh Babaeianjelodar, and Jeanna Matthews. 2020. Studying\nPolitical Bias via Word Embeddings. In Companion Proceedings of the Web Con-\nference 2020 (Taipei, Taiwan) (WWW \u201920) . Association for Computing Machinery,\nNew York, NY, USA, 760\u2013764. https://doi.org/10.1145/3366424.3383560\n[11] Ruth A Harper. 2010. The Social Media Revolution: Exploring the Impact on Jour-\nnalism and News Media Organizations. 2, 03 (2010). http://www.inquiriesjournal.\ncom/a?id=202\n[12] Markus Knoche, Radomir Popovi\u0107, Florian Lemmerich, and Markus Strohmaier.\n2019. Identifying Biases in Politically Biased Wikis through Word Embeddings.\nInProceedings of the 30th ACM Conference on Hypertext and Social Media (Hof,\nGermany) (HT \u201919) . Association for Computing Machinery, New York, NY, USA,\n253\u2013257. https://doi.org/10.1145/3342220.3343658\n[13] Ralf Krestel, Alex Wall, and Wolfgang Nejdl. 2012. Treehugger or petrolhead?:\nidentifying bias by comparing online news articles with political speeches. In\nProceedings of the 21st International Conference on World Wide Web . ACM, 547\u2013\n548.\n[14] Michael Levandowsky and David Winter. 1971. Distance between sets. Nature\n234, 5323 (1971), 34\u201335.\n[15] Jianhua Lin. 1991. Divergence measures based on the Shannon entropy. IEEE\nTransactions on Information Theory 37, 1 (1991), 145\u2013151.\n[16] Alan H Lipkus. 1999. A proof of the triangle inequality for the Tanimoto distance.\nJournal of Mathematical Chemistry 26, 1-3 (1999), 263\u2013265.\n[17] Media Bias Fact Check. 2019. The Most Comprehensive Media Bias Resource .\nAccessed May, 2020 from https://mediabiasfactcheck.com/.\n[18] Corrado Monti, Alessandro Rozza, Giovanni Zappella, Matteo Zignani, Adam\nArvidsson, and Elanor Colleoni. 2013. Modelling Political Disaffection from Twit-\nter Data. In Proceedings of the 2nd International Workshop on Issues of Sentiment\nDiscovery and Opinion Mining (WISDOM) .\n[19] Fred Morstatter, Liang Wu, Uraz Yavanoglu, Stephen R Corman, and Huan Liu.\n2018. Identifying Framing Bias in Online News. ACM Transactions on Social\nComputing 1, 2 (2018), 5.\n[20] Adithya Rao and Nemanja Spasojevic. 2016. Actionable and Political Text Classifi-\ncation using Word Embeddings and LSTM. In Proceedings of the 5th International\nWorkshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM) .\n[21] Filipe N Ribeiro, Lucas Henrique, Fabricio Benevenuto, Abhijnan Chakraborty,\nJuhi Kulshrestha, Mahmoudreza Babaei, and Krishna P Gummadi. 2018. Media\nbias monitor: Quantifying biases of social media news outlets at large-scale. In\nTwelfth International AAAI Conference on Web and Social Media .\n[22] Claude Shannon. 1948. A mathematical theory of communication. Bell System\nTechnical Journal 27, 3 (1948), 379\u2013423.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "An information theory approach to detect media bias in news websites", "author": ["VP Aires", "J Freire", "FG Nakamura", "AS da Silva"], "pub_year": "2020", "venue": "Proc. ACM KDD \u2026", "abstract": "News websites and portals are, together with social media, major sources of information  nowadays. However, such types of media may be biased regarding, especially, political and"}, "filled": false, "gsrank": 18, "pub_url": "https://www.sentic.net/wisdom2020aires.pdf", "author_id": ["O-w-XhYAAAAJ", "sSzAlq0AAAAJ", "wuftXNUAAAAJ", "Ka6w1vgAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:22DwQaluL54J:scholar.google.com/&output=cite&scirp=17&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D10%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=22DwQaluL54J&ei=BbWsaMyPOeHUieoP9LKZ6AI&json=", "num_citations": 9, "citedby_url": "/scholar?cites=11398450855132815579&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:22DwQaluL54J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.sentic.net/wisdom2020aires.pdf"}}, {"title": "Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts", "year": "2025", "pdf_data": "arXiv:2506.12552v1  [cs.CL]  14 Jun 2025Profiling News Media for Factuality and Bias Using LLMs\nand the Fact-Checking Methodology of Human Experts\nZain Muhammad Mujahid1,2Dilshod Azizov1Maha Tufail Agro1Preslav Nakov1\n1Mohamed bin Zayed University of Artificial Intelligence, UAE\n2University of Copenhagen, Denmark\n{zain.mujahid, dilshod.azizov, maha.agro, preslav.nakov}@mbzuai.ac.ae\nAbstract\nIn an age characterized by the proliferation of\nmis- and disinformation online, it is critical\nto empower readers to understand the content\nthey are reading. Important efforts in this direc-\ntion rely on manual or automatic fact-checking,\nwhich can be challenging for emerging claims\nwith limited information. Such scenarios can\nbe handled by assessing the reliability and the\npolitical bias of the source of the claim, i.e.,\ncharacterizing entire news outlets rather than\nindividual claims or articles. This is an impor-\ntant but understudied research direction. While\nprior work has looked into linguistic and social\ncontexts, we do not analyze individual articles\nor information in social media. Instead, we\npropose a novel methodology that emulates the\ncriteria that professional fact-checkers use to\nassess the factuality and political bias of an en-\ntire outlet. Specifically, we design a variety of\nprompts based on these criteria and elicit re-\nsponses from large language models (LLMs),\nwhich we aggregate to make predictions. In ad-\ndition to demonstrating sizable improvements\nover strong baselines via extensive experiments\nwith multiple LLMs, we provide an in-depth\nerror analysis of the effect of media popular-\nity and region on model performance. Further,\nwe conduct an ablation study to highlight the\nkey components of our dataset that contribute\nto these improvements. To facilitate future re-\nsearch, we released our dataset and code.1\n1 Introduction\nIn an age where digital media dominate and infor-\nmation spreads quickly, profiling news media out-\nlets in terms of their political bias and factuality is\nof utmost importance. Media organizations signifi-\ncantly shape public discourse (Sajwani et al., 2024),\ninfluence policy, and shape public opinion, making\nthe detection of political bias essential (Pennycook\nand Rand, 2021).\n1https://github.com/mbzuai-nlp/llm-media-profilingTraditional methods for characterizing political\nbias in news media, such as subjective assessments\nand manual content analysis, are labor-intensive\nand prone to human biases. Automated content\nand social media analysis techniques (Baly et al.,\n2018, 2020b) have been developed for this, but they\nface limitations, including the laborious process of\nobtaining and annotating news articles.\nEvaluating the factuality of the news reporting\nis equally important. Assessing the accuracy and\ntruthfulness of news articles is the key to maintain\nthe integrity of information dissemination (Baly\net al., 2018). Conventional fact-checking methods\nare resource-intensive and struggle to keep up with\nthe rapid production of news content. LLMs, such\nas the OpenAI GPT series (Radford et al., 2018,\n2019; Brown et al., 2020), offer a promising solu-\ntion. Trained on a vast amount of text datasets,\nLLMs can understand and generate human-like\ntext, providing a new avenue for analyzing media\nbias and factuality at scale.\nIn this paper, we propose a novel methodology\nthat leverages LLMs to predict political bias and\nthe factuality of the reporting of entire news me-\ndia outlets. Our approach (shown in Figure 1) in-\nvolves crafting custom prompts to elicit responses\nfrom LLMs, thus enabling the detection of political\nbias and the factuality of a news outlet, without\nrelying on the manual analysis of individual news\narticles. By employing the criteria used by pro-\nfessional fact-checkers, we aim to provide a more\nsystematic and accurate assessment of political bias.\nWe also conduct a case study to demonstrate the\nlimitations of LLMs in the absence of well-defined\nguidelines, highlighting the importance of expert-\ndriven prompts.\nFurthermore, we conduct detailed error analysis\nto examine the impact of media popularity and\nregion on the performance of the models, revealing\nbiases in favor of more popular and U.S.-based\noutlets.\nMBFC Database\nwww .huffpost.com\nwww .bbc.com\nwww .foxnews.comNews Media Bias\nLeft\nLeft-Center\nRightFactuality\nMixed\nHigh\nMixed\nPerson/T opicPrompt Categories\nTrustworthinessHot Topic\nChatGPTResponse on\nPerson/T opic\nResponse on\nTrustworthinessResponse on\nHot TopicPrompt Responses\nNews MediaText Classifier Bias\nFactualityPredictions(a)\n(c)(b)\nPrompt\nHot Topic\nResponse on\nHot TopicPrompt Response\nNews MediaText Classifier\nBiasPredictions ChatGPT\nExpert's\nGuidelines\u2295Figure 1: Overview of our methodology: (a) collection of gold labels from the MBFC2database, (b) data curation\nusing handcrafted prompts, followed by text classification, (c) data curation using systematic prompts based on\nexpert guidelines, followed by text classification.\nWe also perform an ablation study to identify\nkey components of our dataset, demonstrating the\nimportance of combining leaning and reasoning\ninformation for optimal results. This analysis not\nonly highlights the strengths of our approach but\nalso uncovers critical areas for future improvement.\nThe following summarizes our key contributions:\n\u2022We release a large-scale dataset to model the\nfactuality and political bias of news media.\n\u2022We leverage knowledge from LLMs to predict\nthe factuality and the political bias of news\nmedia.\n\u2022We are the first to emulate the exact criteria\nused by professional fact-checkers when rat-\ning the political bias of the news media.\n\u2022We achieve sizeable improvements over base-\nlines and zero-shot prompting for two tasks:\npredicting (i)the factuality of reporting and\n(ii)the political bias of news outlets.\n\u2022We conduct a comprehensive error analysis\nto examine the influence of media popularity\nand region on model performance.\n\u2022We perform an ablation study to evaluate the\ncontribution of dataset components, showing\nthat the reasoning extracted from LLMs is the\nmost critical part for accurate predictions.2 Related Work\nThe digital age has democratized the creation and\ndissemination of information via numerous me-\ndia platforms, but this has also fueled misinforma-\ntion (Naeem et al., 2021). News media profiling for\npolitical bias and factuality is essential to empower\nusers and fact-checkers, ensure accountability, and\nsupport research (Nakov et al., 2024).\nPolitical bias refers to systematic inclinations\ntowards a candidate or ideology (Waldman and\nDevitt, 1998). Detecting political bias has been\nexplored using a variety of features and method-\nologies, with predictive models operating across\ndifferent levels of granularity, including media out-\nlets, individual articles, and even sentences. For\nexample, Baly et al. (2018) employed features from\nthe NELA toolkit (Horne et al., 2018), while Kulka-\nrni et al. (2018) examined article-level political bias\nby analyzing textual content and URLs, leveraging\nsite-level annotations from AllSides3. At the outlet\nlevel, political bias can be detected by comparing\nmedia language to political speeches (Gentzkow\nand Shapiro, 2006).\n2www.mediabiasfactcheck.com\n3www.allsides.com\nIt can be also done by classifying articles\nalong ideological axes such as left vs. right, or\nhyper-partisan vs. mainstream (Potthast et al.,\n2018; Saleh et al., 2019). Many of these mod-\nels are based on distant supervision and are\ncommonly trained on relatively small, English-\nlanguage datasets (Da San Martino et al., 2023;\nBarr\u00f3n-Cede\u00f1o et al., 2023; Barr\u00f3n-Cede\u00f1o et al.,\n2023; Azizov et al., 2023, 2024).\nFactuality prediction at the source level remains\nunderexplored. Early research estimated the reli-\nability of news sources by analyzing their stance\non true or false claims rather than using explicit\nlabels for medium-level factuality (Dong et al.,\n2016; Baly et al., 2019; Popat et al., 2016, 2018).\nBaly et al. (2018) explored political bias and fac-\ntuality by extracting features from news articles,\nWikipedia entries, Twitter metadata, and URLs,\nshowing that integrating these sources improved\nclassification accuracy. Later, Baly et al. (2019)\nfound that the joint prediction of political bias and\nfactuality was more effective. Baly et al. (2020a)\nused both the linguistic aspects and the social con-\ntext. This included analyzing the text of articles,\naudio content, and social media reactions and dis-\ncussions on platforms such as Facebook, Twitter,\nand YouTube, as well as Wikipedia content about\nthe medium. Further, Azizov et al. (2024) exam-\nined a cross-lingual evaluation of political bias and\nfactuality.\nAlso, some research has focused on developing\nLLMs such as GPT (Brown et al., 2020) and Chat-\nGPT, demonstrating versatility in general-purpose\nreasoning tasks, including assessing factual accu-\nracy and detecting political bias (Qin et al., 2023).\nYang and Menczer (2025) evaluated ChatGPT\u2019s\nability to gauge news outlet credibility across do-\nmains, including non-English and satirical sources,\nfinding a moderate correlation with human expert\nevaluations (Spearman\u2019s \u03c1= 0.54,p < 0.001).\nMehta and Goldwasser (2024) proposed an inter-\nactive framework combining graph-based models,\nLLMs, and human input to profile news sources and\nidentify biased content. Although effective, Man-\nzoor et al. (2025) proposed an approach to profile\nnews media by integrating graph neural network\nrepresentations with pre-trained language models,\nsignificantly boosting performance. Wang et al.\n(2024) addressed concerns about the factual ac-\ncuracy of LLM outputs, proposing solutions for\nannotating LLM-generated responses.Unlike the above studies, we do not use LLM-\ngenerated credibility ratings or human labor. In-\nstead, we use questions on various levels of factu-\nality and political bias, prompting LLMs to gather\ninsights based on their internal parametric knowl-\nedge, which we then aggregate to make predictions.\n3 Methodology\nTo predict the political bias and the factuality of\na news outlet, we use a data-gathering technique\nwhere we prompt the LLM with targeted queries on\nvarious trendy or controversial topics. This method\noperates under the assumption that LLMs already\nencompass a vast amount of data (Yang et al., 2024)\nand possess knowledge that would allow them to\nprofile media outlets effectively. We use the Ope-\nnAIPython package to query the APIendpoint of\nthegpt-3.5-turbo-0125 (GPT-3.5) (Ouyang et al.,\n2022; Brown et al., 2020), released on January 25,\n2024. Specifically, we formulate two approaches,\nshown in Figure 1: first, we manually craft 18 ques-\ntions to query the LLM regarding political bias and\nthe factuality of a media outlet. Second, we inte-\ngrate expert guidelines into our prompts to obtain\nimproved responses to classify the political bias of\nthe outlet. These LLM responses are concatenated\nand passed to text classification models. We also\npresent two case studies where we obtain zero-shot\npredictions from the LLMs by providing the media\nname and some of its recently published articles.\n3.1 Handcrafted Prompts\nFor this approach, we design a set of 18 questions\nto query the LLM. These questions are grouped\ninto three categories (described below). Each ques-\ntion is asked in a separate APIcall to mitigate and\nminimize any potential bias of an ongoing session,\nwhich is standardized across our study.\n3.1.1 Category 1: Stance on Public Figures or\nTopics\nThis category includes seven questions that aim\nto determine a website\u2019s stance on a public figure,\npersonality, or topic. We use the following prompt:\nsystem_prompt = \"You are an assistant tasked with\ndetermining the stance of a website on a\npublic figure personality or a topic.\",\u2192\n,\u2192\nuser_prompt = \"Is {domain}\npro- {topicPublicFigure} . The assistant must\nprovide the stance and reasoning in JSON\nformat, and should not provide anything\nelse.\",\u2192\n,\u2192\n,\u2192\n,\u2192\nThe{domain} and{topicPublicFigure} place-\nholders in the prompts are dynamically replaced\nwith the relevant domain ( e.g.,foxnews.com ) and\ntopics or public figures ( e.g., Trump ,Biden ,Putin ,\nAmerican ,Russian ,German , orPalestinian ) to ana-\nlyze media\u2019s stance and determine the political bias.\nA sample GPT-3.5 response for foxnews.com can\nbe seen in the Appendix A.1.\n3.1.2 Category 2: Stance on Current Popular\nTopics\nIn this category, we present five questions that aim\nto determine the stance of a news source on a trend-\ning topic. We use the following prompt:\nsystem_prompt = \"You are an assistant to help the\nuser determine the stance of a website on a\ncertain topic.\",\u2192\n,\u2192\nuser_prompt = \"What is the stance of {domain} on\n{topic}. The assistant should return the\nstance and the reason in JSON format, and\nshould not provide anything else.\",\u2192\n,\u2192\n,\u2192\nThe{domain} placeholder represents the target\ndomain, and {topic} include options like Ukraine ,\nClimate Change ,Gun Control ,Immigration , or\nAbortion . A sample GPT-3.5 response for foxnews.\ncomis given in the Appendix A.2.\n3.1.3 Category 3: Factuality of the Reporting\nThis category includes six questions aimed at deter-\nmining the factuality of a news outlet\u2019s reporting.\nWe use the following prompt:\nsystem_prompt = \"You are an assistant to help the\nuser determine the credibility of a website.\" ,\u2192\nEach user prompt incorporates a common {de-\nfault_text} directive as shown below:\ndefault_text = \"Assistant must return the\n'answer 'and 'reason 'in JSON format, and do\nnot provide anything else.\",\u2192\n,\u2192\nuser_prompt_q1 = \"Can I trust {domain} . Answer in\n1 word. {default_text} \" ,\u2192\nuser_prompt_q2= \"Has {domain} failed fact-checks?\nAnswer in one word. {default_text} \" ,\u2192\nuser_prompt_q3 = \"Is {domain} LEFT, CENTER, or\nRight. Answer in one word. {default_text} \" ,\u2192\nuser_prompt_q4 = \"Is {domain} Biased.\n{default_text} \" ,\u2192\nuser_prompt_q5 = \"Characterize {domain} in 3\nwords. {default_text} \" ,\u2192\nuser_prompt_q6 = \"Characterize the bias of\n{domain} in 3 words. {default_text} \" ,\u2192The{domain} placeholder represents the target\ndomain. A sample GPT-3.5 response for foxnews.\ncomis given in the Appendix A.3.\n3.2 Systematic Prompts\nThe second method adopts a systematic approach\nto querying an LLM for profiling news media po-\nlitical bias, leveraging the methodology employed\nby fact-checking journalists from Media Bias/Fact\nCheck (MBFC)4. This methodology rates edito-\nrial political bias across 16 policy areas: Gen-\neral Philosophy ,Abortion ,Economic Policy ,Edu-\ncation Policy ,Environmental Policy ,Gay Rights ,\nGun Rights ,Health Care ,Immigration ,Military ,\nPersonal Responsibility ,Regulation ,Social View ,\nTaxes ,Voter ID , and Worker\u2019s/Business Rights . The\nleft- and right-wing definitions for these topics are\ndetailed in the Appendix B.1. In our LLM prompt\nbelow, the {topic} placeholder represents one of\nthe 16 policy areas, while {defLeft} and{defRight}\nare replaced with their respective definitions. The\n{topic} placeholder is replaced by the domain being\nqueried.\nsystem_prompt = '''You are an expert journalist\ntasked with determining the bias leaning of a\nmedia outlet on the topic, {topic}. Return\nthe 'leaning 'and the 'reason 'in JSON\nformat. Consider your training data as\nevidence. Return 'unknown 'if you cannot\ndetermine the leaning.,\u2192\n,\u2192\n,\u2192\n,\u2192\n,\u2192\n,\u2192\nLabels Definition:\n- left: {defLeft}\n- right: {defRight} '''\nuser_prompt = \" {domain} \"\nA sample GPT-3.5 response for foxnews.com\nis given in the Appendix B.2. Since expert defini-\ntions are only available for left and right labels, this\nmethod is not used for experiments in the 5-class\npolitical bias setting.\n4 Case Study: Zero-Shot Predictions\nWe evaluate zero-shot capabilities of several LLMs,\nincluding Mistral 7B (Jiang et al., 2023), LLaMA2\n7B, LLaMA3 (8B and 70B) (Touvron et al., 2023)\nand GPT-3.5 (Ouyang et al., 2022; Brown et al.,\n2020). Predictions for political bias and factuality\nare obtained using two methods: (a) providing only\nthe media outlet name and (b) providing recent\narticles from the media along with the media name.\nWe describe both approaches in \u00a7 4.1 and \u00a7 4.2.\n4www.mediabiasfactcheck.com/methodology/\n4.1 LLM Prompt Given a Media Name\nThe following is the prompt we used, where we\nonly provided the media name as the input to obtain\npredictions about the factuality of the reporting:\nsystem_prompt = '''You are an expert in media\nanalysis. Classify the factual reporting\nlevel of a media given its name ONLY from one\nof three categories from the list provided\nbelow:,\u2192\n,\u2192\n,\u2192\n,\u2192\n- high\n- mixed\n- low\nReturn -1 if you can not classify. '''\nuser_prompt = \" {domain} \"\nThe{domain} placeholder is replaced with the\ndomain of interest. If the LLM is unable to deter-\nmine the label or the outlet is unknown, it admits\nits uncertainty and returns -1, an example output\nof which is shown in Appendix C.1. The prompt\nused to predict political bias is given in Appendix\nC.2. For fine-grained predictions, we add two more\nclasses, left-center andright-center , in this prompt.\n4.2 LLM Prompt Given Media Articles\nFor this case study, we collected five articles from\neach media outlet, focusing on political, economic,\nand social issues. Due to the small number of\narticles, they were not divided across different\ntimeframes. Each article was summarized using\nLLaMA3-70B with the prompt described in the Ap-\npendix C.3. These summaries served as evidence,\nwhich we attached to our prompt and asked LLMs\nto predict the factuality based on this evidence as\nfollows:\nsystem_prompt = '''You are an expert in media\nanalysis. Classify the overall factual\nreporting level of the given news article of\n{media} into one of three categories from the\nlist provided below:,\u2192\n,\u2192\n,\u2192\n,\u2192\n- high\n- mixed\n- low\nReturn -1 if you can not classify. '''\nuser_prompt = \" {article} \"\nThe{domain} placeholder is replaced with the\ndomain of interest, while the {article} placeholder\nis replaced with the summarized article. The\nprompt used to predict political bias is given in\nthe Appendix C.4. For fine-grained predictions,\nwe include two additional classes, left-center and\nright-center , in this prompt.For each media outlet, we applied hard-voting\nfrom five predictions to assign a final label for eval-\nuation. The summarized articles provided context\nfor LLMs, improving accuracy, while the hard-\nvoting technique ensured that the final label reflects\nconsensus from multiple pieces of evidence. This\nreduces variability and potential bias from individ-\nual articles, offering a more balanced assessment\nof a media outlet\u2019s political bias and factuality.\n5 Experiments and Evaluations\n5.1 Dataset\nTo evaluate our system, we use the political bias\nand factuality labels provided by MBFC. An ex-\nample annotation for cnn.com is shown in the Ap-\npendix D. Factuality is assessed on a three-point\nscale: low,mixed , and high. Political bias was orig-\ninally modeled on a seven-point scale, but previous\nresearch (Baly et al., 2020a; Panayotov et al., 2022)\nsimplified it to a three-point scale ( left,center , and\nright ), which we adopt for consistency with prior\nwork. Table 5 in the Appendix D presents the label\ndistribution in our dataset, which is larger and more\ngranular than previous datasets, including fringe\nlabels, such as left-center andright-center .\n5.2 Experimental Setup\nWe use data collected in \u00a7 3.1 & 3.2 to train our\nmodels. Initially, the data is vectorized using TF-\nIDF to train an SVM classifier for the prediction\nof political bias and factuality. A grid search is\nconducted to tune Cand\u03b3for the RBF kernel.\nFor experimentation with transformer-based\nmodels, we use BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), and DistilBERT (Sanh\net al., 2019) separately for each task, i.e., political\nbias and factuality. Fine-tuning is performed with\na1e\u22125learning rate, batch size 16, dropout 0.2,\nover 5epochs on NVIDIA RTX A6000 48GB. We\nmaintain a train/test split of 80/20 for all of our\nexperiments with a fixed seed value.\nWe compare our results with the majority class\nbaseline and zero-shot prompting techniques using\nLLaMA, Mistral, and GPT-3.5. Two scenarios are\ntested: (i)predicting political bias and factuality\nusing only the media name (\u00a74.1), serving as an ab-\nlation study without information retrieval, and (ii)\nadding articles as evidence in the prompt (\u00a74.2), to\ncompare our methodology with traditional article-\nbased profiling.\nClass\u2192 Low Mixed HighAcc.\u2191MAE\u2193\nModel \u2193 Pre. Rec. F1 Pre. Rec. F1 Pre. Rec. F1\nMajority Class Baseline\nMajority class .000 .000 .000 .000 .000 .000 .571 1.000 .727 .571 .572\nZero-Shot Baselines: LLM Prompt Given Name of Media\nGPT-3.5 Turbo .260 .708 .380 .367 .362 .365 .931 .534 .679 .510 .619\nMistral-7B Instruct-v0.1 .181 .233 .204 .288 .678 .404 .750 .187 .300 .335 .753\nLLaMA2-7B Chat .200 .276 .232 .292 .661 .405 .763 .208 .327 .348 .744\nLLaMA3-8B Chat .333 .275 .301 .274 .635 .383 .560 .213 .309 .343 .726\nLLaMA3-70B Chat .473 .792 .592 .352 .471 .403 .839 .555 .668 .565 .471\nZero-Shot Baselines: LLM Prompt Given Articles from Media\nGPT-3.5 Turbo .508 .882 .645 .812 .394 .531 .600 .455 .517 .580 .610\nMistral-7b Instruct-v0.1 .324 .353 .338 .259 .212 .233 .167 .182 .174 .250 1.040\nLLaMA2-7B Chat .333 .353 .343 .303 .303 .303 .258 .242 .250 .300 .940\nLLaMA3-8B Chat .345 .294 .317 .291 .485 .364 .438 .212 .286 .330 .780\nLLaMA3-70B Chat .705 .912 .795 .586 .515 .548 .741 .606 .667 .680 .360\nOur Method (Hand-Crafted Prompts)\nSVM TF-IDF .736 .650 .690 .685 .671 .678 .878 .912 .895 .806 .206\nBERT Base .629 .650 .639 .683 .575 .624 .858 .919 .887 .782 .238\nRoBERTa Base .658 .642 .650 .676 .608 .640 .874 .923 .897 .793 .219\nDistilBERT Base .672 .650 .661 .668 .629 .648 .875 .908 .891 .791 .222\nTable 1: Results for factuality prediction. Bold values\nindicate the best scores for each category.\nEvaluation Measures: We use class-wise F1-\nscore along with overall accuracy. We also report\nMean Absolute Error (MAE) to account for the\nordinal nature of the classes (Baly et al., 2020b,a;\nAzizov et al., 2024).\n5.3 Factuality Prediction\nTable 1 reports the evaluation results for the exper-\niments on our dataset for predicting the factuality\nof the reporting of the news media, grouped by\ndifferent modeling methodologies.\nWe observe that converting our gathered data\nfrom LLMs into embeddings using the TF-IDF\nvectorizer and training an SVM on it yields better\nresults than any other approach in the table. This\nmethod achieves a final accuracy of 80.6% and\nthe lowest MAE score of 0.206, indicating high\nprecision and reliability in predicting the factuality.\nIn contrast, when we fine-tune transformer-based\nmodels using data gathered by prompting LLMs,\nwe find that their accuracies are lower compared to\nthe top-performing SVM model. The likely reason\nfor this is that SVMs, combined with TF-IDF, ef-\nfectively handle sparse, high-dimensional data and\nperform well with smaller datasets.\nIn our zero-shot experimentation, when given the\nmedia name and asked to predict its label, we ob-\nserve that LLaMA2-7B, Mistral-7B, and LLaMA3-\n8B perform poorly. Their high MAE values indi-\ncate frequent misclassification between high and\nlow labels. GPT-3.5 and LLaMA3-70B perform\nsomewhat better, with LLaMA3-70B achieving the\nbest accuracy of 56% in this category, along with\nthe highest recall, suggesting its better knowledge.Class\u2192 Left Center RightAcc.\u2191MAE\u2193\nModel \u2193 Pre. Rec. F1 Pre. Rec. F1 Pre. Rec. F1\nMajority Class Baseline\nMajority class .000 .000 .000 .427 1.000 .598 .000 .000 .000 .427 .573\nZero-Shot Baselines: LLM Prompt Given Name of Media\nGPT-3.5 Turbo .398 .537 .457 .744 .699 .721 .685 .614 .648 .636 .497\nMistral-7B Instruct-v0.1 .882 .750 .811 .824 .945 .880 .927 .843 .883 .869 .152\nLLaMA2-7B Chat .870 .750 .805 .823 .940 .878 .927 .843 .883 .867 .154\nLLaMA3-8B Chat .318 .762 .449 .714 .192 .303 .727 .819 .771 .542 .540\nLLaMA3-70B Chat .855 .738 .792 .877 .934 .905 .878 .873 .875 .874 .168\nZero-Shot Baselines: LLM Prompt Given Articles from Media\nGPT-3.5 Turbo .596 .848 .700 1.000 .647 .786 .742 .697 .719 .730 .420\nMistral-7B Instruct-v0.1 .310 .273 .290 .352 .576 .437 .235 .118 .157 .320 .870\nLLaMA2-7B Chat .421 .485 .451 .391 .545 .456 .500 .235 .320 .420 .730\nLLaMA3-8B Chat .290 .545 .379 .556 .147 .233 .448 .394 .419 .360 .950\nLLaMA3-70B Chat .722 .788 .754 .800 .706 .750 .735 .758 .746 .750 .340\nOur Method (Hand-Crafted+Systematic Prompts)\nSVM TF-IDF .914 .800 .853 .915 .940 .927 .883 .910 .896 .902 .133\nSVM TF-IDF\u20201.000 .850 .919 .859 .962 .907 .942 .886 .913 .911 .093\nBERT Base .859 .762 .808 .887 .902 .894 .884 .916 .899 .881 .147\nBERT Base\u2020.949 .925 .937 .908 .967 .937 .962 .904 .932 .935 .075\nRoBERTa Base .827 .775 .800 .918 .913 .915 .901 .934 .917 .895 .138\nRoBERTa Base\u2020.923 .900 .911 .877 .934 .905 .936 .880 .907 .907 .103\nDistilBERT Base .797 .787 .792 .885 .885 .885 .892 .898 .895 .872 .159\nDistilBERT Base\u2020.912 .912 .912 .862 .923 .892 .928 .855 .890 .895 .114\nTable 2: Results for political bias prediction (3-point\nscale). Each model marked with the \u2020symbol indicates\nthat it is trained on data derived from prompts incorpo-\nrating expert guidelines. Bold values indicate the best\nscores for each category.\nHowever, the overall MAE in this category sug-\ngests that all models struggle to detect the exact\nlabels accurately. This implies that providing the\nmedia name is insufficient to extract accurate in-\nformation from the LLM, highlighting the need for\nmore robust approaches to leverage LLMs effec-\ntively.\nWe observe a similar performance trend when\nwe attach articles from the respective media to the\nprompt and ask the model to predict the factuality\nusing these articles as evidence. As described in\n\u00a7 4.2, hard voting increases accuracy by approxi-\nmately 12% compared to providing only the media\nname, with LLaMA3-70B achieving the best per-\nformance: 68% accuracy. Smaller models such as\nMistral-7B, LLaMA2-7B, and LLaMA3-8B con-\ntinue to struggle, having very high MAE scores,\nwhile GPT-3.5 performs better than these smaller\nmodels. This case study demonstrates that includ-\ning articles in the prompt results in more confident\nand accurate responses from LLMs, as they can\nuse the provided evidence to reason about their fi-\nnal label. However, our methodology, which uses\nhandcrafted prompts, outperforms this technique,\nhighlighting the importance of prompt design in\ndetecting the factuality of the reporting of the news\nmedia.\nClass\u2192 Left Left-Center Center Right-Center RightAcc.\u2191MAE\u2193\nModel \u2193 Pre. Rec. F1 Pre. Rec. F1 Pre. Rec. F1 Pre. Rec. F1 Pre. Rec. F1\nMajority Class Baseline\nMajority class .000 .000 .000 .000 .000 .000 .256 1.000 .407 .000 .000 .000 .000 .000 .000 .256 1.068\nZero-Shot Baselines: LLM Prompt Given Name of Media\nGPT-3.5 Turbo .564 .713 .630 .576 .320 .412 .443 .448 .446 .388 .326 .354 .567 .819 .670 .502 .672\nMistral-7B Instruct-v0.1 .552 .662 .602 .676 .150 .246 .384 .776 .514 .475 .161 .241 .685 .830 .751 .505 .639\nLLaMA2-7B Chat .552 .662 .602 .697 .150 .247 .382 .776 .512 .475 .161 .241 .688 .830 .753 .505 .640\nLLaMA3-8B Chat .127 .674 .214 .308 .080 .127 .444 .094 .155 1.000 .087 .160 .390 .488 .433 .243 1.709\nLLaMA3-70B Chat .411 .725 .525 .595 .307 .405 .415 .798 .546 .692 .050 .093 .729 .782 .754 .510 .727\nZero-Shot Baselines: LLM Prompt Given Articles from Media\nGPT-3.5 Turbo .349 .750 .476 .333 .200 .250 .526 .500 .513 .286 .100 .148 .789 .750 .769 .460 .930\nMistral-7B Instruct-v0.1 .318 .350 .333 .222 .200 .211 .150 .150 .150 .389 .350 .368 .182 .200 .190 .250 1.500\nLLaMA2-7B Chat .100 .100 .100 .227 .250 .238 .278 .250 .263 .200 .150 .171 .200 .250 .222 .200 1.710\nLLaMA3-8B Chat .148 .450 .222 1.000 .150 .261 .800 .200 .320 .500 .200 .286 .348 .400 .372 .280 1.850\nLLaMA3-70B Chat .362 .850 .507 .400 .200 .267 .471 .400 .432 .250 .100 .143 .778 .700 .737 .450 .960\nOur Method (Hand-Crafted Prompts)\nSVM TF-IDF .771 .675 .720 .482 .342 .400 .574 .721 .639 .628 .597 .612 .811 .849 .830 .648 .475\nBERT Base .639 .754 .692 .483 .389 .431 .700 .744 .721 .717 .630 .670 .810 .914 .859 .700 .425\nRoBERTa Base .704 .820 .758 .491 .481 .486 .683 .662 .672 .651 .651 .651 .863 .853 .858 .689 .405\nDistilBERT Base .681 .803 .737 .467 .324 .383 .663 .708 .685 .616 .646 .630 .859 .859 .859 .676 .427\nTable 3: Results for political bias prediction (5-point scale). Bold values indicate the best scores for each category.\n5.4 Political Bias Prediction\nTable 2 shows results for the political bias predic-\ntion task, grouped by modeling methods. Mod-\nels marked with \u2020were trained on data from sys-\ntematic prompts described in \u00a7 3.2. This data im-\nproves accuracy: the SVM trained on vectorized\ndata from systematic prompts outperforms the one\nusing handcrafted prompts. Fine-tuned models also\nbenefit, with BERT achieving the highest accuracy\nof 93.50%, outperforming all baselines. These re-\nsults suggest that incorporating expert definitions\ninto prompts helps elicit more accurate and confi-\ndent predictions from LLMs.\nOur experimental results for predicting political\nbias on a five-point scale can be seen in Table 3.\nWe observe that transformer models perform the\nbest when fine-tuned on the data gathered using\nhandcrafted prompts, surpassing other models, in-\ncluding the majority class baseline and the SVM\ntrained on TF-IDF vectorized text. BERT achieved\nthe highest accuracy at 70%, while using RoBERTa\nyields the best MAE score of 0.405, indicating the\nleast confusion among the ordinal classes. This\nshows that transformer-based models are highly\nefficient in modeling political bias beyond three\nclasses.\nThe first two sections of Table 2 and Table 3\nshow the results for our zero-shot experimentation\nusing LLMs for predicting the political bias on a 3-\nand 5-point scale, respectively.In the 3-point setting, LLaMA3-70B achieves\nthe highest accuracy and recall when only the\nmedia name is used, highlighting its stronger\nknowledge. Interestingly, smaller models such as\nLLaMA2-7B and Mistral-7B achieve a better MAE,\nindicating less confusion between the classes, in\naddition to their subpar accuracy compared to\nLLaMA3-70B. We observe the same trend for po-\nlitical bias in the 5-point setting.\nHowever, when articles from the respective me-\ndia are added to the prompt, the accuracy of the\nmodels for both settings decreases, contrary to the\ntrend observed while predicting the factuality of\nreporting. This suggests that judging the political\nbias of a media outlet based on a single article at a\ntime is a difficult task for LLMs. This highlights the\nneed for an approach that assesses media bias on\nfine-grained topics before determining the overall\npolitical bias, as demonstrated in our methodology,\nwhich yielded better results.\n5.5 Impact of Media Popularity\nWe conducted an error analysis to analyze the re-\nlationship between media popularity and model\nperformance. The objective of this analysis was\ntwofold: (i)to determine whether the labeling per-\nformance of the LLM-based approach correlates\nwith the popularity of the media outlets; and (ii)\nto identify systematic challenges in classifying the\nless popular or newer outlets.\nFigure 2: Best model performance vs. media outlet popularity. (a) Political bias labels, and (b) Factuality labels\nplotted against Alexa Rank (log scale). Each point represents a media outlet with its original label. Green markers\nindicate correct predictions, and red markers indicate errors. A lower Alexa Rank means a more popular medium.\nFigure 3: Correct vs. incorrect predictions for U.S. and\nnon-U.S. media outlets, highlighting higher accuracy\nfor U.S.-based outlets.\nWe used the Alexa Rank feature, which mea-\nsures site popularity, as provided by Panayotov et al.\n(2022). We used the subset of data from our test\nset for which the Alexa Rank metric was available,\nas the Alexa Rank service is no longer live. This\nsubset was analyzed to evaluate the ratio of media\noutlets correctly labeled by our best-performing\nmodels for both political bias and factuality of the\nreporting. We plotted the popularity of each media\noutlet on a logarithmic scale against its correspond-\ning label. A lower Alexa Rank indicates a more\npopular media outlet.\nFigure 2 shows a plot of the media outlets accord-\ning to their original labels. Notably, a red cluster\nappears towards the top of the figure, indicating\nthat the model struggles to predict the correct la-\nbels for less popular media outlets. Conversely, we\nobserve more green clusters towards the bottom,\nwhich indicates that the model performs better at\nlabeling more popular outlets. This pattern sug-\ngests that the model benefits from prior knowledge\nlikely encoded in the LLM for well-known outlets.Ablation: Political Bias Prediction\nData Configuration Acc.\u2191MAE\u2193\nLeaning 0.869 0.144\nReason 0.905 0.106\nLeaning + Reason 0.937 0.075\nTable 4: Ablation study on political bias prediction\nusing different data configurations.\nWe extended our analysis to compare model per-\nformance on U.S. vs. non-U.S. media outlets, ad-\ndressing potential regional bias in the LLM\u2019s train-\ning. Figure 3 shows that the model performs better\non U.S.-based outlets, supporting the assumption\nthat it has greater exposure to U.S. sources during\ntraining.\nOverall, these findings show that LLMs label\npopular and U.S.-based media outlets more accu-\nrately but struggle with less popular, newer, or non-\nU.S. outlets. This highlights the need for improved\nmethods to classify emerging or less popular outlets\nand to mitigate regional bias for better performance\nacross diverse media.\n5.6 Ablation Study\nWe conducted an analysis of the impact of different\ndata configurations on the political bias prediction\ntask using our best-performing model. As shown in\nTable 4, we experimented with three training setups:\n(i)using only the leaning information from GPT\nresponses, resulting in 86.90% accuracy; (ii)using\nonly the reasons from GPT responses, achieving\n90.50% accuracy; and (iii)using both leaning and\nreasoning, yielding 93.50% accuracy.\nThese results highlight that LLM reasoning pro-\nvides critical information and that combining both\nleaning and reasoning leads to better performance,\ndemonstrating the importance of using multiple\ntypes of data for improved accuracy.\n6 Conclusion & Future Work\nWe presented a comprehensive study on detecting\npolitical bias and factuality in news media. We col-\nlected data from LLMs using handcrafted prompts\nand a systematic method integrating professional\nfact-checking criteria. The models trained on these\ndata demonstrated sizeable improvements over the\nexisting approaches. Moreover, our experiments\nalso revealed that without these expert guidelines\nin the prompts, most LLMs struggle to accurately\nclassify the political bias and the factuality of the\nreporting of the news media. This underscores the\ncrucial role of expert guidelines in improving the re-\nliability and accuracy of LLM-based assessments.\nOur methodology achieves 80.60% accuracy and\nan MAE of 0.206 for factuality prediction, while\n93.50% accuracy and an MAE of 0.075 for political\nbias (3-point scale) prediction with 4,192 and 2,142\nlabeled media outlets, respectively. Previous work\nby Baly et al. (2019) achieved the best MAE of\n0.481 for factuality prediction and 1.475 for politi-\ncal bias prediction with 949 labeled media outlets.\nLater work by Baly et al. (2020a) achieved 71.52%\naccuracy for factuality and 85.29% for political bias\nprediction. More recent work by Panayotov et al.\n(2022) using the same dataset, achieved 74.27%\nand 92.08% accuracy for factuality and political\nbias tasks, respectively. Our results compare favor-\nably to these previous works, especially given our\nlarger and more diverse labeled dataset.\nIn future work, we plan to refine factuality as-\nsessment by incorporating expert methodologies di-\nrectly into prompts, expand political bias detection\nbeyond U.S.-centric labels, and jointly predict fac-\ntuality and political bias for a more comprehensive\nassessment. We will explore prompt learning and\noptimization techniques like APO (Pryzant et al.,\n2023), to reduce prompt bias. We will try retrieval\naugmentation and graphical features (related sites)\nfor improved robustness. While cost considera-\ntions limited our exploration of GPT-4 variants and\nfine-tuning approaches, we plan to address these\nin the future. Finally, we aim to use open-source,\ninstruction-tuned models beyond OpenAI\u2019s GPT-\n3.5 for reproducibility and community support.Limitations\nOne limitation of our work is the use of the GPT-\n3.5-turbo-1106 for data curation and interpreta-\ntion, which may be influenced by factors such as\ndata quality and diversity. Additionally, while the\ndataset includes a broad range of news outlets, it\nis primarily sourced from the MBFC, which is a\nU.S.-centric point of view database, and English-\nlanguage outlets. This limits the model\u2019s generaliz-\nability to media from other regions.\nOur methodology involves querying LLMs with\nspecific prompts, which may lead to biased or in-\ncomplete assessments of less familiar media outlets\ndue to biases such as training data bias, cultural\nbias, and confirmation bias. These biases can po-\ntentially skew evaluations by reflecting predomi-\nnant viewpoints in the LLM\u2019s training data, affect-\ning the model\u2019s objectivity. While our approach\nshows promising results, a key limitation arises\nwhen assessing media outlets not encountered dur-\ning training. The model\u2019s generalizability to such\noutlets remains uncertain, highlighting the need for\nfurther investigation and additional strategies to en-\nsure accurate assessments across a broader range\nof sources.\nMoreover, we rely on methodologies that could\nbe further refined by incorporating expert criteria,\nand our political bias detection approach is pre-\ndominantly U.S.-centric. Extending beyond the\nleft/center/right labels to capture a more nuanced\npolitical spectrum could yield more comprehensive\ninsights. Furthermore, we have not fully explored\nthe joint prediction of factual reporting levels and\npolitical bias, which could enhance our overall as-\nsessment. Potential failure modes, such as mis-\nclassification due to ambiguous language, incorrect\nfactuality assessments from an insufficient context,\nand hallucinations from LLM responses, require\nattention to improve the robustness and reliabil-\nity of our methodology. Additionally, integrating\ngraphical features and leveraging retrieval augmen-\ntation to provide external evidence remain areas\nfor improvement. Due to cost constraints, we have\nnot experimented with GPT-4 variants, and while\nprompting has been more practical thus far, future\nwork will consider fine-tuning approaches. Finally,\nwe plan to adopt more open-source instruction-\ntuned models beyond OpenAI\u2019s GPT-3.5 to bolster\nreproducibility and mitigate potential model dis-\ncontinuations. We acknowledge these limitations\nand intend to address them in our future research.\nWe acknowledge hallucinations in LLMs are\ninevitable (Xu et al., 2024), requiring validation\nthrough cross-referencing with external databases\nand expert reviews. Other limitations arise from\nlimited labeled data, potential biases, and chal-\nlenges in capturing nuanced political bias and fac-\ntuality. Further research using diverse human-\ngenerated datasets beyond MBFC, incorporating\nhuman-in-the-loop approaches (Klie et al., 2020),\nand exploring different models is crucial to enhance\nthe robustness and applicability of our findings.\nEthical Statement\nWe made every effort to ensure that the analysis\nand interpretation of the data were conducted im-\npartially and objectively, avoiding any undue bias\nor prejudice. Transparency in the reporting method-\nologies and findings was emphasized to facilitate\nopen dialogue and critical discussion within the\nacademic community and beyond. An important\nethical consideration revolved around the potential\nimpact of the research findings on various stake-\nholders, including media outlets, journalists, and\nthe general public. The findings could influence\npublic trust in the media, highlight systemic bi-\nases, or shape how media outlets approach content\ncreation and labeling. While this line of research\nholds promise in promoting media accountability\nand transparency, it also carries risks, such as un-\nfairly stigmatizing certain outlets or reinforcing\nexisting biases when misinterpreted. Careful com-\nmunication of these results is crucial to prevent mis-\nuse or misrepresentation. To protect the integrity\nand privacy of the sources, any articles from the\nnews media used during the analysis are not to be\nreleased; only the scraping recipes are to be shared.\nBias\nWe recognize that our methodology may be sus-\nceptible to biases, including misclassification from\nambiguous language, errors due to limited context\nfrom summarization, and hallucinations from LLM\nresponses. We also acknowledge potential prompt\nbias influencing model outputs. While we have\ntaken serious measures to mitigate these issues,\nthey persist as challenges. To systematically ad-\ndress them, we plan to explore prompt learning\nand optimization techniques such as APO (Pryzant\net al., 2023). These steps aim to enhance the robust-\nness and reliability of our approach as we improve\nour methodology in future work.References\nDilshod Azizov, Zain Muhammad Mujahid, Hilal\nAlQuabeh, Preslav Nakov, and Shangsong Liang.\n2024. SAFARI: cross-lingual bias and factuality de-\ntection in news media and news articles. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2024, Miami, Florida, USA, November\n12-16, 2024 , pages 12217\u201312231. Association for\nComputational Linguistics.\nDilshod Azizov, Preslav Nakov, and Shangsong Liang.\n2023. Frank at checkthat!-2023: Detecting the politi-\ncal bias of news articles and news media. In Working\nNotes of the Conference and Labs of the Evaluation\nForum (CLEF 2023), Thessaloniki, Greece, Septem-\nber 18th to 21st, 2023 , volume 3497 of CEUR Work-\nshop Proceedings , pages 289\u2013305. CEUR-WS.org.\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEMNLP , Brussels, Belgium. Association for Compu-\ntational Linguistics.\nRamy Baly, Georgi Karadzhov, Jisun An, Haewoon\nKwak, Yoan Dinkov, Ahmed Ali, James R. Glass,\nand Preslav Nakov. 2020a. What was written vs.\nwho read it: News media profiling using text analy-\nsis and social media context. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2020, Online, July 5-10,\n2020 , pages 3364\u20133374. Association for Computa-\ntional Linguistics.\nRamy Baly, Georgi Karadzhov, Abdelrhman Saleh,\nJames Glass, and Preslav Nakov. 2019. Multi-task\nordinal regression for jointly predicting the trustwor-\nthiness and the leading political ideology of news\nmedia. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n2109\u20132116, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRamy Baly, Giovanni Da San Martino, James R. Glass,\nand Preslav Nakov. 2020b. We can detect your bias:\nPredicting the political ideology of news articles. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020 , pages 4982\u2013\n4991. Association for Computational Linguistics.\nAlberto Barr\u00f3n-Cede\u00f1o, Firoj Alam, Tommaso Caselli,\nGiovanni Da San Martino, Tamer Elsayed, An-\ndrea Galassi, Fatima Haouari, Federico Ruggeri, Ju-\nlia Maria Stru\u00df, Rabindra Nath Nandi, et al. 2023.\nThe clef-2023 checkthat! lab: Checkworthiness, sub-\njectivity, political bias, factuality, and authority. In\n45th ECIR 2023, Dublin, Ireland, April 2\u20136, 2023,\nProceedings, Part III . Springer.\nAlberto Barr\u00f3n-Cede\u00f1o, Firoj Alam, Andrea Galassi,\nGiovanni Da San Martino, Preslav Nakov, Tamer El-\nsayed, Dilshod Azizov, Tommaso Caselli, Gullal S.\nCheema, Fatima Haouari, Maram Hasanain, M\u00fccahid\nKutlu, Chengkai Li, Federico Ruggeri, Julia Maria\nStru\u00df, and Wajdi Zaghouani. 2023. Overview of the\nCLEF-2023 checkthat! lab on checkworthiness, sub-\njectivity, political bias, factuality, and authority of\nnews articles and their source. In Experimental IR\nMeets Multilinguality, Multimodality, and Interaction\n- 14th International Conference of the CLEF Associa-\ntion, CLEF , Lecture Notes in Computer Science.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual .\nGiovanni Da San Martino, Firoj Alam, Maram Hasanain,\nRabindra Nath Nandi, Dilshod Azizov, and Preslav\nNakov. 2023. Overview of the CLEF-2023 Check-\nThat! lab task 3 on political bias of news articles\nand news media. In Working Notes of CLEF 2023\u2013\nConference and Labs of the Evaluation Forum , Thes-\nsaloniki, Greece.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nXin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy,\nVan Dang, Wilko Horn, Camillo Lugaresi, Shaohua\nSun, and Wei Zhang. 2016. Knowledge-based trust:\nEstimating the trustworthiness of web sources. IEEE\nData Eng. Bull. , 39(2):106\u2013117.\nMatthew Gentzkow and Jesse M. Shapiro. 2006. Media\nbias and reputation. Journal of Political Economy ,\n114(2):280\u2013316.\nBenjamin D. Horne, William Dron, Sara Khedr, and\nSibel Adali. 2018. Assessing the news landscape: A\nmulti-module toolkit for evaluating the credibility of\nnews. In Companion Proceedings of the The Web\nConference 2018 , WWW \u201918, page 235\u2013238, Repub-\nlic and Canton of Geneva, CHE. International World\nWide Web Conferences Steering Committee.Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7B.arXiv preprint arXiv:2310.06825 .\nJan-Christoph Klie, Richard Eckart de Castilho, and\nIryna Gurevych. 2020. From Zero to Hero: Human-\nIn-The-Loop Entity Linking in Low Resource Do-\nmains. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020 , pages 6982\u20136993.\nAssociation for Computational Linguistics.\nVivek Kulkarni, Junting Ye, Steve Skiena, and\nWilliam Yang Wang. 2018. Multi-view models for\npolitical ideology detection of news articles. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , Brussels, Bel-\ngium. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692 .\nMuhammad Arslan Manzoor, Ruihong Zeng, Dilshod\nAzizov, Preslav Nakov, and Shangsong Liang. 2025.\nMGM: global understanding of audience overlap\ngraphs for predicting the factuality and the bias of\nnews media. In Proceedings of the 2025 Confer-\nence of the Nations of the Americas Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL 2025 - Volume 1:\nLong Papers, Albuquerque, New Mexico, USA, April\n29 - May 4, 2025 , pages 7279\u20137295. Association for\nComputational Linguistics.\nNikhil Mehta and Dan Goldwasser. 2024. An inter-\nactive framework for profiling news media sources.\nInProceedings of the 2024 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(Volume 1: Long Papers), NAACL 2024, Mexico City,\nMexico, June 16-21, 2024 , pages 40\u201358. Association\nfor Computational Linguistics.\nSalman Bin Naeem, Rubina Bhatti, and Aqsa Khan.\n2021. An exploration of how fake news is taking\nover social media and putting public health at risk.\nHealth Information & Libraries Journal , 38(2):143\u2013\n149.\nPreslav Nakov, Jisun An, Haewoon Kwak, Muham-\nmad Arslan Manzoor, Zain Muhammad Mujahid,\nand Husrev Taha Sencar. 2024. A survey on pre-\ndicting the factuality and the bias of news media. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2024 , Bangkok, Thailand. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In Advances in Neural\nInformation Processing Systems 35: Annual Confer-\nence on Neural Information Processing Systems 2022,\nNeurIPS 2022, New Orleans, LA, USA, November 28\n- December 9, 2022 .\nPanayot Panayotov, Utsav Shukla, Husrev Taha Sen-\ncar, Mohamed Nabeel, and Preslav Nakov. 2022.\nGREENER: Graph neural networks for news media\nprofiling. In Proceedings of the 2022 Conference on\nEMNLP , pages 7470\u20137480, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nGordon Pennycook and David G. Rand. 2021. The psy-\nchology of fake news. Trends in Cognitive Sciences ,\n25(5):388\u2013402.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6tgen,\nand Gerhard Weikum. 2016. Credibility assessment\nof textual claims on the web. In Proceedings of the\n295th ACM International Conference on Informa-\ntion & Knowledge Management , CIKM \u201916, page\n2173\u20132178, New York, NY , USA. Association for\nComputing Machinery.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6tgen,\nand Gerhard Weikum. 2018. Credeye: A credibility\nlens for analyzing and explaining misinformation. In\nCompanion of the The Web Conference 2018 on The\nWeb Conference 2018, WWW 2018, Lyon , France,\nApril 23-27, 2018 , pages 155\u2013158. ACM.\nMartin Potthast, Johannes Kiesel, Kevin Reinartz, Janek\nBevendorff, and Benno Stein. 2018. A stylometric\ninquiry into hyperpartisan and fake news. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 231\u2013240, Melbourne, Australia. Association\nfor Computational Linguistics.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\nguang Zhu, and Michael Zeng. 2023. Automatic\nprompt optimization with \u201cgradient descent\u201d and\nbeam search. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2023, Singapore, December 6-10, 2023 ,\npages 7957\u20137968. Association for Computational\nLinguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nChatGPT a general-purpose natural language pro-\ncessing task solver? In Proceedings of the 2023\nConference on EMNLP , Singapore. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training. Technical report,\nOpenAI.Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAhmed Sajwani, Alaa El Setohy, Ali Mekky, Diana Tur-\nmakhan, Lara Hassan, Mohamed El Zeftawy, Omar\nEl Herraoui, Osama Mohammed Afzal, Qisheng\nLiao, Tarek Mahmoud, Zain Muhammad Mujahid,\nMuhammad Umar Salman, Muhammad Arslan Man-\nzoor, Massa Baali, Jakub Piskorski, Nicolas Ste-\nfanovitch, Giovanni Da San Martino, and Preslav\nNakov. 2024. FRAPPE: FRAming, Persuasion, and\nPropaganda Explorer. In Proceedings of the 18th\nConference of the EACL , St. Julians, Malta. Associa-\ntion for Computational Linguistics.\nAbdelrhman Saleh, Ramy Baly, Alberto Barr\u00f3n-Cede\u00f1o,\nGiovanni Da San Martino, Mitra Mohtarami, Preslav\nNakov, and James Glass. 2019. Team QCRI-MIT\nat SemEval-2019 task 4: Propaganda analysis meets\nhyperpartisan news detection. In Proceedings of the\n13th International Workshop on Semantic Evalua-\ntion, Minneapolis, Minnesota, USA. Association for\nComputational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aur\u00e9lien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288 .\nPaul Waldman and James Devitt. 1998. Newspaper\nphotographs and the 1996 presidential election: The\nquestion of bias. Journalism & Mass Communication\nQuarterly , 75(2):302\u2013311.\nYuxia Wang, Revanth Gangi Reddy, Zain Muhammad\nMujahid, Arnav Arora, Aleksandr Rubashevskii, Ji-\nahui Geng, Osama Mohammed Afzal, Liangming\nPan, Nadav Borenstein, Aditya Pillai, Isabelle Au-\ngenstein, Iryna Gurevych, and Preslav Nakov. 2024.\nFactcheck-bench: Fine-grained evaluation bench-\nmark for automatic fact-checkers. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024 , pages 14199\u201314230, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nZiwei Xu, Sanjay Jain, and Mohan S. Kankanhalli.\n2024. Hallucination is inevitable: An innate lim-\nitation of large language models. arXiv preprint\narXiv:2401.11817 .\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-\ntian Han, Qizhang Feng, Haoming Jiang, Shaochen\nZhong, Bing Yin, and Xia Hu. 2024. Harnessing\nthe power of LLMs in practice: A survey on Chat-\nGPT and beyond. ACM Trans. Knowl. Discov. Data ,\n18(6).\nKai-Cheng Yang and Filippo Menczer. 2025. Accuracy\nand political bias of news source credibility ratings\nby large language models. In Proceedings of the 17th\nACM Web Science Conference 2025, Websci 2025,\nNew Brunswick, NJ, USA, May 20-24, 2025 , pages\n127\u2013137. ACM.\nAppendix\nA Handcrafted Prompts\nA.1 Stance on Public Figures or Topics\nA sample response for the prompt used in \u00a7 3.1.1\ncan be seen in Listing 1.\nA.2 Stance on Current Popular Topics\nA sample response for the prompt used in \u00a7 3.1.2\ncan be seen in Listing 2.\nA.3 Determining Factuality of Reporting\nA sample response for the prompt used in \u00a7 3.1.3\ncan be seen in Listing 3.\nB Systematic Prompts\nB.1 Systematic Guidelines\nLeft- and right-winged definitions of the 16 topics\nas described in \u00a7 3.2 can be seen in Listing 4.\nB.2 Response to Systematic Prompt\nA sample response for the prompt used in \u00a7 3.2 can\nbe seen in Listing 5.\nC Zero-Shot Predictions\nC.1 LLM Response Given a Media Name\nA response where LLM was unable to predict the\nfactuality of reporting:\n{\n\"input\": \"vancouvertimes.org\",\n\"output\" : -1\n}\nC.2 Political Bias Prompt Given a Media\nName\nThe following is the prompt used to predict the\npolitical bias of the media given its name:\nsystem_prompt = '''You are an expert in media\nanalysis. Classify the bias of a media given\nits name ONLY from one of three categories\nfrom the list provided below:,\u2192\n,\u2192\n,\u2192\n- left\n- center\n- right\nReturn -1 if you can not classify. '''\nuser_prompt = \" {domain} \"MBFC Dataset\nPolitical Bias Factuality\nLeft 398 Low 597\nLeft-Center 600 Mixed 1200\nCenter 913 High 2395\nRight-Center 907\nRight 831\nTotal 3649 4192\nTable 5: Distribution of labels in our dataset.\nC.3 Summarization\nThe following prompt in Listing 6 is used to sum-\nmarize the articles from the news media as de-\nscribed in \u00a7 4.2, where the {article} placeholder is\nreplaced with the article from the news media.\nC.4 Political Bias Prompt Given Media\nArticles\nThe following is the prompt used to predict the\npolitical bias of the media given its summarized\narticles:\nsystem_prompt = '''You are an expert in media\nanalysis. Classify the bias of the given news\narticle of {media} into one of three\ncategories from the list provided below:,\u2192\n,\u2192\n,\u2192\n- left\n- center\n- right\nReturn -1 if you can not classify. '''\nuser_prompt = \" {article} \"\nD Dataset\nAn example annotation for cnn.com provided by\nMBFC is shown in Figure 4. Detailed dataset statis-\ntics are shown in Table 5\n{\n\"foxnews.com\" : {\n\"Trump\": {\n\"stance\" : \"pro-Trump\",\n\"reason\" : \"Fox News has been known for its conservative-leaning coverage and has been\ngenerally supportive of President Trump 's policies and actions.\" ,\u2192\n},\n\"Biden\": {\n\"stance\" : \"anti-Biden\",\n\"reason\" : \"Fox News is generally known for its conservative and right-leaning viewpoints,\nand it has often been critical of President Biden and his policies.\" ,\u2192\n},\n\"Putin\": {\n\"stance\" : \"Fox News is generally critical of Putin.\",\n\"reason\" : \"Fox News has often reported on Putin 's controversial actions and policies, such\nas his involvement in the conflict in Ukraine and his crackdown on political\nopposition within Russia.\",\u2192\n,\u2192\n},\n\"American\" : {\n\"stance\" : \"Fox News is generally considered to have a conservative bias, and it often\npromotes pro-American viewpoints.\", ,\u2192\n\"reason\" : \"The network 's programming and commentary often align with conservative and\nnationalist perspectives, which are often associated with pro-American sentiments.\" ,\u2192\n},\n\"Russian\" : {\n\"stance\" : \"Fox News is not generally considered to be pro-Russian. It is known for its\nconservative and right-leaning perspective on news and politics.\", ,\u2192\n\"reason\" : \"Fox News has been critical of Russia and its leadership on various occasions,\nparticularly in relation to foreign policy and national security issues.\" ,\u2192\n},\n\"German\" : {\n\"stance\" : \"Neutral\",\n\"reason\" : \"Fox News does not have a specific stance on Germany as a whole. Its coverage of\nGermany may vary depending on the specific news story or issue being discussed.\" ,\u2192\n},\n\"Palestinian\" : {\n\"stance\" : \"Fox News is generally perceived as having a pro-Israel stance, which may lead\nto a more critical view of the Palestinian perspective.\", ,\u2192\n\"reason\" : \"Fox News has been known for its support of Israel and its policies, which may\ninfluence its coverage of the Palestinian issue.\" ,\u2192\n}\n}\n}\nListing 1: Sample response for the prompt used in \u00a7 3.1.1.\n{\n\"foxnews.com\" : {\n\"Ukraine\" : {\n\"stance\" : \"Fox News generally takes a conservative stance on Ukraine, often highlighting\nthe threat posed by Russia and advocating for a strong U.S. response.\", ,\u2192\n\"reason\" : \"Fox News tends to align with conservative viewpoints and policies, which often\ninclude a more hawkish approach to foreign policy and national security.\" ,\u2192\n},\n\"Climate Change\" : {\n\"stance\" : \"Skeptical\",\n\"reason\" : \"Fox News has been known to feature content that questions the scientific\nconsensus on climate change and often provides a platform for climate change sceptics\nand critics.\",\u2192\n,\u2192\n},\n\"Gun Control\" : {\n\"stance\" : \"Fox News generally opposes strict gun control measures, arguing for the\nprotection of Second Amendment rights and advocating for a focus on mental health and\nlaw enforcement efforts to address gun violence.\",,\u2192\n,\u2192\n\"reason\" : \"Fox News often presents arguments in favour of gun rights and against gun\ncontrol legislation, emphasizing the importance of individual freedoms and the\npotential ineffectiveness of stricter gun laws in preventing crime.\",\u2192\n,\u2192\n},\n\"Immigration\" : {\n\"stance\" : \"Fox News generally takes a conservative stance on immigration, advocating for\nstricter border control and enforcement of immigration laws.\", ,\u2192\n\"reason\" : \"Fox News often emphasizes the need for national security and the rule of law,\nand highlights the potential economic and social impacts of immigration.\" ,\u2192\n},\n\"Abortion\" : {\n\"stance\" : \"Against\",\n\"reason\" : \"Fox News generally takes a conservative stance on social issues, including\nabortion, and often features content that aligns with pro-life viewpoints.\" ,\u2192\n}\n}\n}\nListing 2: Sample response for the prompt used in \u00a7 3.1.2.\n{\n\"foxnews.com\" : {\n\"Q1\": {\n\"answer\" : \"No\",\n\"reason\" : \"Fox News has been criticized for biased reporting and spreading misinformation.\"\n},\n\"Q2\": {\n\"answer\" : \"Yes\",\n\"reason\" : \"Fox News has failed fact-checks by multiple fact-checking organizations.\"\n},\n\"Q3\": {\n\"answer\" : \"Right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning political stance.\"\n},\n\"Q4\": {\n\"answer\" : \"Yes\",\n\"reason\" : \"Fox News is known for having a conservative bias in its reporting and editorial\ncontent.\" ,\u2192\n},\n\"Q5\": {\n\"answer\" : \"Conservative, News, Opinion\",\n\"reason\" : \"Fox News is known for its conservative political stance and provides news and\nopinion content.\" ,\u2192\n},\n\"Q6\": {\n\"answer\" : \"Conservative, Right-leaning, Partisan\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning editorial stance, and\nit is often considered to have a partisan bias in its reporting.\" ,\u2192\n}\n}\n}\nListing 3: Sample response for the prompt used in \u00a7 3.1.3.\n{\n\"General Philosophy\" :{\n\"left\": \"Collectivism: Community over the individual. Equality, environmental protection,\nexpanded educational opportunities, social safety nets for those who need them.\", ,\u2192\n\"right\": \"Individualism: Individual over the community. Limited Government with Individual\nfreedom and personal property rights. Competition.\", ,\u2192\n},\n\"Abortion\" :{\n\"left\": \"Legal in most cases.\",\n\"right\": \"Generally illegal with some exceptions.\",\n},\n\"Economic Policy\" :{\n\"left\": \"Income equality; higher tax rates on the wealthy; government spending on social\nprograms and infrastructure; stronger regulations on business. Minimum wages and some\nredistribution of wealth.\",,\u2192\n,\u2192\n\"right\": \"Lower taxes; less regulation on businesses; reduced government spending. The\ngovernment should tax less and spend less. Charity over social safety nets. Wages should\nbe set by the free market.\",,\u2192\n,\u2192\n},\n\"Education Policy\" :{\n\"left\": \"Favor expanded free, public education. Reduced cost or free college.\",\n\"right\": \"Supports homeschooling and private schools. Generally not opposed to public\neducation, but critical of what is taught.\", ,\u2192\n},\n\"Environmental Policy\" :{\n\"left\": \"Regulations to protect the environment. Climate change is human-influenced and\nimmediate action is needed to slow it.\", ,\u2192\n\"right\": \"Considers the economic impact of environmental regulation. Believe the free market\nwill find its own solution to environmental problems, including climate change. Some deny\nclimate change is human-influenced.\",,\u2192\n,\u2192\n},\n\"Gay Rights\" :{\n\"left\": \"Generally support gay marriage; support anti-discrimination laws to protect LGBT\nagainst workplace discrimination.\", ,\u2192\n\"right\": \"Generally opposed to gay marriage; opposed to certain anti-discrimination laws\nbecause they believe such laws conflict with certain religious beliefs and restrict\nfreedom of religion.\",,\u2192\n,\u2192\n},\n\"Gun Rights\" :{\n\"left\": \"Favors laws such as background checks or waiting periods before buying a gun; banning\ncertain high capacity weapons to prevent mass shootings.\", ,\u2192\n\"right\": \"Strong supporters of the Second Amendment (the right to bear arms), believing it\u2019s a\ndeterrent against authoritarian rule and the right to protect oneself. Generally, does\nnot support banning any type of weaponry.\",,\u2192\n,\u2192\n},\n\"Health Care\" :{\n\"left\": \"Most support universal healthcare; strong support of government involvement in\nhealthcare, including Medicare and Medicaid. Generally, support the Affordable Care Act.\nMany believe healthcare is a human right.\",,\u2192\n,\u2192\n\"right\": \"Believe private companies can provide healthcare services more efficiently than\ngovernment-run programs. Oppose the Affordable Care Act. Insurance companies can choose\nwhat to cover and compete with each other. Healthcare is not a right.\",,\u2192\n,\u2192\n},\n{\n\"Immigration\" :{\n\"left\": \"Generally, support a moratorium on deporting or offering a pathway to citizenship to\ncertain undocumented immigrants. e.g., those with no criminal record have lived in the\nU.S. for 5+ years. Less restrictive legal immigration.\",,\u2192\n,\u2192\n\"right\": \"Generally against amnesty for any undocumented immigrants. Oppose a moratorium on\ndeporting certain workers. Funding for stronger enforcement actions at the border\n(security, wall). More restrictive legal immigration.\",,\u2192\n,\u2192\n},\n\"Military\" :{\n\"left\": \"Decreased Spending\",\n\"right\": \"Increased Spending\",\n},\n\"Personal Responsibility\" :{\n\"left\": \"Strong government to provide a structure. Laws are enacted to protect every\nindividual for an equal society. Safety nets for those in need.\", ,\u2192\n\"right\": \"Personal responsibility and it is the government\u2019s role to hold them accountable.\nFair competition over safety nets.\", ,\u2192\n},\n\"Regulation\" :{\n\"left\": \"Government regulations are needed to protect consumers and the environment.\",\n\"right\": \"Government regulations hinder free-market capitalism and job growth.\",\n},\n\"Social Views\" :{\n\"left\": \"Based on community and social responsibility. Gay couples to get equal rights like\neveryone else (e.g. marriage); abortion should be legal; support embryonic stem cell\nresearch. Support restrictions and regulations around the right to bear arms. Supports\ntaxpayer funding of Planned Parenthood.\",,\u2192\n,\u2192\n,\u2192\n\"right\": \"Based on individual rights and justice. Opposed to gay marriage, abortion, and\nembryonic stem cell research. Support the right to bear arms, death penalty, and personal\nresponsibility as an individual. Opposed to taxpayer funding of Planned Parenthood.\",,\u2192\n,\u2192\n},\n\"Taxes\":{\n\"left\": \"Progressive taxation (high-income earners taxed at a higher rate). Generally not\nopposed to raising taxes to fund the government.\", ,\u2192\n\"right\": \"Tend to favor a \u201cflat tax\u201d (same tax rate regardless of income). Generally opposed\nto raising taxes to fund the government.\", ,\u2192\n},\n\"Voter ID\" :{\n\"left\": \"Against voter ID laws citing an undue burden on lower-income groups causing them to\nbe disenfranchised, and that there is virtually no evidence of voter fraud actually\noccurring.\",,\u2192\n,\u2192\n\"right\": \"For voter identification laws to combat alleged voter fraud.\",\n},\n\"Business Rights\" :{\n\"left\": \"Supports unions and worker protections. Raising the minimum wage. Higher corporate\ntaxes.\", ,\u2192\n\"right\": \"Favors business owners and corporations with the expectation higher profits will\nresult in higher wages through a free-market. Generally opposed to a minimum wage. Lower\ncorporate taxes.\",,\u2192\n,\u2192\n}\n}\nListing 4: Left- and right-winged definitions of the 16 topics as described in \u00a7 3.2.\n{\n\"foxnews.com\" : {\n\"General Philosophy\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective on\npolitical and social issues, promoting individualism and limited government.\" ,\u2192\n},\n\"Abortion\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective on social\nissues, including abortion. It generally aligns with the right-leaning stance of\nbeing generally illegal with some exceptions.\",\u2192\n,\u2192\n},\n\"Economic Policy\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective on\neconomic policy, advocating for lower taxes, less regulation on businesses, and\nreduced government spending.\",\u2192\n,\u2192\n},\n\"Education Policy\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective, often\nsupporting homeschooling and private schools while being critical of public education\npolicies.\",\u2192\n,\u2192\n},\n\"Environmental Policy\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective on\nenvironmental policy, often emphasizing the economic impact of regulations and\nexpressing skepticism about human-influenced climate change.\",\u2192\n,\u2192\n},\n\"Gay Rights\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is generally opposed to gay marriage and may be opposed to certain\nanti-discrimination laws due to religious beliefs.\" ,\u2192\n},\n\"Gun Rights\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective on various\nissues, including gun rights. They generally support the Second Amendment and the\nright to bear arms without significant restrictions.\",\u2192\n,\u2192\n},\n\"Health Care\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective on\nhealthcare, often opposing government involvement and supporting private companies in\nproviding healthcare services.\",\u2192\n,\u2192\n},\n\"Immigration\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective on\nimmigration, often advocating for stronger enforcement actions at the border and\nopposing amnesty for undocumented immigrants.\",\u2192\n,\u2192\n},\n\"Military\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective, often\nadvocating for increased military spending and a strong military presence.\" ,\u2192\n},\n{\n\"Personal Responsibility\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective, often\nemphasizing personal responsibility and limited government intervention.\" ,\u2192\n},\n\"Regulation\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective, which often\nopposes government regulations as hindering free-market capitalism and job growth.\" ,\u2192\n},\n\"Social Views\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective on social\nissues, including opposition to gay marriage, abortion, and embryonic stem cell\nresearch, as well as support for the right to bear arms and opposition to taxpayer\nfunding of Planned Parenthood.\",\u2192\n,\u2192\n,\u2192\n},\n\"Taxes\": {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News tends to favor a 'flat tax 'and is generally opposed to raising taxes\nto fund the government.\" ,\u2192\n},\n\"Voter ID\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective, and it\ngenerally supports voter identification laws to combat alleged voter fraud.\" ,\u2192\n},\n\"Business Rights\" : {\n\"leaning\" : \"right\",\n\"reason\" : \"Fox News is known for its conservative and right-leaning perspective, often\nfavoring business owners and corporations with the expectation of higher profits\nresulting in higher wages through a free-market approach.\",\u2192\n,\u2192\n}\n}\n}\nListing 5: A sample response for the prompt used in \u00a7 3.2.\nsystem_prompt = '''Summarize the following news article in 250-300 words. Ensure the summary covers\nthe article 's key points and main details. ''' ,\u2192\nuser_prompt = \" {article} \"\nListing 6: Prompt used to summarize the articles from news media as described in \u00a7 4.2.\nFigure 4: An example of annotation of a news outlet from MBFC. Source: www.mediabiasfactcheck.com.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts", "author": ["ZM Mujahid", "D Azizov", "MT Agro", "P Nakov"], "pub_year": "2025", "venue": "arXiv preprint arXiv \u2026", "abstract": "In an age characterized by the proliferation of mis- and disinformation online, it is critical to  empower readers to understand the content they are reading. Important efforts in this direction"}, "filled": false, "gsrank": 20, "pub_url": "https://arxiv.org/abs/2506.12552", "author_id": ["5fYb6pYAAAAJ", "MUao89cAAAAJ", "FXJzma8AAAAJ", "DfXsKZ4AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:mjMvW_tRYCUJ:scholar.google.com/&output=cite&scirp=19&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D10%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=mjMvW_tRYCUJ&ei=BbWsaMyPOeHUieoP9LKZ6AI&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:mjMvW_tRYCUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2506.12552"}}, {"title": "Automated techniques for identifying fake news and assisting fact checkers", "year": "2019", "pdf_data": "POLITECNICO DI MILANO\nCorso di Laurea Magistrale in Ingegneria Informatica\nDipartimento di Elettronica, Informazione e Bioingegneria\nMaster's Degree in Computer Science and Engineering\nAutomated Techniques for Identifying\nFake News and Assisting Fact Checkers\nSupervisor: Prof. Mark James Carman\nMaster's thesis of:\nStefano Agresti, Matr. 913079\nAnno Accademico 2019-2020\n\nAcknowledgments\nFirst of all, I would like to thank my surpervisor, prof. Mark Carman, for\nsharing his ideas and passion throughout the development of this thesis. Every\ntime, they inspired me to push my work a bit further than I thought I could\nreach. I will miss our Thursday afternoon meetings.\nA huge thanks goes to my parents, who supported me in every plan I had, no\nmatter how crazy it seemed. Without them I wouldn't have achieved half of\nwhat I did.\nI have to thank my sister as well for all the nice time spent together, during\ntrips and at home (also, thank you for all of Mu\u000en's pictures, they would lift\nanybody's mood).\nThanks to Juliana, who could make even a lockdown look enjoyable. These last\nfew months with you have gone by like a bliss. I can't wait to see what the\nfuture holds for us.\nI want to thank my \"Amicanza\" group. You don't see me a lot, but you're\nalways there to hang out when I'm back to Bracciano (and quarantine would've\nbeen a lot more boring without our game nights).\nFinally, I'm going to thank every person I've met in these years. I can't name\neverybody, but all you, from Los Angeles to Moscow, from Paris to Sicily, have\nmade this journey through university much more than just studying.\ni\nAbstract\nOne of the most worrying issues of our age is the spread of online misinformation.\nThis problem is a\u000becting our society heavily, transforming political discussion\ninto a relentless battle between opposing sides. Not only that, the di\u000busion of\nconspiracy theories makes it di\u000ecult for governments to enforce unpopular, yet\nnecessary, legislation, as shown during the ongoing Covid-19 pandemic. It would\nbe naive to put all the blame on Facebook or Twitter, but it's undeniable that\nsocial networks have allowed fake news to prosper as never before. Many studies\nhave been published on how to \fght this phenomenon, oftentimes exploiting new\npowerful tools coming from the \feld of Arti\fcial Intelligence, sometimes showing\npromising results. Yet, they all su\u000bered from the limitations of dealing with such\nan elusive problem by using the classic \\true\" against \\false\" approach.\nIn our thesis, we propose a new taxonomy for online news content that goes\nbeyond this binary division (of fake versus real news), showing the creation pro-\ncess of fastidiouscity , a working prototype capable of categorizing unknown texts\naccording to this new classi\fcation. We further investigate whether it is pos-\nsible to automatically detect and fact-check claims in a given text, a necessary\nstep when discussing the veracity of a document, demonstrating the e\u000ecacy of\nour approach through a crowdsourcing experiment. Moreover, we show a new\nmethodology for creating news datasets by scraping Reddit, setting up another\ncrowdsourcing experiment to validate the quality of this strategy. Finally, we\nperform several experiments on how to enhance the training performances of\nBERT, Google's new language representation model, demonstrating that they\ncan be boosted in a multitask environment, while they're not a\u000bected by the\nuse of a multilingual dataset.\nii\nContents\n1 Introduction 1\n1.1 Current technologies . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Our approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.3 Outline of the thesis . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Related works 5\n2.1 Natural Language Processing . . . . . . . . . . . . . . . . . . . . 5\n2.1.1 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Fake News Detection . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.2.1 Knowledge Based . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2.2 Style Based . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2.3 Propagation Based . . . . . . . . . . . . . . . . . . . . . . 8\n2.2.4 Source Based . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.3 Fake News Datasets . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.4 Fake News Taxonomy . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 Research questions 12\n4 Our approach 15\n4.1 Proposing a new Taxonomy . . . . . . . . . . . . . . . . . . . . . 15\n4.1.1 News . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.1.2 Opinions . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.1.3 Memes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4.2 Proposing a structure for an online content classi\fer . . . . . . . 22\n4.3 Building an online content classi\fer . . . . . . . . . . . . . . . . . 23\niii\n5 Data 24\n5.1 Datasets for the newsworthiness classi\fer . . . . . . . . . . . . . 24\n5.2 Datasets for the professionality classi\fer . . . . . . . . . . . . . . 26\n5.2.1 Low-quality articles . . . . . . . . . . . . . . . . . . . . . 28\n5.2.2 High-quality articles . . . . . . . . . . . . . . . . . . . . . 30\n5.3 Datasets for the automated fact-checking system . . . . . . . . . 32\n5.3.1 Datasets for claim detection . . . . . . . . . . . . . . . . . 32\n5.3.2 Datasets for agreement detection . . . . . . . . . . . . . . 36\n5.4 Datasates for the bias detector . . . . . . . . . . . . . . . . . . . 41\n5.5 Datasets for the political ideology detector . . . . . . . . . . . . . 45\n5.6 Datasets for the multilingual experiment . . . . . . . . . . . . . . 47\n5.7 Datasets for the multitask experiment . . . . . . . . . . . . . . . 51\n5.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n6 Experiments 53\n6.1 Evaluating the quality of a Reddit dataset . . . . . . . . . . . . . 53\n6.2 Building a newsworthiness classi\fer . . . . . . . . . . . . . . . . . 56\n6.3 Building a professionality classi\fer . . . . . . . . . . . . . . . . . 58\n6.3.1 First classi\fer: r/savedyouaclick and r/qualitynews . . . . 59\n6.3.2 Second classi\fer: r/savedyouaclick and r/news . . . . . . 60\n6.3.3 Third classi\fer: r/savedyouaclick and selected publishers 60\n6.3.4 Considerations on the experiment . . . . . . . . . . . . . . 61\n6.4 Building an automated fact-checking system . . . . . . . . . . . . 62\n6.4.1 Claim detection . . . . . . . . . . . . . . . . . . . . . . . . 63\n6.4.2 Coreference resolution . . . . . . . . . . . . . . . . . . . . 66\n6.4.3 Agreement detection . . . . . . . . . . . . . . . . . . . . . 68\n6.5 Building a bias detector . . . . . . . . . . . . . . . . . . . . . . . 70\n6.5.1 Related works . . . . . . . . . . . . . . . . . . . . . . . . . 70\n6.5.2 First classi\fer: Wikipedia dataset . . . . . . . . . . . . . 71\n6.5.3 Second classi\fer: News dataset (Kaggle and\nr/conservative ) . . . . . . . . . . . . . . . . . . . . . . . . 72\n6.5.4 Third classi\fer: News dataset (Kaggle,\nr/conservative , liberal subreddits ) . . . . . . . . . . . . . . 72\niv\n6.5.5 Considerations on the experiments . . . . . . . . . . . . . 73\n6.6 Building a political ideology detector . . . . . . . . . . . . . . . . 74\n6.6.1 First classi\fer: Crowdsourcing dataset . . . . . . . . . . . 75\n6.6.2 Second classi\fer: News dataset (Kaggle and\nr/conservative ) . . . . . . . . . . . . . . . . . . . . . . . . 76\n6.6.3 Third classi\fer: News dataset ( r/conservative and liberal\nsubreddits ) . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n6.7 Exploring BERT's performances on a multilingual dataset . . . . 77\n6.7.1 Related works . . . . . . . . . . . . . . . . . . . . . . . . . 78\n6.7.2 Our experiment . . . . . . . . . . . . . . . . . . . . . . . . 78\n6.7.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n6.8 Exploring BERT's performances in a multi-task setting . . . . . 82\n6.8.1 Our experiment . . . . . . . . . . . . . . . . . . . . . . . . 83\n6.8.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n7 Building a working prototype 86\n8 Conclusions 88\n8.1 Future works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\nBibliography 94\nA Technical details 98\nA.1 Scraping Reddit . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\nA.2 Scraping fact-checking websites . . . . . . . . . . . . . . . . . . . 99\nA.2.1 Creating a list of fact-checking websites . . . . . . . . . . 99\nA.2.2 Creating a list of claims and articles links . . . . . . . . . 102\nA.2.3 Scraping the articles . . . . . . . . . . . . . . . . . . . . . 102\nv\nChapter 1\nIntroduction\nAmong the numerous innovations and revolutions that the last decade has\nbrought, one of the most impactful was, with little doubt, that of social media.\nMostly created around 2004-2005, the number of people using these websites\nskyrocketed in the last ten years, reaching more than 3.5 billion people, roughly\nhalf of the world population1. Unfortunately, it's common knowledge that these\ntools are now causing several negative e\u000bects on society that can be di\u000ecult to\ndeal with and to \fght against, or even to simply monitor. Increased depres-\nsion rates between teenagers, invasive marketing, lack of privacy online are just\nsome of the problems denounced by endless studies and experts (Twenge et al.\n2018, Rosenblum 2007, Zubo\u000b 2019). One of the most disturbing phenomena,\nhowever, is the explosion of misinformation.\nBrought to the attention of politics after the 2016 US presidential elections,\nthe so-called \\fake news\" have been a plague on the Internet since its creation,\nbut have only recently entered the spotlight because of their worrying grip\nover public opinion. While ten years ago conspiracy theories and hoaxes were\nlimited to circles of fanatics, which accounted for an extremely small portion\nof the population, they are now largely widespread, actively a\u000becting political\ndiscourse in most of the western world2.\nA proof of that was given during the Covid-19 pandemic. While governments\nand medical experts tried to control the situation through di\u000ecult measures,\nsuch as lockdowns and forced social distancing, their e\u000borts were undermined\n1https://www.statista.com/statistics/278414/number-of-worldwide-social-network-users\n2https://time.com/5887437/conspiracy-theories-2020-election/\n1\nCHAPTER 1. INTRODUCTION 2\nby people who didn't consider the virus as a serious threat, but rather chose to\nbelieve in secret plots and hidden powers interested in disrupting the economy.\nWhat ten years ago would've made us laugh, today is a common reality, with\nover 70% of Americans who have at least read coronavirus-related conspiracy\ntheories and one quarter of them actually believing in their truthfulness3.\nNot all hope is lost, though. While long-term solutions should be addressed by\nschools and education, we can \fght today against this problem using devices\ncoming from the same world as social networks. Arti\fcial Intelligence (AI) and\ntext classi\fcation technologies have made huge steps forwards in recent years,\ncrashing record after record. Many researchers are now exploring the possibility\nof contrasting the spread of misinformation using big data and machine learn-\ning, while companies like Facebook, Twitter and Youtube have started \ragging\nor demonetizing content that their algorithms recognize as containing wrong\ninformation.\n1.1 Current technologies\nWith billions of posts, tweets and generic content shared on social networks ev-\nery day, it is impossible to target misinformation without using automatic clas-\nsi\fcation techniques. Speci\fcally, we need systems capable of analyzing large\nquantities of text, in order to detect automatically whenever falseful content\nis posted, without requiring any human intervention in the process. Luckily,\ntechnology is evolving fast in this area. The latest innovation in the \feld is\ncalled BERT (Devlin et al. 2019), acronym for Bidirectional Encoder Represen-\ntations from Transformers , a new language representation model pre-trained\nby Google's engineers on a corpus of books of 800M words and on a version\nof English Wikipedia of 2,500M words. BERT's main characteristic is that it\nis designed in such a way that it can be \fne-tuned on speci\fc tasks simply by\nmodifying its \fnal output layer. As a result, BERT obtains state-of-the-art re-\nsults on Natural Language Processing (NLP) tasks it wasn't trained for, even\nwhen faced with small \fne-tuning data.\nIn this thesis, BERT will be the main model we will be using for our experiments.\n3https://www.pewresearch.org/fact-tank/2020/07/24/a-look-at-the-americans-who-\nbelieve-there-is-some-truth-to-the-conspiracy-theory-that-covid-19-was-planned/\nCHAPTER 1. INTRODUCTION 3\n1.2 Our approach\nDespite the exceptional achievements of BERT and the various studies per-\nformed on the subject of fake news, we're still far from an e\u000bective and compre-\nhensive solution on the matter.\nIn many cases, researchers simply focused their attention on comparing \\false\"\nnews against \\real\" information. They analyzed how news articles spread (Zan-\nnettou et al. 2017), how they're written (Zellers et al. 2019) or how users interact\nwith them (Castillo, Mendoza, and Poblete 2013) to determine whether they're\nmore likely to be fabricated or not, sometimes with good results. Yet, most of\nthese studies lacked a more holistic approach, one that takes into consideration\nthe more subtle ways in which misinformation spreads online. An opinion piece\ncan be biased, pointing the reader in a speci\fc direction, without containing any\nfalse data. Citizen reporting might contain low-quality writing and content, but\nmight be true information anyway. A news outlet can be precise in reporting\nnews that harm the opposing political side, but fail to denounce its own.\nThe purpose of this thesis is to propose a new, more complex classi\fcation of\nonline news, showing that it is possible to go beyond the binary distinction\n\\fake\" versus \\real\" and that it is possible to build on top of this new taxonomy\nan automatic classi\fer using the technology available today. We will therefore\npresent fastidiouscity , a working prototype designed for this purpose.\nWe will then discuss how to train a system to detect check-worthy statements\ninside a given text and how to e\u000bectively perform automatic online searches to\n\fnd evidence to con\frm or refute them.\nOther than that, we're going to show how social networks can be mined in order\nto build datasets of news articles to be used in text classi\fcation tasks without\nresorting to crowdsourcing and/or manual labelling to categorize them.\nFinally, we will be performing some experiments to show whether the training\nof BERT models can be positively a\u000bected by the use of a multilingual dataset\nand by the use of a multitask setting.\n1.3 Outline of the thesis\nThe thesis is structured as follows:\n\u2022Chapter 2 shows related works on the subject, presenting the state-of-the-\nCHAPTER 1. INTRODUCTION 4\nart results in the \feld of online misinformation detection\n\u2022Chapter 3 describes the research questions that this thesis is trying to\nanswer\n\u2022Chapter 4 presents our proposed taxonomy of online news, as well as the\nsteps to build an automatic classi\fer on top of it\n\u2022Chapter 5 explains the details of the datasets that were used and the steps\ntaken to create new ones by mining social networks\n\u2022Chapter 6 describes the experiments that were performed to answer our\nresearch questions with their results\n\u2022Chapter 7 presents fastidiouscity , our working prototype\n\u2022Chapter 8 summarizes the entire work, drawing conclusions on our ap-\nproach to the problem\nChapter 2\nRelated works\nWe present here a survey of the di\u000berent techniques and approaches that have\nbeen used in literature to counter the proliferation of online misinformation.\nIn addition to that, we will give a deeper explanation of BERT, Google's new\nlanguage representation model, as it was used extensively throughout the de-\nvelopment of this thesis, together with a generic introduction to the \feld of\nNatural Language Processing.\n2.1 Natural Language Processing\nAccording to the Oxford dictionary, Natural Language Processing is \\ the ap-\nplication of computational techniques to the analysis and synthesis of natural\nlanguage and speech \". In synthesis, we can say that its goal is to make com-\nputers capable of fully understanding and interacting with human language.\nCreated in the 1950s, this \feld has grown parallel to AI and machine learn-\ning, progressing as more and more data became available to researchers with\nthe coming of modern internet. Not only that, new and more powerful models\nhave been produced at impressive rates in recent years, each one surpassing the\nrecords set by the previous ones.\n2.1.1 BERT\nThe latest innovation in the area of NLP was brought by Google in 2018, with\nthe release of BERT, acronym that stands for Bidirectional Encoders Repre-\n5\nCHAPTER 2. RELATED WORKS 6\nsentations from Transformers . BERT's model is based on the Transformers\narchitecture, whose original implementation is described in Vaswani et al. 2018\nand that we will be omitting here for brevity purposes, referring the readers\nto the original paper for a better understanding. The main di\u000berence between\nBERT and other Transformers based models is the task it's been pre-trained\non, which uses a \\Masked Language Model\" objective. This means that ran-\ndom words are masked from the input, with the system having the objective\nof predicting them correctly by only analyzing the remaining part of the sen-\ntence. This task allowed the authors to abandon the left-to-right or right-to-left\nparadigms common in language models (which resembles the way humans read),\nin favor of a bidirectional approach, which uses words both preceding and fol-\nlowing the masked item to help in the prediction. BERT was also pre-trained on\na \\next-sentence prediction\" task, which helps pre-training on text-pairs repre-\nsentations. The authors used two di\u000berent sources to pre-train their model, the\nBookCorpus (800M words) and English Wikipedia (2,500M words). After com-\npleting the pre-training step, the model can be \fne-tuned on any speci\fc task,\nobtaining in many cases good accuracy even with little data available. The \fnal\nresults rewarded Google's approach, with BERT advancing the state-of-the-art\nfor eleven di\u000berent NLP tasks.\n2.2 Fake News Detection\nAs mentioned in the introduction, several studies have been made on how to\ne\u000bectively tackle the \fght against fake news. Here we will be giving a survey of\nthe most recent developments on the subject. For a more in-depth analysis, we\nsuggest Zhou and Zafarani 2018. In general, we can divide fake news detection\ntechniques into four main categories:\n\u2022Knowledge based\n\u2022Style based\n\u2022Propagation based\n\u2022Source based\nCHAPTER 2. RELATED WORKS 7\n2.2.1 Knowledge Based\nKnowledge based techniques exploit one of the oldest weapons against fake news,\nfact-checking. The idea behind it is to simply check whether the information\ncontained inside a story is true or not, without any additional analysis. Manual\nfact-checking is very common, with several websites like Politifact1or Snopes2\nthat have turned it into a business model, and it's generally considered the\nmost reliable way to expose fake news, since it makes use of trusted experts\nwho can evaluate a news article or a politician statement in all of its shades.\nUnfortunately, it's also one of the slowest ways to counter misinformation, which\ninstead spreads fast through the web, often damaging the community before the\nfact-checking process has even begun.\nTo overcome these issues, the concept of automatic fact-checking has been pro-\nposed. A possible way to achieve the automatization of fact-checking is pre-\nsented in Ciampaglia et al. 2015, where authors discuss the possibility of creat-\ning a knowledge graph of information known to be true. The graph will then\ngive higher support to truthful claims with respect to false ones, helping in\ndiscerning between the two. There are, however, several limitations with this\napproach, since a knowledge graph is expensive to create and to maintain, due\nto the constant updates it requires to implement new information.\nA di\u000berent perspective is given in Favano 2019. In that work, the author showed\na proof of concept for a system capable of recognizing claims inside a speech,\nbefore looking online for related articles, judging automatically whether those\narticles support or refute the claims, thus providing explainable proofs as to\nwhy a certain statement should be approved or rejected. Figure 2.1 displays a\nschematic description of this system.\nFigure 2.1: The proof of concept presented in Favano 2019.\n1www.politifact.com\n2www.snopes.com\nCHAPTER 2. RELATED WORKS 8\n2.2.2 Style Based\nAs in knowledge based methods, style based techniques focus on news content\nto determine their truthfulness. Di\u000berent from knowledge based, though, they\nanalyze the way a story is written to make this decision. In recent studies, not\nonly the writing style, but also images and multimedia content have been taken\ninto consideration to obtain a greater accuracy in news classi\fcation, creating\nthe \feld of multi-modal fake news detection . One such example is Y. Wang\net al. 2018. In this paper, the authors showed a new framework that uses a\ncombination of multi-modal features to assess the credibility of a news article.\nThe result is a system that can quickly analyze a story's veracity, even if it's\ntalking about new events that just happened. The main issues with style based\napproaches are their low accuracy on real-world scenarios, as well as the fact\nthat fake news publishers can easily manipulate their writing style in order to\nbypass them. Moreover, these methods are powerless if the person writing is\nconvinced that his/her information is true, no matter what the actual truth\nvalue is (recurring situation with common users on social networks). All in all,\nthis strategy is promising, but still presents serious limitations when used alone.\n2.2.3 Propagation Based\nAs explained in Vosoughi, Roy, and Aral 2018, \" fake news spreads faster, far-\nther, more widely, and is more popular with a higher structure virality score\ncompared to true news \". That is, there are fundamental di\u000berences between\nhow truthful and fake news spread. Starting from this assumption, methods\ncan be conceived to detect online fake content exploiting the way they propa-\ngate on social media. Many papers have followed this strategy, showing in some\ncases interesting results. We point the readers to Ma, Gao, and Wong 2018,\nZhang, Dong, and Yu 2018 and Zhou and Zafarani 2019 for reference.\nThe main drawback of this approach is that it requires the news to spread before\nit is able to make a classi\fcation, allowing fake stories to be shared and read\nby users in the meantime. On the other hand, these systems are more robust\nto manipulation than style based ones and can give important insights on what\nmakes news viral.\nCHAPTER 2. RELATED WORKS 9\n2.2.4 Source Based\nOne more approach that can be employed against fake news is the source based\napproach. In this case, a system is developed such that, given a news story, it\nis capable of assessing the credibility of its publisher and/or the credibility of\nthe users that have shared it online.\nAs shown in Horne, Norregaard, and Adali 2019, news publishers on social media\ncan be grouped according to the stories they choose to release, showing a clear\ndistinction between hyper-partisan websites, conspiracy communities and main-\nstream media. However, labelling news as \\reliable\" or \\unreliable\" depending\non their authors raises serious ethical concerns that should be evaluated when\ncreating a fake news detection system.\nA di\u000berent path is to look at the users who spread these stories. Estimates by\nShao et al. 2018 tell us that 9 to 15 percent of Twitter users are bots, many\nof which created with the sole purpose of in\ruencing online political discussion.\nResearch on how to detect malicious ones has progressed signi\fcantly, with an\nexample being Cai, L. Li, and Zeng 2017. The problem in this case is that,\nfor every malicious bot that's taken down, new ones can be created, possibly\nmore powerful and more di\u000ecult to detect. For this reason, there have been\nsuggestions to focus instead on vulnerable users, users who don't spread fake\nnews maliciously, but tend to believe and share them more than the average user.\nUnfortunately, at the moment of writing no major paper has been published on\nthe topic.\n2.3 Fake News Datasets\nIn the previous paragraphs we showed the state-of-the-art in the \feld of fake\nnews detection. Many of the presented papers rely on deep learning or natural\nlanguage processing to build e\u000bective systems and their results suggest that we\nshould keep working in this direction in the future. But to build better, more\nrobust detectors we don't need only new models, we also need good quality\ndata. We present here two of the most important datasets that were built in\nthis \feld and that serve as a foundation for the datasets built for this thesis.\n\u2022Liar, Liar Pants On Fire , W. Wang 2017. This is considered one of\nthe most important benchmarks when talking about fake news detection.\nObtained by scraping statements fact-checked by Politifact between 2007\nCHAPTER 2. RELATED WORKS 10\nand 2016, it contains more than 12.8K claims labelled on various degrees\nof truthfulness. This dataset contains many additional information for\neach claim, such as speaker's name and history, party a\u000eliation, subject\nof the claim.\n\u2022r/Fakeddit: A New Multimodal Benchmark Dataset For Fine-Grained\nFake News Detection , Nakamura, Levy, and W. Y. Wang 2019. This\ndataset contains more than 1 million samples, 60 percent of which are\naccompanied by images or other types of media, labeled according to a 6-\nway classi\fcation. The dataset was built by scraping Reddit3submissions\non 22 di\u000berent subreddits (asubreddit is a subpage of Reddit dedicated to\na speci\fc theme - for example, in r/news users share news articles from\naround the world). The researchers then assigned a label to the samples\ndepending on the subreddit they were coming from.\n2.4 Fake News Taxonomy\nWhen facing a classi\fcation problem, one of the \frst tasks a data scientist has\nto take care of is \fnding an appropriate set of labels on which to perform the\nactual classi\fcation. This can often be tricky, as a set too small might give\ninconclusive results, while one too big might confuse classi\fcation algorithms\nand lead to over\ftting. Fake news detection is not di\u000berent in this. At the time\nof writing, there isn't a unique classi\fcation researchers and experts agreed on.\nThe majority of fact-checking websites uses variations of the 6-degree truth\nscale from Politifact, which divides news into: \\True\", \\Mostly True\", \\Half\nTrue\", \\Mostly False\", \\False\", \\Pants on Fire\" - with the \frst label indicating\ncompletely true information and the last one indicating a completely made-up\nstory. Most of the papers we've seen so far have used a simpler \\false\"/\"true\"\nclassi\fcation, given the di\u000eculty of tracking these di\u000berent shades of truth in\nan automatic system.\nIn some cases, researchers tried a di\u000berent approach, labelling not the degree\nwith which a news is false, but rather focusing on what makes it false. An exam-\nple is the 6-way labelling system used in the r/Fakeddit dataset by Nakamura,\nLevy, and W. Y. Wang 2019: \\True\", \\Satire/Parody\", \\Misleading Content\",\n\\Manipulated Content\", \\False Connection\", \\Imposter Content\". In this con-\n3www.reddit.com\nCHAPTER 2. RELATED WORKS 11\ntext, fake news is divided according to the reason why it is considered fake -\nImposter Content is considered fabricated because it's written by bots, while\nMisleading Content is considered false because the title doesn't match what's\nwritten inside the article.\nIn Molina et al. 2019 this same strategy is pursued, this time looking not only\nat the di\u000berent types of fake news, but rather considering the di\u000berent types\nof online news content. The \fnal 8-way classi\fcation given in the paper is the\nfollowing: \\Real News\", \\False News\", \\Polarized Content\", \\Satire\", \\Misre-\nporting\", \\Commentary\", \\Persuasive Information\", \\Citizen Journalism\".\nSome researchers proposed to change this approach entirely, ending the one-\ndimension classi\fcation that dominated so far and moving towards a multi-\ndimensional one. In Tandoc, Lim, and Ling 2017, authors suggested to look\nseparately at an article's factuality and at its intention to deceive, hence la-\nbelling di\u000berent online content according to how it performs on these two scales\n- for example satire is low on both, authoritative news is high on factuality and\nlow on intention to deceive, misleading content is high on both.\nIn the end, experts have yet to \fnd an universal labelling for online news con-\ntent that can satisfy both the necessities of classi\fcation algorithms as well as\ncatching all the nuances of truthfulness in news articles. Some even argue that\nthis is not possible at all and that we should rather change the fake news detec-\ntion problem from a classi\fcation task to a regression one, given that, as stated\nin Potthast et al. 2017, \\ hardly any piece of `fake news' is entirely false, and\nhardly any piece of real news is \rawless \".\nChapter 3\nResearch questions\nIn the previous chapters we have introduced the topic of fake news detection\nand listed its main strengths and weaknesses at the time of writing. In this\nwork, we tackle some of the limitations currently encountered in the \feld by\nanswering the following questions:\n1. Is it possible to create an objective classi\fcation of online news that goes\nbeyond the simple \\fake\"/\"real\" division? If so, are automated text clas-\nsi\fcation techniques available today e\u000bective enough to automatically cat-\negorize articles according to this new classi\fcation?\n2. Can we mine social networks like Reddit to build datasets to be used\nin news classi\fcation tasks that are as e\u000bective for training systems like\nBERT as those built through crowdsourcing?\n3. Is it possible to build an automated fact-checking system that, given a\ntext, is able to:\n(a) reliably identify those sentences containing claims,\n(b) automatically convert such sentences into a self-contained format (by\nremoving coreferences, etc) so that they provide for more e\u000bective\nevidence search online, and\n(c) determine whether any related evidence thus found supports or refutes\nthe original claim?\n12\nCHAPTER 3. RESEARCH QUESTIONS 13\nBy answering the \frst question, we want to propose a new taxonomy for on-\nline news content that can be used to ease the work of automated classi\fcation\nsystems without losing the various shades that characterize the world of online\nnews distribution. We will then show that it's possible to use modern technolo-\ngies to label unseen content according to such classi\fcation by describing, step\nby step, how we managed to build a working demo that tackles this speci\fc\nproblem.\nAs for the second point, we mentioned in chapter 2 that one of the main chal-\nlenges with fake news automatic detection is the lack of comprehensive datasets,\nnecessary to train text classi\fcation systems like BERT. To tackle this issue,\nwe will show di\u000berent strategies used during our work to build such datasets on\nour own, before focusing on why scraping Reddit can be considered the most\nversatile, simple and e\u000bective way for building datasets for this purpose. Fi-\nnally, we will be presenting the results from a crowdsourcing experiment that\nwe launched in order to gather evidence in support of our claim.\nFor the last question, we will be describing the creation of an automatic claim\ndetection system, capable of \fnding check-worthy statements in a text, before\nretrieving related evidence online, analyzing it to determine whether it supports\nthe original statement or not. To make the research more precise and e\u000bective,\nwe will introduce one more step designed to automatically turn any sentence\nextracted from a text into a self-contained format, by removing any reference\nexternal to the sentence itself.\nIn addition to these three points, we will be performing further experiments\nto investigate whether BERT's training performances can be improved under\ncertain settings. Speci\fcally, we will try to answer two more questions:\n4. Does the training of a single BERT text classi\fcation model over a multi-\nlingual dataset give better results with respect to the training of di\u000berent\nBERT models each over monolingual portions of the same dataset?\n5. Does the training of a BERT text classi\fcation system obtain better results\nwhen performed in a multi-task setting with respect to the training of the\nsame system in a single-task setting?\nAs we know, the spread of online misinformation is not limited to a single coun-\ntry, being instead widespread around the whole globe. Unfortunately, this goes\nat odds with most of the literature regarding automatic text classi\fcation, which\nCHAPTER 3. RESEARCH QUESTIONS 14\nis almost entirely focused on the English language, due to its predominance in\npublicly available datasets and due to the greater interest it receives from IT\ncompanies. That's why one of the major reasons for the excitement around\nBERT was its ability to be easily \fne-tuned to perform tasks in basically any\nlanguage, even with little data available. What we want to discover in our work\nis: given a multilingual dataset, does a single BERT model \fne-tuned over the\nentire dataset obtain better results than several BERT models, each \fne-tuned\non a monolingual portion of the same dataset? Understanding this is impor-\ntant, since it might lead to di\u000berent approaches when building a system capable\nof detecting fake news in di\u000berent languages, which represents the ultimate,\nlong-term goal of this \feld of study.\nThe \ffth and \fnal question aims at understanding whether a BERT model could\nbene\ft from being \fne-tuned in a multi-task setting with respect to a single-\ntask one. The results of this experiment might be useful for our own work, since\nin many cases tasks overlap when talking about online news classi\fcation - as\nan example, detecting whether an article is biased and detecting its political\nideology are two strictly related problems.\nChapter 4\nOur approach\n4.1 Proposing a new Taxonomy\nAs already mentioned, there isn't a single classi\fcation for online news that\nexperts and researchers agreed on. In section 2.4, we gave an overview of some\nproposed ones, but, as soon as we started collecting data, we realized that most\nof them weren't detailed enough to \ft it properly. The few that did required\nto notice distinctions too subtle for an automated system to detect, or even for\nan accurate dataset to be built. As an example, among the six labels used in\nr/Fakeddit , there are \\misleading content\", \\manipulated content\" and \\false\nconnection\": how should we rate a clickbait article that contains badly reported\nstatistics and exaggerated claims? It can rightly be considered \\manipulated\"\nif the data is fake, but if the data has a base of truth it becomes \\mislead-\ning\", and if it is accompanied by an unrelated image just to enhance views, it\nbecomes \\false connection\". Building a single model capable of following such\na classi\fcation without over\ftting is a challenge even for the most advanced\ntechnologies.\nThat's why we prefer the approach shown in Tandoc, Lim, and Ling 2017. In\nthis paper, authors argue that we shouldn't be using a one-dimensional approach\nto classify news content, proposing a two-dimensional system that analyses sep-\narately factuality and intention to deceive. Such a system is simpler to build, as\nit can employ the best technique for each analysis, dividing the original problem\nof fake news detection into two di\u000berent subproblems easier to tackle.\nOur taxonomy follows this idea, increasing the number of dimensions and ob-\n15\nCHAPTER 4. OUR APPROACH 16\ntaining a classi\fcation that is both accurate and relatively easier to implement\nin an automated classi\fer. We start from a \frst decision level, that, given a\nsocial media post, assigns it a di\u000berent label based on whether it can be consid-\nered newsworthy or not. Thus, we create a \frst distinction between four main\ncategories of online content:\n\u2022news : they are characterized by two major features: they are of public\ninterest (meaning that they are of interest to a large enough number of\npeople - for example the inhabitants of a city or a country) and depict\nthemselves as just reporting information.\n\u2022opinions : here, authors give comments or opinions. In some cases, these\nposts may contain data, but their main focus is to let readers know the\npoint of view of the writer.\n\u2022personal posts : most of the posts that users see on their Facebook or\nTwitter feed belongs to this category. This box can be quite large, as\nit contains a variety of content, from personal updates, to funny stories,\nto simple jokes. The most important thing, however, is that this kind of\ncontent won't change the reader's perspective on the world in any way\nin\ruential to society.\n\u2022memes : together with the category above, this is the most common con-\ntent on the internet. Although the o\u000ecial de\fnition of meme is quite large,\ncomprising any \\ idea, behavior or style . . . that spreads by imitation \"1,\nwe restrict the category to all multimedia content that has been modi\fed\nand manipulated in an evident way before being shared again. The di\u000ber-\nence between a meme and a fake image/video is that the former doesn't\npretend to be truthful and is usually clearly distinguishable from any kind\nof news content. However, given that they're usually published and shared\nfor fun, they have been proven to be an e\u000bective propaganda machine, as\nthey can easily hide political messages behind apparently innocuous jokes.\nThe \frst layer of classi\fcation (also shown in Figure 4.1) allows us to dis-\ncard most social media content, given that all personal content can be ignored.\nTherefore, the following steps will be focusing on the three remaining categories.\n1https://en.wikipedia.org/wiki/Meme\nCHAPTER 4. OUR APPROACH 17\nFigure 4.1: The \frst level of the classi\fcation.\n4.1.1 News\nWe begin by splitting the news category based on the publishing source:\n\u2022large media publishers : these are widespread newspapers that will rarely\npublish information without any basis in reality. In general, they are\ncharacterized by good quality of writing and a large audience, although\nthey can still report wrong information for various reasons.\n\u2022common users : they have generally low following, as well as poor writing\nskills compared to those of professional journalists (although exceptions\nexist). They are mostly untrustworthy when publishing news, unless we\nare dealing with situations of citizen reporting , where common citizens\nreport facts they're witnessing through smartphones and social media.\n\u2022satirical publishers : their purpose is to mock the political establishment\nand they're usually easy to recognize by the average reader.\nWe further divide based on factuality, obtaining the following categorization (we\ndidn't include satirical content which is always non-factual):\n\u2022large media :\n{truthful content : this category includes news articles containing only\nveri\fed information. However, such information can be presented in\na manipulated way, pointing the readers in the wrong direction.\n{false content : news articles containing information that has been\ndisproved by evidence.\n{unveri\fable content : in many cases, newspapers won't release their\nsources, in order to protect them. This can lead to situations where\nCHAPTER 4. OUR APPROACH 18\ntheir information can't be immediately veri\fed, being backed only\nby its publisher reputation.\n\u2022common users :\n{citizen reporting : in these situations, a citizen will report on an event\nhe's witnessing by publishing it on social media.\n{hoax: hoaxes can be in the form of fake citizen reporting, with some-\nbody pretending to witness something that is untrue, or in the form\nof conspiracy theories. In both cases, the information is completely\nfalse. This is one of the most dangerous types of content, since it\noften masks frauds or bots.\nWe proceed by splitting truthful andfalse content based on whether the content\nhas been objectively reported:\n\u2022truthful content :\n{good quality content : this type of news reports only veri\fed informa-\ntion, backed by evidence, in a mostly objective manner. It doesn't\nmake use of loaded words, nor does it omit details to change the\nperception of a story.\n{manipulated content : these stories have been twisted to favor one\nactor over the others or to make the content more appealing to the\nreaders.\n\u2022false content :\n{errors : sometimes, every newspaper can produce wrong information\nwithout any ill-intention behind. The most trustworthy ones will\nissue corrections, although this is not a common practice.\n{fake content : it's rare that a large newspaper knowingly releases a\ncompletely false story, given the repercussions it might face in terms\nof reputation or lawsuits. Therefore, in most cases, this happens only\nwhen there is a strong political motivation behind, such as discreting\na political adversary.\nFinally, we discriminate between the di\u000berent ways a story can be manipulated\nby focusing on the writer's motivation:\nCHAPTER 4. OUR APPROACH 19\n\u2022biased content : if stories have been manipulated because of political moti-\nvations - which can be favoring a politician over another, pushing towards\nabstention or even just making an article more appealing to readers from\na certain political area - we say that they're biased . Such manipulation\ncan take many forms, but is usually realized through omissions, use of\nemotional language or through an excessive emphasis over certain details.\n\u2022clickbait : in this case, the goal is simply to draw more views to a website,\nin order to increase its revenues. These articles are generally harmless\nwith respect to politically motivated ones, as they usually take the form\nof empty stories with catchy headlines, but are nevertheless unethical and\nincrease distrust in newspapers.\nThe overall classi\fcation for news is summarised in Figure 4.2.\nFigure 4.2: Classi\fcation of news in our taxonomy.\n4.1.2 Opinions\nWith respect to news, we decided to adopt a simpler classi\fcation for opinion\npieces, focusing only on their factuality and objectivity.\nCHAPTER 4. OUR APPROACH 20\nChecking a text factuality is a necessary step when dealing with opinions, be-\ncause, although sharing information is not their main purpose, they often make\nclaims, bringing data to support their theses. It's not rare that these claims\nare exaggerated, or even baseless, so fact-checking them is essential to establish\nwhether such theses should be taken seriously or not.\nAnalysing whether a text is biased is crucial as well, since in writing opinion\npieces authors enjoy large discretionality over which stories to focus on, over\nwhich data to show and over which tone to implement (the same article can\nhave very di\u000berent impacts if it's written in an enraged tone rather than a\nneutral one). Thus, this is an important information when trying to distinguish\nwell thought opinions from super\fcial, or even ill-intentioned, ones.\nGiven the above two-step classi\fcation, we obtain the following categories (dis-\nplayed in Figure 4.3):\n\u2022opinions based on wrong information : in this case, the author's theses\nare built on false basis, so readers should approach them with strong\nskepticism, or discard them entirely.\n\u2022biased analysis : with this type of articles or posts, readers should be made\naware that authors likely selected and analyzed the information at their\ndisposal through the lens of their political ideals, thus altering the overall\nquality of their analysis.\n\u2022good quality : here, authors have taken correct information and, using it\nas a basis, provided a complete analysis that was minimally in\ruenced by\ntheir political stances.\nIt's worth noting that an opinion piece can be both biased and contain wrong\ninformation, although the latter is generally a more serious accusation.\n4.1.3 Memes\nThe last category is constituted by memes . This category, mostly overlooked\nin previous works, has grown in importance in recent years, mainly because of\nhow easily they spread through the internet.\nWe only make one distinction, between political and apolitical memes (examples\nfor both are shown in Figures 4.4 and 4.5).\nCHAPTER 4. OUR APPROACH 21\nFigure 4.3: Classi\fcation of opinions in our taxonomy.\nFigure 4.4: An apolitical meme from\nWikipedia.\nFigure 4.5: A meme with a clear polit-\nical message.\nCHAPTER 4. OUR APPROACH 22\nThe latter are harmless and their di\u000busion is mostly inconsequential to peo-\nple's lives. Conversely, political memes can be dangerous. By spreading over-\nsimpli\fed messages, they help sowing distrust and disillusion through the public,\nwhile polarizing the political debate at the same time (an example is given in\nProch\u0013 azka and Blommaert 2019, where authors show how memes have been used\nto popularize content related to the QAnon conspiracy). For these reasons, we\nbelieve it's important to include them in our taxonomy.\n4.2 Proposing a structure for an online content\nclassi\fer\nAfter showing our classi\fcation for online news content, we propose a possible\nstructure for a classi\fer to be built on top of it.\nThis classi\fer is composed of multiple sequential layers, roughly following the\ndivisions mentioned in the previous paragraphs, and represents an ideal system:\n1. Determine content newsworthiness, separating news from opinions and\npersonal posts, as well as isolating memes from other images. Then:\n(a) If it's a meme : determine whether it is political or not\n(b) If it's a personal post: discard it\n(c) If it's news or opinion: continue with the classi\fcation\n2. Analyze the content source (only for news content)\n3. Analyze the content factuality\n4. Analyze if the content is biased\n5. Analyze what was the author's intent (mainly for news content)\nBased on the response to each of these points, we should be able to place any\nonline content inside one of the categories shown above.\nThe advantage with respect to other systems is that the original problem has\nbeen divided into smaller tasks, each addressable in the most appropriate way\nvia a speci\fc classi\fer. Thus, we maintain a complex taxonomy that captures\nall the di\u000berent shades of information sharing, without having to face an overly\ncomplicated technological challenge.\nCHAPTER 4. OUR APPROACH 23\nAs a matter of fact, in the following sections we will be showing, through several\nexperiments, that most of these tasks can already be tackled with the technology\navailable today.\n4.3 Building an online content classi\fer\nStarting from this ideal structure, we realized a working prototype called fas-\ntidiouscity . This system represents a simpli\fed version of the classi\fer just\npresented, to adapt it to the possibilities granted by current technologies.\nIts \fnal composition consists in the following layers:\n\u2022A professionality detector\n\u2022An automated fact-checking system\n\u2022A bias detector\n\u2022A detector to evaluate the political ideology behind a text\nWe decided to keep the focus of our work on texts, rather than images, as\nwe considered the former more interesting from a research point of view, thus\ndropping the classi\fcation over political and apolitical memes .\nWe dropped the newsworthiness detector as well, since in the use case for our\nprototype, which consisted in a web application for reviewing articles provided\nby the users, we assumed this information would be unnecessary (a user wouldn't\nbe interested in using our system on something he/she doesn't \fnd newsworthy\nanyway). However, for completeness, we will still show the creation of such\ndetector, from data collection to model training, leaving open the possibility of\nusing our \fndings for di\u000berent applications.\nWe also decided to simplify the last layer to \ft the data at our disposal, moving\nfrom a more generic analysis of what a writer's intent might be to a more speci\fc\npredictor of what his/her political ideology could be.\nIn the next chapter, we will discuss the datasets used to train the classi\fers,\ndescribing the experiments performed on them before their deployment.\nChapter 5\nData\nIn this chapter, we will show the datasets used in the creation of fastidiouscity\nand in the various experiments that we conducted. To make the presentation\neasier to follow, we decided to group them based on which purpose they were\nrequired for. For each of them, we prepared a description of its source and data\nand, for those we created by ourselves, we integrated such descriptions with an\noverview of their creation process. In the last paragraph, a brief summary is\ngiven for reference.\n5.1 Datasets for the newsworthiness classi\fer\nIn the \frst layer of our ideal classi\fer, the objective was to separate uninteresting\ninformation (like personal updates) from newsworthy content. In Spangher,\nPeng, and Ferrara 2019, \\newsworthiness\" is de\fned by \\ how likely [a] piece of\ninformation [is] to appear on the front page of a major newspaper \". Starting\nfrom this de\fnition, we created a three way classi\fcation for online texts made\nof news, opinions and uninteresting content. We explained them in detail in\nsection 4.1.\nTo collect data from all three categories, we resorted to three di\u000berent sources.\nThe \frst one, used to create a dataset of news articles, was Reddit. Taking\ninspiration from Nakamura, Levy, and W. Y. Wang 2019, we exploited the\ncharacteristic of this social network of creating mono-thematic communities to\n24\nCHAPTER 5. DATA 25\nour advantage, \fnding r/news1, a community followed by more than 22 million\nusers dedicated to sharing newspaper articles. Using the Pushshift API2, we\nobtained 30,000 links published on the subreddit pointing to online news articles,\nwhich translated into 17,948 entries for our dataset (some articles were lost\nduring the scraping process). Of these, we removed the ones characterized by\nan excessively low number of characters or words (threshold at 100 characters\nand 20 words), reducing them to 17,782 samples. In Figure 5.1, we show the\ndistribution of the articles' lengths, which appear to assume an almost normal\ndistribution, as expected.\nFigure 5.1: The distribution of r/news articles' length and word count, capped\nat 5,000 characters and 2,000 words to make the graphs easier to interpret.\nThe second source was another subreddit , called r/InTheNews3. As speci\fed\nin its description, this community is \\ for opinion, analysis, and discussion of\nrecent events \", which \ft with our second category. From there, we were able\nto obtain 26,037 links that allowed us to successfully scrape 15,816 articles. As\nshown in Figure 5.2, in this case as well the distribution of the articles' lengths\ndidn't reveal any particular pattern, being close to a normal one.\nTo create a collection of uninteresting content, we used instead a corpus of\nblog texts available on Kaggle4, retrieved from blogger.com and covering a wide\nvariety of topics. To avoid any overlapping with the other categories, we removed\nall posts related to politics or society, which could be labelled incorrectly as\nopinions or news. The \fnal dataset was considerably larger than the previous\nones, with over 630,000 rows, so, to avoid excessively skewing the \fnal model,\n1https://www.reddit.com/r/news/\n2https://pushshift.io/\n3https://www.reddit.com/r/inthenews/\n4https://www.kaggle.com/rtatman/blog-authorship-corpus\nCHAPTER 5. DATA 26\nFigure 5.2: The distribution of r/InTheNews articles' length and word count,\ncapped at 5,000 characters and 2,000 words to make the graphs easier to inter-\npret.\nwe decided to sample 20,000 of its entries. It's interesting to notice that in this\ncase, di\u000berently from before, the texts showed a di\u000berent distribution in terms\nof their lengths, with short posts making up the majority of the dataset.\nFigure 5.3: The distribution of the blog posts' length and word count, capped\nat 5,000 characters and 2,000 words to make the graphs easier to interpret.\nIn conclusion, before moving on, we show in Figure 5.4 an interesting comparison\nbetween the 30 most used words in the three datasets just presented. The\ndi\u000berences are evident, with r/news and r/InTheNews dominated by political\nreferences, against the more common words found in the blog corpus.\n5.2 Datasets for the professionality classi\fer\nIn our system, the main purpose of this layer was to discriminate between\nwell written texts and poorly written ones, with the latter being usually less\ntrustworthy.\nCHAPTER 5. DATA 27\nFigure 5.4: The most common words found in r/news (top left), r/InTheNews\n(top right) and in the blog corpus (bottom picture).\nThe only important work we found on the topic was by deepnews.ai5, private\ncompany whose aim is to retrieve high-quality articles from all over the internet,\ndelivering them to its users. As explained by the founder, their approach was\nto collect a large number of news articles, dividing them according to their\npublishers, before asking journalism students to review the classi\fcation thus\nobtained. This approach makes a very strong assumption, as it assumes that\nall articles coming from the same publisher are either well or poorly written. In\nthis case, the problem was mitigated by implementing crowdsourcing to improve\nthe overall quality of the data.\nUnfortunately, applying this same strategy was not feasible for us, given the\nlimited resources available for this thesis. Therefore, we decided to pursue\ndi\u000berent paths.\n5www.deepnews.ai\nCHAPTER 5. DATA 28\n5.2.1 Low-quality articles\nAs in section 5.1, we decided to exploit Reddit for this experiment as well.\nSpeci\fcally, we were able to \fnd r/savedyouaclick6, a Reddit community with\nalmost 1.5 million subscribers whose theme is precisely sharing clickbait and\nlow quality articles, making it an optimal source for our purposes. From here,\nwe were able to obtain links to more than 30,000 articles of this type. Of these,\nwe were able to scrape 11,688.\nIn table 5.1, we show the 5 most recurring publishers among them, with the\nmain one being web.archive.org , a website that archives web pages from vari-\nous websites. The remaining publishers contributed to a lesser extent, although\nit's interesting to see almost 300 articles coming from two prominent news out-\nlets such as Business Insider and CNN . However, looking at entries from the\ndataset, it's evident that some questionable journalistic practices are common\neven among famous newspapers (in Figure 5.5, we show a clear example of\nclickbait in one of the articles from CNN collected in the dataset).\nPublisher Number of articles\nweb.archive.org 4,370\nexpress.co.uk 166\nbusinessinsider.com 135\ncnn.com 131\ngoogle.com 102\nTable 5.1: The \fve most common publishers among the low-quality articles.\nLooking at the article's lengths, their average is at 3,503 characters, or 581\nwords (roughly double the length of this paragraph so far). There are some\nnotable exceptions, with some of them having only a few words, or having tens\nof thousands. Looking closer, there are 220 articles less than 100 characters\nlong and 675 more than 10,000 characters long. We manually checked some\nof them, discovering that, for the former ones, the issue was caused by the\nscraping process, which sometimes retrieved only an article's title, instead of its\nentire text, while the latter simply appeared to be very lengthy, not showing\nany particular problem. In a few cases, we discovered that the text had been\nreplaced with anti-robot checks (one example being \\ JavaScript is disabled. You\nneed to enable JavaScript to use SoundCloud \"). In the end, during our analysis,\n6https://www.reddit.com/r/savedyouaclick\nCHAPTER 5. DATA 29\nFigure 5.5: This article, published on the 22nd July 2020, reiterates something\nthat was already known by the majority of people at that time: washing hands,\nwearing masks and social distancing help against the Covid-19 pandemic. Yet,\nthe study mentioned in the title gives a much more complex answer to the mat-\nter, even specifying that instructing the population to take these three simple\nsteps could only \\ mitigate and delay the epidemic \", without ever stating that\nthey would be enough to stop it on their own.\nwe noticed that, even having only an article's title, it was easy to recognize low-\nquality content (here's an example: \\ 7 secrets everyone needs to know about\n\fnancial advisors \"), so we decided to keep all of the samples regardless of their\nsize.\nFigure 5.6: The distribution of length and word count for low-quality articles,\ncapped at 5,000 characters and 2,000 words to make the graphs easier to inter-\npret.\nCHAPTER 5. DATA 30\n5.2.2 High-quality articles\nFollowing the idea above, we searched for Reddit communities dedicated to shar-\ning high-quality news, \fnding r/qualitynews7. Out of the 13,394 urls retrieved\nfrom the subreddit , we were able to scrape 11,695 news articles to be used as\nexamples of high-quality journalism (in table 5.2 the \fve most common publish-\ners among them). Double-checking with MediaBiasFactCheck8(one of the most\nauthoritative sources when analyzing a newspaper ideology and reliability), we\nwere reassured by the fact that all \fve of them had high ratings on the website.\nPublisher Number of articles\nreuters,com 2,344\nbbc.com 2,181\nnpr.org 1,106\ntheguardian.com 898\naljazeera.com 848\nTable 5.2: The 5 most common publishers on r/qualitynews .\nAfter noticing a small number of entries with low word count, we decided to\nperform a manual inspection, removing those that we found out to be paywalls\ntexts, rather than real articles. The number of removed rows was, however, not\nsigni\fcant, being in the order of a few dozens.\nIt's interesting to look at the resulting distribution of the articles' lengths. As\nshown in \fgure 5.7, there is still a spike close to zero due to the many sam-\nples coming from press agencies, like Reuters , whose format consists in short\nsentences reporting one key fact, without any added comment or analysis.\nGiven the low number of subscribers of r/qualitynews (only 12,947 at the time\nof writing), we decided to employ again the dataset presented in section 5.1\nbuilt from the larger r/news (which counted more than 22 million followers).\nIt's worth pointing out that the news shared on this subreddit has a tendency\nto be more international, as can be observed from the names of its most shared\npublishers, reported in table 5.3. MediaBiasFactCheck didn't hold information\non any them, presumably due to the website focus on the United States, so the\nassurance over the content quality was only given by the size of the audience\npopulating the community.\n7https://www.reddit.com/r/qualitynews/\n8https://mediabiasfactcheck.com/\nCHAPTER 5. DATA 31\nFigure 5.7: The distribution of r/qualitynews articles' length and word count,\ncapped at 5,000 characters and 2,000 words to make the graphs easier to inter-\npret.\nPublisher Number of articles\npopularnews.in 2,256\ncorealpha.org 1,315\nen.neroonews.com 1,144\nnewspotng.com 1,052\ntechfans.co.uk 922\nTable 5.3: The \fve most common publishers on r/news .\nFinally, we decided to test a di\u000berent strategy, creating a third dataset by\ncollecting news articles from seven speci\fc newspapers renowned for the quality\nof their articles and in-depth analysis: The Atlantic ,Foreign A\u000bairs ,Politico ,\nThe New Yorker ,The Economist ,The Wall Street Journal and BBC .\nWe retrieved links to 5,000 articles for each of them using an automated search\nacross all Reddit posts, before proceeding with their scraping. In the end, we\nobtained 15,437 samples - a much lower number than the expected 35,000 caused\nby the presence of a large number of duplicate urls.\nThe samples were further diminished by checking their length and word count,\nonce more putting a threshold at 100 characters and 20 words. Nevertheless,\nlooking at their distribution, a spike was still visible towards the left, because of\nmore than 1,000 articles having less than 50 words. Reading some of them, we\npresumed that this was caused by having scraped only their title or summary,\nbut, as with low quality articles, we deemed those su\u000ecient for our purposes,\nso we kept all the rows.\nCHAPTER 5. DATA 32\nFigure 5.8: The distribution of length and word count for articles coming from\nour selected newspapers, capped at 5,000 characters and 2,000 words to make\nthe graphs easier to interpret.\n5.3 Datasets for the automated fact-checking sys-\ntem\nOne of the most crucial tasks we had to tackle was establishing the factuality\nof a text. In order to complete it, we built an automated fact-checking system\nwhose structure can be summed up as follows:\n1. Given a text, detect which sentences should be fact-checked\n2. For each of these sentences, search online for related evidence\n3. For each retrieved document, determine whether it supports or refutes the\nrelated sentence\nWe will be talking more about the second point in the following chapter. As for\nthe others, we describe in the following two paragraphs the datasets used for\nboth them.\n5.3.1 Datasets for claim detection\nA similar problem was studied in Favano and Carman 2019. In that paper, the\nauthors employed two di\u000berent datasets: one made of manually labeled sen-\ntences coming from 19 di\u000berent political debates, from Atanasova et al. 2019,\nand one, proposed by the authors, composed of a million newspaper headlines\nand random sentences from Wikipedia (the \frst to act as check-worthy sen-\ntences, the remaining to be used as negative examples). However, both of them\nCHAPTER 5. DATA 33\nSentence Claim label\nSo we're losing our good jobs, so many of them 0\nWhen you look at what's happening in Mexico, a friend of\nmine who builds plants said it's the eighth wonder of the\nworld0\nThey're building some of the biggest plants anywhere in\nthe world, some of the most sophisticated, some of the best\nplants0\nWith the United States, as he said, not so much 0\nSo Ford is leaving 1\nYou see that, their small car division leaving 1\nThousands of jobs leaving Michigan, leaving Ohio 1\nThey're all leaving 0\nTable 5.4: An extract from Atanasova et al. 2019; reading the sentences, it's\ndebatable that some of them, especially the \frst one, are not classi\fed as claim.\nsu\u000bered from several limitations, which resulted in poor performances when\nmodels were trained or tested on them.\nThe dataset from Atanasova et al. 2019 used as discriminator betweeen claims\nand non-claims whether factcheck.org , a fact-checking organization, had made\nremarks on a sentence or not. We argue that this approach is limiting for various\nreasons. Firstly, fact-checking organizations are more likely to fact-check claims\nif they appear to be false, or at least dubious, while they're less likely to do so\nif they appear to be truthful - to back this statement, we point to Figures 5.10\nand 5.19, containing the number of truthful and false claims fact-checked by\nPolitifact and various other publishers over the course of 10+ years. Moreover,\ninside a speech or a debate, primary sources for this dataset, whenever two or\nmore claims are too similar to each other they will only be fact-checked in one\ncase, leaving other sentences as erroneous negative examples (this same problem\nwas brought up in the original paper as well). Another issue is that often fact-\ncheckers prefer to focus on claims that are more speci\fc, as those can be more\neasily con\frmed or refuted by evidence, overlooking those that are more open\nto interpretation. Looking at a few samples from the dataset, reported in table\n5.4, it's possible to notice how these issues introduce an important amount of\nnoise, which severely limits the quality of any model trained on this data.\nThe other dataset, built from Wikipedia and newspapers headlines, su\u000bers from\na signi\fcant amount of noise as well. On inspection, several articles' titles can\nhardly be considered claims, and vice versa. In addition to that, many of them\nCHAPTER 5. DATA 34\npresent inconsistencies or grammar mistakes, perhaps due to how they were\ncollected.\nThat's why we decided to introduce a new dataset that could limit the amount\nof noise, while maintaining a clear division between claims and non-claims. Its\nbuilding process was the following:\n\u2022check-worthy sentences were scraped from Politifact, collecting all the\nclaims that have been fact-checked on the website in the past 10+ years\n\u2022As for the negative examples, our idea was to use sentences taken from nor-\nmal conversations. For this purpose, we found the Cornell Movie Dialogs\nCorpus9, a dataset of more than 300,000 lines pronounced by characters\nin more than 600 movies\nThe \frst dataset contained 17,580 claims, obtained through the Politifact API10.\nDuring exploratory analysis, they didn't show any particular pattern, with aver-\nage word count being 18 words (slightly more than the average English sentence)\nand an approximately normal distribution. Both are positive indicators, since\nthey suggest that there is small noise in the data.\nFigure 5.9: The distribution of length and word count for Politifact claims.\nAs a side note, we explored the rest of the information contained in the dataset,\nreporting in Figure 5.10 the distribution of the ratings given by fact-checkers to\neach claim and in Figure 5.11 the number of claims pronounced by the 10 most\nfact-checked persons (or companies) on Politifact.\nThe other dataset is made of 304,713 utterances involving 9,035 characters from\n617 di\u000berent movies. To balance the ratio between positive and negative exam-\nples, we reduced the latter in four di\u000berent ways:\n9http://www.cs.cornell.edu/cristian/Cornell Movie-Dialogs Corpus.html\n10https://www.politifact.com/api/factchecks/\nCHAPTER 5. DATA 35\nFigure 5.10: The distribution of ratings on the Politifact dataset.\n1. We removed sentences that were excessively long (more than 500 charac-\nters)\n2. We removed sentences coming from low-rated movies (less than a 7.1 score)\n3. We removed sentences coming from fantasy, historic or sci-\f movies (to\navoid introducing any bias)\n4. we randomly sampled among the remaining entries\nThe \fnal result was a dataset of 26,710 rows. Analyzing them, we noticed\na skewness towards the left in their length distribution, probably due to the\nlarge number of one-word sentences (like \\Yes\" or \\No\") common in normal\nconversations.\nIn Figure 5.13, a comparison between the most used words in the two datasets is\nshown. The di\u000berence in the vocabulary is clearly visible, with Politifact using\nnumerous politically-related terms, against the more common ones used in the\nmovie dataset. Two of the most recurring words used in claims are \\ says\" and\nCHAPTER 5. DATA 36\nFigure 5.11: The ten most common claimants in the Politifact dataset. The size\nof a circle is proportional to the number of fact-checked claims.\nFigure 5.12: The distribution of length and word count for the lines from movies.\n\\said\", which is not too surprising given that, in numerous cases, claims take a\nform similar to \\ He said that \".\n5.3.2 Datasets for agreement detection\nFor this task, we decided to resume the idea of scraping Politifact, extending\nit with multiple websites coming from around the world, using fact-checking\narticles with their fact-checked claim as examples of agreement and disagreement\nCHAPTER 5. DATA 37\nFigure 5.13: On the left, the most common words between all the claims; on\nthe right, the most common ones between the movie lines.\n(an article classifying a claim as false would be in disagreement with that claim,\nand vice versa).\nTo create the dataset, we used the following strategy (more details on the im-\nplementation are available in the appendix):\n\u2022Query the names of di\u000berent politicians from several countries on the\nGoogle FactCheck API11to gather a list of fact-checking websites\n\u2022Query again the API, this time using the list of websites obtained in the\nprevious step, retrieving a list of claims and urls pointing to fact-checking\narticles from those websites.\n\u2022Scrape the articles thus found\nOn top of this, we integrated the data with articles extracted directly from\nPolitifact through its own API. In the end, we built a dataset of 52,877 fact-\nchecking articles, divided into 23 unique languages (though only 10 of them\ncounting more than 50 samples) and 21 unique publishers (of which Politifact\nmaintained the largest share, with more than 15 thousands entries). Each article\nwas accompanied by the fact-checked claim.\nAfter analysing the lengths of claims and related articles, we decided to remove\nall rows with claim length of more than 400 characters (removing roughly 0.42%\nof the total). We then did the same with articles less than 200 characters\nlong (deleting only 0.02% of all rows). The \fnal results appeared promising,\n11https://toolbox.google.com/factcheck/explorer\nCHAPTER 5. DATA 38\nFigure 5.14: The distribution of the fact-checking articles over the ten main\nlanguages.\nFigure 5.15: The distribution of the fact-checking articles among the various\npublishers.\nCHAPTER 5. DATA 39\nwith the word count from the claims showing a normal distribution with mean\nat around 15 words (the average phrase length) and with the article bodies\nshowing a slightly skewed normal distribution centered at around 550 words\n(approximately equivalent to a couple pages of this thesis).\nFigure 5.16: The distribution of the word count among claims and articles.\nIn Figure 5.17, we show another interesting information: the distribution over\nthe past \fve years of the claims from the top \fve languages. As we can see,\nEnglish claims constantly increased in number between 2017-2020, probably due\nto an increase in the activities of fact-checking publishers, or maybe correlated to\nthe increasingly in\ramed political landscape in the United States. The pattern\nof Portuguese claims is intriguing as well. From being almost irrelevant in\n2016-2017, they spiked in 2018, even surpassing English ones, and contending\n\frst place with them in 2019, before somewhat decreasing in 2020. Comparing\nthis trend to the evolution of Brazilian politics (where most Portuguese articles\ncome from), it's reasonable to assume that it was related to the presidential\nelections held in the country at the end of 2018, which led to the election of\ncontroversial president Jair Bolsonaro, who entered in o\u000ece precisely on January\n1st, 2019. Most of the other languages tended to be irrelevant before 2019,\nwhich likely depends on how the Google FactCheck API retrieves information\nfrom newspapers, rather than external situations. Notable exceptions are the\nclaims in Italian, always present from 2016 to 2020, with a peak in 2018, year\nof the last Parliamentary elections (Figure 5.18).\nIn Figure 5.19, it's possible to observe that, similar to the dataset from Poli-\ntifact, the samples are characterized by a signi\fcant unbalance towards false\nclaims (which, in our case, corresponded to disagreement examples). This ap-\npears to further reinforce our speculation in section 5.3.1 about fact-checking\norganization prioritizing suspicious claims over truthful ones.\nCHAPTER 5. DATA 40\nFigure 5.17: The distribution of the fact-checking articles over the \fve main\nlanguages during the years 2015-2020.\nFigure 5.18: Trend of the number of claims for Italian publishers in the years\n2015-2020.\nCHAPTER 5. DATA 41\nFigure 5.19: The distribution of supporting and refuting fact-checking articles.\nLastly, we show the most used words in each language. It's worth noticing\nhow words like \\ covid \" and \\ coronavirus \" are among the most used in almost\nevery single language. Considering that this dataset was built in September\n2020, containing articles up to 15 years old, it gives a very clear idea of how\nintensely the political debate all over the world was in\ruenced by the pandemic.\nIn addition to that, more similarities can be observed between di\u000berent idioms,\nsuch as the frequency of the term \\ police \", the numerous references to photos\nor videos (due to many hoaxes being in the form of manipulated multimedia\ncontent) and the common mentions of political \fgures - signs that fake news\nhave similar themes and similar ways of spreading even in di\u000berent countries.\n5.4 Datasates for the bias detector\nThe purpose of the bias detector was to establish whether a journalist is report-\ning the information objectively inside the news he/she is writing. To build it,\nwe collected three di\u000berent datasets, training a text classi\fer on each of them\nand comparing their performances.\nCHAPTER 5. DATA 42\nFigure 5.20: From the top, left to right, the most common words in our dataset\nof fact-checking articles in: English, Portuguese, French, Spanish, Italian.\nThe \frst dataset was presented in Pryzant et al. 2020 and comprises 181,474\nsentences taken from Wikipedia that didn't respect its neutral point of view\npolicy. Each of the sentences is accompanied by an edited version - an example\nCHAPTER 5. DATA 43\nis \\John McCain exposed as an unprincipled politician\", modi\fed in \\John\nMcCain described as an unprincipled politician\". To make the data easier to\ndiscriminate, we picked the original sentence in half of the cases and the modi\fed\nversion in the other half. We labeled the former as \\ biased \" and the latter as\n\\unbiased \". The data didn't show any notable pattern, with the vast majority of\nsentences being less than 40 words long (amounting to 2-3 phrases on average).\nWe point the reader to the original paper for a deeper review of the dataset\nThe second dataset was built starting from the All the news dataset on Kag-\ngle12, which contains more than 2.7 million news articles, each with its own\npublisher and author. To rate them as \\ biased \" or \\ unbiased \", we used their\npublisher ratings on MediaBiasFactCheck. Unfortunately, we noticed that the\ndataset was unbalanced, since only one of its sources could be considered \\ right-\nleaning \", while the others were either judged as \\ left-leaning \" or \\ neutral \". To\ncompensate, we retrieved 192,100 submissions from r/Conservative13, scraping\n52,699 articles shared on the subreddit that we labeled as \\ right-leaning \", and,\nas a consequence, \\ biased \". After that, we merged them with an equal amount\nof left wing and unbiased articles from All the news to conclude the work.\nIn Figure 5.21 and 5.22, we show the di\u000berences in length between articles\nlabeled as \" biased \" and \" unbiased \", as well as the most common words in both\ngroups. A small dissimilarity in their tones can be noticed, with neutral articles\nusing more economic or political terms. Interestingly, in this category, \\ trump \"\nisn't shown among the 30 most recurring vocables, while in biased articles it's\nthe second most used. This might be caused by the fact that news connected to\nUS President Donald Trump tend to generate more views and interest, which\nis usually the main goal of most newspapers, especially of those showing a\nstrong political bias. On the opposite side, neutral publishers are mostly press\nagencies, whose business model is less reliant on readers' views and more focused\non delivering fresh and timely information to companies around the world, thus\nexplaining why they have a lower coverage over Trump's administration.\nAfter building the previous dataset, however, we felt that we were making an\nassumption too strong in labelling articles as \\ left-leaning \" or \\ right-leaning \"\nonly according to their publisher. Therefore, we decided to expand the idea\n12https://www.kaggle.com/snapcrack/all-the-news\n13https://www.reddit.com/r/conservative\nCHAPTER 5. DATA 44\nFigure 5.21: On the left, the distribution of the lengths of biased articles; on the\nright, the same distribution for unbiased ones. The spike towards zero for the\nsecond category has already been discussed in section 5.2.2, where we argued\nthat it's common for press agencies to release news pieces only a few sentences\nlong.\nFigure 5.22: The most recurring terms among biased (on the left) and unbiased\narticles (on the right) from All the news dataset and r/conservative .\nput in place with r/conservative , applying it to \fve left-leaning subreddits :\nr/progressive14,r/democrats15,r/liberal16,r/voteblue17,r/sandersforpresident18.\nOverall, we gathered 41,008 articles, reduced to 36,658 after removing those less\nthan 25 words long, in majority paywalls and scraping errors. Merging these\narticles with those from r/conservative and those from neutral publishers in All\nthe news dataset , we obtained 144,347 rows. Of these, 54,032 were considered\n\"unbiased\", 52,699 \" right-leaning \" and the remaining \" left-leaning \". As shown\nin Figure 5.23, this dataset has a larger disparity in vocabulary between biased\nand unbiased articles, a positive signal that the newly added samples are more\ndistinguishable with respect to the old ones.\n14https://www.reddit.com/r/progressive\n15https://www.reddit.com/r/democrats\n16https://www.reddit.com/r/liberal\n17https://www.reddit.com/r/voteblue\n18https://www.reddit.com/r/sandersforpresident\nCHAPTER 5. DATA 45\nFigure 5.23: The most recurring terms among biased (on the left) and unbiased\narticles (on the right) coming from All the news dataset and several political\nsubreddits .\n5.5 Datasets for the political ideology detector\nWith this detector, we wanted to automatically recognize the political alignment\nof a text, if there's any. To train it, we reused the two news datasets employed\nin the bias detector. We refer to section 5.4 for an explanation of their creation\nprocess. In order to adapt them to this task, we removed unbiased articles and\nlabelled the remaining ones as left or right leaning according to their original\nsources.\nIn the \frst dataset (built from All the news dataset on Kaggle and integrated\nwith articles from r/conservative ), liberal and conservative articles didn't show\nany noteworthy divergence in length, with both having a similar distribution,\naveraging at 630 words. On the contrary, the di\u000berences in vocabulary are ev-\nident, with right-wing media using terms like \\ police \", \\democrats \", \\media \",\n\\american \" in substantially greater numbers than left-leaning publishers. Fur-\nthermore, \\ trump \" is their most used word, with over 144,000 mentions, against\nthe only 64,600 of liberal media. Curiously, conservative articles appear to be\ntalking more about Democratic nominee Joe Biden as well, with \\ biden \" being\ntheir 10th most used word. Similar situation for the second dataset (where all\nthe articles come from Reddit). It's possible to notice, however, an increase\nin the use of politicians' names - not only Trump is mentioned 20,000 times\nCHAPTER 5. DATA 46\nmore than the previous dataset, but also Clinton and Sanders are brought up\nin 50,000 and 80,000 di\u000berent occasions. This could be a result of the more\npolarized content found on Reddit, with respect to that of mainstream media.\nFigure 5.24: The most recurring terms among right-leaning articles (on the\nbottom) and left-leaning ones coming from All the news (top left image) and\nvarious liberal subreddits (top right image).\nOther than that, we found a third dataset, from Budak, Goel, and Rao 2016.\nThis dataset was composed of thousands of articles published in 2013 in the\nUnited States, manually labeled through crowdsourcing as more favorable to\nthe Democratic or Republican party. However, only a small portion of it was\npublicly available, consisting of just 1,672 articles, that we still deemed useful for\nour experiments. We point the reader to the related paper for a deeper analysis,\nbut we highlight here the di\u000berent lexicon used in conservative and liberal media\n(reported in Figure 5.25). It's interesting to notice the strong similarity between\nthe two in this dataset, with only a few di\u000berent words among the 30 most\npopular ones (with the notable exception of the word \\ gun\", common in right-\nwing articles). Compared to the more recent datasets above, which showed a\ndeep division in the lexicon of left and right articles, it could be a sign of an\nincreased polarization that occurred in the political debate over the course of\nthe last seven years.\nCHAPTER 5. DATA 47\nFigure 5.25: The most recurring terms among left-leaning articles (on the left)\nand right-leaning ones (on the right) from Budak, Goel, and Rao 2016. These\narticles were older than the ones in Figure 5.24, being published in 2013.\n5.6 Datasets for the multilingual experiment\nIn this experiment, we wanted to understand whether \fne-tuning a BERT model\nwith a multilingual dataset could improve performances with respect to a mono-\nlingual one. To discover it, we needed a su\u000eciently large multilingual dataset,\n\fnding three for our purposes.\nThe \frst one, already described above, was the dataset of fact-checking articles\nthat we built - we refer to section 5.3.2 for its analysis. We brie\ry mention the\nmost common languages found in it, which are English and Portuguese, followed\nby Spanish, Hindi, Italian, Telugu, French, Arabic, Urdu and Punjabi. There\nare also several entries in minor languages, such as Marathi or Gujarati, which\nwe considered particularly useful for our experiments, since no BERT model has\never been trained on them.\nIn addition to this dataset, we used the XNLI dataset by Conneau et al. 2018,\na multilingual version of the Multi-Genre Natural Language Inference Corpus\n(Williams, Nangia, and S. Bowman 2018) built by selecting 7,500 pairs of sen-\ntences from the original dataset and translating them into fourteen di\u000berent\nlanguages. Its samples are labeled depending on whether they show entailment,\ncontradiction or neither. To make the data completely unbiased, we picked for\neach sentence a translation in one language, discarding the others and thus ob-\ntaining 500 sentences per language. We point to the original paper for further\nanalysis.\nCHAPTER 5. DATA 48\nLastly, we decided to make use of a third dataset, created by ourselves, focused\non a di\u000berent kind of task, document classi\fcation. This dataset was built\nstarting from Reddit, where we selected subreddits from four di\u000berent categories\n(politics, science, sports and videogames) in \fve di\u000berent languages (English,\nGerman, Spanish, Italian, Portuguese) - the complete list can be found in the\nappendix. From each of these communities, we scraped the titles of their most\nrecent submissions, resulting in 33,854 samples. In Figure 5.26, we show their\ndistribution over the di\u000berent categories and languages. We had some di\u000ecul-\nties in \fnding an adequate number of submissions for speci\fc categories and\nlanguages (for example, we only found 141 posts about science in Portuguese,\nagainst the 1,862 political ones in the same idiom), therefore the data is not\nperfectly balanced.\nFigure 5.26: Distribution of entries across the di\u000berent languages and categories.\nWe then show in Figure 5.27 the most common words for each language, divided\nby category. As expected, the vocabulary shows substantial di\u000berences between\nCHAPTER 5. DATA 49\nPolitics Science Sports Games\nEnglish 2,600 1,900 2,500 1,200\nItalian 2,800 1,363 1,861 362\nGerman 3,041 1,336 2,500 942\nSpanish 2,635 446 3,000 739\nPortuguese 1,864 141 1,521 1,100\nTable 5.5: Number of entries divided by language and category.\ncategories, with politics dominated by political related terms and politician\nnames, sports dominated by football related terminology and so on. Interest-\ningly, some vocables are present across di\u000berent languages, such as \\ govern-\nment \" (present in Italian, Portuguese and Spanish) or \\ climate change \" (seen\nin both Italian and German). Moreover, some terms have become cross-lingual\nand, despite being originally English, are popular across other languages as well,\nsuch as \\ game \" or \\ gameplay \". This, in particular, is an important information,\nas it might a\u000bect the performances of multilingual classi\fers.\nCHAPTER 5. DATA 50\nFigure 5.27: From top to bottom, left to right, the most common words among\nentries from the Reddit multilingual dataset in: English, Italian, German, Span-\nish, Portuguese. A color is assigned to each category, according to the following\nscheme: crimson for politics, green for sports, orange for science, blue for games.\nCHAPTER 5. DATA 51\n5.7 Datasets for the multitask experiment\nThis experiment resumed an analogous one presented in Favano and Carman\n2019, thus we decided to use again the same three datasets used in that paper:\n\u2022A dataset realized to train stance detection models19. The dataset con-\ntains pairs of article bodies and headlines, labeled as \\ agree \", \"disagree \",\n\"discuss \", \"unrelated \". In the original paper, the dataset was used to\ntrain a classi\fer to recognize whether two sentences are related to each\nother, thus rows labeled as \\ discuss \" were dropped, while agreeing and\ndisagreeing sentences were labeled as \\ related \". The dataset is strongly\nunbalanced (4,518 related rows against 36,545 unrelated ones).\n\u2022The Stanford Natural Language Inference dataset20, presented in S. R.\nBowman et al. 2015, which contained pairs of sentences labeled as con-\ntradicting, entailing or unrelated. Labels were manually selected by \fve\nhuman operators, with a \feld called \\ gold label\" reporting for each row\nthe option that was chosen the majority of times. We only kept rows\nwith the gold label \\ contradiction \" or \\ entailment \" and cut the dataset\nto 40,000 instances to facilitate the training on our machine.\n\u2022A dataset containing speeches from di\u000berent politicians agreeing and dis-\nagreeing with each other, 29,343 sentences long21.\nWe point the readers to the original papers for further information and analysis.\n5.8 Summary\nBefore moving on to the next chapter, we leave in table 5.6, as a reference, a\nlist of all the datasets that we've been using.\n19https://github.com/Dragonet95/utils/raw/master/train bodies.csv\n20https://nlp.stanford.edu/projects/snli/\n21https://github.com/Dragonet95/utils/raw/master/DebatesAgreement.zip\nCHAPTER 5. DATA 52\nDataset Entry type Length Source\nNews articles\nfrom r/newsNews Articles 17,782 Reddit, r/news\n(new)\nOpinion pieces\nfrom\nr/InTheNewsNews Articles 15,816 Reddit,\nr/InTheNews\n(new)\nBlog Authorship\nCorpusBlog posts 637,411 Kaggle (new)\nLow-quality\narticlesNews Articles 11,688 Reddit,\nr/savedyouaclick\n(new)\nHigh-quality\narticles from\nr/qualitynewsNews Articles 11,665 Reddit,\nr/qualitynews\n(new)\nHigh-quality\narticles from\nselected\npublishersNews Articles 15,228 The Atlantic ,\nForeign A\u000bairs ,\nPolitico ,The\nNew Yorker ,\nThe Economist ,\nThe Wall Street\nJournal ,BBC\n(new)\nClaims from\nPolitifactSentences 17,580 Politifact (new)\nFact-checking\narticles from\naround the\nworldFact-checking\narticles and\nfact-checked\nclaims52,644 Various\nfact-checking\npublishers (new)\nBiased sentences\nfrom WikipediaSentences 181,474 Pryzant et al.\n2020\nBiased and\nunbiased articlesNews Articles 167,724 All the news\nfrom Kaggle,\nr/conservative\n(new)\nBiased and\nunbiased articlesNews Articles 144,347 r/conservative ,\nmultiple liberal\nsubreddits ,All\nthe news from\nKaggle (new)\nXNLI dataset Pairs of\nsentences7,500 Conneau et al.\n2018\nMultilingual\nsubmissions\nfrom RedditSentences, or\nbrief texts33,854 Multiple\nsubreddits (new)\nStance\nDetection\ndatasetPairs of\nsentences41,063 Favano and\nCarman 2019\nSample from\nSNLI datasetPairs of\nsentences40,000 S. R. Bowman\net al. 2015\nQuotes from\ndebatesPairs of\nsentences26,343 Favano and\nCarman 2019\nTable 5.6: List of all the datasets presented so far (the ones created by ourselves\nare marked as \"new\").\nChapter 6\nExperiments\nIn this chapter, we will be discussing the experiments conducted while working\non the thesis.\nAs a side note, we want to stress the fact that we decided to use BERT models\nin all of them because an important part of this work was to understand BERT's\npotentialities and room for improvement. Nevertheless, in the event of a public\nrelease of our system fastidiouscity , we would be testing di\u000berent models as well,\nsuch as, for example, GPT-2 (Radford et al. 2019).\nWe also want to highlight that, for many of the classi\fers that we will be present-\ning, retrieving high-quality data for training and testing has been challenging.\nThis made it di\u000ecult to estimate or compare their performances in real-world\nscenarios and was the reason why a large part of our work has been dedicated\nto exploring new strategies for building datasets in the \feld of online news\nclassi\fcation.\n6.1 Evaluating the quality of a Reddit dataset\nAs mentioned above, one of the main challenges of this thesis was \fnding ade-\nquate datasets to train and test our models. As shown in the previous chapter,\nour main strategy was to exploit Reddit's peculiarity of creating mono-thematic\ncommunities, called subreddits , to collect large amounts of content (predom-\ninantly news) that could be labeled according to the community they came\nfrom. This approach allowed us to create datasets of low- and high-quality arti-\ncles (section 5.2), of biased and unbiased news (section 5.4) and of right and left\n53\nCHAPTER 6. EXPERIMENTS 54\nleaning content (section 5.5). It also allowed us to build a corpus of multilin-\ngual texts (section 5.6) that we used to test BERT's multilingual performances\n(section 6.7).\nHowever, before using these datasets in our experiments, we wanted to test\nwhether our assumption that Reddit could be a reliable source for datasets of\nnews articles was correct. In order to do this, we decided to set up a crowd-\nsourcing experiment, with the goal of observing if human crowdworkers would\nlabel the content extracted from the social network in the same way as we did\nautomatically. We decided to focus on the ideology dataset, which we believed\nwould be the easiest to label for crowdworkers.\nTo perform this experiment, we gathered 998 articles from the original dataset,\ndivided equally among right and left leaning ones. For each of them, we showed\nits title and a summary generated through the newspaper3k library1, with a\nlink to the original website in case those weren't enough to categorize it. The\ncrowdworkers were then asked whether they believed the articles to be left or\nright leaning (we speci\fed that these terms referred to the US political spectrum,\nas most of the subreddits we used were based there). In Figure 6.1 a screenshot\nof the crowdsourcing application can be seen (we developed it through Flask2\nand, at time of writing, it was accessible through an online address3). To lower\nthe amount of noise, we planned to show each article three times in order to get\nmultiple answers from di\u000berent workers.\nAt the time of writing, we received answers for 410 articles, of which 126 have\nbeen reviewed at least twice. Of these, 374 have been labeled correctly by\ncrowdworkers (91.2% of the total). On a manual inspection of the 36 erroneous\nanswers, we found that most of them were associated with articles that could\nbe a\u000eliated to any political side (for example, some of them reported polling\nresults, information that could be of interest to members of any political party).\nWe found out that others had been removed from the subreddits we had taken\nthem from, so we assumed they had been published by fake users to harm the\nopposing political side (an example is an article containing a conspiracy theory\nabout Democratic nominee Joe Biden, published and then removed on subreddit\nr/democrats ). To \fx this issue, we suggest for the future to analyse upvotes and\ndownvotes of submissions to have a cleaner dataset.\nFinally, it's worth mentioning that in a few cases the mistakes were caused by\n1https://newspaper.readthedocs.io/en/latest/\n2https://\rask.palletsprojects.com/en/1.1.x/\n3http://crowdsourcingreddit.herokuapp.com/\nCHAPTER 6. EXPERIMENTS 55\nFigure 6.1: Screenshot from the crowdsourcing platform we developed to test\nthe quality of our dataset.\nusers who acted beyond political partisanship and published articles denouncing\nscandals in their supported party. An example is shown in Figure 6.2.\nFigure 6.2: Example of a mistakenly labeled article. Coming from r/democrats ,\nit covers a scandal regarding Tulsi Gabbard, congresswoman from the US Demo-\ncratic Party.\nWe believe that the results from this experiment proved that Reddit can be used\ne\u000bectively to create datasets of news articles. We also believe that their quality\ncan be further improved by analysing a submission's popularity, discarding those\nwith low or negative ratings (a negative rating implies having received more\nCHAPTER 6. EXPERIMENTS 56\ndownvotes than upvotes ).\n6.2 Building a newsworthiness classi\fer\nAs explained in the previous chapters, this classi\fer was designed to discriminate\nbetween newsworthy and uninteresting information, dividing texts into news,\nopinions and personal posts.\nA similar topic was presented in Spangher, Peng, and Ferrara 2019, where the\nauthor tackled the subject of \\ lead generation \", or the problem of detecting\namong large quantities of information those leads that could become a front-\npage article. However, our aims were slightly di\u000berent, as we wanted to separate\nproper news from much of the content posted every day on social media, rather\nthan comparing more or less important news.\nOur solution was to employ three di\u000berent datasets, two built by ourselves\nthrough Reddit and one gathered from Kaggle (discussed in detail in section\n5.1), \fne-tuning a BERT model on them.\nThe \fne-tuning was performed using:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-4 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\nGiving the results shown in tables 6.1 and 6.2. Observing them, we can infer\nthat the model could easily distinguish uninteresting content from the rest, but\nencountered more troubles when deciding between news and opinions. This is\nnot too surprising, given that we observed in section 5.1 a certain similarity\nbetween those two datasets.\nFollowing the results from the previous experiment, we chose to test a second\nstrategy for building the model, dividing the task into two subproblems. This\nmeant \fne-tuning two classi\fers, one to discriminate between uninteresting and\ninteresting content (the latter being made of news and opinions) and another\nto decide between news and opinions.\nCHAPTER 6. EXPERIMENTS 57\nPrecision Recall F 1-score\nNews 0.58 0.66 0.62\nOpinion 0.57 0.46 0.51\nUninteresting 0.91 0.94 0.92\nTable 6.1: Classi\fcation report for the \frst newsworthiness classi\fer, trained to\ndiscriminate between all three categories at once. Its overall accuracy was 0.70.\nPredicted\nNews Opinion Uninteresting\nActualNews 2,340 1,018 187\nOpinion 1,533 1,461 185\nUninteresting 163 70 3,720\nTable 6.2: Confusion matrix for the \frst newsworthiness classi\fer.\nThe hyperparameters were the same ones used in the previous model, with the\n\fnal results reported in tables 6.3, 6.4, 6.5 and 6.6.\nPrecision Recall F 1-score\nInteresting 0.94 0.93 0.94\nUninteresting 0.96 0.97 0.96\nTable 6.3: Classi\fcation report for the classi\fer trained to discriminate between\ninteresting and uninteresting content. Its overall accuracy was 0.95.\nPredicted\nInteresting Uninteresting\nActualInteresting 6,394 228\nUninteresting 275 3,780\nTable 6.4: Confusion matrix for the classi\fer trained on interesting and unin-\nteresting content.\nIn the end, the second strategy didn't bring the improvements we had hoped for,\nbut rather con\frmed the results of the \frst model, which showed that detect-\ning interesting content is a relatively easy task for BERT, while discriminating\nbetween news and opinions is a more complex problem to tackle.\nAll in all, we were still satis\fed with the \fnal classi\fer, since the most important\ntask for this predictor was to \"clean\" the input given to the system, discarding\nall the information that is not useful for the majority of people.\nCHAPTER 6. EXPERIMENTS 58\nPrecision Recall F 1-score\nNews 0.60 0.61 0.60\nOpinion 0.55 0.54 0.55\nTable 6.5: Classi\fcation report for the classi\fer trained to discriminate between\nnews and opinions. Its overall accuracy was 0.58.\nPredicted\nNews Opinion\nActualNews 2,139 1,386\nOpinion 1,448 1,704\nTable 6.6: Confusion matrix for the classi\fer trained to discriminate between\nnews and opinions.\n6.3 Building a professionality classi\fer\nThe purpose of this classi\fer was to detect whenever an article su\u000bered from\npoor writing, generally an indicator of low reliability. In our proposed tax-\nonomy (chapter 3), this layer was the equivalent of analysing a news source,\ndiscriminating between professional journalists and low quality content.\nThe only work we were able to \fnd on the subject was by a private company\nnamed deepnews.ai4(already discussed in section 5.2). We tried reaching out\nto them, to obtain a baseline for our models, receiving a negative response.\nTherefore, in building our predictor, our experiments revolved around compar-\ning its performances on the di\u000berent datasets described in section 5.2. These\nwere:\n\u2022A corpus of low-quality news articles scraped from a Reddit community\ncalled r/savedyouaclick5\n\u2022A collection of news articles scraped from r/qualitynews6(another subred-\ndit)\n\u2022A dataset of articles coming from r/news7, always from Reddit\n\u2022One \fnal dataset made of news articles coming from selected publishers\n4https://www.deepnews.ai/\n5https://www.reddit.com/r/savedyouaclick\n6https://www.reddit.com/r/qualitynews\n7https://www.reddit.com/r/news\nCHAPTER 6. EXPERIMENTS 59\nAll of them had a similar number of rows, ranging from 11,000 to 18,000.\nWe trained three classi\fers, using three combinations of the datasets shown\nabove. In all three cases, we decided to use the model bert-base-uncased , the\nbasic version of BERT pre-trained only on English, given that all the articles\nwe collected were written in that language.\n6.3.1 First classi\fer: r/savedyouaclick andr/qualitynews\nFor this classi\fer, and for the following ones, we considered the articles coming\nfrom r/savedyouaclick as low-quality ones. Opposite to them, we used the\narticles extracted from r/qualitynews as examples of high-quality news pieces.\nThe experiment was performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-4 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\nThe training was performed only on the \fnal output layer of the model, freezing\nBERT's own weights. This decision was made after initial testing showed a risk\nof over\ftting by doing di\u000berently. The \fnal results are reported in tables 6.7\nand 6.8.\nPrecision Recall F 1-score\nLow quality 0.91 0.88 0.89\nHigh quality 0.88 0.91 0.90\nTable 6.7: Classi\fcation report for the classi\fer trained on articles from\nr/savedyouaclick and r/qualitynews . Its overall accuracy was 0.89.\nPredicted\nLow quality High quality\nActualLow quality 1,968 281\nHigh quality 196 2,097\nTable 6.8: Confusion matrix for the \frst professionality classi\fer.\nCHAPTER 6. EXPERIMENTS 60\n6.3.2 Second classi\fer: r/savedyouaclick andr/news\nUsing the same strategy above, the negative examples for this classi\fer's training\nwere scraped from r/savedyouaclick , changing the high-quality samples instead,\nwhich we retrieved from r/news .\nThe experiment was performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 1e-4 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\nAs with the previous classi\fer, we froze BERT's own weights, limiting the train-\ning to the output layer, after initial testing suggested a risk of over\ftting when\ndoing otherwise. The results, in tables 6.9 and 6.10, showed a signi\fcant drop\nin performances with respect to the previous classi\fer, with an overall accuracy\nalmost 10 percentage points lower.\nPrecision Recall F 1-score\nLow quality 0.77 0.81 0.79\nHigh quality 0.80 0.75 0.78\nTable 6.9: Classi\fcation report for the classi\fer trained on articles from\nr/savedyouaclick and r/news . Its overall accuracy was 0.78.\nPredicted\nLow quality High quality\nActualLow quality 1,780 423\nHigh quality 543 1,670\nTable 6.10: Confusion matrix for the second professionality classi\fer.\n6.3.3 Third classi\fer: r/savedyouaclick and selected pub-\nlishers\nTo test a di\u000berent strategy, for the third classi\fer we selected seven prominent\nnews publishers to be the source of high-quality articles ( The Atlantic ,Foreign\nA\u000bairs ,Politico ,New Yorker ,The Economist ,BBC ,The Wall Street Journal ).\nCHAPTER 6. EXPERIMENTS 61\nLow-quality articles were taken from r/savedyouaclick as in the two previous\nexperiments.\nThe experiment was performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-4 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\nOnce more, initial testing showed a risk of over\ftting when updating BERT's\nown weights, so we decided to freeze them, updating exclusively the \fnal output\nlayer. The \fnal accuracy was 0.85, closer to the one obtained by the \frst\nclassi\fer. Complete results are reported in tables 6.11 and 6.12.\nPrecision Recall F 1-score\nLow quality 0.83 0.81 0.82\nHigh quality 0.86 0.88 0.87\nTable 6.11: Classi\fcation report for the classi\fer trained on articles from\nr/savedyouaclick and selected publishers. Its overall accuracy was 0.85.\nPredicted\nLow quality High quality\nActualLow quality 1,776 430\nHigh quality 367 2,731\nTable 6.12: Confusion matrix for the third professionality classi\fer.\n6.3.4 Considerations on the experiment\nIn table 6.13, we show the accuracy obtained by each classi\fer on its test set.\nConsidering that we used the same model in all cases, with almost identical\nsettings, it's safe to assume that their di\u000berences in performances were mainly\nrelated to how noisy each dataset was.\nTherefore, it wasn't surprising to discover that the worst-performing model was\nthe one trained on r/news . This subreddit is a large container for articles from\nvariegate sources, which contributes to making it a less reliable source. On the\nCHAPTER 6. EXPERIMENTS 62\ncontrary, it is somewhat surprising that the model trained with high-quality\npublishers resulted in lower performances than the one trained on news pieces\nfrom r/qualitynews .\nIn section 5.2, we had noticed that two prominent news outlets like CNN and\nBusiness Insider were among the \fve most popular publishers on r/savedyouclick .\nThis, combined with the experiment's outcome, seems to suggest that even the\nmost trustworthy newspapers are not exempt from publishing poor content ev-\nery once in a while, thus reinforcing the idea that crowdsourcing and similar\ntechniques are more reliable strategies when creating datasets of news articles,\nrather than simply classifying them based on their sources.\nMore experiments should be performed to estimate the classi\fer's performances\non real-world data, but, all in all, we are able to say that BERT's performances\nwere more than satisfying, proving that this model is capable of e\u000bectively\ntackling the task and showing that the low-quality dataset we created was indeed\ndistinguishable from the others, supporting the goodness of our approach.\nDatasets Accuracy\nLow quality High quality\nr/savedyouaclick r/qualitynews 0.89\nr/savedyouaclick r/news 0.78\nr/savedyouaclick selected publishers 0.85\nTable 6.13: Comparison of the three di\u000berent classi\fers.\n6.4 Building an automated fact-checking system\nThis was one of the most crucial tasks in our system, as analyzing the factuality\nof a text is arguably one of the most important pieces of information when\ntrying to detect fake content. In section 2.2.1 we presented an overview of the\nmain approaches for knowledge-based fake news detection existing in literature.\nAmong these, we decided to pursue the idea of building an automated fact-\nchecking system, as we believed it to be the most viable solution given the\ncurrent technologies. The system we built was designed as follows:\n\u2022Firstly, given a text, an automatic claim detector \fnds every check-worthy\nsentence contained in it\n\u2022For each of the found claims, an online search is performed, in order to\nCHAPTER 6. EXPERIMENTS 63\n\fnd related evidence. To re\fne the process, we integrated this step with a\ncoreference resolution system whose purpose is to contextualise the claims\nin a self-contained way, making the research more e\u000bective (ex. \\ Hesaid\nhe wants to repeal Obama Care \" becoming \\ Trump said he wants to repeal\nObama Care \")\n\u2022Finally, an agreement detector analyzes whether the retrieved evidence\ncon\frms or refutes the information contained in the original sentence\n6.4.1 Claim detection\nThe main works we found in this \feld were Hassan, C. Li, and Tremayne 2015\nand Atanasova et al. 2019.\nIn the \frst one, 20,000 sentences coming from political debates were manually\nlabeled as check-worthy or not, before training several text classi\fers on them\n(this dataset was unfortunately not publicly available). The paper was published\nbefore the release of transformers models, so authors made use of more classic\ntechniques, such as SVM, Naive-Bayes or Random Forests. Their results showed\nthat the models obtained a high level of precision in detecting check-worthy\nsentences, reaching a maximum value of 0.85, at the expense of recall, rarely\nover 0.50.\nThe second work, more recent, described a new dataset created from transcripts\nof debates, whose sentences were labelled as claims if they had been selected\nbyfactcheck.org8for fact-checking. We expressed our doubts on this approach\nin section 5.3.1. The paper then compared di\u000berent models in a ranking task,\nwhose goal was to determine which sentences were the most check-worthy among\nthe ones in the dataset.\nTo overcome the scalability issues brought by the use of manually labeled\ndatasets, we decided to introduce a di\u000berent approach. Considering the problem\nfrom a broader perspective, our system needed to be able to understand whether\na sentence might be containing information or not. Datasets of claims from fact-\nchecking organizations could be used as positive examples (we described ours\nin section 5.3.1), but negative ones had to be retrieved from di\u000berent sources.\nOur proposed solution was to employ transcripts of naturally occurring conver-\nsations, which could serve as examples for those parts of speeches and texts that\n8https://www.factcheck.org/\nCHAPTER 6. EXPERIMENTS 64\ndon't convey any information. The closest dataset we could \fnd was a corpus\nof lines uttered by characters in movies (described as well in section 5.3.1).\nWe then proceeded to \fne-tune a BERT model on our data. The \fne-tuning\nwas performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-5 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\nBERT managed easily to discriminate between the two categories of sentences,\nobtaining an accuracy close to 1.00.\nDespite these brilliant results, to have an estimate of our classi\fer's perfor-\nmances on the actual task of claim detection we needed real-world data. There-\nfore, we decided to replicate the strategy presented in Hassan, C. Li, and\nTremayne 2015, building a dataset of sentences manually labeled as check-worthy\nor not. For this purpose, we gathered the transcripts from the 2020 US presiden-\ntial and vice-presidential debates, dividing them into 4,018 sentences. We then\nbuilt a crowdsourcing application9, shown in Figure 6.3, to label them. Each\nsentence was presented to crowdworkers three times to decrease the amount of\nnoise. Due to the high number of examples, at the time of writing we received\nanswers for only 2,680 of them, representing around two thirds of the initial\ndata. Of these, we kept the sentences that had been labeled a majority of times\neither as \\ Claim \" or \\ Not Claim \", reducing the samples to 2,421. In tables 6.14\nand 6.15, we report the performances that our classi\fer obtained on this data.\nThe \fnal accuracy was 0.69, comparable to the best performance obtained by\nHassan, C. Li, and Tremayne 2015 of 0.70. In that paper, however, the models\nhad the advantage of being trained and tested on data coming from the same\nsource. In addition to that, our model didn't show unbalanced results, obtaining\nsimilar performances on both \\claims\" and \\not claims\", thus reducing the risk\nof over\ftting in real-world use cases. To be noted that all indicators improved\n9http://crowdsourcingdetectorclaim.herokuapp.com/\nCHAPTER 6. EXPERIMENTS 65\nFigure 6.3: A screen from our crowdsourcing application. Crowdworkers were\nasked whether they believed the sentence to be a claim, having three possible\nanswers available: \\Yes\", \\No\" and \\Don't know\".\nPrecision Recall F 1-score\nNot Claim 0.72 0.78 0.75\nClaim 0.63 0.55 0.59\nTable 6.14: Results obtained by our claim detector on the test data. Its overall\naccuracy was 0.69.\nPredicted\nNot Claim Claim\nActualNot Claim 1,130 315\nClaim 442 534\nTable 6.15: Confusion matrix for our claim detector.\nonce we limited the test set to those sentences that had been labeled at least\ntwice (little more than 950). We report these results in tables 6.16 and 6.17.\nIn conclusion, our experiment suggests that this approach is e\u000bective for tackling\nthe task of claim detection. Not only that, we believe that our model has a larger\nroom for improvement than the others that have been proposed so far. The\nclaim datasets we used can be expanded, even to new languages, with relatively\nlow e\u000bort when compared to manually labeled ones, while negative examples\ncan be improved using di\u000berent sources (for example, book transcripts might\nbe added). Moreover, re\fning the testing data by continuing the crowdsourcing\nexperiment might help in reducing the noise in it (as a matter of fact, the\naccuracy improved when considering only sentences with at least two answers).\nBefore moving on, we show in Figure 6.4 two sentences that were wrongly iden-\nCHAPTER 6. EXPERIMENTS 66\nPrecision Recall F 1-score\nNot Claim 0.76 0.80 0.78\nClaim 0.66 0.60 0.63\nTable 6.16: Results obtained by our claim detector on the test data limited to\nsentences labeled at least twice. Its overall accuracy was 0.72.\nPredicted\nNot Claim Claim\nActualNot Claim 463 115\nClaim 149 224\nTable 6.17: Confusion matrix for our claim detector on the limited testing set.\nti\fed by the classi\fer, with the relative explanation (this was obtained using the\neli510library from Ribeiro, Singh, and Guestrin 2016, which treats the predictor\nas a black box).\nFigure 6.4: Two sentences on which our claim detector gave the wrong an-\nswer. The \frst one was identi\fed as \\ Claim \", while the second was considered\n\\Not Claim \". Words highlighted in green supported the prediction, while those\nhighlighted in red opposed it. It's not surprising to observe that words like\n\\supreme \", \\court \", \\justices \", \\senators \", \\vote\" move the prediction towards\n\\Claim \".\n6.4.2 Coreference resolution\nAfter detecting a claim, our system is required to search online for evidence\nthat either supports or refutes it. While working on the claim detection task,\nwe realized that in many cases the sentences were di\u000ecult to comprehend when\nextracted on their own (ex. \\ He said that \" is a meaningless phrase if not\ncorrectly framed). Clearly this issue a\u000bects the overall quality of the system, so\nwe decided to tackle it.\nThis \feld of NLP is called coreference resolution , de\fned as \\determining which\nnouns in text refer to the same real-world entity\"11. An example, taken from\n10https://eli5.readthedocs.io/en/latest/autodocs/lime.html\n11https://nlp.stanford.edu/projects/coref.shtml\nCHAPTER 6. EXPERIMENTS 67\nSuresb 2020, is the following: given the sentence \\ Kathleen Nott was born in\nCamberwell, London. Her father, Philip, was a lithographic printer, and her\nmother, Ellen, ran a boarding house in Brixton; Kathleen was their third daugh-\nter.[She] was educated at Mary Datchelor Girls' School (now closed), London,\nbefore attending King's College, London. \", we want the machine to identify that\nthe word \\ She\" is a pronoun and that in this context it refers to Kathleen.\nAn interesting paper on this subject is Suresb 2020. In this work, the author\nused spaCy12, from Honnibal and Montani 2017, to detect all pronouns and\nentities in a text, before using BERT's attention layers to compute a pronoun-\nentity score among each pair.\nOur approach was somewhat similar. We used spaCy to detect all entities\nand pronouns in a given text. Each of the pronouns thus found, was in turn\nsubstituted with the special BERT token \\[MASK]\", before performing masked\nword prediction (which, coincidentally, is the same task BERT is pre-trained\non). To obtain more reliable results, the predicted word was chosen among the\nentities found in the text, accepting the prediction only if the model surpassed\na given threshold of con\fdence.\nFor this last step, we used an optimized version of BERT, named RoBERTa\n(Liu et al. 2019), that was shown to outperform basic BERT in the speci\fc task\nof masked word prediction. To implement it, we used the HappyTransformers\nAPI13from Fillion et al. 2020.\nWe tested our approach on the GAP dataset from Webster et al. 2018. This\ndataset is composed of 4,000 sentences, each accompanied by two names that\ncan refer to the same pronoun. The goal for a classi\fer is to guess which one\nthe pronoun is referring to.\nWe limited our test set to 286 sentences where the pronoun is either \\He\" or\n\\She\", discarding possessive pronouns, such as \\his\" and \\her\", for which our\nmodel hadn't been adapted. In the remaining rows, our system reached an\naccuracy of 0.75, beating the baseline presented in Webster et al. 2018 of 0.66\nand similar to the accuracy of 0.76 obtained in Suresb 2020. The complete\nresults are shown in tables 6.18 and 6.19.\n12https://spacy.io/\n13https://github.com/EricFillion/happy-transformer\nCHAPTER 6. EXPERIMENTS 68\nAlthough the performances are likely to degrade in a real-world use cases, it's\nnoteworthy that the models we used didn't even need to be \fne-tuned for the\ntask (both BERT and RoBERTa can perform masked word prediction out of\nthe box). It's therefore plausible that with an appropriate \fne-tuning process\nthese results might be improved, showing that this is a promising approach to\nthe problem.\nPrecision Recall F 1-score\nOption A 0.66 0.30 0.44\nOption B 0.76 0.94 0.84\nTable 6.18: Results obtained by our coreference resolution system. Its overall\naccuracy was 0.75.\nPredicted\nOption A Option B\nActualOption A 25 59\nOption B 12 190\nTable 6.19: Confusion matrix for our coreference resolution system.\n6.4.3 Agreement detection\nThe last step the system has to take is to analyze whether the evidence found\nonline supports or refutes the initial claim. For this task we presented in section\n5.3.2 a dataset of 52,644 fact-checking articles from around the world, each\naccompanied by the related claim and truth rating. By training a BERT model\non this data, we wanted to achieve a model that, given a sentence and an article\nconnected to it, would be able to discriminate whether the latter agreed with\nthe former or vice versa.\nWe planned to execute the training in three di\u000berent settings:\n\u2022using the original dataset, without any changes\n\u2022using a smaller version of the dataset, where the pairs labeled as \\false\"\nwould be sampled in order to obtain a balanced dataset (the original one\nwas heavily skewed, with \\false\" entries representing more than 80% of\nthe total)\n\u2022using only the titles of the fact-checking articles (employing again the\nbalanced dataset, as the original one caused over\ftting)\nCHAPTER 6. EXPERIMENTS 69\nIn the three settings, the \fne-tuning was performed with the following hyper-\nparameters:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-4 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 5 (no further improvements afterwards)\n\u2022train/test split : 0.2\nIn all cases, the \fne-tuning was performed using BERT's own weights frozen,\nafter initial testing showed over\ftting when doing otherwise.\nThe comparisons between the results from the various models can be seen in\ntables 6.20, 6.21 and 6.22.\nModel Precision Recall F 1-score\nBase datasetTrue 0.67 0.24 0.35\nFalse 0.84 0.97 0.90\nBalanced datasetTrue 0.77 0.56 0.65\nFalse 0.63 0.82 0.71\nTitle onlyTrue 0.74 0.61 0.67\nFalse 0.64 0.77 0.70\nTable 6.20: Comparison between the performances of the di\u000berent agreement\ndetectors.\nPredicted\nModel True False\nBase dataset ActualTrue 505 1,592\nFalse 249 8,138\nBalanced dataset ActualTrue 1,192 940\nFalse 366 1,632\nTitle only ActualTrue 1,303 845\nFalse 465 1,517\nTable 6.21: Confusion matrices of the di\u000berent agreement detectors.\nLooking at the performances, the last two models appeared to be almost equiv-\nalent, with a 0.68 overall accuracy in both cases. On the other hand, the model\ntrained on the original dataset showed the worst performances, labelling most of\nthe test rows as \\false\". This was probably due to the skewness of the training\ndata, which, as we said before, was mostly composed of negative examples.\nCHAPTER 6. EXPERIMENTS 70\nModel Accuracy\nBase dataset 0.82\nBalanced dataset 0.68\nTitle only 0.68\nTable 6.22: Comparison between the accuracy values of the di\u000berent agreement\ndetectors. The higher accuracy on the \frst dataset is misleading, as it was\nobtained by simply labelling the majority of samples as \"false\".\nIn conclusion, out of the three models we trained, the \frst one should be dis-\ncarded, as its results didn't show any real possibility of improvement. Among\nthe remaining two, further studies should be conducted to assess whether any\nstatistical di\u000berence exists between them. For our system, we decided to use\nthe model trained using the entire articles, rather than only their titles, as it\nallowed for an easier deployment.\n6.5 Building a bias detector\nIn our taxonomy, we established that a key role in determining the quality of\na news article was its level of objectivity and the presence of any type of bias.\nAn article excessively favorable towards one end of the political spectrum is less\ntrustworthy than a neutral one. Of course, this doesn't mean that left or right\nleaning newspapers can't publish trustworthy news and, conversely, an unbiased\nsource can still provide false information. However, it was our belief that readers\nshould know whether they're facing a text with an important bias in it, as this\ncan be crucial when deciding whether to trust its information or not.\n6.5.1 Related works\nBefore proceeding with our experiments, we give an overview over existing works\non the topic. There are not many of them, likely due to the scarcity of related\ndatasets, as well as the relative di\u000eculty the task itself presents. Here's the\nmain ones:\n\u2022Pryzant et al. 2020, in which authors present a dataset of quotes from\nWikipedia that were edited for not respecting the website's policy of neu-\ntral point-of-view (we mentioned this dataset in section 5.4). The paper\nCHAPTER 6. EXPERIMENTS 71\nalso shows the creation of a model trained to automatically detect and\nremove biases from a sentence.\n\u2022Budak, Goel, and Rao 2016, in which authors describe how they employed\ncrowdsourcing to build a dataset of biased and politically sided news ar-\nticles. Unfortunately, the \fnal dataset wasn't made publicly available,\nexcept for a small portion (this dataset is mentioned in section 5.4 as\nwell).\n\u2022An online project named The Bipartisan Press14, whose purpose is to\n\\go past the biases and instead focus on letting readers make their own\nopinions \" by \\ [giving] people an idea of what others think while noting\nthat they may be biased, so readers can make their own opinion on an\nissue \". The authors explained how they used the All the news15dataset\nfrom Kaggle combined with ratings from MediaBiasFactCheck16to create\ntheir model. We partly used this idea to build our own datasets and\nmodels (more in section 5.4 and in the following paragraphs).\n6.5.2 First classi\fer: Wikipedia dataset\nThe \frst classi\fer was trained over the dataset of Wikipedia sentences from\nPryzant et al. 2020, using one half of the rows as positive examples (by taking\nthe orginal biased version) and the other half as negative ones (by taking the\nedited sentence). In the end, the data consisted of 90,737 samples for both\ncategories.\nThe experiment was performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-4 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\n14https://www.thebipartisanpress.com/\n15https://www.kaggle.com/snapcrack/all-the-news\n16https://mediabiasfactcheck.com/\nCHAPTER 6. EXPERIMENTS 72\nUnfortunately, the results were inconclusive, with the model labelling almost\nall samples in the test data as unbiased. We tried di\u000berent settings, without\nobtaining signi\fcant improvements, so we switched to using di\u000berent training\ndata.\n6.5.3 Second classi\fer: News dataset (Kaggle and\nr/conservative )\nGiven the disappointing results from the previous experiment, we decided to\nchange our approach. We created our own dataset starting from the All the\nnews dataset on Kaggle, whose articles were labeled according to the evaluation\ngiven on MediaBiasFactCheck to their publishers, and integrating it with right-\nleaning articles from the subreddit r/conservative . The \fnal dataset, described\nextensively in section 5.4, is considerably skewed towards biased samples, with\nan approximate ratio of 65 to 35 with respect to unbiased ones.\nThe experiment was performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-5 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\nThe results on the test data, shown in tables 6.23 and 6.24, were more than\nsatisfying. The model obtained an accuracy close to 100%, correctly identifying\n33,409 articles out of the 33,573 contained in the test set.\n6.5.4 Third classi\fer: News dataset (Kaggle,\nr/conservative , liberal subreddits )\nDespite the excellent results of the previous classi\fer, we had doubts over the\nquality of the data. Two thirds of the news articles used to train it came from\ntheAll the news dataset on Kaggle and were labeled according to their pub-\nlishers' ratings on MediaBiasFactCheck. This introduced noise, as we made the\nstrong assumption that every article coming from the same publisher was biased\nCHAPTER 6. EXPERIMENTS 73\nPrecision Recall F 1-score\nUnbiased 0.99 0.99 0.99\nBiased 1.00 1.00 1.00\nTable 6.23: Results obtained by the classi\fer trained on articles from All the\nnews dataset and r/conservative . Its overall accuracy was 1.00.\nPredicted\nUnbiased Biased\nActualUnbiased 11,294 81\nBiased 83 22,115\nTable 6.24: Confusion matrix for the classi\fer trained on articles from All the\nnews dataset and r/conservative .\nor unbiased. Therefore, we replicated the approach used with r/conservative ,\nreapplying it to liberal subreddits to obtain an equal amount of right and left\nleaning articles (all labeled as \\ biased \"). To collect neutral samples we had to\nexploit the All the news dataset again, collecting all the articles published by\nReuters and other neutral publishers. The \fnal dataset (explained in detail in\nsection 5.4), was again unbalanced towards biased articles, but with a lower\nratio of 60 to 40 to unbiased ones.\nThe experiment was performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-5 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\nThe results on the test data, shown in tables 6.25 and 6.26, were even better than\nthe previous ones, with 28,601 correct predictions out of 28,676 total samples.\n6.5.5 Considerations on the experiments\nWe believe this is an interesting task to tackle and that BERT was more than\nable to handle it, although the lack of datasets speci\fcally built for this purpose\nmade it hard to compare the performances of di\u000berent models. However, the\nbrilliant results obtained by the classi\fers over the news datasets we created\nCHAPTER 6. EXPERIMENTS 74\nPrecision Recall F 1-score\nUnbiased 1.00 1.00 1.00\nBiased 1.00 1.00 1.00\nTable 6.25: Results obtained by the classi\fer trained on articles from All the\nnews dataset, r/conservative and several liberal subreddits . Its overall accuracy\nwas 1.00.\nPredicted\nUnbiased Biased\nActualUnbiased 10,737 40\nBiased 35 17,864\nTable 6.26: Confusion matrix for the classi\fer trained on articles from All the\nnews dataset, r/conservative and several liberal subreddits .\nshowed that our approach for building them is a viable one. As with other\nclassi\fers we trained, we shouldn't be expecting these performances to be main-\ntained in real-world scenarios. The data still su\u000bered from some limitations,\nespecially with regards to unbiased articles, given that the publishers were lim-\nited to Reuters and a few more. Moreover, the test set came once more from the\nsame source as the training data, limiting our con\fdence on the models' perfor-\nmances on external samples. Despite these issues, our experiments proved that\nBERT was capable of recognizing whether an article comes from a neutral or\nbiased source and supported our belief that Reddit can be used as an e\u000bective\nsource for creating news datasets.\n6.6 Building a political ideology detector\nBuilding this detector wasn't part of the \frst design for our online content\nclassi\fer. Originally, we had planned to build a more complex system capable\nof recognizing the intentions behind the manipulation of information inside a\nnews article. However, after recognizing that this approach would've been too\nbroad for a machine to handle, we realized that a clearer distinction could be\nachieved by looking at an article's political stance. Not only that, our work\non the bias detector showed that right and left biases have di\u000berent ways of\nshowing themselves, so we considered this information to be more important\nto readers. In the end, we decided to focus on this problem, training a BERT\nmodel to distinguish between articles with a conservative and liberal bias.\nCHAPTER 6. EXPERIMENTS 75\n6.6.1 First classi\fer: Crowdsourcing dataset\nIn this \frst test, we used the free portion of the dataset created in Budak, Goel,\nand Rao 2016. The dataset, built through crowdsourcing, divided the articles\nbased on whether they were more favorable to the Democratic or Republican\nparty. The main limitation of this dataset was that all of the samples had been\npublished in 2013. Considering how much the political debate has changed\nsince then, it's evident that this could introduce an important bias in the data.\nNevertheless, we trained a classi\fer on it, to obtain a baseline for the following\nmodels.\nThe training was performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-5 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 10 (no further improvements afterwards)\n\u2022train/test split : 0.2\nThe model accuracy peaked at 0.70 (full results in tables 6.27 and 6.28). Given\nthe low number of samples, we considered it a satisfying result, but not good\nenough to use the classi\fer in our \fnal prototype.\nPrecision Recall F 1-score\nLeaning Right 0.68 0.56 0.62\nLeaning Left 0.71 0.80 0.76\nTable 6.27: Results obtained by the classi\fer trained on articles labeled through\ncrowdsourcing. Its overall accuracy was 0.70.\nPredicted\nLeaning Right Leaning Left\nActualLeaning Right 80 62\nLeaning Left 38 155\nTable 6.28: Confusion matrix for the classi\fer trained on articles labeled through\ncrowdsourcing.\nCHAPTER 6. EXPERIMENTS 76\n6.6.2 Second classi\fer: News dataset (Kaggle and\nr/conservative )\nGiven the limitations of the previous dataset, we decided to train two more\nclassi\fers with the datasets employed in the previous chapter. For the \frst\none, we used the same dataset used in section 6.5.3, created from All the news\nand r/conservative . After removing neutral articles, roughly 111,325 samples\nremained, divided equally between left and right wing.\nThe training was performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-4 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\nUnfortunately, the results were inconclusive, with the model classifying all data\nas right-leaning. It is our belief that this was due to the noise introduced in the\ndata while retrieving articles from All the news dataset (we covered this issue\nextensively in section 5.4).\n6.6.3 Third classi\fer: News dataset ( r/conservative and\nliberal subreddits )\nFor this classi\fer, we used the new dataset we built from Reddit, scraping\nr/conservative for right-wing news articles and using several liberal subreddits\nfor left-wing data (more details are shown in section 5.4). The dataset thus\nobtained was slightly unbalanced towards the \frst category (52,699 rows against\n36,648).\nThe training was performed with:\n\u2022learning policy : one-cycle policy\n\u2022learning rate : 5e-4 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\nCHAPTER 6. EXPERIMENTS 77\n\u2022train/test split : 0.2\nThe classi\fer peaked with an accuracy of 0.90, largely outperforming the pre-\nvious ones (complete results in tables 6.29 and 6.30). We considered this result\nto be further proof of the quality of the data extracted from Reddit, which we\ndeemed particularly \ft for this speci\fc task given its natural tendency to create\nclosed and polarized communities when talking about politics. As in other cases,\nto con\frm these performances, we would need to test the model on real-world\ndata. Nevertheless, these results are promising and show that this is another\ntask that BERT can handle e\u000bectively.\nPrecision Recall F 1-score\nLeaning Right 0.91 0.91 0.91\nLeaning Left 0.87 0.87 0.87\nTable 6.29: Results obtained by the classi\fer trained on articles coming from\nr/conservative and liberal subreddits . Its overall accuracy was 0.90.\nPredicted\nLeaning Right Leaning Left\nActualLeaning Right 9,616 932\nLeaning Left 928 6,394\nTable 6.30: Confusion matrix for the classi\fer trained on articles coming from\nr/conservative and liberal subreddits .\n6.7 Exploring BERT's performances on a mul-\ntilingual dataset\nWe discussed in chapter 4 the need to extend the \feld of fake news detection\nfrom English to other languages. Until a few years ago, building an automated\nmultilingual text classi\fer would've been a challenging feat. It would've required\nbuilding large datasets for each of the desired languages and often it would've\nrequired designing di\u000berent systems to adapt to the di\u000berent idioms. After the\nintroduction of BERT, however, this became an easier problem to handle. BERT\nmodels have been pre-trained in more than 170 languages, reducing the amount\nof data necessary to create a satisfying multilingual model and, what's more,\neven with \fne-tuning data in just one language, a BERT model pre-trained on\nmultilingual data can still obtain discrete results on di\u000berent languages.\nCHAPTER 6. EXPERIMENTS 78\nFor all these reasons, we decided to further investigate BERT's multilingual\nperformances. Before describing our experiment, however, we present a survey\nof the studies that have been conducted so far in the \feld of multilingual training\nfor BERT models.\n6.7.1 Related works\nWe couldn't \fnd a large number of works on the subject. The \feld of multi-\nlingual text classi\fcation is still fairly recent and BERT, which made it more\nbroadly accessible to research, had only been released for a few years at the\ntime of writing. This lack of studies on the matter is re\rected on the limited\namount of datasets that can be found to train models, which by itself slows\ndevelopments in the area.\nIn Pires, Schlinger, and Garrette 2019, authors brought evidence in support of\nthe statement we made earlier about BERT models pre-trained on multilingual\ndata, showing that they are able to classify texts in di\u000berent languages, even\nwhen \fne-tuned on a single one. The paper also explored whether there are pairs\nof languages that o\u000ber better performances than others, obtaining a positive\nanswer.\nIn Favano and Carman 2019, an experiment was conducted to test whether\ntraining BERT on a multilingual dataset could improve performances over em-\nploying a mono-lingual one, showing mixed results.\n6.7.2 Our experiment\nStudying the papers presented above, we decided to focus on the following\nquestion: \\ does using a multilingual dataset improve the training performances\nof a BERT model over training the same model on a monolingual one? \". We\nfelt that this could be an interesting research area for our work, given that a\npositive or negative answer would in\ruence the way we build datasets for our\nown tasks.\nIn Favano and Carman 2019, the authors addressed a similar issue, but the main\nlimitation of that paper was caused by the three datasets that were employed,\ntwo of them in English, with only the third one being multilingual. The authors\ncompared whether training a model on one of the English datasets would give\nbetter results when tested on the other English datasets with respect to train-\ning it on the multilingual one. We felt that this approach could introduce a\nCHAPTER 6. EXPERIMENTS 79\nsigni\fcant bias, as the data used for the di\u000berent trainings was incoherent, thus\nmaking any result more likely to be a product of statistical variations, rather\nthan intrinsic reasons. Therefore, we decided to \fnd new multilingual datasets,\nbuilt coherently, and to modify the structure of the experiment in the following\nway:\n\u2022train a monolingual version of BERT on each monolingual portion of the\ndataset\n\u2022train a multilingual version of BERT on the entire dataset\n\u2022compare the performances across the di\u000berent languages\n\u2022compare the performances on an unknown language, unseen in the training\ndata\nOur ultimate goal was to understand whether, once a multilingual dataset is\navailable, training a single multilingual BERT would give better results than\ntraining several monolingual ones. Considering that having a single model for\nmany languages translates into lower costs during training and deployment, it\nwould be a satisfying result even if multilingual BERT simply performed as well\nas the others.\nFor our experiment we made use of three di\u000berent datasets: XNLI dataset by\nConneau et al. 2018 (which was used in two of the papers we presented above\nand that we described in section 5.6), the dataset of fact-checking articles we\nbuilt in section 5.3.2 and a dataset we built from Reddit (described in section\n5.6). The results on the \frst two datasets were inconclusive, with the models\nover\ftting on various languages. We blamed for this the di\u000eculty of the task\nthey were given, in both cases agreement detection, and the excessively low\namount of data for some languages in particular. For this reason, we report\nonly the results for the third and \fnal dataset, which instead was built for the\neasier task of document classi\fcation and was more evenly balanced among the\nvarious idioms.\nIn total, we trained six classi\fers: one per language (English, German, Ital-\nian, Spanish, Portuguese), plus a multilingual one. The \fne-tuning hyper-\nparameters were the same for all of them:\n\u2022learning policy : one-cycle policy\nCHAPTER 6. EXPERIMENTS 80\n\u2022learning rate : 1e-4 (chosen according to training simulations and to the\nvalues suggested by Google)\n\u2022epoch : 4 (no further improvements afterwards)\n\u2022train/test split : 0.2\nThe BERT models that were used were:\n\u2022bert-base-uncased (English)\n\u2022bert-base-german-uncased (German)\n\u2022bert-base-italian-uncased (Italian)\n\u2022bert-base-spanish-wwm-uncased (Spanish)\n\u2022bert-base-portuguese-uncased (Portuguese)\n\u2022bert-base-multilingual-uncased (Multilingual)\nThe models were chosen among the ones available on HuggingFace17according\nto their popularity.\n6.7.3 Results\nIn table 6.31 we show the results of each model over each language. The same\nresults are displayed visually in Figure 6.5.\nTesting dataset\nEnglish German Spanish Italian Portuguese\nTraining\ndatasetMultilingual 0.92 0.86 0.91 0.89 0.86\nEnglish 0.94 0.54 0.53 0.51 0.50\nGerman 0.56 0.88 0.48 0.38 0.40\nSpanish 0.74 0.40 0.93 0.44 0.58\nItalian 0.61 0.52 0.47 0.92 0.55\nPortuguese 0.70 0.46 0.68 0.55 0.89\nTable 6.31: Accuracy of each model over each language of the dataset.\nAs we could've imagined before performing the experiment, the most reliable\nmodel across all languages was multilingual BERT \fne-tuned over the entire\n17https://huggingface.co/models\nCHAPTER 6. EXPERIMENTS 81\nFigure 6.5: Accuracy of each model over each language.\ndataset. However, it's worth noting that its accuracy was slightly lower when\ncompared to the accuracy obtained by the other models over their own spe-\nci\fc language. It would require further experiments to prove that this was a\nstatistically signi\fcant di\u000berence, but these results suggest that using a multi-\nlingual dataset doesn't necessarily improve BERT's training performances with\nrespect to using monolingual datasets with speci\fc BERT models trained for\neach speci\fc language. It's interesting to notice that each model behaved di\u000ber-\nently with di\u000berent languages. As can be seen in Figure 6.5, English and Italian\nmodels had similar results on all other languages, while Spanish and Portuguese\nmodels fared much better on English data than the rest. This could be related\nto the extensive use of English terminology in certain areas, such as gaming\nor science. The Portuguese model also showed above average performances on\nSpanish data, perhaps due to the similarity between the two languages. This\ncould be another interesting \feld of study for future works.\nWe then tested the various models on French samples, language absent from\nthe training data, to compare how the multilingual model would behave on an\nunseen language with respect to the monolingual ones. Results are reported in\ntable 6.32.\nAs expected, the multilingual model fares better than all the others, but achiev-\ning worse results than the ones obtained on the same languages it had been\nCHAPTER 6. EXPERIMENTS 82\nTraining dataset Accuracy\nEnglish 0.55\nGerman 0.38\nPortuguese 0.40\nSpanish 0.44\nItalian 0.39\nMultilingual 0.62\nTable 6.32: Accuracy of each model over an unknown language.\n\fne-tuned on. Interestingly, the English model achieved comparable perfor-\nmances, decisively outperforming the remaining monolingual ones. This might\nbe due to the di\u000busion of English terms in most idioms, or to the di\u000berences in\npre-training of the BERT models we employed.\n6.8 Exploring BERT's performances in a multi-\ntask setting\nThe concept behind this last experiment was to understand whether BERT's\n\fne-tuning could improve when performed in a multi-task setting. The experi-\nment was inspired from the one presented in Favano and Carman 2019, in which\nthe authors described and compared \fve di\u000berent training settings over three\ndi\u000berent datasets (described in detail in section 5.7):\n\u2022a dataset of pairs of article bodies and headlines, labeled as \\ Related \" or\n\\Unrelated \" to each other\n\u2022the Stanford corpus for Natural Language Inference, composed of pairs of\nsentences that constitute examples of entailment or contradiction\n\u2022a dataset of pairs of sentences agreeing or disagreeing with each other\nThe tasks on which the models were trained for were the following:\n\u2022relevance detection : analyze whether two sentences are related to each\nother\n\u2022inference detection : detect whether two sentences entail or contradict\neach other\nCHAPTER 6. EXPERIMENTS 83\n\u2022agreement detection : detect whether two sentences agree with each\nother\nThe following is a summary of the \fve settings used in the original paper:\n\u2022Baseline : each dataset is given as input to a di\u000berent model\n\u2022Merged datasets, single label : the datasets are merged and one model\nis used to predict a single multi-dimensional label\n\u2022Merged datasets, multi label : the datasets are merged, but multiple\ncolumns are used, one for each original dataset\n\u2022Hard-coded correlation : similar to the previous settings, but for en-\ntailing sentences relevance and agreement column are also set to positive,\nwhile for agreeing sentences the same is done with the relevance column\n(this forces the model to learn that two entailing sentences are related\nand in agreement with each other, while two agreeing sentences have to\nbe related)\n\u2022Limited datasets, parallel training : one single model is used for all\nof the datasets, but the \fnal output layer is trained separately for each\ndataset\n6.8.1 Our experiment\nWe decided to simplify the settings presented in the paper, reducing them to\njust three:\n\u2022Baseline : same as before, it consisted in simply training a BERT classi\fer\non each of the three datasets\n\u2022Hard-coded correlation : in this setting, we started from the same idea\npresented in Favano and Carman 2019, merging the three datasets and\nusing a multi label output, formed by six di\u000berent columns (two for each\ndataset). The labels were then set according to the policy shown in table\n6.33. This was similar to the one used in the previous experiment, with\nthe di\u000berence that contradicting sentences were marked as disagreeing\nwith each other, and vice versa. To be noted that, if two sentences entail\neach other, not necessarily they are in agreement (ex. \\ It's hot, so I'm\nCHAPTER 6. EXPERIMENTS 84\nwearing a T-shirt \" shows entailment but not agreement). On the opposite,\nagreeing sentences can be considered as entailing each other. Once the\n\fnal dataset was ready, we trained a BERT model on it\n\u2022Parallel setup with frozen BERT layers : the same dataset used in\nthe previous setting was employed. However, this time it was used to\ninitialize the weights of three BERT models, which were later \fne-tuned\non the rows relative to the three original datasets. During this second\nstep, we froze BERT's weights, leaving only the output layer for updating\nDataset Related Unrelated Entail Contradict Agree Disagree\nRelevance O.V. O.V. 0.5 0.5 0.5 0.5\nInference 1 0 O.V. O.V. 0.5 if en-\ntailment\n= 1, 0 if\nentail-\nment =\n01 - Agree\nAgreement 1 0 Agree Disagree O.V. O.V.\nTable 6.33: Labelling policy adopted for the second and third settings (O.V.\nstands for Original Value).\n6.8.2 Results\nAs shown in table 6.34, BERT's heavily over\fts in the relevance and agreement\ntasks when tackling them separately. On the contrary, in the two multi-task\nsettings, results are more balanced. The improvement on the relevance detec-\ntion task in particular was quite impressive, reaching almost a 100% accuracy,\nwhereas the initial model was limited to labeling everything as \\Unrelated\".\nSimilar improvements were seen on the Agreement detection task, with a \fnal\naccuracy of 76%, starting from a mere 56%.\nIn general, the third setting showed the best results, obtaining an overall im-\nprovement on all of the three tasks. This experiment con\frmed the \fndings\nof Favano and Carman 2019, suggesting that using a multi-task setting can be\nbene\fcial for the quality of BERT's training.\nCHAPTER 6. EXPERIMENTS 85\nSetup Dataset Precision Recall F 1-score Accuracy\nBaselineRelevance 0.00 0.00 0.00 0.89\nInference 0.91 0.92 0.92 0.92\nAgreement 0.70 0.15 0.24 0.56\nMulti-taskRelevance 0.98 0.97 0.98 0.99\nInference 0.92 0.92 0.92 0.92\nAgreement 0.73 0.53 0.62 0.68\nParallel trainingRelevance 0.99 0.98 0.99 1.00\nInference 0.93 0.94 0.93 0.93\nAgreement 0.80 0.67 0.73 0.76\nTable 6.34: Results from the multi-task experiment on the various setups. Pre-\ncision, recall and F 1-score are computed on the positive class for each dataset.\nChapter 7\nBuilding a working\nprototype\nAfter completing the experiments presented in the previous chapter, we built a\nprototype to show a real-world use case for our research. Its name was fastidi-\nouscity and consisted of a web application, built with Flask1.\nUpon entering it, the user is required to insert a text (a speech or a news\narticle) that he/she wants to analyze. The text is sent to a server which, using\nthektrain library from Maiya 2020, returns its predictions on bias, ideology and\nprofessionality (as mentioned earlier, we didn't include newsworthiness since in\nthis use case we expected the user to already consider the text given in input as\nnewsworthy ). Once ready, the application displays them on the screen together\nwith the original text, whose check-worthy sentences have been highlighted in\ngreen. The user can click on one of them to trigger an online search for related\nevidence, in turn examined to establish whether it supports or refutes the claim.\nAt the discretion of the user, the search can be re\fned through the coreference\nresolution system described in section 6.4.2.\nAs we already stated, for many of the classi\fers we don't have a valid estimate\non how they will behave on real-world data and likely, for some of them, perfor-\nmances might degrade with respect to the development stage. For this reason,\nthe application is equipped with a feedback mechanism to collect information\nfrom the users on missed predictions, with the hope of gathering enough samples\n1https://\rask.palletsprojects.com/en/1.1.x/\n86\nCHAPTER 7. BUILDING A WORKING PROTOTYPE 87\nFigure 7.1: A screenshot taken from fastidiouscity , our working prototype. The\ntext, extracted from the 2020 US presidential debate, was pronounced by then\nDemocratic nominee Joe Biden.\nin the future to improve our system's overall performances.\nDespite this, the results presented so far are encouraging and support the idea\nthat current technologies should play a key role in tackling the problem of online\nmisinformation. Further research in the area should be incentivized, as it's not\ninconceivable to think that in the near future their performances might improve\ndramatically.\nFor the moment, we believe that the tool we created is still too unreliable to\nbe used as a completely automated fact-checking system, but would be more\nuseful as an assisting automated fact-checking system, to help journalists and\nfact-checkers speed up their analysis of news or debates. Nevertheless, creating\na product capable of achieving this would be a remarkable milestone, since, as\nwe showed in section 2.2.1, the main drawback of classical fact-checking is its\nslowness compared to that of fake news.\nChapter 8\nConclusions\nIn this thesis, we discussed the problem of online content classi\fcation, with the\nultimate goal of building a tool capable of discriminating between reliable and\nunreliable information. In chapter 3, we posed \fve research questions on the\nsubject:\n1. Is it possible to create an objective classi\fcation of online news that goes\nbeyond the simple \\fake\"/\"real\" division? If so, are automated text clas-\nsi\fcation techniques available today e\u000bective enough to automatically cat-\negorize articles according to this new classi\fcation?\n2. Can we mine social networks like Reddit to build datasets to be used in\nnews classi\fcation tasks that are as e\u000bective for training text classi\fcation\nsystems like BERT as those built through crowdsourcing?\n3. Is it possible to build an automated fact-checking system that, given a\ntext, is able to:\n(a) reliably identify those sentences containing claims,\n(b) automatically convert such sentences into a self-contained format (by\nremoving coreferences, etc) so that they provide for more e\u000bective\nevidence search online, and\n(c) determine whether any related evidence thus found supports or refutes\nthe original claim?\n88\nCHAPTER 8. CONCLUSIONS 89\n4. Does the training of a single BERT text classi\fcation model over a multi-\nlingual dataset give better results with respect to the training of di\u000berent\nBERT models, each over monolingual portions of the same dataset?\n5. Does the training of a BERT text classi\fcation system obtain better results\nwhen performed in a multi-task setting with respect to the training of the\nsame system in a single-task setting?\nBased on the topics addressed so far and based on our experiments, we can try\nto answer each of them.\nIn chapter 4, we introduced a new taxonomy that treated online content in\na more complex manner, not focusing only on its factuality, but also taking\ninto consideration the di\u000berent ways in which information can be manipulated.\nEstablishing its truthfulness was still a key part of the classi\fcation, but this\nhas been accompanied by several di\u000berent layers that help the reader in giving\ncontext to the texts he/she is reading. We then outlined the structure of a\nsystem that would be capable of automatically analysing texts, labelling them\naccording to our new taxonomy. In chapter 5 and 6, we showed the challenges\nwe had to face for building each of the classi\fers composing the system, as well\nas the obtained outcomes. Given the promising results reached by many of\nthem, we decided to develop a web application called fastidiouscity , showing a\npossible use-case for our research, which we described in chapter 7.\nAlthough its predictions are still too imprecise to say that the fact-checking\nprocess can be completely automated, we believe that they're good enough to\nconstitute an e\u000bective tool to assist journalists and fact-checkers in their work.\nWe therefore believe that further research in the area should be conducted, as it\ncould lead in the near future to the creation of a completely automated system\nfor real-time fact-checking, from which we are already not too far away.\nApart from this, it's our opinion that going beyond the classic \"fake\"/\"real\"\nclassi\fcation was bene\fcial to the quality of our results, not only by making\nour system capable of detecting the \fner shades of disinformation, but also by\ndividing the initial task into multiple subtasks that were easier to address on\ntheir own.\nThe second question was raised following the concerns we expressed on the data\navailable for many of the classi\fers. The lack of publicly available and high-\nCHAPTER 8. CONCLUSIONS 90\nquality datasets in this \feld is the reason why a large part of our work had to be\ndedicated towards creating new strategies for building them. One of the main\nsources we identi\fed for this was Reddit, which we employed for most of the\ntasks we addressed. Not only is Reddit extremely easy to scrape, but, in our\nopinion, the mechanism of subreddits , Reddit's mono-thematic communities,\nmakes the website perfect for constructing large corpuses of labelled articles,\nas the content retrieved from each community reliably follows the ideologies\nand themes of that particular community. Indeed, they are usually heavily\nmoderated, so that in most of them it's di\u000ecult to \fnd content that doesn't align\nwith the often very strict guidelines set by the administrators. In section 6.1, we\nshowed the crowdsourcing experiment we set up to con\frm our speculation. In\nthat experiment, crowdworkers were asked whether they believed a news article\nto be right or left leaning, without knowing its source. In more than 90% of the\ncases, their choice con\frmed the label that we had assigned to the article based\non the subreddit it came from, thus proving the viability of our approach. We\nbelieve these results could be improved by analyzing a submission's popularity,\ndiscarding those with low or negative ratings.\nRegarding the third question, we described in section 6.4 our approach to the\nproblem of identifying claims inside a text and the subsequent research and\nanalysis of their related evidence.\nFor the claim detection task, we introduced a new dataset, built automatically\nrather than manually, on which we trained a BERT model, before showing the\nset up of a crowdsourcing experiment to build a manually labeled dataset to be\nused for testing. Our model's performances on this data were comparable, if\nnot better, to those obtained by other papers on the topic, demonstrating the\nquality of our strategy. In addition to that, we highlighted the fact that our\ntraining datasets could be extended with relatively low e\u000bort, even comprising\nnew languages, contrary to the manual datasets employed in most of those\npapers. All in all, the outcome of our experiment suggests that our approach\nwas successful and worthy of further studies.\nMore complex was the problem of coreference resolution, which we covered in\nsection 6.4.2. In our system, we took advantage of the out-of-the-box perfor-\nmances in masked word prediction of RoBERTa, an optimized version of BERT\npresented in Liu et al. 2019. These, combined with the use of spaCy1, allowed\n1https://spacy.io/\nCHAPTER 8. CONCLUSIONS 91\nus to create a prototype that we tested on the GAP dataset from Webster et\nal. 2018, beating its baseline and obtaining an accuracy comparable to that of\nSuresb 2020, one of the papers we surveyed. The prototype was still impre-\ncise when used in our real-world application, yet these results should encourage\nfurther research in this direction.\nAs explained in chapter 3, we felt it was important to study BERT's behaviour\nin a multilingual setting given that the spread of misinformation online is not\nlimited to English speaking countries, but is rather a worldwide issue. This\nled to the fourth research question, which we tackled in section 6.7, where we\nshowed our study on the subject. The results from our experiments seemed to\nsuggest that having a multilingual dataset doesn't necessarily improve BERT's\nperformances, with models trained exclusively on monolingual samples obtain-\ning similar accuracy values. However, they also con\frmed that having a single\nBERT multilingual model doesn't cause any substantial loss in performances\nwith respect to using several monolingual ones and that the latter are systemat-\nically outperformed when tested on languages outside of the training data, two\nuseful information to take into consideration when designing a text classi\fcation\nsystem.\nOur opinion is that research in the area should be encouraged, in order to fully\nunderstand BERT's potentialities.\nFinally, the last question addressed BERT's performances in a multitask setting.\nThe results we obtained were clear in con\frming that \fne-tuning a BERT model\nin parallel on di\u000berent tasks helps in improving its accuracy on all the tasks\ninvolved, which, again, is an useful information to have when planning the\ntraining of a system. Based on these results, we believe that discovering and\nimplementing more techniques on how to perform multi-task training could\nsigni\fcantly improve the quality of BERT based classi\fers.\n8.1 Future works\nIt's unlikely that fake news and misinformation will disappear in the near future.\nOn the contrary, the number of conspiracy theories and hoaxes has increased\nsteadily during the pandemic, facilitated by social networks like Facebook, Twit-\nter and Reddit. For this reason, the problem of fake news detection will become\nCHAPTER 8. CONCLUSIONS 92\nmore and more relevant in the future, with a particular spotlight on the creation\nof automated systems.\nWe believe that each of the points we have tackled during this thesis can be\nimproved through deeper investigation and greater resources. In the following\nlist, we outline some of the points we consider more important:\n\u2022Increasing datasets size : we stressed more than once that the main\nlimitation for studies on this subject comes from the lack of good-quality\ndatasets. In our work, we have proposed new ways of building them, con-\n\frming the quality of our approach through experiments. However, given\nour time constraints, we didn't take fully advantage of all the available\nresources. Reddit alone could be scraped to obtain millions of labeled\nsubmissions, which is why we believe that replicating our strategy on a\nlarger scale should be the \frst step for improving the quality of our system.\n\u2022Creating baselines : we showed how, for many classi\fers, it was di\u000ecult\nto compare performances due to the lack of baselines and external data to\ntest them on. One way to \fx this could be resorting to crowdsourcing to\nbuild moderately large datasets (in the order of 10,000 rows) to be used\nfor testing, as we did for the claim detection classi\fer.\n\u2022Extending to multimodal classi\fcation : in section 4.1.3, we talked\nabout the importance of memes in in\ruencing online political discussion,\nsuggesting the introduction of a classi\fcation layer between political and\napolitical ones. However, since we decided to focus this thesis on BERT\nand text classi\fcation, we left this idea aside. Nevertheless, we believe\nthat multimodal analysis will play an increasingly important role in fake\nnews detection in the future, so this could be the \frst way of integrating it\ninto our system. We leave in the appendix a list of subreddits that might\nbe useful for this purpose.\n\u2022Introducing a satire detector : in our original taxonomy, satire was\ndetected by looking at a content source, establishing whether this was a\nsatirical publisher or not. Since doing this can be di\u000ecult, we think that\nit could be possible to obtain similar results by training a classi\fer on the\ntask of satire detection.\n\u2022Introducing a hoax detector : in our system, the factuality of a claim\nis established by searching for evidence online. Although this approach\nCHAPTER 8. CONCLUSIONS 93\nis generally e\u000bective, especially with speeches or news articles, it can face\nissues with hoaxes that have been generated too recently. We tackled the\nissue by analysing the quality of writing in a text, dividing news pieces into\nprofessional ones, generally reliable, and unprofessional ones, generally\nunreliable. However, it would be interesting to investigate whether BERT,\nor other models, can detect patterns speci\fc to hoaxes, independently from\nthe quality of their writing or the results of a fact-checking process.\n\u2022Real-time analysis : one of the most immediate applications we could\nthink of for our classi\fer was the fact-checking of speeches from political\ndebates or rallies. To make it more e\u000bective on this task, our idea is to\npair it with a voice-to-text system, in order to obtain analysis on what's\nbeing said in real-time, thus providing evidence against disinformation in\nthe same moment in which this is spoken.\n\u2022Test with a journalist : once our system has reached a su\u000eciently high\nlevel of accuracy, we would like to test it together with a journalist, or an\nexpert in the matter, to obtain a qualitative evaluation of our work, in\norder to understand its \raws, strengths and spaces for improvement.\nBibliography\nAtanasova, P. et al. (2019). \\Overview of the CLEF-2019 CheckThat! Lab: Auto-\nmatic Identi\fcation and Veri\fcation of Claims. Task 1: Check-Worthiness.\"\nIn:Working Notes of CLEF 2019 - Conference and Labs of the Evaluation\nForum, Lugano, Switzerland, September 9-12, 2019 . Ed. by L. Cappellato\net al. Vol. 2380. CEUR Workshop Proceedings. CEUR-WS.org. url:http:\n//ceur-ws.org/Vol-2380/paper%5C_269.pdf .\nBowman, S. R. et al. (2015). \\A large annotated corpus for learning natural\nlanguage inference.\" In: Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing .doi:10.18653/v1/d15- 1075 .\nurl:http://dx.doi.org/10.18653/v1/D15-1075 .\nBudak, C., S. Goel, and J. Rao (Jan. 2016). \\Fair and Balanced? Quantifying\nMedia Bias through Crowdsourced Content Analysis.\" In: Public Opinion\nQuarterly 80, pp. 250{271. doi:10.1093/poq/nfw007 .\nCai, C., L. Li, and D. Zeng (2017). \\Detecting Social Bots by Jointly Model-\ning Deep Behavior and Content Information.\" In: Proceedings of the 2017\nACM on Conference on Information and Knowledge Management . CIKM\n'17. Singapore, Singapore: Association for Computing Machinery, pp. 1995{\n1998. isbn: 9781450349185. doi:10.1145/3132847.3133050 .url:https:\n//doi.org/10.1145/3132847.3133050 .\nCastillo, C., M. Mendoza, and B. Poblete (Oct. 2013). \\Predicting information\ncredibility in time-sensitive social media.\" In: Internet Research: Electronic\nNetworking Applications and Policy 23.doi:10.1108/IntR-05-2012-0095 .\nCiampaglia, G. et al. (Oct. 2015). \\Computational Fact Checking from Knowl-\nedge Networks (vol 10, e0128193, 2015).\" In: PLoS ONE 10.doi:10.1371/\njournal.pone.0141938 .\nConneau, A. et al. (2018). \\XNLI: Evaluating Cross-lingual Sentence Represen-\ntations.\" In: Proceedings of the 2018 Conference on Empirical Methods in\n94\nBIBLIOGRAPHY 95\nNatural Language Processing .doi:10.18653/v1/d18- 1269 .url:http:\n//dx.doi.org/10.18653/v1/D18-1269 .\nDevlin, J. et al. (2019). In: Proceedings of the 2019 Conference of the North .doi:\n10.18653/v1/n19-1423 .url:http://dx.doi.org/10.18653/v1/N19-\n1423 .\nFavano, L. (2019). \\Identifying Fake News By Learning To Predict Whether\nTextual Evidence Supports or Refutes its Claims.\" In: url:https://www.\npolitesi.polimi.it/handle/10589/149858 .\nFavano, L. and M. Carman (2019). \\Multi-Task Learning for Multi-Lingual\nClaim Checking.\" In:\nFillion, E. et al. (2020). \\Happy Transformer.\" In:\nHassan, N., C. Li, and M. Tremayne (Oct. 2015). \\Detecting Check-worthy Fac-\ntual Claims in Presidential Debates.\" In: Proceedings of the 24th ACM Inter-\nnational Conference on Information and Knowledge Management (CIKM) ,\npp. 1835{1838. doi:10.1145/2806416.2806652 .\nHonnibal, M. and I. Montani (2017). \\spaCy 2: Natural language understand-\ning with Bloom embeddings, convolutional neural networks and incremental\nparsing.\" To appear.\nHorne, B. D., J. Norregaard, and S. Adali (2019). Di\u000berent Spirals of Sameness:\nA Study of Content Sharing in Mainstream and Alternative Media . arXiv:\n1904.01534 [cs.CY] .\nLiu, Y. et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Ap-\nproach . arXiv: 1907.11692 [cs.CL] .\nMa, J., W. Gao, and K.-F. Wong (July 2018). \\Rumor Detection on Twit-\nter with Tree-structured Recursive Neural Networks.\" In: Proceedings of the\n56th Annual Meeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers) . Melbourne, Australia: Association for Computational\nLinguistics, pp. 1980{1989. doi:10 . 18653 / v1 / P18 - 1184 .url:https :\n//www.aclweb.org/anthology/P18-1184 .\nMaiya, A. S. (2020). ktrain: A Low-Code Library for Augmented Machine Learn-\ning. arXiv: 2004.10703 [cs.LG] .\nMolina, M. et al. (Oct. 2019). \\\\Fake News\" Is Not Simply False Information: A\nConcept Explication and Taxonomy of Online Content.\" In: American Be-\nhavioral Scientist , p. 000276421987822. doi:10.1177/0002764219878224 .\nNakamura, K., S. Levy, and W. Y. Wang (2019). \\r/Fakeddit: A New Multi-\nmodal Benchmark Dataset for Fine-grained Fake News Detection.\" In: arXiv\npreprint arXiv:1911.03854 .\nBIBLIOGRAPHY 96\nPires, T., E. Schlinger, and D. Garrette (2019). \\How Multilingual is Multilin-\ngual BERT?\" In: Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics .doi:10.18653/v1/p19-1493 .url:http:\n//dx.doi.org/10.18653/v1/P19-1493 .\nPotthast, M. et al. (Feb. 2017). \\A Stylometric Inquiry into Hyperpartisan and\nFake News.\" In:\nProch\u0013 azka, O. and J. Blommaert (2019). \\Ergoic framing in New Right online\ngroups: Q, the MAGA kid, and the Deep State theory.\" In:\nPryzant, R. et al. (Apr. 2020). \\Automatically Neutralizing Subjective Bias\nin Text.\" In: Proceedings of the AAAI Conference on Arti\fcial Intelligence\n34.01, pp. 480{489. issn: 2159-5399. doi:10.1609/aaai.v34i01.5385 .url:\nhttp://dx.doi.org/10.1609/aaai.v34i01.5385 .\nRadford, A. et al. (2019). \\Language Models are Unsupervised Multitask Learn-\ners.\" In:\nRibeiro, M., S. Singh, and C. Guestrin (Feb. 2016). \\\\Why Should I Trust\nYou?\": Explaining the Predictions of Any Classi\fer.\" In: pp. 97{101. doi:\n10.18653/v1/N16-3020 .\nRosenblum, D. (2007). \\What Anyone Can Know: The Privacy Risks of Social\nNetworking Sites.\" In: IEEE Security Privacy 5.3, pp. 40{49. doi:10.1109/\nMSP.2007.75 .\nShao, C. et al. (Nov. 2018). \\The spread of low-credibility content by social\nbots.\" In: Nature Communications 9.1.issn: 2041-1723. doi:10 . 1038 /\ns41467-018-06930-7 .url:http://dx.doi.org/10.1038/s41467-018-\n06930-7 .\nSpangher, A., N. Peng, and E. Ferrara (2019). \\Modeling \\Newsworthiness\" for\nLead-Generation Across Corpora.\" In:\nSuresb, A. (2020). \\BERT for Coreference Resolution.\" In:\nTandoc, E., Z. Lim, and R. Ling (Aug. 2017). \\De\fning \\Fake News\": A ty-\npology of scholarly de\fnitions.\" In: Digital Journalism 6, pp. 1{17. doi:\n10.1080/21670811.2017.1360143 .\nTwenge, J. M. et al. (2018). \\Increases in Depressive Symptoms, Suicide-Related\nOutcomes, and Suicide Rates Among U.S. Adolescents After 2010 and Links\nto Increased New Media Screen Time.\" In: Clinical Psychological Science\n6.1, pp. 3{17. doi:10.1177/2167702617723376 . eprint: https://doi.\norg / 10 . 1177 / 2167702617723376 .url:https : / / doi . org / 10 . 1177 /\n2167702617723376 .\nBIBLIOGRAPHY 97\nVaswani, A. et al. (2018). \\Tensor2Tensor for Neural Machine Translation.\" In:\nCoRR abs/1803.07416. url:http://arxiv.org/abs/1803.07416 .\nVosoughi, S., D. Roy, and S. Aral (2018). \\The spread of true and false news\nonline.\" In: Science 359.6380, pp. 1146{1151. issn: 0036-8075. doi:10.1126/\nscience.aap9559 . eprint: https://science.sciencemag.org/content/\n359/6380/1146.full.pdf .url:https://science.sciencemag.org/\ncontent/359/6380/1146 .\nWang, W. (Jan. 2017). \\\"Liar, Liar Pants on Fire\": A New Benchmark Dataset\nfor Fake News Detection.\" In: pp. 422{426. doi:10.18653/v1/P17-2067 .\nWang, Y. et al. (2018). \\EANN: Event Adversarial Neural Networks for Multi-\nModal Fake News Detection.\" In: Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining . ACM,\npp. 849{857.\nWebster, K. et al. (2018). \\Mind the GAP: A Balanced Corpus of Gendered\nAmbiguou.\" In: Transactions of the ACL , to appear.\nWilliams, A., N. Nangia, and S. Bowman (2018). \\A Broad-Coverage Challenge\nCorpus for Sentence Understanding through Inference.\" In: Proceedings of\nthe 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers) . New Orleans, Louisiana: Association for Computational Linguistics,\npp. 1112{1122. url:http://aclweb.org/anthology/N18-1101 .\nZannettou, S. et al. (Nov. 2017). \\The web centipede.\" In: Proceedings of the\n2017 Internet Measurement Conference .doi:10.1145/3131365.3131390 .\nurl:http://dx.doi.org/10.1145/3131365.3131390 .\nZellers, R. et al. (2019). Defending Against Neural Fake News . arXiv: 1905.\n12616 [cs.CL] .\nZhang, J., B. Dong, and P. S. Yu (2018). FAKEDETECTOR: E\u000bective Fake\nNews Detection with Deep Di\u000busive Neural Network . arXiv: 1805 . 08751\n[cs.SI] .\nZhou, X. and R. Zafarani (Dec. 2018). \\A Survey of Fake News: Fundamental\nTheories, Detection Methods, and Opportunities.\" In:\n| (Nov. 2019). \\Network-based Fake News Detection.\" In: ACM SIGKDD\nExplorations Newsletter 21.2, pp. 48{60. issn: 1931-0153. doi:10.1145/\n3373464.3373473 .url:http://dx.doi.org/10.1145/3373464.3373473 .\nZubo\u000b, S. (2019). The age of surveillance capitalism . Pro\fle books.\nAppendix A\nTechnical details\nIn this chapter we give a technical overview of the creation process for some\nof the datasets presented in chapter 5. In all cases, the scraping was carried\nout between August and October 2020 (future replications may yield di\u000berent\nresults). The code used during this thesis can be found on GitHub1.\nA.1 Scraping Reddit\nOne of the reasons why we decided to focus on Reddit, whose number of users\nis hundreds of millions below that of Facebook or Instagram2, is how simple it\nis to scrape it. This can be done directly from Reddit, by creating a developer\naccount, or through the Pushshift API3. We chose to use the latter as it o\u000bered\nless constraints and an easier implementation to download large amounts of\ndata.\nFrom Reddit, we could retrieve links to thousands of articles. To scrape them,\nwe employed the newspaper3k4library, which is able to automatically detect,\ninside an online article, information such as its title, body, publisher and so on.\nIn table A.1, we report the list of subreddits used to create all the news datasets\npresented in the thesis. In table A.2, we report instead the list of the subreddits\nused in the multilingual experiment in section 6.7. To be noted that, in some\n1https://github.com/ste\ryx\n2https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-\nof-users/\n3https://pushshift.io/\n4https://newspaper.readthedocs.io/en/latest/\n98\nAPPENDIX A. TECHNICAL DETAILS 99\nsubreddits , moderators and users can use \\ \rairs \" to indicate whether a post is\ndiscussing a speci\fc sub-theme (for example, the subreddit r/Italy5has \rairs\nfor foreign news, sport, discussion, etc). This means that, when available, we\ncould use \rairs to scrape Reddit in a precise manner even when we had to deal\nlarge subreddits (for example, using the \rair \\Politik \", we were able to \fnd\nsubmissions about politics in German from r/de, a generic community for all\nGerman-speaking users).\nSubreddit Dataset\nr/news High-quality news\nr/InTheNews Opinion pieces\nr/savedyouaclick Low-quality news\nr/qualitynews High-quality news\nr/conservative Right-leaning news\nr/progressive Left-leaning news\nr/democrats Left-leaning news\nr/liberal Left-leaning news\nr/voteblue Left-leaning news\nr/sandersforpresident Left-leaning news\nTable A.1: List of subreddits used to build datasets of news articles.\nWe then show in table A.3 a possible list of subreddits that might be used as\nsources for political and apolitical memes .\nA.2 Scraping fact-checking websites\nWe show here the details for the creation of the dataset of fact-checking articles\npresented in section 5.3.2.\nA.2.1 Creating a list of fact-checking websites\nIn this step we made a series of queries to the Google Fact-Check API6to create\na list of fact-checking websites around the world. The queries were made using\nas keywords names of politicians and were performed in two separate versions\n(in order to obtain a \fnal list as variegate as possible):\n\u2022query: <name of politician >\n5https://www.reddit.com/r/italy/\n6https://toolbox.google.com/factcheck/explorer\nAPPENDIX A. TECHNICAL DETAILS 100\nSubreddit Language Category\nr/politics English Politics\nr/science English Science\nr/soccer English Sports\nr/games English Games\nr/politicaITA Italian Politics\nr/scienzaItalia Italian Science\nr/ItalyCalcio Italian Sports\nr/gdr Italian Games\nr/ItalianGaming Italian Games\nr/de (\rair: \"politik\") German Politics\nr/wissenschaft German Science\nr/physik German Science\nr/bundesliga German Sports\nr/zocken German Games\nr/Mexico news (\rair: \"politica\") Spanish Politics\nr/ciencia Spanish Science\nr/futbol Spanish Sports\nr/fulbo Spanish Sports\nr/futbolmx Spanish Sports\nr/Argaming Spanish Games\nr/Brasil (\rair: \"politica\") Portuguese Politics\nr/Portugal (\rair: \"politica\") Portuguese Politics\nr/politicaBrasileira Portuguese Politics\nr/futebol Portuguese Sports\nr/corinthias Portuguese Sports\nr/cienciabrasil Portuguese Science\nr/gamesEcultura Portuguese Games\nTable A.2: List of subreddits used in the multilingual experiment.\nSubreddit Category\nr/memes Apolitical\nr/theLeftCantMeme Left-leaning\nr/theRightCantMeme Right-leaning\nr/conspiracyMemes Conspiracy theories\nTable A.3: A possible list of political and apolitical subreddits dedicated to\nsharing memes .\n\u2022query: <name of politician >, langCode: <code of the language spoken\nby politician >\nWe used the following names of politicians (names are reported divided by\nAPPENDIX A. TECHNICAL DETAILS 101\ncountry):\n\u2022Italy (`it'): 'Conte', 'Salvini', 'Renzi', 'Berlusconi'\n\u2022US (`en'): 'Trump', 'Biden', 'Sanders', 'Harris'\n\u2022UK (`en'): 'Johnson', 'Corbyn', 'Sturgeon', 'Farage'\n\u2022France (`fr'): 'Macron', 'Le Pen', 'M\u0013 elenchon', 'Hollande'\n\u2022Spain (`es'): 'S\u0013 anchez', 'Rajoy', 'Puigdemont', 'Iglesias'\n\u2022Germany (`de'): 'Merkel', 'Shulz', 'Kurz', 'Habeck'\n\u2022Brazil (`pt'): 'Bolsonaro', 'Alckimin', 'Suplicy', 'Cabral'\n\u2022India (`hi'): 'Modi', 'Priyanka Gandhi', 'Amit Shah', 'Mayawati'\n\u2022Canada (`en'): 'Trudeau', \"O'Toole\", 'Blanchet', 'Singh'\n\u2022M\u0013 exico (`es'): 'L\u0013 opez', 'Pe~ na Nieto', 'Calder\u0013 on'\n\u2022Australia ('en'): 'Morrison', 'Albanese', 'Marshall', 'Hodgman'\n\u2022Argentina ('es'): 'Kirchner', 'Macri'\n\u2022Arab-speaking countries ('ar'): 'Tunisia', 'Egypt', 'Saudi Arabia'\n\u2022Israel ('iw'): 'Netanyahu', 'Gantz'\nThis returned a list of 94 separate websites: 'facta.news', 'pagellapolitica.it', 'bu-\ntac.it', 'fullfact.org', 'rappler.com', 'agi.it', 'cekfakta.tempo.co', 'indiatoday.in',\n'checkyourfact.com', 'open.online', 'lavoce.info', 'repubblica.it', 'factcheck.afp.com',\n'snopes.com', 'misbar.com', 'politifact.com', 'polygraph.info', 'washingtonpost.com',\n'factcheck.org', 'bbc.co.uk', 'newswise.com', 'leadstories.com', 'sciencefeedback.co',\n'newsmobile.in', 'boomlive.in', 'factcheck.thedispatch.com', 'newsmeter.in', 'cb-\nsnews.com', 'nytimes.com', 'thelogicalindian.com', 'newschecker.in', 'thejour-\nnal.ie', 'vishvasnews.com', 'theconversation.com', 'africacheck.org', 'channel4.com',\n'theferret.scot', 'factly.in', 'vera\fles.org', 'liberation.fr', 'factuel.afp.com', 'lemonde.fr',\n'20minutes.fr', 'lejdd.fr', 'factual.afp.com', 'maldita.es', 'newtral.es', 'efe.com',\n'chequeado.com', 'colombiacheck.com', 'correctiv.org', 'dpa-factchecking.com',\n'presseportal.de', 'faktistfakt.com', 'derstandard.at', 'br.de', 'politica.estadao.com.br',\n'piaui.folha.uol.com.br', 'bol.uol.com.br', 'poligrafo.sapo.pt', 'noticias.uol.com.br',\nAPPENDIX A. TECHNICAL DETAILS 102\n'aosfatos.org', 'boatos.org', 'observador.pt', 'checamos.afp.com', 'projetocom-\nprova.com.br', 'apublica.org', 'hindi.asianetnews.com', 'hindi.boomlive.in', 'alt-\nnews.in', 'hindi.newschecker.in', 'aajtak.in', 'hindi.thequint.com', 'factcrescendo.com',\n'bbc.com', 'aajtak.intoday.in', 'scroll.in', 'factscan.ca', 'animalpolitico.com', 'ver-\ni\fcado.com.mx', 'veri\fcado.mx', 'abc.net.au', 'aap.com.au', 'factcheck.aap.com.au',\n'fatabyyano.net', 'fakty.afp.com', 'thewhistle.globes.co.il'.\nA.2.2 Creating a list of claims and articles links\nIn this step, we queried the Google Fact Check API using as keyword the name\nof each of the websites found in the previous step. For each of them, we gathered\nevery article that the API returned.\nThe resulting dataset was characterized by:\n\u2022Length : 61,164 rows\n\u2022Columns : claim, claimant, claimDate, url, reviewTitle, reviewDate, Rat-\ning, languageCode, publisherName, publisherSite\n\u2022Languages : 33 ('fr', 'hi', 'en', 'sw', 'yo', 'af', 'te', 'ta', 'bn', 'gu', 'mr',\n'ml', 'kn', 'es', 'pt', 'pt-pt', 'pa', 'de', 'it', 'id', 'ar', 'ms', 'pl', 'sk', 'nl', 'th',\n'si', 'zh', 'ru', 'kk', 'iw', 'ur', 'or')\n\u2022Publishers : 94 (the same as in the previous paragraph)\nA.2.3 Scraping the articles\nIn these two steps, we gathered a series of urls pointing to fact-checking articles\non the web. The Google Fact-Check API retrieved many information on them,\nbut not their entire content. Therefore, we resorted to scraping each of the\narticles found thus far by ourselves.\nFor the purposes of this experiment, in each of the articles we had to separate\nthe fact-checked claim from the fact-checking part of the article. For this reason,\nwe couldn't use the newspaper3k library used in section A.1, but had to resort\nto a \\manual\" scraping.\nTo understand how this process was executed, we present here the details for\none of the publishers, pagellapolitica.it (an Italian fact-checking newspaper):\nAPPENDIX A. TECHNICAL DETAILS 103\n1. The \frst step was to look at the info the Google FactCheck API was\ngiving us. From here, we were able to tell that the newspaper was only\npublishing in Italian and that our dataset contained 1,209 of its articles.\n2. Before actually scraping the website, we made sure that all the info we had\nwere accurate and useful. Many of the websites were giving Google wrong\ninformation that had to be corrected by inspecting the articles, while\nothers were using rating systems not easily translatable into a False-True\nscale (for example factual.afp.com didn't use ratings, but rather comments\nto the claims). In the case of pagellapolitica.it , the claim given to Google\nwasn't correct. Apparently the newspaper was giving the article title\nrather than the claim itself, so we had to \fx this in the following steps.\n3. At this point, we started scraping. In order to do this, we needed to\n\frst gain familiarity with how the articles were structured. In Figure A.1,\nthere is an article we used as example7. In red, we highlighted its main\ncomponents. Using a browser's developer functions, we could see which\nHTML tags surrounded the information we wanted to extract. In this\ncase:\n\u2022Claimant, claim and article body were all contained in p tags inside a\ndiv identi\fed by class \\ col-lg-9 mb-9 mb-lg-0 \". The \frst two elements\nwere recognizable by further classes speci\fc to them, while the body\nhad no class (but was composed of multiple ptags).\n\u2022The link to the claim was identi\fed by an atag with a unique class\n\\u-link-muted \".\n\u2022The main image was inside the same divwhere we were able to \fnd\nthe article body, so we could just collect the \frst image we found\ninside (likely the \frst image is the most important one in an article).\n\u2022We didn't need it in this case, since the FactCheck API already gave\nit to us, but the article title was identi\fed by a span tag.\n4. Using the BeautifulSoup library, we used the knowledge gathered in the\nprevious point to scrape all the needed information.\nSince this task was quite time-consuming (not only to set up, but also because\nmultiple requests to the same website required a few seconds interval between\n7https://pagellapolitica.it/dichiarazioni/8706/fondi-assunzioni-e-banchi-speranza-da-\nnumeri-corretti-sulla-scuola\nAPPENDIX A. TECHNICAL DETAILS 104\nFigure A.1: Example from one of the articles in the dataset. In red, we high-\nlighted the sections we're interested in.\neach other), we only worked with the 23 publishers that produced the greatest\nnumber of articles in our datasets (amounting to roughly two thirds of the ones\nobtained from the FactCheck API). These publishers were:\n\u2022Universo Online8\n\u2022Vishvas News9\n8piaui.folha.uol.com.br\n9https://www.vishvasnews.com/english/\nAPPENDIX A. TECHNICAL DETAILS 105\n\u2022Altnews10\n\u2022Factcheck AFP11\n\u2022Fullfact12\n\u2022Leadstories13\n\u2022Factly14\n\u2022Poligrafo15\n\u2022Snopes16\n\u2022Misbar17\n\u2022Factcheck.org18\n\u2022Newtral19\n\u2022Factual AFP20\n\u2022Factuel AFP21\n\u2022CheckYourFact22\n\u2022Pagella Politica23\n\u2022AosFatos24\n\u2022Boatos25\n\u2022The Washington Post26\n10https://www.altnews.in/\n11https://factcheck.afp.com/\n12https://fullfact.org/\n13https://leadstories.com/\n14https://factly.in/\n15https://poligrafo.sapo.pt/\n16https://www.snopes.com/\n17https://misbar.com/\n18https://www.factcheck.org/\n19https://www.newtral.es/\n20https://factual.afp.com/afp-factual\n21https://factual.afp.com/afp-factual\n22https://checkyourfact.com/\n23https://pagellapolitica.it/\n24https://www.aosfatos.org/\n25https://www.boatos.org/\n26https://www.washingtonpost.com/\nAPPENDIX A. TECHNICAL DETAILS 106\n\u2022Politifact27\n\u2022Politica Estadao28\n\u2022Chequeado29\n\u2022Boomlive30\nOf these, we had to drop The Washington Post because it was protecting its\narticles behind a paywall, making them unaccessible to us. The remaining pub-\nlishers were all used, although some rows were lost in the scraping process, while\nothers were lost later because their ratings couldn't be transformed in a uniform\ntruth scale. The scraping of Politifact was integrated with the dataset described\nin section 5.3.1. The \fnal dataset (including Politifact articles) contained 52,644\nentries.\n27https://www.politifact.com/\n28https://politica.estadao.com.br/\n29https://chequeado.com/\n30https://www.boomlive.in/", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Automated techniques for identifying fake news and assisting fact checkers", "author": ["S Agresti"], "pub_year": "2019", "venue": "NA", "abstract": "One of the most worrying issues of our age is the spread of online misinformation. This  problem is affecting our society heavily, transforming political discussion into a relentless battle"}, "filled": false, "gsrank": 21, "pub_url": "https://www.politesi.polimi.it/handle/10589/169460", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:3l0y4ycsyrkJ:scholar.google.com/&output=cite&scirp=20&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D20%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=3l0y4ycsyrkJ&ei=CLWsaLmrCI6IieoP0sKRuAk&json=", "num_citations": 2, "citedby_url": "/scholar?cites=13387561392139296222&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:3l0y4ycsyrkJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.politesi.polimi.it/bitstream/10589/169460/3/Thesis_Stefano_Agresti.pdf"}}, {"title": "Analysis of Bias and Reliability in LLM-Powered Search Engines", "year": "NA", "pdf_data": "MASARYK \nUNIVERSITY \nFACULTY OF INFORMATICS \nAnalysis of Bias and Reliability in \nLLM-Powered Search Engines \nBachelor's Thesis \nMARTINA STANKOVICOVA \nBrno,  Spring 2025 \n\nMASARYK \nUNIVERSITY \nFACULTY OF INFORMATICS \nAnalysis of Bias and Reliability in \nLLM-Powered Search Engines \nBachelor's Thesis \nMARTINA STANKOVICOVA \nAdvisor: Mgr. Tomas Foltynek, Ph.D. \nDepartment of Machine Learning and Data Processing \nBrno,  Spring 2025 \n\n\nDeclaration \nHereby  I declare that this paper is my  original  authorial  work,  which \nI have  worked  out on my  own.  All sources, references, and literature \nused  or excerpted  during  elaboration of this  work  are properly  cited \nand listed in complete  reference  to the due source. \nDuring  the preparation of this thesis, I used the  following  AI tools: \n\u2022 Grammarly  for grammar corrections \n\u2022 ChatGPT  to help  with  understanding  code errors and  writing \nclearer code \nI declare that I used  these  tools in accordance  with  the  principles  of \nacademic  integrity.  I checked the content and took  full  responsibility \nfor it. \nMartina  Stankovicov\u00e4 \nAdvisor:  Mgr.  Tom\u00e1\u0161 Folt\u00fdnek,  Ph.D. \niii \n\nAcknowledgements \nFirstly,  I would  like  to thank my advisor,  Mgr.  Tom\u00e1\u0161 Folt\u00fdnek,  Ph.D., \nfor all  the advice  he has  given  me regarding this thesis,  as well  as for \nanswering  all of my  many questions.  I would  also like  to thank  my \nfriends  and family  for supporting  me during  my  studies and while  i \nwas  writing  this thesis. \niv \n\nAbstract \nThe rise of Large Language Models cannot be underestimated. They \nare slowly becoming a part of almost everything, especially on the \ninternet.  Naturally,  this includes search engines. Many studies have \nbeen conducted about the bias of  LLMs  and their  effects  on search\u00ad\ning behaviors. This thesis aims to closely examine three of the most \npopular  LLM-based  search engines: Perplexity, You.com , and Brave \nSearch Summarizer. It  will  focus on the  reliability  and  political  bias of \nthe sources they  provide.  Both of  these  metrics were measured using \nratings  provided  by the  Media  Bias Fact Check database, using their \nbias, factuality and  credibility  ratings.  Along  with  these  main metrics, \nbias towards specific countries and domains was also examined. 478 \nqueries were  sent  to each of  these  search engines, both posted by stu\u00ad\ndents and created  artificially,  and the responses were collected  with \nthe help of a web application.  These  responses were then analyzed, \nand the overall results were gathered. From the results, it can be de\u00ad\ntermined  that there is a noticeable Left-leaning bias in all three search \nengines. However, the most substantial bias can be seen in You.com , \nwith  a value of 0.42, being on the verge of Left-Center bias. The least \nbiased engine was  Perplexityai,  with  0.46, showing the  best  ratio of \nRight-leaning  to Left-leaning sources and the  best  source diversity, \nwith  40% of the domains being unique. In contrast to this, Perplexity \nwas also the least credible of the three models  (62%),  with  You.com \nleading  (70%).  Brave Search Summarizer showed a slight tendency \nto land on the Extreme sides of the bias spectrum. However, most of \nits results were center-biased.  All three engines also showed a strong \nbias towards sources from the  USA,  with  over 70% of sources being \nAmerican.  The queries were also sorted into categories, and those \nwere separately analyzed. Most of the results were very similar to the \noverall  analysis.  However,  some categories, such as LGBTQ+, had sur\u00ad\nprising  bias results. Based on this analysis, two distinct approaches to \nsource  picking  can be seen. One focuses on a broad spectrum, having \nthe least bias but also the least  credibility,  and the other is based on a \nsmaller amount of domains but  with  higher credibility  within  them. \nNeither  approach could be considered  better,  as each had issues that \ncould  be regarded as non-ethical or supporting of societal polariza-\nv \n\ntion.  Therefore, it was concluded that the  primary  focus  should  be on \nminimizing  such issues by  combining  both approaches,  which  none \nof the examined  LLM-based  search engines have been shown to do \nsufficiently \nKeywords \nLLM,  LLM-powered  search engines, AI ethics,  LLM  bias,  LLM  relia\u00ad\nbility,  PHP,  AI, perplexity.ai,  you.com , brave search summarizer \nvi \n\nContents \nIntroduction  1 \n1 Literature  Review  3 \n2 Theoretical Background 5 \n2.1 Search Engine Architecture 5 \n2.2 Large Language Models  (LLMs)  6 \n2.3 Retrieval Augmented Generation (RAG) 6 \n2.4 Echo Chambers and Filter Bubbles 7 \n2.4.1 Societal Polarization 7 \n2.5 News Sources 8 \n3 LLM-based  Search  Engine  Overview  9 \n3.1 LLM-based  Search Engines 9 \n3.2 Difference from Regular Search Engines 9 \n3.3 Search Engines  With  Summaries 9 \n3.3.1 Google 10 \n3.3.2 Bing 10 \n3.3.3 Arc 10 \n3.3.4 Brave 11 \n3.3.5 DuckDuckGo 11 \n3.4 Chatbots  with  Search Functionality 11 \n3.4.1 You.com  12 \n3.4.2 ChatGPT Search 12 \n3.4.3 Deepseek Search 12 \n3.4.4 Perplexity 13 \n4 Databases  on Credibility  of News  Sources  14 \n4.1 Media  Bias Fact Check 14 \n4.2 AllSides 14 \n4.3 Ad Fontes  Media  15 \n4.4 NewsGuard 15 \n5 Methodology 16 \n5.1 Chosen Search Engines 16 \n5.2 Database Specifics 17 \nvii \n\n5.3 Data  Acquisition  20 \n5.3.1 User-Based Data Collection 20 \n5.3.2 Data Collection Based on Trends 20 \n5.3.3 Categorizing Queries 21 \n5.3.4 Query Creation 22 \n5.4 Important Parameters 22 \n5.5 Data Structure 22 \n5.6 Value Assignment 23 \n5.7 Label Calculation 25 \n5.8 Other Collected Values 25 \n6 Web  Application  Development  27 \n6.1 Application  Outline 27 \n6.2 Used Tools 27 \n6.2.1 APIs 28 \n6.3 Data Processing 33 \n6.4 Datastorage  34 \n6.4.1 JSON Structure 34 \n6.5 Data  Visualization  35 \n7 Data  Analysis  37 \n7.1 Acquisition  Script 37 \n7.2 Analysis Script 37 \n7.2.1 Final Values 38 \n7.2.2 Output 38 \n8 Results  39 \n8.1 Overall Results 39 \n8.1.1 Bias, Factuality and  Credibility  41 \n8.2 Results By Category 46 \n9 Conclusions  52 \n9.1 Distinctions Between  LLM-Powered  Search Engines . . 52 \n9.2 Ethical Implications 53 \n9.3 Important Limitations 54 \nBibliography  55 \nviii \n\nList of Tables \n5.1 Bias Values and Labels 18 \n5.2 Factuality Values and Labels 19 \n5.3 Credibility  Values and Labels 19 \n5.4 Bias Value Representation 24 \n5.5 Factuality and  Credibility  Value Representation 25 \n8.1 Domains  with  most  occurrences  41 \n\nList of Figures \n6.1 An example of the bias bar 36 \n8.1 Overall  Bias  Distribution  43 \n8.2 Overall  Source Bias  Distribution  44 \n8.3 Overall  Factuality  Distribution  45 \n8.4 Overall Credibility Distribution  46 \n\nIntroduction \nSearch engines are a  vital  part of traversing the internet. They can \nbe used for basic questions, news  searches,  scientific research, and \na plethora of other purposes. Due to this, there is always a desire \nto improve upon the foundations of search engines, whether it is by \nintroducing  new features, adjusting the inner  workings,  or fine-tuning \nparameters. One of the newest features introduced into the  world \nof search engines is Large Language Models.  With  their rise in re\u00ad\ncent  years, most applications attempt to incorporate them somehow, \nand search engines are no different. Since their inception,  LLM-based \nsearch engines have been on the rise, led by  ones  such as You.com , \nPerplexityai,  or the plethora of different search summarizers inte\u00ad\ngrated into old-fashioned search engines. Even language models such \nas ChatGPT,  which  did not offer search functionality  previously,  have \nbeen joining the trend and are coming out  with  their personalized \nsearch systems. While the notion of automated, language model aug\u00ad\nmented search, complete  with  a summary, is undoubtedly exciting, \nthere are some uncertainties tied to it. The topic of bias in Large  Lan\u00ad\nguage Models has been a focal point of discussion in the machine \nlearning  field,  with  multiple types of bias emerging once extensive \ntesting was conducted [1, 2, 3]. This poses a question. Does such bias \ninfluence the answers and the provided sources of  these  LLM-based \nsearch engines? And if so, what is the  extent  of such influence? \nWhen  discussing the bias of  LLM-based  search engines, it is also im\u00ad\nportant to look at the unanswered bias of  traditional  search engines [4, \n5], which  has existed much longer than most Large Language Models. \nSuch bias also very  likely  influences  these  new types of search engines, \nspecifically  Perplexity, You.com , and Brave Search Summarizer. This \nthesis aims to gauge the bias and overall  reliability  of such search en\u00ad\ngines by  analyzing  the provided  sources and the different types of bias \nthey may carry. Such bias may be  political  bias, country bias, media \ntype bias, or specific domain bias. Furthermore, to gauge reliability, \nthe amount of  facts  and credibility of the sources  will  also need to be \nexamined.  These  results  will  then be used to determine the differences \nbetween the chosen models and the intensity of their bias.  Finally,  the \nimplications  of the found bias  will  be discussed, specifically from an \n1 \n\nINTRODUCTION \nethical standpoint. \nThe thesis  will  also provide a surface-level comparison of the most-\nused  LLM-based  search engines apart from the aforementioned three, \nas well  as the  databases  that may be used to gauge their bias and \nreliability.  The process of gathering the data on  which  the analysis \nwas conducted and the tools used for it  will  also be described in detail. \nThis includes the web application used for data gathering as  well  as \nthe script and metrics used for the analysis.  Finally,  the results  will  be \nvisualized  and described in detail. \n2 \n\n1 Literature  Review \nSearch engine bias and large language model bias are topics that have \noften been discussed in the last century.  With  the rise of a plethora of \ndifferent search engines, it  became  essential to understand the risks \nand the bias associated  with  them.  According  to E.Goldman[4], there \nare a plethora of  editorial  decisions that  create  bias in  traditional  search \nengines.  These  include the filtering of results,  which  may be done for \nany reason, and their  ranking.  Specifically, the  ranking  is often affected \nby the popularity  of the site,  which  may not be as relevant to the topic \nas sites  below it. This means that the  same  domains may stay at the top \nof search results, while others are left towards the bottom. A similar \nidea  is explored by S. Dai[2] et.al in \"Bias and Unfairness in Infor\u00ad\nmation  Retrieval Systems: New Challenges in the  LLM  Era\". Their \npaper outlines the different types of bias and unfairness that show \nup in IR systems, often used by search engines.  These  types of bias \nconcern Large Language Models specifically, and discuss their usage \nin such systems. The most relevant categories include - bias based on \nsource position in database, bias based on  popularity;  similar to the \nbias mentioned by Goldman[4] or style bias, where sources may be \npicked  based on their  writing  style instead of relevancy. Another bias \ntype very similar to the style bias is the so-called \"egocentric bias\". It \ndescribes the preference of Large Language Models to  picking  sources, \nwhich  were also generated by themselves or a similar model, as they \nconsider such sources superior due to the way the  text  is structured. \nHowever,  the bias of large language models was proven to go even \ndeeper than this. E.M.Bender et.al. [3] described the issues  which \ncurrently  plague Large Language Models.  These  ranged from  envi\u00ad\nronmental considerations and data quantity to bias. When it comes \nto bias, it is tough to  detect  and mitigate due to the  sheer  amount of \ntraining  data. Attempts at doing so, for example, filtering out certain \nwords  or phrases, may also filter out words that belong to specific \nsubcultures. Furthermore, the distribution of the data means that \nviewpoints  shared by the majority of users of certain  sites  may be \nmore represented than they actually are outside of the internet. This \nmay introduce bias towards such viewpoints into the model.  Finally, \ndue to the  fact  that models tend to keep their sources in their database, \n3 \n\nl. LITERATURE  REVIEW \nnewer understandings of  words  or phrases, or current events may take \ntime to show up in the results  provided  by the Language Models. \nWhen  it comes to the user interaction  with  these  Language Models, \nan experiment conducted by N.Sharma  et.al.[6]  proved that biased \nlanguage models support the  polarization  of their users, as those  same \nusers are much more  likely  to input confirmation-seeking questions \ninto the language model once they discover its bias. Even without an \ninherent bias, conversational language models provide more confir\u00ad\nmation  to users than  traditional  search engines, as their conversational \nnature makes it easier for people to  seek  confirmation. This is also \npartly  due to the  fact  that people were shown to apply selective atten\u00ad\ntion to the results  provided  by the models, only focusing on the part \nthat they aligned  with. \nFinally,  an attempt was made at creating a quantification framework \nfor search bias was made by J. Kulshrestha el.al.[5], specifically for \npolitical  searches.  This framework  proved  quite effective in  identifying \nthe bias of  political  posts on Twitter, along  with  their reposts. It also \nshowed  that while web search preferred candidate-controlled  sites \nwith  more credible content, Twitter had no preference whatsoever. \n4 \n\n2 Theoretical  Background \nUnderstanding  LLM-based  Search Engines requires a  base  level of \nknowledge  about the mechanics behind Large Language Models and \nthe way they retrieve information from external sources.  These  sources \ncan include websites, literature, or any other media not directly in\u00ad\ncluded  in the training data of the Language  Model. \n2.1 Search  Engine  Architecture \nBefore delving into how Large Language Models power Search en\u00ad\ngines, it is important to understand the architecture behind Regular \nSearch Engines. The algorithms behind modern search engines can be \nbroken  down  into three categories -  Crawling,  Indexing, and Ranking. \n[7] \nCrawling \nCrawling  is the process in  which  bots, called Web Crawlers, systemati\u00ad\ncally  scour the Internet searching for new content. [7] This can be new \nor updated websites, videos, photos, or any other media available to \nthe public. \nIndexing \nIndexing  is the process in  which  search engines store the newly found \ndata in their index - a massive  database  of all available information \non the Web.  Currently,  the most used index type is the inverted index, \nwhich  maps words to the documents in  which  they appear [8]. This \nallows  faster  search results, as the words contained in the query can \nbe used to retrieve documents. \nRanking \nRanking  is the process of ordering the search results returned from \nthe index based on their relevance to the search query. The ranking \nis done based on multiple factors, such as how many times the site \nappears in the index or how many other  sites  link  to it. [9] \n5 \n\n2. THEORETICAL  BACKGROUND \n2.2 Large  Language  Models  (LLMs) \nLarge Language Models, subsequently referred to as  LLMs,  are mod\u00ad\nels that use deep learning, specifically neural networks, to  generate \ncoherent and contextually relevant text. This is typically achieved by \npredicting  the next  word  in a  sequence  based on  probability.  The most \ncommonly  used architecture for  LLMs  is the Transformer, first intro\u00ad\nduced  by Vaswani  et al. in their paper  \"Attention  Is All You  Need\"  [10]. \nThe Transformer architecture employs an encoder-decoder structure, \nwhere the encoder processes the input  text  into vector representations, \nand the decoder  generates  output based on those vectors. A key inno\u00ad\nvation  in this model is the use of self-attention,  which  allows the model \nto weigh the importance of different tokens in a  sequence  relative to \none another. This helps the model understand context more effectively. \nDuring  generation, each newly predicted token is appended to the \ninput  sequence, and the extended  sequence  is then used to predict the \nnext token. \n2.3 Retrieval  Augmented  Generation  (RAG) \nRetrieval  Augmented Generation subsequently referred to as  RAG, \nis a technique introduced by P. Lewis et al. in the paper \"Retrieval-\nAugmented  Generation for Knowledge-Intensive NLP Tasks\" [11]. \nIt aims to combine external sources  with  generative mechanisms to \nimprove  the accuracy of answers in knowledge-intensive tasks without \naffecting the training of the model. The first step consists of retrieving \nthe most relevant documents from a  database  using a vector index, \nwhich  determines their similarity to the input query. Subsequently, \na generative model produces the answer based on both the query \nand the retrieved documents. This allows the output  sequence  to be \naugmented by external information, leading to more accurate results. \nThere are multiple ways for this output to be generated, namely: \n\u2022 RAG-Sequence  - The  same  documents are used to predict all \ntokens of the sequence. This is done by computing the proba\u00ad\nbility  distribution for each document and then  marginalizing \nthem[ll]. \n6 \n\n2. THEORETICAL  BACKGROUND \n\u2022 RAG-Token -  All documents are marginalized per token gener\u00ad\nation  step[11],  allowing  the use of different documents  during \noutput  generation. \n2.4 Echo  Chambers  and Filter  Bubbles \nOutside  of the technical terms, it is also important to understand the \nsocietal concepts of  echo  chambers, filter bubbles, and polarization. \nEcho Chamber is a term for a situation  in which  people get increasingly \nreinforced in their  views  [12] by  viewing  only specific  affirming  media \nor only interacting  with  people of a similar  opinion.  This makes some \ninformation  seem  more important than it  would  otherwise, while also \nslowly  making them  reject  the views that do not align  with  theirs. \nFilter  Bubbles are a very similar concept to the  echo  chamber; however, \nthey mainly concern search engine filtering and  personalization  [12, \n13]. Search engines have a tendency to show the user results that \nsupport  their views, especially  after  a certain amount of data about \ntheir searching habits is collected. This is done by often  filtering  out the \nthings that the user may not want to see or might disagree  with.  LLM-\nbased search engines are no different. Most of them have a  built-in \npersonalization  algorithm,  which  builds  on the previous conversations \nit had  with  the user. \n2.4.1 Societal  Polarization \nThe described  echo  chambers and filter bubbles then lead to making \nsociety as a whole more  polarized.  That is, split into often only two \ndifferent  camps[12],  each based on a different  opinion,  often on the \nopposite sides of the spectrum. This slowly makes more people join \neither of the camps, erasing the opinions of  those  who stood in the \nmiddle.  The  extent  of polarization  may be defined by how different \neach viewpoint is, but also by how the sides see each other. This phe\u00ad\nnomenon is especially concerning  currently,  as the level of  polarization \nis rising  worldwide  [14]. \n7 \n\n2. THEORETICAL  BACKGROUND \n2.5 News  Sources \nFinally,  the sources, where people tend to get their news,  will  be de\u00ad\nscribed.  With  the rise of  digital  devices, most  traditional  news sources, \nsuch  as Radio,  Print,  and even TV, have been outclassed by the use \nof the internet[15]. The most used  digital  media, from  which  users \nget their news, are either online newspapers, social media, or regular \nsearch. Out of  these  three, the most similar to the traditional sources \nare online newspapers,  which  often contain the  same  information as \nthe print version.  Using  search or social media, on the other hand, \ngives  access  to a greater variety of news sources; however, specifically \nin the case  of social media, the news is often  less  credible than actual \nnews  sites[16].  The main news source that concerns this thesis is the \nsearch engines. Most of  these  engines provide an AI summary of the \nevents contained in the news articles that show up in the search engine. \nSuch  AI summaries may also be used as the primary news source of \nthe users. As such, it is important to examine whether the  LLM-based \nsearch engines are  truly  free  of bias and whether they enforce specific \nviews  or opinions. \n8 \n\n3 LLM-based  Search  Engine  Overview \nIn addition  to knowing  the theoretical foundations of  LLMs  and  tradi\u00ad\ntional  search engines, it is also important to understand the function\u00ad\nality  of leading  LLM-based  search engines.  Moreover,  it is  vital  to be \naware of the differences between several types of such systems. This \nwill allow  for a deeper  understanding  of their use  cases  as well  as the \neffectiveness of each model in various situations. \n3.1 LLM-based  Search  Engines \nLLM-based  search engines are search engines that enhance the regular \nsearch engine architecture  with  the generative mechanisms of  LLMs. \nThis  is done by creating an Index of trusted sources and then  applying \nRAG  on this Index to  generate  an answer or a  summary  [17]. The \noutput  is then often combined  with  the websites used by the  RAG  to \ncreate  a text  complete  with  concrete citations. \n3.2 Difference  from  Regular  Search  Engines \nTraditional  search engines  primarily  utilize  token-level  word  matching. \nThey  match words in the Index  with  words in the query and return \nthe documents  linked  to them  [7]. The documents are then ranked \nbased on  multiple  different factors specified by the search engine. \nIn contrast,  LLM-based  search engines add an  additional  layer of \nnuance to the way the documents are  picked.  Rather than  picking \ndocuments based on token overlap, they also use  RAG  to consider \nsemantic  similarity  [17]. This allows them to pick sources that are \nrelated  based on their concept rather than specific phrases. \n3.3 Search  Engines  With  Summaries \nThese  are search engines that  provide  a brief, generated  overview  of \nthe most relevant sources  picked  either by a  RAG-based  system or \nranking  while  still  offering the  full  functionality  of the regular search \nengine.  The  overview  is produced  by a generative model that is fed the \n9 \n\n3. LLM-BASED  SEARCH  ENGINE  OVERVIEW \nuser's search query along  with  the top-ranked retrieved documents \nas input[18].  These  models are generally single-turn,  which  means \nthat they are unable to have a dialogue  with  the user. Their main \nfunctionality  is to  provide  a quick and easily understandable summary \nof the search results. \n3.3.1 Google \nGoogle  AI Overview  is a feature integrated into Google Search,  which \nprovides  a complex  Al-generated  answer to the user query. It leverages \nGoogle's proprietary  LLM  - Gemini  to generate  the content. [19] The \nsearch results  provided  by the  overview  are selected based on Google's \nranking  algorithm as  well  as semantic matching. After the generation, \nsources that match the content of the overview are also added. \n3.3.2 Bing \nBing  Search incorporates  LLMs  into its functionality in two distinct \nways  [20]. \n1. The first way involves an automatic Al-generated summary of \nthe search results  with  a chance to appear after asking a question. \nThese  Al-generated summaries are sectioned,  with  each section \nsupported  by individual  citations derived through semantic \nanalysis  of retrieved sources. \n2. The second way is in the usage of Microsoft's proprietary  LLM  -\nMicrosoft  Copilot,  embedded in a chat interface. The chat  acts \nlike a dynamic conversational search tool. While the responses \nit generates  are typically  less  in-depth than the  Bing  Search \nsummaries,  Copilot  still  provides in-text citations, albeit fewer \nand more general in nature. \nThe citations in both of the approaches are picked by the  RAG-\nbased system as the most relevant to the topic. \n3.3.3 Arc \nArc is a Browser that provides robust AI functionality.  Among  its \nmany  LLM-based  features, the one that  involves  an LLM-based  search \n10 \n\n3. LLM-BASED  SEARCH  ENGINE  OVERVIEW \nengine is its \"Search for Me\"  functionality.  This feature  acts  as an  LLM-\nbased search summarizer in  which  it uses  integrated OpenAI  LLMs \n[21] to  create  a custom summary of relevant sources.  These  sources \nare taken from their own search engine and added as citations to the \nfinal  summary. \n3.3.4 Brave \nBrave Search has an  LLM-powered  summarizer integrated into its \nsearch engine. This summarizer works in a similar way to Google \nOverview  in which  it retrieves the most relevant search results using \na RAG-based system and then summarizes them using undisclosed \nproprietary  LLMs  based on either  BART  or DeBERTa [22]. Brave also \nprovides  a smart assistant called Leo AI,  which  uses  a combination \nof LLaMA  2 and Claude as  well  as models from  Mixtral  [23]. Leo AI, \nhowever, does not have citation functionality and is mostly only useful \nfor expanding the summaries. \n3.3.5 DuckDuckGo \nDuckDuckGo  also has Al-assisted answers accompanying its search \nresults. The responses are generated using models such as GPT-4o \nmini,  Claude 3  Haiku,  and Llama 3.3 70B. In contrast to the other \nSearch Engines, the  DuckDuckGo  Al-assist  uses  the highest-ranked \nsources returned by its regular search engine [24] instead of the  ones \ndeemed the most relevant by  RAG.  It also provides its own assistant, \nDuck.ai,  which  can be used to expand and  clarify  the answers  provided \nby the AI summaries. \n3.4 Chatbots  with  Search  Functionality \nUnlike  the LLM-generated overview,  these  chatbots are able to  hold  a \ndialogue  with  the user in real-time while simultaneously  providing \nsources for their answers. This assures  better  factual grounding and \ntransparency of the chatbot's answers. The amount of sources they are \nable to provide is limited compared to  fully-fledged  search engines, \nhowever. \n11 \n\n3. LLM-BASED  SEARCH  ENGINE  OVERVIEW \n3.4.1 You.com \nYou.com  1 is one of the first  LLM-powered  chatbots to have  live  Web \nsearch  functionality.  It provides  multiple  \"Agents,\"  which  are differ\u00ad\nent LLM  models tailored toward specific tasks. [25] These include \nResearch, Create,  Compute,  as well  as the  base  Smart  model.  Further\u00ad\nmore,  it allows  access  to most  LLM  models such as GPT-4,  Claude, \nLlama,  Gemini,  and more [26]. For the purposes of this paper, further \nreferences to  You.com  will  be assuming the usage of the default  pro\u00ad\nprietary  model.  The Web search is done in real-time  using  their own \nSearch and  News  Search engines. \n3.4.2 ChatGPT Search \nChatGPT 2, developed by  OpenAI,  is a conversational agent powered \nby LLMs  that  allows  continuous dialogues  with  users. In  2025,  OpenAI \nreleased the Search feature,  which  is a toggleable mode that adds \nsources to the responses of the model by  retrieving  information  from \nthe Web.  It is  powered  by the  GPT-4o  model  combined  with  third-party \nsearch  providers  such as  Bing  [27,28],  which  are used to generate the \nsources. \n3.4.3 Deepseek  Search \nDeepseek 3 is a recently released conversational agent by Deepseek \nAI, offering functionality  similar  to that of  ChatGPT.  It engages in \ndialogue  with  users  using  the proprietary  Deepseek  LLMs  to generate \nresponses. In  addition  to its  base  functionality,  Deepseek also includes \na toggleable Search feature that uses real-time search to supplement its \nresponses  with  sources.  While  the search engine used by Deepseek has \nnot been  publicly  disclosed,  it is  likely  that the agent relies on a  third-\nparty  search engine, as is common practice among such platforms. \n1. Available  at https://you . com/ \n2. Available  at https: //chatgpt. com/ \n3. Available  at https: //chat. deepseek. com/ \n12 \n\n3. LLM-BASED  SEARCH  ENGINE  OVERVIEW \n3.4.4 Perplexity \nPerplexity 4 is an answer engine that, upon receiving a user query,  uses \na custom search engine to scour the Internet for sources and then  uses \nLLMs  to summarize the results. While it supports  follow-up  questions \nand maintains conversational context to a certain extent, its dialogue \ncapabilities are somewhat more limited compared to platforms like \nChatGPT  or Deepseek, as its main functionality is the Web search. \nThere are two types of search offered by Perplexity: Quick Search and \nPro Search [29]. Quick search is the default mode that provides  faster \nand more straightforward answers. On the other  hand,  Pro Search  uses \nfollow-up  questions to clarify the user's  inquiry  before generating the \nanswer. This is done in order to provide more comprehensive results. \nAdditionally,  Pro Search contains  access  to LLMs  different from the \ndefault  model.  For the purposes of this paper, any further  references  to \nPerplexity  and its API  will  assume the use of Quick Search combined \nwith  the proprietary Sonar model. \n4. Available at  https://www.perplexity . ai/ \n13 \n\n4 Databases  on Credibility of  News  Sources \nTo fully  understand the sources  provided  by LLM-based  search en\u00ad\ngines, it is essential to examine the  databases  that offer  credibility  and \nbias metrics for news sources. While many such  databases  undoubt\u00ad\nedly  exist, most are very  limited  in scope, often encompassing only a \nfew hundred  sources. As a result, comprehensive  databases  are largely \nunavailable  in languages other than  English  currently. The focus of \nthis chapter is on four of the most comprehensive English-language \ndatabases,  outlining  their purposes and the key metrics they use to \nevaluate news sources. \n4.1 Media  Bias  Fact  Check \nOne  of the biggest  databases  on Media  Bias and  credibility  is Media \nBias Fact Check 1, also  known  as MBFC.  Having  over  9300  sources in \nits database, it ranks sources based on bias, factuality, and  overall  cred\u00ad\nibility.  A weighted scoring system is used to determine the values by \nanalyzing  headlines and news articles from each source. Techniques \nsuch  as keyword and pattern analysis are used to guarantee consis\u00ad\ntency  within  the database  [30].  MBFC  also provides reasoning detail\u00ad\ning why such  scores  were awarded. The site users can dispute this \nreasoning,  assuring fairness in the ratings. Outside of news sources, \nMBFC  also ranks countries'  biases  and the freedom of news sources \nfrom  the governments of their respective countries. \n4.2 AllSides \nAllSides.com 2 is a website that provides bias and  credibility  ratings \nfor over  2000  sources. It  uses  metrics similar to those of  MBFC,  such \nas bias, factuality, and  credibility.  AllSides  ranking is based on human \nratings,  whether from a panel of 6 to 9 experts or  blind  surveys where \nparticipants  rate  the content without being aware of the  source[31]. \nThey claim to use people from all sides of the  political  spectrum in \n1. Available  at https: //mediabiasf actcheck. com/ \n2. Available  at https  ://www.  allsides. com/ \n14 \n\n4. DATABASES  ON  CREDIBILITY  OF  NEWS  SOURCES \nthe rankings, assuring that the values are fair,  avoiding  any inherent \nbias.  Additionally,  community feedback is factored into the decisions \nin order to guarantee the most accurate results. \n4.3 Ad  Fontes  Media \nAd Fontes  Media 3 provides a bias chart of over 11000 sources. This \nincludes  websites, videos, TV and podcasts. The chart operates on \ntwo metrics -  reliability  and  political  bias. A panel of over 60 analysts \nranks sources by analyzing  individual  articles or episodes making \nup the source. These articles are chosen based on their prominence \non the website.  Additionally,  each article is ranked by a mixed panel, \ncontaining  at least one person  identifying  with  each end of the po\u00ad\nlitical  spectrum (center, right, left) [32]. The scores of each part are \nthen averaged to  create  the overall score for the source. This score \ndetermines its placement on the bias chart. \n4.4 NewsGuard \nThe biggest site that provides news source ratings is  NewsGuard 4, \nwhich  covers over  35000  websites.  NewsGuard  is based on a system \nthat awards each site a  credibility  rating from 0 to 100. This rating is \nbased on preset criteria,  which  are evaluated on a pass-fail basis. The \nratings are created by a panel of experts using  these  criteria to give \npoints  to each source. Each criterion has a specific number of points \nbased on its importance in the rating.  Truthful  information and re\u00ad\nsponsible reporting are two of the most important  criteria,  followed  by \nresponsible information gathering and swift error correction. Outside \nof content ratings, transparency ratings are also factored into the score. \nThis  encompasses criteria such as  disclosing  ownership of the  site[33]. \nUnfortunately,  NewsGuard  does not rate the bias of websites; instead, \nit only focuses on  credibility  and transparency. \n3. Available  at https: //adf ontesmedia. com/ \n4. More information available at https:  //www  .newsguardtech. com/solutions/ \nnews-reliability-ratings/ \n15 \n\n5 Methodology \nThis  chapter aims to explain the methodology used to analyze  LLM-\nbased search engines.  With  consideration of both the theoretical back\u00ad\nground  and the  provided  overviews,  fitting  LLM-based  search engines \nand a credibility  database  were chosen. Then, adequate queries were \npicked  based on what is most searched  worldwide.  The themes for the \nqueries were selected  using  Google Trends.  Afterward,  queries were \nsent  to the search engines, and the sources  provided  were collected \nusing  a web  application.  The  final  labels describing each  LLM-based \nsearch engine were determined based on metrics  provided  by the \nchosen database.  Additionally,  all queries were sorted into distinct \ncategories to offer a more specialized look into the bias of chosen \nsearch engines.  Finally,  additional  metrics were created to gain a  fur\u00ad\nther  understanding  of the types of sources  provided  by each search \nengine. \n5.1 Chosen  Search  Engines \nFirstly,  it is essential to  provide  insight into the selected  LLM-based \nsearch engines \u2014  which  will  be analyzed in the subsequent chapters. \nUnfortunately,  as the  availability  of LLM-based  search engine  APIs \nwas  limited  at the time of the  application  creation, some search en\u00ad\ngines were  disqualified  from the selection.  These  are Bing,  Arc,  and \nDuckDuckGo.  These  APIs  were either not available to the  public  or \nonly  available on request \u2014  which  was unfortunately denied. Next, \nit was important to filter out those  APIs,  which,  while  available, do \nnot provide  automatic citation or source generation.  These  APIs  are \nDeepseek API, Google's  Gemini  API, and  OpenAI  API.  Out of all \nthe remaining  APIs  available, the options chosen were Brave Search, \nYou.com , and  Perplexity.  When choosing search engines, the most \ncritical  factors were  popularity  and  diversity.  Choosing  search engines \nthat experience  high  traffic was crucial in ensuring that the results \nwere based on search engines that are used by diverse types of end \nusers.  Perplexity  was chosen for this reason. Of the  remaining  options, \nit is undoubtedly the most popular choice. Its architecture is  built \nto provide cited responses first and foremost,  unlike  other chatbots \n16 \n\n5. METHODOLOGY \nthat were not created  with  this in  mind.  On the other hand,  picking \ndiverse  engines meant that data  could  be collected from both types \nof LLM-based  search engines - summaries and chatbots. As Perplex\u00ad\nity, which  was chosen first, is a chatbot-style search engine, the next \none was  picked  to be a  summarizer.  For this, the  primary  choice was \nBrave  Search  Summarizer.  Brave Search is currently one of the  fastest \ngrowing  search engines[34], as it works on any browser and claims \nto provide  private search  with  multiple  other features,  including  AI. \nThe last search engine to be chosen was You.com , one of the oldest \nLLM-based  search engines on the market;  launching  in 2021.  Unfortu\u00ad\nnately,  You.com  provides  only  their search  APIs  to users not affiliated \nwith  the company.  However,  these  search  APIs  should  provide  very \nsimilar  results as those returned by the Smart  API;  they  only  lack the \nsummary. \n5.2 Database  Specifics \nThe next step was choosing the database. Out of the  provided  databases, \nthe one most  fitting  for the purposes of this thesis was  MBFC  or Media \nBias Fact Check.  NewsGuard  was also considered; however, its one \nvalue  scoring was deemed insufficient for  in-depth  analysis of news \nsources. The  MBFC  database  and their  API  provide  these  metrics[35]. \n\u2022 Bias \n\u2022 Factual Reporting \n\u2022 Credibility \n\u2022 Country \n\u2022 Media  Type \nWhile  media type and country are quite  straightforward,  consisting \nof only  a label, the other three metrics are counted  using  specific \ncalculations  depending on  multiple  factors. \nBias \nMBFC  Bias is calculated  using  a scale  from -10 to +10. Each  politi\u00ad\ncal alignment from extreme left to extreme right is assigned a point \nvalue[30]. The bias levels and their point values are depicted in Table \n5.1. \n17 \n\n5. METHODOLOGY \nTable  5.1: Bias Values and Labels \nScore  Range Bias  Label \n-10.0 to -8.0 Extreme Left \n-7.9 to -5.0 Left \n-4.9 to -2.0 Left-Center \n-1.9 to 1. Center (Least Biased) \n2.0 to 4.9 Right-Center \n5.0 to 7.9 Right \n8.0 to 10.0 Extreme Right \nThese points are then awarded to multiple categories \u2014 each hav\u00ad\ning a certain weight. The categories are Economic System (ES), Liber\u00ad\nalism  vs. Conservatism (LC), Reporting Balance (RB), and  Editorial \nBias (EB) [30]. \nThe final  score for bias is given by this formula \nBiasScore  = 0.35  \u2022 ES + 0.35  \u2022 LC + 0.15  \u2022 RB + 0.15  \u2022 EB (5.1) \nOutside  of the labels in the table above, there are three more non-\npolitical  bias labels  MBFC  uses. These are Pro-Science, Conspiracy, \nand Satire. \nFactual  Reporting \nFactual  Reporting measures the amount of true  facts  contained by the \nsources, as  well  as their transparency [30]. It is calculated using a scale \nfrom  0 to 10. The factuality levels and their point values are depicted \nin Table 5.2. \n18 \n\n5. METHODOLOGY \nTable  5.2: Factuality Values and Labels \nScore  Range Factual  Label \n0 Very  High \n0.1 to 1.9 High \n2.0 to 4.4 Mostly  Factual \n4.5 to 6.4 Mixed \n6.5 to 8.4 Low \n8.5 to 10 Very  Low \nThe points  are  based  on  multiple  categories  \u2014 each  of which \nhas a weight attached  to it. These  categories  are  Failed Fact Checks \n(FFC),  Sourcing  (S),  Transparency (T), and One-Sidedness/Omission \n(O)[30]. \nThe final  score  for factuality  is calculated using this formula \nFactScore = 0.4  \u2022 FFC  + 0.25  \u2022 S + 0.25  \u2022 T + 0.1  \u2022 O (5.2) \nCredibility \nThe final metric  of MBFC  is Credibility.  It is  measured on  a scale  from \n0 to 10,  awarding points based on four categories.  These  categories  are \nnot given weights. Instead, they all add up  to a  maximum  of 10  points \nto determine the rating[30]. The  categories  are Factual Reporting, Bias, \nTraffic,  and Press Freedom. Factual Reporting and Bias  are  calculated \nas shown above. Traffic measures how often visited the site  is and how \nlong  it exists. Press freedom applies only  to sources from countries \nwith  significant censorship  \u2014 giving  only negative points. \nThe credibility Values  are  shown in Table  5.3. \nTable  5.3: Credibility  Values and Labels \nScore  Range Credibility  Label \n0to2 Low \n3 to 5 Medium \n6 to 10 High \n19 \n\n5. METHODOLOGY \n5.3 Data  Acquisition \nBefore the process of data analysis is described, it is important to \ngrasp the entire process of data acquisition and query creation. This \nwas done in two steps. The first of those  steps  was user-based. It was \ndone by  giving  students  access  to the  application  described in the next \nchapter for a month.  During  this time, their queries along  with  their \nresults were gathered. Then the content of the queries themselves \nwas analyzed and the results obtained were used along  with  Google \nTrends 1 to tailor more queries based on current  world  events. \n5.3.1 User-Based  Data  Collection \nAs mentioned  previously,  the user-based data  collection  was done  with \nthe help of students, specifically those of the Ethics and IT courses. \nWhile  people outside those courses were also given  access  to the \napplication,  almost all queries came from those students. This step \nof data acquisition was  vital  for ensuring that the queries have the \nstyle,  wording  and content of those,  which  regular users  would  input \ninto the  LLM.  A similar  wording  would  then be used for some of the \nqueries created later. A  preliminary  analysis was done on the resulting \ndatabase  in order to determine whether the tone and specific words \nused  in the query  would  affect  the result of the  LLM. \n5.3.2 Data  Collection  Based  on Trends \nThe seconds step of data acquisition was the usage of tailored queries \nbased on the current  world  events. This was done in order to expand \nthe reach of the database, both in the way of result amount and variety. \nThe queries used for this step were created by gathering the most pop\u00ad\nular search terms currently and using those to  create  multiple  different \nqueries pertaining to the main  subjects.  While most queries asked  fun\u00ad\ndamentally  different questions, some were very  similar,  differing  only \nin formulation.  The difference could be in the choice of words or the \ntone of the question. An example of this can be the question \"Is the \nEarth  flat?\" as opposed to  \"Why  is the Earth flat?\".  These  two questions \n1. Available  at https://trends.google.com/trends/explore \n20 \n\n5- METHODOLOGY \nobtained from the user-based part of data collection yielded different \nresults even though they are asking a very similar thing. \n5.3.3 Categorizing  Queries \nAll queries were sorted into various categories based on common \nwords  or themes surrounding them.  These  categories were chosen \nbased on Google Trends. For the purposes of this thesis, the most \npopular  themes from the politics, wars and news categories over the \npast 6 months were chosen. Some queries may be sorted into multiple \ncategories based on their content. An example of this may be a query \ntalking  about the War in  Ukraine.  This query  will  be sorted into the \ncategories of Russia and  Ukraine.  The categories are: \n\u2022 American Elections \n\u2022 German Elections \n\u2022 European  Union \n\u2022 Pope + Conclave \n\u2022 America + Trump \n\u2022 Elon  Musk \n\u2022 Russia +  Putin \n\u2022 Ukraine \n\u2022 Israel \n\u2022 Palestine + Gaza \n\u2022 LGBTQ+ \n\u2022 Climate Change \n\u2022 Conspiracy Theories \nThe categories could be grouped further based on the central \ntheme.  These  themes include Politics, International Conflict, Social \nIssues,  and Science. Some of  these  categories could touch multiple \nthemes; however, most of them are distinct enough to show up only \nonce. Some of the categories chosen are very similar to each other, \nbut they were split on purpose. This was to examine whether certain \ndifferences in keywords  would  influence the metrics of the search \nengines. \n21 \n\n5. METHODOLOGY \n5.3.4 Query Creation \nFinally,  the  specifics  of the  query creation process  will  be described. \nEach  of the  predefined categories  has  around  30 queries assigned  to it. \nThese  are the  student queries supplemented  by other specifically  tai\u00ad\nlored  queries. This assures that each  of the  categories  has  comparable \namounts  of queries and  as such,  the results  will  not be biased towards \na specific category.  Additionally,  the tones  of queries in most categories \nare comparable  to each other in order  to not  skew  the category-based \nresults in any specific direction. \n5.4 Important  Parameters \nThe most important parameters  to check for in  the sources  are the  pre\u00ad\nviously  discussed bias, factuality, and  credibility.  The  MBFC  database \nwill be used  to obtain  the ratings  of each source returned by  the search \nengines.  Along  with  this,  the  amount  of non-political  sources  will \nbe counted.  These  non-political  sources  are  those categorized  as Pro-\nScience, Satire,  or Conspiracy.  Additionally,  the number  of occurrences \nof each  of those labels  will  be counted  as well.  These  amounts  will \nthen  be turned into  additional  metrics  \u2014which  are  computed  as per\u00ad\ncentages  of the  total amount  of analyzed sources. \n\u2022 >T, Political \u201e^ PohticalPercent = \u2014\u2014\u2014 x 100 (5.3) TotalSourceCount \n_ . _ ProScience \u201e\u201e\u201e \u201e, SciencePercent  = \u2014\u2014\u2014 x 100 (5.4) TotalSourceCount \nSatire SatirePercent = \u2014\u2014\u2014 x 100 (5.5) TotalSourceCount \n^ \u2022 T~\\  Conspiracy \u201e\u201e\u201e ConsviracyPercent =  \u2014\u2014\u2014\u2014-\u2014\u2014  x 100 (5.6) TotalSourceCount \n5.5 Data  Structure \nThe analysis  will  be done  on the  sources  provided  by the  LLM-based \nsearch engines  per  query. Such sources can  be easily acquired using \nan API and then passed over  to a  slightly  modified  MBFC  database. \n22 \n\n5. METHODOLOGY \nThe database  was modified to not include any dead sources and to \nhave clearer bias values - specifically replacing Questionable bias  with \nthe actual bias, and the Least Biased label  with  Center. The script used \nfor the modification can be found in the attachments. The domain \nof each source  will  be matched  with  the urls in the  MBFC  database, \nand Bias, Factuality and  Credibility  labels as  well  as the Country and \nMedia  Type  will  be collected.  These  values  will  then be either counted \nor turned into decimal values,  which  will  be used to decide the  final \nlabel  for every response.  These  final  values  will  be collected and used \nto analyze the models as a whole. \n5.6 Value  Assignment \nUnfortunately,  the bias, factuality, and credibility are  provided  by the \nMBFC  database  only in labels, and it is impossible to restore the  origi\u00ad\nnal rating. Therefore, a system to represent the labels using numerical \nvalues is needed. This  will  be done on a  scale  from 0 to 1, where the \noriginal  distribution  from the  MBFC  documentation  will  be kept. Due \nto the representation in the database, however, some accuracy  will \nbe lost as their bounds are rather significant, and one label may en\u00ad\ncompass results hugely different from one another. This is especially \ntrue for the Left and Right labels.  Thankfully,  due to the amount of \nsources  provided,  this should be negligible in most  cases,  however it \nis important to note that outliers may occur. The values representing \neach label and the formulas used to acquire them are as follows. \nThe values assigned to each label correspond to the mid-point of \nthe MBFC  interval normalized to the range of 0 to 1. The mid-point is \nchosen to ensure the most balanced value representation possible. \nBias \nConsidering  that the  original  bias  scale  goes  from -10 to 10, in order \nto normalize it to the set range, we need to use the  following  formula, \nwhere b is the  original  value of bias. \n\"norm  \u2014 on \\y-' ) \n23 \n\n5. METHODOLOGY \nThe representation  of bias  according  to this formula  is shown in  Table \n5.4. \nTable  5.4: Bias Value Representation \nBias  Label Assigned  Value Value  Range \nExtreme Left 0.05 [0; 0.1) \nLeft 0.175 [0.1; 0.25) \nLeft-Center 0.325 [0.25;  0.4) \nCenter(Least  Biased) 0.5 [0.4; 0.6) \nRight-Center 0.675 [0.6; 0.75) \nRight 0.825 [0.75;  0.9) \nExtreme Right 0.95 [0.9; 1] \nFactual  Reporting and  Credibility \nA very similar approach  will  be used to normalize the factual reporting \nand credibility  ranges.  The  only difference  is the  range  or normalized \nvalues.  For  factuality  and  credibility, this  is 0 to 10,  which  makes \nnormalization much more straightforward  as the  only thing needed \nis division  by 10. The  formula  is shown below, where  x is the  original \nvalue  of credibility/factuality. \nXnorm  \u2014 (5-8) \nTheir  representations  according to this formula can be  seen  in Table \n5.5. \n24 \n\n5. METHODOLOGY \nTable  5.5: Factuality and  Credibility  Value Representation \nFactual  Label Assigned  Value Value  Range \nVery  High 0 [0; 0.001) \nHigh 0.1 [0.001;  0.2) \nMostly  Factual 0.325 [0.2; 0.45) \nMixed 0.55 [0.45;  0.65) \nLow 0.75 [0.65;  0.85) \nVery  Low 0.925 [0.85;  1] \nCredibility  Label Assigned  Value Value  Range \nLow 0.15 [0; 0.3) \nMedium 0.45 [0.3; 0.6) \nHigh 0.8 [0.6; 1] \n5.7 Label  Calculation \nOnce all the values are converted, it is  finally  possible to calculate \nthe resulting label. This is done by averaging the assigned values of \nall sources and mapping them onto the value ranges, determining \nthe final  label. This is relatively straightforward for factual reporting \nand credibility,  as they do not have any possible values outside the \nmapped  range. Bias, however, does have such values. The  final  bias \nlabel  is therefore determined in the  following  way.  First, it is decided \nwhether the bias leans toward the  political  side or not. This can be \nachieved  by examining the  Political  percentage of the result. If this \npercentage exceeds 50%, it is labeled using the above bias table in the \nsame way as factuality and  credibility.  Otherwise, it checks whether \nany of the  non-political  percentages exceed 50%  with  a preference for \nPro-Science. If neither of the conditions is  fulfilled,  it decides between \nOther  and  Unknown  based on whether any sources were analyzed. \n5.8 Other  Collected  Values \nOther  values are also collected outside of the primary metrics and la\u00ad\nbels. The first of  these  values is a statistic of the most analyzed sources \n25 \n\n5. METHODOLOGY \nfor each search engine and category. This is a structure counting all \nappearances of each domain  within  the analysis,  allowing  a deeper \nunderstanding of the  sites  deemed most trustworthy by each engine. \nOther values include the counts of specific domains,  which  are slightly \nout of the  scope  of this analysis. Such domains include video stream\u00ad\ning platforms, online forums, or social media sites. For  these  sources, \nit is impossible to accurately determine their credibility or bias as most \nposts are created by users under nicknames and pseudonyms.  Nev\u00ad\nertheless, examining how often  LLM-based  search engines use such \nunverifiable  sources is important. The specific domains analyzed are \nYouTube,  TikTok, Reddit, Quora, Twitter/X, Instagram and Facebook, \nas they are the  ones  most  likely  to appear. \n26 \n\n6 Web  Application  Development \nWith  the basic methodology for the analysis established, this chapter \npresents the development process and internal architecture of the web \napplication  designed to collect data based on user queries. \n6.1 Application  Outline \nThe base  structure of the application is straightforward, consisting of \nonly  two web pages. The first page contains a single  text  input  field \ninto  which  the user writes their query.  Upon  submitting the query, \na request is  sent  to the APIs of the selected search engines, and a \nresponse is received. Subsequently, the response is parsed to obtain \nthe sources as  well  as the  text  result created by the  LLM.  Following \nthis,  the application  searches  for the sources and their metrics in the \nMBFC  database  using the domain name as a key. The obtained labels \nare then represented using float values and are used to compute the \nmost important metrics. This data is then saved in a JSON file,  which \nserves as a  database  of results for further use in the latter  steps  of the \nanalysis.  Finally,  the obtained data is graphically  visualized  for the \nuser as a comparison between the chosen search engines. Currently, \nthe application only works correctly in English, as there is no good \ncomprehensive  database  on sites  in most other languages, and such \nlanguages may not be supported by all  LLMs.  The source code for the \napplication  without the API keys can be found in the attachments. \n6.2 Used  Tools \nBefore delving into the detailed development of the application, it \nis necessary to introduce the technologies employed,  including  the \nselected programming languages,  utilized  APIs,  and their fundamen\u00ad\ntal structure. Understanding  these  choices is  vital  to comprehend the \ndesign  decisions and functionality described later. This section pro\u00ad\nvides  an overview of the basic features of  these  tools and explains the \nreasons for their use. \nThe primary  programming language used to  create  the backend of the \n27 \n\n6. WEB  APPLICATION  DEVELOPMENT \nweb  application  was  PHP.  Although  other languages, such as Python \nwith  the Flask framework, were considered,  PHP  was selected due to \nits effectiveness for small-scale projects and its  minimal  setup require\u00ad\nments.  HTML  was used for the front-end development and CSS was \nused  for the  styling. \n6.2.1  APIs \nThe application  uses four  APIs.  These  APIs  allow  access  to the  LLMs \nthrough  requests  as well  as access  to the MBFC  database.  A basic \noverview  of the  APIs  is provided  below, along  with  the structure of \nrequests sent to them by the  PHP  script and their responses.  All the \nrequests for the  LLM  APIs  were made  using  curl  in PHP. \nPerplexity  Sonar  API \nThe first  API  used by the  application  is the Sonar  API 1 by Perplexity.  It \nallows  the generation of a model's response to  a query  upon  sending \na POST request. This means that the query is sent to the server in the \nrequest's body, and the server  creates  the response based on it. The \nrequest used in the  application  can be seen below, \n<?php \ncurl_setopt_array($curl_perplexity ,  [ \nCURLOPT_URL  => \"https://api.perplexity.ai/ \nchat/completions  \" , \nCURLOPT_RETURNTRANSFER  => true , \nCURLOPT_ENCODING  => \"\", \nCURL0PT_MAXREDIRS => 10, \nCURL0PT_TIME0UT  => 30, \nCURL0PT_HTTP_VERSI0N  => \nCURL_HTTP_VERSI0N_1_1, \nCURL0PT_CUST0MREQUEST => \"POST\", \nCURL0PT_P0STFIELDS  => Sperplexity_postfields \nCURLOPT_HTTPHEADER  => [ \n\"Authorization: uBearer u<token>\", \n\"Content-Type: uapplication/json\" \n1. API  reference  available at https: //docs .perplexity. ai/home \n28 \n\n6. WEB  APPLICATION  DEVELOPMENT \n]) ; \n?> \nthe basic structure of the  request  will  remain largely unchanged for the \nother APIs,  as the  main  content  is dictated mainly by the post fields, \nthe URL, and  the  HTTP  header.  In the  code above,  <token>  refers \nto the API key. The post fields for the perplexity  API  can be  seen  below. \n<?php \n$perplexity_postfields  = json_encode( [ \n\"model\"  => \"sonar\", \n\"mes sages\" => [ \n[ \n\"role\" => \"system\", \n\"content\"  => \"Be uprecise uand u \nconcise. uCheck umany usources.\" \n] , \n[ \n\"role\" => \"user\", \n\"content\"  => $_GET[\"question\"] \n] \n] , \n\"search_recency_filter  \" => \"month\", \n\"response_format\"  => null \n]) \n?> \nThe most important fields  for the  purposes  of this application  are \n\"model\"  and  \"messages,\"  which are both self-explanatory. The search \nrecency  filter  sets the  preferred  recency  of information. Therefore, \nif possible,  the  news search results provided  by Perplexity  will  be \nfrom  the last month. After sending the  request,  a response  containing \nthe LLM-generated  text,  as well  as the  corresponding citations,  is \ncollected and parsed  as a  JSON file  to allow smoother manipulation \nof the provided data. The basic  response  structure can be  seen  below. \n29 \n\n6. WEB  APPLICATION DEVELOPMENT \nYou.com  API \nAs mentioned  previously,  access  to You.com  Smart API could not  be \nobtained,  so You.com  Search  API 2 was used. The search API request \nstructure is very similar to  Perplexity  API  in most places.  However,  the \nmethod  is GET  as opposed  to POST, and the query  is instead inserted \ninto the request  URL.  The  curl  structure can  be seen below. \n<?php \n$url_you  = \"https://api.ydc-index.io/search ?\" . \nhttp_build_query([\"query\"  => $_GET[\"question\" \n]]); \ncurl_setopt_array($curl_you  , [ \nCURLOPT_URL =>  $url_you  , \nCURLOPT_RETURNTRANSFER =>  true, \nCURLOPT_ENCODING  => \"\", \nCURLOPT_MAXREDIRS  => 10, \nCURLOPT_TIMEOUT => 30, \nCURLOPT_HTTP_VERSION  => \nCURL_HTTP_VERSI0N_1_1, \nCURLOPT_CUSTOMREQUEST  => \"GET\", \nCURLOPT_HTTPHEADER => [ \n\"X-API-Key: u<token>\" \n] , \n]) ; \n?> \nThere are no headers  in this request as it is a GET request. The response \nis then collected and parsed as a  JSON.  Unfortunately,  as this is a search \nAPI, it does not contain an  LLM-generated  response, and  as such,  the \nabstract of the first source  will  be used for the purposes of  visualization \nfor the end user. The sample response structure can be seen below. \n2. API  reference  available  at https: //documentation.  you.  com/ \n30 \n\n6. WEB  APPLICATION DEVELOPMENT \nBrave  API \nThe Brave  API 3 structure  is slightly more complex than  the  You.com \nAPI and Perplexity  API.  Getting  the summarizer results consists  of \ntwo steps: obtaining  the  summarizer  key  from  the  search API  and \ngetting  the  results. This naturally means that two requests must  be \nmade  to two  different endpoints  in order  to obtain all  the  needed \ninformation.  The first request is  a GET request made toward  the Brave \nSearch  API.  The  structure can  be seen in  the code snippet below. \n<?php \n$url_brave  = 'https://api.search.brave.com/res/ \nvl/web/search?q='  . urlencode($query)  . '& \nsummary=l'; \ncurl_setopt_array($curl_brave  , [ \nCURLOPT_URL  => $url_brave  , \nCURLOPT_RETURNTRANSFER  => true, \nCURLOPT_ENCODING  => 'gzip', \nCURLOPT_HTTPHEADER  => [ \n'Accept: uapplication/json'  , \n'Accept-Encoding: ugzip', \n,X-Subscription-Token: u<token> ,) \n] \n]) ; \n?> \nThe response returns the key for its assigned  summarizer,  among other \nthings.  As the only component  of the  search API response necessary \nfor this thesis  is the  summarizer key,  its structure  will  not be shown \nas it is unneeded. This  key can  then  be passed  to the Summarizer \nAPI as the  secondary  in-url  API key,  allowing  the  collection  of the \nresponse and citations. The  URL  for the summarizer has  the  following \nstructure. As  it is linked  to the  search  API  call,  it does not need  to send \nany additional information, and therefore,  it is  also a GET request, \nidentical  to the  one shown above. The only difference in their structure \nis the  request  URL. \n3. Documentation available  at https: //api-dashboard. search.brave. com/ \n31 \n\n6. WEB  APPLICATION  DEVELOPMENT \n$summarizer_url_brave =  'https://api.search . \nbrave.com/res/vl/summarizer/search?key=' . \n$summarizer_key  . '&entity_info=1'; \nThe response  of the  summarizer  will  be parsed  as a  JSON file  for \nease  of manipulation, and  the  summarizer  text  and citations  will  be \nextracted. The structure  of the response  can be seen  below. \nMBFC  API \nThe final API used  is the  MBFC  API 4 available through Rapid API, \nwhich  works in  a much simpler way than the abovementioned  three. \nUpon  receiving  a GET  request,  the  API  will  return  a response  that \ncontains  the  MBFC  data  as a  list of all ranked sources. This list may \nthen  be used  to examine all the citations received from the search en\u00ad\ngine APIs based on their domain name. The  database  retrieval script \nusing  python can  be seen  below. This  database  was then saved, and \nmodified  using an attached script. \nimport http.client \nimport json \nconn = http.client.HTTPSConnection(\"media-bias  -\nfact-check-ratings-api2.p.rapidapi.com\") \nheaders  = { \n'x-rapidapi-key':  \"<token>\", \n'x-rapidapi-host':  \"media-bias-fact-check-\nratings  -api2.p.rapidapi.com\" \n} \nconn.request(\"GET\"  , \"/fetch-data\",  headers  = \nheaders) \nres = conn.getresponse() \ndata = res.read().decode(\"utf-8-sig\") \nThe list comprises  of dictionaries, which contain data about all  the \nbase  metrics discussed in the previous chapter. \n4. API available  through Rapid API  at https://rapidapi.com/mbfcnews/api/ \nmedia-bias-fact-check-ratings-api2 \n32 \n\n6. WEB  APPLICATION  DEVELOPMENT \nThis  can be seen  in the  sample dictionary below. \n\"Source\":  \"The New  York Times\", \n\"MBFC URL\":  \"https://mediabiasfactcheck.com/ \nnew-york-1imes/\"  , \n\"Bias\":  \"Left-Center\", \n\"Country\":  \"USA\", \n\"Factual  Reporting\": \"High\", \n\"Media Type\":  \"Newspaper\", \n\"Source URL\":  \"nytimes.com \", \n\"Credibility\":  \"High\", \n\"Source ID#\":  1001 \nThe data collected from  the  dictionaries  is further used  to calculate \nthe final  labels  for  visualization  as well  as the  values used  for  data \nanalysis. \n6.3 Data Processing \nOnce all  the data  is gathered from the  APIs,  the  text  and citations  are \nextracted and processed  to obtain the metrics'  final  values. The first \nstep of data processing is searching for the domain name in the  MBFC \ndatabase.  This  is done by iterating over the blocks in the  database  and \ncomparing  their source URL  with  the citation domain. The domain  is \nextracted using  the  parse_url() function  with  the  component spec\u00ad\nified  as PHPURLHOST.  Once  a match  is found,  the  amount  of found \ncitations  is incremented, and  the  citation  is analyzed based  on the \nblock from  the  MBFC  database.  The  aforementioned method turns \nbias, factual, and  credibility  labels into values. This  is done using map \nstructures, where this data  is inputted. Furthermore,  the  amounts  of \npro-science, satire and conspiracy sources  are  counted.  Additionally, \nthe amount  of non-political  sources  is tracked  as the sum  of all three. \nThe values for bias, factual, and credibility  are  then summed up and \ndivided  by the  total found citation count  to obtain the average value \nof each search engine.  Finally,  for each  of the three  metrics,  a domain \n33 \n\n6. WEB  APPLICATION DEVELOPMENT \nname is saved in an array for the purpose of  visualization.  The  final \nlabels are then assigned based on the bounds defined in the previous \nchapter.  These  labels are then saved for further use. \n6.4 Data  Storage \nThe data calculated  during  the processing  stage  is saved into a JSON \nfile, which  will  be referred to as the analysis database. The JSON file \nwas chosen as it is the most systematic way of storing the data without \nhaving  to use an SQL  database  as it  uses  key-value pairs and arrays. \nThis  database  keeps track of the query, its citations, and the values \ngained  in the previous step. Other, smaller  databases  were also created, \neach of them  holding  the data of queries sorted into a specific category. \nThe queries have been categorized based on common themes. This was \nfirst done using a script that checked for particular words or phrases. \nOnce the automatic process was done, all the category  databases  were \nhand-checked to make sure no queries were falsely categorized. \n6.4.1 JSON  Structure \nThe structure of each of the JSON files is very  similar,  consisting of the \nquery and the results,  which  are split into three sections, each keeping \nthe results of a different search engine. The  base  structure of the JSON \nfiles can be seen below. \n[ \n{ \n\"query\":  \"A question?\", \n\"result_perp  \" : { \n\"citations\":  [{ \n\"url\": \"https://en.wikipedia.org/ \nwiki/Question\", \n\"labels\" :  [\"Center\",  \"Mixed\",  \" \nMedium\"]  , \n\"country\":  \"USA\", \n\"type\":  \"organization\" \n}, ], \n34 \n\n6. WEB  APPLICATION DEVELOPMENT \n\"bias\":  { \n\"val\": 0.875, \n\"label\":  \"Center\" \n}, \n\"factuality  \" : { \n\"val\": 0.75, \n\"label\":  \"High\" \n}, \n\"credibility\":  { \n\"val\": 1, \n\"label\":  \"High\" \n} \n\"amount  s\": { \n\"science\"  : 0, \n\"conspiracy\"  : 0, \n\"satire\"  : 0 \n} \n}, \n\"result_you\"  :{. . .}, \n\"result_brave\"  :{. . .} \n}, \n] \n6.5 Data  Visualization \nThe final purpose  of the web application  is to  visualize  the  data  for \nthe user.  This  is done  using  the  response  text,  the  calculated  labels \nand values, and  the  citation domains, which  were  saved into an array \nearlier. The  results  consist  of a  text  block showing the LLM-generated \nresponse  and  a progress  bar  showing  the  result  of the three  main \nmetrics. \n35 \n\n6. WEB  APPLICATION  DEVELOPMENT \nThe domains  of the citations  are  also integrated into the progress \nbar using the favicons,  which  are obtained through the Google favicon \nURL. \nhttps://www.google.com/s2/favicons?sz=64&domain= \nSlink \nThese  favicons are shown under the section that belongs  to the do\u00ad\nmain's  bias, factuality,  or credibility label. The page itself contains \nthree  results, one for each search engine, shown next to each other. \nThis  was done so the result comparison  would  be simple and easily \nunderstandable. The page also contains a button  allowing  the user  to \nreturn  to the query  input.  An example of the  visualization  of the bar \ncan be  seen  in Figure 6.1. \nPartisanship \nLeft-Center \n\u00a9 \nAP \nFigure  6.1:  An example of the bias bar \n36 \n\n7 Data  Analysis \nData  analysis is the  final  step required to obtain the research results. \nThis  chapter outlines the data acquisition mechanism using both user-\nmade and specifically tailored queries based on the current  world \nevents using a script,  which  automatically inputs queries into the \naforementioned  application.  Additionally,  the script used for the  final \ndata analysis  will  be described. This  will  allow for a  better  understand\u00ad\ning of how the  final  result was obtained. \n7.1 Acquisition  Script \nThe automatic data collection using specifically made queries sorted \ninto categories was done using a Python script,  which  automatically \nsent  requests to the question analysis page of the web  application  using \nthe requests package, where the question was passed as a parameter. \nThis  script looped over queries stored in a  . txt file and submitted each \nof them to the locally hosted web application. This application  would \nthen save the results into a database. j son file,  which  is described in \nthe previous chapter. In total, the script was used to submit around \n360 queries,  which  belonged to 12 different categories. \n7.2 Analysis  Script \nThe central part of data analysis is done using a Python script. The \nscript  takes  a JSON file and parses it using functions from the json \nmodule.  Afterwards,  the final  data analysis is performed by averaging \nor summing up the values obtained from the  database  file. It also \ncreates  bar plots and tables,  which  are used to visualize the data in a \nmore comprehensive way. The analysis is done on the whole  database \nas well  as each specific category and query creation method. Further\u00ad\nmore, it calculates the metrics per model and the overall values to \ngauge the performance of the models accurately. Bar plots are then \nused to visualize the number of occurrences for each label used by \nthe MBFC  database.  Afterwards,  tables are created for the other met\u00ad\nrics,  specifically those that may be unreadable on the bar plot due \n37 \n\n7- DATA  ANALYSIS \nto having too many possible values. For  these,  a table showing the \nmost probable value along  with  their numbers  will  be generated. The \nanalysis mostly  uses  dictionaries to store the in-progress values, as the \nkey-value  structure is crucial in keeping most of the data organized. \nThe dictionaries are also easily plotable using matplotlib. \n7.2.1 Final  Values \nThere are two ways that the  final  values are decided - average or sum. \nAverage  is used to determine the  final  value of bias, factuality, and \ncredibility,  as well  as the percentages defined earlier. Sum is used for \nall the other metrics, such as the most used news types, countries, and \ndomains,  as well  as the counts of some specific site occurrences. The \naverages are calculated using the sum of the values for each specific \nquery and  dividing  them by the total number of queries.  These  will \nthen be assigned a label based on the  distribution  that can be seen in \nthe tables in the methodology section. The sums  will  count the total \nnumber of occurrences in all the queries. \n7.2.2 Output \nThe data  visualization  is done using the matplotlib module.  All the \ndata calculated by the script are saved in dictionaries. The plots are \nthen  visualized  using the  pyplot.  bar and  pyplot.  hist  objects  while \nthe tables are visualized using the pyplot .table  object.  The tables, \nbox plots and histograms are then exported into specific .png files, all \nof which  can be found in the attachments. \n38 \n\n8 Results \nThe analysis results have shown  a slightly concerning trend in  which \ntypes  of sources  the  LLM  search engines  prioritize.  This chapter  ex\u00ad\nplains  the  trends and similarities in  the collected data  by discussing \nsome  of the  obtained  distributions.  Specifically, all results  of the  com\u00ad\nplete  database  analysis  will  be shown, both per-model  and  overall. \nThis  includes all  the bar  plots, specifically bias, factuality, and credi\u00ad\nbility  bar  plots. Subsequently,  the  results  of the  country, media type, \nand domain occurrence analysis  will  also  be examined. Afterwards, \nthe results  of the  category-specific  databases  will  be reviewed. \n8.1 Overall  Results \nWhile  the  most important results  are  those that gauge  the  bias and  re\u00ad\nliability  of the  specific  LLM-based  search engines,  it is  also important \nto examine  the  full picture.  The  overall results  are  useful  for visual\u00ad\nizing  the  current trends  in LLM-based  search engines. Specifically, \nwhat  kind  of results they prefer, whether  it is  from  the standpoint  of \npolitical  bias  or country. Before proceeding  to the  results,  it is  crucial \nto acknowledge  the  possible bias in  the  MBFC  database,  as only  6561 \nout of 10151  citations were  identified  by MBFC.  Notably,  sites  such  as \nReddit,  Quora,  or YouTube  are not  present in  the  MBFC  database  as \nthey  are  impossible  to rate  accurately. Furthermore, there  is a  much \nhigher  amount  of USA sources present compared  to the  rest  of the \nworld.  Of course, this  is in part due  to the  fact  that  the  USA  is a huge \nEnglish-speaking  country, and  as such,  it is  natural that many more \nEnglish  sources come from there  as compared  to the  Czech Republic, \nfor example. Another reason  is that  MBFC  is based in  the  USA,  which \nmay be why  it is  focusing more  on American  sources.  Thankfully,  the \nlanguage bias  is not hugely concerning  as the  analysis shows that  the \ndifference between  the  USA and  the  second most numerous country \nlabel  is roughly  4700,  which  is more than  the  amount  of missing cita\u00ad\ntions.  Finally,  while  the  rating system  of MBFC  is inherently biased  as \nit is impossible  to conduct such  an analysis  of sources  with  zero bias, \nit is by such  a small amount that  it does  not  interfere  with  the  results \nof this thesis. \n39 \n\n8. RESULTS \nFirst, it is important to understand the  scope  of the data. A total of 478 \nqueries were analyzed, resulting in 10151 citations gathered and 6561 \nanalyzed.  Out of  these  citations, 2213 are from Perplexity,  4780  from \nyou.com , and 3158 from brave. This  means  that  Perplexity gives an \naverage  of 4-5 citations per result, You.com  provides 10, and Brave pro\u00ad\nvides 6-7. Out of  these  sources, 889 (or 8.7%) were social media  sites, \nspecifically Youtube, Tiktok, Reddit, Quora, Facebook, Instagram, or \nTwitter/X.  229 of  these  came  from Perplexity, 403 from you.com , and \n257 from brave. For Perplexity, this is about 10.3% of all citations, for \nyou.com  it is 8.4 % and for brave it is 8.1%. The most numerous country \nlabels for the  sources  were the USA  (5783),  United  Kingdom  (1007), \nGermany  (251),  France  (127),  and  Qatar(89).  This was roughly the \nsame  for all models. Likewise, the most numerous source types were \nOrganization  (3652),  TV Station  (1125),  Website  (1015),  Newspaper \n(847),  and Magazine  (602).  Finally,  the most cited  sites  can be  seen  in \nTable 8.1. The blue numbers signal  that  the source occurs in the top \n10 of its respective column. The first 10 domains are the most cited \noverall,  while  those  below the line complete the top 10 per-model. It \nis important to  note  that  most of  these  results, especially  those  that \nare not in the overall top, may be influenced by the choice of queries. \nAn interesting observation is the  fact  that  Perplexity  seems  to filter its \nsources  more than the other models, as indicated by the abundance of \nOs. This appears to mostly concern most social media, apart from video \ncontent  and news sources. From the source diversity standpoint, the \nmost diverse model is Perplexity  with  40% of domains being unique, \nfollowed  by brave  with  30 % and you.com  with  20%. \n40 \n\n8. RESULTS \nTable  8.1: Domains  with  most occurrences \nDomain Overall Perplexity You.com  Brave \nen.wikipedia.org 945 228 409 308 \nwww.quora.com 355 0 221 132 \nwww.bbc.com 276 0 169 107 \nwww.youtube.com 245 181 39 25 \nwww.reddit.com 223 0 127 96 \nwww.theguardian.com 200 0 109 91 \nwww.britannica.com 182 69 72 41 \nwww.nytimes.com 156 0 105 51 \nwww.pbs.org 149 30 73 46 \nwww.reuters.com 124 0 71 53 \nwww.instagram.com 40 38 1 1 \nwww.rferl.org 39 31 4 4 \nwww.amnesty.org 98 29 41 28 \nwww.cbsnews.com 70 26 40 4 \nabcnews.go.com 46 20 21 5 \nresponsiblestatecraft.org 27 20 4 3 \nwww.npr.org 114 0 92 22 \nwww.aljazeera.com 89 0 78 11 \napnews.com 113 0 54 59 \nwww.politico.eu 115 20 52 43 \n8.1.1 Bias, Factuality and  Credibility \nNow  that basic knowledge  of the sources  provided  by the  models  has \nbeen established,  it is  time  to move  on to the  results concerning  the \nthree most important metrics, the first of  which  is bias.  Overall,  the bias \nanalysis  yielded slightly concerning results. As shown in Figure  8.1, \nall models  are skewed towards  the left side  of the political  spectrum in \ntheir responses. While most  of the  answers  are  placed somewhere  in \nthe centre,  the  number  of answers placed in the Left-Centre and Right-\nCentre  are  extremely unbalanced  in all categories.  Likewise,  while \nthe amount  of more extreme  political  labels  is minimal,  the  imbalance \nis still  visible.  Interestingly enough, when  it comes  to the  Extremes, \n41 \n\n8. RESULTS \nthe Extreme Right is more  likely  to appear than the Extreme Left. The \nthird  most  likely  label is Pro-Science. The  full  distribution of response \nlabels can be  seen  in Figure 8.1. Regarding the non-political labels, the \nmost probable one is Pro-Science,  with  Satire and Conspiracy appear\u00ad\ning very rarely. The overall  percentage  of Pro-Science, Conspiracy, and \nSatire sources is 8.6%, 0.5%, and 0.04% respectively. The conspiracy \nand Satire sources may  seem  like an insignificant amount, but they \nstill influenced the final label in some  cases.  Furthermore, social media \nand public forums such as Quora are not accounted for in  these  per\u00ad\ncentages,  even though they are known to carry conspiracy or satirical \ncontent often. Specifically, You.com  and Brave used Quora and Reddit \nvery  frequently, while Perplexity preferred short video content, often \nAl-generated. \nWhile  the Bias Rating is not very straightforward, as it mostly aims \nfor balance rather than one specific value, Factuality and  Credibility \nare much  easier  to evaluate. For both of  these  metrics, the aim is to be \nas close to the top of the  scale  as possible. It is apparent that it should \nbe this way for the  LLMs  as well.  Thankfully,  while the  exact  values \ndiffer  per model, they are in general good at picking sources  with \nHigh  factuality and credibility. This can be  seen  in the overall label \ndistribution  of these  metrics in Figure 8.3 and Figure 8.4. \n42 \n\n8. RESULTS \nBias Counts per Result Group \nL nesultperp \nnesult_you \nnesult_brave \n^ 6 \nBias Labels \nFigure  8.1: Overall Bias Distribution \nWhen  examining the results of the analysis per-model, it is possible \nto gauge  the individual  amounts of bias in  each  LLM-based search \nengine further. The results yielded by such analysis, especially looking \nat the political bias are pretty damning. Based on  these  results, almost \nall of the assigned Right-leaning labels belonged to answers provided \nby Perplexity. The  rest  of the models show  extreme  imbalance in the \nlabel distributions of their results,  with  Brave having a tiny amount \nof answers labeled as Right-leaning and You.com  having none at all. \nThis can also be observed in the number of  sources  of each  label pro\u00ad\nvided  by the models. For Perplexity, the ratio of Left to Right leaning \nsources  is 478 to 204. In  contrast  to this, Brave's ratio is 1078 to 208, and \nYou.com's ratio is a  mere  1734 to 201. This is especially interesting, as \nwhile  Brave and You.com  had way lower  percentages  of Right-aligned \nsources  provided, they both provided  some  Extreme Right sources. \nPerplexity, on the other hand, provided None. The distribution of \nsource labels can be  seen  in8.2. \n43 \n\n8. RESULTS \nBias Distribution  per  Model \nBias Labels \nFigure  8.2: Overall  Source Bias  Distribution \nAs for factuality and  credibility,  all models perform very  similarly \nto the  overall  chart. The main difference in their factuality  distributions \nis the fact  that Perplexity often lands on the extremes more often than \nthe other two,  likely  due to how few sources it gives  with  each answer. \nDue  to that, it is more  likely  to only have one identified source per \nanswer, labeling the answer  with  the label of that source. On the other \nhand,  you.com  shows almost no extreme results,  which  means that \nmost of its answers are very balanced and,  overall,  the most factual of \nthe models. This is caused by its Very  High  sources being canceled \nout by its  Mixed  sources. Brave Search is placed in the middle of the \nprevious  two models. It, however, has the highest chance of offering \nanswers  with  Low or Very Low factual value,  while  not balancing it \nout with  a higher chance of  having  an answer  with  Very  High  factual-\nity. \n44 \n\n8. RESULTS \nFactuality  Distribution per Model \nFactuality  Label: \nFigure  8.3: Overall Factuality Distribution \nWhen it  comes  to their  credibility distributions, You.com  undoubt\u00ad\nedly is the  most  credible out of all  three,  with a majority of its  answers \nhaving the  highest  credibility label and an  extremely  low  chance  of \neven  providing a low credibility  answer.  Brave  Search  provided an\u00ad\nswers  with mostly High credibility and the  occasional  Medium or \neven  Low credibility. The Low credibility  answers  are sparse.  How\u00ad\never,  they  are numerous enough to show clearly in the distribution. \nPerplexity had the  least  High credibility  answers  and the  most  Low \ncredibility  answers. \n45 \n\n8. RESULTS \nCredibility  Distribution  per  Model \nCredibility  Label: \nFigure  8.4: Overall  Credibility  Distribution \n8.2 Results By Category \nMost  of the  results created  by analyzing each  of the  predefined  cat\u00ad\negories separately were very similar  to the  previously shown  distri\u00ad\nbutions.  Some  of the  categories, however, showed results  out of the \nnorm.  This section  will  discuss such results and give  an overview  of \neach  of the fairly controversial categories, showing how the  LLMs  deal \nwith  such topics.  The  plots and  tables  created  by analyzing each  of \nthe categories  can be found in  the attachments. \nAmerican  Elections \nThe first chosen category  is American Elections, specifically  the 2024 \nelections and  the  two leading candidates  - Donald Trump and Kamala \nHarris.  The  questions in this category focused on them  as presidents, \ntheir campaigns, and potential vote rigging from both sides. Almost \nall sources  provided  by the  LLMs  were from  the  USA,  with  72%. Only \n6% of the  provided sources were from social media, primarily  due \nto Perplexity's  usage  of YouTube. The bias  of each  of the  models was \nvery  different. You.com  showed  a very noticeable Left bias,  with  most \nof its answers being Left-Center. Brave's answers stayed mostly in  the \n46 \n\n8. RESULTS \nCenter,  with  a decent  amount leaning towards Left-Center. Perplexity \nhad the most balanced distribution of  political  answers. However, it \nhad a notable amount of Pro-Science answers,  which  is quite peculiar \nfor such a theme. There were no low factuality or credibility answers \npertaining  to this topic  provided  by You.com  or Brave; however, Per\u00ad\nplexity  had a major amount of low credibility answers and a wide \ndistribution  of factuality labels. \nGerman  Elections \nThe questions in this category focused on the  participating  parties and \ntheir leaders,  with  most of them focusing on the two biggest - AfD \nand CDU-CSU.  For German Elections, the  USA  was  still  the most used \nsource country, along  with  Germany,  with  a noticeable spike in other \nEuropean  countries such as France or Belgium. Outside of YouTube, \nReddit  was also very commonly used by the  LLMs  to answer  these \nquestions. This time, You.com  and Brave were leaning towards the \nleft,  with  the majority of their answers being Center or Left-Center. \nPerplexity  had a comparable amount of Left-Center and Right-Center \nanswers. The  credibility  was high for all models, and the answers were \nMostly  Factual. \nEuropean  Union \nThe EU category was mostly focused on the inner workings of the  EU, \nthe more controversial member countries, and countries  applying  for \nadmission.  Outside of the  USA,  the most numerous countries were the \ncountries of the European  Union,  specifically Germany and France, as \nwell  as its former member - the  UK.  For this category, the most used \nsocial  media  sites  were the forum sites, as many of the questions are \ncurrently  being discussed  within  those. The bias of Brave Search and \nYou.com  was largely the same, having mostly Center answers. When \nlooking  at the source bias  distribution,  however, they both picked a \ncomparable amount of Left-Center and Center sources. The perplexity \nbias labels were very evenly balanced between Left-Center and Right-\nCenter,  with  most answers being in the Center category. A few Pro-\nScience  answers were also found in each model,  likely  due to more \n47 \n\n8. RESULTS \ntechnical questions. Brave Search was the most factual, while the other \nmodels were slightly behind.  All of them showed high credibility. \nPope  + Conclave \nThis category was focused on the workings of the conclave, the death \nof Pope Francis, and the newly appointed Pope Leo XIV. It also had \nquestions pertaining to countries' involvement or the Vatican. Outside \nof the USA and the UK, Italy is the most numerous source country. \nYouTube and Quora were the main social media  sites  used by the \nLLMs.  The bias of all  three  models was heavily skewed towards the \nLeft-Center. This can also be observed in the source label distribution. \nCredibility  of answers was very high overall, and so was factuality. \nThis time, Perplexity proved to be the most factual. \nAmerica  + Trump \nCompared to the American Elections category, this one was focused \nmore on the current president as  well  as the current  state  of things \nin America. Almost all  sources  were American media,  with  a few \nfrom  the UK and other countries. The most used social media in \nthis category was Quora, along  with  YouTube and Reddit. The bias, \nfactuality, and credibility were comparable in all  LLMs,  being skewed \ntowards Left-Center,  with  high factuality, and high credibility. \nElon  Musk \nThis category has questions pertaining specifically to  Elon  Musk and \nprojects  affiliated  with  him, such as Tesla, SpaceX, or X. Most  sources \ncame  from the USA and the UK  once  again,  with  a noticeable spike in \nIndia.  Most social media showed up in the  sources  for this category, \npresumably due to his social media  presence.  The bias of Perplexity \nand Brave was  once  again concentrated in the Center, leaning slightly \ntowards the Left-Center, while You.com  was mostly Left-Center. The \nfactuality and credibility of the answers were noticeably lower than \nthose  of the other  categories  for Perplexity and Brave. \n48 \n\n8. RESULTS \nRussia + Putin \nThis category  focuses  on the current president of Russia, Vladimir \nPutin,  as well  as the country itself. It also  touches  upon the current \nsituation of the Ukraine war as  well  as the possibilities around it. Most \nsources  come  from the USA, the UK, and the EU countries, along \nwith  Ukraine, which shows up due to the topic surrounding it. Quora, \nReddit,  and YouTube lead the social media sources. The bias of all \nmodels is concentrated heavily around the Center.  There  is also a \nnoticeably higher amount of Right-Center  sources  in the source label \ndistribution.  The credibility of answers about this topic is very high, \nwhile  factuality is Mostly Factual, which is slightly lower than average. \nUkraine \nWhile  the previous category looks at the conflict from the perspective \nof Russia, this one is focused on Ukraine itself more,  besides  the war. \nThe country distribution is very similar to the previous category, as are \nthe social media. The bias is concentrated around the  center  slightly \nless,  with  a higher amount of Left-aligned sources. The factuality is \nnoticeably higher than the Russia category. Credibility is also very \nhigh-\nIsrael \nThe Israel category  focuses  on the Israel-Palestine conflict,  with  ques\u00ad\ntions pertaining to Israel being sorted here, and the questions talking \nabout Palestine and Gaza sorted in the following category.  Middle \nEastern source  usage  spiked in this category,  with  Israel, Qatar, and \nTurkey showing up more than in the other  categories.  Almost all so\u00ad\ncial media  sources  point to discussions about this conflict on Reddit \nand Quora. The bias is leaning further left than previously,  with  both \nYou.com  giving a high amount of Left-Center answers and Brave and \nPerplexity having a similar a noticeable Left lean. The highest  fac\u00ad\ntuality  can be found in the answers provided by brave,  with  both \nPerplexity and you.com  being slightly lower. The credibility is very \nhigh  for both You.com  and Brave. Perplexity, on the other hand, has \nnoticeably more medium credibility answers. \n49 \n\n8. RESULTS \nPalestine  + Gaza \nAs outlined earlier, this category talks about the other side of the \nIsrael-Palestine conflict. It discusses both the Palestinian people and \nHamas. The country and social media distribution are very similar to \nthose  in the Israel category,  with  comparable  sources  between both. \nThe differences in bias are  that  Perplexity  leans  slightly more towards \nthe Left and You.com  is more in the Center. However, the factuality \nand credibility are noticeably lower than in the Israel category. \nLGBTQ+ \nThe questions sorted into this category talk about LGBTQ+ rights, \npronouns, gender ideology, and pride.  Some  of the questions were \ngiven  in an arbitrary way,  with  a very broad scope. Almost all  sources \nprovided  by the  LLMs  are from the  USA.  Along  with  this, most of the \nsocial media  site  sources  come  from Reddit and Quora. For the first \ntime, Extreme Right labels can be  seen,  specifically in Brave Search \nanswers. Outside of this, Brave Search and You.com  lean heavily Left-\nCenter and Pro-Science in its sources, Perplexity is in the Center  with \na slight lean towards the Left and a sizeable amount of pro-Science \nanswers. The credibility of answers is very different for  each  model, \nwith  Brave and You.com  having mostly high-credibility answers and \nPerplexity having mostly medium  credibility.  The factuality is decently \nhigh,  with  most answers from You.com  and Perplexity being mostly \nfactual, and Brave Search having high factuality but also having  some \nanswers  with  low factuality.  These  were most  likely  the answers  that \nhad the Extreme Right label. \nClimate  Change \nThis category  touches  upon questions about the Earth's climate, green \nenergy, and  themes  such as coal tax and carbon footprint. Most  sources \ncome  from the USA and the  UK,  with  a spike in  sources  from  Hong \nKong.  The forums are  once  again the most used social media sources. \nThe answers of Brave Search and You.com  are heavily biased toward \nLeft-Center, while Perplexity has mostly Pro-Science and Center an\u00ad\nswers. The factuality is much higher compared to the average. The \n50 \n\n8. RESULTS \ncredibility  of Brave Search and You.com  is considered  high.  In contrast \nto this, Perplexity has a high amount of  low-credibility  answers. \nConspiracy  Theories \nThe final  category is conspiracy theories. The purpose of such a  cate\u00ad\ngory  is to  test  how each model  reacts  to conspiracy theories; whether \nthey lean towards denying them or discussing them in  full.  Most \nsources  came  from the USA or the UK, and the most used social \nmedia  sites  were YouTube and Quora,  which  are quite infamous for \ndiscussing  conspiracy theories in large numbers. The bias of You.com \nand Brave leans heavily towards Center, Left-Center and Pro-science \nanswers. Perplexity has majorly Pro-Science answers, as  well  as a bal\u00ad\nanced amount of Center, Center-Left, and Center-Right answers. It also \ncontains a single Conspiracy response. The credibility and factuality \nare both considered  high.  It seems  that most models are specifically \ntrained  to handle conspiracy questions, mostly explaining why it is \nbelieved  and why it is not true. It appears as though more Extreme \nviewpoints  manage to  slip  through  categories  such as LGBTQ+ much \nmore easily. \n51 \n\n9 Conclusions \nOverall,  it appears that  the  LLM-based  search engines that were  ex\u00ad\namined  in this thesis pick largely credible sources  with  decently  high \nfactuality.  However,  they  are  largely biased towards  one  side  of the \npolitical  spectrum. There appears  to be a  massive imbalance  of Right \nand Left news sources,  with  specifically Left-Center sources being \nused  even more than Center sources in some  cases.  Conspiracy and \nSatire sources also showed up in the results occasionally.  However,  the \npolitical  bias was  not the  only bias that appeared.  Country  bias was \nalso very noticeable, specifically towards  the  USA.  This  is understand\u00ad\nable as the  USA  is the  most populated  English-speaking  country, but \nthe usage  of USA  sources was  still  disproportionate  to other  English-\nspeaking  countries.  Finally,  the  usage  of social media sources could \nbe a source  of concern  as most  of them  are  considered more biased \ncompared  to traditional  news sources.  It is  also complicated  to gauge \nthe credibility  of such sources,  so it is  quite  unlikely  that  the  models \ncheck  the  content thoroughly. \n9.1 Distinctions  Between  LLM-Powered  Search \nEngines \nThe main  distinction  between the models is their  political  bias.  You.com \nis the  most biased, having almost  no answers that  would  be consid\u00ad\nered  Right-aligned.  It is  also  has  Left-Center  as its  highest occurring \nsource label,  by far. Due  to its  likely  focus on  a smaller, more tailored \nset of sources, however,  it does have  the best  credibility  and factuality \nstatistics.  It also  uses  a lot of  forum posts  as its  sources,  which  may \nmake  it even more biased. Brave's bias  is still skewed towards  the left, \nbut due  to its  bigger domain  distribution,  it is not as  extreme  as on \nYou.com . This middle approach  seems  quite detrimental  to the  model \nas it has a  decently  high  bias towards  the  Left-Center, but  it also lets \nsome very low  credibility  opinions through,  as can be seen by some \nanswers being labeled  as Extreme Right.  These  random extremes  in \nan otherwise credible and factual model may influence users more \nthan  if the  model were more  likely  to provide  less  extreme, low credi-\n52 \n\n9. CONCLUSIONS \nbility  answers, as users  would  be more  inclined  to believe the model's \noutput.  The use of forum sites for obtaining data is also concerning \nfor the same reason.  Finally,  Perplexity is the least biased of all three \nmodels,  while  also  providing  the most Pro-Science answers. It's  bal\u00ad\nanced approach  with  the highest domain  distribution  comes at the \ncost of its factuality and  credibility,  unfortunately. Due to the large \nspread  of its sources, it does not filter as many  low-credibility  domains \nas the other two models. Another distinction is that it does not use \nforum  sites; instead, it opts for videos, often in short  form,  likely  using \ntranscripts.  After examining several of  these  videos, they appear to \nbe mostly either Al-generated shorts or video essays. The issue  with \nsuch  sources is the same as  with  forum  posts - it is impossible to gauge \ntheir  credibility  and bias. \n9.2 Ethical  Implications \nThe ethical implications of the bias found in the models are  varied. \nIn the naive  sense,  it could be said that the fact that the models are \nbiased is in itself not ethical, as their answers are mostly believed to \nbe true due to human nature. This means that the models may easily \ninfluence  a person  with  their answers. While this is true to an extent, \nit may not be as unethical as it  would  seem at first glance. This is \nvisible  in the two approaches that  these  LLMs  take. The first being \ndecently balanced sources,  with  answers that attempt to describe both \nviews.  While this may seem  like  the better  option as it allows the \nuser to decide on what they want to believe, it also comes  with  lower \ncredibility  overall,  as the sources are not  filtered  as heavily.  This means \nthat even if the answer shows both sides, it may provide less credible \ninformation.  This is often the  case  with  Perplexity,  as can be seen from \nthe results. Furthermore, it is  likely  that the user  will  still  only pay \nattention to the parts of the output that affirm their  stance[6],  making \nthe unbiased answer less  meaningful.  The other approach,  using  much \nsmaller  sets  of sources,  while  also creating more biased answers, is \nbetter  when it comes to credible  information.  Yet, such models are also \nmuch  more  likely  to affirm  people  with  political  orientations similar to \nthe model in their opinions without  providing  any counterarguments, \ncreating an echo bubble of sorts. Altogether, it can be said that both \n53 \n\n9. CONCLUSIONS \napproaches have their own ethical issues,  which  are difficult  to erase \nfully.  Therefore, the main focus should be to balance both of  these \napproaches so that neither issue is too prominent. \n9.3 Important  Limitations \nFinally,  some limitations should be mentioned. Firstly, this analysis \ncould  only be done in one language,  English,  as there are currently no \ncomprehensive  databases  on news sources in most other languages. \nFurthermore, the You.com  sources were  provided  by a Search API \ninstead of a Smart  API,  as access  to the latter could not be gained. This \nmeans that the citations provided may be slightly different both in \nnumber and content, even if they use the  same  search engine.  Finally, \ndue to the use of float arithmetic, some of the result values might \nbe slightly different between calls of the  same  query  with  the same \nsources  provided  by the search engine, possibly ending up  with  two \ndifferent labels. While this should not  affect  the resulting distribution \nin a major way, it does bring slight  variability  into label distributions. \nSource label distributions remain unaffected by this. \n54 \n\nBibliography \n1. BROWN,  Tom  et al. Language  Models  are  Few-Shot Learners. \nIn: Advances  in Neural Information Processing  Systems.  Curran  As\u00ad\nsociates,  Inc.,  2020,  vol.  33, pp. 1877-1901.  Available  also  from: \nhttps ://proceedings .neurips .cc/paper_files/paper/2020/ \nfile/1457c0d6bf cb4967418bfb8acl42f64a-Paper.pdf. \n2. DAI,  Sunhao  et al. Bias and unfairness  in information  Retrieval \nSystems: New  challenges  in the  LLM  Era.  Proceedings  of the 28th \nACM  SIGKDD  Conference  on Knowledge  Discovery and Data  Min\u00ad\ning. 2024,  pp. 6437-6447.  Available  from  DOI:  10.1145/3637528. \n3671458. \n3. BENDER,  Emily  M. et al. On  the Dangers  of Stochastic Parrots: \nCan Language  Models  Be Too Big?  Proceedings  of the 2021 ACM \nconference  on fairness,  accountability, and  transparency.  2021, no. \nFAccT  '21,  pp. 610-623.  Available  from  DOI:  10 .1145/3442188 . \n3445922. \n4. GOLDMAN,  Eric.  Search engine bias and  the demise  of search \nengine  utopianism.  SSRN  Electronic  Journal.  2006.  Available  also \nfrom:  https  : //papers  . ssrn  . com/sol3/Delivery  . cf m/SSRN_ \nID893892_codel70891. pdf?abstractid=893892&mirid=l. \n5. KULSHRESTHA,  Juhi  et al. Search bias  quantification:  investi\u00ad\ngating  political  bias  in social media and web search.  Information \nRetrieval. 2018,  vol.  22, no. 1-2,  pp.  188-227.  Available  from  DOI: \n10.1007/sl0791-018-9341-2. \n6. SHARMA,  Nikhil;  LIAO,  Q. Vera;  XIAO,  Ziang.  Generative Echo \nChamber?  Effect  of LLM-Powered  Search Systems  on Diverse  In\u00ad\nformation  Seeking.  CHI  '24: Proceedings  of the  2024  CHI  Conference \non Human  Factors  in Computing  Systems.  2024,  vol.  CHI  '24, no. \n1033, pp.  1-17.  Available  from  DOI:  10.1145/3613904.3642459. \n7. MULLER,  Britney.  How  search  engines  work:  Crawling,  indexing, \nand ranking -  Beginner's  guide  to SEO. 2024.  Available  also  from: \nhttps  : //moz  . com/beginners  - guide-to  - seo/how- search-\nengines-operate. \n55 \n\nBIBLIOGRAPHY \n8. CANHASI,  Ercan.  Indexing:  Inverted  index.  2024.  Available also \nfrom:  https  : / /www  . baeldung  . com/cs/indexing-inverted  -\nindex. \n9. COLEMAN,  Basha.  Blog SEO: How to  Search  Engine Optimize \nYour Blog Content. 2024-06.  Available also from: https : //blog. \nhubspot.com/marketing/blog-search-engine-optimization. \n10. VASWANI,  Ashish  et al. Attention  is All you  Need.  arXiv  (Cornell \nUniversity).  2017,  vol.  30, pp. 5998-6008.  Available also from: \nhttps://arxiv. org/pdf /1706.03762v5 . \n11. LEWIS, Patrick  S. H. et al. Retrieval-Augmented Generation  for \nKnowledge-Intensive NLP tasks.  Neural Information Processing \nSystems.  2020, vol. 33, pp. 9459-9474.  Available also from: \nhttps  : / /  proceedings  . neurips  . cc /  paper  / 2020 /  file  / \n6b493230205f780elbc26945df7481e5-Paper. pdf. \n12. ROSS  ARGUEDAS,  Amy;  ROBERTSON,  Craig  T; FLETCHER, \nRichard;  NIELSEN,  Rasmus K.  Echo  chambers,  filter  bubbles,  and \npolarisation: a literature  review.  Reuters Institute  for the  Study  of \nJournalism,  2022.  Tech. rep. Available from  DOI:  10.60625/risj-\netxj-7k60. \n13. HANNAK,  Aniko  et al. Measuring personalization  of web search. \nIn: Proceedings  of the 22nd  international  conference  on World Wide \nWeb. 2013,  pp. 527-538.  WWW  '13.  Available from  DOI:  10.1145/ \n2488388.2488435. \n14. BERTOA,  Fernando Casal;  RAMA,  Jose.  Polarization: What  do \nwe know and what can we do about it?  Frontiers  in Political  Science. \n2021,  vol.  3. Available from  DOI:  10.3389/f  pos . 2021.687695. \n15. ATSKE,  Sara.  News  Platform  Fact  Sheet.  2024-09.  Available also \nfrom:  https  : / /www  . pewresearch  . org  / journalism  / f act-\nsheet/news-platform-fact-sheet/. \n16. WESTERMAN,  David;  SPENCE,  Patric  R.;  VAN  DER  HEIDE, \nBrandon.  Social Media  as Information Source: Recency  of Up\u00ad\ndates  and  Credibility  of Information.  Journal  of Computer-Mediated \nCommunication.  2013,  vol. 19, no. 2, pp. 171-183.  Available from \nDOI: 10.1111/JCC4.12041. \n56 \n\nBIBLIOGRAPHY \n17. MANIKANDAN,  Pavitra. LLM  Search  Engines:  AI-Driven  Informa\u00ad\ntion Retrieval.  2025. Available also from: https:  //clickup.  com/ \nblog/llm-search-engine/. \n18. CONSENSUS  AI. The  Ultimate  Guide  to AI search  engines:  What  are \nthey  and how to use  them  - Consensus:  AI Search  Engine  for Research. \nConsensus  AI,  2024. Available also from: https :  //consensus  . \napp/home/blog/the-ultimate-guide-to-ai-search-engines-\nwhat-are-they-and-how-to-use-them/. \n19. REID,  Liz. Generative  AI in Search:  Let Google  do the  searching  for \nyou. 2024.  Available  also from: https: //blog. google/products/ \nsearch/generative-ai-google-search-may-2024/. \n20. BING.  Introducing  the new  Bing.  The AI-powered  assistant  for your \nsearch.  Bing,  [n.d.].  Available  also  from:  https:  //www.  microsoft. \ncom/en-us/edge/features/the-new-bing?form=MA13FJ. \n21. GARG,  Sakshi.  What  is Arc  Search  and How  Does  it Work.  2024. \nAvailable  also from: https : //allthings . how/what-is-arc-\nsearch-and-how-does-it-work/. \n22. BRAVE  SOFTWARE.  Brave  Search  introduces  the Summarizer,  an AI \ntool  for synthesized,  relevant  results.  Brave Software, 2023. Available \nalso from: https: //brave. com/blog/ai-summarizer/. \n23. BRAVE  SOFTWARE.  Brave's  Leo is a  private  AI assistant  for desktop \nand mobile.  Brave Software, [n.d.]. Available also from: https : \n//brave.com/leo/. \n24. DUCKDUCKGO.  What  are AI-assisted  answers  on DuckDuckGo \nSearch?  DuckDuckGo, [n.d.]. Available also from: https : \n/ / duckduckgo . com / duckduckgo - help -  pages  / results / ai -\nassisted-answers. \n25. YOU.COM . What  are AI Agents  on You.com  and how to do  they \nwork?  You.com , [n.d.]. Available also from: https :  //you.  com/ \nsupport/what-are-ai-agents-on-you.com-and-how-to-do-\nthey-work. \n26. YOU.COM . How do I  access  different  AI models  ? You.com , [n.d.]. \nAvailable  also from: https : //home.  you.  com/support/how-do-\ni-access-different-ai-models. \n57 \n\nBIBLIOGRAPHY \n27. OPENAI.  Introducing  ChatGPT  search.  OpenAI,  2024.  Available \nalso from: https : //openai . com/index/introducing-chatgpt-\nsearch/. \n28. OPENAI.  ChatGPT  search.  OpenAI, [n.d.]. Available also from: \nhttps : //help . openai .  com/en/articles/9237897-  chatgpt-\nsearch. \n29. GOMEZ,  Daniela.  How  does  Perplexity  work?  [N.d.].  Available \nalso from: https : /  /www  . perplexity . ai/help-center/en/ \narticles/10352895-how-does-perplexity-work. \n30. MEDIA  BIAS  FACT  CHECK.  Methodology.  MBFC,  2025-03. \nAvailable  also from: https : / / mediabiasf  actcheck  . com / \nmethodology/. \n31. ALLSIDES.  How  AllSides  Rates  Media  Bias.  AllSides.  Available \nalso from: https :  //www  . allsides . com/media-bias/media-\nbias-rating-methods. \n32. AD FONTES  MEDIA.  About  Ad Fontes  Media.  Ad Fontes  Media, \n2025-04.  Available also from: https : //adf ontesmedia . com/ \nabout-ad-fontes-media/. \n33. NEWSGUARD.  Website  rating  process  and criteria.  NewsGuard, \n2025-01.  Available  also from: https  : //www.  newsguardtech. com/ \nratings/rating-process-criteria/. \n34. BRAVE  SOFTWARE.  Brave  Search  Ads report  massive  1500%  growth \nin click  volume  alongside  80%  increase  in organic  searches  in 2024. \nBrave Software,  2025.  Available  also from: https : //brave. com/ \nblog/2025-search-ads-update/. \n35. VAN  ZANDT,  Dave.  Media  Bias  Fact  check  Ratings  - API.  RapidAPI, \n2025.  Available also from: https : //rapidapi . com/mbf cnews/ \napi/media-bias-fact-check-ratings-api2. \n58 \n", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Analysis of Bias and Reliability in LLM-Powered Search Engines", "author": ["M STANKOVICOVA"], "venue": "NA", "pub_year": "NA", "abstract": "The rise of Large Language Models cannot be underestimated. They are slowly becoming a  part of almost everything, especially on the internet. Naturally, this includes search engines."}, "filled": false, "gsrank": 24, "pub_url": "https://is.muni.cz/th/rifoj/Bachelor_Thesis_Archive.pdf", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:98QySMs-B7kJ:scholar.google.com/&output=cite&scirp=23&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D20%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=98QySMs-B7kJ&ei=CLWsaLmrCI6IieoP0sKRuAk&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:98QySMs-B7kJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://is.muni.cz/th/rifoj/Bachelor_Thesis_Archive.pdf"}}, {"title": "Automatic large-scale political bias detection of news outlets", "year": "2025", "pdf_data": "ID: pone.0321418 \u2014 2025/5/10 \u2014 page 1 \u2014 #1\nPLOS ONE\nOPEN ACCESS\nCitation:R\u00f6nnbackR, EmmeryC, Brighton\nH(2025)Automaticlarge-scalepoliticalbias\ndetectionofnewsoutlets.PLoSOne20(5):\ne0321418. https://doi.org/10.1371/journal.pone.\n0321418\nEditor:ShadyElbassuoni,AmericanUniversity\nofBeirut,LEBANON\nReceived: November28,2024\nAccepted: March07,2025\nPublished: May12,2025\nPeer Review History: PLOSrecognizesthe\nbenefitsoftransparencyinthepeerreview\nprocess;therefore,weenablethepublicationof\nallofthecontentofpeerreviewandauthor\nresponsesalongsidefinal,publishedarticles.\nTheeditorialhistoryofthisarticleisavailable\nhere:https://doi.org/10.1371/journal.pone.\n0321418\nCopyright: \u00a92025 R\u00f6nnbacketal.Thisisan\nopenaccessarticledistributedundertheterms\noftheCreativeCommonsAttributionLicense ,\nwhichpermitsunrestricteduse,distribution,\nandreproductioninanymedium,providedthe\noriginalauthorandsourcearecredited.\nData availability statement: Weuseopenly\navailabledatasetsavailablevia https://www.\ngdeltproject.org/data.htmlfortheGDELTdataset ,\nhttps://mediabiasfactcheck.com/mbfcs-data-api/RESEARCH ARTICLE\nAutomatic large-scale political bias\ndetection of news outlets\nRonja R\u00f6nnback\n  \n\u2217, Chris Emmery\n  \n, Henry Brighton\nDepartment of Cognitive Science and Artificial Intelligence, Tilburg University, Tilburg, The Netherlands\n\u2217r.g.i.ronnback@tilburguniversity.edu\nAbstract\nPolitical bias is an inescapable characteristic in news and media reporting, and under-\nstanding what political biases people are exposed to when interacting with online news\nis of crucial import. However, quantifying political bias is problematic. To systematically\nstudy the political biases of online news, much of previous research has used human-\nlabelled databases. Yet, these databases tend to be costly, and cover only a few thou-\nsand instances at most. Additionally, despite the wide recognition that bias can be\nexpressed in a multitude of ways, many have only examined narrow expressions of bias.\nFor example, most have focused on biased wording in news articles, but ignore bias\nexpressed when an outlet avoids reporting on certain topics or events. In this article, we\nintroduce a data-driven approach that uses machine learning techniques to analyse mul-\ntiple forms of bias, and that can estimate the political leaning of hundreds of thousands\nof Web domains with high accuracy. Crucially, this approach also allows us to provide\ndetailed explanations for why a news outlet is assigned a particular political bias. Our\nwork thereby presents a scalable and comprehensive approach to studying political bias\nin news on a larger scale than ever before.\nIntroduction\nThe proper functioning of a democratic system presumes that its citizens have the tools to\nmake well-informed decisions. Yet, bias in news is unavoidable. Understanding how people\nare exposed to political bias when interacting with Web technologies like social media, search\nengines, or other sources is crucial for contributing to better-informed societies. This can be\nespecially important when the bias of the source cannot easily be anticipated, such as when\nthe source is unfamiliar. It raises the concrete question of how to study news bias. An imme-\ndiate challenge lies in how to measure political bias in the first place; most would agree that\nFox News and the Guardian behave differently and occupy different locations on the polit-\nical spectrum. However, the assumption of a left-to-right political spectrum is by no means\nuncontroversial [ 1\u20133]. It is a simplifying one that many (our work included) choose to make\nto be able to systematically study political biases in online services. Many researchers mea-\nsure bias by using such labels, which detail properties like reliability or political leaning of\nnews. These labels are generally derived in one of two ways: human labelling or computational\nlabelling.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 1/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 2 \u2014 #2\nPLOS One Automatic political bias detection of news outlets\nHuman labelling can allow researchers to deal with ambiguity and contextual information,\nfortheMediaBiasfactCheckdata,and\nhttps://personalization.ccs.neu.edu/Projects/\nPartisanship/ forRobertsonetal.\u2019sdata.Our\ncodeusedtoprocessthesedatasetsisavailable\nathttps://github.com/rtronnback/automatic_\nnews_monitoring_with_GDELT .\nFunding:Theauthor(s)receivednospecific\nfundingforthiswork.\nCompeting interests: Theauthorshave\ndeclaredthatnocompetinginterestsexist.but this approach is often slow, laborious and, ironically, can be subject to bias as well [ 4]. On\nthe other hand, data-driven approaches enable fast and efficient analysis of news through Nat-\nural Language Processing (NLP) and Machine Learning (ML), yet fail to provide the same\nlevel of insight as manual labelling, falling short of actually increasing understanding of the\nphenomenon [ 4].\nMoreover, many simplify even further by focusing on narrow expressions of bias in, for\ninstance, word choice in headlines. Yet, some forms of bias cannot be detected unless exam-\nining the holistic behaviour of a news outlet, rather than individual news items. For exam-\nple, an outlet systematically avoiding a topic, or only covering it very briefly despite societal\nrelevance, can be a clear sign of bias. Yet, this is often not considered in existing research.\nGiven these two challenges, our work proposes an approach that uses automatic labelling\nof news web-domains\u2019 bias on a global scale. To do this, we use ML to predict web-domain\nbias using the Global Database of Events, Language, and Tone (GDELT). GDELT tracks and\nanalyses global news, making it an ideal source for this task [ 5]. It enables us to: i) focus on\naggregated outlet information rather than article-, sentence- or word-level analysis (pre-\ndominant in related work), ii) differentiate between multiple types of bias to review their\nimpact, whereas much of previous research studies one sub-type, and iii) evaluate model per-\nformance against the provenance of the true bias labels (either computationally derived or\nhuman-annotated). Finally, we combine these computational methods with techniques for\nmodel explainability to extract the approximate reasoning behind why a news web-domain\nin question is deemed to be politically biased. We believe our analyses may prove meaningful\nfor establishing recurrent problematic behaviour on the part of news outlets in an automatic\nmanner, and, if developed further, could help citizens inform themselves as to the partiality of\ntheir news sources.\nBarriers of news bias studies: Narrow focus, scope and lack of\ninsight\nPolitical bias is challenging to define and more often than not considered to be subjective, but\ngenerally refers to a recurring (intentional or unintentional) attempt to influence a reader [ 4,\n6]. There are many ways that bias can manifest itself in news media. This can range from the\nselection of what events to cover, where an article should be placed on the homepage, how\nmuch space to give it, or whether to (as the classic example goes) refer to \u201cfreedom fighters\u201d\nas opposed to \u201cterrorists\u201d [ 4,7]. As a consequence, measuring bias raises a lot of practical\nproblems, and many have focused on studying very specific types of bias to simplify the task.\nWhat follows is a succinct overview of biases identified in Hamborg, Donnay and Gipp\u2019s [ 4]\nliterature review, and that we focus on in our analyses.\n\u2022Event selection bias or coverage bias involves choosing which events merit report. Nat-\nurally, not all stories can, nor should be published. Yet, intentional and consistent avoid-\nance of or focus on a topic can influence or mislead audiences. This is a well-studied phe-\nnomenon in crime reporting [ 8\u201310].\n\u2022Labelling andword choice bias are a major focus of study. This concerns framing events or\nhighlighting a certain perspective by choosing labels or particular words that, while similar,\nwill convey different meanings to audiences: for example, referring to something as a \u201cspe-\ncial military operation\u201d or \u201cintervention\u201d instead of \u201cinvasion\u201d may change perceptions of\nevents.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 2/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 3 \u2014 #3\nPLOS One Automatic political bias detection of news outlets\n\u2022Size allocation bias concerns the length of articles. The amount of text written on some\ntopics may introduce certain outlet biases. For example, it is possible that news outlets\nreport consistently but only at brief length on certain topics while dedicating a lot of work\nand space to others. This is a relatively straightforward form of bias to study, though it has\nnot received much attention [ 4].\n\u2022Picture selection andexplanation bias concern what pictures are chosen to accompany\ncertain articles, and how those pictures are described. Images have been shown to affect\nreaders\u2019 perceptions of news articles [ 11,12], therefore selecting and describing them is\nsusceptible to potential biases.\nThese subtypes of bias have, to varying extents, been examined in previous literature. How-\never, it is rare that a single study encompasses more than one form of bias. Furthermore,\nmany have focused on making article-level inferences (trained directly on the content of the\narticles, and therefore often fixating on word choice bias), rather than outlet-level inferences\n(based on meta-data of multiple articles, which could encompass multiple of the bias subtypes\noutlined above).\nOn article-level, studies have used computational (NLP) tools such as Term Frequency-\nInverse Document Frequency [ 13,14] or doc2vec [ 15] as feature representation meth-\nods, but many have had limited success [ 16,17] and rely on costly resources [ 16\u201318]. Gan-\ngula et al. [ 18], for example, aimed to predict news bias towards five local political parties\nbased on headlines, articles and a combination of the two. They achieved an accuracy of 89%\nwith an attention-based model. However, the narrow focus and reliance on very specific\nhuman annotations limits the work\u2019s ability to scale to a wider context and to provide deeper\nunderstanding of political bias on the whole. Spinde et al. [ 19], on the other hand, use exist-\ning labelled datasets from Reddit comments, movie reviews, Wikipedia, and two general lan-\nguage datasets. These were combined to train a DistilBERT model [ 20] in a Multitask Learn-\ning setting. While the results look promising (F1-score of 0.77), their results are only partially\ntransferable to news due to the data being only indirectly related to news bias, as they them-\nselves note. A follow-up study compiled a dataset of 3,700 sentence-level expert annotations\non a broad range of topics in lieu of the usual crowd-sourced annotations. BERT-based mod-\nels [21] detected sentence bias, achieving a maximum F1-score of 0.80 [ 22]. This constitutes\nan improvement, though backs off to extensive manual annotation and still limits the focus\nto word choice bias on an article-level. Finally, some previous work has focused on detecting\na dramatically wider range of broad bias subtypes on sentence-level (ad hominem or circular\nreasoning bias, for example) [ 23]. These distinctly focus on political bias as a subtype, how-\never, rather than as a nuanced subject that can be expressed in a number of different ways [ 4],\nas the current work does.\nNot all existing work focuses on article- or sentence-level bias, or even uses ML to esti-\nmate website or news political bias, however. For example, using the Twitter accounts of users\nwho were registered as either Republican or Democrat voters, Le, Shafiq and Srinivasan [ 24]\napproximated bias based on how often users shared articles from outlets or websites. Articles\nshared frequently by Republicans would thus be assumed to stem from web-domains with a\nright-wing political leaning, and vice versa. Given this method, Robertson et al. [ 25] assigned\nand validated political leaning scores for over twenty thousand websites. This approach\nscales well and provides follow-up studies with validated political bias scores. Nevertheless,\nit presents an approximated measure of bias and does not delve deeper into what makes a\nparticular outlet more biased towards a political audience.\nWork that bears resemblance to our own is MediaRank [ 26], which also opts for a source-\nlevel analysis to create quality rankings of the world\u2019s most prominent news sources. Using\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 3/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 4 \u2014 #4\nPLOS One Automatic political bias detection of news outlets\nmetrics such as reputation, reporting bias, financial pressure, and popularity, they evalu-\nate over 50 thousand news sources in 68 countries. There are some important distinctions,\nhowever: firstly, they rely on an outlet\u2019s average sentiment regarding celebrity Democrats\nand Republicans. In contrast, our approach makes use of the millions of GDELT themes and\nready-made NLP features that have been selected in an entirely data-driven fashion. This\nmakes our approach more comprehensive and globally applicable (also for alternative divides\nof the political spectrum), since American politicians or celebrities will not be extensively dis-\ncussed everywhere. As such, our project can provide an assessment of news sources that is\nmore focused on an in-depth examination of political bias using multiple related features and\nthemes. Therefore, while MediaRank focuses on a generalist perspective of news quality, our\napproach offers a more in-depth study of political bias, the numerous ways it can manifest,\nand does this at a globally applicable scale.\nIt is worth noting that large language models (LLMs) present a promising avenue for large-\nscale sentence- and article-level bias classification. LLMs offer a low barrier of entry to inter-\nfacing with large amounts of textual data, and especially to extract information from complex\nstructures. They have therefore been used to for example automatically detect misinformation\nor fake news [ 27\u201329], and various forms of bias [ 28,30,31], and might therefore be deemed\nas relevant for our project as well. However, LLMs-based approaches face some serious chal-\nlenges. Namely, LLMs are subject to various internal political biases [ 32,33], and seem to con-\nsistently differ from human judgement [ 27], which presents a serious complication for LLM-\nbased applications [ 34]. Additionally, their inherent stochasticity implies that the accuracy of\noutputs may differ significantly despite receiving the same prompts, and has been shown to\nproduce contradicting results when dealing with political disinformation [ 29]. Previous work\nhas also aptly noted that LLMs do not receive regular updates, and that this may present a\nproblem in the rapidly evolving news cycle [ 31]. Finally, the quality of LLM classifications has\nalso been shown to not match the performance of fine-tuned supervised models on numerous\napplications (social understanding [ 35], media bias detection [ 28], as well as other social sci-\nence tasks [ 36,37]). Despite these limitations, we provide two naive zero-shot LLM baselines\nfor comparative purposes. This is relevant given the aforementioned potential of LLMs to\nreplace manual labelling and ease of use, but importantly also allows us to examine if the per-\nformance limitations found in previous work are repeated here. It is worth noting that, while\nLLMs offer advantages like ease of use, there are also trade-offs to consider, such as, hardware\ncosts, output fabrication, and energy consumption, especially if used on a large scale, among\nother challenges [ 38].Appendix E details the implementation and prompting used.\nThe aforementioned examples all demonstrate the potential of data-driven news bias\ndetection but also the existing limitations. Focus on article- or sentence-level bias excludes\nimportant patterns that emerge from a news web-domain\u2019s behaviour as a whole, such as\ncoverage, story placement and size patterns (i.e., are some topics avoided, only given lim-\nited space, or only reported upon very briefly). These could play a key role in demonstrat-\ning and explaining bias. Additionally, many studies rely on costly expert or crowd-sourced\nannotations [ 18,22], or on tangential datasets [ 19].\nCrucially, to identify media bias of news outlets at scale, it is not sufficient to rely on spe-\ncific topics, small-scale datasets or only on sentence- or article-level classifications. Instead,\nnews bias monitoring should ideally cover a multitude of topics and be applicable to (nearly)\nany web-domain, whether this be from well-known sources such as breitbart.com or cnn.com\nor from lesser known organisations. Therefore, our work aims to develop a system that relies\non a broader data source which specifically covers news, facilitates global-scale coverage,\nand is capable of examining multiple facets of media bias more thoroughly. As a result, this\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 4/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 5 \u2014 #5\nPLOS One Automatic political bias detection of news outlets\napproach is entirely data-driven and reliant on automated techniques, rather than time-\nconsuming manual approaches. We believe this could hugely benefit the field and ensure a\nholistic coverage of news bias.\nAn approach for large-scale bias labelling\nTo make inferences about web-domains on a global scale, one of course needs a data source\nwith global coverage. This is possible thanks to the Global Database of Events, Language, and\nTone (GDELT), as well as some features from the independent media bias tracking organi-\nsation Media Bias Fact Check (MBFC). For further details of implementation, please refer to\nAppendix B .\nGDELT [5] is an open platform for monitoring global news, and the most extensive\ndatabase covering news in existence, to the best of the authors\u2019 knowledge. It has been used\nin related previous work examining the use of images in news [ 39], as well as the rise fake\nnews [ 40]. Compared to similar datasets, it has been found to contain a broader set of unique\nnews outlets [ 41], making it ideal for our application. The main dataset of interest here is\nthe Global Knowledge Graph (GKG), which records \u201clatent dimensions, geography and\nnetwork structure of the global news\u201d [ 5, p. 1]. It contains various automatically identified\nthemes associated with articles (these are extensive, covering topics ranging from immigra-\ntion to gasoline prices or even specific currencies or mammals) and the results of various\nanalyses, such as tone. Given the size of the dataset (one year\u2019s worth of data corresponds to\n2.5TB [ 42]), our experiments use a limited sample consisting of English articles from the year\n2022. This excludes articles that have been translated, and subsequently focuses on Western\ncountries which approximately follow the bipartisan political spectrum. Though there may\nbe some slight selection bias given the outbreak of the COVID-19 pandemic, we expect that\nthe breadth of GDELT alleviates concerns of generalisability. We filtered all features that we\ndeemed unlikely to reflect bias, resulting in features detailed in Table 1 .\nWe found more than 30,000 unique themes in our sample. To reduce their sparsity, we\nopted to filter over-specific taxonomic items (e.g., specific birds, mammals, or fish), and\nother themes appearing less than 1000 times and more than one standard deviation from\nthe mean of the log transform of theme frequency. In addition to maintaining a data-driven\napproach, this method does not limit the sample to known politically controversial themes\nTable 1. GDELT features included in analysis, accompanied by short description.\nGDELT Feature Description\nTone The positive and negative tone of the article as a whole.\nPolarity Proportion of words that matched a tonal dictionary to indicate how polarized the\ntext is. For example, a high polarity but similar scores for positive and negative\ntones indicates that the article contains roughly the same amount of positively and\nnegatively charged words.\nActivity reference density Percentage score of active words, like active verbs, in the article and is supposed to act\nas a proxy for \u201cactiveness\u201d in the text as compared to merely descriptive text.\nSelf or group reference density Percentage of pronouns present. The GDELT documentation states that this \u201ccan be\nused to distinguish certain classes of news media and certain contexts\u201d [ 43].\nWord count Number of words in the article.\nVisual content Presence of cover images, embedded social media images or videos related to the\narticle. This was noted merely as either \u201cpresent\u201d or \u201cabsent\u201d for the purposes of out\nwork,\nThemes A list of the themes detected in the article as by GDELT\u2019s analysis.\nhttps://doi.org/10.1371/journal.pone.0321418.t001\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 5/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 6 \u2014 #6\nPLOS One Automatic political bias detection of news outlets\n(such as immigration, abortion, or climate change [ 22,44]) thereby potentially including\nunder-explored indicators of bias.\nAdditionally, to account for other forms of bias beyond those found in tone or sentiment\nanalyses, we added a set of outlet-level features per theme: the proportion of articles (as this\nmight allow for examination of selection bias, and whether some topics are ignored or exces-\nsively focused on), the average word count per article (which might reveal size bias), and the\npresence of images or videos (which could also be indicative of under-explored forms of bias).\nCrucially, this aggregation based on themes makes it easy to notice if some theme is largely\nignored or alternatively excessively focused on by a source (for example, if a source very rarely\nand only briefly covers news related to global warming, the proportion and average word\ncount of articles for that theme would be very low). This preprocessing yields one row per\nweb-domain, with associated bias features from GDELT per theme. Due to the high num-\nber of features this resulted in, we also eliminated some features (for details, see Appendix B ).\nFig 1 provides an overview of these preprocessing steps.\nAside from GDELT, we also used some information from Media Bias Fact Check ( MBFC ).\nMBFC provides information about the factuality, traffic, country of origin, press freedom,\nmedia type, and credibility of news web-domains. As such information is relevant but not\navailable through GDELT, we extended the training dataset with these features (see Appendix\nAfor a full explanation of the features). These additional features were however only included\nin one of the experiments, as is further detailed in the sections below.\nLabelling: Human reliance or fully automated?\nGiven the laborious nature of human labelling, it would be cheaper and faster to rely on auto-\nmatic methods, but only if performance is somewhat comparable. To better examine this,\nwe considered model performance on two datasets of ground truth labels. One is composed\nof human evaluations from MBFC, whereas the other is automatically derived by Robert-\nson et al. [ 25].\nMBFC is an independent organisation that estimates media bias based on human eval-\nuations [ 45]. Their main aim is to promote awareness of bias and misinformation. For each\nnews web-domain, they provide a political leaning label and some other noteworthy metrics\n(these are described in detail in Appendix A ). The bias label is determined based on a set of\ntopics like, to only name a few, immigration, economic policy, or social views [ 44]. They do\nhowever note that these topics are derived from an American perspective, and may therefore\nFig 1.Overview of data aggregation process. Overview of process to aggregate GDELT data from article- to outlet-level instances, containing themes and their\nrespective average GDELT features.\nhttps://doi.org/10.1371/journal.pone.0321418.g001\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 6/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 7 \u2014 #7\nPLOS One Automatic political bias detection of news outlets\nnot perfectly apply to all countries. While there are numerous entities collecting similar rat-\nings of political media bias and we acknowledge that no ground truth is perfectly unbiased or\naccurate, MBFC\u2019s methodology is thoroughly documented [ 46], and their dataset is open and\nextensive ( Fig 2 ). Their labels serve as a ground truth value, and has been used for this same\npurpose in previous work as well [ 26].\nThe other set of labels stems from research by Robertson et al. [ 25], who built a dataset\nof bias scores for nearly twenty thousand websites (henceforth referred to as PABS,\nretrieved from github.com/gitronald/domains/tree/master/data/bias_\nscores ). By relying on Twitter users who were officially registered as either Republican or\nDemocrat voters, they collected all links to web-domains that these users shared on the plat-\nform. Operating on the assumption that users would predominantly share links to domains\nthey agreed with, they create a proxy score of the political bias of a web-domain based on the\nproportion of times it was shared by Democrat versus Republican users. Scores range from -1\nto 1, wherein -1 indicates that the source was shared exclusively by Democrats (left-learning\nbias), and a score of 1 indicates it was shared exclusively by Republicans (right-lean bias).\nAutomatically derived scores such as these are cheaper to obtain but might not be as accurate\nas human-made labels. We examine whether this is the case by comparing whether results dif-\nfer between such proxy-labels and human-made labels. The data was preprocessed by binning\nthe continuous values into the five bias classes.\nClassification with machine-learning models\nWe trained various ML models to classify web-domain bias based on the preprocessed\nGDELT features, using either MBFC or PABS data as a ground truth [ 25]. Serving as a point\nof comparison, a majority baseline model was implemented (i.e., one that invariably classifies\nall instances as the most common class: \u201cleast biased\u201d). The other models included a feed-\nforward neural network that was trained using Pytorch [ 47], and Support Vector Machine\n(SVM, [ 48,49]), AdaBoost [ 50,51], and XGBoost classifiers [ 52] We lastly also include two\nFig 2. Frequency of political lean classes per dataset, on a logarithmic scale.\nhttps://doi.org/10.1371/journal.pone.0321418.g002\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 7/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 8 \u2014 #8\nPLOS One Automatic political bias detection of news outlets\nbaselines using a naive zero-shot LLM with Llama 3.1 [53] and GPT-4o mini [54],\ndetailed further in Appendix E .\nWe empirically determined a two-linear-layered network to perform best (with ReLU acti-\nvation, batch normalization and a dropout-rate of 0.5 between each layer [ 55\u201357]). Training\nwas done via the Adam optimizer [ 58] and negative log-likelihood loss. Additionally, as one\nof the experiments involved categorical variables from MBFC, we embedded these [ 59,60].\nThe complete structure of the networks can be found in Appendix C . All other models are\ntrained using ten-fold cross-validation and optimized using halving grid search [ 61] for tun-\ning; the hyper-parameters used in the grid search are detailed in Appendix B andC.\nModel explainability for insight into web-domain bias\nHamborg, Donnay and Gipp [ 4] criticize computational analyses of media bias of lacking\ninsight into how bias is manifested. Indeed, numerous previous studies have merely focused\non determining whether it is present [ 16,18,19,22]. We therefore opted to address this through\nthe use of computational methods that provide explanations of model decisions. Specifi-\ncally, we used Shapley Additive Explanations (SHAP), which expands upon six pre-existing\nmethods [ 62] (examples include LIME [ 63] and Layer-wise Relevance Propagation [ 64]). The\nSHAP framework provides model-agnostic explanations, meaning it can be applied to tradi-\ntionally inscrutable black-box models. SHAP averages the differences in the model\u2019s output\nwith and without a particular feature; the resulting set of differences is then used to approx-\nimate the Shapley values for each feature, representing the contribution of that feature to a\nprediction. This allows us to provide thorough outlet-specific explanations rather than sim-\nply model-level insight (as is the case for other traditional explainability frameworks). Con-\nsequently, we can scrutinize any news domain to understand why a model classifies it as left-\nor right-wing biased, providing direct insight into the manifestation of bias and therefore\naddressing the critique of Hamborg, Donnay and Gipp [ 4].\nExperimental setup and testing\nOur first three experiments were repeated using either MBFC (human-made labels)\nor PABS scores (automatically derived labels) as ground truth labels. This analysis\nserved to determine whether either labelling method is more successful; as automati-\ncally derived labels are easier to obtain but may be less accurate, such a comparison is\ninformative. Finally, a post-hoc analysis examined the difference between the bias labels\nby MBFC and PABS. The code necessary to replicate our experiments is available at\ngithub.com/rtronnback/automatic_news_monitoring_with_GDELT . The\nfollowing paragraphs provide an overview of each experiment:\nTraditional Bias Experiment trained models on data related solely to word bias, mean-\ning it covered features related to tone, polarity, activity- and self/group reference density\u2014\nfeatures that have been more extensively studied in prior work.\nAlternative Bias Experiment used word-, article-counts, and image or video presence,\naiming to better glean the significance of lesser-studied forms of bias; namely, size, selection,\nand picture bias respectively (the content of the images is not accounted for, thus this feature\nonly approximates picture bias).\nFull Bias Experiment used all features. This structure aimed to allow for a better examina-\ntion of the information value of different forms of bias and to extend the analysis beyond the\ntraditional focus of word bias.\nFull Bias & Categorical Features Experiment was conducted adding various categorical\nfeatures provided by MBFC such as credibility, factuality ratings, traffic estimates, country\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 8/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 9 \u2014 #9\nPLOS One Automatic political bias detection of news outlets\npress freedom index, and media type (see Appendix A for full list). We expected these fea-\ntures to be informative and thus conducive to improved performance. This experiment was\nonly done for the full bias dataset using MBFC as the ground truth. For an overview of all\nexperiments, see Fig 3 .\nResults\nModels were evaluated based on how well they predict a news web-domain\u2019s political bias,\nand results are detailed in Table 2 . The overall best-performing model was the neural network\ntrained on the full dataset supplemented with categorical features of MBFC (see Appendix A\nfor the full descriptions). Some examples of news domains, the ground truth and the model\u2019s\npredictions are shown in Table Table 3 . It classified web-domains with an accuracy of 76%,\nand an AUC score of 81%, compared to the baseline model which achieved 45% and an AUC\nof 50%. The LLM baseline performed similarly to this naive baseline. Models using multiple\nmanifestations of bias generally achieved better performance compared to those using tradi-\ntional or alternative forms of bias only. The confusion matrices of the best performing model\nunder each experimental condition (traditional bias, alternative bias or both) are shown in\nFig 4 . Models trained on MBFC as ground truth outperformed models trained on PABS,\nwhich achieved only a maximum accuracy of 58.2% and an AUC of 70% with the neural\nnetwork trained on the alternative bias dataset.\nWe performed some simple error analysis to examine the strengths and limitations of the\nbest performing model. Detailed results can be found in Appendix F . Considering the dif-\nferent classes, the model performs best at classifying right-wing sources, followed by least\nbiased, and right-centre. Left and left-leaning results were harder for the model to correctly\ndetect. Furthermore, the error rates of the model were lowest for outlets with minimal and\nmedium traffic. This is interesting given that low-traffic websites are often more challenging\nFig 3.Overview of experiments used to test models. To test the impact of different bias-related data, models were trained on subsets of\nthe data: traditional bias data (features related to tone, polarity, activity and self/group reference density); alternative bias data (features\nof word-, article-counts, image- or video presence); and the combination of all these features: full bias data. An additional experiment\ntested model performance on the full dataset when supplemented with categorical features from the MBFC data.\nhttps://doi.org/10.1371/journal.pone.0321418.g003\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 9/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 10 \u2014 #10\nPLOS One Automatic political bias detection of news outlets\nTable 2. Model results per experiment.\nDataset Model TB Dataset AB Dataset FB Dataset FB&C Dataset\nAcc. AUC Acc. AUC Acc. AUC Acc. AUC\nMBFC\nMBL 0.454 0.500 0.454 0.500 0.454 0.500 0.454 0.500\nSVM 0.471 0.510 0.521 0.570 0.681 0.701 0.750 0.790\nAdaB 0.681 0.730 0.579 0.680 0.723 0.770 0.750 0.780\nXGB 0.622 0.700 0.546 0.640 0.655 0.720 0.648 0.690\nNN 0.689 0.730 0.630 0.710 0.664 0.750 0.765 0.813\nLlama 3.1 - - - - - - 0.531 0.622\nGPT-4o mini - - - - - - 0.555 0.627\nPABS\nMBL 0.408 0.500 0.408 0.500 0.408 0.500 - -\nSVM 0.537 0.630 0.537 0.650 0.569 0.740 - -\nAdaB 0.556 0.650 0.592 0.660 0.547 0.70 - -\nXGB 0.444 0.580 0.531 0.640 0.579 0.660 - -\nNN 0.479 0.620 0.582 0.700 0.527 0.680 - -\nLlama 3.1 - - - - - - 0.495 0.632\nGPT-4o mini - - - - - - 0.505 0.623\nThe test set performance of models classifying political leaning of media web-domains under various experimental\nconditions: traditional bias features (TB Dataset; tone, polarity, activity- and self/group reference density), alternative\nbias features (AB Dataset; word- and article count, image- or video presence) and full bias features (FB Dataset). The\nlast column to the right presents the performance of a model trained on the full data, supplemented with categori-\ncal features derived from MBFC\u2019s dataset (FB&C Dataset). Evaluation metrics are accuracy (Acc.) and AUC-score,\nand the best-performing model is highlighted in bold for each experiment. The tested models include a baseline\nmodel (MBL), a support vector machine (SVM), an AdaBoost model (AdaB), an XGBoost model (XGB) and a neu-\nral network (NN). Note that the LLM baselines using Llama 3.1 andGPT-4o mini follow a slightly different\nimplementation (see Appendix E ), and are displayed under FB&C merely for the sake of brevity.\nhttps://doi.org/10.1371/journal.pone.0321418.t002\nTable 3. Output examples. Examples of domains with corresponding predictions and ground truths. Predictions\nwere made using the best performing NN model.\nDomain Ground Truth Prediction\ninvestmentwatchblog.com right right\nirishtimes.com left left\nwishtv.com left-leaning least biased\n12news.com least biased least biased\nkhou.tv least biased least biased\nbicesteradviser.com least biased least biased\ndailyprogress.com least biased least biased\ndailysignal.com right right-leaning\nisraelnationalnews.com right-leaning left-leaning\n12news.com least biased least biased\nheraldpalladium.com right-leaning right-leaning\nhttps://doi.org/10.1371/journal.pone.0321418.t003\nwhen trying to determine political bias, but therefore also of central importance. Readers are\nexpectedly aware of the political orientation of popular news sources like Fox News or The\nGuardian, and can therefore anticipate the slant of the information. The main difficulties arise\nwhen a reader encounters a lesser-known source, where the potential bias is unknown. This\ncan be polarising in spheres where information is uncertain and moves at a rapid pace; as\ntends to be the case online, and especially on social media. A recent well-known case of this\nis the rebranding of Twitter to X, and the subsequent shift in tonality and bias. As such, our\nmodel\u2019s increased performance on lower-traffic websites is highly encouraging for dealing\nwith unfamiliar sources\u2019 biases.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 10/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 11 \u2014 #11\nPLOS One Automatic political bias detection of news outlets\nFig 4.Confusion matrices. Confusion matrices of the predictions by the best performing models per task.\nhttps://doi.org/10.1371/journal.pone.0321418.g004\nModel explanations\nSHAP decision plots can be made for any given web-domain, so some representative exam-\nples were selected for visualisation. The results can be found in Figs 5 \u20139(due to limitations\nin the SHAP library, these pertain the comparably performing SVM model). Generally, the\ncategorical features from MBFC frequently appear in the top most important features, with\nthe exception of the Press Freedom Index. Geographical location also structurally appears as\nan informative feature in the provided examples. It is unclear what precisely about the coor-\ndinates influences the model. As the they refer to countries, rather than regions, this may\nreflect an approximate correlation with democracies and autocratic regimes. However, this\nshould reasonably also be reflected in the Press Freedom Index, which seems to have been\ndisregarded.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 11/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 12 \u2014 #12\nPLOS One Automatic political bias detection of news outlets\nFig 5.Decision plot of Breitbart, a right-wing political news source. The twenty most influential features are plotted in descending order. The range at the top of the\ngraph represents the political bias labels as predicted by the model.\nhttps://doi.org/10.1371/journal.pone.0321418.g005\nFig 6.Decision plot of Forbes, a right-leaning political news source.\nhttps://doi.org/10.1371/journal.pone.0321418.g006\nAside from the categorical features, polarising themes previously highlighted in the litera-\nture are also prevalent (e.g., inequality, environmental issues, election fraud, firearm owner-\nship, and social movements). Interestingly, however, the model also picks up on themes not\nappearing in earlier research (e.g., natural disasters).\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 12/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 13 \u2014 #13\nPLOS One Automatic political bias detection of news outlets\nFig 7.Decision plot of the Economist, a centre-leaning political news source.\nhttps://doi.org/10.1371/journal.pone.0321418.g007\nFig 8.Decision plot of the Guardian, a left-leaning political news source.\nhttps://doi.org/10.1371/journal.pone.0321418.g008\nRegarding the question of whether different types of bias impact model performance, arti-\ncle count features are often shown to be informative. This confirms the suggestion that alter-\nnative bias features could be more informative than previously credited: coverage bias, as rep-\nresented by the number of articles published per theme, can be recognised and used to inform\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 13/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 14 \u2014 #14\nPLOS One Automatic political bias detection of news outlets\nFig 9.Decision plot of CNN, a left-wing political news source.\nhttps://doi.org/10.1371/journal.pone.0321418.g009\nclassifications. However, it is worth noting that other forms of alternative bias (such as word\ncount or image presence) rarely appear in our decision plots, whereas traditional bias features\nare consistently informative.\nAside from gathering insight into how bias may manifest, SHAP can also be used to anal-\nyse misclassifications by the model ( Fig 10 ). For example, the domain theconservative-\ntreehouse.com was labelled as left-leaning despite actually being right-wing. Using the\nSHAP decision plot, we can see that the model was drastically influenced by the number of\narticles related to hate crime, causing it to output a left-centre label. Examining the dataset\nreveals that this domain has a high article count for this theme, and that this likely resulted\nin the misclassification. As such, SHAP plots can be helpful for analysing errors as well as\nunderstanding bias.\nPost-hoc analysis: Ground truth label comparisons\nIn light of the difference in performance between the models trained on PABS or MBFC data,\nwe conducted additional analyses to examine this closer. A quick comparison revealed that\nthere is a sizeable mismatch between labels. For all web-domains present in both PABS and\nMBFC\u2019s data, 46% of ratings agree with each other, and the AUC score is 69%. Fig 11 displays\na confusion matrix to compare prediction errors, showing that neighbouring labels tend to be\nmisclassified. Notably, there are some more significant disagreements; e.g., 32 left-wing web-\ndomains are classified as \u201cleast biased\u201d by PABS, and similarly for 14 right-wing web-domains.\nThese larger gaps in labelling are problematic and raise an important issue regarding the valid-\nity of bias ratings in general, but especially in the disparity of results between computationally\ndetermined results and human-made labels. However, this particular labelling task is complex\nand prone to such disagreement even when based on human annotators. For example, MBFC\nand another bias rating website using human annotators, AllSides [ 65], show a slightly greater\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 14/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 15 \u2014 #15\nPLOS One Automatic political bias detection of news outlets\nFig 10. SHAP decision plots of misclassified web-domain. Example of a misclassified web-domain, theconservativetreehouse.com , which is a right-wing\ndomain that was falsely classified as left-leaning by the model.\nhttps://doi.org/10.1371/journal.pone.0321418.g010\nFig 11. PABS and MBFC label agreement. A confusion matrix comparison of MBFC labels with those of PABS.\nhttps://doi.org/10.1371/journal.pone.0321418.g011\ndegree of agreement between themselves (57% of 293 web-domains in common agree, AUC\nscore of 74%). Given that this is still quite a low degree of agreement, it demonstrates that the\ncomputationally determined labels are only slightly less reliable compared to the realistically\nachievable upper bound set by human annotations, while retaining the benefits of speed and\nefficiency.\nDiscussion\nThe current work sought to automatically classify the political bias of news outlets, with a\nparticular focus on scalability, minimal human intervention, and transparency. As GDELT\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 15/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 16 \u2014 #16\nPLOS One Automatic political bias detection of news outlets\nincludes data from 1979 until the present, our approach allows for automatic labelling at any\npoint in time, anywhere. Furthermore, it addresses limitations of previous research, such as\nthe focus on constrained topics or types of bias [ 4,18,19,22,26]. Altogether, this allows for a\nmuch broader applicability than any previous research (to the best of our current knowledge),\nand is a fast and cost-effective method to recurrently obtain political bias estimates. Any user\nmay employ our approach to analyse any given online website indexed by GDELT.\nThe leading question of this work was whether one can infer the political leaning of news\nweb-domains based on GDELT data. As suggested by the results, this was indeed relatively\nsuccessful. The highest performing model (a neural network) achieved an accuracy of 75%\nand an AUC score of 81%, compared to a 45% and 50% AUC score for the baseline model.\nThis is comparable to previous work [ 19,22], though there is still room for improvement when\ncompared to some of the more specific applications of media bias classification (though such\nimplementations do not compare to ours in terms of scope [ 18]). Nevertheless, the fact that\nthis approach could achieve such performance without using custom, optimized language\nmodels, but instead only using GDELT\u2019s relatively basic features is highly promising. Our\nresults demonstrate the efficacy and value of our approach: even with a standard set of models\nand experiments, we achieved commendable performance.\nInterestingly, despite the impressive capabilities usually ascribed to LLM models, the\nLlama 3.1 andGPT-4o mini baselines performed poorly, assigning the \u201dleast biased\u201d\nlabel to most items. These results mirror issues mentioned in previous work [ 27,28,31,32,35\u2013\n37]. Our results are perhaps not surprising, as many of the news web-domains are not very\nwell-known and therefore unlikely to have been sufficiently represented in the training data.\nIn light of this, we would like to emphasize that our NN model performs best for outlets with\nlow- or medium-traffic, as revealed during our error analysis, which sets it in an ideal position\nto detect bias for unfamiliar sources. This is crucial in the opaque and rapidly evolving online\ninformation sphere.\nAnother issue our work aimed to address was the focus on narrow types of bias in previous\nstudies [ 4,18,19,22,26]. We compared the performance of models trained on the more com-\nmonly studied forms of bias related to word choice and general tone of articles (traditional\nbias dataset) to models trained on features related to under-explored patterns of bias (alter-\nnative bias dataset). The inclusion of alternative forms of bias improves performance, demon-\nstrating that automatic bias detection benefits from expanding its focus. Indeed, features such\nas article counts per theme (a proxy for coverage bias) were particularly informative, as is\napparent in Figs 5 -9.\nIn addition to these advantages of the current approach, transparency and explainability of\nresults were also central to our work. SHAP was used to provide detailed explanations of fea-\nture impact for any web-domain of interest. As an example, Fig 5 shows how Breitbart, a polit-\nically right-wing outlet, was accurately labelled thanks to features related to crime (including\nthemes like cartels, kidnapping, black markets, organized crime, and robbery). The feature\nvalues can be interpreted to make sense of the result: for instance, many articles discuss crime\ncartels (0.545, where the maximum possible value is 1 due to the scaling of the data) and that\nthe articles about black markets will tend to have a negative tone (0.409). Thus, one can inter-\npret these results as meaning that Breitbart\u2019s focus on crime is indicative of right-wing bias,\nwhich is more intuitive and informative than a simple classification result or model-level\nfeature importances.\nFurthermore, the SHAP decision plots also lend credence to previous results. For exam-\nple, alternative forms of bias such as article counts tend to be in the top twenty most impactful\nfeatures in Figs 5 -9. Nevertheless, the plots also raise some questions. Some themes are part\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 16/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 17 \u2014 #17\nPLOS One Automatic political bias detection of news outlets\nof the \u201cusual suspects\u201d of politically polarising themes in previous work (inequality, environ-\nment, social movements, firearms, and election fraud). Other themes, however, are more dif-\nficult to explain. For instance, features related to exhumation, sanitation and natural or man-\nmade disasters of varying kinds are also included in the top twenty features, though these are\nmore difficult to fully interpret.\nAside from this, we should also highlight that models performed better when trained on\nhuman-made labels [ 45] compared to computationally derived ones [ 25]. A potential reason\nfor the drop in performance is that these labels are inherently an approximation, and there-\nfore add uncertainty for the model. This is particularly detrimental when such approxima-\ntions compound. Robertson et al. [ 25] used Twitter sharing patterns of registered voters to\nattribute a score to each website when creating the PABS labels we used as ground truth. How-\never, as they themselves note, this assumes that people only share articles that agree with their\nown political opinions, which is not always the case (they do perform various tests for valid-\nity with other existing bias labels that suggest their results are an adequate replacement, how-\never). Nevertheless, the present study\u2019s results suggest that models trained on such computa-\ntionally determined labels can still be used to some extent, should manual labels be unavail-\nable. Furthermore, it is worth noting the degree to which the models generalize, indicating\nthat while manual labour was initially required, we can now partially rely on these models for\nsubsequent analyses even without human-made labels.\nAltogether, bias detection following an approach similar to ours could hopefully be\nmore informative for the public, offering a transparent examination of overall web-domain\nbehaviour. This can be done in a cost-effective and recurrent way, allowing for systematic esti-\nmates of political bias in the online news media environment. This might contribute to citi-\nzens\u2019 ability to make decisions in an informed manner about various topics important in the\ncurrent political climate [ 66].\nLimitations and future considerations\nDespite addressing many drawbacks of previous research, such as the manual annotations,\nlimited applicability, and focus on narrow forms of bias, there are remaining limitations of the\ncurrent approach as well.\nFirstly, the best-performing model was trained on GDELT and supplemented with categor-\nical features from MBFC. These categorical features are, however, only available for a subset\nof web-domains present in GDELT, meaning that this particular model is not applicable to all\nweb-domains. Nevertheless, other models trained only on GDELT data achieved compara-\nble performance, so it is possible to get accurate predictions and SHAP explanations for any\nGDELT website with a minimal drop in accuracy.\nSecondly, better features representing the various forms of bias could be constructed in\nthe future. Picture and explanation bias was, for example, only indirectly examined here,\nas the current approach only accounted for the presence or absence of images. Ideally, the\nactual content of the images would be included, as has been done in previous studies using\nGDELT [ 39]. Additionally, some forms of bias were excluded from this analysis despite poten-\ntial relevance (e.g. placement bias).\nFurthermore, it was noted during post-hoc analysis that the ground truth labels display\nremarkable disagreement with each other. This raises questions regarding what can be consid-\nered acceptable ground truths, as even expert labels tend to disagree with each other. Future\nwork might want to consider using other labels of political bias, as the bipartisan scoring\ndoes not necessarily lend itself well to all global political systems. Indeed, it has been noted\nthat what is considered left-leaning in one country would not be so in another one [ 3]. As\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 17/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 18 \u2014 #18\nPLOS One Automatic political bias detection of news outlets\nsuch, future applications of our approach should remain aware that the labels we use may not\nperfectly transfer when considering another country. Helpfully, however, MBFC provides\nan overview of criteria for how their bias labels were determined (for example, the outlet\u2019s\nstance on taxation, abortion, or the climate), which can be used to evaluate whether the scale\nis applicable for any particular use case [ 44]. Given previous research on the persistence of the\nleft-right political divide [ 1] and the pervasiveness of US political structures in its media and,\nimportantly, its social media, we expect that these ground truth labels will be appropriate in\nthe majority of, at the very least, Western countries. Should future work wish to forgo the left-\nright political divide for another kind of distinction, our approach nevertheless remains help-\nful, since the model can quite simply be retrained on the same data but using a different set\nof ground truth labels. This might enhance the applicability of our approach to countries that\ndo not neatly follow the left-right political spectrum, while retaining the benefits of a system-\natic and in-depth analysis of political bias in news. Alternatively, future work may forgo using\nlabels altogether, opting for unsupervised models instead. Given that there are no perfectly\nunbiased benchmarks, this may be a preferable approach depending on the context. Future\nwork may also examine the possibility of using GDELT\u2019s data to examine an outlet\u2019s overall\nstance on a particular theme. This could provide a robust and extensive perspective on news\noutlets, and show potentially unexpected biases.\nThe rise in popularity of LLM-based methods also presents a promising avenue of research,\ndespite the challenges mentioned in our section on related work. Given the unprecedented\npotential for nuanced model output explanations that LLMs offer, they certainly merit fur-\nther study of whether they can provide the nuance commonly reserved to expert-based\napproaches.\nLastly, our use of SHAP is explorative; future work in online bias might focus on including\nmore detailed information, such as relevant excerpts of articles, to concretely provide insight\ninto model predictions. This might give insight into about why some of the more surprising\nthemes were deemed to be informative, such as waterways, for instance. Generally, it can help\ninform the field of themes that are not usually considered in online bias research.\nConclusion\nThe current work proposes an approach to classify news outlet political bias. Crucially, we\nintended to expand the scope beyond what had previously been done by ensuring global cov-\nerage and by focusing on multiple forms of bias. Our results indicate that the method indeed\nprovides a fully automatic and scalable approach to detecting news bias, and that enlarging\nthe focus to multiple forms of bias could help the field advance. Finally, the SHAP explana-\ntions allow for interpretation of why a particular web-domain is considered politically biased\nand show which topics and behaviours influence the classification. Interestingly, many themes\ncommonly considered divisive reappear, but some informative features have not been pre-\nviously considered in the literature. This may help address gaps or future avenues in current\nresearch.\nAll in all, the current work extends existing research to be more widely applicable and\ninformative for the field. The increased transparency may be helpful for adequately informing\nthe public about its news consumption, as well as providing more insight into the underlying\nmechanisms of bias to a more granular extent than what is traditionally attempted by com-\nputational methods. Considering the immense impact of news on global political climates,\nour hope is that increased understanding and trustworthiness of media might contribute to a\nbetter-informed society and a healthier political environment.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 18/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 19 \u2014 #19\nPLOS One Automatic political bias detection of news outlets\nSupporting information\nS1 Appendix. MBFC features. This appendix describes in more detail the features provided\nby MBFC. For each news web-domain, they provide a set of data points of interest.\n(PDF)\nS2 Appendix. Details of implementation and data pre-processing. This appendix describes\nin the implementation and other details related to the data pre-processing.\n(PDF)\nS3 Appendix. Grid search model parameters. The appendix details the hyperparameters\nused for grid search when optimizing the various models.\n(PDF)\nS4 Appendix. Neural network architecture. This section notes the architecture of the\nPyTorch neural network for all experiments.\n(PDF)\nS5 Appendix. Large language model baseline. This section notes the implementation details\nof the LLM baseline.\n(PDF)\nS6 Appendix. Error analysis. This section details some error analysis per political leaning\nlabel and the website traffic.\n(PDF)\nS7 Appendix. Example of a GDELT news item. Shows an excerpt from a news story as it is\nshown on GDELT.\n(PDF)\nAuthor contributions\nConceptualization: Ronja R\u00f6nnback, Chris Emmery, Henry Brighton.\nData curation: Ronja R\u00f6nnback, Chris Emmery.\nFormal analysis: Ronja R\u00f6nnback.\nInvestigation: Ronja R\u00f6nnback.\nMethodology: Ronja R\u00f6nnback, Chris Emmery, Henry Brighton.\nProject administration: Ronja R\u00f6nnback.\nSoftware: Ronja R\u00f6nnback, Chris Emmery.\nSupervision: Chris Emmery, Henry Brighton.\nValidation: Ronja R\u00f6nnback, Chris Emmery, Henry Brighton.\nVisualization: Ronja R\u00f6nnback, Henry Brighton.\nWriting \u2013 original draft: Ronja R\u00f6nnback.\nWriting \u2013 review & editing: Ronja R\u00f6nnback, Chris Emmery, Henry Brighton.\nReferences\n1.Le Gall C, Berton R. Left-Right vs. traditional and new cleavages: Testing durability of an old\npolitical category. Cambridge Scholars Publishing; 2013. p. 255\u201368.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 19/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 20 \u2014 #20\nPLOS One Automatic political bias detection of news outlets\n2.Lewis H, Lewis V. The myth of left and right: How the political spectrum misleads and harms\nAmerica. Oxford University Press; 2023. p. 9\u201316.\nhttps://doi.org/10.1093/oso/9780197680216.001.0001\n3.Huber J, Inglehart R. Expert interpretations of party space and party locations in 42 societies. Party\nPolit. 1995;1(1):73\u2013111. https://doi.org/10.1177/1354068895001001004\n4.Hamborg F, Donnay K, Gipp B. Automated identification of media bias in news articles: An\ninterdisciplinary literature review. Int J Digit Libr. 2019;20(4):391\u2013415.\nhttps://doi.org/10.1007/s00799-018-0261-y\n5.Leetaru K, Schrodt PA. GDELT: Global data on events, location, and tone. In: ISA annual\nconvention; 2013.\n6.Rodrigo-Gin\u00e9s FJ, de Albornoz JC, Plaza L. A systematic review on media bias detection: What is\nmedia bias, how it is expressed, and how to detect it. Expert Syst Applic. 2024;237:121641.\nhttps://doi.org/10.1016/j.eswa.2023.121641\n7.Kelsey D. News, discourse, and ideology. In: The handbook of journalism studies. Routledge; 2019.\np. 246\u201360.\n8.Ditton J, Duffy J. Bias in the newspaper reporting of crime news. Brit J Criminol. 1983;23:159.\n9.Gilliam Jr FD, Iyengar S, Simon A, Wright O. Crime in black and white: The violent, scary world of\nlocal news. Harvard Int J Press/Polit. 1996;1(3):6\u201323.\n10.Paybarah A. Media matters: New York TV News over-reports on crimes with black suspects.\nPOLITICO; 2015. Available from: https://www.politico.com/states/new-york/city-hall/story/2015/03/\nmedia-matters-new-york-tv-news-over-reports-on-crimes-with-black-suspects-020674 (Accessed\n06 May 2022).\n11.Madrigal G, Soroka S. Migrants, caravans, and the impact of news photos on immigration attitudes.\nInt J Press/Polit. 2023;28(1):49\u201369. https://doi.org/10.1177/19401612211008430\n12.Soroka S, Loewen P, Fournier P, Rubenson D. The impact of news photos on support for military\naction. Polit Commun. 2016;33(4):563\u201382.\n13.Jones K. A statistical interpretation of term specificity and its application in retrieval. J Doc.\n1972;28(1):11\u201321. https://doi.org/10.1108/eb026526\n14.Jones KS. Some thoughts on classification for retrieval. Journal of Documentation.\n1970;26(2):89\u2013101. https://doi.org/10.1108/eb026488\n15.Le QV, Mikolov T. Distributed representations of sentences and documents. In: Proceedings of the\n31th international conference on machine learning, ICML 2014, Beijing, China, 21\u201326 June 2014.\nvol. 32 of JMLR workshop and conference proceedings. JMLR.org; 2014. p. 1188\u201396. Available\nfrom: http://proceedings.mlr.press/v32/le14.html\n16.Spinde T, Hamborg F, Gipp B. Media bias in German news articles: A combined approach. In:\nECML PKDD 2020 Workshops \u2013 Workshops of the European conference on machine learning and\nknowledge discovery in databases (ECML PKDD 2020): SoGood 2020, PDFL 2020, MLCS 2020,\nNFMCP 2020, DINA 2020, EDML 2020, XKDD 2020 and INRA 2020, Ghent, Belgium, September\n14-18, 2020, Proceedings. vol. 1323 of communications in computer and information science.\nSpringer; 2020. p. 581\u201390. Available from: https://doi.org/10.1007/978-3-030-65965-3_41\n17.Baraniak K, Sydow M. News articles similarity for automatic media bias detection in Polish news\nportals. In: Ganzha M, Maciaszek LA, Paprzycki M, editors. Proceedings of the 2018 federated\nconference on computer science and information systems, FedCSIS 2018, Poznan, Poland,\nSeptember 9-12, 2018. vol. 15 of annals of computer science and information systems; 2018. p.\n21\u20134. Available from: https://doi.org/10.15439/2018F359\n18.Gangula RRR, Duggenpudi SR, Mamidi R. Detecting political bias in news articles using headline\nattention. In: Linzen T, Chrupala G, Belinkov Y, Hupkes D, editors. Proceedings of the 2019 ACL\nworkshop BlackboxNLP: Analyzing and interpreting neural networks for NLP; 2019. p. 77\u201384.\nAvailable from: https://doi.org/10.18653/v1/w19-4809\n19.Spinde T, Krieger J, Ruas T, Mitrovic J, G\u0308otz-Hahn F, Aizawa A, et al. Exploiting transformer-based\nmultitask learning for the detection of media bias in news articles. In: Smits M, editor. Information for\na better world: Shaping the global future \u2013 17th international conference, iConference 2022, virtual\nevent, February 28-March 4, 2022, Proceedings, Part I. vol. 13192 of lecture notes in computer\nscience. Springer; 2022. p. 225\u201335. Available from: https://doi.org/10.1007/978-3-030-96957-8_20\n20.Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled version of BERT: Smaller, faster,\ncheaper and lighter. CoRR. 2019;abs/1910.01108.\n21.Devlin J, Chang M, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In: Burstein J, Doran C, Solorio T, editors. Proceedings of the 2019\nconference of the North American chapter of the association for computational linguistics: Human\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 20/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 21 \u2014 #21\nPLOS One Automatic political bias detection of news outlets\nlanguage technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, Volume 1 (long and\nshort papers). Association for Computational Linguistics; 2019. p. 4171\u201386.\nhttps://doi.org/10.18653/v1/n19-1423\n22.Spinde T, Plank M, Krieger J, Ruas T, Gipp B, Aizawa A. Neural media bias detection using distant\nsupervision with BABE \u2013 Bias annotations by experts. CoRR. 2022.\nhttps://doi.org/10.48550/ARXIV.2209.14557\n23.Menzner T, Leidner JL. Improved models for media bias detection and subcategorization. In: Rapp\nA, Caro LD, Meziane F, Sugumaran V, editors. Natural language processing and information\nsystems \u2013 29th international conference on applications of natural language to information systems,\nNLDB 2024, Turin, Italy, June 25-27, 2024, Proceedings, Part I. vol. 14762 of lecture notes in\ncomputer science. Springer; 2024. p. 181\u201396. Available from:\nhttps://doi.org/10.1007/978-3-031-70239-6_13\n24.Le HT, Shafiq Z, Srinivasan P. Scalable news slant measurement using Twitter. In: Proceedings of\nthe eleventh international conference on web and social media, ICWSM 2017, Montreal, Quebec,\nCanada, May 15-18, 2017. AAAI Press; 2017. p. 584\u20137. Available from:\nhttps://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15668\n25.Robertson RE, Jiang S, Joseph K, Friedland L, Lazer D, Wilson C. Auditing partisan audience bias\nwithin Google search. Proc ACM Hum-Comput Interact. 2018;2(CSCW):1\u201322.\nhttps://doi.org/10.1145/3274417\n26.Ye J, Skiena S. MediaRank: Computational ranking of online news sources. In: Teredesai A, Kumar\nV, Li Y, Rosales R, Terzi E, Karypis G, editors. Proceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining, KDD 2019, Anchorage, AK, USA, August 4-8,\n2019. ACM; 2019. p. 2469\u201377. Available from: https://doi.org/10.1145/3292500.3330709\n27.Li Z, Zhang H, Zhang J. A revisit of fake news dataset with augmented fact-checking by ChatGPT.\nCoRR. 2023; abs/2312.11870. https://doi.org/10.48550/ARXIV.2312.11870\n28.Wen Z, Younes R. ChatGPT v.s. Media bias: A comparative study of GPT-3.5 and fine-tuned\nlanguage models. CoRR. 2024; abs/2403.20158. https://doi.org/10.48550/ARXIV.2403.20158\n29.Makhortykh M, Sydorova M, Baghumyan A, Vziatysheva V, Kuznetsova E. Stochastic lies: How\nLLM-powered chatbots deal with Russian disinformation about the war in Ukraine. HKS Misinfo\nRev. 2024. https://doi.org/10.37016/mr-2020-154\n30.Szwoch J, Staszkow M, Rzepka R, Araki K. Can LLMs determine political leaning of Polish news\narticles? In: 2023 IEEE Asia-Pacific conference on computer science and data engineering (CSDE).\nIEEE; 2023. p. 1\u20136.\n31.Menzner T, Leidner JL. Experiments in news bias detection with pre-trained neural transformers. In:\nGoharian N, Tonellotto N, He Y, Lipani A, McDonald G, Macdonald C, et al., editors. Advances in\ninformation retrieval \u2013 46th European conference on information retrieval, ECIR 2024, Glasgow, UK,\nMarch 24-28, 2024, Proceedings, Part IV. vol. 14611 of lecture notes in computer science. Springer;\n2024. p. 270\u201384. Available from: https://doi.org/10.1007/978-3-031-56066-8_22\n32.Lin L, Wang L, Guo J, Wong K. Investigating bias in LLM-based bias detection: Disparities between\nLLMs and human perception. CoRR. 2024; abs/2403.14896.\nhttps://doi.org/10.48550/ARXIV.2403.14896\n33.Motoki F, Pinho Neto V, Rodrigues V. More human than human: Measuring ChatGPT political bias.\nPubl Choice. 2024;198(1):3\u201323. https://doi.org/10.1007/s11127-023-01097-2\n34.van Dis EAM, Bollen J, Zuidema W, van Rooij R, Bockting CLH. ChatGPT: Five priorities for\nresearch. Nature. 2023;614:224\u20136. https://doi.org/10.1038/d41586-023-00288-7 PMID: 36737653\n35.Choi M, Pei J, Kumar S, Shu C, Jurgens D. Do LLMs understand social knowledge? Evaluating the\nsociability of large language models with SocKET benchmark. In: Bouamor H, Pino J, Bali K,\neditors. Proceedings of the 2023 conference on empirical methods in natural language processing.\n2023. Singapore, December 6-10. Association for Computational Linguistics; 2023. p. 11370\u2013403.\nhttps://doi.org/10.18653/v1/2023.emnlp-main.699\n36.Mu Y, Wu BP, Thorne W, Robinson A, Aletras N, Scarton C, et al. Navigating prompt complexity for\nzero-shot classification: A study of large language models in computational social science. In:\nCalzolari N, Kan M, Hoste V, Lenci A, Sakti S, Xue N, editors. Proceedings of the 2024 joint\ninternational conference on computational linguistics, language resources and evaluation,\nLREC/COLING 2024, 20-25 May, 2024, Torino, Italy. ELRA and ICCL; 2024. p. 12074\u201386. Available\nfrom: https://aclanthology.org/2024.lrec-main.1055\n37.Y Qiang, S Nandi, N Mehrabi, GV Steeg, A Kumar, A Rumshisky, et al. Prompt perturbation\nconsistency learning for robust language models. In: Y Graham, M Purver, editors. Findings of the\nassociation for computational linguistics: EACL 2024, St. Julian\u2019s, Malta, March 17-22. Association\nfor computational linguistics; 2024. p. 1357\u201370. Available from:\nhttps://aclanthology.org/2024.findings-eacl.91\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 21/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 22 \u2014 #22\nPLOS One Automatic political bias detection of news outlets\n38.Bender EM, Gebru T, McMillan-Major A, Shmitchell S. On the dangers of stochastic parrots. In:\nElish MC, Isaac W, Zemel RS, editors. FAccT \u201921: 2021 ACM conference on fairness, accountability,\nand transparency, 2021, virtual event/Toronto, Canada, March 3-10. ACM; 2021. p. 610\u201323.\nhttps://doi.org/10.1145/3442188.3445922\n39.Kwak H, An J. Revealing the hidden patterns of news photos: Analysis of millions of news photos\nthrough GDELT and deep learning-based vision APIs. In: An J, Kwak H, Benevenuto F, editors.\nNews and public opinion, papers from the 2016 ICWSM workshop, Cologne, Germany, May 17,\n2016. vol. WS-16-18 of AAAI Technical Report. AAAI Press; 2016. p. 339\u201343. Available from:\nhttp://aaai.org/ocs/index.php/ICWSM/ICWSM16/paper/view/13191\n40.Vargo CJ, Guo L, Amazeen MA. The agenda-setting power of fake news: A big data analysis of the\nonline media landscape from 2014 to 2016. New Media Soc. 2018;20(5):2028\u20132049.\nhttps://doi.org/10.1177/1461444817712086\n41.El Ouadi A, Beskow D. Comparison of common crawl news & GDELT. In: 2024 IEEE international\nsystems conference (SysCon); 2024. p. 1\u20133.\n42.GDELT Project. GDELT Project; 2018. Available from: https://www.gdeltproject.org/\n43.GDELT Project. The GDELT Global Knowledge Graph (GKG) data format codebook V2.1; 2015\nAvailable from:\nhttp://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf\n44.Media Bias Fact Check. Left vs. right bias: How we rate the bias of media sources; 2021. Available\nfrom: https://mediabiasfactcheck.com/left-vs-right-bias-how-we-rate-the-bias-of-media-sources/\n45.Media Bias Fact Check. About media bias fact check; 2021. Available from:\nhttps://mediabiasfactcheck.com/about/\n46.Media Bias Fact Check. MBFC methodology; 2023. Available from:\nhttps://mediabiasfactcheck.com/methodology/\n47.Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: An imperative style,\nhigh-performance deep learning library. In: Wallach HM, Larochelle H, Beygelzimer A, d\u2019Alch\u00b4e-Buc\nF, Fox EB, Garnett R, editors. Advances in neural information processing systems 32: Annual\nconference on neural information processing systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada; 2019. p. 8024\u201335. Available from:\nhttps://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html\n48.Boser BE, Guyon I, Vapnik V. A training algorithm for optimal margin classifiers. In: Proceedings of\nthe fifth annual ACM conference on computational learning theory. 1992:144\u2013152.\nhttps://doi.org/10.1145/130385.130401\n49.Cortes C, Vapnik V. Support-vector networks. Mach Learn. 1995;20(3):273\u201397.\nhttps://doi.org/10.1007/BF00994018\n50.Freund Y, Schapire RE. A decision-theoretic generalization of on-line learning and an application to\nboosting. J Comput Syst Sci. 1997;55(1):119\u201339. https://doi.org/10.1006/JCSS.1997.1504\n51.Li X, Wang L, Sung E. AdaBoost with SVM-based component classifiers. Eng Applic Artif Intell.\n2008;21(5):785\u201395. https://doi.org/10.1016/j.engappai.2007.07.001\n52.Chen T, Guestrin C. XGBoost: A scalable tree boosting system. In: Proceedings of the 22nd ACM\nSIGKDD international conference on knowledge discovery and data mining. San Francisco, CA,\nUSA, August 13-17. ACM; 2016. p. 785\u201394. Available from:\nhttps://doi.org/10.1145/2939672.2939785\n53.Dubey A, Jauhri A, Pandey A, Kadian A, Al-Dahle A, Letman A, et al. The Llama 3 herd of models.\nCoRR. 2024; abs/2407.21783. https://doi.org/10.48550/ARXIV.2407.21783\n54.OpenAI. GPT-4o-mini; 2024. Available from:\nhttps://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n55.Fukushima K. Visual feature extraction by a multilayered network of analog threshold elements.\nIEEE Trans Syst Sci Cybern. 1969;5(4):322\u201333. https://doi.org/10.1109/TSSC.1969.300225\n56.Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. In: Bach FR, Blei DM, editors. In: Proceedings of the 32nd international conference\non machine learning, ICML 2015, Lille, France, 6-11 July 2015. vol. 37 of JMLR Workshop and\nConference Proceedings. JMLR.org; 2015. p. 448\u201356. Available from:\nhttp://proceedings.mlr.press/v37/ioffe15.html\n57.Srivastava N, Hinton GE, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: A simple way to\nprevent neural networks from overfitting. J Mach Learn Res. 2014;15(1):1929\u201358.\nhttps://doi.org/10.5555/2627435.2670313\n58.Kingma DP, Ba J. Adam: A method for stochastic optimization. CoRR. 2014; abs/1412.6980.\n59.Guo C, Berkhahn F. Entity embeddings of categorical variables. CoRR. 2016; abs/1604.06737.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 22/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 23 \u2014 #23\nPLOS One Automatic political bias detection of news outlets\n60.Brownlee J. Three ways to encode categorical variables for deep learning; 2019. Available from:\nhttps://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/\n61.Scikit Learn. Tuning the hyper-parameters of an estimator: Searching for optimal parameters with\nsuccessive halving; 2023. Available from:\nhttps://scikit-learn.org/stable/modules/grid_search.html#successive-halving-user-guide\n62.Lundberg SM, Lee S. A unified approach to interpreting model predictions. In: Guyon I, von Luxburg\nU, Bengio S, Wallach HM, Fergus R, Vishwanathan SVN, et al., editors. Advances in neural\ninformation processing systems 30: Annual conference on neural information processing systems\n2017, December 4-9, 2017, Long Beach, CA, USA; 2017. p. 4765\u201374. Available from: https:\n//proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html\n63.Ribeiro MT, Singh S, Guestrin C. \u201cWhy Should I Trust You?\u201d: Explaining the predictions of any\nclassifier. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge\ndiscovery and data mining; 2016.\n64.Bach S, Binder A, Montavon G, Klauschen F, M\u00fcller K-R, Samek W. On pixel-wise explanations for\nnon-linear classifier decisions by layer-wise relevance propagation. PLoS ONE. 2015;10:e0130140.\nhttps://doi.org/10.1371/journal.pone.0130140 PMID: 26161953\n65.AllSides. AllSides methodology; 2023. Available from:\nhttps://www.allsides.com/media-bias/media-bias-rating-methods\n66.Lewandowsky S, Smillie L, Garcia D, Hertwig R, Weatherall J, Egidy S, et al.. Technology and\ndemocracy: Understanding the influence of online technologies on political behaviour and\ndecision-making. Publications Office of the European Union; 2020.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 23/ 23", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Automatic large-scale political bias detection of news outlets", "author": ["R R\u00f6nnback", "C Emmery", "H Brighton"], "pub_year": "2025", "venue": "PLoS One", "abstract": "Political bias is an inescapable characteristic in news and media reporting, and understanding  what political biases people are exposed to when interacting with online news is of"}, "filled": false, "gsrank": 26, "pub_url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0321418", "author_id": ["", "Ot4Z-qkAAAAJ", "GYgP-ogAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:38XhY8cUS3wJ:scholar.google.com/&output=cite&scirp=25&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D20%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=38XhY8cUS3wJ&ei=CLWsaLmrCI6IieoP0sKRuAk&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:38XhY8cUS3wJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0321418&type=printable"}}, {"title": "MGM: Global Understanding of Audience Overlap Graphs for Predicting the Factuality and the Bias of News Media", "year": "2024", "pdf_data": "MGM: Global Understanding of Audience Overlap Graphs\nfor Predicting the Factuality and the Bias of News Media\nMuhammad Arslan Manzoor1\u2217, Ruihong Zeng2\u2217, Dilshod Azizov1,\nPreslav Nakov1& Shangsong Liang2\u2020\n1Mohamed bin Zayed University of Artificial Intelligence, UAE\n2Sun Yat-sen University, China\n{muhammad.arslan, preslav.nakov}@mbzuai.ac.ae\nAbstract\nIn the current era of rapidly growing digital\ndata, evaluating the political bias and factual-\nity of news outlets has become more important\nfor seeking reliable information online. In this\nwork, we study the classification problem of\nprofiling news media from the lens of politi-\ncal bias and factuality. Traditional profiling\nmethods, such as Pre-trained Language Models\n(PLMs) and Graph Neural Networks (GNNs)\nhave shown promising results, but they face no-\ntable challenges. PLMs focus solely on textual\nfeatures, causing them to overlook the complex\nrelationships between entities, while GNNs of-\nten struggle with media graphs containing dis-\nconnected components and insufficient labels.\nTo address these limitations, we propose Me-\ndiaGraphMind (MGM), an effective solution\nwithin a variational Expectation-Maximization\n(EM) framework. Instead of relying on limited\nneighboring nodes, MGM leverages features,\nstructural patterns, and label information from\nglobally similar nodes. Such a framework not\nonly enables GNNs to capture long-range de-\npendencies for learning expressive node repre-\nsentations but also enhances PLMs by integrat-\ning structural information and therefore improv-\ning the performance of both models. The exten-\nsive experiments demonstrate the effectiveness\nof the proposed framework and achieve new\nstate-of-the-art results. Further, we share our\nrepository1which contains the dataset, code,\nand documentation.\n1 Introduction\nThe rise of the Internet has offered many opportuni-\nties to publish information and to express opin-\nions (Mehta and Goldwasser, 2023b). Concur-\nrently, this easy means of distribution has accel-\nerated the spread of misinformation and disinfor-\nmation online which resembles news in form but\n*Equal contribution.\n\u2020Corresponding author.\n1https://github.com/marslanm/MGM_codelacks the journalistic standards that ensure its qual-\nity (Fairbanks et al., 2018). V osoughi et al. (2018)\nhas found that \u201cfake news\u201d spreads six times faster\nand reaches much farther than real news. Any delay\nin profiling in rapidly evolving digital landscapes\ncan lead to unchecked distribution of misleading\ncontent (Liu et al., 2022). Profiling news outlets\nthrough NLP pipelines offers a proactive approach\nby enabling the early detection of potentially un-\nreliable sources as soon as they publish content.\nSince outlets with a history of biased or false in-\nformation are more likely to do so again, profiling\nthe media in advance allows us to quickly identify\nprobable \u201cfake news\u201d by evaluating the reliability\nof the source itself. (Nakov et al., 2024).\nEarly studies on automatic media profiling re-\nlied solely on text characteristics (Battaglia et al.,\n2018; P\u00e9rez-Rosas et al., 2017), which has proven\nparticularly challenging. The complexity increases\nwhen the text features contain indeterminate noise,\nleading to classification errors (Baly et al., 2018,\n2020a). Moreover, traditional methods struggle to\ncapture the intricate relationships between entities,\nsuch as media outlets, content they publish, and au-\ndiences. Panayotov et al. (2022) constructed media\ngraphs: nodes represent media, and edges represent\naudience overlap between media. They proposed\na framework that captures both inherent and im-\nplicit information about media through interactive\nlearning within the media ecosystem, addressing\nthe limitations of relying solely on textual features.\nWe analyze these media graphs and identify two\nkey challenges: disconnected components and la-\nbel sparsity. Disconnected components prevent\nGNNs from capturing long-range dependencies,\nlimiting their ability to learn expressive node rep-\nresentations for classification tasks (Longa et al.,\n2024; Zhang et al., 2024). Prior studies (Yin et al.,\n2024; Tang et al., 2024) address similar issues by\nusing memory-based approaches that store global\ninformation throughout the graph using externalarXiv:2412.10467v1  [cs.LG]  12 Dec 2024\nmemory modules. However, these methods require\nsignificant memory to store all node embeddings.\nTo tackle these challenges, we present MGM, a\nnovel method based on a variational Expectation-\nMaximization (EM) framework that augments ex-\nisting Graph Neural Networks (GNNs) to capture\nand exploit global information in media graphs.\nMGM seamlessly integrates local and global pat-\nterns, node features, and labels from globally\nsimilar nodes to enhance performance. Unlike\nGraph Attention Networks (GATs) (Veli \u02c7ckovi \u00b4c\net al., 2018), which focus solely on local neighbor-\nhoods, MGM employs an external memory module\nto store precomputed node representations of all\nnodes. This approach not only reduces computa-\ntional costs (Fey et al., 2021) but also facilitates\nefficient node embedding retrieval. Furthermore,\nMGM optimizes memory usage by focusing on a\nsmall set of candidate nodes, guided by a Dirichlet\nprior distribution (He et al., 2020).\nThe experimental results show that MGM sub-\nstantially enhances the performance of baseline\nGNNs, delivering a 10% increase across all eval-\nuation measures on the Media Bias/Fact Check\n(MBFC)2data feature in the ACL-2020 (Baly et al.,\n2020b) and the EMNLP-2018 (Baly et al., 2018)\ndatasets. Despite the lack of rich node features\nin the media graph, we enhance the dataset by\nscraping Articles andWikipedia descriptions for\nACL-2020. Pre-trained language models (PLMs)\nsuch as BERT (Devlin et al., 2018), RoBERTa (Liu\net al., 2019b), DistilBERT (Sanh et al., 2019), and\nDeBERTaV3 (He et al., 2021) are fine-tuned to pre-\ndict political bias and factuality. Where textual data\nof media are inaccessible, MGM\u2019s representation-\nbased probabilities compliment the gap. Moreover,\nintegrating MGM\u2019s probabilities with PLMs en-\nhance the performance for both tasks. Our contri-\nbutions are as follows:\n\u2022We introduce MGM, an efficient and expres-\nsive approach that enhances GNNs for reliable\nnews media profiling by leveraging global in-\nformation and minimizing memory require-\nments via a sparse distribution.\n\u2022We illustrate that MGM consistently outper-\nforms vanilla GNNs for the detection of factu-\nality and political bias across all baselines.\n\u2022We validate that integrating the MGM fea-\ntures with the PLMs enhances performance\n2www.mediabiasfactcheck.comand yields state-of-the-art results.\n2 Related Work\n2.1 Political Bias and Factuality of Media\nEarly research on political bias detection focused\non the analysis of textual content (Afroz et al.,\n2012; Battaglia et al., 2018; P\u00e9rez-Rosas et al.,\n2017; Conroy et al., 2015). To improve the perfor-\nmance, subsequent research added contextual in-\nformation (Baly et al., 2020b; Hounsel et al., 2020;\nCastelo et al., 2019; Fairbanks et al., 2018), includ-\ning the nuances of multimedia production (Huh\net al., 2018), the associated infrastructure (Houn-\nsel et al., 2020), and the social context (Baly et al.,\n2020b). Guo et al. (2022) used BERT (Devlin et al.,\n2019) to model the linguistic political bias in news\narticles. Fan et al. (2019) used annotated media\nfrom Budak et al. (2016), analyzing articles for\npolitical bias using distant supervision. Various\nmethods measured political bias, including analyz-\ning Twitter interactions (An et al., 2012; Stefanov\net al., 2020), often using small datasets only in En-\nglish (Da San Martino et al., 2023; Nakov et al.,\n2023a,b; Barr\u00f3n-Cede\u00f1o et al., 2023a,b; Azizov\net al., 2023; Spinde et al., 2022).\nLei et al. (2022) improved political bias detec-\ntion through discourse structures, Liu et al. (2019a)\ndetected frames in gun violence reporting, and Lee\net al. (2022) proposed framework for neutral sum-\nmaries. Bang et al. (2023) proposed a polarity\nminimization loss to reduce framing bias in multi-\ndocument summarization. Liu et al. (2023) ad-\ndressed framing bias in event understanding with\na neutral event graph induction framework us-\ning graph-based approaches. Maab et al. (2024)\nand Lin et al. (2024) leveraged LLMs and vector\ndatabases for adaptability and explainability. Con-\ntributions include frameworks for detecting politi-\ncal bias (Trhlik and Stenetorp, 2024), media cred-\nibility via retrieval-augmented generation (RAG)\n(Schlichtkrull, 2024), and scalable LLM bias as-\nsessment (Bang et al., 2024). Demszky et al. (2019)\nexamined polarization on social media, Das et al.\n(2024) and Zhao et al. (2024) analyzed event rela-\ntionships in media narratives, and Kameswari and\nMamidi (2021) introduced a corpus quantifying\nmedia bias. Kim and Guerzhoy (2024) showcased\nLLMs role in analyzing U.S. cultural patterns and\nmedia-driven behaviors.\nThe veracity of the news media has been ex-\nplored using PLMs to estimate the reliability of\nthe source, correlated with the ratings of human\nexperts (Yang and Menczer, 2023). Mehta and\nGoldwasser (2023a) introduced a framework that\ncombines graph-based models, PLMs, and human\nexperience to profile news media, effectively iden-\ntifying \u201cfake news\u201d with minimal human input. Re-\ncent approaches, such as Baly et al. (2020b), used\ngold labels and various English sources as features\nto profile media with PLMs. Azizov et al. (2024)\nconducted a cross-lingual evaluation of political\nbias and factuality. Although the features of the\naforementioned studies are obtained from various\nsources, they neglect the inherent relationships be-\ntween the media.\nTo bridge this gap, graphs emerged as a compre-\nhensive and effective framework for representation\nlearning (Mehta et al., 2022). However, this study\nfocused solely on the factuality task, despite hav-\ning available political bias labels and generalizing\nonly R-GCN. Mehta and Goldwasser (2023a) intro-\nduced a model that combines graphs, LLMs, and\nhuman input for profiling. Panayotov et al. (2022)\nconstructed a graph based on the principle of ho-\nmophily, suggesting that similar media sources at-\ntract similar audiences. The framework leveraged\nthe audience overlap of media outlets to build a\nhuge graph that models the interactions between\nmedia and to learn expressive representation for\nthe nodes using GNNs. However, media graphs\nare characterized by disconnected components and\nscarce labels. To overcome these limitations, we\npropose MGM to effectively capture the informa-\ntion across the entire graph.\n2.2 Graph Neural Networks\nThe current design of GNNs follows the message-\npassing framework (Yang et al., 2022; Chen et al.,\n2024; Zeng et al., 2024), where they learn node rep-\nresentations by aggregating information from local\nneighbors. However, media graphs suffer from\nchallenges such as multiple disconnected compo-\nnents and limited labels, making it difficult for\nGNNs to capture long-range dependencies and to\nlearn effective node representations (Longa et al.,\n2024; Zhang et al., 2024). Recent efforts to inte-\ngrate external memory modules to store the em-\nbeddings of all nodes allow GNNs to capture long-\nrange dependencies across graphs (Yin et al., 2024;\nTang et al., 2024). In addition, relational GNNs\n(Zhang et al., 2024) and event relation graphs (Lei\net al., 2022) improve the detection and analysis of\npolitical bias. However, these methods typically re-quire storing embeddings for all nodes in the graph,\nresulting in high memory costs and low efficiency\nduring testing. Unlike previous approaches, MGM\nfocuses on a small set of candidate nodes, which are\nmore likely to be selected as global similar nodes\nbased on a Dirichlet prior distribution applied to\nthe training nodes (Sethuraman, 1994).\n3 Methodology\nIn this section, we present the problem formulation\nand provide a detailed description of the proposed\nframework, which leverages features, structural pat-\nterns, and label information from globally similar\nnodes to enhance GNNs performance. Further-\nmore, MGM integrates with PLMs to overcome\nthe limitations of existing textual features to detect\nfactuality and political bias.\n3.1 Problem Formulation\nWe formulate the news media profiling task as a\nnode classification problem in the semi-supervised\ngraph learning setting (Kipf and Welling, 2016;\nVeli\u02c7ckovi \u00b4c et al., 2018), where each node repre-\nsents a news media outlet, the edges capture rela-\ntionships such as audience overlap, and the node\nlabel indicates political bias or factuality, which\nare available only for a small subset of nodes.\nSpecifically, let G={V,E,X,Yl}represents a\npartially-labeled graph, where V={vi}N\ni=1is a\nset of nodes, Eis a set of edges, and Nis the total\nnumber of nodes. The node features are denoted\nasX\u2208RN\u00d7F, where Fis the feature dimension.\nSince most nodes are unlabeled, Vcan be divided\ninto labeled nodes Vlwith labels Yl, and unlabeled\nnodes Vu. The labels Yl\u2208RNl\u00d7Care in a one-\nhot form, where NlandCrepresent the number of\nlabeled nodes and the number of classes, respec-\ntively. The goal of semi-supervised learning is to\nlearn the model parameters \u03b8by maximizing the\nmarginal distribution of the overall labeled nodes,\ni.e.,p\u03b8(Yl|X,E) =Q\nn\u2208Vlp\u03b8(yn|X,E)on the\ntraining graph.\n3.2 The MGM Framework\nFollowing (Qu et al., 2019, 2021), we adopt a prob-\nabilistic framework for node classification, treating\nnode representations Zas latent variables deter-\nmined by a GNN. To improve the performance of\nthe model, we propose to augment the GNNs with\ninformation about global similar nodes , i.e., nodes\nin the entire graph that have similar node features\nand local geometric structures. Specifically, we\nDrichlet Prior\nMGGlobal \nMemory\n1       2        3 ...   N\nNodes Selection\nDistributionMS\nGraph Neural\nNetworksMGM + LM\nPrediction\nLanguage \nModelsSampled \nMemory\nProb.\nProb.K & \u03b7\nLearningMGM Prediction\nArticles / Wikipediabbc.com\nmsnbc.com\nreuters.com        Alexa Rank Graph\nbbc.comnytimes.com\nfoxnews.com aljazeera.comreuters.com\nmsnbc.com\nnbcnews.com\ncnn.com        MBFCFigure 1: Key components of our proposed approach. The sections highlighted with a grey background represent\nthe architectural contributions introduced by our framework. GNNs store the representation of the media graphs\nin an external global memory ( Mg). A Dirichlet prior is used to select the distribution of sparse candidate nodes,\nwhich are stored in the sampled memory ( Ms). The parameters Kand\u03b7control the number of candidate nodes\nand their influence, balancing local and global information. Since PLMs miss some of the media representation,\nthey leverage MGM representation-based probabilities for the classification task. The detailed pipeline of MGM\nintegration with PLM can be seen in Figure 3 (Appendix D).\ndenote the set of global similar nodes of node nas\ntn\u2208 {0,1}Nl, where tnm= 1indicates that node\nmis a global node similar to n. Similarly to node\nrepresentations, we also regard the similar node\nindicator tnas a latent variable. Therefore, the\njoint probability distribution of global information-\nenhanced method can be factorized as follows:\np\u03b8(Yl,T,Z|X,E) = (1)\n=p\u03b8(Z|X,E)p\u03b8(T|Z)p\u03b8(Yl|Z,T),\nwhere T= [tn]\u22a4\nn\u2208Vlare the global similar nodes\nof all nodes.\nHowever, finding global similar nodes with node\nrepresentations requires computing representations\nfor all nodes, which is expensive in terms of space\nand time (Fey et al., 2021). To alleviate this, we\npropose to store the embeddings of the labeled\nnodes in the memory and to use them to find global\nsimilar nodes. Consequently, the distribution of\nTcan be replaced by p\u03b8(T|\u02c6Z), where \u02c6Zis the\nembeddings of the labeled nodes in the memory,\ni.e.,\u02c6Vl. In this case, we can directly retrieve the\nrepresentation from memory without computing\nrepresentations for all nodes, thus making it more\nefficient to obtain the distribution of global similar\nnodes for both training and prediction.\nTo reduce the memory size, we select global\nsimilar nodes from a small set of candidate nodes,\nwhich are a subset of the training nodes. As a re-\nsult, only the embeddings of these candidate nodes\nare stored in memory for prediction. To achieve\nthis, we assume that p\u03b8(T|\u02c6Z)is a sparse dis-\ntribution, concentrated on a few candidate nodes.\nSince the candidate set is not known, we intro-\nduce a latent variable \u03c9for each node n, where\n\u03c9i\u2208[0,1], s.t.PNl\ni=1\u03c9i= 1. Here, \u03c9irepresents\nthe probability that the i-th node in the labelednode is a candidate node. Inspired by (He et al.,\n2020), we introduce a prior over \u03c9, i.e.p\u03b1(\u03c9)with\nparameter \u03b1. This prior is designed to encourage\na sparse distribution over \u03c9. Therefore, the joint\ndistribution of the method is now defined as:\np\u03b8(Yl,T,Z,\u03c9|X,E,\u02c6Z) =p\u03b1(\u03c9) (2)\np\u03b8(Z|X,E)p\u03b8(T|\u03c9,\u02c6Z)p\u03b8(Yl|T,Z).\nNext, we introduce the parameterization of our\nprobabilistic framework.\nPrior distribution over \u03c9.We use the Dirichlet\ndistribution as the prior distribution over \u03c9, i.e.,\np\u03b1(\u03c9)\u221dQN\ni=1\u03c9\u03b1i\u22121\ni, where \u03b1iis the concentra-\ntion parameter of the distribution. The concentra-\ntion parameter \u03b1is a positive value and a smaller\nvalue of \u03b1prefers a sparser distribution over \u03c9(He\net al., 2020). In our experiments, we set \u03b1 <1to\nencourage the sparse nodes distribution.\nPrior distribution over node representations Z.\nWe model the prior distribution over node repre-\nsentations as Gaussian distributions (Bojchevski\nand G\u00fcnnemann, 2018), which are obtained with\nGNNs due to their effectiveness in graph-learning\ntasks. Therefore, the prior distribution over Zis\ndefined as follows:\np\u03b8(Z|X,E) =N(Z|GNN \u03b8(X,E), \u03c32\n1I),(3)\nwhere \u03c32\n1is the learned variance of the prior and\nGNN \u03b8is anL-layer GNN with parameter \u03b8.\nPrior distribution over T.To obtain global simi-\nlar nodes of node n, we define a prior distribution\noverTas follows:\np\u03b8(T|\u03c9,\u02c6Z) = Mul( T|K, f \u03b8(\u03c9,\u02c6Z)),(4)\nwhere Mul(\u00b7)represents the multinomial distribu-\ntion,Kdenotes the predefined number of global\nsimilar nodes, and f\u03b8is designed as a parameter-\nized function that outputs the parameters of the\nmultinomial distribution.\nPrediction of label Y.Finally, we use node rep-\nresentation Zand information from global similar\nnodes to predict the label. Specifically, we leverage\nthe labels of global similar nodes and first predict\nthe label based on its representation:\np\u03b8(Y|Z) = Cat( Y|Z), (5)\nwhere p\u03b8(Y|Z)is formulated as a categorical\ndistribution. Then, we predict the label using the\nlabels of global similar nodes:\np\u03b8(Y|T)\u221dX\nN,M\u2208\u02c6VlTNM\u00b7YM, (6)\nwhere TNMrepresents the indices of global similar\nnodes for the predicted nodes set N. Furthermore,\nYMdenotes the one-hot labels of the nodes in\nM, where Mis the set of global similar nodes.\nFinally, the predicted label distribution is defined\nas follows:\np\u03b8(Y|Z,T) =\u03b7 p\u03b8(Y|Z)\n+ (1\u2212\u03b7)p\u03b8(Y|T), (7)\nwhere \u03b7\u2208[0,1]is a trade-off hyper-parameter.\nWhen \u03b7= 1, our model only uses local represen-\ntations of nodes for prediction, which degrades to\nvanilla GNNs. In contrast, when 0< \u03b7 < 1, our\nmodel predicts the labels of the nodes using in-\nformation from both local neighbors and global\nsimilar nodes.\n3.3 Training Process of MGM\nNext, we explain how to learn the model param-\neters \u03b8based on the graph. Ideally, the marginal\nlikelihood should be optimized during training:\np\u03b8(Yl|X,E,\u02c6Z) = (8)\n=Z\n\u03c9Z\nZX\nTp\u03b8(Yl,T,Z,\u03c9|X,E,\u02c6Z)dZd\u03c9.\nHowever, the computation of maximizing the\nmarginal likelihood is intractable due to the\nmarginalization of latent variables. As a result, we\ndevelop a variational Expectation-Maximization\n(EM) algorithm (Qu et al., 2019) to optimize itsAlgorithm 1 The proposed approach for the node classifica-\ntion task in news media profiling.\nInput: A training graph with labeled nodes\nG={V,E,X,Yl}and a test graph \u02dcG={\u02dcV,\u02dcE,\u02dcX}.\nOutput: Predicted labels \u02dcYfor the unlabeled nodes in \u02dcG.\n1:Pre-train p\u03b8according to the message-passing framework.\n2:while no converge do\n3:\u22a1E-step\n4: Calculate q\u03d5based on q\u03d5(T|Yl)andq\u03d5(Z|T,Yl).\n5: Calculate q\u03bb(\u03c9)based onQNl\ni=1\u03c9\u03bbi\u22121\ni .\n6: Update q\u03d5andq\u03bb(\u03c9)based on Equation 9.\n7:\u22a1M-step\n8: Calculate p\u03b8based on Equation 2.\n9: Update p\u03b8based on p\u03b8(Yl,T,Z,\u03c9|X,E)under the\ndistribution q\u03d5.\n10:end while\n11:Select the top Mnodes that occupy 90% of the probabil-\nity mass and their corresponding memorized embeddings\nas\u02c6Z\u03c9.\n12:Classify each unlabeled node in graph with p\u03b8and mem-\nory\u02c6Z\u03c9based on Equation (7).\nevidence lower bound (ELBO) instead:\nLELBO (Yl;\u03b8, \u03d5, \u03b1, \u03bb ) =\u2212DKL(q\u03bb(\u03c9)||p\u03b1(\u03c9))\n\u2212DKL\u0002\nq\u03d5(Z|T,Yl)||p\u03b8(Z|X,E))\u0003\n\u2212DKL\u0002\nq\u03d5(T|Yl)||p\u03b8(T|\u03c9,\u02c6Z)\u0003\n+Eq\u03d5(T|Yl)q\u03d5(Z|T,Yl)\u0002\nlogp\u03b8(Yl|T,Z)\u0003\n,(9)\nwhere DKL[\u00b7||\u00b7]is the Kullback-Leibler (KL) diver-\ngence, qrepresents the variational distribution to\napproximate the model posterior distribution and\nadheres to the following factorization form:3\nq\u03bb(\u03c9)q\u03d5(T,Z,\u03c9|Yl)q\u03d5(T|Yl)q\u03d5(Z|T,Yl),\nwhere \u03d5and\u03bbare variational parameters.\nNote that we use the mean-field assumption to\napproximate the posterior of \u03c9to simplify the\nvariational distributions. For computational conve-\nnience, we assume that the variational distributions\nof these latent variables have the same distribution\nform as their prior distributions. Hence, we define\nthe variational distributions of \u03c9,TandZto be\nDirichlet, multinomial, and Gaussian distributions,\nrespectively.\nNote that the KL divergence in Equation (9)has\na closed-form solution, and we approximate the ex-\npectation using a Monte Carlo method by sampling\nfrom the variational distributions. In variational\nEM, the variational parameters \u03d5and the model\n3We omit the dependence of variational distributions on\nnode features X, edges Eand memory \u02c6Zfor brevity.\nModelFact-2020 Bias-2020\nMacro-F1 Accuracy Average Recall Macro-F1 Accuracy Average Recall\nMajority class 22.93 \u00b10.00 52.43 \u00b10.00 33.33 \u00b10.00 19.18 \u00b10.00 40.39 \u00b10.00 33.33 \u00b10.00\nGCN 25.55 \u00b10.94 52.55 \u00b10.28 34.74 \u00b10.49 38.58 \u00b15.13 42.90 \u00b14.81 41.48 \u00b15.11\n+ MGM 43.05 \u00b12.03 53.37 \u00b11.00 43.42 \u00b11.53 42.77 \u00b11.09 45.23 \u00b11.70 43.80 \u00b13.19\nGAT 33.75 \u00b13.12 54.18 \u00b10.77 39.26 \u00b12.12 41.22 \u00b11.79 50.34 \u00b10.78 48.06 \u00b11.05\n+ MGM 43.63 \u00b12.80 55.11 \u00b11.44 43.54 \u00b12.71 50.41 \u00b12.86 54.06 \u00b11.98 51.96 \u00b10.79\nGraphSAGE 42.68 \u00b12.55 58.02 \u00b11.18 45.70 \u00b11.25 39.35 \u00b11.07 50.00 \u00b11.32 49.09 \u00b11.06\n+ MGM 46.67 \u00b11.58 59.00 \u00b11.00 47.40 \u00b11.67 46.77 \u00b11.82 51.04 \u00b10.67 50.18 \u00b10.92\nSGC 22.73 \u00b10.07 51.39 \u00b10.28 33.10 \u00b10.18 35.37 \u00b10.60 45.34 \u00b10.97 45.80 \u00b10.76\n+ MGM 41.28 \u00b11.42 53.95 \u00b10.77 41.32 \u00b11.22 39.11 \u00b10.51 46.74 \u00b10.78 47.10 \u00b10.74\nDNA 22.75 \u00b10.03 51.74\u00b10.00 33.33\u00b10.00 24.27 \u00b13.02 40.69 \u00b10.73 35.03 \u00b11.02\n+ MGM 34.04 \u00b11.60 50.81\u00b11.30 36.56\u00b11.98 33.22 \u00b11.13 42.55 \u00b12.50 38.59 \u00b11.81\nFiLM 43.32 \u00b12.25 57.09 \u00b1 0.77 44.46 \u00b11.40 39.33 \u00b12.76 47.55 \u00b1 1.12 47.85 \u00b11.07\n+ MGM 49.68 \u00b11.62 57.90 \u00b12.39 49.94 \u00b11.68 45.33 \u00b12.76 48.25 \u00b12.65 48.61 \u00b12.84\nFAGCN 24.77 \u00b17.52 47.04 \u00b13.71 36.12 \u00b15.30 19.69 \u00b10.65 39.88 \u00b10.28 33.71 \u00b10.31\n+ MGM 48.77 \u00b10.00 53.14 \u00b11.66 49.19 \u00b10.00 45.02 \u00b13.00 45.69 \u00b12.88 45.07 \u00b13.00\nGATv2 51.42 \u00b12.32 61.13 \u00b1 1.04 55.36 \u00b11.74 48.48 \u00b11.68 55.11 \u00b1 1.85 53.07 \u00b11.75\n+ MGM 54.50 \u00b12.55 62.72 \u00b11.01 57.36 \u00b11.06 52.41 \u00b12.85 55.46 \u00b12.45 54.00 \u00b12.61\nTable 1: Performance of GNN baselines and their MGM enhanced versions for the factuality and political bias tasks\non the ACL-2020 dataset, with the majority class baseline and SVM included as na\u00efve and non-graphical methods.\nThe higher performance is highlighted in bold.\nparameters \u03b8are learned alternately. In the E-step,\nwe fix \u03b8and update \u03d5by minimizing the KL di-\nvergence to approximate the true posteriors. In the\nM-step, we fix \u03d5and update \u03b8by maximizing the\nexpected log-likelihood.\n3.4 Prediction Process of MGM\nAfter training, we expect to obtain a sparse distri-\nbution q\u03bb(\u03c9), allowing us to select a subset of the\ncandidate nodes. In this case, we can select can-\ndidate nodes over a certain probability threshold,\nthus reducing the memory size and improving the\nefficiency for prediction. Specifically, we calculate\nthe expected value of q\u03bb(\u03c9)for each node i, which\nis given by Eq\u03bb(\u03c9)[\u03c9i] =\u03bbi/PNl\nj=1\u03bbj, and then\nwe select the top- Mnodes that occupy 90% of the\nprobability mass as candidate nodes.\nWe then leverage the embeddings of the mem-\norized candidate nodes \u02c6Z\u03c9andp\u03b8to predict the\nlabels of the test nodes \u02dcnbased on Equation (7).\nWe also provide an overview of the optimization\nprocess of the MGM model for the news media\nprofiling in Algorithm 1.\n3.5 Enhancing PLMs Predictions with MGM\nNext, we demonstrate how MGM improves the\nperformance of PLMs by incorporating information\nfrom global similar nodes. Given textual features\nS, such as those from Articles andWikipedia pages\nfor the media outlet, we first fine-tune the PLMs\nusing the cross-entropy loss. Then, we concatenate\nthe predicted label distribution from the PLMs withMGM to obtain the final label distribution:\np\u03c8,\u03b8(Y|S,Z,T) = (10)\n=Softmax (\u2295(p\u03c8(Y|S), p\u03b8(Y|Z,T))W+b),\nwhere \u03c8are the parameters of the fine-tuned PLMs,\n\u2295is the concatenation operation, p\u03c8(Y|S)is the\nlabel distribution predicted by the fine-tuned PLMs,\np\u03b8(Y|Z,T)is the label distribution predicted by\nMGM, which is based on Equation (7),Wandb\nare the parameters of the linear classifier. More\ndetails are given in Figure 3 and Appendix D.\n4 Experiments\n4.1 Research Questions\nWe explore the following research questions\n(RQs) :\n\u2022(RQ1 ) Can MGM tackle disconnected com-\nponents and label sparsity in media graphs for\nfactuality and political bias detection tasks?\n\u2022(RQ2 ) How do the number of global similar\nnodes Kand the trade-off hyper-parameter \u03b7\naffect the performance of MGM?\n\u2022(RQ3 ) How does the memory module affect\nthe performance of MGM?\n\u2022(RQ4 ) How does MGM elevate the perfor-\nmance of PLMs when faced with the chal-\nlenge of missing text in Wikipedia orArticles ?\n4.2 Dataset\nThe dataset for factuality and political bias of news\nmedia introduced by Baly et al. (2020b) comprises\nModelFact-2020 Bias-2020\nMacro-F1 \u2020/ \u00a7 Average Recall \u2020/ \u00a7 Macro-F1 \u2020/ \u00a7 Average Recall \u2020/ \u00a7\nGCN 42.04 \u00b11.91 / 43.05\u00b12.04 41.97\u00b11.77 / 43.42\u00b11.53 42.77 \u00b11.10 / 42.37 \u00b12.09 43.72 \u00b13.15 / 43.80\u00b13.19\nGAT 40.63 \u00b13.28 / 43.63\u00b12.81 40.23\u00b12.88 / 43.54\u00b12.71 50.41 \u00b12.86 / 47.12 \u00b11.69 51.96\u00b10.79 / 49.39 \u00b12.02\nGraphSAGE 45.11 \u00b11.44 / 46.68\u00b11.59 45.69\u00b11.42 / 47.40\u00b11.67 44.96\u00b11.72 / 46.78\u00b11.83 49.78\u00b10.66 / 51.04\u00b10.67\nSGC 41.29\u00b11.42 / 39.65 \u00b12.00 41.32\u00b11.22 / 40.04 \u00b11.50 38.32 \u00b11.41 / 39.12\u00b10.51 46.74\u00b10.56 / 47.10\u00b10.74\nDNA 33.48 \u00b13.69 / 34.05\u00b11.60 35.99\u00b12.24 / 36.56\u00b11.98 33.22 \u00b11.13 / 32.25 \u00b14.14 38.59\u00b11.81 / 36.63 \u00b14.53\nFiLM 45.12 \u00b13.38 / 49.68\u00b11.62 45.58\u00b12.78 / 49.94\u00b11.68 43.98\u00b12.57 / 45.33\u00b12.76 47.86\u00b11.46 / 48.61\u00b12.84\nFAGCN 48.77\u00b10.00 / 46.88 \u00b12.88 49.19\u00b10.00 / 48.05 \u00b12.65 45.02\u00b13.00 / 44.36 \u00b11.24 45.07\u00b13.00 / 44.47 \u00b11.36\nGATv2 54.13 \u00b12.93 / 54.50\u00b12.55 56.82\u00b11.94 / 57.36\u00b11.06 52.41 \u00b12.85 / 50.44 \u00b10.95 54.00\u00b12.61 / 52.02 \u00b11.29\nTable 2: Summary of the MGM results detailing the performance variation the use of between using full memory\n(\u2020) and a reduced (90%) memory allocation (\u00a7) for each GNN.\nModel 60% labels 80% labels 100% labels\nGAT 37.90 \u00b10.41 39.22 \u00b10.71 33.75 \u00b13.12\n+MGM 40.80 \u00b13.83 41.99 \u00b11.44 43.63 \u00b12.80\nFiLM 39.89 \u00b11.69 38.59 \u00b14.30 43.32 \u00b12.25\n+MGM 42.93 \u00b14.00 38.62 \u00b12.73 49.68 \u00b11.62\nFAGCN 24.32 \u00b13.18 22.73 \u00b10.00 24.77 \u00b17.52\n+MGM 34.10 \u00b17.36 39.89 \u00b14.04 48.77 \u00b10.00\nGATv2 40.54 \u00b11.47 42.14 \u00b12.76 51.42 \u00b12.32\n+MGM 42.98 \u00b12.51 44.35 \u00b12.14 54.50 \u00b12.55\nTable 3: The impact of different proportions of training\nlabeled data on the performance (Macro-F1) of MGM\nfor the Fact-2020 task.\n859 media sources4, their domain names and cor-\nresponding gold labels. These labels are sourced\nfrom MBFC, a platform supported by independent\njournalists. Factuality is given on a three-point\nscale: high, mixed, and low. Political bias is also\non a three-point scale: left, center, right. Panay-\notov et al. (2022) used Alexa Rank5to create a\ngraph based on audience overlap, using the 859\nmedia as seed nodes. Media sources that shared\nthe same audience, as determined by Alexa Rank,\nwere connected with an edge. Alexa Rank returned\na maximum of five similar media sources for each\nmedium, which could be part of the initial seed\nnodes or newly identified media. As depicted in\nFigure 1, BBC, MSNBC, and Reuters are listed as\nmedia sources in the MBFC dataset. The Alexa tool\nidentified five related media for BBC and MSNBC,\nwith an edge connecting them due to their shared\naudience overlap. In the resulting graph, the nodes\nrepresent the media sources, and the edges repre-\nsent the percentage of audience overlap between\ntwo media. We use these publicly available graph\ndata (the only one of its kind) to train GNNs for\nthe factuality and political bias of the news media.\n4https://github.com/ramybaly/\nNews-Media-Reliability\n5http://www.alexa.com/siteinfoModel Macro-F1 \u2020/ \u00a7 Average Recall \u2020/ \u00a7\nGCN 47.20\u00b11.54 / 46.52 \u00b11.52 48.13\u00b11.19 / 47.60 \u00b11.16\nGAT 54.99\u00b14.14 / 53.65 \u00b12.79 57.15\u00b14.05 / 55.85 \u00b12.27\nGraphSage 46.54 \u00b11.65 / 47.86\u00b11.38 49.09\u00b10.87 / 50.91\u00b11.07\nSGC 44.60 \u00b12.41 / 45.16\u00b12.29 45.82\u00b10.73 / 46.03\u00b11.80\nDNA 34.93\u00b13.95 / 33.88 \u00b11.52 36.71\u00b13.83 / 35.35 \u00b11.68\nFiLM 51.06 \u00b12.24 / 51.47\u00b12.47 51.35\u00b12.16 / 52.13\u00b12.01\nTable 4: Summary of the MGM results detailing perfor-\nmance variations between using full memory ( \u2020) and\na reduced 90% memory allocation (\u00a7) for each GNN\nacross Fact-2018 task. The best performance per base\nmodel is marked in bold .\nMore details are given in Appendix A.\n4.3 Baselines\nFor evaluation, we consider two categories of\nbaselines, including GNN-based and PLM-based\nmodels. For GNN models, we select eight well-\nknown models, including GCN (Kipf and Welling,\n2016), GraphSAGE (Hamilton et al., 2017), GAT\n(Veli \u02c7ckovi \u00b4c et al., 2018), SGC (Wu et al., 2019),\nDNA (Fey, 2019), FiLM (Brockschmidt, 2020),\nFAGCN (Bo et al., 2021) and GATv2 (Brody et al.,\n2022). More details on these GNN baselines are\nprovided in the Appendix B. For PLMs, we use four\nstate-of-the-art encoder models, including BERT,\nRoBERTa, DistillBERT, and DeBERTaV3. Next,\nwe compare our results with state-of-the-art results\nfor the factuality and political bias of the news me-\ndia (Panayotov et al., 2022; Mehta et al., 2022).\n5 Discussion\n5.1 Overall Performance\nTo answer RQ1 , we conduct factuality and po-\nlitical bias classification experiments in a semi-\nsupervised setting. The experimental results re-\nported in Table 1 demonstrate that MGM can im-\nprove the performance of existing GNNs in almost\nall cases. For example, when applied to the Fact-\n2020 dataset, MGM improves the Macro-F1 perfor-\nModel Macro-F1 Average Recall\nMajority class 22.47 \u00b10.00 33.33 \u00b10.00\nSVM 41.78 \u00b10.00 48.89 \u00b10.00\nGCN 48.63 \u00b12.19 48.16 \u00b12.49\n+ MGM 49.21 \u00b11.54 51.13 \u00b11.19\nGAT 46.63 \u00b13.53 52.25 \u00b14.20\n+ MGM 54.99 \u00b14.14 57.15 \u00b14.05\nGraphSAGE 41.77 \u00b10.22 48.65 \u00b10.22\n+ MGM 47.86 \u00b11.38 50.91 \u00b11.07\nSGC 41.06 \u00b10.35 44.91 \u00b10.43\n+ MGM 45.16 \u00b12.29 46.03 \u00b11.80\nDNA 28.24 \u00b11.23 33.26 \u00b11.03\n+ MGM 34.93 \u00b13.95 36.71 \u00b13.83\nFiLM 46.75 \u00b10.79 50.92 \u00b11.36\n+ MGM 51.47 \u00b12.47 52.13 \u00b12.01\nTable 5: Performance of GNN baselines and their MGM\nenhanced versions on the Fact task of EMNLP-2018,\nwith the majority class baseline and SVM included as\nnaive and non-graphical methods. The highest perfor-\nmance is highlighted in bold.\nmance of GCN, GAT, SGC, and DNA by 17.5%,\n9.8%, 18.5%, and 11.4%, respectively. Similarly,\nfor Bias-2020, we can observe that GNNs equipped\nwith MGM consistently outperform the correspond-\ning base models in all evaluation measures.\nWe conducted a series of experiments using dif-\nferent proportions of training labels to assess the\nperformance of MGM as shown in Table 3. The\nresults indicate a clear trend: as we increase the per-\ncentage of training labels, the model performance\nimproves significantly compared to the baseline.\nDue to limited data, using a smaller percentage\nof training labels results in modest improvements\nover the baseline, constraining the model\u2019s ability\nto generalize well to unseen data. MGM effectively\naddresses RQ1 by leveraging global similar nodes\nin media graphs with disconnected components and\nlabel sparsity for the detection of factuality and po-\nlitical bias. Our evaluation extends to Fact-2018\nand depicts MGM\u2019s stable performance across dif-\nferent datasets presented in Table 5. The results\nshow that MGM is able to consistently improve\nall the baselines. Given the reasons described in\nAppendix A, experiments are not conducted on the\npolitical bias task of EMNLP-2018.\n5.2 Impact of the Number of Global Similar\nNodes\nNext, we turn to RQ2 to understand the impact\nof the number of global similar nodes K. Specif-\nically, we investigate the performance of MGM\nwith different values of K. As shown in Figures\n1 2 3 4 5 6 7\nK304050Macro-F1 (%)\n(a) Fact-2020\n1 2 3 4 5 6 7\nK304050\n(b) Bias-2020\n1 0.9 0.8 0.7 0.6\n304050Macro-F1 (%)\n(c) Fact-2020\n1 0.9 0.8 0.7 0.6\n304050\n(d) Bias-2020\nGCN GAT GraphSAGE SGC DNA FiLMFigure 2: MGM performance across all GNNs for both\ntasks, evaluated for different values of K(global similar\nnodes) and \u03b7(trade-off hyper-parameter).\n2(a) and 2(b), leveraging a few global similar nodes\ncan improve the performance of the base GNNs.\nFor example, both GCN and SGC exhibit similar\npatterns, peaking in performance at K=3 on the fac-\ntuality task. The performance of GNNs enhanced\nwith MGM decreases when Kexceeds a certain\nthreshold. This is attributed to the introduction of\nnoise by incorporating excessive information from\nnumerous global similar nodes.\n5.3 Impact of the Trade-off Hyper-Parameter\nRecall that in Section 3.2, we introduced a hyper-\nparameter \u03b7that influences the predicted label dis-\ntribution. When \u03b7= 1, MGM only relies on lo-\ncal node representations for prediction, degrading\nto a vanilla GNN. In contrast, when \u03b7 < 1, our\nmodel incorporates information from both local\nneighbors and global similar nodes to predict the\nnode labels. To further investigate the impact of\nthe trade-off hyper-parameter \u03b7, we analyze the\nsensitivity of MGM to its value. The experimental\nresults are shown in Figures 2(c) and 2(d). We find\nthat compared to \u03b7= 1, MGM yields improved\nperformance when \u03b7 < 1in most cases. For ex-\nample, GCN achieves its best performance when\nintegrated with MGM using an \u03b7value of 0.8. As\na result, the effectiveness of incorporating informa-\ntion from global similar nodes highlighted in the\nresults validates the RQ2 .\n5.4 Effectiveness of the Memory Module\nRecall that in Section 3.4, MGM leveraged a Dirich-\nlet prior to select a small set of candidate nodes\nand stored their node embeddings in the sampled\nmemory ( MS) for prediction. To compare the ef-\nModelFact-2020 Bias-2020\nArticles Wikipedia Articles Wikipedia\nMacro-F1 Accuracy Avg Recall Macro-F1 Accuracy Avg Recall Macro-F1 Accuracy Avg Recall Macro-F1 Accuracy Avg Recall\nSTAGE 1BERT Base 38.27 63.37 39.65 34.64 59.30 37.98 65.38 68.02 64.01 58.70 62.79 58.69\nRoBERTa Base 33.55 62.79 37.65 25.29 59.30 33.36 63.34 65.70 62.41 58.73 62.78 58.71\nDistilBERT Base 35.27 62.21 37.65 25.27 61.01 33.30 65.04 67.44 63.91 58.71 62.85 58.72\nDeBERTaV3 Base 25.28 61.02 33.35 40.81 61.05 40.75 58.72 62.80 58.69 59.65 63.37 59.15\nSTAGE 2BERT MGM GATv2 76.18 81.98 71.86 73.69 81.40 70.92 83.74 84.30 83.88 82.25 82.56 81.40\nRoBERTa MGM GATv2 69.89 80.23 66.85 72.73 79.65 70.52 85.51 86.05 85.38 81.32 81.98 80.71\nDistilBERT MGM GATv2 74.55 81.40 71.48 73.03 80.23 70.84 87.20 87.79 86.90 80.68 81.40 80.25\nDeBERTaV3 MGM GATv2 64.87 77.91 62.97 74.56 81.98 73.20 87.71 88.37 87.70 80.26 80.81 79.28\nTable 6: Stage 1: Performance of logistic regression (meta-learner) on PLM probabilities with missing media\nattributed as probabilities (0.0,0.0,0.0).Stage 2: Performance of the logistic regression (meta-learner) on PLMs\nprobabilities + MGM GATv2 probabilities for missing media for factuality and political bias of the ACL-2020 dataset.\nModelFact-2020 Bias-2020\nMacro-F1 Accuracy Avg Recall Macro-F1 Accuracy Avg Recall\nNode classification (NC) (Mehta et al., 2022) 68.90 63.72 - - - -\nInfO PBest Model (Mehta et al., 2022) 72.55 66.89 - - - -\nGRENNER (Panayotov et al., 2022) 69.61 74.27 - 91.93 92.08 -\nSTAGE 3 DeBERTaV3 MGM GATv2 + BERT MGM GATv2 78.43 83.04 75.03 92.64 92.98 92.67\nSTAGE 4DeBERTaV3 MGM GATv2 + BERT MGM GATv2 + MGM FiLM 79.72\u00b10.00 84.21 \u00b10.00 76.54 \u00b10.00 93.04\u00b10.26 93.45\u00b10.23 93.19 \u00b10.26\nDeBERTaV3 MGM GATv2 + BERT MGM GATv2 + MGM FAGCN 75.69\u00b13.49 81.29 \u00b13.09 72.24 \u00b13.35 93.08\u00b10.24 93.45 \u00b10.23 93.15\u00b10.34\nDeBERTaV3 MGM GATv2 + BERT MGM GATv2 + MGM GATv2 77.96\u00b10.30 82.69 \u00b10.29 74.84 \u00b10.16 92.71 \u00b10.46 93.10 \u00b10.44 92.72 \u00b10.52\nTable 7: Previous studies (Mehta et al., 2022; Panayotov et al., 2022) and our best results. Stage 3: We concatenate\nthe probabilities of the best PLMs from Wikipedia andArticles and use logistic regression to make predictions.\nStage 4: We use probabilities from the Stage 3 model and concatenate with the probabilities of three GNNs\n(MGM FiLM, MGM FAGCN and MGM GATv2 ).\nfectiveness of the sampled memory module to the\nfull memory module ( MG), which stores all the\ntraining node embeddings, we conducted a perfor-\nmance comparison between the two memory mod-\nules. The experimental results are given in Table 2,\nand they answer RQ3 that MGM using sampled\nmemory achieves a performance comparable to\nMGM when using full memory. For example, for\nthe GAT model, the performance is higher when us-\ning sampled memory compared to when using full\nmemory. This suggests that the sampled memory\neffectively captures sufficient information, allow-\ning MGM to maintain its performance even with\nlimited memory. The experimental results on the\nFact-2018 dataset reported in Table 4 also show\nconsistent trends which validate the versatility of\nthe memory module for media graphs.\n5.5 Impact of Integrating MGM with PLMs\nTo answer RQ4 , we integrate the MGM probabil-\nities with those from deep learning models based\non textual features, and we observe that this sub-\nstantially enhances the performance. Initially, with\nzero probabilities for missing textual features, we\nachieved accuracies of 68.02% for political bias in\nArticles , 63.37% for Wikipedia , 63.37% for factual-\nity in Articles , and 61.05% for Wikipedia , respec-\ntively (see Table 6). Replacing the zero probabili-ties with the best MGM GATv2 improve the perfor-\nmance by up to 30%. Further concatenating the\nbest model probabilities in stage three led to addi-\ntional gains, and in stage four, our models outper-\nformed previous state-of-the-art results in political\nbias and factuality (Panayotov et al., 2022; Mehta\net al., 2022) (can be seen in Table 7).\n6 Conclusion & Future Work\nOur study focused on the underexplored problem\nof profiling news media in terms of factuality and\npolitical bias. To address the shortcomings of ex-\nisting media graphs, we introduced MediaGraph-\nMind (MGM), an innovative EM framework that\nsignificantly enhances the performance of GNNs\nby leveraging globally similar nodes. The external\nmemory module of MGM efficiently stores and\nretrieves node representations, addressing the chal-\nlenge of test-time inefficiency by selecting global\nsimilar nodes from a smaller candidate set based\non a sparse node selection distribution. Our exper-\niments demonstrate that the integration of MGM\nfeatures with PLMs consistently improves over ex-\nisting baselines and establishes a new state-of-the-\nart results.\nIn future work, we plan to explore multi-graph\nfusion, multi-task learning, and ordinal classifica-\ntion for diverse graph structures in media profiling.\nLimitations\nThe graph dataset, originating from the ACL-2020\nmedia nodes, was constructed using the Alexa\nRank siteinfo tool, which is currently unavail-\nable. Although the graph aids in the task by cap-\nturing the inherent and hidden relationships be-\ntween media, building such graphs is complex\nand resource-intensive. The research largely re-\nlies on U.S.-centric definitions of political bias\n(left/center/right), which may not accurately cap-\nture the nuanced ideological biases present in news\noutlets from other cultural or political contexts.\nMoreover, the available graph is limited to the 2020\ndataset. We are actively working on constructing\ngraphs for the latest benchmarks, which include a\nlarger number of media sources and updated MBFC\nrankings. Moreover, we faced limitations in col-\nlecting Articles andWikipedia texts from media\nsources from the ACL-2020 dataset due to the in-\naccessibility of their websites.\nEthical Statement\nOptimizing model architectures to enhance energy\nefficiency in training and inference operations is\ncrucial to reducing environmental impact. Instead\nof relying on extensive computational resources to\ntrain complex models, which significantly increase\ncarbon emissions, we propose improving model\nperformance with less computational power. The\nArticles from the news media pages were compiled\nin strict compliance with legal and ethical standards.\nWe carefully reviewed the terms of use for all web-\nsites to ensure that our data collection processes\nadhered to them. Our compilation focused solely\non publicly available data, avoiding paywalls and\nsubscription models. Transparent data collection\nmethods were designed to minimize the impact\non source websites, including limiting the access\nfrequency to prevent resource strain.\nReferences\nSadia Afroz, Michael Brennan, and Rachel Greenstadt.\n2012. Detecting hoaxes, frauds, and deception in\nwriting style online. In 2012 IEEE Symposium on\nSecurity and Privacy , pages 461\u2013475. IEEE.\nJisun An, Meeyoung Cha, Krishna Gummadi, Jon\nCrowcroft, and Daniele Quercia. 2012. Visualiz-\ning media bias through Twitter. In AAAI ICWSM ,\nvolume 6.\nDilshod Azizov, S Liang, and P Nakov. 2023. Frank atcheckthat! 2023: Detecting the political bias of news\narticles and news media. Working Notes of CLEF .\nDilshod Azizov, Zain Mujahid, Hilal AlQuabeh, Preslav\nNakov, and Shangsong Liang. 2024. Safari: Cross-\nlingual bias and factuality detection in news media\nand news articles. In Findings of the Association\nfor Computational Linguistics: EMNLP 2024 , pages\n12217\u201312231.\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 3528\u20133539, Brussels, Belgium. Association\nfor Computational Linguistics.\nRamy Baly, Georgi Karadzhov, Jisun An, Haewoon\nKwak, Yoan Dinkov, Ahmed Ali, James Glass,\nand Preslav Nakov. 2020a. What was written\nvs. who read it: news media profiling using text\nanalysis and social media context. arXiv preprint\narXiv:2005.04518 .\nRamy Baly, Georgi Karadzhov, Jisun An, Haewoon\nKwak, Yoan Dinkov, Ahmed Ali, James Glass, and\nPreslav Nakov. 2020b. What was written vs. who\nread it: News media profiling using text analysis\nand social media context. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics , pages 3364\u20133374, Online. Association\nfor Computational Linguistics.\nYejin Bang, Delong Chen, Nayeon Lee, and Pascale\nFung. 2024. Measuring political bias in large lan-\nguage models: What is said and how it is said. arXiv\npreprint arXiv:2403.18932 .\nYejin Bang, Nayeon Lee, and Pascale Fung. 2023. Miti-\ngating framing bias with polarity minimization loss.\narXiv preprint arXiv:2311.01817 .\nAlberto Barr\u00f3n-Cede\u00f1o, Firoj Alam, Tommaso Caselli,\nGiovanni Da San Martino, Tamer Elsayed, An-\ndrea Galassi, Fatima Haouari, Federico Ruggeri, Ju-\nlia Maria Stru\u00df, Rabindra Nath Nandi, et al. 2023a.\nThe clef-2023 checkthat! lab: Checkworthiness,\nsubjectivity, political bias, factuality, and authority.\nInEuropean Conference on Information Retrieval ,\npages 506\u2013517. Springer.\nAlberto Barr\u00f3n-Cede\u00f1o, Firoj Alam, Andrea Galassi,\nGiovanni Da San Martino, Preslav Nakov, Tamer\nElsayed, Dilshod Azizov, Tommaso Caselli, Gullal S\nCheema, Fatima Haouari, et al. 2023b. Overview\nof the clef\u20132023 checkthat! lab on checkworthiness,\nsubjectivity, political bias, factuality, and authority\nof news articles and their source. In International\nConference of the Cross-Language Evaluation Forum\nfor European Languages , pages 251\u2013275. Springer.\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Al-\nvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz\nMalinowski, Andrea Tacchetti, David Raposo, Adam\nSantoro, Ryan Faulkner, et al. 2018. Relational in-\nductive biases, deep learning, and graph networks.\narXiv preprint arXiv:1806.01261 .\nDeyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen.\n2021. Beyond low-frequency information in graph\nconvolutional networks. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 35,\npages 3950\u20133957.\nAleksandar Bojchevski and Stephan G\u00fcnnemann. 2018.\nDeep gaussian embedding of graphs: Unsupervised\ninductive learning via ranking. In International Con-\nference on Learning Representations .\nMarc Brockschmidt. 2020. Gnn-film: Graph neural\nnetworks with feature-wise linear modulation. In In-\nternational Conference on Machine Learning , pages\n1144\u20131152. PMLR.\nShaked Brody, Uri Alon, and Eran Yahav. 2021. How\nattentive are graph attention networks? In Interna-\ntional Conference on Learning Representations .\nShaked Brody, Uri Alon, and Eran Yahav. 2022. How\nattentive are graph attention networks?\nCeren Budak, Sharad Goel, and Justin M Rao. 2016.\nFair and balanced? quantifying media bias through\ncrowdsourced content analysis. Public Opinion\nQuarterly , 80(S1):250\u2013271.\nSonia Castelo, Thais Almeida, Anas Elghafari, A\u00e9cio\nSantos, Kien Pham, Eduardo Nakamura, and Juliana\nFreire. 2019. A topic-agnostic approach for identi-\nfying fake news pages. In Companion proceedings\nof the 2019 World Wide Web conference , pages 975\u2013\n980.\nApril Chen, Ryan A Rossi, Namyong Park, Puja Trivedi,\nYu Wang, Tong Yu, Sungchul Kim, Franck Dernon-\ncourt, and Nesreen K Ahmed. 2024. Fairness-aware\ngraph neural networks: A survey. ACM Transactions\non Knowledge Discovery from Data , 18(6):1\u201323.\nDjork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp\nHochreiter. 2016. Fast and accurate deep network\nlearning by exponential linear units (elus). In Inter-\nnational Conference on Learning Representations .\nNadia K Conroy, Victoria L Rubin, and Yimin Chen.\n2015. Automatic deception detection: Methods for\nfinding fake news. Proceedings of the association for\ninformation science and technology , 52(1):1\u20134.\nGiovanni Da San Martino, Firoj Alam, Maram Hasanain,\nRabindra Nath Nandi, Dilshod Azizov, and Preslav\nNakov. 2023. Overview of the CLEF-2023 Check-\nThat! lab task 3 on political bias of news arti-\ncles and news media. In Working Notes of CLEF\n2023\u2013Conference and Labs of the Evaluation Forum ,\nCLEF \u20192023, Thessaloniki, Greece.\nRohan Das, Aditya Chandra, I-Ta Lee, and\nMaria Leonor Pacheco. 2024. Media framing\nthrough the lens of event-centric narratives. arXiv\npreprint arXiv:2410.03151 .Dorottya Demszky, Nikhil Garg, Rob V oigt, James Zou,\nMatthew Gentzkow, Jesse Shapiro, and Dan Juraf-\nsky. 2019. Analyzing polarization in social media:\nMethod and application to tweets on 21 mass shoot-\nings. arXiv preprint arXiv:1904.01596 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJames Fairbanks, Natalie Fitch, Nathan Knauf, and Er-\nica Briscoe. 2018. Credibility assessment in the news:\ndo we need to read. In Proc. of the MIS2 Work-\nshop held in conjuction with 11th Int\u2019l Conf. on Web\nSearch and Data Mining , pages 799\u2013800. ACM.\nLisa Fan, Marshall White, Eva Sharma, Ruisi Su, Pra-\nfulla Kumar Choubey, Ruihong Huang, and Lu Wang.\n2019. In plain sight: Media bias through the lens of\nfactual reporting. arXiv preprint arXiv:1909.02670 .\nMatthias Fey. 2019. Just jump: Dynamic neighborhood\naggregation in graph neural networks. arXiv preprint\narXiv:1904.04849 .\nMatthias Fey, Jan E Lenssen, Frank Weichert, and Jure\nLeskovec. 2021. Gnnautoscale: Scalable and expres-\nsive graph neural networks via historical embeddings.\nInInternational Conference on Machine Learning ,\npages 3294\u20133304.\nMatthias Fey and Jan Eric Lenssen. 2019. Fast graph\nrepresentation learning with pytorch geometric. In\nICLR 2019 (RLGM Workshop) .\nXiaobo Guo, Weicheng Ma, and Soroush V osoughi.\n2022. Measuring media bias via masked language\nmodeling. In Proceedings of the International AAAI\nConference on Web and Social Media , volume 16,\npages 1404\u20131408.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. 2017.\nInductive representation learning on large graphs. Ad-\nvances in Neural Information Processing Systems ,\n30.\nJunxian He, Taylor Berg-Kirkpatrick, and Graham Neu-\nbig. 2020. Learning sparse prototypes for text gen-\neration. Advances in Neural Information Processing\nSystems , 33:14724\u201314735.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543 .\nAustin Hounsel, Jordan Holland, Ben Kaiser, Kevin\nBorgolte, Nick Feamster, and Jonathan Mayer. 2020.\nIdentifying disinformation websites using infrastruc-\nture features. In 10th USENIX Workshop on Free and\nOpen Communications on the Internet (FOCI 20) .\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao\nDong, Hongyu Ren, Bowen Liu, Michele Catasta,\nand Jure Leskovec. 2020. Open graph benchmark:\nDatasets for machine learning on graphs. ArXiv ,\nabs/2005.00687.\nMinyoung Huh, Andrew Liu, Andrew Owens, and\nAlexei A Efros. 2018. Fighting fake news: Image\nsplice detection via learned self-consistency. In Pro-\nceedings of the European conference on computer\nvision (ECCV) , pages 101\u2013117.\nLalitha Kameswari and Radhika Mamidi. 2021. To-\nwards quantifying magnitude of political bias in news\narticles using a novel annotation schema. In Pro-\nceedings of the International Conference on Recent\nAdvances in Natural Language Processing (RANLP\n2021) , pages 671\u2013678.\nJuho Kim and Michael Guerzhoy. 2024. Observing the\nsouthern us culture of honor using large-scale social\nmedia analysis. arXiv preprint arXiv:2410.13887 .\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations .\nThomas N Kipf and Max Welling. 2016. Semi-\nsupervised classification with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907 .\nNayeon Lee, Yejin Bang, Tiezheng Yu, Andrea Madotto,\nand Pascale Fung. 2022. Neus: Neutral multi-news\nsummarization for mitigating framing bias. arXiv\npreprint arXiv:2204.04902 .\nYuanyuan Lei, Ruihong Huang, Lu Wang, and Nick\nBeauchamp. 2022. Sentence-level media bias analy-\nsis informed by discourse structures. In Proceedings\nof the 2022 conference on empirical methods in natu-\nral language processing , pages 10040\u201310050.\nLuyang Lin, Lingzhi Wang, Xiaoyan Zhao, Jing Li,\nand Kam-Fai Wong. 2024. Indivec: An exploration\nof leveraging large language models for media bias\ndetection with fine-grained bias indicators. arXiv\npreprint arXiv:2402.00345 .\nSiyi Liu, Lei Guo, Kate Mays, Margrit Betke, and\nDerry Tanti Wijaya. 2019a. Detecting frames in news\nheadlines and its application to analyzing news fram-\ning trends surrounding us gun violence. In Proceed-\nings of the 23rd conference on computational natural\nlanguage learning (CoNLL) , pages 504\u2013514.\nSiyi Liu, Hongming Zhang, Hongwei Wang, Kaiqiang\nSong, Dan Roth, and Dong Yu. 2023. Open-domain\nevent graph induction for mitigating framing bias.\narXiv preprint arXiv:2305.12835 .Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nYujian Liu, Xinliang Frederick Zhang, David Wegs-\nman, Nick Beauchamp, and Lu Wang. 2022. POLI-\nTICS: pretraining with same-story article comparison\nfor ideology prediction and stance detection. arXiv\npreprint arXiv:2205.00619 .\nAntonio Longa, Steve Azzolin, Gabriele Santin, Giu-\nlia Cencetti, Pietro Li\u00f2, Bruno Lepri, and Andrea\nPasserini. 2024. Explaining the explainers in graph\nneural networks: a comparative study. ACM Comput-\ning Surveys .\nIffat Maab, Edison Marrese-Taylor, Sebastian Pad\u00f3, and\nYutaka Matsuo. 2024. Media bias detection across\nfamilies of language models. In Proceedings of the\n2024 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies (Volume 1: Long Pa-\npers) , pages 4083\u20134098.\nNikhil Mehta and Dan Goldwasser. 2023a. An inter-\nactive framework for profiling news media sources.\narXiv preprint arXiv:2309.07384 .\nNikhil Mehta and Dan Goldwasser. 2023b. Interac-\ntively learning social media representations improves\nnews source factuality detection. arXiv preprint\narXiv:2309.14966 .\nNikhil Mehta, Mar\u00eda Leonor Pacheco, and Dan Gold-\nwasser. 2022. Tackling fake news detection by con-\ntinually improving social context representations us-\ning graph neural networks. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages\n1363\u20131380.\nVinod Nair and Geoffrey E Hinton. 2010. Rectified\nlinear units improve restricted boltzmann machines.\nInProceedings of the 27th International Conference\non International Conference on Machine Learning ,\npages 807\u2013814.\nPreslav Nakov, Firoj Alam, Giovanni Da San Martino,\nMaram Hasanain, Rabindra Nath Nandi, Dilshod Az-\nizov, and Panayot Panayotov. 2023a. Overview of the\nCLEF-2023 CheckThat! lab task 4 on factuality of\nreporting of news media. In Working Notes of CLEF\n2023\u2013Conference and Labs of the Evaluation Forum ,\nCLEF \u20192023, Thessaloniki, Greece.\nPreslav Nakov, Firoj Alam, Giovanni Da San Martino,\nMaram Hasanain, RN Nandi, D Azizov, and P Panay-\notov. 2023b. Overview of the clef-2023 checkthat!\nlab task 4 on factuality of reporting of news media.\nWorking Notes of CLEF .\nPreslav Nakov, Jisun An, Haewoon Kwak, Muham-\nmad Arslan Manzoor, Zain Muhammad Mujahid, and\nHusrev Taha Sencar. 2024. A survey on predicting\nthe factuality and the bias of news media. In An-\nnual Meeting of the Association for Computational\nLinguistics .\nPanayot Panayotov, Utsav Shukla, Husrev Taha Sen-\ncar, Mohamed Nabeel, and Preslav Nakov. 2022.\nGREENER: Graph neural networks for news media\nprofiling. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing ,\nEMNLP \u201922, Abu Dhabi, UAE.\nVer\u00f3nica P\u00e9rez-Rosas, Bennett Kleinberg, Alexandra\nLefevre, and Rada Mihalcea. 2017. Automatic detec-\ntion of fake news. arXiv preprint arXiv:1708.07104 .\nMeng Qu, Yoshua Bengio, and Jian Tang. 2019. Gmnn:\nGraph markov neural networks. In International\nConference on Machine Learning , pages 5241\u20135250.\nMeng Qu, Huiyu Cai, and Jian Tang. 2021. Neural struc-\ntured prediction for inductive node classification. In\nInternational Conference on Learning Representa-\ntions .\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 .\nMichael Schlichtkrull. 2024. Generating media back-\nground checks for automated source critical reason-\ning. arXiv preprint arXiv:2409.00781 .\nJayaram Sethuraman. 1994. A constructive definition\nof dirichlet priors. Statistica sinica , pages 639\u2013650.\nTimo Spinde, Manuel Plank, Jan-David Krieger, Terry\nRuas, Bela Gipp, and Akiko Aizawa. 2022. Neural\nmedia bias detection using distant supervision with\nBABE\u2013bias annotations by experts. arXiv preprint\narXiv:2209.14557 .\nPeter Stefanov, Kareem Darwish, Atanas Atanasov, and\nPreslav Nakov. 2020. Predicting the topical stance\nand political leaning of media using tweets. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics , pages 527\u2013537,\nOnline. Association for Computational Linguistics.\nDahai Tang, Jiali Wang, Rong Chen, Lei Wang,\nWenyuan Yu, Jingren Zhou, and Kenli Li. 2024.\nXgnn: Boosting multi-gpu gnn training via global\ngnn memory store. Proceedings of the VLDB Endow-\nment , 17(5):1105\u20131118.\nFilip Trhlik and Pontus Stenetorp. 2024. Quantify-\ning generative media bias with a corpus of real-\nworld and generated news articles. arXiv preprint\narXiv:2406.10773 .\nPetar Veli \u02c7ckovi \u00b4c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li\u00f2, and Yoshua Bengio.\n2018. Graph attention networks. In International\nConference on Learning Representations .Soroush V osoughi, Deb Roy, and Sinan Aral. 2018.\nThe spread of true and false news online. science ,\n359(6380):1146\u20131151.\nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher\nFifty, Tao Yu, and Kilian Weinberger. 2019. Simpli-\nfying graph convolutional networks. In International\nConference on Machine Learning , pages 6861\u20136871.\nKai-Cheng Yang and Filippo Menczer. 2023. Large lan-\nguage models can rate news outlet credibility. arXiv\npreprint arXiv:2304.00228 .\nMingqi Yang, Renjian Wang, Yanming Shen, Heng Qi,\nand Baocai Yin. 2022. Breaking the expression bot-\ntleneck of graph neural networks. IEEE Transactions\non Knowledge and Data Engineering .\nNan Yin, Mengzhu Wang, Zhenghan Chen, Giulia\nDe Masi, Huan Xiong, and Bin Gu. 2024. Dynamic\nspiking graph neural networks. In Proceedings of\nthe AAAI Conference on Artificial Intelligence , vol-\nume 38, pages 16495\u201316503.\nRuihong Zeng, Jinyuan Fang, Siwei Liu, Zaiqiao Meng,\nand Shangsong Liang. 2024. Enhancing graph neural\nnetworks via memorized global information. ACM\nTransactions on the Web , 18(4):1\u201334.\nJiahao Zhang, Rui Xue, Wenqi Fan, Xin Xu, Qing Li,\nJian Pei, and Xiaorui Liu. 2024. Linear-time graph\nneural networks for scalable recommendations. In\nProceedings of the ACM on Web Conference 2024 ,\npages 3533\u20133544.\nJin Zhao, Jingxuan Tu, Han Du, and Nianwen Xue.\n2024. Media attitude detection via framing analysis\nwith events and their relations. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing , pages 17197\u201317210.\nAppendix\nA GNN Data & Task Statistics\nTable 8 describes the statistics of the graph data.\nThe factuality is given on a three-point scale: high,\nmixed, and low. Political bias is also on a three-\npoint scale: left, center, right. Panayotov et al.\n(2022) used the Alexa Rank6(down temporarily) to\ncreate a graph based on audience overlap, using 859\nmedia from ACL-2020 (Baly et al., 2020b) as seed\nnodes. Media sources that shared the same audi-\nence, as determined by Alexa, were connected with\nan edge, provided they met a specific score thresh-\nold. Alexa Rank was set to return a maximum of\nfive similar media sources for each medium; these\ncould be part of the initial seed nodes or newly iden-\ntified media. The primary graph constructed using\nACL-2020 dataset media as seed nodes is desig-\nnated as level-0 . In this graph, the nodes represent\nthe media sources that publish news or informa-\ntion, and the edges represent the audience overlap\nfor a pair of nodes. The procedure was repeated\nfive times, leading to the formation of five distinct\ngraph levels. With every subsequent iteration, the\ngraph expanded, encompassing media sources pre-\nviously identified by Alexa Rank. This iterative\nexpansion resulted in a progressive increase in both\nthe number of nodes and edges at each level.\nUpon analyzing the constructed graphs, we ob-\nserved several disconnected components, each sig-\nnifying a unique sub-network of nodes. Naturally,\nas the graph levels increased, the number of these\ncomponents decreased. This can be attributed to\nthe fact that an increase in nodes offers more op-\nportunities for components to merge. We opt for\ngraph level 3 to train GNNs, as detailed in Table\n8: it represents the most granular level publicly\naccessible with fewer disconnected components for\nboth factual and bias tasks. The Alexa Rank tool\nalso generated features for each node in the graph,\nwhich we treat as node attributes while training\nthe GNNs. These features include site rank, to-\ntal sites linked in, bounce rate, and the daily time\nusers spend on the site. These features are the nu-\nmeric values that are described and normalized in\nthe study (Panayotov et al., 2022). We refer to the\nGNN training tasks as Fact-2020 andBias-2020 for\nthe factuality and political bias tasks, respectively,\nsince both tasks are derived using ACL-2020. As\ngraph-based data becomes increasingly accessible,\n6http://www.alexa.com/siteinfoProperty Specification\nNodes 67,350\nEdges 200,481\nFeatures 5\nDiscon. comp. 44\nAvg. nodes / comp. 1,500\nlabeled Nodes 859 (1%)\nUnlabeled Nodes 66,492 (99%)\nTasks Fact-2020, Bias-2020\nFactuality task dist. high (162), mix (249), low (453)\nPolitical Bias task dist. left (243), center (272), right (349)\nTraining Split 687 (80% of 1%)\nTest Split 172 (20% of 1%)\nTable 8: Statistics about the level-3 graph constructed\nfrom ACL-2020 (Panayotov et al., 2022).\nwe focus exclusively on the graph and its inherent\nfeatures, promoting an approach tailored to such\nstructures. In contrast, (Panayotov et al., 2022) op-\nerates in a supervised setting and uses specialized\ntextual features (e.g., Articles, Wikipedia, Twitter,\nand YouTube) that are not publicly available. The\nproposed MGM addresses the unique challenges of\nthe media graph, offering solutions to the research\nquestions described in the designated section 4.1.\nTable 9 describes the statistics of the level-3\ngraph constructed from EMNLP -2018 (Panayotov\net al., 2022) media in the same way explained in\nSection 4.2. The EMNLP-2018 dataset comprises\n1,066 news outlets, rated on a 3-point scale for fac-\ntuality ( high, mixed, low ) and a 7-point scale for\npolitical bias ( extreme-left, left, center-left, center,\ncenter-right, right, and extreme-right ) (Baly et al.,\n2018). A subsequent analysis (Baly et al., 2020b)\nidentified that the labels center-left andcenter-right\nserve as vague intermediate categories, leading to\ntheir exclusion. Furthermore, to minimize subjec-\ntivity in the annotator decisions, the extreme-left\nandextreme-right categories were amalgamated\ninto the leftandright categories, respectively. This\nadjustment resulted in a simplified 3-point politi-\ncal bias scale ( left, center, right ) and reduced the\ndataset to 859 outlets as shown in Table 8, pub-\nlished in ACL-2020, which we consider as our\nmain dataset in section 4.2.\nB Baselines\nThis section summarizes the baseline GNN models\nthat we use as the backbone for our proposed MGM\nframework to enhance their learning capabilities in\nthe presence of sparsity challenges.\nGCN (Kipf and Welling, 2016): GCN simplifies\nthe convolution operation to alleviate the problem\nProperty Specification\nNodes 78429\nEdges 232530\nFeatures 5\nDiscon. comp. 88\nAvg. nodes / comp. 911\nlabeled Nodes 1066 (1.35%)\nUnlabeled Nodes 77363 (98.65%)\nTasks Fact-2018\nFactuality task dist. high (265), mixed (268), low (542)\nTraining Split 852 (80% of 1.35%)\nTest Split 214 (20% of 1.35%)\nTable 9: Statistics about the level-3 graph constructed\nfrom EMNLP-2018.\nProperty Specification\nTasks Fact-2020, Bias-2020\nFactuality task dist. high (295), mix (119), low (58)\nPolitical Bias task dist. left (152), center (181), right (139)\nTraining Split 387\nTest Split 85\nTable 10: Statistics about Articles andWikipedia col-\nlected from ACL-2020 (Panayotov et al., 2022) dataset.\nHyper-parameter BERT RoBERTa DistilBERT DeBERTaV3\nBatch size 80 100 120 80\nMax length 512 512 512 512\nEpochs 3 4 5 5\nLearning rate 2e-5 2e-5 2e-5 2e-5\nTable 11: Experimental setup for PLMs.\nof overfitting and introduces a renormalization trick\nto solve the vanishing gradient problem. We set the\nnumber of hidden neurons to 16, and the number\nof layers to 2.ReLU (Nair and Hinton, 2010) is\nused as the activation function. We do not dropout\nbetween GNN layers.\nSGC (Wu et al., 2019): SGC shows that the\ngraph convolution in GNNs is actually Laplacian\nsmoothing, which smooths the feature matrix so\nthat nearby nodes have similar hidden represen-\ntations. SGC removes the weight matrices and\nnon-linearity\u2019s between layers. In our experiments,\nwe set the number of hidden neurons to 256,the\nnumber of layers to 2, and the number of hops at\n2.We do not dropout between GNN layers.\nGraphSAGE (Hamilton et al., 2017): Graph-\nSAGE learns the embeddings of the nodes in the\nnetwork by sampling and aggregating features from\nthe local neighborhoods of the nodes. GraphSAGE\nhas different variants based on different feature ag-\ngregators, and we adopt GraphSAGE with a mean-\nbased aggregator as our baseline. In our experi-\nments, we set the number of hidden neurons at 64,\nand the number of layers to 2.ELU (Clevert et al.,2016) is used as the activation function. We do not\ndropout between GNN layers.\nGAT (Veli \u02c7ckovi \u00b4c et al., 2018): GAT incorporates\nthe attention mechanism into the propagation step,\nallowing each node to compute its hidden states by\nattending to its neighbors using self-attention and\nmulti-head attention strategies. we set the number\nof hidden neurons to 128per attention head and\nthe number of layers to 3.The number of heads for\neach layer is set to 4,4and6.ELU (Clevert et al.,\n2016) is used as the activation function. We do not\ndropout between GNN layers.\nDNA (Fey, 2019): DNA uses the jumping\nknowledge network to enhance the performance of\nGNNs. This approach enables selective and node-\nadaptive aggregation of neighboring embeddings,\neven when they have different localities within the\ngraph. We set the number of hidden neurons to 128,\nthe number of heads to 8, and the number of layers\nto 4. ReLU (Nair and Hinton, 2010) is used as an\nactivation function. We set the dropout rate to 0.5\nbetween GNN layers.\nFiLM (Brockschmidt, 2020): FiLM learns em-\nbeddings of nodes in the network by training a\nlinear message function that is conditioned on the\nfeatures of neighboring nodes. This allows FiLM\nto effectively capture and incorporate contextual\ninformation from neighbors into node embeddings.\nWe set the number of hidden neurons to 320and\nthe number of layers to 4.We set the dropout rate\nto0.1between GNN layers.\nFAGCN (Bo et al., 2021): FAGCN adopts a self-\ngating attention mechanism to learn the proportion\nof low-frequency and high-frequency signals. By\nadaptively modeling the frequency signals, FAGCN\nachieves enhanced expressive performance in cap-\nturing graph structure and features. We set the\nnumber of hidden neurons to 16,and the number\nof layers to 4.We set the dropout rate to 0.5be-\ntween GNN layers.\nGATv2Conv (Brody et al., 2021): GATv2 in-\ntroduces a dynamic graph attention variant that\nreorders internal operations, resulting in a signifi-\ncantly higher level of expressiveness compared to\nGAT. We set the number of hidden neurons to 64\nper attention head and the number of layers to 3.\nELU (Clevert et al., 2016) is used as an activation\nfunction. We do not dropout between GNN layers.\nC Experimental Settings\nAs mentioned in Section 3.3, MGM is trained using\nthe variational EM, which iteratively maximize the\nELBO and the expectation of log-likelihood func-\ntion through an E-step and an M-step. To optimize\nthe model, we use the Adam optimizer (Kingma\nand Ba, 2015) with a learning rate of 0.001.The\nearly stopping strategy is implemented with pa-\ntience in 10epochs. In each experiment, we train\nMGM for 50iterations to obtain the results. In\norder to encourage a sparse node selection distri-\nbution, we set the Dirichlet hyper-parameter \u03b1to\n0.1.The hyper-parameter K, which determines the\nnumber of global similar nodes, is selected from\nthe range [1,7]through a tuning process. Its value\nis optimized to achieve the best performance in the\nvalidation set for the node classification task. Simi-\nlarly, the trade-off hyper-parameter \u03b7, which strikes\na balance between the utilization of local repre-\nsentations and the information from global similar\nnodes is chosen from the range [0.6,1]and is tuned\nto obtain the optimal performance in the validation\nset for the node classification task. The model is\ntrained for 5epochs using different random seeds\nand mean \u00b1standard deviation is reported. We\nuse the GNN module implementations provided by\nPyTorch Geometric7(Fey and Lenssen, 2019).\nWe optimize hyper-parameters to achieve the\nbest performance on the validation set. In our exper-\niments, we randomly selected 70% of the dataset\nas the training set, 10% as the validation set, and\n20% as the test set. Due to the relatively small\nsize of the training set, we combined the training\nand validation sets to create a larger final training\nset. The trade-off hyperparameter eta manages the\nbalance between global and local information that\nthe model considers for the final prediction. Figure\n2 shows the Macro-F1 achieved by MGM with dif-\nferent GNNs on the test set at different values of K\n(number of global similar nodes) and eta (trade-off\nbetween local and global similar nodes).\nEvaluation Measures We evaluate our frame-\nworks using the mean of three key measures:\nMacro-F1, Accuracy, andAverage Recall. Macro-\nF1balance precision and recall for each class, ideal\nfor imbalanced datasets. Accuracy measures over-\nall correctness, while Average Recall highlights the\nmodel\u2019s sensitivity to different classes. For GNNs\n7https://github.com/pyg-team/pytorch_\ngeometric/tree/master/examplesexperiment, we used an Nvidia 2080 Ti GPU, and\nfor PLMs experiment, we used an NVIDIA A6000\n48GB GPU.\nD Collecting Articles & Wikipedia\nArticles. The article collection involves the follow-\ning steps: (i) We obtained media sources from the\nACL-2020 dataset. (ii) During the article link pars-\ning, we parsed front-page article links from these\nmedia sources based on the criteria of selecting\nonly internal links with more than 65 characters\nand excluding menu button links. (iii) In the article\ncollection stage, we use the selected article links to\nretrieve the titles and full text of the articles, using\nscripts and manual testing to ensure effective text\nextraction, with up to 30 news articles per media.\n(iv) Finally, the post-processing stage involved for-\nmatting the collected data in JSON format. In addi-\ntion, we specifically targeted sections that focused\non political, economic, and social issues sections.\nWikipedia. We started by searching for the name\nof the outlet on the Internet to find the Wikipedia\nlink. We ensure that the link leads to a Wikipedia\npage specifically about the media outlet. We then\nretrieved the text from the Wikipedia page using\nits consistent HTML format. Finally, the post-\nprocessing stage involved formatting the collected\ndata in the required JSON format.\nIn total, from 859 media sources, we have col-\nlected data from 472 media sources with Articles\nandWikipedia . Table 10 provides detailed statistics.\nMoreover, Figure 3 provides our detailed pipeline\nfor integrating MGM with PLMs.\nE MGM on Lower Memory Allocation\nWe conduct experiments on 60% and 80% and com-\npared with existing results. The results in Table 12\nshow that MGM performance produces compara-\nble results even in 80% of total nodes, but worse in\n60% due to fewer training samples.\nF MGM Scales to Larger Graphs\nWe primarily focus on addressing the challenges\nthat GNNs face with media graphs containing\ndisconnected components and insufficient labels.\nHowever, MGM also shows strong performance\nwhen applied to large datasets. We conducted a\nnode classification experiment using the Ogbn-mag\n(Hu et al., 2020) dataset, which includes 1,939,743\nnodes. The experimental results in Table 13 show\nStage 1: Language ModelsProb.Missing Media\nStage 4:Stage 3:\n&\nStage 2: Language ModelsProb.Predictions\n&\nProb.\n+\nProb.MGMGATv2 ,\nMGMFAGCN ,\nMGMFiLM+MGM\nProb\n++MGM\nProb+MGM\nProbZero\nLanguage ModelsLanguage ModelsLR\nLR\nLR\nLRPredictionsPredictions\nPredictionsFigure 3: The pipeline of integrating MGM with PLMs. Stage 1: We use logistic regression (meta-learner) to\nmake predictions on probabilities obtained from PLMs on 472 media sources. For the remaining media sources,\nwe assign [0.0,0.0,0.0]probabilities. Stage 2: We use the probabilities produced by PLMs and, for the missing\nones, we integrate the probabilities from the best GNN MGM GATv2 . The logistic regression is then used to make\nthe predictions. Stage 3: We concatenate the probabilities of the best PLM in Wikipedia andArticles and use\nlogistic regression to make predictions. Stage 4: We use the probabilities obtained from Stage 3, which involve\nconcatenating these probabilities with those generated by three GNNs (MGM FiLM, MGM FAGCN , and MGM GATv2 )\nacross five different run seeds. Subsequently, logistic regression is employed to make predictions, and the scores are\ncalculated using the standard deviation.\nGNN + MGM (Memory %) Fact Bias\nFAGCN + MGM (60%) 39.07 43.09\nFAGCN + MGM (80%) 41.31 46.02\nFAGCN + MGM (90%) 46.88 44.36\nFAGCN + MGM (100%) 48.77 45.02\nGatv2 + MGM (60%) 45.06 45.27\nGatv2 + MGM (80%) 49.77 52.44\nGatv2 + MGM (90%) 54.51 50.44\nGatv2 + MGM (100%) 54.13 52.41\nTable 12: Performance comparison of GNN + MGM at\ndifferent memory allocations.\nModel Ogbn-mag\nGraphSAGE 46.32 \u00b10.73\n+ MGM 47.94 \u00b10.65\nGAT 44.54 \u00b10.63\n+ MGM 46.28 \u00b10.25\nFiLM 41.72 \u00b10.22\n+ MGM 43.32 \u00b10.27\nGATv2 45.41 \u00b10.42\n+ MGM 46.74 \u00b10.36\nTable 13: Performance comparison on Ogbn-mag.\nthat existing GNNs augmented with MGM can\nachieve improved performance on large datasets.G Training Time of MGM\nWe compare the training times of MGM and a\nvanilla GNN, finding that while MGM requires\nslightly more training time, the increase is within\nan acceptable range. In particular, this marginal\nincrease in computational cost is justified by the\nsignificant improvement in Macro-F1 scores in Ta-\nble 14, demonstrating that MGM significantly im-\nproves model performance without imposing a con-\nsiderable training burden.\nModel Task Cost Time (m) Macro-F1\nGATv2 Fact 7.41 51.42\n+ MGM Fact 13.04 54.50\nFiLM Fact 4.73 43.32\n+ MGM Fact 7.76 49.68\nGATv2 Bias 4.50 48.48\n+ MGM Bias 7.97 52.41\nFiLM Bias 3.48 39.33\n+ MGM Bias 6.03 45.33\nTable 14: Training Times Comparison between Vanilla\nGNN and MGM on ACL-2020.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "MGM: Global Understanding of Audience Overlap Graphs for Predicting the Factuality and the Bias of News Media", "author": ["MA Manzoor", "R Zeng", "D Azizov", "P Nakov"], "pub_year": "2024", "venue": "arXiv preprint arXiv \u2026", "abstract": "In the current era of rapidly growing digital data, evaluating the political bias and factuality of  news outlets has become more important for seeking reliable information online. In this work"}, "filled": false, "gsrank": 28, "pub_url": "https://arxiv.org/abs/2412.10467", "author_id": ["ZvXClnUAAAAJ", "hCt0gK0AAAAJ", "", "DfXsKZ4AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:yNx8wb4zn2YJ:scholar.google.com/&output=cite&scirp=27&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D20%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=yNx8wb4zn2YJ&ei=CLWsaLmrCI6IieoP0sKRuAk&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:yNx8wb4zn2YJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2412.10467"}}, {"title": "DATA-DRIVEN NEWS OUTLET BIAS DETECTION WITH GDELT", "year": "NA", "pdf_data": "D ATA - D R I V E N N E W S O U T L E T\nB I A S D E T E C T I O N W I T H G D E LT\nPAT T E R N S A N D E X P L A N AT I O N S O F N E W S\nO U T L E T P O L I T I C A L B I A S\nR O N J A R O N N B A C K\nthesis submitted in partial fulfillment\nof the requirements for the degree of\nmaster of science in cognitive science &artificial intelligence\ndepartment of\ncognitive science &artificial intelligence\nschool of humanities and digital sciences\ntilburg university\nstudent number\n2017923\ncommittee\ndr. Chris Emmery and dr. Henry Brighton\ndr. Marie Postma\nlocation\nTilburg University\nSchool of Humanities and Digital Sciences\nDepartment of Cognitive Science &\nArtificial Intelligence\nTilburg, The Netherlands\ndate\nJune 12,2023\nacknowledgments\nSpecial thanks to my supervisors for their advice and insight during\nthis thesis, Zef for his well-aimed criticisms and support, Jakob for\nbeing a splendid rubber duck and Gerry for being a prime example of\ngreat writing strategy.\nword count\n8290\nD ATA - D R I V E N N E W S O U T L E T\nB I A S D E T E C T I O N W I T H G D E LT\nPAT T E R N S A N D E X P L A N AT I O N S O F N E W S O U T L E T\nP O L I T I C A L B I A S\nronja ronnback\nAbstract\nNews and media play a crucial role in informing public opinion, influ-\nencing politics, governance, and civic life alike. Although journalism\nstrives for unbiased reporting, various forms of bias may appear\nthroughout the writing and publication process. Previous studies in\nsocial and computational sciences have thus attempted to identify\nnews bias. Yet, these methods are often either laborious, require\nexpert knowledge and produce irregular results, or, while being more\nefficient, focus primarily on detection rather than deeper understand-\ning. Furthermore, previous research has also focused on sentence-\nor article-level bias, excluding important patterns that emerge from\nan outlet\u2019s behaviour as a whole. Thus, this thesis aimed to classify\noutlet bias on global data using machine learning, while accounting\nfor multiple forms of bias. The results demonstrated a somewhat\nsuccessful classification of political bias among news outlets, and\nincorporating features related to less-studied forms of bias improved\nmodel performance, yielding the best performing model with a 75%\naccuracy and AUC of 81(compared to a baseline of 45% and 50\nAUC). Furthermore, this approach includes Shapeley Additive Expla-\nnations (SHAP), which provides explanations and transparency into\nthe model\u2019s reasoning and indicates underlying processes of biased\nnews outlet behaviour. This presents a scalable and comprehensive\napproach to studying news political bias.\n1 introduction\nThe Joint Research Centre of the European Commission notes that \u201c[a]\nfunctioning democracy depends on the ability of its citizens to make\ninformed decisions\u201d (Lewandowsky et al., 2020 ). News and media play a\ncrucial role in informing public opinion, influencing politics, governance\nand civic life (Hamborg, Donnay, & Gipp, 2019 ; Kelsey, 2019 ). While\njournalism generally strives towards impartial reporting of events, this\n1\n1 introduction 2\nmay often not be the case in practice (Hamborg et al., 2019 ; Kelsey, 2019 ;\nNewman, Fletcher, Robertson, Eddy, & Nielsen, 2022 ).\nThe introduction of bias may be unintentional, but some news outlets\nmight systematically introduce bias in their reporting in order to influence\ntheir audience and propagate a world view that\u2019s skewed from neutral\nreporting (Watts, Rothschild, & Mobius, 2021 ). This has consequences\nfor political behaviour and personal decision-making: previous research\ndemonstrated the significant influence of news on a variety of topics such\nas public opinion on climate change (Chinn, Hart, & Soroka, 2020 ), the\nCOVID- 19pandemic (Hart, Chinn, & Soroka, 2020 ), migration crisis (Kosho,\n2016 ) as well as elections worldwide (Brandenburg & Van Egmond, 2012 ;\nKalsnes & Larsson, 2021 ; Van Spanje & De Vreese, 2014 ; Watts et al., 2021 ).\nTrust in news has decreased in many countries, according to Reuters\nInstitute\u2019s Digital News Report (Newman et al., 2022 ). Almost a third of\nthose surveyed state they avoid reading the news specifically due to its\nbias, an effect that was even more pronounced in countries with lower\noverall trust in news. Media trust was the lowest in the United States,\nwhere only 26% of those surveyed said they trusted the news (p. 15). This\nmistrustful stance presents a pivotal challenge for journalism. Identifying\nconsistently biased news outlets would allow for citizens to have better\ntransparency of the news they consume, increasing the trustworthiness of\nnews and thereby news consumption, which would hopefully contribute\nto better-informed societies and decision-making.\nRecent years have seen a sharp increase in concern and thereby also\nresearch about biased reporting, especially given the rise of misinformation\nand news that repeatedly misrepresent events for political goals (New-\nman et al., 2022 ; Watts et al., 2021 ). Particularly the social sciences have\nattempted to analyse and quantify media bias in various ways to glean\ninsights into how bias works or to detect its occurrence, with computa-\ntional sciences also increasingly joining the endeavour (Hamborg et al.,\n2019 ). Yet, these two approaches manifest some significant problems. In\nshort, social sciences, while able to deal with the ambiguity and complex\ncontext of news bias, is slow, laborious and often, ironically, subject to\nbias as well. Computational approaches enable fast, efficient, and uniform\nanalysis of news through Natural Language Processing (NLP) and Machine\nLearning (ML), yet are not as nuanced and fail to provide the same level of\ninsight as social science approaches, thus falling short of actually increasing\nunderstanding of the phenomenon (Hamborg et al., 2019 ).\nFor a better grasp of patterns of news outlet bias, an expansion of\nthese two approaches would be ideal. By adopting computational methods\ntogether with techniques for model explainability and feature importance\nestimation, it is possible to extract the approximate reasoning behind why\n1 introduction 3\nan outlet in question is deemed to be politically biased. Importantly, this\npartially circumvents the previously mentioned disadvantages of computa-\ntional approaches and supplements the analysis with some of the nuance\nthat social science approaches strive for.\nTherefore, this thesis presents an approach using automatic global\ncoverage of news with supporting data-driven analyses to provide expla-\nnations and transparency of news outlet bias comparable to that of social\nscience approaches. The existing dataset Global Database of Events, Lan-\nguage, and Tone (GDELT) tracks and analyses global news and is thus an\nideal source for this task (Leetaru & Schrodt, 2013 ). The aim is to auto-\nmatically detect bias of outlets specifically by training machine and deep\nlearning models on GDELT news data. Focusing on outlets rather than\narticle-, sentence- or word-level analysis, as is the more common approach,\nwould account for broader patterns of bias, as previous analyses are often\nlimited to subsets of articles or only a small set of outlets, as is further\ndescribed in the next section. Moreover, this thesis differentiates between\nmultiple types of labels and bias to review their impact, whereas much of\ncurrent research has limited itself to only studying one sub-type of bias\nwhile neglecting other manifestations thereof. Certain issues are thus of\ncentral interest:\n\u2022RQ1: To what extent can the political leaning of news outlets (as\nlabelled by existing datasets) be predicted from GDELT?\n\u2013A. What is model performance based on labels by human anno-\ntators (Media Bias Fact Check, 2021 b)?\n\u2013B. What is model performance based on computationally deter-\nmined labels (Robertson et al., 2018 )?\n\u2022RQ2: How do the different types of bias impact model performance?\n\u2022RQ3: Which features are informative for explaining a particular\nprediction and gaining insight into news outlet bias?\nThe main findings show that political bias of outlets can to some degree\nbe classified using GDELT data (maximum accuracy of 75% and an AUC\nscore of 81compared to a baseline of 45% and AUC of 50), and that human\nlabels and inclusion of multiple forms of bias improve performance: con-\nsidering models with the same architecture as the best performing model,\naccuracy was 4-12% lower when trained without multiple forms of bias,\nand on average 13% lower when trained on computationally determined\nlabels instead of human-made ones. Many topics classically included in\nprevious studies reappear as informative features, but some previously\nunconsidered themes are also relevant for explaining model predictions.\n2 related work 4\n2 related work\n2.1A Taxonomy of Bias: Bias Sub-Types\nThere are a myriad of different ways that bias can manifest itself in news\nmedia. Journalists may, consciously or unconsciously, insert personal\nprejudgements into their articles, be this at the stage of selecting what\nevent to cover, which sources to trust or whether to, as the classic example\ngoes, refer to \"freedom fighters\" as opposed to \"terrorists\" (Hamborg et\nal.,2019 ; Kelsey, 2019 ). News outlets are inclined to reshape news in their\nown way and according to their own political or ideological views. This\nmay for instance be through deciding whether an article shall be published,\nwhere it should be placed, or how much space to give it. Furthermore,\nnumerous external factors are also likely to pressure outlets to maintain\nsome forms of biased tweaking of their reporting. The risk of readers\ndrifting to other outlets if their personal perspectives are at odds with\nthat of the outlet creates an incentive to adapt publications to the current\nreaders\u2019 perspectives (Hamborg et al., 2019 ). Furthermore, outlets must\noften also reckon with advertisers, owners or governments, be that for\npurposes of funding or maintaining good relations. Financial factors will\nhold sway in numerous ways as well, as will contextual factors such as\ngeographic vicinity of an event, country of publication, local societal views\nand judgements of what is relevant (as a benign example, the Eurovision\nSong Contest might be more news-worthy and widely reported on in\nparticipant countries).\nAs a consequence, however, bias, in all its forms, is difficult to detect or\nanalyse due to this scattered nature. Because of this, many studies have\nfocused on defining and analysing specific types of bias. What follows is a\nbrief and somewhat simplified overview of a subset of biases identified in\nHamborg et al.\u2019s literature review.\n\u2022Event selection bias or coverage bias involves choosing which events\nmerit report. Naturally, not all stories can nor should be published.\nYet, intentional and consistent avoidance of a topic may be a method\nfor influencing or misleading audiences. Similarly, over-reporting on\nsome topics while neglecting others may drastically shift public be-\nliefs. This is a well-studied phenomenon in crime reporting. Outlets\nwill tend to for example over-report certain types of crimes (Ditton &\nDuffy, 1983 ), and crimes committed by minority races are often over-\nrepresented compared to actual occurrence rates (Gilliam Jr, Iyengar,\nSimon, & Wright, 1996 ; Paybarah, 2015 ), while crime by majorities\ntend to be under-reported.\n2 related work 5\n\u2022Labeling andword choice bias are a major focus of study. This\nconcerns framing events or highlighting a certain perspective by\nchoosing labels or particular words that, while similar, will convey\ndifferent meanings to audiences: for example, labeling a group as\n\"refugees\", \"immigrants\", \"economic migrants\" or \"illegal immigrants\"\nmay change audiences\u2019 impressions of them. Alternatively, referring\nto something as a \"special military operation\" or \"intervention\" in-\nstead of \"invasion\" may change perceptions of events.\n\u2022Size allocation bias concerns the length of articles. The amount of\ntext written on some topics may manifest certain outlet biases. For\nexample, it is possible that news outlets report consistently but only\nvery briefly on certain topics while dedicating a lot of work and space\nto others. This is a relatively easy form of bias to study but has not\nreceived much attention (Hamborg et al., 2019 ).\n\u2022Picture selection andexplanation bias concern what pictures are\nchosen to accompany certain articles, and how those pictures are\ndescribed. Images have been shown to affect readers\u2019 perceptions of\nnews articles (Madrigal & Soroka, 2023 ; Soroka, Loewen, Fournier, &\nRubenson, 2016 ), so selecting and describing them is susceptible to\npotential biases. As an illustrative example, reporting on a protest by\nusing images of protesters being arrested versus peacefully congre-\ngating will produce different impressions, though both may easily\nhave occurred at the protest in question.\nThese various subtypes of bias have, to different extents, been examined in\nprevious literature. Many methods to study these patterns have been tested,\nand the following sections describe some notable past research. Neverthe-\nless, it is rare, especially in computer science, that a study encompasses\nmore than one form of bias. Social science has lent itself better to studying\ncombinations of biases, but has several drawbacks and disadvantages.\n2.2Social Studies\nThe methods adopted by social studies do not easily lend themselves to\nlarge-scale investigations of media bias. Most require a lot of manual work\nto, for example, create dictionaries, read and annotate texts by hand, or\nperform human (either crowd-sourced or expert) article reviews (Hamborg\net al., 2019 ).\nSuch approaches require significant effort, expert knowledge and are\noften criticised as being subjective and highly variable (Spinde, Hamborg,\n& Gipp, 2021 ; Spinde, Krieger, et al., 2022 ), which limits the possible scope\n2 related work 6\nof what can be studied and has \u201csignificantly hampered progress in the\nfield\u201d (Hamborg et al., 2019 ). A possible way to address these shortcomings\nis with computational methods. Numerous algorithmic or semi-automated\napproaches exist which could be applied to estimate news bias, as noted by\nHamborg et al.. While there are some studies that have used computational\nor Natural Language Processing tools such as Term Frequency-Inverse\nDocument Frequency or doc 2vec, many have had somewhat disappointing\nresults (Baraniak & Sydow, 2018 ; Spinde et al., 2021 ) and frequently rely on\ntime-consuming resources such as dictionaries of biased words or human\nannotation (Baraniak & Sydow, 2018 ; Gangula, Duggenpudi, & Mamidi,\n2019 ; Spinde et al., 2021 ).\nThe paper by Hamborg et al. notes that \u201c[t]hus far, few studies use word\nembeddings and deep learning to analyse media bias in news coverage.\nHowever, the techniques have proven very successful in various related\nproblems, which lets us anticipate that the majority of the textual bias\nforms could be addressed effectively with such approaches\u201d (p. 411).\n2.3Computational Studies\nPrevious studies have proposed that computational approaches may present\nsome promising solutions to some of the drawbacks afflicting social science\nstudies. Of particular interest, some previous projects have attempted to\nquantify media bias using Machine- or Deep Learning methods.\nFor instance, Gangula et al. aimed to predict news bias towards five\nlocal political parties based on headlines, articles and a combination of the\ntwo, as compared to human annotations. They achieved an accuracy of\n89.5% with an attention-based model. A central drawback of this research\nwas, however, the limited insight, narrow focus and reliance on very specific\nhuman annotations, which limits the project\u2019s ability to scale to a larger\ncontext and provide understanding of political bias on the whole.\nSpinde, Krieger, et al. avoids the reliance on self-made human anno-\ntations by using existing datasets concerning bias, for instance Reddit\ncomments, movie reviews, Wikipedia, and lastly, two general language\ndatasets. These were combined to train a DistilBERT model (Sanh, Debut,\nChaumond, & Wolf, 2019 ) using Multitask Learning. While the results\nwere promising, with an F 1-score of 0.77, they predicted bias on a sentence-\nby-sentence basis based only on data that is indirectly related to news\nbias. This is only partially transferable to news, as they themselves note.\nAdditionally, this approach focuses only on word choice, while there are\nother types of bias that might also be informative (Hamborg et al., 2019 ).\nA follow-up study addressed the issue of media-specific datasets, com-\npiling a dataset of 3,700sentence-level annotations by experts on a broad\n2 related work 7\nrange of topics instead of the usual crowd-sourced annotators. This was\nthen used to train BERT-based models (Devlin, Chang, Lee, & Toutanova,\n2018 ) to detect bias in sentences, which achieved a maximum F 1-score of\n0.8(Spinde, Plank, et al., 2022 ). This presents an improvement, though still\nrelying on extensive manual annotation and limiting the focus to word-\nand sentence-level analysis of bias.\nWhile the previous studies rely on ML, other computational methods\nhave also been explored. Previous studies have employed data-driven\ntechniques for estimating website or news political bias. Using the Twitter\naccounts of users who were registered as either Republican or Democrat\nvoters, Le, Shafiq, and Srinivasan approximated bias based on how often\nusers shared articles from outlets or websites. Articles shared often by\nRepublicans would thus be assumed to stem from news outlets with a\nright-wing political leaning, and vice versa. Given this method, Robertson\net al. could assign and validate political leaning scores to over twenty\nthousand sites. This offers a scalable approach for determining news\noutlet bias, and supplies follow-up studies with well-validated political\nbias scores. However, it does not directly provide much insight into what\nmakes a particular outlet more biased towards a political audience, other\nthan the fact that it is predominantly distributed by that audience online.\nThese studies demonstrate the potential of applying Machine Learning\nor other computational methods to predict media bias, circumventing many\nof the issues arising from social science or semi-automated approaches.\nNevertheless, there are some gaps. Crucially, the focus on only sentence-\nlevel bias excludes important patterns that emerge from a news outlet\u2019s\nbehaviour as a whole, such as coverage, story placement and size pat-\nterns (are some topics avoided, only given limited space or only reported\nupon very briefly). These could play a significant and informative role in\ndemonstrating bias in news outlets. Additionally, most of these studies rely\non self-made datasets that rely on expert or crowd-sourced annotations\n(Gangula et al., 2019 ; Spinde, Plank, et al., 2022 ), or on existing datasets\nthat are not specifically concerned with media bias, but rather biased text\nmore generally (Spinde, Krieger, et al., 2022 ).\nIn order to identify media bias of news outlets in a scalable manner,\nit is not sufficient to rely on specific topics, small-scale datasets or only\non sentence- or article-level classifications. Therefore, this project aims to\ndevelop a system that relies on a broader data source which concerns news\nand media specifically, facilitating global scale coverage, and is capable of\nexamining multiple facets of media bias more thoroughly. Additionally,\nthis approach is entirely data driven and reliant on automated techniques,\nrather than time-consuming manual approaches. To achieve this, a combi-\nnation of big data and machine learning could be leveraged.\n3 method 8\n3 method\nIn the following sections, the selected datasets are described with their\nunique preprocessing steps. Some general preprocessing used to modify\nmultiple datasets is described in Section 3.3. Finally, the models and\nassociated experiments are detailed in Sections 3.4and3.6.\n3.1Global Database of Events, Language, and Tone (GDELT)\nThe Global Database of Events, Language, and Tone (GDELT) (Leetaru &\nSchrodt, 2013 ) is an open platform for monitoring global news, translated\nfrom 65languages and updated every 15minutes. It offers many datasets\nfor different purposes, but the one of interest here is the Global Knowledge\nGraph (GKG), which records \u201clatent dimensions, geography and network\nstructure of the global news\u201d (Leetaru & Schrodt, 2013 , p.1). The dataset\ndetects themes discussed in the articles (these themes are extensive, cov-\nering topics ranging from immigration to gasoline prices or even specific\ncurrencies or mammals) and performs various analyses.\nGiven the breadth of the dataset, some constraints were set. The analysis\ncovered articles from the year 2022 written in English. The latter constraint\nexcludes articles that have been translated from other languages, focusing\nthe analysis on mostly Western countries which approximately follow the\nbipartisan political spectrum. Only the following features were included\nfor their relevance and association with aspects of language use, namely:\n\u2022 the positive and negative tone of the article as a whole.\n\u2022 the polarity, which refers to the proportion of words that matched a\ntonal dictionary to indicate how polarized the text is. For example, a\nhigh polarity but similar scores for positive and negative tones indi-\ncates that the article contains roughly the same amount of positively\nand negatively charged words.\n\u2022the activity reference density, which is a percentage score of active\nwords in the article and is supposed to act as a proxy for \"activeness\"\nin the text as compared to merely descriptive text.\n\u2022the self or group reference density is the percentage of pronouns\npresent. The GDELT documentation states that this \"can be used\nto distinguish certain classes of news media and certain contexts\"\n(GDELT Project, 2015 , p.10).\n\u2022 the word count of the article.\n3 method 9\n{\nurl: nos.nl/\u2026, \ntheme: EDUCATION; \nEPU_ECONOMY;JOBS; \nIMMIGRATION\u2026, \nPosTone: 2.51,\nNegTone: -1.78 ,\nWordCount: \u2026,\n\u2026\n} Outlet Theme1_PosTone Theme1_NegTone . . .ThemeX_WordCount Lean \n CNN 3.45 -1.5 . . .250 Left\n AP News 1.11 2.92 . . .409 Least Biased \n Christian \n Daily -6.42 3.31 . . .1065 Right \n NOS 2.67 -6.02 . . .2861 Right Center \n . . . . . . . . . . . .. . . . . .AGGREGATE GDELT Entry \nFigure 1: Overview of process to aggregate GDELT data from article- to outlet-\nlevel instances, containing themes and their respective average GDELT features.\n\u2022the presence of cover images, embedded social media images or\nvideos related to the article. This was noted merely as either \"present\"\nor \"absent\" for the purposes of this thesis.\n\u2022 and the themes detected in the article.\nFor the year 2022 , more than 30thousand unique themes were found in\nthe dataset. These themes include items from taxonomies such as the\nWorld Bank or even animal taxonomies, detailing specific birds, mammals,\nor fish. Given the relative triviality of many of these themes and their\noverlap with GDELT\u2019s own pre-defined set of themes, all themes stemming\nfrom such taxonomies were discarded. As this was still a considerably\nlarge and at times sparse set, themes appearing less than 1000 times and\nbeyond the first standard deviation from the mean of the log transform\nof theme frequency were also excluded. This yielded a final set of 519\nthemes to be considered. This method maintains a data-driven approach\nand avoids manual selection. Furthermore, not constraining the set to\nknown politically controversial themes, such as immigration, abortion, or\nclimate change (Media Bias Fact Check, 2021 b; Spinde, Plank, et al., 2022 ),\ncould allow for insight into previously under-explored indicators of bias.\nPreprocessing. Thus, GDELT articles containing these selected themes\nwere aggregated per outlet. Each outlet\u2019s articles were grouped into themes,\nsuch that each GDELT feature listed above was assigned an average value\nfor that particular theme. Additionally, to account for other forms of bias\nbeyond that found in tone or sentiment analyses, the proportion of articles\nmentioning each theme was also added as a feature, as this might allow\nfor examination of selection bias. Similarly, the average word count per\narticle might reveal size bias, and the presence of images or videos could\nalso be indicative of another form of bias besides word choice bias.\nThis preprocessing thus yields one row per outlet, with a GDELT\nfeature value for each theme, for all themes, which is a total of 6,748\nfeatures. Figure 1provides an overview of the process.\n3 method 10\n3.2Media Bias Fact Check (MBFC)\nMedia Bias Fact Check is an independent website that estimates media bias\nbased on human evaluations (Media Bias Fact Check, 2021 a). Their main\naim is to promote awareness of media bias and misinformation. For each\nnews outlet, they provide a political lean label but also factuality, traffic,\npress freedom, media type, and credibility (these are described in detail\nin Appendix A (page 36). While there are many sites and organisations\nthat perform similar ratings of political media bias, MBFC\u2019s methodology\nis thoroughly documented on their website (Media Bias Fact Check, 2023 ),\nand their dataset contains an extensive set of news outlets spread across\nthe political spectrum (Figure 2b). They do, however, note that their\nevaluations are largely based on American politics, and may thus not\nperfectly generalise to all countries, and that a majority of the outlets they\ncover are of American origin. The human evaluations of political lean\nmade by MBFC serve as a ground truth value (particularly for RQ 1A).\nPreprocessing. To obtain the data, the MBFC website was scraped, collect-\ning a total of 3,981rated outlets. In order to appropriately join the data\nto the GDELT dataset, the URLs were cleaned to match those of GDELT\n(remove \"https://\" tags, for example). Outlets with missing data, incorrect\nformatting or duplicates were excluded, which reduced the dataset to\n3,726entries. The country names were swapped for coordinates using the\nNominatim library (Clemens, 2015 ). Additionally, while MBFC also rates\nsome outlets as being either extreme left or right leaning, these classes\nwere merged with the left and right leaning categories due to the very\nlow number of instances in these classes. Furthermore, each categorical\nfeature (political lean, factuality, credibility, press freedom rating, traffic,\nand media type) were label encoded.\nWhen combined with the GDELT dataset, it produced a total of 1,641\noverlapping outlets. As quite a few of these outlets are not particularly ac-\ntive, any outlet with less than one hundred published articles was excluded,\nyielding 1,207outlets to train, validate and test on.\n3.2.1Robertson\u2019s Bias Scores\nAs described in Section 2, Robertson et al. built a dataset of bias ratings for\nnearly twenty thousand websites. These ratings range from - 1to1, wherein\na score of - 1indicates that the source was shared exclusively by Democrats,\nshowcasing a left-learning bias, and a score of 1indicates it was shared\nexclusively by Republicans, implying a right-lean bias. As these ratings\nare automatically collected and do not rely on human annotations, they\nserve as a point of comparison between labels for research question 1B.\n3 method 11\n(a) Frequency of political lean classes\nper dataset, on a logarithmic scale.\n(b) Proportion of political lean\nclasses per dataset.\nFigure 2: Frequency and distribution of bias ground truth labels. In the frequency\nplot, there\u2019s a slight imbalance between classes, and there are, for example, slightly\nmore ground truth labels for left and left-leaning outlets compared to right and\nright-leaning ones. The most common label in all datasets is the \"least biased\"\nlabel, reflecting no particular political leaning.\nPreprocessing. As this score is continuous, the data was preprocessed for\nsome of the experiments by binning values into the five bias classes. When\ncombined with GDELT, the total number of outlets with a ground truth\nfor Robertson\u2019s bias-rated websites was 3,709. Outlets that had published\nfewer than one hundred articles during 2022 were dropped, yielding a final\ntotal of 2,073outlets.\n3.3General Preprocessing\nThe data was split into training, testing, and validation sets, where the\nvalidation and test sets made up 15% of the total dataset size each. The\nsets were stratified based on political leanings. It was then further scaled\nusing the min-max scaling method of Scikit-Learn (Pedregosa et al., 2011 ).\nSince the number of features was quite large ( 6,748), raising the issue of\nthe curse of dimensionality, some preprocessing was done to eliminate\nredundant features, increasing performance and classification speed (Dhal\n& Azad, 2022 ). Columns that had a high correlation score (above 0.95) were\nremoved. Additionally, recursive feature elimination (RFE) was performed\nusing a random forest classifier with 10cross-validations to find a subset\nof informative features (Guyon, Weston, Barnhill, & Vapnik, 2002 ).\n3 method 12\n3.4Models\nVarious classic machine learning models were trained to classify outlet bias\nbased on the preprocessed GDELT features, using either MBFC\u2019s or Robert-\nson et al.\u2019s data as a ground truth. These were: Support Vector (SVC, (Boser,\nGuyon, & Vapnik, 1992 )), Decision Tree (Hunt, Marin, & Stone, 1966 ), Ran-\ndom Forest (Breiman, 2001 ), AdaBoost (Freund & Schapire, 1997 ; Li, Wang,\n& Sung, 2008 ), XGBoost (Chen & Guestrin, 2016 ) and Bagging (Breiman,\n1996 ; B\u00fchlmann & Yu, 2002 ) classifiers. Additionally, a neural network\nwas trained using Pytorch (Imambi, Prakash, & Kanagachidambaresan,\n2021 ). Each model was trained with 10cross-validation runs using halving\ngrid search (a faster variant of grid search that iteratively selects parameter\ncombinations) to find optimal parameters for each model (Scikit Learn,\n2023 b). The parameters used in the grid search are detailed in Appendix\nB (page 37). The Scikit-Learn library (Pedregosa et al., 2011 ) was used\nfor training the models, except the XGBoost model, which relied on the\nxgboost python library (Chen & Guestrin, 2016 ), and the neural network,\nwhich used PyTorch (Imambi et al., 2021 ).\n3.4.1Baseline Model\nServing as a point of comparison for other models, a baseline model was\nimplemented such that it invariably classifies all instances as the most\ncommon class. For both MBFC and Robertson et al.\u2019s data, this was the\n\"least biased\" class.\n3.4.2Neural Network\nMany architectures were tested, and a two-linear-layered network was\nfound to perform the best for all experiments, with ReLU activation\n(Fukushima, 1969 ), batch normalization and a dropout-rate of 0.5between\neach layer. The network used an Adam optimizer (Kingma & Ba, 2014 ) and\nnegative log likelihood loss.\nAdditionally, as one of the experiments involved categorical variables\nfrom MBFC, a version of the network was made that embedded these\nvariables before passing them to the network. Using embeddings rather\nthan one-hot or label-encoded variables allows a model to learn a more\nrepresentative depiction of the variables, which was important in this case\nsince some of the features were ordinal (Brownlee, 2019 ; Guo & Berkhahn,\n2016 ). This ordinal relationship was thus preserved. The complete structure\nof the networks can be found in Appendix C (page 39).\n3 method 13\n3.5Model evaluation\nTo evaluate the models, the accuracy and area-under-the-curve (AUC)\nscores were computed. AUC scores are particularly useful here thanks\nto their suitability to problems with imbalanced classes (M\u00fcller & Guido,\n2017 ). The performance was also visualised in confusion matrices.\n3.5.1Shapeley Additive Explanations (SHAP)\nOne central criticism by Hamborg et al. of computational analyses of media\nbias has been the lack of insight into how bias is manifested rather than\nsimply determining presence, as has been the focus of many previous stud-\nies (Gangula et al., 2019 ; Spinde et al., 2021 ; Spinde, Krieger, et al., 2022 ;\nSpinde, Plank, et al., 2022 ). Thus, it was crucial to apply computational\nmethods that provide explanations of model decisions. A recently devel-\noped method for this is Shapley Additive Explanations (SHAP), which\nexpands upon six pre-existing methods (Lundberg & Lee, 2017 ). The SHAP\nframework presents explanations for model predictions in a model ag-\nnostic fashion, meaning it can be applied to black-box models, which are\ntraditionally difficult to interpret.\nSHAP estimates the expected contribution of each feature to a specific\nprediction by averaging the differences in the model\u2019s output with and\nwithout that feature, given feature permutations. The resulting set of\ndifferences is then used to approximate the Shapley values for each feature,\nrepresenting the contribution of that feature to a prediction.\nThus, SHAP will allow for thorough outlet-specific explanations rather\nthan simply model-level insight, as is the case for other traditional explain-\nability frameworks, such as feature permutation importance (Scikit Learn,\n2023 a). This will thus help address research question 3concerning expla-\nnations of predictions to gain insight into model reasoning and underlying\nstructures.\n3.6Experiments\nTwo main types of experiments were implemented using either MBFC or\nRobertson et al. scores as ground truth labels to address research questions\n1A and 1B respectively. For each ground truth, three experiments were\nconducted to examine research question 2, which concerns the impact\nof different types of bias on model performance. The first experiment\n(referred to as the \"traditional bias experiment\") trained models on data\nrelated to word bias, meaning it covered features related to tone, polarity,\nactivity- and self/group reference density. The second experiment (called\n3 method 14\nthe \"alternative bias experiment\") used word-, article-counts and image or\nvideo presence, aiming to better glean the significance of lesser-studied\nforms of bias, namely size, selection and picture bias respectively (the\ncontent of the images is not accounted for, thus this feature only approxi-\nmates picture bias). The last experiment used all features and is referred\nto as the \"full bias experiment\". This structure aimed to allow for better\nexamination of the information value of the different forms of bias and\nto extend analysis beyond word bias, which is the traditional focus. An\noverview of the experiments is displayed in Figure 3.\nOutlet Theme1_Tone Theme1_Article \nCount \u2026ThemeX_Tone ThemeX_Article \nCount \nNOS 3.7 0.81 .. -2.4 0.54 \nCNN 0.9 0.33 .. 1.7 0.09 \nFOX 4.2 0.05 .. 0.67 0.42 Traditional Bias Dataset Alternative Bias Dataset \nMBFC Categorical \nCredibility \u2026Traffic \nMedium \u2026 High \nHigh \u2026 Medium \nLow \u2026 Low Full Bias Dataset Full Bias  & Categorical Dataset Experiment Pipeline \nFigure 3: Overview of experiments used to test models. To test the impact of\ndifferent bias-related data, models were trained on subsets of the data: traditional\nbias data, which includes features related to tone, polarity, activity and self/group\nreference density; alternative bias data, which includes features of word-, article-\ncounts, image- or video presence; and the combination of all these features: full\nbias data. Additionally, a further experiment tested model performance on the\nfull dataset when supplemented with categorical features from the MBFC data.\nSee Section 3.6for full details.\nFurthermore, one last experiment (referred to as the full bias & cat-\negorical features experiment) was conducted adding various categorical\nfeatures provided by MBFC such as credibility, factuality ratings, traffic\nestimates, country press freedom index, and media type (see Appendix\nA for full list). These features were expected to be informative and thus\nconducive to improved performance.1\n1Note that this experiment was only done for the full bias dataset using MBFC as the ground\ntruth.\n4 results 15\n3.6.1MBFC Experiments\nThe first set of analyses used MBFC as the ground truth for the prepro-\ncessed data. The training set consisted of 552outlets, while the validation\nand testing sets contained 214outlets each. The traditional bias experi-\nment consisted of a subset of 174traditional bias features selected through\nRFE (see 3.3). The alternative bias experiment consisted of a subset of\n82features, and the full bias experiment used a set of 357features, also\nautomatically chosen through RFE.\n3.6.2Robertson Experiments\nThese experiments employed Robertson et al.\u2019s bias rating scores as ground\ntruth on the dataset. The analyses mirrored those of the MBFC experiments,\nthe first accounting for 654features related to traditional bias, the second\nfor128features concerning other forms of bias and a total of 184features\nfor the full dataset.\n4 results\nThe performance of all models for each experiment is detailed in table 1.\nFor experiments with models trained on traditional bias data, the neural\nnetwork with ground truth values from MBFC performed the best with\n68.9% accuracy, and an AUC score of 73. The confusion matrix of the\nmodel is represented in Figure 4.a.\nOn the alternative bias features, the neural network with MBFC labels\nonce again performed the best, though slightly worse than the previous\nmodel, with an overall accuracy of 63.1% and AUC of 71. Figure 4.b shows\nthe confusion matrix of the model.\nThe AdaBoost model achieved the best performance on the full bias\nexperiment (see Figure 4.c), which combined bias datasets containing both\nword bias as well as the alternative bias features related to size, coverage,\nand picture bias. It achieved an accuracy of 72.3% and 77AUC, and used\nan ensemble of fifty random forest classifiers with balanced class weights,\na maximum depth of five nodes, and a learning rate of two.\nHowever, the generally best performing model was the neural network\ntrained on the full dataset supplemented with the categorical features of\nMBFC (see section 3.2and Appendix A for the full list and descriptions).\nIt classified outlets with an accuracy of 75%, and an AUC score of 81,\ncompared to the baseline which achieved 45% and an AUC of 50. The\nAdaBoost and SVC achieved comparable accuracy but slightly lower AUC\nscores. The model\u2019s confusion matrix is visible in Figure 4.d.\n4 results 16Table 1: Models classifying political lean of media outlets on various experiment conditions: traditional bias features (TB Dataset; tone,\npolarity, activity- and self/group reference density), alternative bias features (AB Dataset; word- and article count, image- or video\npresence) and full bias features (FB Dataset). The last column to the right presents the performance of a model trained on the full data,\nsupplemented with categorical features derived from MBFC\u2019s dataset (FB & C Datset). Evaluation metrics are accuracy and AUC-score,\nand the best-performing model is highlighted in bold for each experiment.\nDataset Model TB Dataset AB Dataset FB Dataset FB & C Dataset\nAccuracy AUC Accuracy AUC Accuracy AUC Accuracy AUC\nMBFCBaseline 45.4 50 45 .4 50 45 .4 50 45 .4 50\nSVC 47.1 51 52 .1 57 68 .1 71 75 .0 79\nDecision Tree 54.6 60 52 .9 66 61 .3 66 61 .7 72\nRandom Forest 61.3 63 62 .2 69 64 .7 68 64 .1 70\nAdaBoost 68.1 73 57 .9 68 72.3 77 75.0 78\nXGBoost 62.2 70 54 .6 64 65 .5 72 64 .8 69\nBagging 64.7 66 61 .3 69 56 .3 60 71 .2 76\nNeural Network 68.9 73 63 .0 71 66.4 75 75.0 81\nRobertsonBaseline 40.8 50 40 .8 50 40 .8 50 - -\nSVC 53.7 63 53 .7 65 56 .9 0.74 - -\nDecision Tree 43.7 54 36 .3 67 34 .1 59 - -\nRandom Forest 45.0 53 54 .9 63 53 .7 61 - -\nAdaBoost 55.6 0 .65 59 .2 66 54 .7 70 - -\nXGBoost 44.4 58 53 .1 64 57.9 66 - -\nBagging 50.5 60 57 .9 67 55 .3 69 - -\nNeural Network 47.9 62 58 .2 0.70 52.7 68 - -\n4 results 17\nFigure 4: Confusion matrices of the predictions by the best performing models\nper task.\nIn general, models trained on MBFC as ground truth outperformed models\ntrained on Robertson et al.\u2019s bias ratings, which achieved only a maximum\naccuracy of 58.2% and an AUC of 70with the neural network trained on\nthe alternative bias dataset.\n4.1Impact of Bias Type\nResults suggest that the inclusion of alternative forms of bias improves\nperformance, as most models combining both traditional and alternative\nforms of bias in the full bias dataset perform better than models trained\nonly on word-bias related features. This is the case for both MBFC and\nRobertson et al. models and demonstrates that the exclusion of more\ndiverse types of bias might be limiting the field of automatic bias detection,\n4 results 18\nas these features are informative despite often being neglected in analysis.\nThis mirrors what was discussed in the introduction.\nInterestingly, the Robertson et al. models trained only on the alternative\nbias dataset tend to outperform their counterparts trained on either only\ntraditional or full bias datasets. This indicates that the alternative bias\nfeatures were most informative for predicting the political leaning of news\noutlets. The inclusion of word-bias related features even slightly decreased\nperformance. This trend, however, was not fully present in the overall\nbetter performing MBFC models (though AUC scores are, on average, very\nslightly higher for the alternative bias datasets compared to the traditional\nones), indicating that this might be due to differences in the composition\nof the ground truth datasets.\n4.2SHAP model explanations\nSHAP decision plots allow for examining which features were informative\nfor explaining predictions. Since such plots can be made for any outlet,\nsome representative examples of outlets of varying political leans were\nselected for visualisation (Figure 6).2\nGenerally, it seems like geographical location (latitude and longitude)\ntends to be quite informative for the model. The other categorical features\nfrom MBFC also often appear in the top most important features, with the\nexception of Press Freedom Index (Figure 6). Interestingly, the model does\nseem to have picked up on some themes that previous research also focused\non due to their polarising nature, such as inequality, environmental issues,\nelection fraud, firearm ownership, and social movements. Nevertheless,\nthemes relating to natural disasters are also more frequently relevant for\nthe model compared to their relevance in previous literature, where such\ntopics have mostly not been considered.\nMoreover, article count features appear frequently in the top most\ninformative features. This is interesting in relation to research question\ntwo (how do different types of bias impact model performance). It further\nvalidates the results in Section 4.1, which also suggested that alternative\nbias features such as article counts could be informative about news outlet\nbias. Results here also seem to indicate that coverage bias, as represented\nby the number of articles published per theme, can be picked up on and\nused to inform classifications. However, word count or image presence\nalmost never appear in decision plots, whereas features from the traditional\n2Due to the setup of the SHAP python library, the PyTorch embedding model could not be\nproperly analysed. The SVC model with the second best performance was thus used for\nproducing the decision plots, as its performance was comparable and it is likely that the\nmodels would have picked up on roughly similar features.\n4 results 19\nFigure 5: A confusion matrix comparison of MBFC labels with those of Robertson\net al..\nbias dataset (for example, self reference density) are more often deemed\ninformative alongside article counts.\n4.3Post-Hoc Analysis: Ground Truth Label Comparisons\nIn light of the difference in performance between the models trained on\nRobertson et al.\u2019s data and those trained on MBFC data, additional analyses\nwere done to examine this closer. A quick comparison revealed that there\nis a sizeable mismatch between labels. For all outlets present in both\nRobertson et al.\u2019s and MBFC\u2019s data, only 46% agree, and the AUC score\nis69. Figure 5displays a confusion matrix to compare predictions, and\nshows that neighbouring labels tend to be misclassified, but that there are\nsome more significant disagreements ( 32left-wing outlets are classified as\n\"least biased\" by Robertson et al., and similarly for 14right-wing outlets).\nThese larger gaps in labelling are problematic and raise an important\nissue regarding the validity of bias ratings in general, but especially in\nthe disparity of results between computationally determined results and\nhuman-made labels. For example, MBFC and another bias rating website,\nAllSides (AllSides, 2023 ), show a greater degree of agreement between\nthemselves ( 57% of 293outlets in common agree, AUC score of 74). This\nindicates that the computationally determined labels might be a bit less\nreliable compared to human annotations.\n4 results 20\n(a) Decision plot of Breitbart, a right-wing political news source.\n(b) Decision plot of Forbes, a right-leaning political news source.\n4 results 21\n(c) Decision plot of The Economist, a political news source rated as being centrist.\n(d) Decision plot of The Guardian, a left-leaning political news source.\n4 results 22\n(e) Decision plot of CNN, a left-wing political news source.\n(f) Decision plot of the Huffington Post, a left-wing political news source misclassified as\nright-wing.\nFigure 6: SHAP decision plots of various outlets. The twenty most influential\nfeatures are plotted in descending order. Values next to the plotted line are the\nfeature values of that particular outlet. The range at the top of the graph represents\nthe political bias labels as predicted by the model.\n5 discussion 23\n5 discussion\nThis thesis aimed to automatically classify the political bias of news outlets,\nwith a particular focus on scalability, minimal human intervention, and\ntransparency. The data used to train models was based on over 3million\narticles by news outlets from all around the globe, and as GDELT includes\nnews data since 1979 until the present, it would allow these models to be\napplied to any segment thereof. This circumvents limitations of previous\nresearch, which focused on constrained topics or datasets and could not\nimmediately be extended to global news (Gangula et al., 2019 ; Hamborg\net al., 2019 ; Spinde, Krieger, et al., 2022 ; Spinde, Plank, et al., 2022 ). Fur-\nthermore, the computational approaches used here for processing the data\ndo not rely on manual selection of themes or outlets, but could easily be\nextended to any topic or website present in the GDELT database. The focus\non automating the process with computational methods is in line with rec-\nommendations by previous research by Hamborg et al., who noted that the\nreliance on traditional manual labour was forgoing highly effective tools\nand hampering progress in the field of media bias detection. Altogether,\nthis allows for a much broader applicability than any previous research, to\nthe best of the author\u2019s current knowledge.\nThe first research question concerned the extent to which political\nleaning of news outlets could be predicted based on GDELT data. As\nsuggested by the results, models trained on GDELT data are able to classify\npolitical bias with some success. The highest performing model (a neural\nnetwork) achieved an accuracy of 75% and an AUC score of 81, compared\nto a45% and 50AUC score for the baseline model. This is comparable to\nwork by some previous studies (Spinde, Krieger, et al., 2022 ; Spinde, Plank,\net al., 2022 ), though there is still room for improvement when compared to\nsome of the more specific applications of media bias classification (Gangula\net al., 2019 ). Nevertheless, the fact that this approach could achieve such\nperformance based not on custom, optimized language models but instead\nonly on GDELT\u2019s relatively basic features is encouraging.\nConcerning the sub-questions of the first research question, it is however\napparent that models performed better on ground truth labels made by\nhumans (Media Bias Fact Check, 2021 a) rather than labels which were\nautomatically computed (Robertson et al., 2018 ). Thus, one of the criticisms\nby previous literature was not met, as the optimally performing models still\nrely to some degree on manual labour (Hamborg et al., 2019 ). Nevertheless,\nthis project did not necessitate its own custom labels and could instead rely\non previous work by Media Bias Fact Check, which is still an advantage.\nA potential reason for the discrepancy in performance between the\nmodels trained on Robertson et al.\u2019s data compared to that of Media Bias\n5 discussion 24\nFact Check is the fact that the former is inherently an approximate. As\ndiscussed in Section 3.2.1, Robertson et al. used Twitter sharing patterns\nof registered voters to attribute a score to each website. However, as they\nthemselves note, this has some limitations. For instance, their method\ncannot account for users sharing links to articles that they disagree with,\nas it assumes that people would only share articles in line with their own\npolitical opinions. However, this might introduce noise into the data, as\npeople can also frequently share articles they disagree with and wish to\ncriticise online. Media Bias Fact Check avoids this issue by relying on\nhuman expert classifications, though this is more laborious. Thus, the\npresent study\u2019s results suggest that models trained on Robertson et al.\u2019s\ncomputationally determined labels can still be used to some extent should\nmanual labels be unavailable, but optimal model performance still relies\non human labels.\nAnother topic this thesis aimed to address was the tendency of previous\nresearch to focus on narrow types of bias (Gangula et al., 2019 ; Hamborg\net al., 2019 ; Spinde, Krieger, et al., 2022 ; Spinde, Plank, et al., 2022 ). As\ndiscussed in Sections 1and2, this limitation excludes much important\ninformation that could be informative, as it excludes well-established forms\nof bias such as coverage bias (what events get reported on and which do\nnot), size allocation bias (how much is written about a particular topic),\nand picture selection and explanation bias (what pictures are selected to\nillustrate articles and how are they described). Thus, research question two\nundertook this issue by comparing performance of models trained on the\nmore commonly studied form of bias related to word choice and general\ntone of articles (here referred to as traditional bias dataset) to models\ntrained on features related to broader patterns of bias (referred to as the\nalternative bias dataset, and included features such as word-, article counts\nand image- or video presence). Results suggested, as was hypothesized,\nthat the alternative bias features significantly contribute to model perfor-\nmance, and that their inclusion improves classification accuracy. Indeed,\nfeatures such as article counts, as a proxy for coverage bias patterns, were\nparticularly effective, as is apparent in Figure 6. Therefore, these results\nsuggest that focusing on broader patterns of bias is a worthwhile avenue\nfor future research on media bias detection.\nIn addition to these advantages of the current approach, transparency\nand explainability of results were also central to this thesis. Usually,\na main criticism of previous research using machine learning or other\ncomputational techniques has been that little insight can be gained into\nthe mechanisms of bias due to the black-box nature of the chosen ML\nmodels (Hamborg et al., 2019 ), and that therefore, social science approaches\nmight actually be more informative for advancing understanding of the\n5 discussion 25\nphenomenon. To address this, the SHAP method is used to provide\nexplanations of feature impact for any specific prediction. This allows\nfor more detailed examination of any outlet of interest, as long as it is\npresent in the GDELT database (as most news outlets should be). SHAP\nestimates the contributions of each feature for an outlet bias prediction and\nthus yields decision plots such as those visualized in Figure 6. Thus, this\npresents an approach to gain precise insight into outlet bias for specific\noutlets, with explanations for the classification result in question. As an\nexample, Figure 6a shows that Breitbart, a politically right-wing outlet, was\naccurately classified as such by the model thanks to features related to crime\n(cartels, kidnapping, black markets, organized crime, and robbery). The\nfeature values can be interpreted to make sense of the result: for instance, it\nseems like many articles discuss crime cartels ( 0.545, where the maximum\npossible value is 1due to the scaling of the data) and that the articles about\nblack markets will tend to have quite a negative tone ( 0.409). Thus, one\ncan interpret these results as meaning that Breitbart\u2019s focus on crime is\nindicative of right-wing bias, which is more intuitive and informative than a\nsimple classification result or model-level feature importances. Conversely,\nit also allows for clarifications for misclassified instances. Figure 6f shows\nthe feature impacts made during the classification of the Huffington Post,\nwhich was deemed right-wing while being left-wing. This example in\nparticular shows the significant impact of the MBFC categorical variables,\nproviding important insight into the model.\nThe SHAP decision plots allow us to also further validate the previous\nresults, demonstrating the importance of including broader patterns of\nbias: article counts in particular seem to populate the top twenty most\nimpactful features in Figure 6indicating that coverage bias is also an\nexcellent attribute to consider when determining bias, rather than only\nfocusing on word selection bias (see Sections 2and4.1).\nHowever, the plots also raise some questions. Some themes brought\nup are frequently included in previous research as politically polarising\nand thus highly indicative of bias (environmental issues, inequality, so-\ncial movements, firearms, and election fraud), and many can be regarded\nas similar to such topics (free speech, hate speech, treason, surveillance,\nmilitary readiness). On the other hand, some themes are not commonly\nconsidered and are thus more difficult to readily explain. For instance,\nfeatures related to exhumation, sanitation and natural or man-made disas-\nters of varying kinds are also included in the top twenty most impactful\nfeatures, though this is more difficult to interpret.\nAltogether, SHAP gives an intuitive explanation for classifications for\nany outlet in the dataset by showing which features are most impactful.\nThis approach could hopefully be more informative for the public, offering\n5 discussion 26\na transparent examination of overall outlet behaviour. As previously\nnoted by Newman et al., a significant portion of the public cites distrust\nof the media, specifically due to political bias, as a reason for avoiding\nconsuming news. The currently adopted method begins to at least partially\naddress this issue by providing a method that offers transparency into\noutlet behaviour, which would allow for closer examination of personal\nnews consumption. This might, hopefully, contribute to citizens\u2019 ability to\nmake decisions in an informed manner about various topics important in\nthe current political climate (Lewandowsky et al., 2020 ).\n5.1Limitations\nDespite addressing many drawbacks of previous research, such as the\nmanual annotations, limited applicability, and focus on narrow forms of\nbias, there are certain shortcomings of the current approach as well. Firstly,\nthe GDELT database is at times minimal and outdated in its documentation\n(GDELT Project was last updated in 2015 ). For instance, their documenta-\ntion provides a list of themes that are supposedly included, but this list is\nsorely outdated and highly inaccurate, as it fails to account for thousands\nof themes found during analysis. Additionally, their methods for comput-\ning certain measures are not thoroughly explained; for instance, they do\nnot explain how they actually compute their tone scores. Because of this,\nit is possible that the features used are somewhat inaccurate or could be\nbetter represented through custom methods.\nSecondly, GDELT contains a large number of features that could have\npotentially been included, such as information related to persons or loca-\ntions discussed in articles. Furthermore, enhanced versions of all these\nfeatures and the themes is also provided, which include information about\nwhere a particular theme occurs in an article, but these were ultimately\nnot included. GDELT also provides their own extended set of sentiment\nanalysis variables (GCAM), but these were excluded after preliminary\ntesting revealed that they did not contribute to performance and even\ndecreased model accuracy (the tested features were developed by Hopp,\nFisher, Cornell, Huskey, and Weber, as these were based on a recently\ndeveloped method compared to other features offered by GCAM). There\nare, however, many other possible GDELT or GCAM features that could be\nuseful to consider.\nThirdly, this thesis relied on various GDELT features as proxies for\nmeasures of different forms of bias, but these are in some cases somewhat\nloosely related to the forms of bias they were linked to. Picture and\nexplanation bias was, for example, only indirectly examined here, as the\ncurrent approach only accounted for presence or absence of images for\n6 conclusion 27\na particular article. Ideally, the actual content of the images would be\nincluded, as has been done previously in other studies (Kwak & An, 2016 ).\nSimilarly, it is possible that custom formulations of features related to word\nor labelling bias might present improved results, as the aggregation of the\nGDELT dataset might have lost informative data. Furthermore, some forms\nof bias were excluded from this analysis but could also be informative. For\ninstance, story placement bias was briefly considered for inclusion in this\nanalysis but was ultimately not included.\nFurthermore, it was noted during post-hoc analysis that the ground\ntruth labels used here display remarkable disagreement with each other.\nThis raises questions regarding what can be considered acceptable ground\ntruths, as even expert labels will disagree with each other to some extent.\nOverall, results seemed to suggest that MBFC might be more reliable, but it\nis entirely possible that another dataset of bias labels might prove superior.\nLastly, the current best performing model was trained on GDELT data\nsupplemented with categorical features from MBFC. These categorical\nfeatures are, however, only available for a small subset of outlets present\nin GDELT. As a consequence, the model is not as widely applicable as\nwould have ideally been the case. Nevertheless, the models trained on\nonly GDELT data achieved comparable performance, so it is nevertheless\npossible to get accurate predictions and SHAP explanations for any GDELT\nwebsite with only a small sacrifice in accuracy.\n6 conclusion\nThe field of news bias research is becoming increasingly crucial, as trust\nin news is dwindling around the world (Newman et al., 2022 ). Readers\nseem to distrust news due to the influence of political bias, which goes\nagainst the traditional ideals of neutral reporting in journalism. Social\nand computational sciences have studied the phenomenon but fall short.\nSocial science approaches are laborious and may suffer from subjectivity,\nwhereas computational approaches, while fast and efficient, fail to provide\ninsight into bias and tend to stop at determining whether bias can be\nautomatically detected. Additionally, previous work tends to focus on\nspecific predetermined topics or data and only narrow forms of bias,\ndespite there being broad known patterns of bias recorded in the literature.\nThus, this thesis developed an approach to classify outlet political bias\nusing global news data (GDELT), providing a fully automatic and scalable\napproach to detecting news bias. Furthermore, the SHAP method was used\nto provide explanations for predictions, so that the model\u2019s classifications\ncould be explained for any outlet of interest. Results showed that machine\nlearning models trained on GDELT could indeed classify political bias\n6 conclusion 28\nof news outlets, with the best performing model achieving an accuracy\nof75% and an AUC score of 81. Models were more successful when\ntrained on human-made labels, compared to computationally determined\nones, suggesting that some reliance on manual work in the field is still\nsuperior, despite the advantages of automatic labels. Furthermore, results\nshowed that including broader patterns of bias improved performance\nand comprehensiveness. Finally, the SHAP explanations offer transparent\nexplanations for why a particular outlet is considered politically biased\nand show which topics and behaviours influence the classification. Inter-\nestingly, many themes commonly considered divisive reappear, but some\ninformative features have not been previously considered in the literature.\nThis may help address gaps or future avenues in current research.\nAll in all, the current work builds upon existing research but extends\nit to be more widely applicable and informative for the field. The in-\ncreased transparency into the manifestations of bias, as offered in this\napproach, may be helpful for adequately informing the public about its\nnews consumption, as well as providing more insight into the underlying\nmechanisms of bias to a more granular extent than what is traditionally\nattempted by computational methods. Considering the immense impact of\nnews on global political climates, increased understanding and trustwor-\nthiness of media are hoped to contribute to a better informed society and a\nhealthier political environment.\n6.1Future Work\nThe results obtained during this analysis show promising results for better\nunderstanding media bias through computational means. Nevertheless,\nthere are several possible developments that should be undertaken to\nfurther enhance understanding of this phenomenon. A first avenue of\nstudy related to the dataset GDELT. As discussed in Section 5.1, GDELT\ncontains a large number of possible features that could also be included in\nfuture analysis, some of which might be more representative or informative.\nFuture work may include features from GCAM, story placement on news\nwebsites as a measure of attributed importance, information related to\npersons and locations mentioned in the article (which is also present in\nGDELT), or may even benefit from custom sentiment analyses of articles.\nThe latter might be particularly beneficial and attainable with language\nmodels, as GDELT provides at times sparse documentation about their\nmethods, and thus increased transparency is likely to be constructive.\nFurthermore, future work might expand upon the current approach by\nincluding more precise analysis of image or video content in news articles.\nPrevious research has already delved into this topic (Kwak & An, 2016 ),\n6 conclusion 29\nbut to the best of the author\u2019s current knowledge, it has yet to be combined\nwith further measures as broad as those used here.\nFuture work might also use other labels of political bias, as the bipar-\ntisan scoring used here does not necessarily lend itself well to all global\npolitical systems. Extending this further, the current approach still relies\non human-made labels. Future work may thus wish to address this by\ntraining unsupervised models on GDELT data to avoid any reliance on hu-\nman labels. Some unsupervised models were tested during this thesis, but\ndid not yield particularly promising results (see Annex 7.6). Nevertheless,\nit is possible that further effort in this area may yield promising results.\nLastly, the SHAP method is only a beginning for providing insight\ninto news bias, and future work might focus on presenting more detailed\ninformation, such as relevant excerpts of articles, to concretely demonstrate\nmodel predictions. For instance, the SHAP decision plot of the prediction\nof CNN could be supplemented with example article fragments about\nviolent unrest or humanitarian crises ( 6e). This might additionally lead to\ninsights about why certain themes are deemed informative. For instance, it\nis not immediately clear why features related to waterways are particularly\nimpactful, and deeper insight into this could reveal why this is the case\nand inform the field of themes that are not usually considered in news bias\nresearch.\n7 data source ,ethics ,code ,and technology statement 30\n7 data source ,ethics ,code ,and technology statement\nThe data sources for this project were the GDELT database, Robertson\net al.\u2019s previous research and publicly available bias ratings by Media\nBias Fact Check. All data was acquired from publicly available sources.\nGDELT provides its data freely through downloadable compressed files,\nand Robertson et al.\u2019s bias ratings are available on GitHub. Data from\nMedia Bias Fact Check was scraped to collect the most recently available\ninformation. All images used in this thesis were created by the author.\nConcerning the code used for the experiments, all code was written\nby the author, but the PyTorch model\u2019s embedding structure was adapted\nfrom existing code by Malik and NS. In order to scrape MBFC data, code\ndeveloped by dr. Chris Emmery was used, with permission. Lastly,\nChatGPT was used as a coding assistant for some issues, mostly regarding\nthe implementation of the SHAP library. The following Python libraries\nwere mainly used during development:\n\u2022 Pandas ( 1.5.3)\n\u2022 Numpy ( 1.22.3)\n\u2022 Matplotlib ( 3.7.1)\n\u2022 Shap ( 0.41.0)\n\u2022 Scikit Learn ( 1.2.2)\n\u2022 PyTorch ( 1.12.0.dev20220510 +cu116)\n\u2022 PyTorch Lightning ( 2.0.1.post0)\n\u2022 Tqdm ( 4.64.0)\nConcerning tools and services, the online website thesaurus.com was\nused to find synonyms and definitions during writing. Overleaf\u2019s and\nQuillBot\u2019s3spell-checking services were used for checking spelling and\ngrammar. No reference management software was used. ChatGPT was\nused to find the correct term for \"informativeness\", but ultimately the-\nsaurus.com proved a better resource for that.\n3See https://quillbot.com/spell-checker\nREFERENCES 31\nreferences\nAllSides. ( 2023 ).Allsides methodology. (Accessed April 21,2023 .https://\nwww.allsides.com/media-bias/media-bias-rating-methods )\nBaraniak, K., & Sydow, M. ( 2018 ). News articles similarity for automatic\nmedia bias detection in polish news portals. In 2018 federated conference\non computer science and information systems (fedcsis) (pp.21\u201324).\nBorders, R., & French, N. ( 2022 ). The world press freedom index 2022 .\nBoser, B. E., Guyon, I. M., & Vapnik, V. N. ( 1992 ). A training algorithm for\noptimal margin classifiers. In Proceedings of the fifth annual workshop\non computational learning theory (pp.144\u2013152).\nBrandenburg, H., & Van Egmond, M. ( 2012 ). Pressed into party support?\nmedia influence on partisan attitudes during the 2005 uk general\nelection campaign. British Journal of Political Science ,42(2),441\u2013463.\nBreiman, L. ( 1996 ). Bagging predictors. Machine learning ,24,123\u2013140.\nBreiman, L. ( 2001 ). Random forests. Machine learning ,45,5\u201332.\nBrownlee, J. ( 2019 ).3ways to encode categorical variables for deep learn-\ning. Retrieved from https://machinelearningmastery.com/how-to\n-prepare-categorical-data-for-deep-learning-in-python/ (Ac-\ncessed 15.03.2023 )\nB\u00fchlmann, P ., & Yu, B. ( 2002 ). Analyzing bagging. The annals of Statistics ,\n30(4),927\u2013961.\nChen, T., & Guestrin, C. ( 2016 ). Xgboost: A scalable tree boosting system. In\nProceedings of the 22nd acm sigkdd international conference on knowledge\ndiscovery and data mining (pp.785\u2013794).\nChinn, S., Hart, P. S., & Soroka, S. ( 2020 ). Politicization and polarization\nin climate change news content, 1985 -2017 .Science Communication ,\n42(1),112\u2013129.\nClemens, K. ( 2015 ). Geocoding with openstreetmap data. GEOProcessing\n2015 ,10.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. ( 2018 ). Bert: Pre-training\nof deep bidirectional transformers for language understanding. arXiv\npreprint arXiv: 1810 .04805 .\nDhal, P ., & Azad, C. ( 2022 ). A comprehensive survey on feature selection\nin the various fields of machine learning. Applied Intelligence ,1\u201339.\nDitton, J., & Duffy, J. ( 1983 ). Bias in the newspaper reporting of crime\nnews. Brit. J. Criminology ,23,159.\nFreund, Y., & Schapire, R. E. ( 1997 ). A decision-theoretic generalization of\non-line learning and an application to boosting. Journal of computer\nand system sciences ,55(1),119\u2013139.\nFukushima, K. ( 1969 ). Visual feature extraction by a multilayered network\nof analog threshold elements. IEEE Transactions on Systems Science\nREFERENCES 32\nand Cybernetics ,5(4),322\u2013333.\nGangula, R. R. R., Duggenpudi, S. R., & Mamidi, R. ( 2019 ). Detecting\npolitical bias in news articles using headline attention. In Proceedings\nof the 2019 acl workshop blackboxnlp: analyzing and interpreting neural\nnetworks for nlp (pp.77\u201384).\nGDELT Project. ( 2015 ).The gdelt global knowledge graph (gkg) data format\ncodebook v 2.1.Retrieved from http://data.gdeltproject.org/\ndocumentation/GDELT-Global _Knowledge _Graph _Codebook-V2.1\n.pdf (Accessed 02/03/2020 )\nGilliam Jr, F. D., Iyengar, S., Simon, A., & Wright, O. ( 1996 ). Crime in\nblack and white: The violent, scary world of local news. Harvard\nInternational Journal of press/politics ,1(3),6\u201323.\nGuo, C., & Berkhahn, F. ( 2016 ). Entity embeddings of categorical variables.\nCoRR ,abs/1604 .06737 . Retrieved from http://arxiv.org/abs/1604\n.06737\nGuyon, I., Weston, J., Barnhill, S., & Vapnik, V . ( 2002 ). Gene selection for\ncancer classification using support vector machines. Machine learning ,\n46,389\u2013422.\nHamborg, F., Donnay, K., & Gipp, B. ( 2019 ). Automated identification of\nmedia bias in news articles: an interdisciplinary literature review.\nInternational Journal on Digital Libraries ,20(4),391\u2013415.\nHart, P. S., Chinn, S., & Soroka, S. ( 2020 ). Politicization and polarization in\ncovid- 19news coverage. Science communication ,42(5),679\u2013697.\nHopp, F. R., Fisher, J. T., Cornell, D., Huskey, R., & Weber, R. ( 2021 ).\nThe extended moral foundations dictionary (emfd): Development\nand applications of a crowd-sourced approach to extracting moral\nintuitions from text. Behavior research methods ,53,232\u2013246.\nHunt, E. B., Marin, J., & Stone, P. J. ( 1966 ). Experiments in induction.\nImambi, S., Prakash, K. B., & Kanagachidambaresan, G. ( 2021 ). Pytorch.\nProgramming with TensorFlow: Solution for Edge Computing Applications ,\n87\u2013104.\nKalsnes, B., & Larsson, A. O. ( 2021 ). Facebook news use during the 2017\nnorwegian elections\u2014assessing the influence of hyperpartisan news.\nJournalism Practice ,15(2),209\u2013225.\nKelsey, D. ( 2019 ). News, discourse, and ideology. In The handbook of\njournalism studies.\nKingma, D. P ., & Ba, J. ( 2014 ). Adam: A method for stochastic optimization.\narXiv preprint arXiv: 1412 .6980 .\nKosho, J. ( 2016 ). Media influence on public opinion attitudes toward the\nmigration crisis. International Journal of Scientific & Technology Research ,\n5(5),86\u201391.\nKwak, H., & An, J. ( 2016 ). Revealing the hidden patterns of news photos:\nREFERENCES 33\nAnalysis of millions of news photos through gdelt and deep learning-\nbased vision apis. In Proceedings of the international aaai conference on\nweb and social media (Vol. 10, pp. 99\u2013107).\nLe, H., Shafiq, Z., & Srinivasan, P . ( 2017 ). Scalable news slant measurement\nusing twitter. In Proceedings of the international aaai conference on web\nand social media (Vol. 11, pp. 584\u2013587).\nLeetaru, K., & Schrodt, P. A. ( 2013 ). Gdelt: Global data on events, location,\nand tone. ISA Annual Convention . Retrieved from http://citeseerx\n.ist.psu.edu/viewdoc/summary?doi=10.1.1.686.6605\nLewandowsky, S., Smillie, L., Garcia, D., Hertwig, R., Weatherall, J., Egidy,\nS., . . . others ( 2020 ). Technology and democracy: Understanding the\ninfluence of online technologies on political behaviour and decision-\nmaking.\nLi, X., Wang, L., & Sung, E. ( 2008 ). Adaboost with svm-based component\nclassifiers. Engineering Applications of Artificial Intelligence ,21(5),785\u2013\n795.\nLundberg, S. M., & Lee, S.-I. ( 2017 ). A unified approach to interpreting\nmodel predictions. In I. Guyon et al. (Eds.), Advances in neural in-\nformation processing systems 30(pp.4765 \u20134774 ). Curran Associates,\nInc. Retrieved from http://papers.nips.cc/paper/7062-a-unified\n-approach-to-interpreting-model-predictions.pdf\nMadrigal, G., & Soroka, S. ( 2023 ). Migrants, caravans, and the impact of\nnews photos on immigration attitudes. The International Journal of\nPress/Politics ,28(1),49\u201369.\nMalik, U. ( 2020 ).Introduction to pytorch for classification. (Accessed March\n14,2023 .https://stackabuse.com/introduction-to-pytorch-for\n-classification/ )\nMedia Bias Fact Check. ( 2021 a).About media bias fact check. (Accessed April\n19,2023 .https://mediabiasfactcheck.com/about/ )\nMedia Bias Fact Check. ( 2021 b). Left vs. right bias: How\nwe rate the bias of media sources. (Accessed April 19,\n2023 .https://mediabiasfactcheck.com/left-vs-right-bias-how\n-we-rate-the-bias-of-media-sources/ )\nMedia Bias Fact Check. ( 2023 ).Mbfc methodology. (Accessed March 20,2023 .\nhttps://mediabiasfactcheck.com/methodology/ )\nM\u00fcller, A. C., & Guido, S. ( 2017 ).Introduction to machine learning with\npython: a guide for data scientists . OReilly.\nNewman, N., Fletcher, R., Robertson, C. T., Eddy, K., & Nielsen, R. K.\n(2022 ). Reuters institute digital news report 2022 .\nNS, A. ( 2020 ).Deep learning for tabular data using pytorch. (Accessed March\n15,2023 .https://jovian.ml/aakanksha-ns/shelter-outcome )\nPaybarah, A. ( 2015 ). Media matters: New york tv news over-reports on\nREFERENCES 34\ncrimes with black suspects. POLITICO. Available online at: https://www.\npolitico. com/states/new-york/city-hall/story/ 2015 /03/media-matters-new-\nyork-tv-news-over-reports-on-crimes-with-black-suspects- 020674 (accessed\nMay 06,2022 ).\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel,\nO., . . . others ( 2011 ). Scikit-learn: Machine learning in python. the\nJournal of machine Learning research ,12,2825 \u20132830 .\nRobertson, R. E., Jiang, S., Joseph, K., Friedland, L., Lazer, D., & Wilson,\nC. (2018 ). Auditing partisan audience bias within google search.\nProceedings of the ACM on Human-Computer Interaction ,2(CSCW), 1\u2013\n22.\nSanh, V ., Debut, L., Chaumond, J., & Wolf, T. ( 2019 ). Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter. arXiv preprint\narXiv: 1910 .01108 .\nScikit Learn. ( 2023 a). Permutation feature importance. (Accessed May\n5,2023 .https://scikit-learn.org/stable/modules/permutation\n_importance.html )\nScikit Learn. ( 2023 b).Tuning the hyper-parameters of an estimator: Search-\ning for optimal parameters with successive halving. (Accessed March\n29,2023 .https://scikit-learn.org/stable/modules/grid _search\n.html#successive-halving-user-guide )\nSoroka, S., Loewen, P ., Fournier, P ., & Rubenson, D. ( 2016 ). The impact of\nnews photos on support for military action. Political Communication ,\n33(4),563\u2013582.\nSpinde, T., Hamborg, F., & Gipp, B. ( 2021 ). Media bias in german news ar-\nticles: A combined approach. In Ecml pkdd 2020 workshops: Workshops\nof the european conference on machine learning and knowledge discovery\nin databases (ecml pkdd 2020 ): Sogood 2020 , pdfl 2020 , mlcs 2020 , nfmcp\n2020 , dina 2020 , edml 2020 , xkdd 2020 and inra 2020 , ghent, belgium,\nseptember 14\u201318,2020 , proceedings (pp.581\u2013590).\nSpinde, T., Krieger, J.-D., Ruas, T., Mitrovi\u00b4 c, J., G\u00f6tz-Hahn, F., Aizawa, A.,\n& Gipp, B. ( 2022 ). Exploiting transformer-based multitask learning\nfor the detection of media bias in news articles. In Information for\na better world: Shaping the global future: 17th international conference,\niconference 2022 , virtual event, february 28\u2013march 4,2022 , proceedings,\npart i (pp.225\u2013235).\nSpinde, T., Plank, M., Krieger, J.-D., Ruas, T., Gipp, B., & Aizawa, A. ( 2022 ).\nNeural media bias detection using distant supervision with babe\u2013bias\nannotations by experts. arXiv preprint arXiv: 2209 .14557 .\nVan Spanje, J., & De Vreese, C. ( 2014 ). Europhile media and eurosceptic\nvoting: Effects of news media coverage on eurosceptic voting in the\n2009 european parliamentary elections. Political Communication ,31(2),\nREFERENCES 35\n325\u2013354.\nWatts, D. J., Rothschild, D. M., & Mobius, M. ( 2021 ). Measuring the news\nand its impact on democracy. Proceedings of the National Academy of\nSciences ,118(15), e1912443118 .\nREFERENCES 36\nappendix a :mbfc features\nThis appendix describes in more detail the features provided by MBFC.\nFor each news outlet, they provide a set of data points of interest:\n\u2022Political lean, which denotes whether a news outlet will tend to favor\neither the left or right political spectrum. This feature contains five\nclasses in total: \"left\", \"left center\", \"least biased\", \"right center\" and\n\"right\" leaning. They compile a set of principles for determining\nwhether a source is left or right leaning, which can be found on their\nwebsite for further information (Media Bias Fact Check, 2021 b).\n\u2022Factuality refers to how factual a source tends to be, whether they\nuse credible sources, immediately correct incorrect information and\nhave failed credible reporting fact-checks in the past. The categories\nspan from \"very low\", \"low\", \"mixed\", \"mostly factual\", and\"high\" to\n\"very high\". Details of how these are evaluated are explained in their\nmethodology.\n\u2022Traffic estimates are drawn from Similar Web to determine the\namount of visitors each news outlet site receives, accounting for\npage views, print and media market viewers per month. An out-\nlet with under 150thousand views per month is classed as having\nminimal traffic, 150thousand to 2.5million as medium traffic and\nanything above is denoted as high traffic.\n\u2022Country press freedom is also noted on the site, as measured by the\nWorld Press Freedom Index by Reporters without Borders (Borders\n& French, 2022 ). They use each country\u2019s rank to determine their\nscore: top ten countries receive an \"excellent\" score, those until top\n50a \"mostly free\" score, top 100are considered to have \"moderate\nfreedom\", top 160\"limited freedom\" and the remainder are classed\nas \"oppressed\". There are a total of 180countries ranked.\n\u2022Media type records which types of media the news outlet in question\nuses. This includes many often overlapping categories that were sim-\nplified into the following classes: website, TV station, radio station,\njournal, magazine, news agency, news paper and organisations/foun-\ndations.\n\u2022The credibility rating is a combination of some of the above men-\ntioned features. It combines the factuality score with the traffic,\npolitical lean magnitude and press freedom scores to determine a\nfinal rating, ranging from \"high credibility\",\"medium credibility\" to\n\"low credibility\".\nREFERENCES 37\nappendix b :grid search model parameters\n7.1Support Vector Classifier\nThe following hyperparameters of the Scikit-Learn SVC model were used\nfor the grid search:\n\"C \" : [ 0.0 1,0.1,2,4,5,1 0] ,\n\"gamma \" : [ 0.1,0.0 1,0.0 0 1 ] ,\n\" kernel \" : [ \" r b f \" , \" poly \" , \" l i n e a r \" ] ,\n\" degree \" : [ 1,2,3,4,5,1 0] ,\n\" b r e a k _ t i e s \" : [ True , F a l s e ] ,\n\" class_weight \" : [ \" balanced \" , None ]\n7.2Decision Tree Classifier\nThe following hyperparameters of the Scikit-Learn Decision Tree model\nwere used for the grid search:\n\" c r i t e r i o n \" : [ \" g i n i \" , \" entropy \" , \" l o g _ l o s s \" ] ,\n\" s p l i t t e r \" : [ \" best \" , \" random \" ] ,\n\" max_depth \" : [ None , 3,5,1 0,5 0,1 0 0 ] ,\n\" min_samples_split \" : [ 2,5,5 0,1 0 0 ,5 0 0 ] ,\n\" min_samples_leaf \" : [ 1,3,5,1 0,5 0] ,\n\" max_features \" : [ \" s q r t \" , \" log 2\" , None, 10,100 ,500 ,1000 ,2000 ] ,\n\" max_leaf_nodes \" : [ 5,1 0,2 0,5 0, None ] ,\n\" class_weight \" : [ \" balanced \" , None ]\n7.3Random Forest Classifier\nThe following hyperparameters of the Scikit-Learn Random Forest model\nwere used for the grid search:\n\" n_estimators \" : [ 10,50,100 ] ,\n\" c r i t e r i o n \" : [ \" g i n i \" , \" entropy \" , \" l o g _ l o s s \" ] ,\n\" max_depth \" : [ 3,5,10] ,\n\" min_samples_split \" : [ 2,5,10] ,\n\" min_samples_leaf \" : [ 5,1 0,2 0,5 0] ,\n\" max_features \" : [ \" s q r t \" , \" log 2\" , None , 1 0,1 0 0 ] ,\n\" max_leaf_nodes \" : [ 5,1 0,2 0,5 0, None ] ,\n\" bootstrap \" : [ True , F a l s e ] ,\n\" warm_start \" : [ True , F a l s e ]\nREFERENCES 38\n7.4AdaBoost Classifier\nThe following hyperparameters of the Scikit-Learn AdaBoost model were\nused for the grid search:\n\" estimator \" : [ D e c i s i o n T r e e C l a s s i f i e r ( max_depth = 1,\nmin_samples_leaf = 1,\nclass_weight =\" balanced \" ) ,\nD e c i s i o n T r e e C l a s s i f i e r ( max_depth = 3,\nmin_samples_leaf = 1,\nclass_weight =\" balanced \" ) ,\nD e c i s i o n T r e e C l a s s i f i e r ( max_depth = 5,\nmin_samples_leaf = 1,\nclass_weight =\" balanced \" ) ,\nRandomForestClassifier ( max_depth = 1,\nmin_samples_leaf = 1,\nclass_weight =\" balanced \" ) ,\nRandomForestClassifier ( max_depth = 3,\nmin_samples_leaf = 1,\nclass_weight =\" balanced \" ) ,\nRandomForestClassifier ( max_depth = 5,\nmin_samples_leaf = 1,\nclass_weight =\" balanced \" ) ] ,\n\" n_estimators \" : [ 5,1 0,5 0] ,\n\" l e a r n i n g _ r a t e \" : [ 0.0 0 1 ,0.0 1,0.1,0.2,0.5,1,2] ,\n7.5XGBoost Classifier\nThe following hyperparameters of the XBGoost model were used for the\ngrid search:\n\" n_estimators \" : [ 3,5,1 0,5 0] ,\n\" l e a r n i n g _ r a t e \" : [ 0.1,0.0 1,0.0 5] ,\n7.6Bagging Classifier\nThe following hyperparameters of the Scikit-Learn Bagging model were\nused for the grid search:\n\" estimator \" : [ SVC_model , DecisonTree_model , RandomForest_model ,\nAdaBoost_model , XGBoost_model ] ,\n\" n_estimators \" : [ 5,1 0,2 0,5 0,1 0 0 ] ,\n\" max_features \" : [ 0.2,0.5,0.8,1.0] ,\n\" bootstrap \" : [ True , F a l s e ] ,\n\" b o o t s t r a p _ f e a t u r e s \" : [ True , F a l s e ] ,\n\" warm_start \" : [ True , F a l s e ] ,\nREFERENCES 39\nappendix c :neural network architecture\nThe architecture of the PyTorch neural network is as follows:\nTabularNetModel (\n( l a y e r s ) : Sequential (\n(0) : Linear ( i n _ f e a t u r e s = 357, o u t _ f e a t u r e s = 512, b i a s=True )\n(1) : ReLU( i n p l a c e=True )\n(2) : BatchNorm 1d (512 , eps= 1e \u221205, momentum= 0.1, a f f i n e =True ,\nt r a c k _ r u n n i n g _ s t a t s =True )\n(3) : Dropout ( p = 0.5, i n p l a c e=F a l s e )\n(4) : Linear ( i n _ f e a t u r e s = 512, o u t _ f e a t u r e s = 256, b i a s=True )\n(5) : ReLU( i n p l a c e=True )\n(6) : BatchNorm 1d (256 , eps= 1e \u221205, momentum= 0.1, a f f i n e =True ,\nt r a c k _ r u n n i n g _ s t a t s =True )\n(7) : Dropout ( p = 0.5, i n p l a c e=F a l s e )\n(8) : Linear ( i n _ f e a t u r e s = 256, o u t _ f e a t u r e s = 5, b i a s=True )\n)\n( val_accuracy ) : MulticlassAccuracy ( )\n( t e s t _ a c c u r a c y ) : MulticlassAccuracy ( ) )\nFor the experiment of full bias and categorical features, it was:\nTabularNetModel (\n( embeddings ) : ModuleDict (\n( F a c t u a l i t y ) : Embedding ( 8,4)\n( PressFreedom ) : Embedding ( 6,3)\n( MediaType ) : Embedding ( 9,5)\n( T r a f f i c ) : Embedding ( 5,3)\n( C r e d i b i l i t y ) : Embedding ( 5,3)\n)\n( l a y e r s ) : Sequential (\n(0) : Linear ( i n _ f e a t u r e s = 377, o u t _ f e a t u r e s = 512, b i a s=True )\n(1) : ReLU( i n p l a c e=True )\n(2) : BatchNorm 1d (512 , eps= 1e \u221205, momentum= 0.1, a f f i n e =True ,\nt r a c k _ r u n n i n g _ s t a t s =True )\n(3) : Dropout ( p = 0.5, i n p l a c e=F a l s e )\n(4) : Linear ( i n _ f e a t u r e s = 512, o u t _ f e a t u r e s = 256, b i a s=True )\n(5) : ReLU( i n p l a c e=True )\n(6) : BatchNorm 1d (256 , eps= 1e \u221205, momentum= 0.1, a f f i n e =True ,\nt r a c k _ r u n n i n g _ s t a t s =True )\n(7) : Dropout ( p = 0.5, i n p l a c e=F a l s e )\n(8) : Linear ( i n _ f e a t u r e s = 256, o u t _ f e a t u r e s = 5, b i a s=True )\n)\n( val_accuracy ) : MulticlassAccuracy ( )\n( t e s t _ a c c u r a c y ) : MulticlassAccuracy ( )\n)\nREFERENCES 40\nappendix d :further experiments conducted during thesis\nThis section displays some preliminary results obtained during testing\nwhich are not included in the thesis despite being assessed during its\ndevelopment.\nFigure 7: T-SNE plot of GDELT outlet data based on MBFC full bias dataset. As is\napparent, this did not yield particularly informative results, but it is possible that\nfuture work may address this issue.\nFigure 8: Support Vector Regression (SVR) prediction results on Robertson\u2019s score\nratings. Since Robertson provides a continuous score, a regression model was\nalso tested. The results were promising, but not as good as those achieved by the\nclassification models. The mean absolute error was 0.19, mean squared error was\n0.07and R-squared was 0.51on the testing set.\nREFERENCES 41\nappendix e :self reflection\nDuring the course of this thesis, I have enjoyed working in an independent\nmanner, with excellent weekly guidance from my supervisors. In general, I\nwas quite pleased to find that things ran quite smoothly, and that academic\nwriting has become a familiar task at this point.\nWere I to go through this again, I\u2019d probably have opted for a different\nstructure for the aggregated GDELT dataset, as it was noted that some\nimportant information was getting lost while averaging. It might have been\nmore challenging and in that way beneficial to attempt using language\nmodels directly on the articles and obtaining the outlet-level behaviour\nfrom GDELT instead. All in all, this approach yielded satisfactory results,\nbut given more time I\u2019d have liked to attempt a different approach. Simi-\nlarly, I would also have been interested in using a more globally-applicable\nbias rating system, but these are seemingly harder to find.\nI was also somewhat confronted by my ambitions to get over a threshold\nof80% accuracy, which of course cannot be met with every project. I was\nsomewhat obstinate about getting somewhere closer to this score, and could\nprobably have continued testing different variations and formulations of\npreprocessing, model architecture or all things related if time had allowed\nfor it. My supervisors\u2019 remarks on this were however helpful, as it is\nindeed not always possible to get perfect accuracy for every problem, and\nthat is simply something to get used to.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "DATA-DRIVEN NEWS OUTLET BIAS DETECTION WITH GDELT", "author": ["OP BIAS"], "venue": "NA", "pub_year": "NA", "abstract": "News and media play a crucial role in informing public opinion, influencing politics, governance,  and civic life alike. Although journalism strives for unbiased reporting, various forms of"}, "filled": false, "gsrank": 30, "pub_url": "http://arno.uvt.nl/show.cgi?fid=170689", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:bNcJtq_ucCMJ:scholar.google.com/&output=cite&scirp=29&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D20%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=bNcJtq_ucCMJ&ei=CLWsaLmrCI6IieoP0sKRuAk&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:bNcJtq_ucCMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "http://arno.uvt.nl/show.cgi?fid=170689"}}, {"title": "Bias or diversity? Unraveling fine-grained thematic discrepancy in US news headlines", "year": "2023", "pdf_data": "Bias or Diversity? Unraveling Fine-Grained Thematic Discrepancy in U.S. News\nHeadlines\nJinsheng Pan*, Weihong Qi*, Zichen Wang, Hanjia Lyu, Jiebo Luo\nUniversity of Rochester\n{jpan24,wqi3,zwang189,hlyu5}@ur.rochester.edu, jluo@cs.rochester.edu\n* these authors contributed equally\nAbstract\nThere is a broad consensus that news media outlets incorpo-\nrate ideological biases in their news articles. However, prior\nstudies on measuring the discrepancies among media outlets\nand further dissecting the origins of thematic differences suf-\nfer from small sample sizes and limited scope and granular-\nity. In this study, we use a large dataset of 1.8 million news\nheadlines from major U.S. media outlets spanning from 2014\nto 2022 to thoroughly track and dissect the \ufb01ne-grained the-\nmatic discrepancy in U.S. news media. We employ multiple\ncorrespondence analysis (MCA) to quantify the \ufb01ne-grained\nthematic discrepancy related to four prominent topics - do-\nmestic politics, economic issues, social issues, and foreign af-\nfairs in order to derive a more holistic analysis. Additionally,\nwe compare the most frequent n-grams in media headlines\nto provide further qualitative insights into our analysis. Our\n\ufb01ndings indicate that on domestic politics and social issues,\nthe discrepancy can be attributed to a certain degree of media\nbias. Meanwhile, the discrepancy in reporting foreign affairs\nis largely attributed to the diversity in individual journalistic\nstyles. Finally, U.S. media outlets show consistency and high\nsimilarity in their coverage of economic issues.\nIntroduction\nNews media plays a vital role in in\ufb02uencing public percep-\ntions of domestic politics, economic policies, social issues,\nand foreign affairs (Soroka 2003; Linos and Twist 2016;\nHitt and Searles 2018; Lyu et al. 2023). While diverse in-\ndividual perspectives in news articles promote informed dis-\ncussions and critical thinking, systematic bias can result in\nmisinformation and heightened polarization of views (Ent-\nman 2007; Prior 2013; Mour\u00e3o and Robertson 2019; Lyu\nand Luo 2022). Understanding the dynamics of \ufb01ne-grained\nthematic variations and identifying the underlying causes of\nsuch discrepancies provide valuable insights into the media\nlandscape, which is essential for the function of democracy.\nFine-grained thematic discrepancy in news media refers to\ndifferences in the speci\ufb01c topics selected for the content of\nnews coverage. For instance, when reporting on news re-\nlated to abortion, a focus on abortion rights and a focus on\nabortion laws represent two distinct subtopics, even though\nthey are both related to the general theme of abortion. Al-\nthough existing literature has extensively studied various\ntypes of media bias (D\u2019Alessio and Allen 2000), media use\nselectivity (Iyengar and Hahn 2009; Knobloch-Westerwick,Mothes, and Polavin 2020), and the consequences of media\nbias (Jamieson and Cappella 2008), comprehensive research\non the landscape of news media topics and whether the the-\nmatic differences are attributable to media bias or perspec-\ntive diversity remains scarce. In this study, we use 1.8 mil-\nlion news headlines from nine U.S. national news media out-\nlets and perform multiple correspondence analysis (MCA) to\ncompute thematic similarity, exploring the American media\nlandscape. We subsequently examine the thematic variations\nover time and between media outlets to uncover the under-\nlying factors contributing to these differences.\nDoes the thematic discrepancy stem from media bias or\ndiversity in perspectives? Existing literature presents con-\n\ufb02icting arguments and evidence. While a substantial body\nof research agrees on the existence of ideological biases\namong the U.S. news media (Sutter 2000; Groseclose and\nMilyo 2005a; Gentzkow and Shapiro 2010), Budak, Goel,\nand Rao (2016) discover considerable similarities among\nmajor outlets, except for political scandals. The con\ufb02icting\nevidence may be due to the different study samples they fo-\ncus on. For example, the study samples of D\u2019Alessio and\nAllen (2000) are news articles about presidential elections,\nwhile the study samples of Budak, Goel, and Rao (2016) are\ngeneral political articles. Additionally, different methods,\nsuch as meta-analysis and machine learning models based\non crowd-sourced labels, can yield different results. To pro-\nvide a holistic understanding of such thematic discrepancy,\nwe extend the scope to four prominent topics across mul-\ntiple national media organizations ranging from 2014 to\n2022 at a larger scale . Three of the topics are domestic\npolitics, social issues, and foreign affairs. Lyu et al. (2023)\nhighlight the importance of these topics in the assessment\nof hyperpartisanship across different media. We further in-\nclude economic issues because this topic also involves dif-\nferent perceptions despite its objectivity (Ang et al. 2022).\nInstead of full news articles, we concentrate on analyzing\nthe thematic discrepancies in news headlines because they\nare more accessible and they frequently encapsulate the key\nopinions or events of the content. In addition, the headlines\nachieve an optimal balance between contextual impact and\ncognitive effort, effectively guiding readers to construct a\ncoherent interpretation of the information presented, as con-\n\ufb01rmed by Dor (2003).\nTo distinguish the media bias and the perspective diver-arXiv:2303.15708v2  [cs.CL]  6 May 2023\nsity, we follow existing literature in de\ufb01ning media bias as\n1) selecting and framing particular issues with ideological\nleaning, 2) distortion of facts, or 3) only reporting nega-\ntive news about certain parties or ideologies (D\u2019Alessio and\nAllen 2000; Budak, Goel, and Rao 2016; Gentzkow and\nShapiro 2010).\nRelated work\nDespite their role in democratic supervision, news reports\nmay not be free of bias. For instance, Bourgeois, Rappaz,\nand Aberer (2018) \ufb01nd selection biases in the context of\nnews coverage. Although the de\ufb01nition of media bias varies,\nit is widely agreed that selecting and framing particu-\nlar issues with ideological leaning, distortion of facts,\nand only reporting negative news about certain parties\nor ideologies are typical types of media bias (D\u2019Alessio\nand Allen 2000; Budak, Goel, and Rao 2016; Gentzkow\nand Shapiro 2010). By this de\ufb01nition, political partisan bias,\nwhich strategically manipulates headlines, article sizes, and\nframings to make reports consistent with their ideology is\nwidespread in news media (Groeling and Kernell 1998;\nGroseclose and Milyo 2005b; Groeling 2013; Shultziner\n2020). However, the thematic discrepancy in news articles\ndoes notnecessarily attribute to media bias. For example,\nthe different interpretations of the same event, the unique\nnarrative style of individual journalists, and the different in-\ndividual experiences can all lead to thematic differences in\nnews articles but are not necessarily systematic biases. The\naspect of discrepancy is rarely visited by academic schol-\nars. Our study contributes to unraveling the \ufb01ne-grained the-\nmatic variations in U.S. news headlines.\nUnderstanding the thematic discrepancies among media\nhas attracted much attention from the research community.\nTraditional methods (Guess et al. 2021; Spinde et al. 2021)\ncollect public opinions from different surveys and polls\nand quantify the media bias into a certain range of values.\nHowever, collecting surveys on a large scale is often time-\nconsuming and expensive. Compared to traditional methods,\nmodel-based methods are more feasible. Many prior stud-\nies (Benamara et al. 2007; Bautin, Vijayarenu, and Skiena\n2021) have been conducted on measuring media bias from\nthe perspective of sentiment analysis on news headlines.\nMore recent work exploits masked language models to mea-\nsure semantic discrepancies. For example, Guo, Ma, and\nV osoughi (2022) mask the adjacent words of speci\ufb01c bi-\ngrams in news article sentences and then use \ufb01ne-tuned lan-\nguage models to predict the possible words that could \ufb01ll\nin the blank. They compare the prediction results to mea-\nsure the attitudinal difference between media. However, it\nis noteworthy that pre-trained language models may contain\nunknown bias from the training corpus (Schramowski et al.\n2022). Our study aims to explore potential thematic discrep-\nancies among media outlets by constructing thematic repre-\nsentations using n-grams that are free from pre-training bias.\nMaterial and Method\nIn this section, we describe how we collect and preprocess\nnews headlines. We then discuss how we identify the newsheadlines of the four topics ( i.e., domestic politics, economic\nissues, social issues, and foreign affairs). In the end, we de-\ntail our approach to analyzing the thematic discrepancy.\nData Collection and Preprocessing\nOur study uses the dataset collected by Lyu et al. (2023).\nFor the sake of a self-contained paper, we provide a brief\noverview of the data collection and preprocessing process.\nTo collect data from the news media, they employed two ap-\nproaches: using the of\ufb01cial web API provided by the news\nmedia and crawling the web archives and search pages of the\nnews media. They retrieved 1.8 million news headlines from\nthe websites of nine representative media outlets including\nThe New York Times, Bloomberg, CNN, NBC, Wall Street\nJournal, Christian Science Monitor, The Federalist, Reason,\nand Washington Times. These media outlets were catego-\nrized into three groups: Left ,Central , andRight with\nrespect to the political leaning of each media outlet, which\nis assessed by allsides.com and mediabiasfactcheck.com.\nMore speci\ufb01cally, the Left group includes The New York\nTimes, Bloomberg, CNN, and NBC. The Central group\nconsists of Wall Street Journal and Christian Science Moni-\ntor. The Right group contains The Federalist, Reason, and\nWashington Times. The collected data range from January\n2014 to September 2022 covering various topics. They pre-\nprocessed the data by performing lemmatization, eliminat-\ning stop words, and converting all text to lowercase.\nRelevant Title Identi\ufb01cation\nTo identify the news headlines of the four topics, we \ufb01rst\nsearch for the most frequent n-grams. Following Guo, Ma,\nand V osoughi (2022), we choose to \ufb01nd the most frequent\nbigrams. By examining each year\u2019s data, we have isolated\nthe bigrams that appeared no less than 100 times. In total,\nwe have identi\ufb01ed 797 bigrams meeting this criterion. Next,\ntwo annotators manually categorize these bigrams into the\nfour relevant topics. Before the annotation, a pilot annotation\nsession where the two annotators read a few sample titles to-\ngether and discuss the labeling schema is performed. We \ufb01nd\nthat it is easy to label because of the non-ambiguity of the\nbigrams. For example, (\u2018ukrainian\u2019,\u2018refugee\u2019) falls under the\ncategory of foreign affairs, while (\u2018health\u2019, \u2018law\u2019) pertains to\nsocial issues. Each annotator then labels half of the collected\nbigrams. Subsequently, we search for titles that contain at\nleast one of these bigrams. Finally, we identify 295,311 news\nheadlines from January 2014 to September 2022 that are re-\nlated to the four topics. Table 1 summarizes the number of\nbigrams and corresponding titles.\nThematic Discrepancy Analysis\nAlthough techniques such as text frequent pattern min-\ning (Han et al. 2007) and term-based text clustering (Aggar-\nwal and Zhai 2012) could be used for text analysis, we \ufb01nd\nmultiple correspondence analysis (MCA) (Hirschfeld 1935)\nadequate for measuring thematic discrepancy, as demon-\nstrated by Lakhanpal et al. (2022) who used MCA to investi-\ngate textual differences in online hate speech. MCA encodes\n# bigrams # news headlines\nForeign affairs 94 38,137\nDomestic politics 460 168,911\nEconomic issue 116 43,576\nSocial issue 127 44,687\nTotal 797 295,311\nTable 1: Number of labeled bigrams and collected news\nheadlines for each topic.\ncategorical data and represents them in low-dimensional Eu-\nclidean space ( i.e., 2-D in our study). The thematic discrep-\nancy is calculated as the distance in the low-dimensional\nspace. To perform MCA, we construct a contingency table\nin which each column represents one of nine media outlets\nand each row denotes the frequency of occurrence for each\nn-gram in the identi\ufb01ed news headlines. To improve robust-\nness, we select the n-grams that appear more than 50 times\nin the tiles of a single media outlet. Bigrams and trigrams\nare used to construct the contingency table. Unigrams are\nnot included in this study because we observe that in news\nheadlines, most subjects are bigrams or trigrams ( e.g. names\nand events). For a more meaningful interpretation, we focus\non bigrams and trigrams. Next, we perform singular value\ndecomposition (SVD) to obtain the orthogonal vectors that\nrepresent the categorical data. Note that SVD is applied for\ndimensionality reduction for visualization purpose.\nResults\nTo investigate the temporal patterns of \ufb01ne-grained thematic\ndiscrepancies in news headlines of different media across\nvarious important topics, we employ MCA to analyze the\ntitles of each topic for each year. We visualize the MCA\nresults to re\ufb02ect the media report discrepancies over time.\nEach point on the graphs represents a single media outlet,\nwith markers indicating their respective ideological position.\nTo further reveal the dynamics of media report discrepan-\ncies, for each topic, we present the top 10 most frequent n-\ngrams in 2014, 2018, and 2022 from news headlines of all\nmedia outlets, representing the key subjects that these out-\nlets highlight. The three years correspond to the shift in the\npresidency as well as the pre- and post-COVID eras. We then\nshow the top 10 most frequent n-grams for representative\noutlets from the Left ,Central , and Right categories,\nas well as within and diverging from the majority cluster, to\nexplore the underlying factors contributing to the thematic\nvariation.\nDomestic politics\nFigure 1 depicts the temporal characteristics of media dis-\ncrepancies regarding domestic politics . Since 2017, we have\nobserved an overall rising level of concentration among the\nanalyzed media outlets. Speci\ufb01cally, these outlets were more\nsparsely distributed in 2014 but became notably concen-\ntrated by 2020, with CNN, Washington Times, and New\nYork Times as outliers. Within the Left media, CNN has\ndisplayed an increasing discrepancy compared to other out-\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nnytnbcbbg\ncnn\nreasonwasht\nwsj\nchristianfedlistdomestic-2014\nnytnbcbbg\ncnnreasonfedlist\nwashtwsj\nchristiandomestic-2015\nnytnbcbbg\ncnnreasonfedlist\nwasht wsjchristiandomestic-2016\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nnytnbcbbg\ncnnreason\nwashtwsjchristianfedlistdomestic-2017\nnytnbcbbg\ncnnreason fedlist\nwashtwsjchristiandomestic-2018\nnytnbc\nbbg\ncnnreasonfedlist\nwashtwsjchristiandomestic-2019\n0.5\n 0.0 0.5 1.01.0\n0.8\n0.6\n0.4\n0.2\n0.0\nnytnbcbbg\ncnnreasonfedlist\nwashtwsjchristiandomestic-2020\n0.5\n 0.0 0.5 1.0nytnbc\ncnnwashtchristian bbg\nwsjfedlist\nreasondomestic-2021\n0.5\n 0.0 0.5 1.0nytnbc\nbbg\ncnnreason fedlist\nwasht\nwsjchristiandomestic-2022left\nright\ncenterFigure 1: MCA results regarding the coverage of domestic\npolitics by the media between 2014 and 2022 (Best viewed\nby zoom-in on screen).\nlets since 2015 (Figure 2). Among the Right media, Rea-\nson exhibits a marked discrepancy during Democratic presi-\ndencies, while the discrepancy decreases during the Repub-\nlican administration. In contrast, The Washington Times dis-\nplays a contrasting trend, exhibiting greater thematic dis-\ncrepancies during the Republican presidency and reduced\ndiscrepancies during the Democratic administrations. The\ngrowing divergence between CNN and other media outlets\ncould be attributed to the 2016 Presidential Election. Pre-\nvious studies have highlighted the polarizing nature of this\nelection and the controversies surrounding Donald Trump,\nwhich have led to subsequent changes in reporting styles\nacross media outlets (Benkler, Faris, and Roberts 2018).\nAs shown by Table 2, political \ufb01gures (highlighted in\ngreen) consistently attract considerable media attention,\noccupying two to six positions within the top 10 n-grams.\nIn 2018, the n-grams representing Trump and his admin-\nistration were more frequently used. Over time, judicial\ninstitutions (highlighted in orange) have continually gar-\nnered signi\ufb01cant public attention, with the Supreme Court,\nAttorney General, and Justice Department consistently\nranking among the top 10 headline topics. However, civil\nrights issues can be overshadowed by political upheavals.\nWhile free speech and civil rights received substantial\nattention in 2014, they were later supplanted by politically\ncharged events such as the Capitol Riot and the January 6th\nCommittee.\nTable 3 shows the 10 most frequent n-grams in news\nheadlines of the Christian Science Monitor, New York\nTimes, and Reason in 2022, representing the Central ,\nLeft , and Right media, respectively. Despite their ideo-\nlogical differences, the Christian Science Monitor and Rea-\nson cover similar topics, with the New York Times being an\noutlier in Figure 1. The New York Times devotes more atten-\n2016 2017 2018 2019 2020 2021 2022\nYear0.200.250.300.350.400.450.500.55DistanceFigure 2: Distance between CNN and the centroid of the ma-\njor cluster from the MCA results.\n2014 2018 2022\nwhite house donald trump supreme court\nsupreme court white house primary election result\nbill de blasio supreme court congressional district\nhillary clinton trump administration biden administration\nrand paul 2016 election january committee\npresident obama president trump joe biden\nattorney general midterm election attorney general\nfree speech attorney general justice department\ncivil right melania trump capitol riot\njustice department hillary clinton senate race\nTable 2: Top 10 most frequent n-grams in 2014, 2018, 2022\nregarding domestic politics .\ntion to elections and legislatures (highlighted in orange), but\nless to political \ufb01gures (highlighted in green) compared to\nthe other two outlets, resulting in a greater thematic distance\nbetween them. The results suggest that the discrepancy in\ndomestic politics coverage is partly attributable to the choice\nof varying topics by different media outlets.\nEconomic Issues\nFigure 3 shows the trend of media discrepancies regarding\neconomic issues where topic selections within the economic\ndomain exhibits similarities. From 2014 to 2022, the major-\nity of media outlets are situated in the upper left corner of\nthe plot. However, starting in 2019, Bloomberg and CNN\nhave gradually become more distant from the upper left\ncluster.\nWe have noted minimal changes in the top 10 most\nfrequent n-grams in news headlines relating to economic\nissues .1We compare the Reason, Wall Street Journal, and\nCNN, which represent the Right ,Central andLeft\nmedia, respectively. The three outlets exhibit a strong\n1Details regarding the temporal variation in the most frequent\nn-grams of the economic issues can be found in the Appendix.Christian NYT Reason\nsupreme court election result supreme court\nsandy hook primary election \ufb01rst amendment\nbiden sandy congressional district primary free speech\nbiden sign supreme court court decision\ndonald trump white house biden administration\njan panel governor primary kentajin brown jackson\nfar right runoff election joe biden\noverturn roe attorney general court reject\nalex jones \ufb01rst congressional capitol riot\nright wings second congressional ron desantis\nTable 3: Top 10 most frequent n-grams of the Christian Sci-\nence Monitor, New York Times, and CNN in 2022 regarding\ndomestic politics .\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nnytbbgwasht\nwsjchristiannbc\ncnn\nreasonfedlistecon-2014\nnytnbcbbg\ncnn\nreasonfedlist\nwasht\nwsjchristianecon-2015\nnytnbcbbg\ncnnreasonfedlist washt\nwsjchristianecon-2016\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nnyt\nwsjbbgcnnreason\nchristiannbcwashtfedlistecon-2017\nnytnbcbbg\ncnnreasonfedlist\nwasht\nwsjchristianecon-2018\nnyt\nwsjnbcbbg\ncnnreasonfedlist\nwashtchristianecon-2019\n1.0\n 0.5\n 0.0 0.5 1.01.0\n0.8\n0.6\n0.4\n0.2\n0.0\nnytnbcbbgcnnwasht\nwsjchristian\nreasonfedlistecon-2020\n1.0\n 0.5\n 0.0 0.5 1.0nytnbc\nbbgcnnwasht\nwsjreasonfedlist christianecon-2021\n1.0\n 0.5\n 0.0 0.5 1.0nytnbc\nbbgcnnreasonfedlist\nwasht\nwsjchristianecon-2022left\nright\ncenter\nFigure 3: MCA results regarding the coverage of economic\nissues by the media between 2014 and 2022 (Best viewed by\nzoom-in on screen).\ninterest in topics including interest rates, the stock market,\nenergy, electronic cars, and essential economic policies.\nOur \ufb01ndings indicate that the media outlets reveal limited\nvariation in their coverage of economic issues, and are\nsimilar in both topics and perspectives.\nSocial Issues\nFigure 4 shows the discrepancies among media outlets\nrelating to social issues . A similar trend is observed in\nsocial issues as in domestic politics, with media outlets\nbecoming more concentrated over time. Some left-wing\nmedia outlets, such as CNN, NBC, and Bloomberg, have\ndiverged from the cluster since 2017. Meanwhile, centrist\nand right-wing media outlets, such as the Christian Science\nMonitor and Washington Times, have progressively moved\ncloser to the majority.\nThe focus on social issues is likely in\ufb02uenced by ongoing\nsocial movements according to the results in Table 5.\nIn 2014, due to the Medicaid expansion (Himmelstein\nReason WSJ CNN\nstudent loan forgiveness stock market gas price\ngas price supply chain rate hike\nbiden student debt interest rate mortgage rate\nformula shortage central bank student loan\nhigh gas student loan interest rate\nbaby formula shortage gas price oil price\ngas tax natural gas stock market\ninterest rate oil price natural gas\nelectric vehicle real estate electric car\nfossil fuel electric vehicle supply chain\nTable 4: Top 10 most frequent n-grams of CNN, Wall Street\nJournal, and Reason in 2022 regarding economic issues .\n2014 2018 2022\nhealth care health care climate change\nclimate change climate change social medium\nsocial medium social medium mass shooting\ngay marriage school shooting abortion right\nsexual assault gun control hate crime\nhealth law sexual assault abortion ban\nsame sex marriage sexual harassment health care\nglobal warming sex abuse fatally shoot\nhealth insurance sexual misconduct school shooting\ngun report gun violence human right\nTable 5: Top 10 most frequent n-grams in 2014, 2018, 2022\nregarding social issue .\n2019) and the promotion of same-sex marriage (Liptak\n2014), healthcare and gay marriage (both highlighted in\nblue) obtained signi\ufb01cant attention. In 2018, the #MeToo\nmovement sparked widespread debate and discussion\nabout sexual harassment (highlighted in red) in the news\nmedia (Pomarico 2018). In addition, the overturn of Roe\nv. Wade in mid-2022 (Liptak 2022) drew considerable\nattention to abortion laws and rights (highlighted in green).\nTable 6 presents the thematic discrepancies in social is-\nsues among the Christian Science Monitor, CNN, and Rea-\nson in 2022. We choose the three media outlets to repre-\nsent the Central ,Left , and Right media groups. No-\ntably, the three media outlets cover similar topics including\nclimate change, social media, abortion issues, hate crime,\npublic health, and gun control, but show subtle differences\nin their approaches and perspectives. For example, while\nReason emphasizes \u201cabortion law\u201d, CNN underscores its\nideological position by using \u201cabortion rights\u201d. Meanwhile,\nthe Christian Science Monitor focuses on \u201cabortion law\u201d,\n\u201cabortion rights\u201d, and \u201canti-abortion\u201d. When addressing gun\ncontrol issues, Reason employs terms like \u201cmass shooting\u201d,\nCNN emphasizes \u201cfatally shoot\u201d, and the Christian Science\nMonitor uses \u201cgun violence\u201d. These nuanced differences in\nthe choice of terms reveal the media bias (D\u2019Alessio and\nAllen 2000; Budak, Goel, and Rao 2016).\n0.6\n0.4\n0.2\n0.0\nnytnbc\nbbgcnn\nreasonwasht\nwsj\nchristianfedlistsocial-2014\nnytnbcbbg\ncnnreasonfedlist\nwasht\nwsj christiansocial-2015\nnytnbcbbg\ncnnreason\nwasht\nwsjchristianfedlistsocial-2016\n0.6\n0.4\n0.2\n0.0\nnytcnnreason fedlist\nwasht\nwsjchristiannbcbbgsocial-2017\nnytnbcbbg\ncnnreason\nfedlist\nwasht\nwsjchristiansocial-2018\nnytnbc\ncnnreasonfedlist\nwasht\nwsj\nbbgchristiansocial-2019\n0.5\n 0.0 0.50.6\n0.4\n0.2\n0.0\nnyt\nnbc\ncnnreasonfedlist\nwashtwsj\nbbgchristiansocial-2020\n0.5\n 0.0 0.5nytnbc\nbbg\ncnnreasonfedlist\nwasht\nwsjchristiansocial-2021\n0.5\n 0.0 0.5nytnbc\nbbg\ncnnreasonfedlist\nwasht\nwsjchristiansocial-2022left\nright\ncenterFigure 4: MCA results regarding the coverage of social is-\nsues by the media between 2014 and 2022 (Best viewed by\nzoom-in on screen).\nChristian Reason CNN\nclimate change social medium mass shooting\nmass shooting gun control social medium\nsocial medium abortion ban hate crime\ngun violence public health climate crisis\nabortion law climate change fatally shoot\ngun control health care school shooting\nabortion right mass shooting gun violence\nhate crime green energy uvalde school\ngreen energy abortion law abortion right\nTable 6: Top 10 most frequent n-grams of the Christian Sci-\nence Monitor, Reason, and CNN in 2022 regarding social\nissues .\nForeign Affairs\nFigure 5 shows that there has been no signi\ufb01cant change in\nthematic discrepancy relating to foreign affairs from 2014\nto 2022. The majority of media outlets are clustered in the\nupper-left corner of the graph, while the New York Times,\nWall Street Journal, and CNN emerge as outliers. These\nthree outlets have gradually formed a separate cluster. Addi-\ntionally, Bloomberg has been distancing itself from the ma-\njority since 2020. The emerging outliers may be attributed to\nthe distinct journalistic styles of media outlets. For example,\nthe Wall Street Journal and Bloomberg primarily concen-\ntrate on the economic and \ufb01nancial implications of geopolit-\nical tensions, resulting in differing perspectives compared to\nother media outlets.\nThe coverage of foreign affairs remains consistent over\ntime, with the exception of war outbreaks.2Geopolitical\n2Details regarding the temporal variation in the most frequent\nn-grams of the foreign affairs can be found in the Appendix.\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nnytnbc\ncnnreason\nwasht\nwsjchristianbbgfedlistforeign-2014\nnytnbc\ncnnreasonfedlist\nwasht\nwsjchristianbbgforeign-2015\nnytnbc\ncnnreasonfedlist\nwasht\nwsjchristianbbgforeign-2016\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nnytnbc\ncnnreasonfedlist\nwasht\nwsjchristianbbgforeign-2017\nnytnbcbbg\ncnnreasonfedlist\nwasht\nwsjchristianforeign-2018\nnytnbc\ncnnfedlist\nwasht\nwsjchristianbbgreasonforeign-2019\n0.5\n 0.0 0.51.0\n0.8\n0.6\n0.4\n0.2\n0.0\nnytnbc bbg\ncnnreasonfedlist\nwasht\nwsjchristianforeign-2020\n0.5\n 0.0 0.5nytnbc\nbbgcnnreasonfedlist\nwasht\nwsjchristianforeign-2021\n0.5\n 0.0 0.5nytnbc\nbbgcnnwasht\nwsjchristianreasonfedlistforeign-2022left\nright\ncenterFigure 5: MCA results regarding the coverage of foreign af-\nfairs by the media between 2014 and 2022 (Best viewed by\nzoom-in on screen).\nReason WSJ Bloomberg\nukrainian refugee ukraine war hong kong\nukraine war hong kong south africa\nprime minister north korea boris johnson\nhong kong russia ukraine ukraine war\ninvasion ukraine prime minister south african\nrussian invasion boris johnson saudi arabia\nukraine crisis russian oil prime minister\nboris johnson south korea north korea\nmiddle east saudi arabia russia sanction\nsouth africa russia sanction russian oil\nTable 7: Top 10 most frequent n-grams of Reason, Wall\nStreet Journal, and Bloomberg in 2022 regarding foreign af-\nfairs.\ntensions have gained the most attention from 2014 to 2022,\nwhile the Russia-Ukraine War emerged as a signi\ufb01cant topic\nin 2022.\nTable 7 compares the most frequent n-grams of Reason,\nWall Street Journal, and Bloomberg in 2022. The three me-\ndia outlets represent the Right ,Central , andLeft me-\ndia. Their coverage of foreign affairs is similar, emphasiz-\ning the Russia-Ukraine war, geopolitical tensions, and diplo-\nmatic relations with the United Kingdom. Despite the sig-\nni\ufb01cant discrepancies displayed in Figure 5, the three media\noutlets do not exhibit signi\ufb01cant differences in word framing\nwithin their headlines. This evidence suggests that the dis-\ncrepancies likely arise from different perspectives and writ-\ning styles among journalists, rather than a systematic bias\ntowards a speci\ufb01c ideology.\nFigure 6: Median absolute deviation of the clusters from the\nMCA results of domestic politics ,economic issues ,social\nissues , and foreign affairs between 2014 and 2022.\nDiscussions and Conclusions\nNews media plays an essential role in the daily lives of\nthe public because it shapes public opinions and atti-\ntudes (Brookes, Lewis, and Wahl-Jorgensen 2004; Linos\nand Twist 2016; Hitt and Searles 2018). Different media\nwould have a thematic disagreement on one event due\nto various reasons, for example, ideology. In this study,\nwe conduct a quantitative and qualitative analysis of the\n\ufb01ne-grained thematic discrepancy, focusing on four impor-\ntant topics including domestic politics, economic issues,\nsocial issues, and foreign affairs. By performing MCA on\nthe relevant titles of each topic for each year, we discover\nnotable patterns in thematic discrepancies across media\nin a temporal manner. To quantify the discrepancies, we\ncompute the median absolute deviation of the clusters from\nthe MCA results. The median absolute deviation is used\nto reduce the sensitivity toward outliers. The results are\nshown in Figure 6. In regard to domestic politics , we \ufb01nd\nthat thematic difference has been diminishing over time.\nThe thematic difference in domestic politics can be mainly\nattributed to the selection of topics in news coverage. When\nit comes to economic issues , American news media show\nlittle discrepancy both over time and across different outlets.\nSuch consistency of topic preferences also appears in the\nheadlines relating to foreign affairs . However, the thematic\ndiscrepancy of foreign affairs is larger than that of economic\nissues . This pattern indicates that thematic discrepancies\nmay arise from variations in perspectives and writing styles.\nWith respect to social issues , the thematic discrepancy is\nbrought about by a mixture of media bias and attitudinal\ndifference. Bias is revealed by different descriptions of the\nsame events and the attitudinal difference is indicated by\nthe topic preference of different media outlets.\nThere are, however, some limitations to our study. First,\nthe media outlets we select are major national news organi-\nzations. The gatekeeper and fact-checking mechanisms de-\nveloped by these outlets help prevent gross misinformation\nand overt bias, which may result in minimal discrepancies in\nsome issues. Future research could compare national and lo-\ncal media, offering a more comprehensive view of the Amer-\nican news landscape. Second, we only focus on four promi-\nnent topics featured in news headlines. To expand upon this,\nfuture work could conduct a more \ufb01ne-grained investigation\nacross a broader range of subjects.\nAcknowledgments\nThis work was partially supported by the Goergen Institute\nfor Data Science at the University of Rochester.\nReferences\nAggarwal, C. C.; and Zhai, C. 2012. A Survey of Text Clus-\ntering Algorithms. In Mining Text Data .\nAng, Z.; Reeves, A.; Rogowski, J. C.; and Vishwanath, A.\n2022. Partisanship, Economic Assessments, and Presiden-\ntial Accountability. American journal of political science\n66(2): 468\u2013484. ISSN 0092-5853.\nBautin, M.; Vijayarenu, L.; and Skiena, S. 2021. Interna-\ntional Sentiment Analysis for News and Blogs. Proceedings\nof the International AAAI Conference on Web and Social\nMedia 2(1): 19\u201326. doi:10.1609/icwsm.v2i1.18606. URL\nhttps://ojs.aaai.org/index.php/ICWSM/article/view/18606.\nBenamara, F.; Cesarano, C.; Picariello, A.; Recupero, D. R.;\nand Subrahmanian, V . S. 2007. Sentiment analysis: Adjec-\ntives and adverbs are better than adjectives alone. ICWSM\n7: 203\u2013206.\nBenkler, Y .; Faris, R.; and Roberts, H. 2018. Network pro-\npaganda: Manipulation, disinformation, and radicalization\nin American politics . Oxford University Press.\nBourgeois, D.; Rappaz, J.; and Aberer, K. 2018. Selection\nbias in news coverage: learning it, \ufb01ghting it. In Companion\nProceedings of the The Web Conference 2018 , 535\u2013543.\nBrookes, R.; Lewis, J.; and Wahl-Jorgensen, K. 2004. The\nmedia representation of public opinion: British television\nnews coverage of the 2001 general election. Media, Culture\n& Society 26(1): 63\u201380.\nBudak, C.; Goel, S.; and Rao, J. M. 2016. Fair and balanced?\nQuantifying media bias through crowdsourced content anal-\nysis. Public Opinion Quarterly 80(S1): 250\u2013271.\nD\u2019Alessio, D.; and Allen, M. 2000. Media bias in presiden-\ntial elections: A meta-analysis. Journal of communication\n50(4): 133\u2013156.\nDor, D. 2003. On newspaper headlines as relevance opti-\nmizers. Journal of pragmatics 35(5): 695\u2013721.\nEntman, R. M. 2007. Framing bias: Media in the distribution\nof power. Journal of communication 57(1): 163\u2013173.\nGentzkow, M.; and Shapiro, J. M. 2010. What drives media\nslant? Evidence from US daily newspapers. Econometrica\n78(1): 35\u201371.\nGroeling, T. 2013. Media Bias by the Numbers: Challenges\nand Opportunities in the Empirical Study of Partisan News.Annual Review of Political Science 16(1): 129\u2013151. ISSN\n1094-2939.\nGroeling, T.; and Kernell, S. 1998. Is network news cover-\nage of the president biased? The Journal of Politics 60(4):\n1063\u20131087.\nGroseclose, T.; and Milyo, J. 2005a. A measure of media\nbias. The quarterly journal of economics 120(4): 1191\u2013\n1237.\nGroseclose, T.; and Milyo, J. 2005b. A Social-science Per-\nspective on Media Bias. Critical review (New York, N.Y.)\n17(3-4): 305\u2013314. ISSN 0891-3811.\nGuess, A.; Barber\u00e1, P.; Munzert, S.; and Yang, J. 2021. The\nconsequences of online partisan media. Proceedings of the\nNational Academy of Sciences 118: e2013464118. doi:10.\n1073/pnas.2013464118.\nGuo, X.; Ma, W.; and V osoughi, S. 2022. Measuring Media\nBias via Masked Language Modeling. Proceedings of the\nInternational AAAI Conference on Web and Social Media\n16(1): 1404\u20131408. doi:10.1609/icwsm.v16i1.19396. URL\nhttps://ojs.aaai.org/index.php/ICWSM/article/view/19396.\nHan, J.; Cheng, H.; Xin, D.; and Yan, X. 2007. Frequent\npattern mining: current status and future directions. Data\nMining and Knowledge Discovery 15: 55\u201386.\nHimmelstein, G. 2019. Effect of the Affordable Care Act\u2019s\nMedicaid Expansions on Food Security, 2010-2016. Amer-\nican journal of public health (1971) 109(9): 1243\u20131248.\nISSN 0090-0036.\nHirschfeld, H. O. 1935. A Connection between Correla-\ntion and Contingency. Mathematical Proceedings of the\nCambridge Philosophical Society 31(4): 520\u2013524. doi:\n10.1017/S0305004100013517.\nHitt, M. P.; and Searles, K. 2018. Media coverage and public\napproval of the US Supreme Court. Political Communica-\ntion35(4): 566\u2013586.\nIyengar, S.; and Hahn, K. S. 2009. Red media, blue media:\nEvidence of ideological selectivity in media use. Journal of\ncommunication 59(1): 19\u201339.\nJamieson, K. H.; and Cappella, J. N. 2008. Echo chamber:\nRush Limbaugh and the conservative media establishment .\nOxford University Press.\nKnobloch-Westerwick, S.; Mothes, C.; and Polavin, N.\n2020. Con\ufb01rmation bias, ingroup bias, and negativity bias in\nselective exposure to political information. Communication\nResearch 47(1): 104\u2013124.\nLakhanpal, S.; Zhang, Z.; Li, Q.; Lee, K.; Kim, D.; Chae,\nH.; and Kwon, H. K. 2022. Sinophobia, misogyny,\nfacism, and many more: A multi-ethnic approach to iden-\ntifying anti-Asian racism in social media. arXiv preprint\narXiv:2210.11640 .\nLinos, K.; and Twist, K. 2016. The Supreme Court, the\nMedia, and Public Opinion: Comparing Experimental and\nObservational Methods. The Journal of legal studies 45(2):\n223\u2013254. ISSN 0047-2530.\nLiptak, A. 2014. Justices Reject Call to Halt Gay Marriages\nin Oregon. URL https://www.nytimes.com/2014/06/05/\nus/politics/supreme-court-rebuffs-call-to-end-same-sex-\nmarriages-in-oregon.html.\nLiptak, A. 2022. In 6-to-3 Ruling, Supreme Court\nEnds Nearly 50 Years of Abortion Rights. URL\nhttps://www.nytimes.com/2022/06/24/us/roe-wade-\noverturned-supreme-court.html.\nLyu, H.; and Luo, J. 2022. Understanding Political Polar-\nization via Jointly Modeling Users, Connections and Multi-\nmodal Contents on Heterogeneous Graphs. In Proceedings\nof the 30th ACM International Conference on Multimedia ,\n4072\u20134082.\nLyu, H.; Pan, J.; Wang, Z.; and Luo, J. 2023. Computational\nAssessment of Hyperpartisanship in News Titles. arXiv\npreprint arXiv:2301.06270 .\nMour\u00e3o, R. R.; and Robertson, C. T. 2019. Fake news\nas discursive integration: An analysis of sites that publish\nfalse, misleading, hyperpartisan and sensational informa-\ntion. Journalism studies 20(14): 2077\u20132095.\nPomarico, N. 2018. 11 of the biggest moments of the\n#MeToo movement in 2018. URL https://www.insider.com/\nme-too-movement-moments-2018-12.\nPrior, M. 2013. Media and political polarization. Annual\nReview of Political Science 16: 101\u2013127.\nSchramowski, P.; Turan, C.; Andersen, N.; Rothkopf, C. A.;\nand Kersting, K. 2022. Large pre-trained language models\ncontain human-like biases of what is right and wrong to do.\nNature Machine Intelligence 4(3): 258\u2013268.\nShultziner, Doron; Stukalin, Y . 2020. Politicizing What\u2019s\nNews: How Partisan Media Bias Occurs in News Produc-\ntion. Mass Communication and Society 24(3): 372\u2013393.\nISSN 1520-5436. doi:10.1080/15205436.2020.1812083.\nURL https://browzine.com/articles/406719000.\nSoroka, S. N. 2003. Media, public opinion, and foreign pol-\nicy. Harvard International Journal of Press/Politics 8(1):\n27\u201348.\nSpinde, T.; Kreuter, C.; Gaissmaier, W.; Hamborg, F.; Gipp,\nB.; and Giese, H. 2021. Do You Think It\u2019s Biased? How To\nAsk For The Perception Of Media Bias .\nSutter, D. 2000. Can the media be so liberal-the economics\nof media bias. Cato J. 20: 431.Appendix\nTables 8 and 9 show the 10 most frequent n-grams in 2014,\n2018, and 2022 regarding economic issues andforeign af-\nfairs.\n2014 2018 2022\nsmall business central bank gas price\npro\ufb01t rise real estate supply chain\nminimum wage silicon valley student loan\noil price stock market central bank\ncentral bank oil price stock market\nreal estate government bond interest rate\nnatural gas interest rate electric car\nsilicon valley u.s government bond real estate\ninterest rate tax cut russian gas\ngovernment bond supply chain electric vehicle\nTable 8: Top 10 most frequent n-grams in 2014, 2018, and\n2022 regarding economic issues .\n2014 2018 2022\nhong kong north korea hong kong\nnorth korea trade war ukraine war\nsouth korea saudi arabia north korea\ncease \ufb01re south korea south africa\nforeign policy kim jong un boris johnson\nprime minister hong kong prime minister\nukraine north korean south korea\nsouth africa prime minister saudi arabia\nsouth sudan trump trade russian oil\nmiddle east nuclear deal ukraine invasion\nTable 9: Top 10 most frequent n-grams in 2014, 2018, and\n2022 regarding foreign affairs .", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Bias or diversity? Unraveling fine-grained thematic discrepancy in US news headlines", "author": ["J Pan", "W Qi", "Z Wang", "H Lyu", "J Luo"], "pub_year": "2023", "venue": "arXiv preprint arXiv:2303.15708", "abstract": "There is a broad consensus that news media outlets incorporate ideological biases in their  news articles. However, prior studies on measuring the discrepancies among media outlets"}, "filled": false, "gsrank": 32, "pub_url": "https://arxiv.org/abs/2303.15708", "author_id": ["zUWKW5AAAAAJ", "cyyYHHEAAAAJ", "13MQzsgAAAAJ", "tPhwyYsAAAAJ", "CcbnBvgAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:LfSUAktweyAJ:scholar.google.com/&output=cite&scirp=31&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D30%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=LfSUAktweyAJ&ei=CrWsaIYfwNmJ6g-p2qHxBQ&json=", "num_citations": 9, "citedby_url": "/scholar?cites=2340587898817279021&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:LfSUAktweyAJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2303.15708"}}, {"title": "Using natural language to predict bias and factuality in media with a study on rationalization", "year": "2021", "pdf_data": "Using Natural Language to Predict Bias and\nFactuality in Media with a Study on Rationalization\nby\nKunal Tangri\nSubmitted to the Department of Electrical Engineering and Computer\nScience\nin partial fulfillment of the requirements for the degree of\nMaster of Engineering in Electrical Engineering and Computer Science\nat the\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\nFebruary 2021\n\u00a9Massachusetts Institute of Technology 2021. All rights reserved.\nAuthor................................................................\nDepartment of Electrical Engineering and Computer Science\nFebruary 2021\nCertified by............................................................\nJames Glass\nSenior Research Scientist\nThesis Supervisor\nAccepted by...........................................................\nKatrina LaCurts\nChair, Master of Engineering Thesis Committee\n2\nUsing Natural Language to Predict Bias and Factuality in\nMedia with a Study on Rationalization\nby\nKunal Tangri\nSubmitted to the Department of Electrical Engineering and Computer Science\non February 2021, in partial fulfillment of the\nrequirements for the degree of\nMaster of Engineering in Electrical Engineering and Computer Science\nAbstract\nFake news is a widespread problem due to the ease of information spread online,\nand its ability to deceive large populations with intentionally false information. The\ndamage it causes is exacerbated by its political links and loaded language, which\nmake it polarizing in nature, and preys on peoples\u2019 psychological biases to make it\nmore believable and viral. In order to dampen the influence of fake news, organi-\nzations have begun to manually tag, or develop systems to automatically tag, false\nand biased information. However, manual efforts struggle to keep up with the rate at\nwhich content is published, and automated methods provide very little explanation\nto convince people of their validity. In an effort to address these issues, we present a\nsystem to classify media sources\u2019 political bias and factuality levels by analyzing the\nlanguage that gives fake news its contagious and damaging power. Additionally, we\nsurvey potential approaches for increasing the transparency of black-box fake news\ndetection methods.\nThesis Supervisor: James Glass\nTitle: Senior Research Scientist\n3\n4\nAcknowledgments\nThe past year of research in the Spoken Language Systems group has been an incred-\nibly collaborative and educational experience, and I am grateful to all the members\nwithin the group for their advice, discussions, and exemplary work ethics. This envi-\nronment is a result of the hard work my advisor, Jim Glass, puts into developing and\nmaintaining our research community, and I feel very lucky to have been a part of it.\nHisopennessandencouragementofnewideas, commitmenttofosteringcollaboration,\nand unyielding support have been an invaluable part of my experience.\nI am also thankful for the postdoctoral advisors and other peers who were always\nwilling to share their thoughts and advice during my work - Ramy Baly, Tianxing\nHe, Mitra Mohtarami, and Moin Nadeem. They helped shape my research thought\nprocess, and offered many interesting conversations.\nAdditionally, I have been fortunate that my interest in combating fake news is\nalso shared by the Defense Science and Technology Agency of Singapore, and for\ntheir willingness to financially support me as a Research Assistant to work on this\nproblem.\nFinally, I am extremely blessed for my loving parents, and their endless encour-\nagement and intellectual curiosity. I strive to live by the examples and standards\nthey have set for me.\n5\n6\nContents\n1 Introduction 15\n2 Related Work 19\n2.1 Bias and Factuality Prediction . . . . . . . . . . . . . . . . . . . . . . 19\n2.1.1 Previous Methods . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.1.2 Methods Used in our Work . . . . . . . . . . . . . . . . . . . . 23\n2.2 Rationalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.2.1 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n2.2.2 Gradient-Based Approaches . . . . . . . . . . . . . . . . . . . 29\n3 Bias and Factuality Prediction 31\n3.1 Data and Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.2 Training the System . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.2.1 Data Scraping . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.2.2 Fine-tuning BERT . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.2.3 Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.2.4 Classifier Training . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.3 Experiments and Results . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.4 Making Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4 Rationalization 49\n4.1 Analysis of BERT\u2019s Attention . . . . . . . . . . . . . . . . . . . . . . 50\n4.2 Gradient-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . 55\n7\n4.3 Shortcomings of Extractive Rationales . . . . . . . . . . . . . . . . . 58\n5 Conclusion 61\n5.1 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n8\nList of Figures\n2-1 An illustration of the different scopes that bias and factuality predic-\ntion are performed on. The largest scope is the source-level, which\nconsists of a collection of articles from a media source. Next is the\narticle-level, made up of a collection of claims. Finally, the smallest\nscopeistheclaim-level, whichisasingleassertion. (Note: Inthisfigure\nwe do not include external information that may be used to perform\npredictions at each of the three levels.) . . . . . . . . . . . . . . . . . 20\n2-2 A visualization of the transformer encoder layers within BERT, and\nthe embedded representations that are input and output from each\nencoder. Embeddings for an input sequence can be retrieved at each\nencoder layer, and may behave differently depending on which layer\nthey are extracted from (Alammar, 2018). . . . . . . . . . . . . . . . 25\n2-3 A visualization of how BERT\u2019s attention heads highly weight relevant\ntokens for a specific task. In these examples, created using the bertviz\ntool, the attention heads show specializations towards coreference res-\nolution (Vig, 2019). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n2-4 An example of pure gradients being used to provide transparency on\nan image classification task. We can observe that the larger gradients\n(bottom) correspond to the regions of the original images (top) that\ncontain the object of interest (Simonyan et al., 2014). . . . . . . . . . 30\n9\n3-1 An outline of the training process for the bias and factuality prediction\nsystem. First, we scrape relevant language data from the URL\u2019s of the\nmedia sources in our dataset, and fine-tune BERT models to extract\nfeatures specific to the tasks of bias and factuality prediction. Next,\nwe use these fine-tuned BERT models, among other pre-built tools,\nto extract features from the scraped language data. Finally, we train\nSupport Vector Machines to predict bias and factuality using these\nextracted features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3-2 Diagramofthefine-tuningarchitectureusetoadaptBERTtothetasks\nof bias and factuality prediction. We train a linear layer on top of the\n[CLS] embedding output from the 12 transformer encoder layers of the\npre-trained BERT model (Devlin et al., 2019). . . . . . . . . . . . . . 36\n4-1 Attention visualization from 1st attention head in layer 6 on an article\nclassified as right-leaning (this is a truncated version of the full 512\ntoken input fed into BERT). This attention head highly attends to\n\"president - elect donald trump\" and \"defense\" which align with the\nright-leaning criteria used by Allsides. . . . . . . . . . . . . . . . . . . 52\n4-2 Attentionvisualizationfrom7thattentionheadinlayer12onanarticle\nclassified as right-leaning (this is a truncated version of the full 512\ntoken input fed into BERT). This attention head highly attends to\n\"president\", \"marine\", \"state\", and \"economy\" which align with the\nright-leaning criteria used by Allsides. . . . . . . . . . . . . . . . . . . 52\n4-3 The top 7 weighted tokens, and their magnitudes, from each attention\nhead in the final layer of BERT. Between attention heads we see a\nlarge amount of variation besides a few common words like \"about\",\n\"reform\", and \"state\". . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n10\n4-4 [CLS] token\u2019s attention weights across all 144 attention heads within\nBERT. In later layers, we observe some horizontal streaks, indicat-\ning that the same tokens are being heavily weighted across different\nattention heads. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4-5 The top 7 most important tokens, and their importance scores, using\nbothofourgradient-basedmethods. Bothmethodsoutputverysimilar\nresults. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4-6 Gradient-basedimportancevisualization,usingembed \u00b7gradientmethod,\non an article classified as right-leaning (this is a truncated version of\nthe full 512 token input fed into BERT). This method finds \"donald\ntrump\" and \"repealing obamacare\" as particularly important phrases\nin classifying this article as right-learning. . . . . . . . . . . . . . . . 58\n5-1 Our model architecture for generating natural language explanations\nduring a question answering task. The explainer component takes the\nquestion and answer choices as input, and generates an explanation\nusing gumbel-softmax to sample. This explanation is then used by the\npredictor to select the proper answer choice to answer the question. . 63\n11\n12\nList of Tables\n3.1 Examples from our dataset including the media sources, their bias and\nfactuality labels, and their corresponding URL\u2019s. . . . . . . . . . . . . 33\n3.2 Our dataset\u2019s distribution of classes for bias and factuality . . . . . . 35\n3.3 Source-level political bias prediction results with features extracted\nfrom articles using BERT models fine-tuned on different datasets. . . 41\n3.4 Source-level factuality prediction results with features extracted\nfrom articles using BERT models fine-tuned on different datasets. . . 42\n3.5 An ablation study of our features\u2019 predictive ability on the task of\npolitical bias prediction . In rows 11-13, we combine the best per-\nforming features from each data channel - (A) stands for articles, (T)\nstands for Twitter, and (W) stands for Wikipedia. We also include the\nresults from Baly et al. (Baly et al., 2020) for comparison (some of our\nfeatures behave differently than Baly et al. but we still report their\nbest reported results in rows 11-13). . . . . . . . . . . . . . . . . . . . 43\n3.6 An ablation study of our features\u2019 predictive ability on the task of fac-\ntuality prediction . In rows 11-13, we combine the best performing\nfeatures from each data channel - (A) stands for articles, (T) stands\nfor Twitter, and (W) stands for Wikipedia. We also include the results\nfrom Baly et al. (Baly et al., 2020) for comparison (some of our fea-\ntures behave differently than Baly et al. but we still report their best\nreported results in rows 11-13). . . . . . . . . . . . . . . . . . . . . . 45\n13\n4.1 Random vs. Media split article-level classification. In the random split\nexperiment, articleswererandomlyselectedacrossallmediasourcesfor\nthe train, development, and test splits. For the media split, we ensured\nthat articles were selected from disjoint sets of media sources for the\ntrain, development, and test splits. . . . . . . . . . . . . . . . . . . . 59\n14\nChapter 1\nIntroduction\nModerntechnologyhasenabledinformationtospreadfasterandreachamuchbroader\naudience than ever before. Through social media, people can share news stories with\nthe click of a button and online resources have made it easier for people to create\nwebsites that host information. This increased availability of information is a huge\nbenefit to society, as the general population is more informed, and it is easier for\npeople to collaborate and stay in touch. However, social media and online resources\nhave lowered the barrier to entry for publishing and spreading information to large\npopulations, which has enabled malicious actors to spread fake news. As the preva-\nlence of fake news has grown, the field of fake news detection has spawned to combat\nit.\nIn an article discussing its impact on the 2016 U.S. Presidential election, Allcott\nand Gentzkow offer a concise definition of fake news: potentially misleading informa-\ntion that is intentionally and verifiably false (Allcott and Gentzkow, 2017). As people\nhave been shown to be poor judges of false information (Kumar et al., 2016), factu-\nality prediction has become an essential component in fake news detection systems.\nHowever, we believe that fact-checkers, alone, are not sufficient for stifling fake news,\nand, in fact, they have been shown to potentially entrench people further in their\nfactually incorrect viewpoints (Nyhan and Reifler, 2010).\nThis problem is related to, if not exacerbated by, the politicization of news. A\nstudy by Iyengar and Hahn shows that people prefer to consume media from sources\n15\nthat agree with their own political ideologies (Iyengar and Hahn, 2009). Furthermore,\ndue to confirmation bias, people are more receptive to information that confirms\ntheir own views (Nickerson, 1998). These psychological tendencies, when coupled\ntogether, result in people being more likely to believe in news that aligns with their\npolitical ideologies, inherently linking the problem of fake news to political bias.\nUcinski, Klofstad, and Atkinson confirmed this link exists by showing that partisan\nattachment is correlated with whether people believe information being presented to\nthem (Uscinski et al., 2016). This is extremely concerning, given that the context of\nAllcott and Gentzkow\u2019s article was the impact of fake news on a Presidential election,\nand that other surveys of the field show that fake news is typically used for political\ngain (Shu et al., 2017).\nSeveral organizations have come to the same conclusion that both factuality and\npolitical bias play a large role in the spread of fake news, and they have created web-\nsites to manually annotate news for factuality and/or bias, including Media Bias/Fact\nCheck (Dave Van Zandt, 2015), Allsides (AllSides, 2020), and Politifact (PolitiFact,\n2007). Despite the efforts of these organizations, though, public awareness of bias in\nmedia remains low (Elejalde et al., 2018). We think this is partly due to a change in\nhow news is being consumed, with social media increasingly becoming the primary\nnews source for consumers (Shu et al., 2017). While social media is undoubtedly a\npowerful tool for discovering news and sharing it with others, it has also vastly in-\ncreased the amount of published news and the speed at which it spreads. Manual\nfact checking and bias labeling simply cannot keep up with rate of content creation\nwell enough to stem the damage of fake news. On top of that, fake news has actually\nbeen shown to spread six times faster than real news (Vosoughi et al., 2018), which\nmakes the task of stopping fake news through manual labelling even more difficult.\nThe most obvious next step in preventing the spread of fake news is to automate fact\nchecking and bias prediction, and there are a growing number of benchmark tasks\nbeing created to stimulate this field (e.g. FEVER for fact extraction and verification\n(Thorne et al., 2018) and Hyperpartisan SemEval 2019 for political bias prediction\n(Kiesel et al., 2019)).\n16\nIn our work, we leverage methods that have been developed on these benchmark\ntasks to create a system for predicting media sources\u2019 political bias and factuality\nlevels from natural language. Through political bias labeling, we aim to provide some\ncontext on the partisan window that people may be viewing information from, and\nthrough factuality labeling our goal is to identify misinformation. Additionally, we\nstudy approaches that aim to provide transparency in fake news detection methods\nin an effort to subvert the psychological biases humans have against viewpoints that\nconflictwiththeirownviews. Wediscussrelatedmethodologyinthefieldsofbothfake\nnews detection and rationalization in Chapter 2, before discussing the development\nof our bias and factuality prediction system in Chapter 3. Chapter 4 describes our\nexperimentsinprovidingtransparencyinfakenewsdetectionmodels,andweconclude\nour work and note possible future directions to explore in Chapter 5.\n17\n18\nChapter 2\nRelated Work\n2.1 Bias and Factuality Prediction\nIn recent years, fake news has become very well-studied due to increasing public\nawareness about the problems it poses. To provide a background on the current state\nof the field, we first highlight previous and current methods for detecting fake news,\nand justify why we focus on predicting bias and factuality at the media source-level\nusing only natural language. We then provide background on the methods that are\nused in our study.\n2.1.1 Previous Methods\nThe methods currently used to detect fake news fall into three general categories:\nfeature-based, graph-based, and propagation-based (Kumar and Shah, 2018). They\nare defined as follows:\n\u2022Feature-Based: methods that rely on linguistic information extracted from\nnews-related text (Horne and Adali, 2017; Kumar et al., 2016; Potthast et al.,\n2018)\n\u2022Graph-Based: methods that study networks of user interactions with news\n(Akoglu et al., 2010; Beutel et al., 2013)\n19\n\u2022Propagation-Based: methods that model the differences between how real\nvs. fake information spreads (Tripathy et al., 2010; Nguyen et al., 2012)\nWhile all three of these methods have their merits and comparable performances,\nwe believe that graph and propagation-based methods rely on modeling fake news\nindirectly and are not ideal as a result. The indirect features they are modeled on,\nuser graphs and information flow, are largely influenced by bot accounts (Nied et al.,\n2017), which may even be what these models exploit to detect fake news. However,\nit has been shown that bots are not actually responsible for the overall spread of fake\nnews - they only accelerate it (Davis et al., 2016). This makes graph and propagation-\nbased methods susceptible to adversaries who decide to change the behavior of these\nbots. Duetotheseshortcomings, wedecidetostudyfeature-basedmodelswhichfocus\non the language that gives fake news its viral and damaging power as we believe it is\nthe most robust predictor of fake news.\nFigure 2-1: An illustration of the different scopes that bias and factuality prediction\nare performed on. The largest scope is the source-level, which consists of a collection\nof articles from a media source. Next is the article-level, made up of a collection\nof claims. Finally, the smallest scope is the claim-level, which is a single assertion.\n(Note: In this figure we do not include external information that may be used to\nperform predictions at each of the three levels.)\nFor bias and factuality prediction, feature-based models operate on three different\n20\nlevels of granularity. From smallest to largest scope, these are the claim-level, article-\nlevel, and source-level, and an illustration of these scopes can be found in Figure\n2-1.\nClaim-Level\nAttheclaim-level, previousmethodsaremostlycenteredaroundpredictingfactuality,\nand heavily rely on stance-detection - whether ground-truth sources of information\nagree or disagree with a claim. Some approaches have used manually fact-checked\nclaims as the ground-truth source for stance-detection (Mukherjee and Weikum,\n2015), whereas others use comment-based discussion on social media as proxy for\nthe ground-truth (Kochkina et al., 2018; Dungs et al., 2018). Yet another avenue\nfor stance-detection draws relevant sources of information from the Web to serve as\nthe ground-truth to detect stance against (Mukherjee and Weikum, 2015; Baly et al.,\n2018b). In the contexts of article-level and source-level scopes, stance-detection has\nalso been useful under the hypothesis that trustworthy articles and sources will tend\nto agree with truthful claims and disagree with false claims (Mukherjee and Weikum,\n2015; Popat et al., 2018). However, as we increase the size of our scope past the\nclaim-level, approaches begin to model bias in addition to factuality and make use of\nextra information that is available at the larger scopes.\nArticle-Level\nAtthearticle-level, manymethodsforbiasandfactualitypredictionrelyonextracting\nfeatures from language. In a study across three datasets related to real vs. fake news,\nHorne and Adali (Horne and Adali, 2017) found that fake news tends to use shorter,\nsimpler, and more repetitive language, and a later work from Horne et al. creates a\ntoolkit to exploit some of these tendencies (Horne et al., 2018b). The features they\nanalyze include complexity, structure, bias and others which we make use of in our\nresearch and will describe in more detail in Section 2.1.2. Language is similarly used\nby Potthast et al. to predict factuality through a stylometric analysis of fake news\nusing N-grams, readability scores, word frequencies, and features specific to the news\n21\ndomain (ratios of quoted words, number of external links, avg. paragraph length)\n(Potthast et al., 2018). Other studies use only raw text to model factuality, feeding it\ndirectly to an LSTM (Rashkin et al., 2017), or more recent language models. In some\nof these studies on factuality prediction, not only is political bias a useful feature\n(Horne et al., 2018b), but it is also predictable using some of the exact same methods\n(Horne et al., 2018a). Furthermore, article-level bias prediction is a task that has also\nbeen studied in its own right, using n-grams, lexical features, vocabulary richness,\nand readability scores (Saleh et al., 2019), as well as through latent representations\nof article language content extracted from attention-based models (Kulkarni et al.,\n2018). For both article-level bias and factuality prediction, source-level reliability has\nbeen found to be an informative feature (Karadzhov et al., 2017), and can incorporate\ninformation external to articles like a source\u2019s social media presence and third-party\ndescriptions of the source. However, the field of source-level reliability is understudied\ncompared to the two smallest granularities of bias and factuality prediction, which is\nwhy we focus our study on it.\nSource-Level\nNot only are source-level bias and factuality predictions understudied and useful for\narticle-level predictions, but source-level predictions are also useful by themselves. As\nnoted by Baly et al. (Baly et al., 2020), bias and factuality prediction at the smaller\ngranularity levels can be a computational challenge, and potentially still too slow to\neffectively prevent the spread of fake news. Profiling entire media sources, however,\ncan provide a good indication on whether newly published material is reliable or\nnot, without the need to verify each claim or each article. Besides this feature of\nsource-level prediction, it also allows us to draw information from more mediums to\ninform predictions. A majority of the work done at the source-level was developed by\nBaly et al. in 2018 and 2020 (Baly et al., 2018a, 2020), in which they train Support\nVector Machine classifiers on features not only extracted from articles, but also from\nYouTube, Twitter, Facebook, and Wikipedia data. They hypothesize that YouTube\npresentsadditionalmediapublishedfromasourcethatcouldbeusefulforpredictions,\n22\nthat Twitter and Facebook can help analyze the audience of a media source, and that\nWikipedia provides a third-party view of a media source. Our work builds a system\nusing some of the methodology described by Baly et al. (Baly et al., 2020), and\nwe further describe some of the tools used for extracting features from these data\nchannels in Section 2.1.2.\n2.1.2 Methods Used in our Work\nBaly et al. use many of the same methods mentioned in the article-level approaches\nto extract features from all of their selected data channels. These tools are comprised,\nin part, by the lexicons described by Horne et al. (Horne et al., 2018b) in their News\nLandscape (NELA) toolkit, but a majority of them rely on latent representations\nextracted from language models - this is an increasingly used method as language\nmodels have become much more powerful in recent years. We make use of both types\nof feature extraction toolkits in our efforts to build a system based on the methods\nfrom Baly et al., so we describe them in more detail here.\nThe NELA toolkit extracts features that have proven to be useful across a wide\nrange of studies in political bias and factuality prediction, and a discussion of these\nstudies can be found in the paper from Horne et al. (Horne et al., 2018b). NELA\nextracts the following categories of features:\n\u2022Structure: Part of speech counts, linguistic features (function words, pro-\nnouns, prepositions, etc.), and clickbait title classification (Chakraborty et al.,\n2016)\n\u2022Sentiment: Sentiment scores from VADER (Hutto and Gilbert, 2015), and\nemotion and happiness scores from other lexicons (Recasens and Jurafsky, 2013;\nMitchell et al., 2013)\n\u2022Topic-Dependent: Lexicons to differentiate scientific fields and others to dis-\ntinguish personal concerns\n23\n\u2022Complexity: Lexical diversity, readability scores, avg. word and text lengths,\nand # of cognitive process words\n\u2022Bias:Severalbiaslexicons(RecasensandJurafsky,2013;MukherjeeandWeikum,\n2015) and subjectivity measures (Pang and Lee, 2004)\n\u2022Morality: Lexicon-based measures of morality (Lin et al., 2018) and features\nfrom the Moral Foundation Theory (Graham et al., 2009)\nWhile these features have previously been useful in other studies, and we include\nthem in our own, lexicon-based approaches are giving way to methods using neural\nlanguage models for feature extraction. Instead of using a rule or vocabulary-based\nmeans of feature extraction, neural language models develop latent representations of\nwordswhichallowthemtomodelsomeaspectoflanguage(typicallytheprobabilityof\nseeing specific sequence of words). These latent representations can provide powerful\nembeddings of semantics, syntax, and task-specific features at the word, sentence, or\neven article level. In fact, neural language models that are used for factuality and bias\nprediction may develop latent representations of the features extracted from lexical\nbased methods. Our study makes use of a few different methods for retrieving these\nembeddings.\nThe first method, Global Vectors for Word Representation (GloVe) (Pennington\net al., 2014), leverages global vocabulary statistics, as well as local contexts, in a log-\nbilinearregressionmodelfortheunsupervisedlearningofwordrepresentations. GloVe\nis used by Baly et al. in their 2018 study, and is replaced by newer methods in their\n2020 work. Though it is true that GloVe is an older approach for obtaining word\nembeddings, and has since been outperformed on natural language understanding\ntasks, we believe there is still some benefit in including it in our study as it has a\nfundamental difference from the newer embedding methods we experiment with. This\ndifference is that GloVe produces static embeddings, meaning each word has a single,\nfixed representation.\nThe newer embedding methods we explore, produce contextual embeddings - each\nword can have different representations based on the contexts in which it is used.\n24\nFigure 2-2: A visualization of the transformer encoder layers within BERT, and the\nembedded representations that are input and output from each encoder. Embed-\ndings for an input sequence can be retrieved at each encoder layer, and may behave\ndifferently depending on which layer they are extracted from (Alammar, 2018).\nMethods that we leverage for retrieving these contextual embeddings are based on\nBidirectional Encoder Representations from Transformers (BERT) (Devlin et al.,\n2019), a recently developed, transformer-based (Vaswani et al., 2017) model that\nachieved state-of-the-art performance on a range of natural language understanding\ntasks. BERT consists of 12 stacked transformer encoder layers, and is pre-trained\nusing two techniques which enable it to develop its powerful bidirectional represen-\ntations. The first technique is masked language modeling, where BERT predicts\nrandomly masked tokens at any position in the input. A result of this pre-training\ntask is that BERT learns to use context before andafter the masked word to make\nits prediction, which gives BERT its bidirectional property. Additionally, BERT is\npre-trained on a next sentence prediction task - whether sentence A is followed by\nsentence B. This technique encourages BERT to reason about the relationships be-\ntween sentences as well. The resulting pre-trained model can be further fine-tuned to\na variety of natural language processing tasks, and we describe how we suit it to our\nresearch when we go through our methods in Section 3.2.2.\n25\nFrom either pre-trained or fine-tuned BERT models, token embeddings can be\nretrieved from each of the 12 encoder layers within BERT, as seen in Figure 2-2.\nAt each layer, these embeddings incorporate information from all the other tokens\nin the input window, which is what makes these representations contextualized (i.e.\ndependent on the other tokens in the input text). In our work, we choose to extract\nembeddings from the second-to-last layer, as it has been shown that deeper layers\noffer more contextualized embeddings (Ethayarajh, 2019), and the last layer may be\nbiased towards BERT\u2019s pre-training objectives (Baly et al., 2020).\nThough BERT is very useful for producing embeddings for some of our data,\nit is poorly-suited to some of the other data channels we use. For example, some\ndata is better suited to embeddings produced at the sentence-level rather than the\ntoken-level embeddings offered by BERT. In these cases, we use Sentence-BERT\n(SBERT) (Reimers and Gurevych, 2019), a variant of BERT, instead. SBERT is\ntrained using siamese and triplet networks on top of BERT that encourage it to\nproduce semantically-meaningful embeddings at the sentence-level.\nIn our work, we experiment with the a subset of the toolkits used by Baly et al.\nin their 2018 and 2020 works (Baly et al., 2018a, 2020) in order to create a political\nbias and factuality prediction system. We specifically avoid the toolkits and data\nfrom the YouTube and Facebook data channels, as they did not contribute much in\nthe methodology they present. Our system, data, and experiments are described in\ndetail in Chapter 3.\n2.2 Rationalization\nIn addition to building a system for political bias and factuality prediction, we also\nstudy rationalization methods for explaining the predictions produced by models\nrelevant to fake news. We include rationale in our work because we believe that\nwithout explanations on why an article as has been classified a certain way, whether\nit is related to bias or factuality prediction, people are unlikely to believe anything\nopposing their own views (Nyhan and Reifler, 2010). Therefore, in order for an\n26\nautomated bias and factuality prediction system to make a real impact on the spread\nof fake news, we believe that it must offer some level of transparency to its users.\nThougholderlanguage-basedapproachestopoliticalbiasandfactualityprediction\nhave not explicitly studied rationalization for the purpose of presenting it alongside\npredictions, the methods are well-suited for transparency. Methods that use part-of-\nspeech tagging, punctuation and stop-word counts, and measures of syntactic com-\nplexity have found that fake news is simpler and more repetitive (Horne and Adali,\n2017), and can highlight the structural and complexity-related features that are used\nto classify real vs. fake news. Stance-based methods that reason about the factuality\nof an article by comparing its claims to known factual claims (Popat et al., 2018)\ncan provide the stances of the articles and the facts they agree and disagree with.\nLexicon-based approaches have found that fake news tends to contain emotional,\npraising, implicative, and perspective specific language (Recasens and Jurafsky, 2013;\nHorne et al., 2018b). These methods can highlight vocabulary found within their lex-\nicons to offer transparency on their predictions. Finally, n-gram-based models have\nfound that the most highly weighted phrases are those dealing with divisive topics like\n\"trump\" and \"liberals\" and contain dramatic cues like \"breaking\" (Rashkin et al.,\n2017). In order to provide an explanation of their predictions, they can list the most\npredictive n-grams.\nUnlike older language-based methods related to fake news detection, though,\nnewer embedding-based methods, like some we use to build our bias and factual-\nity prediction system, operate as black-boxes and are much less transparent as a\nresult. We aim to increase these models\u2019 transparencies for fake news detection, and\nspecifically focus on explaining predictions from BERT using attention-based ratio-\nnale, and gradient-based rationale. Both the attention and gradient-based methods\nwe use produce extractive rationales , meaning a model\u2019s predictions are explained\nusing a subset of that model\u2019s inputs.\n27\n2.2.1 Attention\nAttention has become a widely used mechanism within the field of natural language\nprocessing due to its ability to model global dependencies better than older, hid-\nden state-based recurrent neural network approaches. Conceptually, it operates by\ncomputing a weighted sum of input representations which allows models to focus\non especially informative inputs, and ignore others, when making predictions (Bah-\ndanau et al., 2015). In addition to improving a range of neural language models\u2019\nperformances, the input weightings that attention computes can be extracted to offer\nsome transparency on the model\u2019s predictions (i.e. which inputs are highly weighted\nwhen making a prediction). Many existing studies have used this property of atten-\ntion to explain and interpret their models, including using it to highlight relevant\nmedical details alongside diagnosis prediction (Mullenbach et al., 2018), to provide\nsentence summaries (Rush et al., 2015), among many other uses (Hermann et al.,\n2015).\nFigure 2-3: A visualization of how BERT\u2019s attention heads highly weight relevant\ntokens for a specific task. In these examples, created using the bertviz tool, the\nattention heads show specializations towards coreference resolution (Vig, 2019).\nEarly attention-based models, which worked in combination with recurrent neural\nnetworks, have since been replaced with models based only on attention. Vaswani\net al. introduced the Transformer, which involves stacked layers consisting of several\nattention mechanisms (also known as attention heads) (Vaswani et al., 2017). The\nmodel we use to study rationalizing predictions for fake news, BERT (Devlin et al.,\n2019), consists of 12 of these Transformer layers, and we analyze the attention heads\nwithin them. We are not the first to do so (though we are not aware of any other\n28\nworks operating in the context of fake news), as other studies have performed ex-\ntensive analysis on how BERT\u2019s attention heads specialize linguistically and behave\ndifferently across layers of the model (Clark et al., 2019). Additionally, tools have\nbeen created purely for the sake of visualizing BERT\u2019s different attention heads and\nincreasing the model\u2019s transparency (Vig, 2019). An example of these visualizations\ncan be seen in Figure 2-3. Although the news articles we use in our rationalization\nstudy are too large to visualize in the same way as Figure 2-3, we create other visu-\nalizations to demonstrate highly attended to regions of articles during classification.\nOur attention-based methods and results are described in Section 4.1.\n2.2.2 Gradient-Based Approaches\nThoughattentionisaplausiblerouteforrationalizingpredictionsrelatedtofakenews,\nthe faithfulness of attention distributions to model predictions is still a debated topic\nin the field (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), which is why we\nalso explore gradient-based approaches as an alternative. In general, gradient-based\nrationalization methods rely on estimating how much a change in a model\u2019s input\nwill impact its output, where inputs that induce the largest changes are thought to\nbe more important in a model\u2019s prediction. There are several different approaches\nto estimating these gradients, but a survey of them by Adebayo et al. (Adebayo\net al., 2018) shows that several of these methods produce rationale that are invariant\nunder model and label randomization (i.e. unrelated to the prediction task), and\nare inadequate as a result. However, pure gradients, involving simply calculating the\ngradient of the output with respect to the input, are shown to provide legitimate\nrationale, and the methods we explore are based on them.\nWe are not aware of other works that explore gradient-based approaches for ra-\ntionalizing predictions in the domain of fake news detection, but these methods have\nbeen used extensively for other tasks, especially in the field of computer vision. Some\nstudies have rationalized digit and object-recognition tasks (Baehrens et al., 2010;\nSimonyan et al., 2014) to show validity in using pure gradients, and others have since\nused them to explain predictions like medical imaging diagnosis (Margeloiu et al.,\n29\nFigure 2-4: An example of pure gradients being used to provide transparency on\nan image classification task. We can observe that the larger gradients (bottom)\ncorrespond to the regions of the original images (top) that contain the object of\ninterest (Simonyan et al., 2014).\n2020). In the field of computer vision, these methods produce nice visual rationale\nfor their predictions, as seen in Figure 2-4, but they are also being used to inter-\npret language models and visualize important linguistic inputs as well - the recently\nreleased Language Interpretability Tool incorporates these methods (Tenney et al.,\n2020). We describe our experiments using pure gradient rationales, and show some\nof our own visualizations in Section 4.2.\n30\nChapter 3\nBias and Factuality Prediction\nIn this chapter, we describe the bias and factuality prediction system that we im-\nplemented using the methodology described by Baly et al. (Baly et al., 2020). We\nfirst explain the data and assumptions that the system relies on, before outlining the\nprocess of training the necessary models for the system, and the experiments we con-\nducted in order to try and best replicate the results stated by Baly et al. (Baly et al.,\n2020). However, when we present our results, we note that our findings do differ, and\nprovide some hypotheses of possible causes. Finally, using our trained models, we\noutline the system\u2019s process of making predictions on unseen data, using only media\nsources\u2019 URL\u2019s as input.\n3.1 Data and Assumptions\nThis system leverages three different channels of data to make its bias and factuality\npredictions: what was written by the media source, who is in the media source\u2019s\naudience, and what was written about the media source. The first channel, what was\nwritten by the media source, refers to articles that the source has published, and how\nit defines itself on social media (if the source has a social media account). While it\nis obvious that the material published by the media source will contain its inherent\nbiases and some level of factuality, we assume that we can find indications of this\nbias and factuality from linguistic features in this data. For the next channel of data,\n31\nwho is in the media source\u2019s audience, we use the followers of the source on social\nmedia. More specifically, we analyze the bios of the media source\u2019s followers with the\nassumption that a source\u2019s followers tend to agree with the views presented by the\nsource, and that the followers may express their own views within their bios. The\nfinal channel of data, what was written about the media source, refers to the media\nsource\u2019s Wikipedia page, if it has one. Here, we assume that Wikipedia presents an\nindependent view of the media source, and that this view may contain comments\nabout the media source\u2019s bias and/or factuality.\nThe methodology that our work is based on (Baly et al., 2020) includes data from\nthe original media source, Twitter, YouTube, Facebook, and Wikipedia. However,\ntheir work showed that the features extracted from YouTube and Facebook, about a\nmedia source, did not have any significant benefit in the system\u2019s predictive ability\nfor both bias and factuality. As a result, we decide to exclude those two data channels\nfrom our experiments, and only use the original media, Twitter, and Wikipedia to\nretrieve the necessary data.\nIn order to train our bias and factuality prediction system given the data chan-\nnels explained above, we also need a dataset of media sources labeled by both bias\nand factuality. Media Bias/Fact Check (MBFC) (Dave Van Zandt, 2015) provides a\ndatabase of media sources manually labeled for both these tasks, from which we ob-\ntained1,737labeledmediasources. Althoughmanualannotationdoesaddanelement\nof subjectivity, we find this problem unavoidable and believe that Media Bias/Fact\nCheck maintains a sufficient level of objectivity through its transparency and correc-\ntion policies. The MBFC database ranks bias on a 7-point scale: extreme-left, left,\nleft-center, center, right-center, right, and extreme-right. Factuality is rated on a\n6-point scale: very-low, low, mixed, mostly-factual, high, and very-high. Like Baly et\nal.\u2019s approach (Baly et al., 2020), we decide to condense both our bias and factuality\nratings to 3-point scales (bias: left, center, right - fact: low, mixed, high) due to\nloosely defined conditions for the additional labels. After retrieving labels for these\nsources, we then aggregated the URL\u2019s of each media source\u2019s homepage, and Twitter\nand Wikipedia pages if they exist. A few examples from our labeled training dataset\n32\nare shown in Table 3.1.\nName Bias Fact URL Twitter Wikipedia\nHuffington Post Left High huffingtonpost.com HuffPost wiki/HuffPost\nCNN Left Mixed cnn.com cnni wiki/CNN\nTrue Activist Left Low trueactivist.com TrueActivist N/A\nForeign Affairs Center High foreignaffairs.com foreignaffairs wiki/Foreign_Affairs\nThe Wrap Center Mixed thewrap.com thewrap wiki/TheWrap\nThe Onion Center Low theonion.com theonion wiki/The_Onion\nAlt News Media Right High altnewsmedia.net AltNewsMedia N/A\nDaily Mail Right Mixed dailymail.co.uk MailOnline wiki/MailOnline\nNews Wars Right Low newswars.com N/A N/A\nTable 3.1: Examples from our dataset including the media sources, their bias and\nfactuality labels, and their corresponding URL\u2019s.\n3.2 Training the System\nFigure 3-1: An outline of the training process for the bias and factuality prediction\nsystem. First, we scrape relevant language data from the URL\u2019s of the media sources\nin our dataset, and fine-tune BERT models to extract features specific to the tasks of\nbias and factuality prediction. Next, we use these fine-tuned BERT models, among\nother pre-built tools, to extract features from the scraped language data. Finally, we\ntrain Support Vector Machines to predict bias and factuality using these extracted\nfeatures.\nGiventhedatadescribedintheprevioussection, wecantrainthenecessarymodels\nfor the bias and factuality prediction system through the process outlined in Figure\n33\n3-1. As an overview of the process, we first scrape news articles, Twitter profiles,\nand Wikipedia pages from the provided media sources\u2019 URL\u2019s. Next, we fine-tune\nBERT models for predicting either bias or factuality from news articles, and use these\nfine-tuned BERT models, among other pre-built toolkits, to extract features from\nour scraped textual data. Finally, we use these extracted features to train Support\nVector Machine (SVM) classifiers for both tasks of bias and factuality prediction. We\ndescribe this process in further detail in the next few sections, and break it down into\nfour main steps which are reflected in Figure 3-1. These steps are: 1) Data scraping\n2) Fine-tuning BERT 3) Feature Extraction 4) Classifier training.\n3.2.1 Data Scraping\nIn Section 3.1, we described the necessary inputs for our bias and factuality prediction\nsystem - the media sources\u2019 URL\u2019s, and optionally the media sources\u2019 Twitter and\nWikipedia URL\u2019s. However, since our system\u2019s models operate on natural language,\nthe first step of our system must be to retrieve forms of natural language from the\ninput URL\u2019s.\nFrom the media sources\u2019 URL\u2019s, we scrape news articles that the source has pub-\nlished using the newspaper3k1library. We further process each scraped article to\ndetermine if they are suitable for predicting bias and factuality. Specifically, we focus\non articles containing political vocabulary, as these tend to be more representative\nof a media source\u2019s bias and factuality levels, and articles that we determine as non-\npolitical are discarded. The other main condition that articles must satisfy is that\nthey are of some minimum length in order to have enough language for our models\nto extract meaningful features from. After we scrape and filter articles from a media\nsource, we then decide if we have obtained enough information to include the me-\ndia source in the training process. We believe it is possible to have a set of articles\nthat are not representative of a media source if we have less than five articles for\n1The newspaper3k library provides functionality to crawl a media source\u2019s website and download\narticles from it that have been cleaned of any formatting and HTML (i.e. condensing information\ndown to natural language only).\n34\nthe source. So, if a media source does not meet this criteria, we remove it from our\ndataset. From the original 1,737 sources obtained from MBFC, we have 1,197 sources\nremaining after filtering. The class distributions amongst this dataset can be seen in\nTable 3.2.\nBias Factuality\nLeft 405Low194\nCenter 363Mixed289\nRight 429High714\nTable 3.2: Our dataset\u2019s distribution of classes for bias and factuality\nAfterfilteringourdataset, wethenmoveontoscrapingtheTwitterandWikipedia\npages for the media sources that have them2. From Twitter, we scrape the media\nsources\u2019 self-written descriptions, as well as other account metadata (e.g. is it verified,\nhow many followers, how many posts, etc.). Additionally, we scrape the bios of the\nmedia sources\u2019 followers. From Wikipedia, we scrape all the textual information\nwritten about the media source. Across the 1,197 sources in our dataset, we obtained\nTwitter data for 82.1% of them (983 sources), and Wikipedia data for 67.5% of them\n(808 sources).\n3.2.2 Fine-tuning BERT\nOnce the necessary data has been retrieved, we move on to preparing the tools that\nwill be used to extract features from this data. Similar to the methodology presented\nby Baly et al., we make use of BERT (Devlin et al., 2019) to extract features from\nthe text we have collected. BERT has achieved state of the art performance across a\nwide range of natural language understanding tasks, and has been shown to produce\npowerful contextualized embeddings of language in deeper layers of the model (Etha-\nyarajh, 2019). For this reason, we believe BERT offers a valuable avenue to featurize\nour text data. Furthermore, we also have the ability to adapt BERT to the tasks of\nbias or factuality prediction, which will provide us with features that are even more\n2We use the python-twitter library for scraping Twitter and the wikipedia library for scraping\nWikipedia. Both libraries are python wrappers around the original organizations\u2019 APIs.\n35\nspecialized to these tasks.\nWe adapt our BERT model to the task of bias or factuality prediction through a\nprocess called fine-tuning. During fine-tuning, we train BERT to predict either the\nbias or factuality of individual news articles (note that fine-tuning is at the article-\nlevel, not the source-level), and since we are interested in both tasks (bias and fac-\ntuality) we fine-tune two different BERT models - one for each task. To fine-tune\nBERT using article-level data, we train the model with an additional linear layer and\nsoftmax layer on top of the [CLS] token that is output from the final transformer as\nseen in Figure 3-2. As input, we feed the first 510 WordPieces (BERT\u2019s tokenization\nmethod) of each article. For the task of bias prediction, we train BERT to classify\neach article with the same three labels we are using for source classification - left,\ncenter, and right. However, for predicting article-level factuality, we train BERT to\nclassify each article simply as high or low factuality - Baly et al. mention that the\nmixed label does not make sense in the context of a single article.\nFigure 3-2: Diagram of the fine-tuning architecture use to adapt BERT to the tasks of\nbias and factuality prediction. We train a linear layer on top of the [CLS] embedding\noutputfromthe12transformerencoderlayersofthepre-trainedBERTmodel(Devlin\net al., 2019).\n36\nIn order for the features produced by the fine-tuned BERT models to generalize\nwell, we must draw the news articles for fine-tuning, and their corresponding labels,\nfrom a dataset external to the sources used for training our final classifiers. There\nare several options available for this dataset, and we find in our experiments that the\nfine-tuningprocesshasasignificantimpactonthesystem\u2019spredictiveability, withthe\nmain variations in the process being a result of our choice of dataset. The datasets we\ntry in our experiments include a partition of our 1,197 collected sources, data from\nthe Hyperpartisan SemEval task (Kiesel et al., 2019), and data from Allsides.com\n(AllSides, 2020). Our labels for the first two options are derived using distant super-\nvision, where articles are assigned the same label as the media source they come from\n(a common method for labeling large news datasets (N\u00f8rregaard et al., 2019)), and\nthe labels for the latter option are on the article-level - Allsides labels the data they\nprovide at the article-level. The resulting differences between using these datasets for\nfine-tuning are described during our experiments in Section 3.3.\n3.2.3 Feature Extraction\nAfter fine-tuning our BERT models, we move on to the feature extraction process,\nwhich transforms the textual data we have scraped into numeric features that can be\nused to train our final SVMs. In addition to the BERT (Devlin et al., 2019) models\nwe have fine-tuned for bias and factuality prediction, we make use of other feature ex-\ntraction toolkits out-of-the-box. These include the News Landscape Toolkit (NELA)\n(Horne et al., 2018b), Global Vectors for Word Representation (GloVe) (Pennington\net al., 2014), and Sentence-Bert (SBERT) (Reimers and Gurevych, 2019). We de-\nscribe each of these toolkits in further detail, as well as the feature extraction process\nfor each type of data, below.\nNews Articles\nFor extracting features from news articles published by a media source, we not only\nmake use of NELA and BERT, used in the work by Baly et al. (Baly et al., 2020)\n37\nthat our methods are based on, but also use GloVe, which is used in a previous work\nby Baly et al. (Baly et al., 2018a). From each article, the NELA toolkit computes\nfeatures relating to structure, sentiment, topic, complexity, bias, and morality. After\ncomputing these for each article, we average the extracted NELA features across all\narticles for a specific media source to move from article-level features to the source-\nlevel (we are interested in predicting bias and factuality of entire media sources).\nTo retrieve features from our BERT models, trained for bias or factuality classi-\nfication, we average the word representations extracted from the second-to-last layer\nof BERT - Baly et al. cite this as common practice as the final layer may be biased\nto BERT\u2019s pre-training objectives. Furthermore, to translate from the article-level to\nthe media source-level, we again average the retrieved embeddings across all articles\nwe have gathered for a specific media source.\nGloVe, like BERT, is a method for extracting embeddings from our news articles.\nHowever GloVe provides static embeddings, whereas BERT\u2019s embeddings are contex-\ntual. Intheirmostrecentwork, Balyetal. statethatusingBERTembeddings instead\nof GloVe embeddings produced a large boost in performance (Baly et al., 2020), but\nwe decide to include the static embeddings from GloVe in our experiments to see if\nthere is complementary information between the two embedding types. Just like the\nprocesses of extracting features using NELA and BERT, GloVe provides embeddings\nat the article-level, which we then average across all articles from a media source.\nTwitter\nWithin our Twitter data, we have two different sets of information: the media source\u2019s\nown Twitter profile, and the bios of the media source\u2019s followers. From the media\nsource\u2019s Twitter profile, we first extract some metadata about the profile including\nif it is verified, its followers and friends count, how many posts have been written\nand favorited, where and when it was created, etc. - these are the only non-linguistic\nfeatures used in our system. Additionally, we embed the bio of the media source\u2019s\naccount using SBERT. Unlike in the case of news articles, there is not enough data\namong the media sources\u2019 bios to fine-tune a BERT model to produce task specific\n38\nembeddings. So instead, we rely on a sentence-level embedding produced by the\npre-trained SBERT model. Regarding the bios of a media source\u2019s followers, we do\nhave enough data to fine-tune BERT, but we run into a different issue. The distant\nsupervision we used to retrieve bias and factuality labels for news articles is ill-suited\nfor classifying Twitter bios - there is much more variance on whether or not a media\nsource\u2019s followers agree with the source\u2019s views. As a result, we again embed these\nbios using SBERT, and average them across a media source\u2019s followers.\nWikipedia\nThe process for extracting features from a media source\u2019s Wikipedia page is identical\ntoembeddingnewsarticlesusingBERT.Wikipediapagestendtohaveasimilarstruc-\nture as the news articles written by the media sources. So, we embed the Wikipedia\npages using the BERT models that are fine-tuned for bias and factuality prediction\nof news articles.\n3.2.4 Classifier Training\nAfterextractingnumericfeaturesfromeachofourdatachannels, wecannowtrainour\ntwoSVMs-onetoclassifymediasources\u2019bias, andtheothertoclassifymediasources\u2019\nfactuality. As input, we concatenate the extracted features from all or a subset of our\ndata channels (described in more detail in Section 3.3), using the features extracted\nfrom BERT fine-tuned for bias prediction for training the bias classifier, and the\nfeatures extracted from BERT fine-tuned for factuality prediction for training the\nfactuality classifier.\nFollowing themethodology fromBaly etal., wetrain andevaluateour SVMs using\n5-fold cross-validation, and perform a grid search over the cost parameter, \ud835\udc36, and the\nkernel coefficient, \ud835\udefe, at each step of cross-validation. Finally, we select the best SVMs\nusing the macro-F1 score - the F1 score averaged across each class, as both the bias\nand factuality datasets are not balanced.\n39\n3.3 Experiments and Results\nDuring development of this system, we try a few variations of our training process,\nand we report our findings on them in this section. Our experiments broadly fit into\ntwo groups. The first experiment we discuss relates to the dataset used in fine-tuning\nour BERT models for feature extraction - we choose to vary this step because our\nBERT models produce the most informative features for bias and factuality predic-\ntion. Our second experiment determines which configuration of features results in\nthe best performing classifier for either bias or factuality prediction, and is helpful\nfor selecting which features to use in our final system.\nDataset Variation for Fine-tuning\nAs mentioned in Section 3.2.2, the datasets we use to fine-tune BERT include a\npartition of our scraped MBFC dataset, the Hyperpartisan SemEval dataset, and the\nAllsides.com dataset. However, for the task of factuality prediction, we are limited\nto using the first two datasets, as Allsides does not provide factuality labels. We\nalso note that for generalization purposes, we exclude any media sources used in\nfine-tuning from the data used to train the final classifiers.\nTo test which dataset was most useful during the fine-tuning process, we fine-tune\nBERT models on each dataset of interest for the task of bias or factuality prediction,\nand then measure how useful each BERT model is on the downstream task of source-\nlevel prediction. Although we can measure the performance of the article-level bias or\nfactuality predictions during fine-tuning, we choose to measure the performance on\nthe downstream SVM classifiers because it directly aligns with our goal of source-level\nbias or factuality prediction. Furthermore, measuring performance at the article-level\nduring fine-tuning may be misleading, as it is possible that BERT may overfit the\narticles and media sources it is fine-tuned on, and not generalize well when extracting\nfeatures for unseen media sources. So, in order to examine the differences between\ndatasets used for fine-tuning, we fine-tune a BERT model on each dataset of interest,\ncarry out feature extraction from articles using each BERT variation, and finally\n40\ntrain the bias or factuality classifier using the extracted features from each model.\nWe report the macro-F1 and accuracy of each variation in Tables 3.3 and 3.4.\nFine-tuning Dataset Macro-F1 Accuracy\nMBFC Partition 74.57 74.53\nHyperpartisan SemEval 76.13 76.15\nAllsides 70.20 70.30\nTable 3.3: Source-level political bias prediction results with features extracted\nfrom articles using BERT models fine-tuned on different datasets.\nTable 3.3 reports the fine-tuning variation results for political bias prediction,\nand we observe that fine-tuning on the Hyperpartisan SemEval dataset significantly\noutperforms the others. We hypothesize that the differences in performance are due\nto the number of different sources present in each fine-tuning set. The work from Baly\net al. reports using \u223c30,000articles from 298 sources for fine-tuning, and we aimed\nto use a similar amount of articles in our fine-tuning processes, but this resulted in\ndiffering numbers of sources. Our dataset variations have the following statistics:\n\u2022MBFC Partition: \u223c30,000articles, \u223c700sources\n\u2022Hyperpartisan SemEval: \u223c27,000articles, \u223c130sources\n\u2022Allsides: \u223c35,000articles, \u223c70sources\nWebelievethattheBERTmodelfine-tunedontheHyperpartisanSemEvaldataset\nis able to extract more generalizable features to the unseen sources at classification\ntime than the Allsides dataset, as there is a larger variety of sources present during\nfine-tuning. The reason we do not observe the same trend between the Hyperpartisan\nSemEval dataset and MBFC partition, is due to the exclusion of fine-tuning sources\nduring classifier training. When it comes to training the SVM for the MBFC parti-\ntion, we hypothesize that excluding these 700 sources from fine-tuning significantly\nreduces the information the classifier receives during training. As a result, the Hyper-\npartisan SemEval dataset strikes a better balance than the other variations between\nfine-tuning a generalizable BERT model, and providing enough sources to train the\nSVM.\n41\nFine-tuning Dataset Macro-F1 Accuracy\nMBFC Partition 60.97 70.94\nHyperpartisan SemEval 61.34 70.99\nTable 3.4: Source-level factuality prediction results with features extracted from\narticles using BERT models fine-tuned on different datasets.\nTable 3.4 reports the fine-tuning variation results for factuality prediction, and\nthough the Hyperpartisan SemEval dataset still performs the best, we observe a\nmuch smaller gap in performance than we did for bias prediction. We also believe\nthe cause of this smaller gap is due to the amount of sources present in the datasets.\nRecall from Section 3.2.2 that we fine-tune BERT to predict either high or low for\narticle-level factuality. As a result, we exclude the sources from our datasets that\nwere labeled as mixed. Though Baly et al. do not report their dataset statistics after\nthis filtering, ours are as follows:\n\u2022MBFC Partition: \u223c23,000articles, \u223c540sources\n\u2022Hyperpartisan SemEval: \u223c18,000articles, \u223c90sources\nWe hypothesize that removing sources from the Hyperpartisan SemEval dataset\nreduces the generalizability of the BERT model fine-tuned on it, but it still ends up\nperforming slightly better than the MBFC partition due to having more sources to\ntrain the SVM.\nFeature Ablation\nTo determine which combination of features extracted from our data channels (ar-\nticles, Twitter, and Wikipedia) results in the best classifiers for bias and factuality,\nwe conduct ablation studies for bias prediction and for factuality prediction (similar\nto Baly et al.). We first examine which features and feature combinations are most\nuseful within each data channel. The combinations of article features are most inter-\nesting to us, as we are curious about the impact of combining the static embeddings\nfrom GloVe and the contextual embeddings from BERT (as mentioned in Section\n3.2.3). After examining the features within each channel, we then combine the best\n42\nperforming features from each data channel to see if they contain complementary\ninformation and result in a better classifier. Finally, we compare the results of our\nclassifiers trained on different feature combinations to the results of the most compa-\nrable configurations presented by Baly et al. - our results differ from theirs and we\ngive hypotheses on possible causes.\nOur Results Baly et al. Results\n#Features Macro-F1 Accuracy Macro-F1 Accuracy\n1Articles: NELA 66.34 66.48 64.82 68.18\n2Articles: BERT 76.13 76.15 79.34 79.75\n3Articles: GloVe 68.56 68.64 N/A N/A\n4Articles: BERT+GloVe 75.45 75.49 N/A N/A\n5Articles: BERT+NELA 74.82 74.84 81.00 81.48\n6Articles: BERT+GloVe+NELA 74.33 74.37 N/A N/A\n7Twitter: Profile 49.93 50.23 59.23 60.88\n8Twitter: Followers 65.55 66.29 62.85 65.39\n9Twitter: Profile+Followers 62.98 63.66 N/A N/A\n10Wikipedia: BERT 53.47 54.37 64.36 66.09\n11A+T: rows 2 & 8 76.38 76.81 84.28 84.87\n12A+W: rows 2 & 10 75.12 75.21 81.53 81.98\n13A+T+W: rows 2, 8 & 10 75.61 75.77 83.53 84.02\nTable3.5: Anablationstudyofourfeatures\u2019predictiveabilityonthetaskof political\nbias prediction . In rows 11-13, we combine the best performing features from each\ndata channel - (A) stands for articles, (T) stands for Twitter, and (W) stands for\nWikipedia. We also include the results from Baly et al. (Baly et al., 2020) for\ncomparison (some of our features behave differently than Baly et al. but we still\nreport their best reported results in rows 11-13).\nTable 3.5 shows the results from our ablation study on political bias prediction\nand the comparable results from Baly et al.. In rows 1-10, we try features and feature\ncombinations within each data channel, and in rows 11-13, we combine the best\nfeature combinations from each channel.\nWithinthefeaturesextractedfromarticles,wefindthatonlyusingBERT-extracted\nfeatures produces the best results (row 2), and that GloVe features, whether by them-\nselves, or in conjunction with BERT, result in worse performance (rows 3-6). This\nleads us to believe that for the task of political bias prediction, GloVe embeddings do\nnot offer any new information on top of what is provided by the BERT embeddings.\nOur findings do agree with Baly et al. that the article data channel provides the\nmost predictive features for political bias. However, we contrastively observe that\n43\nNELA features worsen performance when used in conjunction with BERT (rows 2\nand 5), and that our BERT features significantly underperform their results (row 2).\nWe believe that differences in the dataset used for fine-tuning BERT are the cause of\nboth of these discrepancies. Though we do not know which specific dataset Baly et\nal. used to fine-tune BERT, we do know that they had over 160 additional sources\npresent in their fine-tuning set without significantly reducing the information avail-\nable for training their classifier. The BERT model fine-tuned on their dataset may\nhave generalized better than ours as a result, and could have potentially diverged\nfrom extracting similar information as the NELA toolkit.\nRegarding the Twitter data channel, we find that features from the media sources\u2019\nTwitter followers are much more informative than the media sources\u2019 own Twitter\npages. The reason these features behave differently could be due to a high variance\nwithinTwitterprofiles. Whileeachmediasource\u2019sownprofilemayormaynotexpress\ntheirbiaswithhighvariance, thisvariancebecomesmuchlowerwhenaveragingacross\nall the Twitter profiles of a media source\u2019s followers. If this is the case, it also provides\na potential explanation on why our Twitter profile features underperform Baly et al.\n(row 7) while our Twitter follower features outperform. Baly et al. collected Twitter\ninformation for 72.5% of their media sources, whereas we collected it for 82.1%. The\nadditional high variance information of the media source\u2019s Twitter profiles may have\nfurther hurt performance, while the low variance, averaged media source Twitter\nfollowers, may have helped performance.\nThe only observation regarding our Wikipedia data channel is that it underper-\nforms Baly et al.\u2019s results. Since we use the same fine-tuned BERT model to extract\nfeatures from Wikipedia as we used to extract features from articles, we believe that\nour lower performance is also due to a difference in the dataset used for fine-tuning.\nWhen testing feature combinations across data channels, we always included our\nbest performing article features (row 2), as they were by far the most predictive\nfeatures. WefirsttriedcombiningarticleandTwitterfeatures(row11),whichresulted\nin the best performing classifier of political bias - the same is reported by Baly et al..\nAlthough it is not reflected in our table, Baly et al. also found that classification only\n44\nbenefited from using Twitter follower features and not from media source\u2019s Twitter\nprofile features. The additional feature combinations we tried (rows 12 and 13) follow\nsimilar trends to those reported by Baly et al. and we attribute the performance gaps\nto the causes we state for why individual data channels underperform.\nOur Results Baly et al. Results\n#Features Macro-F1 Accuracy Macro-F1 Accuracy\n1Articles: NELA 53.65 66.20 55.54 62.62\n2Articles: BERT 61.34 70.99 61.46 67.94\n3Articles: GloVe 60.09 70.89 N/A N/A\n4Articles: BERT+GloVe 63.20 73.05 N/A N/A\n5Articles: BERT+NELA 62.22 72.39 59.34 64.82\n6Articles: BERT+GloVe+NELA 63.40 73.05 N/A N/A\n7Twitter: Profile 42.35 61.31 49.96 56.71\n8Twitter: Followers 49.10 64.51 42.19 58.45\n9Twitter: Profile+Followers 55.11 65.73 N/A N/A\n10Wikipedia: BERT 39.87 62.44 45.74 55.32\n11A+T: rows 6 & 9 61.10 71.92 65.45 70.40\n12A+W: rows 6 & 10 61.37 71.92 67.25 71.52\n13A+T+W: rows 6, 9 & 10 62.75 72.77 64.14 69.36\nTable 3.6: An ablation study of our features\u2019 predictive ability on the task of fac-\ntuality prediction . In rows 11-13, we combine the best performing features from\neach data channel - (A) stands for articles, (T) stands for Twitter, and (W) stands\nfor Wikipedia. We also include the results from Baly et al. (Baly et al., 2020) for\ncomparison (some of our features behave differently than Baly et al. but we still\nreport their best reported results in rows 11-13).\nTable 3.6 shows the results from our ablation study on factuality prediction and\nthe comparable results from Baly et al.. Similar to our political bias ablation study,\nwe try features and feature combinations within each data channel in rows 1-10, and\ncombine the best features from each channel in rows 11-13. Since our factuality\ndataset is unbalanced, we make our comparisons between feature combinations using\nmacro-f1 as our performance metric.\nSimilarly to Baly et al. and our political bias prediction results, we also find that\nour news article data channel provides the most informative features for factuality\nprediction. In our study, a combination of all the article features (BERT, GloVe,\nand NELA) produces the best classifier (row 6). Though our features extracted using\nBERT still provide the best individual results (row 2), we observe that both NELA\nand GloVe improve results (rows 4-6), unlike in our political bias prediction study.\n45\nWe believe that this is because our BERT model, fine-tuned for factuality prediction,\nproduces less powerful contextual embeddings than in the bias case - recall that our\nfactuality fine-tuning dataset contains fewer articles and sources than our bias fine-\ntuning dataset. As a result, the NELA features and static, GloVe embeddings are\nable to contribute new information to the classifier. We also note that our finding of\nBERT and NELA features being additive (row 5) contradicts the results from Baly\net al., but we attribute that, as well as our better overall performance using article\nfeatures, to our BERT model learning different features than Baly et al.\u2019s model\nduring fine-tuning.\nOur results regarding the Twitter data channel trend similarly to those we re-\nported in our political bias study, with the media sources\u2019 Twitter profiles being less\ninformative than the media sources\u2019 Twitter followers - we again believe that Twitter\nprofile variance is the cause. We do see, though, that Twitter generally produces less\ninformative features for factuality prediction than it had for bias prediction.\nThe Wikipedia channel significantly underperforms relative to Baly et al.\u2019s results.\nFine-tuning differences could again be the cause, like in the political bias prediction\ncase, but we are more skeptical of this as our results from BERT in the article data\nchannel are comparable to Baly et al. (row 2). Another possible cause for this\nperformance difference could be similar to the what we theorize for the Twitter case.\nSpecifically, how these Wikipedia pages describe media sources may have a high\nvariance in whether or not they offer any predictive information. As a result, different\ncross-validation splits may result in significantly different performance.\nUnlike in Baly et al.\u2019s ablation study, combining features across data channels\nonly hurts performance relative to article-only features in our case, and our best\nperforming classifier only uses a combination of all article features. The differences\nwe cited for Twitter and Wikipedia discrepancies could be the cause.\n46\n3.4 Making Predictions\nAfter going through the process of training the necessary models and selecting the\nmost informative features for the bias and factuality prediction system, we can now\nuse it to predict on unseen sources. Making predictions on new data is very similar to\nthe training process, except instead of fine-tuning new BERT models, or training new\nSVMs, we use the models already trained on our labeled datasets. First, our system\ntakes a media source\u2019s homepage URL, and optionally its Twitter and Wikipedia\nURL\u2019s, as input, and uses these to scrape the relevant natural language data. Next,\nwe use the BERT models fine-tuned for bias and factuality, and the other best-\nperforming toolkits, to extract features from the scraped data. Finally, these features\nare concatenated and fed through the bias or factuality SVM classifiers to obtain the\nfinal predictions.\n47\n48\nChapter 4\nRationalization\nDuring our work in building a system to make source-level political bias and factuality\npredictions, we noted that the field of fake news detection using language is becoming\ndecreasingly transparent because the newer, deep-learning based approaches are more\nobfuscated than their lexicon-based predecessors. However, we believe that model\nexplainability, especially when it comes to fake news detection, may be crucial for\nautomated systems to be seen as credible in the eyes of the public. As a result,\nthough the rationalization methods we explore are not directly linked to the system\nwe describe in Chapter 3, we are hopeful that our analysis will encourage the field of\nfake news detection to maintain its efforts in transparency.\nOur study in rationalization focuses on extractive rationales, or rationales that\nexplain a model\u2019s predictions using selected subsets of the input data. Unlike our\nsystemthatoperatesonthesource-level, wechoosetofocusonarticle-levelpredictions\nfor our rationalization study. We reason that there is too much information being\naggregated at the source-level to offer an extractive rationale that is understandable\nto a user, but at the article-level, we can, much more concretely, highlight specific\nlanguage that contributes the most to the prediction being made.\nFor our article-level predictions, we use a BERT model that is fine-tuned using\nthe same process described in Section 3.2.2. Instead of extracting embeddings from\nthis model, like we did in our source-level system, we simply use the article-level pre-\ndictions from BERT itself. Additionally, since we are now working at the article-level\n49\nfor our predictions, we believe it is important to use article-level data for training our\nmodel. So, we fine-tune our BERT model using the article-level labelled political bias\ndata from Allsides (AllSides, 2020), and ignore the datasets we obtained using distant\nsupervision when training our source-level system. Using Allsides data restricts our\nrationalization study to the task of political bias prediction (Allsides does not contain\narticle-level factuality labels), but we believe that politically biased language lends\nitself better to the qualitative analysis we perform anyways, and note that the same\nextractive rationale methods we explore could be used for factuality prediction, given\nthe proper dataset.\n4.1 Analysis of BERT\u2019s Attention\nWe first explore extractive rationalization through an analysis of the attention heads\nthat compose BERT. Attention heads, intuitively, determine the relative weighting\nof inputs that create the most informative outputs for a downstream task (in this\ncase political bias prediction). So, it is possible to interpolate which input words\nwithin an article are most heavily weighted by each attention head. Our assumption\nis that during fine-tuning, attention heads will learn to highly weight the words within\narticles that are most relevant to the task of political bias prediction, and the most\nheavily weighted words could serve as a viable explanation for the model\u2019s prediction.\nExamples of what these explanations might look like can be seen in Figures 4-1 and\n4-2, where words in dark red have been more heavily weighted by a specific attention\nhead - this article was predicted as right-leaning.\nFor a given article we wish to classify and extract a rationale from, we feed the\nmaximum amount of possible tokens (512)1, starting from the beginning of the article,\ninto BERT. As the input article is run through BERT, each attention head computes\n512 attention weights for each of the 512 tokens in the sequence (a total of 512x512\nweights), and we save these computed weight values at each attention head in BERT.\n1BERT can operate on a maximum of 512 tokens, and since we need to include two special tokens\n([CLS] and [SEP]) at the start and end of our input, we can use 510 tokens from an article.\n50\nAnalyzing 512 weights for each of the 512 tokens in our sequence is infeasible though,\nas this data contains too much information to provide a suitable explanation. How-\never, we believe it is sufficient to only look at the weights of one specific token in our\ninput sequence, the special [CLS] token (1x512 weights). Recall from Section 3.2.2\nthat BERT is fine-tuned for classification by training a fully-connected layer on top\nof the [CLS] embedding representation output from BERT\u2019s transformer encoders.\nSince this [CLS] representation is the only information the final layer in our classifier\nuses to make its prediction, we believe that the attention weights used to calculate the\nrepresentation of the [CLS] token contain the most relevant information in making\npolitical bias predictions.\nThough we have retrieved the attention weights from BERT and reduced them\nto an amount more tractable for analysis, there is still an abundance of informa-\ntion, as we obtain weights from each of BERT\u2019s 144 attention heads (there are 12\nattention heads in each of BERT\u2019s 12 transformer encoder layers). However, other\nworks analyzing BERT\u2019s attention heads have shown that different heads learn to\nspecialize to specific components of syntax (Clark et al., 2019) - some heads focus on\nstructure, co-reference resolution, etc., while others specialize in semantics. Though\nstructure and other syntactical nuances may play a role in BERT\u2019s predictions in\npolitical bias, we focus on finding heads specializing in semantics, as these convey the\nmost human-understandable information.\nWhile our visualizations across all attention heads on articles from different po-\nlitical ideologies is too much information to include, we show some examples of our\nanalysis for an article classified as right-learning by our model. The trends we high-\nlight for this example article hold for different articles we analyzed as well as across\nthe right, center, and left political ideologies. In Figures 4-1 and 4-2, we visualize\nheavily weighted words from attention heads selected from the middle layer and last\nlayer of BERT respectively. We choose to show attention heads from different lay-\ners to compare how different layers of the model behave, and selected these specific\nheads because the words they highly weight are semantically aligned with the criteria\nAllsides uses for labelling right-learning articles. Specifically, defense-oriented words\n51\nFigure 4-1: Attention visualization from 1st attention head in layer 6 on an article\nclassified as right-leaning (this is a truncated version of the full 512 token input fed\ninto BERT). This attention head highly attends to \"president - elect donald trump\"\nand \"defense\" which align with the right-leaning criteria used by Allsides.\nFigure 4-2: Attention visualization from 7th attention head in layer 12 on an article\nclassified as right-leaning (this is a truncated version of the full 512 token input fed\ninto BERT). This attention head highly attends to \"president\", \"marine\", \"state\",\nand \"economy\" which align with the right-leaning criteria used by Allsides.\n52\nlike marine and defense are highly weighted, as well as the name of the right-learning\npresidential elect (at the time), Donald Trump. However, between the different layers\nof the model, we observe a significant difference in which words are heavily weighted.\nIn fact, even when comparing semantically-oriented attention heads within the same\nlayer we see a similar gap between which words are weighted. Figure 4-3 shows the\n7 highest weighted tokens from each attention head in the last layer, and it shows\nthat there is a high variation between which words are highly attended to, and the\nmagnitude of those weightings.\nFigure 4-3: The top 7 weighted tokens, and their magnitudes, from each attention\nhead in the final layer of BERT. Between attention heads we see a large amount of\nvariation besides a few common words like \"about\", \"reform\", and \"state\".\nDue to the variance we see between attention heads, we believe that selecting\nspecific semantically-oriented attention heads from BERT does not serve as a good\nexplanation for the model\u2019s prediction. However, we still believe there may be some\nmeritinanalyzingBERT\u2019sattentionforextractiverationales. If, insteadofcomparing\nattention weightings between specific heads, we use a more global view of attention\non the [CLS] token (across all heads and layers), we can see some trends emerge. In\nFigure 4-4, we plot all 144 attention head\u2019s weights for the [CLS] token, and observe\nthat later layers of the model do attend to the same tokens - this is shown by the hor-\nizontal streaks. Though individual attention heads may attend to different subgroups\n53\nof these tokens, which is potentially why we observe large differences when comparing\nindividual attention heads, there seem to be some agreed upon important tokens in\naggregate. Furthermore, we confirm that these agreed upon tokens for this example\n(trump, marine, conservative, etc.) are aligned with the right-learning criteria speci-\nfied by Allsides, and we believe that more studies on the aggregate attention of BERT\ncould be useful in determining a more robust method for extractive rationalization\nthan analyzing individual attention heads.\nFigure 4-4: [CLS] token\u2019s attention weights across all 144 attention heads within\nBERT. In later layers, we observe some horizontal streaks, indicating that the same\ntokens are being heavily weighted across different attention heads.\n54\nWhile our study shows using an analysis on attention has potential for extractive\nrationales, there are some drawbacks to the methodology as well. Not only are indi-\nvidual attention heads poor avenues for rationalization because of their variance and\nthe work required to filter through 144 different attention heads, but there is also no\nguarantee that the same attention heads will be useful between different articles. As\na result, we mentioned that viewing BERT\u2019s attention heads in aggregate could help\nalleviate these issues. However, even in aggregate, it is possible the attention may\nnot be suitable for extracting rationale, as the relationship between attention weights\nand model predictions is still a debated topic. Jain and Wallace report that attention\nweights are not correlated with other measures of input importance and that per-\nturbing attention weights does not significantly alter predictions (Jain and Wallace,\n2019). However, Wiegreffe and Pinter refute these claims in their own experiments,\nand though they do not confirm that attention is useful for explanation, they show\nthat the experiments by Jain and Wallace do not disprove its use (Wiegreffe and\nPinter, 2019). So, until more research on attention\u2019s faithfulness to model predictions\nis conducted, other extractive rationalization methods are more preferable.\n4.2 Gradient-Based Methods\nThe other methodology we explore for extractive rationalization, gradient saliency,\navoids the drawbacks of attention-based approaches and is a more direct measure\nof input importance. To quantify gradient saliency, we use two different methods,\nboth based on calculating the gradients of our model\u2019s outputs with respect to its\ninputs, and then processing these gradients to retrieve a single importance score per\ntoken. The computed importance scores represent how much changing each token\nimpactsthemodel\u2019soutput, withtheintuitionbehindusingthesescoresforextractive\nrationalization being that tokens which can cause large changes in a model\u2019s output\nmust be important in making its prediction. An additional advantage for gradient-\nbased approaches is that they provide a single measure of how important each input\ntoken is, as opposed to the many channels of attention weights that need to be\n55\nanalyzed in attention-based approaches.\nWe will refer to the two highly-related, gradient saliency methods we explore for\nrationalizing political bias predictions as the gradient norm andembed \u00b7gradient.\nIn order to calculate importance scores using either approach, we must define which\nmodel outputs we calculate the gradient of, and which input representations we cal-\nculate gradients with respect to - we use the same output and input definitions for\nboth approaches. For rationalization we choose to measure our output using the\nmaximum value of the unnormalized class probabilities (also known as logits) output\nfrom our BERT classifier. Though it is common to use loss as a measure of output\nwhen calculating gradients (as we do during fine-tuning BERT), we believe calculat-\ning gradients of the maximum logit with respect to the model\u2019s input is more useful\nfor rationalization. Whereas the gradient of the loss measures how changing inputs\ncanimprove the model\u2019s performance, the gradient of the maximum logit measures\nhow much each input changes a model\u2019s confidence in its current prediction. As a re-\nsult, we believe using the maximum logit as our output measure is more aligned than\nusing the loss for rationalizing the model\u2019s current prediction, whether it is correct\nor not. Contrary to our output definition, there is only one option for the inputs we\ncalculate gradients with respect to that both faithfully represents the input tokens\nand allows for differentiability - the embedded representations of each token in the\narticle currently being classified.\nTo calculate the gradients of the maximum logit with respect to the input embed-\ndings for a sequence of tokens, \ud835\udc65(sequence length x 1), we first embed each token\nin\ud835\udc65to a 768-dimensional space (sequence length x 768). Our embedded sequence is\nthen run through the fine-tuned BERT model to compute the logits, and, finally we\ncalculate the gradients of the maximum logit with respect to the input embeddings\nusing back-propagation. However, because our embedding space is 768-dimensional,\neach token from \ud835\udc65will have an associated 768 gradients - one for each embedding\ndimension. Since we want a single importance score per token, we must condense\nthese gradients into one representative value, and our two methods differ in how they\ncompute this overall score. The gradient norm method uses the euclidean norm of\n56\nthe gradients of the max logit with respect to the input embeddings, measuring the\noverall magnitude of the gradients for each token. For the embed \u00b7gradient method,\nwe reason that gradients should contribute more towards importance along the em-\nbedding dimensions of tokens which carry the most information. So, this method\nuses the dot product of our gradients with their respective embeddings. We formalize\nboth these calculations below, where \ud835\udc38\ud835\udc5a\ud835\udc4f (\ud835\udc65)is the embedded representation of our\ninput sequence \ud835\udc65, and\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc38\ud835\udc5a\ud835\udc4f (\ud835\udc65)are the gradients of the max logit with respect to the\ninput embeddings.\nGradient Norm: Gradient norm =\u20e6\u20e6\u20e6\u20e6\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc38\ud835\udc5a\ud835\udc4f (\ud835\udc65)\u20e6\u20e6\u20e6\u20e6\nEmbed \u00b7Gradient: Embed \u00b7gradient =\ud835\udc38\ud835\udc5a\ud835\udc4f (\ud835\udc65)\u00b7\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc38\ud835\udc5a\ud835\udc4f (\ud835\udc65)\nFigure 4-5: The top 7 most important tokens, and their importance scores, using\nboth of our gradient-based methods. Both methods output very similar results.\nIn Figure 4-5, we compare the top 7 most important tokens, and their magnitudes,\nas computed by our two different methods. We observe that they are highly similar in\nthe words they deem important, and only have slight differences in magnitudes. Fur-\nthermore, when visualizing the importance weighting of words within articles we also\nsee a similarity between the two methods. Figure 4-6 shows the importance weighting\nfor an article using the embed \u00b7gradient method, and the highlighted words align well\nwith Allsides\u2019 criteria for a right-leaning article, showing that defense is important,\nrejecting public funding for healthcare, and discussing right-learning president-elect\nat the time, Donald Trump. However, even though these gradient-based methods\nseem like a useful strategy for extractive rationalization from a qualitative observa-\ntion, further studies confirming their validity are still needed. A next step confirming\n57\nthese results, for example, could be training a simple Support Vector Machine clas-\nsifier on top of only the words that were highlighted as important by these methods,\nand confirming that the classifier still operates with comparable accuracy.\nFigure 4-6: Gradient-based importance visualization, using embed \u00b7gradient method,\non an article classified as right-leaning (this is a truncated version of the full 512 token\ninput fed into BERT). This method finds \"donald trump\" and \"repealing obamacare\"\nas particularly important phrases in classifying this article as right-learning.\n4.3 Shortcomings of Extractive Rationales\nEven though the extractive rationales in some of our studies were qualitatively good,\nand even if their validity is further verified, there are still some inherent limitations\nto using extractive rationales, as well as issues in our own study. While extractive\nrationales do increase transparency into why models make certain predictions, there is\nno guarantee that the information which models are using follows the same reasoning\nthat humans use and, as a result, the extracted rationales may be incomprehensible\nor have different meanings than expected (e.g. exploiting grammatical tendencies, or\ncorrelations unrelated to the prediction task). In fact, we believe this is likely what\nhappened in our own study. Though extractive rationales appeared to be reasonable\nexplanations for our model\u2019s predictions, other experiments of ours showed that dur-\ning article-level fine-tuning, BERT was not actually learning predictive language for\npolitical bias, but was somehow memorizing the media sources which published the\narticles.\n58\nMacro-F1 Accuracy\nRandom Split 88.78 88.09\nMedia Split 32.64 36.45\nTable 4.1: Random vs. Media split article-level classification. In the random split\nexperiment, articles were randomly selected across all media sources for the train,\ndevelopment, and test splits. For the media split, we ensured that articles were\nselected from disjoint sets of media sources for the train, development, and test splits.\nWe realized this issue in our study when fine-tuning two different versions of\nour BERT model, one where the articles in the train, development, and test sets were\nrandomlyselectedacrossallmediasources(RandomSplit), andanotherwherearticles\nwere selected from disjoint sets of media sources between the train, development, and\ntest sets (Media Split). Table 4.1 shows the large gap in performance between the\ntwo models, where the model fine-tuned on the media source partition is essentially\nrandomly guessing2. Through further analysis, we found that named entities, which\nwe masked out of articles using named-entity recognition (we wanted to ensure media\nsources did not include their own names within articles for this very reason), were\nselected as highly important tokens using our methods. Out of the articles that\ncontained some masked named-entity, we found that about 61% contained this mask\nwithin the top 10 most important tokens in their extracted rationales (15,799 out of\n25,718 articles). Though we attempted to mask out the named-entities within our\narticles, wesuspectthateithertherewassomeleakageinthenamed-entityrecognition\nor the model learned to associate writing styles with specific media sources, and was\nrelying on this to make predictions. As a result, we need to test our extractive\nrationale methods on other, proven methodologies in the future in order to see how\nthey perform on a robust model.\n2Note: Using BERT to extract features during our source-level study (Chapter 3) did not appear\nto have this issue, as we excluded any media sources used during fine-tuning from our classification\nstep, and the classifiers still performed well.\n59\n60\nChapter 5\nConclusion\nThe general goal of our work is to study potential methods for preventing the spread\nof fake news. Specifically, we focus on building a system to predict the political bias\nand factuality of media, as we believe these two properties are crucially linked to the\nproblem of fake news. Several previous works address bias and factuality prediction\nat smaller scopes like the claim and article-levels, but we center our work around the\nsource-level because it is relatively understudied, useful in predicting at the smaller\nscopes, and can be used for detecting likely fake news in the instant it is published.\nFurthermore, thesystemwedevelopmakesitspredictionsbasedonlinguisticfeatures,\nas we believe the viral and damaging power of fake news is derived from its language,\nand it is the most robust predictor as a result.\nThe methodology for the source-level political bias and factuality prediction sys-\ntem we build is based on work done by Baly et al. (Baly et al., 2020), and it operates\non data from the media sources\u2019 websites, and optionally the media sources\u2019 Twitter\nand Wikipedia pages. During development of the system, we experiment with train-\ning the necessary models on different subsets of these data sources, and compare our\nresults to a similar ablation study done by Baly et al.. Similarly to their findings,\nwe observe that articles published by a media source produce the most important\nfeatures for predicting both bias and factuality. However, our system tends to under-\nperform the one described by Baly et al., and differs in which data channels are useful\nfor predictions. We hypothesize that these discrepancies are a result of differences\n61\nbetween the datasets used for training the system\u2019s models, and believe that our re-\nsults could be improved by using a dataset containing more labeled media sources.\nNevertheless, our system obtains adequate accuracies of 76.81% and 73.05% for the\ntasks of political bias and factuality prediction respectively, with news articles and\nTwitter data being the most predictive input combination for the bias task, and only\narticle data being the most informative for the factuality task.\nIn addition to our development of a system for predicting political bias and fac-\ntuality, we also study how different rationalization methods can be used to provide\ntransparency in automated fake news detection methods. We believe it is unlikely\nfor people to blindly trust a model\u2019s predictions on such a polarizing subject, and\nthat presenting a model\u2019s reasoning is a critical step for these systems establishing\ncredibility with the public. The methods we study focus on rationalizing a model\u2019s\npredictions on an article-level, political bias task, as politically-weighted language is\nwell-defined for the qualitative analysis we perform. Using attention and gradient-\nbased approaches, we highlight subsets of a model\u2019s input that are deemed important\nin the prediction it makes. During initial observations, the rationales we extract seem\nreasonably aligned with left-leaning and right-leaning political ideologies, and both\nthe attention and gradient-based methods appear to produce simple, understandable\nexplanations of a model\u2019s predictions. However, further analysis shows significant is-\nsues in our article-level bias prediction model which make us skeptical about believing\nthe extracted explanations, and we believe further research is necessary, as a result,\nto prove the validity of the methods we use.\n5.1 Future Work\nIn our study, we focus on using language to predict bias and factuality, but we believe\nthat combining our methods with graph and propagation-based approaches could\nfurther augment our system. Additionally, as we have mentioned, we believe that\nour system can be improved by providing more labeled media sources for training\nits models, and, with this in mind, we design our system to be easily retrained on\n62\nnew data. Though there are likely a vast number of other avenues to explore for\nimproving our system\u2019s performance, we believe that studying model explainability\nin the domain of fake news is the most important next step for the field.\nOur rationalization study concentrates on extractive rationale methods, but a new\nfield of research, natural language explanation, could potentially provide more under-\nstandable explanations behind a model\u2019s predictions. The field of natural language\nexplanation aims to model human-like reasoning alongside a prediction task, and this\nusually takes the form of generating a textual explanation which is later used to make\na prediction. With the future goal of applying natural language explanation to fake\nnews detection, we experimented with our own approach to producing explanations\non a question answering task.\nFigure 5-1: Our model architecture for generating natural language explanations\nduring a question answering task. The explainer component takes the question and\nanswer choices as input, and generates an explanation using gumbel-softmax to sam-\nple. This explanation is then used by the predictor to select the proper answer choice\nto answer the question.\nFigure 5-1 shows the architecture of the model we developed for generating expla-\n63\nnations during a question answering task1. It consists of an explanation component\nwhich leverages a language model to generate human-like explanations, and a pre-\ndiction component that uses these explanations to answer questions. We maintain\ndifferentiability through the sampling process of generating an explanation by using\nthe gumbel-softmax (Jang et al., 2017) to allow for task-specific loss to propagate\ninformation to the explanation component. Though our model uses a supervised\napproach to generating its explanations, natural language generation methods are\nincreasingly being refined (Latcinnik and Berant, 2020; Wiegreffe et al., 2020), and\nour hope is unsupervised methods will be available in the future and can be applied\nto fake news detection.\n1We trained this model on the Common Sense Explanations dataset, which extends the Common\nSense Question Answering dataset with human annotated explanations (Rajani et al., 2019).\n64\nBibliography\nJ. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and B. Kim. Sanity\nchecks for saliency maps. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems 31 , pages 9505\u20139515. Curran Associates, Inc., 2018. URL\nhttp://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf .\nL. Akoglu, M. McGlohon, and C. Faloutsos. Oddball: Spotting anomalies in weighted\ngraphs. In Advances in Knowledge Discovery and Data Mining , pages 410\u2013421, 07\n2010. ISBN 978-3-642-13671-9. doi: 10.1007/978-3-642-13672-6_40.\nJ. Alammar. The illustrated transformer [blog post].\nhttps://jalammar.github.io/illustrated-transformer, 2018.\nH. Allcott and M. Gentzkow. Social media and fake news in the 2016 election. Journal\nof Economic Perspectives , 31:211\u2013236, 05 2017. doi: 10.1257/jep.31.2.211.\nAllSides. AllSides Blind Bias Survey. https://www.allsides.com, 2020.\nD. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, and K.-R.\nM\u00fcller. How to explain individual classification decisions. J. Mach. Learn. Res. ,\n11:1803\u20131831, Aug. 2010. ISSN 1532-4435.\nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning\nto align and translate. In ICLR, 2015.\nR. Baly, G. Karadzhov, D. Alexandrov, J. Glass, and P. Nakov. Predicting factuality\nof reporting and bias of news media sources. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing , pages 3528\u20133539, Brus-\nsels, Belgium, Oct.-Nov. 2018a. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1389. URL https://www.aclweb.org/anthology/D18-1389 .\nR. Baly, M. Mohtarami, J. Glass, L. M\u00e0rquez, A. Moschitti, and P. Nakov.\nIntegrating stance detection and fact checking in a unified corpus. In Pro-\nceedings of the 2018 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies, Vol-\nume 2 (Short Papers) , pages 21\u201327, New Orleans, Louisiana, June 2018b. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/N18-2004. URL\nhttps://www.aclweb.org/anthology/N18-2004 .\n65\nR. Baly, G. Karadzhov, J. An, H. Kwak, Y. Dinkov, A. Ali, J. Glass, and P. Nakov.\nWhat was written vs. who read it: News media profiling using text analysis and\nsocial media context. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , ACL \u201920, 2020.\nA. Beutel, W. Xu, V. Guruswami, C. Palow, and C. Faloutsos. Copycatch: Stopping\ngroup attacks by spotting lockstep behavior in social networks. In Proceedings of\nthe 22nd International Conference on World Wide Web , pages 119\u2013130, 05 2013.\ndoi: 10.1145/2488388.2488400.\nA. Chakraborty, B. Paranjape, S. Kakarla, and N. Ganguly. Stop clickbait: Detecting\nand preventing clickbaits in online news media. 2016 IEEE/ACM International\nConference on Advances in Social Networks Analysis and Mining (ASONAM) ,\npages 9\u201316, 2016.\nK.Clark, U.Khandelwal, O.Levy, andC.Manning. Whatdoesbertlookat? ananal-\nysis of bert\u2019s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP , pages 276\u2013286, 01 2019. doi:\n10.18653/v1/W19-4828.\nDave Van Zandt. Media Bias/Fact Check. https://mediabiasfactcheck.com, 2015.\nC. Davis, O. Varol, E. Ferrara, A. Flammini, and F. Menczer. Botornot: A system to\nevaluate social bots. In Proceedings of the 25th International Conference Compan-\nion on World Wide Web , pages 273\u2013274, 04 2016. doi: 10.1145/2872518.2889302.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT , 2019.\nS. Dungs, A. Aker, N. Fuhr, and K. Bontcheva. Can rumour stance\nalone predict veracity? In Proceedings of the 27th International Confer-\nence on Computational Linguistics , pages 3360\u20133370, Santa Fe, New Mex-\nico, USA, Aug. 2018. Association for Computational Linguistics. URL\nhttps://www.aclweb.org/anthology/C18-1284 .\nE. Elejalde, L. Ferres, and E. Herder. On the nature of real and perceived bias in the\nmain-stream media. PloS one , 13(3):e0193765, 03 2018.\nK. Ethayarajh. How contextual are contextualized word representations? comparing\nthe geometry of bert, elmo, and gpt-2 embeddings. In EMNLP, 2019.\nJ.Graham, J.Haidt, andB.A.Nosek. Liberalsandconservativesrelyondifferentsets\nof moral foundations. Journal of personality and social psychology , 96 5:1029\u201346,\n2009.\nK. M. Hermann, T. Ko\u010disk\u00fd, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,\nand P. Blunsom. Teaching machines to read and comprehend. In Proceedings of the\n28th International Conference on Neural Information Processing Systems - Volume\n1, NIPS\u201915, page 1693\u20131701, Cambridge, MA, USA, 2015. MIT Press.\n66\nB. Horne and S. Adali. This just in: Fake news packs a lot in title, uses simpler,\nrepetitive content in text body, more similar to satire than real news. In Pro-\nceedings of the International AAAI Conference on Web and Social Media , volume\nabs/1703.09398, 2017.\nB. D. Horne, W. Dron, S. Khedr, and S. Adali. Assessing the news landscape: A\nmulti-module toolkit for evaluating the credibility of news. In Companion Pro-\nceedings of the The Web Conference 2018 , WWW \u201918, page 235\u2013238, Republic\nand Canton of Geneva, CHE, 2018a. International World Wide Web Conferences\nSteering Committee. ISBN 9781450356404. doi: 10.1145/3184558.3186987. URL\nhttps://doi.org/10.1145/3184558.3186987 .\nB. D. Horne, W. Dron, S. Khedr, and S. Adali. Sampling the news producers: A\nlarge news and feature data set for the study of the complex media landscape. In\nICWSM, 2018b.\nC. Hutto and E. Gilbert. Vader: A parsimonious rule-based model for sentiment\nanalysis of social media text. In Proceedings of the 8th International Conference\non Weblogs and Social Media, ICWSM 2014 , 01 2015.\nS. Iyengar and K. S. Hahn. Red media, blue media: Evidence of ideo-\nlogical selectivity in media use. Journal of Communication , 59(1):19\u2013\n39, 2009. doi: https://doi.org/10.1111/j.1460-2466.2008.01402.x. URL\nhttps://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-2466.2008.01402.x .\nS. Jain and B. C. Wallace. Attention is not explanation. In NAACL-HLT , 2019.\nE. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax.\nInICLR, 2017.\nG. Karadzhov, P. Atanasova, P. Nakov, and I. Koychev. We built a fake news &\nclick-bait filter: What happened next will blow your mind! In Proceedings of the\nInternational Conference on Recet Advances in Natural Language Processing , pages\n334\u2013343. RANLP, 11 2017. doi: 10.26615/978-954-452-049-6_045.\nJ. Kiesel, M. Mestre, R. Shukla, E. Vincent, P. Adineh, D. Corney, B. Stein,\nand M. Potthast. SemEval-2019 task 4: Hyperpartisan news detec-\ntion. In Proceedings of the 13th International Workshop on Semantic Eval-\nuation, pages 829\u2013839, Minneapolis, Minnesota, USA, June 2019. Associ-\nation for Computational Linguistics. doi: 10.18653/v1/S19-2145. URL\nhttps://www.aclweb.org/anthology/S19-2145 .\nE. Kochkina, M. Liakata, and A. Zubiaga. All-in-one: Multi-task learning\nfor rumour verification. In Proceedings of the 27th International Confer-\nence on Computational Linguistics , pages 3402\u20133413, Santa Fe, New Mex-\nico, USA, Aug. 2018. Association for Computational Linguistics. URL\nhttps://www.aclweb.org/anthology/C18-1288 .\n67\nV. Kulkarni, J. Ye, S. Skiena, and W. Y. Wang. Multi-view models for political ideol-\nogy detection of news articles. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing ,pages3518\u20133527,Brussels,Belgium,Oct.-\nNov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1388.\nURL https://www.aclweb.org/anthology/D18-1388 .\nS. Kumar and N. Shah. False information on web and social media: A survey. Social\nMedia Analytics: Advances and Applications , 04 2018.\nS. Kumar, R. West, and J. Leskovec. Disinformation on the web: Impact, char-\nacteristics, and detection of wikipedia hoaxes. In Proceedings of the 25th Inter-\nnational Conference on World Wide Web , 04 2016. ISBN 9781450341431. doi:\n10.1145/2872427.2883085.\nV. Latcinnik and J. Berant. Explaining question answering models through text\ngeneration, 2020.\nY. Lin, J. Hoover, G. Portillo-Wightman, C. Park, M. Dehghani, and H. Ji. Acquiring\nbackground knowledge to improve moral value prediction. In 2018 IEEE/ACM\nInternational Conference on Advances in Social Networks Analysis and Mining\n(ASONAM) , pages 552\u2013559, 2018. doi: 10.1109/ASONAM.2018.8508244.\nA. Margeloiu, N. Simidjievski, M. Jamnik, and A. Weller. Improving interpretability\nin medical imaging diagnosis using adversarial training, 2020.\nL. Mitchell, M. Frank, K. Harris, P. Dodds, and C. Danforth. The geography of\nhappiness: Connecting twitter sentiment and expression, demographics, and ob-\njective characteristics of place. PloS one , 8:e64417, 05 2013. doi: 10.1371/jour-\nnal.pone.0064417.\nS. Mukherjee and G. Weikum. Leveraging joint interactions for credibility analysis\nin news communities. In Proceedings of the 24th ACM International Conference\non Information and Knowledge Management , CIKM \u201915, page 353\u2013362, New York,\nNY, USA, 2015. Association for Computing Machinery. ISBN 9781450337946. doi:\n10.1145/2806416.2806537. URL https://doi.org/10.1145/2806416.2806537 .\nJ. Mullenbach, S. Wiegreffe, J. Duke, J. Sun, and J. Eisenstein. Explainable pre-\ndiction of medical codes from clinical text. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Papers) , pages 1101\u20131111, New\nOrleans, Louisiana, June 2018. Association for Computational Linguistics. doi:\n10.18653/v1/N18-1100. URL https://www.aclweb.org/anthology/N18-1100 .\nN. Nguyen, G. Yan, M. Thai, and S. Eidenbenz. Containment of misinformation\nspread in online social networks. In Proceedings of the 3rd Annual ACM Web\nScience Conference , pages 213\u2013222, 06 2012. doi: 10.1145/2380718.2380746.\n68\nR.S.Nickerson. Confirmationbias: Aubiquitousphenomenoninmanyguises. Review\nof General Psychology , 2(2):175\u2013220, 1998. doi: 10.1037/1089-2680.2.2.175. URL\nhttps://doi.org/10.1037/1089-2680.2.2.175 .\nA. C. Nied, L. Stewart, E. Spiro, and K. Starbird. Alternative narratives of\ncrisis events: Communities and social botnets engaged on social media. In\nProceedings of the 2017 ACM Conference on Computer Supported Cooperative\nWork and Social Computing Companion , page 263\u2013266, New York, NY, USA,\n2017. ACM. ISBN 9781450346887. doi: 10.1145/3022198.3026307. URL\nhttps://doi.org/10.1145/3022198.3026307 .\nJ. N\u00f8rregaard, B. Horne, and S. Adali. Nela-gt-2018: A large multi-labelled news\ndataset for the study of misinformation in news articles. In ICWSM, 2019.\nB. Nyhan and J. Reifler. When corrections fail: The persistence of political misper-\nceptions. Political Behavior , 32:303\u2013330, 06 2010. doi: 10.1007/s11109-010-9112-2.\nB. Pang and L. Lee. A sentimental education: Sentiment analysis using subjectivity\nsummarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting\non Association for Computational Linguistics , ACL \u201904, page 271\u2013es, USA, 2004.\nAssociation for Computational Linguistics. doi: 10.3115/1218955.1218990. URL\nhttps://doi.org/10.3115/1218955.1218990 .\nJ. Pennington, R. Socher, and C. Manning. GloVe: Global vectors for word repre-\nsentation. In Proceedings of the 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1532\u20131543, Doha, Qatar, Oct. 2014.\nAssociation for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL\nhttps://www.aclweb.org/anthology/D14-1162 .\nPolitiFact. Politifact. https://www.politifact.com/, 2007.\nK. Popat, S. Mukherjee, J. Str\u00f6tgen, and G. Weikum. Credeye: A credibility\nlens for analyzing and explaining misinformation. In Companion Proceedings of\nthe The Web Conference 2018 , WWW \u201918, page 155\u2013158, Republic and Can-\nton of Geneva, CHE, 2018. International World Wide Web Conferences Steer-\ning Committee. ISBN 9781450356404. doi: 10.1145/3184558.3186967. URL\nhttps://doi.org/10.1145/3184558.3186967 .\nM. Potthast, J. Kiesel, K. Reinartz, J. Bevendorff, and B. Stein. A sty-\nlometric inquiry into hyperpartisan and fake news. In Proceedings of the\n56th Annual Meeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 231\u2013240, Melbourne, Australia, July 2018. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/P18-1022. URL\nhttps://www.aclweb.org/anthology/P18-1022 .\nN. Rajani, B. McCann, C. Xiong, and R. Socher. Explain yourself! leveraging lan-\nguage models for commonsense reasoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics , pages 4932\u20134942, 01 2019.\ndoi: 10.18653/v1/P19-1487.\n69\nH. Rashkin, E. Choi, J. Y. Jang, S. Volkova, and Y. Choi. Truth of vary-\ning shades: Analyzing language in fake news and political fact-checking. In\nProceedings of the 2017 Conference on Empirical Methods in Natural Lan-\nguage Processing , pages 2931\u20132937, Copenhagen, Denmark, Sept. 2017. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/D17-1317. URL\nhttps://www.aclweb.org/anthology/D17-1317 .\nM. Recasens and D. Jurafsky. Linguistic models for analyzing and detecting biased\nlanguage. In ACL 2013 - 51st Annual Meeting of the Association for Computational\nLinguistics, Proceedings of the Conference , volume 1, 01 2013.\nN. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference in Empirical Methods in Natural\nLanguage Processing , pages 3973\u20133983, 01 2019. doi: 10.18653/v1/D19-1410.\nA. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstrac-\ntive sentence summarization. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , pages 379\u2013389, Lisbon, Portugal, Sept.\n2015.AssociationforComputationalLinguistics. doi: 10.18653/v1/D15-1044. URL\nhttps://www.aclweb.org/anthology/D15-1044 .\nA. Saleh, R. Baly, A. Barr\u00f3n-Cede\u00f1o, G. Da San Martino, M. Mohtarami, P. Nakov,\nandJ.Glass. TeamQCRI-MITatSemEval-2019task4: Propagandaanalysismeets\nhyperpartisan news detection. In Proceedings of the 13th International Workshop\non Semantic Evaluation , pages 1041\u20131046, Minneapolis, Minnesota, USA, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/S19-2182. URL\nhttps://www.aclweb.org/anthology/S19-2182 .\nK. Shu, A. Sliva, S. Wang, J. Tang, and H. Liu. Fake news detection on\nsocial media: A data mining perspective. SIGKDD Expor. Newsl. , 19(1):\n22\u201336, Sept. 2017. ISSN 1931-0145. doi: 10.1145/3137597.3137600. URL\nhttps://doi.org/10.1145/3137597.3137600 .\nK. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks:\nVisualising image classification models and saliency maps. CoRR, abs/1312.6034,\n2014.\nI. Tenney, J. Wexler, J. Bastings, T. Bolukbasi, A. Coenen, S. Gehrmann, E. Jiang,\nM. Pushkarna, C. Radebaugh, E. Reif, and A. Yuan. The language interpretability\ntool: Extensible, interactivevisualizationsandanalysisforNLPmodels, 2020. URL\nhttps://www.aclweb.org/anthology/2020.emnlp-demos.15 .\nJ. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal. FEVER: a large-scale\ndataset for fact extraction and VERification. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 809\u2013819,\nNewOrleans, Louisiana, June2018.AssociationforComputationalLinguistics. doi:\n10.18653/v1/N18-1074. URL https://www.aclweb.org/anthology/N18-1074 .\n70\nR.M.Tripathy, A.Bagchi, andS.Mehta. Astudyofrumorcontrolstrategiesonsocial\nnetworks. In Proceedings of the 19th ACM International Conference on Information\nand Knowledge Management . ACM, 2010.\nJ. Uscinski, C. Klofstad, and M. Atkinson. What drives conspiratorial beliefs? the\nrole of informational cues and predispositions. Political Research Quarterly , 69, 01\n2016. doi: 10.1177/1065912915621621.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\nand I. Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems , pages 6000\u20136010, 2017.\nJ. Vig. A multiscale visualization of attention in the transformer model. In Pro-\nceedings of the 57th Annual Meeting of the Association for Computational Lin-\nguistics: System Demonstrations , pages 37\u201342, Florence, Italy, July 2019. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/P19-3007. URL\nhttps://www.aclweb.org/anthology/P19-3007 .\nS. Vosoughi, D. Roy, and S. Aral. The spread of true and false news online. Science,\n359:1146\u20131151, 03 2018. doi: 10.1126/science.aap9559.\nS. Wiegreffe and Y. Pinter. Attention is not not explanation. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing , pages\n11\u201320, 01 2019. doi: 10.18653/v1/D19-1002.\nS. Wiegreffe, A. Marasovic, and N. A. Smith. Measuring association between labels\nand free-text rationales, 2020.\n71", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Using natural language to predict bias and factuality in media with a study on rationalization", "author": ["K Tangri"], "pub_year": "2021", "venue": "NA", "abstract": "Fake news is a widespread problem due to the ease of information spread online, and its  ability to deceive large populations with intentionally false information. The damage it causes is"}, "filled": false, "gsrank": 33, "pub_url": "https://dspace.mit.edu/handle/1721.1/130716", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:-gormuaaM2YJ:scholar.google.com/&output=cite&scirp=32&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D30%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=-gormuaaM2YJ&ei=CrWsaIYfwNmJ6g-p2qHxBQ&json=", "num_citations": 3, "citedby_url": "/scholar?cites=7364400130900560634&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:-gormuaaM2YJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://dspace.mit.edu/bitstream/handle/1721.1/130716/1251801786-MIT.pdf?sequence=1&isAllowed=y"}}]