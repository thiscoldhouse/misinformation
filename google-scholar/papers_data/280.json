[{"title": "Tom Jumbo-Grumbo at SemEval-2019 Task 4: hyperpartisan news detection with GloVe vectors and SVM", "year": "2019", "pdf_data": "Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019) , pages 1067\u20131071\nMinneapolis, Minnesota, USA, June 6\u20137, 2019. \u00a92019 Association for Computational Linguistics1067Tom Jumbo-Grumbo at SemEval-2019 Task 4: Hyperpartisan News\nDetection with GloVe vectors and SVM\nChia-Lun Yeh1, Babak Loni2, and Anne Schuth2\n1TU Delft, The Netherlands\n2De Persgroep\nc.yeh-1@student.tudelft.nl\nfbabak.loni, anne.schuth g@persgroep.net\nAbstract\nIn this paper, we describe our attempt to learn\nbias from news articles. From our experi-\nments, it seems that although there is a correla-\ntion between publisher bias and article bias, it\nis challenging to learn bias directly from the\npublisher labels. On the other hand, using\nfew manually-labeled samples can increase\nthe accuracy metric from around 60% to near\n80%. Our system is computationally inexpen-\nsive and uses several standard document repre-\nsentations in NLP to train an SVM or LR clas-\nsi\ufb01er. The system ranked 4th in the SemEval-\n2019 task. The code is released for repro-\nducibility1.\n1 Introduction\nBias is the inclination or prejudice for or against\none person or group. News articles that con-\ntain extreme bias fail to provide fair and multi-\nfaceted views for readers and can create polariza-\ntion within the society (Bernhardt et al., 2008). A\nsystem that can detect bias in news articles is thus\nrelevant, especially in a time where an increas-\ning number of people consume news from online\nsources that might not be trustful.\nThe SemEval-2019 task aims to detect hyper-\npartisan news given the text of the news article,\nwhere hyperpartisan news is de\ufb01ned to be an arti-\ncle that overtly favors a side or view. The details\nof the task can be found in Kiesel et al. (2019).\nWe are provided with a dataset of two parts. The\n\ufb01rst part is labeled by the publishers (e.g. if a pub-\nlisher is decided to be a hyperpartisan source, all\nits articles are labeled as hyperpartisan), and split\ninto a training and validation set with no overlap-\nping publishers (which we will refer to as training-\n1 and validation-1). The second part is crowd-\nsourced and labeled per article (which we will call\ntraining-2).\n1https://github.com/chialun-yeh/\nSemEval2019Due to the large number of labeled samples, we\ndecide to use a supervised classi\ufb01cation approach,\nwhere features are extracted from the text and used\nto train a classi\ufb01er. Bag-of-words (BoW), TFIDF\nweighting, and n-grams have been shown to be\nstrong baselines (Hu and Liu, 2004; Wang and\nManning, 2012). Other features such as Part-Of-\nSpeech (POS), counts of sentiment and bias words\nhave also been studied (Liu, 2012; Mukherjee and\nWeikum, 2015). In a similar setting, Potthast et al.\n(2018) uses features such as n-gram of charac-\nters, readability scores, dictionary, and the ratio\nof quoted words to separate hyperpartisan news\nfrom the mainstream. They trained a random for-\nest classi\ufb01er and achieved an accuracy of 75%.\nKulkarni et al. (2018) build a neural network to\npredict the political ideology of news articles to\nbe either left, right or center. They combine in-\nformation from the headlines, the links within an\narticle, and the content. They use a CNN (Kim,\n2014) for the headlines, a Node2Vec (Grover and\nLeskovec, 2016) to model the links and a hierar-\nchical attention network (HAN) (Yang et al., 2016)\nto extract features from the content. They com-\npare the model with several baselines, including\na BoW LR model, a fully-connected feedforward\nnetwork, and networks with only the individual\ncomponents. Their proposed model performs the\nbest. However, their system is trained and eval-\nuated on only data with publisher labels. They\nrandomly split them into training and testing sets,\nwith overlapping publishers.\nThe main contribution of the paper is two-fold.\nFirst, we analyze the problem of using the dataset\nlabeled by publishers, concluding that it is dif\ufb01cult\ndue to the noisy labels. Second, we train SVM\nclassi\ufb01ers with different representations: TFDIF,\ndoc2vec and GloVe pre-trained vectors. The 300-\ndimensional GloVe vectors obtain the best cross-\nvalidation accuracy as well as the performance\nmetrics on the of\ufb01cial test data.\n1068This paper is organized as follows. In section\n2, we describe the data pre-processing. In section\n3, we present the two systems that we devise and\nexplain how one motivates the other. In section\n4, we present the performance of the \ufb01nal system.\nWe outline our main conclusions and future work\nin section 5.\n2 Pre-processing\nSince the articles are collected from online news\nplatforms, they contain texts that are irrelevant to\nthe news itself. We use the following three steps\nto clean the data:\n(a) Remove online usage including links, hash-\ntags, @-tag, and advertisements.\n(b) Remove parentheses, brackets, and curly\nbrackets that contain additional information be-\ncause the usage is often speci\ufb01c to publishers.\n(c) Remove paragraphs that might reveal publisher\ninformation. Some publishers use headers and\nfooters of speci\ufb01c patterns in their articles. We\ntry to remove them by discarding the \ufb01rst and\nlast paragraphs from the article if the article has\nmore than two paragraphs, assuming that these\ntwo paragraphs have higher probabilities of being\nheaders and footers. This is by no means optimal\nsince the \ufb01rst paragraph often contains important\ncontent if it is not a header. Some publishers also\ninserted short text such as \u201dread more here\u201d be-\ntween paragraphs. To remove these irrelevant texts\nthat can reveal publisher pattern, we remove any\nparagraph with less than ten words. Any article\nwith less than ten words after the cleaning is dis-\ncarded.\nWe consider (a) and (b) as basic data cleaning\nand apply them on all data. On the other hand, (c)\nis a more aggressive cleaning that is done only on\ntraining-1. This is because we have a compara-\ntively large training set where we can afford \ufb01lter-\ning out information and even entire articles.\n3 System Description\n3.1 System 1\nIn the \ufb01rst method, we use training-1 to train our\nmodels, validation-1 to choose hyperparameters,\nand training-2 to test the models. As mentioned\nearlier, training-1 is labeled by publishers. While\na biased publisher publishes more biased articles\non average, it is unlikely that all of its articles are\nbiased. Therefore, the labels are noisy, e.g., somelabels are \ufb02ipped. It is, however, dif\ufb01cult to iden-\ntify the articles that have the wrong labels without\nmanual inspection. We assume that the publisher\nlabels are correlated with true bias labels, thus pro-\nviding information to learn bias. To have an idea\nof to what extent this assumption holds, we inves-\ntigate training-2. We select publishers of whom at\nleast \ufb01ve articles are included in the dataset and\nwhose media bias can be retrieved from Media-\nBias/FactCheck2. This results in a total of 24 pub-\nlishers. The publisher bias ratings on the website\ncan be roughly mapped to 7 categories, extreme-\nleft, left, left-center, center, right-center, right, and\nextreme-right. In Table 1, we list these publish-\ners along with the percentage of the articles that\nare rated as hyperpartisan by crowd workers. The\nnumber of articles per publisher range from 5 to\n24. Figure 1 shows the percentage of hyperpar-\ntisan articles in each category. We see that left-\ncenter and center publishers indeed have consid-\nerably less percentage of hyperpartisan articles.\nHowever, right-center publishers are almost as bi-\nased as right publishers. The observation can be\ndue to the small sample size (the high percentage\nis caused mainly by the publisher RealClearPoli-\ntics). In general, there is a correlation between the\npublisher and true hyperpartisanship.\nFigure 1: Percentage of hyperpartisan articles in the\n7 bias categories: extreme-left (EL), left (L), left-\ncenter (LC), center (C), right-center (RC), right (R),\nand extreme-right (ER).\nWe use BoW and n-grams (n=1,2) as features,\nwith different weighting schemes, including raw\ncounts, binary, and TFIDF. For BoW and n-grams,\nthe feature dimension is 50K and 500K respec-\ntively. We train two classi\ufb01ers on each rep-\nresentation. The accuracy of the classi\ufb01ers on\nvalidation-1 is listed in Table 2. We include exper-\niments where training-1 is not cleaned with pre-\n2https://mediabiasfactcheck.com/\n1069processing step (c) to make sure that the step helps\nthe task.\nFrom the result, we observe that adding bigrams\ndoesn\u2019t improve accuracy. We use the best model\n(BoW and an SVM classi\ufb01er) to predict the arti-\ncles in training-2. The accuracy is 56%, which is\nlower than the majority baseline of 63%.\nAlthough we clean the dataset in an effort to\nprevent the classi\ufb01er from over\ufb01tting on the pub-\nlisher, it seems that the classi\ufb01er cannot general-\nize to unseen publishers, and fails to capture bias.\nWe also experiment with training a CNN (Kim,\n2014) with the headlines, and a HAN (Yang et al.,\n2016) with the content. However, the two mod-\nels again fail to generalize to new publishers. The\nobservation makes us believe that the publisher la-\nbels are too noisy to be used directly to learn true\nbias. Another possible explanation could be that\nthe publishers have too distinct writing styles so\nthat the classi\ufb01er focuses much on those features\nwhen learning.\n3.2 System 2\nDue to the observation in system 1, we decide to\ntreat training-1 and validation-1 as unlabeled sam-\nples that can be used to train a feature extractor in\nan unsupervised setting. We then train the classi-\n\ufb01er using training-2. We use the \ufb01rst part of the\ndata by the following two extractors.\n1. TFIDF: The data is used to build vocabulary\nand record the inverse document frequency.\nAll terms that occur in more than 90% of\nthe documents are discarded, and we kept the\nmost frequent 50K terms.\n2. Doc2Vec: The data is used to train a PV-DM\nmodel proposed by Le and Mikolov (2014).\nWe discard all terms that occur in less than\n10 documents or are shorter than two charac-\nters. We train the model for 20 epochs using\nthe implementation of gensim ( \u02c7Reh\u02dau\u02c7rek and\nSojka, 2010). When inferencing new doc-\numents, the word vectors are \ufb01xed and the\nmodel is trained for 100 epochs.\nIn addition, we experiment with using pre-\ntrained word embeddings since the meaning of\neach word should not differ signi\ufb01cantly in differ-\nent corpora. We use vectors trained with GloVe\nalgorithm (Pennington et al., 2014) on Wikipediaand Gigaword 53. The vectors are chosen because\nthey are trained on Wikipedia and newswire text,\nwhich provides general knowledge and news do-\nmain speci\ufb01c usage. We take the vectors of each\nword in the document and average all the vectors.\nStop words are removed, and if the document has\nmore than 1000 words, we average over the \ufb01rst\n1000 words (we \ufb01nd this to work better in our case\nempirically).\nWe also experiment with a set of features in-\ncluding normalized count of 5 POS tags, 6 read-\nability scores, 8 normalized sentiment and bias\nword counts according to MPQA and bias lexi-\ncons (Wilson et al., 2005; Recasens et al., 2013),\nnumber of quotes, words, capitalized words, stop\nwords, and sentences, and average length of words\nand sentences. This result in a total of 27 features\nwhich we call Feat.\nFor supervised training, we split training-2 into\ntwo sets. The \ufb01rst half, with 322 samples, is used\nto train and choose hyperparameters in a 10-fold\ncross validation setting. The second half, with\n323 samples, is used for testing. We train LR\nand SVM on the features. Both linear SVM and\nSVM with rbf kernels are experimented with. We\nalso have some initial experiments of single layer\nand two-layer neural networks of different hidden\nlayer sizes but the small sample size makes them\ndif\ufb01cult to generalize.\n4 Results\nWe \ufb01rst train LR and SVM with different GloVe\nvector dimensions. Table 3 shows the accuracy on\nthe test set. SVM with rbf kernel works consis-\ntently better. The best vector dimension is 300.\nWe then compare different features, including\nTFIDF, Doc2Vec, GloVe, and the effect of adding\nFeat. Table 4 shows the accuracy on the test set.\nIt shows that SVM performs better than LR, and\nonly in the case of TFIDF does a linear SVM out-\nperforms kernel SVM. It also shows that the pre-\ntrained GloVe vectors achieve better performance\nthan the vectors that are trained on our data. The\nability to generalize might result from the larger\ncorpus that is used to train the vectors. Adding\nsimple lexical and sentiment features hurts the per-\nformance.\nThe three representations are furthered evalu-\nated on another test set (the of\ufb01cial test set of the\n3https://catalog.ldc.upenn.edu/\nLDC2011T07\n1070category publisher doc(%) category publisher doc(%)\nextreme-right thegatewaypundit.com 94.44 left salon.com 100.00\nextreme-right dcclothesline.com 85.71 left gq.com 60.00\nextreme-left trueactivist.com 62.50 left rawstory.com 40.00\nright pjmedia.com 100.00 left opednews.com 100.00\nright express.co.uk 36.84 left people.com 20.00\nright opslens.com 100.00 right-center realclearpolitics.com 92.86\nright insider.foxnews.com 27.27 right-center circa.com 12.50\nright foxnews.com 50.00 left-center cbsnews.com 11.11\nright washingtonexaminer.com 57.14 left-center heavy.com 7.69\nright bizpacreview.com 40.00 left-center nytimes.com 30.00\nright nypost.com 66.67 center snopes.com 8.33\nright bearingarms.com 66.67 center n\ufb02.com 0.00\nTable 1: Selected publishers with their bias categories and percentage of biased articles in the dataset.\nFeatures Classi\ufb01er\nLR SVM\nBoW (without (c)) 58.83 59.72\nBoW 60.67 60.93\nBoW-binary 60.61 60.68\nBoW-TFIDF (without (c)) 60.15 59.61\nBoW-TFIDF 60.86 60.90\nN-grams 60.73 59.13\nN-grams-binary 60.18 59.74\nN-grams-TFIDF (without (c)) 59.65 59.72\nN-grams-TFIDF 60.51 60.61\nTable 2: Validation accuracy after \ufb01ne-tuning. Without\n(c) means that the training set is not cleaned with the\npre-processing step (c). Cleaning helps improve accu-\nracy.\nFeatures Dim. LR SVM\nGloVe 100 72.45 78.33 (rbf)\nGloVe 200 72.76 76.78 (rbf)\nGloVe 300 72.45 79.57 (rbf)\nTable 3: Accuracy of different GloVe vector dimen-\nsions.\nFeatures Dim. LR SVM\nTFIDF 50K 77.09 77.71 (linear)\nGloVe 300 72.45 79.57 (rbf)\nGloVe + Feat 327 75.85 78.33 (rbf)\nDoc2Vec 400 71.83 78.95 (rbf)\nDoc2Vec + Feat 427 77.71 75.85 (rbf)\nTable 4: Accuracy of our model that is trained using\ntraining-2. The majority baseline is 63% accuracy.\ntask) that is labeled by crowd workers. Since the\nadditional feature set does not improve the perfor-mance, it is not further evaluated. In Table 5, the\naccuracy, precision, recall, and F1-score on the\nheld-out test set are shown. Our classi\ufb01ers tend\nto have a higher false negative rate. This can be\ndue to the imbalance in the training data. Further\nexperiments would be required to see whether re-\nsampling to have a balanced training set can im-\nprove that.\nFeatures Acc. Precision Recall F1\nTFIDF 74.36 80.00 64.97 71.70\nGloVe 80.57 85.82 73.25 79.04\nDoc2Vec 73.89 82.61 60.51 69.85\nTable 5: Submission results on the held-out test set,\nwith metrics including accuracy, precision, recall, and\nF1-score.\n5 Conclusion and Future Work\nIn this paper, we present the system we use to com-\npete in the SemEval-2019 hyperpartisan news de-\ntection task. The \ufb01nal model we use is a kernel\nSVM trained with pre-trained GloVe vectors. It\nturns out that a simple method which requires the\nleast training time performs the best in this case.\nBoth system 1 and system 2 have interesting fu-\nture work to be done. For system 1, it is interesting\nto correct the labels or \ufb01lter the articles in order to\nobtain a cleaner data to learn from. For system\n2, we plan to use contextual embeddings (Peters\net al., 2018) or pre-trained language models (Rad-\nford, 2018; Devlin et al., 2018) to extract repre-\nsentations that are then fed into downstream clas-\nsi\ufb01ers. The high performances of the models made\nthem interesting to compare with.\n1071References\nMark Daniel Bernhardt, Stefan Krasa, and Mattias K\nPolborn. 2008. Political polarization and the elec-\ntoral effects of media bias. Journal of Public Eco-\nnomics , 92(5-6):1092\u20131104.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.arXiv preprint arXiv:1810.04805 .\nAditya Grover and Jure Leskovec. 2016. Node2vec:\nScalable feature learning for networks. In Proceed-\nings of the 22Nd ACM SIGKDD International Con-\nference on Knowledge Discovery and Data Mining ,\nKDD \u201916, pages 855\u2013864, New York, NY , USA.\nACM.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the Tenth\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining , KDD \u201904, pages\n168\u2013177, New York, NY , USA. ACM.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 Task 4: Hyperpartisan News Detection. In\nProceedings of The 13th International Workshop on\nSemantic Evaluation (SemEval 2019) . Association\nfor Computational Linguistics.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classi\ufb01cation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , pages 1746\u20131751. As-\nsociation for Computational Linguistics.\nVivek Kulkarni, Junting Ye, Steve Skiena, and\nWilliam Yang Wang. 2018. Multi-view models for\npolitical ideology detection of news articles. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3518\u2013\n3527. Association for Computational Linguistics.\nQuoc Le and Tomas Mikolov. 2014. Distributed repre-\nsentations of sentences and documents. In Proceed-\nings of the 31st International Conference on Inter-\nnational Conference on Machine Learning - Volume\n32, ICML\u201914, pages II\u20131188\u2013II\u20131196. JMLR.org.\nBing Liu. 2012. Sentiment Analysis and Opinion Min-\ning. Morgan & Claypool Publishers.\nSubhabrata Mukherjee and Gerhard Weikum. 2015.\nLeveraging joint interactions for credibility analy-\nsis in news communities. In Proceedings of the\n24th ACM International on Conference on Informa-\ntion and Knowledge Management , CIKM \u201915, pages\n353\u2013362, New York, NY , USA. ACM.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural LanguageProcessing (EMNLP) , pages 1532\u20131543. Associa-\ntion for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) , pages 2227\u2013\n2237. Association for Computational Linguistics.\nMartin Potthast, Johannes Kiesel, Kevin Reinartz, Ja-\nnek Bevendorff, and Benno Stein. 2018. A Stylo-\nmetric Inquiry into Hyperpartisan and Fake News.\nIn56th Annual Meeting of the Association for Com-\nputational Linguistics (ACL 2018) , pages 231\u2013240.\nAssociation for Computational Linguistics.\nAlec Radford. 2018. Improving language understand-\ning by generative pre-training.\nMarta Recasens, Cristian Danescu-Niculescu-Mizil,\nand Dan Jurafsky. 2013. Linguistic models for an-\nalyzing and detecting biased language. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers) , pages 1650\u20131659. Association for Computa-\ntional Linguistics.\nRadim \u02c7Reh\u02dau\u02c7rek and Petr Sojka. 2010. Software Frame-\nwork for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks , pages 45\u201350, Val-\nletta, Malta. ELRA. http://is.muni.cz/\npublication/884893/en .\nSida Wang and Christopher D. Manning. 2012. Base-\nlines and bigrams: Simple, good sentiment and\ntopic classi\ufb01cation. In Proceedings of the 50th An-\nnual Meeting of the Association for Computational\nLinguistics: Short Papers - Volume 2 , ACL \u201912,\npages 90\u201394, Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\nTheresa Wilson, Janyce Wiebe, and Paul Hoffmann.\n2005. Recognizing contextual polarity in phrase-\nlevel sentiment analysis. In Proceedings of the Con-\nference on Human Language Technology and Em-\npirical Methods in Natural Language Processing ,\nHLT \u201905, pages 347\u2013354, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchi-\ncal attention networks for document classi\ufb01cation.\nInProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 1480\u20131489. ACL.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Tom Jumbo-Grumbo at SemEval-2019 Task 4: hyperpartisan news detection with GloVe vectors and SVM", "author": ["CL Yeh", "B Loni", "A Schuth"], "pub_year": "2019", "venue": "\u2026 of the 13th International Workshop on \u2026", "abstract": "In this paper, we describe our attempt to learn bias from news articles. From our experiments,  it seems that although there is a correlation between publisher bias and article bias, it is"}, "filled": false, "gsrank": 436, "pub_url": "https://aclanthology.org/S19-2187/", "author_id": ["", "S19S7TYAAAAJ", "Y3ahb_wAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:ih2usOoaROUJ:scholar.google.com/&output=cite&scirp=435&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D430%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=ih2usOoaROUJ&ei=VLWsaISoBr_SieoPzJnloAQ&json=", "num_citations": 8, "citedby_url": "/scholar?cites=16520358928390692234&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:ih2usOoaROUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aclanthology.org/S19-2187.pdf"}}, {"title": "Mutual hyperlinking among misinformation peddlers", "year": "2021", "pdf_data": "MUTUAL HYPERLINKING AMONG\nMISINFORMATION PEDDLERS\nA P REPRINT\nVibhor Sehgal\nSchool of Information\nUniversity of California, Berkeley\nBerkeley, CA\nsehgalvibhor@berkeley.eduAnkit Peshin\nAvast, Inc.\nEmeryville, CA\nankit.peshin@avast.comSadia Afroz\nAvast, Inc.\nEmeryville, CA\nsadia.afroz@avast.com\nHany Farid\nSchool of Information & EECS\nUniversity of California, Berkeley\nBerkeley, CA\nhfarid@berkeley.edu\nApril 26, 2021\nABSTRACT\nThe internet promised to democratize access to knowledge and make the world more open and\nunderstanding. The reality of today\u2019s internet, however, is far from this ideal. Misinformation, lies,\nand conspiracies dominate many social media platforms. This toxic online world has had real-world\nimplications ranging from genocide to, election interference, and threats to global public health.\nA frustrated public and impatient government regulators are calling for a more vigorous response\nto mis- and disinformation campaigns designed to sow civil unrest and inspire violence against\nindividuals, societies, and democracies. We describe a large-scale, domain-level analysis that reveals\nseemingly coordinated efforts between multiple domains to spread and amplify misinformation. We\nalso describe how the hyperlinks shared by certain Twitter users can be used to surface problematic\ndomains. These analyses can be used by search engines and social media recommendation algorithms\nto systematically discover and demote misinformation peddlers.\nKeywords misinformation\u0001disinformation\n1 Introduction\nBy mid-May of 2020, in the midst of the global pandemic, 28% of Americans believed Bill Gates plans to use COVID-19\nto implement a mandatory vaccine program with tracking microchips [ 39]. Belief in this conspiracy is not unique to\nAmericans. In global surveys [ 34] across Central and South America, the Middle East, Northern Africa, the United\nStates, and Western Europe, 20% of the public believes this bizarre claim. This conspiracy spread primarily through\nsocial media posts.\nThe far-reaching, far-right QAnon conspiracy alleges a cabal of Satan-worshipping cannibalistic pedophiles is running a\nglobal child sex-traf\ufb01cking ring that was plotting against Donald Trump. A recent poll \ufb01nds 37% of Americans are\nunsure whether QAnon is true or false, and 17% believe it to be true [ 20]. The conspiracy was, again, created and\nspread through social media posts, along with an assist from Trump himself [8].\nThe common thread in Bill Gates\u2019 COVID-microchips, QAnon\u2019s Satan-worshipping cannibals, and the long litany of\nconspiracies and lies polluting the internet, is the recommendation algorithms that aggressively promote the Internet\u2019sarXiv:2104.11694v1  [cs.SI]  20 Apr 2021\nAPREPRINT - APRIL 26, 2021\n\ufb02otsam and jetsam onto our news feeds and watch lists, plunging us into increasingly isolated echo chambers devoid of\nreality.\nTackling misinformation at any scale requires striking a balance between public safety and creating an environment that\nallows for an open exchange of ideas. We don\u2019t necessarily advocate a speci\ufb01c solution to achieve this balance, but\nrather seek to provide the tools to help others \ufb01nd this balance.\nBy way of nomenclature, we will refer to a broad category of domains that traf\ufb01c in conspiracies, distortions, lies,\nmisinformation, and disinformation \u2013 whether they are maintained by a state-sponsored actor, a private or public entity,\nor an individual \u2013 as \"misinformational domains.\" All other domains will be referred to as \"informational domains.\" We\ndescribe in more detail, in Section 2.1, how domains are characterized as either informational or misinformational.\nTackling misinformation on a per-post/image/video basis (e.g., [ 26,31,27,1,16,45,44,38,49,28,17,53,15,43,37,\n3,21,42,2,29,18,46]) is leading to a maddeningly massive game of online whack-a-mole. At the same time, social\nnetworks are under intense pressure from the public and government regulators to address the scourge of misinformation.\nWe propose that search engines and social media recommendation algorithms would bene\ufb01t from more aggressively\ndemoting entire domains that are known to traf\ufb01c in lies, conspiracies, and misinformation. To this end, we describe two\ntechniques for rooting out domains that consistently or primarily traf\ufb01c in misinformation. This type of domain-level\nanalysis might also be helpful to fact checkers evaluating the reliability of source material.\nTo better understand the online misinformation ecosystem, we build two networks of misinformational and informational\ndomains: a domain-level hyperlink network and a social-media level link sharing network. The domain-level network\nrepresents the hyperlinking relationship between domains. The social-media network represents the link-sharing\nbehavior of social network users. From these networks, we test two main hypotheses: (1) misinformational domains are\nmore connected (through hyperlinks) to each other than to informational domains; and (2) certain social media users are\nsuper-spreaders of misinformation. Our primary contributions include:\n1. Collating and curating a large set of more than 1000 domains identi\ufb01ed as traf\ufb01cking in misinformation.\n2.Revealing a distinct difference between how misinformational and informational domains link to external\ndomains.\n3. Showing how hyperlink differences can predict if a domain traf\ufb01cs in misinformation.\n4. Revealing that certain Twitter users have predictable patterns in their spread of misinformation.\n5.Building a classi\ufb01er for predicting the likelihood a domain is a misinformation peddler based on how speci\ufb01c\nTwitter users engage with a domain.\n1.1 Related Work\nOver the past \ufb01ve years, academic research on assessing and mitigating misinformation has increased signi\ufb01cantly, as\nhas the public\u2019s and government\u2019s interest in this pressing issue. Research has focused on understanding the nature of\nmisinformation and its impact on the general population [ 34,25,47], understanding how misinformation spreads [ 40,46,\n50, 29], and the automatic detection of misinformation [45, 44, 38, 49, 28, 17, 53, 15, 43, 37, 3, 21, 42, 2, 18, 46, 22].\nWith 53% of Americans getting at least some of their news from social media [ 41], signi\ufb01cant efforts have focused on\nthe promotion and spread of misinformation on social media. V osoughi et al. [ 50], for example, analyzed the spread of\nmisinformation on Twitter and found that false news spreads faster than true news. This study also found false news is\nmore novel than true news and is designed to inspire a strong response of fear, disgust, and surprise, and hence more\nengagement in terms of likes, share, and retweets. Faddoul et al. [ 13] and Tang et al. [ 46] showed how YouTube\u2019s own\nrecommendation algorithms help spread conspiracies and misinformation. Automated tools, such as Hoaxy [ 40], reveal\nin real time how misinformation spreads on Twitter.\nThe distinctive characteristics of false news stories make it somewhat easier to automatically detect them. A wide\nrange of machine learning approaches have been employed including both traditional classi\ufb01ers (SVM, LR, Decision\nTree, Naive Bayes, k-NN) and machine learning (CNN, LSTM, Bi-LSTM, C-LSTM, HAN, Conv-HAN) models to\ndemonstrate that false news can be automatically detected with a high level of accuracy (see [ 22] for a comparative\nstudy of detection approaches). Having a large and accurately labeled list of misinformational news, however, is dif\ufb01cult\nto obtain, which is why most studies use small datasets.\nDetection, debunking and fact-checking alone, however, are unlikely to stem the \ufb02ow of online misinformation.\nIt has been shown, for example, that the effect of misinformation may persist even after false claims have been\ndebunked [9, 24].\n2\nAPREPRINT - APRIL 26, 2021\nmisinfo domain info domain info category\n1hollywoodlife.com espn.com sports\n2radaronline.com adobe.com business\n3newidea.com.au fandom.com entertainment\n4yournewswire.com myshopify.com business\n5madworldnews.com bbc.com newsandmedia\n6thecommonsenseshow.com salesforce.com business\n7disclose.tv nytimes.com newsandmedia\n8pakalertpress.com force.com business\n9collective-evolution.com dailymotion.com entertainment\n10 fellowshipoftheminds.com primevideo.com entertainment\n11 govtslaves.info academia.edu education\n12 investmentwatchblog.com yelp.com business\n13 jewsnews.co.il sciencedirect.com education\n14 occupydemocrats.com mailchimp.com business\n15 twitchy.com line.me business\n16 worldtruth.tv deviantart.com entertainment\n17 abovetopsecret.com quizlet.com education\n18 activistpost.com n\ufb02.com sports\n19 amren.com okta.com business\n20 amtvmedia.com weather.com entertainment\nTable 1: Top-ranked misinformation and information domains in our data set.\nWe systematically study how over 1000 domains previously identi\ufb01ed as peddlers of misinformation, are connected\nwith one another and how this connection can be used to detect and disrupt misinformational networks. This type of\nhyperlink analysis has previously been examined, however not speci\ufb01cally in the space of misinformation. By analyzing\n89 news outlets, for example, Pak et al. [ 35] found that partisan media outlets are more likely to link to nonpartisan\nmedia, but that liberal media link to liberal and neutral outlets, whereas conservative media link more exclusively to\nconservative outlets. In analyzing hyperlinks between news media between 1999 to 2006, Weber et al. [ 51] found that\nestablishing hyperlinks with other, younger news outlets strengthens the position of that organization in the network\nthus boosting traf\ufb01c.\nIn contrast to these previous works, by analyzing signi\ufb01cantly larger networks ( >1000), we demonstrate more\nrobust patterns of hyperlinking, and speci\ufb01cally focus on the growing problem of misinformation and coordinated\nmisinformation peddlers.\n2 Methods\nWe begin by collating and curating several public databases of previously identi\ufb01ed misinformational and information\ndomains. The domain-level hyperlink network is constructed by scraping all hyperlink tags ( <a href=\"...\" </a> )\nfrom these domains. These hyperlinks can be to either an internal or external page. A level-1 scraping collects all\nhyperlinks from the top-level domain; a level-2 scraping collects all hyperlinks by following the level-1 links and\nrepeating the scraping. A graph, G= (V; E)is constructed from the scraped domains. Each vertex/node v2V\ncorresponds to a domain, and each directed edge e= (A; B)2Ecorresponds to a hyperlink from domain Ato\ndomain B. As described below, this graph is used to evaluate our underlying hypothesis and to gain further insight in\ncoordinated efforts by seemingly unconnected domains.\nThe social-media level link sharing network is constructed using the Twitter API to \ufb01nd users who shared links to\nmisinformational domains. We use a user-sharing feature vector as input to linear classi\ufb01er to predict a domain as being\na likely source of misinformation.\n2.1 Data Set\nWe begin by describing the collation and curation of four publicly available misinformation datasets.\n3\nAPREPRINT - APRIL 26, 2021\n\u2022BS Detector1: This data set is based on the \"BS detector\" browser extension2. This extension uses a manually\ncurated list of misinformational domains to label linked articles as reliable or not. This data set consists of 244\nunique domains.\n\u2022Columbia Journalism Review3: This dataset consists of manually curated misinformational stories scraped\nfrom Factcheck4, Fake News Codex5, OpenSources6, PolitiFact7and Snopes8. This data set consists of 155\nunique domains.\n\u2022FakeNewsNet9: This data set consists of manually curated misinformational stories scraped from PolitiFact10\nand GossipCop11. While PolitiFact is primarily focused on political news, GossipCop is primarily focused on\nthe entertainment industry. This data set consists of 898 unique domains.\n\u2022Media Bias Fact Check12: This data set consists of a manually curated and continually updated list of news\nmedia domains with attributes such as Factual Accuracy, Political Bias, Funding/Ownership, Country, etc.\nAdditionally, this data set contains an evolving list of 100 websites categorized as satire, and 310 websites\ncategorized as conspiracy-pseudoscience. This data set consists of 410 unique domains.\nIn total, these four data sets consist of 1,707 domains. There is, however, overlap between these datasets, which once\nremoved yields 1,389 distinct misinformational domains. There are several limitations to immediately using these\ndomains in our analyses. The GossipCop, FakeNewsNet, and PolitiFacts entries, for example, only provide the headline\nof the offending news article, from which we had to perform a reverse heuristic Google search to identify the source\ndomain. This reverse search does not always identify the offending domain; entertainment stories, for example, often\nlead to domains like imdb.com and people.com.\nTo contend with these limitations, we applied a ranking of the 1,389 domains to down-rank mislabelled domains like\nimdb.com. Each domain iin our original data set was assigned a score of si=fiexp(ri=5000) , where fiis the\nfrequency with which domain iappeared in our original data set, and riis the domain\u2019s Alexa top-million ranking. The\nexponential term weights the observation frequency so that highly ranked Alexa domains will have a nearly unit-value\nweight, and lower rated domains will have a higher weight. The domains with the largest scores siwere then categorized\nas misinformational. Despite this ranking system, a dozen clearly non-misinformational domains remained in our\ndata set, like theonion.com and huf\ufb01ngtonpost.com. These domains were manually removed, yielding a total of 1,059\ndomains.\nWe paired these 1,059 misinformational domains with 1,059 informational domains corresponding to the top-ranked\nAlexa domains (which we manually veri\ufb01ed are trustworthy domains). We selected 222 domains from the \"news &\nmedia\" Alexa categorizaton, 198 domains from each of the \"business\", \"education\", \"entertainment\", and \"sports\"\ncategories, \"45\" from \"health\", and 15 from \"religion.\" Shown in Table 1 are the 20top-ranked misinformational\ndomains and informational domains and corresponding categories.\n2.2 Domain Scraping\nWe next used OpenWPM13to scrape the hyperlink on each of the 1,059 domains. The hyperlinks tag ( <a href=\"...\"\n</a> ) is used to link to an internal or external page. OpenWPM is used to scrape the top-level domain for each hyperlink\n(level 1), and to scrape all pages linked from this top-level (level 2). This scraping was performed from a Google Cloud\nMachine with no user login and running Ubuntu 20.04 with 4vCPUs, 16GB RAM and 500GB disk space. Before a\nhyperlink was scraped the browser was reset and all the cookies were deleted. This entire process was repeated once\nevery two weeks over a six-week period between Feb 19, 2021 and Apr 2, 2021. Any domain that returned a 404error\nwere excluded from our analysis, yielding a total of 874/1,059 misinformational domains and 888/1,059 informational\ndomains.\n1https://www.kaggle.com/mrisdal/fake-news\n2https://github.com/Bastlynn/bs-detector\n3https://www.cjr.org/fake-beta#methodology\n4https://www.factcheck.org/2017/07/websites-post-fake-satirical-stories/\n5http://www.fakenewscodex.com/\n6http://www.opensources.co/\n7https://www.politifact.com/\n8https://www.snopes.com/\n9https://github.com/KaiDMML/FakeNewsNet\n10https://www.politifact.com/\n11https://www.gossipcop.com/\n12https://mediabiasfactcheck.com/fake-news/\n13https://github.com/mozilla/OpenWPM\n4\nAPREPRINT - APRIL 26, 2021\nFigure 1: Hyperlink connectivity between misinformational (red) and informational domains for level-1 (left) and\nlevel-2 (right) scraping.. Each node corresponds to a domain and a directed edge between domain iand domain j\nsigni\ufb01es a hyperlink on domain ito domain j. The nodes colored red correspond to misinformational domains, and the\nremaining nodes correspond to different categories of information domains.\nAfter scraping all informational and misinformational domains, we constructed an unweighted, directed graph of\nhyperlinks in which the graph nodes are the domains and a directed edge connects one domain that hyperlinked to\nanother. For example, hoggwatch.com hyperlinks to www.infowars.com/posts/<...> is processed as a directed\nedge from hoggwatch.com toinfowars.com . Shown in Fig. 1 are the level-1 (left) and level-2 (right) graphs in which\nwe can clearly see the strong misinfo-misinfo connections and weak misinfo-info connections.\nTo visualize and analyze the large hyperlink network, we use the open-source tool Gephi [ 4]. We use the Louvain\nmethod [ 32,6] for detecting communities of domains that are connected via hyperlinking relationship. This method is a\nfast heuristic based on modularity optimization. Networks with high modularity have dense connections between the\nnodes within modules but sparse connections between nodes in different modules. Since the misinformational domains\nare more connected to each other than informational domains, we hypothesize the misinformational and informational\ndomains will belong to different communities.\n3 Results: Domain-Level Hyperlinking\n3.1 Overview\nEach domain in the network created from the misinformational and informational domains are assigned a label of\n\"misinfo\", \"info\" or \"none\". The \"none\" categorization is used to classify domains not in the misinformational or\ninformational data set. Shown in the top portion of Table 2 is the number and proportion (%) of level-1 hyperlinks from\nmisinfo and info (rows 1-2) to misinfo, none, and info (columns 1-3). Here we see a huge difference, with 17.90% of\nhyperlinks of misinfo domains linking to misinfo domains, as compared to only 0.62% for info domains. Similarly,\nalbeit a smaller effect, 4.37% of hyperlinks on misinfo domains are to info domains, as compared to 13.45% for info\ndomains.\nShown in the lower portion of Table 2 is the distribution of hyperlinks for level-2 hyperlinks. A similar pattern emerges,\nalbeit not quite as dramatic: 9.27% of hyperlinks on misinfo domains link to misinfo domains, as compared to only\n1.03% for info domains, and 7.31% of hyperlinks on misinfo domains are to info domains, as compared to 9.78% for\ninfo domains. Also shown in this table is the breakdown of links based on domain categories. Here we see that in\nlevel-1 and level-2 \"entertainment\", \"news & media\", and \"religion\" are more likely to link to misinfo domains.\n5\nAPREPRINT - APRIL 26, 2021\nmisinfo none info total\n#links % #links % #links % #links\nmisinfo 1404 17.90 6098 77.74 343 4.37 7844\ninfo 60 0.62 8282 85.94 1296 13.45 9637\nentertainment 21 1.47 1209 84.66 198 13.87 1428\neducation 4 0.27 1204 82.64 249 17.09 1457\nnews&media 28 1.10 2211 87.12 300 11.82 2538\nbusiness 2 0.15 1165 85.22 201 14.70 1367\nsports 5 0.20 2170 88.10 289 11.73 2463\nreligion 1 0.82 107 87.70 14 11.48 122\nhealth 0 0 217 82.20 47 17.80 264\nmisinfo 6595 9.27 56325 82.98 4961 7.31 67881\nnone 1453 2.59 51362 91.65 3228 5.76 56043\ninfo 557 1.03 48008 90.38 5264 9.78 53828\nentertainment 252 3.16 6799 85.28 923 11.58 7973\neducation 113 0.90 11217 89.00 1274 10.11 12604\nnews&media 106 0.81 11888 91.31 1027 7.89 13020\nbusiness 32 0.38 7542 90.38 771 9.24 8345\nsports 44 0.43 9115 89.23 1056 10.34 10215\nreligion 7 1.55 389 86.06 57 12.61 452\nhealth 4 0.33 1054 86.82 156 12.85 1214\nTable 2: Level 1 (top) and level 2 (bottom) hyperlink network analysis. Each entry corresponds to the number and\npercentage of total links from one domain (row) to another (column). The label \"none\" corresponds to domains that are\nnot labeled as misinformational (misinfo) or informational (info). In level 1 analysis, 17.90% of hyperlinks on misinfo\ndomains are to other misinfo domains, as compared to only 0.62% of hyperlinks on info domains. This pattern persists\nin level 2, albeit with a smaller difference, 9.27% versus 1.03%.\nIncluded in the 60 info to misinfo hyperlinks we discovered, is the news & media site drudgereport.com hyper-\nlinking to sites like breitbart.com andthegatewaypundit.com , each of whom have been implicated in spreading\nmisinformation and conspiracies [ 36,23]. Other examples include the entertainment site indiewire.com linking to\nhollywoodlife.com , notorious for spreading gossip and misinformation.\n3.2 Insights\nShown in the hyperlink graph in Fig. 2 is a small, nearly fully-connected clique of eight domains: cancer.news ,\nclimate.news ,food.news ,health.news ,medicine.news ,naturalmedicine.news ,pollution.news , and\nsciences.news . This clique is an example of how individual domains can amplify misinformation by, disproportion-\nately, linking to like-minded domains.\nFollowing a reverse whois lookup ( www.whois.com ), we \ufb01nd all of these misinformational domains are owned by\nWebseed, LLC based in Arizona, USA. A deeper internet search reveals this LLC was created by Mike Texas, a\npseudonym for Mike Adams, founder of Natural News. According to Wikipedia \"Natural News (formerly NewsTarget,\nwhich is now a separate sister domain) is an anti-vaccination conspiracy theory and fake news website known for\npromoting pseudoscience and far-right extremism. Characterized as a \u201cconspiracy-minded alternative medicine website\",\nNatural News has approximately 7 million unique visitors per month.\" [52]\nThe level-2 scraping reveals this clique of eight domains is the tip of the iceberg. Shown in Fig. 3 is a large network of\n102.news domains all owned by Webseed, LLC. In this \ufb01gure, the red nodes and edges correspond to the original\neight-node clique, Fig. 2. The remaining red nodes are those from our original misinformational data set, and the\nmagenta nodes/edges are misinformational domains discovered by the level-2 scraping. This analysis shows the power\nof the hyperlinking theory to discover new domains peddling in misinformation.\nOur level-1 scraping also discovered a smaller clique of three domains: blackeyepolitics.com ,\ngreatamericandaily.com andamericanpatriotdaily.com . Despite forming a fully-connected clique, at \ufb01rst\nglance these domains appear to be unrelated each with the following ownerships: Rising Media News Network LLC,\nGreat American Daily Press LLC, and American Patriot News LLC, respectively. These domains, however are all\nowned by David A. Warrington [ 12]. Warrington also owns other domains including conservativerevival.com and\nliberalpropagandaexposed.com , the former of which we discovered through our level-2 scraping. This analysis\n6\nAPREPRINT - APRIL 26, 2021\nFigure 2: A magni\ufb01ed view of eight almost fully connected .news domains, all owned by Webseed.\nreveals how mutual hyperlinking can reveal seemingly coordinated misinformation efforts despite owners\u2019 efforts to\nconceal their coordination.\nThe above insights were gained by visually inspecting the hyperlink graphs in Fig. 1. As these graphs increase in size,\nhowever, this type of manual approach will quickly become impractical. We, therefore, employ a community detection\nalgorithm [6] to discover connections between a subset of domains.\nWe applied this community detection algorithm to the level-1 graph in Fig. 1. This analysis revealed two commu-\nnities. The \ufb01rst consisted of the following eight domains: globalresearch.ca ,journal-neo.org ,sott.net ,\nstrategic-culture.org ,swprs.org ,theduran.com ,thelibertybeacon.com , and wikispooks.com . The av-\nerage degree \u2013 de\ufb01ned as the sum of all the edges incident to a node \u2013 of this community was 3.12 and the graph density\nwas 0.446 (a graph density of 1 signi\ufb01es a complete graph). Some of these domains have previously been identi\ufb01ed as\nspreading misleading and false COVID-19 related information [ 14,5,19], three of which appear to be controlled by the\nRussian intelligence agency [48, 33, 7].\nThe second community consisted of the following \ufb01ve domains: cnsnews.com ,protrumpnews.com ,\nthegatewaypundit.com ,thepoliticalinsider.com , and waynedupree.com . The average degree of this com-\nmunity was 2.11 and the graph density was 0.12. These domains focus primarily on pro-Trump misinformation. The site\nthegatewaypundit.com , led by Jim Hoft, for example, promoted false rumors about voter fraud and Hillary Clinton\u2019s\nhealth in the 2016 US-national election. Earlier this year, Hoft was banned from Twitter for \"repeated violations of\nTwitter\u2019s civic integrity policy.\" Similarly, cnsnews.com is run by the daughter of Republican mega-donor, Robert\nMercer. Mercer has been implicated in the weaponization of millions of misinformation-spreading Twitter bots [10].\nThese types of communities are quite common. Our analysis revealed misinformational domains dominate the \ufb01ve\nlargest communities: (1) the largest community consisted of 79 domains, 88.6% of which are misinformational;\nfollowed by (2) 75 domains, 46.7% of which are misinformational; (3) 50 domains, 38% of which are misinformational;\n(4) 46 domains, 87.0% of which are misinformational; and lastly (5) 39 domains, 82.1% of which are misinformational.\n4 Results: Link Sharing on Social Media\nDomain cross-linking among misinformational domains adds to the spread of lies and conspiracies. Additionally,\nbillions of world-wide, social-media users are at least equally responsible for spreading misinformation on social media.\nA recent report, for example, Facebook\u2019s own internal research found 111 users are responsible for the majority of\nanti-vaccination misinformation [11].\n7\nAPREPRINT - APRIL 26, 2021\nFigure 3: Level-2 scraping of hyperlinks reveals a network of 102.news domains. The red nodes and edges correspond\nto the original eight-node clique, Fig. 2. The remaining red nodes are those from our original misinformational data set,\nand the magenta nodes/edges are misinformational domains discovered by the second-level scraping.\nWe investigate the ability to identify misinformational domains by tracking the hyperlinks shared by certain social-media\nusers. Because of the relative ease of access, we focus on Twitter\u2019s publicly available user data. In particular, we enlist\ntwo Twitter APIs: (1) The Search Tweets API14allows \ufb01ltering tweets based on a query term against a tweet\u2019s keywords,\nhashtags, or shared URLs. We \ufb01lter tweets by matching shared URLs against our misinfo/info URL dataset, surfacing\nwhich users are sharing a particular domain.; and (2) The Get Tweet Timelines API15allows querying all tweets surfaced\nfrom the Search Tweets API. In our case, we extract the domain URLs shared by the Twitter users surfaced in the\nprevious step. Although we don\u2019t consider them here, the data returned by both APIs contains geo-location, replied-to,\ntime, and other attributes that could be leveraged in the future.\nEach domain in our misinformational and informational data set is represented by a binary-valued vector corresponding\nto whether a particular user shared the domain URL. In order to avoid an overly large and sparse representation, starting\nwith a total of 289,984 users from our initial Twitter search, we eliminated 244,525 users who shared less than 2\ndomains each. The \ufb01nal 45459-D, binary-valued vector serves as the feature vector for each domain.\nWe further remove any domains with fewer than 5total tweeters, yielding a reduction from 961 to 451 misinformational\ndomains and 962 to 705 informational domains. A total of 1156 domains are split into a 75%=25% training/testing split.\nIn order to balance the training data, random oversampling with replacement is applied to the minority (misinformational)\nclass.\n14https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/guides/standard-operators\n15https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/api-reference/get-statuses-user_timeline\n8\nAPREPRINT - APRIL 26, 2021\nFigure 4: Mutual domain URL sharing by Twitter users, where each node is a domain. Undirected edges connect two\ndomains AandBif at least 1% of the users sharing either AorBalso shared the other domain (i.e., the Jaccard index\nis greater than 1%). Nodes with no connections have been dropped.\nWe trained a logistic regression (LR) using 75% of the data and then evaluated it on the rest of the 25% data\nset. The LR hyperparameters are tuned to maximize the F1-score. On testing, the LR classi\ufb01er, with a sup-\nport of 113/176 misinfo/info domains, achieved an F1-score of 0:75=0:85, with precision 0:78=0:84and re-\ncall0:73=0:87. Domains such as dailywire.com ,fortherightnews.com ,newsblaze.com ,nickiswift.com ,\ntherightscoop.com ,trueactivist.com , and usasupreme.com , were correctly classi\ufb01ed as misinformational.\nThe classi\ufb01er incorrectly inferred the following domains as informational, indicating they are shared along with other\ninformational domains: cancer.news ,celebrityinsider.org ,hollywoodreporter.com ,medicine.news ,\nmedicalmedium.com ,science.news , and trump.news .\nTo visualize the informational and misinformational domains shared by Twitter users, we construct an undirected graph\nG= (V; E)where each node v2Vrepresents a domain, and two vertices A; B2Vare connected by an undirected\nedge, e= (A; B)2E, if at least 1% of the users sharing either domain AorBshare both of them (i.e., Jaccard\nSimilarity\u00151%). In this visualization, Fig. 4 \u2013 and consistent with our \ufb01ndings from the previous section \u2013 we see\nstrong misinfo-misinfo connectivity and weak misinfo-info connectivity. In particular, misinfo/info domains have an\naverage of 7.55/1.57 connections, respectively, within their category, and 1.00/0.86 connections outside their category.\nA community analysis reveals some large communities dominated by misinformation domains: (1) the largest such\ncommunity consists of 131 domains, 89.3% of which are misinformational; followed by (2) 58 domains, 86.2% of\nwhich are misinformational; and (3) 46 domains, 91.3% of which are misinformational. We found a particularly\ninteresting community of 14 domains dominated by climate-change deniers, which includes domains like: cfact.org ,\nclimatism.blog ,climatechangedispatch.com ,iceagenow.info ,iowaclimate.org ,notrickszone.com ,\nrealclimatescience.com , and wattsupwiththat.com . A total of 13 of the 14 domains in this community are\n9\nAPREPRINT - APRIL 26, 2021\nmisinformational domains, and one in particular, wattsupwiththat.com , has been identi\ufb01ed by climatologist Michael\nE. Mann as \"the leading climate change denial blog\" [30].\n5 Discussion\nFrom the nature of lies, conspiracies, and rumors, to the methods for their delivery and spread, misinformation has,\nand is likely to continue to be an ever-evolving phenomena. While misinformation is not new, the consequences of its\ncollision with a vast digital landscape has led to signi\ufb01cant of\ufb02ine harms to individuals, marginalized groups, societies,\nand our very democracies. Addressing these harms will require a multi-faceted approach from thoughtful government\nregulation, to corporate responsibility, technological advances, and education.\nAs with most aspects of cybersecurity, technological solutions to addressing misinformation will themselves have to be\nmulti-faceted. With some 500 hours of video uploaded to YouTube every minute, and over a billion posts to Facebook\neach day, the massive scale of social media makes tackling misinformation an enormous challenge. We propose that\nin conjunction with complementary approaches to tackling misinformation, addressing misinformation at the domain\nlevel holds promise to disrupt large-scale misinformation campaigns. Previous studies have found a relatively small\ngroup of individuals are responsible for a disproportionate number of lies and conspiracies. Identifying this group, and\nreducing their reach \u2013 while not necessarily silencing them entirely \u2013 holds the potential to make a large dent in the\nonline proliferation of harmful misinformation.\nWe understand and appreciate the need to balance an open and free internet, where ideas can be debated, with the\nneed to protect individuals, societies, and democracies. Social media, however, cannot hide behind the facade they are\ncreating a neutral marketplace of ideas where good and bad ideas compete equally. They do not. It is well established\nthat social media\u2019s recommendation algorithms favor the outrageous and conspiratorial because it increases engagement\nand pro\ufb01t. As a result, Brandies\u2019 concept that the best remedy for falsehoods is more speech, not less, simply doesn\u2019t\napply in the era of algorithmic curation and ampli\ufb01cation. We propose that identi\ufb01ed misinformation peddlers not\nnecessarily be banned or de-platformed, but that their content simply be demoted in favor of more honest, civil, and\ntrustworthy content.\nAs with any inherently adversarial relationship, all approaches to addressing misinformation \u2013 including ours \u2013 will\nhave to adapt to new and emerging threats. In our case, misinformation peddlers may add decoy hyperlinks to external\ntrustworthy domains to escape being classi\ufb01ed based on their hyperlinks to other misinformational domains. This, in\nturn, will require techniques to root out such decoy links. And so on, and on, and on. While such a cat and mouse game\ncan be frustrating, the end game will be that it will become increasingly more dif\ufb01cult and time consuming to create and\nspread misinformation, with the eventual goal of discouraging most, leaving us to contend with the die-hard adversary.\nWhile this is not a complete success, it will mitigate the risk of misinformation and, hopefully, return some civility and\ntrust to our online ecosystems.\nAcknowledgments\nThis work was supported by funding from funding Avast, Inc. (Sehgal and Farid). We thank Juyong Do and Rajarshi\nGupta for their thoughtful comments and discussions.\nReferences\n[1]AGARWAL , S., F ARID , H., G U, Y., H E, M., N AGANO , K., AND LI, H. Protecting world leaders against deep\nfakes. In IEEE Conference on Computer Vision and Pattern Recognition Workshops (2019), pp. 38\u201345.\n[2]ALDWAIRI , M., AND ALWAHEDI , A. Detecting fake news in social media networks. Procedia Computer Science\n141(2018), 215\u2013222.\n[3]APHIWONGSOPHON , S., AND CHONGSTITVATANA , P. Detecting fake news with machine learning method.\nIn15th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and\nInformation Technology (2018), IEEE, pp. 528\u2013531.\n[4]BASTIAN , M., H EYMANN , S., AND JACOMY , M. Gephi: an open source software for exploring and manipulating\nnetworks. In Proceedings of the International AAAI Conference on Web and Social Media (2009), vol. 3.\n[5] BBC. https://monitoring.bbc.co.uk/product/c201hmp9, 2020.\n[6]BLONDEL , V. D., G UILLAUME , J.-L., L AMBIOTTE , R., AND LEFEBVRE , E.Fast unfolding of communities in\nlarge networks. Journal of Statistical Mechanics: Theory and Experiment 2008 , 10 (2008), P10008.\n10\nAPREPRINT - APRIL 26, 2021\n[7]BULLETIN , T. https://thebulletin.org/2020/10/how-russia-china-and-other-governments-use-coronavirus-\ndisinformation-to-reshape-geopolitics, 2020.\n[8]CATER , B. Trump, Addressing Far-Right QAnon Conspiracy, Offers Praise For Its Follow-\ners. https://www.npr.org/2020/08/19/904055593/trump-addressing-far-right-qanon-conspiracy-offers-praise-for-\nits-followers, 2020.\n[9]CHAN , M.- P. S., J ONES , C. R., H ALL JAMIESON , K., AND ALBARRAC\u00cdN , D. Debunking: A meta-analysis\nof the psychological ef\ufb01cacy of messages countering misinformation. Psychological Science 28 , 11 (2017),\n1531\u20131546.\n[10] DAILYMAIL . https://www.dailymail.co.uk/news/article-4593286/The-man-millions-Twitter-bot-Trump-\nfollowers.html, 2017.\n[11] DWOSKIN , E.Massive facebook study on users\u2019 doubt in vaccines \ufb01nds a small group appears to play a big role\nin pushing the skepticism. https://www.washingtonpost.com/technology/2021/03/14/facebook-vaccine-hesistancy-\nqanon, 2021.\n[12] E NGADGET . https://www.engadget.com/2019-11-11-google-political-ads-polls-email-collection.html, 2019.\n[13] FADDOUL , M., C HASLOT , G., AND FARID , H. A Longitudinal Analysis of YouTube\u2019s Promotion of Conspiracy\nVideos. arXiv 2003.03318 (2020).\n[14] FEEDBACK , H. https://healthfeedback.org/claimreview/listicle-of-facts-about-covid-19-contains-numerous-\ninaccurate-and-misleading-claims/, 2020.\n[15] GIRGIS , S., A MER, E., AND GADALLAH , M. Deep learning algorithms for detecting fake news in online text. In\n13th International Conference on Computer Engineering and Systems (2018), IEEE, pp. 93\u201397.\n[16] G\u00dcERA , D., AND DELP, E. J. Deepfake video detection using recurrent neural networks. In 15th IEEE\nInternational Conference on Advanced Video and Signal Based Surveillance (2018), IEEE, pp. 1\u20136.\n[17] HANSELOWSKI , A., PVS, A., S CHILLER , B., C ASPELHERR , F., C HAUDHURI , D., M EYER , C. M., AND\nGUREVYCH , I.A retrospective analysis of the fake news challenge stance detection task. arXiv:1806.05180\n(2018).\n[18] HOUNSEL , A., H OLLAND , J., K AISER , B., B ORGOLTE , K., F EAMSTER , N., AND MAYER , J.Identifying disin-\nformation websites using infrastructure features. In 10th USENIX Workshop on Free and Open Communications\non the Internet (2020).\n[19] INSIDER . https://www.insider.com/conspiracy-theory-wikispooks-huge-traf\ufb01c-increase-coronavirus-data-2020-6,\n2020.\n[20] IPSOS . More than 1 in 3 Americans believe a \u2018deep state\u2019 is working to undermine Trump.\nhttps://www.ipsos.com/en-us/news-polls/npr-misinformation-123020, 2020.\n[21] KARIMI , H., R OY, P., S ABA-SADIYA , S., AND TANG , J.Multi-source multi-class fake news detection. In 27th\nInternational Conference on Computational Linguistics (2018), pp. 1546\u20131557.\n[22] KHAN, J. Y., K HONDAKER , M. T. I., A FROZ , S., U DDIN , G., AND IQBAL , A. A benchmark study of machine\nlearning models for online fake news detection. Machine Learning with Applications (2021), 100032.\n[23] LEADSTORIES . https://leadstories.com/hoax-alert/2020/03/Fact-Check-Evidence-Does-NOT-Show-The-WHO-\nDirector-General-Overstated-COVID-19-Fatality-Rate.html, 2020.\n[24] LEWANDOWSKY , S., E CKER , U. K., AND COOK , J.Beyond misinformation: Understanding and coping with\nthe \"post-truth\" era. Journal of Applied Research in Memory and Cognition 6 , 4 (2017), 353\u2013369.\n[25] LEWANDOWSKY , S., E CKER , U. K., S EIFERT , C. M., S CHWARZ , N., AND COOK , J.Misinformation and its\ncorrection: Continued in\ufb02uence and successful debiasing. Psychological Science in the Public Interest 13 , 3\n(2012), 106\u2013131.\n[26] L I, Y., AND LYU, S. Exposing deepfake videos by detecting face warping artifacts. arXiv:1811.00656 (2018).\n[27] LI, Y., Y ANG , X., S UN, P., Q I, H., AND LYU, S. Celeb-df: A large-scale challenging dataset for deepfake\nforensics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020),\npp. 3207\u20133216.\n[28] LIU, Y., AND WU, Y.-F. Early detection of fake news on social media through propagation path classi\ufb01cation\nwith recurrent and convolutional networks. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence\n(2018), vol. 32.\n11\nAPREPRINT - APRIL 26, 2021\n[29] MA, J., G AO, W., AND WONG , K.-F. Detect rumors in microblog posts using propagation structure via kernel\nlearning. In 55th Annual Meeting of the Association for Computational Linguistics (Vancouver, Canada, July\n2017), pp. 708\u2013717.\n[30] MANN, M. E. The hockey stick and the climate wars: Dispatches from the front lines . Columbia University Press,\n2013.\n[31] MATERN , F., R IESS, C., AND STAMMINGER , M. Exploiting visual artifacts to expose deepfakes and face\nmanipulations. In IEEE Winter Applications of Computer Vision Workshops (2019), IEEE, pp. 83\u201392.\n[32] NEWMAN , M. E. Modularity and community structure in networks. Proceedings of the National Academy of\nSciences 103 , 23 (2006), 8577\u20138582.\n[33] N EWS, C., 2020.\n[34] NIGHTINGALE , S., AND FARID , H. Examining the Global Spread of COVID-19 Misinformation. arXiv:\n2006.08830, 2020.\n[35] PAK, C., C OTTER , K., AND DECOOK, J.Intermedia reliance and sustainability of emergent media: A large-scale\nanalysis of american news outlets\u2019 external linking behaviors. International Journal of Communication 14 (2020),\n23.\n[36] POLITIFACT . https://www.politifact.com/factchecks/2020/jul/28/stella-immanuel/dont-fall-video-\nhydroxychloroquine-not-covid-19-cu/, 2020.\n[37] REIS, J. C., C ORREIA , A., M URAI , F., V ELOSO , A., AND BENEVENUTO , F.Supervised learning for fake news\ndetection. IEEE Intelligent Systems 34 , 2 (2019), 76\u201381.\n[38] RUCHANSKY , N., S EO, S., AND LIU, Y. CSI: A hybrid deep model for fake news detection. In ACM on\nConference on Information and Knowledge Management (2017), pp. 797\u2013806.\n[39] SANDERS , L. The difference between what Republicans and Democrats believe to be true about COVID-19.\nhttps://today.yougov.com/topics/politics/articles-reports/2020/05/26/republicans-democrats-misinformation, 2020.\n[40] SHAO, C., H UI, P.-M., W ANG , L., J IANG , X., F LAMMINI , A., M ENCZER , F., AND CIAMPAGLIA , G. L.\nAnatomy of an online misinformation network. PloS one 13 , 4 (2018), e0196087.\n[41] SHEARER , E., AND GOTTFRIED , J. https://www.journalism.org/2021/01/12/news-use-across-social-media-\nplatforms-in-2020/, 2020.\n[42] SHU, K., AND LIU, H. Detecting fake news on social media. Synthesis Lectures on Data Mining and Knowledge\ndiscovery 11 , 3 (2019), 1\u2013129.\n[43] SHU, K., W ANG, S., AND LIU, H. Understanding user pro\ufb01les on social media for fake news detection. In IEEE\nConference on Multimedia Information Processing and Retrieval (2018), IEEE, pp. 430\u2013435.\n[44] SHU, K., W ANG, S., AND LIU, H. Beyond news contents: The role of social context for fake news detection. In\n12th ACM International Conference on Web Search and Data Mining (2019), pp. 312\u2013320.\n[45] SHU, K., Z HOU, X., W ANG, S., Z AFARANI , R., AND LIU, H.The role of user pro\ufb01les for fake news detection. In\nIEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (2019), pp. 436\u2013439.\n[46] TANG , L., F UJIMOTO , K., A MITH , M. T., C UNNINGHAM , R., C OSTANTINI , R. A., Y ORK, F., X IONG , G.,\nBOOM , J. A., AND TAO, C. \"down the rabbit hole\" of vaccine misinformation on youtube: Network exposure\nstudy. Journal of Medical Internet Research 23 , 1 (2021), e23262.\n[47] THORSON , E.Belief echoes: The persistent effects of corrected misinformation. Political Communication 33 , 3\n(2016), 460\u2013480.\n[48] TIMES , T. N. Y. https://www.nytimes.com/2020/07/28/us/politics/russia-disinformation-coronavirus.html, 2020.\n[49] TSCHIATSCHEK , S., S INGLA , A., G OMEZ RODRIGUEZ , M., M ERCHANT , A., AND KRAUSE , A. Fake news\ndetection in social networks via crowd signals. In Companion Proceedings of the The Web Conference 2018\n(2018), pp. 517\u2013524.\n[50] VOSOUGHI , S., R OY, D., AND ARAL, S.The spread of true and false news online. Science 359 , 6380 (2018),\n1146\u20131151.\n[51] WEBER , M. S. Newspapers and the long-term implications of hyperlinking. Journal of Computer-Mediated\nCommunication 17 , 2 (2012), 187\u2013201.\n[52] W IKIPEDIA . https://en.wikipedia.org/wiki/Natural_News, 2021.\n[53] ZHANG , C., G UPTA , A., K AUTEN , C., D EOKAR , A. V., AND QIN, X. Detecting fake news for reducing\nmisinformation risks using analytics approaches. European Journal of Operational Research 279 , 3 (2019),\n1036\u20131052.\n12", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Mutual hyperlinking among misinformation peddlers", "author": ["V Sehgal", "A Peshin", "S Afroz", "H Farid"], "pub_year": "2021", "venue": "arXiv preprint arXiv:2104.11694", "abstract": "The internet promised to democratize access to knowledge and make the world more open  and understanding. The reality of today's internet, however, is far from this ideal."}, "filled": false, "gsrank": 438, "pub_url": "https://arxiv.org/abs/2104.11694", "author_id": ["9akMPmsAAAAJ", "_C2_wdsAAAAJ", "u4epKYoAAAAJ", "3OKn_UYAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:c_Nf1hXo2R0J:scholar.google.com/&output=cite&scirp=437&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D430%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=c_Nf1hXo2R0J&ei=VLWsaISoBr_SieoPzJnloAQ&json=", "num_citations": 23, "citedby_url": "/scholar?cites=2151005477534692211&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:c_Nf1hXo2R0J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2104.11694"}}, {"title": "Characterizing and predicting fake news spreaders in social networks", "year": "2022", "pdf_data": "International Journal of Data Science and Analytics\nhttps://doi.org/10.1007/s41060-021-00291-z\nREGULAR PAPER\nCharacterizing and predicting fake news spreaders in social networks\nAnu Shrestha1\u00b7Francesca Spezzano1\nReceived: 24 April 2021 / Accepted: 28 October 2021\n\u00a9 The Author(s), under exclusive licence to Springer Nature Switzerland AG 2021\nAbstract\nDue to its rapid spread over social media and the societal threat of changing public opinion, fake news has gained massiveattention. Users\u2019 role in disseminating fake news has become inevitable with the increase in popularity of social media for\ndaily news diet. People in social media actively participate in the creation and propagation of news, favoring the proliferation\nof fake news intentionally or unintentionally. Thus, it is necessary to identify the users who tend to share fake news to mitigatethe rampant dissemination of fake news over social media. In this article, we perform a comprehensive analysis on two\ndifferent datasets collected from Twitter and investigate the patterns of user characteristics in social media in the presence of\nmisinformation. Speci\ufb01cally, we study the correlation between the user characteristics and their likelihood of being fake newsspreaders and demonstrate the potential of the proposed features in identifying fake news spreaders. Our proposed approachachieves an average precision ranging between 0.80 and 0.99 on the considered datasets, consistently outperforming baseline\nmodels. Furthermore, we also show that the user personality traits, emotions, and writing style are strong predictors of fake\nnews spreaders.\nKeywords Misinformation \u00b7Fake news spreaders \u00b7User characterization \u00b7User classi\ufb01cation\n1 Introduction\nOnline social media platforms such as Facebook and Twitter\nhave drastically changed the landscape of news consump-tion and the pattern of information \ufb02ow in the past decade.\nThe majority of the population relies on social media for\nnews on important events, breaking news, and emergencies.According to Pew Research Center 71% of American adultsever get news through social media in 2020 [ 38]. With the\nincrease in its popularity, social media has signi\ufb01cantly trans-\nformed the way of creating news content, user interactions,and engagement, reshaping the traditional medium to whole\nnew information ecosystems [ 39]. Individuals in social media\nactively participate in creating and sharing news items due toits ease of use, lower cost, and convenience of further sharing\n[4,45]. This shift of the news paradigm has led to an unprece-\ndented transformation in both news quality and quantity thatusers encounter in social media, reducing the credibility of\nBFrancesca Spezzano\nfrancescaspezzano@boisestate.edu\nAnu Shrestha\nanushrestha@u.boisestate.edu\n1Computer Science Department, Boise State University, Boise,ID, USAnews articles and eventually fostering the production and dis-\nsemination of misinformation.\nIndeed, the rapid spread of fake news has become a con-\ncerning problem in online social networks in recent years.Research has found that fake news is more likely to go\nviral than real news, spreading both faster and wider [ 51]\nand people engage more with fake news than real news[47]. Moreover, the worrisome amount of fake news widely\nspreading over social media can negatively in\ufb02uence users\u2019\nopinions creating threats on public health [ 50], emergency\nmanagement and response [ 16,49], election outcomes [ 22],\nand is responsible for a general decline in trust that citizens of\ndemocratic societies have for online platforms [ 1]. Surpris-\ningly, bots are equally responsible for spreading real and fakenews, and human activity causes the considerable spread of\nfake news on Twitter [ 46,51] as people are generally not able\nto accurately identify which news item is fake and whichis real [ 48]. Thus, fake news is successful mainly because\npeople are not able to disguise it from truthful information\n[21,28] and often share news online without even reading its\ncontent [ 11]. Also, even if people recognize news as fake,\nthey are more likely to share it if they have seen it repeatedly\nthan the news that is novel [ 10].\n123\nInternational Journal of Data Science and Analytics\nThus, identifying fake news spreaders in social networks\nis one of the key aspects to mitigate misinformation spreadeffectively. Examples of strategies that could be implemented\ninclude assisting fake news spreaders with credibility indi-\ncators to lower their fake news sharing intent [ 55], and\nmitigation campaign, e.g., target the most in\ufb02uential realnews spreader to maximize the spread of real news [ 44].\nHowever, less is known about the characteristics of fake or\nreal news spreaders.\nTherefore, in this article, we seek to understand the char-\nacteristics of fake news spreaders focusing on different\nattributes such as user writing style, emotions, demographics,personality, social media behavior, and network features. In\nparticular, we leveraged these attributes to perform a compre-\nhensive analysis on two different datasets, namely PolitiFact[44] and PAN [ 34] to investigate the patterns of user charac-\nteristics in social media in the presence of misinformation.\nWe hypothesize that users likely to share fake news hold\nspeci\ufb01c patterns based on these attributes which are differentfrom real news spreaders. To the best of our knowledge, some\nof the features we considered, such as user stress, needs, val-\nues, and tweeting behavior, have not been analyzed before.Furthermore, we investigate to what extent these features can\nbe used to identify users who are likely to share fake news\nby addressing the problem as a binary classi\ufb01cation task.\nOur analysis unveils some interesting characteristics of\nfake news spreaders across the two datasets considered.\nSpeci\ufb01cally, our results show that:\n\u2013 The majority of users under 18 or over 40 may tend to\nshare more fake than real news.\n\u2013 Female users may tend to be more fake news spreaders\nthan male users.\n\u2013 The political orientation of a fake news spreader is more\nlikely to coincide with the source\u2019s political bias of themajority of circulating fake news items.\n\u2013 Fake news spreaders (1) have newer accounts, (2) spend,\non average, less time between two consecutive tweets,and (3) tend to tweet more at night.\n\u2013 Fake news spreaders tend to express more negative emo-\ntions and stress in their tweets than real news spreaders.\n\u2013 Fake news spreaders are estimated to be more extroverted\nand less neurotic than real news spreaders.\n\u2013 Classi\ufb01cation results using our proposed features outper-\nform the results of baseline approaches with n-grams inboth datasets. Speci\ufb01cally, we show that our proposed\nfeatures can identify fake news spreaders with an aver-\nage precision of 0.99 on the PolitiFact dataset (vs. 0.96achieved by the best baseline) and 0.79 on the PAN\ndataset (vs. 0.78 achieved by the best baseline).\n\u2013 Emotions and personality features are strong predictors\nof fake news spreaders in all the considered datasets.The article is organized as follows. Section 2summarizes\nrelated work, Sect. 3describes the dataset we used in this arti-\ncle, Sect. 4presents our proposed features to characterize and\nclassify fake news spreaders, Sect. 5presents the user charac-\nteristics patterns that we found by analyzing the considereddatasets, Sect. 6reports on our experimental evaluations, and\n\ufb01nally, conclusions are drawn in Sect. 7.\n2 Related work\nSeveral studies have been conducted to understand the char-\nacteristics of users that are likely to contribute to spreading\nfake news on social networks. V osoughi et al. [ 51] revealed\nthat the fake news spreaders had, on average, signi\ufb01cantlyfewer followers, followed signi\ufb01cantly fewer people, and\nwere signi\ufb01cantly less active on Twitter. Moreover, bots tend\nto spread both real and fake news, and the considerable spreadof fake news on Twitter is caused by human activity. Shrestha\nand Spezzano showed that social network properties help in\nidentifying active fake news spreaders [ 41]. Shu et al. [ 46]\nanalyzed user pro\ufb01les to understand the characteristics ofusers that are likely to trust/distrust fake news. They found\nthat, on average, users who share fake news tend to be reg-\nistered for a shorter time than the ones who share real newsand that bots are more likely to post a piece of fake news\nthan a real one, even though users who spread fake news are\nstill more likely to be humans than bots. They also show thatreal news spreaders are more likely to be more popular and\nthat older people and females are more likely to spread fake\nnews. Guess et al. [ 15] also analyzed user demographics as\npredictors of fake news sharing on Facebook and found outpolitical-orientation, age, and social media usage to be the\nmost relevant. Speci\ufb01cally, people are more likely to share\narticles they agree with (e.g., right-leaning people tended toshare more fake news because the majority of the fake news\nconsidered in the study were from 2016 and pro-Trump);\nseniors tend to share more fake news probably because theylack digital media literacy skills that are necessary to assess\nonline news truthfulness. The more people post on social\nmedia, the less they are likely to share fake news, most likelybecause they are familiar with the platform and they knowwhat they share.\nShrestha et al. [ 43] analyzed the linguistic patterns used\nby a user in their tweets and personality traits as a predictorfor identifying users who tend to share fake news on Twit-\nter data [ 35,43]. Likewise, Giachanou et al. [ 13] proposed an\napproach based on a convolutional neural network to processthe user Twitter feed in combination with features represent-\ning user personality traits and linguistic patterns used in their\ntweets to address the problem of discriminating between fakenews spreaders and fact-checkers.\n123\nInternational Journal of Data Science and Analytics\nMa et al. [ 25] went beyond the user and news character-\nistics and analyzed the characteristics of diffusion networksto explain users\u2019 news sharing behavior. They found opin-\nion leadership, news preference, and tie strength to be the\nmost important factors at predicting news sharing, whilehomophily hampered news sharing in users\u2019 local networks.Also, people who are driven by grati\ufb01cations of information\nseeking, socializing, and status-seeking were more likely to\nshare news on social media platforms [ 23].\nMoreover, creating hashtags has been widely used to\norganize campaigns, sharing information and opinion about\nevents and news stories on social media. These hashtags havealso been used to draw attention and enhance the topic\u2019s vis-\nibility, eventually causing its wide spread over social media.\nBoth individuals and news organizations have capitalized onthis feature of social media via the massive use of politi-cal hashtags to increase readership and user engagement [ 3].\nThis target turns true and ampli\ufb01es if a user shares the piece\nof news with partisan af\ufb01liation [ 37]. Thus, the political ori-\nentation of a user can provide additional cues about the user\nbeing a fake news spreader or not.\nAs compared to previous work, which has been mainly\ndone on the PAN 2020 dataset [34],this article addresses the\nproblem of characterizing and predicting users that are keen\nto spread fake news on an additional larger dataset with morereliable ground truth extracted from FakeNewsNet [44].We\nconsider several groups of topic-agnostic features, including\nnew features that have not been used in previous work, such\nas behavioral features, stress, needs, and values, to pro\ufb01leand predict fake news spreaders on two datasets and evaluate\nthe relative importance of the considered groups of features.\nWe also highlight feature patterns that are common to bothdatasets.\n3 Datasets\nThis section describes the datasets we used to carry out our\nexperiments, namely the PAN 2020 and PolitiFact (Fake-\nNewsNet) datasets. The size of these datasets is shown inTable 1.\nTable 1 Datasets and statistics\nDataset # Fake news spreader # Real news spreader\nPAN 2020 250 250\nFakeNewsNet (PolitiFact) 648 3983.1 PAN 2020 dataset\nThe \ufb01rst dataset we consider is the one provided by the PAN\nCLEF12020 shared task on pro\ufb01ling fake news spreaders on\nTwitter [ 34]. The dataset has been collected in two languages,\nnamely English and Spanish, and consists of a balanced trainand test set for each language. For each considered language,\nthe training set includes 300 Twitter users and 100 tweets\nfor each user from their Twitter feed, resulting in 30,000English tweets and Spanish 30,000 tweets. The test set con-\ntains 200 users in each language and 100 tweets from their\nfeed for each user, resulting in 20,000 English tweets and20,000 Spanish tweets. In this article, we have considered\nonly the English dataset and combined the train and test set\ntogether in a unique (balanced) dataset.\nIn the PAN 2020 dataset, users that shared fake news in the\npast are labeled as fake news spreaders and real news spread-\ners, otherwise. However, it is worth noting that, because the\ndataset is GDPR compliant,\n2users are labeled as \u201cclass 0\u201d\nor \u201cclass 1,\u201d and the authors of the dataset did not disclose\nwhich one of the two labels corresponds to the class of users\nwho are fake news spreaders. In this article, we assumed oneof the two labels to identify fake news spreaders according to\nfeature patterns that result similar to the ones of the PolitiFact\ndataset. These patterns are described in Sect. 5.\n3.2 PolitiFact (FakeNewsNet)\nThe FakeNewsNet dataset consists of two datasets, PolitiFact\nand GossipCop, from two different domains, i.e., politicsand entertainment gossip, respectively [ 44]. Each of these\ndatasets contains details about news content, publisher, social\nengagement information, and user social network. In thisarticle, we only used the PolitiFact dataset, which contains\nnews with known ground truth labels collected from the fact-\nchecking website PolitiFact\n3where journalists and domain\nexperts fact-checked the news items as fake or real. Wedecided not to use the GossipCop dataset because in our\nprevious work [ 42] we found that gossip news is quite dif-\nferent than political news; hence we focused our attention onthe same news domain as the other dataset we considered,\ni.e., the PAN 2020 dataset. Overall, the considered PolitiFact\ndataset contains 295,469 users (after removing self-claimedbot accounts) sharing 701 news items via tweets and retweets.\nAs this dataset only provides ground truth for news, we com-\nputed the labels for the users (fake news spreader or real newsspreader) as explained here below. First, we \ufb01ltered out those\n1PAN CLEF ( https://pan.webis.de/ ) is a well-known forum that\nfocuses on applying text mining for user pro\ufb01ling.\n2https://www.privacyshield.gov/article?id=European-Union-Data-\nPrivatization-and-Protection .\n3https://www.politifact.com/ .\n123\nInternational Journal of Data Science and Analytics\nusers who had shared the same news item multiple times, and\nthen we selected only those users who had shared at leasteight unique news items. We manually analyzed the pro\ufb01les\nof users who shared the same news item multiple times and\nfound that they were bots; hence we excluded them fromour analysis as research has shown that false news spreadsmore than the truth because of humans, not bots [ 51]. Next,\nthe resulting group of 1,046 users is labeled as fake news\nspreaders or real news spreaders as follows: (1) a user is afake news spreader if at least 60% of the news items they\nshared are fake, or (2) a user is a real news spreader if at least\n60% of the news items they shared are real.\n4We labeled 648\nusers as fake news spreaders and 398 as real news spreaders.\nMoreover, we retrieved additional user data as follows. For\neach user who did not have enough tweets, i.e., more than100 words among all their tweets combined, we crawled alltweets posted one month prior to his \ufb01rst tweet creation time\nin our dataset. These additional tweets were utilized to gen-\nerate personality features and political orientation.\n4 Features\nThis section describes the features we analyzed to charac-terize and classify fake news spreaders in the two datasetsconsidered. Speci\ufb01cally, we study users according to six user\nfeatures groups: demographics, Twitter behavior and net-\nwork, emotions, personality, readability, and writing style.Text-based features such as emotions, personality, and read-ability are computed on the document resulting from the\nconcatenation of all the user tweets. To have a more accu-\nrate estimation of user emotions, personality, readability, andwriting style, retweets are excluded when computing these\nfeatures.\n4.1 Demographics\nThe \ufb01rst group of features we consider deals with user demo-\ngraphics, including age, gender, and political orientation.\nPrevious work has shown how these features in\ufb02uence users\u2019\nnews-sharing behavior. For instance, Reis et al. [ 36]s h o w\nthat white and male users potentially share more news onTwitter. Differently, Shu et al. [ 46] analyzed user pro\ufb01les\nto understand the characteristics of users that are likely to\ntrust/distrust fake news and propagate them on Twitter. Theyalso show that older people and females are more likely to\nspread fake news.\nDemographic features are often not explicitly available\non social media platforms. Therefore, as detailed in the fol-\n4One limitation of this labeling approach is that we may not catch\nfake news spreaders who camou\ufb02age themselves as real news spreadersthrough their news sharing behavior.lowing, we used machine learning -based methods to infer\nsuch attributes in the PolitiFact dataset users. However, as therequired metadata and hashtags are not available for the PAN\n2020 dataset, we were not able to compute demographics for\nthis dataset.\n4.1.1 Age and gender\nWe utilized m3inference [ 53], a deep-learning-based system\ntrained on Twitter data, to infer user demographic charac-\nteristics. Based on the available metadata such as username,\nscreen name, description, and pro\ufb01le image, it predicts thegender of the user as male or female, age of the user\ngrouped in four categories ( \u226418, 19 \u221229, 30 \u221239 and \u226540)\nand whether the given account is handled by an organization\nor not. We utilized only two characteristics (age and gender)for both types of users for our analysis. The m3inference has\nbeen shown to have an F1 score of 0.918 for gender prediction\nand 0.522 for age prediction [ 53].\n4.1.2 Political orientation\nAs the political ideology can provide additional cues about\npro\ufb01ling fake news spreaders, we computed a polarization\nscore to identify their political leaning. We used the method\nde\ufb01ned by Hemphill et al. [ 18] where a polarization score\n(#polar score) for each user is de\ufb01ned by using the hashtags\nfrom the user tweets to estimate their political ideology. Each\nof those hashtags is scored according to how political \ufb01g-ures with known party af\ufb01liation use them. Speci\ufb01cally, we\nimplemented the #polar score as follows. As a political \ufb01gure\ndataset, we used the dataset provided by Chamberlain et al.[6] which contains tweets collected from Jan 04, 2007, to Jan\n03, 2019, and authored by in-of\ufb01ce U.S. Congress members\nduring that time period [ 6]. Then, we classi\ufb01ed each politi-\ncian as Republican or Democrat by using a TF-IDF vectorrepresentation of their tweet hashtags as input features to a\nbinary classi\ufb01er. We experimented with different classi\ufb01ers,\nincluding support vector machine, logistic regression, extratree classi\ufb01er, and random forest with 5-fold strati\ufb01ed cross-\nvalidation using class weighting to deal with class imbalance.\nRandom Forest resulted in being the best classi\ufb01er with 0.69AUROC and 0.67 average precision, providing us with goodcon\ufb01dence in using those hashtags to estimate user political\norientation.\nThen, we generated Chi-Squared scores for each hash-\ntag, and we leveraged these scores as a polar dictionary to\nassign polarization scores to the users in the dataset we con-\nsidered (i.e., PolitiFact). Each hashtag in the tweet is lookedup in the polar dictionary, and Chi-Squared scores of match-\ning hashtags are averaged across the entire hashtags included\nin user tweets de\ufb01ned as polarization score for that user. Apositive polarization score indicates that the user tends to\n123\nInternational Journal of Data Science and Analytics\nincline toward right-leaning political orientation, and a neg-\native score indicates left-leaning political orientation.\n4.2 Behavioral-based features\nThis group of features measures the tweeting/sharing behav-\nior and engagement of the users and consists of the following\nfeatures:\nInsomnia index We analyzed the user tweeting behavior\nwithin the day (24 h). We divided the time into day and\nnight and considered the \u2018night\u2019 window as \u20189PM-6AM\u2019\nand the \u2018day\u2019 window as \u20186:01AM-8:59PM\u2019 (we usedthe local time of the user), and analyzed the normalized\ndifference between the number of tweets shared during\nthese time windows for each user as in [ 9,40].\nWeekend index Similarly to the insomnia index, we com-\nputed the normalized difference in the number of tweets\non weekdays and weekends.\nTime elapsed Average time elapsed between two consec-\nutive tweets of the user.\nAccount duration The duration (in the number of days)\nof the account since it is registered.\n4.3 Network-based features\nV osoughi et al. [ 51] have shown that fake news spreaders had\nfewer followers and followed fewer people than real news\nspreaders. Thus, in this article, we computed the Twitter fol-lower to following (TFF) ratio as in [ 46] to measure user\nconnectivity in the Twitter social network. TFF is computed\nby using the following formula\nTFF=#Follower +1\n#Following +1\nwhich indicates the ratio of the number of followers to the\nnumber of followings of the user. The greater the ratio, thehigher the popularity of the user.\n4.4 Emotions\nFake news is deliberately induced with emotionally chargedwords to in\ufb02uence public opinion and affects the vulnerabil-ities of people by triggering their sentiments such as anger,\nfear, and distrust toward the event, person, and organization.\nMoreover, Ghanem et al. [ 12] recently showed emotions play\na key role in detecting fake news. Therefore, we computed\nemotion features such as anger, joy, sadness, fear, disgust,\nanticipation, surprise, and trust by using the Emotion Inten-sity Lexicon (NRC-EIL) [ 29] and happy, sad, angry, don\u2019tcare, inspired, afraid, amused, and annoyed using Emolex.\n5\nWe started by cleaning tweets by expanding contraction\nwords, correcting misspellings and grammatical mistakes\nusing LanguageTool,6replacing negated words with their\nWordNet antonym, removing stop words, and lemmatizingthe words. Next, we computed feature vectors using theapproaches proposed by Milton et al. [ 26,27]. Speci\ufb01cally,\neach word is looked up in both emotion dictionaries, and\nthe associated affect values of matching words are extracted.Next, we normalized the scores of each emotion category\nby the total number of emotions retrieved from a tweet to\ngenerate an emotion vector. In case the same emotion waspresent in both lexicons, e.g., sad in NRC-EIL and sadness\nin Emolex, we considered the average of the two computed\nvalues.\n4.4.1 Stress\nAlong with these emotions (i.e., positive and negative emo-\ntions), frustrations, worries, and irritations, which are the\ncharacteristics of stress expressed through the language used\nin the user feed, can also progressively accelerate the spreadof fake news. Thus, we incorporate a stress feature computed\nusing the lexical dictionary, a Stress Word Count Dictionary\ncreated by Wang et al. [ 52] as the LIWC tool lacks this cate-\ngory. To compute this feature, we concatenated all the tweetsby each user to form a single document per user. We removed\nwords like \u2019RT,\u2019 \u2018Via,\u2019 and \u2018&amp\u2019 for each document.\n4.5 Personality\nThe IBM Watson Personality Insights service uses linguistic\nanalytics to infer individuals\u2019 intrinsic personality charac-\nteristics, including Big Five personality traits, Needs, and\nValues, from digital communications such as social mediaposts. The tool can work for different languages, including\nEnglish and Spanish. In our case, we concatenated all the\nuser tweets in a unique document to compute their personal-ity characteristics.\nThe features computed by this service are detailed in the\nfollowing (we considered the raw scores provided by the\nservice):\nBig \ufb01ve The Big Five personality traits, also known as\nthe \ufb01ve-factor model (FFM) and the OCEAN model, area widely used taxonomy to describe people\u2019s personality\ntraits [ 30]. This taxonomy\u2019s \ufb01ve basic personality dimen-\nsions are openness to experience, conscientiousness,extraversion, agreeableness, and neuroticism. For each\n5https://sites.google.com/site/emolexdata/ .\n6https://pypi.org/project/language-tool-python/ .\n123\nInternational Journal of Data Science and Analytics\npersonality dimension, IBM Watson Personality Insights\nalso provides a set of additional six facet features. Forinstance, agreeableness\u2019 facets include altruism, cooper-\nation, modesty, morality, sympathy, and trust.\nNeeds These features describe a user\u2019s needs as inferred\nby the text they wrote and include excitement, harmony,curiosity, ideal, closeness, self-expression, liberty, love,\npracticality, stability, challenge, and structure.\nValues These features describe the motivating factors that\nin\ufb02uence a person\u2019s decision-making. They include self-\ntranscendence, conservation, hedonism, self-\nenhancement, and openness to change.\nThese features ranges from 0 to 1. In terms of how precise\nis the IBM Watson Personality Insights service, the of\ufb01cialdocumentation\n7reports an average Mean Absolute Error\n(MAE) for the English language of 0.12 for the Big Five\ndimensions, 0.12 for the Big Five facets, 0.11 for Needs, and0.11 for Values. The reported average MAE scores are based\non a dataset containing user Twitter feeds from between 1500\nand 2000 participants for all characteristics and languages.\n4.6 Readability\nReadability measures the complexity of the text, and when\ncomputed from text written by the user (tweets in our case),\nit also represents which level of text complexity a user canunderstand. To determine that, we used popular readabilitymeasures in our analysis:\nFlesh Reading Ease\nFlesh Kincaid Grade Level\nColeman Liau IndexGunning Fog IndexSimple Measure of Gobbledygook Index (SMOG)\nAutomatic Readability Index (ARI)\nLycee International Xavier Index (LIX)Dale-Chall Score\nFlesch scores range from 0 to 100. Higher scores of\nFlesch reading-ease indicate that the text is easier to read,\nand lower scores indicate dif\ufb01culty to read. Coleman Liau\nIndex depends on the characters of the word to measure theunderstandability of the text. The Gunning Fog Index (that\ngenerates grade level between 0 and 20), Automatic Read-\nability Index, SMOG Index, Flesh Kincaid Grade Level arealgorithmic heuristics used for estimating readability, that is,\nhow many years of education is needed to understand the text.\nFinally, Dale-Chall\u2019s readability test uses a list of words well-known by the fourth-grade students (easily readable words)\n7https://cloud.ibm.com/docs/personality-insights?topic=personality-\ninsights-science#researchPrecise .to determine the dif\ufb01culty of the text. We use this group of\n8 readability features to measure the complexity of a user\u2019swriting style.\n4.7 Writing style\nThis set of features captures the writing style of the tweetsauthored by the same user. Speci\ufb01cally, we computed the\naverage number of certain words, items, and characters per\nuser tweet, which includes the average number of (1) words,(2) characters, (3) lowercase words, (4) uppercase words,\n(5) lowercase characters, (6) uppercase characters, (7) stop\nwords, (8) punctuation symbols, (9) hashtags, (10) URLs,(11) mentions, and (12) emojis and smileys. Also, we con-sidered the (13) percentage of user tweets that are a retweet\nand (14) the percentage of user tweets that are a sharing of\nbreaking news; we considered a tweet sharing breaking newsif the keyword \u2018breaking\u2019 or \u2018breaking news\u2019 was appearing\nin the tweet text. All but features (13) and (14) are computed\nby removing retweets from the user feed.\n5 User characterization\nThis section presents the main patterns characterizing userswho spread fake news that we found by analyzing the fea-\ntures described in the previous section on the two considered\ndatasets. However, as the PAN 2020 dataset only provides100 tweets per user, and user pro\ufb01le meta-data and times-\ntamps are not included, and hashtags are blurred, we were not\nable to compute demographic, behavioral, and network fea-tures for this dataset. All the feature differences discussed inthis section are statistically signi\ufb01cant with a pvalue <0.05\n(ANOV A or Wilcoxon rank-sum according to the data distri-\nbution).\n5.1 Demographics\nDemographics have been shown to be predictors of fake newsspreaders [ 15]. Figure 1shows the distribution of age, gen-\nder, and political orientation on the PolitiFact dataset. Here,\nwe observe that among users who have been predicted tobe under 18 or over 40, the majority of them tend to sharemore fake news than real one. The trend is the opposite for\nusers whose age is predicted to be in the age range of 19\u2013\n39. While previous work has shown that people over 65 tendto share more fake news than the younger generations (age\nrange 18\u201364), the sharing behavior of users under 18 has not\nbeen investigated. Here we observe that these users, togetherwith the ones over 40, may be the most vulnerable to fake\nnews, which is somehow aligned with previous \ufb01ndings. The\nmajority of teenagers are, in fact, unable to assess the credi-bility of the information that \ufb02oods their devices [ 7,54], while\n123\nInternational Journal of Data Science and Analytics\n(a)Age ( p\u22640.003)\n (b)Gender ( p\u22640.001)\n (c)Political Orientation ( p\u22640.006)\nFig. 1 Distribution of user demographics on the PolitiFact dataset\n(a)Account duration from registered\ntime. ( p\u22640.001)\n(b)Average time-lapse between two\nconsecutive tweets. ( p\u22640.001)\n(c)Insomnia index (tweeting behavior\nat night). ( p\u22640.001)\nFig. 2 Boxplots of user behavioral features on the PolitiFact dataset\nseniors are not as adept as the younger generation in assess-\ning online news veracity [ 14]. Regarding the role of gender\nin user sharing behavior, we observe in Fig. 1b that users\nwhose gender is predicted to be female tend to be more fake\nnews spreaders than male users in the considered dataset. One\npossible explanation could be that female users may be lessinterested in political news and, consequently, less informed\nand then more vulnerable on these topics [ 2,33]. Even if the\npresented \ufb01ndings for age and gender seem to be somehowaligned with previous research, it is worth noting that theseuser attributes have been automatically inferred by using a\ntool whose accuracy is not perfect; hence some errors may\nhave been introduced. Also, the age groups are highly unbal-anced, and the last group ( \u226440) is very broad and diverse\ncompared to the other ones. Hence our \ufb01ndings may not be\ngeneral but just limited to the (not very large) considereddataset.\nFigure 1c shows the distribution of fake and real news\nspreaders according to their political orientation. We see that,in the PolitiFact dataset, left-leaning users are more likely tobe fake news spreaders than right-leaning users. Guess et al.\n[15] have shown that, in 2016, conservatives were more likely\nto share articles from pro-Trump fake news domains thanliberals or moderates because those news items were aligned\nwith their believes, and the majority of fake news items that\nwere circulating were right-leaning. What we observe in thePolitiFact dataset is not in contradiction with this \ufb01nding.\nTo show that, we gathered the source bias of the news items\npresent in this dataset from the MediaBias/FactCheck web-site\n8and found that the majority of these news items came\nfrom left-leaning sources and were tweeted much more than\nnews coming from right-leaning sources (9,435 tweets aboutnews from left-leaning sources vs. 3,408 tweets about news\nfrom right-leaning sources). Thus, we also observe in the\nPolitiFact dataset that the political orientation of a fake newsspreader is more likely to coincide with the one of the sourcesof the majority of circulating fake news items (left-leaning\nin this case).\n5.2 User behavior\nThe presence of timestamps in the PolitiFact dataset allows\nus to investigate fake news spreaders tweeting behavior.Figure 2shows the box plots of the considered behavioral\nfeatures on such dataset. Here, we observe that fake news\nspreaders (1) have newer accounts, (2) spend, on average,\n8mediabiasfactcheck.com . The website\u2019s main goal is to edu-\ncate the public on media bias and deceptive news practices. This websitecontains a comprehensive list of news sources, their bias, and their cred-ibility of factual reporting scores. Here, the publisher\u2019s political biasis de\ufb01ned by using seven degrees of bias: extreme-right, right, right-\ncentered, neutral, left-centered, left, and extreme-left .\n123\nInternational Journal of Data Science and Analytics\nFig. 3 Average Twitter follower to following (TFF) ratio on the Politi-\nFact dataset. The difference is statistically signi\ufb01cant ( p\u22640.001)\nless time between two consecutive tweets, and (3) tend to\ntweet more at night (higher insomnia index) than real news\nspreaders.\nThus, fake news spreaders are users who are newer to the\nplatform (we are not considering bot accounts) and may be\nless expert about its functionalities/usage, and who tend to\ntweet more frequently, perhaps to increase their social capital.Also, their higher nighttime online activity may be connected\nwith the presence of a higher stress condition for fake news\nspreaders, as shown in Sect. 5.4.\n5.3 User network\nFigure 3shows the distribution of the average Twitter fol-\nlower to following (TFF) ratio on the PolitiFact dataset. We\nobserve how non-fake news spreaders are much more popular\n(they have around 500 times more followers than following,on average) than fake news spreaders. Thus, users with lowerTFF may tend to spread fake news more to increase their pop-\nularity on Twitter. For instance, users may know a news item\nis fake and spread it anyway because it is funny or of inter-est to user\u2019s friends and hence generate engagement among\nTwitter followers. Another motivation could be that a user\nwith a low TFF is new to the platform and is not familiarwith its features, hence may mistakenly share fake news.\n5.4 User emotions\nFigure 4shows the radar charts of user emotions, while Fig. 5\nshows a comparison of user stress levels on both the con-\nsidered datasets. We notice that, in both cases, fake newsspreaders tend to express more negative emotions (fear, sad-\nness, disgust, and angry) and stress in their tweets than real\nnews spreaders (all pvalues are \u22640.001). Conversely, non-\nfake news spreaders are happier and more inspired, but also\n(a)PolitiFact\n(b)PAN 2020\nFig. 4 Radar charts of the emotion features: PolitiFact and PAN 2020\ndatasets\nmore afraid (all pvalues are \u22640.001). Being induced by\nnegative bias, people generally pay more attention to negative\nnews [ 19,42]. Hence fake news spreaders tend to frame their\ntweets with negative emotions targeting to make it catchierand circulate more among people. On the contrary, non-fake\n123\nInternational Journal of Data Science and Analytics\n(a)PolitiFact ( p\u22640.001)\n (b)PAN 2020 ( p\u22640.001)\nFig. 5 Box plots of user stress level on the PolitiFact and PAN 2020\ndatasets\nnews spreaders are general individuals whose motive of using\nsocial media platforms is to connect with other people or fam-ily, share their achievements, advice, and support [ 31] and are\nmore skeptical about sharing fake news.\n5.5 User personality traits\nUser Big Five personality traits are shown in Fig. 6for both\ntypes of users. Among the \ufb01ve traits, extroversion and neu-roticism are statistically signi\ufb01cant features in both datasets(allpvalues are \u22640.001) and show the same trend, namely,\nfake news spreaders are estimated to be more extroverted\nand less neurotic than real news spreaders. Extroversion isrelated to the number of friends a user has, while neuroti-\ncism is related to frequency of posting [ 17]. Thus, fake news\nspreaders are estimated to be people who may share fakenews to capture the interest of and make fun with their friends\nand/or possibly connect with more people. On the other end,\nsharing fake news is a rarer phenomenon as compared to realnews sharing [ 15]; hence fake news spreaders are estimated\nto be less neurotic because they share less than real news\nspreaders.\nThe other three personality traits are statistically signif-\nicant features only in the PolitiFact dataset (all pvalues\nare\u22640.001), where we found that fake news spreaders are\nestimated to be more agreeable, conscious, and open thanreal news spreaders. Agreeableness is related to the type of\nfeelings (positive or negative) expressed via social media\nupdates, conscientiousness to posting about political news,and openness to the sharing of various forms of media [ 17].\nThus, fake news spreaders are estimated to be people whose\nposting behavior is driven by emotions (either positive or\nnegative) and have more interest in political events.\n5.6 User readability level and writing style\nDifferent from emotional and personality features, readabil-ity features do not generalize across the considered datasets.\nIn general, fake news spreaders in the PolitiFact datasets\nhave a lower readability level than non-fake news spread-ers, while the trend is the opposite in the PAN 2020 dataset.\n(a)PolitiFact\n(b)PAN2020\nFig. 6 Radar charts of the Big-Five personality scores: PolitiFact and\nPAN 2020 datasets\nFigures 7and8show the box plots of two of the readabil-\nity measures we considered on the PolitiFact and PAN 2020dataset, respectively.\nSimilarly, Table 2highlights the pattern of writing style\namong fake news spreaders and real news spreaders. If thevalue of a feature was higher (on average) for real news\nspreaders as compared to fake news spreaders, it is denoted\nasR >F(and R <Fotherwise) in the table. Fake\nnews spreaders tend to use more uppercase characters and\nfewer hashtags in their tweets but share more breaking news\nthan real news spreaders, and this trend generalizes for bothdatasets. Moreover, fake news spreaders in PolitiFact incor-\n123\nInternational Journal of Data Science and Analytics\n(a)Flesch kincaid\n(p\u22640.001)\n(b)Gunning fog\n(p\u22640.001)\nFig. 7 Readability index of tweets written by fake news spreaders ver-\nsus real news spreaders in PolitiFact\n(a)Flesch kincaid\n(p=0 .009)\n(b)Gunning fog\n(p=0 .003)\nFig. 8 Readability index of tweets written by fake news spreaders ver-\nsus real news spreaders in PAN 2020\nTable 2 Writing style features that differ in user feed\nFeatures PolitiFact PAN 2020\nHashtags R> FR > F\nRetweets R>F\nChar R>FR <F\nUppercase char R< FR < F\nLowercase char R>FR <F\nLowercase word R>F\nUppercase word R<F\nBreaking R< FR < F\nEmoji R>F\nTrailing period R>F\nPunctuation R>F\nWord count R>F\nStop words R>F\nURLs R<F\nMentions R>F\nAll differences are statistically signi\ufb01cant ( p\u22640.002 for PolitiFact\nand p\u22640.04 for PAN 2020). Bold indicate the same pattern in both\ndatasets\nporate more uppercase words and URLs but fewer words,\nlowercased characters, punctuation, trailing periods (\u2018...\u2019),\nstop words, and mentions than real news spreaders in theirtweets. In the PAN 2020 dataset, fake news spreaders use\nmore lowercase characters, fewer emojis, and retweet less\nthan real news spreaders. This indicates that fake newsspreaders aim to gain people\u2019s attention by sharing break-ing news and using more uppercase words and URLs in their\ntweets.\n6 Experiments\nThis section reports on our experimental results of using thefeatures described in Sect. 4to automatically identifying fake\nnews spreaders.\n6.1 Experimental setting\nWe addressed the problem of automatically identifying fakenews spreaders as a binary classi\ufb01cation task. In particular,\nwe used the combination of all the groups of features for the\nprediction. Once the features are computed, the classi\ufb01cationis performed by using the best classi\ufb01er among linear supportvector machine (SVM), logistic regression, and random for-\nest. We used class weighting to deal with the class imbalance\nand performed 5-fold cross-validation. Additionally, we alsoused each group of features as input to the best classi\ufb01er to\nexamine the contribution of these features in identifying a\nuser likely to spread fake news. As an evaluation metric, weused Average Precision\n9(AvgP) which is a metric commonly\nused when dealing with unbalanced binary datasets [ 8], as in\nthe case of the PolitiFact dataset. The average precision is thearea under the precision curve, computed by plotting preci-sion against the true positive rate. The average precision score\ngives the probability that a classi\ufb01er will correctly identify a\nrandomly selected positive sample (e.g., a fake news spreaderin our case) as being positive. In our problem, we are inter-\nested in identifying fake news spreaders with high precision.\nThese are the users to target with correction strategies to mit-igate the further spread of fake news. In the tables reported in\nthis section, the best average precision values are highlighted\nin bold.\n6.1.1 Baselines for comparison\nWe compared our proposed approach with the two best per-\nforming approaches used by the participants to the PAN\nCLEF 2020 shared task, namely the approaches proposedby Buda and Bolonyai [ 5] and Pizarro [ 32]. These baselines\nare described here below:\nBuda and Bolonyai [ 5] utilized n-grams based approach\nand combined them with statistical features from thetweets, such as their average length or their lexical\ndiversity. Speci\ufb01cally, they used an ensemble model of\n9We used the average precision implementation provided by the Python\nScikit-learn library: https://scikit-learn.org/stable/modules/generated/\nsklearn.metrics.average_precision_score.html .\n123\nInternational Journal of Data Science and Analytics\nTable 3 Average precision of our proposed features (in input to a\nRandom Forest classi\ufb01er) on PolitiFact and PAN 2020 datasets andcomparison with baselines\nApproach PolitiFact PAN 2020\nBuda and Bolonyai [ 5] 0.737 0.783\nPizarro [ 32] 0.966 0.714\nOur features (random forest) 0.995 0.795\nOur features (linear SVM) 0.595 0.687Our features (logistic regression) 0.672 0.717\nBest values are in bold\nLogistic Regression with \ufb01ve sub-models, namely, logis-\ntic regression, linear SVM, random forest, and XGBoostwith n-grams and XGBoost with statistical features.\nPizarro [ 32] utilized a character and word n-grams-based\napproach with a linear support vector machine as the clas-si\ufb01er.\n6.2 Classification results\nClassi\ufb01cation results are reported in Table 3to allow compar-\nison between the performances of baselines and our method\non both PolitiFact and PAN 2020 datasets. As we can see,\nour proposed features consistently outperform both base-line approaches. Speci\ufb01cally, we got an average precision\nof 0.995 versus the best baseline results of 0.966 achieved by\nPizarro [ 32] on the PolitiFact dataset and an average precision\nof 0.795 versus the best baseline results of 0.783 achieved\nby Buda and Bolonyai [ 5] on the PAN 2020 dataset. Among\nthe considered classi\ufb01ers, random forest achieved the bestperformance. Furthermore, the baseline methods are mainlyn-grams-based and, consequently, they are not easy to inter-\npret. On the contrary, the features we consider in Sect. 4\nachieve better performances and can also be analyzed to pro-vide signi\ufb01cant patterns to characterize fake news spreaders\nas we have shown in Sect. 5.\nIn addition, we investigated the performance of each\nconsidered group of features individually (demographics,\nemotions, behavior, network, readability, personality, and\nwriting style) when the best classi\ufb01er (i.e., random forest)is used. Results are reported in Table 4. We observe that\nemotions and personality features are the most important\ngroups of features for the PAN 2020 dataset. In the Poli-\ntiFact dataset, the writing style is the most important groupof features, while emotions and personality are the second\nmost important groups of features. Hence, our results reveal\nthat emotions and personality are strong predictors of fakenews spreaders in both datasets. Since the Twitter IDs of\nthe users in the PAN 2020 dataset are concealed, it was not\npossible for us to collect the additional user data requiredto generate some features like demographics, behavior, andTable 4 Average precision per feature group on PolitiFact and PAN\n2020 datasets\nFeatures PolitiFact PAN 2020\nDemographics 0.777 \u2013\nEmotions 0.976 0.787Behavior 0.866 \u2013Network 0.776 \u2013Readability 0.897 0.635Personality 0.979 0.786Writing style 0.990 0.713\nnetwork features. However, the features extracted from the\ntext show, in general, better performances than demograph-ics, behavior, and network features in both datasets, as shown\nin Table 4. Combining all the groups of features together fur-\nther improves the average precision of the classi\ufb01cation task(cf. Table 3).\n6.3 Feature importance and Shapley additive\nexplanations\nConsidering all the features from each group, we have a total\nof 91 and 99 features for the PAN and PolitiFact datasets,\nrespectively, which can still be too many for the size of theconsidered datasets (PolitiFact and PAN) to perform real ver-\nsus fake news spreader classi\ufb01cation. Therefore, we used the\nstatistical tests (ANOV A and Wilcoxon rank-sum depend-ing on the data distribution) as in [ 20,42] to perform feature\nselection. For each dataset, only features where the two aver-\nages (real vs. fake news spreader) were signi\ufb01cantly differentaccording to the statistical test ( pvalue <0.05) were con-\nsidered. Also, features are sorted by Fvalue in descending\norder to determine the importance. Among these features, we\nselected the top- kmost important features to feed the classi\ufb01-\ncation algorithm, where kis the square root of the training set\nsize (rule of thumb). For each dataset, the selected important\nfeatures are shown in Fig. 9.\nTable 5shows our classi\ufb01cation results with important fea-\ntures using the best classi\ufb01er, i.e., random forest. We observe\nthat using only important features lowers the performance bya very small margin 0.19% and 0.1% in the PAN 2020 datasetand PolitiFact datasets, respectively. However, it still outper-\nforms the scores of both baselines shown in Table 3.\nFurther, to explain why users are classi\ufb01ed as fake news\nspreaders or real news spreaders, we used the SHAP values\n(SHapley Additive exPlanations) of the selected features, a\nwidely used approach inspired by cooperative game theory[24]. We leveraged a tree explainer which is basically used\nto compute SHAP values for tree-based models. Since we\nwant to learn about how each feature is in\ufb02uencing the deci-sion of the model, we used the global importance, i.e., the\n123\nInternational Journal of Data Science and Analytics\n(a)PolitiFact\n (b)PAN\nFig. 9 SHAP summaries of the important features: PolitiFact and PAN\ndatasets. Y-axis represents the features in order of importance. X-axis\nrepresents the SHAP values, positive values (greater than zero) repre-sents a higher chance of classifying a user as a fake news spreader and\nnegative values represent a higher chance of classifying a user as a real\nnews spreader\nsum of all the absolute Shapley values per feature across\nthe dataset. Figure 9shows the SHAP summary plot that\ndemonstrates the contribution of each feature in predicting\nusers likely to spread fake news. The higher the SHAP value(i.e., closer to 1.0), the higher the probability of being a fake\nnews spreader. As shown in the \ufb01gure, writing style features\n(like frequency of lowercase words, uppercased characters)appear as the most important features in the model for the\nPolitiFact dataset. We observe that users writing tweets with\nfewer lowercased words, more uppercased characters, morebreaking, less punctuation, shorter text, and fewer stopwordsare more likely to be fake news spreaders according to the\nPolitiFact dataset. On the other end, features indicating emo-\ntions like happiness and anger and personality facets such asexcitement, assertiveness, openness to change, artistic inter-\nests appear as the most important features in the model for\nthe PAN 2020 dataset. We see that the users with less con-cern about others\u2019 welfare and interests (self-transcendence),\nless concordance (harmony), and having the willingness to\nchange (openness to change) are more likely to be fake newsspreaders, according to the PAN 2020 dataset.\nAdditionally, we further con\ufb01rm that negative emotions\nlike anger, fear, disgust, stress, and sadness extracted from\nthe tweets of a user are among the most important featuresand indicate that the users likely to spread fake news seemTable 5 Average precision of important features from Fig. 9versus all\nfeatures on PolitiFact and PAN 2020 datasets\nFeatures PolitiFact PAN 2020\nImportant features 0.994 0.776\nAll features 0.995 0.795\nto embrace a language with more negative valiance than real\nnews spreaders in both datasets.\n7 Conclusions\nIn this article, we performed a comprehensive analysis tounderstand the correlation between user characteristics basedon different attributes such as user demographics, person-\nality, emotion, writing style and readability, social media\nbehavior, and the likelihood of a user being a fake newsspreader. We considered two datasets to perform our analy-\nsis, namely the PolitiFact (FakeNewsNet) and PAN datasets,\nand investigated new features such as user tweeting behav-ior and stress level. Furthermore, we addressed the problem\nof identifying users likely to share fake news using the pro-\nposed groups of features in both datasets and compared theperformance with baseline approaches from the PAN shared\n123\nInternational Journal of Data Science and Analytics\ntask. Speci\ufb01cally, we obtained an average precision of 0.99\non the PolitiFact dataset (vs. 0.96 achieved by the best base-line) and 0.80 on the PAN dataset (vs. 0.78 achieved by the\nbest baseline).\nOur results showed the potential of the proposed features\nin identifying fake news spreaders by outperforming baselineapproaches in both considered datasets. Our \ufb01ndings showed\nthat younger generation under 18 or users over 40 may be\nmore vulnerable in case of fake news sharing, and femalesmay be more likely to be fake news spreaders than male\nusers. Similarly, fake news spreaders tend to express more\nnegative emotion and stress in their tweets, and the politicalorientation of a fake news spreader is more likely to coincide\nwith the bias of the source of the majority of circulating fake\nnews items. Besides, the behavioral patterns show that fakenews spreaders have newer accounts, spend less time buttweet more within a short time interval. Likewise, it shows\nthe inferred user personality, writing styles, and readability\nof the user\u2019s tweets have the potential to identify whether theuser is a fake news spreader effectively.\nUsing an automated tool to infer user demographics based\non their screen name, description, and pro\ufb01le image couldbe a potential limitation of our study. Thus, inferred demo-\ngraphics of some of the users might not be entirely accurate.\nHowever, it is impossible to test the tool\u2019s ef\ufb01ciency in theconsidered datasets as such metadata are not explicitly avail-able to be used as ground truth. Labels in the PAN 2020\ndataset are another limitation of the work presented in this\narticle as a user is labeled as a fake news spreader if theyhave shared at least one fake news item in the past. We have\nproposed a way to compute more reliable labels for the Poli-\ntiFact dataset to overcome this limitation. Finally, we haveconsidered only users keen to spread fake political news, and\nwe leave as future work the study of fake news spreaders in\nother domains, e.g., gossip news.\nAcknowledgements This work has been supported by the National Sci-\nence Foundation under awards No. 1943370 and 1820685 and the IdahoGlobal Entrepreneurial Mission/Higher Education Research Councilunder Award No. IGEM22-001. We thank Hongmin (Steven) Kim forcollecting additional Twitter data, Ashlee Milton and Maria Soledad\nPera for providing us the code used in their article [ 26] to compute\nemotional features, and Brian Daley for implementing the code to com-pute user political orientation.\nReferences\n1. Barometer, E.T.: Edelman trust barometer global report. Edelman,\navailable at: https://www.edelman.com/sites/g/\ufb01les/aatuss191/\n\ufb01les/2019-02/2019_Edelman_Trust_Barometer_Global_Report_2.pdf (2019)\n2. Benesch, C.: An empirical analysis of the gender gap in news con-\nsumption. J. Media Econ. 25(3), 147\u2013167 (2012)\n3. Bonilla, Y ., Rosa, J.: # ferguson: digital protest, hashtag ethnog-\nraphy, and the racial politics of social media in the United States.Am. Ethnol. 42(1), 4\u201317 (2015)4. Bruns, A., High\ufb01eld, T., Lind, R.A.: Blogs, twitter, and breaking\nnews: the produsage of citizen journalism. Produs. Theory Digit.World Intersect. Audiences Prod. Contemp. Theory 80(2012), 15\u2013\n32 (2012)\n5. Buda, J., Bolonyai, F.: An ensemble model using n-grams and sta-\ntistical features to identify fake news spreaders on Twitter. In: CLEF(2020)\n6. Chamberlain, J.M., Spezzano, F., Kettler, J.J., Dit, B.: A network\nanalysis of twitter interactions by members of the US congress.ACM Trans. Soc. Comput. 4(1), 1\u201322 (2021)\n7. Common Sense Media: News and America\u2019s kids: How young\npeople perceive and are impacted by the news (2017)\n8. Davis, J., Goadrich, M.: The relationship between precision-recall\nand ROC curves. In: Cohen, W.W., Moore, A.W. (eds.) MachineLearning, Proceedings of the Twenty-Third International Confer-ence (ICML, ACM International Conference Proceeding Series,vol. 148, pp. 233\u2013240. ACM (2006)\n9. De Choudhury, M., Gamon, M., Counts, S., Horvitz, E.: Predicting\ndepression via social media. ICWSM 13, 1\u201310 (2013)\n10. Effron DA, Raj M Misinformation and Morality: Encountering\nFake-News Headlines Makes Them Seem Less Unethical to Pub-lish and Share. Psychological Science. 2020;31(1):75\u201387. https://\ndoi.org/10.1177/0956797619887896\n11. Gabielkov, M., Ramachandran, A., Chaintreau, A., Legout, A.:\nSocial clicks: What and who gets read on twitter? ACM SIGMET-RICS Perform. Eval. Rev. 44(1), 179\u2013192 (2016)\n12. Ghanem, B., Rosso, P., Rangel, F.: An emotional analysis of false\ninformation in social media and news articles. ACM Trans. InternetTechnol. (TOIT) 20(2), 1\u201318 (2020)\n13. Giachanou, A., Rissola, E.A., Ghanem, B., Crestani, F., Rosso,\nP.: The role of personality and linguistic patterns in discriminat-ing between fake news spreaders and fact checkers. In: NaturalLanguage Processing and Information Systems: 25th InternationalConference on Applications of Natural Language to InformationSystems, NLDB 2020, p. 181. Springer\n14. Gottfriedd, J., Grieco, E.: Younger Americans are Better than Older\nAmericans at Telling Factual News Statements from Opinions. PewResearch Center (2018)\n15. Guess, A., Nagler, J., Tucker, J.: Less than you think: prevalence\nand predictors of fake news dissemination on Facebook. Sci. Adv.5(1), eaau4586 (2019)\n16. Gupta, A., Lamba, H., Kumaraguru, P., Joshi, A.: Faking sandy:\ncharacterizing and identifying fake images on Twitter during hur-ricane sandy. In: Proceedings of the 22nd international conferenceon World Wide Web, pp. 729\u2013736 (2013)\n17. Hall, J.A., Pennington, N., Lueders, A.: Impression management\nand formation on Facebook: a lens model approach. New Media\nSoc. 16(6), 958\u2013982 (2014)\n18. Hemphill, L., Culotta, A., Heston, M.: # polar scores: measuring\npartisanship using social media content. J. Inf. Technol. Politics13(4), 365\u2013377 (2016)\n19. Hills, T.T.: The dark side of information proliferation. Perspect.\nPsychol. Sci. 14(3), 323\u2013330 (2019)\n20. Horne, B.D., Adali, S.: This just in: fake news packs a lot in title,\nuses simpler, repetitive content in text body, more similar to satirethan real news. In: The 2nd International Workshop on News andPublic Opinion at ICWSM (2017)\n21. Horne, B.D., Nevo, D., O\u2019Donovan, J., Cho, J., Adali, S.: Rating\nreliability and bias in news articles: Does AI assistance help every-one? In: Proceedings of the Thirteenth International Conference onWeb and Social Media, ICWSM 2019, pp. 247\u2013256 (2019)\n22. Isaak, J., Hanna, M.J.: User data privacy: Facebook, Cambridge\nanalytica, and privacy protection. Computer 51(8), 56\u201359 (2018)\n23. Lee, C.S., Ma, L.: News sharing in social media: the effect of\ngrati\ufb01cations and prior experience. Comput. Hum. Behav. 28(2),\n331\u2013339 (2012)\n123\nInternational Journal of Data Science and Analytics\n24. Lundberg, S.M., Lee, S.I.: A uni\ufb01ed approach to interpreting model\npredictions. In: Guyon, I., Luxburg, U.V ., Bengio, S., Wallach,H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances inNeural Information Processing Systems, vol. 30, pp. 4765\u20134774.Curran Associates, Inc. (2017). http://papers.nips.cc/paper/7062-\na-uni\ufb01ed-approach-to-interpreting-model-predictions.pdf\n25. Ma, L., Lee, C.S., Goh, D.H.: Understanding news sharing in social\nmedia from the diffusion of innovations perspective. In: 2013 IEEEInternational Conference on Green Computing and Communica-tions and IEEE Internet of Things and IEEE Cyber, Physical andSocial Computing, pp. 1013\u20131020. IEEE (2013)\n26. Milton, A., Batista, L., Allen, G., Gao, S., Ng, Y ., Pera, M.S.:\n\u201cDon\u2019t judge a book by its cover\u201d: Exploring book traits childrenfavor. In: Santos, R.L.T., Marinho, L.B., Daly, E.M., Chen, L., Falk,K., Koenigstein, N., de Moura, E.S. (eds.) RecSys 2020: FourteenthACM Conference on Recommender Systems, Virtual Event, Brazil,September 22\u201326, 2020, pp. 669\u2013674. ACM (2020). https://doi.\norg/10.1145/3383313.3418490\n27. Milton, A., Pera, M.S.: What snippets feel: Depression, search,\nand snippets. In: Cantador, I., Chevalier, M., Melucci, M., Mothe,J. (eds.) Proceedings of the Joint Conference of the InformationRetrieval Communities in Europe (CIRCLE 2020), Samatan, Gers,\nFrance, July 6\u20139, 2020, CEUR Workshop Proceedings, vol. 2621.\nCEUR-WS.org (2020). http://ceur-ws.org/V ol-2621/CIRCLE20_\n15.pdf\n28. Mitchell, A., Gottfriedd, J., Barthel, M., Sumida, N.: Distinguish-\ning Between Factual and Opinion Statements in the News. PewResearch Center (2018)\n29. Mohammad, S.: Word affect intensities. In: Proceedings of the\nEleventh International Conference on Language Resources andEvaluation, LREC 2018, Miyazaki, Japan, May 7\u201312, 2018. Euro-pean Language Resources Association (ELRA) (2018)\n30. Neuman, Y .: Computational Personality Analysis: Introduction,\nPractical Applications and Novel Directions. Springer (2016)\n31. Oh, H.J., Ozkaya, E., LaRose, R.: How does online social net-\nworking enhance life satisfaction? The relationships among onlinesupportive interaction, affect, perceived social support, sense ofcommunity, and life satisfaction. Comput. Hum. Behav. 30, 69\u201378\n(2014). https://doi.org/10.1016/j.chb.2013.07.053\n32. Pizarro, J.: Using n-grams to detect fake news spreaders on Twitter.\nIn: CLEF (2020)\n33. Poindexter, P., Harp, D.: The softer side of news. In: P. Poindexter,\nS. Meraz, A.S. Weiss (eds.) Women, menand news: Divided anddisconnected in the news medialandscape, pp. 85\u201396. RoutledgeNew York (2008)\n34. Rangel, F., Giachanou, A., Ghanem, B., Rosso, P.: Overview of the\n8th author pro\ufb01ling task at PAN 2020: pro\ufb01ling fake news spreaders\non Twitter. In: Cappellato, L., Eickhoff, C., Ferro, N., N\u00e9v\u00e9ol, A.\n(eds.) CLEF 2020 Labs and Workshops, Notebook Papers. CEURWorkshop Proceedings (2020). CEUR-WS.org\n35. Rangel, F., Giachanou, A., Ghanem, B., Rosso, P.: Overview of the\n8th author pro\ufb01ling task at pan 2020: pro\ufb01ling fake news spreaderson Twitter. In: CLEF (2020)\n36. Reis, J.C., Kwak, H., An, J., Messias, J., Benevenuto, F.: Demo-\ngraphics of news sharing in the us Twittersphere. In: Proceedingsof the 28th ACM Conference on Hypertext and Social Media, pp.195\u2013204 (2017)\n37. Rini, R.: Fake news and partisan epistemology. Kennedy Inst.\nEthics J. 27(2), E-43 (2017)\n38. Shearer, E., Mitchell, A.: News Use Across Social Media Platforms\nin 2020. Pew Research Center (2020)\n39. Shoemaker, P.J., V os, T.P., Reese, S.D.: Journalists as gatekeep-\ners. In: K. Wahl-Jorgensen, T. Hanitzsch (eds.) The handbook ofjournalism studies, vol. 73, p. 15 (2009)\n40. Shrestha, A., Serra, E., Spezzano, F.: Multi-modal social and\npsycho-linguistic embedding via recurrent neural networks to iden-tify depressed users in online forums. Netw. Model. Anal. Health\nInform. Bioinform. 9(1), 1\u201311 (2020)\n41. Shrestha, A., Spezzano, F.: Online misinformation: from the\ndeceiver to the victim. In: ASONAM \u201919: International Confer-ence on Advances in Social Networks Analysis and Mining, pp.847\u2013850. ACM (2019)\n42. Shrestha, A., Spezzano, F.: Textual characteristics of news title and\nbody to detect fake news: a reproducibility study. In: Advancesin Information Retrieval\u201443rd European Conference on IRResearch, ECIR 2021, Proceedings, Part II, Lecture Notes in Com-puter Science, vol. 12657, pp. 120\u2013133. Springer (2021)\n43. Shrestha, A., Spezzano, F., Joy, A.: Detecting fake news spreaders\nin social networks via linguistic and personality features. In: CLEF(2020)\n44. Shu, K., Mahudeswaran, D., Wang, S., Lee, D., Liu, H.: Fake-\nnewsnet: a data repository with news content, social context anddynamic information for studying fake news on social media, vol.8. arXiv preprint arXiv:1809.01286 (2018)\n45. Shu, K., Sliva, A., Wang, S., Tang, J., Liu, H.: Fake news detection\non social media: a data mining perspective. ACM SIGKDD Explor.Newslett. 19(1), 22\u201336 (2017)\n46. Shu, K., Zhou, X., Wang, S., Zafarani, R., Liu, H.: The role of user\npro\ufb01les for fake news detection. In: ASONAM \u201919: International\nConference on Advances in Social Networks Analysis and Mining,pp. 436\u2013439. ACM (2019)\n47. This analysis shows how viral fake election news stories out-\nperformed real news on facebook, 2016. https://www.buzzfeed.\ncom/craigsilverman/viral-fake-election-news-outperformedreal-news-on-facebook (2016) facebook, 2016. https://www.buzzfeed.\ncom/craigsilverman/viral-fake-election-news-outperformedreal-news-on-facebook (2016)\n48. Spezzano, F., Shrestha, A., Fails, J.A., Stone, B.W.: That\u2019s fake\nnews! Investigating how readers identify the reliability of newswhen provided title, image, source bias, and full articles. In: Pro-ceedings of the ACM on Human Computer Interaction Journal, vol.5 (CSCW1, Article 109) (2021)\n49. Spiro, E.S., Fitzhugh, S., Sutton, J., Pierski, N., Greczek, M., Butts,\nC.T.: Rumoring during extreme events: a case study of deepwaterhorizon 2010. In: Proceedings of the 4th Annual ACM Web ScienceConference, pp. 275\u2013283 (2012)\n50. V ogel, L.: (2017). Viral misinformation threatens public health.\nCMAJ : Canadian Medical Association journal, 189(50), E1567.https://doi.org/10.1503/cmaj.109-5536\n51. V osoughi, S., Roy, D., Aral, S.: The spread of true and false news\nonline. Science 359(6380), 1146\u20131151 (2018)\n52. Wang, W., Hernandez, I., Newman, D.A., He, J., Bian, J.: Twitter\nanalysis: studying us weekly trends in work stress and emotion.\nAppl. Psychol. 65(2), 355\u2013378 (2016)\n53. Wang, Z., Hale, S., Adelani, D.I., Grabowicz, P., Hartman, T.,\nFl\u00f6ck, F., Jurgens, D.: Demographic inference and representativepopulation estimates from multilingual social media data. In: TheWorld Wide Web Conference, pp. 2056\u20132067 (2019)\n54. Wineburg, S., McGrew, S., Breakstone, J., Ortega, T.: Evaluating\ninformation: The cornerstone of civic online reasoning. StanfordDigital Repository. Retrieved January 8, 2018 (2016)\n55. Yaqub, W., Kakhidze, O., Brockman, M.L., Memon, N., Patil, S.:\nEffects of credibility indicators on social media news sharing intent.\nIn: Proceedings of the 2020 CHI Conference on Human Factors inComputing Systems, CHI\u201920, pp. 1\u201314 (2020)\nPublisher\u2019s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional af\ufb01liations.\n123", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Characterizing and predicting fake news spreaders in social networks", "author": ["A Shrestha", "F Spezzano"], "pub_year": "2022", "venue": "International journal of data science and \u2026", "abstract": "Due to its rapid spread over social media and the societal threat of changing public opinion,  fake news has gained massive attention. Users\u2019 role in disseminating fake news has"}, "filled": false, "gsrank": 439, "pub_url": "https://link.springer.com/article/10.1007/s41060-021-00291-z", "author_id": ["8lhmF9oAAAAJ", "u_5TCGoAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:PWkKUViEe0oJ:scholar.google.com/&output=cite&scirp=438&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D430%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=PWkKUViEe0oJ&ei=VLWsaISoBr_SieoPzJnloAQ&json=", "num_citations": 37, "citedby_url": "/scholar?cites=5367028895793703229&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:PWkKUViEe0oJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://par.nsf.gov/servlets/purl/10326101"}}, {"title": "Reliability estimation of news media sources: birds of a feather flock together", "year": "2024", "pdf_data": "Reliability Estimation of News Media Sources: Birds of a Feather Flock\nTogether\nSergio Burdisso\u22c6,1, Dairazalia S\u00e1nchez-Cort\u00e9s1, Esa\u00fa Villatoro-Tello1and Petr Motlicek1,2\n1Idiap Research Institute, Martigny, Switzerland\n2Brno University of Technology, Brno, Czech Republic\n{sergio.burdisso,dairazalia.sanchez-cortes,esau.villatoro,petr.motlicek}@idiap.ch\nAbstract\nEvaluating the reliability of news sources is a\nroutine task for journalists and organizations\ncommitted to acquiring and disseminating ac-\ncurate information. Recent research has shown\nthat predicting sources\u2019 reliability represents\nan important first-prior step in addressing ad-\nditional challenges such as fake news detec-\ntion and fact-checking. In this paper, we in-\ntroduce a novel approach for source reliability\nestimation that leverages reinforcement learn-\ning strategies for estimating the reliability de-\ngree of news sources. Contrary to previous\nresearch, our proposed approach models the\nproblem as the estimation of a reliability de-\ngree, and not a reliability label, based on how\nall the news media sources interact with each\nother on the Web. We validated the effective-\nness of our method on a news media reliability\ndataset that is an order of magnitude larger than\ncomparable existing datasets. Results show that\nthe estimated reliability degrees strongly cor-\nrelates with journalists-provided scores (Spear-\nman=0.80) and can effectively predict reliabil-\nity labels (macro-avg. F 1score=81.05). We\nrelease our implementation and dataset, aiming\nto provide a valuable resource for the NLP com-\nmunity working on information verification.\n1 Introduction\nAs of 2023, the number of internet users is over\n5.18 billion worldwide, meaning that around two-\nthirds of the global population is currently con-\nnected to the WWW (Petrosyan, 2023). The Web\nhas democratized and radically changed how peo-\nple consume and produce information by shifting\nthe paradigm from a news-centred one to a user-\ncentred one. Nowadays, any person on the Web\ncan potentially be a \u201cnews medium\u201d providing in-\nformation either by creating websites, blogs and/or\nby making use of social media platforms.\nNevertheless, news media can no longer perform\nits role as \u201cgatekeeper\u201d deciding which stories todisseminate to the public or not (Munger, 2020)\nsince most of the information on the Internet is\nunregulated by nature (Cuan-Baltazar et al., 2020).\nAs a consequence, an enormous proliferation of\nmisinformation has emerged leaving the public\nvulnerable to incorrect or misleading information\nabout the state of the world which, among others,\nincreased polarization and decreased trust in in-\nstitutions and experts (Lewandowsky et al., 2017;\nStr\u00f6mb\u00e4ck et al., 2020). The World Health Or-\nganization (WHO) recently declared a worldwide\n\u201cinfodemic\u201d characterized by an overabundance of\nmisinformation (Van Der Linden, 2022). The best-\nknown type of misinformation is fake news (Lazer\net al., 2018) defined as \u201cfalse information intention-\nally created to mislead and/or manipulate a public\nthrough the appearance of a news format with an\nopportunistic structure to attract the reader\u2019s atten-\ntion\u201d (Baptista and Gradim, 2022).\nIn an attempt to limit the impact of fake news, a\nlarge number of initiatives have been undertaken\nby media, journalists, governments, and interna-\ntional organizations to identify true and false infor-\nmation across the globe (Shaar et al., 2020). For\ninstance, the Duke University\u2019s center for journal-\nism research, the Reporters\u2019 Lab, lists a total of\n419 fact-checking active sites online1from which\nFactCheck.org, Snopes, Full Fact and Politifact are\nthe most well-known. These sites manually and\nsystematically assess the validity of thousands of\nclaims. However, human annotators will always be\noutnumbered by the claims that need to be verified,\nreducing the impact of such services in a large-\nscale scenario. Consequently, we have witnessed a\ngrowing interest in using different machine learn-\ning models, ranging from non-neural (Kwon et al.,\n2013; Popat et al., 2016; Nguyen et al., 2018) to\ndeep learning-based ones (Ma et al., 2016; Wang,\n1https://reporterslab.org/fact-checking/ (Oct.\n2023)arXiv:2404.09565v1  [cs.CL]  15 Apr 2024\n2017; Popat et al., 2018b; Wang et al., 2018; Fajcik\net al., 2023), to determine the validity of claims,\nnews and online information. Nevertheless, these\nmodels\u2019 performance still has not reached confi-\ndent accuracy values, limiting their applicability in\nreal-world scenarios (Baly et al., 2018).\nA more recent paradigm to fight fake news pro-\nposes to focus on the source rather than on the\ncontent (Baly et al., 2018, 2020), a task referred\nto as profiling news medium. The underlying hy-\npothesis states that even though fake news spreads\nmainly through social media, they still need an ini-\ntial website hosting the news. Hence, if information\nabout websites is known in advance, identifying\npotentially fake news can be done by verifying the\nreliability of the source. In fact, this activity is\nalso performed by journalists, who often consult\nrating services for news websites like NewsGuard2\nor MBFC3. Nonetheless, these services are not ex-\nhaustive and difficult to keep up-to-date as they\nrely on human evaluators, highlighting the need for\nscalable automatic solutions that can be applied in\nreal-world scenarios.\nPrevious research has shown that predicting re-\nliability is an important first-prior step for fact-\nchecking systems (Nguyen et al., 2018; Popat et al.,\n2017; Mukherjee and Weikum, 2015) and also\nthe most important aspect that journalists consider\nwhen manually verifying the trustworthiness of the\ninformation (Baly et al., 2018). Thus, in this paper,\nwe focus on the task of source reliability estima-\ntion, i.e., automatically analyzing the source that\nproduces a given piece of information and deter-\nmining its reliability degree. Concretely, we ad-\ndress the posed task by investigating the following\nresearch question: to what extent can we predict\nthe reliability of a news media source solely based\non its interactions with other sources? Contrary to\nprevious research, our proposed method represents\na scalable and language-independent approach that\ncan be further enriched via content-based features.\nOur performed experiments shed light on the im-\nmediate (positive) effects of profiling news medi-\nums through its interactions with other sources and\nalso in combination with traditional content-based\nattributes. Our research holds the potential to un-\ncover deeper insights by incorporating more recent\ncontent-based technologies to further explore the\nnuanced dynamics of news websites, opening the\n2https://www.newsguardtech.com/\n3https://mediabiasfactcheck.comdoor to a broader NLP research community.\nOverall, the main contributions of this paper\ncan be summarized as follows: (i)we propose a\nmethodology capable of modeling the source re-\nliability estimation problem in a real-world scale\nscenario that contrary to previous research, esti-\nmates the reliability degree ( i.e.a continuous value)\nrather than a categorical value and does not depend\non any third-party resources; (ii)we pioneer the\nintroduction and evaluation of different algorithms\nto estimate the reliability score, exploring a spec-\ntrum from vanilla reinforcement learning strategies\nto task-specific variations; (iii)we build the largest\nnews media reliability dataset available, orders of\nmagnitude larger than existing datasets; (iv)we\npresent empirical evidence that establishes the fea-\nsibility of predicting the reliability of a news media\nsource solely through its interactions with other\nsources (which further improves when content-\nbased features are incorporated); and (v)we release\nboth the dataset and source code to the wider NLP\nresearch community.4\n2 Related Work\nThe task of determining information veracity has\nbeen approached from different angles and perspec-\ntives, from micro to macro, depending on the object\nof study. For instance, fact-checking focuses on\nvalidating a single statement, i.e.the claim; fake\nnews detection analyses a whole document, i.e.the\ncontent of the news article. In this work, we focus\non the source that produces a given piece of infor-\nmation, also known as source reliability estimation .\nWithin social media, the sources are individual\nusers creating the content, and previous work has\nfocused on identifying different types of users such\nasspammers (Liubchenko et al., 2022; Stringhini\net al., 2010), bots (Lei et al., 2023; Knauth, 2019),\nfake profiles (Roy and Chahar, 2020; Ramalingam\nand Chinnaiah, 2018), paid users (Mihaylov et al.,\n2015b), and trolls (Tomaiuolo et al., 2020; Miao\net al., 2020; Mihaylov et al., 2015a), among oth-\ners (Sansonetti et al., 2020; Burdisso et al., 2022).\nHowever, in the broader case of the WWW, sources\nare individual websites (Dong et al., 2015), and in\nour case, news media websites.\nPrevious studies have tangentially addressed\nnews media source reliability as part of the study of\nautomatic fact-checking systems, either as a prior\n4https://github.com/idiap/\nNews-Media-Reliability\nin probabilistic graphical models (Nguyen et al.,\n2018; Mukherjee and Weikum, 2015) or as fea-\ntures for stance classification models (Popat et al.,\n2018a, 2017, 2016). In these studies, reliability\nestimation relied on indirect measures since no\ngold labels were used. For instance, some works\nuse the AlexaRank5andPageRank (Brin and Page,\n1998) scores of the websites as proxies for their\nreliability (Baly et al., 2018; Popat et al., 2016)\nwhile others the proportion of articles that refute\nfalse claims and support true claims (Popat et al.,\n2018a, 2017). However, in the latter, authors rely\non a fact-checking model and the selected true and\nfalse claims while, in the former, on scores that\nonly capture the authority and popularity of the\nsources, not necessarily their trustworthiness \u2014 for\ninstance, think of popular unreliable gossip web-\nsites6or satirical news websites, like The Onion ,7\nhighly popular, attracting huge web traffic.\nRecently, Baly et al. (2020, 2019, 2018) ad-\ndressed the source reliability estimation task on\nits own, modeling it as a classification task using\nsource-level gold annotations. In particular, au-\nthors focused on predicting websites factual report-\ning and political bias using the values published\nby a news media rating service as ground truth.\nHowever, their proposed method relies on collect-\ning and extracting information from multiple ex-\nternal and restricted sources (e.g. Twitter, Face-\nbook, YouTube, etc.) for generating content-based,\naudience-based, and metadata-based features for\nthe classification model, limiting its practical use\non a large-scale scenario. In this paper, we also\naddress the task using gold annotations, however,\nwe adopt an easier-to-scale approach. Specifically,\nwe model the problem as estimating a continuous\nvalue (i.e. the reliability degree ) based simply on\nhow all news media sources interact with each other\non the World Wide Web.\n3 Methodology\n3.1 Problem Formulation\nLetSbe the set of all news media sources on the\nWeb, and G=\u27e8S, E, w \u27e9be the weighted directed\ngraph where there is an edge (s, s\u2032)\u2208Eif source s\ncontains articles (hyper) linked to s\u2032and where the\nweight w(s, s\u2032)\u2208[0,1]is the proportion of total\n5https://www.alexa.com/\n6http://www.ebizmba.com/articles/\ngossip-websites\n7https://www.theonion.com/hyperlinks in slinked to s\u2032. Given two disjoint sub-\nsets\u02c6S+,\u02c6S\u2212\u2282Scontaining, respectively, some\nknown reliable and unreliable news sources, the\ngoal is to estimate the reliability degree \u03c1(s)for all\ns\u2208S. More precisely, a total function \u03c1:S7\u2192R\nsuch that:\n1.\u03c1(s)>0ifsis reliable\n2.\u03c1(s)\u22640ifsis unreliable\n3.\u03c1(s)< \u03c1(s\u2032)ifs\u2032is more reliable than s.\nThe underlying intuition behind using hyperlinks\nto build the graph is that the more frequently one\nsource links to another (i.e., the higher w(s, s\u2032)),\nthe higher the chances of a random reader to\n(click and) reach the reliable/unreliable source s\u2032\nfrom s. Notably, hyperlinks also serve as a proxy\nfor content-based interactions, as they are typi-\ncally used to cite content from the referred arti-\ncle. Thus, a higher w(s, s\u2032)also implies a stronger\ncontent-based relationship. Therefore, this simple\nweighted, hyperlink-based, and source-centered ap-\nproach potentially captures both interaction types\namong news sources simultaneously, while being\nrelatively easy to scale.\n3.2 Reinforcement Learning Strategy\nOur reinforcement learning reliability framework\nmodels reliability in terms of a Markov Decision\nProcess (MDP). An MDP is defined by a 4-tuple\n\u27e8S, A, P a, ra\u27e9where Sis a set of states, Aa set of\nactions, Pa(s, s\u2032)is the probability that action a\nin state swill lead to state s\u2032, and ra(s, s\u2032)is the\nimmediate reward perceived after taking action ain\nstatesleading to state s\u2032(Sutton and Barto, 2018;\nPuterman, 2014; Kaelbling et al., 1996).\nGiven an MDP, a decision process is then speci-\nfied by defining a policy \u03c0that provides the prob-\nability of taking action ain state s. In turn, given\na policy \u03c0we can estimate the value of each state,\nV\u03c0(s), in terms of how good it is to be in that state\nfollowing the policy. In particular, the value V\u03c0(s)\nis given by the Bellman equation which is defined,\nfor any state s\u2208S, recursively as:\nV\u03c0(s) =X\ns\u2032\u2208SP\u03c0(s, s\u2032)[r(s\u2032) +\u03b3V\u03c0(s\u2032)](1)\nwhere \u03b3\u2208[0,1)is known as the discount factor\nandr(s)is the immediate reward received when\nreaching s. Thus, we address the reliability estima-\ntion as an MDP \u27e8S, A, P, r \u27e9such that: (a) The set\nAlgorithm 1 F-Reliability strategy for \u03c1(s).\nSet\u2200s\u2208S, \u03c1(s) = 0\nrepeat\n\u2206 = 0\nfor all s\u2208Sdo\n\u03c1\u2032(s) =P\ns\u2032\u2208SP(s, s\u2032)[r(s\u2032) +\u03b3\u03c1(s\u2032)]\n\u2206 =max(\u2206,|\u03c1\u2032(s)\u2212\u03c1(s)|)\n\u03c1=\u03c1\u2032\nuntil\u2206is small enough\nof states Sare all the news media websites on the\nWeb \u2014 i.e.we have S=S; (b) The set of actions A\ncontains only one element, the \"move to a different\nnews media website\" action; (c) The probability P\nof moving from stos\u2032will be given by the propor-\ntion of hyperlinks in sconnecting to s\u2032\u2014i.e.we\nhaveP(s, s\u2032) =w(s, s\u2032); and (d) The reward rof\nmoving to a source is determined only by the source\nitself, and it will be positive or negative for known\nreliable or unreliable sources respectively \u2014 i.e.\nwe have r(s, s\u2032) =r(s\u2032)where r:S7\u2192Rsuch\nthatr(s) = 1 ifs\u2208\u02c6S+;r(s) =\u22121ifs\u2208\u02c6S\u2212;\nr(s) = 0 otherwise. In simple words, we can think\nof modeling the problem as if there was a \u201cvirtual\nuser\u201d browsing from one news media source to an-\nother with probability proportional to how strongly\nconnected they are, and who will perceive a posi-\ntive or negative signal (the reward) when arriving to\nknown reliable or unreliable sources, respectively.\nGiven this framework, now the challenge is how to\nestimate the reliability scores \u03c1(s).\n3.2.1 Perceived Future Reliability\nUnder this simple framework, our initial approach\ninvolves estimating reliability by \u201clooking to the\nfuture\u201d. To be more precise, we will assume the re-\nliability degree \u03c1(s)is proportional to the expected\nperceived reliability (reward) by the virtual user.\nConsequently, a source is considered more reliable\n(or unreliable) if it is expected to guide the virtual\nuser to well-known reliable (or unreliable) sources.\nTo achieve this, we can simply set \u03c1(s) =V(s),\nas Equation 1 defines V(s)as the discounted long-\nterm future rewards received following a policy\n\u03c0. Note that, given that we only have one pos-\nsible action in A, the policy \u03c0is trivial and thus\nP\u03c0(s, s\u2032) =P(s, s\u2032). Therefore, a source swill\nhave a higher (lower) \u03c1(s)the more positive (neg-\native) its total expected future reward V(s). In\nother words, it reflects how much sis expected toAlgorithm 2 P-Reliability strategy for \u03c1(s).\nSet\u2200s\u2208S, \u03c1(s) = 0\nrepeat\n\u2206 = 0\nfor all s\u2208Sdo\n\u03c1\u2032(s) =r(s) +\u03b3P\ns\u2032\u2208SP(s\u2032, s)\u03c1(s\u2032)\n\u2206 =max(\u2206,|\u03c1\u2032(s)\u2212\u03c1(s)|)\n\u03c1=\u03c1\u2032\nuntil\u2206is small enough\nguide us to known reliable (unreliable) sources,\nas intended. We will refer to this strategy as\n\u201cF-Reliability\u201d . In practice, the computation of\nV(s)can be done using the Value Iteration algo-\nrithm (Sutton and Barto, 2018). Thus, we compute\n\u03c1(s)for our specific MDP as shown in Alg. 1.\n3.2.2 Accumulated Past Reliability\nAn alternative approach is to estimate reliability by\n\u201clooking to the past\u201d rather than the future. Specif-\nically, we assume that the reliability degree \u03c1(s)\nis proportional to the accumulated reliability (re-\nward) perceived by the virtual user in reaching the\ncurrent source s. Consequently, a source becomes\nmore reliable (unreliable) as more known reliable\n(unreliable) sources lead to it.\nTo formalize the above intuition, we leverage the\nreverse Bellman equation introduced by Yao and\nSchuurmans (2013). This equation is recursively\ndefined for any state s\u2208Sas:\nR\u03c0(s) =r(s) +\u03b3X\ns\u2032\u2208SP\u03c0(s\u2032, s)R\u03c0(s\u2032) (2)\nIn contrast to Equation 1 that looks forward\nfrom a state to define its value, this equation looks\nbackward to define it \u2014note P\u03c0(s, s\u2032)is swapped\ntoP\u03c0(s\u2032, s). More precisely, while V(s)defines\nthe value of a state based on the forward accumu-\nlated reward, R(s)does so in terms of the histor-\nical accumulated reward. Therefore, by setting\n\u03c1(s) =R(s), a source swill have a higher (lower)\n\u03c1(s)the more positive (negative) is the accumu-\nlated reward R(s)\u2014i.e.the more known reliable\n(unreliable) sources lead to s, as intended. We will\nrefer to this strategy as \u201cP-Reliability\u201d . In practice,\nwe can again employ Value Iteration to compute\n\u03c1(s)using R(s)as shown in Algorithm 2.\n3.2.3 Past and Future Perceived Reliability\nLastly, we can explore an approach that combines\nboth \u201cthe future and the past\u201d. Intuitively, we can\nargue that the transfer of reliability between news\nAlgorithm 3 I-Reliability strategy for \u03c1(s).\nSet\u2200s\u2208S, \u03c1(s) =r(s)\nrepeat\nfor all s\u2208Sdo \u25b7Investment step\ntotalcredits (s) =P\ns\u2032\u2208Sw(s\u2032, s)\u03c1(s\u2032)\nfor all s\u2208Sdo \u25b7Credit collection step\nprofit =P\ns\u2032\u2208Sw(s, s\u2032)credits s(s\u2032)\n\u03c1(s) =\u03c1(s) +profit\nuntil ntimes\nmedia sources and their neighboring sources is\nasymmetric. Specifically, the impact on the re-\nliability, \u03c1(s), of a source swhen referencing a\nreliable source is not equivalent to the effect of\na reliable source referencing s.8Moreover, this\nasymmetry extends to both reliable and unreliable\nsources. That is, a reliable source referencing s\ncarries a different weight than an unreliable one\nreferencing s, and vice versa.9In a broader sense,\nwe can think of a source sincreasing its reliability\n\u03c1(s)as more reliable sources link to it, while losing\nreliability as it links to more unreliable sources.\nTo formalize this asymmetric behavior, we can\nincorporate both R(s)andV(s)into our reliability\nmodel. More precisely, let V\u2212(s)beV(s)where\nonly negative rewards r(s)are allowed, and analo-\ngously, R+(s)with only positive rewards, then we\ncan define \u03c1(s)as:\n\u03c1(s) =V\u2212(s) +R+(s) (3)\nAs a result, a source swill have a higher reli-\nability \u03c1(s)the more reliable sources link to it\n(i.e.the higher R+(s)), and lower reliability the\nmore it links to unreliable sources ( i.e.the lower\nV\u2212(s)). We will refer to this strategy as \u201cFP-\nReliability\u201d .\n3.3 Reliability Investment Strategy\nA well-established algorithm used in the field of\ntruth discovery (Li et al., 2016) is the Investment\nalgorithm (Pasternack and Roth, 2010). This algo-\nrithm is an iterative method in which two interde-\npendent steps are repeated: (1) sources uniformly\n\u201cinvest\u201d their trustworthiness among their claimed\nvalues; (2) sources collect credits back from the\nclaimed values which update, in turn, their trust-\nworthiness. Inspired by this \u201cinvest and collect\u201d\n8e.g.it is not the same for your reputation as a news media\nif The New York Times references you as you referencing it.\n9e.g. The New York Times referencing you has not the\nsame impact on your reputation as a fake news media refer-\nencing you, or as you referencing a fake news media.CC-News Graph\nsnapshot #articles #nodes #edges\n2019/08 17M 6,799 171,810\n2020/08 23M 11,427 276,666\n2021/08 28M 10,938 315,447\n2022/08 35M 10,607 354,386\nall above 103M 17,057 909,354\nTable 1: CC-News snapshots and the obtained graphs.\nThe last row corresponds to our final graph.\nintuition, we now formulate an algorithm based\non the same principle. Initially, each source will\ndistribute its reliability \u03c1(s)among neighboring\nsources in proportion to the strength of their links,\ni.e.\u221dw(s, s\u2032). In essence, during the investment\nstep, the total credits invested in each source sis\ndefined as follows:\ntotalcredits (s) =X\ns\u2032\u2208Sw(s\u2032, s)\u00b7\u03c1(s\u2032)(4)\nIn the subsequent credit collection step, the total\ncredits are distributed among investors, s\u2032, in pro-\nportion to their contribution to the source s:\ncredits s\u2032(s) =ws\u2032(s)\u00b7totalcredits (s)(5)\nHere, ws\u2032(s)\u2208[0,1]represents the proportion of\ntotal inbound hyperlinks in soriginating from s\u2032.\nThe reliability degree is then updated by collect-\ning the credits back in proportion to the invested\npercentage:\n\u03c1(s) =\u03c1(s) +X\ns\u2032\u2208Sw(s, s\u2032)\u00b7credits s(s\u2032)(6)\nFinally, we repeat this process ntimes to update\n\u03c1(s)considering values from up to n-hop-away\nsources in the graph, as illustrated in Algorithm 3.\n4 Data\n4.1 A real-world scale news media graph\nThe Common Crawl Foundation10maintains the\nCommon Crawl News Dataset (CC-News ), the\nworld\u2019s largest collection of news articles crawled\nfrom global news web sites since 2016. The data is\nupdated daily and published as a series of snapshots\norganized by year and month.\nWe developed a Python CC-News processing\npipeline that takes care of building the news me-\ndia graph, G, from CC-News snapshots (details in\nAppendix C). Similar to the CCNet pipeline (Wen-\nzek et al., 2020), our pipeline utilizes the language\nclassifier from fastText (Joulin et al., 2017; Grave\net al., 2018) to categorize news articles into 176\nlanguages. Consequently, for a given CC-NEWS\n10https://commoncrawl.org\nsnapshot URL, the pipeline generates one graph\nfor each supported language showing how news\nsources relate to each other in that language. How-\never, in this paper, we focus exclusively on the\nEnglish graph due to the predominance of available\nground truth data for experimentation in this lan-\nguage. Specifically, for experimentation, we will\nuse the English graph obtained from joining four\ndifferent CC-News snapshots corresponding to Au-\ngust over the past four years (2019 to 2022).11As\nindicated in Table 1, this process resulted in a uni-\nfied graph containing around 17k English-speaking\nnews media sources and nearly 1M connections\n\u2014graph shown in Figure 2 (Appendix B).\n4.2 Ground truth datasets\nTo facilitate a comparative analysis with previous\nstudies on the source reliability classification task,\nwe employ the largest dataset published earlier by\nBaly et al. (2018). This dataset encompasses 1066\nannotated news media URL domains extracted\nfrom Media Bias/Fact Check (MBFC) \u2014refer to\nthe first row of Table 2 for details.12Furthermore,\nfor a more comprehensive evaluation, we employ\nan extended dataset meticulously created by merg-\ning ground truth labels from various sources, as\noutlined below:\n\u2022MBFC: we followed a similar process as in Baly\net al. (2018) but crawling the entire MBFC web-\nsite to extract 4138 ground truth labels. Follow-\ning Gruppi et al. (2020), we aggregated these la-\nbels into three classes: \u201creliable\u201d for sources with\nhigh or very high factual reporting, \u201cunreliable\u201d for\nsources flagged as conspiracy, pseudoscience, or\nwith low/very low factual reporting, and \u201cmixed\u201d\nfor sources with mixed factual reporting.\n\u2022Wikipedia\u2019s perennial sources: the platform\nhosts a list of sources discussed by the commu-\nnity regarding their reliability and use on the plat-\nform.13We extracted 553 ground truth labels from\nthis list applying the following policy: sources\nmarked as generally reliable were labeled as \u201cre-\nliable\u201d; sources marked as generally unreliable ,\ndeprecated , orblacklisted were labeled as \u201cunreli-\nable\u201d; and sources marked as no consensus ,stale\ndiscussions ordiscussion in progress as \u201cmixed\u201d;\n11By selecting the same month, we ensure a consistent 4-\nyear time span while limiting the processed news articles to\napproximately 100M.\n12Original factuality labels were transformed into reliability\nlabels following Gruppi et al. (2020) strategy.\n13https://en.wikipedia.org/wiki/Wikipedia:\nReliable_sources/Perennial_sourcesLabel distribution\nDataset unreliable mixed reliable\nBaly et al. (2018) 256 268 542\nOur own 1425 1461 2446\nMBFC 546 1363 2229\nWikipedia 298 98 157\nFake News 556 - -\nNewsGuard 25 - 60\nTable 2: Datasets details. Bottom part shows individual\ncontributions to our final dataset.\n\u2022Fake news: we manually collected a list of 556\nunreliable sources from fake news websites, in-\ncluding the Wikipedia list of fake news websites14\nand a report from the Institute for Strategic Di-\nalogue identifying active and inactive fake news\ndomains (ISD, 2020).\n\u2022NewsGuard: a paid rating service similar to\nMBFC, provides both a verdict and a reliability\nscore based on predefined journalistic criteria (de-\ntails in Appendix D). Due to license limitations,\nwe could only use the 85 ground truth values in-\ncluded in the NELA-GT-2018 dataset (N\u00f8rregaard\net al., 2019). However, as detailed in Section 5.2,\nthe inclusion of NewsGuard enables us to measure\nthe correlation between the estimated reliability de-\ngrees \u03c1(s)and the scores provided by journalists.\nHence, our final aggregated dataset comprises\n5332 news URL domains, each annotated with 3-\nclass reliability labels. As illustrated in Table 2, its\nscale surpasses that of the largest one to date (Baly\net al., 2018), being an order of magnitude larger.\nFor evaluation, it is crucial that the source sis\npresent in the graph, as we want to assess how\nwell the reliability degree \u03c1(s)is computed from it.\nTherefore, we limit our experimentation to using\nthe subset of the ground truth dataset correspond-\ning to the nodes within our graph. This subset con-\ntains approximately 40% of the total ground truth\nsources. In particular, 400 sources from Baly et al.\n(2018) (294 \"reliable,\" 85 \"mixed,\" and 21 \"unre-\nliable\") and 2117 sources from our own dataset\n(1630 \"reliable,\" 321 \"mixed,\" and 166 \"unreli-\nable\"). Additionally, since our goal is to evalu-\nate the ability of \u03c1(s)to distinguish reliable from\nunreliable sources (see conditions 1 and 2 in Sec-\ntion 3.1), we merge \"unreliable\" and \"mixed\" labels\nto create the following three experimentation sets:\n\u2022ExpsetA: 294reliable and 106 unreliable sources\n14https://en.wikipedia.org/wiki/List_of_fake_\nnews_websites\nfrom Baly et al. (2018).\n\u2022ExpsetB: 1630 reliable and 487 unreliable\nsources from our dataset.\n\u2022ExpsetB\u2212:1630 reliable and 166 unreliable\nsources. A simpler version of ExpsetB removing\n\u201cmixed\u201d from unreliable sources.\n5 Experiments and Evaluation Results\nFor experimentation, we define the reward values\nbased on ground truth labels as r(s) = 1 if the\nlabel is \u201creliable\u201d, r(s) =\u22121if \u201cunreliable\u201d, and\nr(s) = 0 otherwise. In addition, selecting ap-\npropriate hyperparameter values is crucial. For\nI-Reliability ,ncontrols how far to look in the neigh-\nborhood for investments (how many nodes away).\nSimilarly, in the reinforcement learning strate-\ngies, the discount factor \u03b3controls the distance\nof looking back/forward; \u03b3\u22480focuses mostly\non present reward r(s), while \u03b3\u22481considers all\nhistory/future to compute \u03c1(s). We performed a\ngrid search over n\u2208[1,10]and\u03b3\u2208[0.05,0.95]to\ndetermine the best hyperparameter values on each\nof the three experimental sets. The grid search was\nperformed using 5-fold cross validation selecting\nthenand\u03b3that obtained the best macro avg. F 1\non the reliability classification task, as described\nin Section 5.1. We observed that, independently of\nthe dataset, better reliability estimation is achieved\nwhen looking mostly at nearby sources , as better\nperformance was obtained with small n(n\u22642)\nand\u03b3(\u03b3 <0.5) values \u2014details in Appendix A.\n5.1 Reliability Classification Results\nIn this section, we focus on evaluating the first two\nconditions for \u03c1(s)given in Section 3.1. These\nconditions allow us to measure the ability of \u03c1(s)\nto distinguish reliable from unreliable sources. For\ncomparison, we follow the evaluation procedure\nfrom Baly et al. (2018) and report results for 5-\nfold cross-validation. More precisely, in each k-\nfold iteration, we only use ground truth rewards\nr(s)from four folds to compute \u03c1(s)for all 17k\nsources in the graph, and using conditions 1 and 2,\nallsin the hold-out fold are classified as reliable\n(\u03c1(s)>0) orunreliable (\u03c1(s)\u22640).\nTable 3 shows the evaluation results obtained\non the three experimentation sets along with two\nnaive baselines for reference, random and major-\nity class classifiers.15In addition, for ExpsetA , we\n15For a comprehensive view of additional metrics, such as\nprecision, recall, and confidence intervals, refer to Table 7 in\nthe Appendix.StrategyF1score\nData macro avg. reliable unreliable Acc.ExpsetAM-BL 42.33 84.66 0.00 73.44\nR-BL 48.85 61.76 35.94 52.33\nBaly18 67.87 84.81 50.92 76.95\nBaly20 65.24 82.99 47.50 74.37\nF-R 61.52 87.62 35.42 79.26\nP-R 72.67 90.05 55.29 83.79\nFP-R 69.28 89.23 49.34 82.29\nI-R 72.81 90.03 55.60 83.77ExpsetBM-BL 43.50 87.00 0.00 77.00\nR-BL 47.48 62.17 32.80 51.63\nF-R 61.85 79.72 43.98 70.34\nP-R 74.69 88.29 61.10 82.00\nFP-R 55.95 68.08 43.82 59.38\nI-R 75.51 89.30 61.72 83.28ExpsetB\u2212M-BL 47.58 95.15 0.00 90.76\nR-BL 39.17 63.04 15.31 48.55\nF-R 62.18 90.23 34.12 83.02\nP-R 78.90 95.83 61.97 92.48\nFP-R 59.20 84.66 33.74 75.11\nI-R 81.05 96.71 65.39 93.99\nTable 3: 5-fold cross-validation average results for re-\nliability classification. The best-performing values are\nunderlined , while the 2nd-best results appear in bold\nfont. R-BL and M-BL refer to random and majority\nclass baselines; Baly18 and Baly20 refer to Baly et al.\n(2018) and Baly et al. (2020); and *-R stands for *-\nReliability.\nalso report the results obtained using the classifi-\ncation models introduced in previous works (Baly\net al., 2018, 2020). These classifiers combine mul-\ntiple content-based, audience-based, and metadata-\nbased features about the sources. Authors released\nthe pre-computed features values for the Baly et al.\n(2018) dataset and thus, using their source code,16\nwe were able to train and evaluate their classifiers\non the ExpsetA set. However, since building these\nfeatures relies on multiple external sources ( e.g.\nTwitter, Facebook, YouTube, etc.), we were not\nable to evaluate their method in our new dataset\ngiven current API restrictions to access them.\nObserving the performance across the different\ndatasets, all four strategies outperform the random\nand majority class baselines by a statistically signif-\nicant difference (paired t-test with p-value <0.02).\nIn addition, both P-Reliability andI-Reliability\nconsistently outperform other strategies, includ-\ning those presented in previously published works\n(p-value <0.03), however, the difference between\nthese two strategies is not statistically significant\n16github.com/ramybaly/News-Media-Reliability .\nStrategyF1score\nmacro avg. reliable unreliable Acc.\nP-Reliability 72.67 90.05 55.29 83.79\n+Baly18 77.11 87.75 66.47 82.11\n+Baly20 74.36 86.02 62.70 79.69\nI-Reliability 72.81 90.03 55.60 83.77\n+Baly18 77.47 87.89 67.06 82.34\n+Baly20 72.88 85.46 60.30 78.74\nTable 4: Ensemble results for P-Reliability and I-\nReliability strategies on ExpsetA . The best performance\nresults are underlined , while the 2nd-best appear in\nbold font.\n(p-value >0.5). From the results we can also see\nthat, regardless of the chosen strategy and dataset,\ntheF1score for the unreliable class consistently\nremains lower when compared to the reliable class.\nThis suggests that identifying unreliable sources\nis more challenging, likely due to the dataset im-\nbalance favoring reliable sources \u2014note that in\nboth ExpsetA and ExpsetB , only approximately\n25% of the sources are unreliable. This imbalance\nresults in models having fewer negative signals\n(i.e.rewards r(s) =\u22121) to learn to identify un-\nreliable sources effectively. Another contributing\nfactor is the inclusion of \u201cmixed\u201d labels in the un-\nreliable group, making the task more challenging\nby incorporating unreliable sources whose relia-\nbility is not clearly defined. This hypothesis is\nsupported by the results from ExpsetB\u2212, where\nthe removal of \u201cmixed\u201d labels results in improve-\nments across all metrics \u2014note that the highest\nunreliable F1score is achieved while the dataset\nis significantly more unbalanced (ten times fewer\nunreliable sources than reliable ones). Concerning\nthe reinforcement learning strategies, P-Reliability\nsignificantly ( p-value \u22640.02) outperformed F-\nReliability suggesting that the reliability of a source\nis more significantly influenced by its origins than\nby the destinations it reaches . On the other hand,\nFP-Reliability shows poor performance, mainly\ndue to the different nature of V\u2212(s)andR+(s)\nin Equation 3 \u2014note that V(s)is defined as an\nexpectation, whereas R(s)is not.17\nFinally, to assess the complementarity of our\n17Transition probabilities are not normalized in both direc-\ntions, only in the forward direction. Consequently, there is\nno inherent mathematical symmetry that ensures V\u2212(s)and\nR+(s)will equilibrate to zero ( \u03c1(s) = 0 ) when they cor-\nrespond to an equivalent number of unreliable and reliable\nsourcesRank Domain Score \u02c6\u03c1(s)\n1 bbc.co.uk 100.0 0.995\n2 cnbc.com 95.0 0.995\n3 dailysignal.com 92.5 0.830\n4 thinkprogress.org 90.0 0.907\n5 independent.co.uk 87.5 0.968\n1 sputniknews.com 7.5 -0.992\n2 truepundit.com 12.5 -0.957\n3 dailymail.co.uk 15.0 -0.998\n4 theduran.com 17.5 -0.954\n5thegatewaypundit.com 20.0 -0.994\nTable 5: NewsGuard top-5 unique most scored (top part)\nand least scored sources (bottom part) along with the\nestimated \u03c1(s)given by P-Reliability .\ngraph-based strategies with content-based ones, we\nperformed an additional experiment: a simple vot-\ning ensemble between our strategies and Baly\u2019s\nmodels. Specifically, sources were classified as\nreliable only when both models agreed on the clas-\nsification. Table 4 presents the obtained results\nfor our two best-performing models, P-Reliability\nandI-Reliability , onExpsetA .18The ensemble ap-\nproach enabled the models to further improve their\nperformance, particularly in detecting unreliable\nsources, achieving the highest macro avg. F1score\non this dataset (77.47).\n5.2 Correlation with human judgment\nIn this section, we focus on evaluating the final\ncondition in the definition of \u03c1(s)in Section 3.1.\nThis condition enables \u03c1(s)to assess the reliability\nofsrelative to other sources, allowing the ranking\nof sources based on their reliability degrees. For\nevaluation, we use the NewsGuard dataset intro-\nduced in Section 4.2 containing the 85 ground truth\nreliability scores provided by trained journalists.\nThe score ranges from 0 to 100 and is obtained by\nanswering 9 questions that address different jour-\nnalistic criteria \u2014details in Appendix D. Table 5\nshows examples of NewsGuard scores and their\nestimated reliability degree.19\nWe measure the correlation between these\nhuman-provided scores and their estimated reli-\nability degree by computing the Pearson correla-\ntion coefficient (PCC) and the Spearman\u2019s rank\ncorrelation coefficient (SRCC). PCC measures the\nlinear relationship between two variables, whereas\nSRCC assesses the monotonic relationship, making\n18Full results included in Table 7 (Appendix).\n19For ease of comparison, in this table, \u03c1(s)is normalized\nin the range [\u22121,1]by dividing it by the maximum (when\n\u03c1(s)\u22650) and minimum value (when \u03c1(s)<0).\nStrategy PCC p-value SRCC p-value\nRandom baseline 0.058 0.6 0.066 0.6\nPageRank baseline 0.313 0.008 0.544 1e-06\nF-Reliability\u26620.556 5e-07 0.295 1e-02\nP-Reliability\u26620.647 1e-09 0.668 2e-10\nFP-Reliability\u26620.636 3e-09 0.677 3e-09\nI-Reliability\u26620.589 7e-08 0.657 5e-10\nF-Reliability\u26630.927 1e-30 0.544 1e-06\nP-Reliability\u26630.912 9e-33 0.801 6e-17\nFP-Reliability\u26630.929 8e-32 0.775 7e-15\nI-Reliability\u26630.757 2e-19 0.792 8e-12\nTable 6: Correlation between \u03c1(s)and journalist-\nprovided reliability scores. \u2663: w/ rewards; \u2662: w/o\nrewards.\nitmore suitable for evaluating the relative reliabil-\nityof sources, as it captures ranked associations\nregardless of the exact numerical values (condition\n3 for \u03c1). In particular, we perform the evaluation\nunder two scenarios, when sources are known to\nbe (un)reliable and, more challenging, when their\nreliability is not known in advance.20In other\nwords, we perform the evaluation following, re-\nspectively, two experimental settings: ( \u2663) we use\nall the ground truth rewards from the largest exper-\nimental set, ExpsetB , to learn the reliability degree\n\u03c1(s)of all 17k sources in the graph; and ( \u2662) we\nrepeat the same process but removing the rewards\nfor all the 85 domains used for evaluation.\nTable 6 shows the obtained results along with\ntwo baselines for reference, random and PageRank\nalgorithm (Brin and Page, 1998),21scatter plots\nin Appendix E. We can observe that, as expected,\ncorrelations are weaker under the hardest scenario\nwithout rewards, specially in terms of PCC which\nis more sensitive to the bias introduced by the re-\nwards.22Nevertheless, in both scenarios, obtained\ncorrelation coefficients are statistically significant\n(p-value \u22645e-07for PCC, p-value \u22641e-06for\nSRCC) and higher than the baselines, except for F-\nReliability . In general, the strategies that correlate\n20For instance, is \u03c1(cnbc.com )< \u03c1(bbc.co.uk )? that is, is\n\u201cbbc.co.uk\u201d more reliable than the \u201ccnbc.com\u201d? knowing in ad-\nvance that both are reliable ( r(cnbc.com ) =r(bbc.co.uk ) =\n1) vs. not knowing it ( r(cnbc.com ) =r(bbc.co.uk ) = 0 ).\n21Note that PageRank is unsuitable for classification experi-\nments as its non-negative scores always predicted the positive\nclass, reducing it to a majority-class classifier. Hence, its\nexclusion from Table 3.\n22Note that sources with r(s) = 1 will naturally tend to\nhave a final \u03c1(s)close to 1while sources with r(s) =\u22121\nclose to \u22121, this bias is heavily reduced when instead of using\nthe actual \u03c1(s)value we use its ranking (as in SRCC).more strongly with the journalist-provided scores\nareP-Reliability andFP-Reliability , showing both\na strong linear (PCC) and ranking-based (SRCC) re-\nlationship independently of whether rewards were\nused or not.23FP-Reliability results suggest that\ncombining F-Reliability andP-Reliability strate-\ngies could be advantageous for estimating relative\nreliability.24Overall, results are inspiring consider-\ning that the learning process for all \u03c1(s)values in\nthe graph leverages only a subset of binary ground\ntruth rewards ( r(s) =\u22121or1),without any ex-\nplicit notion of ground truth score or degree . In\ncontrast to reliability scores derived from various\nqualitative journalistic criteria, \u03c1(s)approximates\nthe reliability degree solely based on the propa-\ngation of these initial rewards throughout the net-\nwork\u2019s structure.25\n6 Conclusion and Future Work\nIn this study, we introduced an approach for as-\nsessing the reliability of news media through their\nnetwork interactions. This approach diverges from\nprevious models that depend on content, audience\nfeedback, and/or metadata. Moreover, unlike in\nprevious works, our method estimates a reliability\ndegree rather than a reliability label. We assessed\nthe quality of the estimated values in terms of relia-\nbility classification and correlation with journalists-\nprovided scores. We found that a source\u2019s origins\nis more indicative of its reliability than its reach\nand show evidence that it is feasible to predict the\nreliability of news media using only their network\ninteractions, providing an easier-to-scale approach\nthan prior methods. As future work, we plan to\nexpand the study by building a larger graph and de-\nsigning more sophisticated strategies that leverage\ncontent-based features. Additionally, we aim to\nexplore the estimation of other news source proper-\nties, such as political bias, using the same approach.\nFinally, we intend to investigate the use of the esti-\nmated reliability values in downstream tasks like\nfact-checking and fake news detection.\n23In fact, we performed an additional experiment in which\nwe set \u03c1(s) =\u03c1p(s)+\u03c1fp(s)\n2, i.e., the reliability degree was\ndefined as the average of the values obtained by the best per-\nforming strategies, P-Reliability andFP-Reliability , obtaining\nthe strongest correlation values (PCC=0.933, SRCC=0.803\nand PCC=0.715, SRCC=0.697 with and without rewards, re-\nspectively).\n24In contrast to reliability classification, where \u03c1(s) = 0 is\nthe fixed threshold separating reliable from unreliable sources.\n25We are releasing the list of estimations for all 17k sources\nalong with this paper.\n7 Ethical Considerations\nThe work presented in this paper has been done\nin the scope of the CRiTERIA project26that fol-\nlows the H2020 ethical standards and guidelines.\nThe Consortium Agreement includes the partners\u2019\ncommitment to FAIR (findable, accessible, interop-\nerable and re-usable) data management practices\nand responsible research practices. The framework\nof the research questions and preliminary results\nwere reviewed by the Project Ethics Check and\nAudit committee in the form of an on-going work\ndeliverable.\nIn the present research paper there is no gen-\nder bias to be investigated or addressed. The data\ncomes from hyperlinks and URL domain names\nthat can not be associated to a gender. As de-\nscribed in Section 4, there is no intentional col-\nlection of personal data in any form, and as a con-\nsequence, there is no need for data anonymisation\nor pseudonymisation. Similarly, there is no need\nof informed and singed consents since there is no\ndirect human participation in the construction of\nthe graphs to calculate the reliability values.\nRegarding data and processing security, a snap-\nshot of CC-News data is transferred, held in the\nlocal servers and processed for the reliability esti-\nmation. The data can be deleted at any time and\neasily downloaded from the original public CC-\nNews sources (as described in Section 4). Any\nfurther data processing carried out, is going to be\npublicly available, along with the reference to this\npaper.\nRegarding other sources, only data collected by\nother sources (under Apache or open source li-\ncenses) with well described data collection method-\nology, validated with published results and that is\npublicly available was used. The overview of the\ndatasets is described in Section 4.2, the description\nincludes the annotations distribution in Table 2,\nand the original sources are properly acknowledge\nalong the paper.\nThe estimated reliability values were obtained\nbased on initial ground truth labels. This labels cap-\nture mainly the factuality of reporting and do not\nconsider other aspects like, for instance, political\nbias or press freedom rating. Therefore, computed\nreliability values should not be considered as de\nfacto values.\nFrom a societal perspective, this paper brings a\npositive impact, improving the situational aware-\n26https://www.project-criteria.eu/ness of decision makers, including fact-checkers.\nThe mathematically defined algorithms are robust\nto content-related biases since they are both lan-\nguage and content independent (political, religious,\nracial, etc.).\n8 Limitations\n8.1 Methodology\nThe main constraint of the proposed methodology\nlies in the requirement for news media sources to\nbe included in the graph for their reliability to be\ncalculated. This limitation may arise due to tempo-\nral or size constraints in the data used to construct\nthe graph. For instance, a recently emerged news\nsource might not be referenced by others until some\ntime has passed. To address this limitation to a cer-\ntain degree, assigning \u03c1(s) = 0 to such sources can\nbe considered. This implies that their reliability is\nindeterminate, indicating an unknown or undeter-\nmined status, meaning they are neither reliable nor\nunreliable.\n8.2 Experimentation\nThe main three limitations of the present work re-\ngarding the experimentation and evaluation of the\nproposed approach are:\n1.Only English-speaking news sources: the\nproposed methodology is content- and\nlanguage-independent. However, we focused\nexclusively on the English-speaking news\nsources due to the predominance of available\nground truth data for experimentation in this\nlanguage. Further studies need to be done\nwith ground truth for non-English-speaking\nsources to assess the robustness of the method-\nology across languages.\n2.Restricted graph size: the graph we used was\nbuilt processing around 100M news articles\nfrom 4 months spanning a 4-year time window.\nThis imposes not only a temporal limitation\nbut also restricts the number of sources in\nthe graph. However, along with this paper,\nwe release and open source, under Apache\n2.0 license, the Python CC-News processing\npipeline and the dataset for the community to\nreproduce the proposed methodology in larger\nscale.\n3.Corpus used to build the graph: although\nCC-News is continuously growing on a daily\nbasis, the crawling of news articles started in\n2016. Therefore, CC-News does not contain\narticles prior to 2016 and news sources that\nexisted before 2016 but not after, will not be\nreachable. Consequently, for these sources to\nbe included, their articles need to be crawled\nfrom a different corpus or manually from the\nWeb.\n9 Acknowledgments\nThis work was supported by CRiTERIA, EU\nproject funded under the Horizon 2020 program,\ngrant agreement number 101021866. We would\nalso like to express our sincere gratitude to the\nanonymous reviewers for their valuable feedback,\nwhich has helped us enhance the quality of this\nwork.\nReferences\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 3528\u20133539, Brussels, Belgium. Association\nfor Computational Linguistics.\nRamy Baly, Georgi Karadzhov, Jisun An, Haewoon\nKwak, Yoan Dinkov, Ahmed Ali, James Glass, and\nPreslav Nakov. 2020. What was written vs. who\nread it: News media profiling using text analysis\nand social media context. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics , pages 3364\u20133374, Online. Association\nfor Computational Linguistics.\nRamy Baly, Georgi Karadzhov, Abdelrhman Saleh,\nJames Glass, and Preslav Nakov. 2019. Multi-task\nordinal regression for jointly predicting the trustwor-\nthiness and the leading political ideology of news\nmedia. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n2109\u20132116, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJo\u00e3o Pedro Baptista and Anabela Gradim. 2022. A\nworking definition of fake news. Encyclopedia , 2(1).\nSergey Brin and Lawrence Page. 1998. The anatomy of\na large-scale hypertextual web search engine. Com-\nputer Networks and ISDN Systems , 30(1):107\u2013117.\nProceedings of the Seventh International World Wide\nWeb Conference.\nSergio Burdisso, Juan Pablo Zuluaga-Gomez, Esa\u00fa\nVillatoro-Tello, Martin Fajcik, Muskaan Singh, Pavel\nSmrz, and Petr Motlicek. 2022. IDIAPers @ causalnews corpus 2022: Efficient causal relation identi-\nfication through a prompt-based few-shot approach.\nInProceedings of the 5th Workshop on Challenges\nand Applications of Automated Extraction of Socio-\npolitical Events from Text (CASE) , pages 61\u201369, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nJose Yunam Cuan-Baltazar, Maria Jos\u00e9 Mu\u00f1oz-Perez,\nCarolina Robledo-Vega, Maria Fernanda P\u00e9rez-\nZepeda, and Elena Soto-Vega. 2020. Misinforma-\ntion of covid-19 on the internet: infodemiology study.\nJMIR public health and surveillance , 6(2):e18444.\nXin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy,\nVan Dang, Wilko Horn, Camillo Lugaresi, Shaohua\nSun, and Wei Zhang. 2015. Knowledge-based trust:\nEstimating the trustworthiness of web sources. Proc.\nVLDB Endow. , 8(9):938\u2013949.\nMartin Fajcik, Petr Motlicek, and Pavel Smrz. 2023.\nClaim-dissector: An interpretable fact-checking sys-\ntem with joint re-ranking and veracity prediction. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023 , pages 10184\u201310205, Toronto,\nCanada. Association for Computational Linguistics.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018) , Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nMaur\u00edcio Gruppi, Benjamin D. Horne, and Sibel Adali.\n2020. NELA-GT-2019: A large multi-labelled news\ndataset for the study of misinformation in news arti-\ncles. CoRR , abs/2003.08444.\nISD. 2020. Anatomy of a Disinformation Empire: In-\nvestigating NaturalNews . Institute for Strategic Dia-\nlogue.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efficient\ntext classification. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 427\u2013431, Valencia, Spain. Association\nfor Computational Linguistics.\nLeslie Pack Kaelbling, Michael L Littman, and An-\ndrew W Moore. 1996. Reinforcement learning: A\nsurvey. Journal of artificial intelligence research ,\n4:237\u2013285.\nJ\u00fcrgen Knauth. 2019. Language-agnostic Twitter-bot\ndetection. In Proceedings of the International Con-\nference on Recent Advances in Natural Language\nProcessing (RANLP 2019) , pages 550\u2013558, Varna,\nBulgaria. INCOMA Ltd.\nSejeong Kwon, Meeyoung Cha, Kyomin Jung, Wei\nChen, and Yajun Wang. 2013. Aspects of rumor\nspreading on a microblog network. In Social Infor-\nmatics: 5th International Conference, SocInfo 2013,\nKyoto, Japan, November 25-27, 2013, Proceedings 5 ,\npages 299\u2013308. Springer.\nDavid MJ Lazer, Matthew A Baum, Yochai Ben-\nkler, Adam J Berinsky, Kelly M Greenhill, Filippo\nMenczer, Miriam J Metzger, Brendan Nyhan, Gordon\nPennycook, David Rothschild, et al. 2018. The sci-\nence of fake news. Science , 359(6380):1094\u20131096.\nZhenyu Lei, Herun Wan, Wenqian Zhang, Shangbin\nFeng, Zilong Chen, Jundong Li, Qinghua Zheng, and\nMinnan Luo. 2023. BIC: Twitter bot detection with\ntext-graph interaction and semantic consistency. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 10326\u201310340, Toronto, Canada.\nAssociation for Computational Linguistics.\nStephan Lewandowsky, Ullrich KH Ecker, and John\nCook. 2017. Beyond misinformation: Understanding\nand coping with the \u201cpost-truth\u201d era. Journal of\napplied research in memory and cognition , 6(4):353\u2013\n369.\nYaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su,\nBo Zhao, Wei Fan, and Jiawei Han. 2016. A sur-\nvey on truth discovery. SIGKDD Explor. Newsl. ,\n17(2):1\u201316.\nNataliia Liubchenko, Andrii Podorozhniak, and Vasyl\nOliinyk. 2022. Research application of the spam\nfiltering and spammer detection algorithms on social\nmedia. In CEUR Workshop Proceedings , volume\n3171, pages 116\u2013126.\nJing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,\nBernard J. Jansen, Kam-Fai Wong, and Meeyoung\nCha. 2016. Detecting rumors from microblogs with\nrecurrent neural networks. In Proceedings of the\nTwenty-Fifth International Joint Conference on Artifi-\ncial Intelligence , IJCAI\u201916, page 3818\u20133824. AAAI\nPress.\nLin Miao, Mark Last, and Marina Litvak. 2020. Detect-\ning troll tweets in a bilingual corpus. In Proceedings\nof the Twelfth Language Resources and Evaluation\nConference , pages 6247\u20136254, Marseille, France. Eu-\nropean Language Resources Association.\nTodor Mihaylov, Georgi Georgiev, and Preslav Nakov.\n2015a. Finding opinion manipulation trolls in news\ncommunity forums. In Proceedings of the Nine-\nteenth Conference on Computational Natural Lan-\nguage Learning , pages 310\u2013314, Beijing, China. As-\nsociation for Computational Linguistics.\nTodor Mihaylov, Ivan Koychev, Georgi Georgiev, and\nPreslav Nakov. 2015b. Exposing paid opinion ma-\nnipulation trolls. In Proceedings of the International\nConference Recent Advances in Natural Language\nProcessing , pages 443\u2013450, Hissar, Bulgaria. IN-\nCOMA Ltd. Shoumen, BULGARIA.Subhabrata Mukherjee and Gerhard Weikum. 2015.\nLeveraging joint interactions for credibility analy-\nsis in news communities. In Proceedings of the\n24th ACM International on Conference on Informa-\ntion and Knowledge Management , CIKM \u201915, page\n353\u2013362, New York, NY , USA. Association for Com-\nputing Machinery.\nKevin Munger. 2020. All the news that\u2019s fit to click:\nThe economics of clickbait media. Political Commu-\nnication , 37(3):376\u2013397.\nAn Nguyen, Aditya Kharosekar, Matthew Lease, and\nByron Wallace. 2018. An interpretable joint graphi-\ncal model for fact-checking from crowds. Proceed-\nings of the AAAI Conference on Artificial Intelligence ,\n32(1).\nJeppe N\u00f8rregaard, Benjamin D Horne, and Sibel Adal\u0131.\n2019. Nela-gt-2018: A large multi-labelled news\ndataset for the study of misinformation in news arti-\ncles. In Proceedings of the international AAAI con-\nference on web and social media , volume 13, pages\n630\u2013638.\nJeff Pasternack and Dan Roth. 2010. Knowing what\nto believe (when you already know something). In\nProceedings of the 23rd International Conference\non Computational Linguistics (Coling 2010) , pages\n877\u2013885, Beijing, China. Coling 2010 Organizing\nCommittee.\nAni Petrosyan. 2023. Internet usage worldwide - statis-\ntics & facts. Statista .\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6tgen,\nand Gerhard Weikum. 2016. Credibility assessment\nof textual claims on the web. In Proceedings of the\n25th ACM International on Conference on Informa-\ntion and Knowledge Management , CIKM \u201916, page\n2173\u20132178, New York, NY , USA. Association for\nComputing Machinery.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6t-\ngen, and Gerhard Weikum. 2017. Where the truth\nlies: Explaining the credibility of emerging claims on\nthe web and social media. In Proceedings of the 26th\nInternational Conference on World Wide Web Com-\npanion , WWW \u201917 Companion, page 1003\u20131012,\nRepublic and Canton of Geneva, CHE. International\nWorld Wide Web Conferences Steering Committee.\nKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6tgen,\nand Gerhard Weikum. 2018a. Credeye: A credibility\nlens for analyzing and explaining misinformation. In\nCompanion Proceedings of the The Web Conference\n2018 , WWW \u201918, page 155\u2013158, Republic and Can-\nton of Geneva, CHE. International World Wide Web\nConferences Steering Committee.\nKashyap Popat, Subhabrata Mukherjee, Andrew Yates,\nand Gerhard Weikum. 2018b. DeClarE: Debunk-\ning fake news and false claims using evidence-aware\ndeep learning. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing , pages 22\u201332, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nMartin L Puterman. 2014. Markov decision processes:\ndiscrete stochastic dynamic programming . John Wi-\nley & Sons.\nDevakunchari Ramalingam and Valliyammai Chinnaiah.\n2018. Fake profile detection techniques in large-scale\nonline social networks: A comprehensive review.\nComputers & Electrical Engineering , 65:165\u2013177.\nPradeep Kumar Roy and Shivam Chahar. 2020. Fake\nprofile detection on social networking websites: A\ncomprehensive review. IEEE Transactions on Artifi-\ncial Intelligence , 1(3):271\u2013285.\nGiuseppe Sansonetti, Fabio Gasparetti, Giuseppe\nD\u2019aniello, and Alessandro Micarelli. 2020. Unre-\nliable users detection in social media: Deep learning\ntechniques for automatic detection. IEEE Access ,\n8:213154\u2013213167.\nShaden Shaar, Nikolay Babulkov, Giovanni Da San Mar-\ntino, and Preslav Nakov. 2020. That is a known lie:\nDetecting previously fact-checked claims. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3607\u2013\n3618, Online. Association for Computational Lin-\nguistics.\nGianluca Stringhini, Christopher Kruegel, and Giovanni\nVigna. 2010. Detecting spammers on social networks.\nInProceedings of the 26th annual computer security\napplications conference , pages 1\u20139.\nJesper Str\u00f6mb\u00e4ck, Yariv Tsfati, Hajo Boomgaarden,\nAlyt Damstra, Elina Lindgren, Rens Vliegenthart,\nand Torun Lindholm. 2020. News media trust and its\nimpact on media use: Toward a framework for future\nresearch. Annals of the International Communication\nAssociation , 44(2):139\u2013156.\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\nment learning: An introduction . MIT press.\nMichele Tomaiuolo, Gianfranco Lombardo, Monica\nMordonini, Stefano Cagnoni, and Agostino Poggi.\n2020. A survey on troll detection. Future Internet ,\n12(2).\nSander Van Der Linden. 2022. Misinformation: sus-\nceptibility, spread, and interventions to immunize the\npublic. Nature Medicine , 28(3):460\u2013467.\nWilliam Yang Wang. 2017. \u201cliar, liar pants on fire\u201d:\nA new benchmark dataset for fake news detection.\nInProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2:\nShort Papers) , pages 422\u2013426, Vancouver, Canada.\nAssociation for Computational Linguistics.\nYaqing Wang, Fenglong Ma, Zhiwei Jin, Ye Yuan,\nGuangxu Xun, Kishlay Jha, Lu Su, and Jing Gao.\n2018. Eann: Event adversarial neural networks for\nmulti-modal fake news detection. In Proceedings\nof the 24th acm sigkdd international conference on\nknowledge discovery & data mining , pages 849\u2013857.Guillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference , pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nHengshuai Yao and Dale Schuurmans. 2013. Reinforce-\nment ranking. arXiv preprint arXiv:1303.5988 .\nA Hyperparameter Optimization\nWe performed a grid search to determine the best\nhyperparameter values on each of the three experi-\nmental sets. More precisely, as in Section 5.1, the\nevaluation was performed using 5-fold cross val-\nidation on the reliability classification task. For\nreinforcement learning strategies, we evaluated\n\u03b3from 0.05to0.95in increments of 0.05(i.e.,\n\u03b3\u2208 {0.05,0.1,0.15, . . . ,0.95}). For I-Reliability ,\nnfrom1to10(i.e.,n\u2208 {1,2,3, . . . ,10}). Finally,\nthe hyperparameter values obtaining the best macro\navg. F 1score were the one selected on each exper-\nimental set. Namely, the selected values for each\nstrategy were:\n\u2022F-Reliability: \u03b3= 0.05forExpsetA andEx-\npsetB\u2212,\u03b3= 0.5forExpsetB .\n\u2022P-Reliability: \u03b3= 0.15forExpsetA ,\u03b3= 0.3\nforExpsetB , and \u03b3= 0.2forExpsetB\u2212.\n\u2022FP-Reliability: \u03b3= 0.1forExpsetA ,\u03b3=\n0.05forExpsetB andExpsetB\u2212.\n\u2022I-Reliability: n= 1forExpsetA andExpsetB ,\nandn= 2forExpsetB\u2212.\nAs shown in Figure 1, we can observe that bet-\nter reliability estimation is achieved when looking\nmostly at nearby sources , as better performance is\nobtained with small nand\u03b3values, namely n\u22642\nand\u03b3 <0.5. Furthermore, P-Reliability (orange\nline) outperforms the other reinforcement learning\nstrategies, consistently, while being more robust to\nthe choice of \u03b3, except when \u03b3 >0.7from which\nperformance starts to decrease.\nFinally, for the Baly et al. (2018) and Baly et al.\n(2020) classifiers in Table 3, we follow the same\nprocess described by the authors to tune the SVM\nhyperparameters, i.e., the cost C, the kernel type,\nand the kernel width \u03b3using the 5-fold cross vali-\ndation maximizing the F1score as with our meth-\nods.27\nB Temporal Ablation Analysis\nTo evaluate the robustness of our proposed ap-\nproach concerning both the graph size and the\ntemporal span used in its construction, a tempo-\nral ablation study was conducted. In addition to\nthe graph used for experimentation, illustrated in\n27We used the author\u2019s source code containing the\nhyperparameter search in it ( github.com/ramybaly/\nNews-Media-Reliability ).Figure 2, we generated four different graphs, each\ncorresponding to one of the four CC-News snap-\nshots (refer to Table 1 in Section 4.1 for detailed\ninformation on each graph). Subsequently, em-\nploying each of these four graphs, we replicated\nthe evaluation procedure described in Section 5.1.\nThese evaluations allowed us to measure how the\nperformance of the proposed strategies changed,\nwhen changing the graph, compared to the reported\nvalues in Table 3.\nFigure 3 shows, without loss of generality, the\nresults obtained with the two best-performing\nstrategies reported in Table 3, P-Reliability andI-\nReliability , on the largest experimental set ExpsetB .\nWe observed that, independently of the strategy and\nthe dataset, the best results were always obtained\nwith the largest (in size and time) graph joining\nall the snapshots. We also observed that not all\nstrategies exhibit equal robustness to changes in\nthe graph. For instance, we can see in Figure 3 how\nP-Reliability is more sensitive than I-Reliability\nto the choice of snapshot for graph construction.\nDespite this variation, both strategies demonstrate\nimprovement and achieve their best results with re-\nduced uncertainty when considering all snapshots.\nC Graph Construction Steps\nTheCommon Crawl News Dataset (CC-News )28\nis published as WARC files29grouped by year and\nmonth, called snapshots.30To construct the news\nmedia graph, G=\u27e8S, E, w \u27e9, from CC-News snap-\nshots, we follow to the subsequent steps:\n1.Download each WARC file and parse each\nnews article in it to extract its URL and all the\nhyperlinks in its body. At the end of this step,\nwe have a set of news article URLs, U, and a\nset of hyperlinks Lufor each article u\u2208U.\n2.Generate the graph nodes SfromUsimply as\nS={domain (u) :u\u2208U}which contains\nthe URL domain names ( e.g.\u201cnytimes.com\u201d,\n\u201ccnn.com\u201d, etc.) of all processed news articles.\n3.For each domain s\u2208Screate the list of\nall its hyperlinks, Ls, by aggregating the\nhyperlinks of all its articles, i.e. Ls=S\nu\u2208U:domain (u)=sLu.\n28https://commoncrawl.org/blog/\nnews-dataset-available\n29A file format that resembles the raw HTTP request and\nresponse of each crawled web page.\n30https://data.commoncrawl.org/crawl-data/\nCC-NEWS/index.html\nFigure 1: Performance variation across searched values of n(left side) and \u03b3(right side) on the ExpsetB (solid line)\nandExpsetB\u2212(dashed line) datasets. The lines represent the mean values across the 5 folds, and 95% confidence\nintervals are depicted. Markers highlight selected hyperparameter values.\nFigure 2: News media graph built from all four CC-\nNews snapshot (only English-speaking sources) and\nused for experimentation.\n4.Finally, generate the graph edges (s, s\u2032)\u2208\nEfor each s\u2208Sby creating an edge to\neach unique domain s\u2032in its hyperlinks Ls\nweighted by the proportion of links whose\ndomain is s\u2032,i.e.w(s, s\u2032) =|{l\u2208Ls:\ndomain (l) =s\u2032}|/|Ls|.\nD NewsGuard Score Details\nIn Section 5.2, we used the scores from the News-\nGuard dataset introduced in Section 4.2 to measure\nthe correlation with estimated \u03c1(s)values. News-\nGuard employs a team of journalists and experi-\nenced editors to produce these reliability scores for\nFigure 3: 5-fold cross-validation results obtained on\ntheExpsetB dataset with the two best strategies, P-\nReliability andI-Reliability , using different graphs. The\nx-axis represents the CC-News snapshot used to build\nthe graph, and the y-axis the macro averaged F 1score .\nnews and information websites. The score ranges\nfrom 0 to 100 and NewsGuard is transparent about\nthe methodology used to compute it. Namely, they\ncompute this reliability score based on the follow-\ning nine apolitical criteria, each is worth the indi-\ncated number of points, based on importance:\n1.Does not repeatedly publish false content?\n(22 points)\n2.Gathers and presents information responsi-\nbly? (18 points)\n3.Regularly corrects or clarifies errors? (12.5\npoints)\nFigure 4: Scatter plot showing the correlation between\nthe rankings obtained by PageRank values (y-axis) and\nNews Guard scores (x-axis).\n4.Handles the difference between news and\nopinion responsibly? (12.5 points)\n5.Avoids deceptive headlines? (10 points)\n6.Website discloses ownership and financing?\n(7.5 points)\n7.Clearly labels advertising? (7.5 points)\n8.Reveals who\u2019s in charge, including any pos-\nsible conflicts of interest? (5 points)\n9.Provides information about content cre-\nators? (5 points)\nMore details can be found in the \u201cRating Process\nand Criteria\u201d section of their website.31\nE Detailed Results\nTable 7 shows the detailed evaluation results ob-\ntained on the three experimentation sets along with\ntwo naive baselines for reference, random and ma-\njority class classifiers. Furthermore, for ExpsetA ,\nwe also report the results obtained using the classi-\nfication models introduced in previous works (Baly\net al., 2018, 2020) along with the ensemble results.\nWe can see that on ExpsetA , unlike on the other\nexperimental sets, our strategies have 100% pre-\ncision for the unreliable sources but its recall on\nthe same sources is quite low (from 22.20% to\n38.72%) showing the models are detecting only a\nsmall portion of unreliable sources but with high\nprecision (probably only the easiest cases). By\n31https://www.newsguardtech.com/ratings/\nrating-process-criteria/performing the ensemble with Baly models, the re-\ncall for the unreliable group increases allowing the\ngraph-based strategies to identify more unreliable\nsources, in turn improving the macro average F 1\nscores.\nFinally, in Figure 4 is shown the scatter plot\nshowing the correlation between the ranking ob-\ntained by PageRank values and the one obtained\nby the News Guard scores. Likewise, Figures 5,\n6, 7, and 8 show the scatter plots for F-Reliability ,\nP-Reliability ,FP-Reliability ,I-Reliability , respec-\ntivelly.\nData StrategyPrecision Recall F 1score\nmacro avg. reliable unreliable macro avg. reliable unreliable macro avg. reliable unreliable Acc.ExpsetAM-BL 36.72 \u00b11.29 73.44 \u00b12.57 0.00 \u00b10.00 50.00\u00b10.00 100.00\u00b10.00 0.00 \u00b10.00 42.33\u00b10.85 84.66 \u00b11.70 0.00 \u00b10.00 73.44\u00b12.57\nR-BL 51.37 \u00b14.21 74.63 \u00b13.49 28.11 \u00b16.08 51.56\u00b15.30 52.87 \u00b16.72 50.26 \u00b17.86 48.85\u00b15.29 61.76 \u00b15.63 35.94 \u00b16.94 52.33\u00b15.52\nBaly18 70.11 \u00b15.74 82.48\u00b15.02 57.74 \u00b18.72 67.68\u00b17.52 87.65 \u00b14.31 47.70 \u00b116.75 67.87\u00b16.89 84.81 \u00b12.65 50.92 \u00b111.97 76.95\u00b14.11\nBaly20 71.03 \u00b19.32 78.42 \u00b13.14 63.64 \u00b120.83 64.64\u00b13.62 88.74 \u00b17.19 40.55 \u00b18.33 65.24\u00b14.03 82.99 \u00b12.06 47.50 \u00b16.75 74.37\u00b12.84\nF-R 89.02 \u00b11.75 78.03 \u00b13.51 100.00\u00b10.00 61.10\u00b14.70 100.00\u00b10.00 22.20 \u00b19.40 61.52\u00b16.83 87.62 \u00b12.19 35.42 \u00b111.92 79.26\u00b13.59\nP-R 90.96\u00b10.88 81.91\u00b11.75 100.00\u00b10.00 69.30\u00b13.53 100.00\u00b10.00 38.59\u00b17.05 72.67\u00b14.10 90.05\u00b11.05 55.29\u00b17.87 83.79\u00b11.59\nFP-R 90.29 \u00b11.07 80.57 \u00b12.14 100.00\u00b10.00 66.53\u00b13.31 100.00\u00b10.00 33.06 \u00b16.61 69.28\u00b13.91 89.23 \u00b11.32 49.34 \u00b17.13 82.29\u00b12.07\nI-R 90.95\u00b11.02 81.89\u00b12.04 100.00\u00b10.00 69.36\u00b12.75 100.00\u00b10.00 38.72\u00b15.51 72.81\u00b13.18 90.03\u00b11.23 55.60\u00b15.86 83.77\u00b11.81\nF-R+Baly18 76.54\u00b13.69 87.12 \u00b12.64 65.95 \u00b17.81 75.85\u00b13.02 87.65 \u00b14.31 64.06 \u00b17.47 75.84\u00b13.01 87.29 \u00b12.02 64.39 \u00b14.59 81.32\u00b12.67\nF-R+Baly20 77.80\u00b17.68 84.57 \u00b12.43 71.04\u00b117.10 74.79\u00b12.98 88.74\u00b17.19 60.84 \u00b13.90 75.34\u00b14.18 86.38 \u00b12.69 64.30 \u00b15.75 80.30\u00b13.67\nP-R+Baly18 77.47\u00b13.96 87.99\u00b11.21 66.95 \u00b18.47 77.23\u00b11.76 87.65 \u00b14.31 66.82\u00b13.31 77.11\u00b12.95 87.75\u00b12.06 66.47\u00b14.27 82.11\u00b12.72\nP-R+Baly20 77.04\u00b17.79 83.89 \u00b12.25 70.19\u00b117.23 73.70\u00b12.95 88.74\u00b17.19 58.66 \u00b14.34 74.36\u00b14.10 86.02 \u00b12.57 62.70 \u00b15.84 79.69\u00b13.50\nFP-R+Baly18 74.25\u00b14.78 85.08 \u00b13.73 63.42 \u00b18.54 72.67\u00b14.33 87.65 \u00b14.31 57.68 \u00b18.86 73.01\u00b14.32 86.24 \u00b12.66 59.79 \u00b16.25 79.54\u00b13.72\nFP-R+Baly20 76.01\u00b17.86 82.96 \u00b13.47 69.06 \u00b117.57 72.10\u00b14.12 88.74\u00b17.19 55.46 \u00b19.83 72.67\u00b14.28 85.46 \u00b12.11 59.88 \u00b17.00 78.72\u00b12.99\nI-R+Baly18 77.79\u00b13.45 88.29\u00b11.75 67.30 \u00b17.86 77.73\u00b11.47 87.65 \u00b14.31 67.82\u00b14.62 77.47\u00b12.40 87.89\u00b11.89 67.06\u00b13.40 82.34\u00b12.40\nI-R+Baly20 76.12\u00b17.77 82.85 \u00b12.25 69.38 \u00b117.58 72.10\u00b12.28 88.74\u00b17.19 55.46 \u00b13.69 72.88\u00b13.46 85.46 \u00b12.36 60.30 \u00b14.77 78.74\u00b13.13ExpsetBM-BL 38.50 \u00b10.04 77.00 \u00b10.09 0.00 \u00b10.00 50.00\u00b10.00 100.00\u00b10.00 0.00 \u00b10.00 43.50\u00b10.03 87.00 \u00b10.06 0.00 \u00b10.00 77.00\u00b10.09\nR-BL 51.09 \u00b11.39 78.05 \u00b11.30 24.13 \u00b11.49 51.53\u00b11.96 51.72 \u00b13.06 51.34 \u00b13.70 47.48\u00b11.83 62.17 \u00b12.45 32.80 \u00b11.93 51.63\u00b12.21\nF-R 61.74\u00b13.19 83.63 \u00b10.71 39.85 \u00b15.85 63.24\u00b12.10 76.38 \u00b16.35 50.11 \u00b12.82 61.85\u00b13.12 79.72 \u00b13.69 43.98 \u00b12.61 70.34\u00b14.36\nP-R 74.66\u00b12.51 88.44\u00b11.09 60.89\u00b14.20 74.78\u00b12.26 88.16\u00b11.62 61.41\u00b13.72 74.69\u00b12.31 88.29\u00b11.15 61.10\u00b13.52 82.00\u00b11.73\nFP-R 59.01\u00b11.95 85.73 \u00b11.42 32.29 \u00b12.67 62.60\u00b12.66 56.63 \u00b15.39 68.57\u00b13.22 55.95\u00b13.21 68.08 \u00b14.09 43.82 \u00b12.55 59.38\u00b13.98\nI-R 76.68\u00b13.08 87.98\u00b11.10 65.39\u00b15.31 74.60\u00b12.40 90.67\u00b11.68 58.53 \u00b13.78 75.51\u00b12.61 89.30\u00b11.22 61.72\u00b14.02 83.28\u00b11.87ExpsetB\u2212M-BL 45.38 \u00b10.05 90.76 \u00b10.10 0.00 \u00b10.00 50.00\u00b10.00 100.00\u00b10.00 0.00 \u00b10.00 47.58\u00b10.03 95.15 \u00b10.06 0.00 \u00b10.00 90.76\u00b10.10\nR-BL 49.82 \u00b12.41 90.62 \u00b12.36 9.02 \u00b12.47 49.43\u00b17.19 48.34 \u00b10.88 50.52 \u00b114.11 39.17\u00b12.54 63.04 \u00b11.09 15.31 \u00b14.20 48.55\u00b11.71\nF-R 60.60\u00b12.73 94.14 \u00b10.58 27.07 \u00b15.03 66.84\u00b12.97 86.69 \u00b13.07 46.99 \u00b15.59 62.18\u00b13.15 90.23 \u00b11.75 34.12 \u00b14.90 83.02\u00b12.77\nP-R 77.75\u00b15.20 96.46\u00b10.83 59.04\u00b19.83 80.48\u00b14.25 95.21\u00b11.42 65.74\u00b17.79 78.90\u00b14.50 95.83\u00b10.98 61.97\u00b18.04 92.48\u00b11.74\nFP-R 59.20\u00b10.39 95.98 \u00b10.63 22.41 \u00b10.49 72.21\u00b11.94 75.77 \u00b12.32 68.66 \u00b15.99 59.20\u00b10.54 84.66 \u00b11.23 33.74 \u00b10.83 75.11\u00b11.60\nI-R 83.22\u00b14.09 96.13\u00b10.72 70.32\u00b17.83 79.41\u00b13.58 97.30\u00b10.85 61.52 \u00b17.02 81.05\u00b13.40 96.71\u00b10.60 65.39\u00b16.22 93.99\u00b11.09\nTable 7: 5-fold cross-validation detailed results for reliability classification. Mean and standard deviation is shown for each metric. Bold indicates 2nd-best-performing values,\nwhile underlined the best-performing values in each dataset. R-BL and M-BL refer to random and majority class baselines; Baly18 and Baly20 refer to Baly et al. (2018) and Baly\net al. (2020); and *-R stands for *-Reliability.\nFigure 5: Scatter plot showing the correlation between the rankings obtained by F-Reliability values (y-axis) and\nNews Guard scores (x-axis). Left side without rewards and right side with rewards.\nFigure 6: Scatter plot showing the correlation between the rankings obtained by P-Reliability values (y-axis) and\nNews Guard scores (x-axis). Left side without rewards and right side with rewards.\nFigure 7: Scatter plot showing the correlation between the rankings obtained by FP-Reliability values (y-axis) and\nNews Guard scores (x-axis). Left side without rewards and right side with rewards.\nFigure 8: Scatter plot showing the correlation between the rankings obtained by I-Reliability values (y-axis) and\nNews Guard scores (x-axis). Left side without rewards and right side with rewards.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Reliability estimation of news media sources: birds of a feather flock together", "author": ["S Burdisso", "D S\u00e1nchez-Cort\u00e9s", "E Villatoro-Tello"], "pub_year": "2024", "venue": "arXiv preprint arXiv \u2026", "abstract": "Evaluating the reliability of news sources is a routine task for journalists and organizations  committed to acquiring and disseminating accurate information. Recent research has shown"}, "filled": false, "gsrank": 440, "pub_url": "https://arxiv.org/abs/2404.09565", "author_id": ["XOD8lrAAAAAJ", "rL3daVYAAAAJ", "GzaiunYAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:bsLj8oUa5SUJ:scholar.google.com/&output=cite&scirp=439&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D430%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=bsLj8oUa5SUJ&ei=VLWsaISoBr_SieoPzJnloAQ&json=", "num_citations": 4, "citedby_url": "/scholar?cites=2730617911678059118&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:bsLj8oUa5SUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2404.09565?"}}, {"title": "Robust document representations for hyperpartisan and fake news detection", "year": "2019", "pdf_data": "Robust Document Representations for\nHyperpartisan and Fake News Detection\nAuthor: Talita Anthonio, MA\nAdvisors: Dr. Rodrigo Agerri, Prof. Dr. Malvina Nissim\nErasmus Mundus Master in Language and Communication Technologies\nFinal Thesis\nJune 12, 2019\nDepartment : Computer Systems and Languages, University of the Basque Country\nUPV/EHU\nHyperpartisan News Detection ii/80\nHyperpartisan News Detection iii/80\nAbstract\nHyperpartisan news is characterized by extremely one-sided content from a left-wing or\nright-wing political perspective. This thesis is concerned with automatically detecting\nsuch news through supervised text classi\fcation. We work with data from the recent\nshared task on hyperpartisan news detection (SemEval-2019 Task 4). We use two\nclassi\fcation techniques: Support Vector Machines (SVMs) and Recurrent Neural\nNetworks. We experiment with document representations using bag-of-words,\nbag-of-clusters, word embeddings and contextual character-based embeddings. We also\ntry to improve our classi\fers by adding local features, such as POS n-grams, stylistic\nfeatures and the sentiment of a text. Our aim is to build robust classi\fers across tasks\nrelated to fake news, for di\u000berent domains and text genres. Although local features help\nto model the task in-domain, this thesis shows that dense document representations work\nbetter across domains and tasks. We obtain very competitive results in the hyperpartisan\nnews detection task and state-of-the-art results in an out-of-domain evaluation on fake\nnews.\nkeywords : hyperpartisan news detection, fake news, supervised text classi\fcation\nHyperpartisan News Detection iv/80\nContents\n1 Introduction 1\n1.1 Hyperpartisan News . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2 Related Work 7\n2.1 Hyperpartisan Content Detection . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.1 Automatic Detection . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.2 Manual Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 Fake News Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2.1 Feature-based Supervised Learning . . . . . . . . . . . . . . . . . . 10\n2.2.2 Supervised Learning with Neural Networks . . . . . . . . . . . . . . 11\n2.3 Biased Language Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3 Data 15\n3.1 The SemEval-2019 Task 4 Hyperpartisan News Dataset . . . . . . . . . . . 15\n3.1.1 By-publisher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.1.2 By-article . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.2 By-publisher Subset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.3 Fake News Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4 Document Representations 22\n4.1 Bag-of-words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.1.1 Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.2 Bag-of-clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.2.1 Data and Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n4.3 Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n4.3.1 Materials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n4.3.2 Word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.3.3 GloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.3.4 FastText . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.4 Contextual Character-based Embeddings . . . . . . . . . . . . . . . . . . . 30\n4.4.1 Flair Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.4.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n5 Experimental Set-up 33\n5.1 General Set-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n5.2 Baseline and Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.3 Approach 1: Classi\fcation with a SVM . . . . . . . . . . . . . . . . . . . . 35\nHyperpartisan News Detection v/80\n5.3.1 Feature Selection for Document Representations . . . . . . . . . . . 35\n5.3.2 Pre-processing and Parameter Tuning . . . . . . . . . . . . . . . . . 36\n5.4 Additional Features for the SVMs . . . . . . . . . . . . . . . . . . . . . . . 37\n5.4.1 Sentiment Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.4.2 Linguistic Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n5.4.3 Stylistic Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n5.5 Approach 2: Classi\fcation with Recurrent Neural Networks . . . . . . . . . 39\n5.5.1 Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n6 Model Development Results 41\n6.1 Document Representations in SVMs . . . . . . . . . . . . . . . . . . . . . . 41\n6.1.1 Bag-of-words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n6.1.2 Bag-of-clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n6.1.3 Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n6.1.4 Overview of the Best Classi\fers . . . . . . . . . . . . . . . . . . . . 44\n6.2 Additional Features for SVM . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n6.2.1 Linguistic Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n6.2.2 Sentiment Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n6.2.3 Stylistic Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n6.3 Adding Local Features to Document Representations with SVM . . . . . . 48\n6.3.1 Bag-of-words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n6.3.2 Bag-of-clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n6.3.3 Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n6.4 Experiments with Recurrent Neural Networks . . . . . . . . . . . . . . . . 51\n6.4.1 Best Classi\fer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n6.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n7 In-domain Results 55\n7.1 Results with Support Vector Machines . . . . . . . . . . . . . . . . . . . . 55\n7.1.1 Results on the By-article Test Set . . . . . . . . . . . . . . . . . . . 55\n7.1.2 Results on the By-publisher Test Set . . . . . . . . . . . . . . . . . 56\n7.2 Results with Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . 57\n7.3 Best Classi\fer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n7.3.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n8 Across-datasets and Out-of-domain Evaluation 60\n8.1 Across-datasets Evaluation on Hyperpartisan using SVM . . . . . . . . . . 60\n8.1.1 Training on By-article . . . . . . . . . . . . . . . . . . . . . . . . . 60\n8.1.2 Training on By-publisher . . . . . . . . . . . . . . . . . . . . . . . . 61\n8.2 Across-datasets Evaluation on Hyperpartisan using Neural Networks . . . . 62\n8.3 Out-of-domain Evaluation on Fake News . . . . . . . . . . . . . . . . . . . 63\n8.3.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n8.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\nHyperpartisan News Detection vi/80\n9 Discussion 66\n9.1 Best Classi\fer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n9.1.1 Common Confusions . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n9.1.2 Important Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n9.2 In-domain Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n9.3 Results Across-datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n9.4 Out-of-domain Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n10 Conclusion 73\n10.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n10.2 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nHyperpartisan News Detection vii/80\nList of Figures\n1 An example of an article with hyperpartisan content from the by-article\ntraining set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2 An example of a hyperpartisan text (right-leaned) that is not about politics. 19\n3 An example of a fake news article from the CelebrityNews dataset of P\u0013 erez-\nRosas et al. (2018). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4 An intuitive illustration of bag-of-clusters. The left graph represents a simpli\fed\nword embedding model. The graph on the right shows the result of applying a\ncluster algorithm on the word embeddings. The table presents a \fctive example\nof two documents vectorized as bag-of-clusters. . . . . . . . . . . . . . . . . . 25\n5 The procedure of creating the contextual string embedding for the word\nWashington from Akbik et al. (2018, p.4). . . . . . . . . . . . . . . . . . . 31\n6 Three graphs showing the performance of the best classi\fer with neural\nnetworks over 120 epochs. . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n7 The top 30 most important features for predicting hyperpartisan (positive\nvalues) and mainstream news (negative values). . . . . . . . . . . . . . . . 68\nHyperpartisan News Detection viii/80\nList of Tables\n1 The distribution of hyperpartisan and bias labels our by-publisher subset. . 17\n2 Corpus statistics of the datasets that we used for classi\fcation purposes. . 18\n3 Corpus statistics of the datasets of P\u0013 erez-Rosas et al. (2018) that we used\nfor out-of-domain evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4 The frequency distribution of the Top 20 most frequent publishers of hyper-\npartisan news in the by-publisher subset.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5 The frequency distribution of the Top 20 most frequent publishers of main-\nstream news in the by-publisher subset.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n6 The frequency distribution of the Top 20 most frequent publishers of hyper-\npartisan news in the by-article training set. . . . . . . . . . . . . . . . . . . 21\n7 The frequency distribution of the Top 20 most frequent publishers of main-\nstream news in the by-article training set. . . . . . . . . . . . . . . . . . . 21\n8 A simpli\fed illustration of bag-of-word uni-grams for two documents. . . . 23\n9 A description of the clusters that we used to represent documents as bag-\nof-clusters. All clusters were applied on word embeddings trained with\nWord2vec. The amount of words represent the number of words that occur\nin each cluster \fle in either the wikipedia, hyperpartisan or fake news cluster\n\fles. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n10 An overview of the word embeddings that we used. All word vectors had a\ndimension of 300. We show the size in million or billion words. . . . . . . . 28\n11 The performance of the baseline system that only uses the negative sen-\ntiment of a text as features. The system is trained on the complete by-\npublisher training set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n12 The values over which we performed the grid-search to tune the parameters\nof the tf-idf vectorizer and the C-parameter of the SVM. . . . . . . . . . . 37\n13 The Top 5 bag-of-word classi\fers trained on the by-article training set and\nevaluated via 5-fold cross-validation. . . . . . . . . . . . . . . . . . . . . . 42\n14 The Top 5 bag-of-word classi\fers trained on the by-publisher subset and\nevaluated via 5-fold cross-validation. . . . . . . . . . . . . . . . . . . . . . 42\n15 The Top 5 results obtained with Wikipedia clusters on 5-fold cross-validation\nusing default parameter settings. . . . . . . . . . . . . . . . . . . . . . . . 43\n16 The Top 5 results obtained with fake/legitimate news clusters on 5-fold\ncross-validation using default parameter settings. . . . . . . . . . . . . . . 43\n17 The Top 5 results obtained with hyperpartisan/non-hyperpartisan clusters\non 5-fold cross-validation using default parameter settings. . . . . . . . . . 44\n18 Results for the pre-trained word embeddings for the models that we trained\non the by-article training set via 5-fold cross-validation. . . . . . . . . . . . 45\n19 Results for the pre-trained word embeddings for the models that we trained\non the by-publisher subset via 5-fold cross-validation. . . . . . . . . . . . . 45\nHyperpartisan News Detection ix/80\n20 The Top 5 results on 5-fold cross-validation of the SVM classifers using POS\nfeatures (linguistic features) with tf-idf weighting. . . . . . . . . . . . . . . 47\n21 The Top 5 most predictive sentiment features for the classi\fers that we\ntrained and evaluated via 5-fold cross-validation. All features were computed\nwith VADER (Hutto and Gilbert, 2014). . . . . . . . . . . . . . . . . . . . 47\n22 The Top 5 most predictive stylistic features for the classi\fers that we trained\nand evaluated via 5-fold cross-validation. The usage of assertives was rep-\nresented by binary representation. . . . . . . . . . . . . . . . . . . . . . . . 48\n23 The results of the models that use pre\fxes and additional features on 5-fold\ncross-validation using the by-article training data. . . . . . . . . . . . . . . 49\n24 The results of the models that use pre\fxes and additional features on 5-fold\ncross-validation using the by-publisher subset. . . . . . . . . . . . . . . . . 49\n25 The results of the models that use additional features and bag-of-clusters\non 5-fold cross-validation on the by-article training set. . . . . . . . . . . . 50\n26 The results of the models that use additional features and bag-of-clusters\non 5-fold cross-validation on the by-publisher subset. . . . . . . . . . . . . 50\n27 The scores on 5-fold cross-validation for models that use FastText pre-\ntrained word embeddings and additional features trained and evaluated on\nby-article training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n28 The scores on 5-fold cross-validation for models that use FastText pre-\ntrained word embeddings (without stop words) and additional features trained\nand evaluated on the by-publisher subset. . . . . . . . . . . . . . . . . . . . 52\n29 The performance of the classi\fers that use recurrent neural networks on\nevaluated on 10% of the by-article training data and trained on 80% of the\nby-article training set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n30 The performance of the best classi\fer over \fve runs that use recurrent neural\nnetworks on predicting hyperpartisan evaluated on 10% of the by-article\ntraining data and trained on 80% of the by-article training set. . . . . . . . 53\n31 The performance of the best SVM classi\fers trained on the by-article train-\ning set and evaluated on the by-article test set. . . . . . . . . . . . . . . . 56\n32 The performance of the best bag-of-words, bag-of-clusters and word em-\nbedding model trained on by-publisher training and evaluated on the by-\npublisher test set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n33 The results on the by-article test set of the classi\fer using Flair and GloVe\nembeddings using Recurrent Neural Networks. . . . . . . . . . . . . . . . . 58\n34 Results the by-article training data on 5-fold cross-validation showing the\ne\u000bects of the pre-processing procedures of the best classi\fer on the by-article\ntest set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n35 Results of the previous best classi\fer and the current best classi\fer on the\nby-article test set. We also show the results of the winning system in the\ncompetition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\nHyperpartisan News Detection x/80\n36 The results of the best SVM classi\fers trained on the by-article training\nset and evaluated on the by-publisher test set. The \frst row represents the\nscores of best system on the by-article test set from the in-domain evaluation\nexperiments, of which we show the performance on the by-publisher test set\nin this table. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n37 The results of the best SVM classi\fers trained on the by-publisher subset\nand evaluated on the by-article test set. . . . . . . . . . . . . . . . . . . . . 62\n38 The across-datasets performance of the best neural network classi\fer using\nthe Flair and GloVe embeddings. The classi\fer was trained on the by-article\ntraining set and evaluated on the by-publisher test set. . . . . . . . . . . . 63\n39 The results on 5-fold cross-validation of the classi\fers trained on the Fake-\nNewsAMT dataset of P\u0013 erez-Rosas et al. (2018). Each classi\fer used either\none of the best features from the best bag-of-words, bag-of-clusters or word\nembeddings classi\fer that we developed for fake news detection. . . . . . . 64\n40 Out-of-domain evaluation on the CelebrityNews dataset of P\u0013 erez-Rosas et al.\n(2018). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n41 Results of the SVM classi\fers using the best document representations of\nour systems on the by-article test set. The systems are trained on the Fak-\neNewsAMT data and evaluated on the CelebrityNews dataset from P\u0013 erez-\nRosas et al. (2018). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n42 The performance of our best classi\fer on hyperpartisan news detection, eval-\nuated on the by-article test set. The classi\fer scored an accuracy of 0.7866. 67\n43 The three most important clusters for predicting hyperpartisan news (left:\nmost important, right: less important). The three clusters were also the\nmost important features to detect hyperpartisan news. . . . . . . . . . . . 69\nHyperpartisan News Detection 1/80\n1 Introduction\nRespecting the truth and the public right to the truth is the \frst obligation\nof journalists. A journalist, in recognition of this commitment, defends the\nprinciples of freedom and the right to comment and critique, while \fnding and\nreproducing the news properly. A journalist reports only the facts obtained\nfrom a trustworthy source, he does not suppress important information and he\ndoes not falsify material. He only uses fair methods to \fnd information, photos\nand other materials. In case he accidentally publishes news which later prove\nto be wrong, he will correct it.\n-The International Federation of Journalists (1954).\nRecently, there has been a radical change in the way news is being consumed and\nproduced in society. Namely, social media such as Twitter and Facebook have become\npopular places for people to read, distribute and discuss news. Unlike traditional media\noutlets, anyone can register as a news publisher on social media. Consequently, news\npublishers that do not obey the standards of professional journalism, which are formulated\nin the quote, can also produce and distribute news via social media (Ribeiro et al., 2018).\nOne e\u000bect of this is the production of news articles containing false or biased information.\nThis becomes problematic when such news stories a\u000bect the political opinions of readers,\nwhich may in turn in\ruence the outcome of political elections. This is likely to happen\nbecause of the crucial role that news has in shaping citizens opinions, choices and thoughts.\nBesides, a recent study of Bhatt et al. (2018) showed that there were a large amount\nof biased and false news websites during the presidential elections of 2016. The fact that\na large proportion of those websites disappeared after the elections suggests that these\nnews articles may have been speci\fcally aimed at in\ruencing the outcome of the elections.\nFurthermore, another study revealed that many supporters of Hillary Clinton received their\nnews from mainstream sources, such as the New York Times and The Washington Post ,\nwhile Trump supporters were more exposed to publishers of right-leaning biased news on\nsocial media (Marwick and Lewis, 2017).\nUnfortunately, social media users are often unaware that they read biased or fake news.\nOne reason could be that publishers of legitimate news are also active in social media\nwebsites. This makes it di\u000ecult for users to distinguish between legitimate and illegitimate\nnews publishers. A possible solution would therefore be to monitor and limit the production\nof illegitimate news, such as done in traditional news media by scholars and watchdog\ngroups (Ribeiro et al., 2018). However, this is a challenging, perhaps even unsolvable\ntask on social media, where large amounts of textual contents are spread globally every\nsecond. A more e\u000bective solution might therefore be to build a computational systems that\nautomatically detect whether a news article relies on false information, or uses subjective\nlanguage that expresses approve or disapproval towards a political approach. The latter\nwill be the main purpose of this thesis.\nHyperpartisan News Detection 2/80\n1.1 Hyperpartisan News\nThe most popular term to refer to politically biased news is hyperpartisan news. It has been\nde\fned as a type of news that is characterized by extremely one-sided content (Kiesel et al.,\n2019). The term was \frst mentioned in an article from the New York times Magazine1,\nwhere it was formulated as a style of reporting that departs from the traditional notions of\njournalistic balance by providing a biased picture of one side of the political debate. The\nterm was coined in context of the US Elections from 2016, implying bias that was either\nleft-leaned, showing overt support of the Democratic Party, or right-leaned, which expresses\nsupport to the Republican Party. Hyperpartisan news has additionally been characterized\nas a speci\fc type of fake news. Fake news is the kind of news in social media that spreads\nmore successfully than the others and are typically extremely one-sided (hyperpartisan),\nin\rammatory, emotional and composed of untruths (Potthast et al., 2017). Although\nrelated, we believe that hyperpartisan news and fake news are two distinct types of news.\nReporting facts in a subjective manner is not the same as reporting false facts. However,\nsince the two types of news report on similar topics and use similar styles to do so, we\nsurmise that it is likely that features for hyperpartisan news detection are helpful for fake\nnews detection.\nIn order to further illustrate what hyperpartisan news is, we provide an example of an\narticle that is left-learning below.\nPresident Donald Trump urged Congress Thursday morning to launch an in-\nvestigation of the news media, wondering online why so much of our news is\njust made up. He did not single out a speci\fc story or media outlet\nthat he believed to be guilty of inaccurate reporting. Trump's fake\nnews complaints have been a staple of his political rhetoric, a label\nhe often applied to stories that feature negative reporting about him\nor his presidency. Most recently, Trump has railed against reports that have\ncharacterized his administration's hurricane recovery e\u000borts in Puerto Rico as\ninadequate, as well as against an NBC News report that Secretary of State Rex\nTillerson called the president a moron over the summer and nearly resigned.\nThe phrases in the example, especially the ones in bold, are used to express disapproval\nof Donald Trump and his presidency. In particular, the underlying message of this article\nseems to be that Donald Trump is pretending that the negative reports about him or his\npresidency are examples of fake news. The author seems to make this point stronger by\nmentioning that Trump was not able to come up with an example to prove his statement,\ni.e., \\He did not single out a speci\fc story or media outlet that he believed to be guilty of\ninaccurate reporting\". Moreover, the author uses certain bias-laden words to strengthen\nher point, for instance by using the words urged and railed .\nFollowing this, one approach that could be used to detect hyperpartisan news automat-\nically would be to develop a system that relies on a list of words that are frequently used\n1J. Herman. (2016). Inside Facebook's (Totally Insane, Unintentionally Gigantic Hyperpartisan)\nPolitical-Media Machine. (24 August 2016). https://nyti.ms/2k82R8I\nHyperpartisan News Detection 3/80\nto express approval or disapproval towards an entity. Still, this method is only probable to\nbe e\u000bective when the lexicon is large and when it is used by a system that is able to detect\nvarieties of the same word. Even then, the lexicon is likely to be non-exhaustive. Another\napproach could therefore be to train a classi\fer in such a way that it knows which words\noccur more frequently in hyperpartisan news than in mainstream news by representing\ndocuments as bag-of-words. However, language is rich, and people can use di\u000berent words\nto express disapproval or support towards an entity. Thus, when a system is trained to\ndetect hyperpartisan news by means of a speci\fc set of words, this system may not work\nwell when it sees words that are not in the vocabulary of the training data. Moreover, even\nwithin hyperpartisan news, there are di\u000berences in the way how writers express bias. Let\nus consider another example:\nDonald Trump ran on many braggadocios andlargely unrealistic campaign\npromises. One of these promises was to be the best, the hugest, the most\ncompetent infrastructure president the United States has ever seen. Trump\nwas going to \fx every infrastructure problem in the country and Make America\nGreat Again in the process. That is, unless you are a brown American. In that\ncase, you're on your own, even after a massive natural disaster like Hurricane\nMaria.\nSimilar to the other example, the author uses certain adjectives to express disapproval\nof trump, such as braggadocious . However, these adjectives did not occur in the previous\nexample. Thus, it would perhaps be more e\u000bective to represent documents in such a\nway that words are grouped by their purpose or general topic, by using knowledge from\nadditional sources than the training data. Ideally, a system that detects hyperpartisan news\nshould be able to know that Donald Trump ,Hillary Clinton ,Barack Obama are political\n\fgures and that the words braggadocious and unrealistic are used to express disapproval.\nA more sophisticated solution to detect hyperpartisan news would therefore be to use\ndenser representations than bag-of-words, by grouping words with similar meanings and/or\nwords that come from the same topic together in a cluster. On the other hand, this could\ncause us to miss important information. Furthermore, the extent to which we succeed\nin grouping those words may depend too much on the quality of the resources that are\nnecessary to e\u000eciently group those words. Thus, it is a challenging task to determine\nwhich features we should select.\nIn related studies about the automatic detection of linguistic bias in texts, the problem\nwas usually tackled by building systems that relied on a large combination of local features\nfrom the documents, such as: the overall sentiment, the readability, the usage of certain\nPOS n-grams and the occurrence of speci\fc bias-laden words (see Chapter 2). These\nfeatures could be interesting for hyperpartisan news detection as well, especially when\nthey would be combined with features to capture sarcasm in documents. Nonetheless,\nbecause of the stylistic di\u000berences between hyperpartisan news articles, we believe that\nsystems relying on such features will not work well on all texts. For instance, the second\nexample that we showed contains more passages that indicate a negative sentiment or\nsarcasm than the previous article. Still, it could be the case that these features are helpful\nHyperpartisan News Detection 4/80\nwhen they are used in combination with an e\u000bective, dense document representation of a\ntext. There are however, to the best of our knowledge, no studies that experimented with\nthis combination of features.\n1.2 Research Questions\nThe purpose of this research is to build a classi\fer that automatically identi\fes for a given\nnews document whether it is an instance of hyperpartisan news or not. In particular,\nwe approach this task by building a supervised machine learning classi\fer that learns\nappropriate textual features to discriminate between hyperpartisan and legitimate news.\nOur research can therefore be motivated by the following general question: Is it possible to\nbuild an accurate supervised machine learning classi\fer that detects hyperpartisan news? .\nIn the previous section, we surmised that the best way to approach this task would\nbe to \fnd an e\u000bective method to vectorize documents, rather than building systems that\nrely on a combination of local features. Therefore, the \frst step in answering our main\nresearch question will be to \fnd the most e\u000bective document representation technique to\nvectorize documents for hyperpartisan news detection. We therefore formulate our \frst\nresearch question as:\nRQ1: What is the most e\u000bective method to vectorize documents for hyperpar-\ntisan news detection?\nIn order to answer this question, we will try to create a simple classi\fer that only relies on\none document representation technique, for instance by averaging the word embeddings in a\ndocument. Despite its simplicity, we try to build a system that receives competitive results\non a test set from a shared task on hyperpartisan news detection. However, some document\nrepresentation techniques are perhaps more powerful when they are used in combination\nwith other document representation techniques or when they are used together with local\nfeatures, such as the sentiment or the readability of a text. Because of this, the next step\nof our study will be to add helpful local features to the classi\fers that rely on one of the\ndocument representations techniques that we use. In this way, our second research question\ncan be formulated as:\nRQ2: Can we improve our classi\fers by combining document representation\ntechniques and/or by adding local features?\nIn addition to building an accurate classi\fer on the test set of the competition, our aim is\nalso to build a classi\fer that performs well on a di\u000berent dataset of hyperpartisan news.\nIn particular, we attempt to create a classi\fer that is able to perform similarly well across\ndatasets, rather than being speci\fcally tuned to one training set. In other words, we\naim to build a system that provides robust results across datasets of hyperpartisan news.\nTherefore, we formulate the third research question as:\nRQ3: Is it possible to build a classi\fer for hyperpartisan news detection that is\nrobust enough to perform well across di\u000berent datasets of hyperpartisan news?\nHyperpartisan News Detection 5/80\nFinally, we try to take robustness one step further by evaluating our most promising systems\non a di\u000berent but related task, without using speci\fc feature tuning on this task. The task\nthat we select for this purpose is fake news detection. We have mentioned in the previous\nsection that fake news is, to a certain extent, related to hyperpartisan news. Because of\nthis, it would be interesting to develop a system that is able to detect both types of news.\nThus, our last research question can be motivated by:\nRQ4: Is it possible to build classi\fers for hyperpartisan news detection that\ncan be applied to other related tasks?\nWe will answer our research questions by evaluating the performance of our systems\nthrough di\u000berent set-ups. Moreover, we will experiment with di\u000berent document represen-\ntation techniques to address the sparsity problem in text classi\fcation. More speci\fcally,\nwe will work with dense word representations such as word embeddings that aim to com-\npute semantic relatedness between words. These representations also help to tackle the\nout-of-vocabulary words problem given that they are pre-trained on large amounts of un-\nlabelled data which contains a huge vocabulary size. Despite the promising aspects of\nthese techniques, there is, to the best of our knowledge, few or no work available that has\nexperimented with these type of semantic features for hyperpartisan/fake news detection.\nWe therefore experiment with di\u000berent types of document and word representations and\nhighlight the extent to which they are robust and e\u000bective for hyperpartisan news.\n1.3 Contributions\nThe main contributions of this thesis are the following:\n\u000fWe provide a systematic comparative analysis of document representation techniques\nfor hyperpartisan news detection that can easily be applied to other text classi\fcation\ntasks.\n\u000fWe conduct a comparative analysis of document representation techniques for text\nclassi\fcation.\n\u000fWe present the \frst study to use a wide range of semantic features based on word rep-\nresentations obtained from both in-domain and out-of-domain data for hyperpartisan\nnews detection.\n\u000fWe provide examples of feature types that are speci\fcally useful for hyperpartisan\nnews detection, but also for tasks that concern the detection of linguistic bias.\n\u000fWe conduct one of the \frst studies to experiment with Flair (Akbik et al., 2018), a\nnovel NLP toolkit which allows to combine word embeddings and that uses several\ntechniques to e\u000bectively use these word embeddings in text classi\fcation.\n\u000fWe provide a comparative study of the e\u000bectiveness of current techniques to create\nword embeddings with Word2vec, GloVe, FastText and Flair.\nHyperpartisan News Detection 6/80\n\u000fWe describe and test an empirical method to develop robust models for fake news\nand related tasks.\n\u000fWe obtain very competitive results for in-domain evaluation in the o\u000ecial hyperpar-\ntisan news detection benchmark.\n\u000fWe outline a robust set of features that perform well across tasks related to fake news\ndetection. Our systems outperform previous work in an out-of-domain evaluation on\nfake news without performing any speci\fc tuning on the classi\fers that we used for\nhyperpartisan news detection.\nIn the next chapter, we position our project in the existing body of research that has\nbeen conducted on hyperpartisan news detection and related tasks. In Chapter 3, we\npresent the data that we used for this study. The Chapters 4 and 5 describe the methods\nand materials that we used to answer our research questions. In the former, we introduce\nthe di\u000berent document representations that we used and highlight their advantages and\ndisadvantages. The latter describes the experimental set-up of the project. We subse-\nquently present the results of our studies in the Chapters 6, 7 and 8. We \fnish our work\nwith an analysis of the results in Chapter 9 and conclude our research in Chapter 10.\nHyperpartisan News Detection 7/80\n2 Related Work\nIn this chapter we present the theoretical framework that is related to hyperpartisan news\ndetection. Hyperpartisan news is a relatively new phenomenon, and has therefore received\nlittle attention in computational linguistics. Therefore, we present studies concerned with\ntasks which aim to automatically detect linguistic bias. Moreover, since we also develop\nclassi\fers for fake news detection in this project, we additionally discuss studies concerned\nwith this task. Despite the di\u000berences between the tasks and domains, we hypothesize\nthat the features and systems discussed in these works aid to develop systems that identify\nhyperpartisan news. After all, the tasks can be approached as sub-tasks within detecting\nbiased language.\n2.1 Hyperpartisan Content Detection\nThis section consists of two parts. In the \frst part, we describe the related work that is\nconducted on automatically detecting hyperpartisan news. In the second part, we present\na body of research concerned with the manual detection of political bias.\n2.1.1 Automatic Detection\nThe study of Potthast et al. (2017) is one of the few available studies that is particularly\nfocused on detecting hyperpartisan in addition to fake news. They approached fake news\ndetection by investigating the writing style of fake news in relation to hyperpartisan news.\nTheir corpus consisted of 1,627 articles. 826 of these documents were mainstream news\narticles and the remaining documents were hyperpartisan news articles. Furthermore, 256\nof those articles were on the left-wing and 545 right-wing of the political spectrum. To\ndetect hyperpartisan news, Potthast et al. (2017) used a writing style model that relied on\ncommonly used stylistic features and some speci\fc news domain features combined with\ndictionary-based features. The latter implied the usage of the General Inquirer Dictionaries\n(Stone et al., 2007). Domain speci\fc features are based on the number of quoted words\nand external links, the number of paragraphs and their average length in an article. In\norder to avoid over-\ftting, they discarded all features that occurred in less than 10 percent\nof the documents. Using these stylistic features, the authors managed to build an accurate\nclassi\fer ( accuracy = 0.75 ) that discriminates between hyperpartisan and mainstream\nnews. Hence, it is possible to build a simple classi\fer that only relies on stylistic features.\nNonetheless, they did not test their best system on fake news detection, which would have\nbeen interesting to test how speci\fcally useful these features are for hyperpartisan news\ndetection.\nAnother study that focuses on hyperpartisan news detection is the work of Hutto et al.\n(2015). Their study is particularly centered on bias detection in news reports and they aim\nto quantify the amount of the bias. The features that they considered in their study were\nbased on a survey that investigated factors that a\u000bect the perception of bias in news stories.\nThey worked with several types of features. First, they performed a structural analysis at\nHyperpartisan News Detection 8/80\nsentence level containing sentiment scores using VADER (Hutto and Gilbert, 2014). They\nalso retrieved the subjectivity score, modality score and general mood of the text. Finally,\nthey computed the readability score using Flesh-Kinchaid Grade (Kincaid et al., 1975).\nThe second type of features that they used were motivated by Recasens et al. (2013),\nof which they used features such as factive verbs, implicative verbs, assertive verbs and\nhedges. In addition, they also used coherence markers and several type of words from the\nLinguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015), such as causation\nwords, certainty words, tentative words, third-person pronoun, achievement words, work\nwords, conjunctions, prepositions and adverbs. They used a statistical linear regression\nmodel and both forward and step-wise (AIC) to measure the relative quality of each feature\nfor the degree of bias. Their results showed that sentence levels of subjectivity were less\nmeaningful, whereas sentence level measure for modality was a stronger indicator of bias\nthan LIWC certainty words. In addition, their \fndings proved that the readability was\nunrelated to the degree of perceived bias. Other poor indicators were implicative verbs,\ndegree modi\fers, coherence markers, causation words, conjunctions, adverbs, prepositions\nand auxiliary verbs. By removing those features their system reached an accuracy greater\nthan 97 percent and accounted for 85.9 of the variance in the human judgments of perceived\nbias in news.\nFinally, when we started with this project, there was a shared task on hyperpartisan\nnews detection organized by SemEval 2019 (Kiesel et al., 2019). We used the data of this\nshared task (see Chapter 3) for the current project and were able to continue submitting\nruns on the test set through the software TIRA (Potthast et al., 2019). Furthermore,\nbefore starting the thesis, we submitted a system to the competition which obtained an\naccuracy of 0.621 on the test set (30th out of 42 participants). We use this system as the\nbaseline for this project (see Chapter 5). Moreover, the winning system scored an accuracy\nof 0.822 on the test set. Since the description papers of the submitted systems were not yet\npublished during the thesis, we were unable to gain knowledge about the winning system\nand to build further on this knowledge during the thesis.\n2.1.2 Manual Detection\nThe work of Yano et al. (2010) is another study that is concerned with identifying political\nbias in texts is the work of Yano et al. (2010). Nonetheless, this study is particularly\naimed at identifying linguistic indicators of bias in American political blog posts of 2008.\nArguably, this falls into a di\u000berent text genre than news articles. Furthermore, the authors\ndid not test the usefulness of their potentially relevant features by deploying a text classi-\n\fcation system. Instead, Yano et al. (2010) created a corpus of texts with labeled by their\noverall bias. The \frst step that they conducted to built this corpus was to extract 261.073\nsentences from their collected blog corpus, which contained an equal amount of conserva-\ntive and liberal texts. To identify the sentences, the authors ensured that the sentences\nsatis\fed at least one of three conditions. The \frst was that the sentence should contain\none of the \\sticky\" partisan bi-grams, which were de\fned as terms that are particular to\none side in the political debate. In order to capture those terms from the sentences, the\nHyperpartisan News Detection 9/80\nauthors created two lists of those bi-grams: one representing liberal terms (495 bi-grams)\nand one for the conservative side (539 bi-grams). The second condition implied that the\nsentence should have at least one of the words in the list of emotional words that they\ncreated. This list contained several items from the LIWC dictionary (Pennebaker et al.,\n2015), including words that could be used to express negative emotion, positive emotion,\ncausation or anger. The \fnal condition was concerned with the membership of \\killing-\nrelated verbs\". The potential relevance of those verbs for determining political bias was\nderived from Greene and Resnik (2009). Greene and Resnik (2009) observed that when\ncomparing Millions of people starved under Stalin toStalin starved millions of people the\nlatter was generally viewed as being more negative towards Stalin. They concluded that\nkilling-related verbs provide strong examples of this phenomenon because they exhibit a\nparticular set of semantic properties that are associated with the transitive verb. There-\nfore, Yano et al. (2010) used the 11 \\kill verbs\" from the study of Greene and Resnik\n(2009), such as slaughter, assassinate, shoot, poison, strangle, smother and su\u000bocate in\ntheir third condition.\nBased on these three conditions, Yano et al. (2010) built a corpus of `biased' sentences\nin which annotators had to quantify the bias as none ,somewhat orvery. The results led\nYano et al. (2010) construct a list of biased words and their relative frequency of the bias\nmark. For instance, the words administration ,Americans ,woman andsingle were strongly\nassociated with liberal bias. In contrast, the words illegal ,Obama's and corruption were\nstrong predictors of bias leaning towards the conservative political spectrum. However, the\nauthors observed that although the participants managed to detect bias correctly, it was\ndi\u000ecult for them to detect the type of bias. In particular, the annotators had an overall\nagreement of 0.55.\nAnother study which showed that determining the strength of the bias in an article\nis the work of Vincent and Mestre (2019). Annotators disagreed about one third of the\narticles when they had to label the amount of bias in a text. More speci\fcally, the authors\ncollected a dataset of 1,273 articles labeled by three annotators through crowd-sourcing.\nAll articles were assured to be about political news. 50 percent of this dataset was used for\nthe shared task on hyperpartisan news as a test set and the remaining was publicly made\navailable for the participants to train their systems on. We therefore further explain this\ndataset in the next chapter.\n2.2 Fake News Detection\nOn one hand, there is a body of research available on fake news detection that relied on\ntraditional supervised classi\fcation techniques, such as SVM (Vapnik, 1995). We present\na few of these studies in the \frst part of this section. On the other hand, there are several\nstudies that used neural networks for this task. Because of recent technological advances\nand the success of deep learning in NLP, it is likely that future studies will continue to use\nthem. We therefore present the studies that worked with neural networks in the second\npart of this section.\nHyperpartisan News Detection 10/80\n2.2.1 Feature-based Supervised Learning\nOne interesting study on fake news detection is the work of P\u0013 erez-Rosas et al. (2018). The\nauthors worked with two fake news datasets. The \frst dataset was created by humans\nvia crowd-sourcing. Each participant received several legitimate news articles coming from\nthe following domains: business, entertainment, politics, technology, and education and\nsports. Then, the task was to rewrite a rewrite a shorter `fake' version while emulating a\njournalistic style. The outcome of this procedure was a parallel corpus of 250 news articles\nand their respective fake versions (500 articles in total). The second dataset was a corpus of\nfake news that naturally occurs on the web, which was about celebrity news. It contained\n500 articles, with an even frequency distribution for legitimate and fake news. P\u0013 erez-Rosas\net al. (2018) used the following set of features:\n\u000fUni-grams and bi-grams from the bag-of-words representation of each news article\nwith tf-idf weighting.\n\u000fPunctuation features that denoted the type of punctuation that was used, derived\nfrom the Linguistic Inquiry (LIWC) and Word Count Software (Pennebaker et al.,\n2015).\n\u000fPsycho-linguistic features, also derived from the LIWC.\n\u000fSyntactic features representing the context-free grammar of a document.\n\u000fReadability features.\nThe readability features were mainly derived by extracting content features such as the\nnumber of characters, complex words, number of syllables, word types, number of para-\ngraphs and long words. In addition, the authors calculated several readability metrics,\nsuch as Flesh Kinchaid Grade (Kincaid et al., 1975), the Automated Readability Index\n(Smith and Senter, 1967), the Gunning Fog Grade Readability Index (Gunning, 1952)\nand the SMOG readability index (McLaughlin, 1969). The features were used in a linear\nSupport Vector Machine (SVM) classi\fer with default parameters and evaluated on 5-fold\ncross-validation. For both datasets, the most accurate systems were classi\fers that relied\non all features. In particular, their systems reached an accuracy of 0.74 on the fake news\ncorpus that was created through crowd-sourcing and 0.76 on its counterpart.\nP\u0013 erez-Rosas et al. (2018) also performed an across-domain evaluation in which the\nmodels were trained using the crowd-sourced fake news corpus and tested on the other\ndataset, and the other way around. In both cases, the accuracy of the systems dropped\nsigni\fcantly, as they varied between 0.48 and 0.65. A further interesting \fnding was that\nreadability features seemed to be equally relevant in both domains. Thus, P\u0013 erez-Rosas\net al. (2018) proved that the domain of the article is an important aspect that should be\ntaken into account when classifying fake news. However, it is crucial to bear in mind that\nthe authors used small datasets and that the datasets di\u000ber in another aspect that their\ndomain, which is that only one of them consists of `natural' fake news. This could also be\na valid explanation for the decreased performance.\nHyperpartisan News Detection 11/80\nHorne and Adali (2017) approached fake news detection in a similar manner. They\nused a similar set of features as P\u0013 erez-Rosas et al. (2018), such as the readability of the\ntext, LIWC features and stylistic features involving the style of the writers and the syntax\nof the text, such as the amount of nouns and verbs. Besides, the authors also used a linear\nSVM classi\fer. A small di\u000berence is that Horne and Adali (2017) also used sentiment\nfeatures to detect fake news, by means of the tool SentiStrength (Thelwall et al., 2012),\nwhich measures the intensity of positive and negative emotion in a document. However,\nthe main di\u000berence with their study is that they also study the title of the articles and\ntheir importance to distinguish between fake and legitimate news. In particular, Horne\nand Adali (2017) observed that fake news articles are longer than legitimate news articles.\nThey also contain simpler words. Furthermore, fake news titles use more capitalized words,\nsigni\fcantly more proper nouns, whereas fewer stop-words and nouns overall. The authors\nsurmised that writers of fake news attempt to squeeze as much content into the titles by\nskipping stop words and nouns to increase the amount of proper nouns and verb phrases.\nOverall, Horne and Adali (2017) were able to achieve an accuracy score of 0.71 when\nthe body of the text was used and 0.78 when using only the title of the text on 5-fold\ncross-validation. Still, it is crucial to bear in mind that the authors used two relatively\nsmall datasets. One of them contained 36 instances of legitimate news and 35 fake news\ntexts. The other dataset had 75 mainstream news articles and 75 fake news articles.\nBesides, all articles were about politics. We have seen in the study from P\u0013 erez-Rosas\net al. (2018) that the predictive power of features can di\u000ber per domain. Thus, it remains\nquestionable whether the authors would obtain a similar performance when there would\nbe more documents and/or when they would work with di\u000berent topics.\nThe previously discussed studies conducted by P\u0013 erez-Rosas et al. (2018) and Horne\nand Adali (2017) seem to suggest that fake news detection can best be approached by\nusing a large set of di\u000berent features. This makes us wonder whether it would also be\npossible to use a less sophisticated approach. This is addressed in the study of Ahmed\net al. (2017), who solely used n-gram modeling with tf-idf weighting to detect fake news.\nThey only performed a few data pre-processing steps, such as stop word removal and\nstemming. Another major di\u000berence is that Ahmed et al. (2017) used a large dataset of\n12,600 fake news articles and 12,600 legitimate news articles. Besides, the authors also\nexperimented with other classi\fcation techniques in addition to a linear SVM classi\fer:\nStochastic Gradient Descent, K-Nearest Neighbour, Decision Trees and Support Vector\nMachines (SVM) with non-linear kernels. Still, the highest result was obtained by a linear\nSVM classi\fer, yielding an accuracy score of 0.92. This classi\fer used 1 as the value for\nnand 50,000 as the number of features for the bag-of-words model. Thus, it is possible\nto detect fake news with a simple linear SVM classi\fer that only relies on bag-of-word\nuni-grams.\n2.2.2 Supervised Learning with Neural Networks\nWithin the theoretical framework of fake and hyperpartisan news detection, there are a\nfew studies that have used neural networks to tackle the classi\fcation task. One example\nHyperpartisan News Detection 12/80\nof such a study is the work of Bajaj (2017), who experimented extensively with various\nneural network architectures. In contrary to the previously mentioned works, Bajaj (2017)\nworked with a large dataset of fake news (63,000 articles). The neural network structures\nthat were used in this study are described below.\n\u000fA simple two-layer feed-forward neural network that used the average of all vectors\ncorresponding to the average of all words in the news story. Thus allowed to normalize\nthe vectors by their length while taking every content word into account.\n\u000fA bi-directional Recurrent Neural Network (RNN) to consider the entire length of\neach news article. To deal with the length of each article Bajaj (2017) used a limit of\n200 time steps (and shorter examples were padded with zero vectors at the beginning).\nThe author also experimented with Gated Recurrent Units (GRUs) (Cho et al., 2014)\nand Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).\n\u000fConvolutional Neural Networks (Krizhevsky et al., 2012) with max pooling.\n\u000fAttention-augmented Convolutional Neural Networks.\nIn general, the Recurrent Neural Networks outperformed the other classi\fcation techniques.\nIn particular, the highest scores were achieved with GRUs ( f-score=0.84 ), LSTMs ( f-\nscore=0.81 ) and BiLSTMs ( f-score=0.81 ). Thus, the study showed that GRUs and LSTMs\nare e\u000bective classi\fcation architectures for fake news detection.\nAnother study which proved that LSTMs are useful for fake news identi\fcation is the\nstudy of Ajao et al. (2018). They worked with three di\u000berent types of LSTMs: LSTM\nwithout dropout regularization, LSTM with dropout regularization and LSTM with Con-\nvolutional Neural Networks (CNN). Their dataset consisted of 5,800 Tweets. Even though\nall LSTMs performed well, the highest accuracy was achieved by the LSTM that did not\nuse dropout regularization ( accuracy = 0.8229 ). However, the size of their data is smaller\nthan ours and the length of the documents is shorter, since Ajao et al. (2018) are not\nworking with news articles but with Tweets.\nSinghania et al. (2017) proposed a di\u000berent method, in which they built a hierarchical\nattention network to represent the internal structure of a news article. The model contained\nfour levels: one for words, the headline (both using GloVe word embeddings and a bi-\ndirectional GRU), sentences (using a bidirectional GRU), and the headline. It constructed\na news vector by processing an article from a hierarchical bottom-up manner. Furthermore,\ntheir model relied on three assumptions: words form sentences, sentences form the body\nand the headline with the body forms the article. The headline was viewed as a distinctive\nfeature of the article that contains a concise summary of the article body and contains useful\ninformation in the form of its stance with respect to the body. To extract relevant words\nof a sentence, Singhania et al. (2017) created a sentence representation that is formed with\nan attention layer. The same was done to extract relevant features from the headline. On\nthe \fnal level, relevant sentences were identi\fed in the formation of the body by using an\nattention layer. Using this hierarchical attention network, the authors were able to obtain\nHyperpartisan News Detection 13/80\naccuracy scores between 0.9481 and 0.9677. These scores outperformed the classi\fers that\nsimply modeled the structure of the article by concatenating the headline to the body of\nthe article. However, the results were only slightly higher. For instance, a simple SVM\nmodel that used bag-of-n-grams with tf-idf weighting already reached an accuracy of 0.9247\non their corpus of 20.372 fake news articles and 20,932 mainstream articles. Other bag-of-\nwords classi\fers obtained accuracy scores between 0.9121 and 0.9247. Still, the fact that\ntheir attention model managed to outperform these high accuracy scores provides evidence\nthat taking the hierarchical structure of the article into account yields a more accurate\napproach than bag-of-words.\n2.3 Biased Language Detection\nIn addition to news, there is another genre of online texts in which bias can occur, which is\nWikipedia articles. Wikipedia articles are a popular resource for related work on linguistic\nbias detection in general. An example of a work that is concerned with this is the study\nof Recasens et al. (2013). This work used a corpus of 7464 Wikipedia articles. All articles\ncontained edits from authors that were associated with a neutral point of view (NPOV) tag.\nThus, these edits were likely to be used for the purpose of removing bias. Furthermore,\nthey categorized bias language into two types. The \frst, which was etymological bias,\ninvolves prepositions that are commonly agreed to be true or false and are presupposed,\nentailed, asserted or hedged in a text. In order to identify this type of bias, the authors\nused factive verbs, entailments, assertive verbs and hedges.\nThe other type of bias is framing bias, which occurs when subjective one-sided words are\nused to reveal the author's stance. This type of bias was detected by extracting subjective\nintensi\fers, one-sided terms (e.g., liberated, captured, pro-life). They trained a logistic\nregression model that takes each word as a feature and its surrounding context using a 5-\ngram window capturing the two words to the left and the three words to the right. Given\nthe word w, they extracted around 32 features, including the lemma, POS tag of the left\nword, POS tag of the left-most word before w, whether one or two words around wis an\nassertive, factive, report or implicative verb and whether the word wis in the list of Liu\net al. (2005) list of positive words and/or negative words. A few of the most contributing\nfeatures were: the word wunder analysis, the POS-tag of the word after w, whether wis a\nreport verb or a positive or negative word. They also computed the readability of the text,\nusing the same metrics as P\u0013 erez-Rosas et al. (2018). The authors evaluated their system\nby outputting as the highest bias ranked word or the two or three lowest ranked words,\non which they reached an accuracy score of 0.3435, 0.4652 and 0.5870 respectively. They\nreceived the best results by using all features, which showed that contextual, linguistic and\nsentiment-related characteristics are important.\nThe work of Hube and Fetahu (2018) also focused on bias detection in Wikipedia. Their\nset-up di\u000bers from Recasens et al. (2013) in that they detected bias in 1000 statements\nfrom NPOV Wikipedia articles. The most informative features, which were selected by\nthe chi-square selection algorithm and in a Random Forest classi\fer, were related to the\nratio of biased words in a statement and their context, LIWC features based on psycho-\nHyperpartisan News Detection 14/80\nlogical and psychometric analysis and the context in which words from speci\fc lexicons\nappeared that represented epistemological and framing bias. They used lexicons with re-\nport verbs, assertive verbs, factive verbs, positive words, negative words, implicative verbs.\nFurthermore, they created an automatic dictionary of biased words by crawling all articles\nunder politics from Conservapedia (11,793), which is a wikipedia page according to right-\nconservative ideas including strong criticism and attacks especially on liberal politics and\nmembers of the Democratic Party of the United States. They used this data to train word\nrepresentations with Word2vec (Mikolov et al., 2013; Mikolov, 2013). They worked with a\nseed word list and looked to the list of closest words in their word representation. By using\nthese features, they were able to score an accuracy of 0.73 on the test set with a Random\nForest classi\fer.\n2.3.1 Summary\nThe related studies that we described in this chapter provided interesting features and clas-\nsi\fcation architectures for (related tasks to) hyperpartisan news detection. For instance,\nwe have seen in the previous studies concerned with hyperpartisan news detection that\nstylistic features are useful (Potthast et al., 2017). The studies that were concerned with\nbias detection in other texts genres than news, pointed out that using a combination of\na large set of di\u000berent features is particularly useful to detect bias linguistically. These\nfeatures were mainly local features, such as the sentiment, POS n-grams, the usage of an\nassertive and/or factive verb and the lemma of a word (Recasens et al., 2013; Hutto et al.,\n2015). Some features that were mentioned in these studies were also e\u000bective for fake news\ndetection, such as dictionary-based features from the Linguistic Inquiry and Word Count\nSoftware (Pennebaker et al., 2015) and readability features. Because of this, we will also\nconduct experiments with a few of these features the current work. We will refer to them as\n`additional features' or `local features', since we will use them in addition to our document\nrepresentations.\nMoreover, the works discussed in this chapter proved that a Support Vector Machine is\nan e\u000ecient classi\fcation technique for fake news detection, especially when it uses a linear\nkernel (Ahmed et al., 2017; Gilda, 2017). We therefore decided to work with Support\nVector Machines in this study as well. Furthermore, the studies that used neural networks\nfor fake news detection showed that Recurrent Neural Networks (RNNs) are the way to\ngo when using neural networks. One study experimented with a variant of RNNs, namely\nLong short-term memory (Ajao et al., 2018), which obtained promising results for future\nwork on fake news detection. In the work of Bajaj (2017) however, the author compared\nthe performance of a LSTM to another variant of RNN, namely Gated Recurrent Units\n(GRUs). The latter performed better than LSTM. Because of the promising results with\nRecurrent Neural Networks in these related studies, we decided to experiment with them\nas well. We particularly experimented with GRUs, since they received better results than\nLSTMs (Bajaj, 2017).\nHyperpartisan News Detection 15/80\n3 Data\nIn this chapter, we present the two datasets that we will use in the rest of this thesis.\nThe \frst is the hyperpartisan news dataset from SemEval-2019 Task 4 on hyperpartisan\nnews detection (Kiesel et al., 2019). The second is the fake news dataset from the study of\nP\u0013 erez-Rosas et al. (2018), which we will use to test our models in an out-of-domain setting.\n3.1 The SemEval-2019 Task 4 Hyperpartisan News Dataset\nThe complete dataset that was used for the shared task consists of two corpora: the by-\npublisher and the by-article corpus. The former is labeled by the overall bias has provided\nby Buzzfeed journalists or Mediabiasfactcheck.com. In contrast, the by-article dataset is\nlabeled through crowd-sourcing (Vincent and Mestre, 2019). We describe both sets in\nSection 3.1.1 and 3.1.2 respectively.\n3.1.1 By-publisher\nThe by-publisher dataset is automatically annotated based on a list of publishers of hy-\nperpartisan news (Kiesel et al., 2019). It is further divided into three sets: a training\nset, a validation set and a test set. The training set was available for participants in the\nshared task. It contains 600.000 documents, and there are 60 di\u000berent publishers. Half of\nthese documents are labeled as hyperpartisan news (hyperpartisan= true). The remaining\ndocuments are labeled as legitimate news (hyperpartisan= false). Furthermore, the articles\nare labeled by their position on the political spectrum. This position is indicated by either\none of the following classes: left, left-center, least, right-center and right. Articles that\nare labeled as being on the right orleftside of the political spectrum are automatically\nlabeled as being hyperpartisan. However, when using the by-publisher dataset, we focused\non binary prediction (i.e., the hyperpartisan class: true or false), since this was also the\nmain task in the competition.\nThe validation set has 150.000 documents. The frequency distribution of the classes\nis also balanced. Furthermore, this dataset was available for participants that signed up\nfor the shared task, similar as the training set. None of the publishers that occur in this\nset also occur in the training set. In this way, the organizers ensured that the participants\nwould try to extract features for hyperpartisan news, rather than modeling the features of\nthe publishers themselves (i.e., publisher attribution).\nDespite the attractive size of the by-publisher training and validation set, we decided to\nnot train systems on the complete by-publisher training nor validation set. Since we were\nplanning to conduct a large amount of experiments, it was more practical to use a smaller\nbut representative dataset of the by-publisher corpus. This set contains articles from both\nthe by-publisher training and validation set. We will further present this dataset in Section\n3.2. Nonetheless, we used the by-publisher training set for other purposes in this study.\nWe will describe this in more detail in the next chapters.\nHyperpartisan News Detection 16/80\nFinally, the test set of the by-publisher corpus contains 4000 articles: 2000 of them\nare hyperpartisan news and 2000 are mainstream news. This dataset was not used for\nthe competition, but participants could test their classi\fers on this dataset for further\nexperimentation. However, the by-publisher dataset was hidden for the participants. It\nwas also not published after the evaluation period of the shared task. However, it is possible\nto test systems on this dataset by using the virtual machine provided by the organizers.\nThis system is called TIRA and further described in Potthast et al. (2019).\n3.1.2 By-article\nThe complete by-article corpus contains 1,273 articles of political news from hyperpartisan\nand mainstream websites. The data was manually annotated by three annotators through\ncrowd-sourcing (Vincent and Mestre, 2019). The annotators were asked to grade the bias\nof the article on a 5-point Likert scale:\n1. no hyperpartisan content\n2. mostly unbiased, non-hyperpartisan content\n3. not sure\n4. fair amount of hyperpartisan content\n5. extreme hyperpartisan content\nThe organizers Kiesel et al. (2019) removed the articles of the dataset with a low agreement\nscore and the articles that received a score of 3. The labels were binarized by labeling the\narticles with a score of 4 and 5 as hyperpartisan. The articles that scored 1 or 2 were\nlabeled as not being hyperpartisan. The result of this procedure was a collection of 1,273\narticles that were either labeled as being hyperpartisan or not. This dataset was used in\nthe shared task and divided into two parts: one was used for training and the other one\nwas used for testing the participants' systems. The available training set contains 645\narticles, of which 238 (37%) articles are hyperpartisan and 407 (63%) are not. The second\nexample that we presented in Chapter 1 was a part of one of the hyperpartisan articles in\nthis dataset. We show the complete version of this article in Figure 1. We also outline the\nrelevant statistics of the by-article dataset in Table 2. Furthermore, similar as for the by-\npublisher subset, we present the frequency distribution of the publishers of hyperpartisan\nnews and mainstream news in Figure 6 and 7 respectively. It additionally presents the\nstatistics of the other dataset that we used to train our systems, which is a subset of the\nby-publisher corpus. We describe this dataset in the next section.\nThe remaining 628 articles (50% hyperpartisan and 50% mainstream) from the by-\narticle corpus was used as the o\u000ecial test set for the competition. Hence, the frequency\ndistribution of the labels in test set is equally distributed, in opposition to the training set.\nIn order to evaluate a system on the test set, participants had to use the same procedure\nas for the by-publisher test set. Thus, they had to submit their system through TIRA\nHyperpartisan News Detection 17/80\nTrump Just Woke Up & Viciously Attacked Puerto Ricans On Twitter Like A Cruel\nOld Man\nDonald Trump ran on many braggadocios and largely unrealistic campaign promises. One of\nthose promises was to be the best, the hugest, the most competent infrastructure president the\nUnited States has ever seen. Trump was going to \fx every infrastructure problem in the country\nand Make America Great Again in the process. That is, unless you're a brown American. In\nthat case, you're on your own, even after a massive natural disaster like Hurricane Maria. Puerto\nRico's debt, which the Puerto Rican citizens not in government would have no responsibility\nfor, has nothing to do with using federal emergency disaster funds to save the lives of American\ncitizens there. The infrastructure is certainly a mess at this point after a Category 5 hurricane\nripped through the island, and 84 percent of Puerto Rican people are currently without electricity.\nEmergency e\u000borts after Hurricanes Irma and Harvey reportedly went very well and Trump praised\nhimself as well and even saw his disastrous approval ratings tick up slightly as a result. However,\nthe insu\u000ecient response in Puerto Rico has nothing to do with Trump, in his mind, and can only\nbe blamed on the people there who do not live in a red state and have no electoral college votes to\no\u000ber the new president for 2020. They're on their own. Twitter responded with sheer incredulity\nat Trump's vicious attack on an already su\u000bering people. YouTube\nFigure 1: An example of an article with hyperpartisan content from the by-article training\nset.\nHyperpartisan\nclassAmount Bias class Amount\ntrue24066\n49.65%least11361\n23.44%\nfalse24401\n50.35%left11659\n24.06%\ntotal 48467 left-center12272\n25.32%\nright12742\n26.29%\nright-center433\n0.89%\ntotal 48467\nTable 1: The distribution of hyperpartisan and bias labels our by-publisher subset.\nHyperpartisan News Detection 18/80\nMetric By-publisher subset By-article training\nTotal tokens 29M 412K\nTotal types 14M 180K\nTotal sentences 1.5M 16K\nAvg sentences per doc 29.18 25.35\nAvg words per doc 598.45 638.55\nAvg types per doc 286.69 279.21\nTokens in longest doc 3336 6389\nTypes in longest doc 1274 1338\nTable 2: Corpus statistics of the datasets that we used for classi\fcation purposes.\n(Potthast et al., 2019). Furthermore, to ensure that the classi\fers would not pro\ft from\nover-\ftting to publisher style, the authors constructed the data in such a way that there\nwere no articles from the same publishers in the training and test set.\n3.2 By-publisher Subset\nWe used a subset of the by-publisher corpus that was created by members of the University\nof Groningen. This dataset contains 48467 articles. The documents come from the training\nand validation set. The frequency distribution of the hyperpartisan labels (either true or\nfalse) and bias labels were presented in Table 1. The table reveals that the frequency\nof the hyperpartisan labels is equally distributed (approximately 50-50). In contrast, the\ndistribution of the bias labels is skewed, as only 0.89% of the documents are labeled as\nbeing right-centered on the political spectrum.\nMoreover, the frequency distribution of the Top 20 most frequent publishers of hy-\nperpartisan news and mainstream news are shown in Figure 4 and 5 respectively. Both\n\fgures reveal that for both classes, there is a relatively large amount of articles that come\nfrom one particular publisher. In case of hyperpartisan news, this publisher is foxbussi-\nness (N=10747). For mainstream news it is abqjournal.com (N=7505). Because of this,\nthe frequency distribution follows a long-tail pattern: there are many articles from a few\npublishers.\nNonetheless, it is worthwhile to mention that there are two sources of errors in the\nby-publisher data which had also permeated our subset. First, the presence of articles that\nare not about politics, such as the example in Figure 2. More speci\fcally, this article is\nlabeled as being on the right-side of the political spectrum. The second issue is that the\narticles are automatically annotated. Thus, we assumed that there would be errors in the\ndataset: some articles may have been labeled as being hyperpartisan, whereas they are\nnot.\nHyperpartisan News Detection 19/80\nKISS band members got patriotic during Saturday night`s concert in Louisiana, temporarily\npausing the classic rock show to lead the crowd in the Pledge of Allegiance. After \fnishing up\ntheir signature song, (I Wanna) Rock and Roll All Nite at the Gretna Heritage Festival, guitarist\nPaul Stanley thanked the U.S. military and gave a shout-out to Army Maj. Steve Roberts, who\nwas in attendance, The Times-Picayune reported. It`s always cool to love your country, Mr.\nStanley told the crowd. The concert in Gretna wrapped up the band`s KISSWORLD 2017 Tour\nin North America and Europe, and it wasn`t the \frst time the pledge was made part of the show.\nFigure 2: An example of a hyperpartisan text (right-leaned) that is not about politics.\n3.3 Fake News Dataset\nWe selected the fake news dataset presented in P\u0013 erez-Rosas et al. (2018) to test the perfor-\nmance of our systems in an out-of-domain evaluation set-up. We have described the general\nset-up of their study in the previous chapter (see Section 2.2). The most important reason\nwhy we decided to use this dataset for this purpose is that the authors evaluated their\nclassi\fers across datasets of fake news.\nThe fake news corpus of P\u0013 erez-Rosas et al. (2018) contains two sets: the FakeNewsAMT\nand the CelebrityNews dataset. We present the relevant corpus statistics of these datasets\nin Table 3. The fake news articles of FakeNewsAMT were written by participants in a\ncrowd-sourcing set-up. We explained in the previous chapter how this data was created and\nthat the news articles came from the following domains: sports, business, entertainment,\npolitics, technology, and education.\nMetric FakeNewsAMT CelebrityNews\nTotal tokens 68K 1.2M\nTotal types 45K 29K\nTotal sentences 2540 10K\nAvg sentences per doc 5292 21.60\nAvg words per doc 141 2467.58\nAvg types per doc 93.89 58.88\nTokens in longest doc 3336 77371\nTypes in longest doc 1274 87\nTable 3: Corpus statistics of the datasets of P\u0013 erez-Rosas et al. (2018) that we used for\nout-of-domain evaluation.\nThe FakeNewsAMT dataset contains 240 instances of fake news and 240 of legitimate\nnews. In contrast, the CelebrityNews dataset includes 250 samples of fake news and 250\nsamples of legitimate news. Another di\u000berence is that all articles come from the same\ndomain, namely celebrity news. The articles of this dataset were manually collected by\nthe authors. Thus, in contrast to the FakeNewsAMT dataset, the CelebrityNews dataset\ncontains documents that occur naturally on the internet. All articles are about celebrities\nand other public \fgures. For further illustration, we show an example of a fake news article\nHyperpartisan News Detection 20/80\nabout celebrities from this dataset in Figure 3.\nKanye West Wants to Enter Cosmetics Business Like Kylie Kanye West wants to go\nhead-to-head with his famous sister-in-law ... diving head \frst into the cosmetic biz dominated by\nKylie Jenner. Kanye`s \fled legal docs declaring his intention to produce DONDA brand makeup,\nperfumes, lotions and other cosmetics. Donda, of course, is Kanye`s beloved mom who passed\naway in 2007. He`ll be up against some sti\u000b family competition. Kylie`s cosmetics sell out within\nminutes ... some resell on eBay for 10 times the retail value. Kanye`s application to snag the\nDONDA cosmetics line is currently being processed but our sources say at this time he`s only\n\fled the paperwork in case something develops.He`s made it clear ... Kanye wants to be the new\nMartha Stewart, creating a lifestyle brand that includes credit cards, cars, wallpaper screens,\nfurnishings, video games, amusement parks, hotels, \ftness centers and healthy fast food. So\nwhich one`s gonna give Kim lip?\nFigure 3: An example of a fake news article from the CelebrityNews dataset of P\u0013 erez-Rosas\net al. (2018).\nHyperpartisan News Detection 21/80\nRank publisher freq.\n1 foxbusiness.com 10747\n2 counterpitch.org 4010\n3 truthdig.org 3072\n4 thedailybeast.com 1762\n5 dissentmagazine.org 509\n6 mintpressnews.com 426\n7 dcclotheslines.com 233\n8 therealnews.com 231\n9 fair.org 222\n10 bizpacreview.com 220\n11 washingtontimes.com 199\n12 joemygod.com 198\n13 freebacon.com 195\n14 forwardprogressives.com 185\n15 leftvoice.org 168\nTotal 22096\nTable 4: The frequency distribution of the\nTop 20 most frequent publishers of hyperpar-\ntisan news in the by-publisher subset.Rank publisher freq.\n1 abqjournal.com 7505\n2 nbcnews.com 4868\n3 calwatchdog.com 2955\n4 natmonitor.com 2500\n5 reuters.com 2039\n6 billmoyers.com 1130\n7 ivn.us 941\n8 factcheck.org 733\n9 studionewsnetwork.com 390\n10 reviewjournal.com 282\n11 thetrace.org 206\n12 foreignpolicyjournal.com 151\n13 thewhim.com 142\n14 greensboro.com 59\n15 newsandguts.com 45\nTotal 25069\nTable 5: The frequency distribution of the\nTop 20 most frequent publishers of main-\nstream news in the by-publisher subset.\nRank publisher freq.\n1 thegatewaypundit.com 17\n2 opslens.com 14\n3 realclearpolitics.com 13\n4 nypost.com 10\n5 salon.com 8\n6 express.co.uk 7\n7 opednews.com 7\n8 dcclothesline.com 6\n9 turtleboysports.com 6\n10 pjmedia.com 6\n11 trueactivist.com 5\n12 video.gq.com 4\n13 bearingarms.com 4\n14 breakingisraelnews.com 4\n15 washingtonexaminer.com 4\nTotal 115\nTable 6: The frequency distribution of the\nTop 20 most frequent publishers of hyperpar-\ntisan news in the by-article training set.Rank publisher freq.\n1 cbsnews.com 24\n2 circa.com 21\n3 express.co.uk 13\n4 heavy.com 12\n5 snopes.com 11\n6 nbcnews.com 8\n7 n\r.com 8\n8 insider.foxnews.com 8\n9 nytimes.com 7\n10 businessinsider.com.au 7\n11 king5.au 6\n12 kiro7.com 6\n13 nypost.com 5\n14 kcra.com 5\n15 bradenton.com 4\nTotal 145\nTable 7: The frequency distribution of the\nTop 20 most frequent publishers of main-\nstream news in the by-article training set.\nHyperpartisan News Detection 22/80\n4 Document Representations\nIn this chapter, we describe four di\u000berent techniques that we used to create document rep-\nresentations in our systems: bag-of-words/characters, bag-of-clusters, word embeddings\nand contextual character-based embeddings. We highlight the advantages and disadvan-\ntages of these techniques. We present the resources that we used to create the document\nrepresentations that we used as well.\n4.1 Bag-of-words\nWhen documents are vectorized as bag-of-words, the length of each vector is equal to\nthe vocabulary size of the training data. Each document is then represented as a vector\nof its word frequencies. We illustrate the bag-of-words vectorization procedure for two\ndocuments in Table 8. Note that the illustration is simpli\fed and that it only shows a\nproportion of the document vectors.\nThe bag-of-words approach is established upon the assumption that the relevance be-\ntween documents can be indicated by the frequencies of their words. In text classi\fcation,\nvectorizing documents as bag-of-words is useful when there are words in the corpus that\noccur more frequently in documents from a speci\fc class than in the other class. Thus, in\nour case, vectorizing documents as bag-of-words would be useful when there is a speci\fc\nset of words that occur more frequently in hyperpartisan news than in non-hyperpartisan\nnews.\nThere are several variants of the bag-of-words method. One of them is to use bag-\nof-n-grams, such as word bi-grams or tri-grams. This is a simple technique to represent\ndocuments by counting how often a sequence of words occur in a document. In some cases,\nit might be more e\u000bective to count how often sequences of words occur in a document,\ninstead of single words (i.e., uni-grams). For instance, in the example in Table 8, it might\nbe useful to count how often Donald Trump occurs in a document, instead of Trump . If\nMelanie Trump occurs four times in the next document, then the vectorizer will not add\nfour to the amount of times that it has seen Donald Trump , which would happen if the\nvectorizer would have used word uni-grams.\nAnother popular variant is to use bag-of-character n-grams, which can be used to\ncapture sequences of characters. In a bag-of-characters representation, the word Trump\ncould for instance be represented by 1-to-3-grams as: Tru,rum and ump. In general,\nthere are two advantages of character n-grams. The \frst is that they are able to detect the\nmorphological units of a word. This is speci\fcally interesting for highly in\rected languages,\nbut also for English. The second advantage is closely related to the \frst, which is that bag-\nof-characters are more capable of capturing misspellings and out-of-vocabulary words than\nbag-of-words. For example, suppose that we would train a classi\fer on the two documents\nin Table 8 using character 1-to-4-grams. If the word large would occur in the test set, then\nwe would still be able to use this word in our classi\fcation procedure, as we have seen the\nword largely in the training data.\nHyperpartisan News Detection 23/80\nDocument 1 Document 2\nDonald Trump ran on many\nbraggadocios and largely unre-\nalistic campaign promises . One\nof those promises wastobe\nthe best, thehugest, themost\ncompetent infrastructure presi-\ndent the United States has ever\nseen. Trump was going to\fx\nevery infrastructure problem in\nthecountry and Make America\nGreat Again in theprocess.KISS band members got pa-\ntriotic during Saturday night's\nconcert in Louisiana, temporar-\nily pausing theclassic rock show\ntoleadthecrowd in the Pledge\nof Allegiance. After \fnishing up\ntheir signature song, (I Wanna )\nRock and Roll All Nite at the\nGretna Heritage Festival, gui-\ntarist Paul Stanley thanked the\nU.S. military and gave a shout.?y?y\nthe trump kiss todonald members rock wanna promises ....\nDocument 1 6 2 1 2 1 0 0 0 2\nDocument 2 6 0 0 1 0 1 2 1 0\n...\nTable 8: A simpli\fed illustration of bag-of-word uni-grams for two documents.\n4.1.1 Features\nIn our systems, we used the following features for bag-of-words/characters:\n\u000fCharacter 1-to-3 grams (uni-grams, bi-grams, tri-grams).\n\u000fCharacter 3-to-5 grams (tri-grams, four-grams, \fve-grams).\n\u000fSu\u000ex of a word (the last 1-to-3 and/or 1-to-4 character n-grams of a word).\n\u000fPre\fx of a word (the \frst 1-to-3 and/or 1-to-4 character n-gram of a word).\n\u000fWord uni-grams.\nFor the pre\fxes and su\u000exes, we tried several set-ups:\n\u000fPre\fxes with 1-to-1, 1-to-2, 1-to-3 and/or 1-to-4 characters.\n\u000fTri-gram pre\fxes (f.i., Tru, derived from Trump ).\n\u000fFour-gram pre\fxes (f.i., Trum , derived from Trump .\n\u000fBoth 1-to-3-gram pre\fxes and 1-to-4 pre\fxes. The former would be T,Tr,Truand\nthe latter T,Tr,Tru,Trum for the word Trump .\nHyperpartisan News Detection 24/80\nIn order to vectorize the documents, we applied Term Frequency{inverse Document\nFrequency (tf-idf) (Jones, 1972) weighting on the word frequencies in the documents. This\nis a popular alternative to using the normal counts of the documents, because it allows\nto give a larger weight to rare words and to e\u000bectively ignore common words (i.e., stop\nwords and function words). We used the tf-idf vectorizer in scikit-learn2and tuned the\nparameters of the vectorizer with a Grid-search. We describe this procedure in more detail\nin Chapter 5.\n4.2 Bag-of-clusters\nThe second technique has been explained in the work of Kim et al. (2017) and Turian\net al. (2010), where it is presented as an alternative vectorization method to bag-of-words.\nThe authors referred to this technique as bag-of-concepts, but we will refer to it as bag-\nof-clusters. This term seemed more suitable to us because it is immediately clear that the\nmethod is about groups of words (`clusters').\nWe illustrate the procedure to vectorize documents as bag-of-clusters in Figure 4. The\n\frst step is to take a word embedding model (described in Section 4.3) trained on large\ncorpus of texts, as showed in the left graph of the \fgure. The next step is to apply a\nclustering technique, such as k-means, on top of these word embeddings. The result of this\nprocedure is shown on the right of the \fgure, which shows groups or 'clusters' of words\nthat are related to each other. For instance, one of the clusters contains the words german,\nenglish, dutch, spanish and french . These words are related to each other because they refer\nto natural languages. Clusters such as this one can then be used to vectorize documents\nby counting for each cluster how many words of that cluster occur in a document. This is\nillustrated in the table in Figure 4.\nThere are several strong points of the bag-of-clusters method compared to the previous\nrepresentation technique that we presented. One is that the procedure yields denser vectors,\nsince the length of each vector will be determined by the amount of clusters that we use.\nEspecially when large collections of documents are used, this will result to less sparse\nvectors than when using bag-of-words, where the length of the vector is equal to the\nvocabulary size. The second advantage is that the representations encode semantic features\nof a document, since we are grouping words by their semantic similarity/relatedness while\npreserving the same level of interpretability of the bag-of-words method. The third bene\ft\nis that this method is more robust towards unseen words than bag-of-words. There are\ntwo reasons for this. First, the clusters are generated on word embeddings in vector\nspaces which are created with large corpora. Therefore, it is likely that there are words\nin the clusters that do not occur in the training data. This tackles the problem of out-of-\nvocabulary words in the test data. Secondly, since we work with clusters, words that occur\nin the same cluster will be treated the same.\nIn addition to these advantages, there was another reason why we decided to use bag-of-\n2https://scikit-learn.org/stable/modules/generated/sklearn.feature_\nextraction.text.TfidfVectorizer.html\nHyperpartisan News Detection 25/80\nspaingermany\nuk\nbelgiumnetherlandsspanishgerman\nenglish\nfrenchdutchbrown\nblackblue\nwhiteyellow\nspaingermany\nuk\nbelgiumnetherlandsgerman\nenglish\nfrenchdutchbrown\nblackblue\nwhiteyellow\nspanish1. Train word vectors from collection of documents 2. Apply cluster algorithm to create clusters\nCluster 1Cluster 2\nCluster 3\n3. Vectorize documents as frequencies of\neach cluster\nCluster 1 Cluster 2 Cluster 3 Cluster 4 ...\nDocument 1 18 1 2 4\nDocument 2 0 5 6 3\n...\nFigure 4: An intuitive illustration of bag-of-clusters. The left graph represents a simpli\fed\nword embedding model. The graph on the right shows the result of applying a cluster algorithm\non the word embeddings. The table presents a \fctive example of two documents vectorized as\nbag-of-clusters.\nclusters as one of our document representation techniques. Namely, it seemed reasonable\nwhy vectorizing documents as bag-of-clusters could be e\u000bective for hyperpartisan news\ndetection. The idea was that if there are clusters (i.e., groups of words) that occur more\nfrequently in hyperpartisan than in mainstream news, then it should be possible to train an\naccurate classi\fer on bag-of-clusters. Besides, a similar e\u000bect has been noted in previous\nexperiments with bag-of-clusters in other tasks, such as in opinion mining (Agerri and\nRigau, 2018). Thus, bag-of-clusters seemed to be an interesting alternative vectorization\nprocedure to bag-of-words.\n4.2.1 Data and Methods\nWe present an overview of the three types of clusters that we used in Table 9. The clusters\nwere trained on either one of the following datasets: the Wikipedia dataset using Wikipedia\n(20121208), the fake news dataset from Kaggle3or the by-publisher data from the shared\ntask (both the training and validation set, see Chapter 3). We decided to work with these\nclusters because of their domain di\u000berences. The clusters of the by-publisher data are the\nmost related to our data, whereas the remaining clusters are less related. Besides, the\n3Dataset available on https://www.kaggle.com/c/fake-news .\nHyperpartisan News Detection 26/80\nWikipedia clusters contain the most words, even more words than the clusters that we\ntrained on the by-publisher data. By comparing the performance of these three clusters,\nwe were able to \fnd out whether it would be possible to use bag-of-clusters when the\ndata is too small to train word embeddings and clusters. This could happen when there is\nclassi\fcation problem of which there is only a small amount of data available for training\nand testing. With this set-up, we were also able to investigate the importance of the\nassociation between the task and the corpus of the clusters. In other words, we could \fnd\nout what would be the most important component to e\u000bectively use bag-of-clusters: the\ndomain that it used or the size of the corpus that we used to obtain clusters.\nCorpusCorpus\nsizeAmount of\nclustersWord embed.\ndimensionsWords\nBy-publisher training\n+ validation498MMin: 100\nMax: 80050, 300 661501\nWikipedia (20141208) 1700MMin: 100\nMax: 50050,100,150,200 2053053\nFake news (Kaggle) 23MMin: 100\nMax: 80050,300 68455\nTable 9: A description of the clusters that we used to represent documents as bag-of-\nclusters. All clusters were applied on word embeddings trained with Word2vec. The\namount of words represent the number of words that occur in each cluster \fle in either the\nwikipedia, hyperpartisan or fake news cluster \fles.\nAs mentioned in Table 9, all word embeddings that we used to create clusters were\ntrained with Word2vec (Mikolov et al., 2013; Mikolov, 2013). We explain this technique\nlater in this chapter (see Section 4.3.2). For now, it is su\u000ecient to know that the dimensions\nthat we presented in Table 9 refer to the dimensions of the word embeddings trained with\nWord2vec (using the Skip-gram algorithm). Furthermore, the clusters were created with\nk-means (MacQueen, 1967). This algorithm can be used to cluster data into knumber of\ngroups or clusters. The value for kis de\fned before obtaining the clusters (i.e., apriori ).\nWe worked with 16 \fles for the clusters that we trained on the by-publisher data. Half\nof these \fles contained the clusters that were generated on the word embeddings with\n50 as their dimension. The other half were generated on the word embeddings that had\na dimension of 300. We had 8 \fles of each of them because we worked with \fles that\ncontained 100, 200, 300, 400, 500, 600, 700 or 800 clusters. Each \fle contained 661501\nwords. The fake news clusters were divided in a similar manner, but contained 68455\nwords per \fle. The reason is the lower size of the fake news dataset. The Wikipedia\nclusters however were organized in a di\u000berent way than its counterparts. We worked with\nsix \fles of which each contained 400 clusters. Half of these clusters were obtained on word\nembeddings trained on a window of 10 tokens. The dimension of the embeddings of each\n\fle was: 100, 150 and 200. The remaining three \fles were obtained on word embeddings\ntrained with a window of 5 tokens. The dimension of the word embeddings of the clusters\nHyperpartisan News Detection 27/80\nof each \fle was: 50, 100 and 150. In addition, we also worked with one \fle that contained\n500 clusters, trained on embeddings with a dimension of 50 and a window size of 5. Each\n\fle of the Wikipedia clusters contained 2053053 words.\nFinally, the clusters that we used in this project (see Table 9) were already available on\nthe server of the university of the Basque Country. Using these \fles, we obtained the word\nrepresentation for the current word win document dby using a simple look-up system: we\nchecked for the current lowercased word wif the word occurred in the clusters. If this was\nthe case, then we took the cluster number as a feature (i.e., used in the header of the table\nin Figure 4). When there was no match, then we did not store any value for the word in\nthe document-term matrix. In this way, we could encode for each document how often a\ncluster occurred in a document. After obtaining the counts, we applied tf-idf weighting to\nobtain a \fnal vector representation of the documents. Thus, we used the same procedure\nas for the bag-of-words (described in Section 4.1).\n4.3 Word Embeddings\nWord embeddings are word representations of k-dimensional vectors of real numbers. The\ncurrent techniques to create those word embeddings, such as Word2vec and GloVe, allow\nto encode the meaning of the words. When these word embeddings are used to vectorize\ndocuments, we are able to capture the meaning of the words. This has powerful bene\fts\ncompared to bag-of-words, where there is no notion of the meaning of words and their\nsemantic relatedness.\nThe rest of this section consists of two parts. In Section 4.3.1, we present the materials\nthat we used to work with word embeddings in our experiments. In the other sections,\nwe brie\ry explain the techniques which were used to create the word embeddings that we\nuse in our study: Word2vec (in Section 4.3.2), GloVe (in Section 4.3.3) and FastText (in\nSection 4.3.4).\n4.3.1 Materials\nWe present a complete overview of the word embeddings that we used in Table 10. We\nworked with two types of word embeddings. The \frst were pre-trained word embeddings,\nwhich were provided on the web and trained by other researchers. These word embeddings\nare described in the \frst \fve rows in Table 10. They were trained with Word2vec, FastText\nor GloVe. The second type of word embeddings that we used were the ones that we\ntrained. These word embeddings are described in the last row of the table. We trained\nthese word embeddings on the complete by-publisher training set (see Chapter 3) using\nFastText. We chose to train them with FastText because we obtained the highest results\nwith this technique when we were using the pre-trained word embeddings (see Chapter\n6). In particular, we trained our word embeddings with the same settings as the FastText\nword embeddings trained on Common Crawl. Therefore, we trained our word embeddings\nwith CBOW. This could easily be done with the FastText package4.\n4https://fasttext.cc/docs/en/unsupervised-tutorial.html\nHyperpartisan News Detection 28/80\nMethod Training data Words Vocabulary size\nWord2vec Google News 100B 3M\nFastTextWikipedia (2017)\nstamt.org news dataset\nUMBC webbase corpus1M\n16B1M\nFastText Common Crawl 600B 2M\nGloVeWikipedia (2014)\nGigaword 5th edition6B 400.000\nFastText By-publisher training 374M 761.343\nTable 10: An overview of the word embeddings that we used. All word vectors had a\ndimension of 300. We show the size in million or billion words.\nWe applied a simple procedure to use the word embeddings to create document rep-\nresentations. The \frst step involved obtaining a vector representation for each word w\nin document D. Then, we obtained a vector for the whole document by averaging the\nvectors of the words in a document (Kim et al., 2017; Xing et al., 2014). We repeated this\nprocedure twice: once to experiment with stop words removal and once to experiment with\nlower casing the words.\nThere are two important advantages of using word embeddings to build document rep-\nresentations. The \frst is speci\fcally related to the way how we create the document repre-\nsentations from the word embeddings. Since we obtain the document vector by averaging\nall the word vectors, we are able to obtain a dense vector representation. In particular, it is\nthe most dense representation compared to the bag-of-words and bag-of-pre\fxes. Another\nadvantage of using word embeddings is that it is more robust towards unseen words in the\ntest set than for instance bag-of-words. Similar to the bag-of-clusters method, we relied on\nadditional resources to obtain a vector representation. These resources were created from\nlarge corpora.\n4.3.2 Word2vec\nWord2vec (Mikolov et al., 2013; Mikolov, 2013) is a collection of tools that can be used\nto compute continuous vector representations of words from large datasets (\\word embed-\ndings\"). The most important idea of Word2vec is that word vectors that share common\ncontexts in the training data are located in close proximity to one another. In other terms,\nwords that are surrounded by the same words have the same meaning, and should there-\nfore be positioned closely together. Thus, the underlying principle of word embeddings\nthat are generated by Word2vec is based on distributional semantics, which implies that\nthe meaning of words can be derived from the context in which they appear in (where\nthe context is de\fned as their surrounding words) and that words with a similar meaning\noccur in a similar context. Hence, words with similar contexts will have similar numerical\nrepresentations.\nIn Word2vec, the distributed representations of words are learned by a two-layer feed-\nHyperpartisan News Detection 29/80\nforward neural network. The input of this neural network is a preferably large text corpus\nand the output is a set of vectors for words. To create the word embeddings, the network\nperforms two steps. First, it is trained to predict words/a word for a certain task. There\nare two kinds of task that can be used for this purpose. In the \frst, the task is to train the\nneural network to predict the current word based on its context (i.e., the words surrounding\nthe current word). The algorithm that uses this task is called continuous bag-of-words\n(CBOW). The other task is similar, but instead of predicting the current word based on\nthe context, it tries to predict the surrounding words (i.e., the context) given the current\nword. This variant is called skip-gram. Once either of the tasks is completed, the weight\nmatrix that was learned in the hidden layer is used to form the word embedding: each row\nfor a word will be the word embedding of that given word.\nThere are two important parameters that the user needs to set when training word\nembeddings: the window size and the dimensions of the word embeddings. The former\nrefers to the amount of words that would involve the context in the algorithms. Usually,\nthis value is set to \fve, as based on a rule of thumb. Furthermore, the dimensions of the\nword embeddings is about the size of the embeddings. In general, the larger the vectors,\nthe more information can be encoded. The most commonly used value is 300 (Yin and\nShen, 2018).\n4.3.3 GloVe\nBoth the CBOW and the skip-gram architecture of Word2vec have shown promising per-\nformances in NLP tasks. However, one of the weaknesses of Word2vec is that it does not\ne\u000bectively make use of global statistics, since its algorithms are trained to predict local\ncontexts. One technique that does take advantage of global count statistics is GloVe (Pen-\nnington et al., 2014), which stands for Global Vectors for Word Representation. GloVe is a\ncount-based model that learns the representation of a word by constructing a co-occurrence\nmatrix which counts how frequently a word appears in a certain context in the corpus. This\nidea is based on techniques that use matrices with global count information to vectorize\ndocuments (i.e., global vectorization methods), as in Latent Semantic Analysis (Landauer\net al., 1998). Here, the main idea is that words have `a similar' meaning when they oc-\ncur in the same contexts. This means that the ratios of the co-occurrence probabilities\nshould be approximately the same. For instance, we would expect words related to water\nto appear equally in the context of ice and scream, and thus they would have a similar\nco-occurrence ratio. To account for co-occurrences that occur infrequent and tend to be\nnoisy and unreliable, the authors apply a weight function to weight the occurrences. Then,\nthe result can be used as a vector representation.\n4.3.4 FastText\nFastText (Bojanowski et al., 2017; Joulin et al., 2016) is another technique to create word\nembeddings. Word embeddings that are trained with FastText are robuster towards out-\nof-vocabulary words than Word2vec and GloVe, as words are represented as character\nHyperpartisan News Detection 30/80\nn-grams. In particular, words are portrayed as a bag of character n-grams, where the\noverall word embedding is the sum of these character n-grams. For example, for the\nword where and n=3, the representation consists of the following: <wh, whe, her, ere,\nre>. The symbols <and>are boundary symbols, thus indicating the beginning and the\nend of words. This allows to distinguish pre\fxes and su\u000exes from the other character\nn-grams. Moreover, the word itself, where is also used. Hence, in addition to character\nn-grams, FastText also learns a representation for the corresponding word. Then, to build\na representation, FastText uses the same skip-gram algorithm with negative sampling from\nMikolov (2013). The only di\u000berence is the method that is used to compute the similarity\nbetween the target word and the context word within the context. Instead of using the dot\nproduct between them, the dot product between two words represented as sums of n-grams\nis used.\n4.4 Contextual Character-based Embeddings\nOne fundamental problem of the models that we presented to train word embeddings is that\nthey generate the same embedding for the same word in di\u000berent contexts. For instance,\nthe word bank will have one vector, despite the di\u000berent meanings of the word. The\nword bank can namely be used to indicate a \fnancial institution or refer to a river bank .\nNonetheless, this issue has recently been addressed by researchers that aimed to develop\nso-called contextual word embeddings (Peters et al., 2018). In our study, we work with\nanother interesting type of embeddings that were developed to tackle this issue, namely\nFlair embeddings (Akbik et al., 2018). We brie\ry describe this technique in Section 4.4.1\nand present the methods that we used in Section 4.4.2.\n4.4.1 Flair Embeddings\nThe authors of Flair (Akbik et al., 2018) propose a method of which the end product is\na collection of contextualized character-based embeddings. The most important aspect of\ntheir approach is that it models words and their context as sequences of characters. Thus,\nwords are represented as sequences of characters in context (the characters of the word plus\nthe surrounding words). In order to do this, characters are used as the atomic units of a\nlanguage model. This is achieved by passing a sequence of characters into a Long short-term\nmemory (LSTM) (Hochreiter and Schmidhuber, 1997), that predicts the next character for\neach character in the sequence. Akbik et al. (2018) use a forward and a backward language\nmodel. In the \frst, states of the LSTM aims to capture the probability distribution of\nthe characters given the previous characters. In the latter, the predictions are computed\nthe other way around: the LSTM learns how to predict the upcoming characters given the\nprevious characters. Then, the authors concatenate the following properties:\n\u000f\\From the forward LSTM, we extract the output hidden state after the last character\nin the word. Since the LSTM is trained to predict likely continuations of the sentence\nafter this character, the hidden state encodes semantic-syntactic information of the\nsentence up to this point, including the word itself\" (Akbik et al., 2018, p.4).\nHyperpartisan News Detection 31/80\n\u000f\\From the backward LSTM, we extract the output hidden state before the word's\n\frst character from the backward LSTM to capture semantic-syntactic information\nfrom the end of the sentence to this character\" (Akbik et al., 2018, p.4).\nThe output states are subsequently concatenated to form the \fnal embedding that captures\nthe semantic information of a word and its surrounding context.\nFigure 5: The procedure of creating the contextual string embedding for the word Wash-\nington from Akbik et al. (2018, p.4).\nWe show an example of how the embedding for the word Washington is build in Figure\n5. The sentence is passed as a sequence of characters in a character-level language model\nto form embeddings. The forward LSTM is depicted by the arrow that points to the right.\nThe output hidden state is extracted after the last character of the word. The backward\nLSTM is presented by the arrow that points to the left. Here, the authors extract the\noutput hidden states before the \frst character. Then, the output states are concatenated.\n4.4.2 Methods\nWe worked with pre-trained Flair embeddings in this project. In order to use these embed-\ndings, we loaded them through the package Flair (Akbik et al., 2018). Flair is a recently\ndeveloped toolkit that can be used to conduct several NLP tasks (e.g., text classi\fcation,\nsequence labeling) with state-of-the-art results. One interesting aspect is that Flair has a\nsimple interface to load and combine static word embeddings (e.g., Word2vec, GloVe and\nFastText) and contextualized embeddings.\nAccording to Akbik et al. (2018), Flair embeddings work the best when they are used\nin combination with other word embeddings, to which they can be concatenated. We\ntherefore decided to use the best pre-trained word embedding model (either trained on\nWord2vec, GloVe or FastText) in combination with the Flair embeddings. We also decided\nto combine the Flair embeddings with the character embeddings from Lample et al. (2016).\nThese character embeddings are similar to Flair embeddings, since they use characters to\nform representations of sequences. The main di\u000berence is that the character embeddings\nfrom Lample et al. (2016) are not pre-trained such as Flair, but that they are trained on\nHyperpartisan News Detection 32/80\ndata of the current task. Therefore, they are able to capture task-speci\fc features and can\ne\u000bectively complement the Flair embeddings. Besides, the developers of Flair, Akbik et al.\n(2018), conducted experiments with these character embeddings and Flair embeddings.\nThey obtained better results with the systems that used Flair and character embeddings\nthan the systems that only used Flair embeddings.\n4.5 Summary\nIn this chapter, we presented several techniques that we used to vectorize documents for\nhyperpartisan news detection: bag-of-words/characters, bag-of-clusters, word embeddings\nand contextual character-based embeddings. We also showed the features and materials\nthat we employed to create these representations. In the next chapter, we explain how we\nused the presented document representation techniques for hyperpartisan news detection\nand how we conducted feature selection to make e\u000ecient document representations.\nHyperpartisan News Detection 33/80\n5 Experimental Set-up\nIn this chapter, we describe how we employed the document representation techniques\nthat we presented in the previous chapter to train classi\fers for hyperpartisan news detec-\ntion. We show the two supervised classi\fcation techniques and describe the general set-up\nthat we used in this study. The latter implies that we explain how we selected the best\nclassi\fers and how we continued to evaluate them in other set-ups. We also outline the\nadditional/local features that we used, which we selected based on the work presented in\nChapter 2.\n5.1 General Set-up\nThe aim of the current work is to \fnd the best classi\fer that distinguishes between hyper-\npartisan and mainstream news using the datasets described in Chapter 3. We de\fne our\nbest model to be the system that has the highest accuracy score on the by-article test set,\nfollowing the o\u000ecial shared task. In addition, we worked with two supervised classi\fcation\napproaches: Support Vector Machines (SVM) and Recurrent Neural Networks (RNN). We\ndiscuss each technique in Section 5.3 and 5.5 respectively. In the rest of the current section,\nwe outline how we evaluated the classi\fers within the two approaches.\nWe selected the best SVM classi\fers via 5-fold cross-validation (see Section 6). We\ntrained our classi\fers twice: once on the by-article training set and once on the by-publisher\nsubset. Then, we evaluated the best classi\fers of each dataset through two set-ups: in-\ndomain and across-datasets. The in-domain evaluation procedure implied that we tested\nthe classi\fer on a test set derived from the same corpus. Thus, the classi\fers that we trained\non the by-article training set were evaluated on the by-article test set, while the systems\ntrained on the by-publisher subset were evaluated on the by-publisher test set (see Chapter\n7). In contrast, the evaluation across-datasets implied that we trained our systems on data\nfrom one corpus and tested it on data from the other corpus of the hyperpartisan news\ndatasets. Hence, the classi\fers that we trained on the by-publisher subset were evaluated\non the by-article test set. In turn, the classi\fers that we trained on the by-article training\nset were evaluated on the by-publisher subset (see Chapter 8).\nWe conducted a similar procedure to evaluate the performance of the Recurrent Neural\nNetworks, as we evaluated them in-domain and across-data as well. However, there are\nthree di\u000berences in the experimental set-up. The \frst is that we only trained the classi\fers\non the by-article training set. The second di\u000berence is that we used a di\u000berent procedure\nthan cross-validation to select the best classi\fer, which we further describe in Section\n5.5. Thirdly, we decided to only evaluate the best classi\fer from our model development\nexperiments for further evaluation in-domain and across-datasets. Therefore, we tested\nour best classi\fer in-domain by evaluating it on the by-article test set and across-datasets\nby testing it on the by-publisher test set.\nFinally, after de\fning the best systems on the by-article test set, we evaluated the\nbest SVM and Recurrent Neural Network classi\fer across-datasets by testing it on the\nby-publisher test set. We also performed an additional evaluation procedure, which we\nHyperpartisan News Detection 34/80\nrefer to as out-of-domain evaluation. In this evaluation procedure, we used the features of\nthe classi\fers with the most promising results according to our observations on the fake\nnews dataset of P\u0013 erez-Rosas et al. (2018). We further describe this experiment in Chapter\n8.\n5.2 Baseline and Evaluation Metrics\nAs mentioned previously, we evaluated our SVM classi\fers through 5-fold cross-validation.\nFor the in-domain evaluation set-up and the evaluation across-datasets, we additionally\nused the precision, recall and F1-score as shown below. It is important to bear in mind\nthat we work with a binary classi\fcation task in which we either predict hyperpartisan\nnews or non-hyperpartisan news.\naccuracy =TP+TN\nTP+TN+FP+FN(1)\nprecision =TP\nTP + FP(2)\nrecall =TP\nTP+FN(3)\nF1 =2\u0001precision \u0001recall\nprecision +recall(4)\nAs a baseline, we used the classi\fer submitted to the o\u000ecial shared task before starting\nthis project. This classi\fer was trained on the complete by-publisher training data and\nonly used the negative sentiment from VADER (Hutto and Gilbert, 2014). We describe\nthis type of feature later in Section 5.4. We show the scores of this baseline system on\nthe by-publisher and the by-article set in Table 11. It obtained the 30th place out of 42\nparticipants on the by-article test set and the 20th position out of 28 submissions on the\nby-publisher test set. We mainly decided to use this system as our baseline because of\nits simplicity and the relatively low positions that it obtained on both test sets in the\ncompetition.\nby-article test set by-publisher test set\nAccuracy 0.621 0.589\nPrecision 0.582 0.575\nRecall 0.860 0.681\nF1-score 0.694 0.623\nTable 11: The performance of the baseline system that only uses the negative sentiment\nof a text as features. The system is trained on the complete by-publisher training set.\nHyperpartisan News Detection 35/80\n5.3 Approach 1: Classi\fcation with a SVM\nA Support Vector Machine (SVM) is a popular binary classi\fcation technique. The algo-\nrithm has been successfully employed in several text classi\fcation tasks, such as sentiment\nanalysis (Jahdav and Vaghela, 2016; Mullen and Collier, 2004; Zainuddin and Selamat,\n2014), stance detection (K\u007f u\u0018 c\u007f uk and Can, 2018), opinion mining (Sabuj et al., 2017), au-\nthorship attribution (Diederich et al., 2003) and named entity recognition (Ekbal and\nBandyopadhyay, 2010). In Chapter 2, we have seen that they have also proved to be useful\nfor fake news classi\fcation. Because of this, we decided to use this classi\fcation technique\nas one of the two main approaches in this thesis.\nA SVM (Vapnik, 1995) performs classi\fcation by creating a line or hyperplane that\nseparates the data into two classes. This is done by trying to \fnd the best separating\nhyperplane such that the distances between the data points of the two classes is maximized.\nThe best separating hyperplane is determined by support vectors, which are the critical\nelements of the training set. The C-parameter of the SVM constructs this hyperplane. In\nparticular, the C-parameter of the SVM classi\fer determines how much this hyperplane is\nallowed to make misclassi\fcations: the higher the value of the C-parameter, the more the\noptimization procedure tries to create a hard margin that does not make any errors. The\nlower the C-value, the more the SVM attempts to create a wide margin, and the less strict\nit is towards making errors. Thus, the C-parameter can a\u000bect the ability of the classi\fer\nto generalize on unseen data, and its robustness.\nWe use the LinearSVC implementation of Pedregosa et al. (2011) in scikit-learn5.\nHence, we worked with Linear SVMs, which means that the hyperplane separates the data\nof the two classes in a linear manner. We decided to work with this type of SVM because\nthey were also used in previous studies (see Chapter 2). We conducted a few experiments\nwith non-linear SVMS using rbf-kernels. However, the results were not signi\fcantly higher\nthan the ones obtained with linear SVMs. Besides, training SVMs with rbf-kernels is more\ntime consuming than training linear SVMs.\n5.3.1 Feature Selection for Document Representations\nWe tested three of our document representation techniques in SVMs: bag-of-words, bag-\nof-clusters and word embeddings. In particular, we started testing each technique in a\nSVM without using additional features. In these experiments, we conducted 5-fold cross-\nvalidation to select the best features and/or corpora to work with:\n\u000fBag-of-words using the features described in Section 4.1, such as characters, pre\fxes\nand words.\n\u000fBag-of-clusters, using the clusters described in Section 4.2.\n\u000fWord embeddings, using the materials described in Section 4.3.\n5https://scikit-learn.org/stable/index.html\nHyperpartisan News Detection 36/80\nWe trained classi\fers on the by-article training data and on the by-publisher subset respec-\ntively. Afterwards, we selected the best bag-of-words, bag-of-clusters and word embeddings\nclassi\fer of each dataset for in-domain evaluation and evaluation across-datasets. Once we\nfound the best bag-of-words, bag-of-clusters and word embedding classi\fers on the two\ndatasets via 5-fold cross-validation, we tried to improve the accuracy of these models by\nadding the features that we present in Section 5.4 and/or by combining document repre-\nsentations.\n5.3.2 Pre-processing and Parameter Tuning\nPre-processing The by-article training set was provided in XML-\fles of which we ex-\ntracted the body of the documents and their corresponding labels. We did not further\nclean the documents, as the documents were reasonably clean. The data of the by-publisher\nsubset already contained the extracted documents from the XML-\fles. Similar as to the\nby-article training set, we used the body of the documents. As there was still a substantial\namount of noise in the documents of the by-publisher subset, we performed a few clean-\ning procedures to remove the remaining HTML tags in the documents. We additionally\nremoved random occurrences of question marks in the body of the documents.\nParameter Tuning We tuned two types of parameters: the C-parameter of the SVM\nclassi\fer and the parameters of the tf-idf vectorizer. We performed parameter tuning with a\ngrid-search6using the values described in Table 12. Furthermore, we tuned the parameters\nof the following systems on the by-publisher subset and the by-article training set:\n\u000fThe best bag-of-words classi\fer.\n\u000fThe best bag-of-clusters classi\fer.\n\u000fThe best classi\fer using linguistic features.\nThese classi\fers were the systems with the highest accuracy on 5-fold cross-validation, in\nwhich they were using the default settings of the SVM and the tf-idf vectorizer.\nNote that the linguistic features belong to the additional features and that we will\ndescribe them in Section 5.4. Since the linguistic features are represented by n-grams and\ntf-idf weighting, we decided to tune the parameters of these features as well. As a point\nof departure, we experimented with uni-grams, bi-grams and tri-grams via 5-fold cross-\nvalidation on both datasets. Once we found the best value for nvia 5-fold cross-validation\nfor the by-publisher subset and the by-article set, we used a grid-search to tune the tf-idf\nvectorizer parameters and the C-parameter to optimize the performance. The values that\nwe used to perform the grid-search were the same as presented in Table 12.\n6https://scikit-learn.org/stable/modules/generated/sklearn.model_\nselection.GridSearchCV.html\nHyperpartisan News Detection 37/80\nName Values Description\nlowercase True, False Lowercases the words\nmindf 1,2,3 Excludes terms that appear in fewer than n documents\nsublinear tfTrue, False Replaces term frequency with 1 + log(tf)\nuseidf True, False Enables inverse-document-frequency reweighting\nC 0.01, 1.0, 100 The C-parameter of the SVM\nTable 12: The values over which we performed the grid-search to tune the parameters of\nthe tf-idf vectorizer and the C-parameter of the SVM.\n5.4 Additional Features for the SVMs\nAfter obtaining the best SVM classi\fers using either of bag-of-words, bag-of-clusters or\nword embeddings, we tried to increase the performance of the SVMs by adding of local\nfeatures and/or by combining the most e\u000bective document representations. In this section,\nwe describe the local features that we used for this purpose. We selected a collection\nof features that were mentioned to be e\u000bective for related tasks to hyperpartisan news\ndetection presented in Chapter 2. The features that we continued to experiment with in\nour study can be divided into three distinct groups: sentiment features (see Section 5.4.2),\nlinguistic features (see Section 5.4.2) and stylistic features (see Section 5.4.3).\n5.4.1 Sentiment Features\nIn order to extract the sentiment of each document, we worked with dictionary-based\nfeatures and automatic approaches. The latter implied that we used NLTK , in particular\nthe VADER sentiment analysis tool (Hutto and Gilbert, 2014) to compute the sentiment\nscore of each document7. Valence Aware Dictionary for Sentiment Reasoning (VADER) is\na simple rule-based model for sentiment analysis that represents the intensity the sentiment\nof a document by calculating a negative, positive and neutral sentiment and a compound\nscore. The tool was constructed by using a combination of qualitative and quantitative\nmethods that led to an empirically validated gold-standard list of lexical features along\nwith their sentiment intensity measures. Then, these features were used in \fve di\u000berent\nrules that consider grammatical and syntactic aspects to further determine the intensity\nof a sentiment.\nWe conducted experiments with the positive, negative and compound score. We sur-\nmised that mainly the positive and negative score could help to indicate bias in the texts.\nIn addition, the compound score seemed to be useful because it allows to compute the sen-\ntiment of a text in a single one-dimensional metric and encodes both the neutral, positive\nand negative scores. The latter was computed by summing up the valence scores of each\nword in the lexicon. Then, the result was adjusted according to the rules and normalized\nto be between -1 (most extreme negative) and 1 (most extreme positive). The positive and\nnegative scores were expressed in percentages. Thus, with the neutral sentiment, the sum\n7https://www.nltk.org/_modules/nltk/sentiment/vader.html\nHyperpartisan News Detection 38/80\nof the positive, negative and neutral sentiment should sum up to 1. We experimented with\nthe following set-ups using:\n\u000fonly the negative sentiment\n\u000fonly the positive sentiment\n\u000fonly the compound score\n\u000fboth positive and negative score\n\u000fthe positive score added to the negative score.\nIn addition to VADER, which proved to be useful for bias detection in news reports\n(Hutto and Gilbert, 2014), we also tried to capture the sentiment of texts by using lexicons\nwith positive and negative words. More speci\fcally, we worked with the lexicon of positive\nand negative words from Liu et al. (2005). We used this lexicon because it was also used\nin previous studies, such as in the work of Recasens et al. (2013).\nWe worked with two methods to use the sentiment of the texts as features. In the \frst,\nwe followed the binary representation method described in Recasens et al. (2013). This\nimplied that if a word from a document occurred in the list of positive words, then the\nvalue for positive sentiment would be set to `true', and the same would be done for the\nnegative words. The second type of feature representation involved counting the amount\nof positive or negative words. In particular, we represented the intensity of the negative\nsentiment by using the total number of negative words occurring in the document from the\nlexicon. We repeated the same procedure to calculate the negative sentiment by counting\nthe total amount of words occurring in the lexicon of negative sentiment words.\n5.4.2 Linguistic Features\nThe linguistic features that we used in this study involved experimenting with Part-of-\nspeech (POS) n-grams. In previous studies, we have seen that tri-grams and uni-grams\nwere particularly useful for this purpose (Horne and Adali, 2017; Recasens et al., 2013).\nThese studies showed that writers can use several linguistic devices to express bias, such as\nthe adjectives great, beautiful, good and intensi\fers such as very. When these devices are\nused to express bias, they speci\fcally occur in combination with nouns denoting names,\norganizations, objects, decisions, etc. On the other hand, in mainstream texts, authors\ncould use speci\fc words to denote objectivity, such as might ,could . Furthermore, one\ncould use speci\fc POS tri-grams to use sources in an article, such as is said to be .\n5.4.3 Stylistic Features\nWe additionally experimented with features that aimed to capture the stylistic elements of\nthe texts. One method how we did this was by looking at the usage of certain words in the\ntexts. In this light, we considered two groups of words: assertive verbs and factive verbs\nHyperpartisan News Detection 39/80\n(Hooper, 1975). Our decision to use them was based on their motivation and presented\nresults of Recasens et al. (2013). We used a list of assertive and factive words that we took\nfrom Wyse (2009), since there was, to the best of our knowledge, no other way to obtain\nthe lexica. The list of assertive verbs included 65 verbs such as claim, predict, believe,\nthink, suppose, expect, decide . The list of factive verbs contained 63 words, such as realize,\nremember, amuse .\nAnother method that we applied to capture stylistic elements was by using readabil-\nity scores. We decided to use these scores because biased texts tend to have an easier\nreadability level than non-biased texts, (Hutto and Gilbert, 2014; Horne and Adali, 2017;\nPotthast et al., 2017). The same was mentioned about the readability level of fake news\ncompared to legitimate news (P\u0013 erez-Rosas et al., 2018). Following these works, we decided\nto experiment with the following readability metrics: Flesh Kinchaid Grade (Kincaid et al.,\n1975), the Automated Readability Index (Smith and Senter, 1967), the SMOG readabil-\nity index (McLaughlin, 1969) and the Gunning Fog Grade Readability Index (Gunning,\n1952). We used the implementations of the package textstat8to compute the readabil-\nity scores. We tested the performance of each metric individually on both the by-article\nand by-publisher subset. We subsequently conducted experiments on both datasets where\nwe used all readability metrics.\n5.5 Approach 2: Classi\fcation with Recurrent Neural Networks\nApart from that neural networks have revolutionized machine learning, there were three\nspeci\fc reasons why we decided to work with them in this project. The \frst was that it\nseemed interesting to test the best word embeddings from the experiments with SVM in a\ndi\u000berent classi\fcation technique. This would enable us to shed more light on the individual\nperformance of the two classi\fcation techniques when using word embeddings. Secondly,\nusing neural networks would allow us to build document representations from embeddings\nwith a more sophisticated method than simply averaging the vectors. The third reason\nwas that we were interested in working with contextual character-based word embeddings,\nin particular the Flair embeddings (see Section 4.4.1). The most e\u000ecient method to work\nwith these Flair embeddings was by using the text classi\fers provided in the Flair package,\nwhich rely on neural networks.\nIn Flair, there are several ways to use word embeddings to build document repre-\nsentations for text classi\fcation, including the method that we used to form document\nembeddings in SVMs (see Section 4.3). However, as mentioned previously, we decided to\nuse a di\u000berent method to create document representations. The procedure that we used\ninvolved the usage of document embeddings through Recurrent Neural Networks. The \frst\nstep was to create document embeddings by using the document embeddings object from\nFlair9using the default settings. Because of this, the gated recurrent unit (GRU) was\nused as the type of the RNN. After obtaining the document embeddings, the next step by\n8https://pypi.org/project/textstat/\n9https://github.com/zalandoresearch/Flair/blob/master/resources/docs/\nTUTORIAL_5_DOCUMENT_EMBEDDINGS.md\nHyperpartisan News Detection 40/80\ndefault was to feed these word embeddings to a linear neural network layer that predicts\nthe class10. We used GPUs to train the models, which were available at the University of\nthe Basque Country.\nFurthermore, training and evaluating a classi\fer with Flair requires the usage of a\ntraining, development and test set. Because of this, we divided the by-article set into a\ntraining set (516 documents, 80%), a development set (65 documents, 10%) and a test set\n(64 documents, 10%). We used the strati\fed sampling technique from scikit-learn11to\nhave an approximately equal amount of instances from the hyperpartisan and mainstream\nclass in the three sets. Furthermore, the performance was evaluated using the precision,\nrecall, and F1-score as presented in Section 5.2. While Flair provides the micro and macro\naccuracy, we decided to use only the micro accuracy because of the imbalanced frequency\ndistribution of the classes in the by-article training set.\nOnce we found the best classi\fer from the development experiments, we trained this\nclassi\fer for another four times. Afterwards, we selected the best model out of these\nruns for further evaluation on the o\u000ecial by-article test set (in-domain evaluation). The\nbest model was determined by the accuracy on the predictions of the test set derived\nfrom the by-article training set. We decided to use this procedure because deep learning\narchitectures tend to produce di\u000berent outcomes for each experiment.\n5.5.1 Feature Selection\nWe trained three classi\fers on each of the following set-ups:\n\u000fThe most e\u000bective word embeddings from our SVM experiments. In our case, these\nwere the FastText word embeddings trained on Common Crawl.\n\u000fTwo character-based contextual Flair embeddings ( flair-news-forward-fast )\nand ( flair-news-backward-fast )12plus the previous features.\n\u000fThe character embeddings from Lample et al. (2016) and the two types of Flair\nembeddings.\nThe \frst set-up allowed us to compare the e\u000bectiveness of the RNN architecture to the\nresults of the SVM classi\fers in which we averaged the word embeddings. The second\nset-up was useful to \fnd out how well the best `static' word embeddings (i.e., trained with\nWord2vec, GloVe or FastText) would perform when they would be combined with the two\nkinds of Flair embeddings, and the other way around. We performed the latter set-up\nbecause of the promising results of the experiments from Akbik et al. (2018).\n10https://github.com/zalandoresearch/Flair/blob/master/Flair/models/text_\nclassification_model.py\n11https://scikit-learn.org/stable/modules/generated/sklearn.model_\nselection.train_test_split.html\n12We had to select the `fast' version of the embeddings due to lack of GPU memory\nHyperpartisan News Detection 41/80\n6 Model Development Results\nIn this chapter, we present the results of the experiments that we conducted to determine\nthe most promising classi\fers for in-domain evaluation and evaluation across-datasets on\nthe test sets of the competition. The purpose of the experiments that we present here was\nto \fnd the following classi\fers:\n1. The best SVM classi\fers trained with any of the three document representations\n(bag-of-words, bag-of-clusters or word embeddings), presented in Section 6.1.\n2. The best SVM classi\fers trained with any of the three document representations that\nwe improved with additional features, described in Section 6.3.\n3. The best classi\fer that relied on contextual character-based embeddings using Re-\ncurrent Neural Networks, described in Section 6.4).\nWe \frst studied the performance of the additional features in isolation (i.e., without\nusing them in combination with document representation techniques), to decide which\nfeatures we should add to the classi\fers in (1). We present these results in Section 6.2.\nWe additionally describe the results of the experiments where we combined document\nrepresentations and additional features in Section 6.3. Note that we only show the most\nrelevant results of our experiments, as the purpose of this chapter is to present the most\ne\u000bective features to the reader.\n6.1 Document Representations in SVMs\nWe report the results of the classi\fers using any of the three document representations\nthat we used in SVM and presented in Chapter 4. We present the performance of these\ndocument representations without using any other features in the classi\fer.\n6.1.1 Bag-of-words\nWe show the results of the models using the bag-of-word features described in Section\n4.1 in Table 13 and 14. The former represents the results on 5-fold cross-validation for\nthe by-article training set. The latter outlines the results for the by-publisher subset.\nWe additionally present the parameter settings of each classi\fer in the Tables 13 and 14.\nWhen we mention in the tables that we used the tuned parameter settings, we mean that\nwe used other settings than the default values of the tf-idf vectorizer and the default value\nof the C-parameter of the SVM. These settings were the result of the grid-search procedure,\ndescribed in Section 5.3.2. Moreover, when we mention that we used the default parameter\nsettings, we mean that the default settings were the outcome of the grid-search.\nOverall, the results in the tables show that we achieved higher accuracy scores on the\nby-publisher subset than on the by-article training set on 5-fold cross-validation. The\nscores of the by-publisher subset varied between 0.8871 and 0.9185, whereas the accuracy\nscores of the by-article training set varied between 0.7629 and 0.7891. The results also\nHyperpartisan News Detection 42/80\nshow that word uni-grams, character 3-to-5 grams and pre\fxes (1-to-3 and 1-to-4) are\npowerful features for hyperpartisan news detection. More speci\fcally, we obtained the\nbest results with bag-of-pre\fxes on the by-article training data ( accuracy=0.7891 ). In\ncontrast, the best performance on the by-publisher subset was received by the classi\fer\nusing word uni-grams ( accuracy=0.9185 ).\nBased on these results, we selected the best classi\fer using bag-of-words/characters\nfeatures of each dataset for in-domain evaluation and evaluation across-datasets on the\ntest sets. Thus, we continued to work with pre\fxes when training models on the by-article\ntraining set. We subsequently experimented with word uni-grams when we trained classi-\n\fers on the by-publisher subset. We also used pre\fxes and word uni-grams in combination\nwith other features in the upcoming cross-validation experiments.\nSystemParameter\nsettingsAccuracy\nPre\fxes (1-to-3 and 1-to-4) Default 0.7891\nWord uni-grams Tuned 0.7845\nCharacter 3-to-5 grams Tuned 0.7815\nPre\fxes (1-to-3) Default 0.7752\nPre\fxes (1-to-4) Default 0.7629\nTable 13: The Top 5 bag-of-word classi\fers trained on the by-article training set and\nevaluated via 5-fold cross-validation.\nSystemParameter\nsettingsAccuracy\nWord uni-grams Tuned 0.9185\nCharacter 3-to-5 grams Tuned 0.9013\nPre\fxes (1-to-3 and 1-to-4) Default 0.9007\nPre\fxes (1-to-4) Default 0.8908\nCharacter 1-to-3 grams Default 0.8871\nTable 14: The Top 5 bag-of-word classi\fers trained on the by-publisher subset and evalu-\nated via 5-fold cross-validation.\n6.1.2 Bag-of-clusters\nIn this section, we present the performance of our SVM classi\fers using the clusters de-\nscribed in Section 4.2, Table 9. We use a speci\fc format in our tables to refer to the clusters.\nIn particular, the \frst component refers to the corpus on which the word embeddings were\ntrained, which is either the Wikipedia dataset using Wikipedia (20121208) ( wiki ), the fake\nnews dataset from Kaggle ( fake ) or the by-publisher training and validation set ( hyp).\nHyperpartisan News Detection 43/80\nThen, the name is followed by an s- number , which refers to the dimension of the word em-\nbeddings on which the clusters were applied. For instance, the name hyp-s50-15.400\ndescribes a set of 400 clusters trained on the complete by-publisher training set, using 50\ndimension word embeddings trained with a 5 token window.\nThe accuracy scores in Tables 15 to 17 reveal that bag-of-clusters is a helpful docu-\nment representation technique for our task. However, their performance on 5-fold cross-\nvalidation is generally lower than the classi\fers working with bag-of-words (see Table 13\nand 14). This is speci\fcally the case for the classi\fers using the by-publisher subset, of\nwhich the scores varied around 0.90. Moreover, the results show that the clusters generated\non hyperpartisan/mainstream news were the most e\u000bective. We obtained the second best\nresults with the fake news clusters and the lowest results with the Wikipedia clusters.\nWhen we selected the best cluster of each table for further parameter tuning, the default\nsettings proved to be the best. Consequently, the hyperpartisan/mainstream news clusters\nwere still the best ones. Because of this, we continued experimenting with these clusters\nin the remaining set-ups: in the experiments that used the best classi\fers from Table 17\nof each dataset combined with additional features, in the in-domain evaluation procedure\nand in the evaluation across-datasets.\nSystem Accuracy\nWiki-s50-w5.400 0.7335\nWiki-s50-w5.500 0.7241\nWiki-s50-w5.400 0.7164\nWiki-s100-w10.400 0.7149\nWiki-s100-w5.400 0.7134\n(a) Results by-article training set.System Accuracy\nWiki-s100-w5.400 0.7745\nWiki-s100-w10.400 0.7613\nWiki-s200-w10.400 0.7600\nWiki-s50-w5.400 0.7502\nWiki-s50-w5.400 0.7498\n(b) Results on by-publisher subset.\nTable 15: The Top 5 results obtained with Wikipedia clusters on 5-fold cross-validation\nusing default parameter settings.\nCluster Accuracy\nFake-s300-w5-700 0.7629\nFake-s50-w5-600 0.7599\nFake-s300-w5-800 0.7551\nFake-s50-w5-700 0.7519\nFake-s300-w5-400 0.7505\n(a) Results on by-article training set.Cluster Accuracy\nFake-s300-w5-700 0.8123\nFake-s300-w5-600 0.8076\nFake-s50-w5-700 0.8018\nFake-s50-w5-800 0.7963\nFake-s50-w5-600 0.7953\n(b) Results on the by-publisher subset.\nTable 16: The Top 5 results obtained with fake/legitimate news clusters on 5-fold cross-\nvalidation using default parameter settings.\nHyperpartisan News Detection 44/80\nCluster Accuracy\nHyp-s300-w5-700 0.7645\nHyp-s50-w5-700 0.7598\nHyp-s300-w5-500 0.7583\nHyp-s50-w5-400 0.7581\nHyp-s50-w5-800 0.7566\n(a) Results on the by-article training set.Cluster Accuracy\nHyp-s300-w5-800 0.8556\nHyp-s300-w5-700 0.8498\nHyp-s300-w5-500 0.8465\nHyp-s300-w5-600 0.8457\nHyp-S50-w5-800 0.8429\n(b) Results on the by-publisher subset.\nTable 17: The Top 5 results obtained with hyperpartisan/non-hyperpartisan clusters on\n5-fold cross-validation using default parameter settings.\n6.1.3 Word Embeddings\nIn this section, we present the results of the SVM classi\fers using the pre-trained word\nembeddings described in Table 10. The results on the by-article training set are described\nin Table 18. The results on the by-publisher subset are presented in Table 19.\nIn general, it can be noted that we received the best scores with pre-trained word\nembeddings from FastText on both datasets. More speci\fcally, the best classi\fer on the\nby-article training set was the one that used FastText word embeddings trained on Common\nCrawl. These word embeddings also reached the highest score on the by-publisher subset.\nStill, the accuracy slightly improved when we removed the stop words of the documents\n(see Table 19).\nAnother observation is that our own trained word embeddings, which we trained on\nthe by-publisher training set, received a fair accuracy score. The classi\fer that used these\nembeddings namely obtained the third best score on the by-publisher subset (see Table\n19). Still, the results of these word embeddings were not among the Top 5 best classi\fers\nthat we trained on the by-article training set.\nIn addition, the results of the di\u000berent pre-processing procedures that we applied show\nthat removing stop words helped to increase the performance for some classi\fers, but not\nfor all. Moreover, lowering the case of the words did not generally improve the results.\nNone of the results of the models that used this pre-processing procedure were among the\nTop 5.\nFor the in-domain evaluation and the evaluation procedure across-datasets, we selected\nthe best classi\fer from Table 18, and the best classi\fer from Table 19. We also used\nthese classi\fers in combination with other features in the remaining experiments on 5-fold\ncross-validation.\n6.1.4 Overview of the Best Classi\fers\nSo far, we received promising results for all three document representation techniques on\n5-fold cross-validation. We selected the classi\fers with the best features of each document\nrepresentation technique as a point of departure for further development and evaluation.\nHyperpartisan News Detection 45/80\nTechnique CorpusPre-processing\nsettingsAccuracy\nFastText Common Crawl None 0.7754\nFastText Common Crawl Without stop words 0.7723\nWord2vec Google News Without stop words 0.7708\nGloVe 840B.300D Without stop words 0.7662\nGloVe 840B.300D None 0.7647\nTable 18: Results for the pre-trained word embeddings for the models that we trained on\nthe by-article training set via 5-fold cross-validation.\nTechnique CorpusPre-processing\nsettingsAccuracy\nFastText Common Crawl Without stop words 0.8125\nFastText Common Crawl None 0.8100\nFastText By-publisher training None 0.8097\nWord2vec Google News Without stop words 0.8038\nFastText Wikipedia None 0.8037\nTable 19: Results for the pre-trained word embeddings for the models that we trained on\nthe by-publisher subset via 5-fold cross-validation.\nIn particular, the results of our experiments led us select the following classi\fers that we\ntrained on the by-article training data:\n\u000fThe best classi\fer using bag-of-words, which used pre\fxes (1-to-3/4) as features.\n\u000fThe best classi\fer using bag-of-clusters which worked with the clusters\nhyp-s300-w5-700 .\n\u000fThe best classi\fer using word embeddings, which worked with pre-trained word em-\nbeddings trained on Common Crawl with FastText ( FT-Crawl ).\nThe highest performance among these classi\fers was received by the classi\fer using\npre\fxes ( accuracy = 0.7891 ). The second best result was from the classi\fer using word\nembeddings, namely FT-Crawl . The lowest performance was obtained by the bag-of-\nclusters classi\fer, which scored an accuracy of 0.7629.\nIn addition, we continued experimenting with the following SVM classi\fers when work-\ning with the by-publisher subset:\n\u000fThe best classi\fer using bag-of-words, which used word uni-grams.\n\u000fThe best classi\fer using bag-of-clusters, which is the classi\fer using the clusters\nhyp-s300-w5-800 .\nHyperpartisan News Detection 46/80\n\u000fThe best classi\fer using word embeddings, which were the pre-trained word embed-\ndings on Common Crawl from FastText, excluding stop words\n(FT-Crawl-excl-stop-words ).\nThe best performance was from the bag-of-words classi\fer, which received an accuracy\nscore of 0.9185 on cross-validation. The second best accuracy score was obtained by the\nclassi\fer using word embedding ( accuracy = 0.8123 ). The classi\fer that used bag-of-\nclusters yielded the lowest performance ( accuracy = 0.8123 ).\nHence, on both datasets, the bag-of-words classi\fers scored the best and bag-of-clusters\nthe lowest when they were evaluated via 5-fold cross-validation. One one hand, this pattern\nseemed to be in contrast to what we expected, since it seemed more likely to use that the\ndense document representations would perform better than the sparse ones. One the other\nhand, we were surmising that the better performance of the sparse systems could be related\nto the fact that the systems were still evaluated on a test set derived from the same corpus\nas the training data. This implied that there was a low amount of out-of-vocabulary words\nin the test sets in the cross-validation procedure. Therefore, we hypothesized that the\nsparse representations would also receive better results than the dense representations in\nthe in-domain evaluation procedure. However, we surmised that we would see the opposite\npattern in the results of the evaluation procedure across-datasets.\n6.2 Additional Features for SVM\nBefore we present the scores of the SVM classi\fers in which we combined the best classi\fers\nfrom the previous section with additional features, we show the results of the experiments\nin which we studied the individual contribution of the additional features. This will be\ndone in the current section.\n6.2.1 Linguistic Features\nThe results of the experiments with linguistic features (i.e., POS tag n-grams) are shown\nin Table 20. The tables reveal that POS n-grams were helpful features for hyperpartisan\nnews detection. In particular, the classi\fers showed a similar behavior as the ones that\nused bag-of-words as features. Moreover, POS uni-grams were the best linguistic features\nto predict hyperpartisan content in the by-article training data, whereas POS bi-grams\nworked better on the by-publisher subset. Because of these promising results, we decided\nto use linguistic features as the major method to improve the accuracy of classi\fers by\nmeans of additional features.\nHyperpartisan News Detection 47/80\nPOS\nsequenceParameter\nsettingsAccuracy\nUni-grams Default 0.7830\nBi-grams Tuned 0.7768\nTri-grams Tuned 0.7349\n(a) Results on the by-article training set.POS\nsequenceParameter\nsettingsAccuracy\nBi-grams Default 0.9188\nTri-grams Tuned 0.9150\nUni-grams Tuned 0.9088\n(b) Results on the by-publisher subset.\nTable 20: The Top 5 results on 5-fold cross-validation of the SVM classifers using POS\nfeatures (linguistic features) with tf-idf weighting.\n6.2.2 Sentiment Features\nAmong the sentiment features, the ones that we obtained with VADER were the most\nuseful. Nonetheless, the results in Table 21 show that sentiment features had minimal\npredictive power when they are used without additional features (i.e., in isolation). How-\never, at this stage of the experimental procedure, this did not rule out the possibility that\nthey could improve the performance of other models. Hence, despite the low predictive\npower of the sentiment features on their own, we continued experimenting with sentiment\nfeatures in the experiments where we trained SVM classi\fers on document representations\nand additional features.\nSystem Accuracy\nNegative sentiment 0.6326\nPositive and\nnegative sentiment0.6326\nPositive plus\nnegative sentiment0.6326\nCompound score 0.6310\nPositive sentiment 0.6348\n(a) results on the by-article training set.System Accuracy\nPositive and\nnegative sentiment0.5769\nPositive sentiment 0.5703\nPositive plus\nnegative sentiment0.5617\nCompound score 0.5268\nNegative sentiment 0.5105\n(b) results on the by-publisher subset.\nTable 21: The Top 5 most predictive sentiment features for the classi\fers that we trained\nand evaluated via 5-fold cross-validation. All features were computed with VADER (Hutto\nand Gilbert, 2014).\n6.2.3 Stylistic Features\nThe stylistic features are more e\u000bective on the by-article data than on the by-publisher sub-\nset (see Table 22). In particular, the average accuracy scores on by-publisher varied around\n0.50, whereas they \ructuated around 0.63 on the by-article training data. Furthermore,\nwe could not increase the predictive power of the readability features by combining the\nHyperpartisan News Detection 48/80\ndi\u000berent metrics. The generally low scores discouraged us to work with stylistic features\nin the classi\fers that would combine the best classi\fers of Section 6.1 and the additional\nfeatures presented in this section.\nSystem Accuracy\nAssertives 0.6310\nSmog 0.6310\nAutomatic readability index 0.6295\nKinchaid grade 0.6279\nAll readability metrics 0.5833\n(a) Results on the by-article training set.System Accuracy\nFlesh Easy 0.5039\nSmog 0.5035\nAssertives 0.5035\nAll readability metrics 0.5015\nKinchaid grade 0.5012\n(b) Results on the by-publisher subset.\nTable 22: The Top 5 most predictive stylistic features for the classi\fers that we trained and\nevaluated via 5-fold cross-validation. The usage of assertives was represented by binary\nrepresentation.\n6.3 Adding Local Features to Document Representations with\nSVM\nIn the previous section, we presented the individual performance of the local/additional\nfeatures. In the current section, we describe the results of the experiments in which we\ntried to \fnd the three best SVM classi\fers that used the most promising local features and\nat least one of the three document representation techniques. However, as we mentioned\nin Chapter 5 (see Section 5.4), we also built classi\fers that combined several document\nrepresentation techniques. We will present the results of these experiments here as well.\n6.3.1 Bag-of-words\nWe begin by showing the results of our experiments aimed at increasing the accuracy of the\nbest bag-of-words model on the by-article training set in Table 23. We started by adding the\nbest linguistic feature from Table 20a, which was POS uni-grams. This addition increased\nthe performance of the bag-of-pre\fxes classi\fer from 0.7891 to 0.7924. Afterwards, we tried\nto increase the performance by adding the sentiment feature with the highest accuracy from\nTable 21a. However, this was decremental to the accuracy of the classi\fer that used pre\fxes\nand POS uni-grams. As we did not obtain promising results with the stylistic features,\nwe decided to add the best bag-of-clusters features hyp-s300-w5-700 . This procedure\nneither outperformed the results of the classi\fer using POS uni-grams and pre\fxes. Thus,\nin the end, the best classi\fer was the one that used pre\fxes and POS uni-grams.\nThe results of the classi\fers that we trained on the by-publisher subset are depicted\nin Table 24. Since we obtained accuracy scores around 0.90 on the by-publisher subset\nwith the classi\fers using bag-of-words, we only conducted two experiments to optimize\nHyperpartisan News Detection 49/80\nSystem Accuracy\nPre\fxes (1-to-3/4) 0.7891\nPre\fxes (1-to-3/4) + POS uni-grams 0.7924\nPre\fxes (1-to-3/4) + POS uni-grams + negative sentiment 0.7908\nPre\fxes (1-to-3/4) + POS uni-grams + hyp-s300-w5-700 0.7877\nPre\fxes (3-to-4) + POS uni-grams + hyp-s300-w5-700\n+ negative sentiment0.7861\nTable 23: The results of the models that use pre\fxes and additional features on 5-fold\ncross-validation using the by-article training data.\nthe performance of the classi\fer. In the \frst, we tried to increase the performance of the\nSVM classi\fer by adding the sentiment feature with the highest accuracy from the cross-\nvalidation experiments (see Table 21b). Similarly, we added the best linguistic feature\nfrom Table 20b in our second experiment. The latter resulted to the highest accuracy\nscore on cross-validation ( accuracy=0.9261 ). Hence, this was the best model that relied\non bag-of-words plus additional features. Therefore, we continued to evaluate this model\non the by-article (across-datasets) and by-publisher test set (in-domain).\nSystem Accuracy\nWord uni-grams 0.9185\nWord uni-grams + positive + negative sentiment 0.9190\nWord uni-grams + POS uni-grams 0.9261\nTable 24: The results of the models that use pre\fxes and additional features on 5-fold\ncross-validation using the by-publisher subset.\n6.3.2 Bag-of-clusters\nThe performance of the classi\fers on the by-article training data are shown in Table 25. The\nhighest accuracy score was achieved by the model that combined several types of features:\npre\fxes, POS uni-grams, bag-of-clusters (using the clusters hyp-s300-w5-700 ) and the\nnegative sentiment of a text. With this model, we were able to score an accuracy of 0.7954\non 5-fold cross-validation. Thus, we decided to use this classi\fer for further evaluation in\nthe next chapters.\nIn the next table, we show the results of our experiments conducted on the by-publisher\nsubset. When we added POS bi-grams to the clusters, we increased the accuracy from\n0.8556 to 0.9178. We also tried to increase the results by adding the best sentiment\nfeatures on the by-publisher subset, which were the positive and the negative sentiment.\nHowever, this classi\fer did not outperform the current best score of 0.9178. Since the\nresults of the stylistic features were unpromising, we decided to experiment with the best\nbag-of-word features from Table 24. This increased the accuracy to 0.9194. Thus, the best\nHyperpartisan News Detection 50/80\nSystem Accuracy\nHyp-s300-w5-700 + 0.7645\nHyp-s300-w5-700 + POS uni-grams 0.7846\nHyp-s300-w5-700 + POS uni-grams+ pre\fxes +\nnegative sentiment0.7954\nHyp-s300-w5-700 + character 3-to-5 grams 0.7815\nTable 25: The results of the models that use additional features and bag-of-clusters on\n5-fold cross-validation on the by-article training set.\nmodel was the classi\fer using bag-of-clusters (using the cluster hyp-s300-w5-800 ) and\nbag-of-word uni-grams. Consequently, we used this classi\fer for evaluation on the test sets\nof the competition.\nSystem Accuracy\nHyp-s300-w5-800 0.8556\nHyp-s300-w5-800 + POS bi-grams 0.9178\nHyp-s300-w5-800 + word uni-grams 0.9194\nHyp-s300-w5-800 + positive + negative sentiment 0.8560\nTable 26: The results of the models that use additional features and bag-of-clusters on\n5-fold cross-validation on the by-publisher subset.\n6.3.3 Word Embeddings\nThe results of the experiments in which we tried to \fnd the best classi\fer that uses word\nembeddings and additional features are presented in Tables 27 and 28. We show the results\non the by-article training data in the former, and the results on the by-publisher subset in\nthe latter.\nWe used two ways to improve the results of the classi\fer using FT-crawl and the by-\narticle training set. The \frst was by adding the features of the best classi\fer that relied on\neither one of the three document representation techniques. These features were pre\fxes.\nThe second method was by adding the best linguistic feature from Table 20. As the latter\ndecreased the results, we discontinued using them in the next experiments. Instead, we\ntried to increase the performance of the classi\fer using FT-crawl features and pre\fxes\nby adding the negative sentiment as features. Even though the negative sentiment was the\nbest feature from Table 21, we did not manage to increase the performance in the current\nexperiment. We were also unable to improve the results of the classi\fer using FT-crawl\nby combining it with the best cluster features from the earlier experiments.\nBecause of this, the best result from Table 27 is derived from the classi\fer that uses\ntwo document representation techniques: word embeddings and bag-of-pre\fxes. With\nHyperpartisan News Detection 51/80\nthese features, we were able to obtain an accuracy of 0.8093 on 5-fold cross-validation.\nThis is also the highest accuracy that we obtained in general on the by-article training set.\nSystem Accuracy\nFT-crawl 0.7754\nFT-crawl + pre\fxes (1-to-3/4) 0.8093\nFT-crawl + POS uni-grams 0.7909\nFT-crawl + pre\fxes (1-to-3/4) + negative sentiment 0.8093\nFT-crawl + hyp-s300-w5-700 0.7831\nFT-crawl + word uni-grams + hyp-s300-w5-800 0.7985\nTable 27: The scores on 5-fold cross-validation for models that use FastText pre-trained\nword embeddings and additional features trained and evaluated on by-article training.\nFurthermore, the results in Table 28 show that the performance of our best classi\fer\nusing word embeddings ( FT-crawl-excl-stop-words ) could be improved by adding\nword uni-grams. In particular, we were able to increase the accuracy from 0.8125 to 0.9206.\nWe initially decided to experiment with word uni-grams because it was used in the best\nbag-of-words classi\fer. In contrast, we subsequently experimented with pre\fxes in another\nexperiment because they proved to be useful on the other dataset and the third best\nbag-of-words features on the by-publisher subset. However, the pre\fxes did not increase\nthe performance as much as the word uni-grams. We \fnally attempted to optimize the\nperformance of the classi\fer using FT-crawl-excl-stop-words and word uni-grams\nby adding both the positive and the negative sentiment as features. These sentiment\nfeatures achieved the best score when they were used in isolation on 5-fold cross-validation\n(see Table 21). Nonetheless, this only increased the accuracy slightly. Hence, the best\nclassi\fer of the experiments with additional features and FT-crawl-excl-stop-words\nwas the one that we combined with word uni-grams.\n6.4 Experiments with Recurrent Neural Networks\nWe show the results of the experiments that we conducted with the Flair embeddings in\nTable 29. It is important to recall that we used a di\u000berent procedure for feature selection\nthan the SVM classi\fers (see Section 5.5) and that there are two types of Flair embeddings:\nbackward and forward embeddings (see Chapter 4).\nThe results in the table show that we did not manage to obtain promising results with\nthe neural networks. In particular, we were unable to obtain high results with the set-ups\nthat we initially used (described in Section 5.5.1), The results of these set-ups are presented\nby the results of the \frst three classi\fers in Table 29. Besides, the results reveal that the\nFastText word embeddings did not yield high results when they were used with Recurrent\nNeural Networks. The same can be said about the classi\fer that relied on FastText word\nembeddings and Flair embeddings. Furthermore, even the classi\fer that used character\nembeddings and Flair embeddings received only a micro accuracy of 0.5422. This result\nHyperpartisan News Detection 52/80\nSystem Accuracy\nFT-crawl-excl-stop-words 0.8125\nFT-crawl-excl-stop-words + pre\fxes (1-to-3/4) 0.9063\nFT-crawl-excl-stop-words + word uni-grams 0.9206\nFT-crawl-excl-stop-words + word uni-grams\n+ positive + negative sentiment0.9208\nTable 28: The scores on 5-fold cross-validation for models that use FastText pre-trained\nword embeddings (without stop words) and additional features trained and evaluated on\nthe by-publisher subset.\nis lower than the classi\fer which used FastText embeddings and the two \rair embeddings\n(micro-accuracy=0.5802 ).\nBecause of the results of our initial set-ups, we decided to experiment with other pre-\ntrained word embeddings, such as the ones from GloVe. When we experimented with\nGloVe, we obtained higher results than with FastText. The highest accuracy is 0.4828,\nwhich was from the system using GloVe embeddings and the character embeddings of\nLample et al. (2016). Furthermore, the lowest accuracy score was received by the classi\fer\nthat only used the embeddings from FastText. Recall that we obtained the best results\nwith these word embeddings in SVM (see Section 6.1.3).\nFeatures Precision Recall F1-score AccuracyMicro\nAccuracy\nFT-Crawl WE 0.6667 0.5000 0.5714 0.4000 0.5610\nFT-Crawl WE +\nFlair (backward) +\nFlair (forward)0.6842 0.5417 0.6047 0.4333 0.5802\nCharacterEmbeddings\n(Lample et al., 2016) +\nFlair (backward) +\nFlair (forward)0.6190 0.5417 0.5578 0.4062 0.5422\nGloVe 840B.300D +\nCharacterEmbeddings\n(Lample et al., 2016)0.7368 0.5833 0.6511 0.4828 0.6203\nGloVe 840B.300D +\nFlair (backward) +\nFlair (forward)0.7000 0.5833 0.6363 0.4667 0.6000\nTable 29: The performance of the classi\fers that use recurrent neural networks on evaluated\non 10% of the by-article training data and trained on 80% of the by-article training set.\nHyperpartisan News Detection 53/80\n6.4.1 Best Classi\fer\nDespite the low results, we decided to use the second best model of Table 29 for fur-\nther evaluation. According to the micro accuracy, the second best classi\fer used GloVe\nembeddings and the two Flair embeddings. However, we mainly decided to continue ex-\nperimenting with this classi\fer because we were interested in the performance of the Flair\nembeddings combined with one of the pre-trained word embeddings that we evaluated in\nSVMs. Because of the low results with the FastText embeddings, we decided to use the\nGloVe embeddings for this purpose.\nWe trained the classi\fer that used GloVe embeddings plus the FastText embeddings\nfor four other times and selected the model with the highest micro accuracy over the \fve\nruns for further evaluation. We show the best results of this procedure in Table 30. The\nperformance of this classi\fer is similar to the best classi\fer that we presented in Table\n29. In addition, we visualize the performance of this classi\fer over 120 epochs in Figure 6.\nThe graphs show that the accuracy and F1-score on the development set increased over the\namount of epochs. Nonetheless, this pattern is not visible in the most upper graph, which\nindicates that the loss on the training set slowly increased over epochs. Ideally, we would\nhave seen the opposite pattern. Despite the unpromising results, we decided to evaluate\nthis classi\fer on the by-article test set (in-domain evaluation) and on the by-publisher test\nset (evaluation across-datasets).\nMetric Score\nPrecision 0.7000\nRecall 0.5033\nF1-score 0.6365\nAccuracy 0.4667\nMicro accuracy 0.6363\nTable 30: The performance of the best classi\fer over \fve runs that use recurrent neural\nnetworks on predicting hyperpartisan evaluated on 10% of the by-article training data and\ntrained on 80% of the by-article training set.\nHyperpartisan News Detection 54/80\nFigure 6: Three graphs showing the performance of the best classi\fer with neural networks\nover 120 epochs.\n6.5 Summary\nIn this chapter, we showed that it is possible to build accurate classi\fers for hyperpartisan\nnews detection with SVMs. In particular, we were able to obtain promising results with\nbag-of-words, bag-of-clusters and word embeddings. This was the case for both the by-\narticle training set and the by-publisher subset. We therefore selected the best bag-of-\nwords, bag-of-clusters and word embeddings classi\fer of each dataset for further evaluation.\nWe also managed to increase the performance of the best bag-of-words, bag-of-clusters and\nword embeddings classi\fer by combining each classi\fer with other features. Despite the\ndiscouraging results that we obtained with Recurrent Neural Networks, we decided to\ncontinue evaluating the best classi\fer of these experiments to gain more knowledge about\ntheir performance for hyperpartisan news detection.\nHyperpartisan News Detection 55/80\n7 In-domain Results\nThis chapter shows the results of the best classi\fers for each classi\fcation approach in\nan in-domain setting. We present the results of the SVM classi\fers in Section 7.1. We\nalso describe the results of the Recurrent Neural Networks in Section 7.2. We \fnish this\nchapter by presenting the classi\fer with the highest result on the by-article test set, which\nwas used to determine the rankings in the shared task on hyperpartisan news detection.\n7.1 Results with Support Vector Machines\nThis section is divided into two parts. In the \frst part, we show the performance of the\nmodels that we trained on the by-article training set and evaluated on by-article test set.\nIn the second part of this section, we present the results of the classi\fers that we trained\non the by-publisher subset and evaluated on the by-publisher test set.\n7.1.1 Results on the By-article Test Set\nWe show two types of results in Table 31. The \frst are the results from the three clas-\nsi\fers that we trained on any of the three document representations: bag-of-words using\npre\fxes, bag-of-clusters ( hyp-s300-w5-700 ) or word embeddings ( FT-crawl ). The\nsecond type of results is from the classi\fers that use the document representations and\nadditional features. These results are from the best classi\fers of Section 6.3, where we\ncombined document representations with additional features via 5-fold cross-validation.\nWe continued experimenting with the best classi\fers from each of the following tables of\nSection 6.3: Tables 23, 25, and 27.\nIn general, the results in Table 31 show that we managed to obtain better results with\nthe presented classi\fers than the baseline, which scored an accuracy of 0.621 on the by-\narticle test set (see Table 11). In particular, we already outperformed the accuracy of the\nbaseline with the systems that only used one of the document representation techniques.\nNonetheless, the best result was obtained by the classi\fer that combines several features,\nnamely: bag-of-clusters (using hyp-s300-w5-700 ), bag-of-pre\fxes (1-to-3/4), POS uni-\ngrams and the negative sentiment. This classi\fer received an accuracy score of 0.7850 on\nthe test set.\nThe results also show that combining features generally improved the performance of\nthe classi\fers on the test set. The three best accuracy scores are namely received by the\nclassi\fers that used a combination of features. This is similar to the results on 5-fold\ncross-validation, where adding features generally increased the performance of the SVMs\n(see Section 6.3). However, the improvements were only minor, since the accuracy scores\ncame close to 0.80, but never exceeded it.\nIn addition, the results of the classi\fers using either bag-of-pre\fxes, bag-of-clusters\n(using hyp-s300-w5-700 ) or word embeddings ( FT-crawl ) show that the highest ac-\ncuracy score was received by the classi\fer using pre\fxes. We namely achieved an ac-\ncuracy score of 0.7800. It also received the highest precision of all classi\fers in Table\nHyperpartisan News Detection 56/80\n31. This is in parallel to the results that we obtained on 5-fold cross-validation. In par-\nticular, the sequence of the best to least accurate classi\fer is: pre\fxes, FT-CRAWL and\nhyp-s300-w5-700 . The same pattern is visible in Table 31. Besides, there was only\na slight di\u000berence between the accuracy on 5-fold cross-validation and the test set (see\nSection 6.1.4). For instance, the SVM classi\fer that used pre\fxes as features received an\naccuracy score of 0.7891 on 5-fold cross-validation and 0.7803 on the by-article test set.\nIn any case, it is disappointing that the denser representation techniques (bag-of-clusters\nand word embeddings) did not obtain better results when they were used in isolation.\nHowever, we surmised in the previous chapter that this pattern could appear in the in-\ndomain evaluation, similar as in the results on cross-validation. We mentioned the few\namount of out-of-vocabulary words as a possible reason.\nSystem Accuracy Precision Recall F1-score\nPre\fxes (1-to-3/4) 0.7800 0.8121 0.7293 0.7685\nPre\fxes (1-to-3/4) + word uni-grams 0.7802 0.8014 0.7452 0.7722\nHyp-s300-w5-700 0.7500 0.7716 0.7102 0.7396\nHyp-s300-w5-700 + negative sentiment\n+ pre\fxes (1-to-3/4) + POS uni-grams0.7850 0.7993 0.7611 0.7789\nFT-crawl 0.7548 0.7941 0.6879 0.7372\nFT-crawl + pre\fxes (3-to-4) + negative\nsentiment0.7818 0.8062 0.7420 0.7728\nTable 31: The performance of the best SVM classi\fers trained on the by-article training\nset and evaluated on the by-article test set.\n7.1.2 Results on the By-publisher Test Set\nWe show the results on the by-publisher test set in Table 32. We present the results in the\nsame way as in Table 31. Thus, we show the results of the classi\fers using either one of the\nthree document representations and the results of the classi\fers that use a combination of\nfeatures. The latter consists of the best classi\fer of each of the following tables: Tables\n24, 26 and 28.\nThe results in Table 32 show that all classi\fers outperformed the baseline, which scored\nan accuracy of 0.589 on the by-publisher test set (see Table 11). Moreover, the best score\nwas achieved by the classi\fer that only relied on bag-of-word uni-grams. However, this\nscore di\u000bers substantially with the result obtained via 5-fold cross-validation. In particular,\nthe classi\fer received an accuracy of 0.6525 on the test set (see Table 32), but 0.9185 on\ncross-validation (see Table 14). There is a similar gap visible between the results on 5-fold\ncross-validation and the test set of the other classi\fers in Table 32. For instance, the\nclassi\fer that only used bag-of-clusters as features received an accuracy score of 0.8123\non 5-fold cross-validation, but 0.6218 on the test set. The only similarity between the\nscores in our preliminary experiments and the test set was the ranking of the classi\fers'\nHyperpartisan News Detection 57/80\nperformance. The best model was namely the classi\fer using bag-of-words, the second best\nbag-of-clusters and the worst word embeddings.\nIn addition, the results show that we were unable to improve the performance of the best\nclassi\fer by adding linguistic features. In contrast, we were able to increase the performance\nof both the classi\fer using bag-of-clusters and the classi\fer using word embeddings. In\nparticular, the accuracy of the classi\fer using FT-crawl-excl-stopwords was 0.6010,\nand it was increased to 0.6420 by adding bag-of-word uni-grams and sentiment features.\nHowever, none of the other classi\fers outperformed the score of the classi\fer that only\nused word uni-grams.\nFinally, comparing the results in Table 32 to the results on the by-article test set (see\nTable 31) reveals that we obtained higher results on the by-article test set than on the\nby-publisher test set. In particular, the scores on the by-article test set varied between\n0.7803 and 0.7850, whereas the scores on the other test set varied between 0.6010 and\n0.6525. Nonetheless, the performance of the systems that the participants submitted in\nthe shared task on the by-publisher test set shows that we obtained reasonably fair results\non this test set. In the competition, the highest accuracy score on the by-publisher test set\nwas 0.706. Besides, if we would have evaluated the best system on the by-publisher test\nset during the competition, we would have obtained the 5th out of 28th position.\nSystem Accuracy Precision Recall F1-score\nWord uni-grams 0.6525 0.6346 0.7190 0.6742\nWord uni-grams + POS bi-grams 0.6388 0.6225 0.7050 0.6612\nHyp-s300-w5-800 0.6218 0.6084 0.6835 0.6437\nHyp-s300-w5-800 + word uni-grams 0.6418 0.6280 0.6955 0.6600\nFT-crawl-excl-stopwords 0.6010 0.5790 0.7420 0.6504\nFT-crawl-excl-stopwords +\nword uni-grams +\npositive + negative sentiment0.6420 0.6310 0.6840 0.6564\nTable 32: The performance of the best bag-of-words, bag-of-clusters and word embedding\nmodel trained on by-publisher training and evaluated on the by-publisher test set.\n7.2 Results with Recurrent Neural Networks\nIn Table 33, we describe the scores of the classi\fer using Recurrent Neural Networks and\nFlair embeddings. The results show that we received a generally low performance on the\nby-article test set. More speci\fcally, we received an accuracy score of 0.5892, which did\nnot outperform the performance of the baseline. Furthermore, the classi\fer seemed to\nrandomly predict the documents. However, the results are approximately similar to the\nones that we obtained in the previous evaluation set-up (see Table 29). More speci\fcally,\nwhen we evaluated the system presented in Table 33 on the test set that we derived from\nthe by-article training set, we received an accuracy score of 0.4600. The micro accuracy\nHyperpartisan News Detection 58/80\nscore of this system was 0.6000. Thus, the results were not contradictory to the expected\nresults that we had based on the previous results.\nMetric Score\nAccuracy 0.5892\nPrecision 0.5848\nRecall 0.6146\nF1-score 0.5994\n(a) Evaluation scores.True False\nTrue 193 121\nFalse 137 177\n(b)\nMatrix showing\npredictions (columns)\nand gold standard (rows).\nTable 33: The results on the by-article test set of the classi\fer using Flair and GloVe\nembeddings using Recurrent Neural Networks.\n7.3 Best Classi\fer\nAfter evaluating the best SVM and neural network classi\fers from Chapter 6 on the test\nsets, we took the classi\fer with the highest accuracy on the by-article test set for further\nimprovement. This was the best SVM classi\fer presented in Table 31. It used the following\nfeatures: the clusters hyp-s300-w5-700 , POS uni-grams, pre\fxes (1-to-3/4) and the\nnegative sentiment. Furthermore, this classi\fer received an accuracy score of 0.7954 on 5-\nfold cross-validation (see Table 25). Even though it did not obtain the best score on 5-fold\ncross-validation, it managed to achieve the best score on the by-article test set among all\nclassi\fers.\nWe conducted three additional experiments where we trained SVM classi\fers on the\nby-article test set through 5-fold cross-validation while using the same features of the best\nclassi\fer on the by-article test set. In these experiments, we tried to increase the perfor-\nmance of our best classi\fer by applying pre-processing techniques. In the \frst experiment,\nwe removed the stop words of the texts. In the second experiment, we balanced the labels\nof the dataset13. Finally, we concatenated the title to the body of the text, similar as in\nthe SVM classi\fers from Singhania et al. (2017).\nWe show the results of these techniques on 5-fold cross-validation in Table 34. The best\nresult was achieved by the classi\fer where we removed the stop words of the documents.\nIn particular, we managed to increase the accuracy score on 5-fold cross-validation from\n0.7954 to 0.8017. We also improved the results on the by-article test set with this classi\fer,\nwhich we show in Table 35. As shown in the table, the classi\fer scored an accuracy of\n0.7866, which is close to the performance of the winning system in the shared task. In\nparticular, if we would have submitted this system in the competition, then we would\nhave obtained the 8th place out of 42 participants. In addition, this system substantially\noutperforms the baseline that we used in this project, which received an accuracy of 0.621.\n13using strati\fed sampling from Sklearn, https://scikit-learn.org/stable/modules/\ngenerated/sklearn.model_selection.train_test_split.html\nHyperpartisan News Detection 59/80\nSystem Accuracy\nBest classi\fer 0.7954\nBest classi\fer + stop word removal 0.8017\nBest classi\fer + balanced labels in training data 0.7905\nBest classi\fer + concatenate title to body 0.7955\nTable 34: Results the by-article training data on 5-fold cross-validation showing the e\u000bects\nof the pre-processing procedures of the best classi\fer on the by-article test set.\nAccuracy Precision Recall F1-score\nWinning system in competition 0.8220 0.8710 0.7555 0.8090\nBaseline 0.6210 0.5820 0.860 0.6940\nBest classi\fer 0.7850 0.7993 0.7611 0.7789\nBest classi\fer without\nstopwords0.7866 0.8041 0.7580 0.7803\nTable 35: Results of the previous best classi\fer and the current best classi\fer on the\nby-article test set. We also show the results of the winning system in the competition.\n7.3.1 Summary\nOur best classi\fer on by-article test set received an accuracy score of 0.7866. This SVM\nclassi\fer relied on bag-of-clusters (using the clusters hyp-s300-w5-700 ), pre\fxes, POS\nuni-grams and the negative sentiment of the text. Nonetheless, we were already able\nto obtain a good performance on the by-article test set by building classi\fers that used\neither one of the three document representations: bag-of-pre\fxes, bag-of-clusters or word\nembeddings. We managed to increase the performance of these classi\fers by combining\nfeatures, but only slightly. The results on the by-publisher test set remained relatively\nlow, despite the size of the by-publisher subset, which is much larger than the by-article\ntraining set. Finally, we were unable to receive similar good results with the Recurrent\nNeural Networks, which did not even exceed the scores of the baseline.\nHyperpartisan News Detection 60/80\n8 Across-datasets and Out-of-domain Evaluation\nIn this chapter, we present the results of two types of evaluation settings. First, across-\ndatasets, namely, training on by-article and evaluating on by-publisher and vice versa.\nThe second setting is out-of-domain, for which we use the best features from the classi\fer\nwith the highest results on the by-article test set, without any speci\fc tuning, to perform\nthe out-of-domain evaluation on two fake news datasets as previously performed by P\u0013 erez-\nRosas et al. (2018).\n8.1 Across-datasets Evaluation on Hyperpartisan using SVM\nIn this section, we present the results of the best bag-of-words, bag-of-clusters and word\nembeddings classi\fers across-datasets. We also show the results of the SVM classi\fers using\nat least one of the three document representations and additional features. We present the\nresults of the classi\fers that we trained on the by-article training set in Section 8.1.1 and\non the by-publisher subset in Section 8.1.2.\n8.1.1 Training on By-article\nWe show the results of the classi\fers that we trained on the by-article training set and\nevaluated on the by-publisher test set in Table 36. In the \frst row of the table, we present\nthe scores of the classi\fer with the highest accuracy score on the by-article test set via\nin-domain evaluation. Thus, this was the best classi\fer that we trained on the by-article\ntraining data and evaluated on the by-article test set. This classi\fer received an accuracy\nscore of 0.7866 on the by-article test set, but an accuracy of 0.5458 on the by-publisher\ntest set (i.e., across-datasets). Hence, the classi\fer with the highest performance on the\nby-article test set did not obtain the best result on the by-publisher test set.\nInstead, the highest accuracy ( accuracy=0.5820 ) on the by-publisher test set was\nachieved by the classi\fer that only used word-embeddings ( FT-crawl ). The lowest accu-\nracy score was received by the classi\fer using pre\fxes (1-to-3/4) and bag-of-word uni-grams\n(accuracy=0.5330 ). Furthermore, the classi\fer that only used bag-of-clusters received a\nhigher accuracy ( accuracy=0.5648) on the test set than the classi\fer using pre\fxes ( accu-\nracy=0.5353 ). These results con\frmed our theory that denser representations would yield\nbetter performance across-datasets than in-domain.\nMoreover, the results in Table 36 show that adding features did not increase perfor-\nmance of the classi\fers. For instance, the classi\fer that only used pre\fxes scored an accu-\nracy of 0.5353, whereas the classi\fer that used pre\fxes and bag-of-word uni-grams received\nan accuracy of 0.5330. A similar e\u000bect is visible for the the classi\fer using clusters and\nthe word embeddings classi\fer. However, the results of the in-domain and cross-validation\nexperiments show the opposite, since adding features generally improved the accuracy in\nthat set-up.\nIn conclusion, we received the best accuracy score in this set-up when using the most\ndense document representations without additional features. Adding features only de-\nHyperpartisan News Detection 61/80\ncreased the performance across-datasets when we trained systems on the by-article training\nset and evaluated them on the by-publisher test set. Furthermore, we generally received\nlow results on the by-publisher test set with the systems that we trained on the by-article\ntraining set. However, these results are fair when we compare them to the results that we\nachieved in our in-domain evaluation set-up. As mentioned in the previous chapter, the\nbest classi\fer that we trained on the by-publisher subset and evaluated on the by-publisher\ntest set received an accuracy of about 0.65.\nSystem Accuracy Precision Recall F1-score\nHyp-s300-w5-700 + POS bi-grams +\npre\fxes + (1-to-3/4) + negative sentiment\n+ stopword removal0.5458 0.5604 0.4245 0.4831\nPre\fxes (1-to-3/4) 0.5353 0.5457 0.4210 0.4753\nPre\fxes (1-to-3/4) + bag-of-word\nuni-grams0.5330 0.5402 0.4435 0.4871\nHyp-s300-w5-700 0.5648 0.5979 0.3965 0.4767\nHyp-s300-w5-700 + POS bi-grams +\npre\fxes (1-to-3/4) + negative sentiment0.5485 0.5637 0.4290 0.4872\nFT-crawl 0.5820 0.6137 0.4425 0.5142\nFT-crawl + pre\fxes (1-to-3/4)\n+ negative sentiment0.5560 0.5734 0.4375 0.4963\nTable 36: The results of the best SVM classi\fers trained on the by-article training set and\nevaluated on the by-publisher test set. The \frst row represents the scores of best system\non the by-article test set from the in-domain evaluation experiments, of which we show the\nperformance on the by-publisher test set in this table.\n8.1.2 Training on By-publisher\nWe present the results of the classi\fers that we trained on the by-publisher subset and\nevaluated on the by-article test set in Table 37. The best result was scored by the classi\fer\nthat only used word embeddings ( FT-crawl-excl-stopwords ). On one hand, this\nwas another con\frmation that denser word representations perform better across-data.\nOn the other hand, the classi\fer that only used bag-of-clusters had a lower performance\n(accuracy=0.6369 ) than the classi\fer that only used bag-of-words ( accuracy=0.6417 ).\nSimilar to the results in Table 36, adding features did not increase the performance\non the test set. For example, the lowest accuracy score was achieved by the classi\fer\nthat used FT-crawl-excl-stopwords and additional features. However, one main\ndi\u000berence between the results is that the classi\fers in Table 37 received a high F1-score,\nwhich varied between 0.6953 and 0.7066. The F1-scores of the results across-datasets on\nthe by-publisher test set however, varied between 0.4767 and 0.5142. The higher F1-score\nis related to the substantially higher recall of the classi\fers in Table 37. Thus, despite\nHyperpartisan News Detection 62/80\nthe large size of the by-publisher subset compared to the by-article training set, we were\nunable to obtain similar scores on the by-article test set as we did with by-article training\nset.\nSystem Accuracy Precision Recall F1-score\nBag-of-word uni-grams 0.6417 0.5982 0.8631 0.7066\nBag-of-word uni-grams + POS bi-grams 0.6369 0.5931 0.8726 0.7062\nHyp-s300-w5-800 0.6354 0.5934 0.8599 0.7022\nHyp-s300-w5-800 + bag-of-word\nuni-grams0.6354 0.5938 0.8567 0.7014\nFT-crawl-excl-stopwords 0.6465 0.5991 0.8854 0.7147\nFT-crawl-excl-stopwords + bag-of-word\nuni-grams + negative + positive\nsentiment0.6274 0.5881 0.8503 0.6953\nTable 37: The results of the best SVM classi\fers trained on the by-publisher subset and\nevaluated on the by-article test set.\n8.2 Across-datasets Evaluation on Hyperpartisan using Neural\nNetworks\nWe show the results of the classi\fer using Recurrent Neural Networks in Table 38. Recall\nthat this classi\fer used the Flair embeddings (both forward and backward) and GloVe\nembeddings ( 840B.300D ). The scores that the classi\fer obtained on the by-publisher\ntest set were slightly lower than the scores that it received in the in-domain evaluation\nprocedure (see Table 33). In particular, the classi\fer scored an accuracy of 0.5458 on\nthe by-publisher test set. The results also show that the classi\fer randomly classi\fed\nthe documents, similar as in the in-domain evaluation. This is particularly visible in the\nconfusion matrix. Because of these results, we concluded that the Flair embeddings and\nthe Recurrent Neural Networks were unable to perform well in general, since the baseline\noutperformed the results of the neural networks in both evaluations. We will pinpoint a\nfew of the possible reasons in Chapter 9.\nHyperpartisan News Detection 63/80\nMetric Scores\nAccuracy 0.5425\nPrecision 0.5478\nRecall 0.4870\nF1-score 0.5156\n(a) Evaluation scores.True False\nTrue 974 804\nFalse 1026 1196\n(b)\nmatrix showing\npredictions (columns)\nand gold standard (rows).\nTable 38: The across-datasets performance of the best neural network classi\fer using the\nFlair and GloVe embeddings. The classi\fer was trained on the by-article training set and\nevaluated on the by-publisher test set.\n8.3 Out-of-domain Evaluation on Fake News\nIn this section, we present the results of three classi\fers that we evaluated through two\ndi\u000berent set-ups using the two datasets of P\u0013 erez-Rosas et al. (2018): the FakeNewsAMT\nand CelebrityNews dataset. In the \frst set-up, we trained the classi\fers via 5-fold cross-\nvalidation on the FakeNewsAMT dataset. In the second set-up, we trained these classi\fers\non the FakeNewsAMT dataset and evaluated them on the CelebrityNews dataset. Thus,\nwe tried to reproduce the experimental set-up described in P\u0013 erez-Rosas et al. (2018) (see\nChapter 2). In this experiment, the authors performed an across-domain evaluation by\ntraining their classi\fers on the FakeNewsAMT dataset and evaluating them on the Celebri-\ntyNews dataset, and vice versa. However, in our experiment, we used our best classi\fers\nwithout performing speci\fc feature tuning for fake news detection, this set-up allowed us\nto investigate the robustness of our features.\nIn particular, we worked with the features that were used by the best bag-of-words\n(pre\fxes), bag-of-clusters ( hyp-s300-w5-700 ) and word embeddings ( FT-crawl ) clas-\nsi\fers which we selected based on the results on the by-article test set (see Table 31). We\ndecided use these classi\fers instead using their alternative versions with additional fea-\ntures. The reason was that the results of the experiments across-datasets in Section 8.1\n(see Table 36 and 36) showed that adding features generally decreases the performance\nof the classi\fers. It should also be noted that we decided to work with the systems and\nfeatures that we trained on the by-article training data because of the substantially lower\nperformance of the systems that we trained on the by-publisher subset on other settings\nthan 5-fold cross-validation.\n8.3.1 Results\nWe show the results of the classi\fers evaluated on 5-fold cross-validation in Table 39. The\ntable shows that we obtained the best result with the classi\fer using word embeddings\n(FT-crawl ) (accuracy=0.638 ). Moreover, the accuracy score of the classi\fer using pre-\n\fxes was 0.5120, which was slightly higher than the classi\fer that relies on clusters. This\nHyperpartisan News Detection 64/80\nclassi\fer scored an accuracy of 0.5104.\nSystem Accuracy\nPre\fxes 0.5104\nHyp-s300-w5-700 0.5120\nFT-Crawl 0.6438\nTable 39: The results on 5-fold cross-validation of the classi\fers trained on the Fake-\nNewsAMT dataset of P\u0013 erez-Rosas et al. (2018). Each classi\fer used either one of the best\nfeatures from the best bag-of-words, bag-of-clusters or word embeddings classi\fer that we\ndeveloped for fake news detection.\nWe show the accuracy scores of the classi\fers that we trained on FakeNewsAMT and\nevaluated on CelebrityNews in Table 40. We additionally show the results of the classi\fers\nfrom P\u0013 erez-Rosas et al. (2018) using the same experimental set-up. We show a more\ndetailed overview of the performance of our systems on the FakeNews dataset in Table 41.\nSystem Training Testing Features Accuracy\nP\u0013 erez-Rosas et al. (2018) FakeNewsAMT CelebrityNewsLIWC\nReadability\nAll features0.480\n0.520\n0.500\nOur work FakeNewsAMT CelebrityNewsPre\fxes (1-to-3/4)\nhyp-s300-w5-700\nFT-Crawl0.608\n0.632\n0.588\nTable 40: Out-of-domain evaluation on the CelebrityNews dataset of P\u0013 erez-Rosas et al.\n(2018).\nFeatures Class Precision Recall F1-score Accuracy\nPre\fxes (1-3/4)Fake 0.60 0.67 0.630.608Real 0.62 0.54 0.58\nHyp-s300-w5-700Fake 0.61 0.75 0.670.632Real 0.67 0.52 0.58\nFT-crawlFake 0.57 0.73 0.640.588Real 0.62 0.44 0.52\nTable 41: Results of the SVM classi\fers using the best document representations of our\nsystems on the by-article test set. The systems are trained on the FakeNewsAMT data\nand evaluated on the CelebrityNews dataset from P\u0013 erez-Rosas et al. (2018).\nThe results show that all three classi\fers outperformed the results of P\u0013 erez-Rosas et al.\n(2018). Their best system, which used readability features, received an accuracy score of\n0.520. Our best classi\fer scored an accuracy score of 0.632 on the CelebrityNews dataset\nHyperpartisan News Detection 65/80\nand the classi\fer with the lowest performance received an accuracy of 0.588. The classi-\n\fer with the highest performance relied on clusters ( hyp-s300-w5-700 ), whereas the\nclassi\fer with the lowest results used word embeddings FT-crawl . The latter is surpris-\ning, since we achieved the best results with this system when we evaluated it via 5-fold\ncross-validation (see Table 39). However, the classi\fer using clusters ( accuracy=0.608 )\nperformed better than the pre\fxes ( accuracy=0.632 ), which suggests that denser repre-\nsentations perform better out-of-domain.\nBased on these results, we concluded that our features yielded more robust systems than\nlocal features, which were mainly used in the systems of P\u0013 erez-Rosas et al. (2018). All our\nSVM systems, especially those that did not use local features, outperformed the SVM clas-\nsi\fers of P\u0013 erez-Rosas et al. (2018). Besides, the results of the experiments across-datasets\nshowed that local features decreased the performance. Thus, local features decrease the\nperformance vastly when applied across-datasets and in out-of-domain settings.\n8.4 Summary\nIn this chapter, we presented two types of results. We started by presenting the perfor-\nmance of our systems across-datasets. We evaluated the classi\fers that we trained on\nthe by-article training set on the by-publisher test set, and the classi\fers that we trained\non the by-publisher subset on the by-article test set. In both evaluations, we obtained\nthe best results with the classi\fers that only used word embeddings. In particular, these\nwere the word embeddings from FastText, trained on Common Crawl ( FT-crawl and\nFT-crawl-excl-stopwords ). The results also showed that adding local features de-\ncreases the performance of the classi\fers when they are evaluated across-datasets. This\nwas apparent on both evaluations.\nWe also presented the results of the experiments in which we used the settings of the best\nbag-of-words, bag-of-clusters and word embeddings classi\fers without additional features\nfor fake news detection. In one experiment, we evaluated these systems on 5-fold cross-\nvalidation using only one of the datasets from P\u0013 erez-Rosas et al. (2018). We achieved the\nbest results with the classi\fer that used the most dense vector document representations\n(i.e., word embeddings). In the other experiment, we trained the same systems from\nthe \frst experiment on one dataset of P\u0013 erez-Rosas et al. (2018) and evaluated it on the\nother. With our systems, we were able to outperform the results of P\u0013 erez-Rosas et al.\n(2018), whose systems only relied local features. This led us conclude that local features\nare detrimental to the performance of classi\fers in out-of-domain evaluations. Finally,\nsince the pre\fxes received the lowest performance, we concluded that denser document\nrepresentations perform better out-of-domain than sparser document representations.\nHyperpartisan News Detection 66/80\n9 Discussion\nIn this chapter, we provide an analysis of the results that we obtained in our experiments.\nFirst, we discuss the results and features of our best classi\fer in Section 9.1. Then, we\nanalyze the results of the in-domain and across-datasets evaluations, in Sections 9.2 and\n9.3 respectively. We \fnish this chapter by discussing the out-of-domain results on fake\nnews in Section 9.4.\n9.1 Best Classi\fer\nWith our best classi\fer, were able to reach an accuracy of 0.7866 on the o\u000ecial test set of the\nshared task on hyperpartisan news detection. This system performed close to the winning\nsystem of the competition, even though it was simple to implement. If we would have\nsubmitted this model during the evaluation procedure of the competition, then we would\nhave obtained the 8th out of 42th position in the competition. The classi\fer used SVM and\nrelied on a combination of features: bag-of-clusters using the clusters hyp-s300-w5-700 ,\nbag-of-pre\fxes (1-to-3/4), POS uni-grams and the negative sentiment. The classi\fer was\ntrained on a training set derived from the same corpus as the o\u000ecial test set, which\nconsisted of manually annotated data.\nIn the rest of this section, we provide more information about our best classi\fer. We\ndivide the section into two parts. In the \frst, we provide more information about our\nbest classi\fer by discussing the confusion matrix. In the second part, we discuss the most\nimportant features that the classi\fer used.\n9.1.1 Common Confusions\nWe show the confusion matrix in Table 42. The results show that 238 instances were\ncorrectly labeled as being hyperpartisan (\\true\"), but that 58 instances were incorrectly\nlabeled as being hyperpartisan news. In addition, 256 documents were correctly labeled as\nnon-hyperpartisan news. However, 76 documents were labeled as not being hyperpartisan,\nwhereas they were hyperpartisan according to the gold standard. This suggests that the\nsystem is likely to incorrectly label a document as being mainstream/non-hyperpartisan\nnews. This could be related to the substantially higher amount of mainstream news doc-\numents in the training set.\n9.1.2 Important Features\nWe attempted to expose the most important features that our classi\fer selected by observ-\ning the weight coe\u000ecients that the SVM classi\fer assigned to the features. We were unable\nto do this for our best classi\fer on the by-article test set, because it was not possible to\ninspect the weights of the sentiment features. Therefore, we only show the most important\nfeatures for a classi\fer that is almost the same as our classi\fer. The only di\u000berence is\nthat this classi\fer did not use sentiment features. Thus, this classi\fer relied on clusters\nHyperpartisan News Detection 67/80\nPrecision Recall F1-score Support\nTrue 0.8041 0.7580 0.7803 314\nFalse 0.7711 0.8153 0.7926 314\n(a) precision, recall and f1-score.True False\nTrue 238 76\nFalse 58 256\n(b)\nmatrix showing\npredictions (columns)\nand gold standard (rows).\nTable 42: The performance of our best classi\fer on hyperpartisan news detection, evaluated\non the by-article test set. The classi\fer scored an accuracy of 0.7866.\nhyp-s300-w5-700 , bag-of-pre\fxes (1-to-3/4) and POS uni-grams. We also removed the\nstop words of the documents, following the same procedure of the best classi\fer.\nWe show the Top 30 most important features for predicting hyperpartisan (the positive\ncoordinates) and mainstream news (the negative coordinates) in Figure 7. The longer\nthe bars, the higher the importance of the features. The results show that clusters are\nimportant features to predict hyperpartisan news. More speci\fcally, 14 out of 30 of the\nmost contributory features for hyperpartisan news detection are clusters, as well as the\nthree most important features.\nWe show a proportion of the words of the clusters that are among the top three most\nimportant features for predicting hyperpartisan news detection in Table 43. It is worthwhile\nto note that the words in the clusters from Table 43 are related to topics and words that\nare associated with hyperpartisan news. Some examples of such words are hate,anti-\nimmigrant , and dogwhistle . This observation reveals that the classi\fer correctly models the\ntask of hyperpartisan news detection by making e\u000eciently use of the occurrence of themes\nand words that are related to hyperpartisan news. It also explains why it is important to\nwork with clusters that are related to the task.\nIn addition to clusters, pre\fxes are also important features for hyperpartisan news\ndetection, according to Figure 7. Even though the pre\fxes were able to capture important\nsequences of words, such as Don and Dona , they mainly extracted features related to\npunctuation marks. Following this, it is possible that a more e\u000bective pre-processing to\nremove punctuation marks might have helped to improve performance.\nFinally, Figure 7 also shows that POS uni-grams are helpful features for hyperpartisan\nnews detection. The POS uni-grams seem to share characteristics with the pre\fx features.\nThere are namely some words in the POS uni-grams which are also captured by the pre\fxes,\nsuch as the words said and men. However, the e\u000bectiveness of the POS uni-grams is also\npartly related to the same issue as we mentioned about the pre\fxes, as there appear several\npunctuation marks in the POS uni-grams as well.\nHyperpartisan News Detection 68/80\nFigure 7: The top 30 most important features for predicting hyperpartisan (positive values)\nand mainstream news (negative values).\nHyperpartisan News Detection 69/80\nCluster 428 Cluster 182 Cluster 438\nthreats\nherinsensitive\nanti-indigenous\ndogwhistle\nanti-defamation\nanti-hispanic\nsilencing\nklanner\nhateful\nwhite-supremacist\nslights\nk.k.k.\ndecry\nfascistic\nanti-blackness\nhate\nblasphemous\nanti-immigrant\ndivisiveness\nracistsalwaysa\nddresspolicies\nnicer\nabout.republicans\nbecause\nblink-182\nmaybe\nobamacare.they\nsomewhere\ncrummy\nanything\nadvertisementyes\nluckily\nit.donald\ncommiserate\nseeing\nfeels\nhappier\namericaplan\nobama-bi-partisans\nrepressively\nrebelling\nelectoralism\njacksonians\nneofascists\noutmaneuvered\nrevolutionaries\nsubordinating\nnon-ideological\nanticolonial\npotentates\nsuperstate\nanti-revolutionary\noverthrowing\nplutocratic\ncounter-revolutionary\ntheocracies\nrepressiveness\nplebeians\nTable 43: The three most important clusters for predicting hyperpartisan news (left: most\nimportant, right: less important). The three clusters were also the most important features\nto detect hyperpartisan news.\nHyperpartisan News Detection 70/80\n9.2 In-domain Results\nWe scored an accuracy of 0.7866 on the by-article test set, which would yield the 8th\nout of 42th position in the competition. Our best system on the by-publisher test set\nscored an accuracy of 0.6525, which would have resulted to the 5th out of 28th position\nin the ranking. The most probable reason why we did not manage to obtain a similar\nscore on the by-publisher test set might be related to the lesser quality of the annotations\nin the by-publisher data in general. The documents from the by-publisher corpora were\nnamely semi-automatically annotated, whereas the documents from the by-article data\nwere manually annotated. Besides, we have seen that there were documents in the data\nwhich were not about political news, even though they were labeled as being hyperpartisan\nnews (see Chapter 3).\nOverall, we obtained good results with the SVM classi\fers in the in-domain evalua-\ntion procedure. This supports the previous studies that we discussed, which proved that\nSVMs are e\u000bective for related tasks to hyperpartisan news detection, including fake news.\nFurthermore, the results revealed that, when tested on their own, sparse document repre-\nsentations yield better results than dense document representations. We explained that it\ncould be the case that there were a low amount of out-of-vocabulary words in the test set.\nBesides, we have seen in Section 9.1.2 that pre\fxes, for instance, tended to extract patterns\nthat are not directly associated with hyperpartisan news. Instead, they captured elements\nthat occurred speci\fcally in the corpus of consideration, such as punctuation sequences. It\ncould be the case that these punctuation sequences occurred frequently in the test set as\nwell.\nIn addition, the results of the SVMs showed that local features and/or combining several\ndocument representations increased the performance of classi\fers that only relied on either\none of the three document representation techniques that we used in SVM. Examples of\nlocal features were POS uni-grams, POS bi-grams and the sentiment of the text. We\nmentioned in Chapter 2 that these features proved to be e\u000bective in studies that focused\non bias detection in Wikipedia, such as Recasens et al. (2013). Thus, our results con\frm\nthe previous \fndings on local features for related tasks. However, our results also showed\nthat in addition to combining local features, combining document representations also\nhelps to increase the results. This aspect was not discussed in the works of the theoretical\nframework that we outlined, since these studies focused on combining a large amount of\nlocal features, with minimal usage or experimentation on document representations.\nLastly, we were unable to obtain good results with Recurrent Neural Networks, despite\nthat they were the most sophisticated method of document representation. One reason\ncould be the size of the by-article training set, which might have been too small to train\nthe neural networks. Another issue could be the length of the documents. Perhaps the\ndocuments were too long for the Recurrent Neural Networks, despite using the GRU vari-\nant. It could also be the case that the Flair Embeddings were not suitable for this task.\nNonetheless, we believe that more in-depth work would be required to understand and im-\nprove the results for this task using Recurrent Neural Networks and Flair. A good point of\ndeparture would for instance be to study the obtained representations with Flair. It would\nHyperpartisan News Detection 71/80\nalso be worthwhile to experiment in more detail with the parameters of the Recurrent\nNeural Networks, since we only used the default settings.\n9.3 Results Across-datasets\nWe obtained remarkable results when we evaluated our SVM systems across-datasets,\nsince we received the best results on both the by-article and by-publisher test set with the\nclassi\fers that only used word embeddings. More speci\fcally, these systems used the word\nembeddings from FastText (trained on Common Crawl). The classi\fer using bag-of-clusters\nobtained better results than the pre\fxes on the by-publisher test set when we trained the\nclassi\fers on the by-article training set. However, the clusters scored lower than the bag-\nof-words on the by-article test set, when we trained our systems on the by-publisher subset.\nWe suspect that this is related to the corpus of the clusters, since the clusters were trained\non the training and validation set of the by-publisher corpus. Consequently, this could\nhave limited the amount of out-of-vocabulary words (i.e., clusters) on the by-publisher test\nset.\nThe results also showed that, in contrary to the in-domain results, adding features\ngenerally decreases the performance. This was apparent in both the results of the by-\npublisher and by-article test set. One explanation could be that the local features, such as\nsentiment, tend to capture elements that are highly associated with characteristics from\nthe corpus, rather than hyperpartisan news. Moreover, the reason that neither combining\ndocument representations did not increase the results could be that this resulted to over-\n\ftting on the training data. It could also be the case that the document representations\nfail to e\u000bectively complement each other. It would therefore be interesting to perform a\nsimilar study of the most important features as we did in Section 9.1.2, speci\fcally when\nevaluating a system across-datasets.\nMoreover, we were unable to obtain better results on the by-article test set when we\ntrained our classi\fers on the by-publisher subset, despite the substantially larger size of\nthe by-publisher subset. On one hand, this could be explained by the di\u000berence in the\nquality of the annotations of the by-publisher subset. As mentioned before, the quality\nof the annotations is lower than the quality of the annotations in the by-article training\nset. On the other hand, we did not manage to obtain good results on the by-publisher\ntest set when we trained the classi\fers on the by-article training set. The concern with\nthis set-up could be the small size of the by-article training set. However, the low results\non the by-publisher test set can also be related to the noise in the by-publisher corpus in\ngeneral, especially since the competition results on the by-publisher test set were generally\nlower than on the by-article test set.\n9.4 Out-of-domain Results\nEven though our features were tuned for hyperpartisan news detection, our systems outper-\nformed the classi\fers from P\u0013 erez-Rosas et al. (2018) for fake news detection across-domains.\nHyperpartisan News Detection 72/80\nWe achieved this with our our best bag-of-pre\fxes, bag-of-clusters (using hyperpartisan/-\nmainstream news clusters) and word embeddings classi\fers. Since the systems of P\u0013 erez-\nRosas et al. (2018) fully relied on local features, such as the readability of the texts, we\nconcluded that document representation features work better across fake news domains.\nThus, it seems that classi\fers which are trained for a downstream-task on either bag-of-\npre\fxes, bag-of-clusters and word embeddings are capable of working well across tasks and\ndomains.\nIn addition, we scored the best results with the denser document representations. We\nobtained the best results on 5-fold cross-validation with word embeddings. This could\nbe related to the same reasons as we mentioned about the good performance of word\nembeddings across-datasets. The word embeddings are trained on very large corpora (i.e.,\nCommon Crawl) and are therefore less sensitive to the negative e\u000bects of out-of-vocabulary\nwords, which is more problematic for sparse representations such as pre\fxes. Something\nsimilar occurred with the clusters-based representations, which obtained the best results in\nthis evaluation setting. Our \fndings are therefore particularly promising for experimental\nset-ups where it is di\u000ecult to train task and/or domain speci\fc clusters due to scarcity or\nthe unavailability of large corpora. A simple solution would then be to train clusters on a\nrelated task and to use these clusters for the current task.\nHyperpartisan News Detection 73/80\n10 Conclusion\nIn this \fnal chapter, we answer the questions that we posed at the start of this thesis. We\nalso discuss what we would have done if we would have had more time to continue working\non this project.\n10.1 Summary\nThis thesis described our work to develop competitive classi\fcation systems on the shared\ntask on hyperpartisan news detection (SemEval-2019 Task-4). Our best system obtained\nan accuracy of 0.7866 on the o\u000ecial test set of the competition and used the following\nfeatures: bag-of-clusters (using closely domain-related clusters), bag-of-pre\fxes, POS uni-\ngrams and the negative sentiment of the text. However, the models which used only dense\nword representations based on word clusters or word embeddings behaved more robustly\nwhen considering out-of-domain evaluation settings and evaluation settings across-datasets.\nIf we look back at our \frst research question, What is the most e\u000bective method to\nvectorize documents for hyperpartisan news? , the results indicated that clusters and word\nembeddings work best, although there were di\u000berences. For in-domain evaluations, com-\nbining them with local and sentiment features generally improved the results. However,\nas the \fndings of our experiments across-datasets (by-publisher and by-article) and across\ntasks (fake news) showed, the most robust models are those that use only clusters or word\nembeddings.\nWith respect to the second question, we were wondering whether we could improve the\nresults of our classi\fers by combining document representations and/or by adding local\nfeatures. Our \fndings showed that this is indeed the case, but only in in-domain evalua-\ntion. Nonetheless, the improvements were only minor, and the extent to which the features\ne\u000bectively complemented each other remained relatively uncertain. In addition, when we\nevaluated our systems across-datasets, we observed that combining document represen-\ntations and adding features were detrimental to the results. Instead, we received better\nresults with the systems that only relied on either one of the three document representa-\ntions that we used in the SVM classi\fers: bag-of-words/characters, bag-of-clusters or word\nembeddings.\nThe third research question was: Is it possible to build a classi\fer for hyperpartisan news\ndetection that is robust enough to perform well across di\u000berent datasets of hyperpartisan\nnews? . In our experiments, it was challenging to build such a system with the datasets\nthat we used. On one hand, we worked with a large data set of hyperpartisan/mainstream\nnews articles, but the quality of the annotations was relatively lower than the other data\nset. On the other hand, we also worked with a dataset with a good quality of annotations,\nbut the dataset was relatively small. However, the results obtained when evaluating on\nthe by-publisher test set showed that using clusters trained on the by-publisher training\nset helped to improve results. This means that using in-domain data to obtain dense word\nrepresentations is the way to go in this setting.\nIn addition, our \fndings showed that it is possible to train classi\fers for hyperpartisan\nHyperpartisan News Detection 74/80\nnews detection and to use the most e\u000bective features of this experiment for fake news\ndetection. Since we only conducted a few experiments, this \fnding can be seen as an\ninteresting starting point for further research on transfer learning. Besides, there are\ncurrently more datasets available for fake news detection than for hyperpartisan news\ndetection. These studies may go hand-in-hand with future research in journalism, to shed\nmore light on the di\u000berences and similarities between fake news and hyperpartisan news.\nHowever, we did not manage to obtain good results with the Recurrent Neural Networks\nand contextual-character based embeddings. If we would have had the opportunity to\ncontinue this work, we would have studied the obtained contextual character-based word\nembeddings from Flair. Therefore, further research will have to be carried out to shed\nmore light on the performance of recurrent neural networks and contextual-character based\nembeddings for hyperpartisan news detection.\nSummarizing, when the aim is to build a robust classi\fer that is able to perform well\nacross-datasets on hyperpartisan news detection, we suggest to use dense document repre-\nsentations. However, to optimize the performance of a system on a speci\fc data set, it is\nbetter to use a combination of dense word representations with task-speci\fc features such\nas sentiment.\n10.2 Closing Remarks\nWe presented a systematic comparative analysis to study the performance of di\u000berent\ndocument representation techniques for hyperpartisan news detection. We obtained very\ncompetitive results for in-domain evaluation on a recent, o\u000ecial shared task on hyper-\npartisan news detection. We also described a robust set of features that can be used for\nhyperpartisan news detection that also work well on fake news detection, without using\nspeci\fc tuning for fake news.\nHyperpartisan news detection is an interesting problem that can be tackled with simple\ntext classi\fers. We hope that the body of research on hyperpartisan news detection will\ncontinue to grow in the future. In particular, we would like to see the task extended in two\nways. First, by asking the systems to justify its decisions, namely, a type of \\interpretable\nhyperpartisan news detection\". Second, in terms of the granularity of analysis in order to\nbe able to detect who said what, when and to whom. We believe that this would allow\nto better understand the reasons why systems (and humans) identify a given document as\nhyperpartisan.\nAcknowledgments\nWe gratefully acknowledge the support of NVIDIA Corporation with the donation of the\nTitan V GPU used for this research at the University of the Basque Country.\nHyperpartisan News Detection 75/80\nReferences\nRodrigo Agerri and German Rigau. Language independent sequence labelling for opinion\ntarget extraction. Arti\fcial Intelligence , 268:85{95, 12 2018. doi: 10.1016/j.artint.2018.\n12.002.\nHadeer Ahmed, Issa Traore, and Sherif Saad. Detection of online fake news using n-gram\nanalysis and machine learning techniques. In Proceedings of the International Conference\non Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments ,\npages 127{138, 10 2017. ISBN 978-3-319-69154-1. doi: 10.1007/978-3-319-69155-8 9.\nOluwaseun Ajao, Deepayan Bhowmik, and Shahrzad Zargari. Fake news identi\fcation\non twitter with hybrid cnn and rnn models. In Proceedings of the 9th International\nConference on Social Media and Society , SMSociety '18, pages 226{230, New York,\nNY, USA, 2018. ACM. ISBN 978-1-4503-6334-1. doi: 10.1145/3217804.3217917. URL\nhttp://doi.acm.org/10.1145/3217804.3217917 .\nAlan Akbik, Duncan Blythe, and Roland Vollgraf. Contextual string embeddings for se-\nquence labeling. In COLING 2018, 27th International Conference on Computational\nLinguistics , pages 1638{1649, 2018.\nSamir Bajaj. \\ the pope has a new baby ! \" fake news detection using deep learning,\n2017. URL https://web.stanford.edu/class/cs224n/reports/2710385.\npdf. CS 224N, Winter 2017.\nShweta Bhatt, Sagar Joglekar, Shehar Bano, and Nishanth Sastry. Illuminating an ecosys-\ntem of partisan websites. In Companion Proceedings of the The Web Conference 2018 ,\nWWW '18, pages 545{554, Republic and Canton of Geneva, Switzerland, 2018. Interna-\ntional World Wide Web Conferences Steering Committee. ISBN 978-1-4503-5640-4. doi:\n10.1145/3184558.3188725. URL https://doi.org/10.1145/3184558.3188725 .\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word\nvectors with subword information. Transactions of the Association for Computational\nLinguistics , 5:135{146, 2017. ISSN 2307-387X.\nKyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On\nthe properties of neural machine translation: Encoder-decoder approaches. CoRR ,\nabs/1409.1259, 2014. URL http://arxiv.org/abs/1409.1259 .\nJoachim Diederich, J\u007f org Kindermann, Edda Leopold, and Gerhard Paass. Authorship\nattribution with support vector machines. Applied Intelligence , 19, 07 2003. doi: 10.\n1023/A:1023824908771.\nAsif Ekbal and Sivaji Bandyopadhyay. Named entity recognition using support vector ma-\nchine: A language independent approach. International Journal of Electrical, Computer,\nand Systems Engineering , 4:155{170, 2010.\nHyperpartisan News Detection 76/80\nS. Jerril Gilda. Evaluating machine learning algorithms for fake news detection. 2017 IEEE\n15th Student Conference on Research and Development (SCOReD) , pages 110{115, 2017.\nStephan Greene and Philip Resnik. More than words: Syntactic packaging and implicit\nsentiment. In NAACL , pages 503{511, 2009.\nR. Gunning. The technique of clear writing . McGraw-Hill International Book Co, New\nYork: NY, 1952.\nSepp Hochreiter and J\u007f urgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735{1780, 1997.\nJoan B. Hooper. On assertive predicates. In J. Kimball, editor, Syntax and Semantics\nVolume 4 , volume 4, pages 91 { 124. Academic Press, New York, 1975.\nBenjamin D. Horne and Sibel Adali. This just in: Fake news packs a lot in title, uses\nsimpler, repetitive content in text body, more similar to satire than real news. CoRR ,\nabs/1703.09398, 2017.\nChristoph Hube and Besnik Fetahu. Detecting biased statements in wikipedia. In Compan-\nion Proceedings of the The Web Conference 2018 , WWW '18, pages 1779{1786, Republic\nand Canton of Geneva, Switzerland, 2018. International World Wide Web Conferences\nSteering Committee. ISBN 978-1-4503-5640-4. doi: 10.1145/3184558.3191640. URL\nhttps://doi.org/10.1145/3184558.3191640 .\nC.J. Hutto, Scott Appling, and Dennis Folds. Computationally detecting and quantify-\ning the degree of bias in sentence-level text of news stories. HUSO 2015: The \frst\ninternational conference on HUman and Social Analytics , pages 30{34, 2015.\nClayton J. Hutto and Eric Gilbert. Vader: A parsimonious rule-based model for sentiment\nanalysis of social media text. In Eytan Adar, Paul Resnick, Munmun De Choudhury,\nBernie Hogan, and Alice H. Oh, editors, ICWSM . The AAAI Press, 2014. URL http:\n//dblp.uni-trier.de/db/conf/icwsm/icwsm2014.html#HuttoG14 .\nBhumika Jahdav and Vimalkumar Vaghela. Sentiment analysis using support vector ma-\nchine based on feature selection and semantic analysis. International Journal of Com-\nputer Applications , 146:26{30, 07 2016. doi: 10.5120/ijca2016910921.\nKaren Sp\u007f arck Jones. A statistical interpretation of term speci\fcity and its application in\nretrieval. Journal of Documentation , 28:11{21, 1972.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for\ne\u000ecient text classi\fcation. CoRR , abs/1607.01759, 2016. URL http://arxiv.org/\nabs/1607.01759 .\nHyperpartisan News Detection 77/80\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Emmanuel Vincent, Payam Adineh, David\nCorney, Benno Stein, and Martin Potthast. Semeval-2019 task 4: Hyperpartisan news\ndetection. In Proceedings of the 13th International Workshop on Semantic Evaluation\n(SemEval-2019) , Minnesota, United States, June 2019. Association for Computational\nLinguistics.\nHan Kyul Kim, Hyunjoong Kim, and Sungzoon Cho. Bag-of-concepts: Comprehending\ndocument representation through clustering words in distributed representation. Neuro-\ncomputing , 266:336 { 352, 2017. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.\n2017.05.046. URL http://www.sciencedirect.com/science/article/pii/\nS0925231217308962 .\nJ.P. Kincaid, R.P Fishburne, R.L. Rogers, and B.S. Chissom. Derivation of new read-\nability formulas (automated readability index, fog count and \resh reading ease formula.\nResearch Brand Report , pages 8{75, 1975. Chief of Naval Technical Training: Naval Air\nStation Memphis.\nAlex Krizhevsky, Ilya Sutskever, and Geo\u000brey E. Hinton. Imagenet classi\fcation with\ndeep convolutional neural networks. In Proceedings of the 25th International Confer-\nence on Neural Information Processing Systems - Volume 1 , NIPS'12, pages 1097{1105,\nUSA, 2012. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?\nid=2999134.2999257 .\nDilek K\u007f u\u0018 c\u007f uk and Fazli Can. Stance detection on tweets: An svm-based approach. CoRR ,\nabs/1803.08910, 2018. URL http://arxiv.org/abs/1803.08910 .\nGuillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and\nChris Dyer. Neural architectures for named entity recognition. CoRR , abs/1603.01360,\n2016. URL http://arxiv.org/abs/1603.01360 .\nT.K. Landauer, P.W. Foltz, and D. Laham. An introduction to latent semantic analysis.\nDiscourse processes , 25:259{284, 1998.\nBing Liu, Minqing Hu, and Junsheng Cheng. Opinion observer: analyzing and comparing\nopinions on the web. In In Proceedings of WWW 2005 , pages 342{351, 2005.\nJ. MacQueen. Some methods for classi\fcation and analysis of multivariate observations. In\nL. M. Le Cam and J. Neyman, editors, Proceedings of the Fifth Berkeley Symposium on\nMathematical Statistics and Probability , pages 281{297, Berkeley, CA, 1967. University\nof California Press.\nAlice Marwick and Rebecca Lewis. Media Manipulation and Disinformation Online . Data\nand Society, 2017.\nG. McLaughlin. Smog grading | a new readability formula. Journal of Reading , 12:\n639{646, 1969.\nHyperpartisan News Detection 78/80\nThomas Mikolov. Distributed representations of words and phrases and their composition-\nality. Proc. of ICLR , pages 1{9, 01 2013.\nTomas Mikolov, Kai Chen, Greg Corrado, and Je\u000brey Dean. E\u000ecient estimation of word\nrepresentations in vector space. CoRR , abs/1301.3781, 2013. URL http://arxiv.\norg/abs/1301.3781 .\nTony Mullen and Nigel Collier. Sentiment analysis using support vector machines with\ndiverse information sources. In In Proceedings of Conference on Empirical Methods in\nNatural Language Processing , 2004.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-\ndel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in python.\nJournal of Machine Learning Research , 12:2825{2830, 2011.\nJames Pennebaker, Ryan Boyd, Kayla Jordan, and Kate Blackburn. The development and\npsychometric properties of liwc2015. Technical report, University of Texas at Austin, 09\n2015.\nJe\u000brey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors\nfor word representation. In EMNLP , volume 14, pages 1532{1543, 2014.\nVer\u0013 onica P\u0013 erez-Rosas, Bennett Kleinberg, Alexandra Lefevre, and Rada Mihalcea. Auto-\nmatic detection of fake news. In Proceedings of the 27th International Conference on\nComputational Linguistics , pages 3391{3401. Association for Computational Linguistics,\n2018. URL http://aclweb.org/anthology/C18-1287 .\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-\nton Lee, and Luke Zettlemoyer. Deep contextualized word representations. CoRR ,\nabs/1802.05365, 2018. URL http://arxiv.org/abs/1802.05365 .\nMartin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendor\u000b, and Benno Stein.\nA stylometric inquiry into hyperpartisan and fake news. CoRR , abs/1702.05638, 2017.\nURL http://arxiv.org/abs/1702.05638 .\nMartin Potthast, Tim Gollub, Matti Wiegmann, and Benno Stein. TIRA Integrate Re-\nsearch Environment. In Nicola Ferro and Carol Peters, editors, Information Retrieval\nEvaluation in a Changing World - Lessons Learned from 20 Years of CLEF . Springer,\n2019.\nMarta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. Linguistic mod-\nels for analyzing and detecting biased language. In ACL (1) , pages 1650{1659. The\nAssociation for Computer Linguistics, 2013.\nHyperpartisan News Detection 79/80\nFilipe Ribeiro, Lucas Henrique, Fabr\u0013 \u0010cio Benevenuto, Abhijnan Chakraborty, Juhi Kul-\nshrestha, Mahmoudreza Babaei, and Krishna P Gummadi. Media bias monitor: Quanti-\nfying biases of social media news outlets at large-scale. In In Proc. AAAI Intl. Conference\non Web and Social Media (ICWSM), Stanford, USA, June 2018 , 09 2018.\nMir Shahriar Sabuj, Zakia Afrin, and K. M. Hasan. Opinion mining using support vector\nmachine with web based diverse data. In International Conference on Pattern Recogni-\ntion and Machine Intelligence , pages 673{678, 11 2017. ISBN 978-3-319-69899-1. doi:\n10.1007/978-3-319-69900-4 85.\nSneha Singhania, Nigel Fernandez, and Shrisha Rao. 3han: A deep neural network for fake\nnews detection. In Proceedings of the International Conference on Neural Information\nProcessing , 11 2017. doi: 10.1007/978-3-319-70096-0 59.\nE.A. Smith and R. Senter. The automated readability index. Aerospace Medical Research\nLaboratories (6570th) , 1, 1967.\nPhilip J. Stone, Robert F. Bales, J. Zvi Namenwirth, and Daniel M. Ogilvie. The general\ninquirer: A computer system for content analysis and retrieval based on the sentence\nas a unit of information. Behavioral Science , 7:484 { 498, 10 2007. doi: 10.1002/bs.\n3830070412.\nMike Thelwall, Kevan Buckley, and Georgios Paltoglou. Sentiment strength detection for\nthe social web. J. Am. Soc. Inf. Sci. Technol. , 63(1):163{173, January 2012. ISSN 1532-\n2882. doi: 10.1002/asi.21662. URL http://dx.doi.org/10.1002/asi.21662 .\nJoseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. Word representations: A simple and\ngeneral method for semi-supervised learning. In Proceedings of the 48th Annual Meeting\nof the Association for Computational Linguistics , pages 384{394, Uppsala, Sweden, July\n2010. Association for Computational Linguistics. URL https://www.aclweb.org/\nanthology/P10-1040 .\nVladimir N. Vapnik. The Nature of Statistical Learning Theory . Springer-Verlag, Berlin,\nHeidelberg, 1995. ISBN 0-387-94559-8.\nEmmanuel Vincent and Maria Mestre. Crowdsourced measure of news articles bias: As-\nsessing contributors' reliability. In CEUR Workshop Proceedings , volume 2276, 2019.\nB. Wyse. Factive / non-factive predicate recognition within question generation sys-\ntems. Technical Report 2009/09, Department of Computing Faculty of Mathematics,\nComputing and Technology The Open University, Walton Hall, Milton Keynes, MK7\n6AA, September 2009. URL http://computing-reports.open.ac.uk/2009/\nTR2009-09.pdf .\nChao Xing, Dong Wang, Xuewei Zhang, and Chao Liu. Document classi\fcation with\ndistributions of word vectors. Signal and Information Processing Association Annual\nSummit and Conference (APSIPA), 2014 Asia-Paci\fc , pages 1{5, 2014.\nHyperpartisan News Detection 80/80\nTae Yano, Philip Resnik, and Noah A. Smith. Shedding (a thousand points of) light\non biased language. In Proceedings of the NAACL HLT 2010 Workshop on Creating\nSpeech and Language Data with Amazon's Mechanical Turk , CSLDAMT '10, pages 152{\n158, Stroudsburg, PA, USA, 2010. Association for Computational Linguistics. URL\nhttp://dl.acm.org/citation.cfm?id=1866696.1866719 .\nZi Yin and Yuanyuan Shen. On the dimensionality of word embedding. In\nS. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems 31 , pages 895{\n906. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\n7368-on-the-dimensionality-of-word-embedding.pdf .\nNurulhuda Zainuddin and Ali Selamat. Sentiment analysis using support vector machine.\nInI4CT 2014 - 1st International Conference on Computer, Communications, and Con-\ntrol Technology, Proceedings , pages 333{337, 09 2014. doi: 10.1109/I4CT.2014.6914200.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Robust document representations for hyperpartisan and fake news detection", "author": ["T Anthonio"], "pub_year": "2019", "venue": "NA", "abstract": "Hyperpartisan news is characterized by extremely one-sided content from a left-wing or right-wing  political perspective. This thesis is concerned with automatically detecting such news"}, "filled": false, "gsrank": 441, "pub_url": "http://www.ixa.eus/master/sites/default/files/filefield_paths/2066/MAL-Talita_Rani_Anthonio.pdf", "author_id": ["3xUoKkwAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:2MlzF_198nMJ:scholar.google.com/&output=cite&scirp=440&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D440%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=2MlzF_198nMJ&ei=VbWsaOTRJ7XCieoP4PfQ0A8&json=", "num_citations": 7, "citedby_url": "/scholar?cites=8354878784699353560&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:2MlzF_198nMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "http://www.ixa.eus/master/sites/default/files/filefield_paths/2066/MAL-Talita_Rani_Anthonio.pdf"}}, {"title": "What news do people get on social media? Analyzing exposure and consumption of news through data donations", "year": "2024", "pdf_data": "HAL Id: hal-04618579\nhttps://inria.hal.science/hal-04618579v1\nSubmitted on 20 Jun 2024\nHAL is a multi-disciplinary open access\narchive for the deposit and dissemination of sci-\nentific research documents, whether they are pub-\nlished or not. The documents may come from\nteaching and research institutions in F rance or\nabroad, or from public or private research centers.L\u2019archive ouverte pluridisciplinaire HAL , est\ndestin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents\nscientifiques de niveau recherche, publi\u00e9s ou non,\n\u00e9manant des \u00e9tablissements d\u2019enseignement et de\nrecherche fran\u00e7ais ou \u00e9trangers, des laboratoires\npublics ou priv\u00e9s.\nDistributed under a Creative Commons Attribution 4.0 International License\nWhat News Do People Get on Social Media? Analyzing\nExposure and Consumption of News through Data\nDonations\nSalim Chouaki, Abhijnan Chakraborty , Oana Goga, Savvas Zannettou\nT o cite this version:\nSalim Chouaki, Abhijnan Chakraborty , Oana Goga, Savvas Zannettou. What News Do People Get on\nSocial Media? Analyzing Exposure and Consumption of News through Data Donations. WWW 2024 -\nThe ACM International W orld Wide W eb Conference, May 2024, Singapore, Singapore. pp.2371-2382,\n\uffff10.1145/3589334.3645399\uffff. \uffffhal-04618579\uffff\nWhat News Do People Get on Social Media? Analyzing Exposure\nand Consumption of News through Data Donations\nSalim Chouaki\nLIX, CNRS, Inria, Ecole Polytechnique, Institut\nPolytechnique de Paris\nsalim.chouaki@inria.frAbhijnan Chakraborty\nIIT Delhi, India\nabhijnan@iitd.ac.in\nOana Goga\nLIX, CNRS, Inria, Ecole Polytechnique, Institut\nPolytechnique de Paris\noana.goga@cnrs.frSavvas Zannettou\nTU Delft, Netherlands\nszannett@mpi-inf.mpg.de\nABSTRACT\nUnderstanding how exposure to news on social media impacts\npublic discourse and exacerbates political polarization is a signifi-\ncant endeavor in both computer and social sciences. Unfortunately,\nprogress in this area is hampered by limited access to data due to\nthe closed nature of social media platforms. Consequently, prior\nstudies have been constrained to considering only fragments of\nusers\u2019 news exposure and reactions. To overcome this obstacle,\nwe present an innovative measurement approach centered on do-\nnating personal data for scientific purposes, facilitated through\na privacy-preserving tool that captures users\u2019 interactions with\nnews on Facebook. This approach offers a nuanced perspective\non users\u2019 news exposure and consumption, encompassing differ-\nent types of news exposure: selective, incidental, algorithmic, and\ntargeted, driven by the diverse underlying mechanisms governing\nnews appearance on users\u2019 feeds. Our analysis of data from 472\nparticipants based in the U.S. reveals several interesting findings.\nFor instance, users are more prone to encountering misinformation\nbecause of their active selection of low-quality news sources rather\nthan being exposed solely due to friends or platform algorithms.\nFurthermore, our study uncovers that users are open to engaging\nwith news sources with opposite political ideology as long as these\ninteractions are not visible to their immediate social circles. Over-\nall, our study showcases the viability of data donation as a means\nto provide clarity to longstanding questions in this field, offering\nnew perspectives on the intricate dynamics of social media news\nconsumption and its effects.\n1 INTRODUCTION\nOver the past two decades, there has been a sea change in how peo-\nple consume news. Earlier, people had to search and actively select\nthe news sources they would like to read from, leading to selective\nexposure [20]. More recently, with the growing popularity of social\nmedia, news has started to appear on people\u2019s social media feeds as a\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nConference\u201917, July 2017, Washington, DC, USA\n\u00a92024 Copyright held by the owner/author(s).\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM.\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnbyproduct of their social relations (i.e., posts shared by friends) and\nrecommendation algorithms (i.e., posts recommended by the plat-\nforms). In this new media environment, people are unintentionally\nexposed to news during their diverse online interactions [ 30,59,61].\nA vast literature in both social and computer science has since\nstudied the impact of such incidental exposure on public discussion\nquality (abundance of fast, junk, or fake news) [ 21,22,57], and\nto which extent it may exacerbate political polarization and filter\nbubbles [ 31,37,46]. The findings are mixed with evidence sug-\ngesting both (a) incidental exposure leads to receiving information\nfrom a significantly narrower spectrum of sources compared to web\nsearch [ 36], and (b) social media users get exposed to significantly\nmore news sources than people who do not use social media at\nall [48]. While the topic has received wide attention, research has\nbeen hindered by the lack of access to data due to the closed nature of\nonline platforms \u2013 external researchers do not have access to what\nnews users see on social media. We tackle this limitation in this\nwork by proposing a measurement methodology that can provide a\ncomprehensive picture of users\u2019 news exposure on social media and\ncapture user interactions at a finer granularity .\nThe first contribution of our paper is to present a measurement\nmethodology based on donations of personal data for scientific re-\nsearch that allows us to study exposure and consumption of news on\nsocial media in a realistic and fine-grained manner. The key to this\nmethodology is to enable users to donate their data in an automated\nand inconspicuous manner that does not disturb their regular ac-\ntivity. We enable this by building a monitoring tool CheckMyNews\nwhich is able to capture, in the background, all posts related to news\nthat appear in users\u2019 Facebook feeds, how users interact with them\n(e.g., share, click), and the news articles users read on news websites\noutside of Facebook. CheckMyNews ensures that data donations\nare pseudonymized and do not include any non-news private posts\nor data from friends. We posted about the tool and the surrounding\nconcept of data donation on Prolific [ 45]; 889 U.S. residents agreed\nto install our tool and keep it active for six weeks. For the analysis,\nwe filtered out users with minimal activity on Facebook, ending\nup with 472users exposed to 143,129 news posts during the data\ncollection period (November 2020 to February 2021).\nTo capture a wide range of news posts, we aggregated a list\nof over 12,000 U.S. news sources for our tool to monitor. We first\nconsult two independent news ecosystem auditors \u2013 Media Bias\nFact Check [ 33] and News Guard [ 35] \u2013 who list a total of 4,149\nConference\u201917, July 2017, Washington, DC, USA Salim Chouaki, Abhijnan Chakraborty, Oana Goga, and Savvas Zannettou\nnews domains. We further develop a method to discover an addi-\ntional 8,084 under-the-radar news sources not listed by journalistic\nauthorities but that claim to be news organizations on Facebook. To\nour knowledge, this is the most extensive compilation of U.S. outlets\nclaiming to be news organizations .\nThesecond contribution of the paper is to use the realistic andfine-\ngrained representation of users\u2019 news exposure and consumption\nto answer several longstanding questions about social media news\nthat have been only partially answered till now, due to the lack\nof access to data. First, we measure the political diversity of users\u2019\nnews diets on Facebook \u2013 how much they are exposed to a varied\npolitical spectrum. Second, we assess the quality of news users\nreceive and how prevalent misinformation is on their feeds. Finally,\nwe measure how users engage with news and the extent to which\nexposure to news transforms into real consumption .\nContrasting (some) previous works [ 9,48], we argue in this pa-\nper that we need to move beyond treating the news exposure of\nusers on social media as a whole, as there are different underlying\nmechanisms through which news appears on users\u2019 Facebook feeds.\nFirst, users can choose to follow news media outlets on Facebook,\nand as a result, they will see news posts from these outlets in their\nfeeds. This constitutes a form of selective news exposure . Second,\nusers can see news posts in their feeds because their friends or\ncommunities share them. This is classically considered as inciden-\ntal news exposure . Third, some news posts appear in users\u2019 feeds\nbecause the platform predicts that the users might be interested\nin such posts based on their past behavior. These posts are labeled\nas \u201cSuggested for you\u201d posts on Facebook. We call this algorithmic\nnews exposure . And finally, much less realized by the community,\nthere is a fourth type \u2013 the targeted news exposure . Targeted news\nexposure is brought by the emergence of online advertising where\nself-interested third parties can pay ad platforms to show specific news\nto particular groups of people .\nTo understand the impact of these different types of exposure,\nwe assess the diversity, quality, and consumption of news per un-\nderlying mechanisms and make the following observations:\n\u2022On average, 5.1%of news posts users encounter on Facebook\nare from sources known to post misinformation repeatedly. When\nexamining each category separately, we find that selective news\nexposure has the highest fraction ( 5.8%) of posts from misinforma-\ntion sources, while targeted news exposure has the lowest fraction\n(2.5%). These results suggest that users are more likely to expose\nthemselves to sources known to spread misinformation than be\nexposed to them due to their friends or the platform\u2019s algorithms.\n\u2022Targeted, algorithmic, and incidental news exposures are signifi-\ncantly more politically balanced than selective exposure, indicating\nthat while users actively subscribe to news sources of the same\npolitical leaning, they get exposed to sources from the opposite\npolitical leaning through other mechanisms on Facebook.\nWe further analyze three types of interactions with news posts:\n(i)visible interactions like commenting, sharing, or liking a post that\nare visible to other Facebook users, (ii) hidden interactions that are\nnot visible to a user\u2019s friends, such as clicking on the post to visit\nthe actual article, clicking on the Facebook page of the publisher,\nor saving the post, and (iii) visibility time that captures the time a\npost was visible on the screen of the user.\u2022We find that users interact with only 5.1%of news posts they see\non Facebook (visible interactions on 2.6%and hidden interactions on\n2.8%of news posts). Users had both visible and hidden interactions\non<0.5%of posts. Interestingly, users accessed the landing URLs\nof only 14%of the news posts they shared. This indicates that users\nmostly limit themselves to reading the news post\u2019s text and/or images\nbefore sharing rather than reading the actual news article .\n\u2022While the fraction of visible interactions on selective and inci-\ndental news posts is close to the fraction of hidden interactions, the\nfraction of visible interactions on algorithmic and targeted news\nposts is 1.8 to 6 times lower than the fraction of hidden interactions.\nThis suggests that algorithmic and targeted exposure might have\nan inhibitory effect on users\u2019 willingness to share or comment on\nnews posts publicly.\n\u2022Finally, we observed more visible interactions on posts published\nby sources with a matching political ideology than the opposing\nideology of a user. For instance, Republicans interacted with 3.1%of\nposts from the right-leaning media vs. 1.8%from left-leaning ones;\nwhereas Democrats interacted with 3.9%of posts from left-leaning\nsources vs. 3.1%from right-leaning ones. Surprisingly, when it\ncomes to hidden interactions, we observed that both Republicans\nand Democrats interacted more with posts with opposing political\nideology. For example, Republicans interacted with 4.1%of posts\nfrom left-leaning sources vs. 2.3%from right-leaning media, while\nDemocrats interacted with 2.9%of posts from right-leaning sources\nvs.2.4%from left-leaning ones. This suggests that users are indeed\nwilling to engage with opposing views, albeit in a private manner .\nBesides answering multiple open questions in the literature re-\nlated to how users interact with news posts, our work shows that\ndata donations from social media users are both feasible and critical\nin uncovering the impact of current technologies on society, as\nwell as for the advancement of scientific research. Our codebase is\npublicly available1to be audited and to encourage other researchers\nto build on this methodology.\nRelated Work. Prior works have attempted to reconstruct users\u2019\nnews exposure using computational [3,4] and survey-based meth-\nods[7,20,38]; but both approaches provide only an incomplete\npicture. For example, one computational approach suggests using a\nuser\u2019s public activities (e.g., all news articles they publicly shared\nor commented on Twitter or Facebook) to reconstruct their news\nexposure [ 3,10,22]. This approach introduces two kinds of biases.\nFirst, it can only consider users who publicly share content, while\nprior studies have shown that only a small fraction of users take\nsuch explicit actions [ 60]. Second, it can only observe the content\nusers are comfortable sharing publicly. A newer computational\napproach consists of analyzing users\u2019 web browsing history (e.g.,\nthe news articles a user has clicked on) [ 6,18,23\u201325,32,39]. While\nthis approach addresses previously mentioned limitations, it still\nprovides data only on a subset of news articles users are exposed to\n(i.e., the ones they subsequently click on). We tackle these limita-\ntions in this work by adopting a measurement methodology based\non data donation, providing a comprehensive picture of the news\nlandscape on social media.\n1https://anonymous.4open.science/r/CheckMyNews-AE8B\nWhat News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations Conference\u201917, July 2017, Washington, DC, USA\n2 METHODOLOGY AND DATASET\nOur measurement methodology consists of building a non-intrusive\ntool enabling people to donate data about the news content they see\non Facebook. In this section, we describe the design and technical\nconsiderations of the measurement infrastructure.\n2.1 Monitoring tool\nTo enable users to donate data about the content they encounter,\nin a manner that does not disturb their regular activity, we imple-\nmented CheckMyNews, a privacy-preserving browser extension for\nGoogle Chrome that automatically collects, in the background, data\nabout the content users see when browsing Facebook. The browser\nextension collects the following information from users:\ni.News posts: CheckMyNews detects and collects posts related to\nnews. To detect these posts, we compiled an extensive list of over\n12,000 news outlets and the Facebook pages with which they are\nassociated (see Section 2.2 for details on how we compile this list).\nWe consider a Facebook post as a news post if: (a) it was published by\na page from our list of Facebook pages or (b) the landing URL of the\npost points to one of the news outlets we have in our list. A news\npost can be published or shared by the Facebook page of a news\noutlet or a random Facebook profile or page. A news post can be\nprivate (only a limited group of users can see it) or public (all users\ncan see it). CheckMyNews collects the text, the media (e.g., image,\nvideo), the publisher, and the landing URL if the news post is public.\nOn the contrary, if the news post is private, CheckMyNews only\ncollects the landing URL and a hashed version of the publisher\u2019s\nusername to keep it pseudonymous.\nii.Other posts: To evaluate the coverage of our list of news outlets\nand extend it, upon user permission, CheckMyNews can collect the\nnon-news public andnon-news targeted posts users see in their feeds,\nin addition to the news posts. This data is also useful to calculate\nthe proportion of news posts among all the posts received by users,\nand compare how users interact with news posts vs. other posts.\nNote that CheckMyNews does not collect any private non-news\nposts (see Appendix A.1 for more details).\niii.Visibility time of posts: CheckMyNews collects how much time\nthe post was visible on the user\u2019s screen for every post received\non Facebook. For this, we check what post is visible on the user\u2019s\nscreen every 0.5seconds, and we start a time counter each time a\nnew post becomes visible. The timer counts as long as more than\n30%of the post is still visible.\niv.Interactions with posts: CheckMyNews collects both visible and\nhidden interactions of users with all the collected posts. The visible\ninteractions are actions such as whether the user liked, disliked,\ncommented, or shared a post. The hidden interactions are actions\nsuch as visiting the landing URL of the post, clicking on one of the\nimages of the post, or checking who is the publisher of the post by\nvisiting their Facebook profile. The hidden interactions are invisible\nto friends, while visible interactions are visible to friends.\nv.Survey data: CheckMyNews has an option to send surveys to\nthe user panel. It allowed us to request the participants to volun-\ntarily disclose their demographic information, such as, gender and\npolitical affiliations (Democrat, Republican, or Independent).\nCheckMyNews identifies users using a one-way hashed version\nof their Facebook IDs; their PIIs, such as names, usernames, oremails, are never sent to our servers. Overall, we have collected\nthe news posts received by users on Facebook, their visibility time,\nand how users interact with them. To our knowledge, none of\nthe prior works have looked at such granular information. This\nis partly due to the difficulty of collecting such data as Facebook\nchanges its HTML markup, sometimes adversarially, to disturb data\ncollections from tools such as ours [ 27]. We devoted considerable\ntime to developing a monitoring tool that can enable reliable data\ndonation (see Appendix A.2 for more details).\n2.2 Extended lists of news outlets\nThe list of news outlets we monitor determines the breadth of our\nview of news posts that users are exposed to and consume. To\nhave a comprehensive view, we need an extensive list of news outlets\nto monitor . However, most previous studies have only monitored\na limited number of traditional news media [ 24,31,48], and the\nresearch community lacks a comprehensive list of news outlets\nactive on social media. To overcome the limitation, we employ\nthree approaches.\nFirst, we rely on News Guard and Media Bias Fact Check, two\nindependent data providers that survey news outlets and provide\nqualitative information about them (e.g., political leaning, quality).\nNews Guard contains 2,939 news sites, while Media Bias Fact Check\ncontains 2,062 news sites. We call the aggregate list of 4,149 news\noutlets the Established News Sites list . For every news outlet,\nwe also collected the corresponding Facebook page. Overall, we\nhave a list of 4,323 Facebook pages corresponding to the established\nnews domains, which we call Established Facebook Pages list .\nWe know from recent reports that there is an emergence of\nsites that claim to be news organizations, especially before elec-\ntions [ 43]. Hence, it is essential to go beyond established media\nsites and consider news outlets that simply claim to be news media,\nirrespective of their reputation, popularity, or whether they create\noriginal content or are simply content farms. We refer to these sites\nasunder-the-radar news outlets, and we hypothesize that some use\nFacebook to advertise their content, as they probably have a small\norganic reach. Hence, our second approach consists of grabbing\nfrom the Facebook Political Ad Library [ 17] all Facebook pages that\npromoted a political ad in the U.S. between June 2018 and June 2020\nand claim to be \u201cNews Media.\u201d Then, for every Facebook page, we\nextract the website mentioned in the \u201cAbout\u201d page. Although not all\nFacebook pages mention a website, we could gather 8,084 websites\nfor 9,679 different Facebook pages. Hence, this method provides us\nwith a significant number of sites claiming to be news outlets that\nNews Guard and Media Bias Fact Check have not reviewed.\nThird , as mentioned earlier, CheckMyNews can collect the tar-\ngeted and public posts on users\u2019 feeds. Most of these posts come\nfrom Facebook pages. Hence, we select all Facebook pages that\nclaim to be \u201cNews Media\u201d in their \u201cAbout\u201d section and extract the\nnews domain if they mention it. This method gave us 404additional\nnews domains associated with 449Facebook pages.\nIn total, our list of under-the-radar news media sites (collected\nwith the second and third methods) contains 8,489 different do-\nmains associated with 10,128 different Facebook pages. We call the\ncorresponding lists the Under-the-Radar News Sites list and\ntheUnder-the-Radar Facebook Pages list . Overall, our list of\nConference\u201917, July 2017, Washington, DC, USA Salim Chouaki, Abhijnan Chakraborty, Oana Goga, and Savvas Zannettou\nnews media websites contains 12,638 different news domains and\n14,451 Facebook pages. To the best of our knowledge, this is the\nmost extensive list of (both established and under-the-radar) news\ndomains in the US.2\n2.3 Ethical considerations\nCheckMyNews collects sensitive and personal data from the study\u2019s\nparticipants. Before collecting any data, we obtained the necessary\napprovals from the Data Protection Officers and the Ethical Re-\nview Board of our institution and the participants\u2019 explicit consent.\nAdditionally, we use various strategies to minimize security and\nprivacy risks for the study\u2019s participants, and we comply with the\nEU General Data Protection Regulation (GDPR). We provide more\ndetails about our ethical considerations and security and privacy\nrisk minimization strategies in Appendix A.3.\n2.4 User recruitment and dataset\nWe posted about the tool and the surrounding concept of data do-\nnation on Prolific, and 889 users living in the U.S. agreed to install\nCheckMyNews on the computer they use to connect to Facebook\nand keep it active for six weeks (between November 2, 2020, and\nFebruary 15, 2021, centered around the U.S. presidential elections).\nWhile we can not guarantee representativeness, we tried to reach\nout to Prolific users across various U.S. states and ethnicities. We\nprovide a detailed breakdown of age, location, ethnicity, and politi-\ncal affiliation as reported by the users in Appendix B.2.\nSome users dropped out from the study and uninstalled the ex-\ntension in between (see Appendix B.1). Hence, our analysis focuses\non data from 472users who spent at least 30minutes browsing\nFacebook or received more than ten news posts in their feeds. We\nfound that all 472users had typical browsing activity on Facebook,\nwith a reasonable number of posts with respect to the time spent\non Facebook. Hence, it is unlikely that any of these users are bots\nbrowsing on Facebook (see Appendix C.2).\nOur dataset contains 143,129 news posts. For each user, on aver-\nage, we collected 303news posts ( \ud835\udc40(\ud835\udc5a\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc5b)=57,\ud835\udc60(\ud835\udc60\ud835\udc61\ud835\udc51)=789)\nand 6news posts per active day ( \ud835\udc40=3,\ud835\udc60=10). Moreover, we have\ncollected a total of 8,612 user interactions with 5,386 different news\nposts. Each user interacted on average with 18posts (\ud835\udc40=2,\ud835\udc60=60).\n(see Appendix C.1 for more statistics).\n2.5 Limitations\nDespite our best efforts, our data collection methodology has two\nmain limitations. First, our monitoring tool cannot capture news ex-\nposure and consumption on users\u2019 mobile phones. Sadly, providing\nsuch a tool for mobile phones is technically very challenging. There\nis currently no data suggesting that Facebook news diets on mobile\nphones significantly differ from news diets on web browsers in\nterms of composition, quality, and diversity of news posts. Second ,\nsince the study requires the collection of personal data from users,\nunderstandably, many users are uncomfortable with donating data,\nand thus, they would be reluctant to install a monitoring tool. This\nmakes it challenging to obtain a large and representative sample of\nusers. Despite the limitations, we believe that the compiled dataset\n2Downloadable at anonymous.4open.science/r/US_News_Outlets_Dataset-342F\nT argeted\n news postsSelective\n news postsIncidental\n news postsAlgorithmic\n news posts020406080100Proportion of posts (%)\nFigure 1: The distribution of the proportion of selective, in-\ncidental, targeted, and algorithmic news posts received by\neach user in our dataset.\ngoes much beyond previously compiled datasets in terms of com-\nprehensiveness and detail, and provides much-needed answers to\nseveral long-standing questions in the community.\n3 WHAT NEWS DO PEOPLE GET ON SOCIAL\nMEDIA?\nThis section attempts to answer longstanding questions about the\nexposure and consumption of news that appear on users\u2019 social me-\ndia feeds. We answer these questions by examining the underlying\nmechanisms responsible for news appearing in users\u2019 feeds.\n3.1 Types of Social Media News Exposure\nPrior works have looked at exposure to news in social media in silos.\nFew works have focused on incidental exposure [ 20,48,56], while\nothers on selective exposure [ 9]. For instance, [ 20,56] considered\nthat incidental exposure consists of all news users encounter on\nsearch engines or social media platforms when they use them for a\npurpose different than seeking news; whereas [ 9,34] considered\nthat all tweets a user potentially receives are selective news. Both\ninterpretations are justifiable, even if they seem contradictory.\nWe argue, however, that we need to differentiate social media\nexposure based on the underlying mechanisms responsible for the\nnews appearing in users\u2019 feeds, especially if we want to characterize\nthe consumption pattern and the quality and diversity of news.\nSuch a systematic perspective allows a better understanding of root\ncauses and finding potentially better technological and algorithmic\ndesigns. We identify the following four mechanisms through which\nnews posts (and posts in general) can appear on users\u2019 feeds.\nA. Selective news exposure : On Facebook, users can follow or\nlike the pages of their preferred news media sites. This enables them\nto receive posts published by these pages in their feeds. If a user no\nlonger wants to receive such content from a particular Facebook\npage, they can unfollow it. Hence, the user controls which posts\nfrom which news outlets appear in their feeds \u2013 this is a form of\nselective news exposure.\nB. Targeted news exposure : Advertisers can pay the Facebook ad\nplatform to show specific content in the feeds of users who satisfy\ndistinct characteristics (i.e., targeting criteria). While most of the\nads users see are for commercial products, these targeting mech-\nanisms are increasingly being used for political propaganda [ 26]\nand propagating news items. For example, an advertiser can pay\nFacebook to show ads containing links to a specific news article to\nan audience \u201cinterested in climate change, living in San Francisco.\u201d\nWhat News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations Conference\u201917, July 2017, Washington, DC, USA\nThese news posts appear in users\u2019 feeds because advertisers want\nthem. Advertisers on Facebook can utilize 250\ud835\udc3e+attributes to de-\nfine their audiences, which allows them to expose people with very\nprecise interests to particular news stories [1].\nC. Algorithmic news exposure : Facebook users receive \u201cSug-\ngested for You\u201d posts on their feeds. These are personalized posts\nthat Facebook\u2019s algorithms determine to be relevant for users, pri-\nmarily based on their previous engagement and behavior on Face-\nbook [ 16]. Note that these posts are neither shared by friends nor\npaid by advertisers. While not all \u201cSuggested for You\u201d are news-\nrelated, a fraction of them could be.\nD. Incidental news exposure : Finally, all other posts with links to\nnews articles, which are not targeted and do not result from a user\nfollowing a news page, form incidental news exposure. It includes\n(a) posts from friends, groups, or pages that either directly share or\nre-share posts with a link to a news article and (b) posts from news\nsources\u2019 Facebook pages that share links to other news sites.\nAppendix C.3 describes in more detail how we technically identify\nthe four types of exposure in our dataset.\nThe first elemental question we ask is what proportion of news\nexposure on social media is selective, incidental, algorithmic, or tar-\ngeted . Figure 1 shows the distribution of the proportion of selective,\nincidental, algorithmic, and targeted news posts received by users.\nMedian values across all users show that a user\u2019s Facebook feed\ncontains 50%of incidental, 24%of selective, 4%of algorithmic, and\n9%of targeted news posts. It is noteworthy that only 38%,13%, and\n11% of users did not receive algorithmic, targeted, and selective\nnews during the study period. Hence, most users follow different\nnews providers; they are targeted with news by advertisers and\nare exposed to algorithmic news on Facebook, resulting in all four\ntypes of exposure being prevalent across users.\nMoreover, we find that the ratio of targeted, selective, incidental,\nand algorithmic news posts varies with time (see Figure 8 in Ap-\npendix C). Precisely, we find that targeted news posts reached their\nhighest proportion at two different periods: (i) at the beginning of\nNovember, coinciding with the general election day on November\n3\ud835\udc5f\ud835\udc51, and (ii) at the end of December 2020, coinciding with the vote of\nthe Electoral College members on December 14\ud835\udc61\u210e[12]. Hence, the\nstudy participants were targeted with more news advertisements\nduring these two sensitive periods.\nThese results indicate that major offline events might directly\nimpact the composition of news on social media, emphasizing the\nneed to distinguish and analyze each type of exposure separately.\nPrecisely, the fact that the users were targeted more around two\nimportant dates during the U.S. elections is particularly alarm-\ning considering that some news publishers are exempt from the\nFacebook ad authorization process when targeting U.S. users with\npolitical advertisements, and their ads are not labeled as related to\npolitics and are not listed in the Facebook Ad Library [ 15]. This\nopens up the possibility of voter manipulation through targeted\nadvertisements in a stealth mode, evading scrutiny.\n3.2 Quality of Social Media News\nThis section assesses the quality of news users receive on Facebook\nand how prevalent misinformation is on their feeds. We characterize\nthe quality of a news post at the source level, i.e., we consider a postAll\nnews\npostsTargeted\nnews\npostsSelective\nnews\npostsIncidental\nnews\npostsAlgorithmic\nnews\nposts\nFactual\nnews sources63% 52% 64% 66% 64%\nMisinformation\nnews sources5.1% 2 .5% 5 .8% 5 .0% 4 .5%\nUnder-the-radar\nnews sources13% 32% 16% 7% 12%\nTable 1: Fraction of posts from factual, misinformation, and\nunder-the-radar news sources across all users. The table ex-\ncludes posts from under-the-radar sources and posts from\nestablished sources rated as Mixed.\nto have the same quality as the media source publishing the news.\nRecall that we collect posts from both news organizations reviewed\nby Media Bias Fact Check and News Guard ( Established News\nSites list ) and news from under-the-radar websites associated\nwith Facebook pages claiming to be news providers ( Under-the-\nRadar News Sites list ). Based on the available information, we\nuse three indicators to evaluate the quality of a news source:\n(a) Whether it is considered as repeatedly spreading misinformation\nand conspiracy theories by either Media Bias Fact Check or News\nGuard. Out of the 4,149 domains in the Established News Sites\nlist,456(11%) are considered low-quality (i.e., repeatedly spreading\nmisinformation). These domains are associated with 467Facebook\npages (see Appendix C.4 for details on the misinformation ratings\nprovided by Media Bias Fact Check and News Guard).\n(b) Whether it is an under-the-radar news source not covered by\nMedia Bias Fact Check and News Guard. While not all under-the-\nradar sites necessarily spread low-quality information, we know\nthat advocacy groups have created under-the-radar news sources\nshortly before the U.S. elections to spread information (or misinfor-\nmation) [44]. Hence, we consider them as potentially suspicious.\n(c) Whether it is considered as spreading mostly factual information\nby Media Bias Fact Check or News Guard. There are 2,942 such news\nsources, that correspond to 3,074 Facebook pages (see Appendix C.4\nfor more details on the factualness ratings provided by Media Bias\nFact Check and News Guard).\nThere are 723domains in the Established News Sites list that\nare considered to be sharing mixed content (factual and misinfor-\nmation) and 28domains for which Media Bias Fact Check and News\nGuard do not have a quality evaluation.\nTable 1 represents the fraction of posts from factual, misinforma-\ntion, and under-the-radar sources across all users. It shows that 5.1%\nof news posts users are exposed to are from news sources known\nfor repeatedly spreading misinformation or conspiracy theories.\nAdditionally, we observe that selective exposure has the highest\nrate of posts from these low-quality sources ( 5.8%), followed by inci-\ndental ( 5.0%), algorithmic ( 4.5%), and targeted exposure ( 2.5%). The\ndifferences are statistically significant (Pearson\u2019s chi-squared [ 41];\n\ud835\udc5d<0.001), and hence, it seems that users are more likely to expose\nthemselves to sources known to spread misinformation than be\nexposed through their communities (i.e., incidental exposure) or\nplatform\u2019s algorithms (i.e., algorithmic exposure). This is a new and\nintriguing observation.\nConference\u201917, July 2017, Washington, DC, USA Salim Chouaki, Abhijnan Chakraborty, Oana Goga, and Savvas Zannettou\nAll\nnews postsTargeted\nnews postsSelective\nnews postsIncidental\nnews postsAlgorithmic\nnews posts\n83 .8% 70 .7% 47 .0% 81 .5% 71 .0%\nTable 2: Fraction of users with diverse news diets, who were\nexposed to posts from both left and right-biased sources.\nDespite the prevalence of misinformation, on average, 63%of\nnews users receive are from factual news sources. Incidental, se-\nlective, and algorithmic exposures have higher rates of posts from\nmostly factual sources (66%, 64%, and 64%, respectively) compared\nto targeted news exposure (52%). On the other hand, 13%of news\nreceived by users are published by under-the-radar Facebook pages\nor have URLs to under-the-radar news sites. Targeted exposure has\nthe highest rate of posts from under-the-radar news sources ( 32%),\nfollowed by selective ( 16%), algorithmic ( 12%), and incidental expo-\nsure ( 7%). This indicates that targeted advertising is the main driver\nthat exposes users to under-the-radar content and may represent a\nthreat to the quality of news diets.\n3.3 Diversity of Social Media News\nAn important question that has captured attention is whether users\u2019\nonline news exposure is politically diverse. Prior works have pre-\nsented conflicting evidence, uncovering both the presence and ab-\nsence of so-called \u201cfilter bubbles\u201d [ 36,48]. We revisit this question\narmed with information about actual news exposure of users, in con-\ntrast to prior works\u2019 reliance on approximations: user surveys [ 40],\nweb browsing histories [ 5,48], or the social network (e.g., tweets\nfrom accounts followed by a user) [9].\nWe attempt to provide a realistic diversity landscape as our data\ncollection includes (i) a complete and precise list of the news posts\nusers have received/seen on Facebook, and (ii) reliable labeling for\nnews sources\u2019 political leanings provided by Media Bias Fact Check\nand News Guard (see Appendix C.4 for details). 86% of all news\nposts in our dataset were published by or with landing URLs to\nestablished news sources having the corresponding political leaning\nlabels. We only consider these posts for our analyses. Furthermore,\nfor each category of news posts, we only consider users who have\nreceived at least 10 news posts, resulting in 232, 351, 140, and 145\nusers for selective, incidental, targeted, and algorithmic news posts\nrespectively (414 users if we consider all categories).\nSimilar to [ 19], we use two different metrics to measure political\ndiversity. The first metric is diversity which captures whether\na user has been exposed to news sources from both sides of the\npolitical spectrum (left and right). It is a binary metric, taking the\nvalue of 1 if a user received at least one post from a left-leaning\nnews source and at least one post from a right-leaning news source.\nThe second metric, termed balance , focuses on the ratio of left vs.\nright-leaning sources (or right vs. left-leaning sources, if the latter\nis higher) that a user encounters. It is a measure of the proportion\nof posts from left sources compared to right sources (or vice versa)\nto which a user was exposed. It varies between 0 and 1, where\n0 signifies that a user did not receive any posts from one of the\npolitical leanings (left or right), 0.5 indicates that a user received\ntwice as many posts from one leaning vs. another, and 1 indicates\nthat a user received an equal number of posts from both sides.\nOverall T argeted Selective Incidental Algorithmic0.00.20.40.60.81.0Balance\nFigure 2: Distribution of balance: fraction of news posts from\nleft vs. right-leaning sources (or right vs. left if the latter is\nhigher) per user. Value 0 represents users who did not receive\nany posts from one of the political leanings (left or right); 0.2\nrepresents users who received1\n5th posts from one political\nleaning compared to the other; while a value of 1 indicates\nan equal number of posts from left and right news sources.\nWe calculate the values for these two metrics for each user, con-\nsidering all categories of news posts together as well as separately.\nTable 2 presents the proportion of users with diverse news feeds.\nWe observe that 83.8%of users received at least one post from both\nsides of the political spectrum. When considering targeted, inciden-\ntal, and algorithmic news posts separately, most users have received\npolitically diverse news for these three categories. However, when\nit comes to selective news posts, more than half of the users did\nnot encounter even a single story from the other side. These results\nindicate that users mainly subscribe to news sources of the same\npolitical leaning (selective exposure) but get exposed to sources\nfrom the opposite side of the political spectrum through incidental,\ntargeted, and algorithmic news exposure.\nFigure 2 presents the diversity of news exposure captured through\nthebalance metric. We observe that algorithmic and targeted news\ndiets are the most balanced ones, followed by incidental news di-\nets, while selective news diets are the least balanced ones. If we\nconsider a news diet to be well-balanced when the balance metric\n\u22650.5(i.e., at least one-third of posts are from each ideological\nleaning), we find that 28% and 22% of users have well-balanced\nalgorithmic and targeted news diets, while only 17%and 13%of\nusers have well-balanced incidental and selective news diets. These\nobservations are consistent with \u201cdiversity\u201d results supporting that\nalgorithmic, targeted, and incidental news exposure leads users to\nmore balanced news diets than selective exposure alone.\n3.4 Consumption of Social Media News\nCheckMyNews captures how users interact with news posts, in-\ncluding the time a post was visible on users\u2019 screens, whether they\nread the corresponding news article, checked the publisher\u2019s page,\nand commented, liked, or shared it with their friends.\n3.4.1 Visible vs. hidden interactions. There are two types of inter-\nactions: visible and hidden. Visible interactions are actions visible to\nother Facebook users (including a user\u2019s friends), such as comment-\ning, sharing, or liking a post. Hidden interactions are interactions\nthat are not visible to a user\u2019s friends, such as clicking on the post to\nvisit the actual article, clicking on the page of the publisher, clicking\non the image of the post, saving, reporting, or hiding the post. We\nWhat News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations Conference\u201917, July 2017, Washington, DC, USA\nmake this distinction because users might behave differently when\ntheir actions are visible to their friends (e.g., they might click on a\nnews post but may not want their friends to know). Interestingly,\nwe find that users performed visible interactions on only 2.6%and\nhidden interactions on only 2.8%of the news posts they received \u2013 a\ntiny minority of news they get exposed to on Facebook. We further\nsee that users performed both hidden and visible interactions on\n<0.5%of posts, suggesting that the visible and hidden interactions\nare performed mostly on different sets of posts. In fact, we find that\nusers accessed the landing URLs of only 14%of the news posts they\nhave shared. It is a very surprising finding hitherto unreported in\nany prior work.\nAnother essential information CheckMyNews captured is the\ntime a particular news post was visible on the user\u2019s screen. To\naccount for cases where a user has stopped scrolling and moved\naway while a Facebook post is visible on their screen, we use the\ninterquartile range to detect large values and exclude them from\nthe analysis. In addition, CheckMyNews collects the visibility times\nof non-news content to use them as a reference point. We classify\nnon-news content into non-news ads (i.e., posts with a \"Sponsored\"\ntag) and non-news posts (from friends or communities). We find that\nthe visibility time of news posts on user\u2019s screens (median 5.6seconds)\nis higher compared to the visibility time of non-news ads (median\n4.0seconds) and non-news posts (median 4.1seconds) (Kolmogorov-\nSmirnov [ 8];\ud835\udc5d<0.001). While we can quantitatively measure how\nlong a post was visible, we do not know if users spend more time\non news posts because they find them more interesting or if the\ncognitive load of reading a news post is higher than the cognitive\nload of other posts. Nevertheless, even though users do not click or\nreact to most news posts, they do see them for an amount of time\nlonger than other posts. Hence, this raises an important question\nfor future work: to what extent does reading a news post without\ngoing to the landing URL affect a user\u2019s beliefs and knowledge, and\nhow long do users need to look at a post to remember it?\n3.4.2 Effect of the type of exposure. We next investigate whether\nusers interact differently with selective, incidental, algorithmic, and\ntargeted news posts. Figure 3 shows the fraction of news posts on\nwhich users make visible and hidden interactions as well as the\nmedian visibility time of news posts on user\u2019s screens. The figure\nshows significant differences in how users interact with selective,\nincidental, algorithmic, and targeted news posts (Pearson\u2019s chi-\nsquared;\ud835\udc5d<0.001for both visible and hidden interactions, and\nKolmogorov-Smirnov; \ud835\udc5d<0.001for visibility time). While the\nfraction of visible and hidden interactions is similar for selective\nand incidental news posts; the fraction of hidden interactions is\n1.8 to 6 times higher than visible interactions on algorithmic and\ntargeted news posts. It may be possible that users perceive the\nunderlying mechanisms through which the news posts appear in\ntheir feeds differently, and this might inhibit visible interactions\non algorithmic and targeted news posts (e.g., users might avoid\nsharing an article that was recommended to them by Facebook\u2019s\nalgorithms, while they will share more freely an article that comes\nfrom their friends). We leave a causal validation of this hypothesis\nthrough a randomized controlled trial as future work.\n3.4.3 Impact of news source quality. Next, we investigate whether\nusers interact differently with news posts from sources of varying\n05\nTime (s)4.25.9 6 6\nT argeted\n news postsSelective\n news postsIncidental\n news postsAlgorithmic\n news posts024Proportion of posts (%)0.32.23.4\n2.7\n1.82.23.34.8 Visible interactions\nHiddent interactionsSelective, incidental, algorithmic and targeted news posts\n05\nTime (s)66.7\n5.3\nFactual\n news sourcesMisinformation\n news sourcesUnder-the-radar\n news sources024Proportion of posts (%)2.62.4\n1.42.72.93.2Visible interactions\nHiddent interactions\nPosts from factual, misinformation and under-the-radar\nsources\n05\nTime (s)76.4 6 6.2\nRepublicans on\nLeft SourcesRepublicans on\nRight SourcesDemocrats on\nLeft SourcesDemocrats on\nRight Sources0.02.55.07.5Proportion of posts (%)1.83.13.9\n2.44.1\n2.33.1 2.9Visible interactions\nHiddent interactions\nPosts from left/right sources received by democrats and re-\npublicans\nFigure 3: Fraction of news posts on which users make visible\nand hidden interactions as well as the median visibility time\nof news posts on user\u2019s screens.\nquality. We split news posts into three categories: (a) news posts\nfrom the Established News Sites list that News Guard and Media\nBias Fact Check consider to be factual, (b) news posts from the\nEstablished News Sites list that these two agencies consider to\nbe repeatedly spreading misinformation, and (c) news posts from\ntheUnder-the-Radar News Sites list .\nFigure 3 presents the consumption statistics for factual, mis-\ninformation, and under-the-radar news posts. The figure shows\nthat news posts from misinformation news sources have a longer\nvisibility time on users\u2019 screens (median: 6.7 seconds) than news\nposts from factual news sources (median: 6.0 seconds) (Kolmogorov-\nSmirnov; p < 0.001). This result is intriguing and raises questions on\nwhether a different cognitive process gets triggered when faced with\nmisinformation compared to factual news sources (e.g., they spend\nmore time to be sure about the factuality) . We leave the analysis of\nthe root cause of this observation as future work.\nAdditionally, users generally pay less attention to news posts\ncoming from under-the-radar news sources (median: 5.3 seconds).\nFurthermore, while the fraction of visible and hidden interactions is\nConference\u201917, July 2017, Washington, DC, USA Salim Chouaki, Abhijnan Chakraborty, Oana Goga, and Savvas Zannettou\nnot significantly different for posts from factual and misinformation\nsources, the fraction of visible interactions on under-the-radar posts\nis significantly lower than the fraction of hidden interactions.\n3.4.4 Impact of concurrence in political ideology. Finally, we in-\nvestigate how users interact with posts with similar or opposite\npolitical leaning. In our survey, we asked users about their political\nleaning (Republican, Lean Republican, Independent, Lean Demo-\ncrat, Democrat, or Other). We consider four sets:\n(a) posts from right-leaning sources received by Democrats,3\n(b) posts from right-leaning sources received by Republicans,4\n(c) posts from left-leaning sources received by Democrats, and\n(d) posts from left-leaning sources received by Republicans.\nFigure 3 shows the consumption behavior. When considering\nviewing time, we do not observe statistically significant differences\nbetween these four scenarios, however, the difference begins to\nemerge when we consider visible and hidden interactions (Pearson\u2019s\nchi-squared; p < 0.001). The figure expectedly shows that users per-\nform more visible interactions on posts published by sources with\na matching political ideology (Republicans interact with 3.1%of\nposts from right-leaning sources vs. 1.8%from left-leaning sources;\nDemocrats interact with 3.9%of posts from left-leaning sources vs.\n3.1%from right-leaning sources). However, when it comes to hidden\ninteractions, we observe that users interact more with posts from\nsources with the opposite political ideology (Republicans interact\nwith 4.1%of posts from left sources vs. 2.3%from right sources;\nDemocrats interact with 2.9%of posts from right sources vs. 2.4%\nfrom left sources). This provides a hopeful insight into users\u2019 online\nbehavior, which attests to their willingness to engage with opposing\nviews, albeit in stealth mode.\n4 CONCLUDING DISCUSSION\nIn this work, we attempted to provide a realistic view of users\u2019\nnews exposure and consumption on Facebook. Our methodology\nconsists of building a non-invasive monitoring tool that can allow\na large number of Facebook users to donate data on the news posts\nthey receive on Facebook and how they interact with them. We\ncould provide a realistic ,fine-grained , and broad representation\nof users\u2019 exposure to news and their consumption behavior by\ncapturing the actual news users are exposed to on Facebook and\nprecisely what users see on their screens. Overall, our measurement\napproach provided immensely valuable data on news exposure that\nonly online platforms have had access to until now and no outside\nresearcher could have the opportunity to avail.\nOf course, implementing such measurement infrastructure is\ntechnically challenging. First, we had to ensure that the data collec-\ntion was reliable and did not miss data due to the variability in how\nonline platforms serve their content. Second, we had to ensure the\nprivacy and safety of the data collection and storage. Our codebase\nis publicly available5to help other groups adopt such methodology\nand encourage developing infrastructure to study news exposure\nand consumption on other social media platforms [58].\n3Users who self-identify as Democrat or Lean Democrat.\n4Users who self-identify as Republican or Lean Republican.\n5Available at https://anonymous.4open.science/r/CheckMyNews-AE8BImplications of results. Our research offers a unique opportunity\nto delve into users\u2019 precise exposure and engagement with news-\nrelated content, allowing us to reliably measure the prevalence of\nmisinformation and the political diversity within Facebook news\ndiets. We highlight three noteworthy aspects of our findings:\nMechanistic perspective on news exposure and consumption. We\nfocused on investigating news exposure and consumption patterns,\nspecifically considering how news articles appear in users\u2019 feeds\nbased on the underlying mechanisms. We found statistically signif-\nicant distinctions in diversity, quality, and consumption behavior\nwithin four exposure categories: selective, algorithmic, incidental,\nand targeted. For instance, we observed that incidental exposure\nleads to more balanced and factual news consumption, while users\nare less inclined to share targeted news posts. These differences\nhighlight the importance of adopting a mechanistic lens when at-\ntempting to model and understand dissemination and consumption\nof news on social media platforms.\nTransparency for targeted news exposure. Recent research works\nhave shown a significant shift from using targeted advertising as a\nway to promote products to a way to promote information [ 2,50,55].\nFor the subcategory of ads with political messages, such powerful\ntargeting is now being regarded as a danger, and many lawmakers\nare proposing to regulate such practices through increased trans-\nparency and targeting restrictions (e.g., Digital Services Act [ 14],\nEuropean Democracy Action Plan [ 13]). However, news organiza-\ntions are currently exempted from such obligations and restrictions.\nWe believe that malicious actors can easily leverage AI-driven tar-\ngeting technologies to promote news-related posts that leverage\nuser\u2019s data to deceive and manipulate them , by targeting news that\nresonate with each user [ 28,29,47,49,54]. Our results show that\nexposure to targeted news is a general phenomenon and represents\nan important fraction of users\u2019 news exposure. Therefore, we be-\nlieve that the same levels of transparency for targeted news should\nbe imposed similar to political advertising.\nNews consumption behavior. Our study uncovered a number\nof hitherto unknown and intriguing news consumption patterns:\nusers engage significantly less with targeted and algorithmic news-\nrelated posts; they tend to spend more time reading content from\nmisinformation sources compared to factual news sources; and\nshare news-related posts without reading the actual articles. We\nalso found evidence that users engage with posts from sources with\nopposing political ideologies through hidden interactions.\nScaling data donation. Installing monitoring tools on personal\ncomputers can be unsettling for many users. It\u2019s understandable, as\nsuch tools could potentially exploit users\u2019 trust, jeopardizing their\nsecurity and privacy. To address this concern, we take a transparent\napproach by making our code publicly available for auditing by\nanyone. Nonetheless, it is essential to emphasize the significance\nof data donations in uncovering risks with current technologies\nand promoting this practice more widely. While we acknowledge\nthat achieving perfect representativeness may be challenging, we\nmade efforts to reach out to Prolific users from diverse backgrounds,\nincluding various U.S. states and ethnicities. We are aware that the\npresence of a monitoring tool on users\u2019 computers might influence\ntheir behavior, potentially leading to altered usage patterns. This\nWhat News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations Conference\u201917, July 2017, Washington, DC, USA\nissue is not uncommon and has been encountered in previous re-\nsearch analyzing browsing histories. However, by requesting users\nto keep the tool installed for an extended period, we anticipate that\ntheir behavior will stabilize over time.\nFuture work. We believe that our paper can trigger a plethora of\nfuture research works seeking to understand the underlying fac-\ntors and implications of these consumption behaviors. Some of the\npotential research questions include: Does targeted and algorith-\nmic exposure exert a lesser impact on users\u2019 opinions compared to\nselective and incidental exposure? Can the amount of time spent\non news posts from misinformation sources provide insights into\nwhether users question the information presented? To what extent\ndoes merely seeing a news post without reading the source article\ninfluence the user\u2019s opinion on a particular topic? Do interactions\nwith posts from sources holding opposing political ideologies rein-\nforce or prompt users to question their own political preferences?\nAddressing these questions could shed further light on the com-\nplexities of news consumption patterns on social media, the role of\nexposure mechanisms, and their effects on users\u2019 opinions.\nREFERENCES\n[1]A Andreou, G Venkatadri, O Goga, K Gummadi, P Loiseau, A. Mislove. 2018.\nInvestigating ad transparency mechanisms in social media: A case study of\nFacebook\u2019s explanations. NDSS 2018-Network and Distributed System Security\nSymposium (2018), 1\u201315.\n[2]A Andreou, M Silva, F Benevenuto, O Goga, P Loiseau, A. Mislove. 2019. Measur-\ning the Facebook advertising ecosystem. NDSS 2019-Proceedings of the Network\nand Distributed System Security Symposium (2019), 1\u201315.\n[3]Hunt Allcott and Matthew Gentzkow. 2017. Social media and fake news in the\n2016 election. Journal of economic perspectives 31, 2 (2017), 211\u2013236.\n[4]Hunt Allcott, Matthew Gentzkow, and Chuan Yu. 2019. Trends in the diffusion\nof misinformation on social media. Research & Politics 6, 2 (2019). https://doi.\norg/10.1177/2053168019848554\n[5]Ana S. Cardenal, Carlos Aguilar-Paredes, Carol Galais, and Mario P\u00e9rez-Montoro.\n2019. Digital Technologies and Selective Exposure: How Choice and Filter Bubbles\nShape News Media Exposure. The International Journal of Press/Politics 24, 4\n(2019), 465\u2013486. https://doi.org/10.1177/1940161219862988\n[6]Annie Y. Chen, Brendan Nyhan, Jason Reifler, Ronald E. Robertson, and\nChristo Wilson. 2023. Subscriptions and external links help drive re-\nsentful users to alternative and extremist YouTube channels. Science\nAdvances 9, 35 (2023), eadd8080. https://doi.org/10.1126/sciadv.add8080\narXiv:https://www.science.org/doi/pdf/10.1126/sciadv.add8080\n[7]DA Parry, BI Davidson, C Sewall JR, JT Fisher, H Mieczkowski, DS Quintana.\n2021. A systematic review and meta-analysis of discrepancies between logged and\nself-reported digital media use. Nature Human Behaviour 5, 11 (2021), 1535\u20131547.\n[8]Y Dodge. 2008. Kolmogorov\u2013Smirnov Test . Springer New York, New York, NY,\n283\u2013287. https://doi.org/10.1007/978-0-387-32833-1_214\n[9]Gregory Eady, Jonathan Nagler, Andy Guess, Jan Zilinsky, and Joshua A. Tucker.\n2019. How Many People Live in Political Bubbles on Social Media? Evidence\nFrom Linked Survey and Twitter Data. SAGE Open (2019). https://doi.org/10.\n1177/2158244019832705\n[10] Gregory Eady, Tom Paskhalis, Jan Zilinsky, Richard Bonneau, Jonathan Nagler,\nand Joshua A Tucker. 2023. Exposure to the Russian Internet Research Agency\nforeign influence campaign on Twitter in the 2016 US election and its relationship\nto attitudes and voting behavior. Nature communications (2023).\n[11] Laura Edelson, Minh-Kha Nguyen, Ian Goldstein, Oana Goga, Damon McCoy, and\nTobias Lauinger. 2021. Understanding Engagement with U.S. (Mis)Information\nNews Sources on Facebook. In Proceedings of the 21st ACM Internet Measurement\nConference (Virtual Event) (IMC \u201921) . Association for Computing Machinery, New\nYork, NY, USA, 444\u2013463. https://doi.org/10.1145/3487552.3487859\n[12] Electoral College. 2020. A 2020 Presidential Election Timeline. Retrieved 31 July\n2023 from https://crsreports.congress.gov/product/pdf/IF/IF11641\n[13] European Commission. 2020. European Democracy Action Plan: making EU\ndemocracies stronger. Retrieved 31 July 2023 from https://ec.europa.eu/\ncommission/presscorner/detail/en/IP_20_2250\n[14] European Commission. 2023. Digital Services Act package. Retrieved 31 July 2023\nfrom https://ec.europa.eu/digital-single-market/en/digital-services-act-package\n[15] Facebook. 2019. A Better Way to Learn About Ads on Facebook. Retrieved 31 July\n2023 from https://about.fb.com/news/2019/03/a-better-way-to-learn-about-ads/[16] Facebook. 2023. Suggested Posts on Facebook Feed. Retrieved 31 July 2023 from\nhttps://www.facebook.com/business/help/1082519118875784\n[17] Facebook Ad Library. 2023. Retrieved 31 July 2023 from https://www.facebook.\ncom/ads/library/\n[18] Seth Flaxman, Sharad Goel, and Justin M. Rao. 2016. Filter Bubbles, Echo\nChambers, and Online News Consumption. Public Opinion Quarterly (2016).\nhttps://doi.org/10.1093/poq/nfw006\n[19] Richard Fletcher and Rasmus Nielsen. 2018. Automated Serendipity: The effect of\nusing search engines on news repertoire balance and diversity. Digital Journalism\n6 (09 2018), 976\u2013989. https://doi.org/10.1080/21670811.2018.1502045\n[20] Richard Fletcher and Rasmus Kleis Nielsen. 2018. Are people incidentally exposed\nto news on social media? A comparative analysis. New Media & Society (2018).\nhttps://doi.org/10.1177/1461444817724170\n[21] Christine Geeng, Savanna Yee, and Franziska Roesner. 2020. Fake News on\nFacebook and Twitter: Investigating How People (Don\u2019t) Investigate. Proceedings\nof the 2020 CHI Conference on Human Factors in Computing Systems (2020), 1\u201314.\nhttps://doi.org/10.1145/3313831.3376784\n[22] Andrew Guess, Jonathan Nagler, and Joshua Tucker. 2019. Less than you think:\nPrevalence and predictors of fake news dissemination on Facebook. Science\nAdvances (2019). https://www.science.org/doi/abs/10.1126/sciadv.aau4586\n[23] Andrew M. Guess, Pablo Barber\u00e1, Simon Munzert, and JungHwan Yang. 2021.\nThe consequences of online partisan media. Proceedings of the National Academy\nof Sciences (2021). https://www.pnas.org/doi/abs/10.1073/pnas.2013464118\n[24] Andrew M Guess, Brendan Nyhan, and Jason Reifler. 2020. Exposure to untrust-\nworthy websites in the 2016 US election. Nature human behaviour 4, 5 (2020),\n472\u2013480.\n[25] Homa Hosseinmardi and Amir Ghasemian and Aaron Clauset and Markus Mobius\nand David M. Rothschild and Duncan J. Watts. 2021. Examining the consumption\nof radical content on YouTube. Proceedings of the National Academy of Sciences\n(2021). https://www.pnas.org/doi/abs/10.1073/pnas.2101967118\n[26] Jeanna Matthews. 2022. Radicalization pipelines: How targeted advertis-\ning on social media drives people to extremes. Retrieved 31 July\n2023 from https://theconversation.com/radicalization-pipelines-how-targeted-\nadvertising-on-social-media-drives-people-to-extremes-173568\n[27] Jeremy Merrill and Ariana Tobin. ProPublica. 2019. Facebook moves to block\nad transparency tools\u2013including ours. Retrieved 31 July 2023 from https:\n//www.propublica.org/article/facebook-blocks-ad-transparency-tools\n[28] Jeremy Merrill and Hanna Kozlowska. QUARTZ. 2019. How Facebook fueled a\nprecious-metal scheme targeting older conservatives. Retrieved 31 July 2023\nfrom https://qz.com/1751030/facebook-ads-lured-seniors-into-giving-savings-\nto-metals-com/\n[29] Julia Carrie Wong. The Guardian. 2019. The Cambridge Analytica scandal\nchanged the world\u2013but it didn\u2019t change Facebook. Retrieved 31 July 2023\nfrom https://www.theguardian.com/technology/2019/mar/17/the-cambridge-\nanalytica-scandal-changed-the-world-but-it-didnt-change-facebook\n[30] Neta Kligler-Vilenchik, Alfred Hermida, Sebasti\u00e1n Valenzuela, and Mikko Villi.\n2020. Studying incidental news: Antecedents, dynamics and implications. Jour-\nnalism 21, 8 (2020), 1025\u20131030. https://doi.org/10.1177/1464884920915372\n[31] Ro\u2019ee Levy. 2021. Social Media, News Consumption, and Polarization: Evidence\nfrom a Field Experiment. American Economic Review 111, 3 (March 2021), 831\u201370.\nhttps://doi.org/10.1257/aer.20191777\n[32] Benjamin A. Lyons, Jacob M. Montgomery, Andrew M. Guess, Brendan Nyhan,\nand Jason Reifler. 2021. Overconfidence in news judgments is associated with\nfalse news susceptibility. Proceedings of the National Academy of Sciences (2021).\nhttps://www.pnas.org/doi/abs/10.1073/pnas.2019527118\n[33] Media Bias Fact Check. 2023. Retrieved 31 July 2023 from https://\nmediabiasfactcheck.com/\n[34] Mohsen Mosleh and David G Rand. 2022. Measuring exposure to misinformation\nfrom political elites on Twitter. nature communications 13, 1 (2022), 7144.\n[35] News Guard. 2023. Retrieved 31 July 2023 from https://www.newsguardtech.com/\n[36] Dimitar Nikolov, Diego FM Oliveira, Alessandro Flammini, and Filippo Menczer.\n2015. Measuring online social bubbles. PeerJ computer science 1 (2015), e38.\n[37] Brendan Nyhan, Jaime Settle, Emily Thorson, Magdalena Wojcieszak, Pablo\nBarber\u00e1, Annie Y Chen, Hunt Allcott, Taylor Brown, Adriana Crespo-Tenorio,\nDrew Dimmery, et al .2023. Like-minded sources on Facebook are prevalent but\nnot polarizing. Nature (2023), 1\u20138.\n[38] Anne Oeldorf-Hirsch. 2018. The Role of Engagement in Learning From Active\nand Incidental News Exposure on Social Media. Mass Communication and Society\n(2018). https://doi.org/10.1080/15205436.2017.1384022\n[39] Katherine Ognyanova, David Lazer, Ronald E. Robertson, and Christo Wilson.\n2020. Misinformation in action: Fake news exposure is linked to lower trust in\nmedia, higher trust in government when your side is in power. Harvard Kennedy\nSchool Misinformation Review 1 (06 2020). https://doi.org/10.37016/mr-2020-024\n[40] Eli Pariser. 2011. The filter bubble: What the Internet is hiding from you . penguin\nUK.\n[41] Karl Pearson. 1900. X. On the criterion that a given system of deviations from\nthe probable in the case of a correlated system of variables is such that it can\nbe reasonably supposed to have arisen from random sampling. The London,\nConference\u201917, July 2017, Washington, DC, USA Salim Chouaki, Abhijnan Chakraborty, Oana Goga, and Savvas Zannettou\nEdinburgh, and Dublin Philosophical Magazine and Journal of Science 50, 302\n(1900), 157\u2013175.\n[42] PopulationPyramid. 2021. Population of the United States by age as of 2021.\nRetrieved 31 July 2023 from https://www.populationpyramid.net/united-states-\nof-america/2021/\n[43] Priyanjana Bengani. Columbia Journalism Review. 2019. Hundreds\nof \u2018pink slime\u2019 local news outlets are distributing algorithmic sto-\nries and conservative talking points. Retrieved 31 July 2023 from\nhttps://www.cjr.org/tow_center_reports/hundreds-of-pink-slime-local-news-\noutlets-are-distributing-algorithmic-stories-conservative-talking-points.php\n[44] Priyanjana Bengani. Columbia Journalism Review. 2021. Advocacy groups and\nMetric Media collaborate on local \u2018community news\u2019. Retrieved 31 July 2023\nfrom https://www.cjr.org/tow_center_reports/community-newsmaker-metric-\nmedia-local-news.php\n[45] Prolific. 2023. Retrieved 31 July 2023 from https://www.prolific.co\n[46] Ronald E Robertson, Jon Green, Damian J Ruck, Katherine Ognyanova, Christo\nWilson, and David Lazer. 2023. Engagement outweighs exposure to partisan and\nunreliable news within Google Search. Nature (2023). https://doi.org/10.1038/\ns41586-023-06078-5\n[47] S Lewandowsky, L Smillie, D Garcia, R Hertwig, J Weatherall, S. Egidy, RE Robert-\nson, C O\u2019Connor, A Kozyreva, P Lorenz-Spreen and others. 2020. Technology\nand democracy: Understanding the influence of online technologies on political\nbehaviour and decision-making. Publications Office of the European Union (2020).\n[48] Michael Scharkow, Frank Mangold, Sebastian Stier, and Johannes Breuer. 2020.\nHow social network sites and other online intermediaries increase exposure to\nnews. Proceedings of the National Academy of Sciences (2020). https://www.pnas.\norg/doi/abs/10.1073/pnas.1918279117\n[49] Scott Shane. The New York Times. 2017. These Are the Ads Russia Bought on\nFacebook in 2016. Retrieved 31 July 2023 from https://www.nytimes.com/2017/\n11/01/us/politics/russia-2016-election-facebook.html\n[50] Vera Sosnovik and Oana Goga. 2021. Understanding the Complexity of Detecting\nPolitical Ads. Proceedings of the Web Conference 2021 (2021), 2002\u20132013. https:\n//doi.org/10.1145/3442381.3450049\n[51] Statista. 2023. Distribution of Facebook users in the United States as of July 2021,\nby gender. Retrieved 31 July 2023 from https://www.statista.com/statistics/\n266879/facebook-users-in-the-us-by-gender/\n[52] Statista. 2023. Resident population of the United States by race from 2000 to\n2020. Retrieved 31 July 2023 from https://www.statista.com/statistics/183489/\npopulation-of-the-us-by-ethnicity-since-2000/\n[53] Statista. 2023. Total population in the United States by gender from 2010 to\n2025. Retrieved 31 July 2023 from https://www.statista.com/statistics/737923/us-\npopulation-by-gender/\n[54] Daniel Susser, Beate Roessler, and Helen Nissenbaum. 2019. Technology, au-\ntonomy, and manipulation. Internet Policy Review 8, 2 (2019), 1\u201322. https:\n//doi.org/10.14763/2019.2.1410\n[55] Teresa Wright. Toronto Star. 2018. Canadian government spending tens of\nmillions on Facebook ads, sponsored posts. Retrieved 31 July 2023 from\nhttps://www.thestar.com/news/canada/canadian-government-spending-tens-\nof-millions-on-facebook-ads-sponsored-posts/article_8312818a-b489-5233-\n9aaa-05ae55dd3530.html\n[56] David Tewksbury, Andrew J Weaver, and Brett D. Maddex. 2001. Accidentally\nInformed: Incidental News Exposure on the World Wide Web. Journalism & Mass\nCommunication Quarterly (2001). https://doi.org/10.1177/107769900107800309\n[57] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false\nnews online. Science (2018). https://doi.org/10.1126/science.aap9559\n[58] Duncan J. Watts, David M. Rothschild, and Markus Mobius. 2021. Measuring\nthe news and its impact on democracy. Proceedings of the National Academy of\nSciences (2021). https://www.pnas.org/doi/abs/10.1073/pnas.1912443118\n[59] Brian E. Weeks, Daniel S. Lane, Dam Hee Kim, Slgi S. Lee, and Nojin Kwak.\n2017. Incidental Exposure, Selective Exposure, and Political Information Sharing:\nIntegrating Online Exposure Patterns and Expression on Social Media. Journal\nof Computer-Mediated Communication (2017). https://doi.org/10.1111/jcc4.12199\n[60] Stefan Wojcik and Adam Hughes. 2019. Sizing up Twitter users. PEW research\ncenter 24 (2019), 1\u201323. https://www.pewresearch.org/internet/2019/04/24/sizing-\nup-twitter-users/\n[61] Borchuluun Yadamsuren and Sanda Erdelez. 2010. Incidental Exposure to Online\nNews. Proceedings of the 73rd ASIS Annual Meeting on Navigating Streams in an\nInformation Ecosystem - Volume 47 , Article 22 (2010), 8 pages.\nA MEASUREMENT METHODOLOGY\nA.1 Types of Facebook posts collected\nWe collect information about three types of user posts on Facebook:\n1.News posts \u2013 news posts (or news-related posts) include Face-\nbook posts that fall into two categories. First, posts published bypages listed in our list of Facebook pages associated with news\nsources ( Established Facebook Pages list orUnder-the-Radar\nFacebook Pages list ), and second, posts that contain a landing\nURL directing to one of the news domains in our lists ( Established\nNews Sites list orUnder-the-Radar News Sites list ). These\nposts can be targeted (i.e., ads), suggested by Facebook, and have\ndifferent privacy settings, ranging from private (visible to a limited\ngroup of users) to public (visible to all users).\n2.Non-news targeted posts \u2013 Posts advertisers pay Facebook to send\nto specific groups of users [ 2]. Note that CheckMyNews detects the\nnews posts before detecting the targeted ones. Hence, all the posts\nit collects as targeted posts are not news-related.\n3.Non-news public posts \u2013 All non-targeted and non-news posts.\nThey are shared by users, groups, or Facebook pages and set as\npublic by the publishers, making them visible to anyone.\nWhile non-news targeted and public posts do not meet our cri-\nteria for being news-related based on the filter we applied during\nthe data collection, their inclusion was valuable in expanding our\nlists of news domains and their Facebook pages. By analyzing the\nFacebook pages from which we collected these public and targeted\nposts, we identified 404additional news domains associated with\n449Facebook pages with the \"News Media\" category.\nA.2 Reliability of the monitoring tool\nHaving a reliable monitoring tool that does not miss any post a\nuser sees on Facebook is necessary to have correct and coherent\nmeasurements. To ensure the tool works well for all users during the\ndata collection, we have implemented several tests at the extension\nlevel to detect when our collection functions do not work correctly\n(e.g., the user is on Facebook, but we do not detect any post for\nmore than 120seconds). We send error messages to the server,\nand we have developed a monitoring page that we consult daily to\ncheck for aggregate and per-user statistics and consult the error\nmessages. The targeted posts are more challenging to detect because\nFacebook renders them using complex changing HTML objects\nthat sometimes differ between users. To cope with this, we first\nmake sure that the targeted posts we miss are collected as public\nposts (that are simpler to detect). We then manually check users\nfrom whom we have collected only public posts and investigate\nhow targeted ones are rendered for them. We finally updated our\nextension to detect the targeted posts rendered in this new way.\nOverall, the monitoring tool can easily be installed by users,\nworks silently in the background, has a minimal impact on browser\nperformance, and does not affect the user experience.\nA.3 Compliance with ethical principles\nWe use various strategies to minimize user security and privacy\nrisks and sought the necessary approvals from Data Protection\nOfficers and Ethical Review Boards. The personal data we collect\nis handled following the EU General Data Protection Regulation\n2016/679. Personal data is processed lawfully, fairly, and in a trans-\nparent manner. To ensure privacy, confidentiality, security, and\nlegality, we took the following measures:\n(a)Data minimization : The tool collects information about the con-\ntent users receive and not the content they share. Additionally, we\nonly collect the landing URLs or private news posts.\nWhat News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations Conference\u201917, July 2017, Washington, DC, USA\n(b)Pseudonymization: We do not send our servers any personally\nidentifiable information of users (e.g., email, name, phone number).\nNo summary data is disclosed that would allow inference about an\nindividual\u2019s personal or private data. Each user is identified by a\nrandom identifier generated at each new tool installation.\n(c)Explicit consent: Every user installing our tool is shown a page\ndescribing precisely the data given and the use of this data. We ask\nfor the user\u2019s explicit consent to donate data and participate in the\nresearch study. The consent form is submitted (electronically) for\neach user installing the tool, and we keep proof of this consent.\n(d)Detailed privacy and security risks assessment: We passed a se-\ncurity homologation from our institution and wrote a detailed\ndocument that analyzes security and privacy risks at every level of\nthe data transfer and worked with network and system engineers\nfrom our University to secure the application at every level.\n(e) To use our tool, users must confirm being at least 16years old.\n(f)Data removal/leaving the study: We informed the participants of\ntheir right to access, correct, request portability, and delete personal\ndata, and we gave them the contact details of our Data Protection\nOfficer (DPO) to exercise their rights. Participants could leave the\nstudy at any moment and ask for their data to be removed.\nB RECRUITING AND REPRESENTATIVENESS\nB.1 User recruiting\nWe posted about our study on Prolific and 889 U.S.-based partic-\nipants agreed to install CheckMyNews and keep it active for six\nweeks (between November 2020 and February 2021). To compen-\nsate the participants for their time in installing and answering the\nsurvey questions, we offered them an initial payment right after\nthe installation and a bonus payment at the end of the six weeks if\nthere was a minimum required activity level. Only 720successfully\ninstalled the extension, and only 580logged into Facebook after\ninstalling it. Finally, only 472users kept the tool active for a long\nperiod and respected the minimum Facebook activity condition\n(at least 30 minutes); we do our analysis only on these 472users.\nWe consider that users have dropped out of our study when we\nstop collecting their activity. We do not know whether they have\nuninstalled/disabled the browser extension or stopped using the\ncomputer or the browser on which they have installed it. Though\nthe initial study was launched over six weeks, we extended our\ndataset to include data over three months (until February 15, 2021)\nsince we had many users who kept running the extension.\nB.2 User representativeness\nOur users are 65%males and 35%females (compared to 45%males\n55%females for U.S. users on Facebook [ 51], and 49%and 51%for\nthe U.S. population [ 53]) and live across 48states in the U.S. The\nusers are part of different ethnic groups: 74%White, 11%African\nAmerican, and 11%Asian (compared to 76%,13%and 6%for the U.S\npopulation [ 52]). Figure 4 presents the age distribution of our users,\ncompared to the overall U.S. population [ 42] and the Facebook U.S.\nusers [ 2]. More than half of them are between 20and 40(54%).\nHence, our database has more young users than the normal U.S.\npopulation, but we have users of all age pools. According to the\nsurvey, 76%of users consider themselves Democrats, while 16%are\nRepublicans and 6%are Independents.\n18-24 25-34 35-44 45-54 55-64 65+05101520253035\nOur users\nU.S. population\nFacebook users\nAge poolsFrequency (%)Figure 4: Age distribution of our users vs. U.S. population\nand U.S. Facebook users.\n100101102103104\nNumber of posts0.00.20.40.60.81.0Cumulative distribution function\nNews posts\nT argeted posts\nPublic posts\nFigure 5: Number of news posts, targeted posts and public\nposts across all users.\nC DATA COLLECTION AND PROCESSING\nC.1 News exposure and consumption data\nOur dataset includes 889,438 Facebook posts received by 472users;\n143,129 ( 16%) posts are news-related, 205,469 ( 23%) are non-news\ntargeted and 548,152 ( 61%) are non-news public. Out of all the\nnews-related posts, 108,659 posts have a link to one of the news\ndomains in our lists ( Established News Sites list orUnder-the-\nRadar News Sites list ) and 85,066 were published by pages in our\nlists of Facebook pages associated with news media ( Established\nFacebook Pages list orUnder-the-Radar Facebook Pages list ).\nFigure 5 represents the number of news posts, targeted posts, and\npublic posts received by users. A median user received 125(\ud835\udc65=\n435,\ud835\udc60=855) targeted posts, 58(\ud835\udc65=303,\ud835\udc60=789) news posts, and\n387(\ud835\udc65=1,161,\ud835\udc60=2,266) public posts. We have also collected a total\nof 37,300 user interactions with 24,486 different posts: 9,804 are\nvisible interactions while 27,496 are hidden interactions. Figure 6\npresents the number of actions performed by each user on all news\nposts. We can see that a median user performed 20interactions\n(\ud835\udc65=79,\ud835\udc60=189).\nC.2 Unexpected user behavior and bot detection\nConsidering the relevancy of the research questions we address\nin this study, it is crucial to ensure that none of the participants\nused bots during data collection. Figure 7 presents the median and\nmaximum times spent on Facebook per day, over all active days\nfor each of the 472participants. The figure shows that an average\nConference\u201917, July 2017, Washington, DC, USA Salim Chouaki, Abhijnan Chakraborty, Oana Goga, and Savvas Zannettou\n100101102103\nNumber of user actions0.00.20.40.60.81.0Cumulative distribution function\nFigure 6: Number of interactions with news posts by user.\n103\n102\n101\n100101\nTime (hours)0.00.20.40.60.81.0Cumulative distribution function\nMedian time per day on Facebook\nMax time per day on Facebook\nFigure 7: CDF of the maximum and the median time spent\non Facebook per day per user.\nuser spent 0.06hours ( 3.6minutes) on Facebook on a day with\nmedian activity and 0.53hours ( 32minutes) on the most active\nday. Furthermore, the figure reveals that 10users spent more than\n7hours on Facebook on their busiest days. Upon investigating\nthe posts collected from these users, we found no evidence of bot\nactivity. Our analysis suggests that these users have left Facebook\nopen on their browsers without actively browsing on the platform.\nC.3 Distinguishing news posts categories\nOur monitoring tool collects all news posts on users\u2019 Facebook\nfeeds. This section presents how we technically divide these posts\ninto targeted, selective, incidental, and algorithmic news posts.\nSelective news exposure : We select posts originating from the official\nFacebook pages of news media sites ( Established Facebook Pages\nlist andUnder-the-Radar Facebook Pages list ). We then check\nwhether these posts contain a landing URL that directs users to their\nrespective news media website. For instance, if the Facebook page of\nCNN\u2013https://www.facebook.com/cnn\u2013publishes a post that links\nto an article on cnn.com\u2013https://edition.cnn.com/...), we consider it\nas selective exposure. However, when a news media Facebook page\nshares a post containing a link to a news article from an external\nsite, we do not consider it selective exposure. In such cases, the\nuser\u2019s exposure to the external site\u2019s content does not result from\ntheir explicit following of the external site\u2019s Facebook page.\nTargeted news posts : This category includes all targeted posts that\npromote articles from news media sites, irrespective of the Facebook\npage that promotes them. While such posts are rendered similarly\nto regular Facebook posts, they include a \"Sponsored\" tag. We use\nseveral HTML and CSS selectors to identify this tag.\nAlgorithmic news posts : This category includes Facebook news-\nrelated posts that Facebook suggests to users. Such posts have the\n\"Suggested for you\" tag that we detect using CSS selectors. We\n50001000015000\n09/11 223/11 47/12 621/12 84/1 1018/1 121/2 1415/2\nWeek0204060Median proportion of posts (%)\nT argeted news posts\nSelective news posts\nIncidental news posts\nAlgorithmic news postsFigure 8: Median proportion of selective, incidental, algo-\nrithmic, and targeted news posts received by users by week.\nAbove: weekly number of collected news posts.\nanalyze the HTML objects of all identified news-related posts and\nconsider algorithmic exposure all news posts that include this tag.\nIncidental news posts : For each news post, we extract the landing\ndomain and the Facebook page\u2019s ID. We then verify if one of the\nfollowing conditions is met: (a) the page\u2019s ID is not in our list of\nFacebook pages of news sites ( Established Facebook Pages list\nandUnder-the-Radar Facebook Pages list ), but the landing\ndomain is among our list of news sources ( Established News\nSites list andUnder-the-Radar News Sites list ), or (b) both\nthe page\u2019s ID and the landing domain are present in the respective\nlists, but the Facebook page belongs to another source.\nOur dataset contains a total of 143,129 news posts; 62,434 are\nselective, 60,529 are incidental, 11,566 are targeted, and 8,600 are\nalgorithmic. Figure 8 illustrates the changing proportion of inciden-\ntal, selective, targeted, and algorithmic news posts by week, during\nour data collection period.\nC.4 Metadata on news outlets\nTo assess the quality of Facebook news diets, we measure the pro-\nportion of posts originating from (a) mostly factual news sources\nand (b) sources spreading misinformation, fake news, and conspir-\nacy theories. We also evaluate the political diversity of Facebook\nnews diets by measuring the proportion of news posts from sources\nacross the political spectrum. We assign quality (factual, misinfor-\nmation, or mixed) and political bias (left, center, or right) labels at\nthe source level for each news domain. All posts originating from a\nspecific domain inherit these labels. We do not make any judgment\non the quality and political bias of news domains. Instead, similar\nto previous work [ 11], we rely on evaluations provided by Media\nBias Fact Check and News Guard.\nMetadata on misinformation. News Guard describes whether\na news source has a history of sharing misinformation in the \"Top-\nics\" column of their data file, while Media Bias Fact Check provides\nit in the \"Detailed\" section of their website\u2019s source evaluation.\nThough the two agencies used different terminology to capture the\nfull spectrum of misleading news practices, they always included\nthe terms \"Conspiracy,\" \"Fake News,\" or \"Misinformation.\" Conse-\nquently, we flagged a source as spreading misinformation if one of\nthese terms was used to describe it. The two data providers agreed\non this measure; only 33 domains were the subject of disagreement.\nWe resolved these disagreements by applying the misinformation\nlabel. Overall, we labeled 456news sources associated with 467as\nspreading misinformation.\nWhat News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations Conference\u201917, July 2017, Washington, DC, USA\nMetadata on factualness. News Guard assigns a credibility\nscore (between 0 and 100) and Media Bias Fact Check provides a\nfactual_reporting text label for each news publisher. We apply filters\nto both fields: (a) News Guard scores of 75 or higher (indicating\nthat a news source has high credibility or is generally credible), and\n(b) a positive Media Bias Fact Check factual reporting (High, Very\nhigh, or Mostly factual). If a news publisher has ratings from both\nagencies, we consider it factual only if both consider it factual. If\na news publisher has an evaluation from only one agency, we use\nthat rating alone. Overall, we have labeled 2,942 news sources as\nfactual, corresponding to 3,074 Facebook pages.\nMetadata on news sources political bias. News Guard provides\nthe political leaning for 2,939 different news sites (Far Left, Slightly\nLeft, Center, Slightly Right, Far Right) and Media Bias Fact Checkfor 1,711 different news sites (extreme-left, far-left, left, left-center,\ncenter, right-center, right, far-right, extreme-right). We normalize\nthe evaluations from both sources by keeping the News Guard\nscale, and we cast the Media Bias Fact Check evaluations into it\nby considering (a) extreme-right and extreme-left as far-right and\nfar-left, (b) right as far-right and left as far-left and (c) right-center\nas slightly-right and left-center as slightly-left. We have 41domains\nfor which we have different evaluations from the two sources; we\nprefer to use the values of Media Bias Fact Check for these.\nIn total, from the 4,149 news sites in the Established News\nSites list , we have the political leaning for 4,107 of them ( 99%).\n64% of them are rated as Center, 20% as Left ( 7%Far-Left + 13%\nSlightly-Left), and 17%as Right ( 10%Far-Right + 7%Slightly-Right).", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "What news do people get on social media? Analyzing exposure and consumption of news through data donations", "author": ["S Chouaki", "A Chakraborty", "O Goga"], "pub_year": "2024", "venue": "Proceedings of the ACM \u2026", "abstract": "Understanding how exposure to news on social media impacts public discourse and  exacerbates political polarization is a significant endeavor in both computer and social sciences."}, "filled": false, "gsrank": 444, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3589334.3645399", "author_id": ["H0bs5H8AAAAJ", "21oQO9oAAAAJ", "re_squoAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:pBUCgjcs5lwJ:scholar.google.com/&output=cite&scirp=443&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D440%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=pBUCgjcs5lwJ&ei=VbWsaOTRJ7XCieoP4PfQ0A8&json=", "num_citations": 3, "citedby_url": "/scholar?cites=6694086513048819108&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:pBUCgjcs5lwJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://inria.hal.science/hal-04618579/file/ie.pdf"}}, {"title": "Taking a Stand on Not Taking a Stand: Media Bias in the Online Reporting of COVID-19", "year": "2021", "pdf_data": "TAKING  A STAND  ON NOT  TAKING  A STAND:  MEDIA  BIAS  IN THE  ONLINE\nREPORTING  OF COVID-19\nBy\nKyle  Johnson,  B.A.\nA Thesis  Submitted  in Partial  Fulfillment  of the Requirements  \nfor the Degree  of\nMaster  of Arts\nin\nProfessional  Communication\nUniversity  of Alaska  Fairbanks\nMay 2021\nAPPROVED:\nRichard  Hum  PhD,  Committee  Chair  \nKaren  Taylor  PhD,  Committee  Member  \nKristin  Timm  M.S.,  Committee  Member  \nCharles  Mason  M.S.,  Department  Chair  \nDepartment  of Communication  and \nJournalism\nEllen  Lopez  MPH,  PhD,  Dean  \nCollege  of Liberal  Arts\nRich  Collins  PhD,  Director  of the Graduate  School\n\nAbstract\nThis  thesis  was written  to examine  the digital  communication  strategies  of three  major  \nnews  organizations  when  reporting  on COVID-19  in the U.S.  for bias.  The research  looked  at \nsocial  media  posts,  online  article  counts  and themes,  main  websites  of each  organization  and \naudio/visual  broadcast  segments  from  all three  organizations  posted  online.  This research  used \nan advocacy  approach,  examining  the tension  between  entertainment  and journalism  ethics  by \nholding  news  organizations  to journalism  standards  to see how they compare.  Results  showed  \nthat NPR  and Fox News  produced  more  online  articles  than MSNBC  and linked  to their  own \narticles  on twitter  more.  The audiovisual  content  from  MSNBC  and Fox News  did not follow  the \ncode  of ethics  created  by the Society  of Professional  Journalists.  All three  organizations  used \nbiased  methods  for providing  information  to the public,  during  a time period  where  public  \nknowledge  is key to managing  a pandemic.\niii\n\n\nTable  of Contents\nPage\nAbstract  .........................................................................................................................................  iii\nTable  of Contents ............................................................................................................................ v\nList of Tables  ................................................................................................................................  vi\nList of Appendices  ........................................................................................................................  vi\nIntroduction ..................................................................................................................................... 1\nBackground  ..................................................................................................................................... 4\nPolitical  Landscape  in 2020  ........................................................................................................ 4\nDigital  Media  and McLuhan  ....................................................................................................... 4\nAgenda-Setting,  Frames,  and Pump-Valves  ............................................................................... 6\nDefining  Media  Bias  ................................................................................................................... 9\nMethodology  ................................................................................................................................. 11\nArticle  Sampling  ........................................................................................................................ 13\nAudiovisual  Sampling ................................................................................................................ 14\nTweet  Sampling  ......................................................................................................................... 15\nResults ........................................................................................................................................... 17\nWebsite  Ethnographies  .............................................................................................................. 17\nArticle  Counts  and Title  Themes  ............................................................................................... 19\nAudiovisual  Analysis  ................................................................................................................. 23\nTweet  Counts  and Linking ......................................................................................................... 23\nDiscussion  ..................................................................................................................................... 27\nResearch  Question  One Discussion  ........................................................................................... 27\nResearch  Question  Two  Discussion ........................................................................................... 29\nConclusion  .................................................................................................................................... 33\nWorks  Cited  .................................................................................................................................. 35\n\nList of Tables\nPage\nTable  1: Data  Streams  Collected ................................................................................................... 12\nTable  2: Number  of articles  per week  from  each organization  searching  \u201cCOVID-19 \u201d............. 19\nTable  3: Number  of articles  per week  from  each organization  searching  \u201cCoronavirus \u201d ...........20\nTable  4: Number  of Fox News  articles  each  week  using  search  terms  .......................................20\nTable  5: Number  of tweets  from  each organization  that link to their  own articles  when  searching  \n\u201cCOVID-19 \u201d ................................................................................................................................. 21\nTable  6: Number  of tweets  from  each organization  that link to their  own articles  when  searching\n\u201cCoronavirus \u201d .............................................................................................................................. 21\nTable  7: Article  counts  by theme:  Health  tools  and testing  .........................................................22\nTable  8: Article  counts  by theme:  Infection  and mortality  ........................................................... 22\nTable  9: Article  counts  by theme:  Political  discourse  .................................................................22\nTable  10: Article  counts  by theme:  Business  Impact ...................................................................23\nTable  11: Article  counts  by theme:  Immigration  .........................................................................23\nTable  12: Tweets  per week  from  each  organization  searching  \u201cCOVID-19 \u201d ............................24\nTable  13: Tweets  per week  from  each  organization  when  searching  \u201cCoronavirus \u201d .................25\nList of Appendices\nPage\nAppendix  A: Article  Theme  Codebook  .......................................................................................40\nAppendix  B: Tucker  Carlson  Show  \u201cLocal  Leaders  Failed  Cities \u201d Segment  .............................41\nAppendix  C: NPR  Morning  Edition  \u201cCoronavirus  Guidance  Across  Texas  Is Not Consistent \u201d\nSegment ....................................................................................................................................... 49\nAppendix  D: Rachel  Maddow  \u201cStates  Wait  For Federal  Help  At Their  Own  Peril  As Coronavirus\nCrisis  Builds \u201d Segment  ................................................................................................................ 54\n\n\nIntroduction\nThe world  is currently  facing  a pandemic  that is far-reaching  and having  an impact  on all \naspects  of daily  life not seen  before  in our lifetime.  COVID-19  is a potentially  fatal respiratory  \nvirus  that has spread  to every  country  on earth  during  2020.  Within  the United  States,  according  \nto the Johns  Hopkins  coronavirus  resource  center  as of March  4, 2021,  we currently  have  had \nnearly  twenty-nine  million  cases  reported  since  the first diagnosis  in the country  in January  2020.  \nThe United  States  currently  has the highest  number  of both reported  cases  and deaths  attributed  \nto COVID-19  compared  to all other  countries  with reported  data according  to the Johns  Hopkins  \ncoronavirus  resource  center.  With  a new virus  that is so dangerous  and contagious,  residents  of \nthe United  States  have  had a need  for reliable  information  coming  from  news  organizations  on \nhow to best deal with  the current  crisis.  Concurrently,  political  views  of Americans  have  become  \nmore  polarized  over  the past twenty-five  years  (Kiley,  2017),  with key factors  being  racial  \ndiscrimination,  government  assistance,  and the United  States'  involvement  around  the world  \n(Doherty,  2020).  The COVID-19  pandemic  combined  with  political  division  in the United  States  \nproduces  an ideal  communicative  environment  for examining  bias in the news.\nThere  has been  a rise in incorrect  information  spread  online  about  COVID-19  (Young  et \nal, 2021),  and the disease  has become  a tool within  both  the political  system  and media  system  to \ndrive  discourse  and disagreement.  This  comes  at a time  when  incorrect  information  spread \nonline  is still believed  by its audience  (Moravec  et al, 2019).  Within  the United  States,  only \nthirty-three  percent  of people  agree  that news  organizations  are trustworthy  (Newman  et al, n.d.). \nThis  perception  that news  organizations  are biased  can come  from  what  information  is presented,  \nhow it is presented,  and through  what  systems  the information  is shared.  These  news  \norganizations  are caught  between  the financial  incentive  to entertain  and journalistic  ethics  for \n1\n\ntruthfully  informing.  Journalism  is expected  to serve  as the watchdog  role, meaning  it should \ninform  the public  about  those  in power  so they can take appropriate  action  (Francke,  1995).  \nSome  news  organizations  still work  to fill the watchdog  role,  but also recognize  that this may be \naffected  by the need  to create  content  that their  audiences  find appealing  (Nelson  and Tandoc  Jr., \n2018).  These  practices  go against  the core beliefs  of the field of journalism.\nThe Society  of Professional  Journalists  (2014)  states  that ethical  journalists  should  act \nwith  and strive  for integrity,  and report  accurate,  thorough,  and fair news.  This  isn't  always  the \ncase though.  There  have  been  many  cases,  just in the past two decades,  like Jayson  Blair's  \nplagiarism  in the New  York  Time  in 2003,  to Brian  Williams  and the fabrication  of a dangerous \nhelicopter  ride in Iraq,  to the criticism  of NBC  by the Society  of Professional  Journalists  (SPJ)  \nfor inviting  on air a retired  army  general  for analysis  of the Iraq War as he was being  paid to be a \nconsultant  for defense  services  and earning  income  from  the war (Society  of Professional \nJournalists,  2008).  This  goal for journalistic  integrity  is key to the influence  and power  for the \nfield,  and is necessary  to keep  those  in power  accountable,  while  keeping  the public  informed  \n(SPJ,  2014).  The main  purpose  of this research  is to analyze  ways  in which  major  news  \norganizations  use tools  of bias in their  online  news  communication  to influence  their audience  \nmembers  on a health  topic  (Shao  and Hao,  2020),  and to analyze  the overall  communication  \nstrategies  from  both  organizations.  Tools  of bias can be ways  in which  information  is presented,  \nexcluded,  or interpreted  for an intended  interpretation  from  an audience.  Because  these  \norganizations  are self-identifying  as news  organizations,  they can and should  be held to \njournalistic  standards.\nCOVID-19  was selected  because  it is an apolitical  topic,  as a pandemic  does  not have  any \ninherent  political  bias in itself,  but the coverage,  resulting  opinion  from  the public,  and \n2\n\nmanagement  of the pandemic  can. Political  news  coverage  of public  crises  can directly  impact  \ngovernment  policy,  as well as the actions  from  the general  public,  who may change  the reality  of \nthe situation.  This acceptance  or denial  of a health  crisis  can have  real world  impacts,  and news  \norganizations  have  significant  influence  over  individuals'  beliefs  (Kleinnijenhuis  et al, 2020).\nThe content  from  MSNBC,  NPR,  and Fox News  was selected  as they are a popular  \nsource  of news  information  in the U.S. (Grieco,  2020).  Online  digital  content  was also selected  \nas it has become  a popular  channel  to collect  information,  with more  than eighty  percent  of \nAmericans  reporting  that they get their  news  from  digital  devices  (Shearer,  2021),  it is critical  to \nunderstand  what  is being  spread  by major  news  organizations  on online  platforms,  where  \ninformation  can be so easily  disseminated.  This new digital  medium  poses  new challenges  and \nopportunities  for interactive  communication  that are vastly  different  from  broadcast  methods  \nsuch  as radio  and television  available  just a generation  ago. The ability  to share  information  in a \ndigital  setting  so easily  (Purcell  and Rainie,  2019),  along  with audiences  gravitating  towards  \ninformation  online  that aligns  with their own views  (Schmidt  et al, 2017),  leads  to a need  for a \ncritical  analysis  of the tension  between  the financial  motivation  of entertainment  and journalism \nethics,  given  the increased  ability  to tailor  to audience  preferences.\n3\n\nBackground\nPolitical  Landscape  in 2020\nThe political  divide  within  the United  States  was evident  during  the year 2020,  and the \nCOVID-19  pandemic  was not exempt  from  feeling  this schism.  With  2020  being  an election  \nyear,  politics  in the country  were contentious,  with  both  Democrats  and Republicans  seeing  the \nother  as possibly  causing  lasting  harm  on the country  if their opponent  won (Pew  Research  \nCenter,  2020a).  With  such a dramatic  view  on the election,  the pandemic  occurring  didn't  help to \ncreate  bridges  to cross  the divide.\nThe public's  perception  on how the country  was handling  the pandemic  was split down  \npolitical  party  lines.  Those  identifying  as Republican  view  the response  with a near eighty  \npercent  favorability,  while  only twenty-nine  percent  of those  not identifying  as Republican  see \nthe response  as well-handled  (Pew  Research  Center,  2020b).  With  a divide  so clear on a major  \nissue,  it should  also come  as no surprise  that over three  quarters  of Americans  feel that the \ncountry  is more  divided  than ever (Devlin  and Connaughton,  2020).  This divide  is important  to \nnote to better  understand  the landscape  in which  news  organizations  are being  examined,  \nparticularly  for bias around  the subject  of COVID-19,  an apolitical  subject,  which  impacts  public  \nperception  and management  of response.\nDigital  Media  and McLuhan\nThe term  \u201cmedia \u201d is often  used to describe  the actual  news  organizations  themselves,  but \nin this research  digital  media  will be defined  as the content  and delivery  method  from  news  \norganizations  being  presented  to their audiences  through  digital  channels  and platforms.  To \nfurther  expand,  the term digital  media  means  both the content  being  produced  by news  \norganizations  and being  posted  on the internet,  whether  it be social  media  or news  articles  posted  \n4\n\non their  websites  and viewed  by wide  audiences  through  a variety  of devices.  Digital  media  \nfurther  includes  visual  content  produced  and published  on their  websites  intentionally,  or \nsegments  of broadcast  media  then reposted  digitally.  Broadcast  is defined  by the Oxford  English  \nDictionary  as media  distributed  by television  or radio  (Oxford  English  Dictionary,  n.d.).  \nMarshall  McLuhan  coined  the phrase  \u201cmedium  is the message, \u201d meaning  that the medium  in \nwhich  information  is sent has the actual  impact,  while  the information  sent is supplementary  \n(McLuhan,  1964).  With  a shift  from  older  broadcast  mediums  with limited  access  points,  to a \nnew digital  medium  with greater  diversity  of access  points,  presentation  of information  can have  \nmore  impact  on audience  segmentation.  Information  presented  to look like news  coming  from  \ntraditional  media  can be perceived  as more  credible  online,  regardless  of what  the information  is \nactually  saying  (Curry  and Stroud,  2017).  To understand  the impact  of perceiving  information \nshared  online  and portrayed  as credible  to be true, it is important  to understand  how information  \ncan be shared  over  these  new mediums  now.\nIt is key to understand  the shifts  from  traditional  media  systems  such as broadcast  media,  \nto digital  media  in terms  of McLuhan's  theories.  McLuhan  examined  the shift from  older  written  \nmediums  to broadcast  mediums  and foresaw  a shift to a new global  medium  where  information  \nand views  can be shared  and becoming  connected  individually,  but on a global  scale.  His term \n\u201cthe global  village \u201d was used  as a metaphor  to illustrate  this: the interconnectedness  of a small  \nvillage,  but those  connections  spread  out and are constant  on a global  scale  through  a new \nmedium  with the increased  ability  to communicate  interpersonally  at broadcast  scales.  What  this \nmeans  is that we can share  information  quickly  and often  similar  to a small  community,  but now \nwith  the ability  to communicate  around  the world.  Comparing  this to news  organizations  and \nmedia,  there  is now a shift from  the broadcast  abilities  of older  mediums,  to a new digital  \n5\n\nmedium  with interpersonal  capabilities  and a more  interactive  audience  around  the world.  This \nrapid  dissemination  of information,  as well as discussion  across  vast differences  in \u201cthe global  \nvillage, \u201d can have  much  larger  impacts  than traditional  media  systems,  over  a shorter  window  of \ntime.  To better  understand  these  impacts,  we need  to look at agenda-setting  theory,  framing,  and \ndigital  pump-valves.\nAgenda-Setting,  Frames,  and Pump-Valves\nWhile  McLuhan's  work  looks  at mediums  and media  itself  it is important  to understand  \nhow those  can be influenced.  With  the idea of looking  at influence  on media  and the medium  in \nmind,  a guiding  theory  of this research  is digital  pump-valves  (Entman  and Usher,  2018).  To \nbetter  understand  pump-valves,  it is key to understanding  the theories  that have  come  before  it.\nAgenda-Setting  Theory  originated  from  a study  conducted  by Maxwell  McCombs  and \nDonald  Shaw  called  the Chapel  Hill Study.  The study  showed  residents  of Chapel  Hill, North  \nCarolina  often  reported  that the issues  the local  and national  news  media  were  consistently  \nreporting  on as the most  important  election  issue  (1972).  McCombs  and Shaw  used this study  to \nshow  that news  organizations  have  the ability  to influence  the perceptions  of their audience  on \nwhat  information  and topics  are important  which  biases  the objective  information,  but they did \nnot examine  how topics  are discussed  at an interpersonal  level,  which  is where  framing  is \nappropriate  to consider.\nThis  need  to examine  the influence  on topics  on an interpersonal  level  leads  to the \nconcept  of framing.  While  agenda-setting  theory  determines  the issues  discussed,  the concept  of \nframing  provides  the link to how topics  are discussed,  a different  but key insight.  Framing,  best \ndescribed  by Entman,  is the,\u201d[selection  of] some  aspects  of a perceived  reality  and make  them  \nmore  salient  in a communicating  text, in such  a way as to promote  a particular  problem  \n6\n\ndefinition,  causal  interpretation,  moral  evaluation,  and/or  treatment  recommendation  for the item \ndescribed, \u201d (p. 52, 1993).  In other  words,  to frame  something  is to show  deliberately  selected  \naspects  of the situation  to convey  a certain  perspective  on an issue  or topic.  Because  of this, news  \norganizations  have  a great  deal of influence  on how issues  are perceived  based  on what  and how \ninformation  is presented  to audiences.  This shows  a key difference  between  agenda-setting  \ntheory  and framing,  where  both can work  together:  agenda-setting  theory  can determine  what \nissues  are covered,  and what  frames  are applied  to determine  how we think  about  those  topics  \n(Mateus,  2020).  This leads  to the theory  of digital  pump-valves.\nThe digital  pump-valve  theory  was created  by Entman  and Usher  created  (2018)  to better  \naccount  for modern  digital  media  environments  and their  control  on the flow  of information.  To \nbetter  understand  a theoretical  pump-valve,  and description  of a physical  pump-valve  is needed.  \nA valve  is used  to control  the flow of water  through  the pump.  When  looking  at the theory,  we \ncan look at how each  theoretical  pump-valve  controls  the flow  of information  through  the digital  \nformat.  The new digital  features  from  all institutions  can either  pump  (strengthen),  or restrict  \n(weaken)  the communication  channels  from  the news  organizations.  The theory  recognizes  how \nmodern  tools  are used  to make  information  seem  interpersonal  and breaks  political  information  \ncommunication  channels  into five digital  pump-valves:  analytics,  platforms,  algorithms,  rogue  \nactors,  and ideological  media.  These  pump-valves  are the area where  the tension  between \nentertainment  and ethical  journalism  is being  played  out. It is important  to note that pump-valves  \ncan use each  other. Analytics  is data about  audiences  and their behavior  and reaction  due to \nnews.  This data about  audience  response  can shape  content  and delivery  methods.  Platforms  are \nsoftware  that is home  to content  created  by people  outside  of the platform  owner.  Examples  of \nthis would  be social  media  companies  like Twitter  and Facebook,  who structure  delivery  and \n7\n\noften  operate  under  financial  models  not developed  to promote  a code  of ethics  similar  to the \nSociety  of Professional  Journalists.  Algorithms  are processes  for taking  input  from  media  \ncreators  and then channeling  output  to audiences  based  on a specific  system  of steps  and rules  to \nmimic  interpersonal  communication  on a global  scale.  Rogue  actors  are those  that are working  \noutside  of the typical  media  corporate  structure  of traditional  broadcast  organizations,  often  \nusing  new digital  tools  or other  digital  pump-valves  to share  their  information.  Examples  of these  \nwould  be bots or hackers.  Lastly,  ideological  media  is defined  as more  traditional  news  \norganizations.  These  are news  sources  that have  been  around  since  before  the rise of the internet  \nand the use of social  media;  MSNBC,  FOX,  and NPR  are all examples.  Ideological  media  is the \nfocus  of this study  in relation  to how they use secondary  pump-valves.\nAs stated  before,  NPR,  MSNBC,  and Fox News  fall under  the digital  pump-valve  of \nideological  media.  Both  traditionally  created  content  for broadcast  mediums  such  as television  \nand radio,  but now are creating  more  digital  media  due to the rise of the internet.  Both  MSNBC  \nand Fox News  produce  generally  similar  content  consisting  of online  articles,  social  media  posts,  \nand broadcast  media  that is then edited  down  to a digital  format  and posted  on their  website.  The \nprimary  difference  between  the two organizations  is their  intended  audience.  MSNBC  is \ntraditionally  associated  with a more  left-leaning  or liberal  audience  (Pew  Research  Center,  \n2019).  MSNBC  is a part of NBCUniversal  LLC,  a mass  media  organization  owned  by Comcast.  \nFox News  is associated  with  right-leaning  or conservative  audiences  and has a right-leaning  slant  \n(Budak  et al, 2016).  Fox News  is a part of Fox News  Media  which  is owned  by the Fox \nCorporation,  which  was founded  by Rupert  Murdoch,  an Australian-born  media  mogul.  The \nnews  organization  is still primarily  owned  by Murdoch  and his family.  NPR  is a private  and \n8\n\npublicly  funded  radio  broadcasting  organization  that began  in 1970.  The organization  was \ncreated  by the Public  Broadcasting  Act of 1967,  and has stations  spread  across  the U.S. \nDefining  Media  Bias\nThe term  \u201cmedia  bias\u201d has become  a difficult  term  to define,  due to the evolving  media  \nlandscape  with information  now being  shared  across  new mediums.  These  new mediums  lead to \nnew methods  in which  bias is shown,  and the subjectivity  of how each individual  determines  \nwhat  is biased.  A useful  beginning  for identifying  bias,  is to define  the inverse:  what  is not bias. \nHopmann,  Van Aelst,  and Legnante  (2011)  analyze  bias in a two-party  political  system  like the \nUnited  States,  and a central  component  of maintaining  objectivity  is balance  in coverage  of both \nparties.  In some  situations,  the balance  of coverage  cannot  be equal  though.  On certain  topics,  \none party  or the other  may take a stance  that requires  more  coverage,  and possibly  more  criticism  \nor affirmation,  than the other. An interesting  concept  that Hopmann,  Van Aelst,  and Legnante  \nuse in determining  bias is examining  the favorability  towards  the evaluation  of actors  (2011).  If \nthis is expanded  to also include  evaluations  of political  parties,  this favorability  can be a key \nidentifier  of bias in media.  This  builds  into the work  of Kenski,  Jamieson,  and Lichter,  who \nstruggle  to define  this term  in their own research  but provides  examples  of what  it serves  best \n(2017).  Kenski  et al states  that most  previous  research  has focused  on the study  of bias in relation  \nto political  ideology  and negativism  within  news.  This research  looks  to use both  the favorability  \nwithin  coverage  as well as the negativism  within  news  together  in relation  to bias:  bias in digital  \nnews  media  as an attempt  to advance  a certain  political  ideology,  and/or  the focus  of negative  \naspects  of any opposition.\n9\n\nThe research  questions  for this are listed  below:\nRQ1:  What  were  the overall  communication  strategies  of both Fox News  and MSNBC  \nduring  the initial  spread  of COVID-19  in the United  States?\nRQ2:  What  bias is shown  through  these  communication  strategies?\n10\n\nMethodology\nI do not personally  follow  or typically  consume  any media,  digital  or other,  from  any of \nthese organizations.  As someone  with a background  in studying  journalism  and digital  media,  I \ndo not follow  these  organizations  because  their reporting  style does  not personally  appeal  to me. \nTherefore,  I do not typically  consume  media  from larger  news  organizations  such as these.  The \ndata streams  were  also selected  because  of my interest  and background  study  in digital  media  \nand culture.  Audio  transcription  and analysis  was done after taking  a course  in linguistic  \nanthropology  and recognizing  the importance  of language  in power  and knowledge.  \nTranscription  was incorporated  into the work  after looking  at the work  of indexing  (Silverstein,  \n2003)  and the analysis  of other  political  media  members  in other  mediums  (Shoaps,  1999).\nDigital  ethnographies  were completed  for the main  webpages  for all three organizations.  \nA content  analysis  of online  article  titles from the three organizations  was conducted.  Then,  a \nrhetorical  and content  analysis  of digital  broadcast  news segments  posted  on both websites  and \nsocial  media  was conducted.  Digital  ethnographies  were completed  for the main  webpages  for all \nthree  organizations.  Lastly,  a content  review  of tweets  from all three  organizations  was done.\nThe reasoning  for the collection  of data was to look at the overall  communication  \nstrategies  from the organizations  during  the month  of March  2020. The intent  was to look at \nwhat  was being  reported  on, how often  was this being  reported,  how were  they sharing  this \ninformation,  and what  was actually  being  stated  in their  reporting.  With  this information  \ncollected,  insight  could  be gathered  on overall  communication  and if there  was any bias within.  \nWithin  table 1, all of the data collected  is listed,  with \u201cyes\u201d meaning  that the data was available  \nor applicable,  and \u201cno\u201d meaning  that the data was not applicable  or available.\n11\n\nTable  1: Data  streams  collected\nMSNBC NPR FOX\nCOVID-19\nTweets Yes Yes Yes\nCoronavirus  \ntweets Yes Yes Yes\nArticle  counts Yes Yes Yes\nArticle  title\ntheme  coding  \ncounts Yes Yes Yes\nLong  form  \narticle  style Yes Yes No\nShort  form \narticle  style No No Yes\nAudio  \ntranscription Yes Yes Yes\nVideo  Analysis Yes No Yes\nMain  webpage  \nethnography Yes Yes Yes\nThe three  news  organizations  selected  were  MSNBC,  Fox News,  and National  Public  \nRadio  (NPR).  MSNBC  and Fox News  were  selected  as they fall on opposite  locations  on a media  \nbias scale  according  to Media  Bias/Fact  Check  (Search  and Learn,  2020).  NPR  was selected  as a \ncontrol  because,  although  it is slightly  left-centered  according  to the website,  it was more  central  \nthan both of the other  organizations  and can be used  for comparison  . To look at each \norganization's  communication  strategy,  research  was done  over  multiple  channels  to discover  the \nscope  of the digital  communication  (Wellbrock,  Kure,  and Buschow,  2020).  To do this, articles  \nposted  to each  organization's  website  were  counted,  as well social  media  posts  on Twitter.  \nArticle  Sampling\nThe sampling  time  period  was from  March  1-28,  2020  and was selected  because  this was \nthe time period  when  COVID-19  infection  rates  first began  to rise in the United  States  (Johns  \n12\n\nHopkins  University  Coronavirus  Resource  Center,  2020).  The time period  of higher  infection  \nrates  in the United  States  was selected  due to an expected  increase  in coverage,  since  both \nMSNBC  and Fox News  are U.S.-based  organizations.  This  time  period  was divided  into four \nseven-day  periods:  March  1st-7th,  March  8th-14th,  March  15th-21st,  and March  22nd-28th.  This \nwas done  to see whether  there  would  be any change  in content  over  time,  as well as to separate \ndata into manageable  groups.  Online  articles  were  searched  for in each  organization's  search \nengine,  with \u201carticle \u201d being  a content-type  parameter.  Multiple  searches  were  done,  using  either  \n\u201ccoronavirus \u201d or \u201cCOVID-19 \u201d as the search  term. Due to the large  number  of articles  from  Fox \nNews,  the section  parameters  of \u201cU.S,\u201d ''Tech, \u201d ''Science, \u201d ''Health, \u201d and \u201cWorld \u201d in Fox News'  \nown search  engine  were  used  to focus  on reporting  of the actual  virus.  NPR  article  identification  \nwas done  through  advanced  Google  searches  as the NPR  website  would  not allow  enough \narticles  to be displayed  to see all of the determined  time  frame.  The search  terms  were  entered  \ninto the search  bar, as well as narrowing  the results  to NPR.org . The weekly  time frames  were  set \nas search  parameters,  and then each  link was selected  individually  to determine  if it was an \narticle.  The content  displayed  needed  to focus  on the written  content  to be listed  as an article.  \nThis  means  that the content  needed  to have  more  than one to two paragraphs  describing  the audio  \ncontent  presented.\nGrounded  theory  coding  was utilized  in this research.  This  approach  means  that as \narticles  were  counted,  themes  were  determined  to code  for, and as more  coding  for themes  \nhappened,  if a new theme  became  evident  it was then coded  for as well (Hernandez,  2009). \nOnline  article  titles  were  individually  analyzed  and coded  into four separate  themes  dependent \non the content  in their  titles.  Those  themes  were:  health  tools  and testing,  business  impact,  \nimmigration,  infection  and mortality  rates,  and political  discourse.  These  themes  were  \n13\n\ndetermined  by examining  what  the five most  common  topics  covered  by online  articles  and \nmedia  during  this time.  Examples  of terminology  and concepts  associated  with the health  tools  \nand testing  theme  are: supplies,  testing  kits, medical  supplies,  drugs,  and vaccines.  Examples  of \nterminology  and concepts  associated  with the immigration  theme  are: immigrant,  migrant,  and \ninternational  travel. Examples  of terminology  and concepts  associated  with the business  impact  \ntheme  are: business,  stimulus,  price,  company,  and layoff.  Examples  of terminology  and \nconcepts  associated  with the infection  and mortality  rates  theme  are: infection,  case,  spread,  \ninfection  statistics,  spike,  drop,  epicenter,  death,  and case.  Lastly,  examples  of terminology  and \nconcepts  associated  with political  discourse  will be politicians  or political  parties  such  as Trump,  \nPelosi,  Democrat,  Republican,  or Senate.\nAudiovisual  Sampling\nAudiovisual  segments  were  selected  for rhetorical  analysis  from  leading  contributors  at \nboth  MSNBC  and Fox News  as well.  News  clips  were  selected  from  Rachel  Maddow  and Tucker  \nCarlson  as they were  two of the highest  rated  news  broadcasts  during  the first quarter  of 2020 \n(Grynbaum  2020  and Joyella,  2020)  . The dates  of clips  available  was narrowed  into the last two \nweeks  of March  as the reported  cases  started  rising  dramatically.\n14\n\nPrevention,  2020).Figure  1. COVID-19  Cases  by day in the U.S. (Center  for Disease  Control  and\nSegments  were  selected  with similar  topics,  primarily  focused  on states'  responses  to the \npandemic.  Then,  using  the advanced  search  feature  on Twitter,  the term \u201cCOVID-19 \u201d was \nsearched  on each  of the presenters'  twitter  accounts,  and the clips  were  selected.  An audio  \nsegment  from  NPR  was selected  based  on the leading  suggestion  for a daily  broadcast,  Morning  \nEdition , and a news  segment  was selected  based  on a similar  date with the two other  audiovisual  \nclips  from  the other  news  sources.  The audio  of the Fox News  segment  was transcribed  in \nInqscribe.  The NPR  and MSNBC  segments  were  transcribed  in Google  Docs,  and then analyzed  \nfor rhetorical  tools  in language,  and significance  of word  choice  and grammatical  structure.  The \nvisual  production  of the two audiovisual  segments  were  analyzed  by looking  at language,  \nsentence  structure,  and production  decisions,  including  supplemental  clip selection  during  the \nbroadcast.\nTweet  Sampling\nTweets  were  collected  using  a similar  time  period.  Using  Twitter's  advanced  search  \nfunction,  both \u201ccoronavirus \u201d and \u201cCOVID-19 \u201d were  searched  for in tweets  from  each  new \n15\n\norganizations'  main  twitter  handle,  those  being  @MSNBC,  @NPR,  and @Foxnews.  Tweets  \nwere  then individually  counted  and analyzed  to see if links  in the tweet  were  directed  to an \nonline  article  from  the news  organization.  The online  website  tool Tally  Counter  Store  \n(https://tallycounterstore.com/ online-counter )was used to count  articles  for each website  to \nmaintain  consistency.  This website  has a counter  system  with both addition  and subtraction  \nbuttons,  where  every  click of the addition  button  raises  the number  displayed  on the screen  by \none, and every  click  of the subtraction  button  lowers  the number  by one. There  is a symbol  to \nthen restart  the count  back  to zero.\n16\n\nResults\nRQ1:  What  were  the overall  communication  strategies  of both Fox News  and MSNBC  \nduring  the initial  spread  of COVID-19  in the United  States?\nRQ2:  What  bias is shown  through  these  communication  strategies?\nWebsite  Ethnographies\nBoth  MSNBC  and Fox News  have  similar  websites  that mirror  each  other  with their\nstyles.  MSNBC's  main  webpage,  https://www.msnbc.com/ , has a menu  row across  the top, with \noptions  leading  to different  content  styles,  including  their  live broadcast.  In the top right corner,  \nthere  is a magnifying  glass symbol  that allows  visitors  to search  the website  for specific  content.  \nBelow  this is another  row menu  with links to specific  media  personalities  and shows  to view  \ntheir content.  The main  focus  of the page is below  this. A large  banner  with the three leading  \ngoes across  the middle  of the page  with large  images  and titles  for major  news stories  the website  \nis highlighting  at the time. Below  this are smaller  stories  and links  to other  articles  and video  \ncontent  on the website.  When  searching  for articles,  I selected  the magnifying  glass symbol  and \nwas brought  to a search  screen  and was given  the options  for filters  of content  type and date to \nnarrow  in on specific  content.  The overall  design  of the webpage  is to look sleek  and modern,  \nwith black,  blue, and white  being  the central  color  themes  at the top of the page. Article  length  \non MSNBC  differs  from article  length  on Fox News.  The few articles  posted  by MSNBC  during  \nthe sampling  period  followed  a long-form  format,  with multiple  sources  listed  and images  \nshown,  with word  counts  typically  above  of 800 words,  and similar  to an investigative  article.\nThe Fox News  main  webpage,  https://www.foxnews.com/# , is structured  similarly  to \nMSNBC  with some  key differences.  When  the website  is visited  there  is a similar  top menu  \nacross  the web page linking  to different  content  types  and subjects,  with a white  magnifying  \n17\n\nglass  symbol  in the top right  corner  as well.  Below  this, there  is a single  main  banner  image  with \na title listed  below  it, showcasing  the main  news  story  at the time  from  the website.  Similarly,  to \nMSNBC,  further  down  on the web page  are links  to other  online  articles  and digital  media  across  \na range  of subjects.  I selected  the white  magnifying  glass  symbol  when  searching  for online  \narticles  and was given  a menu  with  multiple  options  and parameters  to narrow  in focus  of search.  \nThe overall  color  scheme  of the website  is a dark navy  blue,  red, and white  conveying  a sense  of \npatriotism  throughout  the website.  As stated  earlier,  the article  style  of Fox News  was different  \nfrom  MSNBC.  The articles  published  by Fox News  were  of shorter  length,  typically  only a few \nparagraphs  and around  400 words,  with one or two main  sources  and quotes,  with an occasional  \nimage  or video  clip posted  above  the article.\nThe NPR  main  webpage,  found  at https://www.npr.org/ , has an initial  similar  format  to \nboth of the other  news organizations,  but it has some  important  differences  due to the content  \nproduced.  The overall  background  of the website  is a light  gray, with content  and menus  having  \na white  background.  The NPR  logo is located  in the top left corner  of the webpage,  with a red \nbutton  labeled  \u201cdonate  now\u201d with a heart  symbol  located  in the top right  corner.  Next  to this \nbutton  is an option  to sign into an NPR  account,  and a link to the NPR  shop. In the far-right  \ncorner  on the top of the screen,  there  is a dark blue square  with a play symbol  providing  the \nopportunity  for a listener  to begin  listening  to the NPR  24-hour  program  stream,  the hourly  news,  \na direct  feed to what  is being  played  live, or a playlist.  Beneath  this top bar is a dark gray  bar, \nwith links in white  text linking  to different  content  types such as \u201cNews \u201d, \u201cArts & Life\u201d, \n\u201cMusic \u201d, \u201cShows  & Podcasts \u201d, and a search  option.\nBeneath  the top bar is an area where  content  previews  and stories  are displayed.  With one \nstory  taking  the primary  focus  of the screen  as the visually  largest  preview,  as well as being  \n18\n\nplaced  above  all other  stories,  multiple  smaller  story  previews  are displayed  below.  After  the \ninitial  top story  with  multiple  smaller  stories  listed  underneath,  there  is an \u201cEditor's  Choice \u201d \nstory,  followed  by a listing  of stories  similar  to both MSNBC  and Fox News.  The search  function  \nin the NPR  website  has a text area where  a search  term  can be entered.  Beneath  this are multiple \nfilters  to narrow  in on content.  There  is an option  to select  if the search  term  was \u201cheard  on air,\u201d \nor to narrow  in by date,  with  the date options  being  the date of the search,  the past seven  days, \nthe past thirty  days,  past 90 days,  or the past year.  There  is also a drop-down  menu  to narrow  the \nsearch  according  to program  type.  Stories  are then displayed  from  multiple  channels  with \nheadlines  and dates  listed,  and the search  term  highlighted  in yellow  in either  the headline  or \nstory  preview.  The articles  written  by NPR  are both  long-form  and short  form,  with  similar  word  \ncounts  for each  category  compared  to the other  organizations.\nArticle  Counts  and Title  Themes\nFox News  published  more  articles  than MSNBC  (Tables  2,3,4)  . NPR  was found  to have \nan article  count  in between  both organizations,  with  more  articles  published  than MSNBC,  but \nfewer  than the large  number  consistently  published  by Fox News.\nTable  2: Number  of articles  per week  from  each  organization  searching  \"COVID-19\"\nWeek MSNBC FOX NPR\n3/1-3/7 1 194 59\n3/8-3/14 1 461 65\n3/15-3/21 2 685 279\n3/22-3/28 3 710 190\n19\n\nTable  3: Number  of articles  per week  from  each  organization  searching  \"Coronavirus\"\nWeek MSNBC FOX NPR\n3/1-3/7 1 355 71\n3/8-3/14 1 825 81\n3/15/3/21 2 1327 164\n3/22-3/28 3 1424 208\nTable  4: Number  of Fox News  articles  each  week  using  search  terms\nCoronavirus COVID-19\n3/1-3/7 71 141\n3/8-3/14 149 226\n3/15/3/21 308 403\n3/22-3/28 353 434\nWhen  looking  at the articles  published  by the news organizations,  it was noticed  that \nMSNBC  had long-form  articles,  while  Fox News  published  short-form  articles.  NPR  printed  a \ncombination  of both long-form  and short-form  articles,  tending  to be more short-form.  NPR also \nlinked  to their own organization's  content  in more  tweets  compared  to the other  organizations  \n(Tables  5 and 6).\n20\n\nTable  5: Number  of tweets  from  each  organization  that link to their  own  articles  when  \nsearching  \u201cCOVID-19 \u201d\nWeek MSNBC FOX NPR\n3/1-3/7 0 0 18\n3/8-3/14 0 0 16\n3/15-3/21 0 2 34\n3/22-3/28 0 0 42\nTable  6: Number  of tweets  from  each  organization  that link to their  own  articles  when  \nsearching  \u201cCoronavirus \u201d\nWeek MSNBC FOX NPR\n3/1-3/7 0 0 43\n3/8-3/14 0 0 39\n3/15-3/21 0 12 38\n3/22-3/28 0 21 72\nIn terms  of themes  coded,  MSNBC  focused  primarily  on health  testing  (Table  7). \nConversely,  Fox News  focused  primarily  on infection  and mortality  rates  (Table  8). NPR  was \nfound  to focus  on political  discourse  and business  impact,  the former  theme  actually  being  \nincluded  after  seeing  the large  amount  of content  NPR  was creating  around  it (Table  9). When  \ninfection  rates  within  the United  States  were  rising  at the fastest  rates  of the month  according  to \nthe Johns  Hopkins  coronavirus  resource  center,  the weeks  of 3/15-3/21  and 3/22-3/28,  NPR  \ncreated  articles  most  aligned  with themes  of political  discourse  and business,  while  Fox News  \ncreated  articles  most  aligned  with the theme  of infection  and mortality  rates.  NPR  articles  \ndiscussing  political  discourse  and business  during  these  weeks  were  primarily  focused  on the \n21\n\nactions  and criticisms  of the Trump  administration's  handling  of the pandemic  at the time,  and \non the COVID-19  relief  bill that was being  created  and voted  on at the time.\nTable  7: Article  counts  by theme:  Health  tools  and testing\nWeek MSNBC FOX NPR\n3/1-3/7 1 14 13\n3/8-3/14 1 18 16\n3/15-3/21 1 31 13\n3/22-3/28 2 43 33\nTable  8: Article  counts  by theme:  Infection  and mortality\nWeek MSNBC FOX NPR\n3/1-3/7 0 43 18\n3/8-3/14 0 43 18\n3/15-3/21 0 91 21\n3/22-3/28 0 99 34\nTable  9: Article  counts  by theme:  Political  discourse\nWeek MSNBC FOX NPR\n3/1-3/7 0 35 10\n3/8-3/14 0 56 18\n3/15-3/21 0 60 28\n3/22-3/28 0 83 84\n22\n\nTable  10: Article  counts  by theme:  Business  Impact\nWeek MSNBC FOX NPR\n3/1-3/7 0 26 19\n3/8-3/14 0 25 12\n3/15-3/21 1 46 27\n3/22-3/28 3 43 45\nTable  11: Article  counts  by theme:  Immigration\nWeek MSNBC FOX NPR\n3/1-3/7 0 1 7\n3/8-3/14 0 3 9\n3/15-3/21 0 3 11\n3/22-3/28 0 11 15\nAudiovisual  Analysis\nEach  video/audio  segment  provided  an interesting  insight  into the style of reporting  from  \neach organization,  or at least the journalist  themselves.  Both Tucker  Carlson  and Rachel  \nMaddow  utilize  the red, white,  and blue color  scheme  to evoke  some  sense  of U.S. patriotism.  \nBoth Carlson  and Maddow  are in the center  of the camera  shot, posed  as the central  focus  of the \nshot to be listened  to. Tucker  Carlson  uses more  video  clips from officials  in his video  segment  \ncompared  to the one used in Rachel  Maddow's.  Transcripts  of the audio  and audiovisual  \nsegments  can be found  in appendices  B, C, and D.\nTweet  Counts  and Linking\nTweeting  rates appear  to be inverse  from article  publishing  rates. MSNBC  tweets  more  \nthan Fox News,  with  NPR  tweeting  consistently,  but not as often as MSNBC  (Tables  12 and 13). \nSome  interesting  details  to note about  MSNBC's  tweets  is that they primarily  link to video  \n23\n\ncontent,  and in particular,  shortened  clips  from  their personalities'  broadcasts.  Another  \ninteresting  detail  is while  they never  linked  to their own online  article  content,  they did \noccasionally  link to articles  posted  on the NBC  News  website.  In terms  of Fox News'  twitter,  \ninitial  results  were  surprising  as they had not used their  main  twitter  account  from  November  \n2018  until  March  18, 2020.  This provides  insight  into their online  communication  strategy  \nduring  the COVID-19  pandemic,  as they clearly  see value  in being  able to post content  online  to \ntheir  audience  through  a quick  and simple  channel.  Also,  their  consistent  linking  to their online  \narticle  content  provides  area for more  research.  One might  assume  that they are simply  using  this \nmethod  to expose  their  online  articles  to a wider  audience,  but since  the links  redirect  the reader  \nto the Fox News  website,  they may be using  this as a tool to bring  more  audience  members  from  \na social  media  platform  like Twitter  directly  to their  website.  NPR  was once  again  between  both \nstyles  of Fox News  and MSNBC.  NPR  consistently  linked  to their own online  articles  and \ncontent  and appeared  to use twitter  as a tool to drive  audiences  to their written  content.  NPR  \nprimarily  linked  to articles  in their  tweets,  with very few links  to audio  content.\nTable  12: Tweets  per week  from  each  organization  when  searching  \"COVID-19\"\nTweets  per week  searching  \"COVID-19\"\nWeek MSNBC FOX NPR\n3/1-3/7 34 0 19\n3/8-3/14 32 0 19\n3/15-3/21 66 3 37\n3/22-3/28 115 0 47\n24\n\nTable  13: Tweets  per week  from  each  organization  when  searching  \u201cCoronavirus \u201d\nTweets  per week  searching  \"Coronavirus\"\nWeek MSNBC FOX NPR\n3/1-3/7 85 0 48\n3/8-3/14 270 0 47\n3/15-3/21 314 14 51\n3/22-3/28 301 27 93\n25\n\n26\n\nDiscussion\nWhen  discussing  these  results,  it is key to acknowledge  my own bias on the subject  \nmatter.  As mentioned  previously,  I do not consume  media  from these  news organizations,  one \nreason  being  I do not find their media  personally  appealing.  As someone  with a background  \nstudying  journalism,  with a strong  emphasis  on ethics  and partisan  reporting  styles,  I do not \npersonally  seek the styles  of these  news organizations,  leading  to an approach  through  a very \ncritical  research  lens. In a best attempt  to be as impartial  as possible,  the SPJ code of ethics  will \nbe used to compare  with the data collected.  This is done so that the analysis  is not evaluated  by \nown personal  values,  but an established  code of ethics  from a respected  organization.\nResearch  Question  One Discussion\nResults  for all data sets yielded  interesting  and unexpected  results.  Expectations  before  \ncoding  of article  counts  and themes  were  that article  counts  would  be similar,  with possibly  more  \nonline  articles  written  by MSNBC  and NPR  considering  that those  organizations  are left-leaning  \nand would  be more  critical  of the response  to the pandemic.  As proven  by the data, not only was \nthis loose  expectation  that they would  write  more articles  unsupported,  but it was also shown  to \nbe the complete  opposite.\nTo evaluate  the bias in these  communication  strategies  when  looking  at the research  \nquestions,  we can begin  with the first on the overall  communication  strategies  of the \norganizations.  All three  organizations  had similar  main  websites  that can be used to control  the \nflow of information.  These  organizations  are ideological  media  pump-valves  and are using  their \nmain  website  to exert  their  control  on the articles  shown  to their audience.  This dictates  the flow \nof information  to their audience,  determining  what  information  their audience  can actually  \nconsume.  This determination  of the information  discussed  is an example  of agenda-setting  \n27\n\ntheory,  where  the organization  is deciding  what  topics  are being  pushed  through  the platform  \nvalve  of a traditional  website  to further  control  of information  on platforms  where  increased  \ninterpersonal  feedback  can occur  with the analytics  pump-valve  of audience  interaction.\nThese  ideological  media  pump-valves  then decide  how much  each  topic  should  be \nfocused  on. MSNBC  consistently  reported  on health  tools  and testing,  and also the business  \nimpact  with all of the articles  written  focused  on these  two themes.  Fox News  wrote  more  \narticles  than MSNBC  and wrote  more  often  about  infection  and mortality  rates  than either  of the \nother  two organizations.  NPR  wrote  articles  primarily  on political  discourse  and the business \nimpact.  All three  organizations  use their  website  to determine  what  information  is shared,  and \nthrough  the themes  within  their  articles,  determine  how that information  is discussed.\nThese  organizations  also used  their  social  media  to control  what  information  is covered  \nand how.  NPR  consistently  linked  to their  online  articles  using  their Twitter,  with  the highest  rate \nout of all three  organizations.  Halfway  through  the month  of March,  Fox News  decided  to start \nusing  their main  Twitter  account  to start tweeting  about  COVID-19,  often  linking  to their  own \ndigital  articles.  While  MSNBC  didn't  link to their  own digital  articles,  they tweeted  more  than \nthe other  two organizations,  and would  link to digital  media  from  other  organizations  as part of \nNBCUniversal.  This shows  these  ideological  media  pump-valves  using  secondary  pump-valves  \nof platforms  to spread  the information  that they decide  to cover.  This control  over  what  is \ntweeted,  how often  it is tweeted,  and this linking  back  to digital  media  from  the organization  is \nmade  possible  through  the digital  medium.\nThese  platforms  also allow  for audience  response  and feedback,  as well as this \ninterpersonal  engagement  that comes  with  the digital  medium.  This feedback  from  audiences  \nproduces  the data involved  in the analytics  pump-valve.  Again,  these  ideological  media  pump \u00ad\n28\n\nvalves  use these secondary  analytics  pump-valves  to look at audience  engagement  and feedback  \nto determine  what  digital  media  they will produce.  This interactive  nature  between  the news  \norganizations  and their audiences  further  emphasizes  the stress  of creating  content  that is \nentertaining  to an audience,  while  also maintaining  journalism  ethics.\nOverall,  these  organizations  which  are ideological  media  pump-valves,  used multiple  \nmethods  in determining  what  digital  media  is created,  what  it is created  about,  and the flow of \ninformation  to their audience.  These  organizations  utilize  their  website  to determine  what  news  is \npresented  to their audience.  This determination  of news  is what  is described  in agenda-setting  \ntheory,  where  news organizations  can decide  what  topics  will be discussed.  These  news  \norganizations  also determine  how they cover  a topic  when  they decide  to write  more  online  \narticles  on a certain  theme  over another.  This framing  of the topic  by deciding  the article  themes  \ndetermines  how the topic  will be discussed.  These  news  organizations  are also ideological  media  \npump-valves,  that then use other  secondary  pump-valves  like platforms  and analytics  to control  \nhow their  information  is spread  and use audience  response  now available  through  the digital  \nmedium  to decide  what  digital  media  they will create.  This increases  the apparent  interpersonal  \nconnection  and feedback  strength  between  the audience  desire  and the organization's  response,  \nthus increasing  the potential  and actual  response  to fall more  towards  entertaining  their audience  \nrather  than journalism  ethics  like the Society  of Professional  Journalists'.\nResearch  Question  Two Discussion\nWhen  answering  the second  research  question,  it is best to refer  back  to the SPJ code of \nethics.  Their  statement  that ethical  journalists  should  act with and strive  for integrity,  and report  \naccurate,  thorough,  and fair news can be used to identify  bias. All three  news organizations  fall \n29\n\nshort  of this through  audiovisual  content,  coverage  of topics,  and their overall  control  on the flow \nof information  to their audience.\nAs an example,  within  appendix  B, Tucker  Carlson  on lines 76-78  interprets  the official's  \nspeech  to fit the narrative  that has been  woven  throughout  the segment.  He also utilizes  the term \n\u201cChinese  coronavirus, \u201d a term perceived  by some  to be racist,  even acknowledging  the \nclassification  himself  on line 148. Carlson  presents  segments  from state officials  discussing  the \ngeneral  public  attending  an event  with New  York  City Chinatown  as the background,  as well as \nstate Senator  John  Liu, an Asian  American,  explaining  why the public  should  not have any fears  \nto be outside.  Carlson  does not provide  context  that the speech  comes  from a press conference  on \nthe second  of February,  over a month  and a half  before  Carlson's  show,  and the officials  are \ndiscussing  the backlash  surrounding  the Chinese  New  Year  Celebration.\nWithin  Rachel  Maddow's  segment,  she provides  her own beliefs  on the subject,  and takes  \npauses  for emphasis.  On line 93, Maddow  uses the word  \u201clament \u201d to criticize  the United  States  \ngovernment's  efforts  to combat  COVID-19,  and their  methods  of support  for health  care workers  \nat the time. Maddow  also often  pauses,  whether  intentional  or unintentional,  for emphasis  of \ninformation  in her segment;  more often  than the other  two segment  hosts. These  pauses  are \nsignified  by the ellipses  in the transcripts.  While  Carlson  only pauses  twice  himself  in his \nbroadcast,  Maddow  does it twenty  times.  This consistent  use often  emphasizes  the drastic  \nconsequences  of not only COVID-19,  but the efforts  made  by the federal  government  to stop the \nspread  of the virus.  Maddow  only uses one video  clip from New  York  Governor  Cuomo  in her \nsegment.  Maddow  then repeats  impactful  statements  to end the video,  leaving  a lasting  \nimpression  of the desperate  measures  being  taken  by state governments.  Again,  emphasizing  the \ngrowing  spread  of COVID-19  within  the United  States,  on the right side of the screen  there is an \n30\n\nimage  of what  has been  often  used  as a COVID-19  cell, along  with the quote  in bold \u201cNow,  that \nsurge  has begun. \u201d On the bottom  third  of the screen,  the graphic  listed  was as breaking  news  \nwith  the statistic  that the U.S.  has reached  18,000+  cases  and 237 deaths.  Both  the statistics  and \nthe quote  are not referred  to during  the video  segment,  and the statistics  are not presented  as \nbreaking  news  verbally  during  the segment.\nIn both audiovisual  segments,  we see commentary  about  the information  by the \njournalist.  While  journalists  are allowed  to have  their  own personal  opinions,  a lack of labeling  \nthese  views  as commentary,  or separate  from  the reporting  they are doing,  is not upholding  the \nethics  journalists  should  strive  for (SPJ  Code  of Ethics,  2014).  With  no clear  delineation  of what  \nis fact and what  is opinion,  both can be interpreted  into the same  message  from  the host and can \nlead to misleading  information.\nThe NPR  audio  segment  had the least  amount  of clear  bias from  the host.  The host Noel  \nKing  is interviewing  Mayor  Betsy  Price  of Fort Worth,  Texas,  and provides  very  little  personal  \ninterpretation  of her own within  the segment.  King  asks questions  of Price  and allows  the mayor  \nto explain  the current  situation  within  the state,  and explain  measures  being  taken.  King  \nintroduces  a sound  byte from  Texas  Lieutenant  Governor  Dan Patrick  with minimal  \ninterpretation  when  introducing  the clip or when  returning  from  it. King  lets the sources  she is \nusing,  both Lieutenant  Governor  Patrick  and Mayor  Price,  provide  the information,  with  King  \nprimarily  structuring  this content  within  the audio  segment.\nAgain,  let's consider  the Society  of Professional  Journalists  and their  statement  that \nethical  journalists  should  act with and strive  for integrity,  and report  accurate,  thorough,  and fair \nnews.  When  television  broadcast  hosts  from  Fox News  and MSNBC,  both  within  the top-rated  \nshows  on their  networks,  include  personal  opinions,  along  with misleading  presentation  of and \n31\n\ninfluence  on the important  information  they are providing,  they fall short  of meeting  the \nrequirement  for fair and accurate  news.\nWe also see where  the topics  covered  by these organizations  lead to biased  journalism.  If \njournalists  should  report  accurate,  thorough,  and fair news,  then the themes  in the online  articles  \nshould  reflect  this. When  in the final week  of the month,  where  reported  cases  are increasing  \ndramatically  within  the country  according  to the Johns  Hopkins  Coronavirus  Resource  Center,  \nand NPR  reports  primarily  on political  discourse  and the business  impact,  they fall short  of being  \nthorough  in their  reporting.\nLastly,  we see pump-valves  using  other  pump-valves,  with unique  tools due to the \nmedium,  to decide  agenda  and dictate  frames,  that creates  bias. These  ideological  institutions  are \nconsidered  news, so when  information  is spread  by them online,  it is seen as news and \nconsidered  correct.  The impact  of this level of control  that the news organizations  have on the \ndigital  media  being  spread  can be seen when  compared  to the SPJ code of ethics.  According  to \nthe SPJ code of ethics  states  that ethical  journalists  should  act with and strive  for integrity,  and \nreport  accurate,  thorough,  and fair news.  With  organizations  determining  what  topics  are covered  \nand how they are covered,  they are not acting  with integrity.  When  organizations  have to decide  \nthat they should  be entertaining  rather  than ethical  to maintain  their audience,  they  cannot  always  \nreport  accurate,  thorough,  and fair news.\n32\n\nConclusion\nTo link the standards  of journalism  with  the theories  discussed,  it shows  a concerning  \nconclusion.  We see that the information  being  reported  on during  this time is determined  by the \nnews  organizations  in specific  ways  for ideological  purposes  to entertain  their  audience,  rather  \nthan follow  journalism  ethics.  They  have  determined  what  themes  they will discuss  more  often,  \nand their  hosts  have  shared  information  in a way that comes  with framing  in mind,  presenting  \nissues  in a particular  light  through  specific  pump-valves.  The news  we are receiving  is filtering  \nthrough  a system  that has determined  what  will be covered,  how it will be covered,  and in a \nrefined  way who will receive  it . If we are to link this with the SPJ code  of ethics  often  \nmentioned  so far, how can this flow  of information  reported  be seen as thorough,  accurate,  and \nfair?\nThe primary  purpose  of this research  is not to lambast  popular  news  organizations  or \nmake  a political  argument.  The primary  purpose  is to show  to those  who do not have  the \nexperience  or knowledge  to identify/recognize  these  reporting  issues,  what  is happening  in the \ncontent  they are consuming.  The goal of this research  is to hold the institutions  to the standards \nthey claim  to follow  and furthermore,  see how they are falling  short  of the values  that journalists  \nshould  strive  to uphold  based  on the SPJ code  of ethics.  This  comes  at a time when  as of \nFebruary  2021,  over  twenty-seven  million  cases  of COVID-19  have  been  reported  in the United  \nStates,  with over  five hundred  thousand  deaths.  Correct  knowledge  of what  is happening  is not \nonly  important,  but also required  for the wellbeing  of ourselves  and others  (Roozenbeek  et al, \n2020).\nThis  is not restricted  to COVID-19  or journalism  alone  though.  When  facing  other  natural  \ndisasters  like disease,  flooding,  and other  impacts  of climate  change,  some  do not adequately  \n33\n\nrespond  due to numerous  factors  (Shao  et al 2017,  Shapira  and Bar-Dayan,  2018).  As stated \nearlier  while  people  may feel split on the handling  of the COVID-19  pandemic,  the need  for \ntruthful  information  being  spread  on digital  media  is key to successfully  informing  the public  \n(World  Health  Organization,  2018). With  the bias in the information  we collect  from  our news, \ncombined  with the impact  this information  has on our perception  of major  health  crises  and \nnatural  disasters,  along  with necessity  for accurate  information  to best prepare  for such \nemergencies,  the need  for an informed  public  is key to protecting  the safety  of everyone  \ninvolved.\n34\n\nWorks  Cited\nBudak,  C., Goel,  S., & Rao,  J. M. (2016).  Fair and Balanced?  Quantifying  media  bias through  \ncrowdsourced  content  analysis.  Public  Opinion  Quarterly,  80(S1),  250-271.  \ndoi:10.1093/poq/nfw007\nCenter  for Disease  Control  and Prevention  (2020).  New  cases  by day [Online  image].  \nhttps://www.cdc.gov/coronavirus/2019-ncov/cases-updates/previouscases.html\nCurry,  A. & Stroud,  N. (2017).  Trust  in online  news. Center  for Media  Engagement.  \nhttps://mediaengagement.org/research/trust-in-online-news\nDevlin,  K., & Connaughton,  A. (2020,  September  23). COVID-19  response  approved  by most  in \n14 nations  with advanced  economies.  Retrieved  February  21, 2021,  from  \nhttps://www.pewresearch.org/global/2020/08/27/most-approve-of-national-response-to-  \ncovid-19-in-14-advanced-economies/#little-consensus-on-whether-the-pandemic-has-  \nbrought-people-together\nDoherty,  C. (2020,  August  28). Americans'  growing  partisan  divide:  8 key findings.  Retrieved  \nMarch  09, 2021,  from https://www.pewresearch.org/fact-tank/2017/10/05/takeaways-on-  \namericans-growing-partisan-divide-over-political-values/\nEntman,  R. M. (1993).  Framing:  Toward  clarification  of a fractured  paradigm.  Journal  of \nCommunication,  43(4),  51-58.  doi:10.1111/j.1460-2466.1993.tb01304.x\nEntman,  R. M., & Usher,  N. (2018).  Framing  in a Fractured  Democracy:  Impacts  of Digital  \nTechnology  on Ideology,  Power,  and Cascading  Network  Activation.  Journal  of \nCommunication,  68, 298-308.  doi:10.1093/ct/jqx019\nFrancke,  W. (1995).  The evolving  watchdog:  The media's  role in government  ethics. The \nAnnals  of the American  Academy  of  Political  and  Social  Science,  537 , 109-121.  \nRetrieved  March  9, 2021,  from http://www.jstor.org/stable/1047758\nGrieco,  E. (2020,  August  18). Americans'  main sources  for political  news  vary by party and age. \nRetrieved  March  06, 2021,  from https://www.pewresearch.org/fact- \ntank/2020/04/01/americans-main-sources-for-political-news-vary-by-party-and-age/\nGrynbaum,  M. (2020,  April  28). Tucker  carlson  beats sean hannity  as trump  briefings  give fox \nnews a boost.  Retrieved  March  05, 2021,  from  \nhttps://www.nytimes.com/2020/04/28/business/media/virus-tucker-carlson-sean-hannity-  \nfox-ratings.html\n35\n\nHernandez,  C. A. (2009).  Theoretical  coding  in grounded  theory  methodology.  Grounded  \nTheory  Review , 8(3), 51-60.\nHopmann,  D. N., Aelst,  P. V., & Legnante,  G. (2011).  Political  balance  in the news:  A review  of \nconcepts,  operationalizations  and key findings.  Journalism:  Theory,  Practice  & \nCriticism,  13(2), 240-257.  doi:10.1177/1464884911427804\nJohns  Hopkins  Coronavirus  Resource  Center.  (2020).  COVID-19  Map. Retrieved  February  1, \n2021,  from https://coronavirus.jhu.edu/map.html\nJoyella,  M. (2020,  March  10). Sean  Hannity  extends  win streak  over rachel  maddow  to 52 \nweeks.  Retrieved  March  05, 2021,  from  \nhttps://www.forbes.com/sites/markjoyella/2020/03/10/sean-hannity-extends-win-streak-  \nover-rachel-maddow-to-52-weeks/?sh=704badb92eb4\nKenski,  K., Jamieson,  K., & Lichter,  S. (2017,  July 27). Theories  of media  bias. In The Oxford  \nHandbook  of Political  Communication.  : Oxford  University  Press.  Retrieved  22 Sep. \n2020,  from \nhttps://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199793471.001.0001/ox  \nfordhb-9780 199793471  -e-44/version/0 .\nKiley,  J. (2020,  July 27). Fewer  now have mix of liberal,  conservative  views  in U.S. Retrieved  \nMarch  09, 2021,  from https://www.pewresearch.org/fact-tank/2017/10/23/in-polarized-  \nera-fewer-americans-hold-a-mix-of-conservative-and-liberal-views/\nKleinnijenhuis,  J., Hartmann,  T., Tanis,  M., & van Hoof,  A. M. J. (2020).  Hostile  media  \nperceptions  of friendly  media  do reinforce  partisanship.  Communication  Research , 47(2), \n276-298.  https://doi.org/10.1177/0093650219836059\nMateus,  S. (2020).  Porous  frontiers:  Priming  as an extension  of agenda  setting  and framing  as a \ncomplementary  approach.  Mediapolis  - Revista  De Comunica\u03c2\u03b1o,  Jornalismo  E Espa\u03c2o  \nP\u00fablico. (10), 19-35.  doi:10.14195/2183-6019_10_2\nMcCombs,  M., & Shaw,  D. (1972).  The agenda-setting  function  of mass media.  The Public  \nOpinion  Quarterly,  36(2), 176-187.  Retrieved  February  22, 2021,  from  \nhttp://www.jstor.org/stable/2747787\nMcLuhan,  M. (1964).  Understanding  media:  The extensions  of  man . New  York:  McGraw-Hill.\n36\n\nMoravec,  P. L., Minas,  R. K., & Dennis,  A. R. (2019).  Fake  news on social  media:  \nPeople  believe  what  they want  to believe  when  it makes  no sense  at All. MIS Quarterly,  \n43 (4), 1343-1360.  https://doi.org/10.25300/MISQ/2019/15505\nNelson,  J. L., & Tandoc  Jr., E. C. (2018).  Doing  \u201cwell \u201d or doing  \u201cgood \u201d: What  audience  \nanalytics  reveal  about  journalism's  competing  goals. Journalism  Studies,  20(13), 1960 \u00ad\n1976. doi:10.1080/1461670x.2018.1547122\nNewman,  N., Fletcher,  R., Kalogeropoulos,  A., & Nielsen,  R. K. (n.d.).  Reuters  Institute  Digital  \nReport  2019  (Rep.).  Reuters  Institute.\nOxford  English  Dictionary.  (n.d.).  Broadcast.  In Oxford  English  Dictionary . Retrieved  March  9, \n2021,  \nhttps://www.oed.com/view/Entry/23507?rskey=Ge76vX&result=1&isAdvanced=false#ei  \nd\nPew Research  Center.  (2019,  December  31). Where  news audiences  fit on the political  spectrum.  \nRetrieved  February  21, 2021,  from  \nhttps://www.journalism.org/interactives/media-polarization/outlet/msnbc/\nPew Research  Center.  (2020a,  October  11). Voters'  feelings  about  the election  and possible  \noutcomes.  Retrieved  February  21, 2021,  from  \nhttps://www.pewresearch.org/politics/2020/10/09/voters-feelings-about-the-election-and-  \npossible-outcomes/#many-see-lasting-harm-if-the-other-party's-candidate-wins-in-  \nnovember\nPew Research  Center.  (2020b,  October  27). Divides  in economic  attitudes  between  supporters  \nand non supporters  of governing  parties  have narrowed  in some  countries,  but not in the \nU.S. Retrieved  February  21, 2021,  from https://www.pewresearch.org/fact-  \ntank/2020/10/28/public-opinion-about-coronavirus-is-more-politically-divided-in-u-s-  \nthan-in-other-advanced-economies/ft_2020-10-28_governingparties_02/\nPurcell,  K., & Rainie,  L. (2019,  December  31). More  information  yields  more  learning  and \nsharing.  Retrieved  February  22, 2021,  from \nhttps://www.pewresearch.org/internet/2014/12/08/more-information-yields-more-  \nlearning-and-sharing/\n37\n\nRoozenbeek,  J., Schneider,  C. R., Dryhurst,  S., Kerr,  J., Freeman,  A. L., Recchia,  G., Van der \nBles,  M., Van der Linden,  S. (2020).  Susceptibility  to misinformation  about  covid-19  \naround  the world.  Royal  Society  Open  Science,  7(10),  201199.  doi:10.1098/rsos.201199\nSchmidt,  A. L., Zollo,  F., Del Vicario,  M., Bessi,  A., Scala,  A., Caldarelli,  G., Stanley,  H., \nQuattrociocchi,  W. (2017).  Anatomy  of news  consumption  on Facebook.  Proceedings  of \nthe National  Academy  of Sciences,  114(12),  3035-3039.  doi:10.1073/pnas.1617052114\nSearch  and Learn  the Bias of News  Media.  (2020,  August  31). Retrieved  September  23, 2020,  \nfrom  https://mediabiasfactcheck.com/\nShao,  W., & Hao,  F. (2020).  Confidence  in political  leaders  can slant  risk perceptions  of \nCOVID-19  in a highly  polarized  environment.  Social  science  & medicine  (1982),  261, \n113235.  https://doi.org/10.1016/j.socscimed.2020.113235\nShao,  W., Xian,  S., Lin, N., Kunreuther,  H., Jackson,  N., & Goidel,  K. (2017).  Understanding  the \neffects  of past flood  events  and perceived  and estimated  flood  risks  on individuals'  \nvoluntary  flood  insurance  purchase  behavior.  Water  Research,  108,  391-400.  \ndoi:10.1016/j.watres.2016.11.021\nShapira,  S., Aharonson-Daniel,  L., & Bar-Dayan,  Y. (2018).  Anticipated  behavioral  response  \npatterns  to an earthquake:  The role of personal  and household  characteristics,  risk \nperception,  previous  experience  and preparedness.  International  Journal  of Disaster  Risk \nReduction,  31, 1-8. doi:10.1016/j.ijdrr.2018.04.001\nShearer,  E. (2021,  January  12). 86%  of Americans  get news  online  from  digital  devices.\nRetrieved  February  19, 2021,  from https://www.pewresearch.org/fact- \ntank/2021/01/12/more-than-eight-in-ten-americans-get-news-from-digital-devices/\nShoaps,  R. (1999).  The many  voices  of Rush  Limbaugh:  The use of transposition  in constructing  \na rhetoric  of common  sense.  Text - Interdisciplinary  Journal  for the Study  of Discourse,  \n19(3).  doi:10.1515/text.1.1999.19.3.399\nSilverstein,  M. (2003).  Indexical  order  and the dialectics  of sociolinguistic  life. Language  & \nCommunication,  23(3-4),  193-229.  doi:10.1016/s0271-5309(03)00013-2\nSociety  of Professional  Journalists  Code  of Ethics  - Society  of Professional  Journalists.  (2014).\nRetrieved  September  21, 2020,  from https://www.spj.org/ethicscode.asp\n38\n\nSociety  of Professional  Journalists.  (2008).  SPJ calls  on NBC  to rectify  lack of disclosure  Of \nanalyst's  conflicts  of interest.  Retrieved  March  07, 2021,  from  \nhttps://www.spj.org/news.asp?ref=855\nWellbrock,  C.-M.,  Kure,  M. A., & Buschow,  C. (2020).  Competition  and media  performance:  A \ncross-national  analysis  of corporate  goals  of media  companies  in 12 countries.  \nInternational  journal  of  communication  [Online],  6154+.\nhttps://link.gale.com/apps/doc/A644959535/LitRC?u=fair43885&sid=LitRC&xid=ddab4  \n85d\nWorld  Health  Organization  (2018).  Communicating  Risk in Public  Health  Emergencies  : A who \nguideline  for emergency  risk communication  . (erc)  policy  and practice  . World  Health  \nOrganization.\n39\n\nAppendix  A: Article  Theme  Codebook\nTheme Definition Examples\nHealth  Tools  and Testing Article  titles  covering  the \ntopics  of supplies,  testing  kits, \nmedical  supplies,  drugs,  and \nvaccines.\u201cAs Testing  Quickly  Ramps  \nUp, Expect  More  U.S. \nCoronavirus  Cases \u201d, NPR,  \nMarch  1, 2020\nImmigration Articles  titles  covering  the \ntopics  of immigration  and \ninternational  travel\u201cTrump  Suspends  All Travel  \nFrom  Europe  For 30 Days  To \nCombat  COVID-19 \u201d, NPR,\nMarch  11, 2020\nBusiness  Impact Articles  titles  covering  the \ntopics  of business,  prices,  and \nlayoffs\u201cPelosi  says Dem coronavirus  \npackage  could  be released  this \nweek,  as Schumer  attacks  \nTrump \u201d Fox News,  March  10, \n2020\nInfection  and Mortality  Rates Article  titles  covering  the \nthemes  of infection,  cases,  \ninfection  statistics,  and \ndeaths.\u201cCoronavirus:  100,000  More  \nCases  Reported  Worldwide  In \nLess Than  2 Weeks \u201d NPR,  \nMarch  20, 2020\nPolitical  Discourse Article  titles related  to \npolitical  figures  and discourse\u201cTrump's  Gut Collides  With  \nScience  On Coronavirus  \nMessaging \u201d NPR,  March  5, \n2020\n40\n\nAppendix  B: Tucker  Carlson  Show  \u201cLocal  Leaders  Failed  Cities \u201d Segment\nMATERIAL: Tucker  Carlson  Show  \u201cLocal  Leaders  Failed  Cities \u201d Segment\nBROADCAST  DATE: 3/26/20\nSOURCE: Youtube  link\nhttps://www.youtube.com/wat ch?v=tNflR2Ia7Hc&feature=youtu.be\nParticipants: Tucker  Carlson  (host)\nTranscription: Autogenerated  by Youtube,  edited  in Inqscribe  by Kyle  Johnson\nSegment  Length: Youtube  clip length  is 00:05:39\n1 CARLSON so you saw what  is happening  in new york\n2 the question  is why is the outbreak\n3 so especially  devastating  there\n4 new york city is\n5 the densest  metro  area In the country\n6 that's  the most  obvious  answer\n7 probably  the biggest  problem\n8 the city also has\n9 an awful  lot of people  traveling\n10 to and from other  infected  countries\n11 and that's significant  too\n12 but it's not the whole  story..\n13 as this deadly  virus\n14 emerged  from eastern  china\n15 and began  to spread  inexorably\n41\n\n4216 across  the globe  clearly  headed  here\n17 leaders  in New  York  not only failed  to shield\n18 their  citizens  from  it,\n19 they took affirmative  and aggressive  steps\n20 to increase  the risk to their  population.\n21 why would  they do that\n22 well because  they\n23 were  worried  fa:r more\n24 about  being  called  racist\n25 than protecting  human  lives\n26 that's  not an overstatement\n27 that's  not hyperbole\n28 watch  the city's  health  commissioner  Cyrus  Barbett\n29 urge new yorkers  to spend\n30 as much  time  as possible  in crowded  public  places\n31 keep  in mind  that this video  is\n32 from  february  second  of this year\n33 that was lo:ng  after  the threat\n34 from  the chinese  coronavirus\n35 was obvious  to anyone\n36 who was paying  attention\n37 BARBETT  the risk to new yorkers  for coronavirus  is low. ..\n38 and that our preparedness\n\n4339 as a city is very high\n40 there  is no: reason\n41 not to take the subway\n42 not to take the bus\n43 not to go out to your favorite  restaurant\n44 and certainly  Not to miss\n45 the parade  next sunday  (voices  in background)\n46 CARLSON  take the subway\n47 go to dinner\n48 people  cheer...\n49 they feel good  about  themselves\n50 future  generations  though\n51 are F<going  to>F\n52 watch  that video  with their  jaws open  In disbelief\n53 how can someone\n54 charged  with protecting  public  health\n55 so recklessly  endanger  it\n56 nowwatch  this performance\n57 from  state  senator  john liu too\n58 liu suggested  that\n59 people  who might  be concerned  in any way\n60 about  contracting  a deadly  disease\n61 or might  be interested\n\n4462 in where  it came  from  must  be\n63 and you guessed  it\n64 bigots\n65 LIU but there's  really  no need  to panic\n66 and to avoid  activities\n67 that we always  do as new yorkers\n68 we are hardy  people\n69 as an asian  american\n70 i've been  somewhat  disturbed\n71 and if not outright  appalled..\n72 At some  of the comments  or gestures\n73 that i have  seen\n74 diseases  originate  from  anywhere\n75 or from  particular  places  in the world\n76 CARLSON in other  words\n77 as a member  of a protected  interest  group\n78 i'm o:rdering  you\n79 to ignore  this threat  to your family\n80 on moral  ground\n81 go to the parade  or else\n82 that's  what  liu is saying\n83 let's hope  thatin  the wake  of this disaster\n84 after  john liu Has lost his job\n\n4585 and we can all think  clearly  again\n86 people  will stop talking  this way in public\n87 for good\n88 it was always  the most  brutal  form  of social  control\n89 masquerading  as sensitivity  and caring.\n90 now we know  it was infecting  the public  with disease\n91 but at the time\n92 sentiments  like this were  universal  among\n93 big city public  officials  like these\n94 state  senator  brain  kavanaugh\n95 for example  decided  to embark\n96 on a crusade  against  What  he called\n97 misinformation.\n98 a euphemism\n99 as it so often...  is for accurate  information\n100 kavnaugh  encouraged  people\n101 to head  to chinatown  for a festival.\n102 KAVANAUGH  it's very important  That we recognize\n103 that this holiday  And this festival\n104 is of tremendous  significance\n105 for many  communities  In our state\n106 and uh It is very  important\n107 that we ensure  that We don't  have  misinformation\n\n46108 and many  in the media  Have  been  covering  this issue\n109 uhh as if it's a terrible  plague  that people  have  to avoid\n110 CARLSON  if you don't  go to a crowded  public  place\n111 immediately\n112 you're  a racis-\n113 look  at those  people  telling  you that\n114 demanding  that you do that.\n115 hectoring  you-\n116 badgering  you\n117 exerting  moral  blackmail  on you to expose  yourself\n118 my early  march  coronavirus  was clearly  becoming\n119 a major  problem  in The united  states\n120 but for elected  officials  In new york\n121 the only problem  was their  constituents\n122 racist  worries  about  staying  alive\n123 on march  second  this march  second\n124 mayor  bill deblasio  tweeted  this quote\n125 since  I'm encouraging  new yorkers\n126 to go on with  your  lives\n127 plus\n128 go out on the town  despite  coronavirus\n129 I thought  I would  offer  some  suggestions\n130 here's  the first\n\n47131 through  thursday  go see the traitor\n132 if the wire  was a true story  plus set in italy  it would  be this film\n133 that's  right\n134 go to a movie  theater\n135 immediately\n136 you must\n137 a week  later  deblasio  was still encouraging  new yorkers\n138 to live it up and mingle  in large  groups\n139 on March  eleventh  which  keep  in mind\n140 was barely  two weeks  ago\n141 deBlasio  was still insisting  that quote\n142 if you're  not sick you should  be going  about  your  life\n143 that very  same  day the NBA  announced  it was\n144 suspending  its whole  season.\n145 italy  was already  several  days  into a national  lockdown.\n146 but they mayor  of one of the worlds  most  crowded  cities\n147 was telling  everyone  to carry  on\n148 as though  the chinese  coronavirus  was just a racist  myth\n149 soon  after  that people  in new york  inevitably  started  to get sick\n150 in some  cases  very  sick and in some  cases  they died\n151 and then suddenly  deblasio  is on meet  the press\n152 shrieking  that donald  trump  had abandoned  his city\n153 it was their  fault\n\n48154 all their  fault\n155 that line almost  always  works  which  is why DeBlasio  did it.\n156 the media  play along\n157 and move  on to the next thing\n158 and no one remembers  what  actually  happened.\n159 will that happen  this time\n160 maybe  not\n161 this time  might  be different\n162 this pandemic  is too horrible  too many  people  are being  hurt\n163 nobody  really  liked  identity  politics  anyway\n164 all it did was help mediocrities  like Bill DeBlasio\n165 get elected  to office\n166 but it was disgusting  and cruel  And divisive\n167 and now we know  it can get people  killed\n168 maybe  we can stop.\n\nAppendix  C: NPR  Morning  Edition  \"Coronavirus  Guidance  Across  Texas  Is Not \nConsistent\"  Segment\nSOURCE:MATERIAL: NPR  Morning  Edition  \u201cCoronavirus  Guidance  Across  Texas  Is Not\nConsistent \u201d Segment\nBROADCAST  DATE: 3/26/20\nNPR  website\nhttps://www.npr.org/transcripts/821848822\nParticipants: Noel  King  (host),  Betsy  Price  (Mayor  of Fort Worth)\nTranscription: Transcribed  by NPR,  edited  by Kyle  Johnson\nSegment  Length: NPR  audio  clip length  is 00:03:51\n1 KING Many  people  in this country  are looking  to\n2 government  officials  to guide  them  through  the chaos\n3 that's  been  caused  by the coronavirus.\n4 But what  if the guidance  isn't the same  across  the state\n5 Some  cities  are making  their  own decisions  about\n6 closing  schools  and businesses,\n7 like Fort Worth  Texas\n8 This  week  the city's  mayor  Betsy  Price  ordered  residents\n9 to stay at home  until  April  third\n10 But Texas  Governor  Greg  Abbott  has not issued  a similar  statewide  order\n11 Mayor  Price  thanks  for being  with us\n12 PRICE: Im glad to be here\n13 Thank  you for having  me\n49\n\n5014 KING Why  did you decide  to make  this decision  for the residents  of Fort Worth?\n15 PRICE You know,  we had issued  an order  that required\n16 a little  less than stay at home\n17 and we wanted  to be sure we were  getting  compliance\n18 But the governor  on Sunday\n19 said he was going  to leave  it to local  officials  to\n20 try to come  together  and do this\n21 Patchwork  is clearly  less than ideal\n22 but we met with the mayors\n23 by phone,  for social  distancing\n24 of the top seven  cities  in Texas  and the top six counties\n25 And we all came  up with a plan that's  very  uniform\n26 there  are minor  tweaks  to it within  each city and county\n27 and decided  that we would  issue  that\n28 On um Monday  morning  or Tuesday  morning  together\n29 And that covers  about  seventy  percent  of the population  of Texas\n30 Those  of us that came  together\n31 we felt like it was key to helping  mitigate  this virus\n32 keep  our hospitals  from  being  overwhelmed  and\n33 allow  our first responders  to not all be\n34 quarantined  or isolated.\n35 KING I was going  to ask you if you were  concerned  about  mixed  messages\n36 but what  you're  telling  me\n\n5137 is that you came  together  with these  other  mayors  in order  to\n38 prevent  mixed  messages.\n39 That  was entirely  the point  of doing  this\n40 everyone  together\n41 even  though  the governor  has said\n42 I'm not going  to do it myself.\n43 PRICE That's  correct.\n44 And the governor  really  felt like local  control  was the way to go on this\n45 because  people  know  their  communities\n46 And the good  news  is not only did the seven  large  counties  do it\n47 but now the small  counties  that surround  us have\n48 most  of them  have  adopted  it in one form  or another\n49 So it's beginning  to cover\n50 the metropolitan  regions  of Texas\n51 which  takes  up roughly  seventy  percent  of the population  in the state\n52 KING Earlier  this week  Texas  Lieutenant  Governor  Dan Patrick  talked\n53 in an interview  about  what  he's willing  to give up\n54 in order  to get Americans  back  to work\n55 in order  to get businesses  back  open\n56 Let's  listen  to what  he said\n57 PATRICK No one reached  out to me and said as a senior  citizen\n58 are you willing  to take a chance  on your  survival  in exchange\n59 for keeping  the America  that all America  loves\n\n5260 for your  children  and grandchildren\n61 And if that's  the exchange\n62 I'm all in\n63 KING Okayso  Lieutenant  Governor  Dan Patrick  is almost  70 years  old\n64 That  makes  him part of an at risk demographic\n65 Do you think  there  are a lot of people  in your  state who agree  with him\n66 You've  got 3.6 million  senior  citizens  in Texas  if I'm correct\n67 Do people  agree  with these  orders  to shelter  in place\n68 Or do you have  a lot of people  like him say no\n69 we want  to get back  out there\n70 PRICE I think  most  of them  agree  with the order  to shelter  in place\n71 I mean  I'm with  Governor  Patrick  I turned  70 this year\n72 very healthy  and most  of us are.\n73 But what ...whose  talent  would  you want  lost\n74 And you know  my children  and my grandchildren  last time I looked\n75 would  still like to have  their  Tootsie  (KING  hums)\n76 and all lives  are (laughter)  that's  my name\n77 all lives  regardless  of age and race,  are valued  in Fort Worth\n78 and again\n79 look  at what  you might  lose and the talent\n80 We all want  America  where  it is\n81 but we may be a new normal\n82 KING Mayor  Betsy  Price  of Fort Worth  Texas,  talking\n\n83 about  that state and local responses  to the corona  outbreak\n84 in parts of Texas\n85 thank  you so much\n86 PRICE Thank  you\n87 Appreciate  y'all's  help\n53\n\nAppendix  D: Rachel  Maddow  \"States  Wait  For Federal  Help  At Their  Own  Peril  As\nCoronavirus  Crisis  Builds\"  Segment\nMATERIAL: Rachel  Maddow  \u201cStates  Wait  For Federal  Help at Their  Own  Peril\nAs Coronavirus  Crisis  Builds \u201d Segment\nBROADCAST  DATE: 3/20/20\nSOURCE: Youtube  link\nhttps://www.youtube.com/watch?v=C  yGlB7WLY0\nParticipants: Rachel  Maddow  (host)\nTranscription: Transcribed  by Kyle  Johnson\nSegment  Length: Youtube  clip length  is 00:04:53\n1 MADDOW italy reported:  its largest  single  day death  toll today\n2 six hundred  and twenty  seven  people  died...\n3 in the last twenty  four hours\n4 and even as italy\n5 struggles  and struggles  and tries to keep up\n6 uh china  just sent a team of experts  who just went  through\n7 their coronavirus  catastrophe\n8 uh to try to advise  and help out with the situation  in northern  italy\n9 the vice president  of the chinese  red cross\n10 on that visit to northern  italy\n11 said that from what  he can observe\n12 what  italy is doing  to try to slow  the rate of new infections\n13 which  are frankly  the kinds  of measures\n54\n\n5514 we are just lackadaisically  starting  right  now\n15 in a few states  in this country\n16 the chinese  officials  who just went  through  this\n17 in their  own country\n18 are visiting  italy and they say what  it italy is doing\n19 is not nearly  strict  enough\n20 to make  enough  of a difference\n21 quote  here in milan  he said\n22 the hardest  hit area by covid  nineteen\n23 the lockdown  measures. .. are very lax\n24 I can see public  transport  is still running\n25 people  are still moving  around\n26 having  gatherings  in hotels  and they are not wearing  masks\n27 quote  I don't  know  what  people  here are thinking\n28 we really  have to stop our usual economic  activities\n29 and our usual  human  interactions\n30 we have to stay at home\n31 and make  every  effort  to save human  lives\n32 it is worth  putting  e;very  cost we have\n33 into saving  lives.\n34 again  that's  what  the head of the chinese  red cross.\n35 is telling  northern  italy\n36 about  how well they are doing  in their  nation  wide  lockdown  effort\n\n5637 their  nation  wide  lockdown  effort  in italy\n38 is effectively  the same  kind  of stay at home  order\n39 that only  thre:e u s states  have  adopted\n40 and those  three  only in the last twenty  four hours. ..\n41 so... here we go\n42 um there  was...\n43 some  more  happy  talk from  the white  house  today\n44 about  how plentiful  are national  supply  of ventilators  is\n45 says there's  no problem  there\n46 and also how plentiful  the supply  of masks\n47 and personal  protective  equipment  is...\n48 there's  tons of that stuff.\n49 there's  murmuring  from  the white  house  that there  will\n50 maybe  be some  additional  manufacturing  of those  things:\n51 but we'll see.\n52 let's hope so\n53 (inaudible)  if those  things  do:\n54 eventually  get manufactured  in greater  numbers\n55 because  the federal  government  decides  to do something  about  it\n56 let's hope all those  things  come  with a time machine  too\n57 to turn back  the clock  on these  last three  months\n58 that have  been  wasted  while  the white  house  was saying\n59 this was a hoax ...\n\n5760 I don't  know  if the federal  government  is ever\n61 going  to get it together. ..\n62 but. ..\n63 as of now\n64 with the first hospitals  in america  reporting  that they are\n65 all filled  up.\n66 we are out of time waiting  for\n67 the federal  government  to get it together\n68 the best run states\n69 the clearest  eyed  leaders  in the best run states\n70 right now are just going.\n71 on their own\n72 they have  to be now\n73 and if your  state isn't\n74 you should  not hope that\n75 there  will be some  sort of federal  response  to step up to fix things\n76 when  things  start to get dark in your  state\n77 over the next few weeks\n78 states  we have learned.\n79 have  to go own their own\n80 yes we need  a national  manufacturing  mandate  for ventilators\n81 we need  that\n82 we need  a national  manufacturing  mandate\n\n5883 and an national  coordination  effort\n84 for the distribution  of protective  equipment\n85 for health  care providers  and the other  crucial  supplies\n86 they are already  running  out of and rationing\n87 and unsafely  reusing...\n88 and yeah  weeks  ago\n89 we needed  a national  effort\n90 to get millions  of coronavirus  tests  in the field\n91 we needed  all of these  things  from  the federal  government\n92 the federal  government  is failing  at all of these  things...\n93 and that is lamentable  and I lament  it\n94 but\n95 it's not stopping  the virus\n96 right\n97 it's not stopping  these  things  from  moving  forward\n98 it's not stopping  the influx  that our hospitals  from  starting\n99 as of today...\n100 so\n101 lament  it all you want  but if you are a state  official\n102 the lesson  is\n103 go\n104 go now\n105 REPORTER  do you want  the uh president  to use the defense  protection  act\n\n59106 to ramp  up production  of medical  supplies\n107 CUOMO look I am I am\n108 If I had. .. a new york state defense  uh protection  act\n109 I would  use it jesse\n110 I don't  have it\n111 so what  I'm saying  is\n112 i'll pay businesses  more\n113 i'll I'll start a new I'll fund a new business\n114 if you can make  these  products\n115 i'm trying  to make  these products\n116 uh if you are in this this line: of work\n117 we need masks\n118 if you're  making  clothing.\n119 uh figure  out if you can make  masks\n120 i'll fund it\n121 so i'm doing  everything  i can\n122 to increase  the production\n123 MADDOW if you're  making  clothing.\n124 figure  out if you can make  masks\n125 for nurses\n126 and new york state will pay you\n127 that's  where  we're  at\n128 that's  where  we're  at\n", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Taking a Stand on Not Taking a Stand: Media Bias in the Online Reporting of COVID-19", "author": ["K Johnson"], "pub_year": "2021", "venue": "NA", "abstract": "This thesis was written to examine the digital communication strategies of three major news  organizations when reporting on COVID-19 in the US for bias. The research looked at social"}, "filled": false, "gsrank": 445, "pub_url": "https://search.proquest.com/openview/679e6197e22e6efdbf9ba02875043b5a/1?pq-origsite=gscholar&cbl=18750&diss=y", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:k-CHVE_QOvUJ:scholar.google.com/&output=cite&scirp=444&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D440%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=k-CHVE_QOvUJ&ei=VbWsaOTRJ7XCieoP4PfQ0A8&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:k-CHVE_QOvUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://scholarworks.alaska.edu/bitstream/handle/11122/12556/Johnson_K_2021.pdf?sequence=1&isAllowed=y"}}, {"title": "CLoSE: Contrastive learning of subframe embeddings for political bias classification of news media", "year": "2022", "pdf_data": "Proceedings of the 29th International Conference on Computational Linguistics , pages 2780\u20132793\nOctober 12\u201317, 2022.2780CLoSE: Contrastive Learning of Subframe Embeddings\nfor Political Bias Classification of News Media\nMichelle YoungJin Kim and Kristen Marie Johnson\nComputer Science and Engineering, Michigan State University, USA\n{kimmic16,kristenj}@msu.edu\nAbstract\nFraming is a political strategy in which journal-\nists and politicians emphasize certain aspects\nof a societal issue in order to influence and\nsway public opinion. Frameworks for detect-\ning framing in news articles or social media\nposts are critical in understanding the spread\nof biased information in our society. In this\npaper, we propose CLoSE, a multi-task BERT-\nbased model which uses contrastive learning to\nembed indicators of frames from news articles\nin order to predict political bias. We evaluate\nthe performance of our proposed model on sub-\nframes and political bias classification tasks.\nWe also demonstrate the model\u2019s classification\naccuracy on zero-shot and few-shot learning\ntasks, providing a promising avenue for fram-\ning detection in unlabeled data.\n1 Introduction\nNews media coverage shapes our attitudes, emo-\ntions, and decisions toward public issues (Iyen-\ngar, 1994; Pan and Kosicki, 1993; Jensen et al.,\n2014). Research shows that people\u2019s perceptions\nof news can be manipulated by presenting the\nsame story with different expressions. For exam-\nple, participants of a study found a terrorist attack\ncaused by \u201cal-Qaeda and associated radical Islamic\ngroups\" considerably more threatening than a ter-\nrorist attack by \u201cdomestic rebel separatist groups,\u201d\nwhich is an equivalent paraphrase (Kapu \u00b4sci\u00b4nski and\nRichards, 2016). Hence, studies on the influences\nof different presentations of issues, or the effects\nofframing , play an essential role in understanding\npolitical discourse.\nFraming refers to emphasizing desired aspects\nof an issue to promote and amplify a particular per-\nspective (Entman, 1993). By selecting and thus\nelevating the salience of a specific angle of a topic,\nmedia sources can present the topic through their\nchoice of frames to induce particular attributes and\njudgments among the public. Framing is widely re-searched on various topics, from its effects on pub-\nlic opinion on political issues such as the U.S. anti-\nnuclear war movement (Entman and Rojecki, 1993)\nand stem cell research (Nisbet et al., 2003) to the\neconomic impact of framing (Liu and Pennington-\nGray, 2015; Van Dalen et al., 2017).\nIn this work, we study framing detection for\nthree politically polarized issues in the U.S. news\nmedia: abortion, gun control, and immigration. We\nfocus on framing in the news discourse to under-\nstand a discrepancy in media consumption and its\ninfluence on political bias polarization. With the\nincrease of news media outlets and social media\nplatforms, the public is overwhelmed with a flood\nof information. Unfortunately, the increase of news\nsources does not yield the sharing of information\nacross political views, often developing into biased\necho chambers on social media platforms. In fact,\nthere are stark differences in the social media and\nnews sources that liberals and conservatives use\nand trust. Fox News is the primary news source for\nnearly half of conservatives, while NPR, MSNBC,\nand the New York Times are the most trusted news\nsources for liberals (Mitchell et al., 2014).\nMachine learning techniques have been applied\nto detect and analyze political frames (Card et al.,\n2015; Guo et al., 2016; Johnson et al., 2017a; Bha-\ntia et al., 2021). In such framing analyses, the\nperformance of a framing detection model is tested\nby predicting the political bias of an article or the\npolitical affiliation or stance of a politician\u2019s tweet\nor speech. However, we suggest incorporating the\npolitical bias information into the development of\nthe actual frame detection model. We focus on gen-\neral liberal and conservative bias rather than the\nspecific stances politicians may take on issues.\nIn this paper, we propose CLoSE, a framing em-\nbedding model that jointly learns to predict political\nbias. CLoSE fine-tunes BERT variants with a con-\ntrastive learning objective to generate (sub)frame\nembeddings for a given input sentence. Then we\n2781add a prediction loss to classify the political bias\n(liberal or conservative) of the embedded text.\nFor the embedding task, we use contrastive learn-\ning to place embeddings of the same subframe\ncloser together. Subframes are fine-grained sub-\ncategorizations of the general political frames of\nBoydstun et al. (2014). The topic-specific lexicons\nof subframes are the subframe indicators used to\nidentify specific framing language within a chosen\ntopic. We use the subframe indicators to identify\ntexts with frames and construct the Framing Triplet\nDataset. This dataset, built explicitly for a con-\ntrastive objective, consists of a triplet of an anchor\nsentence, its positive sample, and its negative sam-\nple, and the political bias label of the anchor sen-\ntence. The contrastive learning objective reduces\nthe distance between the anchor sentence and its\npositive sample, which belongs to the same sub-\nframe, while increasing the distance to its negative\nsample that belongs to a different subframe.\nCLoSE outperforms the baseline models in both\nthe subframe and political bias classification tasks.\nThe results also show that the contexts of subframes\nimprove the performance of the political bias clas-\nsifier. Further, our pre-trained model accomplishes\nsuperior performance in zero-shot and few-shot set-\ntings. Finally, we design a topic modeling method\nfor the subframe embeddings that clusters nearby\nembeddings, extracts statistically significant words\nwith class-based TF-IDF (Grootendorst, 2022), and\nmerges clusters with high overlapping words.\nTo summarize, our main contributions are as fol-\nlows: (1) We collect and release the Triplet Fram-\ning Dataset1, a triplet of sentences that include\nsubframe indicators. (2) We propose CLoSE, con-\ntrastive learning of subframe embeddings model\nthat jointly generates embeddings and predicts\npolitical bias of framed texts. The experiments\ndemonstrate our method\u2019s performance on sub-\nframe and political bias classification tasks and\ninvestigate the effectiveness of political bias infor-\nmation on subframe detection and vice versa. (3)\nThe experiments show that our pre-trained model\nperforms competitively on zero-shot political bias\nclassification and few-shot subframe classification\ntasks. Namely, the pre-trained model can predict\nthe political bias of articles with previously unseen\ntopics and predict subframe groups with a limited\nquantity of labeled data. This greatly reduces the\n1Code and data available at https://github.com/\nMSU-NLP-CSS/CLoSE_framing .cost of data annotation for framing and bias tasks.\n2 Related Work\nFraming is a powerful political strategy that is used\nto influence public opinion. Hence identifying what\nand how frames are used is a critical task in politi-\ncal communications. Framing in news media and\nsocial networks has been studied to analyze polit-\nical polarization (Johnson and Goldwasser, 2016;\nTsur et al., 2015; Tourni et al., 2021). However, an-\nnotating data for framing is challenging due to the\nnuanced language of frames across political issues.\nTo overcome this challenge, computational so-\ncial scientists follow a topic-specific codebook to\nmanually annotate documents with frames (Terk-\nildsen and Schnell, 1997; Baumgartner et al., 2008;\nCard et al., 2015). The most commonly used code-\nbook is the Policy Frames Codebook (Boydstun\net al., 2014), which proposes a general coding\nscheme for fifteen high-level frames across policy\nissues. Based on this codebook, Card et al. (2015)\ncollected and released the Media Frames Corpus\n(MFC). The MFC contains more than 20,000 news\narticles on three policy issues: immigration, smok-\ning, and same-sex marriage.\nFollowing the release of MFC, the first large-\nscale open-source frames dataset, many researchers\nstudied and analyzed frames by leveraging the\nMFC. Johnson et al. (2017b) use political tweets\nto extract phrases that frequently appear in each of\nthe frames and propose a framing detection model\nthat uses both linguistic features and the extracted\nideological phrases. Field et al. (2018) extend the\nU.S. policy frames to Russian policy frames and\nanalyze 13 years of Russian news articles. They de-\nrive the lexicons of each frame from the MFC and\ntranslate them to Russian to generate Russian fram-\ning lexicons. Huguet Cabot et al. (2020) propose\na multi-task model that learns metaphor, framing,\nand emotion in political discourse and uses the\nMFC to predict frames. Although our method also\njointly models political bias and framing, our ap-\nproach differs in that the main task of our model\nis to embed language used in subframes with con-\ntrastive learning.\nAdditional research has narrowed down the gen-\neral policy frames to suggest issue-specific frames.\nJohnson et al. (2017a) add Twitter-specific frames\nto the policy frames and annotate and analyze the\ntweets of U.S. politicians. Roy and Goldwasser\n(2020) build topic-specific lexicons, which they de-\n2782fine as subframe indicators , by using an embedding\nmodel to generalize the MFC lexicon for analyzing\nmedia bias. We utilize their subframe indicators to\ncreate our Framing Triplet Dataset. Our proposed\nmodel also includes an embedding model that gen-\neralizes text containing subframe indicators. How-\never, we integrate political bias information into\nour model directly by adding bias classification as\nan auxiliary task.\nOther approaches for detecting frames are based\non topic modeling algorithms. Latent topics of a\ngiven corpus are extracted with a topic modeling\nalgorithm, often Latent Dirichlet Allocation (LDA)\n(Blei et al., 2003). LDA yields statistically signif-\nicant words of each topic, which are used as can-\ndidate indicators for defining frames. Bhatia et al.\n(2021) provide an open-sourced tool for analyzing\nframes in multilingual texts. Given text inputs, their\nweb-based system sends the LDA topic modeling\noutput to a user\u2019s email so that the user can decide\nand label frames on the given topics. However, we\nnote that the output of LDA is a list of keywords\nin each topic, notframe. The topic-based words\nare helpful guidance in framing annotations and\nexploratory analysis but are not appropriate data to\nbe used for supervised framing analysis.\n3 Framing Triplet Dataset\nWe introduce the Framing Triplet Dataset, which\ncontains 25,627 news articles on three politically\npolarized topics: abortion, gun control, and immi-\ngration. We extend the hyper-partisan news dataset\n(Kiesel et al., 2019) and the dataset by Roy and\nGoldwasser (2020), which consists of news arti-\ncles on the three topics until 2019. Open-source\ncrawlers pygooglenew andnews-please\nwere used to collect recent articles from 2020 to\n2022. We query the Google News RSS feed with\nkeywords of each topic and crawl the articles. The\nkeywords are \u201cabortion\u201d for the abortion data, \u201cgun\u201d\nfor the gun control data, and \u201cimmigrant\u201d and \u201cim-\nmigration\u201d for the immigration data. Then we\nassign the political bias label of each article ac-\ncording to Media Bias Fact Check2, the largest\ncrowdsourced media bias resource on more than\n4,500 media sources and journalists. The Media\nBias Fact Check categorizes each media source into\none of the following five biases: \u201cleft bias,\u201d \u201cleft-\ncenter bias,\u201d \u201cleast bias,\u201d \u201cright-center bias,\u201d and\n\u201cright bias.\u201d For our study, given the partisan divide\n2mediabiasfactcheck.comof current U.S. politics, we only consider articles\nfrom \u201cleft bias\u201d and \u201cright bias\u201d media. In total,\nour dataset includes 12,911 left-biased articles and\n12,761 right-biased articles. The detailed statistics\nof the dataset can be found in Table 1.\nAfter labeling the political bias of the collected\narticles, we extract headlines and sentences in arti-\ncles that contain subframe indicators . The sub-\nframe indicators, suggested by Roy and Gold-\nwasser (2020), are the issue-specific subclassifica-\ntions of the policy frames by Boydstun et al. (2014).\nThese subframe indicators are collected via the fol-\nlowing three steps. First, the top-250 unigrams for\neach of the 15 policy frames are gathered based on\ntheir Pointwise Mutual Information (PMI) scores\n(Church and Hanks, 1990). Then, each paragraph\nin the article is annotated with the policy frames\nbased on the number of unigram matches. Finally,\nrepeating phrases in the annotated paragraphs are\ngrouped to form subframes, which represent topic-\nspecific lexicons. They defined 20 subframes for\nthe topic of abortion, 22 subframes for the topic\nof immigration, and 19 subframes for the topic of\ngun control. The list of subframes can be found\nin Appendix A. We refer to Roy and Goldwasser\n(2020) for the full list of subframe indicators.\nFinally, we construct triplets with the extracted\nsentences. Given an anchor sentence sa, we define\nits positive sample spas a sentence with subframes\nfrom the same subframe group and its negative\nsample snas a sentence with subframes from a\ndifferent subframe group. For example, the sen-\ntence \u201cThe first backlash to the Roe decision came\nprimarily from groups representing U.S. Catholics.\u201d\ncontains a subframe indicator \u201cRoe decision\u201d that\nbelongs to the subframe \u201cRoe v. Wade.\u201d Its positive\nsample ( sp) should be a sentence with a subframe\nindicator of that same subframe, \u201cRoe v. Wade.\u201d\nIts negative example ( sn) will be a sentence with\na subframe indicator from a different, random sub-\nframe group such as \u201cBirth Control.\u201d The triplet\nis formed as (sa, sp, sn). An example of the triplet\ndata is shown in Figure 2.\n3.1 Data Analysis\nWe analyze the usage of subframes in the Framing\nTriplet Dataset by extracting the top-3 subframes\nacross time and issues. The results for the abortion\nsubdataset are shown in Table 2. (See Appendix B\nfor the results on the gun control and immigration\nsubdatasets.) Across the topics, we notice the trend\n2783News(#) Left(#) Right(#) Sent.(#) Left S.(#) Right S.(#) Time Span\nAbortion 8,061 4,518 3,543 10,725 8,032 2,693 1984-2022\nGun control 9,476 4,238 5,238 8,138 3,918 4,220 2000-2022\nImmigration 8,090 4,155 3,935 13,269 8,228 5,041 1996-2022\nTable 1: Statistics of the Framing Triplet Dataset. For each topic, news articles are collected, their biases are\nlabeled according to Media Bias Fact Check, and sentences containing subframe indicators are extracted. Left S. are\nsentences extracted from left-biased news articles. Right S. are extracted from right-biased news articles.\nYears Liberal Conservative\n2020-2022Top-1 Roe v. Wade Roe v. Wade\nTop-2 Abortion Funding Abortion Funding\nTop-3 Birth Control Birth Control\n2018-2020Top-1 Roe v. Wade Roe v. Wade\nTop-2 Birth Control Pregnancy Centers\nTop-3 Abortion Funding Abort. Prov. Economy\n2016-2018Top-1 Birth Control Pregnancy Centers\nTop-2 Health Care Abort. Prov. Economy\nTop-3 Roe v. Wade Abortion Funding\n2010-2016Top-1 Abortion Funding Birth Control\nTop-2 Birth Control Abort. Prov. Economy\nTop-3 \u22c6 Planned Parenthood\n-2010Top-1 Birth Control Roe v. Wade\nTop-2 Roe v. Wade Right of Human Life\nTop-3 \u22c6 Abortion Funding\n\u22c6Potentially offensive or triggering language has been omitted.\nTable 2: Top-3 Subframe Indicators of the Abortion\nTriplet Dataset. Each row indicates the most frequently\nused subframe indicators for liberal and conservative\nbiased media within the specified time frame.\nthat the most used subframes begin to overlap as\ntime passes. Before 2018, the top-3 subframes used\nby the liberal and conservative media significantly\ndiffered. Namely, the subframe indicators that the\nmedia focused on to frame people\u2019s opinions on\nabortion are different based on the media\u2019s politi-\ncal bias. However, the most used subframes of the\nleft and right news are intersecting more recently.\nFrom 2018 to 2020, the most used subframe of\nboth liberal and conservative media was \u201cRoe v.\nWade.\u201d The top-3 subframes from 2020 to 2022 of\nboth views are also identical. Similarly, from 2020\nto 2022, the top-3 subframes for the gun control\nsubdataset, and the top-2 subframes for the immi-\ngration subdataset, of both liberal and conservative\nmedia are identical. These results show that in\nrecent years, news media with opposing political\nbiases cover common issues, sometimes with simi-\nlar framing language. Despite this similar coverage,\npolitical polarization in the U.S. remains dominant\n(Doherty et al., 2021). Hence, we need a methodol-\nogy that captures not only which issues the media\nchooses to spotlight for coverage but also how they\nframe these issues for public perception. By jointly\nFigure 1: Architecture of CLoSE. Our model consists\nof a BERT- or RoBERTa-based encoder whose output\nis fed through a pooling layer to generate sentence em-\nbeddings via a contrastive learning objective. These\nembeddings are then used in the final political bias clas-\nsification task. This example shows an input sentence\nand its corresponding embedding (pink circle), which\nis most similar to other embedded sentences of the Sub-\nframe 1\u2019s group. The final prediction here is liberal.\nlearning to predict political bias from the sentence\nembeddings, our proposed method aims to capture\nthe different contexts of the opposing media.\n4 Methodology\nWe propose CLoSE: a multi-task learning model\nthat jointly learns to embed sentence framing lan-\nguage and predict political bias. As shown in\nFigure 1, the model consists of a BERT-based or\nRoBERTa-based encoder, followed by a pooling\nlayer that generates a sentence embedding, which\nfeeds into a classifier for political bias prediction.\nWe adopt a contrastive learning objective so that\na sentence with subframe indicators close to other\nsentences of the same subframe and far from those\nof different subframes will be similarly reflected in\nthe embedding space.\nEmbedding via pooling. Given a sentence s\nof tokens {t1,\u00b7\u00b7\u00b7, tn}, we embed the sentence us-\ning BERT or RoBERTa to get a sequence of token\nembeddings {e(t1),\u00b7\u00b7\u00b7, e(tn)}. Then a pooling\noperation is applied to the token embeddings to\n2784derive a fixed size sentence embedding e(s). There\nare three possible pooling strategies: (1) taking the\noutput of the CLS token, (2) computing the mean of\nall output vectors, and (3) computing a max-over-\ntime of the output vectors. CLoSE employs the\ndefault pooling operation of mean strategy, which\nhas shown the best experimental results on seman-\ntic textual similarity and natural language inference\ntasks (Reimers and Gurevych, 2019). Thus, the fi-\nnal sentence embedding is e(s) =1\nnPn\ni=1e(ti).\nLearn subframes with a contrastive learning\nobjective. We apply a contrastive learning objec-\ntive to capture the framing embedding space. To\ndo so, we fine-tune the BERT-based (or RoBERTa-\nbased) encoder to encourage embeddings that will\nfall within the same subframe group to be closer\nand the embeddings within a different subframe\ngroup to be more distant from a given anchor sen-\ntence\u2019s embedding. Every anchor sentence sahas\na positive sentence spand a negative sentence sn,\ncorresponding to the triplets within the Framing\nTriplet Dataset. For example, as shown in Figure 2,\nthe anchor sentence has a subframe indicator \u201cRoe\ndecision\u201d (shown in bold), which belongs to the\nsubframe group \u201cRoe v. Wade.\u201d While its posi-\ntive sentence has subframe indicators \u201coverturns\nRoe\u201d and \u201cRoe v. Wade\u201d that belong to the same\nsubframe group, the negative sentence has a sub-\nframe indicator \u201cMarch for Life\u201d that belongs to a\ndifferent subframe group \u201cPro-Life.\u201d\nGiven a triplet of sentences (sa, sp, sn), the con-\ntrastive learning loss is computed as follows:\nLc= max(0 ,\n\u03f5\u2212 \u2225e(sa)\u2212e(sn)\u2225+\u2225e(sa)\u2212e(sp)\u2225)\nwhere the margin \u03f5is a hyperparameter, and \u2225 \u00b7 \u2225\nis the Euclidean distance. e(sa), e(sp), and e(sn)\nare an embedding of the anchor sentence sa, its\npositive sentence sp, and its negative sentence sn,\nrespectively. We set \u03f5as 1 for the experiments.\nPredict political bias. The proposed model adds\na binary classifier to the output of the BERT-based\nencoder to predict the political bias of an input\n(anchor) sentence. The sentence embedding of\nthis anchor sentence is given as an input to the\nclassifier, which predicts whether the sentence is\nfrom a liberal (left) or conservative (right) news\nsource. Given an anchor sentence si, we predict\na stance label yi\u2208 {0,1}. We use binary cross-\nFigure 2: Visualization of the Contrastive Learning Ob-\njective. The contrastive learning objective pushes the\nanchor sentence closer to its positive sentence (decreas-\ning distance to positive example dp) and farther from its\nnegative sentence (increasing distance from dn).\nentropy as the loss function:\nLBCE=\u22121\nNNX\ni=1yi\u00b7log(p(yi))\n+ (1\u2212yi)\u00b7log(1\u2212p(yi))\nwhere Nis the number of data in a batch. In our\nexperimental studies, the classifier is a single-layer\nfeedforward neural network with Dropout (Srivas-\ntava et al., 2014) and ReLU activation (Nair and\nHinton, 2010).\nJointly learn framing and political bias. Fi-\nnally, we combine the learned losses in order to\nbuild an embedding space that can group subframes\nas well as separate the contexts of opposing polit-\nical stances, i.e., distinguish between liberal and\nconservative biased news. The final loss that com-\nputes the weighted sum of the contrastive learning\nlossLcand the classification loss LBCE is:\nL= (1\u2212\u03b1)\u00b7 Lc+\u03b1\u00b7 LBCE (1)\nwhere \u03b1is a hyperparameter. The default value of\n\u03b1is 0.5.\n5 Experimental Setup\nWe evaluate the performance of our proposed\nmethod on the tasks of (1) political bias classifica-\ntion and (2) subframe detection. For the first task,\na model makes a binary prediction of whether a\ngiven text has a liberal or conservative perspective.\nWe compare the classification performance of our\nproposed model to the baseline models. Further,\n2785an ablation study on the classification objective\nis performed, and the classification performance\nof the pre-trained models on unseen topic data is\nstudied. As for the second task, we test the per-\nformance of subframe classification and analyze\nthe effect of the bias classification task on the sub-\nframe classification. Finally, we propose a novel\ntopic modeling method for the output embeddings\nand show experimental results.\n5.1 Training Details\nWe leverage the pre-trained BERT and RoBERTa\nmodels as the encoder for our proposed model.\nWe use the following pre-trained models\nfrom Hugging Face: bert-base-cased ,\nbert-base-uncased , and roberta-base .\nThey are trained on English texts with 12 layers,\n768 hidden units, and 12 attention heads. We use\nAdam (Kingma and Ba, 2015) optimizer and set\nlearning rates to {1e\u22125,2e\u22125}, an epoch to 5,\nand a dropout rate to 0.5. For all models, PyTorch\nwas used for implementation. All experiments are\nconducted on an Nvidia Quatro RTX 5000, 16 GB\nmemory GPU in a machine with Intel(R) Xeon(R)\nSilver 4214 CPU @ 2.20GHz.\n5.2 Baseline Models\nWe compare the performance on the political bias\nclassification task with the following baselines.\nBERT cased : A pre-trained BERT with a single-\nlayer feedforward neural network. The BERT en-\ncoder is pre-trained with case-sensitive English\ntexts.\nBERT uncased : A pre-trained BERT with a\nsingle-layer feedforward neural network. The\nBERT encoder is pre-trained with case-insensitive\nEnglish texts.\nRoBERTa : A pre-trained RoBERTa model with\na single-layer feedforward neural network.\n6 Results & Discussion\n6.1 Political Bias Classification\nWe compare the performance of CLoSE against\nthe baseline models described in Section 5.2 on the\ntask of political bias classification. The results are\nreported in Table 3. Our modeling approach outper-\nforms the baseline models for both gun control and\nimmigration bias classification and showed com-\npetitive results for abortion classification. This in-\ndicates that jointly modeling the embeddings withAbortion Gun Immig.\nBERTcased 0.790 0.698 0.801\nBERTuncased 0.788 0.722 0.824\nRoBERTa 0.831 0.720 0.827\nCLoSEBERT-cased 0.821 0.705 0.841\nCLoSEBERT-uncased 0.803 0.716 0.818\nCLoSERoBERTa 0.758 0.724 0.829\nTable 3: F1Scores for Political Bias Classification.\nScores in bold indicate the best performing model for\nthat topic.\nAbortion Gun Immig.\nCLoSEAbort. \u2014 0.722 0.738\nCLoSEGun 0.749 \u2014 0.735\nCLoSEImmig. 0.747 0.686 \u2014\nTable 4: F1Scores for Zero-Shot Political Bias Classifi-\ncation. Cells with \u2014 indicate the topic subdataset the\nmodel was trained on.\n\u03b1= 0 0.5 1\nAbortion 0.297 0.821 0.787\nGun Control 0.328 0.705 0.514\nImmigration 0.184 0.841 0.734\nTable 5: Ablation Study. We compute F1scores of the\npolitical bias classification, dependent on the values of\n\u03b1. For\u03b1= 0, only the contrastive learning objective is\nused. For \u03b1= 1, only the political bias classification\nloss is calculated. For brevity, other \u03b1values are omitted\nbut reflect similar patterns.\na contrastive learning objective improves bias clas-\nsification.\nInterestingly, our pre-trained model also per-\nforms well in a zero-shot learning setting. That\nis, our model can predict the political bias of arti-\ncles from unseen topics. As shown in Table 4, the\nmodel CLoSE Abort. trained on only the abortion\nsubdataset of the Triplet Dataset is able to predict\nthe political stance of articles from the gun control\nand immigration subdatasets with F1scores above\n0.7. Furthermore, the F1score for predicting bias\nof the gun control topic is 0.722. This score is\nextremely close to the best classification score on\nthe gun subdataset by CLoSE RoBERTa , as shown\nin Table 3. The performance of our modeling ap-\nproach in this zero-shot learning setting indicates\nthe model\u2019s ability to learn and transfer latent fram-\ning indicators for political bias detection across\ntopics.\n2786\n(a) Subframes\n (b) Political Bias\nFigure 3: Visualization of the Embeddings for Immi-\ngration. Subfigure (a) shows the clustering of subframe\ngroups. Subfigure (b) is labeled based on the political\nbias of each article.\nAbortion Gun Immig.\nBERTcased 0.879 0.761 0.734\nCLoSEBERT-cased 0.848 0.853 0.905\nTable 6: F1Scores of the Subframe Classification Task.\n6.2 Ablation Study\nTo further verify the usefulness of our joint learning\nobjective, we compute F1scores of the political\nbias classification dependent on the values of the\n\u03b1hyperparameter of Equation 1. For \u03b1= 0.5, we\ncompute the sum of the contrastive objective and\nclassification loss. When \u03b1= 0, the loss function\nbecomes the contrastive learning objective. On the\nother hand, when \u03b1= 1, only the political bias\nclassification loss is considered. The results are\nshown in Table 5.\nWe notice that when only the contrastive loss\nis applied, the model performs poorly on the po-\nlitical bias classification task. As expected, the\nperformance improves when only the political bias\nclassification loss is used (i.e., when \u03b1= 1). The\ninteresting observation is that when we use both\nthe contrastive loss and the bias classification loss,\nas we have proposed in this work, the F1scores are\nthe highest. These results show that incorporating\nsubframe indicators indeed improves political bias\nclassification.\n6.3 Subframe Classification\nWe evaluate whether our proposed model can cor-\nrectly predict the subframe group of a given sen-\ntence. The results are shown in Table 6. We add\na classifier to the pre-trained BERT-based encoder\nand predict the subframe label of a given input. We\nuseCLoSE BERT-cased , which demonstrated com-\npetitive performance across the data in the classi-\nfication task, as our pre-trained encoder and com-\npare it to the baseline model BERT cased. For bothBERT cased CLoSE BERT-cased\nAcc. F1 Acc. F1\nAbort.50 0.232 0.232 0.300 0.300\n100 0.454 0.454 0.641 0.642\nGun50 0.265 0.270 0.485 0.486\n100 0.666 0.679 0.756 0.759\nTable 7: Accuracy and F1Scores of Subframe Clas-\nsification in Few-shot Learning Setting. CLoSE, pre-\ntrained on immigration data, is fine-tuned on a limited\nnumber (50 or 100 triplets) of abortion or gun control\ndata.\nOriginal Data CLoSE BERT-cased\nAbortion 0.413 (0.294) 0.347 (0.225)\nGun 0.303 (0.189) 0.198 (0.182)\nImmig. 0.490 (0.297) 0.383 (0.281)\nTable 8: Average Differences of Subframe Usage Be-\ntween Political Ideologies. Standard deviations are\nshown in parentheses.\ngun control and immigration subdatasets, our pre-\ntrained model significantly outperforms the base-\nline model. For the abortion data, the F1score of\nthe baseline model is higher; yet the difference in\nscores was not statistically significant.\nTable 7 shows that our model outperforms the\nbaseline model even in few-shot learning. For this\nexperiment, CLoSE BERT-cased , pre-trained on the\nimmigration subdataset, is fine-tuned with a limited\nnumber of unseen data to predict subframes. The\nsize of abortion and gun data is limited to 50 and\n100 triplets. For both topics, our model\u2019s accuracy\nandF1scores are higher than the baseline, when\nthe data size is 50 and 100 triplets. This result\nshows that our pre-trained model only requires a\nsmall amount of labeled data for fine-tuning, which\nimproves the scalability of data annotation.\nIn order to understand the influence of political\nbias on framing, we compute the average differ-\nences in subframe usage between biases. Our con-\ntrastive objective encourages CLoSE to separate\ndifferent subframe groups farther apart. Simulta-\nneously, binary political bias information is given\nas context to the embedding space. We would like\nto verify whether the bias objective distinguishes\namongst subframes that are used by both ideolo-\ngies in similar usage percentages. For each sub-\nframe group, we measure the percentage of its us-\nage by the liberal and conservative media sources\nand compute the average of the difference in those\n2787percentages. We then use the subframe group clas-\nsifications of CLoSE and compare them to the true\nsubframe group labels of the original data.\nThe results are shown in Table 8. The average\ndifferences of our model were smaller than those\nof the original data across the datasets. Namely, the\noverlapping usage of subframes between ideologies\nis more clearly observed in our model.\nFigures 3a and 3b are the visualizations of the\nembeddings of the immigration data. Principal\nComponent Analysis (PCA) IS applied to reduce\ndimensions, and the reduced embeddings ARE plot-\nted and labeled with color. Figure 3a displays the\nclustering of subframe groups, and Figure 3b shows\nthe distribution of the liberal and conservative bi-\nased embeddings. Importantly, Figure 3b shows\nthe intersection of subframes between political bias,\nwhich aligns with the experimental results of Ta-\nble 8. The embedding visualizations of the abortion\nand gun control data can be found in Appendix C.\n6.4 Topic Modeling for Frame Extension\nLastly, we propose a topic modeling method that\nleverages the subframe embeddings output by our\nproposed CLoSE model to predict new indicators\nwhich can be used to extend or generalize the sub-\nframes. First, we cluster the framing embeddings,\nthe outputs of our model, with k-means cluster-\ning. Then, we use class-based TF-IDF to cluster\nthe framing embeddings and to find topics (Groo-\ntendorst, 2022). TF-IDF is a classic keyword ex-\ntraction method that combines term frequency and\ninverse document frequency (Joachims, 1996). The\nTF-IDF score of a token tin a document Dis:\nWt,D=tft,D\u00b7logN\nd ft,\nwhere tft,Dis the frequency of the token tin the\ndocument D,d ftis the total number of documents\nthat contain t, and Nis the total number of doc-\numents. The class-based TF-IDF generalizes TF-\nIDF to clusters by considering all documents in a\ncluster as a single document. It is defined as:\nWt,C=tft,C\u00b7log(1 +A\ntft),\nwhere tft,Cis the frequency of the token tin a clus-\nterC,tftis the frequency of tacross all clusters,\nandAis the average number of tokens per cluster.\nFinally, we merge the clusters with high over-\nlap of keywords, or tokens with high class-based1 reproductive, health, justice, women, freedom\n2 federal, funding, abortions, Hyde, Amendment\n3\u22c6\n4 pregnancy, crisis, centers, Pro-Life, women\n5 Wade, Roe, overturn, supreme, court\n6 rights, anti-abortion, religious, catholic, abortion\n7 Affordable, Care, Act, health, insurance\n8 control, birth, health, save, prescription\n9 Planned, Parenthood, selling, unborn, videos\n10 life, unborn, right, baby, child\n11 March, Life, Washington, national, Trump\n12 industry, abortion, giant, business, profit\n13 fetal, tissue, research, illegally, selling\n14 Hobby, Lobby, access, coverage, insurance\n\u22c6Potentially offensive or triggering language has been omitted.\nTable 9: Topics of Subframe Embeddings for Abortion.\nTF-IDF scores. Suppose there are cclusters C=\n{C1, C2,\u00b7\u00b7\u00b7, Cc}.For a cluster Ci, we rank to-\nkens according to their class-based TF-IDF scores\nand choose the top- knumber of tokens. Then, we\ncompute the overlap of the tokens across clusters:\ns(Ci, Cj) =count (Ci\u2229Cj)\n|Ci|,\nwhere i\u0338=j. Ifs(Ci, Cj)is greater than a threshold\n\u03b2, we merge CiandCj. For our experiments, the\ndefault values are k= 50 and\u03b2= 0.4.\nTable 9 shows the experimental results on the\nabortion data. We observe the subframe indicators\nof the original dataset as keywords. For instance,\ntopic 9 has the subframe indicators: \u201cPlanned Par-\nenthood.\u201d Interestingly, our topic modeling method\nis able to identify important words that are notin\nthe subframe lexicons. Topic 2 has known indica-\ntors from the subframe \u201cAbortion Funding\u201d as well\nas new indicators \u201cHyde\u201d and \u201cAmendment.\u201d3This\nindicates CLoSE\u2019s ability to identify new subframe\nindicators which would allow the development of\nnew frames or unsupervised frame detection. Ap-\npendix D presents experimental results for the top-\nics and subdatasets of gun control and immigration.\n7 Conclusion\nWe have presented CLoSE, a framework that in-\ncorporates subframes and political bias using con-\ntrastive learning and multi-task learning. Our pro-\nposed joint objective outperforms baseline models\n3The Hyde Amendment, which was passed in 1976 by\nthe House, is a legislative provision that prohibits the use of\nfederal funds for performing abortions.\n2788in predicting subframes and political bias. Further,\nour pre-trained model adapts to zero-shot learning\nof political bias and few-shot learning of subframes.\nFinally, we propose a topic modeling method for\nthe subframe embeddings and extract a list of key-\nwords, which can be helpful for future subframe\nextensions and annotations. We plan to extend this\nwork to embed general policy frames for study in\nunsupervised settings.\nReferences\nFrank R Baumgartner, Suzanna L De Boef, and Am-\nber E Boydstun. 2008. The decline of the death\npenalty and the discovery of innocence . Cambridge\nUniversity Press.\nVibhu Bhatia, Vidya Prasad Akavoor, Sejin Paik, Lei\nGuo, Mona Jalal, Alyssa Smith, David Assefa Tofu,\nEdward Edberg Halim, Yimeng Sun, Margrit Betke,\nPrakash Ishwar, and Derry Tanti Wijaya. 2021. Open-\nFraming: Open-sourced tool for computational fram-\ning analysis of multilingual data. In Proceedings of\nthe 2021 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations ,\npages 242\u2013250, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. the Journal of\nmachine Learning research , 3:993\u20131022.\nAmber E Boydstun, Dallas Card, Justin Gross, Paul\nResnick, and Noah A Smith. 2014. Tracking the de-\nvelopment of media frames within and across policy\nissues.\nDallas Card, Amber E. Boydstun, Justin H. Gross, Philip\nResnik, and Noah A. Smith. 2015. The media frames\ncorpus: Annotations of frames across issues. In Pro-\nceedings of the 53rd Annual Meeting of the Asso-\nciation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers) , pages 438\u2013\n444, Beijing, China. Association for Computational\nLinguistics.\nKenneth Church and Patrick Hanks. 1990. Word associ-\nation norms, mutual information, and lexicography.\nComputational linguistics , 16(1):22\u201329.\nCarroll Doherty, Jocelyn Kiley, Nida Asheer, and Calvin\nJordan. 2021. Beyond red vs. blue: The political\ntypology. Pew Research Center .\nRobert M Entman. 1993. Framing: Towards clarifica-\ntion of a fractured paradigm. McQuail\u2019s reader in\nmass communication theory , pages 390\u2013397.\nRobert M Entman and Andrew Rojecki. 1993. Freezing\nout the public: Elite and media framing of the us\nanti-nuclear movement.Anjalie Field, Doron Kliger, Shuly Wintner, Jennifer\nPan, Dan Jurafsky, and Yulia Tsvetkov. 2018. Fram-\ning and agenda-setting in Russian news: a computa-\ntional analysis of intricate political strategies. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3570\u2013\n3580, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nMaarten Grootendorst. 2022. Bertopic: Neural topic\nmodeling with a class-based tf-idf procedure. arXiv\npreprint arXiv:2203.05794 .\nLei Guo, Chris J Vargo, Zixuan Pan, Weicong Ding,\nand Prakash Ishwar. 2016. Big social data analytics\nin journalism and mass communication: Compar-\ning dictionary-based text analysis and unsupervised\ntopic modeling. Journalism & Mass Communication\nQuarterly , 93(2):332\u2013359.\nPere-Llu\u00eds Huguet Cabot, Verna Dankers, David Abadi,\nAgneta Fischer, and Ekaterina Shutova. 2020. The\nPragmatics behind Politics: Modelling Metaphor,\nFraming and Emotion in Political Discourse. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020 , pages 4479\u20134488, Online. Association\nfor Computational Linguistics.\nShanto Iyengar. 1994. Is anyone responsible?: How\ntelevision frames political issues . University of\nChicago Press.\nJakob D Jensen, Courtney L Scherr, Natasha Brown,\nChristina Jones, Katheryn Christy, and Ryan J Hurley.\n2014. Public estimates of cancer frequency: cancer\nincidence perceptions mirror distorted media depic-\ntions. Journal of health communication , 19(5):609\u2013\n624.\nThorsten Joachims. 1996. A probabilistic analysis of the\nrocchio algorithm with tfidf for text categorization.\nTechnical report, Carnegie-mellon univ pittsburgh pa\ndept of computer science.\nKristen Johnson and Dan Goldwasser. 2016. \u201call I know\nabout politics is what I read in Twitter\u201d: Weakly su-\npervised models for extracting politicians\u2019 stances\nfrom Twitter. In Proceedings of COLING 2016, the\n26th International Conference on Computational Lin-\nguistics: Technical Papers , pages 2966\u20132977, Osaka,\nJapan. The COLING 2016 Organizing Committee.\nKristen Johnson, Di Jin, and Dan Goldwasser. 2017a.\nLeveraging behavioral and social information for\nweakly supervised collective classification of politi-\ncal discourse on Twitter. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 741\u2013752,\nVancouver, Canada. Association for Computational\nLinguistics.\nKristen Johnson, I-Ta Lee, and Dan Goldwasser. 2017b.\nIdeological phrase indicators for classification of po-\nlitical discourse framing on Twitter. In Proceedings\nof the Second Workshop on NLP and Computational\n2789Social Science , pages 90\u201399, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nGrzegorz Kapu \u00b4sci\u00b4nski and Barry Richards. 2016.\nNews framing effects on destination risk perception.\nTourism Management , 57:234\u2013244.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 task 4: Hyperpartisan news detection. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation , pages 829\u2013839, Minneapolis,\nMinnesota, USA. Association for Computational Lin-\nguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings .\nBingjie Liu and Lori Pennington-Gray. 2015. Bed bugs\nbite the hospitality industry? a framing analysis\nof bed bug news coverage. Tourism Management ,\n48:33\u201342.\nAmy Mitchell, Jeffrey Gottfried, Jocelyn Kiley, and\nKaterina Eva Matsa. 2014. Political polarization &\nmedia habits.\nVinod Nair and Geoffrey E Hinton. 2010. Rectified\nlinear units improve restricted boltzmann machines.\nInIcml.\nMatthew C Nisbet, Dominique Brossard, and Adrianne\nKroepsch. 2003. Framing science: The stem cell\ncontroversy in an age of press/politics. Harvard In-\nternational Journal of Press/Politics , 8(2):36\u201370.\nZhongdang Pan and Gerald M Kosicki. 1993. Framing\nanalysis: An approach to news discourse. Political\ncommunication , 10(1):55\u201375.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP) , pages\n3982\u20133992, Hong Kong, China. Association for Com-\nputational Linguistics.\nShamik Roy and Dan Goldwasser. 2020. Weakly su-\npervised learning of nuanced frames for analyzing\npolarization in news media. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages 7698\u20137716,\nOnline. Association for Computational Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overfitting. The journal of machine learning\nresearch , 15(1):1929\u20131958.Nayda Terkildsen and Frauke Schnell. 1997. How me-\ndia frames move public opinion: An analysis of the\nwomen\u2019s movement. Political research quarterly ,\n50(4):879\u2013900.\nIsidora Tourni, Lei Guo, Taufiq Husada Daryanto,\nFabian Zhafransyah, Edward Edberg Halim, Mona\nJalal, Boqi Chen, Sha Lai, Hengchang Hu, Margrit\nBetke, Prakash Ishwar, and Derry Tanti Wijaya. 2021.\nDetecting frames in news headlines and lead images\nin U.S. gun violence coverage. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021 , pages 4037\u20134050, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nOren Tsur, Dan Calacci, and David Lazer. 2015. A\nframe of mind: Using statistical models for detec-\ntion of framing and agenda setting campaigns. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 1629\u2013\n1638, Beijing, China. Association for Computational\nLinguistics.\nArjen Van Dalen, Claes de Vreese, and Erik Alb\u00e6k.\n2017. Economic news through the magnifying glass:\nHow the media cover economic boom and bust. Jour-\nnalism Studies , 18(7):890\u2013909.\n2790Appendix A Subframes\nTable 10 presents the list of subframes for each\ntopic. As subframes are issue-specific subclassi-\nfications of the general policy frames, the frames\nare written in bold, and subframes that fall under a\ncorresponding frame are displayed below the frame.\nThis table is from Roy and Goldwasser (2020).\nAppendix B Top-3 Subframes\nTable 11 shows the top-3 subframes used across\nthe period for the topic of gun control. Similarly,\nTable 12 is the results for the topic of immigration.\nHigh overlapping of subframes used by liberal and\nconservative media is observed in both data. As\nfor the gun control data, all top-3 subframes from\nthe liberal and conservative articles from 2020 to\n2022 are identical. Similarly, the top-2 subframes\nfrom the liberal and conservative news from 2020\nto 2022 are the same for the immigration data. We\ndid not extract the top-3 subframes of the immi-\ngration data before 2010 because all the articles in\nthe immigration dataset were from liberal media\nsources.\nAppendix C Embedding Visualization\nFigures 4 and 5 are the embedding visualizations of\nthe abortion data and gun control data, respectively.\nThe dimensions of the output embeddings of our\nproposed method are reduced with PCA, and the re-\nduced embeddings are mapped to 2D plots. Subfig-\nure (a) is labeled with color to show the clustering\nof subframes. Subfigure (b) displays the liberal em-\nbeddings in blue and the conservative embeddings\nin red. As we observed in the embedding plots of\nthe abortion data, Figures 4b and 5b display a high\npercentage of overlap between the subframes used\nby liberal and conservative articles.\nAppendix D Topic Modeling of Framing\nEmbeddings\nTable 13 shows the topics of the subframe embed-\ndings of the gun control data, and Table 14 shows\nthose for the immigration data. After merging the\ntopics with high overlapping keywords, there are\n12 and 15 topics for the gun control and immigra-\ntion data, respectively.\nAs for the topics of the gun control data, names\nand locations related to mass shootings are ob-\nserved in topics 10 and 11. In topic 6, the words\n\u201cIslamic\u201d and \u201cYork\u201d from New York appear along\n(a) Subframes\n(b) Political Bias\nFigure 4: Visualization of the Embeddings of the Abor-\ntion Dataset. Subfigure (a) shows the clustering of sub-\nframe groups. Subfigure (b) is labeled based on the\npolitical bias of each article.\n2791Abortion Gun Control Immigration\nEconomic : Economic : Economic :\n- Health Care - Gun Buyback - Minimum Wage\n- Abort. Provider Program - Salary Stagnation\n- Abortion Funding - Gun Business - Wealth Gap\nFairness & Equality: Capacity & Resources: - Cheap Labor Availability\n- Reproduction Right - School Safety - Taxpayer Money\n- Right of Human Life Cultural Identity: Crime & Punishment:\nLegality, Constitution - White Identity - Deportation: Illegal\nality, Jurisdiction: - Person of Color Identity Immigrants\n- Hobby Lobby Legality, Constitution - Deportation: In General\n- Late Term Abortion ality, Jurisdiction: - Detention\n- Roe V . Wade - Ban on Handgun - Terrorism\nCrime & Punishment: - Second Amendment Security & Defense:\n- Stem Cell Research - Concealed Carry - Border Protection\n- Sale of Fetal Tissue Reciprocity Act Legality, Const., Juri.\n- Sexual Assault Victims - Gun Control to - Asylum\nHealth & Safety: Restrain Violence - Refugee\n- Birth Control Crime & Punishment: - Birth citizenship &\nMorality: - Illegal Gun 14th Amendment\n- Sanctity of Life - Gun Show Loophole Policy Pres. & Eval.:\n- Women Freedom Security & Defense - Amnesty\nQuality of Life: - Background Check - Dream Act\n- Planned Parenthood - Terrorist Attack - Family Separation\n- Pregnancy Centers Health & Safety: Policy\n- Life protection - Gun Research - DACA\nPublic Sentiment: - Mental Health Fairness & Equality:\n- Pro-Life - Gun Homicide - Racism & Xenophobia\n- Anti-Abortion Policy Pres. & Eval.: - Merit Based Immigration\n- Pro-Choice - Assault Weapon - Human Right\nMorality: Cultural Identity:\n- Right to Self-Defense - Racial Identity\n- Stop Gun Crime - Born identity\nTable 10: Subframes of the Three Topics: Abortion, Gun Control and Immigration. Frames are written in bold.\nTop-1 Top-2 Top-3\n2020-2022Left Gun Business Industry Conc. Carry Recip. Act Second Amendment\nRight Gun Business Industry Conc. Carry Recip. Act Second Amendment\n2018-2020Left Background Check School Safety Assault Weapon\nRight Background Check Assault Weapon Second Amendment\n2016-2018Left Gun Business Industry Terrorist Attack Conc. Carry Recip. Act\nRight Gun Business Industry Terrorist Attack Assault Weapon\n2010-2016Left Gun Business Industry Background Check Mental Health\nRight Terrorist Attack Gun Show Loophole Gun Business Industry\n-2010Left Second Amendment Gun Business Industry Terrorist Attack\nRight Gun Business Industry Illegal Gun Second Amendment\nTable 11: Top-3 Subframe Indicators of the Gun Control Triplet Dataset.\n2792Top-1 Top-2 Top-3\n2020-2022Left Asylum Detention Human Rights\nRight Asylum Detention Racial Identity\n2018-2020Left Asylum DACA Human Rights\nRight Asylum DACA Amnesty\n2016-2018Left DACA Human Rights Racial Identity\nRight DACA Amnesty Dream Act\n2010-2016Left DACA Human Rights Detention\nRight Birth Cit. & 14thAmen. DACA Dream Act\nTable 12: Top-3 Subframe Indicators of the Immigration Triplet Dataset.\n(a) Subframes\n(b) Political Bias\nFigure 5: Visualization of the Embeddings of the Gun\nControl Dataset. Subfigure (a) shows the clustering of\nsubframe groups. Subfigure (b) is labeled based on the\npolitical bias of each article.with the subframe indicators related to \u201cterrorism,\u201d\nwhich imply the co-ocurrence of a specific religion\nwith a place.\nSimilarly, Table 13 shows specific names and\nplaces such as \u201cGermany,\u201d \u201cMexico,\u201d and \u201cACLU,\u201d\nwhich is an acronym for the American Civil Lib-\nerties Union that fights for civil rights, including\nthe rights of immigrants. The names of the two\nformer presidents are also found: \u201cTrump\u201d in topic\n14 along with the keywords \u201cbuild\u201d and \u201cwall\u201d and\n\u201cObama\u201d in a keyword \u201cObama-era\u201d in topic 5 with\nkeywords associated with the Dream Act.\n1Amendment, Second, rights, protect,\nindividual\n2background, checks, NICS, FBI,\ncriminal\n3industry, manufacturers, business,\nselling\n4rifles, gun, firearms, semiautomatic,\nautomatic\n5illegal, guns, possessions, convicted,\ndealers\n6 terrorism, terrorist, Islamic, York, Trump\n7 ill, mentally, illness, violence, mental\n8 abusers, shootings, victims, health, age\n9ban, handgun, Concealed, Carry,\nReciprocity\n10school, violence, students, Parkland,\nFlorida\n11contempt, Orlando, Mateen, Black,\nAmericans\n12Buyback, program, deaths, restrictions,\nmandatory\nTable 13: Topics of Subframe Embeddings of the Gun\nControl Data.\n27931detention, center, facilities, ICE,\nchildren\n2Amnesty, plan, seekers, refugee,\nGermany\n3Families, Separation, Policy, Trump,\nparents\n4DACA, Democrats, childhood, arrivals,\nprogram\n5Act, Dream, dreamers, children,\nObama-era\n6asylum, migrants, foreign, illegal,\ncaravan\n7rights, human, Civil, Liberties, Union,\nACLU\n8black, brown, undocumented,\nimmigrants, racial\n9 labor, cheap, workers, wages, manual\n10illegal, deported, alien, deportation,\npersons\n11white, identity, supremacist, nationalist,\nracist\n12 terrorist, threat, border, terrorism, terror\n13birthright, citizenship, Amendment, 14th,\nAnton\n14 build, wall, Trump, Republican, Mexico\n15 minimum, wage, inequlaity, income, raise\nTable 14: Topics of Subframe Embeddings of the Immi-\ngration Data.Appendix E Ethical Considerations\nThe Framing Triplet Dataset is an extension of\nthe existing public dataset by Kiesel et al. (2019)\nand Roy and Goldwasser (2020). Additionally col-\nlected documents are also from news texts that are\nfree to the public. Hence, the text corpus does not\ncontain private or sensitive information.\nThe code for the proposed method is open to\nthe public and can be used to study the impact\nof news framing on public opinion. We do not\nanticipate any significant risks of deployment. Still\nwe urge users not to use this research for malicious\nintentions.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "CLoSE: Contrastive learning of subframe embeddings for political bias classification of news media", "author": ["MYJ Kim", "K Johnson"], "pub_year": "2022", "venue": "\u2026 of the 29th international conference on \u2026", "abstract": "Framing is a political strategy in which journalists and politicians emphasize certain aspects  of a societal issue in order to influence and sway public opinion. Frameworks for detecting"}, "filled": false, "gsrank": 446, "pub_url": "https://aclanthology.org/2022.coling-1.245/", "author_id": ["CTCnWk0AAAAJ", "iHeTIZEAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:D9DtglMvkOUJ:scholar.google.com/&output=cite&scirp=445&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D440%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=D9DtglMvkOUJ&ei=VbWsaOTRJ7XCieoP4PfQ0A8&json=", "num_citations": 17, "citedby_url": "/scholar?cites=16541773467057246223&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:D9DtglMvkOUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aclanthology.org/2022.coling-1.245.pdf"}}, {"title": "Elephants Never Forget: Partisan Schemas and the Continued Influence of Misinformation", "year": "NA", "pdf_data": "Bridgewater State Univ ersity Bridgewater State Univ ersity \nVirtual Commons - Bridgewater State Univ ersity Virtual Commons - Bridgewater State Univ ersity \nHonors Pr ogram Theses and Pr ojects Under graduate Honors Pr ogram \n5-3-2021 \nElephants Ne ver Forget: P artisan Schemas and the Continued Elephants Ne ver Forget: P artisan Schemas and the Continued \nInfluence of Misinformation Influence of Misinformation \nJeremy V . Hermanson \nFollow this and additional works at: https:/ /vc.bridgew .edu/honors_pr oj \n Part of the American P olitics Commons , and the Cognition and P erception Commons \nRecommended Citation Recommended Citation \nHermanson, Jer emy V .. (2021). Elephants Ne ver Forget: P artisan Schemas and the Continued Influence of \nMisinformation. In BSU Honors Pr ogram Theses and Pr ojects.  Item 478. A vailable at: \nhttps:/ /vc.bridgew .edu/honors_pr oj/478 \nCopyright \u00a9 2021 Jer emy V . Hermanson \nThis item is a vailable as par t of Vir tual Commons, the open-access institutional r eposit ory of Bridgewater State \nUniv ersity , Bridgewater , Massachusetts. \n \n   \n  \n \n \n \n \n \nElephants Never Forget: Partisan Schemas and the Continued Influence of Misinformation  \n \n \n \n \nJeremy V. Hermanson  \n \n \n \nSubmitted in Partial Completion of the  \nRequirements for Departmental Honors  in Psychology  \n \nBridgewater State University  \n \nMay 3, 2021  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nDr. Elizabeth Spievak , Thesis Advisor       \n \nDr. Ashley Hansen -Brown , Committee Member   \n \nProf. Stephanie Penley,  Committee Member                       \n \n \n \n \n   \n  \nAuthor Note  \nThis manuscript is submitted in partial completion of the requirements of Departmental Honors \nin Psychology. This research would not have been possible without  Dr. Elizabeth Spievak\u2019s \npsychology lab , and the dedication of many undergraduate research assistants , notably Drew \nRobertson and Noah Wasserman.  \n \n   \n  Abstract  \nIn an age where information is plentiful and access to it is practically unlimited, the veracity of \ninformation is frequently an afterthought. Previous research has demonstrated that individuals \nmay often be  reluctant to alter their beliefs and attitudes even after false information is corrected. \nThis phenomenon is known as the continued -influence effect  or the continued influence of \nmisinformation (CIM).  Misinformation  and \u201cfake news\u201d have grown  more common, and their \neffectiveness may be explained by CIM. Research also shows that schemas can have significant \neffects on how information is processed, and preexisting beliefs, values and attitudes can affect \nwhat information is readily absorbe d, ignored , forgotten or invented . Individuals  with more \nextreme partisan  schemas , particularly conservatives,  may be more vulnerable to misinformation . \nThe cur rent study  was an examin ation of  CIM in college students and the general population  \nwho were  exposed to fake news , corrections of fake news , or both.  The hypothes es that attitudes \nabout initial misinformation and degree of belief change upon correction would vary by p artisan \nschema strength  were partially supported . \n Keywords:  fake news, misinformation, schemas , political beliefs , continued influence of \nmisinformation  \n  \n \n   \n Elephants Never Forget : Partisan Schemas  and the Continued Influence of Misinformation  \nPeople must process an overwhelming amount of information on a daily basis, and  \nverifying the credibility or relevance of information is often an afterthought. Mitchell et al. \n(2018)  found that o nly about a third of U.S. adults were able to correctly identify a set of news \nheadlines as facts or opinions, and were less accurate  if the headlines were aligned with their \nown political ideology; for many, judgments of the accuracy and truthfulness  of news  depend ed \non how much they agree d with it. Despite this, there is a growing awareness of the prevalence of \nmisinformation, more commonly known as \u201cfake news.\u201d  \nApproximately half of Americans currently consider  misinformation  a \u201cvery big \nproblem\u201d today , and 68% agree that it has eroded trust in the  U.S. political system (Mitchel l et \nal., 2019) . For example, in a 2020 survey , most Americans  (71%)  reported that they were  \nfamiliar with a conspiracy theory that claims that the COVID -19 pandemic was planned  or \nintentional  (Mitchell et al., 2020) . However, p artisanship was predictive of  whether or not an \nindividual  actually  belie ved in the COVID -19 conspiracy theory: 34% of Republicans stated that \nthe theory was \u201cprobably or definitely\u201d true compared to only 18% of Democrats , and t his \noccurred d espite little difference in how likely both groups were  to be familiar with the claim. \nRepublicans (63%) were also more likely than Democrats (18%) to say that the pandemic has \nbeen exaggerated  (Mitchell et al., 2020) . In another study , Americans report ed that they still \nbelieve in a conspiracy theory stating that former U.S. president Barack Obama was born outside \nthe U.S., and th ose belief s also fell  along partisan lines: 38% of self-described \u201cvery strong\u201d \nRepublicans believe d the conspiracy theory compared to only 11% of \u201cvery strong \u201d Democrats \n(Jardina & Traugott, 2019) . Regardless of  efforts to debunk and correct it, misinformation  (such \nas conspiracy theor ies) has been shown to  influence beliefs and reasoning , and research suggests \n \n   \n that prior experience and knowledge , or schemas, may play a role in the effective ness of  both the \ninitial misinformation and the corrections.  \nThe perseverance of false beliefs as a result of fake news  demonstrates a  phenomenon \nknown as the continued -influence effect, or the continu ed influence of misinformation (CIM), \nwhich describes how the reconstructive processes of memory  can decrease the likelihood that \nbeliefs will be updat ed after correction or retraction of the original misinformation (Anderson et \nal., 1980; H. M. Johnson & Seifert, 1994; McFarland et al., 2007; Ross et al., 1975) . Perception \nand m emory  are generally understood to be constructions, not copies, o f experiences. Research \nsuggests these constructions are influenced by situational and dispositional factors that impact \nboth the ability to process information (e.g. capacity limits) and the motivation to do so (e.g. \ninterest or involvement). As a result , human memory is rife with errors  of various consequence, \ncolored by assumptions and inferences, mingled with traces of the original events  or details \n(Roediger & DeSoto, 2015) . Bartlett  (1932 ) argued that p erceiving, comprehen ding and \nremembering are a function of an individual \u2019s schemas for understanding of the world.  Schemas \ninclude stereotypes , and those related to strong political and social beliefs have been shown to \nexert considerable  influence over processing of misinformation (Bodenhausen & Wyer, 1985; \nBronstein et al., 2019; Greenstein & Franklin, 2020; Nyhan & Reifler, 2010; Walter et al., 2020) . \nTherefore, i n a polarized political environment awash with misinformation, schematic processing \nmay directly impact the perceiver\u2019s ability t o determine the veracity of informatio n and \nsubsequent correction , particularly for individuals with extreme partisan schemas.  \nSchema s \nSchemas are generally considered functional, adaptive and efficient (Bargh, 1982) ; they \nare representations of past experience that become theories about reality and new experiences \n \n   \n (Bartlett, 1932;  Markus & Zajonc, 1 985). As organization s of preexisting knowledge and \nmemories , schemas include concrete matters (e.g. places, objects, people) and the abstract (e.g. \nbeliefs, relationships, attitudes) (Baldwin, 1992; Brewer & Nakamura, 1984; Brewer & Treyens, \n1981; Judd & Kulik, 1980) . A schema helps to reduce cognitive load  by fitting new inf ormation  \ninto existing concepts rather than having to start from scratch to understand new things, such as \nhow to navigate a  social  situation or the purpose of a novel object. For example, if one were \ndressing for work, the jeans and motorcycle jacket would be ignored ; attention would be directed \ntoward  the schema -appropriate selection of collared shirt s. Similarly , a new pair of jeans would \nnot be mistaken as office appropriate , and with little co mplex thought a rental car can be driven \nthe familiar route to the office . Schemas  work quite well for routine tasks and everyday \ninformation processing, and they allow perceivers to preserve precious cognitive resources for \nmore complex problems. Therefor e, information that is schema -consistent would generally be \neasier to process than information that is schema -inconsistent. Given that most of our \ninformation processing is fairly trivial, schematic thinking tends to be the rule rather than the \nexception ( see Fiske & Neuberg, 1990)  and the costs are quite modest in comparison to the \nenergy -saving benefits.  In essence, a schema is both a framework for organizing existing \ninformation one already has and for how to interp ret new information. As Bartlett himself put it, \n\u201cThe influence of 'schemata' is influence  by the past\u201d  and he argued that memory is not divorced \nfrom context (1932) . \nA long history of research suggests that preexisting beliefs, attitudes and values in the \nform of schemas influence the  likelihood of scrutinizing incoming information , which is \nselectively encoded and organized according to thematic structures. This is of ten referred to as \nthe consistency bias; at titude -relevant information is more easily processed and recalled , new \n \n   \n information and experiences tend to be assimilated into existing schemas, and  conflicting, \nambiguous or confusing information is later remembe red as having been more in line with the  \nindividual\u2019 s preexisting schema  (Bergman & Roediger, 1999; Gawronski, 2012) . Judd and Kulik \n(1980)  asked participants to rate how much they agreed or disagreed with varying statements \nabout contentious social issues (e.g. capital punishment, women\u2019s rights, and majority rule in \nSouth Africa), to rate how pro/anti each statement was for the social issue, and they were asked \nto recall the statements the next day. Participants responded more quickly , and recalled \nstatements more accurately , when they had previously agreed/disagreed more  strongly; \nstatements that participants responded to more moderately were more difficult to remember later.  \nThe authors argued that attitude s, as social  schemas , drive expectations about the type of \ninformation that is likely to be enc ountered and that, particularly for bipolar issues,  people expect \nto encounter very agreeable or quite disagreeable information, which facilitates the encoding and \nretention of information that is either highly schema -consistent or highly contradictory.  In \nanother example, study participants in a graduate student\u2019s office recalled the presence of objects \nconsistent with existing schemas  for a university office  (e.g. books) even if those objects were \nnot present (Brewer & Treyens, 1981) . Information that is inconsistent with an individual\u2019s \nexisting schemas is more likely to be forgotten or i gnored, while information that is consistent is \nrecalled with relative ease and becomes salient in decision -making (Bodenhausen & Wyer, 1985; \nTuckey & Brewer, 2003) . Schemas are vital in what individuals pay attention to, how the \ninformation is processed, what is ultimately lear ned, and the constructive nat ure of recall . Of \nparticular interest for the current study is how schemas likely contribute to the processing and \nretention of misinformation  after correction.  \n  \n \n   \n Schema Theory  and the Continued Influence of Misinformation  \nThe continued influence of misinformation (CIM) tends to be explained as an artifact of \nthe way memory and memory organization (schemas) are  broadly understood to work. \nInformation in the form of an \u201cengram,\u201d or memory trace, is processed and encoded into \nsemantic long -term memory, where it is stored for later inclusion  in a reconstructive process  \n(Baddeley,  1966; Bartlett, 1932; Dudai, 2004; Goldstein, 2014; Hemmer & Steyvers, 2009) . \nDetails may be recalled correctly but misattributed  to another familiar or commonly referenced \nsource, or schema -related knowledge may be used to fill knowledge gaps, resulting in flawed but \nstable memories  rich with plausible elements that contribute to the recaller\u2019s confidence (Kleider \net al., 2008) . In a similar way, corrections and new learning become associated and intertwined \nwith the initial misinformation . Misinformation  is not simply unlearned or removed from \nmemory ; it continues to contribute and interfere in a variety of ways  (Ayers & Reder, 1998) . \nAlternatively, c orrection s of previous information may become an addendum that \u201ctags\u201d the \ninitial misinformation  with a negation, which  may be lost over time  or overlooked under stress  or \ntime constraints (H. M. Johnson & Seifert, 1998) .  \nFactors related to the information itself also appear to influence CIM. For example, much \npast research suggests that misinformation that is causal (i.e. explains events, such as \u201cthe \nbuilding caught on fire because of improperly stored gasoline\u201d) results in gr eater CIM, and that \neffective corrections must be both plausible and causal in nature  (H. M. Johnson & Seifert, 1994; \nRapp & Kendeou, 2007; Schul & Mazursky, 1990; Seifert, 2002) . In other words, corrective \nmessages should  have as much explanatory power as the misinformation in order to be effective. \nEcker et al.  (2011)  applied varying  strengths of encoding for misinformation and then tested the \neffectiveness of  retractions; both strong and weak memory encoding resulted in CIM. While a \n \n   \n strong retraction reduced memory strength  for misinformation, no amount of retraction \neliminated CIM c ompletely . The results supported previous findings that even weakly encoded \nmisinformation was resistant to retraction  (Schul & Mazursky, 1990) .   \nEvidence suggests that s chema -consistent information is perceived and recalled easily \nand quickly, a process referred to as fluency, which likely contributes to CIM . As experience or \ninformation becomes routine, it becomes schematic. Thus , information that has been encountered \nfrequently becomes schematic and is more likely to be considered factual or accurate late r, even \nif the individual was made aware that it was false  (Pennycook et al., 2018) . In one study of this  \n\u201cillusory truth effect ,\u201d DiFonzo et al.  (2016)  found that repetitive exposure to rumors impacted \nthe validity judgements of the rumors. Processin g fluency, or the relative ease with which \nparticipants could process statements due to familiarity, mediated the validity judgements, \nsuggesting that familiarity play ed a role in susceptibility to rumor  acceptance . Pennycook et al.  \n(2018)  found that even relatively implausible headlines from (actual ) fake news articles shared \nonline were judged as more accurate after only a single  previous  exposure  (although extremely \nimplausible statements su ch as  \u201cSmoking cigarettes is good for your lungs\u201d  were unconvincing) . \nThe familiarity effect occurred despite warnings that the headline was disputed or contested by \nfact-checkers . The authors suggest  that such disclaimers are likely  ineffective  weapons ag ainst \nmisinformation because r elatively little exposure to plausible statements  is necessary to increase \nperceived accuracy of information. Fazio et al.  (2015)  also found that prior knowledge was not \nparticularly effective at preventing illusory truth effects, which demonstrat ed a \u201cknowledge \nneglect\u201d in favor of processing fluency.  In short,  many factors influence  memory  encoding, \nrecall and  ultimately  CIM, particularly prior experience and familiarity,  suggesting that schemas  \nmay be a significant factor in misinformation and correction .  \n \n   \n Personal Schemas  and Misinformation  \nSchemas and their components  may be particularly influen tial in th e memory for, and \nresilience to, misinformation , under conditions that reduce  the likelihood of analytical thinking.  \nIn one study , Pennycook and Rand (2019) , found that those with higher scores on the Cognitive \nReflection Test, a measure of likelihood to engage in analytical thinking , were better able to \ndiscern fake news and real news , even if the headlines did not match the participants\u2019 own \npolitical ideology . Those with lower score s were less likely to pick the false or true headlines \napart. Other studies found similar effects (De keersmaecker & Roets, 2017; Zhu et al., 2010) . In \none study of misinformation during political campaigns, Murphy et al. (2019)  demons trated that \nfalse memories relevant to issue -based voting could be experimentally manipulated , particularly \namong those who scored lower in analytical thinking . The study used either real or fake news \nabout scandals on either side of Ireland\u2019s abortion ref erendum campaign, and asked participants \nif they remembered the events. Those in favor of the referendum to repeal an abortion ban were \nmore likely to \u201cremember\u201d a fake news scandal for the opposing campaign; those opposed to the \nrepeal were more likely to  recall a fabricated scandal for those in favor . The effect was stronger \nfor participants who scored low on a measure of cognitive ability.  \nThe tendency to favor  schema tic processing over analytical thinking  has also been linked \nto individual difference variables such as fear of negative evaluation, harm avoidance, and \ncooperativeness , characteristics  that Zhu et al. (2010)  found were predictive of a vulnerability to \nmisinformation. Bronstein et al. (2019)  found that traits such as dogmatism, delusionality, and \nreligious fundamentalism were positiv ely related to belief in fake (but not true) news, and the  \nauthors  noted that the traits also shared a  common link with reduced propensity for analytic \nthinking. Individual differences in sociopolitical views have been connected  to schema -based \n \n   \n processing. A long history of research suggests that value -relevant attitudes, especially political \nor social ones, are among the strongest (Johnson & Eagly, 1989) , and strongly held and value -\nladen attitudes serve to maximize processing of information that matches attitudes and to \nexaggerate the dissimilarity of information that is in contrast, as in schemas (Sherif & Hovland, \n1961) . Schemas  impact how a person re sponds to  information , and partisan schemas representing \nstrong political and social beliefs may be particularly salient in processing information relevant \nto these schemas, often the subject of both real and fake news.  \nThe Influence of Partisan  Schemas  \nSchemas encompass an  individual\u2019s worldview , including their political and social \nbeliefs, values and attitudes , and p olitical bias has been linked to differences in information \nprocessing (Dodd et al., 2012, 2016; Jost et al., 2003) . Research suggests that partisan political \nattitudes  are among the strongest of convictions  and they predict intolerance of opposing beliefs \n(van Prooijen & Krouwel, 2017) . In addition, t hose with stronger partisan schemas more readily \nlearn and accept information that supports those beliefs , and are more likely to ignore corrections \nthat contradict their political views; partisanship affects both learning and memory (Johnson & \nEagly, 1989; Khanna & Sood, 2018; Taber & Lodge, 2 006). For example, in three studies, \nNyhan and Reifler (2010)  found that the effect of corrections was conditional on the strength of \npreexisting attitudes; corrective information was most successful when it alig ned with strongly \nheld partisan beliefs, and was the least successful when the correction was counter -attitudinal. \nThese effects were particularly robust among U.S. conservative s. Researchers have offered \nseveral explanations for such partisan differences in information processing. Some have found a \nconnection between conservative schemas  and disinterest  in novel stimuli (Carraro et al., 2011; \nDodd et al., 2012; Shook & Fazio, 2009) , low tolerance for uncertainty, and a high need for \n \n   \n closure  (Jost et al., 2003) . In short, many of the traits  associated with misinformation \nvulnerability (e.g.  dogmatism , religious fundamentalism , etc.;  Bronstein et al., 2019)  may be \nmore c ommon to those with strong conservative political bias (Jost et al., 2003) .  \nAxt et al.  (2020)  found that s imply calling information \u201cfake news\u201d often appeals t o \nconservative political schemas . The researchers conducted several experiments in order to better \nunderstand what draws people t o make fake news attributions . Participants read about errors \nmade in reporting the news  that either matched their  political  beliefs or were schema -\ninconsistent, and then indicated how much they believed the error was due to intentional \ndeception or incompetence. Results suggested that the fake news label appeal ed to the \nconservative participants; it satisfied a need for structu re and certainty , and a tendency to \nattribute mistakes in the news to a more purposeful  reason (i.e. intentional deception ) rather tha n \nsimple  error s in reporting . In other words, it may be easier to imagine news organizations \nconspiring to release fake news for malicious ends that are predictable  and concrete , rather than \ncomplex reasons that are difficult to pin down or solve. Both increased tendency to make fake \nnews attributions, and vulnerability to fake news itself, are related to partisanship.  \nBecause  schemas exert influence over what individuals learn and remember , those with \nstrong schemas  likely  seek out and more readily learn information  (e.g. real or fake news)  that is \nconsistent with their schemas , while  ignoring or reject ing information that is not. Due to the \nstrength of political beliefs and associated values, m isinformation that appeals to preexisting \nschemas about the nature of political figures may be particularly effective on strong partisan \nschemas . In particular, CIM may have a more powerful  effect  on those with strong conservative \nschemas who are motivated to  manage the threats of uncertainty and complexity  (Jost et al., \n2003) . \n \n   \n Current Studies  \nThe above literature review suggests that message content and delivery, as well as \nindividual differences in the form of political beliefs and schemas, may make people \ndifferentially vulnerable to misinformation by promoting the use of mental  shortcuts , decrease  \nthe likelihood of  analytical thinking, impair memory, and  thus increase CIM , particularly for \nconservative political schemas . The current stud ies were  designed to respond to calls for further \nresearch on the relationship between CIM an d preexisting attitudes and beliefs (Ecker et al., \n2014) , particularly in relationship to political orientation (Nyhan & Reifler, 2010) . Our \nhypotheses were that participants who identif ied as strongly partisan w ould 1) be less critical of  \nmisinformation that was schema -consistent , 2) be less resistant to a correction  that was schema -\nconsi stent , 3) be more critical of misinformation that was schema -inconsistent , and  4) be more \nresistant to a correction that was schema -inconsistent.  The effects were expected to be stronger \namong conservatives.  \nStudy 1  \nMethod  \nParticipants  \n Participants were recruited from the Bridgewater State University SONA subject pool, \nwhich is comprised primarily of students enrolled in the Introduction to Psychology  course  in \norder to earn course credit . There were 287 participants (94 males, 181 females , and 12  who \nindicated  other or did not specify; Mage = 18.62). In terms of ethnicity, participants primarily \nidentified as Caucasian/White (72.8%) , followed by African American/Black (10.5%), \nLatino/Hispanic (7%), Asian/Pacific Islander (1.4%), and  other (3.8%), and some did not \n \n   \n respond (4.5%). There were 103 participants that identified as Democrat, 42 that identified as \nRepublican, and 110 that identified with no  party.  \nMeasures  \n The dependent variable, the continued influence of misinformation, w as measured by the \nLikert scale ratings of the target on 15 different attributes (e.g. \u201cintelligent,\u201d \u201cresponsible ;\u201d see \nAppendix A ). Participants also responded to several attitude measures. The variable of particular \ninterest was political orientation . We included a single item that asked which political party the \nparticipant identified with (Republican, Democrat, or no party ) and one question that asked \nparticipants to rate their  self-identified  political leaning on a 0 -7 scale from extremely liberal to \nextremely conservative . We also included the Political Belief Scale (PBS; Webber et al., 2018) , \nwhich was designed to study extremist beliefs. Participants rated their  agreement with a series of \nstatements that represent liberal, moderate, and conservative policies and values (liberal \u03b1 = .77; \nmoderate \u03b1 = .66; conservative \u03b1 = .83, as reported by Webber et al., 2018a) . Participants also \ncompleted the  Social -Dominance Orientation Scale (Pratto et al., 2011) , which is designed to \ncapture beliefs in societal group hierarchies and generalized prejudice  (\u03b1 = 83, as reported by  \nPratto et al., 1994) . Scale reliability results for all studies are reported in Table 1 . \nProcedure  \nThe project was IRB approved and data collection was completed during the Fall of \n2019. Participants entered the lab and were instructed to sit at a computer with a packet and \npencil in front of them, and after signing the informed consent document, they could begin \ncompletin g the packet. At certain points in the packet, they were prompted to advance a \nslideshow on the computer in front of them . The slide directed the participant to click on links  to \nactual websites which var ied based on condition. This way, the articles appea red in a realistic \n \n   \n manner, and the effect of misinformation and efficacy of corrective news reports increased the \nexternal validity of the study.  \nThe fake news article was hosted on a \u201csatire\u201d news website called Taters Gonna Tate \n(www.tatersgonnatate.com , now defunct;  article archived at http://archive.is/W5iHx ), and \nclaimed that Democrat House Representative Rashida Tlaib  committed election fraud and was \nunder investigation. While the article makes this claim repeatedly, it also contradicts itself \nseveral times, and the article is posted under the category of \u201cSatire and/or Conservative Fan \nFiction,\u201d making it possible for  an astute reader to determine that the article is misinformation \nand the source is unreliable. The correction article was posted on the reputable fact -checking \nwebsite Snopes ( https://www.snopes.com/fact -check/rashida -tlaib-investigation/ ). The Snopes \narticle explicitly correct ed the exact article posted to Taters Gonna Tate, accurately reporting that \nthe original story was a \u201ccomplete fabrication,\u201d on a network of websites \u201cinfamous for \ngenerating politically inflammatory misinformation under the guise of  proffering \u2018satire,\u2019\u201d and \nsuggesting that some misinformation is originally distributed in this manner (Mikkelson, 2019) .  \nThe study was originally designed with six conditions total (3x2). In C1, participants read \nthe fake news article online with negative misinformation about the politician  (Rashida Tlaib) , \nand the correction. Participants completed the individual differences sc ales between reading the \nfake news article and the correction. In C2, participants read the correction only. In C3, \nparticipants read the fake news article only. Participants completed all parts of the packet and \nwere debriefed and dismissed.  The correspon ding experimental conditions in which the target \nwas a Republican politician (C4, C5 and C6) were unable to be completed due to the COVID -19 \npandemic.  \n  \n \n   \n Results  \nA mean target rating was calculated by aggregating the 15 attributes. There were no \nsignificant differences between the ratings of the target of fake news in C1 (M = 2.59) and C3 ( M \n= 2.44) ( p = .27). Additionally, participant ratings of the target of the correction article in C1 ( M \n= 3.19) and C2 ( M = 3.33) were not significantly different ( p = .35).  \nTo test the hypotheses regarding CIM, t he C1 pre-correction ratings were subtracted from \nthe post -correction ratings to create a measure of rating change . For  those who identified as \nDemocrat, Mchange  = 0.84; for those who identified as Republican, Mchange  = -0.01; and for those \nwho identified with no party, Mchange  = 0.63. A one -way ANOVA revealed significant \ndifferences between the means for Republicans and Democrats ( p = .03), Republicans and no \nparty ( p = .03), but not between  Democrats and no party ( F(2,63) = 4.16, p = .37). Republican \nparticipants updated their beliefs significantly less than Democrats and no party participants.  \nConservat ive leaning (on the single -item Likert scale) predicted more critical evaluations \nof the target overall: fake news r(136) = -.17, p = .05; correction r(124) = -.42, p < .01.  However, \nthe correlation between  self-identified  political leaning and rating change did not quite reach \nsignificance , r(64) = -.22, p = .08. Table 2 shows the results of correlations between scores on \nthe PBS, Social Dominance Orientation, and evaluations pre -correction, post -correction, and \nrating  change. Agreement with l iberal  statements on  the PBS was significantly positively \ncorrelated with evaluation of target before and after correction, but not with rating change. \nConservatism scores from the PBS were significantly negatively correlated with eval uation of \ntarget after correction, but not with rating  change. Moderate scores from the PBS were \nsignificantly positively correlated with evaluation of target after correction and with rating  \nchange  (see Table 2 ), but had low reliability (see Table 1 ). \n \n   \n Discussion  \nThe difference in the rating change between participants who identif ied as Republican \nversus those that identif ied as Democrat or with no political party support ed the hypothesis that \nstrong partisan schema s may influence CIM . Politically right -leaning participants updated their \nbeliefs about the Democrat target much less than politically left -leaning or neutral participants, \nas expected.  \nThe correlations indicate d that participants \u2019 pre-existing left or right -leaning politic al \nbeliefs  were more strongly related to overall evaluations  of the target than to  rating  change. \nWhile  moderate political beliefs were positively correlated with rating  change, it is difficult to \ninterpret directionality . It may be  that those high in mode rate political beliefs update d their \nbeliefs to a greater degree, or that those low in moderate political beliefs update d their beliefs to \na lesser degree, or both.  \nLimitations  \nOverall, t here was l ess evidence of CIM than in previous research;  participants tended to \nupdate their beliefs upon correction. This may be due to the relatively short duration between \nreceiving the misinformation and the correction , or because there was not significant power; \nthere were  relatively few participants in C1  who identified with the Republican party  or \notherwise expressed conservative leanings . Moderate scores on the PBS were relatively low in \nreliability and were excluded from further analysis in Study 2 and Study 3.  \nA Republican target news article and assoc iated  survey was planned to be completed in \nthe Spring of 2020, however, the COVID -19 outbreak disrupted data collection  after fewer than \n50 students participated . Instead, a  follow -up study was conducted to repeat the experiment with \nall conditions presen t in a fully online format . This was to ensure that slight differences in \n \n   \n methods, or political beliefs shifting due to the pandemic itself, were  not significant factors in \nany resulting differences.   \nAdditionally, Study 1 used actual fake news articles an d corrections posted online  to real \nwebsites.  However, f inding equivalent fake news articles and corrections across all target \nconditions  was determined to be prohibitively difficult. Instead, Study 2 used a template derived \nfrom an actual news article, an d an original correction template designed by the researchers.  This \nallowed for greater internal validity across conditions by ensuring that the article was very \nsimilar , aside from changing names and affiliations as appropriate.  \nStudy 2  \nMethod  \nParticipants  \nParticipants were once more  recruited from the Bridgewater State University SONA \nsubject pool, which is comprised primarily of students enrolled in the Introduction to Psychology \ncourse . They volunteered  in order to earn course credit. There w ere 2 97 participants total \ncomprised of 76 males, 218 females , and 3 who specified non -binary or bigender (Mage = 19.54 ). \nIn terms of ethnicity, participants  could select more than one category, and  the most frequently \nchosen category was  Caucasian/White ( 75.8%), followed by  African American/Black ( 12.8% ), \nLatino/Hispanic ( 7.1%), Asian/Pacific Islander ( 3.4%), Native American/American Indian \n(1.7%), and other /non-specified (4.4 %), and some chose  not respond ( 1.3%). There were 165 \nparticipa nts that identified as Democrat, 35 that identified as Republican, and 97 that identified \nas no party.  \n  \n \n   \n Measures  \nThe dependent variable was similar to Study 1, with a slight change to more closely align  \nstatements  with Stereotype Content Model  (SCM)  dimensions of competence and warmth : we \nremoved an item that ask ed how \u201cfavorable\u201d the article target is  because nothing similar is  \nincluded in the SCM  scale (see Appendix A) . The Stereotype Content Model (Fiske et al., 2002)  \nhas been used to describe racial and ethnic stereotypes along dimensions of competence and \nwarmth (competence \u03b1 = .94; warmth \u03b1 = .90 as reported by Fiske et al., 2002 ). Study 2 also \nincluded the Intellectual Humility Scale  (IH; Alfano et al., 2018) , which has four subscales \nsupported by five confirmatory factor analyses and validity studies  (convergent and divergent \nwith self -reported and informant personality and behavior) in two languages  (Alfano et al., \n2017) . Previous research found that intellectual humility is related to tolerance of  opposing \nviews , despite the strength of an individual\u2019s own partisan bias (Porter & Schumann, 2018) , and \nwe sought to explore how this might be related to CIM.  Scale reliability results for all studies  are \nreported in Table 1 . \nProcedure  \nParticipants were asked to complete a survey hosted on Qualtrics and distributed online \nvia SONA, and were allowed to complete the survey at a time of their choosing. They were \nrandomly assigned to one of six possible conditions: Democrat fake news and cor rection (C1), \nRepublican fake news and correction (C2), neutral fake news and correction (C3), Democrat \ncorrection only (C4), Republican correction only (C5) or neutral correction only (C6). The fake \nnews articles were fabrication s designed by the research ers for the study . The material was based \non a United States Department of Justice website  article  about a bookkeeper charged with \nembezzlement from their company  (see: https://www.justice.gov/usao -edpa/pr/philadelphia -\n \n   \n bookkeeper -convicted -embezzling -almost -16-million -former -employer ). All names and \nreferences to the company in the article we re changed to refer to either Democr at House Rep. \nAdam Schiff (C1/C4), Republican House Rep. Jim Jordan (C2/C5), or Starbucks CEO Kevin \nJohnson (C3/C6), and appropriate organizations for each figure (see Appendix B). Schiff and \nJordan were selected as they were in equivalent positions within politics and somewhat known \noutside of their respective states ; Johnson was selected as the neutral target due to his association \nwith a familiar company , and neither he nor Starbucks have well -known or clear political \naffili ations. Participants were told that the article  had appeared on social media recently  and was  \nshared from worldnewsdailyreport.com , a website  which  describes itself as \u201csatire\u201d and its \narticles are routinely posted online as fake news. The corrections were framed as also having \nbeen shared on social media , but from www.snopes.com , and were an explicit correction of the \nfake news article. The corrections detail ed the origin of the fake article and stated that no part of \nit was true, and that the website it came from is known for other fake news articles (see \nAppendix C ). As in the previous study, b etween viewing the fake news and correction, or before \nviewing the correction, participants complete d scales including the PBS, the Social Dominance \nOrientation Scale, and the Intellectual Humility scale . Afterwards, participants receive d credit for \ncompleting the study and were dismissed.  \nResults  \nWarmth and competence ratings were c alculated for each target. A multivariate analysis \nof variance of final target ratings indicated there were no main effects for target or participant \npolitical party, condition, and no significant interactions. Regardless of participant or target \npolitical  party, evaluations of the target after correction were close to , but not significantly \ndifferent from,  those who only saw the correction article. To test the hypotheses about CIM, t he \n \n   \n pre-correction ratings were subtracted from the post -correction ratings  to measure  rating change \nin warmth and competence. Figure 1  shows pre and post ratings by participants who identified as \nDemocrats, Figure 2  shows ratings by Republicans, and Figure 3  shows ratings by participants \nwho endorsed no party. In these figures, Republicans appeared to update their competence and \nwarmth ratings for Democrat targets the least. However, analysis of variance by target political \nparty (Democrat, Republican or neutral) and participant political party did not reveal  statistically  \nsignificant main effects or interactions. Participant political party (Democrat, Republican or \nneutral) and target political party (Democrat, Republican or neutral) did not  significantly \ninfluence change in warmth and competence ratings (all p > .05). Low power likely impacted the \nability to find significant differences between groups. Because there were so few Republican \nrespondents, several experimental groups had fewer th an 10 participants.  \nCorrelational results suggested partial  support  for hypotheses 1 and 3 about partisan \nschemas and target evaluation  overall . Social dominance orientation was significantly negatively \ncorrelated with final evaluations of competence ( r(294) = -.15, p = .01) and warmth ( r(294) = -\n.12, p = .04), while the open -mindedness ( r(294) = .16, p = .01) and engagement ( r(294) = .12, p \n= .04) from intellectual humility predicted more positive final competence evaluations.  \nCorrelational results also s uggested support for the hypotheses 2 and 4 regarding CIM and \npartisan beliefs. Agreement with conservative statements on the PBS was significantly \nnegatively correlated with competence rating change ( r(172)  = -.20, p < .01) and warmth rating \nchange ( r(172)  = -.24, p < .01) across all targets. Rating change was also negatively correlated \nwith social dominance orientation ( r(172)  = -.26, p < .01 for competence , and r(172)  = -.25, p < \n.01 for change in warmth ratings). T here were also significant correlations between total \nIntellectual Humility scale scores and both competence rating change ( r(172) = .17, p = .03) and \n \n   \n warmth rating change ( r(172) = .16, p = .04). Individual Intellectual Humility subscales were \ncorrelated with rating change depending on the target\u2019s political affiliation (see Table 3 for more \ndetailed  results).  \nDiscussion  \nANOVA results failed to support the hypothesis that participants would update their \nbeliefs differentially based on their own partisan schemas and the political affiliation of the target \nabout which they received misinformation and correction, likely because of low power. Trends \nindicated that Democrats, Republicans and no party participants may update their beliefs upon \ncorrection to varying degrees based on the target\u2019s political affiliation. Democrats tended to \nupdate for Democrat or neutral targets, and Republicans seemed to update less overall. Figure 2 \nshows what appears to be a clear differ ence by participants who identified as Republican in how \nthey update their competence  and warmth ratings upon correction of fake news. Indeed, they \nindicated a similar level of warmth but much higher competence for the Republican target before  \nthe correcti on than for the Democrat target after  correction . Republican participants  also updated \ntheir ratings the most for a Republican target. While separating by target did not reveal \nstatistically significant differences, the trends  indicated that further resear ch is warranted; \npartisan schemas may have affected how participants responded to targets from different parties.  \nCorrelational analyses indicated that final evaluations of targets \u2019 competence and warmth  \nwere associated with SDO , lending partial support for hypotheses 1 and 3. IH subscales for open -\nmindedness  and engagement were associated with evaluations of target competence . Intellectual \nhumility has been linked  to tolerance of opposing views  and political views in particular,  which \nmay impact CIM. Porter and Schumann (2018)  found that despite holding political beliefs of \nsimilar strength, those higher in intellectual humility were still more tolerant of opposing \n \n   \n political views than those lowe r in intellectual humility . Therefore , we suggest that those with \ngreater intellectual humility may be more likely to update their beliefs upon correction as well. \nHowever, the correlations were fairly weak despite statistical significance. The correlation  \nresults indicated stronger support f or hypotheses 2 and 4 regarding CIM: there were significant \nnegative correlations between  PBS conservatism scores and rating change for competence and \nwarmth. In other words, the more conservative a participant was, the less they changed their \nratings of t argets on correction. The effect  of conservative schemas  appeared to be stronger for \nDemocrat and neutral targets. Rating change was also  negatively  correlated with SDO , indicating  \nthat extreme partisan beliefs about social hierarchies may affect CIM as well . With regard to \nintellectual humility, it was the Republican and neutral targets who benefitted most. Higher \nintellectual humility predicted somewhat more change for those targe ts, and  intellectual humility \nmay be related to how likely one is to update their beliefs upon correction. While we reported \nthese results, we consider them to be exploratory in nature and not directly related to our \nhypotheses.  \nLiberal views and intellect ual humility were linked to a tendency to update beliefs upon \ncorrection. The results also suggest ed that the degree to which individuals update d their beliefs \nupon correction var ied by target political party. Partisan schemas, particularly conservative on es, \nseemed to impact the effectiveness of correction after exposure to fake news.  \nLimitations  \nSome trends could not be interpreted because of low power; there were few participants \nwho identified as Republican or neutral overall, which rendered very small  sample sizes in some \nconditions.  The lack of Republican participants was an issue we sought to correct with Study 3 \nby using a different recruitment strategy.  \n \n   \n Study 3  \nMethod  \nParticipants  \nParticipants were recruited from Prolific, an online subject pool. Recruitment criteria \nwere limited to only requiring U.S . nationality. Unlike the previous studies, participants of all \nage groups and backgrounds were recruited . There were 223 participants total comprised of 93 \nmales, 111 females , 7 who specified non -binary or bigender, and 1 who preferred not to answer \n(Mage = 31.07 ). In terms of ethnicity, participants  could select more than one category, and  the \nmost frequently chosen category was  Caucasian/White ( 66%) followed by  Asian/Pacific Islande r \n(15.6%), African American/Black ( 10.8%), Latino/Hispanic ( 9.0%), Native American/American \nIndian (1. 9%), and other /non-specified ( 1.4%), and some chose  not to respond ( 0.9%). There \nwere 130 participants that identified as Democrat, 27 that identified as Republican, and 55 that \nidentified as no party.  \nMeasures  \nThe same fake news articles were used.  Variables  were  manipulated and measured in the \nsame manner as  Study 2 . Scale reliability results for all studies are reported in Table 1 . \nProcedure  \nParticipants were asked to complete a survey identical to  Study 2 , using the same fake \nnews articles and corrections from Study 2  (any alterations were made to conform with Prolific \nstandards).  Afterwards, participants were thanked,  receive d acknowledgement and compensation \nfor completing the study ($1.98) and were dismissed.    \n \n \n \n   \n Results  \nWarmth and competence ratings were calculated for each target ; means are reported in \nTable 4 for clarity.  A multivariate analysis of variance of final target ratings indicated there were \nno main effects for condition or  participant  political party, or their interac tion. However, there \nwas a significant main effect for target political party ( F(4, 364) = 4.35, p < .01). The univariate \ntest indicated that the effect for target was driven by higher mean ratings for the Democrat ( 0.40) \nand Neutral ( 0.57) targets\u2019 compet ence in comparison with the Republican ( -0.17), F(2, 184) = \n6.39, p < .01. Warmth ratings did not differ ( MDem = -.11, MNeutral  = -.09 and MRepub -.33; F(2, \n184) = .99, p = .37 ). There was also a significant interaction between target and participant \npolitical party (F(8, 364) = 2.17, p = .03). Republican participants rated the Republican target \nsignificantly higher in competence ( M = 0.46) than either Democrats ( M = -0.41) or no party ( M \n= -0.20). Competence ratings for the other targets did not differ. Republican participants also \nrated the Republican target significantly higher in warmth ( M = 0.33) than either Democrats ( M \n= -0.83) or those affiliated with neither party ( M = -0.59). Warmth ratings for the other targets \ndid not differ.   \nFigure 4  shows pre and post ratings by participants who identified as Democrats, Figure 5  \nshows ratings by Republicans, and Figure 6  shows ratings by participants who endorsed no party. \nFigure 7  shows mean ratings of targets post correction by participant political party and target \npolitica l party.  \nAs in Study 2, the pre -correction ratings were subtracted from the post -correction ratings \nto measure rating change in warmth and competence. Analysis of variance by target political \nparty (Democrat, Republican or neutral) and participant politica l party did not reveal significant \nmain effects or interactions. Participant political party (Democrat, Republican or neutral) and \n \n   \n target affiliation (Democrat, Republican or neutral) did not significantly influence change in \nwarmth and competence ratings (all p > .05). While Figure 5 suggests that Republican \nparticipants failed to update their competence and warmth ratings for Democrats, we cannot \nconclude that there was an effect. As in Study 2, low power likely impacted the ability to find \nsignificant differences between groups. Because there were so few Republican respondents, \nseveral experimental groups had fewer than 10 participants.   \nCorrelational analysis indicated differences by target political party. When the target was \na Democrat, t here was a negative correlation between PBS conservatism and competence rating \n(r(65) = -.24, p = .048), and a positive correlation between PBS liberalism and warmth rating \n(r(65) = .30, p = .013). There was also a negative correlation between SDO and comp etence \nrating for the Democrat target ( r(65) = -.27, p = .029). For a Republican target, PBS \nconservatism was positively correlated with both competence rating ( r(63) = .36, p = .003) and \nwarmth rating ( r(63) = .33, p = .007). Additionally, there were posi tive correlations between self -\nidentified political leaning and competence rating ( r(63) = .44, p < .000) and warmth rating \n(r(63) = .32, p = .009).  For the neutral target, there was only a positive correlation between self -\nidentified political leaning and  warmth rating ( r(68) = .24, p = .043). Intellectual  Humility \nsubscales were differentially correlated with rating change depending on the target\u2019s political \naffiliation as well (see Table 5  for more detailed results).  \nDiscussion  \nResults partially supported hypotheses 1 and 3. Part icipants rated targets differently on \ncompetence based on the target\u2019s political affiliation, and this effect was driven mainly by much \nlower ratings for the Republican target when compared to Democrat and neutral targets. This \nwas unsurprising as there we re many more Democrat or no party participants than Republican. \n \n   \n Indeed, Republican participants rated the Republican target much higher on competence and \nwarmth, but rated Democrat and neutral targets similarl y low  on both.  \nAs depicted in Figure 7, Democr at and no party participants rated targets similarly, while \nRepublican ratings for the Republican target were significantly different. Similar to Study 2, \nRepublican participants did appear to update their beliefs less than Democrat or no party ( see \nFigure s 4, 5 and 6), but the results were not  statistically  significant, thus hypotheses 2 and 4 were \nnot fully supported , but the non -significant pattern from Study 2 was replicated .  \nCorrelations between PBS scores and competence and warmth  varied considerably by \ntarget political party  and provided some support for all hypotheses . When the target was a \nliberal , attitudes predicted more positive final evaluations of warmth , while conservative  \nattitude s were linked to negative final evaluat ions of the target\u2019s competence. When the target \nwas a Republican, liberalism did not predict final scores for competence or warmth, however , \nconservatism predicted more positive final ratings o n both dimensions . Correlations between the \nIntellectual Humility Scal e and target ratings of competence and warmth were  also differentially \nsignificant based on the target\u2019s political party. Several subscales were significantly associated \nwith competence for a Democrat target, but other subscales were significantly associat ed with \nwarmth for a Republican target. While Intellectual Humility  Scale  results  were exploratory in \nnature  not directly related to a hypothesis, they may be an interesting direction for future \nresearch.  \nLimitations  \nAs in Study 1 and  Study  2, there were  few participants who expressed conservative \nbeliefs  or identified with the Republican party , resulting in low power . While we hoped  to recruit \nmore Republican participants by using a public recruitment platform, the sample was not \n \n   \n sufficiently politic ally diverse, and we were unable to recruit enough participants overall due to \ncost. Follow -up research will focus on greater numbers of participants for each condition, or \nchange recruitment strategies to target specific political groups.  \nGeneral Discussion  \nOverall, participants did update their beliefs. The magnitude varied by participant and \ntarget political party  and mean target ratings were often vastly different , particularly between \nDemocrat and Republican participants, although low power lim its the interpretation of the \npatterns. Results suggested that Republican participants updated their ratings the le ast for targets \nthat were Democrat or neutral. In Study 1, Republicans were particularly critical and updated \ntheir beliefs less when the target was a Democrat, however, data collection was interrupted \nbefore comparison conditions with Republican or neutral targets could be implemented. We \nsought to correct this issue with Study 2 and Study 3.  We also hoped to enroll a greater number \nof Repu blican or conservative participants.  Study 1 took place in -person and in a lab free of \ndistractions. Participants were directed to use the computer in front of them to access an actual \nfake news article and related correction posted online. Because of the shortened Spring 2020 \nsemester, a similar procedure was not possible. Therefore, in Study 2 and Study 3, we changed \nthe procedure to embed a fake news article and correction designed for the purpose of the study \nso that it could be similar across all condi tions, and to not rely on articles posted online. While \nthis decreased external validity, it increased internal validity.  \nIn Study 2 and Study 3, the images used for the target individuals in both the fake news \narticles and corrections may not have been si milar enough, which could have possibly impacted \nresults (see Appendix A, B, and C). We felt that it was important to use real images of the targets \nin order to have par ticipants believe that they were reading articles transcribed from elsewhere. \n \n   \n The images were selected because they were somewhat unflattering or appeared to show the \ntargets in a negative or critical light (e.g. surrounded by reporters, expressing anger o r \nembarrassment) , and portraying subjects unfavorably  in this manner  is common in fake news . \nHowever, there are differences between the images that could be confounding factors. For \nexample, the image of Adam Schiff shows another well -known politician in t he background, \nwhile the others do not.  \nStudy 2 revealed  a negative correlation between conservatism scores on the PBS and \nrating change for competence and warmth. More conservative participants updated their beliefs \nless than others, and particularly so for Democrat  and neutral targets. Participants who scored  \nhigher in liberalism or intellectual humility updated their beliefs more upon correction. In Study \n3, significant differences were found between participant political party: Republican participants \nrated Democrat  and neutral targets much lower than Republ ican targets. While this is not too \nsurprising, Figure 7 shows that Democrat and no party participants responded similarly to all \ntargets , while Republicans responded quite differently. In addition, the exploratory use of the \nIntellectual Humility Scale in dicated that it may prove to be a useful measure in future research \non partisan schemas.  \nResults may suggest that Republican or conservative participants are less willing to \nupdate their beliefs , and greater intellectual humility is more common among Democ rats or \nliberals which may aid in  receiving corrective information. However,  care must be taken not to \noverextend these results  as if they are monolith ic. Indeed, some research suggests that self -\nidentified conservatives may be more diverse in their politi cal beliefs than are liberals (Feldman \n& Johnston, 2014; Klein & Stern, 2005; Stenner, 2009) , and people on the left and right ar e \n \n   \n equally likely to employ schemas in judgments , relying more on situational cues to engage in \nacceptable behavior (Crawford, 2012) .  \nThe value of diversity  of gender, race, ethnicity , sexual orientation  and more is well -\nestablished , particularly in academia where differing viewpoints and perspectives are vital to \ncritical analysis . However, t he low turn -out of Republican and conservative participants across \nall studies  may represent a different sort of diversity  problem in higher education and  in the field \nof psychology . Ferguson et al.  (2018)  found evidence that introductory textbooks in psychology \ncontained error and liberal -leaning biases that reflected  socio -political homogene ity of the field, \nand argued that the problem  may be exacerbated by APA position statements written by \nideologically invested scholars . When conservative undergraduates enter psychology classes, \nthey may feel unwelcome  or unwilling to contribute (Duarte et al., 2015) . In the psychology  \nmajor,  84% of college professors  identify as liberal, and only 8% identify as conservative  \n(Rothman et al., 2005) , while in the United States as a whole, the ratio of liberals to \nconservatives is roughly 1 t o 2 (Saad, 2010) . Even if one does not agree with conservative  views, \nthe absence of conservative viewpoints in academia, and indeed some hostility towards them , \nmakes it difficult to examine political beliefs in general or to even discuss them for a better \nunderstanding.  \nOverall, t here was  certainly  a diversity pro blem  both in the university subject pool  and on \nProlific  that hinders the ability of the study to be extended to the general population: participants \noverwhelmingly identified as Caucasian/white, liberal and female . The lack of  diversity in other \nways, inc luding  Republican or conservative -leaning participants , was an overall limitation of the \nstudy . For this type of research, the issue could be corrected in the future by specifically recruiting \nRepublican participants. Of course, recruiting a larger sample might bring more demographic \n \n   \n diversity, which can bring viewpoint diversity, but because it was viewpoint diversity that was \nwanted, it may be more effective to pursue it directly.  \nImplications  \nMany Americans currently believe fake news and misinformation  to be a serious issue, \nmore so than other issues such as climate change or racial tension (Mitchell et al., 2019) . This \nfear seems to be healthy, as fake news has caused a multitude of issues by exploiting how we \nprocess information, and the systems we use to do so. Falsehoods spread more readily than truth \nonline (Vosoughi et al., 2018) , people share misinformation unknowingly while acknowledging \nthat it confuses dialogue and agreement on basic facts (Barthel et al., 2016) , and simply \nrepeatedly encountering fake news headlines makes it seem less u nethical to share (Effron & \nRaj, 2020) . More importantly, encountering the same information repeatedly increases the \nlikelihood that an individual will later consider it true, even if they were originally made aware \nthat it is false (DiFonzo et al., 2016) .  \n For example, 10% of Americans still believe t hat the risks of vaccines outweigh the \nbenefits (Hefferon & Funk, 2020) , despite over a decade of efforts to debunk misinformation that \nindicated a link between vaccines and autism (Cent ers for Disease Control and Prevention, \n2020) . The prevalence of this belief , despite overwhelming evidence to the contrary , hints at a \nsevere problem . Vaccine  skepticism  may be  enough to cause outbreaks of diseases that would \notherwise be close to erad ication (Hussain et al., 2018) , an issue of particular relevance given the \ncurrent pandemic . An unpr ecedented amount of harm  to the credibility of  all vaccines  was \naccomplished primarily by a single misleading study , one  that most adherents to anti -vaccination \nbeliefs likely have not read or fully understood (Hussain et a l., 2018) .  \n \n   \n A growing body of evidence suggests that misinformation has become more frequent  and \nmore effective among specific audiences  largely due to the rise of social media  (Del Vicario et \nal., 2016; Guess et al., 2018; Mitchell et al., 2019; Murphy et al., 2019) . For example, Luisi \n(2021)  found that four out of every ten posts on Facebook about the vaccine for human \npapilloma virus (HPV) were negative, and focused on the supposed risks and threats of the \nvaccine, despite there being little evidence to support these claims . The author sugges ts that \nmisinformation that is negative or amplifies perceived risks spreads more easily online, which \nmay help explain the prevalence of it. M any people now receive a majority of their news and \ninformation through social media platforms and social network s (Hermida et al., 2012; Shearer, \n2018) . Despite low expectations regarding accuracy, most Americans obtain  at least some of  \ntheir news from social media sources (Shearer & Mitchell, 2021) . In addition, while  Bergstr \u00f6m \nand Belfrage (2018)  found that many people consider themselves well -informed  consumers of \nnews, t heir behavior indicates a more casual, \u201cincidental\u201d approach to news and information \nconsumption . Pennycook et al. (2020)  found  that many social media users did not consider the \naccuracy of information before sharing it, but would do so if prompted; when asked to rate \ninformation accuracy before sharing it, participants were more likely to correctly discern whether \nthe information was  true or false.   \nWhen i nformation is shared via social media , those affiliated with groups often receive \nclusters of messages and belief -supporting material , both  verifiable information (e.g. scientific) \nand unverifiable (e.g. conspiracy) (Del Vicario et al.,  2016) . In this way, groups of individuals \nmay be more  likely to continue believing in schema -supporting misinformation , which can result \nin homogeneity of information or \u201cecho chambers\u201d that decrease the likelihood of encountering \nopposing views or evidence, and amplify the perceived trustworthiness of misinformation \n \n   \n (Allcott & Gentzkow, 2017; Del Vicario et al., 2016) . These factors have  made misinformation \nvery difficult to correct;  prior beliefs and schemas may make fact-checking unlikely while  \nreduc ing the effectivene ss of corrections.  \nWe argue here that the strength of misinformation and weakness of corrections or fact -\nchecking may be traced to the influence of schemas, as suggested by Johnson and Seifert\u2019s \n(1994)  original  work on the  continued influence effect and research on the bipolar schemas \nrelated to political orientation . Guess et al. (2018)  found that, among the over 2500 Americans \nwho granted the researchers access to their individual web history, 1 in 4 visited fake news \nwebsites during the 2016 election cycle. Those who supported Donald Trump for U.S. pres ident \nwere particularly likely to visit fake news websites; 58.9% of all visits to fake news websites \nwere by 10% of those with the most conservative news diets.  Similar to anti -vaccination beliefs, \nnegative beliefs about other subjects that have widesprea d acceptance within the scientific \ncommunity such as  anthropogenic  climate change , are frequently associated with conservative \npolitical schemas  (Pew Research Center, 2015) . In even more recent events, conservatism \npredicted reduced concerns about the threat  posed by COVID -19, belief in media exaggeration  of \nthe threat , and inability to discern real and fake news related to COVID -19 (Calvillo et al., 2020) . \nThe authors also found that knowledge about and approval of U.S. president Donald Trump \nmediated these eff ects, suggesting that authority figures that an individual trusts could be helpful \nin communicating corrective information and reducing the impact of misinformation.  \nFake news is often negative or critical  and designed to cause s trong emotions , which have  \nbeen linked to  reliance on schemas  in judgments . For example, anger may make individuals rely \non the perceived  credibility of the source rather than the quality of the message (Bodenhausen et \nal., 1 994). In a recent experiment by Greenstein and Franklin (2020), participants were more \n \n   \n vulnerable to schema -consistent misinformation  when they were angry , but anger was not related \nto memory errors for true information . Angry p articipants also made  decisions more quickly and \nwith increased confidence. Innocuous or mood -neutral misinformation may be less effective \noverall, but it is also possible that anger -inducing misinformation would be more  effective in \nprompting reliance on partisan schemas , and  we suggest examining this as a potential direction \nfor future research.  \nConclusion  \nIn another study completed at Bridgewater State University  (Spievak et al., 2020) , \nresearchers asked  participants to rate the  quality and partisan bias of real news sources. \nParticipants indicated that news sources congruent with their  political beliefs were less biased  \nand higher quality,  and incongruent sources  were more biased  and lower quality.  Another trend \nemerged , as well: overall, participants considered very few source s to be \u201cnews,\u201d instead rating \nthem at best, \u201cfair interpretations of the news.\u201d Those who identified as Democrats were more \nlikely to rate sources as high quality news, (e.g. NPR and The New York Times, both considered \nby media bias experts to be reliable  news sources ; see: www.mediabiasfactcheck.com ), and they \nrated  Fox News as extremely low quality. By contrast, Republicans indicated that almost all \nnews sources were low quality and \u201cunfair interpretations of the news,\u201d including Fox News , \nwhich is typically considered to be largely in agreement with Republican and conservative \nschemas . This trend poses a problem: if those with right -leaning partisan schemas  do not \nconsider any sources to be news , or even  fair interpretations of it , even when it is in agreement \nwith their opinions, then perhaps they do not consider any source to be trustworthy. If that is the \ncase, then they may regard the truth as a matter of opinion instead. Those who seek o ut \n \n   \n information that is schema -consistent  and have little interest in  the quality of reporting, analysis, \nor evidence , would be similarly uninterested in correction .  \nCIM may be more  serious than a quirk of human cognition. As attitudes, values and \nbeliefs with regard to politics become ever more polarized in the U.S. (Wilson et al., 2020) , \nmade worse by social media (Bail et al., 2018) , and these beliefs may have a substantial impact \non how misinfo rmation is received and corrected, the re is potential for a further breakdown of \nreasonable d iscourse.  \n  \n \n   \n References  \nAlfano, M., Iurino, K., Stey, P., Robinson, B., Christen, M., Yu, F., & Lapsley, D. (2017). \nDevelopment and validation of a multi -dimensional measure of intellectual humility. \nPLOS  ONE , 12(8), 1 \u201328. https://doi.org/10.1371/journal.pone.0182950  \nAlfano, M., Iurino, K., Stey, P., Robinson, B., Christen, M., Yu, F., & Lapsley, D. (2018). \nIntellectual humility scale . American Psychological Association. \nhttps://doi.org/10.1037/t67360 -000 \nAllcott, H., & Gentzkow, M. (2017). Social media and fake news in the 2016 election. Journal of \nEconomic Perspectives , 31(2), 211 \u2013236. https://doi.org/10.1257/jep.31.2.211  \nAnderson, C. A., Lepper, M. R., & Ross, L. (1980). Perseverance of social theories: The role of \nexplanation in the persistence of discredited information. Journal of Personality and \nSocial Psychology , 39(6), 1037 \u20131049. https://doi.org/10.1037/h0077720  \nAxt, J. R., Landau, M. J., & Kay, A. C. (2020). The psychological appeal of fake -news \nattributions. Psychological Science , 31(1), 848 \u2013857. \nhttps://doi.org/10.1177/0956797620922785  \nAyers, M. S., & Reder, L. M. (1998). A theoretical review of the misinformation effect: \nPredictions from an activation -based memory model. Psychonomic Bulletin & Re view, \n5(1), 1 \u201321. https://doi.org/10.3758/BF03209454  \nBaddeley, A. D. (1966). The influence of acoustic and semantic similarity on long -term memory \nfor word sequences. Quarterly Journal of Experimental Psychology , 18(4), 302 \u2013309. \nBail, C. A., Argyle, L. P.,  Brown, T. W., Bumpus, J. P., Chen, H., Hunzaker, M. B. F., Lee, J., \nMann, M., Merhout, F., & Volfovsky, A. (2018). Exposure to opposing views on social \n \n   \n media can increase political polarization. Proceedings of the National Academy of \nSciences , 115(37), 92 16\u20139221. https://doi.org/10.1073/pnas.1804840115  \nBaldwin, M. W. (1992). Relational schemas and the processing of social information. \nPsychological Bulletin , 112(3), 461 \u2013484. \nBargh, J. A. (1982). Attention and automaticity in the processing of self -relevant  information. \nJournal of Personality and Social Psychology , 43(3), 425 \u2013436. \nBarthel, M., Mitchell, A., & Holcomb, J. (2016). Many Americans believe fake news is sowing \nconfusion . Pew Research Center.  \nBartlett, F. C. (1932). Remembering: A study in experime ntal and social psychology . Cambridge \nUniversity Press.  \nBergman, E. T., & Roediger, H. L. (1999). Can Bartlett\u2019s repeated reproduction experiments be \nreplicated? Memory & Cognition , 27(6), 937 \u2013947. \nBergstr\u00f6m, A., & Belfrage, M. J. (2018). News in social me dia: Incidental consumption and the \nrole of opinion leaders. Digital Journalism , 6(5), 583 \u2013598. \nhttps://doi.org/10.1080/21670811.2018.1423625  \nBodenhausen, G. V., Sheppard, L. A., & Kramer, G. P. (1994). Negative affect and social \njudgement: The differentia l impact of anger and sadness. European Journal of Social \nPsychology , 24, 45\u201362. \nBodenhausen, G. V., & Wyer, R. S. (1985). Effects of stereotypes in decision making and \ninformation -processing strategies. Journal of Personality and Social Psychology , 48(2), \n267\u2013282. https://doi.org/10.1037/0022 -3514.48.2.267  \nBrewer, W. F., & Nakamura, G. F. (1984). The nature and functions of schemas. Center for the \nStudy of Reading Technical Report , 325. \n \n   \n Brewer, W. F., & Treyens, J. C. (1981). Role of schemata in memory for  places. Cognitive \nPsychology , 13(2), 207 \u2013230. https://doi.org/10.1016/0010 -0285(81)90008 -6 \nBronstein, M. V., Pennycook, G., Bear, A., Rand, D. G., & Cannon, T. D. (2019). Belief in fake \nnews is associated with delusionality, dogmatism, religious fundament alism, and reduced \nanalytic thinking. Journal of Applied Research in Memory and Cognition , 8(1), 108 \u2013117. \nhttps://doi.org/10.1016/j.jarmac.2018.09.005  \nCalvillo, D. P., Ross, B. J., Garcia, R. J. B., Smelter, T. J., & Rutchick, A. M. (2020). Political \nideol ogy predicts perceptions of the threat of COVID -19 (and susceptibility to fake news \nabout it). Social Psychological and Personality Science , 2\u201310. \nhttps://doi.org/10.1177/1948550620940539  \nCarraro, L., Castelli, L., & Macchiella, C. (2011). The automatic conservative: Ideology -based \nattentional asymmetries in the processing of valenced information. PLoS ONE , 6(11), \ne26456. https://doi.org/10.1371/journal.pone.0026456  \nCenters for Disease Cont rol and Prevention. (2020, August 25). Autism and vaccines . \nhttps://www.cdc.gov/vaccinesafety/concerns/autism.html  \nCrawford, J. T. (2012). The ideologically objectionable premise model: Predicting biased \npolitical judgments on the left and right. Journal of Experimental Social Psychology , \n48(1), 138 \u2013151. https://doi.org/10.1016/j.jesp.2011.10.004  \nDe keersmaecker, J., &  Roets, A. (2017). \u2018Fake news\u2019: Incorrect, but hard to correct. The role of \ncognitive ability on the impact of false information on social impressions. Intelligence , \n65, 107 \u2013110. https://doi.org/10.1016/j.intell.2017.10.005  \nDel Vicario, M., Bessi, A., Zoll o, F., Petroni, F., Scala, A., Caldarelli, G., Stanley, H. E., & \nQuattrociocchi, W. (2016). The spreading of misinformation online. Proceedings of the \n \n   \n National Academy of Sciences , 113(3), 554 \u2013559. \nhttps://doi.org/10.1073/pnas.1517441113  \nDiFonzo, N., Becks tead, J. W., Stupak, N., & Walders, K. (2016). Validity judgments of rumors \nheard multiple times: The shape of the truth effect. Social Influence , 11(1), 22 \u201339. \nhttps://doi.org/10.1080/15534510.2015.1137224  \nDodd, M. D., Balzer, A., Jacobs, C. M., Gruszczyn ski, M. W., Smith, K. B., & Hibbing, J. R. \n(2012). The political left rolls with the good and the political right confronts the bad: \nConnecting physiology and cognition to preferences. Philosophical Transactions of the \nRoyal Society B: Biological Sciences , 367(1589), 640 \u2013649. \nhttps://doi.org/10.1098/rstb.2011.0268  \nDodd, M. D., Hibbing, J. R., & Smith, K. B. (2016). The politics of attention. In Psychology of \nLearning and Motivation  (Vol. 65, pp. 277 \u2013309). Elsevier. \nhttps://doi.org/10.1016/bs.plm.2016.04.003  \nDuarte, J. L., Crawford, J. T., Stern, C., Haidt, J., Jussim, L., & Tetlock, P. E. (2015). Political \ndiversity will improve social psychological science. Behavioral and Brain Sciences , 38, \ne130. https://doi.org/10.1017/S0140525X14000430  \nDudai, Y. (2004). The neurobiology of consolidations, or, how stable is the engram? Annual \nReview of Psychology , 55(1), 51 \u201386. \nhttps://doi.org/10.1146/annurev.psych.55.090902.142050  \nEcker, U. K. H., Lewandowsky, S., Fenton, O., & Martin, K. (2014). Do people keep believing \nbecause they want to? Preexisting attitudes and the continued influence of \nmisinformation. Memory & Cognition , 42(2), 292 \u2013304. https://doi.org/10.3758/s13421 -\n013-0358 -x \n \n   \n Ecker, U. K. H., Lewandowsky, S., Swire, B., & Chang, D. (2011). Correcting false infor mation \nin memory: Manipulating the strength of misinformation encoding and its retraction. \nPsychonomic Bulletin & Review , 18(3), 570 \u2013578. https://doi.org/10.3758/s13423 -011-\n0065 -1 \nEffron, D. A., & Raj, M. (2020). Misinformation and morality: Encountering f ake-news \nheadlines makes them seem less unethical to publish and share. Psychological Science , \n31(1), 75 \u201387. https://doi.org/10.1177/0956797619887896  \nFazio, L. K., Brashier, N. M., Payne, B. K., & Marsh, E. J. (2015). Knowledge does not protect \nagainst ill usory truth. Journal of Experimental Psychology: General , 144(5), 993 \u20131002. \nhttps://doi.org/10.1037/xge0000098  \nFeldman, S., & Johnston, C. (2014). Understanding the determinants of political ideology: \nImplications of structural complexity: understanding po litical ideology. Political \nPsychology , 35(3), 337 \u2013358. https://doi.org/10.1111/pops.12055  \nFerguson, C. J., Brown, J. M., & Torres, A. V. (2018). Education or indoctrination? The \naccuracy of introductory psychology textbooks in covering controversial topic s and urban \nlegends about psychology. Current Psychology , 37(3), 574 \u2013582. \nhttps://doi.org/10.1007/s12144 -016-9539 -7 \nFiske, S. T., Cuddy, A. J. C., Glick, P., & Xu, J. (2002). A model of (often mixed) stereotype \ncontent: Competence and warmth respectively f ollow from perceived status and \ncompetition. Journal of Personality and Social Psychology , 82(6), 878 \u2013902. \nhttps://doi.org/10.1037/0022 -3514.82.6.878  \nFiske, S. T., & Neuberg, S. L. (1990). A continuum of impression formation, from category -\nbased to individ uating processes: Influences of information and motivation on attention \n \n   \n and interpretation. In M. P. Zanna (Ed.), Advances in Experimental Social Psychology  \n(Vol. 23, pp. 1 \u201374). Academic Press. https://doi.org/10.1016/S0065 -2601(08)60317 -2 \nGawronski, B. (2 012). Back to the future of dissonance theory: Cognitive consistency as a core \nmotive. Social Cognition , 30(6), 652 \u2013668. https://doi.org/10.1521/soco.2012.30.6.652  \nGoldstein, B. E. (2014). Cognitive psychology: Connecting mind, research and everyday \nexperi ence (4th ed.). Cengage Learning.  \nGreenstein, M., & Franklin, N. (2020). Anger increases susceptibility to misinformation. \nExperimental Psychology , 67(3), 202 \u2013209. https://doi.org/10.1027/1618 -3169/a000489  \nGuess, A., Nyhan, B., & Reifler, J. (2018). Select ive exposure to misinformation: Evidence from \nthe consumption of fake news during the 2016 U.S. presidential campaign. European \nResearch Council , 9(3), 49.  \nHemmer, P., & Steyvers, M. (2009). A Bayesian Account of Reconstructive Memory. Topics in \nCognitive Science , 1(1), 189 \u2013202. https://doi.org/10.1111/j.1756 -8765.2008.01010.x  \nHermida, A., Fletcher, F., Korell, D., & Logan, D. (2012). Share, like, recommend: Decoding the \nsocial media news consumer. Journalism Studies , 13(5\u20136), 815 \u2013824. \nhttps://doi.org/10.10 80/1461670X.2012.664430  \nHussain, A., Ali, S., Ahmed, M., & Hussain, S. (2018). The anti -vaccination movement: A \nregression in modern medicine. Cureus , 10(7). https://doi.org/10.7759/cureus.2919  \nJardina, A., & Traugott, M. (2019). The genesis of the birther rumor: Partisanship, racial \nattitudes, and political knowledge. The Journal of Race, Ethnicity, and Politics , 4(1), 60 \u2013\n80. https://doi.org/10.1017/rep.2018.25  \nJohnson, B. T., & Eagly, A. H. (19 89). Effects of involvement on persuasion: A meta -analysis. \nPsychological Bulletin , 106(2), 290 \u2013314. \n \n   \n Johnson, H. M., & Seifert, C. M. (1994). Sources of the continued influence effect: When \nmisinformation in memory affects later inferences. Journal of Expe rimental Psychology: \nLearning, Memory, and Cognition , 20(6), 1420 \u20131436.  \nJohnson, H. M., & Seifert, C. M. (1998). Updating accounts following a correction of \nmisinformation. Journal of Experimental Psychology: Learning, Memory, and Cognition , \n24(6), 1483 \u20131494. https://doi.org/10.1037/0278 -7393.24.6.1483  \nJost, J. T., Glaser, J., Kruglanski, A. W., & Sulloway, F. J. (2003). Political conservatism as \nmotivated social cognition. Psychological Bulletin , 129(3), 339 \u2013375. \nhttps://doi.org/10.1037/0033 -2909.129.3.339  \nJudd, C. M., & Kulik, J. A. (1980). Schematic e ffects of social attitudes on information \nprocessing and recall. Journal of Personality and Social Psychology , 38(4), 569 \u2013578. \nKhanna, K., & Sood, G. (2018). Motivated responding in studies of factual learning. Political \nBehavior , 40(1), 79 \u2013101. https://do i.org/10.1007/s11109 -017-9395 -7 \nKleider, H. M., Pezdek, K., Goldinger, S. D., & Kirk, A. (2008). Schema -driven source \nmisattribution errors: Remembering the expected from a witnessed event. Applied \nCognitive Psychology , 22(1), 1 \u201320. https://doi.org/10.1002 /acp.1361  \nKlein, D. B., & Stern, C. (2005). Professors and their politics: The policy views of social \nscientists. Critical Review , 17(3\u20134), 257 \u2013303. \nhttps://doi.org/10.1080/08913810508443640  \nLuisi, M. L. R. (2021). From bad to worse II: Risk amplification of the HPV vaccine on \nFacebook. Vaccine , 39(2), 303 \u2013308. https://doi.org/10.1016/j.vaccine.2020.11.065  \n \n   \n Markus, H., & Zajonc, R. B. (1985). The cognitive perspective in social psychology. In G. \nLindzey & E. Aronson (Eds.), Handbook of Social Psychology  (3rd ed., pp. 137 \u2013229). \nRandom House.  \nMcFarland, C., Cheam, A., & Buehler, R. (2007). The perseverance effect in the debriefing \nparadigm: Replication and extension. Journal of Experimental Social Psychology , 43(2), \n233\u2013240. https://doi.org/10.1016/j.jesp.2006. 01.010  \nMikkelson, D. (2019, July 22). Is U.S. Rep. Rashida Tlaib under investigation for election fraud?  \nSnopes. https://www.snopes.com/fact -check/rashida -tlaib-investigation/  \nMitchell, A., Gottfried, J., Barthel, M., & Sumida, N. (2018). Distinguishing be tween factual and \nopinion statements in the news . Pew Research Center.  \nMitchell, A., Gottfried, J., Stocking, G., Walker, M., & Fedeli, S. (2019, June 5). Many \nAmericans say made -up news is a critical problem that needs to be fixed . Pew Research \nCenter. ht tps://www.journalism.org/2019/06/05/many -americans -say-made -up-news -is-a-\ncritical -problem -that-needs -to-be-fixed/  \nMitchell, A., Oliphant, B. J., & Klein, Hannah. (2020). Three months in, many Americans see \nexaggeration, conspiracy theories and partisanship  in COVID -19 news . Pew Research \nCenter. https://www.journalism.org/2020/06/29/three -months -in-many -americans -see-\nexaggeration -conspiracy -theories -and-partisanship -in-covid -19-news/  \nMurphy, G., Loftus, E. F., Grady, R. H., Levine, L. J., & Greene, C. M. (20 19). False memories \nfor fake news during ireland\u2019s abortion referendum. Psychological Science , 30(10), \n1449 \u20131459. https://doi.org/10.1177/0956797619864887  \n \n   \n Nyhan, B., & Reifler, J. (2010). When corrections fail: The persistence of political \nmisperceptions. Political Behavior , 32(2), 303 \u2013330. https://doi.org/10.1007/s11109 -010-\n9112 -2 \nPennycook, G., Cannon, T. D., & Rand, D. G. (2018). Prior exposure increases perceived \naccuracy of fake news. Journal of Experimental Psychology: General , 147(12), 1865 \u2013\n1880. htt p://dx.doi.org/10.1037/xge0000465  \nPennycook, G., McPhetres, J., Zhang, Y., Lu, J. G., & Rand, D. G. (2020). Fighting COVID -19 \nmisinformation on social media: Experimental evidence for a scalable accuracy -nudge \nintervention. Psychological Science , 31(7), 77 0\u2013780. \nPennycook, G., & Rand, D. G. (2019). Lazy, not biased: Susceptibility to partisan fake news is \nbetter explained by lack of reasoning than by motivated reasoning. Cognition , 188, 39\u2013\n50. https://doi.org/10.1016/j.cognition.2018.06.011  \nPew Research Cen ter. (2015, July 1). Politics and science: What americans think . Pew Research \nCenter. https://www.pewresearch.org/science/2015/07/01/americans -politics -and-science -\nissues/  \nPorter, T., & Schumann, K. (2018). Intellectual humility and openness to the opposin g view. Self \nand Identity , 17(2), 139 \u2013162. https://doi.org/10.1080/15298868.2017.1361861  \nPratto, F., Sidanius, J., Stallworth, L. M., & Malle, B. F. (1994). Social dominance orientation: A \npersonality variable predicting social and political attitudes. Journal of Personality and \nSocial Psychology , 67(4), 741 \u2013763. https://doi.org/10.1037/0022 -3514.67.4.741  \nPratto, F., Sidanius, J., Stallworth, L. M., & Malle, B. F. (2011). Social dominance orientation \nscale . American Psychological Association. https://doi.org/10.1037/t01146 -000 \n \n   \n Rapp, D. N., & Kendeou, P. (2007). Revising what readers know: Updating text representations \nduring narrative comprehension. Memory & Cognition , 35(8), 2019 \u20132032. \nhttps://doi.org/10.3758/BF03192934  \nRoediger, H. L., & DeSo to, K. A. (2015). Reconstructive Memory, Psychology of. In \nInternational Encyclopedia of the Social & Behavioral Sciences  (pp. 50 \u201355). Elsevier. \nhttps://doi.org/10.1016/B978 -0-08-097086 -8.51016 -2 \nRoss, L., Lepper, M. R., & Hubbard, M. (1975). Perseverance in self -perception and social \nperception: Biased attributional processes in the debriefing paradigm. Journal of \nPersonality and Social Psychology , 32(5), 880 \u2013892. https://doi.org/10.1037/0022 -\n3514.32.5.880  \nRothman, S., Lichter, S. R., & Nevitte, N. (2005).  Politics and professional advancement among \ncollege faculty. The Forum , 3(1). https://doi.org/10.2202/1540 -8884.1067  \nSaad, L. (2010, June 25). In 2010, conservatives still outnumber moderates, liberals . Gallup. \nhttps://news.gallup.com/poll/141032/2010 -Conservatives -Outnumber -Moderates -\nLiberals.aspx  \nSchul, Y., & Mazursky, D. (1990). Conditions facilitating successful discounting in consumer \ndecision making. The Journal of Consumer Research , 16, 442 \u2013451. \nSeifert, C. M. (2002). The continued influence of misi nformation in memory: What makes a \ncorrection effective? In Psychology of Learning and Motivation  (Vol. 41, pp. 265 \u2013292). \nElsevier. https://doi.org/10.1016/S0079 -7421(02)80009 -3 \nShearer, E. (2018, December 10). Social media outpaces print newspapers in the  U.S. as news \nsource . Pew Research Center. https://www.pewresearch.org/fact -tank/2018/12/10/social -\nmedia -outpaces -print -newspapers -in-the-u-s-as-a-news -source/  \n \n   \n Shearer, E., & Mitchell, A. (2021). News use across social media platforms in 2020 . Pew \nResearch Center. https://www.journalism.org/2021/01/12/news -use-across -social -media -\nplatforms -in-2020/  \nShook, N. J., & Fazio, R. H. (2009). Political ideology, exploration of novel stimuli, and attitude \nformation. Journal of Experimental Social Psychology , 45(4), 995 \u2013998. \nhttps://doi.org/10.1016/j.jesp.2009.04.003  \nSpievak, E., Hayes -Bohanan, P., & Hermanson, J. (2020, August 6). Read this, not that: How \nstudents rate media sources in the age of fake news  [Presentation]. New England Library \nAssociation 2020 Annual Conference, Online.  \nStenner, K. (2009). Three Kinds of \u201cConservatism.\u201d Psychological Inquiry , 20(2\u20133), 142 \u2013159. \nhttps://doi.org/10.1080/10478400903028615  \nTaber, C. S., & Lodge, M. (2006). Motivated skepticism in the evaluation of political beliefs. \nAmerican Journal of Political Science , 50(3), 755 \u2013769. https://doi.org/10.1111/j.1540 -\n5907.2006.00214.x  \nTuckey, M. R., & Brewer, N. (2003). The influence of schemas, stimulus ambiguity, and \ninterview schedule on eyewitness memory over time. Journal of Expe rimental \nPsychology: Applied , 9(2), 101 \u2013118. https://doi.org/10.1037/1076 -898X.9.2.101  \nvan Prooijen, J. -W., & Krouwel, A. P. M. (2017). Extreme political beliefs predict dogmatic \nintolerance. Social Psychological and Personality Science , 8(3), 292 \u2013300. \nhttps://doi.org/10.1177/1948550616671403  \nVosoughi, S., Roy, D., & Aral, S. (2018). The spread of true and false news online. Science , \n359(6380), 1146 \u20131151. https://doi.org/10.1126/science.aap9559  \n \n   \n Walter, N., Cohen, J., Holbert, R. L., & Morag, Y. (2020). Fact -checking: A meta -analysis of \nwhat works and for whom. Political Communication , 37(3), 350 \u2013375. \nhttps://doi.org/10.1080/10584609.2019.1668894  \nWebber, D., Babush, M., Schori -Eyal, N., Vazeou -Nieuwenhuis, A., Hettiarachchi, M., \nB\u00e9langer, J. J., Moyano, M., T rujillo, H. M., Gunaratna, R., Kruglanski, A. W., & \nGelfand, M. J. (2018a). The road to extremism: Field and experimental evidence that \nsignificance loss -induced need for closure fosters radicalization. Journal of Personality \nand Social Psychology , 114(2), 270\u2013285. https://doi.org/10.1037/pspi0000111  \nWebber, D., Babush, M., Schori -Eyal, N., Vazeou -Nieuwenhuis, A., Hettiarachchi, M., \nB\u00e9langer, J. J., Moyano, M., Trujillo, H. M., Gunaratna, R., Kruglanski, A. W., & \nGelfand, M. J. (2018b). Political belief sca le. American Psychological Association. \nhttps://doi.org/10.1037/t65993 -000 \nWilson, A. E., Parker, V., & Feinberg, M. (2020). Polarization in the contemporary political and \nmedia landscape. Current Opinion in Behavioral Sciences , 34, 223 \u2013228. \nhttps://doi.or g/10.1016/j.cobeha.2020.07.005  \nZhu, B., Chen, C., Loftus, E. F., Lin, C., He, Q., Chen, C., Li, H., Moyzis, R. K., Lessard, J., & \nDong, Q. (2010). Individual differences in false memory from misinformation: \nPersonality characteristics and their interaction s with cognitive abilities. Personality and \nIndividual Differences , 48(8), 889 \u2013894. https://doi.org/10.1016/j.paid.2010.02.016  \n \n \n \n  \n \n   \n Table 1 \nReliability of Measures for All Studies  \nMeasures  Study 1  Study 2  Study 3  \nSocial Dominance Orientation (SDO)  .88 .88 .94 \nPolitical Belief Scale (PBS)     \nLiberal  .67 .62 .71 \nModerate  .60 .26 .19 \nConservative  .68 .65 .86 \n15 aggregate ratings in Study 1  .94   \nStereotype Content Model (SCM)     \nCompetence   .82 .89 \nWarmth   .86 .93 \nIntellectual Humility (IH)  .85 .73 \nOpen -Mindedness   .72 .82 \nModesty   .69 .79 \nCorrigibility   .78 .77 \nEngagement   .67 .78 \nNote. Values that are not in the table (e.g. Study 1 Competence) are due to not being used in that \nparticular study , or a combined reliability would be inappropriate (e.g. PBS).  \n \n  \n \n   \n Table 2 \nStudy 1 Correlations of Political Belief s and Evaluations  of Target  \nMeasures  Pre-correction  Post-correction  Rating  change  \nLiberalism        .299**        .481**  .151 \nModerate  -.085     .181*      .341**  \nConservatism  -.120     -.360**                 -.222 \nSocial Dominance Orientation  -.114 -.020  .053 \nSelf-identified p olitical leaning    -.168*  -.423                -.218 \n*p < .05. **p < .0 1. \n \n \n   \n Table 3 \nStudy 2 Correlations Between Intellectual Humility Subscales and Rating Change  \nMeasures  Competence  Warmth  \nAll targets  \nIH Total    .167*    .159*  \nIH Open -Mindedness    .157*    .169*  \nIH Modesty  .130 .137 \nIH Corrigibility  .045 .063 \nIH Engagement   .172*  .116 \nDemocrat target  \nIH Total  .217 .171 \nIH Open -Mindedness  .088 .087 \nIH Modesty  .168 .098 \nIH Corrigibility  .175 .127 \nIH Engagement  .229 .202 \nRepublican target  \nIH Total            .146 .211 \nIH Open -Mindedness            .061 .119 \nIH Modesty            .328*      .362**  \nIH Corrigibility           -.026 .083 \nIH Engagement           -.006                                    -.014 \nNeutral target  \nIH Total  .158 .125 \nIH Open -Mindedness    .319*    .274*  \nIH Modesty           -.078 .045 \nIH Corrigibility  .017                                     .038 \nIH Engagement    .278*                                      .166 \n*p < .05. **p < .01.  \n \n  \n \n   \n Table 4 \nStudy 3 Means for Competence and Warmth Rating by Participant Political Party and Target \nPolitical Party  \nTargets  Pre competence  Pre warmth  Post competence  Post warmth  \nAll participants  \nAll targets  -.462 -1.303 .236 -.265 \nDemocrat participant  \nDemocrat  -.430 -1.110 .417 -.020 \nRepublican  -.970 -1.571 -.405 -.834 \nNeutral  -.563 -1.601 .503 -.107 \nRepublican participant  \nDemocrat  .050 -.694 .091 -.571 \nRepublican  .167 -.200 .463 .333 \nNeutral  .083 -1.214 .738 .184 \nNo party participant  \nDemocrat  .017 -1.043 .681 .226 \nRepublican  -.600 -1.622 -.202 -.594 \nNeutral  -.320 -1.186 .395 .338 \n \n  \n \n   \n Table 5 \nStudy 3 Correlations Between Intellectual Humility Subscales and Rating Change   \nCompetence  Warmth  \nAll targets  \nIH Total  .113 .157 \nIH Open -Mindedness    .188*    .203*  \nIH Modesty  .100   .201*  \nIH Corrigibility  .061 .090 \nIH Engagement  .010 .004 \nDemocrat target  \nIH Total    .331*  .156 \nIH Open -Mindedness  .165                                    -.019 \nIH Modesty      .386**  .239 \nIH Corrigibility    .265*  .187 \nIH Engagement  .187 .064 \nRepublican target  \nIH Total  -.173   -.254*  \nIH Open -Mindedness  -.155   -.286*  \nIH Modesty  -.043 -.055 \nIH Corrigibility  -.083 -.139 \nIH Engagement  -.242   -.295*  \nNeutral target  \nIH Total  -.079 -.056 \nIH Open -Mindedness  -.096 -.068 \nIH Modesty  -.059 -.141 \nIH Corrigibility  -.028   .000 \nIH Engagement  -.064   .043 \n*p < .05. **p < .01.  \n  \n \n   \n Figure 1  \nStudy 2 Competence and Warmth Ratings for Democrat, Republican or Neutral Targets by \nDemocrat Participants  \n  -2-1.5-1-0.500.511.52\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2Warmth\nCompetence\nOverall pre-post Democrat target pre-post\nRepublican target pre-post Neutral target pre-post\n \n   \n Figure 2  \nStudy 2 Competence and Warmth Ratings for Democrat, Republican or Neutral Targets by \nRepublican Participant s \n \n  -2-1.5-1-0.500.511.52\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2Warmth\nCompetence\nOverall pre-post Democrat target pre-post\nRepublican target pre-post Neutral target pre-post\n \n   \n Figure 3  \nStudy 2 Competence and Warmth Ratings for Democrat, Republican or Neutral Targets by N o \nParty  Participants  \n \n  -2-1.5-1-0.500.511.52\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2Warmth\nCompetence\nOverall pre-post Democrat target pre-post\nRepublican target pre-post Neutral target pre-post\n \n   \n Figure 4  \nStudy 3 Competence and Warmth Ratings for Democrat, Republican or Neutral Targets by \nDemocrat Participants  \n \n  -2-1.5-1-0.500.511.52\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2Warmth\nCompetence\nOverall pre-post Democrat target pre-post\nRepublican target pre-post Neutral target pre-post\n \n   \n Figure 5  \nStudy 3 Competence and Warmth Ratings for Democrat, Republican or Neutral Targets by \nRepublican Participants  \n \n  -2-1.5-1-0.500.511.52\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2Warmth\nCompetence\nOverall pre-post Democrat target pre-post\nRepublican target pre-post Neutral target pre-post\n \n   \n Figure 6  \nStudy 3 Competence and Warmth Ratings for Democrat, Republican or Neutral Targets by No \nParty Participants  \n -2-1.5-1-0.500.511.52\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2Warmth\nCompetence\nOverall pre-post Democrat target pre-post\nRepublican target pre-post Neutral target pre-post\n \n   \n Figure 7  \nStudy 3 Competence and Warmth Post Correction by Target Political Party  \n \n \n  -2-1.5-1-0.500.511.52\nDemocrat target Republican target Neutral targetCompetence Rating\nDemocrat participant Republican participant Neutral participant\n-2-1.5-1-0.500.511.52\nDemocrat target Republican target Neutral targetWarmth Rating\nDemocrat participant Republican participant Neutral participant\n \n   \n Appendix A  \nHow intelligent do you think this individual is?  \nHow efficient do you think this individual is?  \nHow skillful do you think this individual is?  \nHow friendly do you think this individual is?  \nHow responsible do you think this individual is?  \nHow competent do you think this individual is?  \nHow likable do you think this individual is?  \nHow trustworthy do you think this individual is?  \nHow professional do you think this individual is?  \nHow sincere do you think this individual is?  \nHow warm do you think this individual is?  \nHow positive do you think this individual is?  \nHow favorable do you think this individual is?  \nHow good -natured do you think this individual is?  \nHow well-intentioned do you think this individual is?  \n  \n \n   \n Appendix B  \nDemocrat House \nRepresentative Suspected \nof Embezzling $1.6 Million \nFrom Federal Government  \n \nFri, 21 Aug 2020  \n \n \nOfficials intend to pursue charges ' very soon'  \n  \n U.S. Treasury officials announ ced today that they plan to pursue charges against \nDemocrat House Representative Adam Schiff, 60, of California, for embezzling almost \n$1.6 million from taxpayers over a ten -year period beginning in 2010 and ending in 2020 \nthrough a wire fraud scheme. Schi ff may also be charged with money laundering, \naggravated identity theft, and filing false income tax returns during this period.  \n \nSchiff, a House Representative in California for the Democratic Party since 2001, \nallegedly opened a PayPal account using a fa ke name, transferred federal funds to that \nPayPal account, from that PayPal account to another PayPal account belonging to his \n\n \n   \n wife, and from his wife\u2019s PayPal account to one or more of his personal bank accounts. \nFurther, Schiff allegedly concealed the em bezzlement by falsifying financial entries on \nfiles he maintained for the federal government.  \n \nTreasury officials assert that Schiff failed to pay taxes on his wealth, embezzled at least \n$854,800 and had unreported income of $231,100 in 2015, $215,100 in 2 016, $83,600 \nin 2017, $125,000 in 2018, $152,000 in 2019, and $48,000 in 2020.  If proven true, his \nactions created a tax loss of approximately $240,648.   \n \nOfficials say they plan to charge Schiff with 48 counts of wire fraud, 10 counts of money \nlaundering , six counts of filing a false income tax return, and one count of aggravated \nidentity theft.  Wire fraud and money laundering are punishable by up to 20 years in \nprison.  Filing a false tax return is punishable by up to three years.  Aggravated identity \ntheft is punishable by a mandatory two years of prison that must follow any term \nimposed on the other counts.  Additionally, Schiff may be subject to restitution and/or \nforfeiture of money and substitute assets totaling $1,589,315.   \n \n\u201cWe take offenses like embezzlement, tax fraud and money laundering very seriously,\u201d \nan announcement published online by the U.S. Treasury read. \u201cSchiff stole more than a \nmillion dollars by abusing his position handling finances for the federal government. We \nwill continue to wo rk with our law enforcement partners to protect innocent taxpayers \nfrom being victimized by this type of fraud.\u201d  \n \n\u201cThe role of IRS Criminal Investigation becomes even more important in embezzlement \nand fraud cases due to the complex financial transactions that can take time to unravel,\u201d \nsaid a spokesperson for the IRS CI. \u201cAs we often see, federal tax laws are normally \nviolated in these types of cases and IRS CI is committed to ensuring that everyone pays \ntheir fair share.\u201d  \n \nRepublican House \nRepresentative Suspected \nof Embezzling $1.6 Million \nFrom Federal Government  \n  \n \n   \n Fri, 21 Aug 2020  \n \n \nOfficials intend to pursue charges ' very soon'   \n  \nU.S. Treasury officials announced today that they plan to pursue charges against \nRepublican House Represent ative Jim Jordan, 56, of Ohio, for embezzling almost $1.6 \nmillion from taxpayers over a ten -year period beginning in 2010 and ending in 2020 \nthrough a wire fraud scheme. Jordan may also be charged with money laundering, \naggravated identity theft, and filin g false income tax returns during this period.  \n \nJordan, a House Representative in Ohio for the Republican Party since 2007, allegedly \nopened a PayPal account using a fake name, transferred federal funds to that PayPal \naccount, from that PayPal account to a nother PayPal account belonging to his wife, and \nfrom his wife\u2019s PayPal account to one or more of his personal bank accounts. Further, \nJordan allegedly concealed the embezzlement by falsifying financial entries on files he \nmaintained for the federal govern ment.  \n \nTreasury officials assert that Jordan failed to pay taxes on his wealth, embezzled at \nleast $854,800 and had unreported income of $231,100 in 2015, $215,100 in 2016, \n$83,600 in 2017, $125,000 in 2018, $152,000 in 2019, and $48,000 in 2020.  If prove n \ntrue, his actions created a tax loss of approximately $240,648.   \n \nOfficials say they plan to charge Jordan with 48 counts of wire fraud, 10 counts of \nmoney laundering, six counts of filing a false income tax return, and one count of \naggravated identity t heft.  Wire fraud and money laundering are punishable by up to 20 \nyears in prison.  Filing a false tax return is punishable by up to three years.  Aggravated \nidentity theft is punishable by a mandatory two years of prison that must follow any term \nimposed on the other counts.  Additionally, Jordan may be subject to restitution and/or \n\n \n   \n forfeiture of money and substitute assets totaling $1,589,315.   \n \n\u201cWe take offenses like embezzlement, tax fraud and money laundering very seriously,\u201d \nan announcement published online by the U.S. Treasury read. \u201cJordan stole more than \na million dollars by abusing his position handling finances for the federal government. \nWe will continue to work with our law enforcement partners to protect innocent \ntaxpayers from being victimized  by this type of fraud.\u201d  \n \n\u201cThe role of IRS Criminal Investigation becomes even more important in embezzlement \nand fraud cases due to the complex financial transactions that can take time to unravel,\u201d \nsaid a spokesperson for the IRS CI. \u201cAs we often see, fe deral tax laws are normally \nviolated in these types of cases and IRS CI is committed to ensuring that everyone pays \ntheir fair share.\u201d  \n \nStarbucks CEO Suspected \nof Embezzling $1.6 Million \nFrom Company  \n \nFri, 21 Aug 2020  \n \n \nOfficials intend to pursue charges ' very soon'  \n  \n\n \n   \n FBI officials announced today that they plan to pursue charges against Starbucks CEO \nKevin Johnson, 59, of Washington, for embezzling almost $1.6 million from the \ncompany over a ten -year period beginning in 2010 and ending in 2020 through a wire \nfraud scheme. Johnson may also be charged with money laundering, aggravated \nidentity theft, and filing false income tax returns during this period.  \n \nJohnson, president and CEO of Starbucks Coffee Company since 2017 and chief \noperating office since 2015, allegedly opened a PayPal account using a fake name, \ntransferred company funds to that PayPal account, from that PayPal account to another \nPayPal account belonging to his wife, and from his wife\u2019s PayPal account to one or \nmore of his personal b ank accounts. Further, Johnson allegedly concealed the \nembezzlement by falsifying financial entries on files he maintained for the company.  \n \nFBI and IRS officials assert that Johnson failed to pay taxes on his wealth, embezzled \nat least $854,800 and had un reported income of $231,100 in 2015, $215,100 in 2016, \n$83,600 in 2017, $125,000 in 2018, $152,000 in 2019, and $48,000 in 2020.  If proven \ntrue, his actions created a tax loss of approximately $240,648.   \n \nOfficials say they plan to charge Johnson with 48 counts of wire fraud, 10 counts of \nmoney laundering, six counts of filing a false income tax return, and one count of \naggravated identity theft.  Wire fraud and money laundering are punishable by up to 20 \nyears in prison.  Filing a false tax return is puni shable by up to three years.  Aggravated \nidentity theft is punishable by a mandatory two years of prison that must follow any term \nimposed on the other counts.  Additionally, Johnson may be subject to restitution and/or \nforfeiture of money and substitute a ssets totaling $1,589,315.   \n \n\u201cWe take offenses like embezzlement, tax fraud and money laundering very seriously,\u201d \nan announcement published online by the FBI read. \u201cJohnson stole more than a million \ndollars by abusing his position handling finances for the  company he works for. We will \ncontinue to work with our law enforcement partners to protect innocent investors from \nbeing victimized by this type of fraud.\u201d  \n \n\u201cThe role of IRS Criminal Investigation becomes even more important in embezzlement \nand fraud cas es due to the complex financial transactions that can take time to unravel,\u201d \nsaid a spokesperson for the IRS CI. \u201cAs we often see, federal tax laws are normally \nviolated in these types of cases and IRS CI is committed to ensuring that everyone pays \ntheir f air share.\u201d  \n  \n \n   \n Appendix C  \nDid Democrat House Rep \nAdam Schiff embezzle \nmoney from the federal \ngovernment?  \n  \nWed, 26 Aug 2020  \n \n \nRecently, an article appeared on social media and has subsequently been shared many \ntimes, claiming that Democrat House Representative Adam Schiff engaged in \nembezzlement, wire fraud, and other crimes. This article, shared from \nworldnewsdailyreport.com, details the allegations against Schiff. The article included \nquotes from U.S. Treasury and IRS officials, which made it ap pear genuine at first \nglance.  \n \nHowever, the article was researched by fact -checkers and determined to be a complete \nfabrication. The details of that article were primarily copied from an article hosted on \nJustice.gov about a bookkeeper from Philadelphia wh o embezzled funds from the \ncompany he worked for. The original article is genuine and a factual reporting of events. \n\n \n   \n However, while some details, such as dollar amounts, remained untouched, most other \ndetails were changed. All references to the original de fendant were changed to Adam \nSchiff, and other details were also changed to relate to political institutions and more \nspecifically to Schiff. In other words, the article appears to be intentional misinformation \ntargeting a politician.  \n \nFor example, in the original and factual article, it was a U.S attorney who made the \nannouncement, and provided a later quote. In the altered version, the announcement \nand quote are attributed to the U.S. Treasury generally. Additionally, when the original \narticle was release d, the defendant had already been convicted, while the altered \nversion states that Schiff will be charged. Many other details were either changed or \nremoved entirely.  \n \nThe website that posted the article, worldnewsdailyreport.com, is a well -known purveyor \nof misinformation and hoaxes despite billing itself as \u201csatire.\u201d At the bottom of the \nwebsite is the following disclaimer: \"World News Daily Report assumes all responsibility \nfor the satirical nature of its articles and for the fictional nature of their co ntent. All \ncharacters appearing in the articles in this website \u2013 even those based on real people \u2013 \nare entirely fictional and any resemblance between them and any person, living, dead or \nundead, is purely a miracle.\u201d While some articles simply make outrag eous claims, such \nas \u201cDeath Row Inmate Eats An Entire Bible As Last Meal\u201d, others are more political in \nnature, as demonstrated by the Adam Schiff story.  \n \nNo such allegations have been made against Adam Schiff.  \n \nDid Republican House Rep \nJim Jordan embezzle  \nmoney from the federal \ngovernment?  \n \nWed, 26 Aug 2020  \n \n   \n  \n  \nRecently, an article appeared on social media and has subsequently been shared many \ntimes, claiming that Republican House Representative Jim Jordan engaged in \nembezzlement, wire fraud, and other cri mes. This article, shared from \nworldnewsdailyreport.com, details the allegations against Jordan. The article included \nquotes from U.S. Treasury and IRS officials, which made it appear genuine at first \nglance.  \n  \nHowever, the article was researched by fact -checkers and determined to be a complete \nfabrication. The details of that article were primarily copied from an article hosted on \nJustice.gov about a bookkeeper from Philadelphia who embezzled funds from the \ncompany he worked for. The original article is ge nuine and a factual reporting of events. \nHowever, while some details, such as dollar amounts, remained untouched, most other \ndetails were changed. All references to the original defendant were changed to Jim \nJordan, and other details were also changed to r elate to political institutions and more \nspecifically to Jordan. In other words, the article appears to be intentional \nmisinformation targeting a politician.  \n  \nFor example, in the original and factual article, it was a U.S attorney who made the \nannouncemen t, and provided a later quote. In the altered version, the announcement \nand quote are attributed to the U.S. Treasury generally. Additionally, when the original \narticle was released, the defendant had already been convicted, while the altered \nversion state s that Jordan will be charged. Many other details were either changed or \nremoved entirely.  \n  \nThe website that posted the article, worldnewsdailyreport.com, is a well -known purveyor \nof misinformation and hoaxes despite billing itself as \u201csatire.\u201d At the bot tom of the \nwebsite is the following disclaimer: \"World News Daily Report assumes all responsibility \n\n \n   \n for the satirical nature of its articles and for the fictional nature of their content. All \ncharacters appearing in the articles in this website \u2013 even thos e based on real people \u2013 \nare entirely fictional and any resemblance between them and any person, living, dead or \nundead, is purely a miracle.\u201d While some articles simply make outrageous claims, such \nas \u201cDeath Row Inmate Eats An Entire Bible As Last Meal\u201d, others are more political in \nnature, as demonstrated by the Jim Jordan story.  \n  \nNo such allegations have been made against Jim Jordan.   \n \nDid Starbucks CEO Kevin \nJohnson embezzle money \nfrom the company?  \n \nWed, 26 Aug 2020  \n \n \nRecently, an article appeared on social media and has subsequently been shared many \ntimes, claiming that Starbucks CEO and president Kevin Johnson engaged in \nembezzlement, wire fraud, and other crimes. This article, shared from \nworldnewsdailyreport.com, details the allegations  against Johnson. The article included \nquotes from FBI and IRS officials, which made it appear genuine at first glance.  \n \nHowever, the article was researched by fact -checkers and determined to be a complete \n\n \n   \n fabrication. The details of that article were prim arily copied from an article hosted on \nJustice.gov about a bookkeeper from Philadelphia who embezzled funds from the \ncompany he worked for. The original article is genuine and a factual reporting of events. \nHowever, while some details, such as dollar amoun ts, remained untouched, most other \ndetails were changed. All references to the original defendant were changed to Kevin \nJohnson, and other details were also changed to relate to Starbucks and more \nspecifically to Johnson. In other words, the article appear s to be intentional \nmisinformation targeting a corporate executive.  \n \nFor example, in the original and factual article, it was a U.S attorney who made the \nannouncement, and provided a later quote. In the altered version, the announcement \nand quote are attri buted to the FBI generally. Additionally, when the original article was \nreleased, the defendant had already been convicted, while the altered version states \nthat Johnson will be charged. Many other details were either changed or removed \nentirely.  \n \nThe webs ite that posted the article, worldnewsdailyreport.com, is a well -known purveyor \nof misinformation and hoaxes despite billing itself as \u201csatire.\u201d At the bottom of the \nwebsite is the following disclaimer: \"World News Daily Report assumes all responsibility \nfor the satirical nature of its articles and for the fictional nature of their content. All \ncharacters appearing in the articles in this website \u2013 even those based on real people \u2013 \nare entirely fictional and any resemblance between them and any person, livi ng, dead or \nundead, is purely a miracle.\u201d While some articles simply make outrageous claims, such \nas \u201cDeath Row Inmate Eats An Entire Bible As Last Meal\u201d, others are more political in \nnature.  \n \nNo such allegations have been made against Kevin Johnson.  \n  ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Elephants Never Forget: Partisan Schemas and the Continued Influence of Misinformation", "author": ["JV Hermanson"], "venue": "NA", "pub_year": "NA", "abstract": "In an age where information is plentiful and access to it is practically unlimited, the veracity  of information is frequently an afterthought. Previous research has demonstrated that"}, "filled": false, "gsrank": 448, "pub_url": "https://vc.bridgew.edu/honors_proj/478/", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:O-aIVBkiN2wJ:scholar.google.com/&output=cite&scirp=447&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D440%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=O-aIVBkiN2wJ&ei=VbWsaOTRJ7XCieoP4PfQ0A8&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:O-aIVBkiN2wJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://vc.bridgew.edu/cgi/viewcontent.cgi?article=1479&context=honors_proj"}}, {"title": "PAUSE for Your Health: A New End-User Tool for Evaluating Consumer Health News and Online Resources", "year": "2023", "pdf_data": "PAUSE FOR YOUR HEALTH         1 \n  \n  \nPAUSE for Your Health: A New End -User Tool for Evaluating Consumer Health News \nand Online Resources  \nChristina J. Steffy\n1 and Bradley A. Long2 \n1Alvernia University \n2Penn State College of Medicine  \n \nAuthor Note  \nChristina J. Steffy  \nhttps://orcid.org/0000-0003-2715-6657  \nhttps://www.linkedin.com/in/cjsteffy/  \nTwitter: @ChristinaSteffy  \nBradley A. Long  \nhttps://orcid.org/0000-0002-5363-2891  \nhttps://www.linkedin.com/in/longba76/   \nWe have no conflicts of interest or funding sources to disclose. \nCorrespondence concerning this publication should be addressed to Christina J. Steffy, Library \nDirector, Alvernia University, 400 Saint Bernardine Street, Reading, PA 19607. Email: \nchristina.steffy@alve rnia.edu  \n  \nPAUSE FOR YOUR HEALTH         2 \nAbstract  \nSociety is in the midst of an infodemic \u2014 an overwhelming amount of inaccurate health \ninformation is circulating online. The COVID -19 pandemic exacerbated the infodemic which \nwas already fueled by the general mistrust of traditional credible sources, particularly the media and health professionals. ChatGPT is further upending the information landscape. Librarians are trusted professionals who can help ease the impacts of the infodemic, however we need a simple yet not overly simplified c onsumer health evaluation tool to help us teach people how to evaluate \nhealth information in their daily lives. The authors present the \u201cPAUSE for Your Health\u201d tool designed specifically for this purpose. The tool combines the simplicity of a checklist wit h the \nrobust critical thinking of a framework to encourage end- users to pause, consider attributes of the \nsource, and then decide if this source is beneficial to their health before following the advice or sharing the information.  \n  \nPAUSE FOR YOUR HEALTH         3 \nPAUSE for Your Health: A New End -User Tool for Evaluating Consumer Health News \nand Online Resources  \nSociety is in the midst of what the World Health Organization (2021) has termed an \n\u201cinfodemic\u201d \u2014 an overwhelming amount of inaccurate health information is circulating online. \nThe COVID -19 pandemic exacerbated the infodemic and proved it impacts people\u2019s ability to \ncritically evaluate health information. Unfortunately, distrust of the current media landscape, \nimpending reliance on ChatGPT and other artificial intelligence tools, an d heavier reliance on \nsocial media further compounds this problem. The media are not blameless; politicized news outlets and lack of accurate reporting on health news have added to distrust. Despite the recent onslaught of book challenges, librarians are s till seen as trusted professionals, and they can be \ninstrumental in helping consumers evaluate health news. A number of checklists and frameworks already exist to evaluate information; however, a tool that specifically focuses on helping the \naverage consum er quickly and efficiently evaluate health news is needed. As a result, the authors \npropose the use of a new tool to help the general public to interpret the reliability and safety of online health information: PAUSE for Your Health. \nBackground and Context  of the Problem  \n Although the COVID -19 pandemic highlighted the extent of the infodemic, it didn\u2019t \ncause the infodemic. Prior to the pandemic, Wardle and Derakhshan (2017) realized society was already in the midst of an information disorder stemming from the type of incorrect information circulating online, the intention of the information creators and distributors, and the understanding and interpretation of the information receivers. Their information disorder stressed the differences between misinformati on (false information shared without the intent to cause \nharm), disinformation (false information created and shared with the intent to cause harm), and \nPAUSE FOR YOUR HEALTH         4 \nmalinformation (information that is based in truth but is shared in a misleading and possibly \nexaggerat ed way with the intent to cause harm) (p. 20). Misinformation and disinformation are \nrunning rampant, and it is no secret information disorder is a problem. In fact, it has become such a problem that Mitchell and Walker (2021) found that nearly half of U.S . adults in their survey \nsample felt the government should take steps to restrict the flow of misinformation, and a majority of adults believe social media sites should restrict the flow of misinformation even if these actions curtail their ability to post and share items. Malinformation has not been discussed as frequently as it should be, but it has complicated the information landscape by blurring the lines of what is true and what is false.  \n The ability to successfully navigate the information disorder  is complicated by the fact \nthat the American public is losing trust in the media. According to  Brennan (2019) , Gallup began \nmeasuring media trust in 1972 and since then no more than 21% of Americans had the highest \nlevel of trust in the media; however, th e amount of trust in the media peaked in 2004 and has \nsince declined, with 58% of Americans having little to no trust in the media. Brennan (2019) also points out that trust in the media follows party lines, with Republicans having less trust in the media than Democrats. Unfortunately, Gallup\u2019s 2021 poll about honesty and ethics in professions shows the percentage of people who believe television reporters rank \u201cvery high\u201d or \u201chigh\u201d in honesty and ethics is at an all -time low ( Saad, 2022). Newspaper reporte rs rank slightly \nhigher than television reporters, but both groups experienced more than 10 percentage point decreases in the last year. Nurses and medical doctors saw large percentage point increases, and members of Congress lost 53 percentage points (Saa d, 2022).  \n Interestingly, while the Gallup poll in 2021 showed health professions have an increase in \ntrust, the Pew Research Center survey of 14,497 US adults which ran from November 30, 2021 \nPAUSE FOR YOUR HEALTH         5 \nthrough December 12, 2021, showed a different reaction to scientists. According to Kennedy, \nTyson, and Fund (2022), 29% of U.S. adults surveyed said they had a \u201cgreat deal of confidence\u201d medical scientists would act in the public\u2019s best interest, which was a 40% decrease since November 2020 (para. 2). The Pew sur vey results aligned with Gallup\u2019s results related to \njournalists \u2014 six in 10 of the respondents said they have \u201cnot too much\u201d confidence in journalists to act in the public\u2019s best interest (para. 6).  \nThis is problematic because when teaching people how to evaluate information, librarians \nrely on trusted news sources, medical professionals, and scientists as bastions of facts, and the trust of one group influences the trust of others. In fact, Arora et al. (2019) discussed prior to the pandemic how trust of  medical professionals can be jeopardized by the ways in which health \nnews is reported to the general public. This is often due to the differences in journalism and scientific styles of communication; for example, science is incremental, but news requires \nattention -grabbing headlines, and news will convey certainty when there is no scientific \ncertainty. The authors point out the scientific community also bears some responsibility because they include spin in article press releases that exaggerate or muddle research findings. Ultimately, the authors stress the medical community must support high -quality journalism by \nengaging with the media to provide expert opinion and news tips and correct inaccuracies. Scientists must also rethink their own communication s tyles and consider how their audience \nwill perceive their messages. Ecker et al. (2022) point out that scientists have traditionally relied on the information deficit model of communication, which assumes people do not know something, science provides the information, and people absorb and accept the information. This model does not account for social, political, and psychological forces that interfere with the perception of the message and belief in its credibility. The information deficit model\u2019s shortfal ls \nPAUSE FOR YOUR HEALTH         6 \nwere made clear during the COVID -19 pandemic when scientific communication did not take \ninto account the long -term impacts of its often -muddled messaging combined with the reaction \nto this messaging within politicized social media filter bubbles.  \n Libra rians can lend their credibility to the media and the medical community as trust in \nlibraries remains high. Geiger (2017) explains about 78% of adults believe public libraries help \nthem find trustworthy and reliable information and 76% believe libraries he lp them learn new \nthings. Geiger also found 56% of adults believe librarians help them find information that assists with decisions they need to make, and adults also believe libraries and librarians help them in other ways, too; for example, librarians he lp them protect data and cope in the world. Despite \nrecent book ban attempts by vocal citizen groups and politicians, a high level of trust in librarians and libraries remains. In fact, the American Library Association (2022) found large majorities of vote rs oppose book bans, and 89% of voters across all parties believe public and \nschool libraries are important for communities (para. 3).  \n Librarian credibility may also be strengthened by their codes of ethics.  The American \nLibrary Association (ALA) Profess ional Ethics and Code of Ethics (2021)  provides ethical \nguidance for librarians and staff to follow, especially relating to respecting coworkers and patrons, ensuring privacy, providing high- quality service without interference of personal beliefs, \nprovidi ng accurate information without advancing private interests, and dismantling biases.  The \nMedical Library Association (MLA) Code of Ethics for Health Sciences Librarianship (2010) similarly follows the ALA Code of Ethics.  The section of the MLA code that specifically deals with clients is especially relevant to further developing trust with health -related topics:  \n\u25cf The health sciences librarian works without prejudice to meet the client\u2019s information needs.  \nPAUSE FOR YOUR HEALTH         7 \n\u25cf The health sciences librarian respects the privacy of clients and protects the \nconfidentiality of the client relationship.  \n\u25cf The health sciences librarian ensures that the best available information is provided to the \nclient.  \nFurthermore, both organizations provide further health information guidance.  The  \nALA\u2019s Reference & User Services Association (RUSA) Health and Medical Reference Guidelines (2008) address the role of information services staff, reference transaction professional behavior, and ethics. These guidelines help to ensure that the patron is re ceiving the \nbest possible information, with professional integrity, while also maintaining confidentiality.  One of the primary purposes of MLA\u2019s Consumer Health and Patient Education Caucus (CAPHIS, n.d.) is to promote consumer health librarianship and encourage the development of standards for consumer health information. CAPHIS also strongly endorses the Health on the Net \nFoundation\u2019s (HON) HONCode Principles (n.d.), which will be discussed in detail later in this paper.  \nProfessional certifications and specializations can also add to librarian credibility.  MLA \noffers the Consumer Health Information (CHIS) specialization (n.d.) to any librarian, not just MLA members.  Furthermore, the  National Network of Libraries of Medicine (NNLM) (n.d.) \noffers free sp onsorship for CHIS in addition to offering free continuing education courses.  \nPersonal opinions, information behavior, and our information ecosystem further \ncomplicate credibility and source evaluation. According to the News Literacy Project (2022), confi rmation bias \u2014 when people believe information that confirms beliefs they already hold \nand discount evidence to the contrary \u2014 is a natural tendency people must overcome if they hope to learn from evidence. This is often coupled with motivated reasoning which occurs when \nPAUSE FOR YOUR HEALTH         8 \nour own desires and emotions lead us to find evidence that supports the outcome we already seek \n(News Literacy Project, 2022). Social media filter bubbles and politicized news that caters to different ends of the political spectrum enable and exacerbate confirmation bias and motivated reasoning. Clearly getting people to focus more on the evidence than their own biases is a big hurdle. Unfortunately, once you get past that hurdle, Ecker et al. (2022) tell us people grapple with the \u201ccontinue d influence effect\u201d \u2014 the misinformation that they know is misinformation \nstill lingers and impacts judgment (p. 15). The authors also discovered that continuing education about the topic and presentation of evidence does not alleviate this effect as much as they hoped. Misinformation lives in one\u2019s mind along with the correct information, and the misinformation can be retrieved later on. Because our brains retain and retrieve misinformation even after we learn we should not trust that information, it is cr itical that librarians carefully plan how to teach \nabout misinformation. While it may be fun to put misinformation out there for patrons to debunk, \nwe do not want them to suffer the continued influence effect and have that misinformation in the back of the ir mind.  \n We must also accept there are different typologies of users who interact with information. \nHorrigan (2017) explains that about 22% of U.S. adults are \u201ceager and willing\u201d to engage and learn the skills they need to navigate the information environment (para. 4). At the other end of the spectrum are \u201cthe weary,\u201d a category of people who are the least engaged, have low trust in the news sources, and a low desire to acquire additional skills to engage with resources (para. 8). This describes about 25 % of U.S. adults. There is an entire spectrum of typologies between these \ntwo extremes; librarians must be flexible in their approach to teaching people how to evaluate information.  \nPAUSE FOR YOUR HEALTH         9 \nFinally, in addition to the traditional misinformation landscape, Novembe r 2022 \nintroduced the public to a game -changer in how they interact with information, including health \ninformation \u2014 ChatGPT, a large language model trained to answer questions based on a dataset \nof information. ChatGPT is not a traditional search engine because, as stressed by Bradley (2023), when you ask the program questions, it doesn\u2019t return a list of webpages. Instead, it helps consumers cut through the clutter and get their answers in plain English. Users can also keep asking the program additional questions to refine its answers (p. 37). Unfortunately, the current version of ChatGPT was only trained on information until 2021, and producing incorrect or completely fabricated responses is a known problem that future iterations aim to correct. Thus for the time being, consumers still must understand the importance of finding out where their responses came from and checking them for accuracy and timeliness. Despite these challenges, a growing amount of research shows the promise and general accuracy of Ch atGPT in health \nliteracy. For example, Samann et al. (2023) conducted a study to determine the accuracy (comprehensiveness) of responses to common bariatric surgery questions. The researchers discovered ChatGPT provided comprehensive responses to 86.8% of questions and reproducible responses to 90.7% of questions. The researchers believe ChatGPT could be a useful adjunct to provider information, but not a substitute for provider information (p. 1794).  Furthermore, the authors of this paper have conducted t heir own brief testing of ChatGPT with basic health \nquestions and discovered all answers included advice to consult with a healthcare provider.  This is a promising facet in the fight against health misinformation.  \nNeed for a Health -Specific Evaluation Tool  \n It is impossible to overstate the importance of accurately evaluating health information \nbecause it truly can mean the difference between life and death. As noted above, Mitchell and \nPAUSE FOR YOUR HEALTH         10 \nWalker (2021) describe how Americans\u2019 belief in misinformation is enough of a problem that \nmeasures need to be taken to curb it. While it is comforting to know that Americans understand this is a problem and want to see technology solutions, technology is imperfect; for example, algorithms created to combat misinformation will be overcome by algorithms to surpass those efforts, and technological solutions as well as human fact checkers generally find misinformation after it has circulated widely. People must understand how to evaluate information on their own, and part of tha t challenge is even coming to an agreement on what is misinformation. As Cooke \n(2018) describes, we live in a post -truth era of information disorder, an era when people are more \nlikely to believe information that appeals to their own beliefs and ideals as opposed to believing \ninformation that is proven to be factual (p. 2). This resistance to acknowledging objective facts makes it increasingly difficult for people to come to a consensus on what is real and what is fake or flawed, even when those facts are s cientifically proven health guidelines. The nature of health \nnews also makes this task more difficult. Science is not static; it changes over time as new information is learned, and that adds to the difficulty of determining what is misinformation because something could simply be outdated information. Also, in the case of an evolving public health crisis coupled with the 24/7 news cycle, information is often incomplete and flawed when it is relayed to the public. During non -pandemic times, health information is often delivered in a \ndisjointed way with one study being reported out of context which makes people think health advice changes all the time. When poor scientific and media communication is added to the information disorder, it makes evaluating healt h information more difficult and requires a more \nnuanced approach to evaluation. \nLibrarians can help consumers evaluate health news, but the approach to evaluating \nhealth information should be based primarily on health information rather than the traditional \nPAUSE FOR YOUR HEALTH         11 \napproaches based primarily on source type and political bias. Checklists are often outdated and \noversimplified, and frameworks, while flexible, quickly become too complex and cumbersome for people to use in their daily lives. Caulfield (2021) explains c omplex tasks complicate our \nability to think rationally and make decisions; sometimes, less is more when helping people evaluate sources. Also, frameworks have traditionally grown out of the need to validate non-medical information and spot political bias.  An approach that merges the simplicity of a checklist \nwith the flexibility of a framework and acknowledges the personal and social impacts on health news consumption is necessary.  \nCurrent Tools  \nAlthough the COVID -19 pandemic and the spreading of health information through \nsocial media greatly exacerbated health misinformation and disinformation, the misinformation phenomenon is as old as the internet and predates social media; thus, there are many internet -\nfocused information evaluation tools available. The next section will review both general internet evaluation tools and healthcare information evaluation tools. However, many of these tools either do not specifically address the potential harm posed by incorrect health information resources  and/or the tool may be too complex for the average reader to effectively utilize.  \nGeneral Internet Evaluation Tools  \nSome of the better known and more reputable internet evaluation tools include the \nCRAAP Test, IF I APPLY, SIFT, the PEN America Media Literacy Toolkit, a nd the National \nAssociation for Media Literacy Education (NAMLE) Key Questions to Ask When Analyzing Media Messages.  \nThe CRAAP Test is one of the oldest and possibly the best -known internet evaluation \ntools.  It is a criteria -based tool developed for evaluating websites that was developed in 2004 by \nPAUSE FOR YOUR HEALTH         12 \na team of librarians at California State University, Chico (2010).  The relevancy of the CRAAP \ntest has come into question, especially considering that it predates the prolific spread of mis - and \ndisinformation only encourages vertical as opposed to lateral reading and fact -checking \n(Fielding, 2019). Often, other evaluation tools appear to be influenced by the CRAAP Test, building on it as improvements. \nIF I APPLY originated at Marshall University Libraries (Role s et al, 2016).  This is \nprimarily an academically -oriented tool that significantly covers the necessary aspects of \nevaluating literature; however, its level of complexity may not be as useful for consumer health interpretation.  \nSIFT (The Four Moves) was developed in 2017 by Mike Caulfield (2019), the current \ndirector of blended and networked learning at Washington State University in Vancouver.  SIFT utilizes similar  evaluation principles as CRAAP and IF I APPLY while placing even more \nemphasis on the rea der to conduct background research on the topic; however, SIFT has less \ndetailed guidance for the reader.  Its simplistic format is designed for the ease of the end -user.  \nUnfortunately, it does not specifically address the complexities of health information. \nPEN America is an organization that protects the First Amendment rights through \nchampioning the freedom to write and recognizing the power of the word to transform the world. Its mission is to unite writers and their allies and defend their liberties. Their Media Literacy Toolkit (2021)  is designed with the specific intent of detecting disinformation within the media;  \nhowever, it does not specifically address health- related issues.  \nNAMLE Key Questions to Ask When Analyzing Media Messages may not be as well -\nknown as the above tools (Rogow & Scheibe, 2021).  It was adapted in 2007 by Faith Rogow and Cindy Scheibe from NAMLE\u2019s Core Principles for Media Literacy Education.  The deep \nPAUSE FOR YOUR HEALTH         13 \ndive format used starts looking at \u201cAuthors & Audience\u201d through the aspects  of authorship, \npurpose, economics, and responses.  Then it focuses on \u201cMessages & Meanings\u201d through the \naspects of content, techniques, and interpretations. Finally, it focuses on \u201cRepresentations & Reality\u201d through the aspects of context and credibility.   This is one of the more complex tools on \nthe surface, but it becomes simpler as one dives deeper into the tool.   \nHealthcare Information Evaluation Tools  \nTwo of the better -known healthcare information evaluation tools are the HONCode  and \nthe HealthNewsReviews.org Toolkit.  Both significantly address the unique needs and complexities of health information; however, both tools have disconnected websites that make locating their evaluation tools a bit difficult.  \n\"The Health On the Net Fo undation (HON) is a non- governmental organization with the \nobjective of promoting the development and application of new information technologies, \nnotably in the medical and health domains\" (Health On the Net Foundation, n.d.).  HON\u2019s  purpose is to evalua te health websites and issue its HONcode certifying that the website is \nproviding reliable health information.  HON utilizes an eight -step methodology to determine \nreliability of health websites. The biggest drawback to HONcode methodology is that it is not necessarily intended for end -user utilization.  It is specifically used by HON for certifying \nwebsites with their recognizable HONCode label. While it is nice that end users have an easy visual, the HONCode label is not on every credible website, especially sites that are not dedicated to health information but may have credible health news and information (i.e., news websites and government websites); thus end users must still know how to evaluate information on their own.  \nPAUSE FOR YOUR HEALTH         14 \nFinally, the HealthNewsReviews. org Review Criteria is a 10 -question checklist -based \nsystem for evaluating online health information.  It started out in 2006 as a means to \nsystematically review health -oriented news stories and public relations news releases.  \nHealthNewsReviews.org lost i ts funding through its home institution, the University of \nMinnesota School of Public Health, in 2018, ending their reviews and podcasts; however, the HealthNewsReviews.org still serves an active archive of these stories and still hosts an active blog.  Al so, the website includes A Toolkit for Journals and Consumers page.  Even though the \nReview Criteria for the Toolkit is quite comprehensive and health- focused, it is not necessarily \nan easy -to-use checklist for the average health consumer.  \nPAUSE for Your H ealth  \nWith all these great tools available for evaluating online information, there appears to be \na void for adequately evaluating online health information at the consumer health level. Part of \ndesigning a tool specifically for the evaluation of consumer health information is taking into consideration the need to reduce consumer tensions. In fact, El Sherrif et al.\u2019s (2018) study investigating the reduction of negative outcomes of online consumer health information identified three major preventive strateg ies for reducing consumer tensions:  \n\u25cf Providing credible sources. \n\u25cf Teaching how to evaluate sources.  \n\u25cf Discussing the information found. In order to  fill the void for a tool that specifically addresses consumer health and reduces \ntensions, the authors propose a new, consumer health- oriented acronym -based evaluation tool, \nPAUSE for Your Health .  This acronym is broken down into the components below; note that \nalthough the components are listed separately, they often overlap. Also, since searching and \nPAUSE FOR YOUR HEALTH         15 \nevaluating are recursive, users will often be addressing more than one component at a time, and \nsometimes components will be addressed multiple times.  \nP:  Pause  before using or sharing/reposting.  \nA:  Is the source accurate?  \nU:  Do you fully understand what you are reading?  \nS:  Is the source current, reliable, reputable, and unbiased?   \nE:  Is the source trying to exploit  you, either financially, behaviorally, and/or politically?  \nFor Your Health:   Is this a healthy option for you?  \nThe importance of \u201c P\u201d is to address the serious issue of social media sharing without \nreading, and it is the basis for this tool.  If individuals can be trained to consider what they are posting before blindly sharing it, this is half the battle against health disinformation and misinformation.  \nThe importance of \u201cA \u201d is a theme throughout all the above mentioned online evaluation \ntools.  Consumers want accurate information that will not  be detrimental to their health. A source \ncan be accurate but still not be the best source for the user\u2019s health needs which is why accuracy needs attention at the outset. If the source is not accurate, there is no need to continue evaluating it. If the so urce is accurate, the user must further evaluate its applicability to their health \nsituation, its bias, and other criteria. Readers can determine accuracy by engaging in lateral reading. According to Wineburg and McGrew (2017), in lateral reading, you do not stay inside the document to look for clues for credibility because you can be easily fooled. Instead, you open various browser tabs across your computer screen and you go outside of the article to look up the  \nauthor, their affiliation, and the facts mentioned to determine if the author, their affiliation, and the information are real and credible. If the user needs additional assistance determining a \nPAUSE FOR YOUR HEALTH         16 \nsource\u2019s political bias or even with their own fact -checking during lateral reading, the sites listed \nin the \u201cAdditional Tools\u201d section below can be helpful. Librarians and healthcare professionals \ncan also help users determine source accuracy.  \nInability of the user to \u201cU,\u201d  fully understand what they are reading and just assuming that \nwhat they are reading i s true or applies to their own unique health situation, can also be very \nhazardous to their well -being.  An unscrupulous trick often used by disreputable resources is to \ntalk over a reader\u2019s head, assuming the reader will just take their word because it is  written to \nsound scientific.  However, readers must also be able to distinguish between better resources written for a scientific audience, such as online journals, versus those that are trying to exploit the reader (more about that in a moment).  \nAs the u ser works through \u201cA\u201d  and \u201c U,\u201d the source (\u201cS \u201d) must also be current, reliable, \nreputable, and unbiased.  This is where resources from the National Library of Medicine (NLM) \nsuch as MedlinePlus or PubMed can be very helpful. NLM only provides information f rom \nreputable sources.  Additionally, sources listed on a trusted library\u2019s consumer health page, websites vetted by MLA\u2019s CAPHIS, or websites carrying the HONCode can also be useful indicators of the reliability of the source. Furthermore, there are no ha rd-and-fast rules about the \ncurrency of medical information.  Historically in the health sciences, there was a \u201cRule of 5\u201d \u2014 any research more than five years old is probably outdated and irrelevant.  However, that is not necessarily the case, especially w hen looking at the historical developments of research and \nrecommendations (Gottlieb, 2003).  Additionally, the COVID -19 pandemic demonstrated that \neven information published as recently as five weeks ago could be outdated because in the early stages of th e pandemic, medical and scientific information about the virus was changing daily, if \nnot by the minute. In contrast, consumer health advice for the common cold has not significantly \nPAUSE FOR YOUR HEALTH         17 \nchanged over the years, as demonstrated by just looking at the list of \u201cR eferences\u201d from the \nUpToDate  patient education webpage, titled \u201cPatient education: The common cold in adults \n(Beyond the Basics)\u201d (Sexton & McClain, 2021). The date range for its six citations is 1978-\n2000, and yet this page indicates that its recommendati ons are current as of June 2022.  Thus, the \ninterpretive skills of librarians may be useful in helping end- users to determine the currency of \ntheir information resources.      \nAnother key question for the user to ask prior to sharing is if the resource is trying to \nexploit ( \u201cE\u201d) them through the sales of their product, influence their behavior, or influence their \npolitical viewpoints.  A prime example of this is COVID -19 treatments. Some online res ources \nwere selling dangerously unproven cures to COVID -19, while some were sowing doubt about \nthe safety of the vaccines in the name of personal freedoms.   \nThe ultimate question for the end- user to ask after thoroughly evaluating the \u201cPAUSE\u201d \nsections is if the recommendations of the online resource or posting do in fact convey safe, healthy recommendations.  In other words, is it good \u201c for Your Health\u201d ? It may not always be \npossible for the user to make this determination on their own without speaking to a healthcare provider, and this tool is not meant to supplement the advice of medical professionals; however users make judgments about medical advice they find online every day, and they may seek out medical advice online instead of speaking to a healthca re provider. Also, as discovered in Tan \nand Goonawardene\u2019s (2017) systematic review, patients also search the internet for health information to be better prepared for visits with physicians, to better understand the advice of their physicians, and to take  an active role in their health; in fact, internet -informed patients may  \nhave more trust in their physicians (pp. 11- 12). The PAUSE framework aims to help end- users \nmake smarter choices about what information they heed and discuss with their physicians. At  the \nPAUSE FOR YOUR HEALTH         18 \nvery least, the user can determine whether the health information they are reading is fabricated \nand potentially harmful or if it is worth sharing so others can consider the resource\u2019s benefits.  \nIn summary, pause before using or sharing, thoroughly vet the resource for currency, \naccuracy, reliability, and comprehension; determine whether the resource exploits the reader in any manner.  Finally, decide if the information is good for your health.  Working through this tool can help consumers determine if this source is good for their health and if it is good to share with others.  Above all else, if the end-user is confident after their review of online health information, then \u201c PAUSE for Your Health \u201d has effectively served its purpose.  \nAdditional Tools  \nAt times, end -users may have trouble evaluating a source\u2019s bias on their own, or they \nmay need additional assistance fact -checking politically charged resources. The tools below can \nhelp end-users in their evaluation.  \nMedia Bias  \n\u25cf Ad Fontes Media Bias Chart  (2022): https://www.adfontesmedia.com/\n  \n\u25cf AllSides Media Bias (2022): https://www.allsides.com/media -bias   \n\u25cf Media Bias/Fact Check (2021): https://mediabiasfactcheck.com/  (bias options are listed \non the black tab bar) \nPolitically Neutral Fact Checking  \n\u25cf FactCheck.org SciCheck (n.d.): https://www.factchec k.org/scicheck/  (for science -based \nnews stories)  \n\u25cf News media outlets are beginning to do their own fact checking and post these fact checking articles online, but you must be cautious of news media outlet fact checking.  \nPAUSE FOR YOUR HEALTH         19 \nPlease refer to the above Media Bias  resources to determine if an outlet has a particular \npolitical bias.  \nFurthermore, lists of librarian -vetted health websites may also be helpful in identifying \nreliable online health information.  MLA\u2019s CAPHIS caucus maintains a downloadable list titled \n\u201cMost Trusted Health Websites\u201d (n.d.).  Consumer health pages on library websites are also very \nuseful resources, with many created and maintained as LibGuides.     \nThis is not meant to be a comprehensive list of resources.  There are other useful tools \nconsumers can use, and librarians can help consumers determine if those tools are reputable.  \nMoving Forward  \nThe authors of this paper have begun implementing PAUSE for Your Health  in their \nwork, and preliminary feedback is promising. Within the University Park Curriculum at the Penn State College of Medicine, PAUSE for Your Health  has been introduced to second year \nmedical students preparing to start longitudinal  clinical clerkships.  The intent is to better prepare \nthe students for handling patient education questions that they may not necessarily know the answer to at the time the question was posed. It is intended as a tool that they can use to assist their patients and their families with their research.  It also prompts the students to consider referring pa tients and families to consult with a librarian. The fall 2023 semester will be the \nsecond year that PAUSE for Your Health  is used within the curriculum.  A formal evaluation of \nthis tool\u2019s effectiveness is currently in the planning stages as part of a faculty -student cooperative \nresearch project.  \n In addition to promising feedback used with medical students, the authors plan to \nintroduce the tool to other groups. One of the authors of this paper is planning to use PAUSE for \nYour Health  for Alvernia Univers ity\u2019s fall 2023 Seniors College, a program in which University \nPAUSE FOR YOUR HEALTH         20 \nprofessionals provide free short courses to local senior citizens. She will present a session on \nevaluating health information that discusses PAUSE for Your Health  and allows attendees the \nchan ce to use it to evaluate health information. Reception from University administration is \npositive as senior citizens are often the targets of scams and biased or incorrect health information.  As the tools are used with different groups, the authors will r efine the tool. There is no \nreason to suspect the tool will need anything more than fine -tuning because, in general reference \ntransactions, different aspects of this tool have been utilized (for example, librarians ask patrons to pause before sharing, to c onsider bias, to look at accuracy, etc.). The PAUSE for Your \nHealth  framework pulls these concepts together in an organized process that is specific to \nconsumer health resources.  \nConclusion  \n Accurately evaluating consumer health news is critical because f ollowing incorrect health \nadvice can be the difference between life and death. Unfortunately, there is so much competing information and so much distrust of traditional sources of information that consumer health news evaluation is more difficult than ever . Librarians are still trusted professionals who can help \npeople learn to evaluate health news properly, but recent book challenges across the United States (Flood, 2021; Harris and Alter, 2022; Albanese, 2021) could erode trust in the public and school li brary professions if people do not think librarians are capable of the most basic \ncomponent of their professions. Academic libraries also must worry about the residual impacts of a growing decrease in confidence in higher education, even among college graduates (Miner, 2020). In the post -truth era, we risk losing trust of some members of the public if we encourage \nuse of resources that are more credible but go against personal political affiliation.  \nPAUSE FOR YOUR HEALTH         21 \n Although there are many challenges libraries currently f ace to maintain trust in their \nprofession, the public still believes librarians have their best interests at heart, and that trust \ncould be leveraged to improve trust in the media, or at least to improve media literacy education, so people can make more in formed decisions about health news and, ultimately, the sources \nproducing health news. The PAUSE for Your Health  tool has the simplicity of a checklist yet \nthe robustness of a framework that is appropriate for evaluating consumer health news, and librarian s can confidently use this tool to help patrons quickly and efficiently evaluate health \nnews and online resources.  \n  \nPAUSE FOR YOUR HEALTH         22 \nReferences  \nAmerican Library Association.(2022, March 24). Large majority of voters oppose book bans and \nhave confidence in libraries [Press release]. https:/ /www.ala.org/news/press -\nreleases/2022/03/large- majorities -voters-oppose-book-bans- and-have-confidence-\nlibraries   \nAmerican Library Association. (2008, September 29). Health and medical reference guidelines . \nReference & User Services Association (RUSA). Ret rieved July 8, 2022, from \nhttps://www.ala.org/rusa/resources/guidelines/guidelinesmedical  \nArora, V. M., Rousseau, D., & Schwitzer, G. (2019). Why bolstering trust in journalism could \nhelp strengthen trust in medicine. JAMA , 321(22), 2159\u20132160. \nhttps://doi.org/10.1001/jama.2019.0636   \nBrad ley, P. (2023, April). The future of search is intelligent. Computers in Libraries, 43(3), 36-\n40. https://www.infotoday.com/cilmag/apr23/Bradley --The-Future-of- Search -Is-\nIntelligent.shtml  \nBrenan, M. (2019, September 26). Americans\u2019 trust in mass media edges down to 41% . Gallup. \nhttps://news.gallup.com/p oll/267047/americans-trust- mass -media -edges -down.aspx  \nCalifornia State University, Chico. (2010, September 17). Evaluating information - Applying the \nCRAAP test . Meriam Library. Retrieved July 8, 2022, from \nhttps://library.csuchico.edu/sites/default/files/craap -test.pdf  \nCaulfield, M. (2019, June 19). SIFT (the four moves) . Hapgood. Retrieved July 8, 2022, from \nhttps://hapgood.us/2019/06/19/sift-the-four-moves/  \nCaulfield, M. (2021). Information literacy for mortals  (Provocation Series). Project Information  \nPAUSE FOR YOUR HEALTH         23 \nLiteracy. https://projectinfolit.org/pubs/provocation- series/essays/information -literacy -\nfor-mortals.html \nCode of ethics for health sciences librarianship. (2010, June). Medical Library Association. \nRetrieved July 8, 2022, from https://www.mlanet.org/page/code -of-ethics  \nCommunities home: Consumer and patient health information services. (2022). Medical Library      \nAssociation. Retrieved July 22, 2022, from \nhttps://www.mlanet.org/p/co/ly/gid=12&req=load&fid=462   \nConsumer health information specialization.(n.d.). Medical Library Association. Retrieved July \n8, 2022, from https://www.mlanet.org/p/cm/ld/fid=329  \nConsumer health information specialization. (n.d.). Network of the National Library of Medicine \n(NNLM). Retrieve d July 8, 2022, from https://nnlm.gov/guides/specialization#section6-1  \nCooke, N.A. (2018). Fake news and alternative facts: Information literacy in a post-truth era. \nALA Editions. \nEcker, U. K. H., Lewandowsky, S., Cook, J., Schmid, P., Fazio, L. K., Brashier, N., Kendeou, P.,  \nVraga, E. K., & Amazeen, M. K. (2022). The psychological drivers of misinformation belief and its resistance to correction. Nature Reviews Psychology, 1, 13\u201329. https://doi.org/https://doi.org/10.1038/s44159-021-00006-y\n \nEl Sherif, R., Pluye, P., Tho\u00ebr, C., & Rodriguez, C. (2018). Reducing negative outcomes of  \nonline consumer health inf ormation: Qualitative interpretive study with clinicians, \nlibrarians, and consumers. Journal of Medical Internet Research, 20(5), e169. https://doi.org/10.2196/jmir.9326\n \nPAUSE FOR YOUR HEALTH         24 \nFielding, J. (2019). Rethinking CRAAP: Getting students thinking like fact- checkers in \nevaluations web sources. College & Research Libraries News, 80(11), 620.  \nhttps://doi.org/10.5860/crln.80.11.620   \nFlood, A. (2021, November 25). US libraries report spike in organised attempts to ban books in  \nschools. The Guardian. https://ww w.theguardian.com/books/2021/nov/25/us- libraries -\nreport -spike- in-organised -attempts -to-ban-books- in-schools  \nGottlieb, L. N. (2003). Ageism of knowledge: Outdated research. Canadian Journal of Nursing  \nResearch , 35(3), 3 \u20136. \nGeiger, A. W. (2017). Most Americ ans \u2013 especially Millennials \u2013 say libraries can help them \nfind reliable, trustworthy information. Pew Research Center. \nhttps://www.pewresearch.org/fact -tank/2017/08/30/most- americans -especially -\nmillennials -say-libraries -can-help-them -find-reliable -trustworthy -information/  \nHarris, E. A., & Alter, A. (2022, February 8). Book ban efforts spread across the U.S. The New  \nYork Times . https://www.nytimes.com/2022/01/30/books/book- ban-us-schools.html  \nHealth and medical information on the internet . (n.d.). Health on the Net Foundation (HON). \nRetrieved July 8, 2022, from https://www.hon.ch/Global/doc/HON- depliant -\npatient_en.pdf  \nHorrigan, J. B. (2017). How people approach facts and information. Pew Research Center.  \nhttps://www.pewresearch.org/internet/2017/09/11/how-people- approach -facts -and-\ninformation/  \nKennedy, B., Tyson, A., & Funk, C. (2022). Americans\u2019 trust in scientists, other groups declines .  \nPew Research Center. https://www.pewresearch.org/science/2022/02/15/americans- trust-\nin-scientists -other-groups- declines/  \nPAUSE FOR YOUR HEALTH         25 \nMedia bias chart (version 9) . (2022, January). Ad Fontes Media. Retrieved July 8, 2022, from \nhttps://www.adfontesmedia.com/  \nMedia bias . (2022, February 24). AllSides. Retrieved July 8, 2022, from \nhttps://www.allsides.com/media -bias \nMedia Bias/Fact Check. (2021, July 22). Retrieved July 8, 2022, from \nhttps://mediabiasfactcheck.com/  \nMedia literacy toolkit. (2021, January 28). PEN America. Retrieved July 8, 2022, from \nhttps://pen.org/media -literacy -toolkit/  \nMedical Library Association. (n.d.). Most trusted health websites . Consumer Health and Patient \nEducation Caucus (CAPHIS). Retrieved July 8, 2022, from \nhttps://www.mlanet.org/d/do/13143  \nMiner, M. A. (2020). Unmet promises: Diminishing confidence in education among  \ncollege-educated adults from 1973 to 2018. Social Science Quarterly , 101(6), 2212\u2013\n2331. https://doi.org/10.1111/ssqu.12873  \nMitchell, A., & Walker, M. (2021, August 18). More Americans now say government should \ntake steps to restrict false information online than in 2018. Pew Research Center. \nhttps://www.pewresearch.org/fact -tank/2021/08/18/more- americans -now- say-\ngovernment-should- take- steps -to-restrict -false- information -online- than-in-2018/  \nNews Literacy Proj ect. (2022). In brief: Confirmation bias and motivated reasoning \n[Infographic]. News Literacy Project. https://newslit.org/wp -\ncontent/uploads/2022/07/InBrief-ConfirmationBiasMotivatedReasoning-FINAL.pdf   \nProfessional ethics . (2021, June 29). American Library Association. Retrieved July 8, 2022, \nfrom https://www.ala.org/tools/ethics  \nPAUSE FOR YOUR HEALTH         26 \nRogow, F., & Scheibe. C. (2021, June). Key questions to ask when analyzing media messages . \nNational Association for Media Literacy Education. Retrieved July 8, 2022, from \nhttps://namle.net/wp -content/uploads/2021/06/Key-Questions.pdf  \nRoles, E., Phillips, K., & Thomas, S. (2021, May 19). IF I APPLY - A source evaluation tool: \nHome . Marshall University Libraries. Retrieved July 8, 2022, from \nhttps://libguides.marshall.edu/IFIAPPLY  \nSaad, L. (2022, January 12). Military brass, judges among professions at new image lows .  \nGallup. https://news.gallup.com/poll/388649/military- brass -judges- among -professions-\nnew-image -lows.aspx  \nSamann, J.S., Yeo, Y.H., Rajeev, N., Hawley, L., Abel, S., Ng, W.H., Srinivasan, N., Park, J., \nBurch, M., Watson, R., Liran, O., & Samakar, K. (2023). Assessing the accuracy of responses by the language model ChatGPT to questions regarding bariatric surgery. Obesity Surgery, 33, 1790-1796. https://doi.org/10.1007/s11695-023-06603-5\n  \nSexton, D. J., & McClain, M. T. (2021). Patient education: The common cold in adults (Beyond \nthe Basics). UpToDate . Retrieved July 26, 2022, from \nhttps://ww w.uptodate.com/contents/the-common- cold-in-adults-beyond- the-basics  \nSciCheck. (n.d.). FactCheck.org. Retrieved July 8, 2022, from \nhttps://www.factcheck.org/scicheck/  \nTan, S.S., & Goonawardene, N. (2017). Internet health information seeking and the patient-\nphysician relationship: A systematic review. Journal of Medical Internet Research, 19(1). \n10.2196/jmir.5729  \nPAUSE FOR YOUR HEALTH         27 \nUniversity of Minnesota School of Public Health. (n.d.). Our review criteria . \nHealthNew sReview.org. Retrieved July 8, 2022, from \nhttps://www.healthnewsreview.org/about-us/review- criteria/   \nWardle, C., & Derakhshan, H. (2017). Information disorder: Toward an interdisciplinary  \nframework for research and policy making. Council of Europe. \nhttps://rm.coe.int/information -disorder- toward -an-interdisciplinary -framework -for-\nresearc/168076277c  \nWineburg, S., & McGrew, S. (2017). Lateral reading: Reading less and learning more when \nevaluating digital information (Stanford History Education Group Working Paper No. \n2017- A1). https://dx.doi.org/10.2139/ssrn.3048994  \nWorld Health Organization. (2021). Infodemic . Retrieved September 29, 2022, from  \nhttps://www.who.int/health -topics/infodemic#tab=tab_1  ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "PAUSE for Your Health: A New End-User Tool for Evaluating Consumer Health News and Online Resources", "author": ["CJ Steffy", "BA Long"], "pub_year": "2023", "venue": "The Reference Librarian", "abstract": "Society is in the midst of an infodemic \u2013 an overwhelming amount of inaccurate health  information is circulating online. The COVID-19 pandemic exacerbated the infodemic which was"}, "filled": false, "gsrank": 449, "pub_url": "https://www.tandfonline.com/doi/abs/10.1080/02763877.2023.2276934", "author_id": ["-J3nxHUAAAAJ", "1wE3c4wAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:--6CpW7h-bUJ:scholar.google.com/&output=cite&scirp=448&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D440%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=--6CpW7h-bUJ&ei=VbWsaOTRJ7XCieoP4PfQ0A8&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:--6CpW7h-bUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://scholarsphere.psu.edu/resources/0b6d1b2c-b75f-430f-bc58-ae479752afd8/downloads/34959"}}]