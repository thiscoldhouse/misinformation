[{"title": "COMPARISON OF TOPIC MODELING ALGORITHMS ON NEWS ARTICLES", "year": "NA", "pdf_data": "C O M PA R I S O N O F T O P I C\nM O D E L I N G A L G O R I T H M S O N\nN E W S A RT I C L E S\nA R E S E A R C H O N M E D I A B I A S\nZ E Y N E P S E V G I F E RT\nthesis submitted in partial fulfillment\nof the requirements for the degree of\nmaster of science in data science &society\nat the school of humanities and digital sciences\nof tilburg university\nstudent number\n2079767\ncommittee\ndr. Michal Klincewicz\ndr. Richard Starmans\nlocation\nTilburg University\nSchool of Humanities and Digital Sciences\nDepartment of Cognitive Science &\nArtificial Intelligence\nTilburg, The Netherlands\ndate\nJune 26,2022\nword count\n8.721\nacknowledgements\nI would like to thank my supervisor Dr. Klincewicz for his assistance in the\nwriting of this paper. My sincere thanks also goes to Media Bias Group for\ntheir guidance and help, this project would not have been possible without\nthem.\nI would like to thank my friends and family for their motivation and\nsupport, especially the ones that were with me on this journey. A special\nthanks to Eda for her contribution on various aspects of the thesis, Deste for\nalways being there for me as a mentor, friend and family and my brother\nBar\u0131\u00b8 s who is the director of this episode and all episodes of my life.\nC O M PA R I S O N O F T O P I C\nM O D E L I N G A L G O R I T H M S O N\nN E W S A RT I C L E S\nA R E S E A R C H O N M E D I A B I A S\nzeynep sevgi fert\nAbstract\nIn today\u2019s society, with the impact of technological developments,\nall information is shared through social media. Facebook and Twitter\nare the primary outlets. Hence, media outlets also actively use those\nplatforms in order to share up-to-date news. As a result of the\nuncontrollable social media flow, disinformation has also become\nwidespread.\nAs expected, media bias is used to spread unrealistic and false\nnews. Although studies on media bias have been conducted in\nnumerous social science domains, the number of studies based on\nreal data has not increased in the same way. The overall purpose of\nthis thesis is to learn more about the structure of media bias and to\ninvestigate if article comments can be used to establish bias. As a\nresult, the purpose is to provide a theoretical framework for media\nbias that can be used to future studies.\nThe data is initially scraped for other research and is taken from\nad fontes media that collects a vast amount of user-generated Twitter\ndata as well as article and outlet-specific analytics. By combining the\ndata from the Twitter API, a dataset suitable for research that can\nreveal the relationship between different parameters has emerged.\nThe topics are discovered using topic modeling approaches. These\nmodels are compared based on their coherence scores and the one that\nperforms better is utilized. In the end, a further regression analysis\nis conducted using this data to detect comment traits and determine\nwhether they are signs of an article\u2019s bias. The topics of the articles\nare determined and assessed by the tweets related to those articles.\ncontents\n1Data Source, Code and Ethics 1\n2Societal Relevance, Problem Statement and Research Goal 2\n2.1Societal Relevance 2\n2.2Problem Statement 2\n2.3Research Questions 3\n3Literature Review 5\n3.1Introduction 5\n3.2Understanding Media Bias 5\n3.2.1 Effects of Media Bias 6\n3.2.2 Effects of Readers\u2019 Comments on Media Bias 6\n3.2.3 Existing Approaches on Media Bias 6\n3.3Topic Modeling 7\n3.3.1 Latent Dirichlet Allocation (LDA) 8\n3.3.2 Top2Vec 9\n3.4Topic Modeling as an Approach to Understand Media\nBias 10\n3.5Related Parameters Used in the Analysis 11\n3.5.1 Coherence Score 11\n3.5.2 Sentiment Score 12\n3.5.3 Hate Speech 12\n4Methodology and Experimental Setup 13\n4.1Introduction 13\n4.2Language and Packages 13\n4.3Data Scraping 14\n4.4Data Cleaning 14\n4.5Implementation and Comparison of Topic Modeling\nTechniques 15\n4.5.1 LDA 15\n4.5.2 Top2Vec 16\n4.5.3 Evaluation Method 16\n4.6Regression 17\n5Results 18\n5.1LDA 18\n5.2Top2Vec 19\n5.3Model Comparison and Selection 20\n5.4Regression 21\n5.5Error Analysis 22\n6Discussion 24\n6.1Limitations 25\n6.2Future Work 26\n7Conclusion 28\naAppendix 30\nbAppendix 31\ncAppendix 32\ndAppendix 33\nCONTENTS 4\neAppendix 34\nfAppendix 35\nReferences 36\n1 data source ,code and ethics 1\n,\n1 data source ,code and ethics\nThe dataset for this research is obtained from another research. (Richter,\n2022 ) The author of the thesis acknowledges that they do not have any\nlegal claim to this data or code. The dataset contains data from ad fontes\nmedia , which is scraped and compiled.1\nThe other part of the data is from Twitter API which is also collected\nfor a specific time period and filtered the tweets that are comments on the\narticles. Both of these are merged and compiled as a single dataset. This\nresearch\u2019s code is in line with the aim of the data collector\u2019s project. The\ncode used in this thesis is publicly available in:\nhttps://github.com/Zeynep 1213 /DSSthesis. Work on this thesis did\nnot involve collecting data from human participants or animals.\n1https://adfontesmedia.com/\n2 societal relevance ,problem statement and research goal 2\n2societal relevance ,problem statement and research goal\n2.1Societal Relevance\nIn this section, the motivation of this research, the problem that will be\ntackled and the goal that will be obtained is explained.\nSocial media has evolved into a platform where people use and receive\nnews as technology and digitization have advanced. News sources began\nto reach people through social media platforms as a result of the increased\nuse of social media for communication. News has begun to be presented\nnot only in traditional newspapers but also on social media, as it is a\nplatform where everyone can express themselves. As a result, news outlets\nbegan to publish updated and real-time news on social media, too.\nThe rapid progress of social media is of great importance in spreading\nthe news. Thus, news began to be published in a way that was not possible\nbefore, from many different news sources. This communication, which\nis much more dynamic and faster, has changed the way the media is\nshaped. As the news sources diversified, the quality of the news served by\nthe media became debatable. The news on social media began to shape\npeople\u2019s perspectives in different ways, and the reliability of news resources\nwas disrupted.\n2.2Problem Statement\nFollowing the debut of Facebook in 2004 , social media platforms began to\nattract a lot of attention. Twitter, which was created in 2006 , is the only one\nthat is all based on text and has become the most used in the news media.\nTwitter has evolved into a forum for individuals to share their opinions,\ndiscuss, and receive up-to-date information. The most important effect of\nthis is that the spread of the news becomes uncontrollable day by day.\nSince it has become problematic to understand which source or which\nnews is reliable, the need for research on this issue has arisen. Therefore,\nwebsites such as mediabiasfactcheck \u20192that evaluate whether the news is true\nor not have emerged in order to understand how reliable the news is or\nwhether it is completely true or false.\nThe diversity of news sources, especially on platforms such as Facebook\nand Twitter, where news spreads rapidly, has expectedly caused an increase\nin media bias. It has become a matter of debate about which source is\nthe most reliable and which news is true or correctly spread. Therefore,\nthis research aims to provide a better understanding of how media bias\n2https://mediabiasfactcheck.com/\n2 societal relevance ,problem statement and research goal 3\nis shaped by finding common aspects of news that can misinform people\nand create media bias.\nAs seen in previous studies, with the massive increase of media bias, it\naffects both societal and individual perception such as political decisions,\nreligious beliefs and voting behavior (Ard\u00e8vol-Abreu & Gil de Z\u00fa\u00f1iga,\n2017 ). Also, biased information can affect collective decision-making by\nbringing with it negative consequences such as polarization between people\n(Spinde, 2021 ).\nThe nature and topics of the news shared, as well as the source from\nwhich it is shared, should all be considered when determining media bias.\nPeople who share their opinions on Twitter are also known to only follow\nnews sources that are appropriate for their own perspectives and reflect\ntheir opinions according to their perceptions (Sunstein, 2002 ).\nHowever, according to Barber\u00e1, Jost, Nagler, Tucker, and Bonneau\n(2015 ), there are also findings that the one-sidedness takes place especially\nin political events, but does not occur in more neutral areas such as en-\nvironmental issues. How the news is launched, what it is about, and the\nimpact of this way of launching on society are presented as inseparable\naspects that need to be addressed in order to understand media bias. Of\ncourse, the effects of disinformation on issues that societies feel sensitive\nabout, such as politics, and the change in people\u2019s perceptions are different\nfrom the disinformation on neutral issues such as the lives of celebrities. As\na result, it\u2019s critical to understand which topics the media is more biased\non.\nThe goal of this study is to determine how relevant an article\u2019s topics\nare to media bias and related issues as news outlets have shifted from\nprint newspapers to online media outlets. Inaccurate and untrustworthy\nnews sources have also emerged and grown in popularity, according to the\nDigital News Report 20213. According to the Reuters Institute only 44% of\nrespondents said they \"[...] trust most news most of the time.\" (Newman et\nal.,2021 ).\nAs sources and news are no more reliable as before, it is important to\ndetermine if something is an indicator of bias detection. This thesis, which\ninvestigates how dozens of news circulating on Twitter and the comments\non them affect each other, and how the topics of the articles affect the\nmedia bias of these news, is written to shed light on future research.\n2.3Research Questions\nMisleading information in the media is used to influence or create public\nopinion. As a result, it can influence or be influenced by other metrics\n3https://https://www.digitalnewsreport.org/\n2 societal relevance ,problem statement and research goal 4\nsuch as topics. Aim of this research is to first compare and find the topic\nmodeling algorithm that works best and then, the topics of the articles,\nhate value, and sentiment of tweets written on the related articles will all\nbe considered.\nMost of the research on media bias does not focus on the relationship\nbetween the topic of the article, bias of the article and Twitter comments.\nThis thesis aims to deepen the research and combine them as no such\nresearch on combining these has been conducted yet. Therefore, the\nresearch questions can be divided into two with the related sub-questions:\nRQ1Which of the topic modeling techniques among LDA and Top 2Vec give more\ninterpretable topics?\nRQ2To what extent the topic of a news article influence the article\u2019s bias and\nTwitter outreach?\n\u2022To what extent does an article\u2019s topic influence the sentiment score and hate\nvalue of related articles\u2019 tweets?\nIn order to answer these questions, first the articles are scraped through\narticle URLs in the dataset. Then, topic modeling techniques are used\nto find the topics of those articles and models are compared. Lastly, a\nregression model is used to understand which topics are highly related\nwith the bias score of the articles, the hate values and the sentiment values\nof the tweets.\n3 literature review 5\n3 literature review\n3.1Introduction\nIn terms of media bias, research has already been done in order to de-\ntermine the media bias framework and its characteristics. However, this\nresearch has tried to make a particularly sharp distinction. In this chapter,\nthe media bias will be outlined and past research on the subject will be\nmentioned. The different ideas from different papers will be discussed. In\naddition, topic modeling techniques will be introduced, and state-of-the-art\napproaches will be included.\n3.2Understanding Media Bias\nAlthough there is no general definition of media bias, it has been the subject\nof other research before. The origins of media bias can be traced back to\nthe nineteenth century, which also raises questions about the media\u2019s role\nand purpose. It outlines the duty of newspapers as they should be written\nby a professional, transmit public information, and only reflect the facts\n(Thornton, 2013 ).\nMedia bias has generally been examined in 3different categories in\nprevious studies: gatekeeping bias, coverage bias, and statement bias.\nGatekeeping bias refers to selecting the stories that will be served as news,\nby the editors and authors (D\u2019Alessio & Allen, 2000 ). Gatekeeping bias,\nalso known as selection bias, is the decision of the people who make the\nnews about which news to cover.\nCoverage bias refers to more intense news about a single topic and\ngetting more attention (D\u2019Alessio & Allen, 2000 ). In the related study,\nit has been seen that the news on social media in each country is most\nlikely related to their own geography and that no selection is made. Due\nto that, it has been understood that traditional and regional news have\nalways received more attention, especially in social media, although there\nis different news about different parts of the world, too.\nWhen it comes to statement bias, it\u2019s defined as saying more positive\nthings on behalf of one party than for other parties, especially in political\nnews. Sentiment analysis has been used in studies to determine which\narticles have higher positive values (Saez-Trumper, Castillo, & Lalmas,\n2013 ).\n3 literature review 6\n3.2.1Effects of Media Bias\nMedia bias has an undeniable impact on individuals and societies (Bern-\nhardt, Krasa, & Polborn, 2008 ). When all the news from a single news\nchannel is biased, for example, the reader\u2019s perspective is shaped, and\nthe outlet\u2019s opinion becomes their own. When viewed through the lens\nof popular news outlets, it becomes clear how important media bias is in\nshaping people\u2019s opinions. According to more in-depth studies, media\nbias can also result in more dangerous outcomes for society. According to\nZaller et al. ( 1992 ), the phenomenon of \"group polarization\" undermines\nthe pillars of democracy, as does media bias.\n3.2.2Effects of Readers\u2019 Comments on Media Bias\nIn recent years as the spread of news shifted from traditional newspapers\nto social media, it allowed people to reflect their opinions in a way that is\nvisible to the others. Despite the fact that comment-writing is still concen-\ntrated among the most vociferous news consumers, Lee and Jang ( 2010 )\ndiscovered that readers infer public opinion based on the contributions of\nnumerous anonymous individuals. Schnauber-Stockmann, Meier, and Rei-\nnecke ( 2018 ) states that \u201cUser comments can highlight certain elements of\nthe news, hereby guiding subsequent cognitive processes.\u201d Therefore, the\neffect of the social media started to be inevitably affect people\u2019s perception.\nGunther and Liebhart ( 2006 ), claimed that \"The mass media context\nmight stimulate partisan concerns about the potentially misleading influ-\nence of information on a broad audience.\u201d That would also lead defensive\npeople to perceive objectively neutral or balanced content as unfavorable\nand misleading. Biased processing of a news report is triggered by the\nexpectation that the media will shift public opinion away from one\u2019s own\nstance (Lee, 2012 ).\n3.2.3Existing Approaches on Media Bias\nSocial Sciences Approaches In social sciences, media bias investigations\ncan be separated into two different subtopics: content analysis and frame\nanalysis .\nBefore beginning to analyze the content, researchers define their ques-\ntions and hypotheses. The scientist then collects and analyzes news data in\na methodical manner, marking the sections that contain media bias. Finally,\nresearchers confirm or reject their initial hypotheses (McCarthy, Titarenko,\nMcPhail, Rafail, & Augustyn, 2008 ). On the other hand, frame analysis is\nabout how the readers perceive the news article.\nInterdisciplinary Approaches The volume of topic news and the mean\narticle similarity are related. In one of the approaches, relationship between\n3 literature review 7\nframing and public attention is investigated using Google Trends4data.\nDoc2Vec\u2019s average cosine distance is used to calculate mean similarity.\nSheshadri, Hang, and Singh ( 2018 ) discovered that high-utility keywords\nhave an impact on public opinion.\nAnother approach uses linguistic models to detect biased language\n(Recasens, Danescu-Niculescu-Mizil, & Jurafsky, 2013 ). They implement\nWikipedia data to detect speech tags, verbs, and subjective phrases in\ntheir research. Their linguistically trained algorithm operates almost as\nwell as people in terms of detecting potentially biased terminology and\nsaving time for media professionals. A related analysis utilized a linguistic\nmodel to determine the major characters in a news article and categorise\nthem as heroes, villains, or victims (Gomez-Zara, Boon, & Birnbaum, 2018 ).\nThis method can be helpful in identifying media polarization, especially if\nthe article\u2019s grammar is not complicated and the news article\u2019s content is\nacceptable for deducing those three major roles.\nLazaridou and Krestel ( 2016 ), conducted a research that is about news\nbias detection particularly. They use statements and remarks from politi-\ncians from various political parties. According to UK news, the media\u2019s\nreporting pattern varies, especially during presidential races. They at-\ntempted to analyze these tendencies in terms of the issues and behavior.\nLiterature also shows there is specific research about the Twitter data\nto understand and analyze the media bias. For instance, Wong et al.\nused political orientation of the media outlets using retweets (Wong, Tan,\nSen, & Chiang, 2013 ). Another research done is about not only political\nbut also various kinds of news that takes the regular users into account\n(Lu, Caverlee, & Niu, 2015 ). They created a social infrastructure and\nperformance tuning opinion bias transmission method that improves bias\nprediction performance.Spinde, Hamborg, and Gipp ( 2020 ), have combined\nthe state-of-the-art to further improve bias performance by constructing\na topic-dependent bias lexicon using word embedding in German news\ntexts.\n3.3Topic Modeling\nTopic modeling is an unsupervised machine learning technique which\nscans a set of documents, detects patterns of lexical items, and intelligently\nclassifies groups of words in unstructured data.\nTopic modeling emerged in the 1980 s as a part of the field of \"generative\nprobabilistic modeling\" (Liu, Tang, Dong, Yao, & Zhou, 2016 ). This model\nsuggests that visible factors interact with hidden (or latent) qualities in a\nprobabilistic way to produce data inside a dataset. In 1983 , authors Salton\n4https://trends.google.com/trends/\n3 literature review 8\nand McGill proposed the tf\u2013idf reduction (Salton, 1983 ). Each document in\na corpus is classified according to its lexicon, as well as the frequency of\nevery word is measured to produce a count value, which is then used to\nbuild a term frequency distribution (tf) for the document.\nHofman created the probabilistic LSI or LSA (pLSI/pLSA) model, also\nreferred as the \"aspect model,\" in 1999 . This method uses probabilistic\nmodeling rather than dimensional reduction techniques. Every term in a\ndocument is derived from a mixture model consisting of arbitrary multi-\nnomial variables, each of which is considered a representation of a topic\n(Hofman & Van Oostendorp, 1999 ). Based on Hofman\u2019s model another\napproach by Blei et al. ( 2003 ) is presented called Latent Dirichlet Allocation\n(Blei, Ng, & Jordan, 2003 ). Nonetheless, many of the above-mentioned\ntopic models were first published in the text analysis field for unsupervised\ntopic discovery in a corpus of texts.\nDistributed representations of words and texts as embeddings have\ngained popularity because of their ability to capture semantics. A low-\ndimensional space into which high-dimensional vectors, such as words or\nphrases, can be translated is called an embedding. Doc2Vecand Word 2Vec\nare two well-known unsupervised algorithms for extracting embeddings\nthat represent words and documents (Mikolov, Chen, Corrado, & Dean,\n2013 ), (Le & Mikolov, 2014 ). Recent approaches, such as BERT , allow the\nfinding of contextual embeddings to describe language models (Grove et\nal.,2019 ).\n3.3.1Latent Dirichlet Allocation (LDA)\nLatent Dirichlet Allocation is a probabilistic generative model that created\nin the coding language C in 2003 Blei et al. ( 2003 ). A topic is defined as a\ndistribution over a vocabulary in this technique. It determines the fraction\nof themes in those articles using the number of topics, k. Because it is based\non bag of words (BOW), it trains from a bunch of papers by removing\nterms that are used in the majority of the documents while retaining\nall of the real content. Hidden topics chosen at random are treated as\nprobability distributions over words. LDA distinguishes between words\nthat belong to a topic and words that belong to a topic. The goal of the\nmethod is to determine how many words in a text match to a certain\ntopic. It also attempts to determine how many publications are related to a\ncertain topic based on a single phrase. The most referenced approach in\ntopic modeling research is LDA, which has been used for a wide range of\npurposes (Blei, 2012 ). Technique is prone to errors and requires substantial\nadjustment in order to produce high-quality results; however, it can be\nused in conjunction with other models. It also resulted in a revolution in\n3 literature review 9\ntext and discrete data modeling, and according to Vayansky and Kumar\n(2020 ), is widely mentioned as a starting point in academic studies.\nEven if it is a watershed moment, because it interprets all of the doc-\numents as a single bag of words, it provides non-hierarchical, unrelated\ntopics. Another disadvantage of bag of words is that LDA does not capture\nsentence structure, thus it is then improved in many circumstances (Min,\nSong, Min, et al., 2020 ).\n3.3.2Top2Vec\nTop2Vec is an alternative and new method that uses vectorization of the\ntext (Angelov, 2020 ). Therefore, it is possible to change the placement of\nsemantically similar terms. With Bag of Words (BOW), it uses word and\ndocument vectors in Doc 2Vec. Document vectors in the semantic space are\nsupposed to be the text\u2019s topic, and the closest word vectors to the docu-\nment are the texts\u2019 topics. It employs Uniform Manifold Approximation\nand Projection to address the issue of documents not clustering densely\nenough (UMAP). After dimension reduction, Hierarchical Density-Based\nSpatial Clustering of Applications with Noise (HDBSCAN) is used to dis-\ncover dense areas, and topic vectors are created by locating the centroid\nof the document vectors. When a word appears in many papers, it is\nconsidered noise. Furthermore, stems of the same word are found close\ntogether. Hence, no preprocessing, is necessary during model training.\nWhen the model is trained, it automatically generates the topic size, subject\nwords, and topic number.\nThe research on comparing different models in Twitter data, is capable\nof producing interesting findings (Egger & Yu, 2022 ). As a result in terms\nof extracting original conclusions, research suggests using Top 2Vec over\nLDA. Another research also on Twitter data, as it is considered as short\ntext data, compares LDA and Top 2Vec in order to investigate covid vaccine\nhesitancy (Ma, Zeng-Treitler, & Nelson, 2021 ). In the comparison made,\nTop2Vec was able to generate significantly distinct topics that are intended\nto be found. Both of the researches suggest Top 2Vec in terms of the topic\nnovelty in text data.\nIn terms of longer textual data, there is a research that collects 47.342\narticles from different countries\u2019 news outlets. The topics that are produced\nare more than a thousand, which is almost impossible to classify (Ghasiya\n& Okamura, 2021 ). However, they used the first ten topics for each country\nand all of them were accurate enough to conduct the sentiment analysis af-\nterwards, which gave good results in the end. Another research comparing\nLDA with Top 2Vec in textual long and historical data indicates that \u201cEven\nthough LDA is considered the backbone of topic modeling, it has various\nlimitations compared to state-of-the-art models.\u201d (Narravula, 2021 ). The\n3 literature review 10\ndisadvantages are as follows: Bag of words is unable to decipher syntactic\ninformation in a document. Furthermore, instead of producing discrete\ntopics, topic as a combination of words and words as a combination of\ntopics produces overlaps between the texts. They ended their study by\nstating that standard models are failing to capture contextual information.\nTop2Vec, on the other hand, produced better results when trained with\nDoc2vec, and the subjects created were determined to be the most human\ninterpretable.\n3.4Topic Modeling as an Approach to Understand Media Bias\nFinding the topics of a vast amount of textual data, can create benefits in\ndifferent subjects. One of them is to determine if the topics of the news\narticles affect the bias score of the article.\nA research done in this area is comparing the agenda of newspapers\nwith public interest by detecting the topics of the news (Pinto, Albanese,\nDorso, & Balenzuela, 2019 ). They discovered that the public agenda is\nless diverse than that of the media, and that if the topic is appealing, the\naudience will pay more attention to it. They also discovered that, despite\nthe fact that the newspaper sells less than the others, a good response on\nsocial media networks can spark a public debate.\nAnother research uses topic modeling with critical discourse analysis\nin Muslim world (T\u00f6rnberg & T\u00f6rnberg, 2016 ). According to the findings,\nMuslims are portrayed in Swedish media as a group of people who engage\nin violence and extremism, which are also regarded to be characteristics\nof Islam as a religion. These findings are similar to those previously\ndiscovered about traditional media. As an online amplifier, however, an\ninternet forum would have far more polarizing impacts on the general\npublic. In terms of the bias of religious beliefs, there is a research done by\nKwon, Chadha, and Wang ( 2019 ), about the Twitter conversations about\nmosque shooting took place in Quebec in 2017 . Their study includes\nthe location of the tweet owners, which reveals that tweets sent from\nareas closest to the shooting place are more personal and conflict-oriented.\nTweets from the general public are more likely concerning immigration\npolicies, government responses, and national solidarity than they were\nabout sadness and condolences to victims. Journalists\u2019 subjects, on the\nother hand, were more biased.\nA research done to see how the public opinion changes by using topic\nmodeling, uses articles from 2000 to2017 about the German Renewable\nEnergy Act5(Dehler-Holland, Schumacher, & Fichtner, 2021 ). In the\nchange-point analysis they made, showed that after 2012 , the Act was more\n5https://www.erneuerbare-energien.de/\n3 literature review 11\nclosely related to its prices. That kind of shift may result in influencing the\nrenewable energy policies of the government.\nTo identify the racial bias, Davidson and Bhattacharya ( 2020 ), detect\nhate speech and abusive language in social media by using topic modeling.\nThey discovered some topics that are more racially charged and potentially\nhazardous. They observed that unsupervised learning approaches can\nbe utilized to detect algorithmic bias and discrimination as well as locate\nhidden patterns.\n3.5Related Parameters Used in the Analysis\nAs there are other concepts that is involved both in the dataset and the\nresearch question, the need of explaining relevant concepts have arised.\nCoherence score will be used for the model evaluation. Sentiment value\nand hate speech take place in the dataset and they will be explained in this\nsubsection.\n3.5.1Coherence Score\nCoherence score metric is how well a topic is supported by the corpus\n(R\u00f6der, Both, & Hinneburg, 2015 ).\nThe evaluation of topics as semantically coherent concepts improved\nafter 2000 s. For instance, Chang, Gerrish, Wang, Boyd-Graber, and Blei\n(2009 ), discovered that human judgments are not always well predicted by\nthe likelihood of held-out materials. Others identified \"vacuous\" subjects\nusing disparities between topic-specific word and the corpus-wide word\ndistributions (AlSumait, Barbar\u00e1, Gentle, & Domeniconi, 2009 ).\nThe topic and corpus are inputs to the score evaluation, which produces\na topic coherence for the entire text. It all starts with segmenting the words\nand putting relevant words together. The chance of the terms occurring\nalone or together is then calculated.\nCalculating probabilities throughout the corpus identifies the relation-\nship between two subgroups. The confirmation measure is high when the\nwords in one subset are correlated with the terms in the other subset. In\norder to capture some relationships that the direct confirmation measures\nmay overlook, there are various distinct indirect confirmation measures.\nLastly, the aggregation is calculated to find one single result.\nThe coherence score is used in topic modeling to determine how human\ninterpretable the subjects are. It essentially determines how similar the\nwords are. A topic model is deemed to be better performing if its coherence\nscore is higher. In order to proceed to the next step of the analysis,\n3 literature review 12\ncoherence scores will be compared in order to choose a better performing\nmodel.\n3.5.2Sentiment Score\nSentiment analysis is analyzing the text with respect to its content emo-\ntionally (Enevoldsen & Hansen, 2017 ). Although Hatzivassiloglou and\nMcKeown ( 1997 ) wrote a paper in which they assembled a list of positive\nand negative phrases and anticipated whether intertwined phrases have\nthe same or different orientation using data from the 1987 Wall Street\nJournal corpus, it was not a hot topic until the 2000 s. \"Mining the peanut\ngallery: Opinion extraction and semantic classification of product reviews\"\nis the title of an article that first mentioned semantic analysis in 2003\n(Dave, Lawrence, & Pennock, 2003 ). In the article, sentiment analysis was\ndiscussed in relation to online customer reviews. Since there are so many\nof such reviews, the authors concluded that there should be an automated\nanalysis for them all. After that, sentiment analysis using machine learning\ntechniques evolved and more research published.\nFor example, sentiment analysis is used by Enevoldsen and Hansen\n(2017 ) to detect political bias in Danish newspapers. They also suggest\nthat identifying different types of faults in different texts can be beneficial.\nSentiment analysis is useful in detecting how bias and sentiments are\nassociated in the media. For example, Hamborg, Donnay, Merlo, et al.\n(2021 ) employed sentiment analysis to identify newspaper articles and\ndetermine polarity in another study. Previously conducted research show\nthat sentiment of a text can be valuable when it comes to detect media bias.\n3.5.3Hate Speech\nHate speech is defined as expressing hatred towards a specific targeted\ngroup that shares a common characteristic. Because hate speech varies\nfrom person to person, insults or humiliations based on religion or nation-\nality may occur (Davidson, Warmsley, Macy, & Weber, 2017 ). In order to\nunderstand the criterias of hate speech on social media, Waseem and Hovy\n(2016 ) used a Twitter dataset to identify hate speech based on race and sex.\nThey found out character n-gram based approach provides a strong base.\nIt is apparent that if it is targeted at a specific audience, those who are\nexposed to it will start to think the same way, resulting in polarization and\nits implications. Bias can also be detected by the use of hateful language.\n4 methodology and experimental setup 13\n4 methodology and experimental setup\n4.1Introduction\nThe dataset contains article URLs as well as tweets about those articles.\nThese tweets have hate and sentiment scores. There are also bias scores for\narticles that have been manually identified. In light of this, the section from\ndata parsing through model comparison and results evaluation will be\nassessed in this chapter. The methodology that will be followed is shown\nin the flowchart below.\nFigure 1: Methodology Flowchart\n4.2Language and Packages\nIn this research, for Natural Language Processing and data scraping,\nPython is used. For scraping the data Newspaperk3 package, for topic\nmodeling technique, NLTK and gensim for the LDA model and numpy ,\n4 methodology and experimental setup 14\npandas and top2vec for Top 2Vec. For the regression model, regression in\nR is used.\n4.3Data Scraping\nA separate dataset of 2800 rows of non-repeating URLs was established in\norder to obtain the contents of all the articles from the URLs. Initially, an at-\ntempt was made to scrape articles with the package named BeautifulSoap ,\nwhich decodes the whole web page in HTML format. As there are 255\ndifferent outlets in total, there is no common feature of the article\u2019s starting\npoint between each website when viewed as HTML. In order to avoid\nhaving to look for each outlet and define where the article actually start,\na new package was searched for and newspaperk3 was discovered. It is a\nPython library to extract and curate articles from different URLs. With the\nhelp of this package, only the article part of the web pages are scraped. A\nnew dataset with only the URLs and the text of the articles is created.\nAs dataset includes 2800 different article URLs and the articles of them,\nsome of the articles are formatted in a way that newspaperk3 was not able\nto scrape. The non-scrapable formatted articles were manually reviewed\nand included. Nonetheless, some of the articles were already archived or\ndeleted. Therefore, 81articles out of 2800 were removed before any model\nwas trained. The overview of the dataset is shown below.\nNumber of URLs in the entire dataset 175.807\nNumber of URLs without repetition 2.800\nNumber of articles reached from URLs 2.719\nNumber of tweets in the entire dataset 175.807\nNumber of tweets after deleting\nunreached URLs\u2019 tweets170.702\nTable 1: Overview of the Dataset\n4.4Data Cleaning\nIn order to implement LDA model points indicated below had to be done:\n\u2022Lower case letters: While training the model, capital and lower cases\ninterpreted differently and they do not give another meaning to the\nword. Therefore they had to be changed.\n\u2022Non-ASCII characters: Even though the articles are entirely in En-\nglish, there might still be emojis and different alphabets in them\nwhich are cleaned.\n4 methodology and experimental setup 15\n\u2022 Punctuation: Punctuations are removed from the articles.\n\u2022Stop words: Repetitive and meaningless words are eliminated from\nthe corpus.\n\u2022Lemmatization: Words changed to their base version not to consider\nthem as different entities just because they are in different forms of\nspeech.\n\u2022Tokenization: Phrases are split to smaller units which are also known\nas tokens.\nThese steps are followed as they are required for the LDA model to\nperform without any noise.\n4.5Implementation and Comparison of Topic Modeling Techniques\nIn this section, two different topic modeling techniques will be used on the\ndataset consists of articles and the topics will be found.\nTopic modeling techniques, as mentioned in the literature review, are\nparticularly useful for making long textual data understandable. The\npreviously scraped dataset with only URL and article text will be used for\napplying topic modeling algorithms. Then, the coherence scores of them\nwill be compared and the more interpretable model will be selected.\n4.5.1LDA\nLDA offers both the predictive and latent topic representation of a corpus.\nDue to the fact that the model is built on unsupervised learning, evaluation\nof them is very difficult. However, figuring out if a trained model performs\nobjectively well or poorly is just as crucial, therefore, topic coherence will\nbe used as a means of achieving this.\nLDA as a baseline model is widely used for topic modeling. The\ndata must be cleaned first before the model can be implemented. After\ndownloading the required packages, the dataset containing the articles is\nloaded, and the URL column is removed. The dataset is transformed into a\nlist that turns sentences into separate words. To examine over phrases with\nvarying numbers, bigrams and trigrams were devised. As the number of\nthresholds grows, fewer phrase results are obtained. Hence, the threshold\nwas adjusted to 100by experimenting with various values.\nAfter the cleaning of the data, id2word was used as a dictionary and\nthe document frequency of the new corpus was calculated. After all this\npreparation, the LDA model is built. In order to make this model a baseline\nmodel for the comparison, chunk size is selected as 100with 10passes.\n4 methodology and experimental setup 16\nTopics, keywords, and their weights are seen in the results of the model.\nFinally, the coherence score was calculated in order to compare it with the\nother model.\nWhen it comes to hyperparameter tuning on the model, the number of\ntopics (K), document-topic density, and word-topic density are examined.\nAt this stage, while testing each parameter, the other was kept constant. In\naddition to the results in determining the optimum number of topics, i.e.\nk, the coherence scores of each of these results are taken into account.\n4.5.2Top2Vec\nWord embeddings, in other words vectorization of text data is used in\nTop2Vec which locates semantically similar words and sentences within\nspatial proximity. As Top 2Vec does not need any preprocessing, the data is\nused as is. After loading the sentence encoders, indexings, and transform-\ners, the text data is turned into a list of strings. Doc 2Vec and Universal\nSentence Encoder were tried as embedding models, but since it was giving\nbetter results, universal sentence encoder was chosen.\nA dimension reduction is also carried before density clustering because\nthe vector space typically has a tendency to be sparse. In order to find\ndense areas in the articles, hierarchical density-based spatial clustering\nof applications with noise (HDBSCAN) was utilized to lower the dimen-\nsionality using uniform manifold approximation and projection (UMAP)\n(Angelov, 2020 ). In order to find the topic vector for dense areas, the\ncentroid of the document vectors in the original dimension is determined.\nThe model was applied and 34topics were found. Each of these topics\nhas50words. With topic _sizes , it is seen how many articles are linked\nto each topic. The topic to which the least article is linked has 20articles.\nBy looking at its doc_id, it is also possible to see which article is related to\nwhich topic. Since one of the 34extracted topics consists of conjunctions\nonly, both the topic cloud and the articles were removed from the model.\nIn addition, while being passed to the main dataset with Twitter comments,\nlinks that could not be reached via the URL and tweets about them were\ncompletely excluded from the data.\n4.5.3Evaluation Method\nAlthough topic models include statistical analysis, each method inher-\nently differs from the others and is based on its own set of assumptions.\nQuantitative methods cannot provide a comprehensive grasp of the con-\ntext. Therefore, the interpretation of models still strongly relies on human\njudgement (Egger & Yu, 2022 ).\n4 methodology and experimental setup 17\nTop2vec has the benefit of making some improvements automatically\nin comparison to LDA. There is nothing to adjust or optimize because it\nhandles frequent terms on its own. To have a trade off between accuracy\nand computational costs, Top 2vec offers the option to choose between\nfast-learn, learn, and deep-learn. The learn option is preferred because\ndeep learning used too much memory and took too long. Other options,\nsuch as chunk length (the number of document chunks), embedding batch\nsize, and embedding model (doc 2vec by default), were used by default.\nWhile comparing topic models, coherence, which is the most commonly\nused score, is used in this research. Coherence value of the topics calculates\nthe most possible words in a topic and their co-occurrence in the same\ndocument. Therefore, it is known as the metric to evaluate topics\u2019 quality\n(Li, Wang, Zhang, Sun, & Ma, 2016 ).\nThere is no conventional resource on how to calculate the coherence\nscore for Top 2Vec, although it is relatively easy to find for LDA. Therefore,\nthe coherence score calculation technique known by using gensim is also\nused in Top 2Vec. However, when calculating this score, the data must\nbe preprocessed. So, the articles used for Top 2Vec were subjected to the\nremoval of punctuation, conversion to lower case, and deletion of URLs\nin this stage. In addition, a column in which the word counts of the posts\nare specified has been added to the dataset so that the chunk-length can\nbe defined. The code that calculates a single coherence with the id2word\ndictionary is inserted into the for loop and the coherence scores for the\nfirst10topics are obtained.\nWhen coherence scores were compared, Top 2Vec was shown to be the\nbetter performing model in different chunk lengths; thus, regression on\nthe topics produced from this model was used. A topic cloud of 50words\nwas manually assigned to each of the 34topics, and a regression analysis\nwas conducted.\nAfter obtaining the coherence scores, comparing the models and de-\nciding on which model to use, topics are labeled for each of the 34topic\nclouds containing 50words.\n4.6Regression\nIn this section, after comparing two different topic modeling techniques\nand picking one of them, just to understand better how the relationship\nbetween the topics and other scores in the dataset, regression analysis is\nconducted. Regression analysis can be done with categorical variables as\nwell (Cruce, 2009 ).\nIn order to categorize the topics,word clouds are manually labelled by\ntheir subjects. An example of labeling can be found in appendix A. After\n5 results 18\nthat, to use the categorical variables in regression, dummy variables are\ngiven to the 34topics.\nKeeping in mind the two research questions stated in 2.3, the follow-\ning objectives are of interest: 1) the effect of article\u2019s topics on article\u2019s\ncharacteristics, and 2) the influence of the topics on the article\u2019s bias.\nAs a result of the Top 2Vec model, a theme was manually assigned to\neach of the 34topics consisting of 50words, and a regression analysis\nwas performed over them. Since we are interested in whether or not one\nquantitative factor is related to the topics of an article no standard two-\nsample tests would be suitable in inferring the parameters. Due to the\nnature of the dataset, regression is therefore used.\nAs the second research question is about the relationship between top-\nics and the bias score of the article and sentiment and hate scores of the\ntweets, statements and questions below have derived.\n\u2022Whether the topic of the article has a relationship with the hate\nvalue of the tweets or not.\nhate_score => \u223c\u03b2h0+\u03b2h1\u2217topics category\n\u2022Whether the topic of an article has a relationship with the bias\nscore of the article or not.\nbias_score => \u223c\u03b2b0+\u03b2b1\u2217topics category\n\u2022Whether the topic of the article has a relationship with the senti-\nment value of the tweets or not.\nsentiment_value => \u223c\u03b2s0+\u03b2s1\u2217topics category\nThe significance level alpha of this test is set to 0.05, hence p-values <\n0.05will be considered as significant at a 95% level. Therefore, in addition\nto the model comparison, the relationships between the topics and scores\nare evaluated.\n5 results\nIn this section, the results found out of the two models, how they are\ncompared and the regression is explained.\n5.1LDA\nTopics and word clouds are generated with LDA algorithm to find topics\nof the articles. The results appear to be containing some generic words.\n5 results 19\nEven though they altogether still make sense, some of the words are not\non point. In addition, some of the words are either conjunctions or out of\ncontext.\nTopics First 10Words\nTopic 1even\u2019, \u2019state\u2019, \u2019attack\u2019, \u2019threat\u2019, \u2019political\u2019,\n\u2019terrorist\u2019, \u2019far\u2019, \u2019dangerous\u2019, \u2019also\u2019, \u2019evangelical\u2019\nTopic 2\u2019say\u2019, \u2019people\u2019, \u2019year\u2019, \u2019make\u2019, \u2019child\u2019,\n\u2019time\u2019, \u2019trump\u2019, \u2019family\u2019, \u2019many\u2019, \u2019haitian\u2019\nTopic 3\u2019say\u2019, \u2019receive\u2019, \u2019patient\u2019, \u2019plasma\u2019, \u2019covid\u2019,\n\u2019library\u2019, \u2019case\u2019, \u2019program\u2019, \u2019death\u2019, \u2019child\u2019\nTopic 4\u2019say\u2019, \u2019trump\u2019, \u2019write\u2019, \u2019go\u2019, \u2019agent\u2019,\n\u2019state\u2019, \u2019world\u2019, \u2019also\u2019, \u2019election\u2019, \u2019vote\u2019\nTopic 5\u2019police\u2019, \u2019percent\u2019, \u2019say\u2019, \u2019recycle\u2019, \u2019support\u2019,\n\u2019also\u2019, \u2019get\u2019, \u2019people\u2019, \u2019power\u2019, \u2019movement\u2019\nTable 2: First five topics found using LDA model. The second column lists the top\n10words of the topics.\n5.2Top2Vec\nTop2Vec algorithm produced 34different topics out of 2800 different articles.\nAs it does not require pre-processing it is seen that some of the words\nare the same, just in another version. Yet, it still gives novel words which\nindividually make sense and altogether they have a meaning as well.\nTopics First 10Words\nTopic 1protester\u2019, \u2019protesters\u2019, \u2019protests\u2019, \u2019arrests\u2019, \u2019demonstrators\u2019,\n\u2019antifa\u2019, \u2019assaulted\u2019, \u2019rioters\u2019, \u2019protesting\u2019, \u2019protest\u2019\nTopic 2\u2019bipartisan\u2019, \u2019filibuster\u2019, \u2019manchin\u2019, \u2019pelosi\u2019, \u2019gop\u2019,\n\u2019senate\u2019, \u2019republicans\u2019, \u2019mcconnell\u2019, \u2019repeal\u2019, \u2019legislative\u2019\nTopic 3\u2019presidents\u2019, \u2019outbreaks\u2019, \u2019pandemic\u2019, \u2019cdc\u2019, \u2019statewide\u2019,\n\u2019outbreak\u2019, \u2019populations\u2019, \u2019affected\u2019, \u2019systemic\u2019, \u2019unvaccinated\u2019\nTopic 4\u2019impeachment\u2019, \u2019gop\u2019, \u2019impeach\u2019, \u2019manchin\u2019, \u2019trump\u2019,\n\u2019reelection\u2019, \u2019bipartisan\u2019, \u2019republicans\u2019, \u2019partisan\u2019, republicans\u2019\nTopic 5\u2019biden\u2019, \u2019presidential\u2019, \u2019politico\u2019, \u2019msnbc\u2019, \u2019giuliani\u2019,\n\u2019staffer\u2019, \u2019debates\u2019, \u2019debate\u2019, \u2019democrats\u2019, \u2019partisan\u2019\nTable 3: First five topics found using Top 2Vec model. The second column lists the\ntop10words of the topics.\n5 results 20\n5.3Model Comparison and Selection\nWhen comparing models, before evaluating any metrics, it is possible\nto compare the topic clouds that would have the same labels, that are\ngenerated from different models. For example, in both of the models\nTopic 3is containing words related to Covid- 19pandemic. While LDA\u2019s\nword cloud is giving \"death\" and \"patient\", which can also be associated\nwith different topics in a different context, the words that Top 2Vec found\nare \"pandemic\" and \"unvaccinated\" which are the words that are more\nqualified when the theme is Covid- 19.\nAccording to the table, it is clearly seen that, while LDA is repeating\nthe same topics and also giving some conjunction words that do not affect\nthe topic search, Top 2Vec is giving more accurate results. To find the more\ninterpretable algorithm, the coherence scores of the two different topic\nmodeling algorithms are calculated for different counts.\nThen, by iterating the min_count variable in the range between 5and\n50with a step of 5, it enables to find the optimal model with the higher\ncoherence scores from the models. The default value for min_count is\n50, therefore it ignores words with a lower total frequency than 50in the\ndefault setting (Angelov, 2020 ). After each iteration, coherence scores were\nstored. min_count values with the highest coherence scores were then\napplied to the dataset to construct the optimal model.\nCoherence Score LDA Top2Vec\nmin_count= 5 0.525 0.651\nmin_count= 10 0.508 0.753\nmin_count= 15 0.507 0.732\nmin_count= 20 0.492 0.693\nmin_count= 25 0.489 0.643\nTable 4: Coherence Score Comparison of LDA and Top 2Vec\nThe table shows the differences between the coherence scores of the two\nmodels used in this research. The higher the coherence score means the\nmore interpretable is the topics. The coherence score when the min_count\nof the topics respectively can be seen above. Minimum count of 5is\nperforming higher than the other counts in LDA. However, Top 2Vec\u2019s\nminimum count is the highest in 10.\nIn the end of the comparison of the models, in any count, it is seen that\nTop2Vec model\u2019s coherence scores are higher than the LDA\u2019s coherence\nscore.\n5 results 21\n5.4Regression\nA regression analysis with categorical variables is done to see if the topics\nare related to the bias score of the articles, hate score of the tweets and\nsentiment value of the tweets respectively.\nTable below shows an example of the results of the regression analysis,\nin order to find how the bias_score of the articles\u2019 is related with the topics.\nAs the dataset is from the election period of US, it also shows how the\ntopic is biased. Finance and anti-trust cases are also highly related with\nthe bias score. The rest of the table can be found in Appendix C. In the\nregression, all of the topics found as they have a relationship with the bias\nscore of the articles.\nLabels t-value Pr(>|t|)\nantitrust_cases - 9.895 <2e-16\ncongress/senate 9.739 <2e-16\nconservatives - 20.881 <2e-16\nelections - 27.380 <2e-16\nfinance/stocks - 19.142 <2e-16\nTable 5: Relationship Between the Topics and Bias Score of the Articles\nTable below shows the topics that mostly influence the hate_ score of\nthe tweets. Hate score of the tweets and topics are also mostly related to\nthe topics. Below is an example of the significantly related topics such as\n\u2019lgbtq\u2019 and \u2019racism\u2019. Even though the relationship of the topics and hate\nscore is not as high as bias score, still more than half of the tweets\u2019 hate\nscore have a relationship with the topics. Rest of the regression analysis\ncan be found in Appendix D.\nLabels t-value Pr(>|t|)\nlgbtq 11.004 <2e-16\nracism 17.476 <2e-16\nimpeachment 4.187 2 .83e-05\nmilitary - 4.091 4 .30e-05\nevangelists 4.448 8 .68e-06\nTable 6: Relationship Between the Topics and Hate Score of the Tweets on Articles\nThe regression, in the table below, it is shown the topics that have\nmore influence on sentiment_score of the tweets that are written about\nthe article. Rest of the regression analysis can be found in Appendix E. It\nis seen that there are different kinds of topics in various subjects have a\nrelationship with the sentiment score. However, when a comparison made,\n5 results 22\neven though the sentiment score and most of the topics are highly related,\ntopics like \u2019finance/stocks\u2019, \u2019homelessness\u2019, \u2019homicides\u2019 are the ones that\nare not related to the tweets\u2019 sentiment score.\nLabels t-value Pr(>|t|)\ncelebrities 11.119 <2e-16\nconser-\nvatives9.711 <2e-16\nmilitary 5.719 1 .07e-08\nolympics - 5.518 3 .44e-08\nelection_day 3.564 0 .000365\nTable 7: Relationship Between the Topics and Sentiment Value of the Tweets on\nArticles\nAs it is clear from the results, bias_score of the articles are the most\ninfluenced score in three of the scores that are evaluated. First of all, we\nfound out that the topics about politics, especially about election days are\nthe ones that are mostly related to bias. Secondly, lgbtq, racism, military,\nevangelists etc. are the topics that influence the hate_score of the tweets\nthat are about the articles the most. Lastly, sentiment_score\u2019s correlations\nwith the articles\u2019 topics are varying from celebrities to olympics. Sentiment\nscore of the tweets are the least related score among others.\n5.5Error Analysis\nError analysis is done for only regression. Topic modeling techniques\nare instable and relative. Therefore, error analysis in those models are\nnot applicable (Agrawal, Fu, & Menzies, 2018 ). So that, error analysis is\nnot conducted for topic modeling algorithms. In the last part, as further\nanalysis regression model is used. Error analysis is done for regression\nmodel in order to see the residuals. qqplot is used and the residuals and\nskewness are seen in the plots that can also be found for the other scores\u2019\nmodels in the appendix F.\n5 results 23\nFigure 2: Q-Q Plot for Bias Score Model\nAs an example, bias score\u2019s Q-Q plot is shown above. In both sides\nthere are tails that indicate compared to the normal distribution there is\nlittle more data located at the extremes of the distribution and less data\nin the center of the distribution. Compared to the normal distribution\nthere is more data located at the extremes and less data in the center of the\ndistribution. Even though it resembles the normal distribution, still, Q-Q\nplot gives us the residuals which means the model could have been better\nin predicting.\n6 discussion 24\n6 discussion\nThe goal of this research is to compare different types of topic modeling\nalgorithms to have a better understanding of bias on online media. To\naccomplish this, a large number of news articles from the US media are\ngathered during a period of time chosen at random. Because media bias is\na phrase that is difficult to define, it should be investigated more.\nTo select the most accurate topics, multiple topic modeling techniques\nare applied after scraping the articles. Topic models provide topics in\nthe form of word clouds. The model comparison made, revealed the\ndifferences between two different models in finding topics. While one of\nthe models determines the number of topics, the other requires the topics\nto be determined by the person who established the algorithm.\nAdditionally, Top 2Vec may compile texts and words that are distinctly\nrelated to one another, making it simpler for the researchers to group the\nwords into themes. The built-in search options give easy access to other\nrelevant documents and keywords. In some Top 2Vec topics, words with\nthe same root cause overlap between words inside the topic. This study\nillustrates one potential drawback of utilizing Top 2Vec as one advantage is\nnot having to deal with stopword removal, lemmatization, or stemming.\nHowever, it should be noted that these terms at least covered the same\nground as others that are clearly identical to them. Although this issue\ndid not cause a problem with the data used for this investigation, addi-\ntional data cleaning and preprocessing may be beneficial to prevent the\nappearance of nonsensical keywords in topics.\nLabeling was a little more difficult for LDA because the topics\u2019 common\nterms tended to be more broad and abstract. In addition, Top 2Vec gives\nresults that are more original and easier to group. In LDA, as it is also stated\nin the previous research, some of the topics are universal terms (Rizvi et al.,\n2019 ). Although the topic clouds are accurate when considered altogether,\nas it also aligns with the research of Alnusyan, Almotairi, Almufadhi,\nShargabi, and Alshobaili ( 2020 ), some of the topics that are produced are\nmeaningless.\nEven though from only looking at the topics one might think one of\nthe models is performing better, in order to compare them, coherence\nscores of the models are measured and compared. In this comparison also\nthe Top 2Vec model had higher values. Therefore, it is selected to do the\nfurther analysis. Each article is assigned to a topic, and the model with\nthe highest coherence score is chosen. Finally, regression is done to the\nselected model\u2019s topics.\nAs a result, it is discovered that the article\u2019s bias is linked to the\ntopics. The sentiment and hate value of tweets from the associated post\n6 discussion 25\nare likewise influenced by the topics, according to the further regression\nanalysis. In the error analysis made, it is seen that the model used could\nhave been fitted better.\nThis study allowed for a closer look at the issue of media bias and was\ncarried out to understand more about it. The theoretical foundation is not\npresented because most research on media bias is related to social sciences.\nIt demonstrated how much media bias can vary and how far it may be\npushed to extremes. This research is also aligned with the research done\nby (Pinto et al., 2019 ) in regards of the importance of the popular topics\naffecting bias.\nIn addition, although it is easier to guess that some topics are under\nthe name of media bias, and some of the topics are thought to not contain\nbias, it has been revealed that many different and neutral topics can be the\nsubject of bias. Thus, insights gained about news sources and the impact\nof social media on people. This research fills that gap with regard to the\nframe of media bias.\n6.1Limitations\nThis research, like other research, has multiple limitations. If listed chrono-\nlogically, articles that cannot be found from their URLs may come first.\nURLs taken from 255different outlets belong to 2800 different articles. The\nones that the scraping algorithm cannot find, tried to be found manually.\nHowever, since some of them are deleted, archived or subscription based,\nit was impossible to access.\nAmong the 2800 articles, 81of them could not be used for this reason.\nAnother drawback is that the scraped articles have varied HTML codes due\nto the fact that they originate from different sources.As a result, while the\nright part of the articles was scraped with the package used, some errors\noccurred, and certain texts that may be classified as advertisements entered\nthe dataset. Unintentionally, access links to different articles on various\nwebsites are also included. Even after cleaning with cleaning techniques,\nsome of them could not be entirely removed.\nWhile using topic modeling techniques, the metric used to compare two\nalternative models, the coherence score, has never been utilized in Top 2Vec\nbefore in the literature. As a result, approaches for calculating coherence\nscores that have been employed in other topic models have been tried.\nDespite the fact that the model does not require preprocessing, this method\nhad to do away with it. Because it necessitates the use of a dictionary and\nis executed using several model codes, the outcomes should be evaluated\nand verified by professionals.\n6 discussion 26\nHyper-parameter tuning, in LDA, could have been tried with different\nalgorithms. Due to the limited time it was not possible in this research. In\ndifferent researches after trying, it is found out that meta-heuristic function\noutperforms all the others (Panichella, 2021 ).\nAlthough the coherence score is widely used as a human interpretability\nmeasurement score, it is controversial whether it is an accurate measure-\nment technique considering that relative results are already obtained in\nsuch a study. It has no baseline or ground truth, despite its widespread\nuse as a score (Stevens, Kegelmeyer, Andrzejewski, & Buttler, 2012 ).\nLanguage models generally have trouble understanding human-written\nnuances, terms, idioms, and adages that language brings and may have\ndifferent meanings.The fact that dataset labels are not the gold standard\nis well established Krouska, Troussas, and Virvou ( 2016 ). The dataset\nconsists of certain tweets and retweets on Twitter which are only made to\nthe original article. It may be necessary to include retweets of tweets in the\nTwitter outreach.\n6.2Future Work\nWhile this thesis builds on the findings of another thesis, it also opens the\ndoor to a plethora of new study possibilities. Because of its diversity, the\ndataset could potentially lead to many more studies. For example, as a\nnext step, a sentiment analysis on the articles that generate polarization,\ncollecting more tweets, or obtaining more negative/positive comments\nmay be conducted. Apart from the sentiment and hate value of the tweets\nwritten about them, it is also feasible to investigate the articles\u2019 previously\nunrecognized aspects and uncover their relations with other Twitter API\nmetrics.\nLooking at the definitions of attributes, some of them can clearly be\nidentified as types of hateful language. For example, \"toxicity\", \"severe\ntoxicity\", \"identity attack\", \"insult\", or \" threat\". Through this hateful\nlanguage, it is possible to distinguish between offensive and hateful.\nFurthermore, on Twitter, where anybody can express themselves, it\nis good to examine the topics or sentiments of articles, as well as to\nidentify the relationship between sarcastic remarks and media bias, by\nusing sarcasm detection and sentiment analysis. The titles of the articles\nare another column that isn\u2019t used in the dataset. It is feasible to conduct\na study into the title and how it influences the scores of other attributes\nor tweets. For example, considering that the title is the crucial part of an\narticle and it is of great importance to people whether to read the article or\nnot, it would be a good research topic to observe how it affects the Twitter\noutreach and how it affects the characteristics of the comments made.\n6 discussion 27\nApart from the existing dataset, the option to expand it and add addi-\ntional columns is also possible. If they have more insight into the people\nwho remark on Twitter, for example, they can build a pattern between their\ncommon characteristics. In addition, if information about the characteris-\ntics of these persons is considered, conclusions and research on this subject\ncan be made based on the articles and news outlets that are manually\ncategorized according to reliability and bias scores.\n7 conclusion 28\n7 conclusion\nWith the widespread use of social media platforms by everyone and the\nincreased active usage of the internet around the world, many news stories\nhave begun to be broadcast on our individual screens. The fact that\npeople are staying at home and conducting numerous conversations has\nexacerbated the situation, particularly during the pandemic. The news\nbegan to circulate more widely online, due to online platforms like Twitter\nand Facebook.\nThe distribution of news, in particular, has begun to occur through\nsocial media. The distinction between social media and traditional news\noutlets is that anyone can express their opinion, and these opinions are\nfreely shared with other users. Due to the unpredictability and uncontrol-\nlability of social media, media bias has intensified. Similarly, a lot of news\nthat circulated with various points of view and for multiple motives and\nwhose validity is up for debate, impacted the perceptions of those who\nread it as it happened on social media. Individual and societal perspectives\nbegan to shift as a result of media bias. This study was conducted in order\nto gain a better understanding of media bias and to demonstrate how\narticles on various topics can alter people\u2019s opinions and how journalists\ncan be more prejudiced on particular issues. As a consequence of this\nstudy, the link between news article topics and bias scores was discovered,\nand it was shown that the sentiment and hate values of Twitter comments\nare also related to the topics.\nLast but not least, this thesis demonstrates how critical unbiased report-\ning by the media is, especially when considering media consumption in\ntoday\u2019s changing society.\n7 conclusion 29\nAappendix 30\na appendix\nWords in Topic Cloud Label\n\u2019bipartisan\u2019, \u2019filibuster\u2019, \u2019manchin\u2019, \u2019pelosi\u2019, \u2019gop\u2019, \u2019senate\u2019,\n\u2019republicans\u2019, \u2019mcconnell\u2019, \u2019repeal\u2019, \u2019legislative\u2019, \u2019obamacare\u2019,\n\u2019democrats\u2019, \u2019lawmakers\u2019, \u2019congressional\u2019, \u2019senators\u2019,\n\u2019reelection\u2019, \u2019rasmussen\u2019, \u2019partisan\u2019, \u2019democrat\u2019, \u2019congress\u2019,\n\u2019republican\u2019, \u2019compromise\u2019, \u2019caucus\u2019, \u2019voters\u2019, \u2019biden\u2019, \u2019schumer\u2019,\n\u2019reconciliation\u2019, \u2019legislation\u2019, \u2019legislature\u2019, \u2019senator\u2019,\n\u2019legislators\u2019, \u2019veto\u2019, \u2019centrist\u2019, \u2019federalist\u2019, \u2019democratic\u2019,\n\u2019legislatures\u2019, \u2019bills\u2019, \u2019politico\u2019, \u2019fiscal\u2019, \u2019congresswoman\u2019,\n\u2019conservatives\u2019, \u2019sen\u2019, \u2019electorate\u2019, \u2019impeach\u2019, \u2019reform\u2019, \u2019cuomo\u2019,\n\u2019alleges\u2019, \u2019medicaid\u2019, \u2019overwhelmingly\u2019, \u2019impeachment\u2019congress/senate\n\u2019unvaccinated\u2019, \u2019vaccine\u2019, \u2019vaccinated\u2019, \u2019vaccines\u2019, \u2019vaccination\u2019,\n\u2019vaccinations\u2019, \u2019cdc\u2019, \u2019immune\u2019, \u2019outbreaks\u2019, \u2019immunity\u2019, \u2019narrow\u2019,\n\u2019pandemic\u2019, \u2019viral\u2019, \u2019outbreak\u2019, \u2019flu\u2019, \u2019quarantine\u2019, \u2019shots\u2019,\n\u2019sars\u2019, \u2019reuters\u2019, \u2019anti\u2019, \u2019pfizer\u2019, \u2019rasmussen\u2019, \u2019infectious\u2019,\n\u2019congresswoman\u2019, \u2019systemic\u2019, \u2019frontline\u2019, \u2019disinformation\u2019,\n\u2019risks\u2019, \u2019fda\u2019, \u2019protocols\u2019, \u2019zuckerberg\u2019, \u2019populations\u2019,\n\u2019staffers\u2019, \u2019enforce\u2019, \u2019coverage\u2019, \u2019consensus\u2019, \u2019recipients\u2019,\n\u2019governments\u2019, \u2019urged\u2019, \u2019administrations\u2019, \u2019regulatory\u2019,\n\u2019endorsed\u2019, \u2019spreading\u2019, \u2019gov\u2019, \u2019mccarthy\u2019, \u2019convinced\u2019, \u2019safer\u2019,\n\u2019plaintiffs\u2019, \u2019intercept\u2019, \u2019exposure\u2019vaccination\n\u2019evangelicals\u2019, \u2019churches\u2019, \u2019protesting\u2019, \u2019evangelical\u2019,\n\u2019protesters\u2019, \u2019rallies\u2019, \u2019protester\u2019, \u2019denounced\u2019, \u2019pastor\u2019,\n\u2019protests\u2019, \u2019gop\u2019, \u2019demonstrators\u2019, \u2019protest\u2019, \u2019advocated\u2019, \u2019cult\u2019,\n\u2019elect\u2019, \u2019haitians\u2019, \u2019feared\u2019, \u2019speeches\u2019, \u2019absentee\u2019, \u2019troubling\u2019,\n\u2019nonprofit\u2019, \u2019based\u2019, \u2019distancing\u2019, \u2019fallen\u2019, \u2019presidency\u2019,\n\u2019oppose\u2019, \u2019pray\u2019, \u2019shootings\u2019, \u2019giuliani\u2019, \u2019inaugural\u2019,\n\u2019christian\u2019, \u2019republican\u2019, \u2019extremism\u2019, \u2019overwhelmingly\u2019,\n\u2019inauguration\u2019, \u2019reconciliation\u2019, \u2019pledged\u2019, \u2019inequality\u2019,\n\u2019elected\u2019, \u2019gatherings\u2019, \u2019testimony\u2019, \u2019political\u2019, \u2019praise\u2019,\n\u2019christians\u2019, \u2019spokeswoman\u2019, \u2019lawmakers\u2019, \u2019definition\u2019,\n\u2019republicans\u2019, \u2019radical\u2019]evangelists\n\u2019transgender\u2019, \u2019lgbtq\u2019, \u2019discrimination\u2019, \u2019oppose\u2019, \u2019trans\u2019,\n\u2019advocated\u2019, \u2019kamala\u2019, \u2019repeal\u2019, \u2019naacp\u2019, \u2019activists\u2019, \u2019activist\u2019,\n\u2019legislators\u2019, \u2019legislatures\u2019, \u2019anti\u2019, \u2019lawmakers\u2019, \u2019minorities\u2019,\n\u2019legislature\u2019, \u2019amendment\u2019, \u2019advocates\u2019, \u2019congresswoman\u2019,\n\u2019endorsed\u2019, \u2019transition\u2019, \u2019manchin\u2019, \u2019accepting\u2019, \u2019queer\u2019,\n\u2019controversy\u2019, \u2019justices\u2019, \u2019allegations\u2019, \u2019legislative\u2019, \u2019sen\u2019,\n\u2019gender\u2019, \u2019bipartisan\u2019, \u2019inequality\u2019, \u2019endorsement\u2019, \u2019opposed\u2019,\n\u2019privilege\u2019, \u2019progressive\u2019, \u2019questioning\u2019, \u2019centrist\u2019, \u2019refusing\u2019,\n\u2019federalist\u2019, \u2019statewide\u2019, \u2019equality\u2019, \u2019unclear\u2019, \u2019minority\u2019,\n\u2019protests\u2019, \u2019denied\u2019, \u2019lgbt\u2019, \u2019approve\u2019, \u2019protesters\u2019lgbtq\nTable 8: Word Clouds and Manually Given Labels\nBappendix 31\nb appendix\nParameter Description\nid Unique id of the tweet.\ntext Tweet itself.\ntweet_id ID of the original tweet being mentioned.\ntitle Article headline.\noutlet Article news outlet.\ntwitter_handle Twitter username of the news outlet.\narticle_url URL of the article which is referenced by the original tweet.\nbias_score Bias score of the article.\nreliability_score Reliability score of the article.\npos_score_hate Positive hate score.\nneg_score_hate negative hate score.\nhate_value Hate or non-hate.\npos_score_sentiment Positive sentiment score of the tweet.\nneg_score_sentiment Negative sentiment score of the tweet.\nsentiment Indicates whether the tweet is positive or negative.\nINSULT The score for the Perspective API\nLIKELY_TO_REJECT The score for the Perspective API\nIDENTITIY_ATTACK The score for the Perspective API\nSEVERE_TOXICITY The score for the Perspective API\nTHREAT The score for the Perspective API\nATTACK_ON_COMMENTER The score for the Perspective API\nSEXUALLY_EXPLICIT The score for the Perspective API\nSPAM The score for the Perspective API\nATTACK_ON_AUTHOR The score for the Perspective API\nbias_class Bias class of the outlet.\nreliability_class Reliability class of the outlet.\noverall_reliability Overall reliability of the outlet.\noverall_bias Overall bias of the outlet.\nTable 9: Dataset and Explanations\nCappendix 32\nc appendix\nEstimate Std.Error t-value Pr(>|t|) Significance Code\n(Intercept) 1.1141 0 .2548 4 .372 1 .23e-05 ***\ntopicsafghanistan 1.6711 0 .3652 4 .576 4 .73e-06 ***\ntopicsantitrust_cases - 2.8295 0 .2859 -9.895 <2e-16 ***\ntopicscelebrities - 9.5021 0 .2635 -36.061 <2e-16 ***\ntopicscongress/senate 2.6211 0 .2691 9 .739 <2e-16 ***\ntopicsconservatives - 5.7183 0 .2738 -20.881 <2e-16 ***\ntopicsdisinformation/\nfraudulent_content - 25.9103 0 .3969 -65.280 <2e-16 ***\ntopicseducation - 2.6130 0 .3680 -7.101 1 .25e-12 ***\ntopicselection_day - 3.9441 0 .2607 -15.131 <2e-16 ***\ntopicselections - 8.8935 0 .3248 -27.380 <2e-16 ***\ntopicsenvironmental_crisis - 3.4830 0 .3194 -10.903 <2e-16 ***\ntopicsevangelists 1.2678 0 .3094 4 .097 4 .18e-05 ***\ntopicsfinance/stocks - 7.0908 0 .3704 -19.142 <2e-16 ***\ntopicshomelessness 2.0012 0 .3863 5 .180 2 .22e-07 ***\ntopicshomicides - 7.2404 0 .6756 -10.717 <2e-16 ***\ntopicsimmigrants 4.4746 0 .3234 13 .835 <2e-16 ***\ntopicsimpeachment - 10.1604 0 .2766 -36.730 <2e-16 ***\ntopicsjournalism 6.1665 0 .2972 20 .745 <2e-16 ***\ntopicsjudiciary - 10.6673 0 .3601 -29.626 <2e-16 ***\ntopicslegislation - 2.6961 0 .2872 -9.387 <2e-16 ***\ntopicslgbtq - 3.7130 0 .2968 -12.512 <2e-16 ***\ntopicsmilitary - 1.7091 0 .3659 -4.671 3 .00e-06 ***\ntopicsmurder_investigations - 10.1161 0 .3820 -26.479 <2e-16 ***\ntopicsolympics - 4.0811 0 .2735 -14.922 <2e-16 ***\ntopicspalestine-israel - 5.9225 0 .4674 -12.670 <2e-16 ***\ntopicspresidential_debates 1.2563 0 .2735 4 .594 4 .36e-06 ***\ntopicsprosecution 3.5562 0 .2976 11 .951 <2e-16 ***\ntopicsprotests 0.9140 0 .2619 3 .490 0 .000483 ***\ntopicsracism - 5.7000 0 .2748 -20.739 <2e-16 ***\ntopicsunvaccination 0.4463 0 .3351 1 .332 0 .182905\ntopicsUS_and_Russia_politics 3.5337 0 .2848 12 .408 <2e-16 ***\ntopicsvaccination - 2.1084 0 .2704 -7.797 6 .38e-15 ***\ntopicswildfires/disasters - 1.1644 0 .4571 -2.547 0 .010862 *\nTable 10: Results of Regression Analysis for Bias Score\nDappendix 33\nd appendix\nEstimate Std.Error t-value Pr(>|t|) Significance Code\n(Intercept) - 0.718844 0 .020693 -34.739 <2e-16 ***\ndf$topicsafghanistan 0.036343 0 .029654 1 .226 0 .220358\ndf$topicsantitrust_cases 0.087452 0 .023221 3 .766 0 .000166 ***\ndf$topicscelebrities - 0.003036 0 .021398 -0.142 0 .887187\ndf$topicscongress/senate 0.042377 0 .021856 1 .939 0 .052514 .\ndf$topicsconservatives - 0.056221 0 .022238 -2.528 0 .011468 *\ndf$topicsdisinformation/\nfraudulent_content - 0.035415 0 .032232 -1.099 0 .271875\ndf$topicseducation - 0.080678 0 .029883 -2.700 0 .006939 **\ndf$topicselection_day 0.025433 0 .021168 1 .201 0 .229572\ndf$topicselections - 0.011669 0 .026377 -0.442 0 .658198\ndf$topicsenvironmental_crisis - 0.005188 0 .025941 -0.200 0 .841492\ndf$topicsevangelists 0.111761 0 .025127 4 .448 8 .68e-06 ***\ndf$topicsfinance/stocks - 0.099194 0 .030082 -3.298 0 .000976 ***\ndf$topicshomelessness - 0.112866 0 .031372 -3.598 0 .000321 ***\ndf$topicshomicides - 0.115988 0 .054863 -2.114 0 .034506 *\ndf$topicsimmigrants 0.008054 0 .026265 0 .307 0 .759117\ndf$topicsimpeachment 0.094045 0 .022463 4 .187 2 .83e-05 ***\ndf$topicsjournalism 0.145899 0 .024138 6 .044 1 .50e-09 ***\ndf$topicsjudiciary 0.057248 0 .029240 1 .958 0 .050247 .\ndf$topicslegislation - 0.069192 0 .023325 -2.966 0 .003013 **\ndf$topicslgbtq 0.265186 0 .024098 11 .004 <2e-16 ***\ndf$topicsmilitary - 0.121548 0 .029713 -4.091 4 .30e-05 ***\ndf$topicsmurder_investigations - 0.030059 0 .031025 -0.969 0 .332604\ndf$topicsolympics 0.293715 0 .022210 13 .224 <2e-16 ***\ndf$topicspalestine-israel 0.133692 0 .037959 3 .522 0 .000428 ***\ndf$topicspresidential_debates 0.054516 0 .022208 2 .455 0 .014099 *\ndf$topicsprosecution 0.090331 0 .024164 3 .738 0 .000185 ***\ndf$topicsprotests 0.089419 0 .021266 4 .205 2 .61e-05 ***\ndf$topicsracism 0.390050 0 .022319 17 .476 <2e-16 ***\ndf$topicsunvaccination - 0.047811 0 .027212 -1.757 0 .078916 .\ndf$topicsUS_and_Russia_politics 0.025168 0 .023127 1 .088 0 .276484\ndf$topicsvaccination - 0.079463 0 .021960 -3.619 0 .000296 ***\ndf$topicswildfires/disasters - 0.099780 0 .037123 -2.688 0 .007193 **\nTable 11: Results of Regression Analysis for Hate Score\nEappendix 34\ne appendix\nEstimate Std.Error t-value Pr(>|t|) Significance Code\n(Intercept) - 0.012723 0 .027367 -0.465 0 .641986\ntopicsafghanistan 0.052133 0 .039218 1 .329 0 .183747\ntopicsantitrust_cases 0.036171 0 .030711 1 .178 0 .238878\ntopicscelebrities 0.314670 0 .028300 11 .119 <2e-16 ***\ntopicscongress/senate - 0.060500 0 .028905 -2.093 0 .036347 *\ntopicsconservatives 0.285604 0 .029411 9 .711 <2e-16 ***\ntopicsdisinformation/\nfraudulent_content - 0.121499 0 .042627 -2.850 0 .004369 **\ntopicseducation - 0.069458 0 .039521 -1.757 0 .078836 .\ntopicselection_day 0.099776 0 .027996 3 .564 0 .000365 ***\ntopicselections - 0.094984 0 .034885 -2.723 0 .006474 **\ntopicsenvironmental_crisis 0.121676 0 .034307 3 .547 0 .000390 ***\ntopicsevangelists 0.079675 0 .033231 2 .398 0 .016503 *\ntopicsfinance/stocks 0.040427 0 .039784 1 .016 0 .309554\ntopicshomelessness 0.038903 0 .041490 0 .938 0 .348429\ntopicshomicides - 0.014914 0 .072559 -0.206 0 .837145\ntopicsimmigrants - 0.112959 0 .034737 -3.252 0 .001147 **\ntopicsimpeachment - 0.007069 0 .029709 -0.238 0 .811935\ntopicsjournalism 0.067801 0 .031924 2 .124 0 .033684 *\ntopicsjudiciary - 0.033665 0 .038671 -0.871 0 .384002\ntopicslegislation - 0.069855 0 .030848 -2.264 0 .023546 *\ntopicslgbtq - 0.025934 0 .031871 -0.814 0 .415796\ntopicsmilitary 0.224745 0 .039297 5 .719 1 .07e-08 ***\ntopicsmurder_investigations 0.083122 0 .041031 2 .026 0 .042784 *\ntopicsolympics - 0.162072 0 .029374 -5.518 3 .44e-08 ***\ntopicspalestine-israel - 0.109092 0 .050202 -2.173 0 .029775 *\ntopicspresidential_debates 0.046512 0 .029371 1 .584 0 .113286\ntopicsprosecution - 0.015578 0 .031958 -0.487 0 .625928\ntopicsprotests - 0.011998 0 .028125 -0.427 0 .669665\ntopicsracism 0.100012 0 .029517 3 .388 0 .000704 ***\ntopicsunvaccination 0.111608 0 .035988 3 .101 0 .001928 **\ntopicsUS_and_Russia_politics 0.048912 0 .030586 1 .599 0 .109789\ntopicsvaccination - 0.010406 0 .029043 -0.358 0 .720109\ntopicswildfires/disasters 0.097399 0 .049097 1 .984 0 .047278 *\nTable 12: Results of Regression Analysis for Sentiment Score\nFappendix 35\nf appendix\nFigure 3: Q-Q Plot for Hate Score Model\nFigure 4: Q-Q Plot for Sentiment Score Model\nREFERENCES 36\nreferences\nAgrawal, A., Fu, W., & Menzies, T. ( 2018 ). What is wrong with topic\nmodeling? and how to fix it using search-based software engineering.\nInformation and Software Technology ,98,74\u201388.\nAlnusyan, R., Almotairi, R., Almufadhi, S., Shargabi, A. A., & Alshobaili, J.\n(2020 ). A semi-supervised approach for user reviews topic modeling\nand classification. In 2020 international conference on computing and\ninformation technology (iccit- 1441 )(pp.1\u20135).\nAlSumait, L., Barbar\u00e1, D., Gentle, J., & Domeniconi, C. ( 2009 ). Topic signif-\nicance ranking of lda generative models. In Joint european conference\non machine learning and knowledge discovery in databases (pp.67\u201382).\nAngelov, D. ( 2020 ). Top 2vec: Distributed representations of topics. arXiv\npreprint arXiv: 2008 .09470 .\nArd\u00e8vol-Abreu, A., & Gil de Z\u00fa\u00f1iga, H. ( 2017 ). Effects of editorial media\nbias perception and media trust on the use of traditional, citizen, and\nsocial media news. Journalism & mass communication quarterly ,94(3),\n703\u2013724.\nBarber\u00e1, P ., Jost, J. T., Nagler, J., Tucker, J. A., & Bonneau, R. ( 2015 ).\nTweeting from left to right: Is online political communication more\nthan an echo chamber? Psychological science ,26(10),1531 \u20131542 .\nBernhardt, D., Krasa, S., & Polborn, M. ( 2008 ). Political polarization and\nthe electoral effects of media bias. Journal of Public Economics ,92(5-6),\n1092 \u20131104 .\nBlei, D. M. ( 2012 ). Probabilistic topic models. Communications of the ACM ,\n55(4),77\u201384.\nBlei, D. M., Ng, A. Y., & Jordan, M. I. ( 2003 ). Latent dirichlet allocation.\nJournal of machine Learning research ,3(Jan), 993\u20131022 .\nChang, J., Gerrish, S., Wang, C., Boyd-Graber, J., & Blei, D. ( 2009 ). Reading\ntea leaves: How humans interpret topic models. Advances in neural\ninformation processing systems ,22.\nCruce, T. M. ( 2009 ). A note on the calculation and interpretation of the\ndelta-p statistic for categorical independent variables. Research in\nHigher Education ,50(6),608\u2013622.\nD\u2019Alessio, D., & Allen, M. ( 2000 ). Media bias in presidential elections: A\nmeta-analysis. Journal of communication ,50(4),133\u2013156.\nDave, K., Lawrence, S., & Pennock, D. M. ( 2003 ). Mining the peanut gallery:\nOpinion extraction and semantic classification of product reviews. In\nProceedings of the 12th international conference on world wide web (pp.\n519\u2013528).\nDavidson, T., & Bhattacharya, D. ( 2020 ). Examining racial bias in an\nonline abuse corpus with structural topic modeling. arXiv preprint\nREFERENCES 37\narXiv: 2005 .13041 .\nDavidson, T., Warmsley, D., Macy, M., & Weber, I. ( 2017 ). Automated\nhate speech detection and the problem of offensive language. In\nProceedings of the international aaai conference on web and social media\n(Vol. 11, pp. 512\u2013515).\nDehler-Holland, J., Schumacher, K., & Fichtner, W. ( 2021 ). Topic modeling\nuncovers shifts in media framing of the german renewable energy\nact. Patterns ,2(1),100169 .\nEgger, R., & Yu, J. ( 2022 ). A topic modeling comparison between lda,\nnmf, top 2vec, and bertopic to demystify twitter posts. Front. Sociol. 7:\n886498 . doi: 10.3389 /fsoc.\nEnevoldsen, K. C., & Hansen, L. ( 2017 ). Analysing political biases in\ndanish newspapers using sentiment analysis. Journal of Language\nWorks-Sprogvidenskabeligt Studentertidsskrift ,2(2),87\u201398.\nGhasiya, P ., & Okamura, K. ( 2021 ). Investigating covid- 19news across\nfour nations: A topic modeling and sentiment analysis approach. Ieee\nAccess ,9,36645 \u201336656 .\nGomez-Zara, D., Boon, M., & Birnbaum, L. ( 2018 ). Who is the hero, the vil-\nlain, and the victim? detection of roles in news articles using natural\nlanguage techniques. In 23rd international conference on intelligent user\ninterfaces (pp.311\u2013315).\nGrove, J., Ripke, S., Als, T. D., Mattheisen, M., Walters, R. K., Won, H.,\n. . . others ( 2019 ). Identification of common genetic risk variants for\nautism spectrum disorder. Nature genetics ,51(3),431\u2013444.\nGunther, A. C., & Liebhart, J. L. ( 2006 ). Broad reach or biased source?\ndecomposing the hostile media effect. Journal of Communication ,56(3),\n449\u2013466.\nHamborg, F., Donnay, K., Merlo, P ., et al. ( 2021 ). Newsmtsc: a dataset for\n(multi-) target-dependent sentiment classification in political news\narticles..\nHatzivassiloglou, V ., & McKeown, K. ( 1997 ). Predicting the semantic\norientation of adjectives. In 35th annual meeting of the association for\ncomputational linguistics and 8th conference of the european chapter of the\nassociation for computational linguistics (pp.174\u2013181).\nHofman, R., & Van Oostendorp, H. ( 1999 ). Cognitive effects of a structural\noverview in a hypertext. British Journal of Educational Technology ,30(2),\n129\u2013140.\nKrouska, A., Troussas, C., & Virvou, M. ( 2016 ). The effect of preprocessing\ntechniques on twitter sentiment analysis. In 2016 7 th international\nconference on information, intelligence, systems & applications (iisa) (pp.\n1\u20135).\nKwon, K. H., Chadha, M., & Wang, F. ( 2019 ). Proximity and networked\nREFERENCES 38\nnews public: Structural topic modeling of global twitter conversations\nabout the 2017 quebec mosque shooting. International Journal of\nCommunication ,13,24.\nLazaridou, K., & Krestel, R. ( 2016 ). Identifying political bias in news\narticles. Bulletin of the IEEE TCDL ,12.\nLe, Q., & Mikolov, T. ( 2014 ). Distributed representations of sentences\nand documents. In International conference on machine learning (pp.\n1188 \u20131196 ).\nLee, E.-J. ( 2012 ). That\u2019s not the way it is: How user-generated comments\non the news affect perceived media bias. Journal of Computer-Mediated\nCommunication ,18(1),32\u201345.\nLee, E.-J., & Jang, Y. J. ( 2010 ). What do others\u2019 reactions to news on internet\nportal sites tell us? effects of presentation format and readers\u2019 need\nfor cognition on reality perception. Communication research ,37(6),\n825\u2013846.\nLi, C., Wang, H., Zhang, Z., Sun, A., & Ma, Z. ( 2016 ). Topic modeling\nfor short texts with auxiliary word embeddings. In Proceedings of\nthe39th international acm sigir conference on research and development in\ninformation retrieval (pp.165\u2013174).\nLiu, L., Tang, L., Dong, W., Yao, S., & Zhou, W. ( 2016 ). An overview of topic\nmodeling and its current applications in bioinformatics. SpringerPlus ,\n5(1),1\u201322.\nLu, H., Caverlee, J., & Niu, W. ( 2015 ). Biaswatch: A lightweight system for\ndiscovering and tracking topic-sensitive opinion bias in social media.\nInProceedings of the 24th acm international on conference on information\nand knowledge management (pp.213\u2013222).\nMa, P ., Zeng-Treitler, Q., & Nelson, S. J. ( 2021 ). Use of two topic modeling\nmethods to investigate covid vaccine hesitancy. In 14th international\nconference on ict, society, and human beings, ict 2021 ,18th international\nconference on web based communities and social media, wbc 2021 and\n13th international conference on e-health, eh 2021 -held at the 15th multi-\nconference on computer science and information systems, mccsis 2021 (pp.\n221\u2013226).\nMcCarthy, J., Titarenko, L., McPhail, C., Rafail, P ., & Augustyn, B. ( 2008 ).\nAssessing stability in the patterns of selection bias in newspaper\ncoverage of protest during the transition from communism in belarus.\nMobilization: An International Quarterly ,13(2),127\u2013146.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. ( 2013 ). Efficient estimation of\nword representations in vector space. arXiv preprint arXiv: 1301 .3781 .\nMin, K.-B., Song, S.-H., Min, J.-Y., et al. ( 2020 ). Topic modeling of social\nnetworking service data on occupational accidents in korea: Latent\ndirichlet allocation analysis. Journal of medical internet research ,22(8),\nREFERENCES 39\ne19222 .\nNarravula, G. ( 2021 ). Text embedding based topic modeling on noisy\nhistorical drilling data.\nNewman, N., Fletcher, R., Schulz, A., Andi, S., Robertson, C. T., & Nielsen,\nR. K. ( 2021 ). Reuters institute digital news report 2021 .Reuters\nInstitute for the Study of Journalism .\nPanichella, A. ( 2021 ). A systematic comparison of search-based approaches\nfor lda hyperparameter tuning. Information and Software Technology ,\n130,106411 .\nPinto, S., Albanese, F., Dorso, C. O., & Balenzuela, P . ( 2019 ). Quantifying\ntime-dependent media agenda and public opinion by topic modeling.\nPhysica A: Statistical Mechanics and its Applications ,524,614\u2013624.\nRecasens, M., Danescu-Niculescu-Mizil, C., & Jurafsky, D. ( 2013 ). Linguistic\nmodels for analyzing and detecting biased language. In Proceedings\nof the 51st annual meeting of the association for computational linguistics\n(volume 1: Long papers) (pp.1650 \u20131659 ).\nRizvi, R. F., Wang, Y., Nguyen, T., Vasilakes, J., Bian, J., He, Z., & Zhang,\nR. (2019 ). Analyzing social media data to understand consumer\ninformation needs on dietary supplements. Studies in health technology\nand informatics ,264,323.\nR\u00f6der, M., Both, A., & Hinneburg, A. ( 2015 ). Exploring the space of\ntopic coherence measures. In Proceedings of the eighth acm international\nconference on web search and data mining (pp.399\u2013408).\nSaez-Trumper, D., Castillo, C., & Lalmas, M. ( 2013 ). Social media news\ncommunities: gatekeeping, coverage, and statement bias. In Proceed-\nings of the 22nd acm international conference on information & knowledge\nmanagement (pp.1679 \u20131684 ).\nSalton, G. ( 1983 ). Some research problems in automatic information\nretrieval. In Acm sigir forum (Vol. 17, pp. 252\u2013263).\nSchnauber-Stockmann, A., Meier, A., & Reinecke, L. ( 2018 ). Procrastination\nout of habit? the role of impulsive versus reflective media selection\nin procrastinatory media use. Media Psychology ,21(4),640\u2013668.\nSheshadri, K., Hang, C.-W., & Singh, M. ( 2018 ). The causal link between\nnews framing and legislation. arXiv preprint arXiv: 1802 .05768 .\nSpinde, T. ( 2021 ). An interdisciplinary approach for the automated de-\ntection and visualization of media bias in news articles. In 2021\ninternational conference on data mining workshops (icdmw) (pp. 1096 \u2013\n1103 ).\nSpinde, T., Hamborg, F., & Gipp, B. ( 2020 ). An integrated approach\nto detect media bias in german news articles. In Proceedings of the\nacm/ieee joint conference on digital libraries in 2020 (pp.505\u2013506).\nStevens, K., Kegelmeyer, P ., Andrzejewski, D., & Buttler, D. ( 2012 ). Ex-\nREFERENCES 40\nploring topic coherence over many models and many topics. In\nProceedings of the 2012 joint conference on empirical methods in natural\nlanguage processing and computational natural language learning (pp.\n952\u2013961).\nSunstein, C. R. ( 2002 ). The law of group polarization, 10jpol. PHIL ,175,\n179\u201380.\nThornton, B. ( 2013 ). A brief history of media bias. Defining Ideas: A Hoover\nInstitution Journal .\nT\u00f6rnberg, A., & T\u00f6rnberg, P . ( 2016 ). Muslims in social media discourse:\nCombining topic modeling and critical discourse analysis. Discourse,\nContext & Media ,13,132\u2013142.\nVayansky, I., & Kumar, S. A. ( 2020 ). A review of topic modeling methods.\nInformation Systems ,94,101582 .\nWaseem, Z., & Hovy, D. ( 2016 ). Hateful symbols or hateful people?\npredictive features for hate speech detection on twitter. In Proceedings\nof the naacl student research workshop (pp.88\u201393).\nWong, F. M. F., Tan, C. W., Sen, S., & Chiang, M. ( 2013 ). Quantifying\npolitical leaning from tweets and retweets. In Proceedings of the\ninternational aaai conference on web and social media (Vol. 7, pp.640\u2013649).\nZaller, J. R., et al. ( 1992 ).The nature and origins of mass opinion . Cambridge\nuniversity press.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "COMPARISON OF TOPIC MODELING ALGORITHMS ON NEWS ARTICLES", "author": ["ZS FERT"], "venue": "NA", "pub_year": "NA", "abstract": "In today\u2019s society, with the impact of technological developments, all information is shared  through social media. Facebook and Twitter are the primary outlets. Hence, media outlets also"}, "filled": false, "gsrank": 185, "pub_url": "http://arno.uvt.nl/show.cgi?fid=160878", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:g2MIUa1tS-gJ:scholar.google.com/&output=cite&scirp=184&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D180%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=g2MIUa1tS-gJ&ei=JbWsaMu1E-HUieoP9LKZ6AI&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:g2MIUa1tS-gJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "http://arno.uvt.nl/show.cgi?fid=160878"}}, {"title": "How coordinated link sharing behavior and partisans' narrative framing fan the spread of COVID-19 misinformation and conspiracy theories", "year": "2022", "pdf_data": "Vol.:(0123456789)1 3Social Network Analysis and Mining (2022) 12:118 \nhttps://doi.org/10.1007/s13278-022-00948-y\nORIGINAL ARTICLE\nHow coordinated link sharing behavior and\u00a0partisans\u2019 narrative \nframing fan the\u00a0spread of\u00a0COVID\u201119 misinformation and\u00a0conspiracy \ntheories\nAnatoliy\u00a0Gruzd1 \u00a0\u00b7 Philip\u00a0Mai1 \u00a0\u00b7 Felipe\u00a0Bonow\u00a0Soares1 \nReceived: 2 March 2022 / Revised: 26 July 2022 / Accepted: 28 July 2022 / Published online: 20 August 2022 \n\u00a9 The Author(s) 2022\nAbstract\nThis study examines the presence and role of Coordinated Link Sharing Behavior (CLSB) on Facebook around the \u201cAmerica\u2019s \nFrontline Doctors\u201d press conference, and the promotion of several unproven conspiracy theories including the false assertion \nthat hydroxychloroquine is a \u201ccure\u201d for COVID-19 by Dr. Stella Immanuel, one of the doctors who took part in the press \nconference. We collected 7,737 public Facebook posts mentioning Stella Immanuel using CrowdTangle and then applied the \nspecialized program CooRnet to detect CLSB among Facebook public pages, groups and verified profiles. Finally, we used \na mixed-method approach consisting of both network and content analysis to examine the nature and scope of the detected \nCLSB. Our analysis shows how Facebook accounts engaged in CLSB to fuel the spread of misinformation. We identified a \ncoalition of Facebook accounts that engaged in CLSB to promote COVID-19 related misinformation. This coalition included \nUS-based pro-Trump, QAnon, and anti-vaccination accounts. In addition, we identified Facebook accounts that engaged \nin CLSB in other countries, such as Brazil and France, that primarily promoted hydroxychloroquine, and some accounts in \nAfrican countries that criticized the government's pandemic response in their countries.\nKeywords Coordinated link sharing behavior\u00a0\u00b7 Covid-19\u00a0\u00b7 Misinformation\u00a0\u00b7 Conspiracy theories\u00a0\u00b7 Social media\u00a0\u00b7 Facebook\n1 Introduction\nThe COVID-19 pandemic is proving to be a powerful ral-\nlying cry for conspiracy theorists and fringe groups from \naround the world (Allington et\u00a0al. 2021; Bertin et\u00a0al. 2020; \nBruns et\u00a0al. 2020). It has brought together everyone from \nwellness influencers, to anti-vaxxers, to anti-immigrant \ngroups to white supremacists. This paper will examine one \nexample of this trend. In July of 2020, a group of doctors \nwho called themselves \u201cAmerica\u2019s Frontline Doctors,\u201d con-\nvened a press conference in Washington DC to tout several \nunproven conspiracy theories about COVID-19, including the false assertion that hydroxychloroquine is a \u201ccure\u201d for \nCOVID-19. Dr. Stella Immanuel, one of the doctors on the \npanel and is the key figure of our case study, made a number \nof statements in favor of using hydroxychloroquine to treat \nCOVID-19, citing her own experience of using the drug to \ntreat patients. She also called in question several trials show -\ning the drug\u2019s ineffectiveness against COVID-19 because, \nin her words, they were done by \u201cfake science.\u201d This is the \nsame antimalarial drug promoted by President Trump in \nearly 2020 (Grady et\u00a0al. 2020).\nThe live stream and video recordings of the press con-\nference went viral on social media and were watched by \nmillions of people around the world on Facebook, You-\nTube, Twitter and other social media platforms (Passan-\ntino and Darcy 2020). A few hours after the press con-\nference was live streamed, social media platforms began \nfact-checking and removing videos of the press conference \nfrom their platforms for sharing claims about hydroxychlo-\nroquine which were against existing medical evidence at \nthat time (Gallagher 2020; Jacqueline 2020). The press \nconference and the subsequent viral spread of its video \nrecordings across social media is an illustrative example of  * Felipe Bonow Soares \n fbonowsoares@ryerson.ca\n Anatoliy Gruzd \n gruzd@ryerson.ca\n Philip Mai \n philip.mai@ryerson.ca\n1 Ted Rogers School of\u00a0Management, Social Media Lab, \nToronto Metropolitan University, Toronto, Canada\n Social Network Analysis and Mining (2022) 12:118\n1 3 118 Page 2 of 12\nhow COVID-related misinformation can spread and reach \nmillions in the matter of hours and even minutes.\nCentral to our case study, Dr. Stella Immanuel rose to \nprominence after her statements in support of hydroxy -\nchloroquine received praise from President Trump, and \nafter it was reported that she had a history of attributing \nmedical conditions to non-scientific causes such as witches \nand demons (Smith 2020). Dr. Immanuel quickly became \nthe focus of media attention and a popular topic of discus-\nsion across social media platforms. Considering Dr. Imma-\nnuel\u2019s newly discovered notoriety, we decided to examine \nwho on Facebook (the largest social media platform in the \nworld) mentioned the doctor immediately after the press \nconference and whether these users used this as an oppor -\ntunity to push and amplify COVID-19 misinformation in \na coordinated way.\nSpecifically, we examined data to look for online actors \nwho were engaged in coordinated links sharing behav -\nior (CLSB), by sharing the same links within a few min-\nutes or even seconds apart. The reason for focusing on \nCLSB is because CLSB and coordinated sharing behavior \n(more broadly) have shown to be linked to the propaga-\ntion of problematic content such as misinformation and \nconspiracy theories (Graham et\u00a0al. 2020; Giglietto et\u00a0al. \n2020b, 2020c). Furthermore, Papakyriakopoulos et\u00a0al.\u00a0  \n( 2020 ) showed that highly active coordinated accounts \ncan manipulate political discussions and create bias in rec-\nommender systems to make social media content appear \nto be more popular than it actually is.\nWhile this type of behavior is often associated with \ninauthentic accounts such as bots and fake profiles, \n\u2018authentic\u2019 accounts may also engage in coordinated \nbehavior (Nizzoli et\u00a0al. 2021). For example, harmful \nauthentic accounts such as online trolls may engage in a \ncoordinated action to spread hate speech and attack indi-\nviduals or groups with whom they disagree (Bradshaw \nand Howard 2017). In one study, Tandoc et\u00a0al. (2021) \ndocumented how women journalists were systematically \nharassed on social media by so-called \u201ctroll armies.\u201d\nAlthough research on misinformation is a rapidly \ngrowing field (Righetti 2021), the detection and analy -\nsis of coordinated sharing behavior on social media is \nan understudied area. This is mostly due to the lack of \ndata and tools to detect coordinated behavior on social \nmedia automatically and reliably. In this study, we used a \nrecently developed, specialized R program called CooR -\nnet designed to detect CLSB among Facebook entities \n(Giglietto et\u00a0al. 2020a). By using CooRnet to analyze and \ndetect CLSB, this paper offers empirical evidence to fur -\nther understand the practice of CLSB in the context of \nCOVID-19 discussions and link sharing on Facebook.\nThe following section will review the previous work in \nthis area as it pertains to our research questions.2  Literature review and\u00a0research questions\n2.1  Coordinated link sharing behavior (CLSB)\nMost studies that have explored coordinated sharing \nbehavior have been focused on political events. Giglietto \net\u00a0al. (2020b) analyzed Facebook posts about the 2018 Ital-\nian general election and 2019 European election and dem-\nonstrated that rapid sharing of URLs by the same group \nof entities was associated with coordinated and inauthen-\ntic behavior on Facebook. The authors identified that the \nFacebook accounts engaged in CLSB aimed to manipulate \nthe media and public opinion during the elections. Simi-\nlarly, Nizzoli et\u00a0al. (2021) analyzed Twitter data from the \n2019 UK general election and found that many coordi-\nnated networks included a higher degree of automation \nand several accounts that were later suspended. Yu (2021) \nanalyzed Facebook posts about the 2019 Philippine Mid-\nterm Elections and found that most of the media (URLs, \nphotos, and videos) shared in a coordinated manner were \nno longer available (posts likely removed by Facebook, \nand links to websites that no longer exist), suggesting that \ncoordinated accounts were engaged in sharing problematic \ncontent.\nOther investigations of CLSB have focused on COVID-\n19. Graham et\u00a0al. ( 2020 ) explored coordinated behavior \non Twitter in discussions about COVID-19. The authors \nfound clusters of coordinated accounts promoting the con-\nspiracy theory that COVID-19 was engineered as a bio-\nweapon by China. These coordinated accounts were mostly \nlinked to users expressing conservative and far-right views \n(Pro-Trump, QAnon, and/or Republican). In another study, \nAyers et\u00a0al. (2021) analyzed CLSB in discussions about \nface masks on Facebook. The authors found a similar \ntrend that coordinated accounts contributed to the spread \nof COVID-19 related conspiracy theories on Facebook.\nConsidering the evidence that at least some COVID-19 \nrelated conspiracy theories were shared on social media in \na coordinated way, we ask the first question:\nRQ1: Is there evidence of Coordinated Link Sharing \nBehavior (CLSB) on Facebook related to discussions \nabout Dr. Stella Immanuel after the press conference with \n\u201cAmerica\u2019s Frontline Doctors\u201d?\n2.2  Political polarization and\u00a0narrative framing\nIn addition to exploring a potential link between CLSB \nand sharing of COVID-19 misinformation and conspiracy \ntheories, we wanted to examine whether CLSB might \nhave been used to spread politically motivated narratives, \npotentially leading to a high level of political polarization. \nSocial Network Analysis and Mining (2022) 12:118 \n1 3 Page 3 of 12 118\nIndeed, the spread of problematic content online is often \ninfluenced by political polarization, fueled by partisan \nmedia outlets. Partisan outlets are digital media that \nprovide specific political takes to frame narratives and \ncounter-narratives to support their agendas (Recuero et\u00a0al. \n2020; Kalsnes and Larsson 2021). For example, Recuero \net\u00a0al. (2020) analyzed discussions about the 2018 Bra-\nzilian elections on Twitter and found that as polarization \nbetween users with opposing points of view increased, so \ndid the influence of partisan outlets linked to misinforma-\ntion spread.\nIn some cases, the radicalization of partisan groups \ncan create what Benkler et\u00a0al. (2018) called asymmetric \npolarization. The authors studied information consump -\ntion and news sharing during the 2018 US Presidential \nElection and found that groups on the right of the political \nspectrum were more likely to share content from partisan \noutlets. Therefore, the idea of asymmetric polarization is \nused to describe a polarized environment in which one side \nis strongly associated with misinformation spread. Most \nstudies identified that far-right groups are particularly \nlinked to sharing misinformation (Benkler et\u00a0al. 2018; \nRecuero et\u00a0al. 2020; Kalsnes and Larsson 2021). Neverthe-\nless, even in the context of asymmetric polarization driven \nby far-right groups, some left-leaning groups also rely on \npartisan outlets and share misinformation to support their \npolitical narratives (Recuero et\u00a0al. 2020).\nThese asymmetries are also present in the context of \nthe COVID-19 pandemic. In the USA, republicans were \nmore likely to downplay COVID-19 and believe in mis-\ninformation about the pandemic (Calvillo et\u00a0al. 2020). \nPartisan motivations, in particular support for Donald \nTrump, and conservative media consumption were also \nfound among the reasons for Americans to more likely \nbelieve in COVID-19 conspiracy theories (Uscinski et\u00a0al. \n2020; Stecula and Pickup 2021).\nIn Brazil, Recuero et\u00a0al. (2022) identified that links con-\ntaining pandemic-related misinformation were mostly shared \nby right-wing Facebook pages and groups, while fact-check -\ning links were mostly shared by left-wing pages and groups. \nBrazilians with a right-wing ideology were also found to \nbe more likely to believe in COVID-19 related misinforma-\ntion (Rossini and Kalogeropoulos 2021). In France, Ward \net\u00a0al. ( 2020) found that vaccine hesitancy was associated \nwith political radicalization, as both far-left and far-right \nindividuals were more likely to reject COVID-19 vaccines.\nConsidering, the impact of political polarization on how \nonline actors frame discussions around the pandemic, we \nask:\nRQ2: What is the role of political polarization and par -\ntisan outlets in CLSB? Is CLSB a tactic used by either or \nboth right- and left-wing political actors on Facebook to spread narrative and counter-narrative in support of their \npolitical views?\n2.3  International reach\nThe rapid and massive spread of COVID-19 related mis-\ninformation on social media is a worldwide problem \n(Tangcharoensathien et\u00a0al. 2020; Zarocostas 2020). This \nis because misinformation can easily travel across physi-\ncal borders (Zarocostas 2020; Bridgman et\u00a0al. 2021). For \nexample,\u00a0Nsoesie et\u00a0al. (2020) found evidence that some \nmisinformation topics spread similarly across several \ncountries, such as the conspiracy theory around 5G and \nthe promotion of natural treatments and unproven drugs \nfor COVID-19. In the analysis of a large dataset of tweets \nabout popular pandemic-related misinformation and con-\nspiracy theories, Bridgman et\u00a0al. (2021) demonstrated how \nCOVID-19 related misinformation posted by US-based \naccounts on Twitter spread to Canada, where Canadian \nusers were more likely to encounter misinformation origi-\nnated in the USA than shared by Canadian sources.\nCOVID-19 related misinformation originating in the \nUSA also easily crossed in Brazil. After Donald Trump \nsupported the use of hydroxychloroquine, Brazilian Presi-\ndent Jair Bolsonaro started promoting the unproven drug \nfor COVID-19 (Casar\u00f5es and Magalh\u00e3es 2021). Similarly, \na conspiracy theory that encouraged people to take photos \nand videos of empty hospitals to prove that the pandemic \nwas a hoax started in the USA, but later spread to other \ncountries, including Brazil (Gruzd and Mai 2020).\nIn yet another example of cross-border misinformation \npropagation, The Epoch Times, a US-based far-right media \noutlet, partnered with Tierra Pura, an Argentinian partisan \noutlet, to translate false stories about COVID-19 to Span -\nish and Portuguese from English amplifying this content \nto Latin America and part of Europe (Miguel 2021).\nAlthough the USA is one of the main sources of \nCOVID-19 related misinformation globally, actors \nfrom other countries have also shown to be effective in \nexporting misinformation and conspiracy theories inter -\nnationally. Dotto and Cubbon (2021) found that foreign \nanti-vaccine narratives and conspiracy theories reached \nonline social networks in West African countries. While \na large share of this social media content was in English \nand popularized in the USA, the authors also identified \nmisinformation in other languages, such as French and \nRussian, and originated in other countries. There are also \ncases of operations controlled by foreign actors, such as \nthe evidence of Russian, Chinese, Turkish and Iranian \noutlets sharing conspiracy theories and promoting their \npolitical agendas in Germany, France, and Spain (Rebello \net\u00a0al. 2020).\n Social Network Analysis and Mining (2022) 12:118\n1 3 118 Page 4 of 12\nTo examine the potential presence and impact of cross-\nborder sharing of COVID-19 misinformation and conspiracy \ntheories as related to our case study, we ask:\nRQ3: Was the CLSB in the studied case constrained to \nthe US-based accounts only? Or, was there a spillover effect, \nwhere COVID-19 misinformation originating in the USA \nwas later picked up and shared by Facebook accounts in \nother countries?\n3  Method\nThe data was collected using CrowdTangle, a Meta-owned \npublic content discovery and analysis tool. CrowdTangle \ntracks public posts published by influential Pages (with more \nthan 25\u00a0K likes or followers), Groups (with 95\u00a0K members \nfor non-US-based groups or 2\u00a0K members for US-based \ngroups), and all verified profiles on Facebook (Fraser 2022). \nSince Dr. Immanuel was at the core of this case study, we \nused her full name \u201cStella Immanuel\u201d as a search query. In \ntotal, we retrieved 7,737 public posts shared between July \n27 (the date of the press conference) and July 29 (2 days \nafter the press conference) in 2020. Only public posts not \nyet deleted by users or removed by the platforms at the time \nof data collection were included in our analysis.\nAfter the data was collected, we used a specialized R \nprogram called CooRnet (Giglietto et\u00a0al. 2020a) to discover \nwhat URLs in our dataset were shared most often on Face-\nbook, who shared them and how fast. CooRnet is one of \nfew available tools to detect signs of possible coordinated \nbehavior on Facebook. Most approaches to detect coordi-\nnated behavior focus on the publication of the same content \nfrom different accounts in a very short time (Nizzoli et\u00a0al. \n2021; Yu 2021). For example, Graham et\u00a0al. (2020) looked \nat accounts that retweeted the same messages within one \nsecond of each other to detect bot-nets and then 1\u00a0min of \neach other to detect coordinated accounts in general. Simi-\nlarly, the approach proposed by Weber and Neumann (2021) \nrelies on a temporal analysis combined with other online \nbehaviors, such as retweeting the same message, using the \nsame hashtag, mentioning the same user, and/or sharing the \nsame URL.\nWe chose to use CooRnet because it has been applied \nand validated by academic studies and is a suitable tool \nto work with CrowdTangle data (Giglietto et\u00a0al. 2020b, \n2020c; Ayers et\u00a0al. 2021). One of the unique features of the \nprogram is that it detects when the same URL was shared \nacross multiple Facebook entities (pages, groups and verified \nprofiles) just seconds apart, a sign of potential coordina-\ntion. The main idea behind this approach is that while it is \nnot uncommon for a group of accounts to share the same \nURL(s), it is unlikely for them to do so within seconds. The \nkey to this approach is to determine a threshold of what is \u201cunusually\u201d rapid sharing of the same URL(s). To determine \nthis threshold, CooRnet extracted all unique URLs shared in \nour dataset (899) and separated out 10% of \u201cfastest\u201d shared \nURLs (i.e., URLs with the shortest time between the first \nand second time they were shared). The median time it took \nfor these 10% of the fastest shared URLs to reach 50% of the \ntotal number of shares is then used to set the threshold for \n\u201cunusually\u201d fast sharing. Based on our dataset, the threshold \nwas determined to be 71\u00a0s.\nNext, we used CooRnet to build a \u201ccoordinated\u201d link \nsharing network among Facebook entities from our data-\nset that met the threshold for engaging in \u201cunusually\u201d rapid \nsharing (< = 71\u00a0s) of the same URL(s). The resulting net-\nwork consisted of 64,252 ties connecting 1,390 Facebook \nentities (either a page or group) that shared the same link \nwithin a very short period of time, within 71\u00a0s as automati-\ncally estimated by CooRnet.\nWe used Gephi (Bastian et\u00a0al. 2009), a popular program \nfor social network analysis, to identify densely connected \nclusters of Facebook entities engaged in CLSB using a com-\nmunity detection algorithm.\nLastly, to understand the nature of the discovered clus-\nters, we manually examined the groups, pages and verified \nprofiles within each cluster. As part of this step, we manu-\nally verified if the Facebook entities in our dataset were still \navailable on the platform (18\u00a0months after the initial data \ncollection) and if they had changed their name or privacy \nsetting. We also manually reviewed the posts and content of \nthe links shared by the entities in the dataset, with a particu-\nlar focus on the most shared URLs within each major cluster \nof the resulting CLSB network.\n4  Results\n4.1  Is there evidence of\u00a0Coordinated Link \nSharing Behavior (CLSB) on\u00a0Facebook related \nto\u00a0discussions about\u00a0Dr. Stella Immanuel \nafter\u00a0the\u00a0press conference with\u00a0\u201cAmerica\u2019s \nFrontline Doctors\u201d?\nBased on the CooRnet analysis, we can positively answer \nour first research question that, yes, there was clear evidence \nof CLSB in the collected dataset. Specifically, we identified \na network of 1,390 Facebook entities engaged in coordinated \nlink sharing (Fig.\u00a0 1). Out of the 7,737 public posts shared \nby these entities as captured in our dataset, 2,484 (36.8%) \nwere shared in a coordinated manner, i.e., they met the 71-s \nthreshold for \u201cunusually\u201d fast sharing, as established by \nCooRnet.\nWhile connections between Facebook entities in the net-\nwork are not necessarily a sign of explicit coordination, the \ndiscovered linkages reveal clusters of entities that share similar \nSocial Network Analysis and Mining (2022) 12:118 \n1 3 Page 5 of 12 118\nviews and thus share and discuss similar links. Furthermore, \nthe fact that these entities share the same links within seconds \nsuggests that these are highly mobilized communities of users \nwho are paying attention to each other and to the news, and \nwhose members are ready and willing to disseminate informa-\ntion or misinformation on a moment\u2019s notice. In the network visualization, these clusters are shown as densely connected \ngroups of nodes and highlighted using different colors. The \nfollowing Sects. 4.2 and 4.3 will take a closer look at the enti-\nties and content shared by Facebook entities found within the \nseven largest clusters. These seven clusters contained 76% of \nall nodes in the network.\nFig. 1  Network of likely coordinated link sharing behavior among 1390 Facebook entities (The percentages in the legend \u201cModularity Class\u201d \nrepresent the percentage of nodes affiliated with each cluster)\n Social Network Analysis and Mining (2022) 12:118\n1 3 118 Page 6 of 12\n4.2  What is\u00a0the\u00a0role of\u00a0political polarization \nand\u00a0partisan outlets in\u00a0CLSB? Is CLSB a\u00a0tactic \nused by\u00a0either\u00a0or\u00a0both the\u00a0right\u2011 and\u00a0left\u2011wing \npolitical actors on\u00a0Facebook to\u00a0spread narrative \nand\u00a0counter\u2011narrative in\u00a0support of\u00a0their \npolitical views?\nWhen we explored the entities that comprise each cluster \n(as outlined below), the analysis revealed the involvement \nof groups from both sides of the political divide in the USA. \nThis indicates that CLSB is a tactic used by both right- and \nleft-wing actors.\nOn the right side of the political spectrum, Cluster 2  \nmostly consisted of pro-Trump groups such as \u201cTRUMP \n2020 KEEP AMERICA GREAT !\u201d, \u201cTrump's New Gen-\neration\u201d and \u201cDonald Trump 2020.\u201d In addition to sharing \nlinks to the press conference (live stream or video record-\ning), members of these groups also reposted Fox News \ncoverage of Trump's briefing the day after the \u201cWhite Coat \nSummit\u201d press conference, where the President defended \nthe pro-hydroxychloroquine doctor (Re 2020). Another \nfrequently shared source in this cluster was a story about \nthe press conference by the Gateway Pundit (Laila 2020), \nan extreme right-wing news and opinion website known to \nshare misinformation and conspiracy theories (mediabias-\nfactcheck.com). Cluster 6 has formed around groups advo-\ncating against mandatory vaccination, QAnon conspiracy \ntheory groups (Wendling 2021), and a fan group of Candace \nOwens, an influential pro-Trump conservative commenta-\ntor and political activist. Similar to accounts in Cluster 2, \nCluster 6 accounts shared links to the press conference and \nexpressed their support for hydroxychloroquine.\nOn the left side of the political spectrum, groups in Clus-\nter 1 and 4  like \u201cBlue Wave 2020,\u201d \u201cEVERYONE HATES \nTRUMP,\u201d \u201cJoe Biden For President\u201d as well as a number \nof \u201cOccupy\u201d groups also shared news about the press con-\nference, but mostly citing left-center media outlets like \nCNN (CNN 2020) and Huffington Post (Robins-Early et\u00a0al. \n2020). These media outlets reported on the event but mostly \nfocused on the danger of promoting health misinformation \nand President Trump\u2019s reaction to the event. Some articles \nfrom more partisan sites like The Daily Beast focused on Dr. Immanuel\u2019s previous \u201cbizarre\u201d claims about other medical \ntopics (Sommer 2020).\nTo gauge the level of potentially problematic content \nshared in a coordinated manner within each cluster, we fol-\nlowed the approach used by Yu (2021) and checked how \nmany entities and how many links were no longer available \n18\u00a0months after the initial data collection. One of the main \nreasons for accounts and content to disappear is because \nthey shared problematic content. This is largely due to \nFacebook\u2019s recent efforts to combat pandemic-related mis-\ninformation by taking action such as suspending accounts \nor blocking content in violation of their misinformation \npolicies. In addition, we paid particular attention to whether \nlinks that were shared within each of the US-centric clus-\nters led users to partisan media outlets or decentralized plat-\nforms, both implicated in helping to spread misinformation \nand were often used as a method to avoid the platform\u2019s \ncontent moderation policies, as noted in the literature review.\nTable\u00a0 1 provides the count and percentage of those Face-\nbook entities that have subsequently become unavailable. \nThe table also recorded whether and how many groups or \npages have changed their visibility setting to become private, \nwere renamed, or both. Twenty three percent of right-leaning \nFacebook entities are no longer available (Clusters 2 and 6), \nand about the same percentage (20.8%) of Facebook entities \non the left side of the political spectrum have also disap-\npeared (Clusters 1 and 4). This is in comparison to 17% of \nall entities that became unavailable in the whole network. \nSubject to future research, the similar levels of unavailable \nentities on both sides of the political spectrum may suggest \nthat Facebook might have been removing US-centric enti-\nties engaged in CLSB at about the same rate, irrespective \nof the content they shared. The percentage of unavailable \nentities is somewhat smaller in non-US clusters which will \nbe discussed in the next Sect.\u00a0 4.3.\nWhile the majority of Facebook entities engaged in CLSB \non both sides of the political spectrum remained available, \nwe can not say the same about the links they shared. As \nper the counts presented in Table\u00a0 2, we identified some key \ndifferences in the availability and types of content shared \nwithin each of the US-centric clusters. The left-wing entities \nshared links to both partisan outlets (55.7%) and mainstream \nTable 1  Availability and visibility status of the Facebook entities engaged in CLSB\nColors indicate a higher (green) or lower (yellow) percentage for each category\nSocial Network Analysis and Mining (2022) 12:118 \n1 3 Page 7 of 12 118\nmedia (22%). In contrast, the right-wing entities rarely \ndirected traffic to mainstream media (3.5%), instead most \nof their posts included links to partisan outlets (34.8%) as \nwell as to social media posts that are no longer available \n(43.2%). The latter is notable since unavailable content is \nlikely linked to misinformation, as social media platforms \nworked to remove false and misleading information related \nto the \u2018White Coat Summit\u2019 press conference (Gallagher \n2020; Jacqueline 2020). The right-wing entities also linked \nto the content shared on alternative platforms (6.4%), such as \ndecentralized video sharing sites like BitChute and D.Tube. \nThe strategy to move from mainstream to alternative plat-\nforms is often employed by far-right entities that had their \ncontent moderated and removed from mainstream platforms \n(Rauchfleisch and Kaiser 2021).\nIn short, we identified that both right- and left-wing enti-\nties in the USA were engaged in CLSB. However, the right-\nwing entities rarely directed traffic to mainstream media and \nshared proportionally more links to social media posts that \nwere no longer available (subsequently removed by either \nthe original poster or the platform) than the left-wing enti-\nties did.\n4.3  Was the\u00a0CLSB in\u00a0the\u00a0studied case constrained \nto\u00a0the\u00a0US\u2011based accounts only? Or, \nwas\u00a0there a\u00a0spillover effect, where\u00a0COVID\u201119 \nmisinformation originating in\u00a0the\u00a0USA was\u00a0later \npicked up\u00a0and\u00a0shared by\u00a0Facebook accounts \nin\u00a0other countries?\nThe answer to the final research question is also \u2018yes,\u2019 \nthree clusters in the network (Clusters 3, 5, 7) were formed \nmostly around entities from countries other than the USA. \nThis highlights the fact that we are living in an intercon-\nnected world and that social media makes it easier for \ninformation and misinformation originating in one country \nto spread quickly around the world. This is exactly what \nhappened with videos and posts from and about the \u201cWhite Coat Summit.\u201d Shortly after it was posted online, we see \nthis content was quickly adopted by sympathetic groups in \nother countries, especially those in Brazil (Cluster 3) and \nFrance (Cluster 5). Some accounts from African countries \nalso engaged in CLSB to spread links about the studied \ncase (Cluster 7).\nRelated to our analysis of Cluster 3, similar to the USA, \nBrazil had been struggling to reduce the number of new \nCOVID-19 cases when the data was collected in the sum-\nmer of 2020 (Lovelace Jr 2020 ). Like in the USA, the \ncountry\u2019s leader, President Jair Bolsonaro had been pro-\nmoting hydroxychloroquine to treat COVID-19, including \ntaking the drug himself (Porterfield 2020). As a result, \nmany pro-Bolsonaro Facebook groups shared the video \nfrom the \u2018White Coat Summit\u2019 press conference and criti-\ncized the left-leaning media for attacking the credibility \nof the doctors who participated in the press conference. \nWe further explored the links shared by Brazilian entities \non Facebook. Since Portuguese is the official language in \nBrazil, most links shared by entities found in the Brazilian \ncluster were in Portuguese. This indicates a strategy and \na willingness to expend resources to translate and contex-\ntualize English content for redistribution in Brazil. Nota-\nbly, around three-quarters of the posts shared within this \ncluster linked to stories from Brazilian partisan outlets. In \ncomparison to other clusters in the network, the Brazilian \ncluster had the fewest unavailable entities (5.3%\u2014Table\u00a0 1) \nand links (5.9%\u2014Table\u00a0 2), 18\u00a0months after the initial data \ncollection. Future research is needed to determine factors \nbehind this trend, but one possible explanation is related \nto the fact that the entities in the Brazilian cluster fre -\nquently shared links to external websites (such as parti-\nsan outlets), as opposed to other Facebook posts. While \nFacebook can remove or fact-check native content from \nthe platform, they cannot remove or take down content on \nexternal websites.\nBased on our analysis of Cluster 5, the press conference \nhas also been shared by many Facebook groups from France. Table 2  Types and availability of links shared in a coordinated way\nThe counts in the table refer to the total number of posts that include each type of link, not to the number of unique links in the dataset. Social \nmedia posts include posts on Facebook, tweets and videos on YouTube. When we reviewed the links in the dataset, we only identified social \nmedia posts that are no longer available. All links to partisan outlets, mainstream media, alternative platforms and other web pages were still \nactive 18\u00a0months after the initial data collection\n\u201cOther\u201d category includes links that could not be classified in the other categories, such as a link to the \u2018White Coat Summit\u2019 website or an aca-\ndemic study related to the use of hydroxychloroquine for COVID-19 treatment. Colors indicate a higher (green) or lower (yellow) percentage for \neach category\n Social Network Analysis and Mining (2022) 12:118\n1 3 118 Page 8 of 12\nThese groups have already been advocating for the use of \nhydroxychloroquine as a cure for COVID-19 and posting \nmessages in support of French professeur Didier Raoult who \nwas among the first to promote the anti-malaria drug to treat \nthe disease (Sayare 2020). Many of these groups also pro-\nmoted anti-mask and anti-vaccination policies.\nMost of the links shared by entities within the French \ncluster are no longer available (82%\u2014Table\u00a0 2). Interestingly, \n77.2% of the posts from these entities linked to a single \nFacebook post containing a video that is no longer avail-\nable. The French cluster was also the non-US cluster with \nthe most unavailable entities (14.4%\u2014Table\u00a0 1). Similar to \nthe US right-wing cluster, entities from the French cluster \nused alternative platforms like D.Tube and BitChute to host \nvideos and avoid the removal of the content (6.8%).\nCluster 7 identifies similar URL sharing patterns across \ngroups with membership in African countries like Nigeria, \nSouth Africa and Cameroon. Posts in this cluster often ques-\ntioned their government\u2019s response and suggested that their \ngovernments are keeping hydroxychloroquine to themselves \ndue to the limited supply to use it as a prophylactic treat -\nment. Similar to the French cluster, most of the posts in \nthis cluster linked to a Facebook post containing an una-\nvailable video (84.5%\u2014Table\u00a0 2). Additionally, 10% of the \nentities within this cluster are no longer available on Face-\nbook (Table\u00a0 1). The available posts in this cluster associate \nthe \u2018White Coat Summit\u2019 press conference to local contexts. \nThese included links to a Facebook post from a Nigerian \ninfluencer that shared a video of the press conference (8.5%) \nand to an entertainment website story that highlighted the \nCameroonian background of Dr. Stella Immanuel (7%).\n5  Discussion\nOur analysis of rapid link sharing behavior on Facebook \nreveals a coalition of communities around pro-Trump, \nQAnon and anti-vaccination groups that are ready to mobi-\nlize in unison at a moment's notice. Despite efforts by \nFacebook and other social media platforms to reactively \nfact check claims about COVID-19, such groups have been \neffective at employing a combination of strategies, includ -\ning CLSB, to propagate conspiracy theories and misinfor -\nmation in this area. Below is a summary of some of the \nobserved strategies used by these groups which challenge \nthe platforms\u2019 current approaches to managing misinforma-\ntion around the pandemic (Tangcharoensathien et\u00a0al. 2020; \nZarocostas 2020).\nCLSB was often associated with accounts and content \nthat were later deleted or removed . Speed is crucial in the \nprocess of removing misinformation online. Although a \nlarge share of Facebook entities and links are no longer available, entities that engaged in CLSB posted links to \nproblematic content hundreds of times before their removal \nor deletion. Furthermore, \u201ccoordinated\u201d posts received 24.8 \nshares on average on Facebook (a total of over 61 thou-\nsand shares on Facebook). This can create a cascade effect, \nas posts from coordinated entities are later shared by other \nFacebook accounts. Therefore, platforms must develop and \nimplement better tools and processes to add friction back \ninto their system to curb abuse associated with rapid, coor -\ndinated link sharing to reduce the spread of misinformation \nonline.\nCLSB is a tactic used by both sides of the political spec-\ntrum. While some previous studies found stronger associa-\ntions between CLSB and right-wing entities (Graham et\u00a0al. \n2020; Yu, 2021), our findings are in line with Nizzoli et\u00a0al. \n(2021), who also identified that both right- and left-wing \nentities engage in coordinated behavior. This indicates that \nentities from both sides of the political divide use CLSB \nto push partisan narratives and counter-narratives on social \nmedia, which might contribute to increased political polari-\nzation. This is particularly problematic in the context of a \npandemic when collective and organized action at the soci-\netal level is fundamental to reduce the spread of the virus \nand increase public compliance to protective measures.\nIn polarized contexts, misleading claims by country lead-\ners fueled misinformation spread. Therefore, a challenge \nfaced by social media platforms is when false and mislead-\ning claims are propagated by country leaders like President \nDonald Trump in the USA and President Jair Bolsonaro \nin Brazil. After all, it is hard to fact check someone who \nhas power to shut down a service. As we saw in our analy -\nsis, pro-Trump and pro-Bolsonaro groups were among the \nstrongest contributors in this link sharing network. In par -\nticular, the studied case was associated with the discussion \nabout hydroxychloroquine, an unproven drug for COVID-19 \nthat was promoted by both Trump and Bolsonaro (Casar\u00f5es \nand Magalh\u00e3es 2021).\nIn addition to political leaders, partisan outlets provided \ncontent to mobilize both sides. On the political right, parti-\nsan outlets like Breitbart News reach a receptive audience \namong political extreme right. On the political left, parti -\nsan sites like The Daily Beast shed light on the event while \noffering narratives to discredit doctors involved in the press \nconference. Narratives and counter-narratives propagated by \npartisan sites create an environment where misinformation \nand distrust strive (Recuero et\u00a0al. 2020). In such environ-\nments, fact checking may not work if it is viewed as a politi -\ncally motivated tool rather than a health advice (Shin and \nThorson 2017).\nIn terms of the international reach of misinformation, we \nidentified that what happens in the USA does not stay in \nthe USA. The strong presence of groups and pages from \ncountries other than the USA shows that many of the groups \nSocial Network Analysis and Mining (2022) 12:118 \n1 3 Page 9 of 12 118\nmentioned above are part of a loosely connected global net-\nwork of like-minded individuals who are taking a cue from \ntheir US counterparts and then using this shared narratives \nto propel their own agenda in their countries. This means \nthat any action against the spread of COVID-related misin-\nformation on social media has also to be a global response. \nAn example of such a global effort is a series of international \nmeetings and conferences on infodemic hosted by WHO in \n2020 and 2021, where practitioners and researchers dis-\ncussed interdisciplinary approaches to tackling the COVID-\n19 infodemic around the world (Lancet 2020).\nThe Brazilian cluster had the fewest entities and links that \nwere unavailable. A particular characteristic of this cluster \nis that most posts are linked to content in Portuguese, par -\nticularly stories from Brazilian partisan outlets. The number \nof unavailable entities from the French and African clusters \nwas also lower compared to the clusters from the USA. As \nproblematic actors are using translation to fuel misinfor -\nmation spread worldwide (Miguel 2021), platforms ought \nto find ways to mitigate the influence of problematic con-\ntent in Portuguese and other non-English languages. Some \nstrategies that we identified include partisan outlets sharing \nstories about the press conference in Portuguese and videos \nof Dr. Immanuel\u2019s talk with subtitles in Portuguese and in \nFrench. These strategies make the content more accessible \nto the local population, especially in the case of Brazil with \na relatively low English-proficiency in the country (Educa-\ntion First 2021).\nFinally, we identified strategies of decentralization and \noutsourcing of misinformation. Despite the efforts by the \nmainstream social media platforms, many of the videos from \nthe live-streamed press conference remain easily accessi-\nble via decentralized platforms like D.tube. This highlights \na challenge of combating the spread of COVID-19 misin-\nformation on platforms where there is no single entity that \nis responsible for curation of such content, and where the \nplatforms often rely on a distribution architecture like block -\nchain networks designed with replication and anti-deletion \npolicies in mind (Gruzd 2020). Any future action against \nmisinformation must account for these emerging technol-\nogies and not just focus on the mainstream social media \nplatforms.\nStudy Limitations. Our study has several limitations that \nmotivate future research and development in this area. First, \nCrowdTangle, Meta\u2019s platform that we used for data col -\nlection, monitors a limited number of entities on Facebook \nthat includes Pages with more than 25\u00a0K likes or followers, \nGroups with 95\u00a0K members for non-US-based groups or \n2\u00a0K members for US-based groups, and all verified profiles \n(Fraser 2022). Other entities might have been engaged in \ncoordinated behavior. However, we could not identify them \ndue to this data collection limitation.Second, CrowdTangle does not include the account \nnames of Facebook users that are posted in groups (metadata \nonly includes the name of the group or page). Therefore, we \nare not able to determine if specific users are posting the \nsame link across different groups (spammers). On top of \nthat, we cannot detect how many CLSB accounts are con-\ntrolled by multiple or a single entity. Accounts controlled \nby a single entity (\u201cpuppetmaster\u201d) are referred to as \u201csock \npuppet\u201d accounts, and are often used to manipulate online \ndiscussions (Kumar et\u00a0al. 2017). To explore this issue, we \nneed stronger collaboration between researchers and plat-\nforms, since social media platforms have more data that can \nbe used to detect sockpuppet accounts.\nThird, CooRnet, the library we used to detect CLSB, \nrelies on two metadata fields to identify coordination: 1) the \nlink shared and 2) the time difference between posts from \ndifferent Facebook entities. Although this approach can \ndetect signs of coordination, we cannot necessarily know for \nsure that all entities identified by CooRnet are coordinating \ntheir online action in an explicit manner. This is a limitation \njust to a certain extent, as partisan actors often follow the \nsame information sources and reshare content as soon as \nit becomes available. Therefore, they do not need to be in \nthe same room to agree on what and when to share content \nonline. Future research can further explore the notion of \ncoordination in this context.\n6  Conclusions\nOur analysis of posts mentioning Dr. Immanuel revealed that \nthere exists a US-based coalition of Facebook entities con-\nsisting of pro-Trump, QAnon and anti-vaccination accounts \non Facebook that are acting in concert and engaging in \nCLSB. These entities frequently shared links to content \nfrom conservative news outlets like Fox News and partisan \nwebsites like Breitbart News. Much of the content that they \nlink to are to stories that promoted unproven COVID-19 \ntreatments and conspiracies involving COVID-19.\nInterestingly, but not surprisingly, our analysis shows \nCLSB is a tactic also employed by entities on the left side \nof the political spectrum. Left-wing actors used online dis-\ncourse around Dr. Immanuel to mobilize their supporters \nand counter the misinformation. Anti-Trump and pro-Biden \npages and groups shared links to news about the danger of \npromoting health misinformation and President Trump\u2019s \nreaction to the video involving Dr. Immanuel by left-center \nmedia outlets like CNN and Huffington Post. Some also \nshared articles from more partisan sites like The Daily Beast \nwhich focused on Dr. Immanuel\u2019s previous \u201cbizarre\u201d claims \nabout other medical topics (Sommer 2020).\n Social Network Analysis and Mining (2022) 12:118\n1 3 118 Page 10 of 12\nFinally, we discovered a strong presence of groups and \npages from Brazil, France and some African countries that \nalso engaged in CLSB in the discourse around Dr. Imma-\nnuel. Most of these entities shared links to the video with \nDr. Immanuel and expressed pro-hydroxychloroquine senti-\nments. The presence of non-US entities actively participat-\ning in what was essentially a US-centric discussion rein-\nforces the idea that information and misinformation are not \nbounded by geography. A piece of (mis)information origi-\nnating in one country can spread around the globe in a mat-\nter of minutes, or even seconds. As a result, any action taken \nto mitigate the spread of COVID-19 related misinformation \non social media has to be a global response.\nThe public has the right to demand a faster response, \na better coordination across social media platforms and a \nspeedier fact-checking response, especially when it comes \nto combating COVID-19 misinformation. The case of the \n\u201cWhite Coat Summit\u201d press conference demonstrates that \nsocial media platforms are not ready to handle the viral \nspread of misinformation in highly partisan, internation -\nalized and decentralized information environments. As a \nresult, the \u201cWhac-A-Mole\u201d style approach to combating the \nspread of misinformation as it propagates across different \naccounts and platforms is likely to continue, especially in \nhighly polarized countries like the USA and Brazil, coun-\ntries with a strong presence of influential partisan media and \nwith country leaders whose efforts undermine their health \npolicy advisors and whose actions stand against evidence-\nbased decision making.\nAcknowledgements This research is funded by the Canadian Insti-\ntutes of Health Research (PIs: Veletsianos, Hodson, Gruzd), and is also \nsupported by the Canada Research Chairs program (PI: Gruzd) and a \nCompute Canada computing grant (PI: Gruzd).\nData availability The dataset for this study was collected using Crowd-\nTangle. According to the CrowdTangle data sharing policy, the data is \npublic in nature, but it is only available to organizations and individu-\nals with a CrowdTangle account. Researchers may request access at \nhttps:// help. crowd tangle. com/ en/ artic les/ 43022 08- crowd tangle- for-  \nacade mics- and- resea rchers.\nDeclarations  \nCompeting interest  The authors declare no potential competing inter -\nests with respect to the research, authorship, and/or publication of this \narticle.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAllington D, Duffy B, Wessely S, Dhavan N, Rubin J (2021) Health-\nprotective behaviour, social media usage and conspiracy belief \nduring the COVID-19 public health emergency. Psychol Med \n51(10):1\u20137. https:// doi. org/ 10. 1017/ S0033 29172 00022 4X\nAyers JW, Chu B, Zhu Z, Leas EC, Smith DM, Broniatowski DA \n(2021) Spread of misinformation about face masks and COVID-\n19 by automated software on Facebook. JAMA Intern Med \n181(9):1251\u20131253. https:// doi. org/ 10. 1001/ jamai ntern med.  \n2021. 2498\nBastian M, Heymann S, Jacomy M (2009) Gephi: an open source \nsoftware for exploring and manipulating networks. In: Proceed-\nings of the International AAAI Conference on Web and Social \nMedia, 3(1), 361-362. Retrieved from https:// ojs. aaai. org/ index.  \nphp/ ICWSM/ artic le/ view/ 13937\nBenkler Y, Faris R, Roberts H (2018) Network propaganda: manipu-\nlation, disinformation, and radicalization in American politics. \nOxford University Press, New York\nBertin P, Nera K, Delouv\u00e9e S (2020) Conspiracy beliefs, rejection of \nvaccination, and support for hydroxychloroquine: a conceptual \nreplication-extension in the COVID-19 pandemic context. Front \nPsychol. https:// doi. org/ 10. 3389/ fpsyg. 2020. 565128\nBradshaw S, Howard PN (2017) Troops, trolls and troublemakers: \na global inventory of organized social media manipulation. \nOxford Int Institute 2017:12\nBridgman A, Merkley E, Zhilin O, Loewen JP, Owen T, Ruths D \n(2021) Infodemic pathways: evaluating the role that traditional \nand social media play in cross-national information transfer. \nFront Political Sci. https:// doi. org/ 10. 3389/ fpos. 2021. 648646\nBruns A, Harrington S, Hurcombe E (2020) \u2018Corona? 5G? or both?\u2019: \nthe dynamics of COVID-19/5G conspiracy theories on Face-\nbook. Media Int Australia 177(1):12\u201329. https:// doi. org/ 10.  \n1177/ 13298 78X20 946113\nCalvillo DP, Ross BJR, Garcia RJB, Smelter TJ, Rutchick AM (2020) \nPolitical ideology predicts perceptions of the threat of COVID-\n19 (and susceptibility to fake news about it). Soc Psychol Per -\nsonality Sci 11(8):1119\u20131128. https:// doi. org/ 10. 1177/ 19485  \n50620 940539\nCasar\u00f5es G, Magalh\u00e3es D (2021) The hydroxychloroquine alliance: \nhow far-right leaders and alt-science preachers came together to \npromote a miracle drug. Rev Adm P\u00fablica. https:// doi. org/ 10.  \n1590/ 0034- 76122 02005 56\nCNN (2020) Trump walks out of briefing after CNN question on Dr. \nStella Immanuel - CNN Video. Retrieved from https:// www. cnn.  \ncom/ 2020/ 07/ 23/ health/ hydro  xychl oroqu ine- covid- brazil- study/  \nindex. html\nDotto C, Cubbon S (2021) Disinformation exports: how foreign anti-\nvaccine narratives reached West African communities online. First \nDraft. Retrieved from https:// first draft news. org/ long- form- artic le/ \nforei gn- anti- vacci ne- disin forma tion- reach es- west- africa/\nEducation First (2021) EF english proficiency index: a ranking of 112 \ncountries and regions by english skills. Education First. Retrieved \nfrom https:// www. ef. com/ wwen/ epi/\nFraser L (2022) What data is CrowdTangle tracking? https:// help. crowd  \ntangle.  com/  en/ artic les/ 11409  30- what-  data-  is- crowd  tangle-  track  \ning\nSocial Network Analysis and Mining (2022) 12:118 \n1 3 Page 11 of 12 118\nGallagher J (2020) Coronavirus: Malaria drug hydroxychloroquine \ndoes not save lives. BBC News, UK. Retrieved from\u00a0 https:// www. \nbbc. com/ news/ health- 52937 153\nGiglietto F, Righetti N, Rossi L, Marino G (2020b) It takes a village to \nmanipulate the media: coordinated link sharing behavior during \n2018 and 2019 Italian elections. Inf Commun Soc 23(6):867\u2013891. \nhttps:// doi. org/ 10. 1080/ 13691 18X. 2020. 17397 32\nGiglietto F, Righetti N, Rossi L (2020a) CooRnet. detect coordinated \nlink sharing behavior on social media. Software.\nGiglietto F, Righetti N, Rossi L, Marino G (2020c) Coordinated link \nsharing behavior as a signal to surface sources of problematic \ninformation on Facebook. In: International conference on social \nmedia and society. Association for computing machinery, New \nYork, USA, pp. 85\u201391\nGrady D, Thomas K, Lyons PJ, Vigdor N (2020) What to Know about \nthe Malaria Drug Trump says He is using. The New York Times, \nNew York. Retrieved from\u00a0 https:// www. nytim es. com/ artic le/ hydro  \nxychl oroqu ine- coron avirus. html\nGraham T, Bruns A, Zhu G, Campbell R (2020) Like a virus The coor -\ndinated spread of coronavirus disinformation. Queensland Univer -\nsity of Technology (QUT), Centre for Responsible Technology. \nRetrieved from https:// apo. org. au/ node/ 305864\nGruzd A, Mai P (2020) Going viral: How a single tweet spawned a \nCOVID-19 conspiracy theory on Twitter. Big Data Soc. https://  \ndoi. org/ 10. 1177/ 20539 51720 938405\nGruzd A (2020) Canada\u2019s out-of-date online privacy rules aren\u2019t pro-\ntecting you. The Conversation. Retrieved from https://  theco  nvers  \nation. com/ canad as- out- of- date- online- priva cy- rules- arent- prote  \ncting- you- 142585\nJacqueline H (2020) Another study finds hydroxychloroquine does not \nhelp Covid-19 patients. CNN. Retrieved from https:// www. cnn.  \ncom/ 2020/ 07/ 23/ health/ hydro  xychl oroqu ine- covid- brazil- study/  \nindex. html\nKalsnes B, Larsson AO (2021) Facebook news use during the 2017 \nNorwegian elections\u2014assessing the influence of hyperpartisan \nnews. J Pract 15(2):209\u2013225. https:// doi. org/ 10. 1080/ 17512 786.  \n2019. 17044 26\nKumar S, Cheng J, Leskovec J, Subrahmanian VS (2017) An army of \nme: sockpuppets in online discussion communities. In: Proceed-\nings of the 26th international conference on World Wide Web. \nInternational World Wide Web conferences steering committee, \npp. 857\u2013866\nLaila C (2020) Brave frontline COVID doctor calls out fake News, \nchallenges CNN\u2019s Chris Cuomo to take a urine test to prove He \nisn\u2019t taking hydroxychloroquine (VIDEO). In: The Gateway Pun-\ndit. https:// www. thega teway pundit. com/ 2020/ 07/ front line- covid-  \ndoctor- calls-  fake-  news-  chall  enges-  cnns- chris-  cuomo- take-  urine-  \ntest- prove- isnt- taking- hydro  xychl oroqu ine- video/. Accessed 24 \nMay 2021\nLancet T (2020) The truth is out there, somewhere. The Lancet \n396(10247):291. https:// doi. org/ 10. 1016/ S0140- 6736(20) 31678-0\nLovelace B Jr (2020) WHO says U.S. and Brazil accounted for half of \nnew daily coronavirus cases: too many countries are headed in the \nwrong direction. CNBC, NJ. Retrieved from\u00a0 https:// www. cnbc.  \ncom/ 2020/ 07/ 13/ who-  says- us-  and- brazil- accou  nted-  for- half-  of- \nnew- daily- coron avirus- cases. html\nmediabiasfactcheck.com The Gateway Pundit (2019). In: Media Bias \nFact Check. https:// media biasf  actch eck. com/ the- gatew  ay- pundit/. \nAccessed 24 May 2021\nMiguel R (2021) Tierra Pura, product of the pandemic: a new Span-\nish-language disinformation outlet with connections to the Epoch \nTimes ecosystem. EU DisinfoLab, Spain. Retrieved from\u00a0 https://  \nwww. disin fo. eu/ publi catio ns/ tierr  apura- produ ct- of- the- pande mic/\nNizzoli L, Tardelli S, Avvenuti M, Cresci S, Tesconi M (2021) Coor -\ndinated behavior on social media in 2019 UK general election. In: Proceedings of the international AAAI conference on web and \nsocial media. AAAI, pp 443\u2013454\nNsoesie EO, Cesare N, M\u00fcller M, Ozonoff A (2020) COVID-19 Misin-\nformation spread in eight countries: exponential growth modeling \nstudy. J Med Int Res. https:// doi. org/ 10. 2196/ 24425\nPapakyriakopoulos O, Serrano JCM, Hegelich S (2020) Political com -\nmunication on social media: a tale of hyperactive users and bias \nin recommender systems. Online Soc Netw Med. https:// doi. org/  \n10. 1016/j. osnem. 2019. 100058\nPassantino J, Darcy O (2020) Social media giants remove viral video \nwith false coronavirus claims that Trump retweeted. CNN, \nAtlanta. Retrieved from\u00a0 https:// www. cnn. com/ 2020/ 07/ 28/ tech/  \nfaceb ook- youtu be- coron avirus/ index. html\nPorterfield C (2020) Bolsonaro is taking hydroxychloroquine to treat \nhis coronavirus. Forbes. Retrieved from https:// www. forbes. com/  \nsites/ carli eport erfie ld/ 2020/ 07/ 08/ bolso naro- is- taking- hydro  xychl  \noroqu ine- to- treat- his- coron avirus/\nRauchfleisch A, Kaiser J (2021) Deplatforming the Far-right: an analy -\nsis of youtube and BitChute. SSRN. https://  doi. org/ 10. 2139/  ssrn.  \n38678 18\nRe G (2020) Trump ends press briefing after defending pro-hydrox-\nychloroquine doctor who says virus has a cure. Fox News, New \nYork. Retrieved from\u00a0 https:// www. foxne  ws. com/ polit ics/ trump-  \npress- briefi  ng- coron avirus- hydro  xychl oroqu ine- doctor\nRebello K, Schwieter C, Schliebs M, Joynes-Burgess K, Elswah M, \nBright J, Howard PN (2020) Covid-19 News and Information \nfrom state-backed outlets targeting French German and Spanish-\nspeaking social media users. Oxford Internet Institute, UK\nRecuero R, Soares FBS, Vinhas O, Volcan T, H\u00fcttner LRG, Silva \nV (2022) Bolsonaro and the Far Right: how disinformation \nabout covid-19 circulates on Facebook in Brazil. Int J Commun \n16:148\u2013171\nRecuero R, Soares FB, Gruzd A (2020) Hyperpartisanship, Disin-\nformation and political conversations on Twitter: The Brazilian \npresidential election of 2018. In: Proceedings of the international \nAAAI conference on web and social media. Pp. 569\u2013578\nRighetti N (2021) Four years of fake news: a quantitative analysis of \nthe scientific literature. First Monday. https:// doi. org/ 10. 5210/ fm.  \nv26i7. 11645\nRobins-Early N, Miller H, Cook J (2020) How quack doctors and pow -\nerful GOP operatives spread misinformation to millions. HuffPost. \nRetrieved from https:// www. huffp ost. com/ entry/ how- quack- docto  \nrs- and- power  ful- gop- opera tives- spread- misin forma tion- to- milli  \nons_n_ 5f208 048c5 b6685 9f1f3 3148\nRossini P, Kalogeropoulos A (2021) News and (Mis) information about \nCOVID-19 in Brazil. University of Liverpool, Liverpool, UK. \nRetrieved from https:// www. liver  pool. ac. uk/ commu nicat ion- and-  \nmedia/ resea rch/ groups/ news- and- misin forma tion- covid- 19- brazil/\nSayare S (2020) He Was a Science Star. Then He Promoted a Ques-\ntionable Cure for Covid-19. The New York Times, New York. \nRetrieved from\u00a0 https:// www. nytim es. com/ 2020/ 05/ 12/ magaz ine/  \ndidier- raoult- hydro  xychl oroqu ine. html\nShin J, Thorson K (2017) Partisan selective sharing: the biased dif-\nfusion of fact-checking messages on social media. J Commun \n67(2):233\u2013255. https:// doi. org/ 10. 1111/ jcom. 12284\nSmith D (2020) Spectacular: Trump praises doctor who dismissed face \nmasks after viral video. The Guardian. Retrieved from https://  \nwww. thegu  ardian. com/ us- news/ 2020/ jul/ 28/  trump- covid- 19- brief  \ning- hydro  xychl oroqu ine- video\nSommer W (2020) Trump\u2019s New COVID doctor says sex with \ndemons makes you sick. The Daily Beast, New York. Retrieved \nfrom\u00a0 https:// www. theda ilybe ast. com/ stella- imman uel- trumps-  \nnew- covid- doctor- belie  ves- in- alien- dna- demon- sperm- and- hydro  \nxychl oroqu ine\nStecula DA, Pickup M (2021) How populism and conservative media \nfuel conspiracy beliefs about COVID-19 and what it means for \n Social Network Analysis and Mining (2022) 12:118\n1 3 118 Page 12 of 12\nCOVID-19 behaviors. Res Politics. https:// doi. org/ 10. 1177/ 20531  \n68021 993979\nTandoc EC, Sagun KK, Alvarez KP (2021) The Digitization of Harass-\nment: Women Journalists\u2019 Experiences with Online Harassment in \nthe Philippines. J Pract. https:// doi. org/ 10. 1080/ 17512 786. 2021.  \n19817 74\nTangcharoensathien V, Calleja N, Nguyen T, Purnat T, D\u2019 Agostino M, \nGarcia-Saiso S, Landry M, Rashidian A, Hamilton C, AbdAl-\nlah A, Ghiga I, Hill A, Hougendobler D, van Andel J, Nunn M, \nBrooks I, Sacco PL, Domenico MD, Mai P, Gruzd A, Alaphilippe \nA, Briand S (2020) Framework for managing the COVID-19 Info-\ndemic: methods and results of an online, crowdsourced WHO \ntechnical consultation. J Med Internet Res 22(6):e19659. https://  \ndoi. org/ 10. 2196/ 19659\nUscinski JE, Enders AM, Klofstad C, Seelig M, Funchion J, Everett C, \nWuchty S, Premaratne K, Murthi M (2020) Why do people believe \nCOVID-19 conspiracy theories. HKS Misinfo Rev. https://  doi. org/ \n10. 37016/ mr- 2020- 015\nWard JK, Alleaume C, Peretti-Watel P (2020) The French public\u2019s \nattitudes to a future COVID-19 vaccine: the politicization of a public health issue. Soc Sci Med. https:// doi. org/ 10. 1016/j. socsc  \nimed. 2020. 113414\nWeber D, Neumann F (2021) Amplifying influence through coordi-\nnated behaviour in social networks. Soc Netw Anal Min. https://  \ndoi. org/ 10. 1007/ s13278- 021- 00815-2\nWendling M (2021) QAnon: What is it and where did it come from? \nBBC News, UK. Retrieved from\u00a0 https:// www. bbc. com/ news/  \n53498 434\nYu WES (2021) A framework for studying coordinated behaviour \napplied to the 2019 Philippine midterm elections. In: Proceedings \nof the 6th international congress on information and communica-\ntion technology (ICICT 2021). Springer, pp. 721\u2013731\nZarocostas J (2020) How to fight an infodemic. The Lancet \n395(10225):676. https:// doi. org/ 10. 1016/ S0140- 6736(20) 30461-X\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "How coordinated link sharing behavior and partisans' narrative framing fan the spread of COVID-19 misinformation and conspiracy theories", "author": ["A Gruzd", "P Mai", "FB Soares"], "pub_year": "2022", "venue": "Social network analysis and mining", "abstract": "This study examines the presence and role of Coordinated Link Sharing Behavior (CLSB) on  Facebook around the \u201cAmerica\u2019s Frontline Doctors\u201d press conference, and the promotion of"}, "filled": false, "gsrank": 186, "pub_url": "https://link.springer.com/article/10.1007/s13278-022-00948-y", "author_id": ["x_gQTHcAAAAJ", "ZvD5P8kAAAAJ", "4gftCnwAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:j_Vh0C2AfrUJ:scholar.google.com/&output=cite&scirp=185&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D180%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=j_Vh0C2AfrUJ&ei=JbWsaMu1E-HUieoP9LKZ6AI&json=", "num_citations": 18, "citedby_url": "/scholar?cites=13078031302188463503&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:j_Vh0C2AfrUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://link.springer.com/content/pdf/10.1007/s13278-022-00948-y.pdf"}}, {"title": "Faking fake news for real fake news detection: Propaganda-loaded training data generation", "year": "2022", "pdf_data": "Faking Fake News for Real Fake News Detection:\nPropaganda-loaded Training Data Generation\nKung-Hsiang Huang\u00b9Kathleen McKeown\u00ba\nPreslav Nakov\u00b5Yejin Choi\u00b7\u00b6Heng Ji\u00b9\n\u00b9UIUC\u00baColumbia University\u00b7University of Washington\u00b5MBZUAI\u00b6AI2\n{khhuang3, hengji}@illinois.edu kathy@columbia.edu\npreslav.nakov@mbzuai.ac.ae yejinc@allenai.org\nAbstract\nDespite recent advances in detecting fake news\ngenerated by neural models, their results are\nnot readily applicable to effective detection\nof human-written disinformation. What lim-\nits the successful transfer between them is the\nsizable gap between machine-generated fake\nnews and human-authored ones, including the\nnotable differences in terms of style and under-\nlying intent. With this in mind, we propose\na novel framework for generating training ex-\namples that are informed by the known styles\nand strategies of human-authored propaganda.\nSpeci\ufb01cally, we perform self-critical sequence\ntraining guided by natural language inference\nto ensure the validity of the generated arti-\ncles, while also incorporating propaganda tech-\nniques, such as appeal to authority andloaded\nlanguage . In particular, we create a new train-\ning dataset, P ROPA NEWS, with 2,256 exam-\nples, which we release for future use. Our ex-\nperimental results show that fake news detec-\ntors trained on P ROPA NEWS are better at de-\ntecting human-written disinformation by 3.62\u2013\n7.69% F1 score on two public datasets.1\n1 Introduction\nThe dissemination of false information can cause\nchaos, hatred, and trust issues, and can eventually\nhinder the development of society as a whole (De-\nwatana and Adillah, 2021; Wasserman and Madrid-\nMorales, 2019). In particular, human-written disin-\nformation2is often used to manipulate certain pop-\nulations and had a catastrophic impact on multiple\nevents, such as Brexit (Bastos and Mercea, 2019),\nthe COVID-19 pandemic (van Der Linden et al.,\n2020), and the 2022 Russian assault on Ukraine.\nHence, there is an urgent need for a defense\n1The code and data released on GitHub: https://\ngithub.com/khuangaf/FakingFakeNews\n2There are many types and de\ufb01nitions of fake news , but\nhere we focus on text-only disinformation . Yet, we will also\nuse the less accurate term fake news as it is more common.mechanism against human-written disinformation.3\nTo construct such a mechanism, we need a sub-\nstantial amount of training data to train the detec-\ntors. A na\u00efve solution is to collect human-written\nnews articles that contain inaccurate information\nby crawling untrustworthy news media. However,\nnews articles published by suspicious sources do\nnot necessarily contain false information, which\nmeans that annotators are required to fact-check ev-\nery claim in each untrustworthy article. Moreover,\narticles containing false claims are often removed\nshortly after posting. While some work collected\nhuman-written fake news from fact-checking web-\nsites (Shu et al., 2018; Nguyen et al., 2020), the\nsize of these datasets is limited. The curation pro-\ncess of these websites also requires high manual\nefforts. Hence, such a solution is neither scalable\nnor reliable. Thus, an alternative direction comple-\nmenting the existing efforts would be generateing\ntraining data automatically in a way that avoids\nthese issues.\nOur goal here is to enhance disinformation de-\ntection by generating training examples that are\nbetter informed by the known styles and strategies\nof human-authored disinformation. We started by\ncollecting human-written disinformative articles\nfrom untrustworthy sites4, and we analyzed around\n40 of them that spread false claims. Throughout\nour analysis, we found two characteristics of this\nhuman-written disinformation. First, about 33% of\nthe articles used propaganda techniques to convince\nthe audience that the fake information was actually\nauthentic, and these techniques often involve the\nuse of emotion-triggering language or logical fal-\nlacies (Da San Martino et al., 2019) to increase\nthe impact on the reader. The count of each pro-\npaganda technique used is shown in Appendix A.\n3WARNING: This paper contains disinformation that may\nbe sensitive or offensive in nature.\n4These news sources are rated lowfor the factuality of\nreporting by mediabiasfactcheck.com .arXiv:2203.05386v2  [cs.CL]  15 May 2023\nAJDABIYAH , Libya | Thu Apr 7 , 2011 6:34 pm EDT AJDABIYAH , Libya -LRB- Reuters -RRB- - Rebels \ufb01ghting to\noverthrow Muammar Gadda\ufb01 said \ufb01ve of their \ufb01ghters were killed ... \u201dIn rebel-held eastern Libya, wounded rebels being\nbrought to a hospital Ajdabiyah said their trucks and tanks were hit on Thursday by a NATO air strike outside Brega. NATO\nsaid itwasinvestigatinganattack byitsaircraft onatank column inthearea along theMediter ranean coast onThurs day\n,sayingthesituationwas\u201cunclear and\ufb02uid .\u201dRebels said atleast \ufb01veoftheir \ufb01ghterswere killed when NATO planes\nmistakenly bombed arebel tank column near thecontested port. \u201cAnumberofvehicleswere hitbyaNATO strike \u201d,of\ufb01cers\nfrom UNconcluded. The \ufb01ghting for Brega , the only active front , has dragged on for a week ...\nTable 1: An example of our generated fake news. Given an authentic news article, our approach \ufb01rst identi\ufb01es a\nsalient sentence, which it then replaces with aplausiblebutdisinformativesentence that is coherent to the context.\nFinally, it generates a propaganda sentence to make the article resemble human-written fake news.\nSecond, more than 55% of the articles that we ana-\nlyzed contained inaccurate information mixed with\nthe correct information: in fact, all claims, except\nfor one or two, in these disinformation articles were\nfactual, which makes the few false claims in these\narticles even more believable.\nPrior work has made signi\ufb01cant progress in gen-\nerating fake news using large pre-trained sequence-\nto-sequence (seq2seq) models (Zellers et al., 2019;\nFung et al., 2021; Shu et al., 2021). However, the\narticles generated by these approaches contain an\noverwhelmingly large proportion of false informa-\ntion and do not explicitly use propaganda.\nTo address these issues, here we propose a novel\ngeneration method. Given an authentic news ar-\nticle, we replace a salient sentence with a plausi-\nble but fake piece of information using a seq2seq\nmodel. As the generated texts can often be en-\ntailed by the original contexts, we incorporate a\nself-critical sequence training objective (Rennie\net al., 2017) that incorporates a natural language\ninference (NLI) model into the loss function. Ad-\nditionally, we use the NLI model to \ufb01lter out gen-\nerated sentences that can be inferred from the re-\nplaced ones. Then, we add propaganda techniques\nto mimic how humans craft disinformation. In\nparticular, we automate two commonly used propa-\nganda techniques, appeal to authority andloaded\nlanguage , (Da San Martino et al., 2019) to add\npropaganda into the faked sentences.\nSubsequently, we use the silver-standard train-\ning data generated from these two steps to train\na detector. An example is shown in Table 1. We\nfurther recruited crowdsourcing workers to validate\nthat some of the generated texts were indeed fake,\nso that we could construct a gold-standard training\ndataset.\nComparing our method to state-of-the-art fake\nnews generation approaches, the evaluation results\non two human-written fake news datasets show\nthat detectors are substantially better at spotting\nhuman-written disinformation when trained on ourgenerated fake news dataset. Our ablation stud-\nies con\ufb01rm the effectiveness of incorporating pro-\npaganda into the generated articles for producing\nbetter training data.\nOur contributions can be summarized as follows:\n\u2022We propose an effective method to automat-\nically generate more realistic disinformation\ncompared to previous work.\n\u2022We develop the \ufb01rst automatic methods to gen-\nerate speci\ufb01c propaganda techniques such that\nthe generated articles are closer to disinforma-\ntion written by humans.\n\u2022We demonstrate that detectors trained on our\ngenerated data, compared to generated articles\nusing other methods, are better at detecting\nhuman-written disinformation.\n\u2022We release PROPA NEWS, a dataset for disin-\nformation detection containing 2.2K articles\ngenerated by our approach and validated by\nhumans.\n2 Training Data Generation\nOur process of generating training data for\npropaganda-loaded disinformation consists of two\nmain steps: disinformation generation (\u00a72.1) and\npropaganda generation (\u00a72.2). Below, we describe\neach of these steps in detail.\n2.1 Disinformation Generation\nOur disinformation generation approach aims at\ntwo sub-goals: ( i) replacing a salient sentence in\nthe given article with a sequence of generated co-\nherent texts that looks plausible, and ( ii) ensuring\nthat the generated information cannot be entailed\nby the original masked-out sentence; otherwise,\nthe generated texts will not be disinformative. To\nachieve the \ufb01rst sub-goal, we \ufb01rst identify salient\nsentences using extractive summarization, and we\nthen perform mask-in\ufb01lling with BART (Lewis\net al., 2020). The second sub-goal is accomplished\nusing self-critical sequence training (Rennie et al.,\n2017) with an NLI component, which is used as a\nreward function for generation.\nSalient Sentence Identi\ufb01cation A salient sen-\ntence is critical for the overall semantics of the\narticle. When a salient sentence is manipulated\nor replaced, the complex events described in the\narticle may be drastically changed. Yet, there is\nno salient sentence identi\ufb01cation dataset publicly\navailable. Motivated by the fact that sentences in-\ncluded in an extractive summary are often of higher\nimportance, we take the scores computed by an\nextractive summarization model (Liu and Lapata,\n2019), which predicts how likely each sentence is\nto belong to the summary, to estimate the saliency\nof each sentence. Empirically, we found that this\napproach yields reasonably good sentence saliency\nestimation. For each news outlet, we replace one\nsentence that has the highest extractive summariza-\ntion score with our generated disinformation.\nMask In\ufb01lling with BART To perform in\ufb01ll-\ning, we take an approach that is similar to that of\nDonahue et al. (2020), but we use BART (Lewis\net al., 2020), a pre-trained language model with\nan encoder\u2013decoder architecture. During train-\ning time, we randomly mask out a sentence y\u0098\nfrom a given article x. The bidirectional encoder\n\ufb01rst produces contextualized representations he\u0000\nEncoder \u0000~x\u0006given the article with a masked-out\nsentence ~x\u0000x\u000ey\u0098. Then, the auto-regressive\ndecoder learns a maximum likelihood estimation\nthat aims to maximize the probability of generating\nthe next token y\u0098\ntat time steptgiven all tokens in\nprevious time steps ry\u0098\n0;:::;y\u0098\nt\u000e1x\nand the encoder hidden states heby minimizing\nthe negative log probability of generating y\u0098\ntas\nfollows:\nLm\u0000\u000eT\n=\nt\u00001logP\u0000y\u0098\nt\u00b6y\u0098\n0;:::;y\u0098\nt\u000e1;he\u0006:(1)\nDuring inference time, rather than random mask-\ning,~xis formed by masking out the sentence with\nthe highest score computed by the extractive sum-\nmarization model given the original document x,\nas discussed in the previous paragraph.\nSelf-critical Sequence Training BART opti-\nmized via maximum likelihood estimation alone\nis capable of generating coherent texts. However,\nalthough the generated texts y\u00acmay be very dif-\nferent from the originally masked out sentence y\u0098,\nthere is no guarantee that y\u00accontains incorrect in-\nformation. If the generated texts y\u00accan be entailedby the masked out sentence y\u0098, theny\u00acis actu-\nally not disinformative. An example is shown in\nFigure 2. Here, except for the lack of details, the\ngenerated sentence y\u00acdelivers the same message as\nthe masked out sentence y\u0098. To reduce the prob-\nability thaty\u00accan be entailed by y\u0098, we leverage\nself-critical sequence training (Rennie et al., 2017;\nBosselut et al., 2018) that rewards the model for\ngenerating sequences that cannot be entailed by\nthe masked-out sentences. Self-critical sequence\ntraining (SCST) is a form of the REINFORCE al-\ngorithm (Williams, 1992) that allows direct opti-\nmization on non-differentiable functions. Using a\nbaseline output y\u00ac\u00acof the model to normalize the\nrewards, SCST avoids the challenge of directly es-\ntimating the reward signal or estimating normaliza-\ntion (Rennie et al., 2017). Since our goal is to avoid\nentailment from y\u0098toy\u00ac, we de\ufb01ne the reward as\nthe negative entailment probability computed by\naROBERT A-based (Liu et al., 2019) NLI model\n\ufb01ne-tuned on Multi-NLI (Williams et al., 2018)5,\nr\u0000y\u00ac\u0006\u0000\u000ePnli\u0000y\u0098;y\u00ac\u0006; (2)\nwherer\u0000y\u00ac\u0006is the reward of the sequence sam-\npled from the current policy y\u00ac, andPnli\u0000y\u0098;y\u00ac\u0006is\nthe probability that y\u0098entailsy\u00ac. To generate y\u00ac,\nwe use Nucleus Sampling (Holtzman et al., 2020)\nwithp\u00000:96, as this sampling method has shown\nadvantages in open-ended generation (Holtzman\net al., 2020; Zellers et al., 2019).\nWe generate the baseline output y\u00ac\u00acusing greedy\ndecoding, then obtain the entailment probabilities\nbetweeny\u00acandy\u00ac\u00acfrom the NLI model. We then\ncompute the self-critical sequence training loss:\nLs\u0000\u000e\u0000r\u0000y\u00ac\u0006\u000er\u0000y\u00ac\u00ac\u0006\u0006T\n=\nt\u00001logP\u0000y\u00ac\nt\u00b6y\u00ac\n0;::;y\u00ac\nt\u000e1;he\u0006:(3)\nHerer\u0000y\u00ac\u00ac\u0006is a baseline reward, and r\u0000y\u00ac\u0006\u000er\u0000y\u00ac\u00ac\u0006\nis a normalized reward. This loss function encour-\nages BART to generate y\u00acwhenr\u0000y\u00ac\u0006%r\u0000y\u00ac\u00ac\u0006,\nwhereas it suppresses the probability of decoding\ny\u00acwhenr\u0000y\u00ac\u0006$r\u0000y\u00ac\u00ac\u0006. An overview of SCST is\nshown in Figure 1.\nThe \ufb01nal objective function to minimize is a\nweighted sum of Equation (1) and Equation (3),\nLfinal\u0000\u000bLm\u0011\fLs; (4)\nwhere\u000band\fare the weights for each loss6.\n5We use the \ufb01ne-tuned NLI model from https://\nhuggingface.co/roberta-large-mnli . Its accu-\nracy is 90.2% on the dev set of MNLI, which is on par with\nstate-of-the-art methods.\n6Empirically, we set \u000b\u00001and\f\u00000:01.\nS1S2<mask>S4S5S6S7BARTNLI modelNLI model!\u2032!\u2032\u2032Nucleus sampling\nGreedy decoding$!\"=\u2212'()*!\u2217,!\"$!\"\"=\u2212'()*(!\u2217,!\u2032\u2032)+,Reward\u2212$!\"\u2212$!\"\"-+,-.log'!\"+!/\",\u2026,!+0-\",21)32Figure 1: Illustration of our self-critical sequence training. Given a corrupted input article ~x, BART generates two\nsequences with Nucleus sampling and greedy decoding, respectively. The reward for each sequence is computed\nas the negative entailment probability \u000ePentas output from the NLI model.\nSenior officers of the Nigerian Air Force has begun investigating how a military jetcould drop two bombson a camp for civilians.The campswere attacked by bomberplanes.NLI model!\u2217!\"\"$&'originalgenerated\nFigure 2: An example showing the NLI model predicts\nan entailment from the masked out sentence y\u0098to the\ngenerated sentence y\u00ac.\nPost-processing To further ensure the quality of\nthe disinformation generated, we reuse the NLI\nmodel discussed in the previous paragraph to \ufb01lter\nout invalid outputs y\u00acthat can be entailed from the\nmasked-out sentence y\u0098, as demonstrated in Fig-\nure 2. We found that incorporation of the SCST loss\n(Equation (3)) into the training objective success-\nfully reduces the invalid rate from 7.8% to 3.2%.\n2.2 Propaganda Generation\nAfter generating inaccurate information, we then\nincorporate propaganda into each generated ar-\nticle. We chose two representative propaganda\ntechniques of each type: emotional versus non-\nemotional. Loaded language is an emotional tech-\nnique and it is also by far the most frequent propa-\nganda technique as shown in Table 5 of Da San Mar-\ntino et al. (2019) and Table 2 of Dimitrov et al.\n(2021). Based on these two tables, we also see\nthatappeal to authority is among the most frequent\nnon-emotional techniques.\nAppeal to Authority Appeal to authority is a\npropaganda technique that aims to strengthen or\ninvalidate an argument by referring to a statement\nmade by authorities or experts (Da San Martino\net al., 2019). We \ufb01rst collect experts from var-ious domains, such as economics and immunol-\nogy, from Wikidata7. In particular, we specify the\noccupation (P108) of each expert and \ufb01lter out\nentities that were born before 1940 to ensure re-\ncency. To consider only impactful entities, we rank\nall candidates based on the number of correspond-\ning outcoming statements (i.e. connected concepts\nin Wikidata), inspired by PageRank (Page et al.,\n1999), and add the top 100 entities for each occupa-\ntion into the candidate list Z. Then, we include the\nperson named entities extracted by a name tagger8,\nwhich are more relevant to the local context.\nThis makes sense as we found that more than\n73% of the news articles contain authorities. More\ndetails on how authority candidates Zare collected\ncan be found in Appendix E.\nOnce we collect a candidate list Z, we then gen-\nerate fake arguments made by each zi\"Zwith\ntheBART model that has already been \ufb01ne-tuned\nin \u00a72.1. In particular, a <mask> token is inserted\nright after the \ufb01lled-in sentence y\u00acin the input ar-\nticle to BART so that it knows where to perform\nin\ufb01lling. To inform BART that it should generate\na statement made by an authority, we pre\ufb01x the\ndecoder with a template such as [ zicon\ufb01rmed that\n\u201c], wherezi\"Zis the name of the authority.\nThe pre\ufb01x ends with an opening quotation mark\nto indicate that it should be followed by a statement\nby authority zi. To increase the diversity of the\ngenerated statements, we devise a variety of tem-\nplates, as detailed in Appendix E. Finally, the best\nsequences\u0098is selected with the lowest perplexity\ns\u0098\u0000argminsiPerplexity \u0000si\u0006, wheresidenotes\nthe generated sequence using zias the authority.\nLoaded Language Loaded language is another\npropaganda technique that uses emotion-triggering\nterms or phrases to in\ufb02uence the opinions of the au-\n7https://query.wikidata.org/\n8https://stanfordnlp.github.io/stanza\nTechnique Generated Disinformation and Propaganda\nAppeal to Authority Cairo\u2019s Tahrir Square was the scene of clashes between protesters and police on Wednesday. \u201cAt\nleast three peoplewere killed andmore than 600were injured intheclashes,\u201d saidEgypt\u2019s President.\nLoaded Language Cairo\u2019s Tahrir Square was the scene of deadly clashes between protesters and police on Wednesday.\nTable 2: Examples of the two generated propaganda techniques, as shown by texts inblue. The \ufb01rst row shows\nhow the argument is strengthened by appealing to an authority\u2019s statement, while the second row demonstrates\nhow loaded language is introduced with an emotion-triggering term.\ndience (Da San Martino et al., 2019; Dimitrov et al.,\n2021). Often, loaded language involves the use of\nsensational adverbs or adjectives to exaggerate a\nstatement. Based on this observation, we utilize\nthe propaganda dataset released by Da San Martino\net al. (2019) where propaganda techniques are an-\nnotated at the fragment level (i.e. span level). The\ndataset contains 2,547 loaded language instances.\nYet, not every instance contains adjectives or ad-\nverbs that are emotion-triggering. To create valid\ntraining data for loaded language generation, we\n\ufb01rst use SpaCy to perform part of speech tagging\nand dependency parsing, and then keep the exam-\nples where there exists an adverb pointing to a verb\nor an adjective pointing to a noun through depen-\ndency parsing edges. This results in 1,017 samples\nof valid loaded language instances. Examples of\nthe generated appeal to authority andloaded lan-\nguage are shown in Table 2.\nUpon collecting the training data to generate\nloaded language , we \ufb01ne-tune another BART on\nthis dataset. Na\u00efvely, we can take the articles with\nemotion-triggering adverbs or adjectives removed\nas input to BART and using the original article\nas the decoding target. However, we found that\naround 25% of the time BART does not exactly\nreproduce the unmasked texts due to hallucination.\nThis observation is consistent with Donahue et al.\n(2020)\u2019s \ufb01ndings. To this end, we propose a two-\nstep generation approach. First, we train BART\nto insert a <mask> token into the target sentence\nin the input document marked with special tokens.\nThen, BART learns to in\ufb01ll the <mask> with an\napproach similar to what is discussed in \u00a72.1 but\nwithout the SCST objective. Empirically, we found\nthat this approach successfully reduces the chance\nof failure in generating the exact unmasked con-\ntexts to around 2%.\n2.3 Intermediate Pre-training\nAs the size of TIMELINE 17(Tran et al., 2013)\nand the propaganda dataset (Da San Martino et al.,\n2019) are relatively small, we perform intermedi-\nate pre-training (IPT) on the news articles fromCNN/DM , a large news summarization dataset\n(Hermann et al., 2015), for domain adaptation. De-\ntails of IPT can be found in Appendix F.\n3 Our P ROPA NEWS Dataset\n3.1 Data Source\nWhen selecting the source of data to construct our\ndataset, we consider the following two criteria.\nFirst, the news articles must have high trustworthi-\nness. This ensures that, except for our manipulated\nsentences, the rest of the articles are genuine. Sec-\nond, the news events described in the articles must\nbe important to the general audience. Motivated by\nthese two criteria, we repurpose the TIMELINE 17\ndataset (Tran et al., 2013) as our source of data.\nTIMELINE 17contains 17 timelines, each of which\ncorresponds to a news event. Each timeline is asso-\nciated with a series of news articles that span across\na wide time span, implying the high importance\nand impact of these news events. Additionally, the\nnews articles are from trustworthy media, such as\nThe New York Times and The Guardian. In total,\nthere are 4,535 news articles in T IMELINE 17.\n3.2 Crowdsourcing for Data Curation\nWe use Amazon\u2019s Mechanical Turk (AMT) to ver-\nify the quality and correctness of the generated dis-\ninformation. In total, there are around 400 unique\ncrowdsourcing workers contributing to approxi-\nmately 2,000 Human Intelligence Tasks (HITs).\nFor each HIT, annotators are tasked to look for\nsupporting evidence from trustworthy news media\nto determine whether the sentences generated are\nindeed inaccurate . Only those labeled as inaccu-\nrate will be included in PROPA NEWS, while the\naccurate counterparts are discarded. Appendix H\nprovides the details of the annotation interface.\nTo measure the inter-annotator agreement (IAA),\nwe use the Worker Agreement With Aggregate\n(WAWA) score, following Ning et al. (2020)\nand Sheng et al. (2021). WAWA compares each\nannotator\u2019s answer with the aggregated answer\nobtained via majority votes and micro-averages the\nresults across all samples9. The resulting WAWA\nprecision, recall, and F1are 80.01%, 78.94%,\nand 79.47%, which indicates a moderate to high\nagreement.\n4 Disinformation Detection\nThe disinformation detection task challenges de-\ntectors to determine whether a given input article\ncontains inaccurate information or not. We experi-\nment on four detectors, including HDSF (Karimi\nand Tang, 2019), GROVER (Zellers et al., 2019),\nBERT (Devlin et al., 2019) and ROBERT A(Liu\net al., 2019). HDSF leverages the hierarchical\nstructures of discourse-level features, such as de-\npendency trees, to predict the veracity of a news ar-\nticle. GROVER is an unidirectional seq2seq model\npre-trained on news documents. We use the dis-\ncriminative version for detection which is adapted\nfrom its generative version by feeding the [CLS]\ntoken representations to a multi-layer perceptron.\nSimilarly, BERTandROBERT Atake in the entire\narticle as input and feed the representations of the\n\ufb01rst token to a classi\ufb01cation head to determine the\nveracity of each article. In addition, all models are\noptimized using cross entropy. For fair comparison,\nwe set the maximum sequence length to 512 and\nuse the LARGE variants for all models. Details can\nbe found in Appendix J.\n5 Experiments\nIn our experiments, we aim to (1) analyze the per-\nformance of different models on the PROPA NEWS\ndataset, (2) examine the effect of various train-\ning data sets, and (3) investigate how much silver-\nstandard data is equivalent to gold-standard data.\n5.1 Data\nPROPA NEWS ThePROPA NEWS dataset consists\nof 2,256 distinct articles, with a balanced portion\nof fake and real documents. Within the fake arti-\ncles, 30% of them use appeal to authority , another\n30% include loaded language , and the remaining\n40% simply contains inaccurate information. We\nsplit the data into 1,256: 500: 500 for training,\nvalidation, and testing.\n9We did not use other IAA metrics, such as Cohen\u2019s Kappa\n(Cohen, 1960), as we expect the vast majority of our generated\ndisinformation to be inaccurate. WAWA provides a better\napproximation for inter-annotator agreement in our scenario.Evaluation Data We use two sets of human-\nwritten articles released by Nguyen et al. (2020)\nand Shu et al. (2018) to evaluate the effectiveness\nof our approach. The articles in each dataset are col-\nlected from two fact-checking websites, SNOPES\nandPOLITI FACT, respectively. Articles no longer\naccessible via the given URL are removed. The\nstatistics of both datasets are shown in Appendix I.\nOther generated training data We compare\nPROPA NEWS with the following approaches.\nGROVER -GEN (Zellers et al., 2019) generates\nheadlines which condition on the original body\ntexts, followed by body text generation condition-\ning on the generated headlines. FACTGEN(Shu\net al., 2021) enhances the factual consistency of the\ngenerated article with a fact retriever that fetches\nsupporting information from external corpora. FA-\nKEEVENT (Wu et al., 2022) generates sentences\nsequentially with condition on the manipulated\nknowledge elements of each sentence. Also, we\nform the PN- SILVER dataset by resampling our\ngenerated data but disregarding the annotator vali-\ndation. Furthermore, we construct additional train-\ning sets by replacing the salient sentence in each\narticle with one sentence generated by each base-\nline method, as indicated by -1S ENT. To ensure\nfair comparisons, all generators take in the same\nset of authentic articles as inputs.\n5.2 Results and Discussion\nHuman-written disinformation detection To\nstudy the effectiveness of human-written disinfor-\nmation detection, we train GROVER -LARGE and\nROBERT A-LARGE on different training datasets\nand evaluate them on the SNOPES andPOLITI FACT\ndatasets, as shown in Table 3. Both models perform\nbest when trained on PROPA NEWS, compared to\ntraining on other datasets. Consider ablating hu-\nman validation, detectors trained on PN- SILVER\nstill outperform their counterparts trained on other\ndatasets. This shows that our generative method\nproduces articles that are more similar to human-\nwritten disinformation. To further verify this \ufb01nd-\ning, we measure the similarity between articles\ngenerated by different approaches and disinforma-\ntive articles in the POLOTI FACT dataset using the\nMAUVE metric (Pillutla et al., 2021). MAUVE com-\nputes the similarity between two text distributions\nby adding the areas under a divergence curve, and\nhas been shown to produce better approximations\nthan other metrics such as JS divergence (Martins\nTest Data \u0000 POLITI FACT SNOPES\nDetectors \u0000 ROBERT A-LARGE GROVER -LARGE ROBERT A-LARGE GROVER -LARGE\nTraining Data \u0003\nWithout human validation (silver)\nGROVER -GEN 57.65 (\u00197.6) 52.77 ( \u00192.1) 48.42 ( \u00192.2) 49.53 ( \u00190.1)\nGROVER -GEN-1S ENT 49.65 (\u00195.2) 47.48 ( \u00191.8) 44.44 ( \u00193.2) 50.10 ( \u00192.1)\nFAKEEVENT 46.33 (\u00192.6) 50.27 ( \u00195.9) 45.36 ( \u00191.2) 47.40 ( \u00191.3)\nFAKEEVENT -1S ENT 47.32 (\u00193.2) 50.12 ( \u00193.2) 46.62 ( \u00192.9) 47.29 ( \u00192.7)\nFACTGEN 48.46 (\u00192.2) 51.79 ( \u00193.6) 41.98 ( \u00195.4) 50.47 ( \u00194.9)\nFACTGEN-1S ENT 41.19 (\u00193.5) 40.92 ( \u00194.1) 40.01 ( \u00193.8) 45.52 ( \u00193.7)\nPN- SILVER 60.39\u0098(\u00193.9) 55.23\u0098(\u00195.8) 51.52\u0098\u0098(\u00193.4) 52.39\u0098\u0098(\u00194.1)\nWith human validation (gold)\nPROPA NEWS 65.34\u0098\u0098(\u00194.5) 60.43\u0098\u0098(\u00196.2) 53.03\u0098\u0098(\u00193.7) 54.09\u0098\u0098(\u00192.8)\nw/o AA 63.21\u0098\u0098(\u00193.2) 58.28\u0098\u0098(\u00194.2) 50.78\u0098(\u00191.8) 53.22\u0098\u0098(\u00193.7)\nw/o LL 64.65\u0098\u0098(\u00191.8) 56.93\u0098\u0098(\u00195.3) 51.92\u0098\u0098(\u00193.4) 51.68\u0098(\u00191.4)\nw/o AA & LL 61.83\u0098(\u00194.9) 52.82 ( \u00193.3) 52.77\u0098\u0098(\u00192.7) 50.93 ( \u00192.7)\nTable 3: AUC (in %) of different models on the S NOPES and P OLITI FACT datasets when trained on various data\nsets. The bottom rows show different variants of P ROPA NEWS. AA denotes appeal to authority , whereas LL refers\ntoloaded language . We report the mean and standard deviation of four runs. Statistical signi\ufb01cance over previous\nbest approaches computed using the paired bootstrap procedure (Berg-Kirkpatrick et al., 2012) is indicated with\n\u0098\u0098(p$:01) and\u0098(p$:05) .\net al., 2020). We \ufb01nd that the MAUVE score with\nPOLOTI FACT forPROPA NEWS andGROVER -GEN\nare 17.1% and 13.7%, respectively, suggesting that\nthe generated documents in PROPA NEWS are closer\nto human-written disinformation. These results\ncon\ufb01rm that the advantage of our generated articles\nin defending against human-written disinformation\nis resulted from the closer gap between them.\nComparing each baseline method and its coun-\nterpart that only generates one sentence to be sub-\nstituted for the salient sentence (i.e. -1S ENT), we\nfound signi\ufb01cant performance drops on GROVER -\nGEN andFACTGENwhen only generating one sen-\ntence. This is likely caused by the incoherence\nbetween the right context and the sentence gener-\nated by these approaches due to the left-to-right\nfashion of text generation. While FAKEEVENT\ndoes not see the right context, it additionally con-\nditions on knowledge elements corresponding to\nthe sentence, which discourages it from producing\ntopically irrelevant content and thus does not lead\nto huge performance drop.\nIn Table 4, we show two disinformative articles\nfrom POLITI FACT where ROBERT Ais able to\nclassify them as inaccurate when trained on PN-\nSILVER but fails when trained on GROVER -GEN.\nBoth articles contain propaganda, which are in-\ncorporated into PN- SILVER but not into GROVER -\nGEN. This demonstrates that detectors trained on\nour generated data are better at detecting human-\nwritten disinformation that has such properties.Is propaganda generation helpful for disinfor-\nmation detection? We further conduct an abla-\ntion study to analyze the contributions of each pro-\npaganda technique. As shown in the bottom of Ta-\nble 3, both appeal to authority andloaded language\nprove bene\ufb01cial in enhancing models\u2019 abilities to\ndetect human-written disinformation. Furthermore,\ncomparing PROPA NEWS W /OAA& LL with other\ngeneration approaches, we \ufb01nd that both models\ntrained on our generated data, even without the\nincorporation of propaganda techniques, still out-\nperform their counterparts trained on other datasets.\nThis illustrates that our generated disinformation is\ncloser to those written by humans.\nHow good is the generation quality? To eval-\nuate the quality of our generation approach, we\nasked AMT workers to rate the plausibility of 100\ngenerated articles from PROPA NEWS and deter-\nmine the degree by which their answer to this ques-\ntion is in\ufb02uenced by the generated propaganda.\nEach article is rated by 3 workers. For comparison,\nwe also ask AMT workers to rate the plausibility\nof 100 generated articles from GROVER -GEN. The\naverage plausibility scores for PROPA NEWS and\nGROVER -GEN are 2.25 and 2.15 (out of 3), indi-\ncating that our generation approach has a slight ad-\nvantage over GROVER -GEN in terms of plausibility.\nFurthermore, among the articles in PROPA NEWS\nthat are rated highly plausible, 29.2% of the work-\ners think that the generated propaganda highly af-\nfects their response (i.e. rated 3 out of 3) that the\nArticle and Analysis\nArticle: ...State ment from FDA Com missioner Scott Gottlieb, M.D., on FDA\u2019s ongoing efforts to help improve\neffectiveness of in\ufb02uenza vaccinesFor Immediate Release: ...\nAnalysis: Appealing to authority is common in human-written fake news.\nArticle: ... Regardless of how much we hate Nacy Pelosi, she represents a Congressional District that saw a million\nfraud ulent votes from illegal immigrants...\nAnalysis: The use of loaded language often indicates disinformation.\nTable 4: Examples from P OLITI FACT where R OBERT A-LARGE successfully predicts the veracity when trained\non PN- SILVER , but classi\ufb01es incorrectly when trained on G ROVER -GEN.\ngenerated article is plausible. This demonstrates\nthe effectiveness of our propaganda techniques in\nincreasing the plausibility of generated articles.\nSurvey details and score distributions are discussed\nin Appendix K.\n6 Related Work\nFake News Generation and Detection There\nhas been a focus in prior research on utilizing neu-\nral networks to automatically generate fake news\nas a means of defending against the proliferation of\nmachine-generated fake news. Zellers et al. (2019)\npre-train a generator with the same architecture as\nGPT-2 (Radford et al., 2019) on a large-scale news\ncorpus and demonstrate that this generator is effec-\ntive in detecting neural fake news. More recently,\nFung et al. (2021) improve the controllability of the\ngenerated fake news by conditioning the generator\non knowledge elements, such as entities, relations\nand events, extracted from the original news arti-\ncle. Shu et al. (2021) enhance the factuality of the\ngenerated article by introducing a fact retriever that\nfetches relevant information from external corpora.\nMosallanezhad et al. (2021) utilize adversarial re-\ninforcement learning to generate topic-preserving\narticles. These studies have developed methods\nfor generating fake news that is hard to distinguish\nfrom real news to humans. Nevertheless, due to the\noverwhelming amount of inaccurate information\nintroduced and the lack of propaganda techniques\nin the generated texts, these approaches are sub-\noptimal for detecting human-written fake news, as\nshown in \u00a75.2. In contrast, our work generates\nfake news by incorporating propaganda techniques\nand preserving the majority of the correct infor-\nmation. Hence, our approach is more suitable for\nstudying defense against human-written fake news.\nAlso, since our released dataset is annotated with\nthe exact offset of the disinformative passages, this\nwork opens up future research opportunities on\ninterpretable detection of fake news.Propaganda Generation and Detection There\nis little previous study on propaganda generation.\nZellers et al. (2019) is the only relevant work that\nwe know of that studies the generation of pro-\npaganda to communicate targeted disinformation.\nOur work focuses on generating speci\ufb01c propa-\nganda techniques to bring the generated articles\ncloser to human-written fake news. To the best\nof our knowledge, we are the \ufb01rst to study the\nincorporation of speci\ufb01c propaganda techniques\ninto generated articles. Prior work on propaganda\ndetection mainly focuses on document-level de-\ntection. Early work collects propaganda datasets\nusing distant supervision (Rashkin et al., 2017) by\nassigning the same propaganda label to each news\noutlet under the same source based on the news-\nmedia-level label of corresponding news source\nlisted on trustworthy sites. However, classi\ufb01ers\ntrained on such datasets may only learn to recog-\nnize the bias of each news source instead of propa-\nganda (Da San Martino et al., 2020). Our dataset\navoids such issues by explicitly incorporating pro-\npaganda into each generated article. Furthermore,\nDa San Martino et al. (2019) present a fragment-\nlevel propaganda detection dataset, where speci\ufb01c\npropaganda techniques are labeled onto spans of\ntext instead of each document. Recent approaches\nfor detecting these propaganda techniques rely on\npre-trained transformers (Morishita et al., 2020;\nFeng et al., 2021). By contrast, we focus on detect-\ning disinformative articles with propaganda signals.\n7 Conclusions and Future Work\nWe have proposed a novel method for generat-\ning disinformation that is closer to human-written\nfake news. Evaluation on two human-written fake\nnews datasets, POLITI FACT andSNOPES , demon-\nstrates the effectiveness of our generated data\nPROPA NEWS in enabling better detection perfor-\nmance on human-written fake news. We hope that\nthe dataset presented in this work, PROPA NEWS,\ncan serve as enabling resources for the detection\nof human-written fake news and encourage future\nresearch in this direction. For future work, we\nplan to extend our approach to other languages and\ncover more propaganda techniques. We are also\ninterested in studying other aspects of fake news\ngeneration, such as novelty and elaboration, as well\nas engaging linguistic style.\n8 Limitations\nTo understand the gap between our automatic data\ngeneration method and fake news written by hu-\nmans, we expanded PN- SILVER to different sizes\nand compared the performance of ROBERT A-\nLARGE between trained on these generated data\nand the human-written fake news dataset, SNOPES .\nNote that since the TIMELINE 17dataset only con-\ntains around 4K samples, we additionally crawled\nNew York Times news articles as input to our gen-\nerator for the \u201c5 times\u201d to \u201c10 times\u201d experiments.\nThe results are shown in Figure 3. Although the\ndetector performance improves as we add more sil-\nvertraining data at \ufb01rst, it reaches a plateau after\nthe size is increased to 5 times. This illustrates that\nwhile our approach is more effective compared to\nbaseline generation methods, there is still a clear\ngap between our generated articles and human-\ncrafted fake news, likely in the aspects of styles\n(as discussed in \u00a75.2), intents (i.e. limited model-\ning of propaganda techniques), and falsehood (i.e.\nthe generated content is 100% false).\nDespite the advantages of our generation ap-\nproach, as compared to previous methods, it is un-\ncapable of generating other propaganda techniques\ncovered in (Da San Martino et al., 2019), such as\nstraw man . Thus, our method is not generic enough\nto handle all types of propaganda techniques within\na uni\ufb01ed framework. Moreover, our approach is\nlimited to generating English-only news articles,\nand cannot be applied to other languages.\n2x 4x 6x 8x 10x\nAmount of data5860626466AUC (%)\n Type\nPN-Silver\nSnopes\nFigure 3: Performance comparison of R OBERT A-\nLARGE on the P OLITI FACT dataset when trained on\nSNOPES and different size of PN- SILVER .9 Ethical Statement and Broader Impact\nOur objective for developing a generative approach\nthat produces more realistic news articles is to ad-\nvance the \ufb01eld of disinformation detection and to\nbring awareness that the current approaches for\ngenerating training data for fake news detection are\nsub-optimal.\nWe acknowledge that our generator may produce\ntoxic text as it was \ufb01ne-tuned on a propaganda\ndatasets. We also understand the dual-use concerns\nfor such a generation framework. One potential\nconcern is the possibility of using the generator\nto produce fake news for political gain or to sow\nsocial discord. Another concern is the potential for\nthe generator to be used to generate fake news that\ncould cause harm, such as false medical informa-\ntion or misleading \ufb01nancial advice. Additionally,\nthe generator might be used to create false evidence\nor fabricate information to support false allegations\nin legal or regulatory proceedings.\nTherefore, to contribute to future studies on\nhuman-written disinformation detection, we de-\ncided to release the codebase for only the detectors\nused in the experiments as well as the generated\ndata but not the generator.\nWe highlight some scenarios that illustrate ap-\npropriate and inappropriate uses of our generator:\n\u2022Appropriate: Researchers can use our framework\nto produce more challenging training data for learn-\ning stronger detectors.\n\u2022Inappropriate: The method should not be used to\nintentionally create or propagate false information.\n\u2022Inappropriate: The propaganda generation tech-\nnique should not be used for political campaigns or\nany malicious purposes.\nBoth inappropriate uses could lead to harmful con-\nsequences, such as undermining trust in the media\nand causing social unrest.\nAcknowledgement\nThis research is based upon work supported by U.S.\nDARPA SemaFor Program No. HR001120C0123\nand DARPA MIPs Program No. HR00112290105.\nThe views and conclusions contained herein are\nthose of the authors and should not be interpreted\nas necessarily representing the of\ufb01cial policies,\neither expressed or implied, of DARPA, or the\nU.S. Government. The U.S. Government is\nauthorized to reproduce and distribute reprints\nfor governmental purposes notwithstanding any\ncopyright annotation therein.\nReferences\nMarco T Bastos and Dan Mercea. 2019. The brexit bot-\nnet and user-generated hyperpartisan news. Social\nscience computer review , 37(1):38\u201354.\nTaylor Berg-Kirkpatrick, David Burkett, and Dan\nKlein. 2012. An empirical investigation of statis-\ntical signi\ufb01cance in NLP. In Proceedings of the\n2012 Joint Conference on Empirical Methods in Nat-\nural Language Processing and Computational Nat-\nural Language Learning , pages 995\u20131005, Jeju Is-\nland, Korea. Association for Computational Linguis-\ntics.\nAntoine Bosselut, Asli Celikyilmaz, Xiaodong He,\nJianfeng Gao, Po-Sen Huang, and Yejin Choi. 2018.\nDiscourse-aware neural rewards for coherent text\ngeneration. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) , pages 173\u2013\n184, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\nJacob Cohen. 1960. A coef\ufb01cient of agreement for\nnominal scales. Educational and psychological mea-\nsurement , 20(1):37\u201346.\nGiovanni Da San Martino, Stefano Cresci, Alberto\nBarr\u00f3n-Cede\u00f1o, Seunghak Yu, Roberto Di Pietro,\nand Preslav Nakov. 2020. A survey on computa-\ntional propaganda detection. Proceedings of the\nTwenty-Ninth International Joint Conference on Ar-\nti\ufb01cial Intelligence .\nGiovanni Da San Martino, Seunghak Yu, Alberto\nBarr\u00f3n-Cede\u00f1o, Rostislav Petrov, and Preslav\nNakov. 2019. Fine-grained analysis of propaganda\nin news article. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP) , pages 5636\u20135646, Hong Kong, China. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHernawan Dewatana and Siti Ummu Adillah. 2021.\nThe effectiveness of criminal eradication on hoax in-\nformation and fake news. Law Development Jour-\nnal, 3(3):513\u2013520.\nDimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj\nAlam, Fabrizio Silvestri, Hamed Firooz, Preslav\nNakov, and Giovanni Da San Martino. 2021. Detect-\ning propaganda techniques in memes. In Proceed-\nings of the 59th Annual Meeting of the Associationfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 6603\u20136617,\nOnline. Association for Computational Linguistics.\nChris Donahue, Mina Lee, and Percy Liang. 2020. En-\nabling language models to \ufb01ll in the blanks. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 2492\u2013\n2501, Online. Association for Computational Lin-\nguistics.\nZhida Feng, Jiji Tang, Jiaxiang Liu, Weichong Yin,\nShikun Feng, Yu Sun, and Li Chen. 2021. Al-\npha at SemEval-2021 task 6: Transformer based\npropaganda classi\ufb01cation. In Proceedings of the\n15th International Workshop on Semantic Evalua-\ntion (SemEval-2021) , pages 99\u2013104, Online. Asso-\nciation for Computational Linguistics.\nYi Fung, Christopher Thomas, Revanth Gangi Reddy,\nSandeep Polisetty, Heng Ji, Shih-Fu Chang, Kath-\nleen McKeown, Mohit Bansal, and Avi Sil. 2021.\nInfoSurgeon: Cross-media \ufb01ne-grained information\nconsistency checking for fake news detection. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n1683\u20131698, Online. Association for Computational\nLinguistics.\nKarl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NIPS .\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations .\nHamid Karimi and Jiliang Tang. 2019. Learning hier-\narchical discourse-level structure for fake news de-\ntection. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n3432\u20133442, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings .\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics , pages 7871\u20137880, Online. Association\nfor Computational Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , pages 3730\u20133740, Hong Kong,\nChina. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nPedro Henrique Martins, Zita Marinho, and Andr\u00e9 F. T.\nMartins. 2020. Sparse text generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4252\u20134273, Online. Association for Computational\nLinguistics.\nTerufumi Morishita, Gaku Morio, Hiroaki Ozaki, and\nToshinori Miyoshi. 2020. Hitachi at SemEval-2020\ntask 3: Exploring the representation spaces of trans-\nformers for human sense word similarity. In Pro-\nceedings of the Fourteenth Workshop on Semantic\nEvaluation , pages 286\u2013291, Barcelona (online). In-\nternational Committee for Computational Linguis-\ntics.\nAhmadreza Mosallanezhad, Kai Shu, and Huan Liu.\n2021. Generating topic-preserving synthetic news.\nIn2021 IEEE International Conference on Big Data\n(Big Data) , pages 490\u2013499.\nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav\nNakov, and Min-Yen Kan. 2020. Fang: Leveraging\nsocial context for fake news detection using graph\nrepresentation. In Proceedings of the 29th ACM in-\nternational conference on information & knowledge\nmanagement , pages 1165\u20131174.\nQiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt\nGardner, and Dan Roth. 2020. TORQUE: A reading\ncomprehension dataset of temporal ordering ques-\ntions. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 1158\u20131172, Online. Associa-\ntion for Computational Linguistics.\nLawrence Page, Sergey Brin, Rajeev Motwani, and\nTerry Winograd. 1999. The pagerank citation rank-\ning: Bringing order to the web. Technical Re-\nport 1999-66, Stanford InfoLab. Previous number\n= SIDL-WP-1999-0120.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and ZaidHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. In NeurIPS .\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog , 1(8):9.\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nV olkova, and Yejin Choi. 2017. Truth of varying\nshades: Analyzing language in fake news and po-\nlitical fact-checking. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 2931\u20132937, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh,\nJerret Ross, and Vaibhava Goel. 2017. Self-critical\nsequence training for image captioning. 2017 IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR) .\nNiloufar Salehi, Lilly C. Irani, Michael S. Bernstein,\nAli Alkhatib, Eva Ogbe, Kristy Milland, and Click-\nhappier. 2015. We are dynamo: Overcoming stalling\nand friction in collective action for crowd workers.\nInProceedings of the 33rd Annual ACM Conference\non Human Factors in Computing Systems , CHI \u201915,\npage 1621\u20131630, New York, NY , USA. Association\nfor Computing Machinery.\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and\nNanyun Peng. 2021. \u201cnice try, kiddo\u201d: Investigating\nad hominems in dialogue responses. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 750\u2013767, On-\nline. Association for Computational Linguistics.\nKai Shu, Yichuan Li, Kaize Ding, and Huan Liu. 2021.\nFact-enhanced synthetic news generation. Proceed-\nings of the AAAI Conference on Arti\ufb01cial Intelli-\ngence , 35(15):13825\u201313833.\nKai Shu, Deepak Mahudeswaran, Suhang Wang, Dong-\nwon Lee, and Huan Liu. 2018. Fakenewsnet: A data\nrepository with news content, social context and dy-\nnamic information for studying fake news on social\nmedia. arXiv preprint arXiv:1809.01286 .\nGiang Binh Tran, Tuan Tran, Nam Khanh Tran,\nMohammad Alrifai, and Nattiya Kanhabua. 2013.\nLeveraging learning to rank in an optimization\nframework for timeline summarization.\nSander van Der Linden, Jon Roozenbeek, and Josh\nCompton. 2020. Inoculating against fake news\nabout covid-19. Frontiers in psychology , 11:2928.\nHerman Wasserman and Dani Madrid-Morales. 2019.\nAn exploratory study of \u201cfake news\u201d and media trust\nin kenya, nigeria and south africa. African Journal-\nism Studies , 40(1):107\u2013123.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112\u20131122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine learning , 8(3):229\u2013256.\nXueqing Wu, Kung-Hsiang Huang, Yi Fung, and Heng\nJi. 2022. Cross-document misinformation detection\nbased on event graph reasoning. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers) .\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Process-\ning Systems 32 .\nA Distribution of Propaganda\nFigure 4 shows the distribution of the propaganda\ntechniques used in the human-written fake news we\ncollected and analyzed in \u00a71. Note that one article\nmay contain multiple propaganda techniques.\n0 2 4 6 8\nCountLoaded language\nAppeal to authority\nName calling\nStraw man\nReductio ad hitlerum\nCausal oversimplificationPropaganda technique\nFigure 4: Total counts of the propaganda techniques\nused in the human-written fake news we analyzed.\nB Additional Research Questions\nQ1: Is the detector learning to distinguish be-\ntween fake/real news articles or simply learning\nto detect the use of propaganda information?\nIn Table 3, PROPA NEWS w/o AA & LL is the\nvariant of our proposed dataset with both propa-\nganda techniques removed. By training detectors\non this version of the proposed dataset, the model is\nstill effective in identifying human-written articles\ncontaining false information. Therefore, the detec-\ntors trained on our generated data have learned to\ndistinguish between fake and real articles instead\nof exploiting propaganda information only. On\nthe other hand, comparing the detectors trained\nonPROPA NEWS and their counterparts trained on\nPROPA NEWS w/o AA & LL in Table 3, we see\nthat propaganda information can serve as additional\nevidence that helps improve the detection of real\nhuma-written fake news.\nAdditionally, we wanted to emphasize that fake\nnews detection is an extremely challenging task\nthat requires both factual and stylistic analysis as\ndemonstrated by our experiments as well as by\nthe relatively low performance of the prior SOTA\nmodel.\nQ2: Do real articles make use of propaganda\ntechniques, such as appeal to authority and\nloaded language ?The similarity between our\ngenerated text and the real articles in PolitiFact\nis 7.3% as per the MAUVE metric, which is much\nlower than the similarity between the generated\ntext and the fake news articles, as discussed in \u00a75.2.\nIt is possible that some real news articles can alsocontain propaganda. However, according to the\nMAUVE metric, the real articles in POLITI FACT\ndo not contain much loaded language or appeal to\nauthority.\nC Further Analysis\nC.1 Remaining Challenges\nTo better understand the remaining disinformative\narticles that the detectors failed to identify, we\nconduct an analysis by comparing the ROBERT A\npredictions and the labels. Three major modeling\ncapabilities required for successful detection are\nidenti\ufb01ed, as listed below:\nStatic knowledge enrichment About 30% of\nthe misclassi\ufb01cation is resulted from the lack\nof static knowledge that can be found in public\ndatabases, such as law dictionaries. For example,\nin this article10, Alexandria Ocasio Cortez falsely\nstates that the U.S. Immigration Customs Enforce-\nment (ICE) is required to \ufb01ll 34,000 beds every day.\nAccording to the Appropriations Act of 201611,\nhowever, ICE is only required to detain 34,000\navailable beds. Therefore, to detect such a misin-\nformation, the detector needs to be enriched with\nstatic knowledge bases.\nDynamic knowledge acquisition Around 48%\nof the misclassi\ufb01ed human-written disinformation\nare caused by the inability to acquire dynamic\nknowledge from new news sources. For instance,\nCOVID-related articles are usually published after\n2020, while ROBERTA was pre-trained on news\narticles released before 2019. It is very challenging\nforROBERTA to detect disinformation of such\ntopics unless the detector is equipped with the ca-\npabilities of acquiring dynamic knowledge from\nnews articles. Particularly, ROBERT Aachieves an\naccuracy of 69.0% on detecting fake articles pub-\nlished before 2019, but its accuracy drops to 51.9%\nwhen testing on articles published after 2019.\nMulti-document reasoning The rest of the in-\ncorrect detection is caused by the lack of multi-\ndocument reasoning ability. For instance, a news\narticle12wrongly associates Hillary Clinton with\na \ufb02awed immigration policy of the former govern-\nment, and strengthens such a statement by referring\n10https://tinyurl.com/static-knowledge\n11https://www.congress.gov/114/bills/\nhr2029/BILLS-114hr2029enr.pdf\n12https://tinyurl.com/multi-doc\nto a Senate report and relevant news articles. How-\never, the cited report does not mention Clinton, and\nthe other news articles contain disinformation. To\ncorrectly detect this piece of disinformation, detec-\ntors should be capable of reasoning across multiple\ndocuments.\nD Qualitative Examples of Generated\nArticles\nIn Table 8, we show a comparison of generated\narticles given the same input data across different\ngenerative methods. Our approach produces arti-\ncles with a small fraction of inaccurate information,\nwhich matches a property of human-written fake\nnews discussed in \u00a71.\nE Appeal to Authority Details\nTo recap, we \ufb01rst gather a list of authorities Z\nfor each article from Wikidata and the correspond-\ning context. The best appeal to authority se-\nquences\u0098is selected with the lowest perplexity\ns\u0098\u0000argminsiPPL\u0000si\u0006wheresidenotes the gen-\nerated sequence using zias the authority. However,\nthis process results in every sequence s\u0098contain-\ning the substring \u201ccon\ufb01rms that\u201d, which makes it\ntrivial for detectors to classify these generated doc-\numents as fake by simply detecting such substrings.\nTherefore, we devise an algorithm to diversify the\ntemplates so that these generated articles are not\neasily detectable.\nFirst, we de\ufb01ne a set of verbs Vthat can be\nswapped with \u201ccon\ufb01rms\u201d: V\u0000rsaid,concluded ,\ncon\ufb01rmed ,emphasized ,stated ,arguedx. Then, we\ndiversify the generated structure of the generated\nsentences\u0098by reordering the subject, verb, and\nobject. Next, we swap the verb with a another\nverb fromV. Finally, to diversify the context,\nwe append a preposition from the preposition set\nPP\u0000ron,at,inxto the output of the previous\nstep, and then feed the sequence to BART to gen-\nerate the context. An example of this process is\nprovided in Table 6.\nF Intermediate Pre-training Details\nFor domain adaptation, we perform intermediate\npre-training (IPT) on the CNN/DM dataset, a large\nsummarization corpus containing more than 280K\nnews articles from CNN and Daily Mail. The IPT\nobjectives for disinformation generation and propa-\nganda generation are mostly the same as described\nin previous sections, but with some minor changesDetector Dev Acc. (%) Test Acc. (%)\nHDSF 52.4 ( \u00190.6) 50.6 ( \u00192.4)\nBERT 57.7 (\u00191.0) 58.0 ( \u00191.2)\nGROVER 60.3 (\u00195.8) 63.3 ( \u00195.0)\nROBERT A 70.5 (\u00190.3) 69.8 (\u00191.1)\nTable 5: Evaluation of various detectors on the\nPROPA NEWS development and test set. We report the\nmean and standard deviation of four runs.\ndue to different goals in the IPT phase. When per-\nforming IPT for disinformation generation, we re-\nmoveLsfrom the \ufb01nal loss function (Equation (4))\nas the goal for IPT is only to learn to generate co-\nherent sentences. In addition, to create training\nsamples for loaded language IPT, we gather all\nthe appearances of adjectives pointing to a noun\nor adverbs pointing to a verb via dependency pars-\ning graphs without considering whether the sam-\nples contain loaded terms since the goal here is\nto enable BART to identify where to insert which\nadjectives or adverbs.\nG Benchmarking Detectors\nThe performance of various detectors on the\nPROPA NEWS dataset is shown in Table 5. We \ufb01nd\nthatROBERT AandGROVER demonstrate advan-\ntages over BERT. This could be explained by the\nfact that ROBERT AandGROVER are pre-trained\non news domain corpora, whereas BERT has no\naccess to such domains during pre-training. In ad-\ndition, we \ufb01nd that HDSF performs much worse\nthan the other three models. This re\ufb02ects that large-\nscale pre-training of language models brings more\nbene\ufb01t to detection performance than explicit mod-\neling of discourse-level features.\nH Human Validation Details\nIn this section, we describe the details of human val-\nidation where AMT workers are tasked to validate\nwhether the generated sentences contain inaccurate\ninformation. We recruit AMT workers from the\nUnited States and Canada. To ensure the annota-\ntion quality, only workers who have an acceptance\nrate greater than 95% and have more than 100 ac-\ncepted HITs in the past are allowed to work on our\nannotation task. This greatly reduce the chances of\ncollecting annotations from scammers. Each HIT\nwas designed such that the annotators are rewarded\n$12-$15 per hour, which complies with the ethical\nresearch standards outlined by AMT (Salehi et al.,\n2015). In each HIT, the annotators are presented\nan article with the generated part marked in bold-\nStep Generated Sequence\n1 Panmure GordonanalystPeterHitchens con\ufb01rmed that\u201ctheUSgovernment islikely toagree toreduce its\nestimate ofthesizeofthespill, which would cutBP\ufb01nes \u201d.\n2 \u201c The US government is likely to agree to reduce its estimate of the size of the spill, which would cut BP\n\ufb01nes, \u201d Panmure Gordon analyst Peter Hitchens con\ufb01rmed.\n3 \u201c The US government is likely to agree to reduce its estimate of the size of the spill, which would cut BP\n\ufb01nes, \u201d Panmure Gordon analyst Peter Hitchens said.\n4 \u201cTheUSgovernment islikely toagree toreduce itsestimate ofthesizeofthespill, which would cutBP\n\ufb01nes, \u201dPanmure GordonanalystPeterHitchens saidinaconference.\nTable 6: An illustration of how appeal to authority is performed. In step 1, we generate a statement using BART\nwith the pre\ufb01x \u201cPanmure Gordon analyst Peter Hitchens con\ufb01rmed that \u201c \u201d. In step 2, we move the subject and\nverb to the back of the sentence to diversify the sentence structure. In step 3, we swap the verb with another verb\nfrom the verb set V. In step 4, we append a preposition into the sequence in step 3 and use the resulting sequence\nas pre\ufb01x to BART\u2019s decoder to generate the rest of the context. For step 1 and step 4, we mark the pre\ufb01x sequence\nto the decoder in yellow, and the generated sequence in blue. To increase the diversity of the generated sequences,\nstep 2 to step 4 are each performed 50% of the time.\nface. The questions and guidelines are illustrated\nas follows. (Note that we only use the annotators\u2019\nresponse for Q1 to validate our generated data. The\nannotations for the other questions will be used for\nfuture research.)\nQ1: Is the generated text in boldface Accurate\norInaccurate ? (If you cannot \ufb01nd any supporting\nevidence, please select Inaccurate .) Note that: A\nstatement (in quotation marks) made by a person is\nonly accurate if this person actually made the exact\nsame statement. If the statement in quotation marks\nis just a paraphrase of what the person actually said,\nthen the statement is inaccurate.\n-Inaccurate : Any false information presented\nin the generated text makes it inaccurate.\n-Accurate : All the information in the gener-\nated text must be accurate.\nQ2: Enter the URL of the news article you found\nthat supports your decision in the previous response\nin the below box. Put down \u201cfrom context\" if the\nevidence can be found in the context.\nQ3: Does the generated text in boldface delivers\nthe same sentiment as the rest of the article?\n-False : The sentiment of the generated text is\nNOT the same as the rest of the article.\n-True : The sentiment of the generated text is\nthe same as the rest of the article.\nQ4: Is the discourse of the generated text in\nboldface consistent with the rest of the article?\n-False : The discourse of the generated text is\nNOT consistent with the rest of the article.-True : The discourse of the generated text is\nconsistent with the rest of the article.\nQ5: If there is any grammatical error or inconsis-\ntent discourse, please rewrite the correct the gener-\nated text and put it in the below box. Just put down\nthe corrected generated text in bold is enough. For\nexample, \u201cHarry is a boy. He likes go to school.\u201d\nPlease put in \u201cHe likes to go to school.\u201d in the box\nbelow.\nI Statistics of the Evaluation Datasets\nIn Table 7, we show the statistics of the two evalua-\ntion datasets used in our experiments. The reported\nnumbers are not the same as listed in the original\npapers (Nguyen et al., 2020; Shu et al., 2018) since\nsome of the articles are no longer accessible via the\nprovided URLs.\nDataset # Real # Fake\nSNOPES 430 280\nPOLITI FACT 517 369\nTable 7: Statistics of the two evaluation datasets,\nSNOPES and P OLITI FACT.\nJ Detector Implementation Details\nForBERT adnROBERT Aexperiments, we use\nAdamW (Loshchilov and Hutter, 2019) as the op-\ntimizer with a batch size of 2 and gradient accu-\nmulation steps of 8. We set the learning rate and\nweight decay to 5e-5 and 1e-5 for the parameters\nthat have been pre-trained, and 1e-3 and 1e-3 for\nother parameters. For experiments on the GROVER\ndetector, we follow the original detection setting.\nGROVER is trained using Adam (Kingma and Ba,\n2015) with a learning rate of 2e-5 and a batch size\nof 64. Similarly, we follow the original recipe to\ntrain HDSF , which is optimized with Adam with a\nlearning rate of 1e-2. All detectors are \ufb01ne-tuned\nfor at most 20 epochs where the best model is de-\ntermined by the accuracy on the development set.\nAll experiments are conducted on a Ubuntu\n18.04 machine with NVIDIA Tesla V100. We\nuse PyTorch 1.10.0 and Transformers 4.3.0 for\nconstructing all models and loading pre-trained\nweights, except for GROVER , which operates on\nTensor\ufb02ow 1.13.1. The training time for BERT and\nROBERT A, which contains around 340M param-\neters, is around 2-3 hours, while the training time\nforGROVER , which contains 355M parameters, is\naround 1 hour.\nK Human Evaluation Details\nIn this section, we describe the survey we deliver\nto AMT workers for evaluating the quality of the\ngenerated articles. Annotators are presented a gen-\nerated article and asked to answer a few questions\nregarding the quality of it. Q2is only applicable for\nevaluating generated articles from PROPA NEWS,\nin which we show the sentence that contains pro-\npaganda. The low, medium, and high ratings in\nthe response correspond to 1, 2, and 3 scores de-\nscribed in \u00a75.2. The questions and guidelines are\nillustrated as follows:\nQ1: How plausible do you think of the article\nabove?\n-Low: It likely contains inaccurate informa-\ntion.\n-Medium : Not sure.\n-High : It unlikely contain inaccurate informa-\ntion.\nQ2: How much does this sentence in the article\naffects your decision on the previous answer?\n-Low : This sentence does not affect my answer\nfor the previous question.\n-Medium : This sentence somehow affect my\nanswer for the previous question.\n-High : This sentence largely affects my an-\nswer for the previous question.The score distribution for Q1is shown in Fig-\nure 5. We demonstrate that our approach produces\nhigher quality fake news compared to GROVER -\nGEN.\n0 50 100 150 200 250 300OursGroverMethodHuman Evaluation Breakdown\n1\n2\n3\nFigure 5: Breakdown scores of our human evaluation.\nThe x-axis denotes the counts of evaluators votes for a\nscore, while the y-axis denotes different methods.\nL Scienti\ufb01c Artifacts\nWe re-purposed the Timeline17 dataset (Tran et al.,\n2013) to produce our P ROPA NEWS dataset.\nAmong the software we use, SpaCy\n(\u201cen_core_web_md\u201d) is licensed under the\nMIT License, Transformers and Stanza are li-\ncensed under the Apache License 2.0, and PyTorch\nis released under the Modi\ufb01ed BSD License. In\nterms of models, both BART and the NLI model\nwe used are licensed under the MIT License.\nIn summary, all artifacts are produced with soft-\nwares and models released under open-source li-\ncenses. We will release the PROPA NEWS dataset\nas well as the detectors with licenses compatible\nwith the software discussed above such that they\nare consistent with their intended use.\nMethod Generated Article\nGROVER -GEN Libya torestart three oil\ufb01elds onSept 1:ministerBENG HAZI ,Libya ,Aug 28(Reuters )-Libya\nwillrestart twoeasternoil\ufb01elds inmid-Septem berandresume crude shipment from Tobruk bytheend\nofthemonth ,SNCM deputy oilministerHabib Hummed saidonSunday.Hesaidthewest ofSirte\nwasbeingrestored andathird \ufb01eld intheeasternprovince ofBrega wasabout torestart operations .\u201c\nProductionattheBrega \ufb01eld hasbeen increased bytwotofour percent andwillresume itsproduction\nontheendofthemonth ,\u201dhesaidatapoliticalconference .\u201cThesecondphase ofproductionatthe\neastern\ufb01eld ofSirte came tofullproductionbytheendofthemonth andrestart ingtheproduction\nfrom Brega \ufb01eld willbegininSeptem ber,\u201dhesaid.Hesaidproductionatthewest ofSirte andeast\nofTobruk hadalready reached 95,000 barrelsperday(bpd)ontheir main export terminals . Libya\n, awash with millions of dollars , most of it believed to have been siphoned off through crude theft\n, is pumping around 290,000 bpd in total . The \ufb01rst fuel terminals in Libya have reopened and are\nloaded with diesel and petrol and tankers from Europe are ploughing in cargo to get through a key\nsmuggling route from Derna to the Mediterranean . Libya wassharply divided lastyear when armed\ntribes men struck crude export terminals along itsborder,deny ingitthelucrativegasexports itwas\nusingtoservice south ernEUcoun tries .Libya \u2019ssouth ernoilterminalwashitinSeptem berbymilitant\nattacks .(WritingbyUlfLaess ing;EditingbyDavid Holmes )\nFACTGEN Shares Augmented andstored pipelines replaced bylight pipes .Disputed concerns over biggest damage\ntoLibya since 2003 .Despite concerns oilcould beleaking.\u2019Rus siasays productionissafe andit\nisready toresume production.\u2019Greece could usemore sulfuroilby2020 .\u2019Energy ministerssayit\ncould beused topower two-thirds oftheworld econ omy .\u2019Saudi hadbeen thebiggest oilproducer\nintheworld beforemeltdown .\u201dFrench tocontinue tocontaincrisis.\u201dRus siaonLibyan crisis:\u2018we\narefacingaseriousandcontinuingpublichealth crisisthatisneeded \u2019\u201dBut notinavacuum ofoil\n\u2019\u201dWorld affairs .\u201dGreece :\u2018weneed tobedoingmore tocombatdamagetotheenergy system.\u2019\u201d\nleftasingalloilfrom Libya crisis.\u201d\nOurs ( PN- SILVER )Tue Aug 23 , 2011 4:05 pm EDT * Limited Libyan oil production could start soon * Condition of oil\nexport terminals still unclear * Clearing pipes clogged by waxy crude could delay output By David\nSheppard and Sarah Young NEW YORK/LONDON , Aug 23 -LRB- Reuters -RRB- - Limited Libyan\noil production ought to resume quite quickly as most \ufb01elds appear to have survived the civil war\nunscathed , international oil services companies say , but many are still waiting for more stability in the\ncountry before sending employees back in . In interviews with some of the biggest oil services players\nin pre-civil war Libya , most echoed the view that major oil \ufb01elds had largely avoided serious damage\nduring the six months of \ufb01ghting , though some voiced concerns about damage to export terminals\nand pipelines . OPS International Chairman Gavin De Salis told Reuters Insider television that Libyan\ncrude oil , prized for its high yield of valuable light products such as gasoline and for its low sulfur\ncontent , was quite waxy , which could clog up pipelines if they had been left unused for some time . \u201c\nThere might be a little bit of effort unplugging pipelines , which is two to three months \u2019 worth of effort\nbefore they can resume full production , \u201d De Salis said . \u201c But that will not affect all of the pipelines\nor all of the \ufb01elds , so they can certainly start limited production quite quickly . \u201d Nilsson said contacts\nat Libya \u2019s rebel oil \ufb01rm Arabian Gulf Oil Company -LRB- AGOCO -RRB- informed him there had\nbeen little damage to the oil\ufb01elds in the east of the country during the six-month power struggle . \u201c We\nhave n\u2019t been able to work at the oil\ufb01elds during the civil war as it has not been safe , but I think within\na couple of weeks we could be back to almost normal , \u201d Nilsson said by telephone from his of\ufb01ce in\nStockholm . \u201c The oil income is essential to Libya and the new government so they will want to bring\nit back online as soon as possible . \u201d Nilsson said they had several Swedish , Indian and Sudanese\nemployees who had stayed in the country during the civil war , but total staff numbers in the country\nwere down from around 250-300 . Nilssonsaidthere wasstillalotofwork tobedone inthecoun try\n.DeSalis said that\u201calotofdamage\u201dhadbeen done toLibya \u2019soilinfrastructure,includingthe\ndestruc tionofsome ofthecoun try\u2019smain oilexportterminals,buthesaiditwastooearly toestimate\nthefullextentofthedamage. DAMAGE Oil \ufb01rm \u2019s who supported the rebel government during the\ncivil war are expected to win the lion \u2019s share of contracts to help relaunch the Libyan oil industry ,\nwhich before the war produced some 1.6 million barrels per day of crude ...\nTable 8: A qualitative comparison between generated articles from different approaches. The texts marked in or-\nange indicate disinformation, and the texts in blue denote propaganda. We see that other approaches generate a\nlarge amount of inaccurate information, which contrasts with a property of human-written fake news mentioned\nin \u00a71. We also note that the article generated using F ACTGENappear to be low-quality. This is likely caused\nby the fact that the checkpoints reported in the paper were not released and we train F ACTGENfrom scratch by\nclosely following the recipe described in Shu et al. (2021). It is possible that some details of the training process\nof F ACTGENwere missing from the paper, and hence the low generation quality.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Faking fake news for real fake news detection: Propaganda-loaded training data generation", "author": ["KH Huang", "K McKeown", "P Nakov", "Y Choi"], "pub_year": "2022", "venue": "arXiv preprint arXiv \u2026", "abstract": "Despite recent advances in detecting fake news generated by neural models, their results  are not readily applicable to effective detection of human-written disinformation. What limits"}, "filled": false, "gsrank": 188, "pub_url": "https://arxiv.org/abs/2203.05386", "author_id": ["Yuk2_IMAAAAJ", "ujDhg2sAAAAJ", "DfXsKZ4AAAAJ", "vhP-tlcAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:Efe5J_NU1gMJ:scholar.google.com/&output=cite&scirp=187&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D180%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=Efe5J_NU1gMJ&ei=JbWsaMu1E-HUieoP9LKZ6AI&json=", "num_citations": 77, "citedby_url": "/scholar?cites=276501830450149137&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:Efe5J_NU1gMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2203.05386"}}, {"title": "Political polarization in online news consumption", "year": "2021", "pdf_data": "Political Polarization in Online News Consumption\nKiran Garimella,1Tim Smith,2Rebecca Weiss,2Robert West3\n1MIT,2Mozilla,3EPFL\ngarimell@mit.edu, tdsmith@mozilla.com, rweiss@mozilla.com, robert.west@ep\ufb02.ch\nAbstract\nPolitical polarization appears to be on the rise, as measured\nby voting behavior, general affect towards opposing partisans\nand their parties, and contents posted and consumed online.\nResearch over the years has focused on the role of the Web\nas a driver of polarization. In order to further our understand-\ning of the factors behind online polarization, in the present\nwork we collect and analyze Web browsing histories of tens\nof thousands of users alongside careful measurements of the\ntime spent browsing various news sources. We show that on-\nline news consumption follows a polarized pattern, where\nusers\u2019 visits to news sources aligned with their own politi-\ncal leaning are substantially longer than their visits to other\nnews sources. Next, we show that such preferences hold at\nthe individual as well as the population level, as evidenced\nby the emergence of clear partisan communities of news do-\nmains from aggregated browsing patterns. Finally, we tackle\nthe important question of the role of user choices in polar-\nization. Are users simply following the links proffered by\ntheir Web environment, or do they exacerbate partisan polar-\nization by intentionally pursuing like-minded news sources?\nTo answer this question, we compare browsing patterns with\nthe underlying hyperlink structure spanned by the considered\nnews domains, \ufb01nding strong evidence of polarization in par-\ntisan browsing habits beyond that which can be explained by\nthe hyperlink structure of the Web.\n1 Introduction\nMany people\u2014according to the Pew Research Center, one\nin three Americans in 2018 (Geiger 2019)\u2014prefer to re-\nceive their news online. Market data on digital publishers\nfurther indicates that search engines and social media are\ndominant sources of referrals to online news content, total-\ning over 75% of referral traf\ufb01c (Parse.ly 2016). Taken to-\ngether, these numbers suggest that, for many typical news\nconsumers, the news they see is to a large part determined\nby the online platforms they frequent.\nAt the same time, recent research (Rogowski and Suther-\nland 2016; Webster and Abramowitz 2017; Iyengar et al.\n2019) has highlighted polarization of political beliefs among\nthe general population, with speci\ufb01c attention to the relation-\nship between the agendas of political parties and the partisan\nCopyright \u00a9 2021, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.appeal of online news organizations. As online news con-\nsumption increases and evidence of polarization among the\ngeneral population accumulates, understanding the mech-\nanisms of partisan information-seeking behaviors and the\nproperties of online news environments grows in impor-\ntance.\nNot only do social media constitute an important entry\npoint for discovering news content; endorsements of news\non social media have also been identi\ufb01ed as a factor by\nwhich people trust news sources, presumably because en-\ndorsements represent a vote in favor of the source\u2019s credibil-\nity (Messing and Westwood 2014). Search engines\u2014another\nimportant entry point for discovering news content\u2014are\ndriven by algorithms that exhibit a certain degree of political\npolarization, and additionally, the input queries of individual\nusers themselves are also polarized in terms of their partisan\nalignment (Kulshrestha et al. 2017). Polarization, thus, is the\nproduct of both individual preferences and properties of the\ncontent platforms used to seek out exposure to news.\nIn this paper, we examine the information-seeking behav-\niors of individual Web users in order to contribute to under-\nstanding the relationship between mass polarization and the\nconsumption of news content on the Web. To address this\ngeneral research question, we analyzed a novel, large-scale\ndataset consisting of browsing logs collected via an obser-\nvational \ufb01eld study design.\nWe found evidence that supports prior research (Flaxman,\nGoel, and Rao 2016; Bakshy, Messing, and Adamic 2015)\nindicating the existence of partisan preferences towards on-\nline news exposure, whether measured as the visit share in\nbrowsing histories or as the amount of time spent actively\nengaging with partisan news sources. Indeed, our analysis\nof time spent as opposed to browsing history provides more\nunequivocal evidence of partisan selectivity in news sources,\nstrongly endorsing the perspective that online news con-\nsumers pursue like-minded partisan sources for political in-\nformation. We \ufb01nd that such polarized preferences also hold\nwhen aggregating behavior across users.\nWe also explore the extent to which personal preferences\nfor news sources that cater to like-minded partisan audiences\ndictate polarized browsing behaviors, as opposed to the ho-\nmophilic network structure of news sources on the Web.\nSince online news sources are more likely to contain links\nand references to like-minded partisan content, we consid-\nProceedingsoftheFifteenth InternationalAAAIConferenceonWebandSocial Media(ICWSM2021)\n152\nered the possibility that polarized browsing patterns could\nmostly be attributed to the nature of content options pre-\nsented to a typical online news consumer. Indeed, one of\nour main contributions consists of our efforts to isolate the\nextent to which individual partisan preferences for political\ninformation contribute to polarization patterns in news expo-\nsure, by separating the effects of user preferences from those\nof the biased hyperlink network of online news content. To\ndo so, we compared polarization patterns in two sources of\ndata: a co-browsing graph consisting of the direct observa-\ntion of political information-seeking patterns of users, and a\nhyperlink graph obtained from a crawl of said news content\nduring the same study period. Our analysis provides strong\nevidence that users speci\ufb01cally prefer browsing paths that\nlead towards polarized content, indicating that user choices\nplay an important role in polarization.\nIn summary, we make the following contributions:\n1. We collected browsing histories of tens of thousands of\nusers in order to better understand polarization in online\nnews consumption (Section 3).\n2. We provide new evidence that considering the amount of\ntime spent on news sources typically preferred by par-\ntisans reveals a different extent of polarization than ob-\nserved in the prior literature, which has mostly been based\non whether a user visited a site or not (Section 4.1 and\n4.2).\n3. We \ufb01nd that such polarization is prevalent at both the indi-\nvidual and the population level. We show that aggregated\nco-browsing graphs created from co-visited domains have\na clear community structure consisting of left-, center-,\nand right-leaning domains (Section 4.3).\n4. We address the question of whether polarization of news\nconsumption can be attributed to the biased link structure\nof online news networks alone, or whether users\u2019 explicit\ncontent choices contribute as well. We perform a novel\ncomparison of two distinct data sources (co-browsing\nchoices of users vs. the link structure of the same content)\nand conclude that link structure alone cannot account for\nthe high levels of selective exposure exhibited by online\nnews consumers (Section 5).\n2 Related Work\nSelective exposure in online news browsing. A popular ex-\nplanation given for partisan polarization in news browsing\nisselective exposure (Klapper 1960; Stroud 2010; Sears and\nFreedman 1967), the observation that individuals prefer to\nexpose themselves to information that reinforces their exist-\ning attitudes and interests. As a result, many have suggested\nthat partisan preferences predict the ideological composi-\ntion of news content that people will select for consumption\n(Sunstein 2009; Pariser 2011).\nBut the impact of partisanship on selective exposure re-\nmains an open research question, as studies are mixed as to\nthe extent to which the general population exhibits polarized\nbrowsing habits. Much of the general population appears to\nconsume a centrist media diet, with a long tail of more ex-\ntreme partisans who exclusively visit partisan-leaning web-sites (Nelson and Webster 2017). Flaxman, Goel, and Rao\n(2016) examined the Web-browsing histories of 50,000 US-\nbased users of the Bing toolbar and highlighted that a ma-\njority had the highest density of visits on a few mainstream\nnews outlets. Guess (2018) used data from the online Web-\ntracking service YouGov and concluded that there was a\nsmall group of highly partisan and active users who drove\na disproportionate amount of traf\ufb01c to ideologically slanted\nwebsites.\nVarious laboratory-based studies have illustrated how in-\ndividual-level selective exposure could explain large polar-\nization effects in news exposure. Iyengar and Hahn (2009)\nexperimentally manipulated source labels of news sites to\nshow preferences in sources rather than content, and showed\nthat study participants were consistent in their selection of\nlike-minded sources for both political and non-political in-\nformation. Garrett (2009) found selective preference among\nusers from partisan websites, but also observed that a pref-\nerence for like-minded content did not imply an aversion to\nchallenging information; while people spent time on content\nthat reinforced their beliefs, they also spent time on content\nwith which they disagreed.\nGentzkow and Shapiro (2011) measured polarization as\nthe degree of ideological segregation in online and of\ufb02ine\nnews exposure, as well as face-to-face interactions, and de-\ntermined that the Web was no more polarized than other\nmeans of consuming political information. But in a \ufb01eld\nstudy similar to our panel design, Peterson, Goel, and Iyen-\ngar (2018) scrutinized online browsing histories and con-\ncluded that polarized browsing habits appeared to be 2\u20133\ntimes larger than previous studies had estimated. Thus, there\nappears to be a lack of consensus between \ufb01eld and labora-\ntory studies as to whether the Web enables greater selective\nexposure to political information in news content.\nPolarization in social media and search engines. As news\nconsumption moves to social media (Barthel et al. 2015),\nresearchers have started to investigate how sharing habits of\nnews content on social media platforms are polarized along\npartisan lines. Numerous researchers have focused on po-\nlarization effects in individual interactions on social me-\ndia platforms (Schmidt et al. 2017; Narayanan et al. 2018;\nGarimella et al. 2018). For example, Raghavan, Anderson,\nand Kleinberg (2018) examined the structure of online in-\nteractions in order to determine how different media sources\nare \u201cinvoked\u201d in replies on social media posts, concluding\nthat polarization exists within the context of the way people\nuse social media platforms outside of news exposure.\nExisting research has, however, not reached a consensus\non whether the use of social media networks and search en-\ngines increases the likelihood of polarized browsing habits\n(Nelson and Webster 2017; Flaxman, Goel, and Rao 2016;\nNikolov et al. 2019). For example, Barber \u00b4a et al. (2015)\nshowed that polarization in news consumption on social\nmedia had been overestimated, while studies using survey\ndata (Barthel et al. 2015) showed that people self-report\npolarized browsing habits. Bakshy, Messing, and Adamic\n(2015) compared the role of individual choices and algo-\nrithmic personalization on news consumption on Facebook\n153\nand concluded that personal preferences, rather than content\nrecommendations on social feeds, drove content choices.\nMeasuring political leaning of sources and users. Social\nmedia user behavior has also been used to identify polarity\n(or leaning) scores of news domains and hence to quantify\npolarization. Bakshy, Messing, and Adamic (2015) created\na list of 500 news domains and assigned a leaning score to a\nnews domain based on the fraction of self-identi\ufb01ed con-\nservative users on Facebook who had shared the domain.\nA similar, yet much larger, list was created by Robertson\net al. (2018), who used a panel of Twitter users of self-iden-\nti\ufb01ed political leaning in order to compute leaning scores\nfor sources. Following a similar methodology, Kulshrestha\net al. (2017) used lists of Twitter users who were tagged\nas Democrats or Republicans and quanti\ufb01ed the leaning of\nusers based on whom they followed from these lists. Then,\nbased on the following of various news organizations on\nTwitter, they identi\ufb01ed the leaning of news domains. Ribeiro\net al. (2018) made use of Facebook\u2019s advertising platform\ndata to compute the leaning of a news source by the ex-\ntent to which liberals or conservatives were over- or under-\nrepresented among its audience. Finally, Lahoti, Garimella,\nand Gionis (2018) made use of the relationship between\nuser preferences and their news consumption in order to\njointly estimate both user leaning and news domain leaning\non Twitter.\n3 Data and De\ufb01nitions\nWe randomly recruited participants from the active Firefox\nuser population on 5 April 2018 for enrollment in an exten-\nsion-based study. The sampled participants were presented\nwith a prompt asking whether the user was interested in\nparticipating in a research study involving close scrutiny of\ntheir browsing behavior, including their browsing histories.\nThe prompt and process of informed consent was approved\nby Mozilla\u2019s legal and ethical processes. When a user con-\nsented, a browser extension was installed, which created a\nrecord logging the user\u2019s active dwell time on content or-\nganized by URL for each day of browsing activity. Dwell\ntime is implemented via the nsIdleService API of the\nbrowser, which checks an operating system\u2013level idle de-\ntection API every \ufb01ve seconds. In short, this method creates\na log of activity per URL that is persistent across multiple\ntabs and windows. This is novel compared to many other\nmethods of measuring attention to online content, which typ-\nically measure duration as a result of analytics or tracker in-\nformation visible from server logs or simply browsing his-\ntory without duration measurements. If a participant opened\na link but spent very little time engaging with the content\nof the page, browsing history\u2013based measurements would\nweigh such a visit equally to a high-engagement visit.\nIn contrast to visit counts, the dwell time measurement\nconsists of three distinct states that a site visit can be in:\nactive, idle, or not in session. In our data collection, a visit\nto a website, either through a user opening a new window\nor a new tab, triggers an event for that URL to be logged.\nAfter \ufb01ve seconds of inactivity (measured as a lack of key-\nboard or mouse interaction with the browser), the site visit\n(a) Number of events\n(b) Average dwell time\nFigure 1: (a) Daily number of events logged during the three-\nweek study period. (b) Average dwell time per page visit (in\nseconds). The shaded bands indicate weekends.\nis considered to have entered into an idle state. If the user\nresumes interaction with the content within 30 minutes, we\nresume our logging for the same URL. If the user moves the\nfocus away from the URL (e.g., by opening a new window\nor transferring focus to a new tab) or after 30 minutes of idle\ntime have elapsed, the session is ended.\nActivity statistics of study participants. Our dataset con-\nsists of browsing logs from a panel of 24,036 unique partic-\nipants, based in the US, for a period of three weeks starting\non 5 April 2018. The dataset consists of 241 million distinct\npage visits with a median of 6,074 page visits per individual.\nFigure 1a shows the activity of all users. We observe that ac-\ntivity (number of events logged) decreases over the course of\nthe study period, and that there is a noticeable drop during\nweekends. On the contrary, Figure 1b shows that the aver-\nage dwell time per visit is larger on weekends than during\nthe week, and that, even as the number of events logged de-\ncreases over the course of the study period (Figure 1a), this\nis not the case for the average dwell time. This suggests that,\ndespite participant churn, the general pattern of interactions\nremains consistent. For robustness, we repeated all analy-\nses on the subset of users who remained active throughout\nthe observation period, \ufb01nding no qualitative disagreement\nfrom the results obtained on the set of all users.\nBias and limitations of dataset. Since the data was con-\nstructed from the browsing activity of a panel of opt-in Fire-\n154\nfox users, we acknowledge that the data is likely to be bi-\nased. As we do not have demographic covariates for mem-\nbers of the study panel, we are not able to compare the rep-\nresentativity of this sample against more conventional mea-\nsures of the general population, as Guess (2018) and Pe-\nterson, Goel, and Iyengar (2018) were able to do. Since it\nis possible that the preference to choose Firefox as a pri-\nmary browser could be associated with one\u2019s latent news\npreferences (in that political predispositions could predict\nboth the likelihood to use Firefox and exposure to news con-\ntent), we considered the possibility that the participants in\nthis study might deviate from the general population in their\noverall news preferences. We compared the top 100 most\nvisited news sites in our Firefox logs with the top 100 most\npopular (with respect to unique monthly US visitors) news\nsites according Alexa.com, a prominent website ranking ser-\nvice. Spearman\u2019s rank correlation between the two lists was\n0.85, suggesting that our study participants were generally\nexposed to similar popular sources of online content as the\ngeneral population.\nPartisan leaning computation. All our analysis is at the do-\nmain level. We obtained partisan leaning scores for all do-\nmains from Robertson et al. (2018), where \u201cleaning\u201d is de-\n\ufb01ned as the estimate of a news outlet\u2019s political ideological\nalignment with either a conservative or a liberal audience.\nThe leaning score of a domain assumes a value between 0\nand 1; it is de\ufb01ned as the average self-reported ideology of\nusers who have shared pages from that domain on Twitter,\nwhere 0 corresponds to maximally liberal, and 1 to maxi-\nmally conservative. The original list provided by Robertson\net al. (2018) has leaning scores for over 19,000 domains,\nincluding domains such as facebook.com, google.com, and\ninstagram.com. However, since we are speci\ufb01cally inter-\nested in understanding news consumption behavior, we only\nused a subset of news domains, obtained from the popu-\nlar journalism watch-dog website mediabiasfactcheck.com.\nThe subset contained 2,873 news domains that were still\nonline in September 2020, and further discarding domains\nwithout a leaning score in Robertson et al.\u2019s list resulted in a\nset of 1,295 news domains.\nOne should bear in mind that de\ufb01ning the leaning of a do-\nmain based on the fraction of self-identi\ufb01ed users with a spe-\nci\ufb01c political leaning could be biased due to self-selection.\nAlthough we acknowledge this bias, we argue that any as-\nsignment of a leaning score to a domain (say, by a journalist\nor by a professional organization) will be subject to similar\npersonal biases. The leaning scores we use have been shown\nto be highly correlated with the scores from Bakshy, Mess-\ning, and Adamic (2015), who used a similar methodology on\nFacebook, thus providing some evidence that the scores are\nrobust. We use the list of Robertson et al. (2018) because it\nis larger and more up-to-date. The 1,295 studied domains in-\nclude a mix of news sources (e.g., nytimes.com, washington-\npost.com), cable TV sites (e.g., msnbc.com, foxnews.com),\netc. The distribution of the leaning scores of domains is\nshown in Figure 2a.\nIn this study, we only consider participants in our anal-\nysis who in total made at least 50 visits to pages from the\n0.0 0.2 0.4 0.6 0.8 1.0\nDomain leaning: 0 = left, 1 = right020406080100120140Num domains(a) Histogram of domain leaning scores\n0.0 0.2 0.4 0.6 0.8 1.0\nUser leaning: 0 = left, 1 = right0100200300400500Num users\n(b) Histogram of user leaning scores\nFigure 2: Histogram of leaning scores for (a) domains and\n(b) users.\n1,295 considered news domains. This threshold restricted\nthe dataset to 6,575 participants. We compute the political\nleaning of individual participants as the weighted average\nleaning score of the domains they visited, where each do-\nmain was weighted by the number of times it had been vis-\nited by the respective user. The distribution of participants\u2019\nleaning is shown in Figure 2b. We can see that, on aver-\nage, the studied participants are leaning liberal, as estimated\nthrough their online news browsing history. Note that, in the\nrest of the paper, we shall use the terms leaning/polarity/\nideology, left-leaning/liberal, and right-leaning/conservative\ninterchangeably.\nBrowsing graph. Important parts of our analysis are based\non the bipartite (user-by-domain) browsing graph G =\n(U;D;E), where U=fu1;: ::; ungare the nusers, D=\nfd1;: ::; dmgare the mnews domains, and Eis the set of\nedgesffu i;djg:ui2U;dj2Dgbetween users and domains.\nEach edge fui;djgis associated with a weight wi jthat cap-\ntures the average dwell time spent by user uion pages from\ndomain dj(e.g., if a user on average spends 31 seconds\non pages of nytimes.com, the corresponding weight is 31).\nThe bipartite graph Gmay also be thought of as a matrix\nwhose rows are users, whose columns are news domains,\nand whose entries indicate the average dwell time of a user\non a domain.\nCo-browsing graph. Using the bipartite browsing graph\nG, we performed a one-mode projection onto the news do-\nmains Dby counting all paths of length 2 between two do-\n155\nNodes EdgesAvg.\ndegreeTop PageRank\nCo-browsing 1,295 176,945 273nytimes.com,\nwashingtonpost.com,\ncnn.com,\ntheguardian.com,\nnpr.com,\nHyperlink 1,295 323,036 498washingtonpost.com,\nhuf\ufb01ngtonpost.com,\nnytimes.com,\nbusinessinsider.com,\ntheatlantic.com\nTable 1: Statistics of co-browsing and hyperlink graphs.\nmains. We call the graph resulting from the projection the\nco-browsing graph. Edges between two domains d1andd2\nin the co-browsing graph are weighted in order to represent\nthe number of users who visited both d1andd2. To count\nco-visits of domains, we aggregated URLs at the domain\nlevel, considered only those domains for a given user with\nwhich they had actively engaged for at least 60 seconds in\ntotal (measured via the sum of the user\u2019s dwell times on the\ndomain), and counted multiple visits by the same user to\nthe same domain only once. Summary statistics of the co-\nbrowsing graph are listed in Table 1.\nHyperlink graph. In order to investigate users\u2019 browsing\npatterns in the light of how online news websites link to one\nanother, we constructed the hyperlink graph of the network\nof news domains. The Common Crawl project1regularly\ncrawls the Web and releases datasets of the discovered web-\npages on a regular schedule, which are generally considered\nthe most extensive publicly available sources regarding the\nstructure of the Web (Meusel et al. 2014). Common Crawl\nalso periodically releases versions of the hyperlink graph of\nthe Web aggregated at the domain level, specifying for each\ndomain to which other domains it contains hyperlinks.2Our\nanalysis works at the domain level and is restricted to the\nsubgraph de\ufb01ned by the set of 1,295 news domains Dalso\nused in constructing the browsing and co-browsing graphs.\nIn order to enable a meaningful comparison with the co-\nbrowsing graph, we ignore the directionality of edges and\nconsider the hyperlink graph as an undirected graph. The\nhyperlink graph represents static connections between do-\nmains, and results should be interpreted in this framework.\nSummary statistics of the hyperlink graph are listed in Ta-\nble 1. In particular, we observe that the hyperlink graph is\ndenser than the co-browsing graph.\nThe browsing, co-browsing, and hyperlink graphs cap-\nture different aspects of how users browsing the Web move\nfrom domain to domain. The bipartite browsing graph re-\n\ufb02ects choices made by users; it aggregates each user\u2019s entire\nbrowsing history. The co-browsing graph further aggregates\nacross all users, thus giving a macro picture of the browsing\npatterns of online news. In contrast, the hyperlink graph re-\n1http://commoncrawl.org/\n2e.g., http://commoncrawl.org/2019/02/host-and-domain-\nlevel-web-graphs-nov-dec-2018-jan-2019/\ufb02ects the choices made by content creators to link between\npages. In this sense, the hyperlink graph also represents the\nset of possible direct links that users could have taken while\nbrowsing.\n4 Polarization Patterns in News Browsing\nIn this section, we build on top of the above-described\ndatasets to characterize polarization in news browsing. We\nstart by investigating dwell times spent on various domains\nas a way of showing the existence of polarization. Next, we\nanalyze communities of domains in the co-browsing graph\nin order to show that certain groups of news domains tend to\nbe co-visited by the same user.\n4.1 Polarization in Dwell Times\nFirst, we assess the extent of polarized browsing habits\namong participants by examining whether there is any clear\nstructure in the bipartite browsing graph through the use of\nspectral graph co-clustering techniques (Dhillon 2001; Role,\nMorbieu, and Nadif 2019). Unlike graph clustering tech-\nniques, which can only cluster one dimension (users or do-\nmains) independently, co-clustering techniques \ufb01nd the de-\npendencies between participants and domains. Recall that\nedge weights of the bipartite browsing graph capture each\nuser\u2019s average dwell time for each domain (the average\namount of time they spent actively engaging with pages\nfrom the respective domain), such that, by co-clustering the\nbipartite browsing graph, we obtain clusters of similar do-\nmains visited by similar users. To avoid individual activity\nbiases, a given user\u2019s domain-level dwell times were z-score-\nstandardized across all domains they had visited. Given this\nsetup, polarized browsing habits would appear as tight clus-\nters of domains with average domain leaning scores that sig-\nni\ufb01cantly differ across clusters.\nRunning the co-clustering procedure for a number of co-\nclusters ranging from two to \ufb01ve, we found that when us-\ning three co-clusters, a liberal-leaning (left), a centrist, and\na conservative-leaning (right) cluster of domains emerged.\nSince, moreover, using more than three co-clusters merely\nsplit one of the co-clusters into two, we decided to set the\nnumber of co-clusters to three. For each cluster of domains\nand for each cluster of users, we computed the distribution\nof their leaning scores. Figure 3a shows a box plot of the dis-\ntribution of domain cluster leanings. Similar trends emerged\nin the leaning of user clusters (not shown).\nRecall that the co-clustered browsing graph was based on\ndwell time information only; neither domain nor user lean-\nings were used in the co-clustering process. In this light, Fig-\nure 3a, which shows that the cluster-speci\ufb01c distributions of\nleaning scores differ widely across domain clusters, implies\nthat domains that are visited in similar ways by similar users\ntend to be similar with respect to their leaning\u2014a clear indi-\ncation of partisan polarization in browsing.\nIn order to analyze whether the clustering structure can\nbe identi\ufb01ed by website visits alone (without considering\ndwell times), we also performed a co-clustering on an un-\nweighted version of the bipartite browsing graph. As ob-\nserved in Figure 3b, the separation in the communities is\n156\nCluster1 (left)\n Cluster2 (center)\n Cluster3 (right)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Leaning\n(a) Weighted browsing graph\nCluster1\n Cluster2\n Cluster3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Leaning\n(b) Unweighted browsing graph\nFigure 3: Boxplots of leaning scores per domain cluster, ob-\ntained by co-clustering (a) the weighted bipartite (user-by-\ndomain) browsing graph, (b) an unweighted version of the\nbrowsing graph. Green dots are means, red lines are medi-\nans, box boundaries are quartiles.\nnot as pronounced in the unweighted as in the weighted ver-\nsion, suggesting that dwell time\u2013based measurements reveal\nstronger polarization patterns. Given our novel methodology\nto compute the dwell time of users, this is the \ufb01rst large-\nscale study to establish the existence of polarization in terms\nof time spent by users on partisan websites as opposed to\nbrowsing histories alone.\n4.2 Time Spent \u201cOn the Other Side\u201d\nNext, we investigate whether users of different leanings en-\ngage differently with domains of different leanings. To do\nso, we \ufb01rst split the set of news domains into left-leaning\n(leaning below 0.4), center (leaning between 0.4 and 0.6),\nand right-leaning (leaning above 0.6) domains. Then, for\neach user, we computed their average dwell time for each\ndomain and z-score-standardized their average dwell times\nacross domains (such that, for a \ufb01xed user, the mean across\ndomains is 0, with a standard deviation of 1), in order to re-\nmove effects due to the fact that some users generally spend\nmore, and some less, time engaging with visited pages. Next,\nwe binned users into \ufb01ve buckets based on their leanings\nand computed the mean standardized dwell time in each\nuser bucket, separately for the three leaning-based domain\ngroups.\nFigure 4 shows the dwell time for left-leaning, center, and\nright-leaning sites for users with varying leanings. We ob-\n<=0.2\n>0.2, <=0.4\n >0.4, <=0.6\n >0.6, <=0.8\n>0.8\nUser leaning bucket\n0.6\n0.4\n0.2\n0.0\n0.2Dwell time (z-normalized)\nLeft\nCenter\nRightFigure 4: Average dwell time (z-score-standardized within\nusers) spent by users of different leanings (x -axis) on do-\nmains of different leanings (three curves). Error bars show\n95% con\ufb01dence intervals. We see that users spend signi\ufb01-\ncantly more time when visiting pages on domains aligned\nwith their own leaning.\nserve that the more extreme participants on either side of\nthe leaning spectrum dwell signi\ufb01cantly longer when visit-\ning like-minded online news content. For domains without\na clear leaning (\u201ccenter\u201d), there is no such upward or down-\nward trend. Although Garrett (2009) observed in a lab ex-\nperiment that people spend more\u2014rather than less\u2014time on\ncertain types of content from the other side, we emphasize\nthat our population is several orders of magnitude larger than\ntheirs and that our data was collected \u201cin the wild\u201d, outside\nof a lab setting.\n4.3 Community Structure in Co-browsing Graph\nAbove, we analyzed individual visits to news domains by\nparticipants and found patterns of polarization in the dwell\ntime spent upon visits of domains of various ideological\nalignment. Now, we consider browsing patterns across par-\nticipants by analyzing the community structure of the co-\nbrowsing graph. The co-browsing graph represents users\u2019\nchoices in what domains are co-visited, so if people only\nselect a speci\ufb01c set of domains that appeal to their partisan\nideology, we should observe communities of ideologically\nsimilar domains in the co-browsing graph.\nTo determine whether this is the case, we applied a com-\nmunity detection algorithm based on the Louvain method\n(De Meo et al. 2011) to the co-browsing graph, producing\na partition of the graph nodes into communities that maxi-\nmized modularity. Modularity measures the quality of a par-\ntition and is high if nodes are densely connected inside com-\nmunities, and sparsely connected across communities. The\nmethod, which determines the number of communities auto-\nmatically, identi\ufb01ed three communities. Figure 5a plots the\ndistribution of domain leanings for each of the three commu-\nnities. Although domain leanings were not used in the com-\nmunity detection process, a clear strati\ufb01cation of the three\ncommunities along partisan lines emerges, with clusters 1,\n2, and 3 containing mostly left-leaning, centrist, and right-\nleaning domains, respectively. Table 2 shows the \ufb01ve do-\n157\nCommunity 1 Community 2 Community 3\nnytimes.com theguardian.com foxnews.com\ncnn.com theatlantic.com wsj.com\nnbcnews.com thinkprogress.org breitbart.com\nlatimes.com motherjones.com drudgereport.com\ncbsnews.com dailykos.com dailywire.com\nTable 2: Top \ufb01ve domains with respect to PageRank central-\nity in each community of the co-browsing graph.\nmains with the highest PageRank centrality in each of the\ncommunities.\nWe also used a k-core decomposition (Batagelj and Zaver-\nsnik 2003) of the co-browsing graph to con\ufb01rm this structure\nand found the same three strongly connected cores (commu-\nnities), also separated along the ideological spectrum.\nIn addition to the co-browsing graph, we also ran com-\nmunity detection on the hyperlink graph. Here, too, three\ncommunities emerge; the results are shown in Figure 5b.\nAlthough the three communities of the hyperlink graph are\nqualitatively similar to those of the co-browsing graph (left,\ncenter, right), the strati\ufb01cation is not as clear in the hyperlink\ngraph as in the co-browsing graph (Figure 5a): the modular-\nity of the co-browsing graph is twice that of the hyperlink\ngraph (0.11 vs. 0.05).\n5 Selective Exposure vs. Structure of the Web\nFinally, we explore whether polarization in browsing is due\nto selective information seeking on behalf of participants or\nwhether it is the result of the link structure of the Web envi-\nronment with which they interact.\nHomophily in hyperlink and co-browsing graphs. To be-\ngin, we compute, for both the co-browsing graph and the\nhyperlink graph, the \u201cneighborhood leaning\u201d of each node\n(i.e., domain) d, de\ufb01ned as the average leaning of all nodes\nlinked to dby an edge. (Recall that both the co-browsing\ngraph as well as the hyperlink graph are treated as undirected\ngraphs.) Figure 6a, computed on the co-browsing graph,\nplots one point per domain d, showing d\u2019s own leaning on\nthex-axis, and d\u2019s neighborhood leaning on the y-axis. We\nobserve that, as we move from left to right, the neighbor-\nhood leaning increases, a clear sign of polarization in co-\nbrowsing patterns. Figure 6b, which was computed on the\nhyperlink graph rather than on the co-browsing graph, ex-\nposes a similar pattern, but with a signi\ufb01cantly lower Pear-\nson correlation coef\ufb01cient (0.696 vs. 0.777, p<0:01). The\nhigher correlation coef\ufb01cient for the co-browsing graph in-\ndicates that neighborhoods are more homophilic in the co-\nbrowsing graph, compared to the hyperlink graph.\nAs a caveat, we point out that, as seen in Table 1, the aver-\nage degree of nodes in the hyperlink graph is higher than that\nin the co-browsing graph, which might bias the neighbor-\nhood leaning scores: a higher average degree could a priori\nlead to a more diverse set of neighbors, which could in turn\nlead the hyperlink graph to have less leaning homophily.\nPresence of neighbors in hyperlink versus co-browsing\ngraphs. Next, we compare neighborhoods of nodes in the\nCommunity1 (left)\n Community2 (center)\n Community3 (right)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Leaning\n(a) Co-browsing graph\nCommunity1 (left)\n Community2 (center)\n Community3 (right)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Leaning\n(b) Hyperlink graph\nFigure 5: Boxplots of leaning scores per community of do-\nmains, obtained by running community detection on (a) the\nco-browsing graph, (b) the hyperlink graph. Green dots are\nmeans, red lines are medians, box boundaries are quartiles.\nco-browsing and hyperlink graphs. Intuitively, an edge be-\ning present in the hyperlink graph but absent from the\nco-browsing graph corresponds to participants deliberately\navoiding the link when browsing. To investigate such situa-\ntions more closely, we computed, for each node in the hyper-\nlink graph, the absolute difference in leaning from each of its\nneighbors. We then ranked the neighbors by this difference\nand bucketed them into deciles (only considering nodes with\nmore than 10 neighbors, which eliminated \ufb01ve nodes from\nthe analysis). Next, for each decile, we computed the frac-\ntion of edges present in the co-browsing graph. Our hypoth-\nesis was that, in comparison to the hyperlink graph, edges\nin the co-browsing graph have more homophily, such that a\nlarger fraction of edges would be present in the lower deciles\n(corresponding to neighbors in the hyperlink graph that are\nmore similar in leaning to the focal node). Figure 7a shows\nthat, indeed, as we move from left to right (i.e., as the lean-\ning difference between neighbors in the hyperlink graph in-\ncreases), the fraction of edges present in the co-browsing\ngraph decreases.\nThe above analysis does not take into account the edge\nweights present in the co-browsing graph; i.e., it does not\ndistinguish between the case where one single participant\nco-visited two domains from the case where a large num-\nber of participants did so. To obtain the weighted version of\n158\n0.0\n 0.2\n 0.4\n 0.6\n 0.8\n 1.0\nleaning (0=left)\n0.3\n0.4\n0.5\n0.6\n0.7mean outlink leaning\nCorrelation:0.777(a) Co-browsing graph\n0.0\n 0.2\n 0.4\n 0.6\n 0.8\n 1.0\nleaning (0=left)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8mean outlink leaning\nCorrelation:0.696\n(b) Hyperlink graph\nFigure 6: Leaning of each domain (x -axis) vs. average lean-\ning of the domain\u2019s neighbors (y-axis) in (a) the co-browsing\ngraph and (b) the hyperlink graph.\nFigure 7a, we computed, for each decile, the average weight\nof the corresponding edges in the co-browsing graph, with\nedges absent from the co-browsing graph contributing val-\nues of zero. (Note that the previous analysis is a special case\nof the present analysis, with all non-zero edge weights set\nto the value of 1.) This way, we obtain, for each decile, the\naverage number of users co-visiting the domains linked by\nthe corresponding set of edges in the hyperlink graph. The\nresults, shown in Figure 7b, are consistent with those of the\nunweighted analysis (Figure 7a), con\ufb01rming that dissimilar\n(with respect to leaning) neighbors of a node are less sought\nout by users when browsing.\nAs mentioned above, a higher average degree could lead\nto a more diverse set of neighbors and hence, this result\ncould be drastically different for nodes having vastly differ-\nent degrees. To investigate this possibility, we also repeated\nthe above analysis after stratifying nodes into 10 equally\nsized group with respect to their degree in the hyperlink\ngraph. Inspecting version of Figure 7 plotted separately for\neach of the 10 degree strata (not shown), we observed a sim-\nilar pattern within each stratum, indicating the robustness of\nthe result.\nMulti-hop neighborhoods: random walks. The above\nanalysis only considered the immediate neighborhood of a\nnode when assessing whether personal preferences are more\npolarized than the static link structure. To understand how\n<0.034\n0.034-0.071\n 0.071-0.112\n 0.112-0.158\n0.158-0.21\n 0.21-0.268\n0.268-0.338\n 0.338-0.425\n 0.425-0.546\n 0.546-0.935\nLeaning difference bucket\n0.25\n0.30\n0.35\n0.40\n0.45Frac. edges in co-browse\n(a) Hyperlink graph vs. unweighted co-browsing graph\n<0.034\n0.034-0.071\n 0.071-0.112\n 0.112-0.158\n0.158-0.21\n 0.21-0.268\n0.268-0.338\n 0.338-0.425\n 0.425-0.546\n 0.546-0.935\nLeaning difference bucket\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5Average transitions in decile x\n(b) Hyperlink graph vs. weighted co-browsing graph\nFigure 7: (a) Fraction of edges from the hyperlink graph that\nare also present in the co-browsing graph, when consider-\ning, for each node, only those edges that connect the node to\na neighbor whose leaning difference falls into the respective\ndecile (x -axis; deciles were computed per node over all its\nneighbors). (b) Analogous analysis, but with average edge\nweights, rather than fraction of edges, on the y-axis. The\nplots show that hyperlinks leading to more similar (with re-\nspect to leaning) neighbors are more likely to be chosen by\nusers.\npolarized the broader neighborhood of a node is, we conduct\na random walk\u2013based analysis, as follows. Starting from\neach node u, we perform a random walk and, at each step\nj, measure the absolute difference du jin leaning between u\nand the node at step j. Then, for each step j, we compute\ntheneighborhood distance djat step jas the mean value of\ndu jover all nodes uin the graph. To ensure the robustness\nof the results, we repeat the random walk 100 times for each\nnode uand use the mean du jfor each uand j. If a graph is\nmore polarized, a node\u2019s neighborhood will be more similar\nto the node itself, and the neighborhood distance djwill be\nsmaller.\nWe ran the above procedure (with random walks of length\n10) on the following three graphs: (i) the co-browsing graph\nwithout edge weights, (ii) the co-browsing graph with edge\nweights, and (iii) the hyperlink graph. Random walks in the\ndifferent graphs capture different types of structure and par-\n159\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nNumber of steps in the random walk\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60Absolute leaning difference\nCo-browse\nCo-browse (weighted)\nHyper linkFigure 8: Leaning difference between nodes and their multi-\nhop neighborhoods in the weighted and unweighted co-\nbrowsing graphs and in the hyperlink graph, computed\nby performing random walks of length 10. Error bands\nshow 95% con\ufb01dence intervals. The weighted co-browsing\ngraph is most homophilic, followed by the unweighted co-\nbrowsing graph and the hyperlink graph.\nticipant choices: The weighted co-browsing graph captures\nthe participants\u2019 choices, biasing the random walk towards\nedges that were empirically taken by more participants. The\nunweighted co-browsing graph still takes participant choices\ninto account to a certain extent, but makes transitions to all\nneighboring domains equally likely. Finally, a random walk\non the hyperlink graph simulates random browsing of the\nconsidered domains.\nThe results are shown in Figure 8, with one curve for each\nof the three graphs. Each point indicates the average neigh-\nborhood distance for a given step jalong the random walks.\nThe signi\ufb01cantly lower values of neighborhood distance for\nthe weighted co-browsing graph (red) indicate that partici-\npants\u2019 browsing choices are more biased towards domains\nwith a similar leaning than what would be expected from\nsomeone randomly browsing the hyperlink graph induced\nby the included news domains (green). There are also sig-\nni\ufb01cant differences in the neighborhood distance values be-\ntween the unweighted co-browsing graph (blue) and the hy-\nperlink graph (green), indicating inherently different (more\nhomophilic) structure in the co-browsing graph, compared\nto the hyperlink graph, even after multiple hops of a random\nwalking. Figure 8 thus strengthens the results of Figure 7\nbeyond the immediate neighborhood of nodes.\nTo summarize, the co-browsing graph appears to be sig-\nni\ufb01cantly more polarized than the hyperlink graph, and we\nconclude that the polarization observed in the news con-\nsumption of the participants of our study cannot be ex-\nplained by the hyperlink structure of the Web alone. Rather,\nparticipants\u2019 explicit choices play an important role as well.\n6 Conclusions\nIn this paper, we used browsing logs collected on the client\nside over the course of three weeks from a large opt-in panel\nof Firefox users in order to measure political polarization in\nonline news consumption. In various analyses, we providedevidence of pronounced polarization patterns in online news\nconsumption.\nBy analyzing dwell time (the amount of time during\nwhich a user actively interacts with a webpage), we observed\nthat users engage more deeply with news content aligned\nwith their own leaning (estimated, by proxy, via the average\nleaning of the news they consume overall).\nApplying a co-clustering algorithm to the bipartite brows-\ning graph, which encodes average dwell times for all users\non all visited domains, revealed three coherent, frequently\nco-visited groups of news domains with vastly different\nleaning distributions, corresponding to left-leaning, center,\nand right-leaning news domains, respectively\u2014yet another\nindicator of polarization in news consumption.\nMoving from the individual level to the population level,\nwe contracted the bipartite user-by-domain browsing graph\ninto a domain-by-domain co-browsing graph. Applying a\ncommunity detection algorithm to the co-browsing graph\ngave rise to the same polarized pattern, with each cluster\ncorresponding to one distinct political leaning.\nA similarly polarized pattern\u2014though less pronounced\u2014\nwas discovered by applying community detection to the hy-\nperlink graph spanned by the news domains included in the\nstudy, rather than to the co-browsing graph. Thus, in or-\nder to investigate the possibility that polarization in news\nconsumption might be solely due to the inherent hyperlink\nstructure of the Web, we compared the co-browsing and hy-\nperlink graphs in various ways, concluding that the polar-\nized hyperlink structure alone does not suf\ufb01ce to explain\nusers\u2019 polarized news browsing behaviors. Rather, users\u2019 ex-\nplicit choices also contribute to the observed polarization.\nOur \ufb01ndings are thus overall in line with previous studies\non the role of user choices (Bakshy, Messing, and Adamic\n2015), while contributing results based on a novel dataset of\nunprecedented detail and from novel angles of analysis.\nLimitations. Our work should be seen in the light of its lim-\nitations:\n1. Opt-in sample: While we strongly believe the opt-in na-\nture of our recruitment is necessary for an ethical research\napproach, it may well have provided a biased sample.\n2. Domain-level analysis: We aggregated all pages in a do-\nmain when constructing the browsing, co-browsing, and\nhyperlink graphs, essentially treating all pages in a do-\nmain as interchangeable. This was done in order to avoid\nsparsity issues that would arise when aggregating at the\nURL level instead.\n3. Personalization: Our analysis did not account for the role\nof personalization algorithms in browsing. While this is\nlimiting to an extent, we point out that algorithmic person-\nalization is still rare in online news (Thurman and Schif-\nferes 2012).\n4. Correlation vs. causation: Although we showed that the\npolarization baked into the hyperlink structure of the Web\nalone does not suf\ufb01ce to explain users\u2019 polarized news\nconsumption patterns, we do not claim to have revealed\nthe causes of polarization in news consumption, which\nconstitutes an important direction for future work.\n160\nFuture work: the role of the center. Our analysis raises the\npro\ufb01le of the role of the center as a clearly identi\ufb01able area\nin polarization studies, which proposes a potentially valu-\nable role of centrist websites as a location for bipartisanship.\nThese sites may themselves bene\ufb01t from closer study and\nanalysis, particularly when analyzed at the webpage or topic\nlevel. This becomes increasingly important when, as today,\nwe see degrees of partisanship that are unprecedented and\narguably unhealthy for society. Understanding where social\nworlds intersect provides opportunities for communication\nand potentially reconciliation within a polarized and sepa-\nrated populace. We should ask: Can centrist websites serve\nas a \u201cdemilitarized zone\u201d of the Web, and can an analysis\nsuch as ours, but focused on centrist websites, provide im-\nplications for the improved design of virtual spaces?\nAcknowledgments\nKiran Garimella was supported by a Michael Hammer Fel-\nlowship at MIT. Part of this research was done while KG\nwas at EPFL. Robert West\u2019s lab was partly supported by\ngrants from the Swiss National Science Foundation (grant\n200021 185043) and the Swiss Data Science Center, and by\ngifts from Google, Facebook, and Microsoft.\nReferences\nBakshy, E.; Messing, S.; and Adamic, L. A. 2015. Expo-\nsure to ideologically diverse news and opinion on Facebook.\nScience 348(6239): 1130\u20131132.\nBarber \u00b4a, P.; Jost, J. T.; Nagler, J.; Tucker, J. A.; and Bon-\nneau, R. 2015. Tweeting from left to right: Is online political\ncommunication more than an echo chamber? Psychological\nScience 26(10): 1531\u20131542.\nBarthel, M.; Shearer, E.; Gottfried, J.; and Mitchell, A. 2015.\nThe evolving role of news on Twitter and Facebook. Pew\nResearch Center .\nBatagelj, V .; and Zaversnik, M. 2003. An O(m) algo-\nrithm for cores decomposition of networks. arXiv preprint\ncs/0310049 .\nDe Meo, P.; Ferrara, E.; Fiumara, G.; and Provetti, A. 2011.\nGeneralized Louvain method for community detection in\nlarge networks. In ICISDA.\nDhillon, I. S. 2001. Co-clustering documents and words us-\ning bipartite spectral graph partitioning. In KDD.\nFlaxman, S.; Goel, S.; and Rao, J. M. 2016. Filter bubbles,\necho chambers, and online news consumption. Public Opin-\nion Quarterly 80(S1): 298\u2013320.\nGarimella, K.; De Francisci Morales, G.; Gionis, A.; and\nMathioudakis, M. 2018. Political discourse on social media:\nEcho chambers, gatekeepers, and the price of bipartisanship.\nInWWW.\nGarrett, R. K. 2009. Echo chambers online? Politically moti-\nvated selective exposure among Internet news users. Journal\nof Computer-Mediated Communication 14(2): 265\u2013285.\nGeiger, A. 2019. Key \ufb01ndings about the online news land-\nscape in America. Pew Research Center .Gentzkow, M.; and Shapiro, J. M. 2011. Ideological segrega-\ntion online and of\ufb02ine. The Quarterly Journal of Economics\n126(4): 1799\u20131839.\nGuess, A. M. 2018. (Almost) Everything in Moderation:\nNew Evidence on Americans\u2019 Online Media Diets. Ameri-\ncan Journal of Political Science .\nIyengar, S.; and Hahn, K. S. 2009. Red media, blue media:\nEvidence of ideological selectivity in media use. Journal of\nCommunication 59(1): 19\u201339.\nIyengar, S.; Lelkes, Y .; Levendusky, M.; Malhotra, N.; and\nWestwood, S. J. 2019. The origins and consequences of af-\nfective polarization in the United States. Annual Review of\nPolitical Science 22: 129\u2013146.\nKlapper, J. T. 1960. The effects of mass communication. Free\nPress.\nKulshrestha, J.; Eslami, M.; Messias, J.; Zafar, M. B.;\nGhosh, S.; Gummadi, K. P.; and Karahalios, K. 2017. Quan-\ntifying search bias: Investigating sources of bias for political\nsearches in social media. In CSCW.\nLahoti, P.; Garimella, K.; and Gionis, A. 2018. Joint non-\nnegative matrix factorization for learning ideological lean-\ning on Twitter. In WSDM.\nMessing, S.; and Westwood, S. J. 2014. Selective expo-\nsure in the age of social media: Endorsements trump partisan\nsource af\ufb01liation when selecting news online. Communica-\ntion Research 41(8): 1042\u20131063.\nMeusel, R.; Vigna, S.; Lehmberg, O.; and Bizer, C. 2014.\nGraph structure in the Web\u2014revisited: A trick of the heavy\ntail. In WWW.\nNarayanan, V .; Barash, V .; Kelly, J.; Kollanyi, B.; Neudert,\nL.-M.; and Howard, P. N. 2018. Polarization, partisanship\nand junk news consumption over social media in the US.\narXiv preprint arXiv:1803.01845 .\nNelson, J. L.; and Webster, J. G. 2017. The myth of partisan\nselective exposure: A portrait of the online political news\naudience. Social Media + Society 3(3).\nNikolov, D.; Lalmas, M.; Flammini, A.; and Menczer, F.\n2019. Quantifying biases in online information exposure.\nJAIST 70(3): 218\u2013229.\nPariser, E. 2011. The \ufb01lter bubble: What the Internet is hid-\ning from you. Penguin UK.\nParse.ly. 2016. Network Referrer Dashboard. Data\nretrieved from https://www.parse.ly/resources/data-studies/\nreferrer-dashboard. Accessed: 2021-04-10.\nPeterson, E.; Goel, S.; and Iyengar, S. 2018. Echo chambers\nand partisan polarization: Evidence from the 2016 presiden-\ntial campaign. Political Science Research and Methods .\nRaghavan, M.; Anderson, A.; and Kleinberg, J. 2018. Map-\nping the invocation structure of online political interaction.\nInWWW.\nRibeiro, F. N.; Henrique, L.; Benevenuto, F.; Chakraborty,\nA.; Kulshrestha, J.; Babaei, M.; and Gummadi, K. P. 2018.\nMedia bias monitor: Quantifying biases of social media\nnews outlets at large-scale. In ICWSM.\n161\nRobertson, R. E.; Jiang, S.; Joseph, K.; Friedland, L.; Lazer,\nD.; and Wilson, C. 2018. Auditing partisan audience bias\nwithin Google search. Proceedings of the ACM on Human-\nComputer Interaction (CSCW) 2: 1\u201322.\nRogowski, J. C.; and Sutherland, J. L. 2016. How ideology\nfuels affective polarization. Political Behavior 38(2).\nRole, F.; Morbieu, S.; and Nadif, M. 2019. CoClust: A\nPython Package for Co-Clustering. Journal of Statistical\nSoftware 88(1): 1\u201329.\nSchmidt, A. L.; Zollo, F.; Del Vicario, M.; Bessi, A.; Scala,\nA.; Caldarelli, G.; Stanley, H. E.; and Quattrociocchi, W.\n2017. Anatomy of news consumption on Facebook. Pro-\nceedings of the National Academy of Sciences 114(12):\n3035\u20133039.\nSears, D. O.; and Freedman, J. L. 1967. Selective exposure\nto information: A critical review. Public Opinion Quarterly\n31(2): 194\u2013213.\nStroud, N. J. 2010. Polarization and partisan selective expo-\nsure. Journal of Communication 60(3): 556\u2013576.\nSunstein, C. R. 2009. Republic.com 2.0 . Princeton Univer-\nsity Press.\nThurman, N.; and Schifferes, S. 2012. The future of per-\nsonalization at news websites: Lessons from a longitudinal\nstudy. Journalism Studies 13(5-6): 775\u2013790.\nWebster, S. W.; and Abramowitz, A. I. 2017. The ideological\nfoundations of affective polarization in the US electorate.\nAmerican Politics Research 45(4): 621\u2013647.\n162", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Political polarization in online news consumption", "author": ["K Garimella", "T Smith", "R Weiss", "R West"], "pub_year": "2021", "venue": "Proceedings of the \u2026", "abstract": "Political polarization appears to be on the rise, as measured by voting behavior, general affect  towards opposing partisans and their parties, and contents posted and consumed online."}, "filled": false, "gsrank": 189, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/18049", "author_id": ["PH96F4oAAAAJ", "AXoR9wwAAAAJ", "72vAGXcAAAAJ", "ZiFn598AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:0KERWMzSxDkJ:scholar.google.com/&output=cite&scirp=188&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D180%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=0KERWMzSxDkJ&ei=JbWsaMu1E-HUieoP9LKZ6AI&json=", "num_citations": 81, "citedby_url": "/scholar?cites=4162683730689892816&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:0KERWMzSxDkJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/18049/17852"}}, {"title": "WIKIBIAS: Detecting multi-span subjective biases in language", "year": "2021", "pdf_data": "WikiBias : Detecting Multi-Span Subjective Biases in\nLanguage\nThesis\nPresented in Partial Ful\fllment of the Requirements for the Degree\nMaster of Science in the Graduate School of The Ohio State University\nBy\nYang Zhong, B.S.\nGraduate Program in Computer Science and Engineering\nThe Ohio State University\n2021\nMaster's Examination Committee:\nProf. Wei Xu, Advisor\nProf. Feng Qin, Co-advisor\nProf. Micha Elsner\n\u00a9Copyright by\nYang Zhong\n2021\nAbstract\nBiases continue to be prevalent in modern text and media, especially subjective\nbias { a special type of bias that introduces improper attitudes or presents a statement\nwith the presupposition of truth.\nTo tackle the problem of detecting and further mitigating subjective bias, we\nintroduce a manually annotated parallel corpus WikiBias with more than 4,000\nsentence pairs from Wikipedia edits. This corpus contains annotations towards both\nsentence-level bias types and token-level biased segments. We present systematic\nanalyses of our dataset and results achieved by a set of state-of-the-art baselines in\nterms of three tasks: bias classi\fcation, tagging biased segments, and neutralizing\nbiased text. We \fnd that current models still struggle with detecting multi-span\nbiases despite their reasonable performances, suggesting that our dataset can serve as\na useful research benchmark. We also demonstrate that models trained on our dataset\ncan generalize well to multiple domains such as news and political speeches.\nii\nAcknowledgments\nI would like to express my gratitude to all the people who made this thesis possible\nand because of whom my graduate experience has been one that I will cherish forever.\nFirst and foremost, I would like to thank my advisor Wei Xu, who o\u000bered me\ngreat guidance as well as insightful thoughts to let me explore new research directions\nin the past two years. She worked closely with me for all deadlines and signi\fcantly\nimproved all of my writing pieces and presentations with her valuable suggestions and\nedits. She also created a great environment in which I could spend most of my time\non research and e\u000bectively collaborate with other outstanding researchers. I feel very\nfortunate to work with Wei.\nI would also like to thank the other members of my committee. I want to express\nmy heartiest gratitude to Prof. Feng Qin to help me through out the exam procedures\nand Prof. Micha Elsner for his insightful comments and questions that helped me\nimprove this thesis.\nI am also grateful to my colleagues and friends in the OSU Xu-Ritter NLP group.\nI received valuable advice from Jeniya Tabassum Binte Jafar and Ashutosh Baheti. I\nenjoyed working together with my excellent collaborators Chao Jiang, Wuwei Lan and\nMounica Maddela. I appreciated the generous help from Fan Bai and Shi Zong.\niii\nLast but not least, I owe my sincerest appreciations to my family members,\nespecially my parents and my sister. They are always with me and support me\nthroughout this journey.\niv\nVita\n2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.S. Mathematics,\nThe University of Texas at Austin, USA\n2019-2021 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Graduate Research Associate,\nThe Ohio State University, USA\nFields of Study\nMajor Field: Computer Science and Engineering\nv\nTable of Contents\nPage\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii\nVita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v\nList of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x\n1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n2. Background and Related Work . . . . . . . . . . . . . . . . . . . . . . . 5\n2.1 Wikipedia Edits . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Detection of Subjective Bias . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Debiasing Generation . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3. Construction of the WikiBias Corpus . . . . . . . . . . . . . . . . . . . 8\n3.1 Extracting and Filtering Wikipedia Edits . . . . . . . . . . . . . . 8\n3.2 Fine-grained Human Annotation . . . . . . . . . . . . . . . . . . . 10\n4. Modeling Subjective Bias . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.1 Classi\fcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.1.1 Binary Classi\fcation . . . . . . . . . . . . . . . . . . . . . . 14\n4.1.2 Classi\fcation over Sentence Pairs . . . . . . . . . . . . . . . 16\n4.1.3 Fine-grained Bias Type Classi\fcation . . . . . . . . . . . . . 17\n4.2 Tagging of Biased Language Spans . . . . . . . . . . . . . . . . . . 20\n4.3 Text Generation for Neutralizing Bias . . . . . . . . . . . . . . . . 23\nvi\n5. Generalization to Out-Of-Domain Data . . . . . . . . . . . . . . . . . . . 26\n5.1 Potentially Biased Data . . . . . . . . . . . . . . . . . . . . . . . . 27\n5.1.1 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . 27\n5.1.2 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . 28\n5.2 Neutral Texts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5.2.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n6. Conclusion and future work . . . . . . . . . . . . . . . . . . . . . . . . . 31\n6.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n6.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nAppendices 33\nA. Annotation Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nA.1 Word/phrase Classi\fcation Interface . . . . . . . . . . . . . . . . . 34\nA.2 Generation Classi\fcation Interface . . . . . . . . . . . . . . . . . . 35\nB. Rule-based System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\nvii\nList of Tables\nTable Page\n1.1 Example sentence pairs in our manually annotated WikiBias corpus with\nthree \fne-grained subjective bias types: framing ,epistemological ,not\nanddemographic bias. We annotate at the span-level to identify the cor-\nresponding pre- and post-edits, which are indicated by the same superscript\ncharacters (e.g., in row 1, the highlighted phrase in order to is changed to\nwhich could help during revision). . . . . . . . . . . . . . . . . . . . . . 2\n3.1 Comparison of biased language detection datasets. C,TandGrefer to\nsentence classi\fcation, tagging biased spans, and generation for neutralizing\nbias, respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2 Statistics of our WikiBias corpus with automatically ( \u00a73.1) and manually\n(\u00a73.2) annotations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4.1 Data split and size for the experiments. The automatically constructed\nWikiBias-Auto corpus is used for training only (Train auto). The manu-\nally annotated WikiBias-Manual corpus is split into Train/Dev/Test\nset. SLen represent the average sentence length in terms of the number\nof tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.2 Binary classi\fcation result on test set with di\u000berent training data, reported on\naverage of three runs. \u00a9means the model is further \fne-tuned on Train manual .17\n4.3 Macro and class-level F1 ( Framing ,Epistemological , and Demographic bias)\non test set, averaged across three runs. . . . . . . . . . . . . . . . . . . 18\n4.4 Tagging results. * indicates that model is \fne-tuned on Train manual only\nwhile all others are trained on Train auto.\u00a9indicates further \fne-tuning\non Train manual .yindicates training on the relabelled Train autowith labels\npredicted by the best BERT-LING\u00a9model. Results are averaged over 3 runs. 22\nviii\n4.5 Bias neutralization generation results on the test set. All models are trained\non the noisy Train autodata andymeans we used the o\u000b-the-shelf model\nreleased by their authors. For automatic metrics, rows with asterisks are\nsigni\fcantly di\u000berent than the preceding row. For human evaluation, rows\nmarked with * are signi\fcantly di\u000berent from 0 (according to a t-test with p\n< 0.05).\"/#means higher/lower score is preferred for the corresponding\nmetric. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.1 Samples of frequent multi-word phrases extracted by our tagging model\nfrom each corpus with manual annotation on polarity of stance. The second\ncolumn refers to the partial matching F1 based on 50 manually annotated\nsamples from each corpus. Text colors in the \frst column refer to the opinions\nleaning towards U.S. political parties Liberal/Democratic orConserva-\ntive/Republican . Colored Boxes refer to the target of Republican or\nDemocratic party respectively and +=\u0000signs illustrate whether the phrase\nis supported or against the stance of the target (i.e., totally irresponsible\u0000il-\nlustrates that the speaker uses this phrase to criticize the work of Republican\nParty). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nix\nList of Figures\nFigure Page\n2.1 Wikipedia Revision Example following the NPOV policy. . . . . . . . 6\n3.1 Word Alignment and Bias Annotation example for a pre-edited (top) and post-\nedited (left) sentence pair. Grey block means non-identical word/phrase\nalignment. Three edits are extracted: has bene\fted from !saw,inward\n![NULL] (Deletion), sector!corporations while the \frst is labelled as\nframing bias. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n5.1 Examples of the MATH dataset. . . . . . . . . . . . . . . . . . . . . . 29\nA.1 Annotation interface for Bias Classi\fcation of word/phrase edits. . . . 34\nA.2 Annotation guidelines for the evaluation on text generation results\nalong with a example question. . . . . . . . . . . . . . . . . . . . . . 35\nx\nChapter 1: Introduction\nPeople often rely on reference work like encyclopedias and textbooks to gather\ninformation, as such sources are designed to present facts fairly and objectively. Yet,\nbias is still pervasive in these sources. For instance, the sentence \\ this album is\narranged by many talented arrangers. \" is considered biased as the word talented\ninappropriately re\rects the writer's positive opinion. As a result, methods that can\nautomatically detect and reduce bias are in great demand, which could save human\ne\u000borts and keep the quality of the reference work.\nIn this work, we study how to detect and further mitigate biases in language.\nSpeci\fcally, we focus on a particular type of bias, \\subjective bias\", in which the\nlanguage is skewed towards an obvious feeling, with the presupposed or entailed\nproposition or considering opinions as truth. Contents with the subjective bias can\nmake people be doubtful about the texts' reliability and possibly trigger social unrest\nwith o\u000bensive language. Prior research has used the lexical and grammatical cues\nlike lexicon-syntactic patterns [ 1,2] or various n-gram features [ 3,4,5] to classify\nsentences as either subjective or objective. For instance, in the encyclopedia domain,\nRecasens et al. [ 6] constructed an automatic parallel corpus from Wikipedia revisions\nthat violate the Neutral Point of View (NPOV) policy,1which advocates for \\ fairly\n1https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view\n1\nSource Sentence: pre-edit (biased) Target Sentence: post-edit (neutral)\nIt should be noted thatathe nuclear free zone\nact does not make building land-based nuclear power\nplants illegal, and there is considerablebsupport\nfor nuclear power in order tocmeet Kyoto emis-\nsions targets .The nuclear-free zone act does not make building\nland-based nuclear power plants illegal, and there\nissome business support for investigating nu-\nclear power, which could helpcmeet Kyoto emis-\nsions targets.\nAnti-Americanism is a claimedaphenomenon\nofsubvertbethnic discriminationcand\novert irrationaldhostilityetowardfthe\nUnited States.Anti-Americanism is a globalaphenomenon of\ndiscriminationcandcriticismeoffthe United\nStates.\nHowever the term post-fascist has been used to de-\nscribe their beliefa, owing to apparentbintellec-\ntual roots in neo-fascist third positionismd.However, the term `post-fascist' has been used to de-\nscribe the beliefs of recent National Anarch-\nistsa, owing to theirbintellectual roots\nwhich lie partlycinthird positionism an,\nideology often considered to be neo-fascistd.\nTable 1.1: Example sentence pairs in our manually annotated WikiBias corpus with three\n\fne-grained subjective bias types: framing ,epistemological ,not anddemographic\nbias. We annotate at the span-level to identify the corresponding pre- and post-edits, which\nare indicated by the same superscript characters (e.g., in row 1, the highlighted phrase in\norder to is changed to which could help during revision).\npresenting views with reliable sources and avoiding editor bias \" and introduced the\ntask of identifying the bias-induced word in a statement. They further uncovered\ntwo types of subjective bias through linguistic analysis, which includes framing bias\nsuch as praising or perspective-speci\fc words and epistemological bias related to\npresupposed/entailed propositions. Pryzant et al. [ 7] extended such revision corpus\nand further proposed to transform the biased text into a neutral point of view, adding\na third class of subjective bias, demographic bias, for texts with the presupposition of\ndemographic categories like genders and races.\nHowever, current corpora on subjective bias detection or mitigation tasks su\u000ber\nfrom a set of issues. First, noises from automatically collected datasets [ 6,7] are\nnot neglectable. A pilot study conducted by Pryzant et al. [ 7] on their Wikipedia\nNeutrality Corpus (WNC) demonstrated that over 5% of the revisions are not related\n2\nto bias mitigation and thus wrongly labeled on the sentence level. Meanwhile, existing\nmanually annotated corpora for subjectivity often su\u000ber from the small dataset size\nin Wiebe et al. [ 4] or limited annotation quality: annotator agreement from Hube\nand Fetahu [ 8] falls at 0.124 measured by Krippendor\u000b's Alpha. Moreover, multiple\nedits are often needed when editing a subjective biased framing into a neutral one.\nFor instance, over 30% of Wikipedia revisions for NPOV justi\fcation contain two or\nmore edits in the source side and a diverse set of modi\fcation strategies are involved.\nExisting work [ 6,7] only focused on single word detection, presupposing a single word\nas the source of bias, and failing to utilize rich signals and resources of subjectively\nbiased words or phrases as introduced in [9].\nTo address these problems, we introduce a high-quality manually annotated parallel\ncorpus WikiBias . It includes over 4,000 biased and neutralized sentence pairs, which\ncover both 1,525 single word and 2,068 multiple-word span annotations (building\nupon 53.5k non-identical word alignments with \fne-grained bias types on the source\nsides. Samples of our corpus are shown in Table 1.1. We design an innovative two-\nstage annotation pipeline to help annotators accurately identify biased text segments,\nwhich obtains substantial agreement among di\u000berent annotators. To the best of our\nknowledge, this is the \frst corpus on the multi-word multi-span subjective biased text\nunderstanding.\nBuilding on WikiBias , we conduct a set of comprehensive analyses to better\nmodel subjectivity bias in text via three sub-tasks: bias classi\fcation, tagging biased\nsegments and neutralizing biased text. We found that current state-of-the-art models\nstill struggle with detecting multi-span biases despite their reasonable performances,\nsuggesting that our dataset can serve as a useful benchmark. We also demonstrate that\n3\nmodels trained on our dataset can generalize well to multiple domains such as news\nand political speeches, and uncover a list of challenges facing by the bias detection\nmodels.\n4\nChapter 2: Background and Related Work\n2.1 Wikipedia Edits\nWikipedia is a free, open content online encyclopedia created through the collabo-\nrative e\u000bort of a community of users known as Wikipedians. Anyone registered on the\nsite can create an article for publication and registration is not required to edit articles.\nNowadays it has become one of the platforms people use to gather information, as\nsuch sources are designed to present facts fairly and objectively.\nHowever, with its non-strict requirement for making edits, over 1.9 edits per\nsecond are being updated on Wikipedia and there is no such a guarantee that all\nmodi\fcations are preserving the fact and keep the objectivity of contents. To tackle\nthis problem, the Wikipedia community has proposed a long list of policies to \flter\nout or reject the negative/harmful revision submitted to a page. Figure 2.1 presented\nthe Wikipedia edit example between the old and new version of the same page, where\nthe editor provide the reason to make such revision as NPOV (Neutral Point of View),\nwhich means \\representing fairly, proportionately, and, as far as possible, without\neditorial bias, all the signi\fcant views that have been published by reliable sources\non a topic\"2.Such policy takes the below points into consideration: (1) Avoid stating\n2https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view\n5\nopinions as facts. (2) Avoid stating seriously contested assertions as facts. (3) Avoid\nstating facts as opinions. (4) Prefer nonjudgmental language. and (5) Indicate the\nrelative prominence of opposing views.\nFigure 2.1: Wikipedia Revision Example following the NPOV policy.\n2.2 Detection of Subjective Bias\nThe study of detection of subjectivity can be dated back to 1990s, when pioneers\nstart noticing the subjectivity genre on document level classi\fcation [ 10,11]. Later,\nworks like [ 12,13] bring people's attention to the subjectivity on sentence level. There\nis a long line of research focusing on sentence classi\fcation utilizing methods based\non linguistic features or handcrafted rules [ 1,2,14,15,3,16], then neural models\n[17,18,19,8]. Work of Recasens et al. [6]and Pryzant et al. [7]on detecting biased\n6\nlanguage over single-word edit is closely related to our work, but we study the biased\nlanguage on a broader scale to cover multi-word spans.\n2.3 Debiasing Generation\nGenerating debiased text can be viewed as a stylistic transferring task. Supervised\napproaches with parallel corpus have been shown to be e\u000bective across multiple styles\n[20,21,22,23,24]. More recently, pipeline-based or stepwise approaches [ 25,26,27]\nfocuses on \frst localizing the style to a \fxed portion of the word, then generating\nreplacement based on target style. Pryzant et al. [7]adopts a similar approach by\nincorporating the localized style attribute into a joint-embedding and enforces the\ntext generation model to pay attention to the modi\fcations.\n7\nChapter 3: Construction of the WikiBias Corpus\nWe create the new WikiBias corpus by \frst extracting Wikipedia revisions where\neditors provide Neutral Point of View (NPOV)3justi\fcations [ 6,7,16,28] to construct\nautomatically labeled data ( WikiBias-Auto ); then manually annotating sentences\nwith \fne-grained bias types at the span-level to create clean ground truth ( WikiBias-\nManual ). This is in contrast to the prior work on subjectivity that annotated only\non the sentence-level [ 4,8,18]. In particular, we design a two-stage human annotation\nmethodology to handle sentences with both single- and multi-edits. We describe the\ndetails below.\n3.1 Extracting and Filtering Wikipedia Edits\nAbout 0.1% of revisions in Wikipedia are tagged with \\NPOV\" (or \\POV-check\",\n\\POV-section\", etc.) by editors to indicate that they have identi\fed and rewritten\nbiased content to achieve a more neutral tone. In total, we extracted 557,860 NPOV-\nrelated revisions from the Wikipedia revision history dump (dated 01/01/2021), out\nof the 691 million of revisions that Wikipedia editors made between 2004 and 2021.\nWe also removed duplicated revisions and keep the latest revisions for each pre-edited\n3https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view\n8\nDataset Domain Tasks Annotate Agreement #Sent\nSubjective [4] News C annotators high 1,004\nNPOV-manual [6] Wikipedia T crowd medium 230\nLanguagebias [18] Conservapedia C crowd low 685\nPhrasingbias [8] Wikipedia C crowd low 4,952\nWikiBias-Manual (this work) Wikipedia C;T;G annotators high 8,198\nWNC-word [7] Wikipedia T;G automatic { 111k\nWikiBias-Auto (this work) Wikipedia C;T;G automatic { 421k\nTable 3.1: Comparison of biased language detection datasets. C,TandGrefer to sentence\nclassi\fcation, tagging biased spans, and generation for neutralizing bias, respectively.\ntext based on the timestamp. We eventually acquired a parallel corpus of 214,987\nsentence pairs of preand post-NPOV edits.\nAfter reserving 4,099 sentence pairs for human annotation ( \u00a73.2), we apply a\nrule-based method to extract modi\fcations for the remaining 210,888 sentence pairs\nto construct the WikiBias-Auto . We pair up preand post-edited text spans using\na word di\u000b extractor,4and clean with heuristic rules. More details can be found in\nAppendix B. We then treat edited spans in pre-edits as biased and assigned biased and\nneutral sentence-level labels for the sentence pairs respectively, similar to Pryzant et al.\n[7]. When evaluating on the 4,099 manually annotated sentence pairs, this heuristic\nmethod can obtain 87% accuracy for sentence-level labels, 84.7% precision and 76.6%\nrecall for extracting edited spans on the source side. We provide the statistics of\nWikiBias-Auto in Table 4.1.\n4Following Pryzant et al. [7], we use the simpledi\u000b package to compute a minimal di\u000b at word\nlevel: https://github.com/paulgb/simplediff\n9\nWikiBias WikiBias\nAuto Manual\nSentence level\n# of sent pair 210,888 4,099\n# of biased sent 210,888 3,400\n# of neutral sent 210,888 4,798\nSpan-level revisions\n# of source spans 286,156 5,148\n# of unique source spans 153,598 3,804\naverage # of source spans 1.36 1.25\nSource-side biased spans\n# of framing bias { 2,654\n# of epistemological bias { 808\n# of demographic bias { 131\ntotal number of spans 198,413y3,593\naverage # of spans per input 0.94y1.06\naverage length of spans 2.63y2.93\nTable 3.2: Statistics of our WikiBias corpus with automatically ( \u00a73.1) and manually ( \u00a73.2)\nannotations.\n3.2 Fine-grained Human Annotation\nWhile most of these extracted revisions contain biased content as they were \ragged\nby the editors as POV-related, our manual inspection on a sample of 499 sentences\npairs reveals that about 13% of them are not actually biased. Moreover, Wikipedia\neditors may make multiple changes to a sentence (see examples in Table 1.1). In\ncontrast to previous work [ 7] that has discarded these sentences, we designed a two-\nstage annotation procedure to annotate them and include in our dataset. In particular,\nwe introduce a simple but e\u000ecient step of word/phrase alignment, that has not been\n10\nused before for annotating biased language, to tackle the di\u000eculty in identifying biased\nspans in texts with multiple edits.\nRecognizing Edited Spans via Word Alignment For each pair of pre and post-\nedit sentences, we \frst visualize the using GoldAlign, an annotation tool from Gokcen\net al. [29], then ask two in-house annotators to highlight all word/phrase alignments\n(see example in Figure 3.1). We provide detailed guidelines to the annotators with an\nemphasis on identifying the modi\fed spans and their post-edited counterparts can aid\nin the bias classi\fcation task on span level. In the end, we applied a post-processing\nscript to extract non-identical word/phrase pairs from the alignment annotations.\nThe words and phrases that are added or deleted by the Wikipedia editors are also\nextracted as they are aligned to a special symbol [NULL] .\nLabeling Bias Type for Span Pairs We then classify each non-identical word/phrase\nalignment into one of the following categories, following prior work [ 6]: (1) framing\nbias with the use of one-sided words or phrases containing a particular point of view;\n(2)epistemological bias which includes subtle linguistic features that can a\u000bect\nthe believability of the texts; (3) demographic bias with word/phrase usage under\npresuppositions of a particular demographic factor (i.e., gender or religion); or (4) no\nbias.\nWe designed an annotation interface (see Appendix A.1 for a screenshot) using\nLabel Studio [ 30], and asked two in-house annotators (both are native English speakers\nwith college-level education) to label the type of bias at the span-level as shown in\nTable 1.1. We provided annotators with both the edited span pairs and the original\nsentences, taking into consideration the context dependent biases. The pilot study we\n11\nFigure 3.1: Word Alignment and Bias Annotation example for a pre-edited (top) and\npost-edited (left) sentence pair. Grey block means non-identical word/phrase alignment.\nThree edits are extracted: has bene\fted from !saw,inward![NULL] (Deletion), sector\n!corporations while the \frst is labelled as framing bias.\nconducted in the early stage of annotation shows that the proper extraction of span\npairs can assist in identifying the \fne-grained bias types. For example (Figure 3.1),\nknowing that the phrase \\ in order to \" is replaced by \\ which could help \" is helpful for\nannotators to determine that the the former presupposes the usefulness of the subject\nwhile the latter one behaves less determinate.\nWe ended up with the WikiBias-Manual corpus that contains 4,099 sentence\npairs. In total of 1,525 single- and 2,068 multiple- word spans are annotated as biased,\nof which 2,654 are classi\fed as framing , 808 as epistemological and 131 as demographic\nbiases. We derived the sentence-level labels from the span annotations. The pre-edited\n12\nsentences are labeled as biased if one or more edited spans were classi\fed as biased.\nOtherwise, both sentences are marked as neutral.\nAnnotation Agreement Following previous work, we calculate the inter-annotator\nagreements for word/phrase alignment task by comparing one annotator against\nthe gold arbitrated annotations on non-identical (non-trivial) alignments, which are\n98.4/98.5/98.1 and 89.8/89.9/89.5 measured by Precision/Recall/F1 on the token-\nlevel and phrase-level respectively. The inter-annotator agreement is 0.712 for the\n\fne-grained bias type classi\fcation and 0.734 for binary cases (all three types of biases\nvs. no bias) by Cohen's Kappa [ 31], suggesting a substantial agreement. Note that\nwe have two separate groups of annotators for this task, thus in total of 4 workers.\nFor the \frst task, we had one international college student and one high school intern\nhired through the o\u000ecial k-12 outreach educational program, given that the \frst\nannotation task (word alignment) is rather simple for any \ruent English speaker\nand the high school student's work is of good quality when compared with a PhD\nstudent's for inter-annotator agreement. The second annotation task (bias types)\nwas carried out by two undergraduates who double-majored in CS and Linguistics;\nthey are native speakers and have taken su\u000ecient Linguistic courses to be good at\nlinguistic annotations.To ensure the annotation quality, we constantly monitored\nannotators' agreement over 40 random examples in every batch of 200 instances for\ndouble annotation. Double-annotated contents with diverged opinions are further\nexamined by the \frst author, followed by discussions with two annotators until all\nagreed.\n13\nChapter 4: Modeling Subjective Bias\nSubjective biases shall be modeled di\u000berently for various applications. For instance,\nautomatic bots of online media platforms may choose to \rag and \flter out biased\nsentences directly, for which classifying whether a sentence is biased is essential. When\nhuman editors work on an article, they might need some hints on potentially biased\ntext snippets, as well as alternatives, where tagging biased segments or even generating\na neutralized version becomes important. To this end, we propose three di\u000berent tasks\non top of WikiBias .\n4.1 Classi\fcation\nWikiBias enables the development of classi\fers to detect whether a sentence is\nbiased or not on both coarse- and \fne-grained level. We experiment with pre-trained\nlanguage models and test how well they could pick up the nuance di\u000berences between\nbiased and neutral sentences.\n4.1.1 Binary Classi\fcation\nMost prior work on bias detection [ 18,8,19] focus on predicting the presence of\nsubjective bias in a sentence. We follow their setup. We also utilize the heuristically\ncreated WikiBias-Auto data with noisy labels (10% false positives for model training.\n14\nExperimental Setup. We trained multiple binary classi\fers using di\u000berent data\nsplits: (1) use only human-annotated WikiBias-Manual (i.e., Train manual ) data\nfor training; (2) train on WikiBias-Auto (i.e., Train auto) data. We additionally\nexperimented with two methods from the literature for improving the performance\nwith noisy labels: (3) \fnetune the model trained on noisy labels further using the clean\ndata [ 32]; (4) train on a \fltered version of WikiBias-Auto , with top-5% and top-10%\nof automatically labeled \\biased\" instances with the lowest possibility removed [ 33],\nutilizing a classi\fer trained on the original WikiBias-Auto .\nImplementation Details. We use bert-base-uncased model and Adam [ 34]\nfor optimization. We utilize the sentence representations embedded in the [CLS] token,\nthen project it with a weight matrix and jointly \fne-tune the language model and\nclassi\fcation parameters. Each model is \fne-tuned with a maximum of 3 epochs,\nbatch size of 16, learning rate of 2e-5, gradient clip of 1.0, and no weight decay. We\nset the maximum sequence length 128. We save the checkpoint after each epoch and\npick the model with best performance on dev set for \fnal evaluation. We trained the\nmodel which only used Train manual for 5 epochs. For the two step \fne-tuning, We\nfurther \fne-tuned the pre-trained models on Train manual with 3 epochs.\nResults. We observe that, as shown in Table 4.2, the incorporation of large noisy\ndata improves the prediction. The model experiencing two-stage \fne-tuning on\nTrain autoand Train manual sets obtains the highest F1 and Accuracy. Although model\ntrained on clean data secures the highest precision, the low recall value suggests that\nthe small Train manual set fails to fully cover the variants of biases. Meanwhile, removing\nlow con\fdence \\biased\" samples from the training set brings improvements to recall\n15\nDataset Total (#sent) biased neutral SLen\nTrain auto 421,776 210,888 210,888 29.8\nTrain manual 5,028 2,117 2,911 29.2\nDev 1,066 431 635 30.1\nTest 2,104 852 1,252 30.1\nTable 4.1: Data split and size for the experiments. The automatically constructed\nWikiBias-Auto corpus is used for training only (Train auto). The manually annotated\nWikiBias-Manual corpus is split into Train/Dev/Test set. SLen represent the\naverage sentence length in terms of the number of tokens.\nand F1. In the end, we \fnd that the best model only obtains 66 F1, suggesting that\nbaselines are still having trouble capturing biases on sentence level.\n4.1.2 Classi\fcation over Sentence Pairs\nIn section \u00a74.1.1, we observe that there is exists a huge gap between the models\ntraining on di\u000berent source of data, and both below the human agreements. We\nconjecture that the high human agreement during annotation bene\fted from the\nexposure of pre- and post-edit contexts to the annotators while asking for span pair\nannotations, which make it easier for the annotator to capture the subtle di\u000berence\namong the sentence pairs. We then proposed the task of sentence pair classi\fcation,\nwhich models the three conditions of the bias neutralization attempts: (1) successfully\nremoved bias (2) introduced more bias and (3) edit was not related to bias.\nExperimental Setup. We \frst processed the single sentence datasets into the\ncorresponding sentence pair version. Since a majority of the pairs belonged to \\biased-\nneutral\" category and less than 20% of sentence pairs were both neutral, we randomly\n16\nTrain Data P R F1 Acc\nStandard Dataset\nTrain manual 70.2 38.6 52.1 68.1\nTrain auto 63.6 67.1 65.2 71.8\nTrain auto\u00a968.0 63.9 65.8 73.0\nVariations of Train auto\nTrain auto- 5% positive 61.6 68.5 65.0 69.9\nTrain auto- 10% positive 62.0 72.6 66.3 70.0\nSentence Pair Modeling\nTrain manual sentence pair 62.2 66.3 64.2 70.1\nTable 4.2: Binary classi\fcation result on test set with di\u000berent training data, reported on\naverage of three runs. \u00a9means the model is further \fne-tuned on Train manual .\n\ripped the \\biased-neutral\" instances and introduced the label of \\neutral-biased\".\nWe trained the three way BERT classi\fers on Train manual . For \fnal evaluations, we\nbreak the pairs back into single sentences with their corresponding labels, and compare\nit to the single sentence classi\fcation result.\nResult. The bottom rows in Table 4.2 demonstrates the advantages brought by the\nsentence pair modeling. While both training on Train manual data, the pair modeling\nobtains a gain of 12 F1, which can be attribute to the model's capability in separating\nthe biased and neutral sentences while both are given as input.\n4.1.3 Fine-grained Bias Type Classi\fcation\nInitial analysis on the WikiBias-Manual shows that 7% of the biased sentences\ncontain more than one type of biases associated with multiple spans. We thus frame\n17\nthis task as multilabel classi\fcation where three binary classi\fers predict the presence\nof each of the three subcategories (i.e., framing ,epistemological , and demographic ).\nExperimental Setup. We \fne-tuned BERT-base [ 35] via the HuggingFace Trans-\nformers library [ 36]. Pre-training on binary task was explored with the hope to\nincorporate the inductive bias of binary prediction into the \fne-grained setting. In\ndetail, (1) we \fne-tune a classi\fer with the BERT checkpoint and compare it to (2)\ntheFinetuned model with encoder copied from a BERT classi\fer \fne-tuned on the\nbinary task. (3) Similar to Ferracane et al. [37], we use a Hierarchical model with\ntwo classi\fers to mimic the hierarchy of our label categories: the \frst binary classi\fer\npredicts the presence of bias while the second predicts the \fne-grained label.\nResults. We report macro-averaged F1, which gives equal weight to all classes,\non the test set with an average of three runs (Table 4.3). Fine-grained prediction\nsu\u000bers from the imbalance of class labels. The improvement of 5.1 points on macro-\nF1 illustrates that pre-training the encoder with the binary task contributes to the\n\fne-grained classi\fcation. However, in general, the models' performance is relatively\nModel macro-F1class-level F1\nF E D\nBert 33.9 56.3 22.0 24.2\nFinetuned 39.0 62.1 20.5 35.2\nHierarchical 41.0 61.0 26.5 35.8\nTable 4.3: Macro and class-level F1 ( Framing ,Epistemological , and Demographic bias) on\ntest set, averaged across three runs.\n18\nlow, which primarily attributed to the incorrect prediction of epistemological and\ndemographic bias. Hierarchical obtains the highest macro-F1 and the per class\nresults, showing the additional binary classi\fer helps to reduce the prediction error\nforepistemological bias.\n19\n4.2 Tagging of Biased Language Spans\nTo extract the biased spans from given sentences, we frame it as a sequence tagging\ntask using BIO scheme. We also experiment with a joint model in a multi-task\nlearning fashion, aiming at learning inter-relations between the segment tagging and\nthe sentence classi\fcation tasks.\nBiased Segment Tagging. We experiment with multiple baselines(Table 4.4),\nincluding (1) a BiLSTM-CNN-CRF model [ 38] (2) a BERT Atten baseline which extracts\nwords/phrases receiving high self-attention scores in the BERT encoder \fne-tuned\nfor the binary classi\fcation task ( \u00a74.1.1), (3) a DETECTOR model from [ 7] which\nlabels the word with highest predicted probability, and (4) a \fne-tune BERT tagging\nmodel in which we use the base size checkpoint as the encoder and a linear layer\nto predict token labels. Prior work [ 6,7] demonstrated that linguistic features can\nassist in the detection of subjective bias. Thus, (5) we incorporate the linguistic\nfeatures into the BERT-based tagging model. We concatenate the contextualized\nBERT embedding of each token with the encoded discrete linguistic features5and use\na two-layer feed-forward network for \fnal prediction (BERT-LING). To\nJoint Sentence Classi\fcation and Tagging. We deploy a model to jointly learn\nsentence-level classi\fcation and token-level segmentation of bias. More speci\fcally, we\nutilize a BERT tagging model with an additional sentence classi\fer. The model is\ntrained on Train autothrough a joint loss term. We then assign di\u000berent weights for\nthe classi\fcation loss of biased sentences, the classi\fcation loss of neutral sentences,\n5I.e., lexicons of hedges [ 39], factive verbs [ 40], and subjective clues [ 41], we include a complete\nlist of linguistic features in Table\n20\nand the tagging loss of biased sentences, trading o\u000b on the contribution of each task.\nWe also add the Joint Model-LING, where we incorporate in the linguistic features.\nImplementation Details. We used the open-sourced BiLSTM-CRF model by [ 38]\nfor our baselines. For BERT Atten, we extracted the spans by summing up the counts of a\nword receives the highest attention across all 12 heads and \flter out less important ones\nwith thresholds tuned on the development set. For DETECTOR model, we retrained\nthe detection module from [ 7] on a portion of our collected WikiBias-Auto dataset, which\nonly covers the single-word edit, following the setup in [ 7]. For parameters in the\nBERT model, we followed the default settings and set the learning rate 2e-5 with 0.1\npercent warm-up steps. For BERT-LING model, we used a 1-layered linear projector\nto pre-enrich the discrete features, then concatenated the enriched feature vector with\nthe word embedding extracted from BERT and used a \fnal two-layered MLP for label\nprediction. We set the projector and MLP's hidden layer size at 100.\nResults. We report the phrase-level Exact Match and Partial Match F1 on the\nWikiBias-Manual test set in Table 4.4. We \frst observe that the incorporation\nof large noisy data improves the prediction. The injection of linguistic features\nfurther boosts the performance. The state-of-the-art baselines still struggle with\nmulti-span detection, with less than 50 exact F1 score. Thus, our corpus can serve as\na useful research benchmark for future studies. Manual inspections on tagging results\nsuggest that models mainly failed in detecting spans with content-dependent bias and\nmaintaining the completeness of phrases. The joint model achieves worse performance\non segment tagging task which is mainly attributed to the lower recall, while obtains\nslight performance gain on classi\fcation task.\n21\nModelTagging Classi\fcation\nEX F1 P F1 F1 Acc\nTagging\nBiLSTM-CNN-CRF * 32.7 36.4 { {\nBERT Atten 29.8 37.3 { {\nDETECTOR 26.2 35.9 { {\nBERT * 35.3 42.5 { {\nBERT 47.0 54.6 { {\nBERT-LING 47.9 56.4 { {\nBERT-LING\u00a947.9 56.5 { {\nBERT-LINGy48.3 56.8 { {\nClassi\fcation\nBERT { { 65.2 71.8\nBERT\u00a9{ { 65.7 73.0\nJoint Classi\fcation and Tagging Models\nJoint Model 47.0 55.0 66.3 71.2\nJoint Model-LING 47.7 56.0 67.0 71.9\nTable 4.4: Tagging results. * indicates that model is \fne-tuned on Train manual only while all\nothers are trained on Train auto.\u00a9indicates further \fne-tuning on Train manual .yindicates\ntraining on the relabelled Train autowith labels predicted by the best BERT-LING\u00a9model.\nResults are averaged over 3 runs.\n22\n4.3 Text Generation for Neutralizing Bias\nBias neutralization can also be viewed as a text generation problem [ 7]. In this\nsection, we experiment with multiple generation baselines over WikiBias , including\nSimple Copy (directly copy input as output), LSTM and attention based seq2seq model\n[42], CopyNet [ 43], Transformer [ 44], pre-trained BART [ 45] as well the MODULAR\nmodel in [ 7] as baselines. All models are trained on Train autoexcept for the o\u000b-the-shelf\nMODULAR model, which was trained on WNC corpus and could provide comparisons\nbetween multi-span based generation and single-word edit oriented generation.\nImplementation Details. When we use generation models for neutralizing bias,\nwe adapted OpenNMT [ 46] for LSTM and Attention-based Seq2seq and CopyNet\nbaselines. We also used fairseq [ 47] to implement Transformer and BART model. For\nMethodAutomatic Evaluation Human Evaluation\nBLEU\"Sent\nAcc\"Acc\" Fluency\"Bias# Meaning#\nSource Copy 80.10 0.00 { { { {\nLstm 82.12* 15.26* 68.20* 0.090 -0.367* 0.943*\nTransformer 81.34* 15.49 65.96* 0.119* -0.211* 0.989*\nCopyNet 82.95 * 16.31 65.96 -0.030 -0.507* 0.577*\nBart 82.22 17.84 75.35 * 0.017 -0.588* 0.753*\nModulary80.36* 13.76* 51.04* -0.007 -0.313* 1.074*\nTarget Copy 100.0 100.0 80.63 0.023 -0.578* 1.074*\nTable 4.5: Bias neutralization generation results on the test set. All models are trained\non the noisy Train autodata andymeans we used the o\u000b-the-shelf model released by their\nauthors. For automatic metrics, rows with asterisks are signi\fcantly di\u000berent than the\npreceding row. For human evaluation, rows marked with * are signi\fcantly di\u000berent from\n0 (according to a t-test with p < 0.05). \"/#means higher/lower score is preferred for the\ncorresponding metric.\n23\nSeq2Seq model, we use default setting in OpenNMT and a SGD optimizer with a\nlearning rate of 0.5. For Seq2Seq model, we use the default setting in OpenNMT and\na SGD optimizer with a learning rate of 0.5. For CopyNet, we reuse the attention as\ncopy attention, and we also use a SGD optimizer with a learning rate of 1. For BART\nmodel, we used BART-large and an Adam optimizer. We use a polynomial leaning\nrate scheduler with 500 warmup steps and 3e-5 max learning rate. We also use 0.1\ndropout and 0.1 label smoothing. The setting of Transformer is the same as BART\nexcept that Transformer architecture is randomly initialized.\nAutomatic Evaluation. To evaluate the generated sentences, we compared them\nwith neutralization references based on three generation related metrics: BLEU [ 48],\nSent Acc (the percentage of generated sentences that exactly match with the references)\nas well as Acc (the neutralization success rate using our best-performed classi\fer).\nWe report statistical signi\fcance with bootstrap resampling and a 95% con\fdence\nlevel [49, 50].\nAs shown in Table 4.5, CopyNet improves the performance of Seq2Seq, because the\nmodels still retain most words in the original sentence despite the modi\fed multi-word\nspans. Pre-trained BART model outperforms all other models on generating the same\nsentence as the references, although BLEU of BART does not outperform CopyNet.\nThe inconsistent trend of BLEU and Sent Acc indicates that neither automatic metric\nis perfect enough to measure the naturalness of debiased results. We also observe a\nhuge gap on Acc (15 points) between MODULAR model and all others. We suspect\nthat generation models equipped only with single-word bias detection might not pick\n24\nup the complete mutli-word biased spans, thus fail to generate high-quality sentence\nneutralization.\nHuman Evaluation. We also perform a human evaluation on Amazon Mechanical\nTurk over 100 random sentence pairs for each model. Following [ 7], for each sentence\npair (randomized order), we collect 3 judgments on three criteria: Fluency, Meaning\npreservation, and Bias.6Table 4.5 shows that the pre-trained BART model with multi-\nspan edit information outperformed all others in bias mitigation while maintaining\ntext \ruency and preserving the meaning. In contrast, single-word edit-based model\nMODULAR fails to neutralize the bias and su\u000bers from the loss of information by\ndropping o\u000b a single word, a frequent strategy utilized in [7].\nError Analysis. We examine 100 generation results produced by BART and MOD-\nULAR model and compared to the references, observing several error types: (1) No\nchange (30%), (2) Reinforcing Bias (12%) where generated contents become more\nbiased due to improper modi\fcation. For instance, BART changes \\ himself or herself \"\nto \\himself \", which reinforces the demographic bias related to gender. In another\nexample, BART model change the word \\ Sadly \" to \\ However \", making negative\npoint of view more explicit. (3) Noise (10%) in which generated contents successfully\nmitigate the bias, but do not match with the references.\n6Fluency and bias had scales of -2 to 2, Meaning was evaluated on a scale from 0 (identical) to 4\n(totally di\u000berent).\n25\nChapter 5: Generalization to Out-Of-Domain Data\nCorpus F1 Extracted multi-word spans\nBIDEN 21.7they have a plan+,but there are some bad apples, radical,\ntotally thoroughly discredited\u0000,being ripped down, lies\nTRUMP 14.5because Obamacare is no good\u0000,very powerfully, honest,\ntremendous+,very big, incredibly, huge ,big stu\u000b, real dirt\nNEWS 38.0exposes trumps dirty little apprentice lie\u0000,violently,\nfrustrated hypocrite\u0000,barbaric trumpcare\u0000,creepy\nNEWS 15.4huge scandal\u0000,with this mighty act+,trump triumph+,\nseriously wrong, extraordinary+,slammed over, revenge\nIBC 25.5as skillfully as anyone, slightly more legitimate, impressively,\nless-beloved but more dogged, extraordinary+, catastrophic\nIBC 18.0it should be obvious that+, out of thin air, bloated and cruel,\nfrivolous lawsuits is killing the goose that lays the golden egg\nTable 5.1: Samples of frequent multi-word phrases extracted by our tagging model from\neach corpus with manual annotation on polarity of stance. The second column refers\nto the partial matching F1 based on 50 manually annotated samples from each corpus.\nText colors in the \frst column refer to the opinions leaning towards U.S. political parties\nLiberal/Democratic orConservative/Republican . Colored Boxes refer to the target\nofRepublican orDemocratic party respectively and +=\u0000signs illustrate whether the\nphrase is supported or against the stance of the target (i.e., totally irresponsible\u0000illustrates\nthat the speaker uses this phrase to criticize the work of Republican Party).\n26\n5.1 Potentially Biased Data\nTo demonstrate the out-of-domain generalizability of our tagging model, we per-\nform inferences on three out-of-domain datasets: (1) Ideological Books Corpus ( IBC )\n[51,52] which consists of partisan books and magazine article; (2) News headlines\nof partisan news articles identi\fed as biased according to mediabiasfactcheck.com ;\nand (3) Political speeches of the \frst and third 2020 presidency election debates\nbetween Donald Trump and Joe Biden. All three sets of corpora can be separated\ninto two groups based on their partisan identi\fcations (Liberal/Democratic vs. Con-\nservative/Republican). Examples of extracted spans are shown in Table 5.1. We used\nthe test set of the \frst two datasets released by Pryzant et al. [ 7] and crawl the the\nspeech transcripts online.7\n5.1.1 Qualitative Results\nWe \fnd that: (1) Our tagging model can extract meaningful multi-word phrases,\nas well as subtle metaphor phenomenon. For instance, \\ out of thin air \" in the last row\nof Table 5.1 carries the subjective bias of sudden/mysterious appearing. Interesting\nmetaphors such as \\ but there are some bad apples \" would never be detected by a single-\nword tagger. (2) The extracted phrases from the speeches domain cover signature\nwords of the speaker without in-domain knowledge. \\ have a plan \" is prevalent in\n2020's presidency debates and signature words \\ tremendous \" and \\ very powerfully \"\nof Donald Trump have also been captured. (3) The model can tight the connection\nbetween subjective bias with research over stance detection, especially in the formal\ntext domains [ 53,54,55,56]. With our subjective bias tagger, complete verb phrases\n7https://www.rev.com/blog/transcripts/donald-trump-joe-biden-1st-presidential-debate-\ntranscript-2020.\n27\nor noun phrases can be obtained, which naturally eases the extraction of topics\nand opinions, two necessary components for stance detection problem. For instance,\n\\because Obamacare is no good \" span can su\u000eciently illustrate the opinion of Trump\nthat is against the prior healthcare policy. Meanwhile, \\ frustrated hypocrite \" can\nindicate the left-wing media's dislike of the Republican governor's behavior.\n5.1.2 Human Evaluation\nWe sampled 50 sentences per corpus for human annotations. For each sentence,\n3 quali\fed Turkers were asked to pick the biased spans without length constraints.\nWe consider a span receiving more than one annotator vote the gold label. The\nsecond column in Table 5.1 shows that our model performs well on news headlines, as\nthe annotated spans are mostly single or short multi-word spans given the relative\nshort context. In contrast, low agreements are obtained in speech domain. Manual\ninspections reveal that our model tends to tag phrases including subjective pronouns\nsuch as \\ I\" and \\ we\", which are informing signals in the Wikipedia domain for\nexpressing subjective opinions, but under-perform in speech transcripts.\n5.2 Neutral Texts\nTo further study the performance of our tagging model on texts which supposed\nto be neutral (i.e. a good bias tagger supposed to have low false positive rate), we\nutilized the recently released MATH [57] dataset.8MATH is a challenging problem\nsolving dataset consisting of 12,500 problems from high school math competitions.\nEach problem has a step-by-step solution and a \fnal boxed answer and most contents\nare represented in natural language and LATEX, with examples shown in Figure 5.1.\n8github.com/hendrycks/math/.\n28\nFigure 5.1: Examples of the MATH dataset.\nWe randomly sampled 600 questions across six subcategories (Algebra, Calculus, etc.)\nfrom their test set and used our best tagging model to perform inference on the\nquestion part.\n5.2.1 Results\nWe \fnd that our model marked in total of 264 biased spans among 192 sentences,\nwhich takes 30% of all sampled questions. We conduct manual examinations on the\n29\nmistakenly labelled cases and make the following observations: (1) Similar to the\nsituation in speech domain, 62% of tagged spans are of the length less than 4 and\ncontains subjective pronouns such as \\ I have \", \\we are \" and \\ you\". Such phrases\nfrequently appear in the question description such as \"If we are given that\" but do not\ncarry the subjective biases. (2) One more interesting phenomenon we observe is that\n34% of samples have their full ending question body tagged. For instance, in question\n\\The ratio of Mary 's age to Alice 's age is $3:5$. Alice is 30 years old . How\nmany years old is Mary ? \", the highlighted sequence was tagged as biased. This\ncan be attributed to the inappropriate inductive bias introduced from the original\ndataset. That is, in long sentences with detailed description, editors tend to delete\nthe latter part of sentence, which generally plays an elaboration/explanation role of\nthe prior mentioned terms, containing subjectively biased information. (3) Only a\nsmall portion (4%) of the tagged spans covers the frequent adjectives or spans that\nare the source of bias given their context. This analysis points out the challenges in\nbuilding a robust cross-domain bias detector, especially a better way to deal with\nmodel's over-con\fdence in capturing unrelated surface patterns of the texts.\n30\nChapter 6: Conclusion and future work\n6.1 Summary\nThis thesis explored the problem of detecting and further mitigating subjective\nbias - a special type of bias that introduces improper attitudes or presents a statement\nwith the presupposition of truth - in language, with a focus on Wikipedia domain.\nWe introduce a manually annotated parallel corpus WikiBias with more than 4,000\nsentence pairs from Wikipedia edits. This corpus contains annotations towards both\nsentence-level bias types and token-level biased segments. We present systematic\nanalyses of our dataset and results achieved by a set of state-of-the-art baselines in\nterms of three tasks: bias classi\fcation, tagging biased segments, and neutralizing\nbiased text. We \fnd that current powerful models still struggle with detecting the\nsubtle multi-span biases despite their reasonable performances, suggesting that our\ndataset can serve as a useful research benchmark. We also demonstrate that models\ntrained on our dataset can generalize well to multiple domains such as news and\npolitical speeches, while also pointing out several challenges existed for the cross-\ndomain bias detection.\n31\n6.2 Future Work\nWe are looking at the methods to better utilize the automatically collected dataset\nto improve the model's performance, observing that there still exists a huge gap toward\nthe human performance across the sentence classi\fcation and token tagging task.\nIn order to transfer the bias detection capability better to other domains, we\nare looking at several domain adaptation techniques with the hope that a small\nnumber of in-domain data could help the model avoid making improper predictions in\nnon-Wikipedia domains such as the tagging of subjective pronouns in speeches.\nOne other direction we can do is to harness the human annotation disagreement\nfrom the annotated data to better tackle to ambiguous cases. Fornaciari et al. [ 58] use\nsoft labels (i.e., probability distributions over the annotator labels) as an auxiliary task\nin a multi-task neural network for POS task, demonstrating that the incorporation of\nthe annotation information could reduce the penalty for errors on ambiguous entities\nand thereby mitigates over\ftting problem.\nIn the end, we also hope to extend this corpus into the multi-lingual and multi-\ndomain setting, which could be a valid resource for researchers in the emerging bias\ndetection track and further contribute to the real word applications to help preserving\nthe factuality and objectiveness of the contents.\n32\nAppendix A: Annotation Interfaces\n33\nA.1 Word/phrase Classi\fcation Interface\nFigure A.1: Annotation interface for Bias Classi\fcation of word/phrase edits.\n34\nA.2 Generation Classi\fcation Interface\nFigure A.2: Annotation guidelines for the evaluation on text generation results along\nwith a example question.\n35\nAppendix B: Rule-based System\nGiven a parallel sentence pair, we utilize the di\u000bs9as a starting point. In detail,\nthe package returns a list of edit tuples, each containing a modi\fcation sign and the\ncorresponding sub-sequence of the sentences.10\nWe apply di\u000berent rules for varying scenarios. For sentence pair with one single-\n/multi- word phrase change, we match nearby edit in the extracted di\u000bs with \"-\" and\n\"+\" signs as substitution edit pairs and leave else as one deletion and one addition .\nThis is inspired by the observation that people would replace the old word/phrase\nwith a new one in the same location. Note that we also apply several cleaning rules to\n\flter out non-bias related modi\fcations such as spell correction.\nFor sentence pairs with multiple word/phrase changes, similar to the single edit\nextraction, we \frst aim at extracting all substitution cases. However, due to the\ncomplexity of the multiple changes, even neighboring changes can be non-related. We\nalso \fnd that several phrase pairs are broken into multiple pieces due to the duplicated\nprepositions and determinants. To handle such cases, we \frst parse the raw output\nof the di\u000bs and reconnect the disjoint pieces into complete continuous phrases. We\nthen use a constituency parser [ 59] to check whether two candidate changes belong\n9Following the work of [7], we use the simpledi\u000b package to extract di\u000bs\n10i.e. [(\"=\", [The Irish economy]), (\"-\", [has bene\fted from]), (\"+\", saw) ...] in Figure 3.1.\n36\nto the same type of sub-tree. For the remaining changes, we greedily compute the\nsimilarities between the edit pairs in the pre and post-edited sentence, then utilize a\nthreshold tuned on the dev set to construct more substitutions . In the end, we label\nthe remaining without alignments as deletion oraddition accordingly.\n37\nBibliography\n[1]Ellen Rilo\u000b and Janyce Wiebe. Learning extraction patterns for subjective\nexpressions. In Proceedings of the 2003 conference on Empirical methods in\nnatural language processing , pages 105{112, 2003.\n[2]Janyce Wiebe and Ellen Rilo\u000b. Creating subjective and objective sentence\nclassi\fers from unannotated texts. In International Conference on Intelligent\nText Processing and Computational Linguistics , pages 486{497, 2005.\n[3]Gabriel Murray and Giuseppe Carenini. Predicting subjectivity in multimodal\nconversations. In Proceedings of the 2009 Conference on Empirical Methods in\nNatural Language Processing , pages 1348{1357, 2009.\n[4]Janyce Wiebe, Rebecca Bruce, and Thomas P O'Hara. Development and use of a\ngold-standard data set for subjectivity classi\fcations. In Proceedings of the 37th\nannual meeting of the Association for Computational Linguistics , pages 246{253,\n1999.\n[5]Theresa Wilson and Stephan Raaijmakers. Comparing word, character, and\nphoneme n-grams for subjective utterance recognition. In Ninth Annual Confer-\nence of the International Speech Communication Association , pages 1614{1617,\n2008.\n38\n[6]Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. Linguistic\nmodels for analyzing and detecting biased language. In Proceedings of the 51st\nAnnual Meeting of the Association for Computational Linguistics , pages 1650{\n1659, 2013.\n[7]Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan\nJurafsky, and Diyi Yang. Automatically neutralizing subjective bias in text. In\nProceedings of the AAAI Conference on Arti\fcial Intelligence , pages 480{489,\n2020.\n[8]Christoph Hube and Besnik Fetahu. Neural based statement classi\fcation for\nbiased language. In Proceedings of the 12th ACM International Conference on\nWeb Search and Data Mining , pages 195{203, 2019.\n[9]Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie\nMartin. Learning subjective language. Computational linguistics , 30(3):277{308,\n2004.\n[10]Jussi Karlgren and Douglass Cutting. Recognizing text genres with simple metrics\nusing discriminant analysis. In Proceedings of the 15th International Conference\non Computational Linguistics , 1994.\n[11]Brett Kessler, Geo\u000brey Nunberg, and Hinrich Schutze. Automatic detection of\ntext genre. In Proceedings of the 35th Annual Meeting of the Association for\nComputational Linguistics and 8th Conference of the European Chapter of the\nAssociation for Computational Linguistics , pages 32{38, 1997.\n39\n[12]Rebecca F Bruce and Janyce M Wiebe. Recognizing subjectivity: a case study\nin manual tagging. Natural Language Engineering , 5(2):187{205, 1999.\n[13]Vasileios Hatzivassiloglou and Janyce M. Wiebe. E\u000bects of adjective orientation\nand gradability on sentence subjectivity. In Proceedings of the 18th International\nConference on Computational Linguistics , 2000.\n[14]Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using\nsubjectivity summarization based on minimum cuts. In Proceedings of the 42nd\nAnnual Meeting on Association for Computational Linguistics , page 271{278,\n2004.\n[15]Chenghua Lin, Yulan He, and Richard Everson. Sentence subjectivity detec-\ntion with weakly-supervised learning. In Proceedings of 5th International Joint\nConference on Natural Language Processing , pages 1153{1161, 2011.\n[16]Diyi Yang, Aaron Halfaker, Robert Kraut, and Eduard Hovy. Identifying semantic\nedit intentions from revisions in wikipedia. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing , pages 2000{2010, 2017.\n[17]Fred Morstatter, Liang Wu, Uraz Yavanoglu, Stephen R Corman, and Huan Liu.\nIdentifying framing bias in online news. ACM Transactions on Social Computing ,\n1(2):1{18, 2018.\n[18]Christoph Hube and Besnik Fetahu. Detecting biased statements in wikipedia. In\nCompanion Proceedings of the The Web Conference 2018 , pages 1779{1786, 2018.\n40\n[19]Kartikey Pant, Tanvi Dadu, and Radhika Mamidi. Towards detection of subjective\nbias using contextualized word embeddings. In Companion Proceedings of the\nWeb Conference 2020 , pages 75{76, 2020.\n[20]Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. Paraphrasing\nfor style. In Proceedings of the 24th International Conference on Computational\nLinguistics , pages 2899{2914, 2012.\n[21]Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing.\nToward controlled generation of text. In Proceedings of the 34th International\nConference on Machine Learning , pages 1587{1596, 2017.\n[22]Sravana Reddy and Kevin Knight. Obfuscating gender in social media writing.\nInProceedings of the First Workshop on NLP and Computational Social Science ,\npages 17{26, 2016.\n[23]Wei Xu, Chris Callison-Burch, and Courtney Napoles. Problems in current text\nsimpli\fcation research: New data can help. Transactions of the Association for\nComputational Linguistics , 3:283{297, 2015.\n[24]Sudha Rao and Joel Tetreault. Dear sir or madam, may I introduce the GYAFC\ndataset: Corpus, benchmarks and metrics for formality style transfer. In Proceed-\nings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies , pages 129{140,\n2018.\n[25]Juncen Li, Robin Jia, He He, and Percy Liang. Delete, retrieve, generate: a\nsimple approach to sentiment and style transfer. In Proceedings of the 2018\n41\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages 1865{1874, 2018.\n[26]Wouter Leeftink and Gerasimos Spanakis. Towards controlled transformation of\nsentiment in sentences. arXiv preprint arXiv:1901.11467 , 2019.\n[27]Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos, Graham\nNeubig, Yiming Yang, Ruslan Salakhutdinov, Alan W Black, and Shrimai Prab-\nhumoye. Politeness transfer: A tag and generate approach. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics , pages\n1869{1881, 2020.\n[28]Fabio Massimo Zanzotto and Marco Pennacchiotti. Expanding textual entailment\ncorpora fromwikipedia using co-training. In Proceedings of the 2nd Workshop on\nThe People's Web Meets NLP: Collaboratively Constructed Semantic Resources ,\npages 28{36, 2010.\n[29]Ajda Gokcen, Evan Ja\u000be, Johnsey Erdmann, Michael White, and Douglas Dan-\nforth. A corpus of word-aligned asked and anticipated questions in a virtual\npatient dialogue system. In Proceedings of the 10th International Conference on\nLanguage Resources and Evaluation , pages 3174{3179, 2016.\n[30]Maxim Tkachenko, Mikhail Malyuk, Nikita Shevchenko, and Nikolai Liubimov.\nLabel Studio: A swiss army knife of data labeling and annotation tools, 2020.\nOpen source software available from https://github.com/heartexlabs/label-studio.\n[31]Ron Artstein and Massimo Poesio. Survey article: Inter-coder agreement for\ncomputational linguistics. Computational Linguistics , 34(4):555{596, 2008.\n42\n[32]Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander\nToshev, Tom Duerig, James Philbin, and Li Fei-Fei. The unreasonable e\u000bectiveness\nof noisy data for \fne-grained recognition. In European Conference on Computer\nVision , pages 301{320, 2016.\n[33]Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia\nLi. Learning from noisy labels with distillation. In Proceedings of the IEEE\nInternational Conference on Computer Vision , pages 1910{1918, 2017.\n[34]Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization.\nIn3rd International Conference on Learning Representations , 2015.\n[35]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies , pages\n4171{4186, 2019.\n[36]Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-\nlangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u0013 emi Louf, Morgan Funtowicz,\net al. Transformers: State-of-the-art natural language processing. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations , pages 38{45, 2020.\n[37]Elisa Ferracane, Greg Durrett, Junyi Jessy Li, and Katrin Erk. Did they an-\nswer? Subjective acts and intents in conversational discourse. arXiv preprint\narXiv:2104.04470 , 2021.\n43\n[38]Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional\nLSTM-CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics , pages 1064{1074, 2016.\n[39]Geo\u000b Thompson. Ken hyland, metadiscourse: Exploring interaction in writing.\nLanguage in Society - LANG SOC , 37, 2005.\n[40]Joan Bybee Hooper. On assertive predicates. , volume 4. Syntax and Semantics,\n1975.\n[41]Theresa Wilson, Janyce Wiebe, and Paul Ho\u000bmann. Recognizing contextual\npolarity in phrase-level sentiment analysis. In Proceedings of Human Language\nTechnology Conference and Conference on Empirical Methods in Natural Language\nProcessing , pages 347{354, 2005.\n[42]Minh-Thang Luong, Hieu Pham, and Christopher D Manning. E\u000bective ap-\nproaches to attention-based neural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural Language Processing , pages\n1412{1421, 2015.\n[43]Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. Incorporating copying\nmechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics , pages 1631{1640, 2016.\n[44]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nInAdvances in Neural Information Processing Systems , pages 5998{6008, 2017.\n44\n[45]Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising\nsequence-to-sequence pre-training for natural language generation, translation,\nand comprehension. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 7871{7880, 2020.\n[46]Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush.\nOpenNMT: Open-source toolkit for neural machine translation. In Proceedings of\nthe 55th annual meeting of the Association for Computational Linguistics, System\nDemonstrations , pages 67{72, 2017.\n[47]Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,\nDavid Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence\nmodeling. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics (Demonstrations) , pages 48{53,\n2019.\n[48]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method\nfor automatic evaluation of machine translation. In Proceedings of the 40th annual\nmeeting of the Association for Computational Linguistics , pages 311{318, 2002.\n[49]Philipp Koehn. Statistical signi\fcance tests for machine translation evaluation.\nInProceedings of the 2004 Conference on Empirical Methods in Natural Language\nProcessing , pages 388{395, 2004.\n[50]Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap . CRC\npress, 1994.\n45\n[51]Yanchuan Sim, Brice DL Acree, Justin H Gross, and Noah A Smith. Measuring\nideological proportions in political speeches. In Proceedings of the 2013 Conference\non Empirical Methods in Natural Language Processing , pages 91{101, 2013.\n[52]Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip Resnik. Political\nideology detection using recursive neural networks. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational Linguistics , pages 1113{\n1122, 2014.\n[53]Matt Thomas, Bo Pang, and Lillian Lee. Get out the vote: Determining support\nor opposition from congressional \roor-debate transcripts. In Proceedings of the\n2006 Conference on Empirical Methods in Natural Language Processing , pages\n327{335, 2006.\n[54]Marilyn Walker, Jean Fox Tree, Pranav Anand, Rob Abbott, and Joseph King.\nA corpus for research on deliberation and debate. In Proceedings of the 8th\nInternational Conference on Language Resources and Evaluation , pages 812{817,\n2012.\n[55]Tuhin Chakrabarty, Christopher Hidey, Smaranda Muresan, Kathy McKeown,\nand Alyssa Hwang. AMPERSAND: Argument mining for PERSuAsive oNline\ndiscussions. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on\nNatural Language Processing , pages 2933{2943, November 2019.\n[56]John Lawrence and Chris Reed. Argument mining: A survey. Computational\nLinguistics , 45(4):765{818, 2020.\n46\n[57]Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart,\nEric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem\nsolving with the math dataset. arXiv preprint arXiv:2103.03874 , 2021.\n[58]Tommaso Fornaciari, Alexandra Uma, Silviu Paun, Barbara Plank, Dirk Hovy,\nand Massimo Poesio. Beyond black & white: Leveraging annotator disagreement\nvia soft-label multi-task learning. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 2591{2597, June 2021.\n[59]Nikita Kitaev and Dan Klein. Constituency parsing with a self-attentive encoder.\nInProceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics , pages 2676{2686, 2018.\n47", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "WIKIBIAS: Detecting multi-span subjective biases in language", "author": ["Y Zhong"], "pub_year": "2021", "venue": "NA", "abstract": "Biases continue to be prevalent in modern text and media, especially subjective bias\u2013a  special type of bias that introduces improper attitudes or presents a statement with the"}, "filled": false, "gsrank": 191, "pub_url": "https://rave.ohiolink.edu/etdc/view?acc_num=osu162635979683589", "author_id": ["p6xPukQAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:UkyYSVTSSygJ:scholar.google.com/&output=cite&scirp=190&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D190%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=UkyYSVTSSygJ&ei=J7WsaLrrFfnSieoPxKLpgQ0&json=", "num_citations": 23, "citedby_url": "/scholar?cites=2903645644224220242&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:UkyYSVTSSygJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://etd.ohiolink.edu/acprod/odb_etd/ws/send_file/send?accession=osu162635979683589&disposition=inline"}}, {"title": "Advised or Paid Way to get it right. The contribution of fact-checking tips and monetary incentives to spotting scientific disinformation", "year": "2021", "pdf_data": "Advised or Paid Way to get it right. The contribution of\nfact-checking tips and monetary incentives to spotting\nscientific disinformation\nFolco Panizza1*,Piero Ronzani1, Simone Mattavelli2,Tiffany Morisseau3,4,5, Carlo\nMartini1,6,Matteo Motterlini1\n1Centre for Applied and Experimental Epistemology, Vita-Salute San Raffaele\nUniversity, Via Borromeo, 41, 20811 Cesano Maderno (MB), Italy.\n2Department of Psychology, Bicocca University, Piazza dell'Ateneo Nuovo, 1, 20126\nMilano, Italy.\n3Universit\u0013 e de Paris, Laboratoire de Psychologie et d'Ergonomie Appliqu\u0013 ees,\nBoulogne-Billancourt, France\n4Laboratoire de Psychologie et d'Ergonomie Appliqu\u0013 ees, Universit\u0013 e Gustave Eiffel,\nVersailles, France\n5Strane Innovation, Gif-sur-Yvette, France\n6TINT { Centre for Philosophy of Social Science, Department of Political and\nEconomic Studies, University of Helsinki, P.O. Box 24 (Unioninkatu 40A), 00014\nHelsinki, Finland.\n* corresponding author: panizza.folco@hsr.it\nAbstract\nDisinformation about science can impose enormous economic and public health burdens.\nSeveral types of interventions have been proposed to prevent the proliferation of false\ninformation online, where most of the spreading takes place. A recently proposed\nstrategy to help online users recognise false content is to follow the techniques of\nprofessional fact checkers, such as looking for information on other websites (lateral\nreading) and looking beyond the first results suggested by search engines (click\nrestraint). In two preregistered online experiments (N = 5387), we simulated a\nsocial-media environment and set-out two interventions, one in the form of a pop-up\nmeant to advise participants to follow such techniques, the other based on monetary\nincentive. In Experiment 1, we compared these interventions to a control condition. In\nExperiment 2 another condition was added to test the joint impact of the pop-up and\nthe monetary incentive. We measured participants' ability to identify whether presented\ninformation was scientifically valid or invalid. Results revealed that while monetary\nincentives were overall more effective in increasing accuracy, the pop-up contributed\nwhen the post originated from an unknown source (and participants could rely less on\nprior information). Additional analysis on participants' search style based on both\nself-report responses and objectively measured behaviour revealed that the pop-up\nincreased the use of fact-checking strategies, and that these in turn increased accuracy.\nStudy 2 also clarified that the pop-up and the incentive did not interfere with each\nother, but rather acted complementarily, suggesting that attention and literacy\ninterventions can be designed in synergy.\nSeptember 27, 2021 1/31\nIntroduction 1\nThe massive circulation of inaccurate scientific information can have nefarious societal 2\nconsequences. Successful misconceptions influence the public debate on decisions 3\nregarding the effectiveness of a vaccine, the adoption of solutions mitigating climate 4\nchange, or the cost of a social policy. The sharing of false information is easily fuelled 5\nby political or social motivations that disregard the best scientific evidence on the 6\nmatter. It is indeed tempting to share information on social media without verifying its 7\ntruthfulness, simply because the mere act of sharing allows us to exhibit our position on 8\na given topic and to justify the validity of such a position. This phenomenon is 9\namplified in crisis situations when scarce information is accompanied by multiple and 10\ncontrasting rumours (also called infodemic) that could serve different views. People's 11\npropensity to accept scientifically dubious information can thus become a crucial 12\nproblem for both democracy and public welfare. 13\nThere are structural challenges to fighting the spread of false information on social 14\nmedia. One key issue is that companies often perceive a trade-off between engaging 15\nusers and combating viral but fake content, to the point of favouring the former over 16\nthe latter [1]. Curtailment is made even more difficult when there is a deliberate intent 17\nbehind the dissemination, what researchers refer to as disinformation. For example, at 18\nthe peak of the coronavirus infodemic, only 16% of fact-checked disinformation was 19\nlabelled as such by Facebook's algorithms, partly because content creators were able to 20\nsimply repost content with minor changes, thus escaping detection [2]. It is therefore 21\nessential that, in combination with a systematic change in policy, users themselves are 22\nempowered against malicious or false content. User-based resilience needs to be part of a 23\ntoolkit to fight disinformation: for instance, among the pillars of infodemic management, 24\nEysenbach [3] lists eHealth Literacy, science literacy capacity, and critical thinking 25\nability to fact-check information. Fighting science-related disinformation is harder than 26\ncontrasting other forms of disinformation (e.g. political) because in the former case the 27\nlines between expertise and pseudoexpertise are blurred, and incompetent or otherwise 28\nbiased sources pose as expert sources on topics like epidemiology or climate change. 29\nResearch on countering disinformation has developed substantially over the last 30\ndecade, bringing a wealth of different approaches [4{8]. These include debunking, the 31\nsystematic correction of false claims after they have been seen or heard [9,10], 32\npre-bunking, preventive measures before exposure to disinformation [5,11], nudging, 33\ninterventions affecting users' choices without limiting their freedom of choice [12], and 34\nboosting, the empowering of users by fostering existing competences or instilling new 35\nones [12]. All of the above approaches have proven to be useful in a social media 36\ncontext, not least by adopting ingenious and innovative adaptations of classical 37\nparadigms. Debunking has been extensively studied, with several experiments focusing 38\non the source [13{16] and the timing [17] of fact checking. Research has also explored 39\nwhether evaluations about the quality of contents and sources can be delegated to the 40\nso-called wisdom of crowds, with encouraging results [18{20]. Studies on pre-bunkning 41\nhave largely focused on the concept of inoculation [5,21], namely exposing users to 42\ndisinformation strategies in order to ease their recognition in future settings. 43\nInoculation has demonstrated pronounced and lasting effects when introduced through 44\ngames [22{25]. Nudging was also tested by showing warning labels for unchecked or 45\nfalse claims [26{29], but also by priming users to pay attention to the accuracy of 46\ncontent they might be willing to share [30{32] (however see [33] for a critique of this 47\napproach). Finally, boosting was tested by presenting users with a list of news/media 48\nliteracy tips or guidelines on how to evaluate information on-line [34{38], producing 49\nsome remarkable results and some non-significant ones. 50\nA promising example of media literacy intervention has been carried out by 51\nresearchers interested in understanding how fact checkers search for information about 52\nSeptember 27, 2021 2/31\nunknown but institutional-looking sources [39]. Researchers catalogued fact checkers' 53\nstrategies and distilled a series of questions to evaluate content, a set of skills that was 54\nnamed Civic Online Reasoning [40,41]. Two are the most prominent strategies adopted 55\nby fact checkers. One is lateral reading, namely leaving a website and opening new tabs 56\nalong a horizontal axis in order to use the resources of the Internet to learn more about 57\na site and its claims. The other is click restraint, that is, skipping the first search results 58\nof a browser search to avoid biases created by results-ranking algorithms. These 59\nstrategies seem particularly fit when a content has unknown origins that are hardly 60\nidentifiable or that appear legitimate on the surface, a feature that has been associated 61\nwith content creators spreading scientific disinformation and disinformation [42]. 62\nDetecting scientific disinformation often requires specific expertise to evaluate the 63\ncontent and cross-check sources. Under such conditions, assessing the truthfulness of 64\ninformation becomes tricky. 65\nIn the absence of expertise and content knowledge, users can rely on a number of 66\nexternal cues to infer whether information presented as scientific is reliable [43]. Unlike 67\nfake news, scientific disinformation relies on background knowledge about the expertise 68\nof the source. We can check, for example, whether a piece of information is agreed upon 69\nby the scientific community, or whether a source is a genuine expert one or a 70\npseudo-expert one, and these elements can give us important cues as to whether a 71\nsource conveys scientific, rather than pseudo-scientific, information. Lateral reading and 72\nclick restraint can thus be used when scientific disinformation is deceptively 73\nsophisticated and difficult to detect. Indeed, training on Civic Online Reasoning has 74\nproven very effective in countering disinformation among high school and college 75\nstudents [44{46], as well as elderly citizens [47]. Despite extensive research on Civic 76\nOnline Reasoning, so far little attention has been paid to the application of these 77\ntechniques on social media. It is therefore unclear how effective presenting these 78\nstrategies on a social network can actually be. 79\nCritical thinking strategies might not be the only potentially effective tools in 80\nevaluating scientific (dis)information. For instance users might not be sufficiently 81\nmotivated to evaluate the truthfulness of the content they see. Many users might share 82\nnews simply because they come from a source they trust or like, or because those news 83\nalign with their values, without paying much attention to trust. The spread of scientific 84\ndisinformation then is not only related to false beliefs, but also to motivated behavior, 85\npaired with strong personal identities and values. In order to better exploit the benefits 86\nof critical thinking tools, it is therefore also important to identify the respective effects 87\nof being aware of truth-motivated strategies; i.e., being motivated to know the truth 88\nabout a given topic. It may be that people, while being somehow familiar with 89\nfact-checking and civic online reasoning techniques, are only eager to apply them when 90\nidentifying the truthfulness of the information is reinforced by specific incentives. 91\nOne way to test the effect of motivation then is the use of monetary incentives. In 92\nother words, does paying participants for their being accurate increase their accuracy in 93\nthe evaluation of content? The idea behind this intervention is that money increases 94\nmotivation, and thus the attention paid to otherwise ignored cues about the accuracy of 95\ncontent. Supporting this view, a study conducted with a sample of Mechanical Turk 96\nworkers on comparable settings showed that monetary incentives are the main driver for 97\npeople to spend time on the platform and, even in the face of small average earnings, 98\naspects such as immediate payment play an important role in workers' motivation [48]. 99\nMonetary incentives have been proven to be a cost effective tool to modify behavior 100\nin domains such as health and human development [49], where often an early boost in 101\nmotivation promotes the adoption of cheap preventive behaviours, avoiding this way 102\ncostly consequences [50]. From a psychological perspective, the use of incentives builds 103\non the attention-based account of disinformation spread. This account posits that 104\nSeptember 27, 2021 3/31\ncertain features of social networks favour the dissemination of interesting and 105\nunexpected content at the expense of accuracy [4,51]. Recent research in this field has 106\nfound both laboratory and field evidence that accuracy of content is often overlooked 107\nand that simple cues reminding participants to evaluate the accuracy of content they 108\nshare has an impact in terms of the proportion of fake/true news shared [30,32,52,53]. 109\nIncreasing accuracy through incentives is not an entirely novel idea in social media 110\neither, as shown in a recent initiative promoted by Twitter [54]. Although these 111\npremises indicate that this type of intervention can be very effective, it is not a given 112\nthat economic incentives will have a positive effect on scientific content evaluation. In 113\nan experimental setting in particular, social media content is subject to higher scrutiny 114\nthan when users scroll through their news feed [30]. It is therefore possible that 115\nadditional incentives may not further increase participants' accuracy. 116\nThe aim of the present study was to test and compare the effectiveness of Civic 117\nOnline Reasoning techniques and monetary incentives in contributing to the recognition 118\nof science-related content on social media. We conducted two pre-registered experiments 119\nwhere participants observed and interacted with one out of several Facebook posts that 120\nlinked to an article presenting science-themed information. Participants were free to 121\nconduct further research on external websites in order to form a more accurate idea of 122\nthe scientific validity of the post. Once satisfied with the information they gathered, 123\nparticipants rated how scientifically valid the claims contained in the post were. To test 124\nfor the usefulness of Civic Online Reasoning techniques, we designed a pop-up that 125\npreceded the post presenting the lateral reading and click restraint strategies 1. The use 126\nof a pop-up ensured that participants processed the content before observing the post, 127\nan approach that has also been adopted in previous research [52]. A pop-up could be 128\neasily adapted in a social media setting as regular reminders with the necessary 129\nprecautions to avoid the reduction of their salience with time [55,56]. To test the effect 130\nof monetary incentives instead, we doubled the participation fee (equivalent to an 131\naverage + \u00a38.40/hour) if participants guessed correctly the validity of the post they were 132\nevaluating. 133\nFig 1. Screenshot of the pop-up presented to participants.\nSeptember 27, 2021 4/31\nExperiment 1 134\nIn Experiment 1, we tested separately the efficacy of pop-up and monetary incentives, 135\nand compared their effects to a control condition with no interventions. To assess that 136\nthe effect of the interventions is effective over the widest possible range of contexts, we 137\nused a set of 9 different Facebook posts varying in various properties, such as the 138\nscientific topic, the source reputation, and its level of factual reporting. The original 139\npre-registration of this experiment can be retrieved from osf.io/gsu9j. 140\nMaterials and methods 141\nParticipants 142\nWe recruited 2700 U.K. residents through the online platform prolific.co on 11 March 143\n2021 (for a rationale of sample size, see S1 Methods). All participants gave their 144\ninformed consent for participating in the experiment. Average age was 36 ( SD= 13:5, 8 145\nnot specified), 60.7% of participants were female, (39.1% male, 0.2% other), and 55.6% 146\nhad a Bachelor's degree or higher. Although recruitment explicitly specified that the 147\nexperiment was supported only on computers or laptops, 316 participants (11.7%) 148\ncompleted the experiment on a mobile device. As our hypotheses were based on the 149\nassumption that search would happen on a computer, both stimuli and measures were 150\nnot designed for mobile use. We therefore had to exclude these participants from the 151\nanalyses. Analyses were thus conducted on 2384 participants. 152\nDesign 153\nWe conducted the experiment on Qualtrics and lab.js [57]. During the experiment, 154\nparticipants observed and were able to interact with one out of several Facebook-like 155\nposts (Fig 2 shows three examples; click here for an interactive example from 156\nExperiment 2). Participants' task was to rate the scientific validity of the statements 157\nreported in the title, subtitle, and caption of the post (\"how scientifically valid would 158\nyou rate the information contained in the post?\"; 6-point likert scale from (1) 159\n\"definitely non-valid\" to (6) \"definitely valid\"). Researchers rated independently the 160\nscientific validity of the posts' content in terms of valid/invalid according to 161\npre-specified criteria (see S3 Methods). Participants could take as much time as they 162\nwanted in giving their rating. Crucially, participants were also explicitly told that they 163\nwere allowed to leave the study page before evaluating the post. After the rating, 164\nparticipants completed a questionnaire and were paid \u00a30.70 for their time. Median 165\ncompletion time of the experiment was 5 minutes. 166\nExperimental conditions. Participants were randomly assigned to one of three 167\nexperimental conditions: control, incentive, and pop-up. In the control condition, 168\nparticipants completed the task as described above. In the incentive condition, 169\nparticipants were doubled their participation fee if their rating matched that given by 170\nthe experimenters. Unbeknownst to participants, the correctness of the answer depended 171\nonly on whether the answer was valid or invalid, and not on the extremity of the answer 172\n(e.g. having answered 4 instead of 5), even though we selected unambiguously valid or 173\ninvalid content. In the pop-up condition, presentation of the post was preceded by a 174\npop-up (Fig 1) presenting a list of civic online reasoning techniques (e.g., lateral reading, 175\nclick restraint) as tips to verify the information in the post. 176\nStimuli. Each participant observed one out of nine possible Facebook posts (Fig 2; 177\nsee S1 File for a full list). Posts varied in terms of: (i) scientific validity of the content 178\n(i.e., six valid and three invalid posts, either with verified or debunked information; S3 179\nMethods); (ii) topic (i.e., three on climate change, three on the coronavirus pandemic, 180\nthree on health and nutrition); (iii) factual reporting of the source, based on ratings 181\nSeptember 27, 2021 5/31\nfrom mediabiasfactcheck.com (i.e., three high/very high versus six low/very low); (iv) 182\nsource reputation, as measured in a screening survey (S4 Methods; three categories: 183\ntrusted (2 posts), distrusted (4), unknown source (3)). Posts were balanced to have 184\nthree posts for each topic, one from a source with high factual reporting displaying valid 185\ninformation, one from a source with low factual reporting displaying valid information, 186\nand one from a source with low factual reporting displaying invalid information. 187\nFig 2. Examples of the stimuli presented, varying in topic (Climate Change, Health and\nNutrition, COVID-19), factual reporting (high, low, low), scientific validity (high, low, low),\nand source reputation (trusted, untrusted, unknown source).\nWe standardised emoji reactions across all posts to control for their influence. In 188\naddition, post date, number of reactions and shares were blurred. The rest of the post 189\nwas instead accessible to the participant, who could click on different links to access the 190\nsource Facebook page, the original article, and the Wikipedia page (if present). Text 191\nand image were taken from the article. Captions were short statements of a scientific 192\nnature, i.e. facts or events pertaining to some scientific mechanism. 193\nMeasures 194\nAccuracy. We computed two measures of accuracy{correct guessing and accuracy 195\nscore. Correct guessing refers to a dichotomous variable that tracks whether participant 196\ngave a 'valid' (vs. 'invalid') rating when the post content was actually scientifically valid 197\n(vs. invalid). Accuracy score instead is a standardised measure ranging from zero to one, 198\nwith 0 indicating an incorrect \"1\" or \"6\" validity rating, 0.2 indicating an incorrect \"2\" 199\nor \"5\" rating, 0.4 an incorrect \"3\" or \"4\" rating, 0.6 a correct \"3\" or \"4\" rating, 0.8 a 200\ncorrect \"2\" or \"5\" rating, and 1 a correct \"1\" or \"6\" rating. Accuracy score allows to 201\ndistinguish validity evaluations that are associated with different behaviours: for 202\ninstance, not all participants would be willing to share content that they rated as 4 in 203\nterms of scientific validity. In addition, accuracy score is statistically more powerful 204\nthan correct guessing as it includes more possible responses [58]. We thus considered 205\naccuracy score as our main index. 206\nSearch behaviour. During the evaluation of the post, we tracked participants' 207\nbehaviour on the study page. We measured the time spent both inside and outside the 208\npage, and a series of dummy variables tracking whether participants had clicked on any 209\nof the links present (e.g., Facebook page, article page, Wikipedia page). Based on these 210\ncalculations we were able to estimate participants' response times and search behaviour. 211\nCivic Online Reasoning. After having rated the scientific validity of the post, 212\nparticipants completed a questionnaire investigating those factors that could have 213\ninfluenced their choice. In order to test our hypotheses, we asked participants whether 214\nthey engaged in lateral reading and click restraint. Participant were said to have used 215\nlateral reading if they reported having searched for information outside the study page 216\nSeptember 27, 2021 6/31\n(yes/no question), and if they specifically searched on a search engine among other 217\ndestinations (multiple selection question). Participants were said to have used click 218\nrestraint if they further reported looking beyond the first results suggested by the 219\nsearch engine (multiple choice question). Critically, questions were formulated in such a 220\nway as to avoid any expectation as to which answer to select, and thus reduce the 221\ninfluence of the experimenter. 222\nControl measures. In addition to measures of accuracy and civic online reasoning, 223\nwe included a series of control measures for our analyses (S5 Methods). Other questions 224\nincluded self-report measures of confidence in the validity rating, plausibility of the post 225\ncontent, subjective relevance of obtaining accurate information about the post, 226\nfamiliarity with the source, perceived trustworthiness of the source, subjective 227\nknowledge of the topic, trust in scientists, conspiratorial beliefs, and a scientific literacy 228\ntest. In addition to responses in the questionnaire, we obtained information about 229\nparticipants from the recruiting platform, such as their level of education, 230\nsocio-economic status, social media use, and belief in climate change. 231\nAnalyses 232\nStatistical tests were conducted using base R [59]. We adopted the standard 5% 233\nsignificance level to test against the null hypotheses. All tests were two-tailed unless 234\notherwise specified. Post-hoc tests and multiple comparisons were corrected using the 235\nBenjamini-Hochberg procedure. Non-parametric statistics were log-transformed for 236\nconciseness. For probability differences, the lower boundary indicates the 2.5% quantile 237\nof the effect of the target variable starting from the 2.5% quantile of the baseline 238\nprobability estimate, whereas the upper boundary indicates the 97.5% quantile of the 239\neffect of the target variable starting from the 97.5% quantile of the baseline probability 240\nestimate. Given the small number of stimuli ( N < 10), we do not cluster errors by 241\nFacebook post in our regression analyses. The use of random effects yields however 242\ncomparable results in magnitude and statistical significance unless otherwise reported. 243\nResults 244\nParticipant randomisation was balanced across conditions (Chi squared test, 245\n\u001f2(2) =:016,p=:99). Median time to evaluate the Facebook post was 33 seconds in 246\nthe control condition (incentive condition: 45 seconds; pop-up condition: 35 seconds; 247\nminimum overall time: 2 seconds, maximum overall time: 40 minutes). In the pop-up 248\ncondition, participants spent an additional median time of 11 seconds on the pop-up. On 249\na scale from 1 to 6 (3.5 response at chance level), average accuracy score in the control 250\ncondition was 4.35 ( SD= 1:20; incentive condition 4.48, SD= 1:32; pop-up condition 251\n4.35,SD= 1:19). In the control condition, 78.2% of participants correctly guessed the 252\nscientific validity of the post (incentive condition: 80.1%; pop-up condition: 78.1%). 253\nEffect of interventions 254\nTo test the effect of our interventions on accuracy, we adopted two tests, one for the 255\naccuracy scores, and one for correct guessing (original preregistered analyses are 256\npresented in S1 Analyses). Since accuracy scores were clearly non-normally distributed 257\n(Shapiro-Wilk test, all p<: 001), we used an ordinal logistic regression in place of the 258\nlinear regression to test the effect of condition on accuracy scores. Results showed a 259\nsignificant effect of incentive ( \f=:293 [:092;:494],z= 3:225,p=:003) and a lack of 260\nsignificance for the pop-up ( \f=\u0000:009 [\u0000:207;:188],z=\u00000:103,p=:918). According 261\nto the model, the probability of giving a \"definitely valid\" (\"definitely invalid\") correct 262\nSeptember 27, 2021 7/31\nresponse increases by 4.4% [1.5%,8.2%] in the incentive condition compared to the 263\ncontrol condition. 264\nTechnique adoption 265\nTo compare the adoption of Civic Online Reasoning techniques between experimental 266\nconditions (pre-registered hypothesis 2) we used a logistic regression with technique use 267\n(adoption of both lateral reading and click restraint) as predicted variable and 268\nexperimental condition as predictor. Results revealed that both incentive and pop-up 269\nincreased technique adoption (Fig 3; incentive: \f= 1:042 [:527;1:556],z= 4:728, 270\np<: 001; pop-up: \f= 1:556 [1:065;2:046],z= 7:405,p<: 001), but that the increase 271\nwas markedly higher with the presence of the pop-up than with monetary incentives 272\n(\f=:514 [:157;:871],z= 3:362,p<: 001). 273\nArticle \nSearch engine \nWebsite \nFacebook \nWikipedia \nOther \n  \n  0%\n10%20%30%40%\n50%60%70%80%90%Article \nSearch engine \nWebsite \nFacebook \nWikipedia \nOther \n  \n  0%\n10%20%30%40%\n50%60%70%80%90%Article \nSearch engine \nWebsite \nFacebook \nWikipedia \nOther \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%controlpop-upincentive\nFig 3. Race chart of self-report external search behaviour. Bars indicate the proportion of\nparticipants in each experimental condition reporting to have searched in either category of\nwebsites. Lateral reading is identified with the proportion of participants searching information\non a search engine (light red), whereas click restraint is the subset of these participants who\nreported not stopping at the first algorithmically-ranked results of the search (dark red).\nSince our measure of technique use is based on self-reporting, responses might have 274\nbeen biased by external expectations. We therefore checked whether participants who 275\nreported the use of techniques actually left the study by tracking their behaviour on the 276\npost's web page. According to our measures, 80% of these participants left the study in 277\nthe control condition, compared to 87% in the pop-up and 90% in the incentive 278\nconditions. This result, if anything, suggests that our interventions did not increase the 279\nrate of false reporting. Moreover, even after accounting for false reports, results did not 280\ndiffer (incentive: \f= 1:156 [:594;1:719],z= 4:791,p<: 001; pop-up: \f= 1:626 281\n[1:087;2:166],z= 7:024,p<: 001; pop-up >incentive:\f=:467 [:095;:845],z= 2:920, 282\np=:004; see sections S2 Analyses and S6 Analyses for an in-depth exploration of 283\nparticipants' search behaviour). 284\nDid the use of lateral reading and click restraint actually improve post evaluation? 285\nAnd did the use of techniques mediate the effect of our interventions? To test our first 286\nquestion, we ran an ordinal logistic regression with accuracy score as predicted variable, 287\nand a standard logistic regression with correct guessing as predicted variable, both tests 288\nincluding adoption of techniques as the sole predictor. Results showed that accuracy 289\nscore improved significantly if a participant reported using Civic Online Reasoning 290\ntechniques ( \f=:526 [:274;:778],z= 4:090,p<: 001). According to the model, the use 291\nof Civic Online Reasoning Techniques increased the probability of giving a \"definitely 292\nvalid\" (\"definitely invalid\") correct response by 8.8% [4.0%,14.7%]. This result however 293\nwas not confirmed by the standard logistic regression on correct guessing, which instead 294\nSeptember 27, 2021 8/31\nfound no significant effect of technique adoption ( \f=:219 [\u0000:121;:580],z= 1:228, 295\np=:220). 296\nBased on these results, we proceeded to test whether pop-up and incentives had 297\nsome mediated impact on accuracy score through technique adoption. To test mediation 298\nwe used the R package MarginalMediation [60]. Technique adoption was found to 299\nmoderate the effect of both incentive and pop-up on accuracy score (incentive: 300\nunstandardised \f=:004 [:001;:006],z= 4:728,p<: 001; pop-up: unstandardised 301\n\f=:007 [:003;:012],z= 7:405,p<: 001), suggesting that both interventions affected 302\nindirectly accuracy scores. 303\nResponse times 304\nAs we expected monetary incentives to increase motivation, we tested whether response 305\ntimes (a common proxy for increased deliberation and attention) were affected by our 306\ninterventions. We compared participants' evaluation time of the post across conditions 307\nby way of a Kruskal-Wallis rank sum test. The test was significant ( \u001f2(2) = 67:63, 308\np<: 001), thus we conducted post hoc comparisons. All comparisons were significant, 309\nwith participants in the incentive condition taking significantly more time than control 310\n(log(V) = 8:02,p<: 001) and pop-up (log( V) = 5:54,p<: 001) participants, and 311\npop-up participants taking more time than control (log( V) = 2:41,p=:016). 312\nWe tested whether longer evaluation times predicted higher accuracy scores by 313\nmeans of an ordinal logistic regression with log-transformed evaluation time as predictor 314\nand accuracy score as predicted variable. Results revealed a significant and positive 315\nassociation ( \f=:182 [:095;:268],z= 4:12,p<: 001). The result was confirmed also for 316\ncorrect guessing (logistic regression, \f=:242 [:120;:366],z= 3:87,p<: 001). 317\nWe additionally looked at how much time participants spent outside the study page 318\nwhen they left without clicking any link (a proxy of lateral reading). The Kruskal-Wallis 319\ntest was again significant ( \u001f2(2) = 13:482,p=:001): of those participants who 320\nperformed such external searches, control participants spent less time outside the page 321\nthan participants in both the incentive (log( V) = 2:85,p=:006) and the pop-up 322\nconditions ( log(V) = 3:58,p=:001), whereas we found no significant difference between 323\nincentive and pop-up (log( V) =:92,p=:360). 324\nSource reputation 325\nCivic Online Reasoning techniques were originally designed for helping to evaluate 326\ncontent from seemingly legitimate but unknown websites [39]. We thus analysed 327\ndifferences in our interventions based on the recognisability and perceived 328\ntrustworthiness of the posts' sources. The importance of a source's perceived 329\ntrustworthiness was exemplified by two posts covering the same scientific article, one 330\nfrom BBC News (a source trusted by most participants), and another one from the 331\nDaily Mail (a source barely trusted by most participants). Despite the posts covered the 332\nsame content and presented similar wording, participants' evaluation of the two posts 333\ndiffered considerably: average accuracy score was 4.7 for the BBC piece ( SD= 1:05) 334\nand 4.05 for the Daily Mail piece ( SD= 1:08; ordinal regression: \f= 1:255 [:926;1:584], 335\nz= 7:470,p<: 001), and the proportion of correct guesses was 90.7% and 77.3%, 336\nrespectively (logistic regression: \f= 1:059 [:568;1:576],z= 4:132,p<: 001). 337\nPerhaps not surprisingly, we observed that, in the pop-up condition, adoption of 338\nlateral reading and click restraint was strongly linked with source type (Chi squared test 339\nwith technique adoption and source category as variables, \u001f2(2) = 15:407,p<: 001): 340\nwhen the source was trusted, only 6.7% of participants used these techniques, whereas 341\nthe proportion was 20% when the source was unknown. We then tested differences of 342\nthe interventions by source type in accuracy scores and correct guessing. 343\nSeptember 27, 2021 9/31\nLikelihood-ratio tests confirmed the importance of this variable for both analyses 344\n(p<: 001), however family-wise corrected contrasts revealed only one significant result, 345\nthe effect of incentive on accuracy scores for unknown sources ( \f=:558 [:114;1:001], 346\nz= 3:445,p=:005; Fig 4; see S4 Analyses for results about the uncorrected contrasts). 347\n*****\n*control\nincentivepop-up4.37\n4.624.53*\n*random responseincentivecontrol\npop-up\n4.34.16\n4.15control\nincentivepop-up4.69\n4.624.52unknown distrusted trusted\n3 4 5 6\naccuracy scoreSource reputation\nFig 4. Bootstrap estimates of the average accuracy score by experimental condition and\nsource reputation (Min. 1, Max. 6, random response: 3.5). Asterisks refer to significance of\ncontrasts in the ordinal logistic regression. Black: family-wise corrected contrasts; dark grey:\nuncorrected contrasts. *: p < : 05; **: p < : 01; ***: p < : 001.\nDiscussion 348\nResults from Experiment 1 suggest that paying participants to be accurate does 349\nincrease the accuracy score but not the proportion of participants correctly guessing the 350\nscientific validity of the posts. Compared to control, participants with an incentive gave 351\nmore extreme answers, reported engaging in Civic Online Reasoning techniques more 352\noften (and did leave the page more often), spent more time in searching information 353\noutside the study page, and took longer to evaluate the post (even compared to pop-up 354\nparticipants). These results support the idea that monetary incentives affect accuracy, 355\npossibly by increasing motivation and attention in the task, although this hypothesis 356\nwould need further testing. 357\nBy contrast, the presence of the pop-up seemed not to affect directly any indicator 358\nof accuracy. In spite of that, participants in the pop-up condition reported more lateral 359\nreading and click restraint, as well as the frequency of searches outside the study page. 360\nIn turn, this increment of Civic Online Reasoning techniques (up to +13.5% when 361\nsource is unknown) seems to mediate a small but significant increase in accuracy scores 362\n(marginal mediation analysis), suggesting an indirect effect of the pop-up. An effect of 363\npop-up is possibly seen in posts produced by unknown sources, where correct guessing 364\n(but not accuracy scores) is slightly higher in the pop-up condition than in control (S4 365\nAnalyses). 366\nSeptember 27, 2021 10/31\nThese results suggest that monetary incentives might have more consistent effects 367\nover the presentation of Civic Online Reasoning techniques. At the same time, we 368\nobserve considerable variability in participants' behavior depending on specific features 369\nof the posts. For instance, source reputation seems to have a remarkable effect on the 370\nadoption of Civic Online Reasoning techniques, which were (foreseeably) overlooked by 371\nalmost all participants when looking at posts from generally trusted sources. 372\nOne potential takeaway from these findings is that some initial biases might affect 373\nthe rate at which participants look for information outside the content provided (e.g. 374\nfamiliarity and opinion about the source), as well as in the way they look for such 375\ninformation. To explore this possibility, we designed a second experiment in which we 376\ntried to reduce the presence of initial biases by presenting posts from generally unknown 377\nsources. In addition, we included a fourth condition where we test the combination of 378\nmonetary incentives and Civic Online Reasoning techniques, to explore whether and 379\nhow the two interact. 380\nExperiment 2 381\nIn line with evidence in the literature, we expected an increased impact of our 382\ninterventions in a context where participants could rely on less prior information. We 383\nthus conducted a second experiment that was statistically powered to test for this 384\npossibility. In the Experiment 2 we replicated the format of the first one, with two main 385\nmodifications: 1) we ran a pre-screening survey to identify lesser-known sources of 386\ninformation and only used those sources as the basis for the Facebook posts the 387\nparticipants were asked to evaluate; 2) we added an experimental condition that 388\nincluded both incentive and pop-up interventions, to test the interaction between the 389\ntwo. we advanced the idea that the two intervention strategies might trigger distinct 390\nbehavioral outcomes (i.e., increased time spent on the task and use of Civic Online 391\nReasoning). If this is the case, then combining the two interventions should produce 392\neven stronger effects on accuracy. The original pre-registration of this experiment can 393\nbe retrieved from osf.io/w9vfb. 394\nMaterials and methods 395\nParticipants 396\n3004 U.K. residents were recruited through the online platform prolific.co on 24 May 397\n2021 (for a rationale of sample size, see S2 Methods). All participants gave their 398\ninformed consent for participating in the experiment. Average age was 36 ( SD= 13:2, 6 399\nnot specified), 63.1% of participants were female, (36.7% male, 0.2% other), and 59.4% 400\nhad a Bachelor's degree or higher. Per our pre-registered criteria, we excluded one 401\nparticipant who was not a resident in the United Kingdom. Analyses were thus 402\nconducted on 3003 participants. 403\nDesign 404\nThe second experiment was a replication of the first one, with the major difference that 405\nsources of the Facebook posts were unknown to most participants. In addition, we 406\nincluded a fourth condition where we gave participants a monetary incentive and also 407\nshowed them the pop-up with the Civic Online Reasoning techniques. Thus, the 408\nexperiment had a between-subjects design with 2 factors, pop-up (present, absent) and 409\nmonetary incentive (present, absent). Median completion time of the experiment was 5 410\nminutes. 411\nSeptember 27, 2021 11/31\nStimuli 412\nParticipants observed one out of 6 posts that vared in terms of: the scientific validity of 413\nthe content, i.e. the validity of the scientific statements in the title, subtitle, and 414\ncaption of the post; the topic (climate change, coronavirus pandemic, and health and 415\nnutrition); factual reporting of the source, based on ratings from 416\nmediabiasfactcheck.com (3 high/very high versus 3 low/very low). All posts came from 417\nsources relatively unknown to participants, as measured in a preliminary survey and 418\nconfirmed by participants' familiarity ratings. There were two distinct posts for each 419\ntopic, one from a source with high factual reporting displaying valid information, one 420\nfrom a source with low factual reporting displaying invalid information. 421\nSome titles, subtitles and captions of the posts included references to governmental 422\nor academic institutions. To prevent that these references could affect the evaluation of 423\nthe content, we slightly rephrased some sentences to remove this information. In 424\naddition, we corrected also grammatical mistakes in the text that could have given away 425\nthe reliability of the source. 426\nResults 427\nParticipant randomisation was balanced across conditions (Chi squared test, 428\n\u001f2(1) =:409,p=:52); average N per post, per condition was 125, minimum 106, 429\nmaximum 146. Median time to evaluate the Facebook post was 33 seconds in the 430\ncontrol condition, 48 seconds in the incentive condition, 34 seconds in the pop-up 431\ncondition, and 58 seconds in the incentive + pop-up (minimum overall time: 2.5 432\nseconds, maximum overall time: 22 minutes). When the pop-up was present, 433\nparticipants spent an additional median time of 11 seconds on the pop-up. On a scale 434\nfrom 1 to 6 (3.5 response at chance level), average accuracy score in the control 435\ncondition was 3.96 ( SD= 1:33; incentive condition: 4.20, SD= 1:41; pop-up condition: 436\n4.07,SD= 1:33; incentive + pop-up: 4.29, SD= 1:44; Fig 5). In the control condition, 437\n64.6% of participants correctly guessed the scientific validity of the post (incentive 438\ncondition: 71.2%; pop-up condition: 66.2%; incentive + pop-up: 72.9%). Overall 439\nperformance was generally lower than in Experiment 1, most likely due to the use of 440\nrelatively unknown news sources that forces participants not to rely on source 441\nknowledge to evaluate content. 442\nEffect of interventions 443\nTo test the individual and combined effects of pop-up tips and monetary incentives we 444\nconducted two tests, one for each accuracy index. For accuracy scores, We used two 445\nordinal logistic regression models, one with pop-up, monetary incentive as predictors, 446\nand another regression including the same variables and the interaction between pop-up 447\nand incentive as an additional predictor. For correct guessing, we compared two logistic 448\nregressions, one with correct guessing as dependent variable and pop-up, monetary 449\nincentive as predictors, and another regression including the same variables and the 450\ninteraction between pop-up and incentive as an additional predictor. For both the 451\nindices, we then adopted the model fitting data best according to a likelihood-ratio test. 452\nPerhaps surprisingly, model comparison favoured models without the interaction term 453\n(accuracy score: \u001f2(1) =:032,p=:858; correct guessing: \u001f2(1) =:007,p=:931); we 454\nthus tested the effect of incentives and pop-up assuming that they are (approximately) 455\northogonal. Results revealed a significant effect of incentive on both accuracy scores 456\n(\f=:350 [:194;:505],z= 5:371,p<: 001) and correct guessing ( \f=:313 [:124;:501], 457\nz= 3:954,p<: 001), and a significant effect of pop-up on accuracy scores ( \f=:137 458\nSeptember 27, 2021 12/31\n[\u0000:018;:292],z= 2:115,p=:034)1, but not on correct guessing ( \f=:076 [\u0000:018;:292], 459\nz= 0:966,p=:334). In addition, we found that the combination of the two 460\ninterventions significantly increased both accuracy indices compared to control 461\n(accuracy score: \f=:487 [:268;:705],z= 5:315,p<: 001; correct guessing: \f=:389 462\n[:123;:654],z= 3:496,p<: 001), and that the contribution of incentive was greater than 463\nthe contribution of pop-up (accuracy score: \f=:213 [\u0000:007;:432],z= 2:307,p=:028; 464\ncorrect guessing: \f=:2362 [\u0000:032;:504],z= 2:103,p=:047). According to the ordinal 465\nlogistic regression model, the combination of the two interventions led to a 10.4% 466\n[5.4%,14.2%] increase in correct guessing, and a 6.9% [2.8%,12.4%] increase in 467\n\"definitely\" correct responses compared to control. 468\n*\n**** ***random responsecontrol\npop-up\nincentive\npop-up +\nincentive4.073.97\n4.294.2\n3 4 5 6\naccuracy score\nFig 5. Bootstrap estimates of the average accuracy score by experimental condition (Min. 1,\nMax. 6, random response: 3.5). Asterisks refer to significance of contrasts in the ordinal\nlogistic regression. *: p < : 05; **: p < : 01; ***: p < : 001.\nTechnique adoption 469\nWe tested whether technique adoption was influenced by either interventions following a 470\nsimilar procedure to our test for correct guessing (comparison of two logistic regressions 471\nwith/without interaction). likelihood-ratio tests again favoured the model without 472\ninteraction ( \u001f2(1) =:245,p=:621). Model contrasts revealed several significant 473\ndifferences (Fig 6): both incentive ( \f=:725 [:471;:978],z= 6:829,p<: 001) and 474\npop-up (\f= 1:191 [:926;1:455],z= 10:736,p<: 001) increased significantly the use of 475\nCivic Online Reasoning techniques, but pop-up effect was significantly stronger than the 476\neffect of the incentive ( \f=:466 [:106;:826],z= 3:093,p=:002). In addition, the 477\ncombined effect of pop-up and incentive was also significant ( \f= 1:915 [1:542;2:288], 478\nz= 12:263,p<: 001), leading to an estimated 16.5% [8.6%,26.0%] increase in technique 479\nuse compared to control. 480\nTo test the robustness of these findings, we checked as in Experiment 1 the rate of 481\nfalse reporting (i.e., participants who said they used fact-checking techniques while they 482\ndid not even leave the study page). False reporting was 22.2% in the control condition, 483\n16% in the pop-up condition, 15.3% in the incentive condition, and 12.8% in the 484\ncondition with both interventions. The results did not differ after accounting for false 485\nreporting (pop-up: \f= 1:210 [:924;1:496],z= 10:094,p<: 001; incentive: \f=:761 486\n[:488;1:033],z= 6:669,p<: 001; pop-up>incentive: \f=:449 [:061;:838],z= 2:759, 487\np=:006; pop-up + incentive: \f= 1:971 [1:570;2:372],z= 11:729,p<: 001; see S8 488\nAnalyses for an exploration of participants' search behaviour). 489\n1Mixed-effects regression with errors clustered by post: p=:052.\nSeptember 27, 2021 13/31\nArticle \nSearch engine \nWebsite \nFacebook \nWikipedia \nOther \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%\nArticle \nSearch engine \nWebsite \nFacebook \nWikipedia \nOther \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%Article \nSearch engine \nWebsite \nFacebook \nWikipedia \nOther \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%\nArticle \nSearch engine \nWebsite \nFacebook \nWikipedia \nOther \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%incentivepop-up + incentivecontrolpop-upFig 6. Race chart of self-report external search behaviour. Bars indicate the proportion of\nparticipants in each experimental condition reporting to have searched in either category of\nwebsites. Lateral reading is identified with the proportion of participants searching information\non a search engine (light red), whereas click restraint is the subset of these participants who\nreported not stopping at the first algorithmically-ranked results of the search (dark red).\nTo test whether participants who adopted civic online reasoning techniques 490\nperformed better in the task we run two tests, one for each accuracy index. For 491\naccuracy scores, since accuracy scores were non-normally distributed (Shapiro-Wilk test, 492\nallp<: 001) we used a ordinal logistic regression model, with accuracy score as 493\ndependent variable and adoption of techniques as a dummy predictor variable. For 494\ncorrect guessing, we used a logistic regression, with correct guessing as dependent 495\nvariable and adoption of techniques as a dummy predictor variable. According to the 496\nmodels, participants adopting Civic Online Reasoning techniques were more accurate in 497\nterms of both accuracy score ( \f=:591 [:414;:767],z= 6:560,p<: 001) and correct 498\nguessing (\f=:506 [:281;:738],z= 4:345,p<: 001). According to the ordinal regression 499\nmodel, technique adoption increased the probability of giving a \"definitely valid\" 500\n(\"definitely invalid\") correct response increases by 9.5% [5.9%,13.7%]. 501\nWe also tested whether the use of Civic Online Reasoning techniques mediated the 502\neffect of the interventions with two marginal mediation analyses on accuracy score and 503\nSeptember 27, 2021 14/31\ncorrect guessing. Technique adoption was found to moderate the effect of both incentive 504\nand pop-up on accuracy score (incentive: unstandardised \f=:007 [:003;:010], 505\nz= 6:829,p<: 001; pop-up: unstandardised \f=:011 [:006;:015],z= 10:736,p<: 001) 506\nand correct guessing (incentive: unstandardised \f=:008 [:004;:013],z= 6:829, 507\np<: 001; pop-up: unstandardised \f=:014 [:007;:021],z= 10:736,p<: 001). 508\nResponse times. 509\nWe compared participants' evaluation time of the post across conditions using linear 510\nregressions with rank-transformed time as dependent variable and pop-up and 511\nincentives as predictors, with and without interaction. Again, model comparison 512\nfavoured the model without interaction ( F(1) = 1:104,p=:293). All contrasts were 513\nsignificant: both incentives ( \f= 370 [297;443],t(2928) = 12 :127,p<: 001) and pop-up 514\n(\f= 61 [\u000012;134],t(2928) = 2:011,p=:044) increased evaluation times, however 515\nincentives did so to a greater extent ( \f= 309 [205;413],t(2928) = 7:105,p<: 001). 516\nAlso, the combination of incentives and pop-up led to higher evaluation times than 517\ncontrol (\f= 431 [329;534],t(2928) = 10 :070,p<: 001). We tested whether longer 518\nevaluation times were associated with higher accuracy scores by means of an ordinal 519\nlogistic regression with log-transformed evaluation time as predictor and accuracy score 520\nas predicted variable. Results revealed a significant and positive association association 521\n(\f=:152 [:081;:223],z= 4:22,p<: 001). The result was confirmed also for correct 522\nguessing (logistic regression, \f=:204 [:117;:292],z= 4:56,p<: 001). We also 523\ncompared the duration of non-click external searches across conditions with the same 524\nprocedure as total evaluation times, again finding no interaction between interventions 525\n(F(1) = 0:1746,p=:676). Results showed a significant effect of incentive ( \f= 52 526\n[15;90],t(726) = 3:355,p=:001), pop-up ( \f= 80 [43;116],t(726) = 5:170,p<: 001), 527\nand their combination ( \f= 132 [80;184],t(726) = 6:100,p<: 001), but found no 528\nsignificant difference between the interventions ( \f= 27 [\u000026;80],t(726) = 1:217, 529\np=:224). 530\nDiscussion 531\nResults from Experiment 2 confirmed the effectiveness of monetary incentives on 532\naccuracy, and presented evidence in favour of the potential usefulness of fact-checking 533\ntips when the post's source is unknown. Monetary incentives increased both accuracy 534\nscores and correct guessing, the rate of (self-reported) Civic Online Reasoning 535\ntechniques, as well as the frequency and duration of non-link searches outside the study 536\npage. Participants offered with a monetary incentive spent more time evaluating the 537\npost than those who were not. Lastly, incentives seemed to increase the sharing 538\nintentions of valid information compared to control (S11 Analyses). 539\nContrary to the Experiment 1, the pop-up intervention seems to increase accuracy 540\nscores, but not correct guessing. We observed that the presence of the pop-up 541\ndramatically increased technique adoption (even compared to the presence of incentives) 542\nand the rate of non-link external searches, which in turn were linked to an increase in 543\nboth measures of accuracy. Marginal mediation analyses confirm an indirect effect of 544\npop-up on accuracy measures via an increase of search outside the post page. 545\nIn this experiment, we also tested the interaction between incentive and pop-up. 546\nModel comparison showed no interaction between the two interventions, suggesting that 547\npop-up and monetary incentives contributed separately to the increase in accuracy. We 548\nadditionally observe that monetary incentives increased participants' time spent on 549\nreading the pop-up: median time is 12.3 seconds with incentive compared to 9.6 when 550\nincentive is absent (S7 Analyses). Despite this increase in reading times, our statistical 551\ntests do not detect an increased pop-up effects by any other metric. 552\nSeptember 27, 2021 15/31\nGeneral Discussion 553\nIn this research, we studied whether presenting fact-checking tips and monetary 554\nincentives increases the correct evaluation of science-themed Facebook posts. In two 555\nexperiments, participants rated the scientific validity of the content of one out of several 556\nposts, with some participants receiving a monetary reward when they responded 557\ncorrectly and other participants being shown a pop-up window (superimposed on the 558\nFacebook post itself) that contained a list of fact-checking techniques proposed in the 559\nliterature (Civic Online Reasoning). Results showed that monetary incentives work as 560\nan accuracy booster. Moreover, data on search times and extremity of validity ratings 561\ncorroborated the hypothesis that incentives operate by increasing motivation and, 562\nsubsequently, attention on the content and other features of the post. This effect is 563\nparticularly remarkable given the strong benchmark against which it was compared: in 564\nfact, participants in the control condition were already primed for accuracy [30], and are 565\ntherefore likely to exert a greater degree of attention than when routinely browsing 566\nsocial media. The effectiveness of the pop-up as a way of introducing participants to 567\nfact-checking techniques received support in cases where the source of the post was 568\nrelatively unknown, i.e. when participants could rely on low prior information to 569\nevaluate posts. Furthermore, given that the presence of the pop-up significantly 570\nincreases the adoption of Civic Online Reasoning techniques, and that the use of these 571\ntechniques is, in turn, a strong predictor of participants' performance on the task, 572\nmarginal mediation analyses support the hypothesis that the pop-up may have an 573\nindirect positive effect on performance. 574\nOne of the original aims of this study was to establish whether incentives and 575\ntechniques could be compared in their effectiveness in improving the evaluation of 576\nscientific content, even when not directly accessible without technical expertise. In this 577\nrespect, our results suggest that the presence of the pop-up has less impact on 578\nsubsequent evaluation than monetary incentives. We suspect that the effectiveness of 579\nfact-checking advice may be hampered by several factors. A first explanation is that the 580\nadoption of the techniques might not have been effective enough to avoid the influence 581\nof previous beliefs about the content or of the search style. For example, if participants 582\nconsidered a content to be plausible in the first place, they might have selectively 583\nignored conflicting information even when it was clearly present in the search results 584\n(i.e. confirmation bias [61]); similarly, if a participant relied primarily on certain sources 585\nof information, consulting these sources might have steered the interpretation in the 586\nwrong direction. It is unclear however how such biases might have meaningfully reduced 587\nthe effectiveness of the pop-up but not of the monetary incentives. A second possibility 588\nis that participants failed to follow click restraint recommendations and did not search 589\ndeep enough to find relevant information and instead relied on unreliable sources 590\nfavoured by ranking algorithms. Lastly, the reduced impact of the pop-up may derive 591\nfrom its brevity: Civic Online Reasoning techniques have in fact been tested so far after 592\nbeing taught in extensive courses. It is therefore possible that simply presenting a 593\ncondensed set of tips on the best techniques is not enough to fully understand and 594\nmaster them. This possibility is in line with similar unsuccessful previous interventions 595\npresenting news literacy tips [35,55,62]. Thus, true ability to recognise pseudo-scientific 596\ninformation might only come from a minimal mastery of critical-thinking skills, which 597\ncannot be achieved by simply adding a snippet of information to a post, in the form of a 598\npop-up. Testing whether critical-thinking skills learned in the appropriate context can 599\nboost people's capacity to spot pseudo-scientific information is however problematic, 600\ndue to the subjective nature of critical thinking courses and their instructors in a virtual 601\nor physical classroom setting. 602\nDespite the asymmetric contribution of monetary incentives and fact-checking 603\ntechniques, our results also indicate that the interventions may work in a 604\nSeptember 27, 2021 16/31\ncomplementary way. In particular, Experiment 2 shows that these two interventions do 605\nnot appear to interact with each other. This result, which was replicated by testing 606\ndifferent variables of interest, suggests that the working mechanisms of the interventions 607\nare largely orthogonal, and thus can be combined to achieve an even stronger evaluation 608\nperformance by participants. 609\nOur results on incentives are in line with an attention-based account of information 610\nprocessing on social media; that is, increased deliberation is sufficient to decrease belief 611\nin false content [4]. Our results add to the literature of attention-based interventions by 612\nshowing how monetary incentives can additionally modulate motivation and attention 613\nand increase performance. 614\nThese promising results were not self-evident, as several experiments have cautioned 615\nagainst the universal effectiveness of monetary incentives as a behavioural driver. 616\n[63{65]. In fact, under some circumstances incentives decrease rather than increase 617\nmotivation [66]. One crucial aspect lies in incentives' calibration, as it has been proven 618\nthat if the effect of incentives on performance is non-monotonic and too small incentives 619\nare often counterproductive [66]. Moreover, when explicit incentives seek to modify 620\nbehaviour in areas such as education, environmental actions, and the formation of 621\nhealthy habits, a conflict arises between the direct extrinsic effect of incentives and how 622\nthese incentives may crowd out intrinsic motivations. Seeking accuracy in judging news 623\nis certainly driven by the intrinsic motivations of individuals. In all likelihood, however, 624\nthese intrinsic motivations do not conflict with monetary incentives. Seeking accuracy, 625\nunlike deliberately adopting ecological behaviour or going on a diet, is a largely 626\nautomatic process. 627\nAnother concern was that motivation and attention might not have been sufficient 628\nfor content that is hardly accessible to non-experts. The effectiveness of incentives is 629\nthen even more remarkable when considering that participants were asked to evaluate 630\ninformation based on scientific and technical reports, and thus had to rely external 631\nknowledge and intuition when claims and data were not immediately available. 632\nCompared to work on Civic Online Reasoning [39], our study finds correlational and 633\ncausal evidence supporting the importance of lateral reading and click restraint as 634\npredictors of accurate information, especially (as initially intended) when the 635\ninformation about the source is scarce. Notably, this is the first reported evidence of a 636\ngeneral population intervention in a social media context, extending the evidence for its 637\napplicability. We note however that the connection between our intervention (the 638\npop-up) and technique use is only indirect, as participants were free to ignore 639\nrecommendations. Stronger evidence for the efficacy of Civic Online Reasoning 640\ntechniques could come from within-subject studies that could limit selectively the use of 641\nthe techniques to assess their direct impact on users' behaviour. 642\nOur results also partly support literature on media and news literacy [34]. Previous 643\nsuccessful attempts at using fact-checking tips relied on presenting participants with 644\nsome of the Facebook guidelines for evaluating information [36,37]. Critically, these tips 645\nacted by reducing post engagement (liking, commenting, sharing) and perceived 646\naccuracy of headlines by hyper-partisan and fake-news sources. Given that our results 647\nhighlight the effectiveness of fact-checking tips when participants are less familiar with 648\nthe source, we suspect that the use of such tips is inversely associated to the knowledge 649\nand reputation of the source: that is, the more the source is well-known and widely 650\nrespected, the less participants will rely on guidelines and recommendations. This 651\ninterpretation goes against previous studies in the literature claiming that source 652\ninformation has little impact on the accuracy judgement of social media content [67 {69]. 653\nAlthough we did not directly test for the presence/absence of source information, we did 654\nfind that familiarity with and trust in a source largely affected the search style and 655\nevaluation of the content, suggesting that providing this information to participants had 656\nSeptember 27, 2021 17/31\na meaningful effect on their validity evaluations. One way to reconcile these apparently 657\nantithetical conclusions is by considering the relative capability of participants to assess 658\nthe plausibility of information: source knowledge can be a viable heuristic when 659\ninformation is harder to evaluate. Indeed, we suspect that in our experiment 660\ninformation about the source was often easier to assess than the plausibility of the 661\ncontent itself. In addition, compared to previous experiments, participants could open 662\nthe original article of the post to confirm that it had actually been produced by the 663\nsource and not fabricated, a factor that probably increased reliance on the source. 664\nThese considerations and our findings are not sufficient to ascertain whether and under 665\nwhat circumstances reliance on the source is beneficial or detrimental; however, we 666\nargue that source information is important in many situations [70,71]. 667\nOur study does not come without limitations. Possibly the most critical issue is the 668\nlimited number of stimuli that were used across experiments (15), which did not allow 669\nus to properly control for many features that could impact the evaluation of the posts. 670\nEven though we cannot exclude confounding variables and biases in the selection of 671\nstimuli, we tried as much as possible to follow a standardised procedure with pre-defined 672\ncriteria in order to exclude stimuli that could be considered problematic. Moreover, 673\neven though most of the literature and the present study have focused on standardised 674\nstimuli reporting content from news sources, we recognize that scientific (dis)information 675\ncomes in several formats that also depend on the topic, the audience, and the strategy 676\nof the creator. We decided to exclude other types of formats (e.g. videos or screenshots) 677\nto try to minimise the differences in experience between users, we think however that 678\nfuture research should explore more in depth the impact of varying media on the impact 679\nof disinformation spread and on possible counteracting interventions. Lastly, the study 680\nexplored the effectiveness of interventions when using a computer, as the very concept 681\nof lateral reading is based on browsing horizontally through internet tabs on a computer. 682\nAlthough nothing precludes the use of such techniques on other devices such as a mobile 683\nphone or tablet, the user interface is often not optimised to search for different contents 684\nat the same time, making their use more cumbersome. This is particularly problematic 685\nconsidering that social media are predominantly accessed through mobile devices. A 686\npromising direction in the fight to disinformation will be to study the influence of the 687\ndevice and UI in the ability of users to access high-quality information. Further studies 688\nshould also investigate how much easiness of accessing information from within a specific 689\napp could prompt users to fact-check what they see. For example, many apps allow to 690\ncheck information on the internet via an internal browser without leaving the app itself. 691\nConclusion 692\nThis study set out to assess the relative effectiveness of monetary incentives and 693\nfact-checking tips in recognising the scientific validity of social media content. We found 694\nstrong evidence that incentivising participants increases accuracy evaluations; we also 695\nfound evidence that fact-checking tips increase accuracy evaluation when the source of 696\nthe information is unknown. These results suggest a promising role of attention and 697\nsearch strategies, and open the way to the test of multiple approaches in synergy to 698\nachieve the most effective results. 699\nSupporting information 700\nS1 File. Facebook posts, Experiment 1. The spreadsheet including the list of 701\nFacebook posts for Experiment 1. 702\nSeptember 27, 2021 18/31\nS2 File. Facebook posts, Experiment 2. The spreadsheet including the list of 703\nFacebook posts for Experiment 2. 704\nS1 Methods. Sample size estimation, Experiment 1. Based on related 705\nfindings in the literature [36], we expected a small effect size (Cohen's d\u0019:15\u0000:20). 706\nAssuming no differences across the posts used as stimuli, and hence computing the 707\nsample size based on the main effect of a one-way ANOVA with three levels 708\n(experimental condition) yielded a minimum sample size of 1269 participants assuming 709\n\u000b= 5% and power (1 \u0000\f) = 90%. Aside from the main contrast, we expected also to 710\nanalyse the impact of secondary variables such as the topic of the post or 711\ntrustworthiness of the source. For this reason, we planned to recruit the maximum 712\nnumber of participants possible given our budget constraints. 713\nS2 Methods. Sample size estimation, Experiment 2. Our target sample size 714\nwas 3000 participants. We based our sample size estimation on the main effect of 715\npop-up on one of the two accuracy indices, correct guessing (analysis: logistic 716\nregression [72]). Estimate of this effect was based on the analyses of the first experiment 717\n(8% increase in correct guesses compared to control). To compute this effect size, we 718\nfiltered observations from the first experiment based on two criteria: the source of the 719\npost had to be unknown to most participants, and participants had to have completed 720\nthe task on a computer. Power 1 \u0000\fwas set to 95% and significance \u000bwas set to 5%. 721\nResults yielded a sample size of n= 733 per condition. We thus decided to recruit 750 722\nparticipants per condition, total N= 3000. 723\nWe further simulated achieved power for pre-registered hypotheses 3, 4 and 5, for 724\nboth accuracy indices (correct guessing and accuracy score). Simulations were based on 725\nN= 3000,\u000b= 5%, and effects sizes estimated from the first experiment.For correct 726\nguessing (test: logistic regression), achieved power is 96% for hypothesis 3 (pop-up main 727\neffect), and 88% for hypothesis 4 (incentive main effect). Combined effect of pop-up and 728\nincentive (hypothesis 5) depends on whether the two interventions interact. Therefore, 729\nwe simulate different scenarios exploring the effect of interaction on power. Results 730\nreveal that to achieve at least 95% power for this contrast, the interaction effect should 731\nnot be less than \u00004% (effect: change in the proportion of correct guesses). For accuracy 732\nscores (test: ordinal logistic regression), achieved power is 51% for hypothesis 3 (pop-up 733\nmain effect), and \u0019100% for hypothesis 4 (incentive main effect). Combined effect of 734\npop-up and incentive (hypothesis 5) depends on whether the two interventions interact. 735\nTherefore, we simulate different scenarios exploring the effect of interaction on power. 736\nResults reveal that to achieve at least 95% power for this contrast, the interaction effect 737\nshould not be less than \u00000.25 (effect: change in log odds). 738\nS3 Methods. Scoring of scientific validity. Sources of scientific information 739\nusually comply with standards approved by the community to guarantee that the 740\ninformation provided is obtained using rigorous methods and goes through several 741\nquality checks. In order for a content to be considered scientifically valid it had to 742\nsatisfy the following requirements: 743\n\u2022the original research could be found in a peer-reviewed publication; 744\n\u2022authors of the research had a track record certifying their expertise in their field 745\nof competence; 746\n\u2022research was not falsified by concomitant research in the field; 747\n\u2022there was no potential conflict of interest, or alternately the content had been 748\nindependently evaluated by a source with no conflicts of interest; 749\nSeptember 27, 2021 19/31\n\u2022the media article represented accurately data and claims of the original research. 750\nS4 Methods. Source familiarity and trustworthiness. Since we suspected that 751\nassessing familiarity and perceived trustworthiness of the source could be affected by 752\nthe observation of the Facebook post, we ran two separate surveys with independent 753\nraters to categorise and select the Facebook posts (first survey: N= 100, mean age 754\nM= 26:5,SD= 7:8, 2 not specified; 71 female, 1 not specified; second survey: 755\nN= 100, mean age M= 33:2,SD= 12:4; 68 female, 2 not specified). Raters were 756\nrecruited on the online platform prolific.co and had to assess the familiarity and 757\ntrustworthiness of several sources using a questionnaire taken from a previous study [20] 758\n(Fig 7). To categorise sources based on the raters' responses, we ran an expectation 759\nmaximisation model-based clustering algorithm using the McLust package in R [73]. 760\nResults revealed four clusters, one collecting known, trustworthy sources ( N= 4; e.g., 761\nNational Geographic and BBC), one known, untrustworthy sources ( N= 5; e.g., Daily 762\nMail and Daily Star), one unknown sources ( N= 21; e.g., Duluth News Tribune and 763\nthe American Enterprise Institute), and a last one including sources with mixed 764\nrecognition ( N= 7, e.g., the Washington Times and Live Science). 765\nS5 Methods. Post-rating questionnaire. After rating the post's scientific validity, 766\nthe participant completed a questionnaire. Below is the full list of questions asked: 767\n\u2022Confidence in rating: \"How confident are you in your response?\"; 6-point likert 768\nscale from (1) \"don't know\" to (6) \"absolutely certain\" 769\n\u2022Sharing intention (Experiment 2): Would you consider sharing this story online 770\n(e.g., through social networks or messaging apps)?; Yes/no 771\n\u2022Sharing behaviour (Experiment 2): Approximately how many news articles, 772\nmemes, opinion pieces, etc. have you shared in the last week?; numeric free-text 773\nresponse 774\n\u2022Source familiarity: \"Did you know [name of source] before the experiment?\"; 775\nYes/no 776\n\u2022Source trustworthiness: \"How much do you trust [name of source]?\"; 5-point likert 777\nscale from (1) \"not at all\" to (5) \"entirely\" 778\n\u2022Content plausibility (Experiment 1): \"How plausible do you find the content of the 779\npost?\"; 6-point likert scale from (1) \"totally implausible\" to (6) \"totally plausible\" 780\n\u2022Content plausibility (Experiment 2): \"Please respond as if you did not read the 781\nFacebook post: does it sound plausible to you that [statement based on the 782\npost]?\"; 6-point likert scale from (1) \"Totally implausible\" to (6) \"totally 783\nplausible\" 784\n\u2022External search: \"While you were evaluating the Facebook post, did you look for 785\ninformation outside the study page?\"; Yes/No 786\n\u2022if No: \"Why not?\"; randomised: It did not occur to me to do it; I had enough 787\ninformation already; I thought I would lose the experiment; I thought it was not 788\nallowed; I thought it was not possible; Other (free text entry). 789\n\u2022if Yes: \"Where did you look for information? (select all that apply)\"; randomised: 790\nThe article's web page; Wikipedia; Other web pages from the article's website; 791\nSearch engine (e.g., Google); Facebook. (lateral reading = search engine is 792\nselected) 793\nSeptember 27, 2021 20/31\n97%97% 97%95%97%95% 94%90%\n81%91%\n84%80%\n69%\n58%\n42%\n24% 23%\n16% 15%12%9% 8% 7% 7% 6% 5% 5% 5% 4% 4% 4% 4% 4% 3% 2% 2%0%NoYesDo you recognise the following websites?BBC News\nbbc.co.uk\nThe Guardian\ntheguardian.com\nThe Times\nthetimes.co.uk\nNational Geographic\nnationalgeographic.com\nDaily Mail\ndailymail.co.uk\nThe Sun\nthesun.co.uk\nDaily Mirror\nmirror.co.uk\nDaily Express\nexpress.co.uk\nDaily Star\ndailystar.co.uk\nThe New York Times\nnytimes.com\nThe Washington Post\nwashingtonpost.com\nLondon Evening Standard\nstandard.co.uk\nThe Washington Times\nwashingtontimes.com\nBusiness Insider\nbusinessinsider.com\nReuters\nreuters.com\nLive Science\nlivescience.com\nScienceDaily\nsciencedaily.com\nThe Conversation\ntheconversation.com\nIFLScience\niflscience.com\nFrance 24\nfrance24.com\nUnited Press International\nupi.com\nMedical Xpress\nmedicalxpress.com\nBioscience Resource Project\nindependentsciencenews.org\nThe Mercury News\nmercurynews.com\nAustralian National Review\naustraliannationalreview.com\nAgence France-Presse Fact Check\nfactcheck.afp.com\nFirstpost\nfirstpost.com\nThe Heartland Institute\nheartland.org\nAmerican Enterprise Institute\naei.org\nChildren's Health Defense\nchildrenshealthdefense.org\nCollective Evolution\ncollective-evolution.com\nDuluth News Tribune\nduluthnewstribune.com\nScience Vibe\nsciencevibe.com\nVaccineImpact\nvaccineimpact.com\nUKColumn\nukcolumn.org\nZME Science\nzmescience.com\nCO2 Coalition\nbiogenicco2.org\nNot at allBarelySomewhatA lotEntirely\ntrusted distrusted mixed unknown\nHow much do you trust each of these domains?Fig 7. Familiarity and trustworthiness ratings for a series of sources. Top: familiarity ratings, expressed as\nthe percentage of participants recognising the source. Bottom: trust ratings, with square size indicating the\nproportion of participants for each response and the segmented line indicating the average rating by source.\nColor represents categories derived from expectation maximisation model-based clustering.\n\u2022if Search engine is selected: \"While you were looking at the search browser results, 794\nwhat links did you open?\"; options: The first search results suggested.; The 795\nsubsequent search results.; Both the first and subsequent search results.; I did not 796\nopen any search results. (click restraint = either \"The subsequent search results.\" 797\nor \"Both the first and subsequent search results\" is selected) 798\n\u2022Subjective knowledge of the topic [74] (study 2): \"How much do you know about 799\n[topic]?\"; 6-point likert scale from (1) \"nothing at all\" to (6) \"a great deal\" 800\n\u2022Relevance of obtaining accurate information: \"We are considering compiling a 801\ncomprehensive summary of the scientific discussion behind the content of the post. 802\nIf so, would you be interested in receiving it by private message on your prolific 803\naccount?\"; Yes/No 804\n\u2022Trust in scientists: \"In general, how much do you trust scientists to do what is 805\nright?\"; 6-point likert scale from (1) \"not at all\" to (6) \"A lot\" (adapted from the 806\nSeptember 27, 2021 21/31\nEdelman Trust Barometer Yearly online survey) 807\n\u2022Conspiracy ideation trait [14]: 4, 5-point likert scales combined into a mean index 808\n\u2022Scientific literacy [74]: 15 true/false questions 809\nS6 Methods. Supplementary measures in Experiment 2. Measures of 810\nExperiment 2 were identical to those administered in Experiment 1, with three 811\nexceptions. 812\nScientific validity. In Experiment 1, all the scale points used to measure scientific 813\nvalidity were labelled with an adjective (e.g., 4 corresponded to \"possibly valid\"). We 814\nremoved intermediate labels and left only the ones for 1 and 6 (\"definitely 815\ninvalid/valid\"). We removed these labels to make sure that adjectives could not 816\ninfluence the evaluation in the conditions with incentives, where the participants were 817\nasked to give a response that matched the ratings of the experimenters. 818\nPlausibility. We changed one control measure, plausibility, to reflect more 819\nspecifically on the content of the post than on its general appearance. We thus singled 820\nout on claim from the post and asked participants if it sounded plausible, disregarding 821\nthe information they had gathered during the task . The content of a source should sound 822\nplausible to a participant if their background information is in agreement with the 823\ncontent itself, so measuring plausibility in this allows us to make inferences about a 824\nparticipant's background beliefs regarding the post they were given. 825\nSharing behaviour. As an additional exploratory measure we also asked 826\nparticipants' intention to share the post. This question is widely adopted in the 827\nliterature (see for instance [29]). We also asked participants to estimate their weekly 828\namount of sharing on social media, since this rate could affect the intention to share. 829\nS1 Analyses. Original pre-registered analyses (Experiment 1). We tested 830\ndifferences in accuracy scores using a linear probabilistic model with accuracy score as 831\npredicted variable, and experimental condition as predictor. Contrasts revealed a small 832\nbut significant impact of incentive on accuracy score ( \f=:0264 [\u0000:003;:055], 833\nt(2381) = 2:133,p=:04952), but not of the pop-up ( \f=\u0000:0005 [\u0000:0296;:0285], 834\nt(2381) = \u0000:042,p=:9667); we also found that accuracy scores were higher in the 835\nincentive condition than in the pop-up condition ( \f=:0269 [\u0000:002;:056], 836\nt(2381) = 2:177,p=:0495). To test correct guessing, we used a logistic3regression with 837\nthe guess of participant (i.e., \"valid\" or \"invalid\") as dependent variable and actual 838\nvalidity of the post content, experimental condition, and their interaction as predictors. 839\nNeither experimental condition nor its interaction with post validity yielded significant 840\nresults (all p>: 119), thus we could not reject the null hypothesis that there is no 841\ndifference in terms of correct guessing between conditions. 842\nWe additionally tested whether results differ when excluding participants who either 843\nfailed attention checks, encountered technical issues with the display of the Facebook 844\npost, or who did not close the pop-up (and therefore could not observe the post). Tests 845\nwere robust to all these exploratory exclusions. 846\nS2 Analyses. Differences in recorded search behaviour (Experiment 1). 847\nWe tracked participants' search behaviour on the post page as an additional proxy of 848\ntechnique use. Since the page did not include a link to a search engine, we tracked 849\n2Mixed-effects regression with errors clustered by post: p=:052.\n3Original preregistered analyses proposed the use of a probit regression instead of a logistic regression.\nThe two regressions yield the same results, but since we adopt ordinal logistic regressions for non-\nparametric analyses, we choose to report the results of the logistic regression for ease of comparison\nacross tests.\nSeptember 27, 2021 22/31\nwhether participants in each condition did leave the post page without clicking any link. 850\nResults confirm that more participants in the incentive and pop-up conditions left the 851\npage than participants in the control condition (Fig 8, light red bar; incentive: 852\n\f=:6754 [:3759;:9748],z= 5:282,p<: 001; pop-up: \f=:5226 [:2182;:8270],z= 4:021, 853\np<: 001), however the difference between the two interventions was not significant 854\n(\f=\u0000:1528 [\u0000:4258;:1203],z=\u00001:310,p=:190). 855\nArticle \nExternal \nInfo button \nFacebook \nWikipedia \nRegistration \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%Article \nExternal \nInfo button \nFacebook \nWikipedia \nRegistration \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%Article \nExternal \nInfo button \nFacebook \nWikipedia \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%controlpop-upincentive\nFig 8. Race chart of recorded search behaviour, Experiment 1. Bars indicate the proportion\nof participants in each experimental condition that clicked on one of the available links, on\nFacebook's info button, or left the page without clicking any links (named \"external\" searches).\nThe links available to participants led to the original article, the source's Facebook page, a site\ncontaining the source's domain registration, and a Wikipedia page where one existed.\nS3 Analyses. Response extremity and confidence (Experiment 1). We 856\ntested whether our itnerventions affected the extremity (i.e. 4 versus 5 versus 6) and 857\nconfidence of ratings. To measure extremity of responses we looked at the three levels of 858\nevaluation regardless of their correctness. We ran an ordered logistic regression with 859\nextremity of response as predicted variable and experimental condition as predictor. 860\nParticipants in the incentive condition gave more extreme responses than participants in 861\nthe control ( \f=:4425 [:2351;:6499],z= 5:000,p<: 001) and pop-up ( \f=:4847 862\n[:27709;:6924],z= 5:471,p<: 001) conditions, whereas we found no effect of pop-up 863\nover control ( \f=\u0000:0422 [\u0000:2480;:1636],z=\u0000:481,p=:692). We also compared 864\nconfidence ratings between conditions, but found no statistically significant difference 865\nbetween conditions (ordinal logistic regression, all p>: 05). 866\nS4 Analyses. Uncorrected contrasts for source reputation (Experiment 1). 867\nGiven the smaller power for the exploratory analysis on source reputation, we looked at 868\nuncorrected contrasts. These tests suggested that, for unknown sources, accuracy scores 869\nwere higher in the incentive condition than in the pop-up condition ( \f=:3655 870\n[:0843;:8153],z= 2:226,puncorr =:026), and that correct guessing was higher in the 871\npop-up condition than in the control condition ( \f=:45177 [ \u0000:1323;1:0358],z= 2:118, 872\npuncorr =:034); for generally distrusted sources, participants in the incentive condition 873\nhad higher accuracy scores than control ( \f=:2807 [\u0000:0905;:6519],z= 2:071,p=:038) 874\nand than pop-up participants ( \f=:2788 [\u0000:0845;:6420],z= 2:102,p=:036); lastly, for 875\ngenerally trusted sources, correct guessing was lower in the pop-up condition than in the 876\ncontrol condition ( \f=\u0000:8168 [\u00001:7956;:1621],z=\u00002:285,p=:022). This last 877\ncounter-intuitive result may suggest that providing Civic Online Reasoning techniques 878\nwhen the source is known might actually backfire. However, this interpretation should 879\nbe taken with caution, since all trusted sources in the experiment were in fact 880\npresenting valid information, and thus we cannot exclude the influence of post validity 881\n(see S5 Analyses). 882\nSeptember 27, 2021 23/31\nS5 Analyses. Effect of post type on accuracy (Experiment 1). Here we 883\ntested for any potential post differences in terms of scientific validity and scientific topic. 884\nWhen testing for differences across valid and invalid posts, likelihood-ratio tests 885\nconfirmed the importance of this variable for accuracy scores ( chi2(3) = 92:331, 886\np<: 001) but not for correct guessing ( chi2(3) = 5:479,p<: 140); we thus tested only 887\nfor differences in accuracy scores. Contrasts revealed a significant effect of incentives 888\nwhen posts contained valid information: accuracy scores were higher in the incentive 889\ncondition than in the control ( \f=:3582 [:07329;:6431],z= 3:268,p=:003) and pop-up 890\nconditions ( \f=:3713 [:0913;:6514],z= 3:447,p=:003). Uncorrected contrasts did not 891\nreveal any other significant result. A possible interpretation of these findings is that 892\nthere was a bias in the task favouring the interpretation of the posts' content as 893\nscientifically invalid, and that the increase in time and attention produced by the 894\nincentives mitigated this bias. We do not however have the data to confirm or 895\ndis-confirm this conclusion. We also note that posts from trusted sources were all 896\npresenting valid content, and this could play a potential confound. 897\nWe then tested for differences between posts by scientific topic. Scientific topic had 898\nto have a significant effect on both accuracy scores and correct guessing (likelihood-ratio 899\ntest, allp<: 001). Contrasts reveal a significant effect of incentive on accuracy scores 900\nfor posts about the COVID-19 pandemic (against control: \f=:4016 [\u0000:0426;:8459], 901\nz= 2:476,p=:040; against pop-up: \f=:4556 [:0176;:8936],z= 2:849,p=:020) and 902\nclimate change (against pop-up: \f=:4505 [:0175;:8835],z= 2:850,p=:020). 903\nUncorrected contrasts did not reveal any other significant result. 904\nS6 Analyses. Search behaviour and post evaluation (Experiment 1). As an 905\nexploratory analysis, we tested what type of behaviour on the post page predicted 906\nhigher accuracy scores and correct guessing in the task. We tracked whether 907\nparticipants clicked on the links on the post's web page (Facebook page; original article; 908\nFacebook's info button; who.is, a website tracking information about the source domain; 909\nsource's Wikipedia page, when existing), or if they left the page without clicking any 910\nlinks. We ran an ordinal logistic regression for accuracy score and a logistic regression 911\nfor correct guessing, with predictors a series dummy variables indicating whether the 912\nparticipant performed each behaviour or not. Results revealed that leaving the page 913\nwithout clicking any link was a significant predictor both for accuracy scores ( \f=:4500 914\n[:2649;:6352],z= 4:760,p<: 001) and correct guessing ( \f=:4273 [:1552;:7100], 915\nz= 3:022,p=:003). In addition, participants who opened the original article were 916\nmore likely to correctly guess the validity of the post ( \f=:4137 [:1600;:6754], 917\nz= 3:149,p=:002). 918\nAs a confirmatory test, we ran an expectation maximisation model-based clustering 919\nalgorithm to categorise participants based on their tracked behaviour on the page. 920\nSpecifically, we fed the algorithm with participants' total search time (either reading the 921\ninfo window related to the post or searching outside the page), and the proportion of 922\ntime for each activity. Cluster analyses revealed four clusters of behaviours, plus a fifth 923\ngroup including participants who never left the study page. Results reveal that, for both 924\naccuracy scores and correct guessing, two clusters of participants performed better than 925\nthose who did not leave the study page: those who predominantly searched without 926\nclicking links (accuracy score: \f=:5876 [:3932;:7820],z= 5:920,p<: 001; correct 927\nguessing:\f=:6338 [:3493;:9317],z= 4:272,p<: 001), and those who searched 928\npredominantly via the link to the article (accuracy score: \f=:3130 [:1203;:5056], 929\nz= 3:180,p=:003; correct guessing: \f=:4761 [:1969;:7671],z= 3:277,p=:002). We 930\nspeculate (also based on comments in the post-experimental questionnaire) that 931\nparticipants searching on the original article used this exploration to confirm whether 932\nthe post content was not fabricated, and thus rely more directly on their opinion of the 933\nSeptember 27, 2021 24/31\nsource; we do not have results confirming this hypothesis. 934\nS7 Analyses. Additional pre-registered analyses (Experiment 2). 935\nMain effect of pop-up on adoption of techniques. To test whether the 936\npresence of the pop-up increases the adoption of civic online reasoning techniques, we 937\nused a chi squared test comparing the proportion of participants reporting to adopt the 938\nfact checking techniques (lateral reading and click restraint, dichotomous variable) when 939\npop-up was present versus absent. Proportions were indeed significantly different 940\n(\u001f2(1) = 122:66,p<: 001), with 23.6% of participants adopting lateral reading and click 941\nrestraint when the pop-up was present compared to 8.7% when the pop-up was absent. 942\nEffect of incentive on pop-up reading times. To test whether the monetary 943\nincentive increases attention towards the pop-up, we used a t-test (or an equivalent 944\nnon-parametric alternative) to compare the reading times of the pop-up between 945\nparticipants who did or did not receive a monetary incentive. Given that reading times 946\n(and their log-transformation) were not normally distributed (Shapiro-Wilk test, all 947\np<: 025), we adopted a Wilcoxon rank-sum test. The test was significant 948\n(log(W) = 5:32,p<: 001), with median reading times being 2.1 [1.5,2.8] seconds longer 949\nwhen the incentive was present. 950\nS8 Analyses. Differences in recorded search behaviour (Experiment 2). 951\nWe checked how many participants in each condition left the post page without clicking 952\nany link, a proxy of technique use. Likelihood-ratio test again suggested no interaction 953\nbetween incentive and pop-up ( \u001f2(1) =:678,p=:410). Results confirmed the 954\nsignificant effect of both incentive ( \f=:5239 [:3159;:7320],z= 6:010,p<: 001) and 955\npop-up (\f=:3901 [:1841;:5961],z= 4:519,p<: 001), but did not find any significant 956\ndifference in strength between the two interventions ( \f=:1339 [\u0000:1579;:4256], 957\nz= 1:095,p=:274; Fig 9). 958\nS9 Analyses. Exclusion criteria (Experiment 2). We tested whether results 959\ndiffered when excluding participants who reported being familiar with the source, or 960\nwho were not regular Facebook users. Pre-registered results did not differ with one 961\nexception: when controlling for source familiarity, the contrast comparing the strength 962\nof intervention between incentive and pop-up was no more significant (accuracy score: 963\n\f=:1645 [\u0000:0625;:3916],z= 1:730,p=:084; correct guessing: \f=:2124 964\n[\u0000:0647;:4895],z= 1:830,p=:090). 965\nS10 Analyses. Response extremity and confidence (Experiment 2). We 966\nmeasured differences in extremity of responses and confidence ratings across conditions 967\nas in Experiment 1. Both analyses favoured the model without interaction 968\n(likelihood-ratio tests, all p>: 05). Contrasts for response extremity revealed that 969\nincentives increased the ratio of extreme answers ( \f=:4963 [:3328;:6597],z= 7:245, 970\np<: 001) whereas pop-up did not ( \f=:1191 [\u0000:0434;:2815],z= 1:749,p=:080). 971\nContrasts for confidence ratings revealed that only when incentive and pop-up were 972\ncombined confidence ratings were significantly higher than control ( \f=:2830 973\n[:0617;:5043],z= 3:053,p=:009). 974\nS11 Analyses. Sharing behaviour (Experiment 2). In Experiment 2, after the 975\nrating of the post, we asked participants about their willingness to share it. We tested 976\nthe effect of incentives and pop-up on sharing behaviour. We ran two logistic 977\nregressions, one with sharing intention as predicted variable, and incentive, pop-up, 978\nscientific validity, the interaction between incentive and scientific validity, and the 979\ninteraction between pop-up and scientific validity as predictors, and a second regression 980\nSeptember 27, 2021 25/31\nArticle \nExternal \nInfo button \nFacebook \nWikipedia \nRegistration \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%\nArticle \nExternal \nInfo button \nFacebook \nWikipedia \nRegistration \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%Article \nExternal \nInfo button \nFacebook \nWikipedia \nRegistration \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%\nArticle \nExternal \nInfo button \nFacebook \nWikipedia \nRegistration \n  \n  0%\n10%\n20%\n30%\n40%\n50%60%70%80%90%incentivepop-up + incentivecontrolpop-upFig 9. Race chart of recorded search behaviour, Experiment 2.\nidentical to the first one with the additional interaction between incentive and pop-up. 981\nBoth regressions included also a variable controlling for the self-report number of weekly 982\nshares of posts on social media. Given that one participant reported sharing an 983\nimplausibly large number of posts (50000; S6 Methods) we excluded this participant 984\nfrom this analysis. Comparison of the two models favoured the model without 985\ninteraction between the two interventions ( \u001f2(1) =:072,p=:788). Analyses revealed 986\nonly an increase in sharing intention when the post was valid and participants received 987\na monetary incentive ( \f=:7311 [:3856;1:0765],z= 5:392,p<: 001). One possible 988\ninterpretation of this increase is that the task (assessing the scientific validity of a post) 989\nincreases scepticism towards the content of the post, and that incentives counteract this 990\nscepticism by prompting people to investigate further. 991\nUncorrected contrasts also suggested that such increase was significantly stronger 992\nthan any potential increase due to the pop-up ( \f=:4821 [\u0000:0582;1:0224],z= 2:273, 993\npuncorr =:023). Moreover, pop-up appeared to slightly reduce the number of shared 994\nwhen the content was not valid, both compared to control ( \f=\u0000:3606 [\u0000:7862;:0650], 995\nz=\u00002:159,puncorr =:031) and to the effect of incentive ( \f=\u0000:5416 [\u00001:1933;:1100], 996\nz=\u00002:117,puncorr =:034). Whereas these results indicate an influence of our 997\ninterventions towards sharing behaviour, it is important to keep in mind that despite 998\nthe ecological validity of this task, participants in all conditions were asked to evaluate 999\nSeptember 27, 2021 26/31\nthe validity of the content which they were seeing, which could in turn influence any 1000\nsubsequent sharing intention [30,32,75]. 1001\nAcknowledgments 1002\nThis project has received funding from the European Union's Horizon 2020 research and 1003\ninnovation programme under grant agreement No 870883. The information and opinions 1004\nare those of the authors and do not necessarily reflect the opinion of the European 1005\nCommission. We would like to thank Torbj\u001crn Gundersen, Philipp Lorenz-Spreen, 1006\nDavid J. Gr\u007f uning, and the members of the Prosocial Design Network for their insightful 1007\ncomments and advice. 1008\nReferences\n1.Roose K, Isaac M, Frenkel S. Facebook Struggles to Balance Civility and Growth;\n2020. https://www.nytimes.com/2020/11/24/technology/\nfacebook-election-misinformation.html .\n2. AVAAZ. Facebook's Algorithm: A Major Threat to Public Health; 2020.\nhttps://secure.avaaz.org/campaign/en/facebook_threat_health/ .\n3. Eysenbach G, et al. How to fight an infodemic: the four pillars of infodemic\nmanagement. Journal of medical Internet research. 2020;22(6):e21820.\n4. Pennycook G, Rand DG. The psychology of fake news. Trends in cognitive\nsciences. 2021;.\n5. Lewandowsky S, Van Der Linden S. Countering misinformation and fake news\nthrough inoculation and prebunking. European Review of Social Psychology.\n2021; p. 1{38.\n6. Kozyreva A, Lewandowsky S, Hertwig R. Citizens versus the internet:\nConfronting digital challenges with cognitive tools. Psychological Science in the\nPublic Interest. 2020;21(3):103{156.\n7. Lorenz-Spreen P, Lewandowsky S, Sunstein CR, Hertwig R. How behavioural\nsciences can promote truth, autonomy and democratic discourse online. Nature\nhuman behaviour. 2020; p. 1{8.\n8.Lewandowsky S, Ecker UK, Cook J. Beyond misinformation: Understanding and\ncoping with the \\post-truth\" era. Journal of applied research in memory and\ncognition. 2017;6(4):353{369.\n9.Lewandowsky S, Ecker UK, Seifert CM, Schwarz N, Cook J. Misinformation and\nits correction: Continued influence and successful debiasing. Psychological science\nin the public interest. 2012;13(3):106{131.\n10.Lewandowsky S, Cook J, Ecker U, Albarrac\u0013 \u0010n D, Amazeen MA, Kendeou P, et al..\nThe Debunking Handbook 2020; 2020.\n11. Cook J, Lewandowsky S, Ecker UK. Neutralizing misinformation through\ninoculation: Exposing misleading argumentation techniques reduces their\ninfluence. PloS one. 2017;12(5):e0175799.\n12.Hertwig R, Gr\u007f une-Yanoff T. Nudging and boosting: Steering or empowering good\ndecisions. Perspectives on Psychological Science. 2017;12(6):973{986.\nSeptember 27, 2021 27/31\n13.Walter N, Brooks JJ, Saucier CJ, Suresh S. Evaluating the impact of attempts to\ncorrect health misinformation on social media: a meta-analysis. Health\nCommunication. 2020; p. 1{9.\n14. Bode L, Vraga EK. See something, say something: Correction of global health\nmisinformation on social media. Health communication. 2018;33(9):1131{1140.\n15. Bode L, Vraga EK. In related news, that was wrong: The correction of\nmisinformation through related stories functionality in social media. Journal of\nCommunication. 2015;65(4):619{638.\n16. Colliander J. \\This is fake news\": Investigating the role of conformity to other\nusers' views when commenting on and spreading disinformation in social media.\nComputers in Human Behavior. 2019;97:202{215.\n17. Brashier NM, Pennycook G, Berinsky AJ, Rand DG. Timing matters when\ncorrecting fake news. Proceedings of the National Academy of Sciences.\n2021;118(5).\n18.Allen J, Arechar AA, Pennycook G, Rand DG. Scaling up fact-checking using the\nwisdom of crowds. Science Advances. 2021;7:1{10.\n19.Allen J, Arechar AA, Rand DG, Pennycook G. Crowdsourced Fact-Checking: A\nScalable Way to Fight Misinformation on Social Media; 2020.\n20. Pennycook G, Rand DG. Fighting misinformation on social media using\ncrowdsourced judgments of news source quality. Proceedings of the National\nAcademy of Sciences. 2019;116(7):2521{2526.\n21. McGuire WJ. Inducing resistance to persuasion. Some Contemporary\nApproaches. In: Berkowitz L, editor. Advances in Experimental Social\nPsychology. vol. 1. Academic Press; 1964. p. 191{229. Available from: https:\n//www.sciencedirect.com/science/article/pii/S0065260108600520 .\n22. Roozenbeek J, Van Der Linden S. The fake news game: actively inoculating\nagainst the risk of misinformation. Journal of Risk Research. 2019;22(5):570{580.\n23. Roozenbeek J, van der Linden S, Nygren T. Prebunking interventions based on\n\\inoculation\" theory can reduce susceptibility to misinformation across cultures.\nHarvard Kennedy School Misinformation Review. 2020;1(2).\n24. Roozenbeek J, van der Linden S. Breaking Harmony Square: A game that\n\\inoculates\" against political misinformation. The Harvard Kennedy School\nMisinformation Review. 2020;.\n25.Cook J. Cranky Uncle Vs. Climate Change: How to Understand and Respond to\nClimate Science Deniers; 2020.\n26. Clayton K, Blair S, Busam JA, Forstner S, Glance J, Green G, et al. Real\nsolutions for fake news? Measuring the effectiveness of general warnings and\nfact-check tags in reducing belief in false stories on social media. Political\nBehavior. 2019; p. 1{23.\n27. Mena P. Cleaning up social media: The effect of warning labels on likelihood of\nsharing false news on Facebook. Policy & internet. 2020;12(2):165{183.\n28. Gaozhao D. Flagging Fake News on Social Media: An Experimental Study of\nMedia Consumers' Identification of Fake News. Available at SSRN 3669375.\n2020;.\nSeptember 27, 2021 28/31\n29. Pennycook G, Bear A, Collins ET, Rand DG. The implied truth effect:\nAttaching warnings to a subset of fake news headlines increases perceived\naccuracy of headlines without warnings. Management Science. 2020;.\n30. Pennycook G, Epstein Z, Mosleh M, Arechar AA, Eckles D, Rand DG. Shifting\nattention to accuracy can reduce misinformation online. Nature. 2021; p. 1{6.\n31. Pennycook G, Epstein Z, Mosleh M, Arechar AA, Eckles D, Rand D.\nUnderstanding and reducing the spread of misinformation online. Unpublished\nmanuscript: https://psyarxiv com/3n9u8. 2019;.\n32. Pennycook G, McPhetres J, Zhang Y, Lu JG, Rand DG. Fighting COVID-19\nmisinformation on social media: Experimental evidence for a scalable\naccuracy-nudge intervention. Psychological science. 2020;31(7):770{780.\n33.Roozenbeek J, Freeman AL, van der Linden S. How accurate are accuracy-nudge\ninterventions? A preregistered direct replication of Pennycook et al.(2020).\nPsychological science. 2021; p. 09567976211024535.\n34. Tully M, Maksl A, Ashley S, Vraga EK, Craft S. Defining and conceptualizing\nnews literacy. Journalism. 2021; p. 14648849211005888.\n35.Vraga EK, Bode L, Tully M. Creating news literacy messages to enhance expert\ncorrections of misinformation on Twitter. Communication Research. 2020; p.\n0093650219898094.\n36. Guess AM, Lerner M, Lyons B, Montgomery JM, Nyhan B, Reifler J, et al. A\ndigital media literacy intervention increases discernment between mainstream and\nfalse news in the United States and India. Proceedings of the National Academy\nof Sciences. 2020;117(27):15536{15545.\n37. Lutzke L, Drummond C, Slovic P, \u0013Arvai J. Priming critical thinking: Simple\ninterventions limit the influence of fake news about climate change on Facebook.\nGlobal Environmental Change. 2019;58:101964.\n38. Jones-Jang SM, Mortensen T, Liu J. Does media literacy help identification of\nfake news? Information literacy helps, but other literacies don't. American\nBehavioral Scientist. 2019; p. 0002764219869406.\n39. Wineburg S, McGrew S. Lateral reading: Reading less and learning more when\nevaluating digital information; 2017.\n40. Breakstone J, Smith M, Wineburg S, Rapaport A, Carle J, Garland M, et al.\nStudents' civic online reasoning: A national portrait. Educational Researcher.\n2019; p. 0013189X211017495.\n41. McGrew S, Ortega T, Breakstone J, Wineburg S. The Challenge That's Bigger\nthan Fake News: Civic Reasoning in a Social Media Environment. American\neducator. 2017;41(3):4.\n42. Del Vicario M, Bessi A, Zollo F, Petroni F, Scala A, Caldarelli G, et al. The\nspreading of misinformation online. Proceedings of the National Academy of\nSciences. 2016;113(3):554{559.\n43. Martini C. Ad Hominem Arguments, Rhetoric, and Science Communication.\nStudies in Logic, Grammar and Rhetoric. 2018;55(1).\nSeptember 27, 2021 29/31\n44. McGrew S, Breakstone J, Ortega T, Smith M, Wineburg S. Can students\nevaluate online sources? Learning from assessments of civic online reasoning.\nTheory & Research in Social Education. 2018;46(2):165{193.\n45.McGrew S, Smith M, Breakstone J, Ortega T, Wineburg S. Improving university\nstudents' web savvy: An intervention study. British Journal of Educational\nPsychology. 2019;89(3):485{500.\n46. McGrew S, Byrne VL. Who Is behind this? Preparing high school students to\nevaluate online content. Journal of Research on Technology in Education. 2020; p.\n1{19.\n47.Moore RC, Hancock JT. The Effects of Online Disinformation Detection Training\nfor Older Adults; 2020.\n48. Kaufmann N, Schulze T, Veit D. More than fun and money: Worker motivation\nin crowdsourcing-a study on Mechanical Turk. Working paper. 2011;.\n49.Gneezy U, Meier S, Rey-Biel P. When and why incentives (don't) work to modify\nbehavior. Journal of economic perspectives. 2011;25(4):191{210.\n50. Rickard JA, Russell AM. Interest in Advance and Other Up-front Incentives.\nGraduate School of Management, University of Melbourne; 1986.\n51.Pennycook G, Rand DG. Lazy, not biased: Susceptibility to partisan fake news is\nbetter explained by lack of reasoning than by motivated reasoning. Cognition.\n2019;188:39{50.\n52. Epstein Z, Berinsky AJ, Cole R, Gully A, Pennycook G, Rand DG. Developing\nan accuracy-prompt toolkit to reduce COVID-19 misinformation online. Harvard\nKennedy School Misinformation Review. 2021;.\n53. Jahanbakhsh F, Zhang AX, Berinsky AJ, Pennycook G, Rand DG, Karger DR.\nExploring lightweight interventions at posting time to reduce the sharing of\nmisinformation on social media. Proceedings of the ACM on Human-Computer\nInteraction. 2021;5(CSCW1):1{42.\n54. Crawford E. Introducing Tip Jar; 2021. https:\n//blog.twitter.com/en_us/topics/product/2021/introducing-tip-jar .\n55. Tully M, Vraga EK, Bode L. Designing and testing news literacy messages for\nsocial media. Mass Communication and Society. 2020;23(1):22{46.\n56. Vraga EK, Tully M. Media literacy messages and hostile media perceptions:\nProcessing of nonpartisan versus partisan political information. Mass\nCommunication and Society. 2015;18(4):422{448.\n57. Henninger F, Shevchenko Y, Mertens U, Kieslich PJ, Hilbig BE. lab. js: A free,\nopen, online study builder. PsyArXiv. 2019;.\n58. Taylor AB, West SG, Aiken LS. Loss of power in logistic, ordinal logistic, and\nprobit regression when an outcome variable is coarsely categorized. Educational\nand psychological measurement. 2006;66(2):228{239.\n59.R Core Team. R: A Language and Environment for Statistical Computing; 2018.\nAvailable from: https://www.R-project.org/ .\n60. Barrett TS. MarginalMediation: Marginal Mediation; 2019. Available from:\nhttps://CRAN.R-project.org/package=MarginalMediation .\nSeptember 27, 2021 30/31\n61. Nickerson RS. Confirmation bias: A ubiquitous phenomenon in many guises.\nReview of general psychology. 1998;2(2):175{220.\n62. Vraga E, Tully M, Bode L. Assessing the relative merits of news literacy and\ncorrections in responding to misinformation on Twitter. New Media & Society.\n2021; p. 1461444821998691.\n63.Frey BS, Oberholzer-Gee F. The cost of price incentives: An empirical analysis of\nmotivation crowding-out. The American economic review. 1997;87(4):746{755.\n64. Fryer Jr RG. Financial incentives and student achievement: Evidence from\nrandomized trials. The Quarterly Journal of Economics. 2011;126(4):1755{1798.\n65. Chao M. Demotivating incentives and motivation crowding out in charitable\ngiving. Proceedings of the National Academy of Sciences. 2017;114(28):7301{7306.\n66. Gneezy U, Rustichini A. Pay enough or don't pay at all. The Quarterly journal\nof economics. 2000;115(3):791{810.\n67. Dias N, Pennycook G, Rand DG. Emphasizing publishers does not effectively\nreduce susceptibility to misinformation on social media. Harvard Kennedy School\nMisinformation Review. 2020;1(1).\n68. Pennycook G, Rand DG. Who falls for fake news? The roles of bullshit\nreceptivity, overclaiming, familiarity, and analytic thinking. Journal of\npersonality. 2020;88(2):185{200.\n69. Tsang SJ. Motivated fake news perception: The impact of news sources and\npolicy support on audiences' assessment of news fakeness. Journalism & Mass\nCommunication Quarterly. 2020; p. 1077699020952129.\n70. Kim A, Moravec PL, Dennis AR. Combating fake news on social media with\nsource ratings: The effects of user and expert reputation ratings. Journal of\nManagement Information Systems. 2019;36(3):931{968.\n71.Nadarevic L, Reber R, Helmecke AJ, K\u007f ose D. Perceived truth of statements and\nsimulated social media postings: an experimental investigation of source\ncredibility, repeated exposure, and presentation format. Cognitive Research:\nPrinciples and Implications. 2020;5(1):1{16.\n72.Hsieh FY, Bloch DA, Larsen MD. A simple method of sample size calculation for\nlinear and logistic regression. Statistics in medicine. 1998;17(14):1623{1634.\n73. Fraley C, Raftery AE. MCLUST version 3: an R package for normal mixture\nmodeling and model-based clustering. Washington Univ. Seattle Dept. of\nSatistics; 2006.\n74. Fernbach PM, Light N, Scott SE, Inbar Y, Rozin P. Extreme opponents of\ngenetically modified foods know the least but think they know the most. Nature\nHuman Behaviour. 2019;3(3):251{256.\n75. Pennycook G, Binnendyk J, Newton C, Rand D. A practical guide to doing\nbehavioural research on fake news and misinformation; 2020.\nSeptember 27, 2021 31/31", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Advised or Paid Way to get it right. The contribution of fact-checking tips and monetary incentives to spotting scientific disinformation", "author": ["F Panizza", "P Ronzani", "S Mattavelli", "T Morisseau"], "pub_year": "2021", "venue": "Research \u2026", "abstract": "Disinformation about science can impose enormous economic and public health burdens.  Several types of interventions have been proposed to prevent the proliferation of false"}, "filled": false, "gsrank": 193, "pub_url": "https://iris.imtlucca.it/handle/20.500.11771/22014", "author_id": ["3Czlt7sAAAAJ", "WivYW0gAAAAJ", "Dg1tCoIAAAAJ", "KJvyFSgAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:GD3hCnYW8gkJ:scholar.google.com/&output=cite&scirp=192&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D190%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=GD3hCnYW8gkJ&ei=J7WsaLrrFfnSieoPxKLpgQ0&json=", "num_citations": 6, "citedby_url": "/scholar?cites=716659986949815576&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:GD3hCnYW8gkJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://iris.imtlucca.it/bitstream/20.500.11771/22014/2/Advised%20or%20Paid%20Way%20to%20Truth.pdf"}}, {"title": "Credibility assessment in the news: do we need to read", "year": "2018", "pdf_data": "Credibility Assessment in the News: Do we need to read?\nJames Fairbanks\nGeorgia Tech Research Institute\nAtlanta, GA\njames.fairbanks@gtri.gatech.eduNatalie Fitch\nGeorgia Tech Research Institute\nAtlanta, GA\nnatalie.fitch@gtri.gatech.edu\nNathan Knauf\nGeorgia Institute of Technology\nAtlanta, GA\nnate.knauf@gatech.eduErica Briscoe\nGeorgia Tech Research Institute\nAtlanta, GA\nerica.briscoe@gtri.gatech.edu\nABSTRACT\nWhile news media biases and propaganda are a persistent problem\nfor interpreting the true state of world affairs, increasing reliance\non the internet as a primary news source has enabled the formation\nof hyper-partisan echo chambers and an industry where outlets\nbenefit from purveying \u201cfake news\u201d. The presence of intentionally\nadversarial news sources challenges linguistic modeling of news\narticle text. While modeling text content of articles is sufficient to\nidentify bias, it is not capable of determining credibility. A structural\nmodel based on web links outperforms text models for fake news\ndetection. Analysis of text based methods for bias detection reveals\nthe existence of liberal words and conservative words, but there is\nno analogue for fake news words versus real news words.\nCCS CONCEPTS\n\u2022Machine Learning ;\u2022Natural Language Processing ;\nKEYWORDS\nMachine Learning, Natural Language Processing, Graphical Models,\nFake News Detection, Online Media, Social Media, Media Bias,\nPolitical Bias Detection, WWW, News articles\nACM Reference Format:\nJames Fairbanks, Natalie Fitch, Nathan Knauf, and Erica Briscoe. 2018. Cred-\nibility Assessment in the News: Do we need to read?. In Proceedings of\nWSDM workshop on Misinformation and Misbehavior Mining on the Web\n(MIS2). ACM, New York, NY, USA, 8 pages. https://doi.org/10.475/123_4\n1 INTRODUCTION\nThe adage that \u201ca lie gets halfway around the world before the truth\nhas a chance to get its pants on\u201d has never been more true than in\nthe age of online media, where information (and misinformation)\nspreads widely and quickly. Although fake news is not a new prob-\nlem, its recent reach is unprecedented and must be mitigated with\nnovel strategies to preserve valuable public and private institutions.\nOur recent work focuses on the problem of detecting fake news,\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nMIS2, 2018, Marina Del Rey, CA, USA\n\u00a92018 Copyright held by the owner/author(s).\nACM ISBN 123-4567-24-567/08/06.\nhttps://doi.org/10.475/123_4which we contrast to the problem of detecting mere biasin online\nmedia.\nWhile the term \u201cFake News\u201d may be contested and includes\nmany varieties, such as fabricated stories, clickbait, and negative\ncoverage, our focus is concerned with two distinct problems in\nthe study of problematic journalism. The first problem we denote\nas bias detection, which identifies the political bias of an article\nor publisher (conservative vs liberal). The second problem we de-\nnote as credibility assessment, which determines the truthfulness\n(fact-based reporting) of the article or publisher (credible vs not\ncredible). We frame these problems as binary classification tasks.\nWe use news articles and their metadata from the Global Database\nof Events Language and Tone (GDELT) to train, validate, and test\nour models [ 20]. For each task, we evaluate the performance of two\ndifferent classification methods, one based on article content and\nthe other based on structural properties. The content model serves\nas a baseline approach, using Natural Language Processing (NLP)\nmethods to construct textual features and supervised learning to\npredict an article\u2019s credibility and bias. The structural method is a\nprobabilistic graphical model approach, leveraging the link citation\nnetwork among articles and domains in order to make predictions\nusing a belief propagation algorithm.\nThis paper demonstrates the following: a) fake news articles can\nbe detected sans text using Belief Propagation on the link structure,\nb) while biased articles can be detected using text or links, only\nlinks can reveal the fake news articles and c) this biased article\ndetection model for online media focuses on specific keywords.\nThe following sections detail current research in automated fake\nnews detection, the GDELT dataset, our classification methodology,\nand conclusions.\n2 RELATED WORK\nFake news can be described as articles written in the style of a\nnewspaper that is false and written with the intent to deceive or\nmislead [ 1], but the form it takes may exhibit a large degree of\nvariability. A variety of forms compose the fake news genre, such\nas clickbait, (low quality journalism intended to attract advertising\nrevenue), news stories using digitally altered images or fabricated\nfacts, stories which erroneously describing a photo or video, mis-\npairing a photo with written content, reporting factually on only\none side of a story, counterfeit news sources or twitter accounts,\narticles that cite questionable sources, satire/irony, and conspiracy\nMIS2, 2018, Marina Del Rey, CA, USA Fairbanks, Fitch, Knauf, and Briscoe\ntheories, among other examples. Due to this variety and the high-\nprofile and ubiquitous nature of fake news, especially in politics,\nresearchers are studying methods to mitigate this problem.\n2.1 Political Bias Detection Methods\nAlthough problematic journalism adversely affects many different\nareas, one area that is particularly vulnerable is US politics. The\n2016 US presidential campaign provides salient examples of fake\nnews. Both social media and traditional news outlets perpetuated\nfake news stories during this time. Many survey respondents in one\nstudy admitting to believing false news stories [ 1]. Furthermore,\nAllicott and Gentzkow [ 1] show that fake news headlines from\nthe 2016 campaign were believed at similar rates as well-known\nconspiracy theories, such as \u201cPresident Obama was born in another\ncountry\u201d or the \u201cUS government knew the 9/11 attacks were coming\nbut consciously let them proceed.\u201d\nIt comes as no surprise then that there has been an explosion\nof academic research efforts to combat fake news by specifically\ntackling the task of political bias and propaganda identification.\nMost often political bias prediction methods are built around dis-\nsecting news article text alone, akin to how a human might detect\nbias when reading an article. Attempts to simulate this complex\nprocess of human reasoning usually utilize deep learning methods.\nConvolutional Neural Nets (CNN) and Recurrent Neural Networks\n(RNN) are popular frameworks to train models for bias detection\nin words or sentences as previously reported [10, 13, 22].\nNews articles can be classified by their providers using CNNs in\ncombination with a bidirectional RNN to detect which sentences in\nparticular are \"bias-heavy\" and are the most informative features\nfor the classification task [ 10]. Iyyer et al. points out how simplistic\nlinguistic models such as bag-of-words ignores sentence structure\nand phrasing with respect to political ideology detection. Instead,\nthey use RNNs to combine semantic and syntactic information\nat the sentence level and demonstrate how the composition of a\nsentence is a better predictor of its political ideology [ 13]. Rao et al.\nuse Long Short-Term Memory (LSTM) to classify U.S. political bias\nin twitter messages as supporting either Democratic or Republican\nviewpoints using word embeddings and neural networks [22].\nWord embedding methods are also an important component to\nany NLP type task where text must be transformed into features.\nBoth studies in [ 10] and [ 22] use various methods such as bag-of-\nwords, and pre-trained models such as \"bag-of-vectors\" using GloVe\nvectors [ 19]. We create a baseline content-based model using term\nfrequency inverse document frequency (TF-IDF) weighted matrix\nof singular words and bigrams and also use paragraph vectors for\ncomparison [16].\n2.2 Credibility Assessment Methods\nPrior to the rise of the internet, the only people with access to\nlarge audiences were authors working through editorial systems\nsuch as academic journals, book publishers, and newspaper editors.\nThese editorial systems control access to audiences and enforce\nethical norms such as honesty and objectivity within the academic,\nscholarly, and journalistic communities. The democratization of\nonline media enables anyone to set up a website and publish content,\nwith much of this content being published on social media. However,the desire to control advertising revenue and an influx of political\ncampaign funding has led to the proliferation of websites disguised\nas newspaper sites that are directed to push political agendas. This\nphenomenon is present on both the political left and right with\ndifferences in the issues covered.\nFormal fact checking processes are a modern invention. \u201cA Nexis\nsearch suggests that as recently as 2000 and 2001, no news outlet\nran a \u2018fact check\u2019 after the State of the Union address. In 2003 and\n2004, only the Associated Press fact-checked the annual speech to\nCongress. In 2010 and 2011, by contrast, the address drew dozens\nof fact-checking articles and segments\u201d [ 8]. Manual fact checking\nefforts appear to suffer from slow reaction times which allow false-\nhoods to spread further than truths, and accusations of bias that\nprevent readers from changing their minds in light of fact checker\u2019s\nevidence.\nMost research into online media credibility has been focused\non social media, where content is generated by any user without\nevaluation by knowledgeable gatekeepers or complying with ed-\nitorial standards. There are several logistical benefits of studying\nsocial media including the availability of the data from standard\n\"streams\" offered by the social media service, the homogeneity of\nthe data, and the structured metadata available on social media\nposts. General online media is a more heterogeneous environment\nwith larger engineering burdens on researchers.\nPrior to the advent of modern social media, scholars studied the\nformation of trust networks in the construction of semantic web\nresources, this research focused on trust networks in the authorship\nnetwork [ 7]. Like the traditional media gatekeepers, attention to\ntrust on the web focuses on identifying sources such as publishers,\neditors, and authors that are trustworthy and reputable sources,\nrather than verifying the accuracy of individual articles, stories, or\nfacts. NLP models for detecting rumors on Twitter are quite good\nusing the text of the tweet [21].\nTherefore, we like other researchers in this field view link analy-\nsis as an important tool for fact checking news reports and political\nstatements. Related research shows that it is possible to verify the\naccuracy of statements by politicians about facts in various domains\nsuch as history and geography [ 6] by using the Wikipedia knowl-\nedge graph and shortest path based \u201csemantic proximity\u201d metric.\nThe idea of reputation in linked media has also been explored with\nconnections to linear algebra [ 9], and probabilistic models [ 3]. The\nWeb of Trust is an internet browser extension that uses a reputa-\ntion scheme to protect internet users from cyber security threats.\nHowever, using link information requires link resolution as a pre-\nprocessing step. For this task we turn to Hoaxy for guidance. Hoaxy\nis a platform for tracking online misinformation, which uses the\nTwitter API to identify differences between posts that spread fake\nnews and those that combat it [ 23]. In this paper we adopt Hoaxy\u2019s\nmethod for defining canonical urls: \u201cTo convert the URLs to canoni-\ncal form, we perform the following steps: first, we transform all text\ninto lower case; then we remove the protocol schema (e.g. \u2018http://\u2019);\nthen we remove, if present, any prefix instance of the strings \u2018www. \u2019\nor \u2018m.\u2019; finally, we remove all URL query parameters.\u201d Like all URL\ncanonicalization procedures, this is a heuristic.1\n1One can find online services that route content using urls embedded in query pa-\nrameters of links for example the oembed standard, which means this heuristic can\nremove potentially useful information.\nCredibility Assessment in the News: Do we need to read? MIS2, 2018, Marina Del Rey, CA, USA\n3 DATASET DESCRIPTION\nThe dataset that we use for this study is a collection of articles\nfrom The Global Database of Events Language and Tone (GDELT)\nProject [ 20], whose mission is to monitor the world\u2019s broadcast,\nprint, and web news information in over 100 languages in order to\ncreate a free and open platform for computing. The GDELT Event\nDatabase contains over 431 million events from 1979 to present\nday and accounts for supranational, state, sub-state, and non-state\nactors. We augment the GDELT database, which only stores article\nmetadata, with the text and links from the article sources. A Post-\ngreSQL database stores metadata from new articles obtained from\nthe GDELT event stream every 15 minutes. The source url is then\naccessed and the downloaded content parsed using the Newspa-\nper library.2This semistructured information is stored in a Mongo\ndatabase for later retrieval and analysis [ 5]. In particular, the con-\ntent based approach queries for article text, while the structural\napproach queries for article links.\nSince we use supervised learning, labels were crawled from the\nwebsite Media Bias Fact Check (MBFC) [ 4]. MBFC is a volunteer-\nrun fact checking site rating websites based on political/idealogical\nbias and credibility of factual reporting. Ratings are subjective but\nare based on a structured rubric and numerical scoring system to\nassign labels. For each source (domain), a minimum of 10 headlines\nand 5 stories are reviewed against four categories:\na) Biased Wording/Headlines (Does the source\nuse loaded words to convey emotion to sway\nthe reader? Do headlines match the story?),\nb) Factual/Sourcing (Does the source report\nfacts and back up claims with well sourced evi-\ndence?), c) Story Choices (Does the source re-\nport news from both sides or do they only pub-\nlish one side?), and d) Political Affiliation (How\nstrongly does the source endorse a particular\npolitical ideology? In other words how extreme\nare their views?).\nMBFC computes the average score across the four categories and\nconverts it to five categorical labels for political bias (\u201cRight,\u201d \u201cRight\nCenter,\u201d \u201cCenter,\u201d \u201cLeft Center,\u201d and \u201cLeft\u201d). Conversely, only the\nnumerical score for the second category \u201cFactual/Sourcing\u201d is used\nto produce the five categorical labels for credibility (\u201cVery Low,\u201d\n\u201cLow,\u201d \u201cMixed,\u201d \u201cHigh,\u201d and \u201cVery High\u201d). Defining the credible\nlabels in this way acknowledges that credibility in this paper is\ndefined according to this single, objective metric while related\nmetrics such as \"fairness\" or \"impartiality\" are used to measure\nbias. Note, each of these labels are assigned at the publisher or\ndomain level and therefore every article originating from the same\nsource will have the same set of labels. This method is similar to\nhow people subscribe to (or ignore) entire publications rather than\nindividual articles or authors.\nSince our classifiers predict a binary label, we combine labels\n\u201cRight\u201d/\u201cRight Center\u201d and \u201cLeft\u201d/\u201cLeft Center\u201d to form our labels\n\u201cconservative\u201d and \u201cliberal,\u201d respectively, for the bias problem. Sim-\nilarly, we combine \u201cLow\u201d/\u201cVery Low\u201d and \u201cHigh\u201d/\u201cVery High\u201d to\nform our classifier labels denoted as \u201ccredible\u201d and \u201cnot credible,\u201d\nrespectively, for the credibility assessment problem . Although the\n2https://github.com/codelucas/newspaper/GDELT database is very large, only a fraction of articles could qual-\nify as useful in the content based model. The number of articles that\nhad both a set of labels scraped from MBFC and textual information\nwas 124,300 from 242 domains. Since the structural method does not\nrely on article text, articles with link data were collected to create a\ngraph with 19,786 nodes (domains) and 32,632 edges (links).3One\nbenefit of structural methods is the ability to learn from articles\nthat do not have annotated labels.\n4 METHODS\nTwo separate approaches are developed for bias detection and cred-\nibility assessment. The content model establishes baseline perfor-\nmance and uses more traditional text-based features and classifica-\ntion methods. The structural method leverages more sophisticated\ngraphical and statistical analysis in which we show improved per-\nformance over the content model.\n4.1 Modeling Article Text\nThe content model relies on traditional natural language process-\ning methods to extract textual features, then using classification\nmethods to establish baseline performance for our problem. All\ntext processing and model training, validation, and testing occur\nusing Python and various helpful packages such as scikit-learn [18],\npymongo [12], and spacy [11]. Textual information is aggregated to\nform a corpus and represented as a TF-IDF matrix, which is input\nfor the classifier.\nEach article is treated as an individual data point (i.e. a document).\nFor each article, text is cleaned via the following pipeline:\n(1) foreign character removal/replacement\n(2) contraction expansion (i.e. \u201ccan\u2019t\u201d becomes \u201ccan not\u201d)\n(3) punctuation removal\n(4) stop word removal\n(5) word lemmatization\nThe remaining words and bi-grams are the vocabulary in the TF-IDF\nmatrix representation of the corpus. For this task there are a total\nof 124,300 articles and about 7.5 million items in the vocabulary\nafter text cleaning. Then, these document TF-IDF vectors act as\nfeatures for two different classification models: Logistic Regression\nand Random Forest.\nWe also apply a deep learning doc2vec model to this problem [ 16]\nto contrast the TF-IDF approach. The specific model is the Wikipedia\nDistributed Bag of Words from https://github.com/jhlau/doc2vec. A\nDistributed Bag of Words is used in accordance with the literature\non empirical document vectors [ 15]. These pre-trained paragraph\nvectors are used as features in a Logistic Regression classifier.\nThe total dataset is split into 80% for the training set and 20%\nfor the holdout test set. During the training phase, 3-fold cross-\nvalidation was performed using the training set to construct the best\nclassifier. Then the final classifier was tested against the holdout test\nset. Finally, receiver operating characteristic (ROC) curves and area\nunder the curve (AUC) scores were computed to quantify classifier\nprediction performance for all models.\n3Prior to filtering out social media buttons and sites that only link to themselves there\nare 29,692 unique domains.\nMIS2, 2018, Marina Del Rey, CA, USA Fairbanks, Fitch, Knauf, and Briscoe\n4.2 Reputation in the Graph Structure\nUnlike print newspaper articles which have unstructured citations\nto other articles, online media features structured links in the form\nof HTML tags. The great advances in search engine technology,\nPagerank and HITS, was using link structure instead of textual\ncontent to determine web site importance [ 14,17]. We apply a\nsemi-supervised graphical learning algorithm called loopy belief\npropagation (BP) [ 3] to the information contained in the web struc-\nture of online media. Our implementation is written in the Julia\nProgramming Language [2].\nIn general, the BP approach treats each node as a random vari-\nable xi\u2208{0,1}where the output is a marginal probability p(xi)\nquantifying the belief that a node ibelongs to class xi. A node\u2019s\nbelief denoted b(xi)(or class label probability) is inferred from both\nthe prior knowledge of a node i\u2032sclass (\u201cconservative/liberal\u201d or\n\u201ccredible\u201d/\u201cnot credible\u201d) and also neighbors of node i,N(i). The BP\nalgorithm is iterative and intuitively works by passing messages\ndenoted by mij(xj), which is the message from node ito node j\nabout node j\u2032slikelihood of being in class xj. More formally, the\nmessage update Equation 1 is given below:\nmij(xj)\u2190\u00d5\nxi\u03f5X\u03d5(xi)\u03c8ij(xi,xj)\u00d6\nk\u03f5N(i)/jmki(xi) (1)\nThe function \u03d5(xi)represents the a priori belief that node ibe-\nlongs to class xi, and is used to encode the known labels for the train-\ning set. The posterior beliefs are calculated after the message prop-\nagation is complete. The function \u03c8ij(xi,xj)is a hyper-parameter\nthat determines the conditional probability that if a neighboring\nnode iis of class xi, then its neighbor jwill be of class xj. Table 1\nshows the choice of the affinity matrix \u03c8, for\u03f5>0this choice of \u03c8\nassumes homophily of the labels.\nTable 1: Edge Potentials between Neighboring Nodes\n\u03c8ij(xi,xj)xixj\nxi 1-\u03f5 \u03f5\nxj\u03f5 1-\u03f5\nFinally, the posterior node beliefs are computed from the final\nmessages according to the following Equation 2:\nbi(xi)=k\u03d5(xi)\u00d6\nxj\u03f5N(i)mji(xi) (2)\nThe total number of articles used for this task is larger than the con-\ntent based approach since there are many more articles that link to\nones that do not exist in the GDELT Event Database. Therefore, link\ninformation is captured for these articles but no text information.\nFor this task, there are articles from a total 19,786 domains with\n32,632 links of the types described in Table 2 to create the graph.\nThe graph G=(V,E)is undirected and unweighted where the\nset of domain names are the nodes Vand each link shared between\na source and destination domain corresponds to the set of edges\nE. After the graph is constructed, 3-fold cross-validation is used to\nevaluate prediction performance of the BP algorithm. Specifically, aTable 2: Link Types used in Graph Construction\nHTML Tag Description\n<a> Mutually linked sites (text content)\n<link> Shared CSS (visual style)\n<script> Shared JavaScript files (user interaction)\n<img> Common images, logos, or icons (visual content)\nFigure 1: A subset of the bias graph model color-coded to\nshow bias truth labels (blue=liberal, red=conservative, pur-\nple=centered) illustrating mainstream both left of center\n(huffingtonpost, yahoo news, ibtimes.co.uk) and right of cen-\nter news sources (wall street journal, marketwatch, business-\nwire).\nthird of the nodes\u2019 labels are withheld and assigned an a priori prob-\nability of 50% likely to be in either class, while the other two-thirds\nof the nodes are initialized to have 99% probability as belonging to\ntheir true class label. AUC scores are computed for each test fold\nand the final AUC score is the average across all three folds. An\nexample of the structural bias model is seen in Fig. 1. Nodes are\ncolor-coded according to the computed posterior beliefs (more blue\nfor liberal and more red for conservative) after the BP algorithm\nhas terminated.\n5 DISCUSSION\n5.1 Content vs Structure\nThe simple construction of the content model provides a perfor-\nmance baseline for both bias detection and credibility assessment\nof articles. For both the bias and credibility problems, Logistic Re-\ngression using TF-IDF matrix features out-performed both Random\nForest and Logistic Regression using pre-trained doc2vec features.\nFor the bias problem, the class label distribution is approximately\n60/40 for liberal/conservative articles, respectively. The best AUC\nscore from the content model is 0.926, which is achieved using Lo-\ngistic Regression (TF-IDF), as can be seen in Fig. 2. However, results\nfor the credibility problem in the content-based approach proved\nto be overly optimistic due to extreme class imbalance: 1,107 \u201cnot\ncredible\u201d to 99,969 \u201ccredible\u201d articles. This class imbalance led to a\nclassifier trained to almost always predict the majority class, which\nCredibility Assessment in the News: Do we need to read? MIS2, 2018, Marina Del Rey, CA, USA\nFigure 2: ROC Curves for Content Model Bias Detection\nleads to an inflated AUC score of 0.973. When sample weights were\nadjusted relative to the distribution of labels in each training set\nfold and when randomly under-sampling the holdout test set to\ninclude balanced counts of each label, the holdout test set AUC\nscore drops to 0.358\nOn the other hand, the structural method achieves improved\nperformance for the credibility problem over the content based\napproach despite the class imbalance. The ROC curves for each of\nthe 3 folds and average AUC score is shown in Figure 3 for bias\nand Figure 4 for credibility. The fact that credibility assessment is\nimproved in the structural approach validates our intuition that\ndetecting a source\u2019s credibility of factual reporting is more diffi-\ncult to do based on text alone, since \u201creal\u201d and \u201cfake\u201d news articles\nuse similar words and phrases to report their respective narratives.\nThe application of more meaningful linguistic features such as sen-\ntence structure and sentiment would improve automatic credibility\nassessment of news articles.\nTable 3: Summary of AUC Results\nTask/ModelAUC Content (LogReg) Structural\nbias 0.926 0.931\ncredibility 0.358 0.889\n5.2 Word Play: Informative Words in the Bias\nand Credibility Problems\n5.2.1 Liberal and Conservative Words. Since the content model\nperformed well in classifying bias, we assume that certain words in\nthis dataset may indicate a \"liberal\" vs \"conservative\" bias, which\nallows us to evaluate the most informative words according to\nthe magnitude of coefficients in the logistic regression model. A\nselection of the top 1% of informative conservative and liberal\nwords are featured in Table 4. After some scrutiny, these terms may\ndivulge more about the peculiarities and properties of the dataset\nFigure 3: Structure-Based Method: Bias Detection Perfor-\nmance\nFigure 4: Structure-Based Method: Credibility Assessment\nPerformance\nused rather than any objective truth about which words or phrases\nare good indicators of political bias.\nFirst, a large majority of the top terms (most not repeated here)\nare simply words that point to a specific domain or publisher. For\nexample, the conservative terms \u201csputnik,\u201d \u201ccaller,\u201d \u201c\"wa,\u201d and \u201ctlr\u201d\nrefer to \u201csputniknews.com, \u201d \u201cdailycaller.com,\u201d \u201cthewest.com.au, \u201d and\n\u201cthelibertarianrepublic.com,\u201d respectively, and all of which are la-\nbeled as \u201cRight\u201d or \u201cRight Center\u201d by Media Bias Fact Check. Simi-\nlarly, on the liberal side, the terms \u201cthomson\u201d/\u201creuters\u201d and \u201cdailys\u201d\nrefer to \u201cnews.trust.org\u201d (the news arm of the Thomson Reuters\nFoundation) and \u201celitedaily.com,\u201d respectively, and all of which are\nlabeled as \u201cLeft\u201d or \u201cLeft Center\u201d.\nSecond, many of the top terms can usually be explained by their\nproximity to their sources in that they are either buzzwords or\nrepresent broader topics heavily reviewed by them. For example,\nMIS2, 2018, Marina Del Rey, CA, USA Fairbanks, Fitch, Knauf, and Briscoe\nthe liberal words \"rs\" and \"crore\" are terms for Indian currency\n(where \"Rs\" is the symbol for the rupee and \"crore\" is a short-hand\nterm indicating a large amount of Indian currency). This makes\nsense when we consider that almost 25% of the articles queried are\nsourced from either \"timesofindia.indiatimes.com\" or \"economic-\ntimes.indiatimes.com\", which are liberal leaning sites. Similarly,\nthe conservative term \"perth\" is the capitol city of Western Aus-\ntralia that is mentioned frequently in the conservative-leaning\nsite \"thewest.com.au\". In other words, the resulting distribution\nof sources in which a top word appears is dominated by either\nliberal or conservative sources.\nThird, sometimes terms suggest how a publisher operates or the\nstyle of the publisher. For example, the term \"paywall\", which is\nwhen a site restricts access to certain content with a subscription,\nshows up frequently on one liberal-leaning site \"qz.com\". The liberal\nterms \"getty\" and \"image\" come together to suggest that a number of\nliberal (and probably conservative) leaning sites use the American\nphoto agency Getty Images, Inc to support their reports. On the\nconservative side, the term \"afp\" refers to the Agence France-Presse,\nwhich is the third largest global news agency and of which Getty\nImages, Inc is also a partner.\nFourth, fortunately there are also a couple of interesting terms\nthat do appear to make a statement connecting political bias to\nthe content of an article. For example, one of the top conservative\nwords is \"wire\" or \"wires\". There are a total of 337 articles that\nmention the conservative term \"wire\" or \"wires\", which notably, is\nonly a fraction of the 124,300 articles in the dataset. Therefore we\nwanted to know, why is this term a good predictor of bias? Nearly\nhalf of the articles containing the term \"wire(s)\" (142/337) originate\nfrom the conservatively labeled domain \"nypost.com\" because many\nof their articles contain the phrase \"post wires\". However, of the\nremaining 195 articles containing the term \"wires\" or \"wire\", in the\nliberal-leaning sources there is a 2:1 mention of the term within the\ncontext of \"Obama wire tapping Trump\" vs referring to a theme of\nterrorism to include buzzwords such as \"explosives\" or \"terrorists\"\nor \"bomb\". In other words, liberal sources including the term \"wire\"\nmore likely refers to the talking point concerning \"Obama wire\ntapping Trump\" than the topic of \"terrorism\". On the other hand,\nthe remaining conservative sources containing the term \"wire\" or\n\"wires\" usually refer to it within the context of \"terrorism\" but\nhad zero instances referring to it within the context of \"Obama\nwire tapping Trump\", at least in this dataset. This trend also pairs\nwell with the conservative term \"daesh\" (appearing frequently on\nsputniknews.com), which is a derogatory alternative to the term\n\"ISIS\" meant to delegitimize the terrorist group. One more example\nis the liberal term \"aug\". This term is simply the abbreviation of\nthe month of August. Besides August apparently being a news-\nworthy month, a couple of dates jumped out over and over again.\nOne is August 12th and refers to the recent politically-charged\nCharlottesville protest/riot and the other date is August 25th, which\nrefers to reports of over 400,000 Rohingya fleeing from insurgent\nattacks in Myanmar. It turns out that both conservative and liberal\nsites mention these events, however in this dataset there are 3,382\nliberal leaning articles to only 839 conservative leaning articles that\ntalk about them.Table 4: Most Informative Bias Words in Content-Based\nModel\nConservative Liberal\nsputnik advertisement\nafp aug\ndaesh rs, crore\ntlr getty, image\ncaller thomson, reuters\nwa, perth dailys\nwire paywall\n5.2.2 Credible vs Not Credible Words. The logistic regression\nmodel for credible vs non-credible articles produces interpretable\nlists of words that indicate whether an article is more or less likely\nto be credible. For full results and coefficients see Table 6 and Table 7.\nTable 5 contains selected words from these lists. Credible news arti-\ncles mostly contain words that are typical of newspaper style, such\nas \u201cphoto,\u201d \u201dimage,\u201d \u201dsupport,\u201d and \u201dcampaign\u201d. The non-credible\nword list contains more highly specialized nouns indicating that\nthey refer to specific conspiracy theories, rather than a general\nstyle of writing. For example \u201cwikileaks,\u201d \u201ddnc,\u201d and \u201dfbi\u201d refers\nto the specific conspiracy theories surrounding the 2016 US presi-\ndential race where the FBI investigated wikileaks publications of\nDemocratic National Committee emails. Also, the South American\ncriminal gang MS13 is a subject of right wing conspiracy theories.\nAnother category of words associated with non-credible articles are\nthe last names of specific public figures and places such as \u201cBeck,\u201d\n\u201dGirod,\u201d \u201dArroyo,\u201d \u201dBohlender,\u201d and \u201dGreece\u201d. This analysis shows\nthat one challenge to any content (text) based model of fake news\narticles will be the constantly changing landscape of conspiracy\ntheories and current events that reference specific people, places,\nor organizations.\n6 CONCLUSIONS\nStructural analysis of online media articles can identify fake news\narticles given a fraction of labeled samples. Textual analysis suc-\ncessfully identifies bias in online news articles, but is insufficient to\ndetermine credibility. For bias determination, the content model\u2019s\nmost informative terms reveal patterns and peculiarities in the\nunderlying dataset, but do not always reveal associations with con-\nservative and liberal bias. Relevant words for credibility assessment,\nespecially with respect to non-credible articles, appear to be highly\ntailored to the specific conspiracy theories found in the training set.\nWe believe this is a result of the adversarial writing process where\nfake news authors are trying to convince readers that the article\nif real. Future work should focus on developing more robust and\nnovel features that can generalize to works from unseen publishers\nand topics.\nFurther research should study the credibility problem from a\ngenerative process perspective, understanding how fake news au-\nthors write the articles with an intent to deceive the reader. Taking\nthe economic perspective of click streams and advertising revenue\nis critical to countering the propagation of fake news. We posit\nthat the best techniques for solving the fake news problem will\nCredibility Assessment in the News: Do we need to read? MIS2, 2018, Marina Del Rey, CA, USA\nTable 5: Selected words associated with credible vs non-\ncredible articles. Notice that the credible words are mostly\ngeneric journalistic words, while the noncredible words are\nhighly specific referring to a particular person, organization,\nor location.\nCredible Noncredible\nsaid follow\nphoto investwatchblog\nimage views\njuly antimedia\nwomen [daily] caller\nsupport revolutionizing\npodcast wikileaks\nindia greece\ncampaign christian\nowner arroyo\ncent beck\npicture graviola\npercent antifa\ncare dnc\ndid ms13\nimages wolves\npolitics fbi\napp girod\nindian bohlender\nlikely combine structural information from the web network with\nthe content information in the article text. One example of this\ncombined approach is to insert the predicted probability from the\ncontent model as the a priori\u03d5(xi)probability of the article node\u2019s\nclass label before running Belief Propagation.\nIn order to facilitate research into fake news, it is important to\ncapture the dynamic aspects of the rapidly changing propaganda\nnetworks. Fake articles are edited, challenged, posted at multiple\nsites, and taken down. These dynamics cannot be captured without\naccessing links multiple times and analyzing changes to content. A\ncollaborative network of researchers building a shared dataset will\nbe required to progress research in this field.\nA APPENDIX\nFor completeness we include tables of coefficients for credible vs\nnon credible words.\nACKNOWLEDGMENTS\nThe authors would like to acknowledge Joel Schlosser for his advice\nas well as the feedback from the anonymous reviewers.\nREFERENCES\n[1]Hunt Allcott and Matthew Gentzkow. 2017. Social Media and Fake News in\nthe 2016 Election. Journal of Economic Perspectives 31, 2 (May 2017), 211\u2013236.\nhttps://doi.org/10.1257/jep.31.2.211\n[2]J. Bezanson, A. Edelman, S. Karpinski, and V. Shah. 2017. Julia: A Fresh Approach\nto Numerical Computing. SIAM Rev. 59, 1 (Jan. 2017), 65\u201398. https://doi.org/10.\n1137/141000671\n[3]D. Chau, C. Nachenberg, J. Wilhelm, A. Wright, and C. Faloutsos. 2011. Polonium:\nTera-Scale Graph Mining and Inference for Malware Detection. In Proceedingsof the 2011 SIAM International Conference on Data Mining . Society for Industrial\nand Applied Mathematics, 131\u2013142. http://epubs.siam.org/doi/abs/10.1137/1.\n9781611972818.12 DOI: 10.1137/1.9781611972818.12.\n[4]Media Bias Fact Check. 2017. The Most Comprehensive Media Bias Resource.\n(2017). Retrieved October 31, 2017 from https://mediabiasfactcheck.com/\n[5]Kristina Chodorow and Michael Dirolf. 2010. MongoDB: The Definitive Guide (1st\ned.). O\u2019Reilly Media, Inc., Sebastopol, CA.\n[6]Giovanni Luca Ciampaglia, Prashant Shiralkar, Luis M. Rocha, Johan Bollen,\nFilippo Menczer, and Alessandro Flammini. 2015. Computational Fact Checking\nfrom Knowledge Networks. PLOS ONE 10, 6 (June 2015), e0128193. https://doi.\norg/10.1371/journal.pone.0128193\n[7]Jennifer Golbeck, Bijan Parsia, and James Hendler. 2003. Trust Networks on\nthe Semantic Web. In Cooperative Information Agents VII (Lecture Notes in Com-\nputer Science) . Springer, Berlin, Heidelberg, 238\u2013249. https://doi.org/10.1007/\n978-3-540-45217-1_18\n[8]Lucas Graves and Tom Glaisyer. 2012. The Fact-Checking Uni-\nverse in Spring 2012: An Overview . The New America Founda-\ntion, Washington, DC, USA. https://www.issuelab.org/resource/\nthe-fact-checking-universe-in-spring-2012-an-overview.html\n[9]R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. 2004. Propa-\ngation of Trust and Distrust. In Proceedings of the 13th International Confer-\nence on World Wide Web (WWW \u201904) . ACM, New York, NY, USA, 403\u2013412.\nhttps://doi.org/10.1145/988672.988727\n[10] Nicholas P Hirning, Andy Chen, and Shreya Shankar. 2017. Detecting and\nIdentifying Bias-Heavy Sentences in News Articles. (2017).\n[11] Matthew Honnibal and Mark Johnson. 2015. An Improved Non-monotonic Tran-\nsition System for Dependency Parsing. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing . Association for Computational\nLinguistics, Lisbon, Portugal, 1373\u20131378. https://aclweb.org/anthology/D/D15/\nD15-1162\n[12] David Hows, Peter Membrey, Eelco Plugge, and Tim Hawkins. 2013. Python\nand MongoDB . Apress, Berkeley, CA, 139\u2013169. https://doi.org/10.1007/\n978-1-4302-5822-3_7\n[13] Mohit Iyyer, Peter Enns, Jordan Boyd-graber, and Philip Resnik. 2014. Political\nideology detection using recursive neural networks. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational Linguistics . Association for\nComputational Linguistics, Baltimore, Maryland, USA, 1113\u20131122.\n[14] Jon M. Kleinberg. 1999. Authoritative Sources in a Hyperlinked Environment. J.\nACM 46, 5 (Sept. 1999), 604\u2013632. https://doi.org/10.1145/324133.324140\n[15] Jey Han Lau and Timothy Baldwin. 2016. An empirical evaluation of doc2vec\nwith practical insights into document embedding generation. In Proceedings of\nthe Workshop on Representation Learning for NLP , Vol. 1. Association for Compu-\ntational Linguistics, Berlin, Germany, 78\u201386.\n[16] Quoc Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and\nDocuments. In Proceedings of the 31st International Conference on International\nConference on Machine Learning - Volume 32 (ICML\u201914) . JMLR.org, II\u20131188\u2013II\u2013\n1196. http://dl.acm.org/citation.cfm?id=3044805.3045025\n[17] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The\nPageRank Citation Ranking: Bringing Order to the Web. In Stanford InfoLab .\nCiteseer, Stanford, Palo Alto, CA, 1\u201317.\n[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.\nBlondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-\nnapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine\nLearning in Python. Journal of Machine Learning Research 12 (2011), 2825\u20132830.\n[19] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe:\nGlobal Vectors for Word Representation. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP) . Association for Computational Linguistics, Doha,\nQatar, 1532\u20131543. http://www.aclweb.org/anthology/D14-1162\n[20] The GDELT Project. [n. d.]. Watching Our World Unfold. ([n. d.]). Retrieved\nOctober 31, 2017 from https://www.gdeltproject.org/\n[21] Vahed Qazvinian, Emily Rosengren, Dragomir R. Radev, and Qiaozhu Mei. 2011.\nRumor Has It: Identifying Misinformation in Microblogs. In Proceedings of the\nConference on Empirical Methods in Natural Language Processing (EMNLP \u201911) .\nAssociation for Computational Linguistics, Stroudsburg, PA, USA, 1589\u20131599.\nhttp://dl.acm.org/citation.cfm?id=2145432.2145602\n[22] Adithya Rao and Nemanja Spasojevic. 2016. Actionable and Political Text Classifi-\ncation using Word Embeddings and LSTM. In Proceedings of the Fifth International\nWorkshop on Issues of Sentiment Discovery and Opinion Mining, WISDOM 2016,\nSan Francisco, CA, USA, August 14, 2016 . ACM.\n[23] Chengcheng Shao, Giovanni Luca Ciampaglia, Alessandro Flammini, and Filippo\nMenczer. 2016. Hoaxy: A Platform for Tracking Online Misinformation. In\nProceedings of the 25th International Conference Companion on World Wide Web .\nInternational World Wide Web Conferences Steering Committee, 745\u2013750. http:\n//dl.acm.org/citation.cfm?id=2890098\nMIS2, 2018, Marina Del Rey, CA, USA Fairbanks, Fitch, Knauf, and Briscoe\nTable 6: Words that indicate an article is credible according\nto a logistic regression model.\nword coefficient value\ntlrs 5.32914771988\nsaid 4.39662937235\nphoto 4.22859888024\nimage 3.65477754753\njuly 3.12585411501\nwomen 2.91325593852\nsupport 2.82099051005\npodcast 2.77402098585\nindia 2.73290112857\ncampaign 2.67759733424\nowner 2.64117853155\ncent 2.63603955584\npicture 2.55722733958\nlike 2.54639855492\nclimate 2.54514709161\npercent 2.54333486858\ncare 2.53517993637\ndid 2.43262750028\nimages 2.41333556765\npolitics 2.40167721869\napp 2.39699744523\nindian 2.37924520364\nbusiness 2.3766925319\nnetwork 2.35517915282\nlatest 2.33009229501\ngetty 2.22811232574\ncanada 2.21911409169\nexpress 2.11779836847\nal 2.11420217184\nqatar 2.09988911117\ncoverage 2.09562171716\ncomments 2.08320274736\npardon 2.0746413354\npolitical 2.04672331226\nwork 2.0322289001\nediting 1.96870039433\nrs 1.96399859445\ntrade 1.95351708094\nputin 1.94553348346\nbuild 1.93696184507\nguam 1.90871135341\nadvertisement 1.89738511546\nreuters 1.87704689555\ndelhi 1.86179895342\nleadership 1.85136497359\ngiven 1.84560067318\njune 1.83083067076\ntweets 1.82907466155\nparty 1.82851962563\nstories 1.81999064765Table 7: Words that indicate an article is not credible accord-\ning to a logistic regression model.\nword coefficient value\nfollow -26.6313808837\niwb -18.7858211472\ninvestwatchblog -18.6102589482\nviews -12.4077059541\nread -9.61882848711\nantimedia -9.17618739645\ncaller -6.36197119866\ncaring -6.02746402192\nrevolutionizing -5.44279586416\nwikileaks -5.3874427841\ngreece -5.37036137789\nchristian -5.26378928804\nbible -5.18255221419\nfact -4.83635067973\narroyo -4.72986532666\nfacebook -4.69708655641\nalquds -4.64891841962\ncontent -4.58275487795\nabortion -4.4480731547\nlicensing -4.40700423334\narticle -4.32826287624\ntime -4.32189511592\nmiles -4.2941344646\nbeck -4.22598133714\ngraviola -4.22161030684\nfoundation -4.20427374104\npublisher -4.13008518306\nantifa -4.1085136531\nbarton -4.05063906078\ntypo -4.04743287728\ncreated -4.04199860908\ntwitter -4.03992320386\nprotected -3.98228806112\nsharing -3.93042439758\ndnc -3.91364202056\nms13 -3.87528857916\ndebt -3.8589909956\nreport -3.820507377\nstate -3.81578985852\nmoments -3.79004924637\ngospel -3.78399434845\nwolves -3.77616588737\nfbi -3.76882487816\npopulation -3.73133511986\ngirod -3.72891863502\nbohlender -3.66746513815\nherero -3.65703245664\nsmirnov -3.65245101356\njongun -3.63304263077\nfreedom -3.61125797726", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Credibility assessment in the news: do we need to read", "author": ["J Fairbanks", "N Fitch", "N Knauf"], "pub_year": "2018", "venue": "\u2026 in conjuction with 11th Int'l \u2026", "abstract": "While news media biases and propaganda are a persistent problem for interpreting the true  state of world affairs, increasing reliance on the internet as a primary news source has"}, "filled": false, "gsrank": 196, "pub_url": "http://jpfairbanks.net/doc/mis2news.pdf", "author_id": ["4-y38vkAAAAJ", "", ""], "url_scholarbib": "/scholar?hl=en&q=info:HICfM6vi8NgJ:scholar.google.com/&output=cite&scirp=195&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D190%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=HICfM6vi8NgJ&ei=J7WsaLrrFfnSieoPxKLpgQ0&json=", "num_citations": 59, "citedby_url": "/scholar?cites=15632243531536367644&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:HICfM6vi8NgJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "http://jpfairbanks.net/doc/mis2news.pdf"}}, {"title": "Animal agriculture is the missing piece in climate change media coverage", "year": "2023", "pdf_data": "WellBeing International WellBeing International \nWBI Studies Reposit ory WBI Studies Reposit ory \nSpring 5-2023 \nAnimal Agricultur e Is The Missing Piece In Climate Change Media Animal Agricultur e Is The Missing Piece In Climate Change Media \nCoverage Coverage \nCONI AR\u00c9V ALO \nFaunalytics \nJo Anderson \nFaunalytics \nFollow this and additional works at: https:/ /www .wellbeingintlstudiesr eposit ory.org/es_ag \n Part of the Agricultur al and Resour ce E conomics Commons , Animal Studies Commons , and the Mass \nCommunication Commons \nRecommended Citation Recommended Citation \nAR\u00c9V ALO, CONI and Anderson, Jo, \" Animal Agricultur e Is The Missing Piece In Climate Change Media \nCoverage\" (2023). Agricultur e. 1. \nhttps:/ /www .wellbeingintlstudiesr eposit ory.org/es_ag/1 \nThis material is br ought t o you for fr ee and open access \nby WellBeing International. It has been accepted for \ninclusion b y an authoriz ed administr ator of the WBI \nStudies Reposit ory. For mor e information, please contact \nwbisr-info@wellbeingintl.or g. \n\nA n i m a l\nA g r i c u l t u r e\nI s\nT h e\nM i s s i n g\nP i e c e\nI n\nC l i m a t e\nC h a n g e\nM e d i a\nC o v e r a g e\nMay\n2023\nAuthors:\nConstanza\nAr\u00e9valo\n(Faunalytics),\nJenny\nSplitter\n(Sentient\nMedia),\n&\nDr.\nJo\nAnderson\n(Faunalytics)\n\nC o n t e n t s\nB a c k g r o u n d\n4\nK e y\nF i n d i n g s\n5\nR e c o m m e n d a t i o n s\n6\nFor\nJournalists\n6\nFor\nAnimal\nAnd\nClimate\nAdvocates\n7\nA p p l y i n g\nT h e s e\nF i n d i n g s\n8\nB e h i n d\nT h e\nP r o j e c t\n8\nResearch\nTeam\n8\nAcknowledgements\n8\nResearch\nTerminology\n8\nResearch\nEthics\nStatement\n8\nM e t h o d\n9\nArticle\nCategorization\n9\nAccuracy\nChecks\n11\nAnalyses\n11\nR e s u l t s\n1 2\nOverall\nFrequency\nOf\nThemes\n12\nFrequency\nOf\nAnimal\nAgriculture\nSubsectors\n13\nAnimal\nAgriculture\nAlongside\nOther\nCauses\nOf\nClimate\nChange\n15\nCoverage\nBy\nMedia\nOutlet\n16\nPolitical\nLeaning\nAnd\nAnimal\nAgriculture\nMentions\n18\nThe\nAnimal\nAgriculture\nNarrative\nIn\nClimate\nArticles\n18\nMost\nClimate\nCoverage\nIgnores\nEmissions\nFrom\nAnimal\nAgriculture\n18\nExample\n1:\nCoverage\nOf\nEnergy\nTransition\n18\nExample\n2:\nCoverage\nOf\nClimate\nImpact\nOn\nWildlife\n18\nExample\n3:\nCoverage\nOf\nRegenerative\nAgriculture\n19\n2\n\nEven\nStories\nThat\nCover\nAnimal\nAgriculture\nFail\nTo\nReport\nOn\nEmissions\n20\nExample\n1:\nCoverage\nOf\nGeneral\nAgriculture\n20\nExample\n2:\nCoverage\nOf\nMeat\nReduction\nAs\nClimate\nAction\n21\nExample\n3:\nCoverage\nOf\nRegenerative\nAgriculture\nAs\nA\nSolution\nTo\nAnimal\nAg\n22\nSome\nAnimal\nAgriculture\nStories\nGot\nIt\nRight\n23\nC o n c l u s i o n s\n2 4\nNot\nEnough\nAttention\nIs\nGiven\nTo\nAnimal\nAgriculture\u2019s\nRole\nIn\nThe\nClimate\nCrisis\n24\nMisinformation\nAnd\nMissing\nInformation\nIn\nClimate\nCoverage\n25\nThe\nMedia\u2019s\nRole\nIn\nCommunicating\nClimate\nChange\nInformation\n26\nCaveats\n&\nLimitations\n26\nFuture\nDirections\n27\nS u p p l e m e n t a r y\nM a t e r i a l s\n2 8\nArticle\nExtraction\n28\nComputer-Assisted\nArticle\nReview\n28\nAccuracy\nChecks\n30\nMedia\nOutlet\nLeaning\nAnd\nAnimal\nAgriculture\n30\nDetailed\nCoverage\nBy\nMedia\nOutlet\n33\nChicago\nTribune\n33\nCNN\n34\nLos\nAngeles\nTimes\n35\nNew\nYork\nPost\n36\nReuters\n37\nStar\nTribune\n38\nThe\nBoston\nGlobe\n39\nThe\nNew\nYork\nTimes\n40\nThe\nWall\nStreet\nJournal\n41\nThe\nWashington\nPost\n42\n3\n\nB a c k g r o u n d\nFor\nmany\nyears\nnow,\nclimate\nresearchers\nhave\nbeen\nwarning\nthat\nthe\nworld\ncan\u2019t\nmeet\nits\nParis\nAgreement\nclimate\ngoals\nof\nlimiting\nglobal\nwarming\nto\n1.5\u00b0C\nwithout\nreducing\nmeat\nconsumption\n.\nMultiple\nstudies\nhave\naffirmed\nthat\nbetween\n11.1%\nand\n19.6%\nof\nglobal\nemissions\ncome\nfrom\nmeat\nand\ndairy\nproduction,\nand\nleading\nglobal\nfood\nand\nclimate\nagencies\nare\nalso\nin\nagreement,\nrecommending\nthat\npeople,\nparticularly\nthose\nin\nthe\nGlobal\nNorth,\nreduce\nmeat\nconsumption\nin\nfavor\nof\na\nplant-rich\ndiet.\nThe\neffects\nof\nanimal\nagriculture\non\nthe\nenvironment\nand\nclimate\nare\nvast\n:\nIt\nis\na\nleading\ncause\nof\ndeforestation\n,\nit\u2019s\nresponsible\nfor\nsignificant\nbiodiversity\nloss\nand\npollution\n,\nand\nemits\nlarge\namounts\nof\ngreenhouse\ngases,\nparticularly\nmethane\n.\nMethane\nalone\nis\nthe\ncause\nof\nover\n25%\nof\nglobal\nwarming,\nfor\nwhich\nreason\nreducing\nmethane\nemissions\nis\ncritical.\nIf\nemissions\ncontinue\nas\nthey\nare\nnow,\nthe\nfood\nsector\nalone\nis\nenough\nto\npush\nglobal\nwarming\npast\nthat\n1.5\u00b0C\nlimit\n,\nwhile\njust\nreducing\nmeat\nconsumption\ncould\nget\nthe\nworld\nmuch\ncloser\nto\nour\nemissions\ngoal.\nIn\nthe\nUnited\nStates\n,\nthis\nreduction\nwould\nmean\nthat\nthe\naverage\nperson\nwould\nconsume\nabout\n70%\nfewer\nanimal\nproducts\non\na\ndaily\nbasis,\nwith\nthe\ngreatest\nreductions\ncoming\nfrom\nred\nmeat\nand\nchicken\u201492%\nless\nred\nmeat\nand\n81%\nless\nchicken,\naccording\nto\nEAT-Lancet\nCommission\nrecommendations.\nDespite\nthe\nextensive\nresearch\nsupporting\nthe\nreduction\nof\nanimal\nproduct\nconsumption,\nthere\u2019s\nlong\nbeen\na\ndisconnect\nbetween\nwhat\nthe\nresearch\nshows\nand\nwhat\nthe\npublic\nunderstands.\nAccording\nto\na\nrecent\nconsumer\nstudy\nconducted\nby\nPurdue\nresearchers:\n\u201cThe\nbelief\nthat\n\u2018eating\nless\nmeat\nis\nbetter\nfor\nthe\nenvironment,\u2019\nwhich\nis\nstrongly\nsupported\nby\nmany\nclimate\nand\nenvironmental\nresearchers,\nis\nat\nan\nall-time\nlow\u201d\n(\nLusk\n&\nPolzin,\n2023\n).\nThe\nreason\nfor\nthis\ndisconnect\nis\nmultifaceted,\nbut\nat\nleast\none\nfactor\nis\nthe\ninformation\nthe\npublic\nreceives\nregarding\nthe\nconnection\nbetween\nanimal\nagriculture\nand\nclimate\nchange.\nGiven\nthe\nrole\nof\nthe\nmedia\nin\ninforming\nthe\npublic\nabout\nimportant\nissues\nlike\nclimate\nchange,\nthis\npartner\nproject\nbetween\nFaunalytics\nand\nSentient\nMedia\nsought\nto\nunderstand\nhow\nthe\nmedia\ncommunicates\nthe\nenvironmental\nimplications\nof\nanimal\nagriculture\nto\nreaders.\nBy\nanalyzing\nrecent\nclimate\narticles\nfrom\ntop\nU.S.\nmedia\noutlets,\nwe\ndrilled\ndown\non\nhow\noften\nthe\nmedia\nmakes\nthe\nconnection\nbetween\nanimal\nagriculture\nand\nclimate\nchange\nwhen\nreporting\non\nclimate\nissues,\nand\nhow\nreporting\non\nanimal\nagriculture\nin\nrelation\nto\nclimate\nchange\nmisses\nthe\nmark.\n4\n\nK e y\nF i n d i n g s\n1 .\nO n l y\n7 %\no f\nc l i m a t e\na r t i c l e s\nm e n t i o n e d\na n i m a l\na g r i c u l t u r e\na n d\nt h e y\nr a r e l y\nd i s c u s s e d\ni t s\ni m p a c t\no n\nc l i m a t e\nc h a n g e .\nAcross\nthe\n1,000\narticles\nwe\nexamined,\nonly\na\nhandful\nof\nstories\nreported\nin\ndepth\non\nthe\nconnection\nbetween\nconsuming\nanimal\nproducts\nand\nclimate\nchange.\nMost\narticles\nthat\nmentioned\nanimal\nagriculture\nfailed\nto\ndiscuss\nthe\nemissions\nand\nenvironmental\ndegradation\ncaused\nby\nthe\nindustry,\nlet\nalone\nthe\nimportance\nof\nreducing\nmeat\nconsumption\nor\nswitching\nto\na\nplant-based\ndiet\nto\nfight\nclimate\nchange.\nWhen\ndiets\nwere\ndiscussed,\nthe\neffectiveness\nof\nplant-based\ndiets\nwas\nsometimes\ndownplayed\nor,\nmore\noften\nthan\nnot,\npresented\nalmost\nas\nan\nafterthought\nrather\nthan\na\nlegitimate\nstrategy\nto\nmitigate\nclimate\nchange.\n2 .\nT h e\na n i m a l\na g r i c u l t u r e\ni n d u s t r y\ni s\no f t e n\np o r t r a y e d\na s\na\nv i c t i m\no f\nc l i m a t e\nc h a n g e\nr a t h e r\nt h a n\na\ns i g n i f i c a n t\nc a u s e .\nOur\nqualitative\nanalysis\nrevealed\nthat\ninstead\nof\nciting\nanimal\nagriculture\u2019s\nnegative\nenvironmental\nimpact,\nclimate\narticles\nthat\ndiscussed\nthe\nindustry\nin\nany\ndepth\ngenerally\nfocused\non\nhow\nclimate\nchange\nis\nimpacting\nanimal\nagriculture.\nMultiple\narticles\ndiscussed\nhow\nflooding,\ndrought,\nand\nheatwaves\nhave\ncaused\nlivestock\nlosses\nboth\nin\nthe\nU.S.\nand\nabroad,\nand\nhow\nthis\naffects\nthe\nlivelihoods\nof\nfarmers,\nwhile\nfailing\nto\nmention\nthe\nrole\nthat\nthe\nanimal\nagriculture\nindustry\nplays\nin\nthe\nclimate\ncrisis.\n3 .\nT h e r e\na r e\nc o u n t l e s s\nm i s s e d\no p p o r t u n i t i e s\nt o\nd i s c u s s\na n i m a l\na g r i c u l t u r e\ni n\nt h e\nc o n t e x t\no f\nc l i m a t e\nc h a n g e .\nEnergy,\ntransportation,\nemissions,\nand\nfossil\nfuels\nwere\ngiven\nthe\nspotlight\nin\nclimate\ncoverage:\nThese\ntopics\nwere\nmentioned\nin\nup\nto\n68%\nof\nclimate\narticles\nbut\nwere\nrarely\ntied\nto\nanimal\nagriculture,\ndespite\nthe\nconnections\nand\nparallels\nbetween\nthem.\nFor\ninstance,\ntransportation\nis\nresponsible\nfor\nroughly\nthe\nsame\namount\nof\nemissions\nas\nthe\nanimal\nagriculture\nindustry\nand\nis\npart\nof\nthat\nindustry,\nyet\njust\n8%\nof\nclimate\narticles\nmentioning\ntransportation\nalso\nreferenced\nanimal\nagriculture.\n4 .\nI m p a c t f u l\ns u b s e c t o r s\no f\na n i m a l\na g r i c u l t u r e\na r e\na l s o\nn o t\ng i v e n\ne n o u g h\na t t e n t i o n\nb y\nt h e\nm e d i a .\nCattle\nfarming\nis\nresponsible\nfor\nabout\n62%\nof\nanimal\nagriculture\nemissions\n(\nFAO,\n2022\n),\nyet\ncows\nwere\nmentioned\nin\njust\n30%\nof\nanimal\nagriculture\narticles.\nSimilarly,\nmethane\ncame\nup\nin\n22%\nof\nanimal\nagriculture\narticles\ndespite\naccounting\nfor\n54%\nof\nthe\nsector\u2019s\nemissions.\n5\n\nR e c o m m e n d a t i o n s\nF o r\nJ o u r n a l i s t s\n\u25cf\nL e a d\nw i t h\nt h e\nc o n s e n s u s\no f\ns c i e n t i f i c\ne v i d e n c e .\nWhether\nit\u2019s\nreporting\non\nwhat\ndrives\nfood\nsector\nemissions\nor\nby\nwhat\npercent\nfeed\nadditives\ncan\nreduce\nmethane,\nclimate\nreporting\nshould\nidentify\nareas\nof\nconsensus\nas\nwell\nas\npoints\nof\ndisagreement\nin\nthe\nfield.\nBecause\nthe\nfood-climate\nbeat\nis\na\nrather\nnarrow\none,\nthe\nmedia\nindustry\nneeds\nto\ncreate\nand\nfollow\nmore\naccurate\nreporting\nguides\nfor\nthis\ncoverage\narea.\nSentient\nMedia\nhas\ndeveloped\na\ntip\nguide\nhere:\nFood\nand\nFarming\nReporting\nGuide\n.\n\u25cf\nE v e r y\ns t o r y\ni s\na\nc l i m a t e\ns t o r y \u2014 l o o k\nf o r\no p p o r t u n i t i e s\nt o\ne x p a n d\nc o v e r a g e .\nWhether\nyour\nrole\nis\nreporting\non\nthe\nFarm\nBill\nor\nwriting\nnew\nrecipes,\nall\nfood\nand\nfarming\nstories\nare\nan\nopportunity\nto\ninform\nreaders\nabout\nhow\nwhat\nwe\neat\ndrives\nclimate\nchange.\nAt\nthe\nsame\ntime,\nclimate\njournalists\ncan\nadd\nreporting\non\nfood-related\nemissions\nto\ntheir\ncoverage,\nlike\nthe\nInflation\nReduction\nAct\nor\ndeforestation\nin\nthe\nAmazon.\n\u25cf\nS h o w\nr e a d e r s\nt h e\nc o n n e c t i o n\nb e t w e e n\nw h a t\nw e\ne a t\na n d\nc l i m a t e\ne m e r g e n c i e s .\nMany\nclimate\nstories\nare\ncovering\nclimate\nas\na\ncrisis\nby\nreporting\non\nemergencies\nlike\ndrought,\nflooding\nand\ndeforestation.\nBut\nthe\ndeforestation\nand\ndrought\nthat\nharms\nanimals\nis\nalso\ndriven\nby\nwhat\nwe\neat.\nStories\nabout\nharm\nto\nanimals\nor\ncosts\nto\nfarmers\nare\nalso\nan\nopportunity\nto\nmake\nthe\nconnection\nto\nour\nown\nbehavior\nand\nsteps\nthat\nwe\ncan\ntake\nto\nmake\nan\nimpact.\n\u25cf\nA v o i d\nf r a m i n g\nt h e\ni s s u e s\ni n\na\nw a y\nt h a t\np i t s\no n e\ns e c t o r\na g a i n s t\na n o t h e r .\nWhether\nwe\nshould\nworry\nmore\nabout\ncattle\nranches\nor\nair\ntravel\nis\na\ndistraction.\nClimate\nresearchers\nagree\nthat\nwe\nneed\nto\nbe\nreducing\nemissions\nfrom\nall\nsectors.\n\u25cf\nT r e a t\nf o o d\na n d\nf a r m i n g\nl i k e\na\ns c i e n c e\n.\nWhether\nyou\nare\nreporting\non\nhow\na\nfarm\nis\nimplementing\na\nclimate-smart\nagriculture\ngrant\nor\nlooking\ninto\nnew\nalternative\nproteins,\ncarefully\ncheck\nfacts\nand\nbe\nwary\nof\nresults\nthat\nseem\ntoo\ngood\nto\nbe\ntrue.\nThe\nbest\nways\nto\ndevelop\nscientific\nliteracy\nin\nthis\narea\nare\nto\nspeak\nwith\na\nwide\nrange\nof\nscientists\nin\ndifferent\ndisciplines,\nkeep\nup\nwith\nthe\nlatest\nresearch,\nand\ntake\nnote\nof\nfunding\ndisclosures.\n6\n\nF o r\nA n i m a l\nA n d\nC l i m a t e\nA d v o c a t e s\n\u25cf\nI n t e r a c t\nw i t h\nc l i m a t e\ns t o r i e s\nt h a t\ng e t\ni t\nr i g h t .\nShow\nnews\noutlets\nthe\nclimate\ntopics\nyou\ncare\nabout\nand\nwant\nto\nsee\nmore\nof\nby\nsharing\nthe\narticle\non\nsocial\nmedia,\nposting\na\npositive\ncomment,\nor\neven\nreaching\nout\nto\nthe\nnews\noutlet\nto\ntell\nthem.\nNews\noutlets\nwant\nyou\nto\nengage\nwith\ntheir\nstories\nand\nwill\npublish\nones\nthat\nare\nlikely\nto\nget\nmore\ntraction,\nso\nshow\nthem\nthat\nreporting\non\nanimal\nagriculture\nin\nclimate\nstories\nis\nof\ninterest\nto\nyou\nand\nother\nreaders.\n\u25cf\nS h a r e\ny o u r\nk n o w l e d g e\na n d\no p i n i o n s\nb y\nw r i t i n g\na\nl e t t e r\nt o\nt h e\ne d i t o r\no r\na n\no p - e d .\nIf\na\nnews\noutlet\nhas\nrecently\npublished\na\nclimate\npiece\nthat\nyou\nhave\nstrong\nfeelings\nabout\u2014in\nsupport\nor\ndisagreement\u2014writing\na\nletter\nto\nthe\neditor\nmay\nbe\nan\neffective\nway\nto\nconvey\nyour\nfeedback.\nIf\nyou\u2019re\na\nstrong\nwriter\nand\nhave\na\nlot\nto\nsay,\nyou\ncan\nsubmit\nan\nop-ed\n(opinion\neditorial\nor\narticle).\nThis\nis\na\ngreat\nopportunity\nto\n(politely)\ncorrect\nmisconceptions,\ncite\nbetter\nsources,\nand\ncall\nfor\naction\u2014encourage\nreaders\nto\nreduce\ntheir\nmeat\nconsumption\nor\nto\nfight\nfor\nmore\nfunding\nto\ngo\ntoward\nplant-based\nagriculture\nrather\nthan\nsubsidizing\nanimal\nagriculture.\n\u25cf\nT i p\no f f\nn e w s\no u t l e t s\na b o u t\ni n t e r e s t i n g\nd i s c u s s i o n s\nt h a t\na r e\nh a p p e n i n g\na r o u n d\nt h e\nr o l e\no f\na n i m a l\na g r i c u l t u r e\ni n\nc l i m a t e\nc h a n g e .\nIf\nan\ninteresting\nstudy\nabout\nanimal\nagriculture\u2019s\nimplications\nfor\nclimate\nchange\nis\ngetting\nsome\nbuzz\nin\nthe\nscientific\nor\nanimal\nadvocacy\ncommunities,\nyou\ncan\ntag\na\nnews\noutlet\non\nsocial\nmedia\nor\nreach\nout\nto\nsee\nif\nthey\u2019ll\npick\nup\nthe\nstory.\nLook\nfor\nan\nemail\nor\nphone\nnumber\nto\ncontact\nthem\nwith\ntips\n\u2014\nideally,\nreach\nout\nto\na\nreporter\nor\nthe\neditor\nof\ntheir\nclimate\nor\nscience\nsection\nif\nthey\nhave\none.\nMake\nsure\nto\nprepare\na\nconvincing\npitch,\nshowing\nthat\nyou\nhave\nnewsworthy\ninformation.\n\u25cf\nJ o i n\nf o r c e s\nw i t h\no t h e r\na d v o c a t e s\no r\no r g a n i z a t i o n s\nt o\ni n t e r a c t\nw i t h\nt h e\nn e w s\no n\ns o c i a l\nm e d i a .\nRemember,\nthere\nis\nstrength\nin\nnumbers.\nForm\na\ngroup\nof\nadvocates\nto\nlike\nand\ncomment\non\ngood\nstories\nor\nreach\nout\nto\nnews\noutlets\nto\ncover\nthe\nissue\nof\nanimal\nagriculture\nmore\nextensively\nin\nclimate\nstories.\nIf\nyou\u2019re\na\nclimate\nexpert,\nconsider\nwriting\nan\nop-ed\nwith\nother\nexperts\nin\nyour\nfield\nto\nmake\na\nbigger\nstatement.\nWith\nmore\npeople\nshowing\ninterest\nin\nhow\nanimal\nagriculture\nimpacts\nthe\nclimate,\nnews\noutlets\nmay\nbe\nmore\nlikely\nto\ninclude\nthis\nissue\nin\ntheir\nclimate\ncoverage.\n\u25cf\nU s e\ny o u r\no w n\np l a t f o r m\nt o\ne d u c a t e\nt h e\np u b l i c\no n\nt h e\na n i m a l\na g r i c u l t u r e\ni n d u s t r y ' s\nc o n t r i b u t i o n s\nt o\nc l i m a t e\nc h a n g e .\nAccording\nto\nFaunalytics\n(2023)\n,\nhaving\nclimate\nconcerns\ncan\nmake\na\nbig\ndifference\nin\npeople\u2019s\nopenness\nto\npro-animal\nactions,\nsuch\nas\nsupporting\nMeatless\nMondays.\nAs\na\nresult,\nif\nyou're\nan\nanimal\nadvocate,\nincorporate\nan\nenvironmental\nperspective\ninto\nyour\nadvocacy\nefforts.\nFor\nexample,\nuse\nyour\ndiet\ncampaign\nto\ntell\npeople\nhow\nreducing\ntheir\nmeat\nconsumption\ncan\nhave\na\npositive\nimpact\non\nthe\nclimate,\nor\nlet\npoliticians\nand\ndecision-makers\nknow\nthat\nanimal\nagriculture\nis\na\nleading\ncause\nof\ndeforestation\nand\nfood\nsector\nemissions\nwhen\nyou\nlobby\nfor\nfarmed\nanimals.\n7\n\nA p p l y i n g\nT h e s e\nF i n d i n g s\nWe\nunderstand\nthat\nreports\nlike\nthis\nhave\na\nlot\nof\ninformation\nto\nconsider\nand\nthat\nacting\non\nresearch\ncan\nbe\nchallenging.\nFaunalytics\nis\nhappy\nto\noffer\npro\nbono\nsupport\nto\nadvocates\nand\nnonprofit\norganizations\nwho\nwould\nlike\nguidance\napplying\nthese\nfindings\nto\ntheir\nown\nwork.\nPlease\nvisit\nour\nOffice\nHours\nor\ncontact\nus\nfor\nsupport.\nIf\nyou\u2019re\na\njournalist\nlooking\nto\ncover\nthe\nfood-climate\nintersection,\nremember\nto\ncheck\nout\nSentient\nMedia\u2019s\nFood\nand\nFarming\nReporting\nGuide\nfor\ntips.\nB e h i n d\nT h e\nP r o j e c t\nR e s e a r c h\nT e a m\nThe\nproject\u2019s\nlead\nauthors\nwere\nConstanza\nAr\u00e9valo\n(Faunalytics)\nfor\nthe\nquantitative\nanalyses\nand\nJenny\nSplitter\n(Sentient\nMedia)\nfor\nthe\nqualitative\nfindings.\nDr.\nJo\nAnderson\n(Faunalytics)\nreviewed\nand\noversaw\nthe\nwork.\nA c k n o w l e d g e m e n t s\nWe\nwould\nlike\nto\nthank\nAna\nBradley,\nExecutive\nDirector\nof\nSentient\nMedia,\nwho\nprovided\nvaluable\ninput\nabout\nthis\nresearch\nthroughout\nthe\nprocess.\nWe\nwould\nalso\nlike\nto\nthank\nan\nanonymous\nfunder\nfor\ntheir\ngenerous\nsupport\nof\nthis\nresearch.\nFinally,\nwe\nare\nvery\ngrateful\nto\nall\nthe\nclimate\njournalists\nwho\nare\nalready\nputting\nin\nthe\neffort\nto\nreport\non\nanimal\nagriculture\u2019s\nimpact\non\nclimate\nchange\nand\nthe\nways\nwe\ncan\nall\nhelp\nmitigate\nthis\ncritical\nissue.\nR e s e a r c h\nT e r m i n o l o g y\nAt\nFaunalytics,\nwe\nstrive\nto\nmake\nresearch\naccessible\nto\neveryone.\nWe\navoid\njargon\nand\ntechnical\nterminology\nas\nmuch\nas\npossible\nin\nour\nreports.\nIf\nyou\ndo\nencounter\nan\nunfamiliar\nterm\nor\nphrase,\ncheck\nout\nthe\nFaunalytics\nGlossary\nfor\nuser-friendly\ndefinitions\nand\nexamples.\nR e s e a r c h\nE t h i c s\nS t a t e m e n t\nAs\nwith\nall\nof\nFaunalytics\u2019\noriginal\nresearch,\nthis\nstudy\nwas\nconducted\naccording\nto\nthe\nstandards\noutlined\nin\nour\nResearch\nEthics\nand\nData\nHandling\nPolicy\n.\n8\n\nM e t h o d\nThis\nstudy\nused\nrecent\nclimate\narticles\nfrom\ntop\nU.S.\nmedia\noutlets\nto\nanalyze\nhow\noften\nand\nhow\nwell\nthe\nmedia\nmakes\nthe\nconnection\nbetween\nanimal\nagriculture\nand\nclimate\nchange\nwhen\nreporting\non\nclimate\nissues.\nTo\ndo\nso,\nwe\nobtained\nthe\n100\nmost\nrecent\npublications\nthat\nincluded\nthe\nkeyword\n\u201cclimate\u201d\nin\nthe\ntitle\nfrom\neach\nof\nthe\nfollowing\nten\nmajor\nU.S.\nmedia\noutlets,\nfor\na\ntotal\nof\n1,000\nnews\narticles:\n1.\nThe\nWall\nStreet\nJournal\n2.\nThe\nNew\nYork\nTimes\n3.\nNew\nYork\nPost\n4.\nLos\nAngeles\nTimes\n5.\nThe\nWashington\nPost\n6.\nReuters\n7.\nStar\nTribune\n8.\nChicago\nTribune\n9.\nThe\nBoston\nGlobe\n10.\nCNN\nMedia\noutlets\nwere\nselected\nbased\non\na\ncombination\nof\nsite\nranking\nand\nweb\ntraffic\ndata.\nArticle\ndates\nranged\nfrom\n2/15/21\nto\n9/29/22,\nthough\nhow\nfar\nback\narticles\nwent\nvaried\nby\nnews\noutlet.\nThe\ndate\nranges\nfor\narticles\nfrom\neach\noutlet\ncan\nbe\nfound\nin\nthe\nS u p p l e m e n t a r y\nM a t e r i a l s\n.\nA r t i c l e\nC a t e g o r i z a t i o n\nUsing\nsoftware\nto\nautomate\nthe\nprocess,\nwe\nsearched\nall\n1,000\narticles\nfor\nthe\npresence\nof\nkeywords\ndescribing\nten\nclimate-related\nthemes,\nwhich\nare\nlisted\nin\nTable\n1\nbelow.\nMost\nthemes\nare\nassociated\nwith\nultimate\nhuman-derived\ncauses\nof\nclimate\nchange.\nThat\nis,\nthemes\nthat\nrepresent\nhuman\nactivities\nthat\nare\nknown\nto\ncause\nclimate\nchange.\nExceptions\nto\nthis\nare\nthe\nfossil\nfuels\nand\nemissions\nthemes,\nwhich\ncapture\nproximate\ncauses\u2014direct\ncontributors\nto\nclimate\nchange\nthat\nare\nthe\nresult\nof\nhuman\nactivities\u2014and\nthe\nregenerative\nagriculture\ntheme,\nwhich\nis\noften\ndiscussed\nas\na\nway\nto\nmitigate\nclimate\nchange.\nSee\nthe\nS u p p l e m e n t a r y\nM a t e r i a l s\nsection\nfor\nthe\nfull\nlist\nof\nkeywords\nwithin\neach\ntheme.\nIf\nany\nof\nthe\nkeywords\nfrom\na\nparticular\ntheme\nwere\nfound\nwithin\nthe\nbody\ntext\nof\nan\narticle,\nwe\ncategorized\nit\nas\nincluding\nthat\ntheme.\nMore\nthan\none\ncategory\ncould\napply\nto\neach\narticle.\nFor\nexample,\nan\narticle\ncontaining\nthe\nphrases\n\u201cland\nclearing\u201d\nand\n\u201ccars\u201d\nwould\nbe\ntagged\nas\ncontaining\nthe\nthemes\nof\nland\nuse\nchanges\nand\ntransportation.\n9\n\nT a b l e\n1 .\nC l i m a t e - R e l a t e d\nT h e m e s\n10\n\nAccuracy\nChecks\nWe\nchecked\nthe\naccuracy\nof\nour\nsearch\nmethod\nby\nreviewing\nat\nleast\nthree\nrandomly-selected\narticles\nflagged\nwith\neach\ntheme,\nadjusting\nour\nkeywords\nwhen\nneeded.\nWe\nrepeated\nthis\nprocess\nuntil\nwe\nwere\nconfident\nthat\narticles\nwere\nbeing\ncorrectly\ncategorized\ninto\nthe\nten\nthemes.\nWe\nthen\nconducted\na\nfinal\nstage\nof\nmanual\nreview\nof\nany\narticles\nflagged\nwith\nthe\nanimal\nagriculture\ntheme,\nreading\nthem\nin\ntheir\nentirety\nand\neliminating\nany\nthat\ndidn't\nactually\ndiscuss\nanimal\nagriculture.\nA n a l y s e s\nMost\nquantitative\nanalyses\nfor\nthis\nreport\nwere\nsimple\ndescriptive\nstatistics\nto\ndetermine\npercentages.\nAdditionally,\nwe\nran\nKendall\u2019s\nrank\ntest\nto\nexamine\nthe\ncorrelation\nbetween\na\nmedia\noutlet\u2019s\npolitical\nleaning\nand\nhow\noften\nit\nreferred\nto\nanimal\nagriculture\nin\narticles\nabout\nclimate\nchange.\nPolitical\nleanings\nwere\nsourced\nfrom\nmediabiasfactcheck.com\n.\nFor\nmore\ndetails\non\nhow\nthis\nwas\ncarried\nout,\nplease\nsee\nthe\nS u p p l e m e n t a r y\nM a t e r i a l s\n.\nFor\nthe\nqualitative\nanalysis,\none\nauthor\nread\nall\narticles\nclassified\nas\ncontaining\nthe\nanimal\nagriculture\ntheme,\nanalyzing\nhow\nanimal\nagriculture\nwas\ndiscussed\nin\neach\nstory\nand\nnoting\ntrends\nin\ncoverage.\nShe\nalso\nread\na\nrandomly\nselected\nsample\nof\napproximately\n10\narticles\nn o t\nflagged\nwith\nthe\nanimal\nagriculture\ntheme\nto\nidentify\nany\nmissed\nopportunities\nto\ninclude\nanimal\nagriculture\nin\nthe\nreporting.\n11\n\nR e s u l t s\nThis\nstudy\u2019s\npre-registration,\nanalysis\ncode,\nand\ndata\nare\navailable\non\nthe\nOpen\nScience\nFramework\n.\nPlease\nnote\nthat\nthe\nfull\nlength\narticles\nare\nnot\nincluded\nwith\nour\ndata\nbecause\nmost\nrequire\nsubscriptions\nto\naccess.\nO v e r a l l\nF r e q u e n c y\nO f\nT h e m e s\nWe\ncategorized\n1,000\narticles\nabout\nclimate\nchange\naccording\nto\ntheir\nclimate-related\nthemes,\nas\nshown\nin\nFigure\n1\nbelow.\nOverall,\nanimal\nagriculture\nwas\nmentioned\nin\nonly\n7%\nof\narticles\nabout\nclimate\nchange,\nmaking\nit\none\nof\nthe\nleast-discussed\ncauses\nof\nclimate\nchange\nby\nthe\nmedia.\nMining,\nmanufacturing,\nand\nenergy\nproduction,\nand\nemissions\nwere\ndiscussed\nmuch\nmore\noften,\nappearing\nin\n68%\nand\n67%\nof\narticles,\nrespectively.\nF i g u r e\n1 .\nT h e m e\nC o v e r a g e\ni n\nC l i m a t e\nA r t i c l e s\n12\n\nF r e q u e n c y\nO f\nA n i m a l\nA g r i c u l t u r e\nS u b s e c t o r s\nAs\nnoted\nabove,\nroughly\n7%\nof\nclimate\narticles\nmentioned\nanimal\nagriculture,\nor\n69\narticles\nout\nof\n1,000.\nTable\n2\nbelow\nshows\nthe\nfrequency\nof\nthose\n69\narticles\nmentioning\ndifferent\nanimal\nagriculture\nsubsectors\nlike\nthe\ncattle\nindustry,\npork\nindustry,\nand\nchicken\nindustry.\nIn\nother\nwords,\nwhen\nclimate\narticles\nd o\nmention\nanimal\nagriculture,\nwhich\nsubsectors\ndo\nthey\ntouch\non?\nCows\ncame\nup\nthe\nmost,\nappearing\nin\nabout\n30%\nof\narticles\nthat\nmentioned\nanimal\nagriculture,\nwhile\npigs\nwere\nthe\nleast\nmentioned.\nT a b l e\n2 .\nI n d i v i d u a l\nK e y w o r d\nM e n t i o n s\nb y\nI n d u s t r y\nThe\nfarming\nof\nsome\nanimal\nspecies\nis\nresponsible\nfor\nmore\ngreenhouse\ngas\nemissions\nthan\nothers.\nWe\nmight\nexpect\nthat\nclimate\nmedia\nwould\ncover\nthe\nsubsectors\nresponsible\nfor\nthe\nmost\nemissions\nmore\noften.\nAs\nFigure\n2\nshows,\ncows\nare\nresponsible\nfor\nthe\nmost\nanimal\nagriculture\nemissions\u2014\n62%\n\u2014and\nthey\ndid\nreceive\nthe\nmost\nmedia\ncoverage\nof\nany\nsubsector,\nbut\nonly\n30%\nof\narticles\ntouching\non\nanimal\nagriculture\nmentioned\ncattle\nfarming\nin\nparticular.\nThe\nfigure\nprovides\na\nside-by-side\ncomparison\nof\nactual\ngreenhouse\ngas\nemissions\nfrom\nanimal\nagriculture\n(per\nFAO,\n2022\n)\nversus\nhow\nthey\nare\ncovered\nin\nthe\nmedia.\nIt\nshould\nbe\nkept\nin\nmind\nthat\nmany\nestimates\nexist\nregarding\nanimal\nagriculture\u2019s\ncontributions\nto\nglobal\nemissions,\nranging\nanywhere\nfrom\n11%\nto\nnearly\n20%\n.\n13\n\nF i g u r e\n2 .\nA c t u a l\nG r e e n h o u s e\nG a s\nE m i s s i o n s\nf r o m\nA n i m a l\nA g r i c u l t u r e\nV e r s u s\nM e d i a\nC o v e r a g e\n14\n\nA n i m a l\nA g r i c u l t u r e\nA l o n g s i d e\nO t h e r\nC a u s e s\nO f\nC l i m a t e\nC h a n g e\nAnimal\nagriculture\nwas\ncovered\nin\nclimate\narticles\nalongside\neach\nother\nclimate-related\ntheme\nat\nleast\nonce,\nbut\nsome\npairings\nwere\nmuch\nmore\ncommon\nthan\nothers.\nFor\nexample,\n52%\nof\nthe\narticles\ncategorized\nas\nregenerative\nagriculture\nalso\nbrought\nup\nanimal\nagriculture,\nwhile\nonly\n7%\nof\narticles\nthat\ndiscussed\nemissions\nalso\nmentioned\nanimal\nagriculture.\nCertain\nthemes,\nparticularly\nemissions,\ncould\nhave\nbrought\nup\nanimal\nagriculture\nat\na\nmuch\nhigher\nrate\ngiven\nthe\nrelationship\nbetween\nthe\ntwo.\nWe\nexpand\non\nthis\nmissed\nopportunity\nin\nthe\nA n i m a l\nA g r i c u l t u r e\nN a r r a t i v e\nI n\nC l i m a t e\nA r t i c l e s\nsection.\nF i g u r e\n3 .\nA n i m a l\nA g r i c u l t u r e\nM e n t i o n s\nb y\nT h e m e\n15\n\nC o v e r a g e\nB y\nM e d i a\nO u t l e t\nTo\nbetter\nunderstand\nthe\nextent\nto\nwhich\noutlets\ndiffer\nin\ntheir\ncoverage\nof\nanimal\nagriculture,\nwe\nanalyzed\nhow\noften\neach\nof\nthe\nmedia\noutlets\nreferenced\nanimal\nagriculture\nin\nclimate\narticles,\nas\ncompared\nto\nother\ncauses\nof\nclimate\nchange.\nAnimal\nagriculture\ncame\nup\nmuch\nless\noften\nthan\nother\ncauses\nlike\nemissions\nand\nmining,\nmanufacturing,\nand\nenergy\nproduction,\nwhich\nall\nmedia\noutlets\ndiscussed\nthe\nmost.\nThese\nwere\nalways\nfollowed\nby\nfossil\nfuels,\nthen\ntransportation.\nThe\ngeneral\nagriculture\nand\nresidential\nthemes\nwere\nusually\ncovered\nto\na\ngreater\nextent\nthan\nanimal\nagriculture\nas\nwell.\nAs\na\nreminder,\nwe\nconsidered\n100\nclimate\narticles\nper\nmedia\noutlet.\nAlthough\nall\noutlets\nmentioned\nanimal\nagriculture\nat\nleast\noccasionally,\nit\nwas\nmore\noverlooked\nby\nsome\noutlets\nthan\nothers.\nFor\ninstance,\nthe\nLos\nAngeles\nTimes\nmentioned\nanimal\nagriculture\nin\n14%\nof\narticles,\nwhile\nthe\nNew\nYork\nPost\nonly\nmentioned\nanimal\nagriculture\nin\n2%\nof\nthe\narticles\nwe\nanalyzed.\nIn\ncomparison,\nthe\nLos\nAngeles\nTimes\ndiscussed\nmining,\nmanufacturing,\nand\nenergy\nproduction\nin\n72%\nof\ntheir\nclimate\narticles,\nemissions\nin\n65%,\nand\nfossil\nfuels\nin\n56%.\nThe\nNew\nYork\nPost\nmentioned\nmining,\nmanufacturing,\nand\nenergy\nproduction\nin\n50%\nof\ntheir\narticles,\nemissions\nin\n57%,\nand\nfossil\nfuels\nin\n40%.\nRegardless\nof\noutlet,\nthe\namount\nof\ncoverage\nanimal\nagriculture\ngets\nin\ncomparison\nto\nother\ncauses\nof\nclimate\nchange\nis\nminimal.\nThe\nfigure\nbelow\nshows\nhow\ninfrequently\nthe\nanimal\nagriculture\ntheme\nwas\nbrought\nup\nin\nclimate\narticles\ncompared\nto\nthe\ntop\nthree\nmost\ncommon\nthemes.\n16\n\nF i g u r e\n4 .\nM e d i a\nO u t l e t\nC o v e r a g e\no f\nA n i m a l\nA g r i c u l t u r e\nV e r s u s\nt h e\nT o p - D i s c u s s e d\nC a u s e s\no f\nC l i m a t e\nC h a n g e\n17\n\nTo\nsee\nhow\nall\nten\nthemes\nwere\ncovered\nby\neach\nmedia\noutlet,\ncheck\nthe\nS u p p l e m e n t a r y\nM a t e r i a l s\n.\nPolitical\nLeaning\nAnd\nAnimal\nAgriculture\nMentions\nThe\ncorrelation\nbetween\nmedia\noutlets\u2019\npolitical\nleaning\nand\nanimal\nagriculture\nmentions\npoints\nto\na\nsmall\nbut\nstatistically\nsignificant\nnegative\nassociation\nbetween\nthe\ntwo\nvariables\n(\n\ud835\udf0f\n=\n-.08,\np\n<\n.05).\nSpecifically,\nthe\nmore\nleft-leaning\na\nmedia\noutlet\nis,\nthe\nmore\nlikely\nthey\nwere\nto\nmention\nanimal\nagriculture\nin\narticles\nabout\nclimate\nchange.\nT h e\nA n i m a l\nA g r i c u l t u r e\nN a r r a t i v e\nI n\nC l i m a t e\nA r t i c l e s\nOur\nqualitative\nanalysis\nof\nall\nanimal\nagriculture\narticles\nand\na\nselection\nof\nnon-animal\nagriculture\nones\nfound\nseveral\ntrends.\nWe\npresent\nthese\nwith\nexamples\nbelow,\nciting\nspecific\narticles\nin\nparentheses\nusing\ntheir\nassociated\nnumbers\nfrom\nthis\nspreadsheet\n.\nOverall,\nnews\noutlets\nfailed\nto\nmake\nthe\nconnection\nbetween\nclimate\nchange\nand\nanimal\nagriculture,\nand\nthere\nwas\na\ngeneral\ntendency\nto\nmiss\nopportunities\nto\ndiscuss\nthe\nimpact\nof\ndiet\non\nclimate\nchange\nin\nthe\narticles\nthat\ndid\nbring\nup\nanimal\nagriculture.\nMost\nClimate\nCoverage\nIgnores\nEmissions\nFrom\nAnimal\nAgriculture\nNot\nevery\nclimate\nstory\nis\nan\nanimal\nagriculture\nstory,\nbut\nwe\nobserved\nmany\nexamples\nof\nreporting\nthat\nmissed\na\nvital\nopportunity\nto\naddress\nfood\nsector\nemissions.\nE x a m p l e\n1 :\nC o v e r a g e\nO f\nE n e r g y\nT r a n s i t i o n\nSeveral\narticles\ndiscussed\nenergy\ntransition\u2014that\nis,\nshifting\nthe\nenergy\nsector\nfrom\ndepending\nlargely\non\nthe\nuse\nof\nfossil\nfuels\nto\nthe\nuse\nof\nrenewable\nsources\n(e.g.\nsolar\nand\nwind\nenergy)\u2014but\ndidn\u2019t\ntake\nthe\nopportunity\nto\nbring\nin\nrelevant\ninformation\nabout\nthe\nfood\nsector.\nFor\nexample,\ntwo\nWall\nStreet\nJournal\npieces\n(309,\n762)\nfocused\non\ncongressional\nnegotiations\naround\nclimate\nlegislation\nbut\nneglected\nto\nmention\nhow\nthe\nnew\npolicies\nwould\naddress\u2014or\nfail\nto\naddress\u2014rising\nfood-sector\nemissions.\nInstead\nthey,\nas\nwell\nas\na\nNew\nYork\nTimes\nstory\nabout\nthe\npassage\nof\nclimate\nlegislation\n(7),\nfocused\nprimarily\non\nthe\nissue\nof\nenergy\ntransition.\nE x a m p l e\n2 :\nC o v e r a g e\nO f\nC l i m a t e\nI m p a c t\nO n\nW i l d l i f e\nStories\nthat\ncover\nclimate\nimpacts\non\nwildlife\nare\na\nnatural\nfit\nto\nmention\nmeat\nconsumption\nand\nits\nclimate\nimpact\nbecause\nlivestock\nfarming\nis\none\nof\nthe\nleading\ndrivers\nof\ndeforestation\n,\nwhich\nnot\nonly\nreduces\nnatural\nhabitats\nfor\nwild\nanimals\nbut\nalso\nremoves\nvital\nnatural\ncarbon\nsinks\nthat\nhelp\noffset\ncarbon\nemissions.\nDespite\nthis\nconnection,\nmany\nsuch\nstories\nmade\nno\nmention\nof\ncattle\nranches\nor\nother\naspects\nof\nlivestock\nproduction. \n18\n\nIn\none\nReuters\nstory\n(559),\nthe\ncoverage\nof\nhow\nclimate\nis\nimpacting\nagriculture\nfailed\nto\nmake\nany\nmention\nof\nthe\ncattle\nranching\nindustry\nor\nsoy\nfeed\ncrops\n(\nSong\net\nal.,\n2021\n),\neven\nthough\nboth\nare\nmajor\ncontributors\nto\nfood\nsector\nemissions\n(\nPoore\n&\nNemecek,\n2018\n)\nand\ndeforestation\n(\nPendrill\net\nal.,\n2019\n)\u2014driving\nthe\nvery\nrisks\nthis\nstory\naimed\nto\nexplore\nin\nits\ncoverage.\nThe\nstory\neven\nwent\nso\nfar\nas\nto\ntalk\nabout\nimpacts\nto\nthe\n\u201cwater-intensive\u201d\nbeverage\nindustry\nwhile\nignoring\nentirely\nthe\nwater-intensive\ncattle\nranching\nindustry.\nSimilarly,\na\nLos\nAngeles\nTimes\nstory\nabout\nwild\nanimals\nin\nZimbabwe\n(1000)\nreported\non\nthe\ndevastation\nto\nwildlife\nas\na\nresult\nof\nclimate\nchange\nbut\nmissed\nan\nopportunity\nto\nconnect\nwhat\nwe\neat\nin\nthe\nU.S.\nto\nthese\nimpacts.\nE x a m p l e\n3 :\nC o v e r a g e\nO f\nR e g e n e r a t i v e\nA g r i c u l t u r e\nSome\nstories\nmentioned\nan\napproach\nto\nfarming\ncalled\n\u201cregenerative\nagriculture\u201d\nas\na\nclimate\nsolution\nwithout\nreporting\non\nhow\nthe\nsolution\nfalls\nshort.\nAccording\nto\nresearch\n(\nNewton\net\nal.,\n2020\n),\nregenerative\nagriculture\nis\nusually\nimplemented\nby\nfarms\nthrough\none\nor\nmore\nof\nthe\nfollowing\npractices:\nreducing\nor\neliminating\ntilling\nthe\nsoil,\nplanting\ncover\ncrops\n,\nand\ngrowing\na\nrotation\nof\ncrops\nand\nlivestock\nrather\nthan\nthe\nmore\nindustrialized\napproach\nof\nfarming\nas\nmuch\nof\na\nsingle\ntype\nof\ncrop\nor\nfarm\nanimal\nas\npossible.\nYet\nthere\nare\ntwo\nimportant\nareas\nof\ninquiry\njournalists\nare\nmissing\nin\ntheir\nregenerative\nagriculture\nreporting:\nFirst,\nconflicting\nevidence\non\nwhether\nagricultural\nsoils\ncan\nsuccessfully\nstore\ncarbon,\nand\nsecond,\nthe\nmassive\namount\nof\nland\nthat\nwould\nbe\nrequired\nto\nreplace\nfactory\nfarmed\nbeef\nwith\nregenerative\nbeef.\nSoil\nscientists\nare\nextremely\ndivided\non\nwhether\nregenerative\npractices\nlike\ncover\ncropping\nand\nreduced\ntillage\ncan\nsuccessfully\nsequester\ncarbon\nin\nsoil.\nIn\na\nnutshell,\nin\norder\nto\nsuccessfully\noffset\ncarbon\nemissions,\nthe\npractice\nmust\nbe\npermanent\n.\nThat\nis,\nit\nmust\nremove\ncarbon\npermanently\nfrom\nthe\natmosphere.\nBut\ndue\nto\ndiscoveries\nthat\nupended\nthe\nfield\nof\nsoil\nscience,\nit\u2019s\nbecome\nclear\nthat\ncarbon\nis\nfinicky,\nand\nefforts\nto\nget\nit\nto\nstay\nput\nin\nagricultural\nsoils\nare\ntypically\nnot\neffective.\nIn\naddition,\na\n2020\npeer-reviewed\nstudy\nof\na\nregenerative\nlivestock\noperation\nfound\nthat\nthe\nfarm\nneeded\n2.5\ntimes\nas\nmuch\nland\nas\nconventional\nanimal\nfarming\nto\nraise\nthe\nsame\namount\nof\nmeat\n(\nRowntree\net\nal.,\n2020\n).\nAny\nfood\nsector\nclimate\nsolution\nthat\nneeds\nlarge\namounts\nof\nland\nwill\ncome\nat\na\nclimate\ncost,\nwhat\nscientists\ncall\na\n\u201ccarbon\nopportunity\ncost,\u201d\nbecause\nit\ninvolves\ngiving\nup\nthat\ncarbon\nreserve\nthat\nwas\nkeeping\ncarbon\nout\nof\nthe\natmosphere\n(\nHayek\net\nal.,\n2021\n).\nA\nstory\nfor\nReuters\n(916)\nreported\non\n$3\nbillion\nin\nU.S.\nDepartment\nof\nAgriculture\nfunding\nfor\nclimate-smart\nfarming\nand\nforestry\nprojects:\nT h e\np r o g r a m\nw i l l\nf u n d\n7 0\np r o j e c t s\na c r o s s\n5 0\ns t a t e s\na n d\nP u e r t o\nR i c o\nt h a t\nw o u l d\ne n c o u r a g e\nf a r m e r s\nt o\nc u t\ne m i s s i o n s\ni n\nv a r i o u s\nw a y s .\nT h i s\nw o u l d\ni n c l u d e\np l a n t i n g\nc o v e r\n19\n\nc r o p s\nt o\ne n h a n c e\ns o i l\nh e a l t h\na n d\na b s o r b\nc a r b o n ,\ni m p r o v i n g\nm a n u r e\nm a n a g e m e n t\nt o\nc u t\nm e t h a n e\ne m i s s i o n s ,\na n d\nc o l l e c t i n g\nd a t a\no n\ne n v i r o n m e n t a l l y\nf r i e n d l y\nb e e f\na n d\nb i s o n\ng r a z i n g\np r a c t i c e s .\nYet\nthere\nwas\nno\nreporting\non\nthe\nefficacy\u2014or\nat\nleast\nthe\ndebate\nover\nefficacy\u2014of\nthese\npractices,\nespecially\nthe\nevidence\nsuggesting\ncarbon\nadded\nto\nfarmland\nsoils\ndoes\nnot\nstay\nput\nin\nthe\nground\nand\nthe\nmassive\namount\nof\nland\nrequired\nto\nraise\n\u201cenvironmentally\nfriendly\nbeef.\u201d\nEven\nStories\nThat\nCover\nAnimal\nAgriculture\nFail\nT o\nReport\nOn\nEmissions\nEven\nstories\nthat\nmentioned\nanimal\nfarming\noften\nfailed\nto\nreport\non\nthe\nconnection\nbetween\nfarming\nand\nclimate\nchange\naccurately.\nAccording\nto\nour\nquantitative\nanalysis,\nthe\nstories\nthat\nmost\ncommonly\nincluded\nthe\nanimal\nagriculture\ntheme\nwere\nthose\nthat\ndiscussed\nagriculture\nin\ngeneral\nand\nregenerative\nagriculture\nin\nparticular.\nE x a m p l e\n1 :\nC o v e r a g e\nO f\nG e n e r a l\nA g r i c u l t u r e\nMany\nstories\nthat\nfell\nunder\nthe\ngeneral\nagriculture\ntheme\nframed\ntheir\ncoverage\nin\nterms\nof\nhow\nfarms\nof\nall\nkinds\nhave\nbeen\nimpacted\nby\nclimate\nchange\u2014mentioning\nlivestock\nin\nterms\nof\nlosses\nto\ndrought\nor\nflooding,\nfor\nexample,\nalongside\ncrops\nlike\ncorn.\nA\nBoston\nGlobe\narticle\n(955)\nreported\non\nthe\nimpacts\nof\nthe\ndrought\non\nlivestock\nin\nterms\nof\ncosts\nto\nthe\nfarmer,\nquoting\na\npublic\nhealth\nexpert:\nB e r n s t e i n\ns a i d\nh e \u2019 s\na l s o\nw o r r i e d\na b o u t\nt h e\ne f f e c t\no n\nt h e\nh e a l t h\no f\nf a r m e r s ,\ne s p e c i a l l y\nt h e\ns t r e s s\nf r o m\nw o r r y i n g\na b o u t\nc r o p s\na n d\nl i v e s t o c k .\nR e s e a r c h e r s\nh a v e\no b s e r v e d\ni n c r e a s e d\nr a t e s\no f\ns u i c i d e\ni n\nf a r m i n g\nc o m m u n i t i e s\nd u r i n g\ns e v e r e\nd r o u g h t ,\na n d\nt h e\nf i n a n c i a l\nb u r d e n\nc a n\na f f e c t\nf a r m e r s \u2019\na c c e s s\nt o\nh e a l t h\nc a r e .\nA\nWashington\nPost\narticle\n(402)\nreporting\non\ndrought\nconditions\nalso\nmentioned\nthat\nranchers\nwere\nstruggling\nto\nprovide\nwater\nfor\ntheir\nlivestock.\nThe\nissue\nwas\nfurther\ndiscussed\nin\na\nstory\nby\nCNN\n(871):\nB e n n e t\na n d\nK e l l y\ns a i d\nt h e\nd r o u g h t\ni s\ns e v e r e\nf o r\nf a r m e r s\ni n\nt h e i r\ns t a t e s ,\nw h o\na r e\nb e i n g\nf o r c e d\nt o\nm a k e\nt o u g h\nc h o i c e s\nf o r\nt h e i r\nc r o p s\na n d\nl i v e s t o c k .\nA b o u t\n8 0 %\no f\nA r i z o n a \u2019 s\nC o l o r a d o\nR i v e r\na l l o c a t i o n\ng o e s\nt o w a r d\nw a t e r i n g\nc r o p s ,\nK e l l y\ns a i d .\nDrought\nconditions\nduring\nthe\nsummer\nof\n2022\ndid\nimperil\nlivestock\npopulations,\nforcing\nfarmers\nto\nsell\nanimals\nfor\nslaughter\nwhen\ntemperatures\nmade\nit\nimpossible\nto\nkeep\nthem\ncool\nand\nhydrated.\nBut\nthese\nstories\nalso\nmissed\nan\nopportunity\nto\ninform\nreaders\nof\nhow\ntheir\ndietary\nchoices\ncontribute\nto\nclimate\nemissions.\nBeef\nconsumption,\nfor\nexample,\nis\na\nleading\ndriver\nof\nfood\nsector\nemissions\n.\n20\n\nE x a m p l e\n2 :\nC o v e r a g e\nO f\nM e a t\nR e d u c t i o n\nA s\nC l i m a t e\nA c t i o n\nIn\nsome\nstories\nthe\nreporter\ncharacterized\nmeat\nreduction\nas\nineffective\nindividual\naction\n(77):\n\u201c S o m e\no f\nu s\nw e r e\nt h i n k i n g\nc a k e\na n d\ni c e\nc r e a m .\nS w a i n\nw a s\nt h i n k i n g\nc l i m a t e\nc h a n g e .\nC o u l d\nh e\na s k\nw e l l - w i s h e r s\nt o\np l e d g e\nt o\ns t e p\nu p\nt h e i r\ne f f o r t s\nt o\nr e d u c e\ng r e e n h o u s e\ng a s\ne m i s s i o n s ?\nC o u l d\nh e\nf i n d\na\nw a y\nt o\nu r g e\nt h e m\nt o\ng o\nb e y o n d\ni n d i v i d u a l\nc h o i c e s ,\ns u c h\na s\ne a t i n g\nl e s s\nm e a t\no r\nd r i v i n g\na n\ne l e c t r i c\nc a r ,\na n d\ne m p l o y\nt h e\nc h a n g e - m a k i n g\np o w e r\no f\nd e m o c r a t i c\na c t i o n ? \u201d\nOthers\nframed\nit\nas\na\nfringe\nDemocratic\nculture-war\neffort\nalong\nwith\nbanning\ngas\nstoves\n(11):\nA n o t h e r\nC l i m a t e\nC o r p s\no p t i o n\ni s\nt o\nb e c o m e\na\n\" c l e a n\ne n e r g y\ne d u c a t o r . \"\n. . .\nB r a c e\ny o u r s e l f\nf o r\nt e a c h - i n s\no n\nt h e\ns i n s\no f\nm e a t\ne a t i n g\na n d\nn a t u r a l - g a s\ns t o v e s .\nAnother\nexample\nof\nthe\nlack\nof\nseriousness\ngiven\nto\nmeat\nreduction\nas\na\nclimate\nchange\nmitigation\nstrategy\ncomes\nfrom\na\nChicago\nTribune\nstory\n(80)\nthat\nreported\non\nindividual\nactions\nto\ncombat\nclimate\nchange\nbut\nmade\nno\nmention\nof\neating\nless\nmeat,\ninstead\nrecommending\nthat\nreaders\nbuy\nlocally\ngrown\nfood.\nThis\nis\nnot\nonly\na\nmissed\nopportunity\nbut\nan\nexample\nof\nmisleading\nreporting:\nData\nsuggests\nthat\nmost\nfood\nsector\nemissions\ncome\nfrom\nlivestock\nmethane\nemissions\nand\nland\nuse,\nwhile\ntransportation\nemissions\naccount\nfor\na\nmere\n6%\nof\nfood\nsystem\nemissions.\nShifting\nto\na\nplant-rich\ndiet\nthen,\nprimarily\nreducing\nbeef\nconsumption,\nand\ncurbing\nfood\nwaste\nare\nfar\nmore\neffective\nfor\nreducing\nfood\nsector\nemissions.\nFurthermore,\ntwo\nNew\nYork\nPost\nstories\nabout\ncelebrities\n(549,\n551)\nmentioned\nmeat\nconsumption\nwith\nother\nconsumer\nactions\nlike\nprivate\nflights\nand\nplastic\nuse\nwithout\nany\ncontext\nfor\nthe\nemissions\nassociated\nwith\nany\nof\nthese\nactions.\nMeanwhile,\nanother\nChicago\nTribune\nstory\n(926)\ndid\nnot\nreport\nemissions\nfigures\nbut\ndid\ninclude\nthe\nrecommendation\nto\neat\nless\nmeat:\nB o t h\nY u d t\na n d\nD u n c a n\ns a y\ns o l u t i o n s\no f t e n\ns t a r t\na s\nb a b y\ns t e p s .\nH e r e\na r e\ns o m e\nt h i n g s\na l l\no f\nu s\nc a n\nd o\nt o\nq u e l l\nt h e\nc l i m a t e\nc r i s i s .\n\u25cf\nM i n i m i z e\np l a s t i c\nu s e .\nE v e n\nt h o u g h\nm a n y\nt o w n s\nh a v e\nr e c y c l i n g\np r o g r a m s ,\nD u n c a n\ns a i d ,\no n l y\na\ns m a l l\np e r c e n t a g e\no f\nt h e\nm a t e r i a l s\nc o l l e c t e d\na c t u a l l y\ng e t\nr e c y c l e d .\n\u25cf\nP i c k\nu p\nl i t t e r .\n\u25cf\nB e\nm i n d f u l\no f\nt h e\nc o m p a n i e s\nf r o m\nw h i c h\ny o u\nb u y\np r o d u c t s .\n\u25cf\nC u t\nb a c k\no n\nm e a t\nc o n s u m p t i o n .\nClimate\njournalists\nmay\nbe\nhesitant\nto\nrecommend\nindividual\nactions\nfor\nfear\nof\nletting\ncorporate\npolluters\noff\nthe\nhook,\nbut\nclimate\nmodels\nrecommend\nboth\nforms\nof\naction\nnecessary\nto\ncurb\nemissions:\ninstitutional\nchange\nas\nwell\nas\nindividual\ndietary\nchange.\nA\n2021\nstudy\nfrom\nProject\n21\n\nDrawdown\nfound\nthat\ndietary\nchange\nis\none\nof\nthe\nmost\npowerful\nforms\nof\nhousehold\nclimate\naction\u2014both\nshifting\nto\na\nplant-rich\ndiet\nand\ncurbing\nfood\nwaste.\nE x a m p l e\n3 :\nC o v e r a g e\nO f\nR e g e n e r a t i v e\nA g r i c u l t u r e\nA s\nA\nS o l u t i o n\nT o\nA n i m a l\nA g\nOther\nanimal\nagriculture\nstories\ntouched\non\nthe\ntheme\nof\nregenerative\nagriculture\u2014and\nreported\non\nthis\nfarming\npractice\nas\na\nclimate\nsolution.\nAdvocates\nfor\nit\nclaim\nthat\nregenerative\nagriculture\npractices\ncan\nsequester\ncarbon\nand\nreduce\noverall\nemissions.\nOtherwise\nknown\nas\ncarbon\nfarming\n,\nthe\nidea\nis\nthat\npractices\nlike\ncover\ncropping,\ncrop\nrotations,\nand\ntillage\nreduction\nremove\ncarbon\nfrom\nthe\natmosphere\nand\nstore\nthem\nin\nagricultural\nsoils.\nIn\nsome\ncases,\nregenerative\nfarms\nthat\nraise\nmeat\neven\ncall\ntheir\noperations\n\u201ccarbon\nneutral.\u201d\nHowever,\nas\ndiscussed\nabove,\nnumerous\nclimate\nresearchers\nand\nsoil\nscientists\npoint\nto\ntwo\nproblems\nwith\nthese\nclaims:\nfirst,\nthat\nstudies\nshow\nadded\ncarbon\nin\nsoil\nis\ninconsistent\nand\nnot\npermanent,\nso\nit\nis\nnot\na\ntrue\ncarbon\noffset,\nand\nsecond,\nthat\nregenerative\nfarms\nthat\nproduce\nmeat\nrequire\nsignificantly\nmore\nland\nto\nproduce\nthe\nsame\namount\nof\nfood\nas\nconventional\nanimal\nfarming\n(\nRowntree\net\nal.,\n2020\n).\nTwo\nstories\nreported\non\nfederal\nfunding\nof\nclimate\nsolutions\nin\nthe\nfarm\nsector\u2014in\nboth\ncases\nprimarily\nconservation\nor\nregenerative\nfarm\napproaches\u2014without\ninvestigating\nwhat\ndrives\nfood\nsector\nemissions\nand\nwhether\nthese\nmethods\nare\neffective\nat\ncurbing\nthose\nemissions.\nThe\nfirst,\nan\narticle\nfrom\nThe\nWall\nStreet\nJournal\n(409)\ncovered\nfederal\nfunding\nfor\nfarm\nconservation.\nWhile\nit\naccurately\nreported\nthat\n24%\nof\nemissions\ncomes\nfrom\nthe\nfood\nsector,\nit\nalso\nincluded\ncover\ncrops\nand\nindustry-led\nefforts\nto\nreduce\ncattle\nfarms\u2019\nmethane\nemissions\nthrough\nfeed\nadditives\nwithout\nreporting\non\nthe\nefficacy\nof\neither\nsolution.\nWhat\u2019s\nmissing\nfrom\nmost\nreporting\non\nfeed\nadditives\u2014a\ncommonly\nconsidered\nmethod\nto\nreduce\nmethane\nemissions\u2014is\nthat\nthis\nmethane\nreduction\nstrategy\nis\nonly\npractical\non\nthe\nfeedlot\n.\nBecause\nthat\u2019s\njust\npart\nof\na\ncow\u2019s\nlife\ncycle\n,\nthe\nactual\nefficacy\nfor\nthese\nadditives\nis\nmuch\nlower\nthan\ntypically\nreported,\n8.8%\naccording\nto\ncalculations\nby\nresearchers.\nThe\nsecond\nstory,\nfrom\nThe\nNew\nYork\nTimes\n(413), covered\nregenerative\npractices\nand\n\u201cclimate-smart\nagriculture\u201d\nby\nbeginning\nwith\na\ndescription\nof\na\nfarm\nthat\nraises\ngrass-fed\ncows\nand\ngoats\nand\n\u201creceives\na\nlittle\nhelp\nfrom\nthe\nAgriculture\nDepartment.\u201d\nThe\nreporter\nmentioned\nscientific\ndoubt\nabout\nthe\nefficacy\nof\nregenerative\nfarming\nfor\nsequestering\ncarbon\nbut\ndid\nnot\nfully\ninterrogate\nwhether\nraising\ncattle\ncan\nbe\n\u201cclimate-friendly.\u201d\nThe\narticle\nlacked\neven\na\nbasic\nexplanation\nof\nwhat\ndrives\nfood-sector\nemissions:\nnamely,\nmethane\nfrom\ncows\nno\nmatter\nwhat\nthey\u2019re\nfed,\nand\nsecond,\nland\nuse\nto\ngrow\nfeed\ncrops\nand\nprovide\npasture\nfor\ngrazing.\nThe\nNew\nYork\nTimes\nhas\nprovided\nsimilar\ninformation\nto\nreaders\non\nother\noccasions\nbut\nis\ninconsistent\nin\ndoing\nso.\n22\n\nSome\nAnimal\nAgriculture\nStories\nGot\nIt\nRight\nThough\nrare,\nthere\nwere\nseveral\nbright\nspots\nin\nthe\ncoverage\nof\nanimal\nagriculture,\nnotably\nseveral\nCNN\nstories\nthat\nreported\naccurately\non\nfood-sector\nemissions.\nSome\nstories\nonly\nbriefly\nmentioned\nanimal\nagriculture.\nFor\ninstance,\na\nReuters\nstory\n(408)\nmentioned\nboth\nmeat\nreduction\nand\nplant-based\nfoods\nas\na\nsolution:\nR e s e a r c h\nt o\nb e\np r e s e n t e d\no n\nT u e s d a y\nl o o k e d\na t\nh o w\n4 0\nb i g\nc o m p a n i e s\ni n c l u d i n g\na g r i c u l t u r a l\np r o d u c e r s\na n d\nf o o d\nr e t a i l e r s\nc o u l d\nf a r e\nu n d e r\ns c e n a r i o s\nc a l l e d\nk e y\nt o\nr e d u c i n g\ne m i s s i o n s ,\ns u c h\na s\ni f\ng o v e r n m e n t s\ni m p o s e\nc a r b o n\ne m i s s i o n s\np r i c e s\no r\ni f\nc o n s u m e r s\nr e d u c e\nt h e i r\nc o n s u m p t i o n\no f\nm e a t .\nT h e\ns t u d y ,\ns e e n\nb y\nR e u t e r s\nN e w s ,\nf o u n d\nt h e\nc o m p a n i e s '\nv a l u e\nw o u l d\nd e c l i n e\nb y\na n\na v e r a g e\no f\na r o u n d\n7 %\nb y\n2 0 3 0 ,\ne q u i v a l e n t\nt o\ns o m e\n$ 1 5 0\nb i l l i o n\ni n\ni n v e s t o r\nl o s s e s ,\ni f\nt h e y\nd i d\nn o t\na d o p t\nn e w\np r a c t i c e s .\nA t\nt h e\ns a m e\nt i m e ,\nb u s i n e s s\na r e a s\nl i k e\np l a n t - b a s e d\nm e a t\na n d\nf o r e s t\nr e s t o r a t i o n\no f f e r\nt h e\ns a m e\nc o m p a n i e s\nb i g\nn e w\no p p o r t u n i t i e s ,\nt h e\nr e p o r t\ns t a t e s .\nA\nLos\nAngeles\nTimes\narticle\n(95)\nreferenced\na\nnews\nstory\nfrom\nInside\nClimate\non\nranching\nand\nsoil\nhealth,\nhighlighting\nthat\nthe\nscience\ndoes\nnot\nsupport\nrancher\nclaims\nthat\ntheir\nactivities\nbenefit\nthe\nland.\nThree\nstories\nfrom\nCNN\nreported\nexplicitly\non\nthe\nconnection\nbetween\nmeat\nand\nclimate\nchange.\nOne\narticle\n(466)\nprofiled\na\nscientist\nwho\nadopted\na\nplant-based\ndiet\nafter\nswitching\nhis\nfield\nof\nstudy\nto\nclimate\nscience:\nA t\nt h e\nc r o s s r o a d s\no f\nh i s\nt w o\nm a j o r\ni n t e r e s t s \u2014 h e a l t h\na n d\nc l i m a t e \u2014 S p r i n g m a n n \u2019 s\nr e s e a r c h\nw a s\ns h o w i n g\nh o w\nh i s\np l a n t - b a s e d\nd i e t\nl o w e r e d\nh i s\nc a r b o n\nf o o t p r i n t .\n\u201c M y\ni n i t i a l\nt h o u g h t\nw a s ,\n\u2018 I f\nI\nk n o w\ni t \u2019 s\nh e a l t h i e r\na n d\ni t \u2019 s\nm o r e\ne n v i r o n m e n t a l l y\ns u s t a i n a b l e\n( t o\nn o t\ne a t\na n i m a l\np r o d u c t s ) ,\nw h y\nw o u l d n \u2019 t\nI\nd o\nt h a t ? \u2019 \u201d\nW i t h\nh i s\nm e a t - e a t i n g\nd a y s\no v e r ,\nh e\nd u g\nd e e p e r\ni n t o\nt h e\nc o r r e l a t i o n\no f\nd i e t\na n d\ne n v i r o n m e n t \u2014 s p e c i f i c a l l y ,\nh o w\ne a t i n g\nl e s s\na n i m a l - b a s e d\np r o d u c t s\nc o u l d\nh e l p\ns l o w\nt h e\nc l i m a t e\nc r i s i s .\nI t \u2019 s\na\nt o p i c\nh e\ns a i d\nd i d\nn o t\ng e t\na\nl o t\no f\na t t e n t i o n\n1 5\nt o\n2 0\ny e a r s\na g o .\n\u201c A t\nt h a t\nt i m e ,\nm o s t\no f\nt h e\nc l i m a t e\nc h a n g e\nr e s e a r c h\nw a s\nr e a l l y\ns o r t\no f\no n\nt h e\np r o d u c t i o n\ns i d e \u2014 p o w e r\np l a n t s\na n d\nh o w\nt o\nr e d u c e\nc a r b o n\nd i o x i d e\ne m i s s i o n s .\nT h e\nl i n k\nb e t w e e n\nc l i m a t e\nc h a n g e ,\ng r e e n h o u s e\ng a s\ne m i s s i o n s\na n d\nt h e\nf o o d\ns y s t e m\nw a s\nr e a l l y\ni n\nt h e\nb a b y\ns t a g e . \u201d\nAnother\nstory\n(685)\nreported\non\nthe\nimpact\nof\ncompanion\nanimal\nfood,\nciting\ntheir\n\u201cmeat-heavy\u201d\ningredient\nlist\nas\na\nsource\nof\nemissions.\nAnd\na\nthird\narticle\n(356)\ncovered\nresearch\nthat\nshowed\n23\n\nthe\nbest\nway\nto\nreduce\nyour\ndietary\nemissions\nis\nto\nfocus\non\nwhat\nyou\neat\u2014namely\nanimal-based\nfoods\u2014rather\nthan\nbuying\nlocally-grown\nfoods:\nL a n d\nu s e\na n d\nf a r m - s t a g e\ne m i s s i o n s ,\ni n c l u d i n g\nt h e\na p p l i c a t i o n\no f\nf e r t i l i z e r s\na n d\np r o d u c t i o n\no f\nm e t h a n e\ni n\nt h e\ns t o m a c h s\no f\nc a t t l e ,\na c c o u n t\nf o r\nm o r e\nt h a n\n8 0 %\no f\nt h e\nf o o t p r i n t\nf o r\nm o s t\nf o o d s .\nT r a n s p o r t\ni s\nr e s p o n s i b l e\nf o r\nl e s s\nt h a n\n1 0 %\no f\nt h e i r\nf i n a l\nc a r b o n\ni m p a c t ;\nf o r\nb e e f\ni t \u2019 s\nl e s s\nt h a n\n1 % .\nT h e\nr e m a i n d e r\no f\na\nf o o d \u2019 s\ne m i s s i o n s\nm o s t l y\no c c u r\nd u r i n g\np r o c e s s i n g ,\np a c k a g i n g ,\na n d\nr e t a i l .\n\u201c E a t i n g\nl o c a l l y\nw o u l d\no n l y\nh a v e\na\ns i g n i f i c a n t\ni m p a c t\ni f\nt r a n s p o r t\nw a s\nr e s p o n s i b l e\nf o r\na\nl a r g e\ns h a r e\no f\nf o o d \u2019 s\nf i n a l\nc a r b o n\nf o o t p r i n t , \u201d\nR i t c h i e\nw r o t e\ni n\nt h e\nr e p o r t .\n\u201c F o r\nm o s t\nf o o d s ,\nt h i s\ni s\nn o t\nt h e\nc a s e . \u201d\nC o n c l u s i o n s\nN o t\nE n o u g h\nA t t e n t i o n\nI s\nG i v e n\nT o\nA n i m a l\nA g r i c u l t u r e \u2019 s\nR o l e\nI n\nT h e\nC l i m a t e\nC r i s i s\nAlthough\nall\nnews\noutlets\ncovered\nanimal\nagriculture\nto\nsome\nextent,\nthe\nvast\nmajority\nof\nclimate\nreporting\nincluded\nin\nthis\nstudy\u201493%\u2014made\nno\nmention\nof\nanimal\nagriculture.\nEven\nthe\nsmall\npercentage\nof\nstories\nthat\ndid\ncover\nanimal\nagriculture\nmostly\nfailed\nto\nmake\nthe\nconnection\nbetween\nmeat\nconsumption\nand\nrising\nclimate\nemissions\nand\nenvironmental\ndegradation.\nMost\narticles\nonly\nbriefly\nmentioned\nanimal\nagriculture\nand,\nif\ndiscussed\nin\ngreater\ndetail,\nmore\noften\nthan\nnot\nit\nwas\nin\nterms\nof\nhow\nclimate\nchange\nis\naffecting\nthe\nanimal\nagriculture\nindustry\nrather\nthan\nthe\nother\nway\naround.\nIn\nfact,\nof\nall\nthe\nclimate\nstories\nanalyzed\nin\nthis\nstudy,\nonly\na\nhandful\nexplicitly\ncovered\nanimal\nagriculture\u2019s\neffects\non\nclimate\nchange.\nIn\nmost\nstories\nthat\ntouch\non\nanimal\nagriculture,\nnews\noutlets\nare\nmissing\na\ncritical\nopportunity\nto\ninform\nreaders\nabout\nthe\nimpact\nof\nwhat\nthey\neat.\nThe\nthemes\nmost\ncovered\nby\nall\nnews\noutlets\nwere\nmining,\nmanufacturing,\nand\nenergy\nproduction,\nemissions,\nfossil\nfuels,\nand\ntransportation,\nyet\nthese\nalso\nhappened\nto\nbe\nthe\nthemes\nthat\nwere\nleast\nlikely\nto\nbe\ndiscussed\nalongside\nanimal\nagriculture\nin\nclimate\narticles.\nAnd\nit\nisn\u2019t\ndue\nto\na\nlack\nof\nrelation\nbetween\nthem\n\u2014\nfor\ninstance,\nagriculture\nis\nthe\nnumber\none\nsource\nof\nmethane\nin\nthe\nworld,\nmost\nof\nwhich\ncomes\nfrom\nlivestock\nproduction\n,\nand\nit\u2019s\nestimated\nthat\n20%\nof\nanimal\nagriculture\nemissions\ncome\nfrom\nthe\nuse\nof\nfossil\nfuels\nalong\nsupply\nchains\n(\nFAO,\n2013\n).\nFurthermore,\nat\na\nglobal\nscale,\nanimal\nagriculture\nis\nresponsible\nfor\na\nsimilar\npercentage\nof\ngreenhouse\ngas\nemissions\nas\nthe\ntransportation\nsector\n,\nyet\nit\nreceives\nfar\nless\ncoverage\nin\nthe\nmedia.\n24\n\nAlthough\nresearch\non\nthis\ntopic\nis\nlacking,\nprevious\nresearch\nsupports\nand\nexpands\non\nour\nfinding\nthat\nthere\nis\na\ntendency\nfor\nthe\nmedia\nto\ngive\nlittle\nattention\nto\nhow\nanimal\nagriculture\ncontributes\nto\nclimate\nchange.\nIn\naddition\nto\nfinding\nlow\ncoverage\nof\nanimal\nagriculture\nin\nclimate\nmedia\nin\nthe\nU.S.\nand\nUnited\nKingdom,\none\nstudy\nfound\nthat\ngovernments\nand\nthe\nlarge-scale\nanimal\nagriculture\nindustry\nare\nnot\nheld\nas\naccountable\nas\nconsumers.\nIn\nother\nwords,\nthey\nfound\nmore\nmentions\nof\nthe\nneed\nfor\nindividual\ndietary\nchange\nthan\nto\nreform\ngovernment\npolicies\nor\nagricultural\npractices\n(\nKristiansen\net\nal.,\n2020\n).\nAnother\nstudy\nfound\nthat\ndespite\nscientific\nconsensus\non\nthe\nconnection\nbetween\nanimal\nagriculture\nand\nclimate\nchange,\nthe\nmedia\noften\ntreats\nit\nas\na\ndebate,\npresenting\nboth\n\u201csides\u201d\nto\nan\nargument\nthat\ndoesn\u2019t\nreally\nexist\n(\nFry\net\nal.,\n2022\n).\nConsequently,\nthere\nis\nevidence\nthat\nthe\nmedia\nis\ndownplaying\nthe\nrole\nof\nanimal\nagriculture\neven\nwhen\nit\ni s\ndiscussed\nin\nrelation\nto\nclimate\nchange.\nResearch\nshows\nthat\nfalse\nbalance\nreporting\u2014when\njournalists\npresent\nboth\nsides\nof\nan\nissue,\neven\nwhen\none\nside\nhas\ngreater\nevidence\nto\nback\nit\nup\u2014can\ncause\npeople\nto\ndoubt\nthe\nscientific\nconsensus\non\nissues\nlike\nclimate\nchange\n(\nImundo\n&\nRapp,\n2022\n),\nmaking\nthis\na\nparticularly\ndangerous\napproach\ngiven\nthe\nseriousness\nof\nthe\nissue.\nM i s i n f o r m a t i o n\nA n d\nM i s s i n g\nI n f o r m a t i o n\nI n\nC l i m a t e\nC o v e r a g e\nOverall,\nanimal\nagriculture\ntended\nto\nbe\ncovered\nrather\nbriefly,\nand\nalmost\nalways\nin\nthe\ncontext\nof\nanother\ncause\nof\nclimate\nchange,\nsuch\nas\ntransportation\nor\nmining,\nmanufacturing,\nand\nenergy\nproduction,\nfor\nexample.\nIn\nmany\nof\nthese\nstories,\noutlets\ncovered\nanimal\nagriculture\nas\npart\nof\ngeneral\nagriculture\nor,\nin\nsome\ncases,\nregenerative\nagriculture\u2014often\nin\nways\nthat\nincluded\ninaccuracies\nor\nmissing\nkey\nfacts\nand\ncontext\nabout\nemissions\nfrom\nmeat.\nFor\ninstance,\nin\nthe\ncase\nof\nregenerative\nagriculture,\nthe\npurpose\nis\nto\nmitigate\nclimate\nchange\nand\nenvironmental\ndegradation.\nDespite\nthe\nclear\nscientific\nevidence\nthat\nmost\nagricultural\nemissions\ncome\nfrom\nlivestock\nfarming\nand\nthat\nit\nhas\ndetrimental\nconsequences\non\nthe\nenvironment,\nover\nhalf\nof\nregenerative\nagriculture\narticles\nmentioned\nlivestock\nfarming,\noften\nin\nthe\ncontext\nof\nincorporating\nit\ninto\nthese\n\u201cclimate-smart\u201d\npractices,\nwithout\npresenting\nany\ndata\nabout\nthe\nnegative\neffects\nof\nanimal\nagriculture.\nIn\nanother\nmissed\nopportunity,\nmany\narticles\nbrought\nup\nthe\neffects\nof\nclimate\nchange\non\nfarmers\naround\nthe\nworld\nbut\nfailed\nto\nconsider\nthe\nglobal\nrepercussions\nof\nU.S.\nconsumption\nof\nanimal\nproducts.\nFor\nexample,\nmeat\nconsumed\nin\nthe\nU.S.\nis\noften\nimported,\nso\nan\nincrease\nin\ndemand\nfor\nbeef\nin\nthe\nU.S.\ncan\nresult\nin\nan\nincrease\nin\ndeforestation\nin\nthe\nAmazon\nto\nmake\nroom\nfor\nmore\ncows,\nincreasing\nemissions\nin\nSouth\nAmerica\nand\nreducing\na\nvery\nimportant\nglobal\ncarbon\nsink.\n25\n\nT h e\nM e d i a \u2019 s\nR o l e\nI n\nC o m m u n i c a t i n g\nC l i m a t e\nC h a n g e\nI n f o r m a t i o n\nA\nstudy\nby\nthe\nReuters\nInstitute\n(2022)\nfound\nthat\nin\nthe\nU.S.,\n24%\nof\npeople\npay\nattention\nto\nmajor\nnews\norganizations\nfor\nclimate\nchange\nnews,\nthough\nroughly\nthe\nsame\npercentage\nof\npeople\nsay\nthey\ndon\u2019t\npay\nattention\nto\nclimate\nchange\nat\nall.\nAs\nthe\nresearchers\nfrom\nthe\nstudy\nacknowledge,\npolarized\npolitics\nand\nmedia\ncoverage\nplay\na\nrole\nin\n\u201cdriving\ndown\ninterest\nin\nand\nattention\nto\nclimate\nchange\nas\nan\nissue.\u201d\nAs\nwe\nsaw\nin\nthis\nstudy\u2019s\nresults,\npolitical\nleaning\nmay\nalso\nplay\na\npart\nin\nwhether\nanimal\nagriculture\nis\nbrought\nup\nwhen\ncommunicating\nabout\nclimate\nchange\u2014left-leaning\nmedia\noutlets\ntend\nto\ndiscuss\nanimal\nagriculture\nmore\noften\nthan\nright-leaning\nones.\nHowever,\nall\nnews\noutlets,\nregardless\nof\npolitical\nleaning,\nfailed\nto\ngive\nenough\nattention\nto\nanimal\nagriculture,\nlet\nalone\ndiscuss\nits\nconsequences\non\nthe\nenvironment\nin\ndepth.\nEvidence\nrecently\ncame\nto\nlight\nabout\nthe\nmeat\nindustry\u2019s\ninfluence\nin\nblocking\nthe\nIntergovernmental\nPanel\non\nClimate\nChange\n(IPCC)\nfrom\nrecommending\nplant-based\ndiets\nto\nfight\nclimate\nchange.\nWith\nreports\nof\nglobal\nsignificance\nlike\nthis\nexcluding\nthe\ninfluence\nof\nanimal\nagriculture\nfrom\ntheir\nnarratives,\nalong\nwith\nthe\nmedia\nfailing\nto\nproperly\ncover\nthis\nissue,\nit\u2019s\nnot\nsurprising\nthat\nvery\nfew\npeople\naround\nthe\nworld\nare\naware\nthat\nanimal\nagriculture\nis\na\nleading\ncause\nof\nclimate\nchange.\nThey\ninstead\nthink\nthat\nother\nhuman-derived\ncauses\nof\nclimate\nchange,\nlike\ntransportation,\nare\nof\nmuch\ngreater\nconcern.\nThrough\ntheir\nrole\nin\ncommunicating\nimportant\nissues\nto\nthe\npublic,\nnews\noutlets\nhave\nthe\nunique\nability\nto\nbridge\nthe\ngap\nbetween\nclimate\nscience\nand\npublic\nknowledge.\nHowever,\nas\nthis\nand\nother\nstudies\nshow,\nmore\nneeds\nto\nbe\ndone\nin\nterms\nof\ninforming\nreaders\nabout\nhow\nanimal\nagriculture\nimpacts\nthe\nenvironment\nand\nthe\nimportance\nof\nshifting\nglobal\ndiets\nto\nmitigate\nclimate\nchange.\nC a v e a t s\n&\nL i m i t a t i o n s\nAs\nwith\nall\nreports,\nthis\none\nhas\nsome\nimportant\ncaveats\nand\nlimitations\nto\nbear\nin\nmind.\nFirst,\nour\nstudy\nwas\nlimited\nto\njust\n100\nclimate\narticles\nper\nnews\norganization,\nwhich,\ndepending\non\nthe\nnews\noutlet,\nis\nonly\na\nsmall\nsample\nof\nall\nthe\nclimate\narticles\nthey\nhave\npublished.\nFurthermore,\nthis\nmeant\nthat\nfor\nsome\nnews\noutlets,\narticle\ndates\nspanned\nonly\na\nmonth\nif\nclimate\nchange\nwas\na\nfrequently\ncovered\ntopic,\nwhile\nfor\nothers\nit\nwas\na\nyear\nor\nmore\u2014this\ninfluenced\nthe\nkind\nof\ncontent\nthat\nwas\ncovered.\nWe\nbriefly\nconsidered\nweb\nscraping\n\u2014\nthe\nprocess\nof\nextracting\ndata\nfrom\na\nwebsite\nwith\nan\nautomated\ntool\u2014but\nit\npresented\ntechnical\nand\nlegal\nchallenges\nbecause\nthe\narticles\nare\npaywalled.\nWe\npurchased\na\none-month\nsubscription\nfrom\neach\noutlet\nwhere\nit\nwas\nrequired\nin\norder\nto\nobtain\naccess\nfor\nthis\nstudy.\n26\n\nSecond,\nin\nan\neffort\nto\nselect\nthe\nmost\nrelevant\nclimate\nchange\narticles\nfor\nanalysis\nin\nthis\nstudy,\nwe\nonly\nincluded\narticles\nthat\ncontained\nthe\nword\nc l i m a t e\nin\ntheir\ntitles,\nwhich\nmay\nhave\nexcluded\nsome\nrelevant\narticles.\nHowever,\ndoing\nso\nallowed\nus\nto\nuse\nquick\nand\nobjective\ncriteria\nfor\narticle\nselection.\nOne\nfurther\ncaveat\nis\nthat\na\nfew\narticles\nwere\nincluded\nin\nour\ndata\nset\nmore\nthan\nonce.\nNews\noutlets\nwill\noccasionally\nrepublish\narticles\nthat\noriginally\ncame\nfrom\nanother\noutlet,\nmeaning\nthat\nsome\nof\nthe\narticles\nwe\nlooked\nat\nwere\nrepeated.\nFor\ninstance,\n\u201c\nWorld\nBank\nLeader,\nAccused\nof\nClimate\nDenial,\nOffers\na\nNew\nResponse\n\u201d\n(988)\nwas\noriginally\npublished\nby\nThe\nNew\nYork\nTimes\non\nSeptember\n22,\n2022,\nand\nrepublished\nby\nthe\nBoston\nGlobe\n(987)\nthe\nsame\nday,\nthus\nappearing\ntwice\nin\nour\ndata\nset.\nWe\ndecided\nto\ninclude\nrepeated\narticles\nin\nour\nanalyses\ngiven\nthat\nrepublishing\nexposes\nthem\nto\na\nnew\naudience\nand\nstill\nreflects\nthe\nclimate\nchange\ncontent\nthe\noutlet\nis\nwilling\nto\npublish.\nFinally,\nwe\nwould\nlike\nto\nnote\nthat\nsome\nkeywords\nthat\nwere\nused\nto\nclassify\narticles\ninto\nour\nten\nthemes\ncould\nhave\nbeen\nclassified\ndifferently.\nFor\ninstance,\nthe\nterm\ne n e r g y\ncould\napply\nto\nmultiple\ncategories.\nAlthough\nwe\ndecided\nto\nuse\nit\nas\na\nkeyword\nfor\nour\nmining,\nmanufacturing,\nand\nenergy\nproduction\ntheme,\nit\ncould\nhave\nalso\nbeen\nincluded\nin\nthe\nresidential\nor\ntransportation\nthemes.\nWe\ncategorized\nkeywords\nas\naccurately\nas\npossible,\nbut\narticle\nclassifications\nmay\nhave\nlooked\na\nlittle\ndifferent\nhad\ncertain\nkeywords\nbeen\napplied\ndifferently.\nF u t u r e\nD i r e c t i o n s\nWhile\nthis\nstudy\nanswered\nour\nresearch\nquestions,\nit\nalso\ngave\nrise\nto\na\nfew\nnew\nones.\nFor\ninstance,\nit\ncould\nbe\nuseful\nto\nknow\nhow\ncoverage\nof\nanimal\nagriculture\nwith\nrespect\nto\nclimate\nchange\nhas\nchanged\nover\ntime.\nWhile\n7%\ncoverage\nis\nmuch\nlower\nthan\nit\nshould\nbe,\nit\nbegs\nthe\nquestion\nof\nwhether\nthis\nis\nan\nimprovement\ncompared\nto\na\nfew\nyears\nago.\nAdditionally,\na\ncomparative\nmedia\nanalysis\ncould\nbe\nuseful\nto\nget\na\nsense\nof\nhow\ncoverage\nof\nthis\nissue\nmay\ndiffer\nacross\ncountries.\nThis\ncould\nbe\nparticularly\ninformative\nif\nwe\nwere\nto\ncompare\ncountries\nwhere\nanimal\nagriculture\nplays\nan\nimportant\nrole\nin\nthe\nlocal\nculture\nand\neconomy\nversus\ncountries\nwhere\nthis\nis\nless\ntrue.\nFinally,\ngaining\na\nbetter\nunderstanding\nof\nhow\nnews\noutlets\napproach\nthe\nintersection\nof\nfood\nand\nclimate\nchange\nand\nidentifying\nthe\ndecision\nmakers\nwithin\nthe\noutlets\ncould\nprovide\nfurther\nvaluable\ninformation\nregarding\nthe\ntrends\nobserved\nin\nthis\nstudy.\n27\n\nS u p p l e m e n t a r y\nM a t e r i a l s\nA r t i c l e\nE x t r a c t i o n\nAs\nmentioned\nin\nthe\nM e t h o d\nsection,\nwe\nobtained\n1,000\narticles\nfrom\nten\nmajor\nU.S.\nmedia\noutlets,\nwhich\nwere\nselected\nbased\non\nsite\nranking\nand\nweb\ntraffic\ndata.\nTwo\nnews\noutlets\u2014USA\nToday\nand\nNewsday\u2014were\nexcluded\ndue\nto\nlimited\nsearch\nfunctions.\nTo\nselect\narticles,\nwe\nconducted\na\nsearch\nfor\nthe\nkeyword\n\u201cclimate.\u201d\nWe\nthen\nsorted\nby\ndate\nand\nselected\nthe\n100\nmost\nrecent\narticles\nthat\nincluded\nthe\nword\n\u201cclimate\u201d\nin\ntheir\ntitle.\nWe\nused\nthe\nsame\ncutoff\ndate\nof\nSeptember\n29,\n2022\nfor\narticle\nextractions\nfrom\nall\nmedia\noutlets\u2014all\narticles\nhad\nto\nhave\nbeen\npublished\nbefore\nthat\ndate.\nC o m p u t e r - A s s i s t e d\nA r t i c l e\nR e v i e w\nUsing\nthe\nq u a n t e d a\npackage\nfor\nR\nsoftware,\nwe\nsearched\nfor\nthe\nfollowing\nkeywords\nin\nall\narticles.\nA\n\u201c.\u201d\nindicates\nany\ncharacter,\nso\nthe\nword\nmay\nbe\ncompleted\nin\nany\nway\n(e.g.,\nr a n c h .\ncan\nbe\nr a n c h e s\nor\nr a n c h i n g\n).\nKeywords\nwere\ndetermined\nafter\nreviewing\na\nnumber\nof\nexpert\nsources\non\nthe\ncauses\nof\nclimate\nchange\n(\nUN\nClimate\nAction\n,\nNASA\n,\nEPA\n,\nGovernment\nof\nCanada\n,\nIPCC\n,\nEU\n),\ndiscussion\namong\nthe\nstudy\u2019s\nresearchers,\nand\nlater\nadapted\nfollowing\na\nseries\nof\naccuracy\nchecks.\nNote\nthat\nwe\napplied\nexclusions\nto\ncertain\nkeywords\nto\nensure\nthat\narticles\nwere\nproperly\ncategorized.\nFor\nexample,\nthe\nkeyword\n\u201cfarm.\u201d\nexcluded\nthe\nterms\n\u201cfarmer,\u201d\n\u201csolar\nfarm,\u201d\nand\n\u201cwind\nfarm,\u201d\nwhich\nwould\nhave\nmiscategorized\narticles\ninto\nthe\nanimal\nagriculture\ntheme.\nWhile\nthese\nexclusions\nare\nnot\nshown\nhere,\nthey\ncan\nbe\nfound\nin\nthe\ncategorization\ncode\non\nthe\nOpen\nScience\nFramework\n.\nThe\nfinal\nset\nof\nkeywords\nused\nwas\nas\nfollows:\nA n i m a l\nA g r i c u l t u r e \n\u25cf\nmeat \n\u25cf\ndairy \n\u25cf\nrear \n\u25cf\n.husbandry \n\u25cf\nranch \n\u25cf\nfactory\nfarm. \n\u25cf\npasture. \n\u25cf\nrangeland. \n\u25cf\ngrazing\npractices \n\u25cf\nlivestock \n\u25cf\ncattle \n\u25cf\nruminant.\nG e n e r a l\nA g r i c u l t u r e \n\u25cf\nfarm. \n\u25cf\nfarmland \n\u25cf\n.agricultur. \n\u25cf\nhorticulture \n\u25cf\nvineyard \n\u25cf\ncrop\nproduction. \n\u25cf\nplantation. \n\u25cf\ncultivat. \n\u25cf\nharvest \n\u25cf\ngrowing\nseason\nM i n i n g ,\nM a n u f a c t u r i n g\n& \nE n e r g y\nP r o d u c t i o n \n\u25cf\nindustrial \n\u25cf\nmines \n\u25cf\nmining \n\u25cf\noil\nand\ngas \n\u25cf\nfactories \n\u25cf\nfactory \n\u25cf\nmanufactur. \n\u25cf\nfacility \n\u25cf\nfacilities \n\u25cf\nelectricity \n\u25cf\nenergy\n28\n\n\u25cf\ncows \n\u25cf\nsheep \n\u25cf\nlamb \n\u25cf\nlambs \n\u25cf\npoultry \n\u25cf\nchicken\nfarm. \n\u25cf\nchickens \n\u25cf\nhens \n\u25cf\npig \n\u25cf\npigs \n\u25cf\naquaculture \n\u25cf\naquafarm. \n\u25cf\nfish\nfarm \n\u25cf\npisciculture \n\u25cf\nfisheries \n\u25cf\nseafood \n\u25cf\nsalmon\nfarm \n\u25cf\nsalmon\nhatcher. \n\u25cf\ntuna \n\u25cf\nlobster\nfarm. \n\u25cf\nshrimp\nR e s i d e n t i a l \n\u25cf\nresidential \n\u25cf\nbuilding\ncodes \n\u25cf\nhome\nefficiency \n\u25cf\nrefrigera. \n\u25cf\nair\ncondition. \n\u25cf\nappliance. \n\u25cf\nstove \n\u25cf\nfurnace \n\u25cf\ninsulation \n\u25cf\nlight.bulb \n\u25cf\nhome\nenergy \n\u25cf\nheating\nand \ncooling \n\u25cf\nheating\nsystem. \n\u25cf\nrooftop\nsolar\nL a n d\nU s e\nC h a n g e s \n\u25cf\ndeforestation \n\u25cf\nland\nclearing \n\u25cf\nland\nfragment. \n\u25cf\nlogging \n\u25cf\nforests\nare\nlogged \n\u25cf\ntimber \n\u25cf\nurban\nexpansion. \n\u25cf\nroad\nconstruction \n\u25cf\nurban\nsprawl \n\u25cf\nland.use\nchange \n\u25cf\nraze.\nC o n s u m e r i s m \n\u25cf\nconsumerism \n\u25cf\nelectronics \n\u25cf\npackaging \n\u25cf\nsingle.use\nplastic \n\u25cf\nplastic\npollution \n\u25cf\nplastic\nwaste \n\u25cf\nmicroplastic. \n\u25cf\nlandfills \n\u25cf\nshopping \n\u25cf\nblack\nfriday \n\u25cf\nmaterialism \n\u25cf\nfast\nfashion \n\u25cf\nfood\nwaste\nR e g e n e r a t i v e\nA g r i c u l t u r e \n\u25cf\nregenerative. \n\u25cf\ncarbon\nfarming \n\u25cf\nno-till\nfarming \n\u25cf\nagroecological \nfarming \n\u25cf\nsustainable \nagriculture \n\u25cf\nagroforestry \n\u25cf\ntopsoil\nregeneration \n\u25cf\ncrop\nrotation \n\u25cf\ncover\ncrop. \n\u25cf\norganic\nfarming \n\u25cf\ngo\norganic \n\u25cf\ncompost\n\u25cf\npower\nplant\nT r a n s p o r t a t i o n \n\u25cf\ntransportation \n\u25cf\nvehicle. \n\u25cf\ncar \n\u25cf\ncars \n\u25cf\nautomobile. \n\u25cf\ntrucks \n\u25cf\nbuses \n\u25cf\n.plane \n\u25cf\njet \n\u25cf\njets \n\u25cf\naircraft. \n\u25cf\ncruise \n\u25cf\ntraffic \n\u25cf\nexhaust\nemissions \n\u25cf\ncar\nexhaust \n\u25cf\nautos\nE m i s s i o n s \n\u25cf\nemissions \n\u25cf\nmethane \n\u25cf\nCO2 \n\u25cf\ncarbon\ndioxide \n\u25cf\nnitrous\noxide \n\u25cf\nwater\nvap. \n\u25cf\ngreenhouse.gas. \n\u25cf\nair\npollut. \n\u25cf\nozone \n\u25cf\ncarbon\nfootprint \n\u25cf\nsmog\nF o s s i l\nF u e l s \n\u25cf\ncoal \n\u25cf\noil \n\u25cf\nnatural\ngas \n\u25cf\npetroleum \n\u25cf\nfossil.fuel.\n29\n\nA c c u r a c y\nC h e c k s\nTo\ncheck\nthe\naccuracy\nof\nour\nsearch\nmethod,\nwe\nreviewed\nat\nleast\nthree\nrandomly-selected\narticles\nflagged\nwith\neach\ntheme\nto\nensure\nthat\nthey\nreflected\nthat\ncategory\n(e.g.,\narticles\nflagged\nas\ncontaining\nthe\nanimal\nagriculture\ntheme\nactually\ndiscussed\nanimal\nagriculture)\nand\nthat\nthey\ndid\nnot\ncontain\nmentions\nof\nany\nthemes\nthey\nwere\nnot\nflagged\nas\ncontaining\n(e.g.,\narticles\nflagged\nwith\nthe\nanimal\nagriculture\ntheme,\nbut\nnot\nthe\nconsumerism\ntheme,\nshould\nnot\ninclude\ndiscussions\nof\nconsumerism).\nIf\nwe\nfound\ninstances\nof\nmiscategorization,\nwe\nadjusted\nthe\nkeywords\nto\nbetter\ncapture\nthe\nthemes,\nreran\nthe\nanalysis,\nand\nrepeated\nthe\naccuracy\ncheck.\nWe\nrepeated\nthis\nprocess\n12\ntimes,\nat\nwhich\npoint\nwe\nreached\nour\nfinal\nset\nof\nkeywords.\nFollowing\nthe\ncomputer-assisted\nreview\nof\narticles,\nwe\nconducted\na\nfinal\nstage\nof\nmanual\nreview\nof\nany\narticles\nflagged\nas\ncontaining\nthe\nanimal\nagriculture\ntheme.\nIn\nthis\nfinal\nreview,\nwe\nread\nall\nanimal\nagriculture\narticles\nin\ntheir\nentirety\nand\neliminated\nthose\nthat\nhad\nbeen\nmiscategorized\ndespite\ncontaining\nkeywords\npertaining\nto\nthe\ntheme\n(e.g.,\nspace\ndebris\nlanding\non\na\nsheep\nfarm).\nM e d i a\nO u t l e t\nL e a n i n g\nA n d\nA n i m a l\nA g r i c u l t u r e\nWe\nsought\nto\nevaluate\nwhether\nthere\nwas\nan\nassociation\nbetween\nmedia\noutlets\u2019\npolitical\nleaning\nand\nfrequency\nof\nanimal\nagriculture\ncoverage\nin\ntheir\nclimate\narticles.\nTo\ndo\nthis,\nwe\nassigned\na\n1\nor\na\n0\nto\neach\narticle,\ndepending\non\nwhether\nor\nnot\nit\ndiscussed\nanimal\nagriculture\u20141\nif\nit\nmentioned\nanimal\nagriculture,\n0\nif\nit\ndid\nnot.\nFor\npolitical\nleaning,\nwe\ncreated\na\nscale\nfrom\n1\nto\n5,\nwhere\n1\n=\nleft\nbias,\n2\n=\nleft-center\nbias,\n3\n=\nleast\nbiased,\n4\n=\nright-center\nbias,\nand\n5\n=\nright\nbias,\naccording\nto\nratings\nobtained\nfrom\nmediabiasfactcheck.com\n.\nFrom\nmost\npolitically\nleft-leaning\nto\nmost\npolitically\nright-leaning,\nthe\nmedia\noutlets\nare\n(screenshots\nwere\ntaken\non\n11/11/22):\n1.\nCNN\n(left\nbias)\n2.\nThe\nNew\nYork\nTimes\n(left-center\nbias)\n30\n\n3.\nThe\nWashington\nPost\n(left-center\nbias)\n4.\nLos\nAngeles\nTimes\n(left-center\nbias)\n5.\nStar\nTribune\n(left-center\nbias)\n6.\nThe\nBoston\nGlobe\n(left-center\nbias)\n7.\nReuters\n(least\nbiased)\n8.\nChicago\nTribune\n(right-center\nbias)\n31\n\n9.\nThe\nWall\nStreet\nJournal\n(right-center\nbias)\n10.\nNew\nYork\nPost\n(right-center\nbias)\n32\n\nD e t a i l e d\nC o v e r a g e\nB y\nM e d i a\nO u t l e t\nThe\nfollowing\nfigures\nprovide\nan\nin-depth\nview\nof\nhow\noften\neach\nmedia\noutlet\ncovered\nthe\nten\nclimate-related\nthemes.\nChicago\nT ribune\nClimate\narticles\nfrom\nthe\nChicago\nTribune\nranged\nfrom\nSeptember\n2021\nto\nSeptember\n2022.\nF i g u r e\n5 .\nT h e m e s\nC o v e r e d\nb y\nt h e\nC h i c a g o\nT r i b u n e\n33\n\nCNN\nClimate\narticles\nfrom\nCNN\nranged\nfrom\nMay\n2022\nto\nSeptember\n2022.\nF i g u r e\n6 .\nT h e m e s\nC o v e r e d\nb y\nC N N\n34\n\nLos\nAngeles\nT imes\nClimate\narticles\nfrom\nthe\nLos\nAngeles\nTimes\nranged\nfrom\nApril\n2022\nto\nSeptember\n2022.\nF i g u r e\n7 .\nT h e m e s\nC o v e r e d\nb y\nt h e\nL o s\nA n g e l e s\nT i m e s\n35\n\nNew\nY ork\nPost\nClimate\narticles\nfrom\nthe\nNew\nYork\nPost\nranged\nfrom\nNovember\n2021\nto\nSeptember\n2022.\nF i g u r e\n8 .\nT h e m e s\nC o v e r e d\nb y\nt h e\nN e w\nY o r k\nP o s t\n36\n\nReuters\nClimate\narticles\nfrom\nReuters\nranged\nfrom\nAugust\n2022\nto\nSeptember\n2022.\nF i g u r e\n9 .\nT h e m e s\nC o v e r e d\nb y\nR e u t e r s\n37\n\nStar\nT ribune\nClimate\narticles\nfrom\nthe\nStar\nTribune\nranged\nfrom\nFebruary\n2021\nto\nSeptember\n2022.\nF i g u r e\n1 0 .\nT h e m e s\nC o v e r e d\nb y\nt h e\nS t a r\nT r i b u n e\n38\n\nThe\nBoston\nGlobe\nClimate\narticles\nfrom\nThe\nBoston\nGlobe\nranged\nfrom\nJuly\n2022\nto\nSeptember\n2022.\nF i g u r e\n1 1 .\nT h e m e s\nC o v e r e d\nb y\nT h e\nB o s t o n\nG l o b e\n39\n\nThe\nNew\nY ork\nT imes\nClimate\narticles\nfrom\nThe\nNew\nYork\nTimes\nranged\nfrom\nJuly\n2022\nto\nSeptember\n2022.\nF i g u r e\n1 2 .\nT h e m e s\nC o v e r e d\nb y\nT h e\nN e w\nY o r k\nT i m e s\n40\n\nThe\nW all\nStreet\nJournal\nClimate\narticles\nfrom\nThe\nWall\nStreet\nJournal\nranged\nfrom\nJune\n2022\nthrough\nSeptember\n2022.\nF i g u r e\n1 3 .\nT h e m e s\nC o v e r e d\nb y\nT h e\nW a l l\nS t r e e t\nJ o u r n a l\n41\n\nThe\nW ashington\nPost\nClimate\narticles\nfrom\nThe\nWashington\nPost\nranged\nfrom\nAugust\n2022\nto\nSeptember\n2022.\nF i g u r e\n1 4 .\nT h e m e s\nC o v e r e d\nb y\nT h e\nW a s h i n g t o n\nP o s t\n42\n", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Animal agriculture is the missing piece in climate change media coverage", "author": ["C AR\u00c9VALO", "J Anderson"], "pub_year": "2023", "venue": "NA", "abstract": "For many years now, climate researchers have been warning that the world can\u2019t meet its  Paris Agreement climate goals of limiting global warming to 1.5 C without reducing meat"}, "filled": false, "gsrank": 199, "pub_url": "https://www.wellbeingintlstudiesrepository.org/es_ag/1/", "author_id": ["", ""], "url_scholarbib": "/scholar?hl=en&q=info:ZbB2hyL1ji0J:scholar.google.com/&output=cite&scirp=198&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D190%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=ZbB2hyL1ji0J&ei=J7WsaLrrFfnSieoPxKLpgQ0&json=", "num_citations": 2, "citedby_url": "/scholar?cites=3282830707050066021&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:ZbB2hyL1ji0J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.wellbeingintlstudiesrepository.org/cgi/viewcontent.cgi?article=1000&context=es_ag"}}, {"title": "What's in your PIE? Understanding the contents of personalized information environments with PIEGraph", "year": "2024", "pdf_data": "RESEARCH ARTICLE\nWhat's in your PIE? Understanding the contents of\npersonalized information environments with PIEGraph\nDeen Freelon1,2| Meredith L. Pruden3| Daniel Malmer2,4| Qunfang Wu5|\nYiping Xia6| Daniel Johnson2,4| Emily Chen2| Andrew Crist2\n1Annenberg School for Communication,\nUniversity of Pennsylvania, Philadelphia,Pennsylvania, USA\n2Center for Information, Technology, and\nPublic Life, University of North Carolinaat Chapel Hill, Chapel Hill, NorthCarolina, USA\n3School of Communication and Media,\nKennesaw State University, Kennesaw,Georgia, USA\n4Hussman School of Journalism and\nMedia, University of North Carolina atChapel Hill, Chapel Hill, NorthCarolina, USA\n5Inequality in America Initiative, Harvard\nUniversity, Cambridge,Massachusetts, USA\n6Department of Communication &\nJournalism, Texas A&M University,College Station, Texas, USA\nCorrespondence\nDeen Freelon, Annenberg School forCommunication, University ofPennsylvania, Philadelphia, PA 19104,USA.Email:\ndfreelon@upenn.edu\nFunding informationJohn S. and James L. Knight Foundation,Grant/Award Number: G-2019-58787;William and Flora Hewlett Foundation,Grant/Award Number: 2019-9047Abstract\nSocial media have long been studied from platform-centric perspectives, which\nentail sampling messages based on criteria such as keywords and specific\naccounts. In contrast, user-centric approaches attempt to reconstruct the per-\nsonalized information environments users create for themselves. Most user-\ncentric studies analyze what users have accessed directly through browsers\n(e.g., through clicks) rather than what they may have seen in their socialmedia feeds. This study introduces a data collection system of our own design\ncalled PIEGraph that links survey data with posts collected from participants'\npersonalized X (formerly known as Twitter) timelines. Thus, in contrast with\nprevious research, our data include much more than what users decide to click\non. We measure the total amount of data in our participants' respective feeds\nand conduct descriptive and inferential analyses of three other quantities of\ninterest: political content, ideological skew, and fact quality ratings. Ourresults are relevant to ongoing debates about digital echo chambers, misinfor-\nmation, and conspiracy theories; and our general methodological approach\ncould be applied to social media beyond X/Twitter contingent on data\navailability.\n1|INTRODUCTION\nAs digital communication technologies have diffused\nthroughout society, scholarly attention has focused on\nthe contents and influence of what we call personalized\ninformation environments (PIEs). This term refers toautomatically populated digital information spaces\nassembled through expressed user preferences and/oralgorithmic predictions: examples include social media\nfeeds, RSS feeds, lists of subscribed podcasts, and similarapplications. As such environments have become part ofeveryday life for many, concerns have grown about the\nnature and quality of the information that appears within\nthem. One research tradition investigates the extent towhich personalized environments function as echo cham-\nbers: self-reinforcing information feeds that screen outReceived: 1 May 2023 Revised: 19 December 2023 Accepted: 20 December 2023\nDOI: 10.1002/asi.24869\nJ Assoc Inf Sci Technol. 2024;1 \u201315. wileyonlinelibrary.com/journal/asi \u00a9 2024 Association for Information Science and Technology. 1\n\nideologically unfriendly content (Pariser, 2011 ;\nSunstein, 2018 ). Such research usually attempts to ascer-\ntain the ideological balance of political content in the\ndata, with imbalances tilting too far to one side qualifying\nas echo chambers. A more recent research directioninvolves normative questions of information health that\ndistinguish \u2013implicitly or explicitly \u2013between \u201cgood \u201dand\n\u201cbad \u201dtypes of information. Since the US 2016 presiden-\ntial election, much attention has focused on the bad,\nunder conceptual banners including misinformation ,dis-\ninformation ,problematic context ,fake news ,information\npollution , and many others. Such research has under-\ntaken a number of goals, three of the most prominent\nbeing defining the problem (Freelon & Wells,\n2020 ;\nWardle & Derakhshan, 2017 ), mapping its scope\n(Allcott & Gentzkow, 2017 ; Farhall et al., 2019 ), and test-\ning the efficacy of proposed solutions (Walter\net al., 2020 ). Personalized information environments have\nattracted attention across disciplinary lines, with scholars\nfrom information science, computer science, communica-\ntion, political science, public policy, and public healthmaking substantial contributions.\nOur study emerges from a methodological tradition\nthat attempts to understand information health by map-ping the contents of participants' personalized informationenvironments. This user-centric approach departs sharply\nfrom the platform-centric approach that dominates the\nsocial scientific study of social media (Breuer et al.,\n2022;\nChristner et al., 2022;O h m ee ta l . , 2023). Studies of the\nlatter type typically proceed by collecting and analyzing\ndata from one or more social media platforms based onsampling criteria such as keywords, account names, time\nperiods, and/or geographic locations. The platform-centric\napproach is effective at such tasks as identifying influen-cers, determining the prevalence of the phenomena ofinterest, and finding predictors of user engagement, but it\nis not ideal for understanding users' personalized informa-\ntion environments.\nIn contrast, user-centric approaches attempt to recon-\nstruct personalized information environments using\nmethods that require more user interaction than simplyextracting trace data from a social media platform. Using\na combination of surveys and tools that collect data from\nindividual participants' devices or accounts, suchmethods can link opinions and attitudes with media con-\ntent that users consume or opt in to. User-centric\nmethods have grown in popularity in recent years,spurred by the availability of commercial providers ofsuch data (e.g., Comscore and Netquest) and research\nsoftware development by academics (Menchen-\nTrevino,\n2016 ; Ohme et al., 2023 ; Reeves et al., 2021 ). We\ndeveloped PIEGraph, the software system powering the\ncurrent research, to collect user-centric data that isinaccessible to existing applications. While most user-\ncentric research examines the websites or apps partici-pants visit by clicking links or entering web addresses\ndirectly, PIEGraph reconstructs each user's chronological\nX/Twitter timeline based on the unique set of users theyfollow. Within that platform, we argue ours is a more\nfaithful representation of users' information environ-\nments as it incorporates content participants have chosento view, not the small minority of content they have time\nto click on.\nIn this study, we use PIEGraph to conduct an initial\ninvestigation into the personalized information environ-ments of 790 study participants. We are principally con-\ncerned with four distinct quantities: (1) the total amount\nof content, (2) the amount of content classified as\u201cpolitical, \u201d(3) the environments' ideological leanings,\nand (4) their levels of factual quality. We analyze these\nquantities cross-sectionally, longitudinally, and modelthem in regression analyses, with some of our findings\nbeing consistent with prior research and others departing\nfrom it.\n2|HOW PERSONALIZED\nINFORMATION ENVIRONMENTSHAVE BEEN MEASURED\nThe concept of the \u201cinformation ecosystem, \u201dto which\nthis special issue is devoted, encompasses components\nthat govern the production, distribution, and consump-\ntion of such media, including laws, policies, markets,technological design choices, psychological biases, and\nmore (Carter et al.,\n2023 ; Norris & Suomela, 2017 ). This\nstudy investigates one particular part of theinformation ecosystem, which we call the personalized\ninformation environment , which consists of collections of\nmedia units (posts) assembled by an algorithm and/or\nthe user's choices of which accounts to follow(cf \u201ccontent streams, \u201dBayer et al.,\n2020 ; Carter\net al., 2023 ;\u201ccurated flows, \u201dThorson & Wells, 2016 ).\nSuch environments are one of the main ways users inter-act with social media \u2014it is the default view for Face-\nbook, X/Twitter, Instagram, TikTok, and other platforms.\nPIEs determine what users see and do not see within aplatform \u2014they are the individualized universes within\nwhich users decide how to direct their attention.\nIn the fields of communication and political science,\nthe standard method of studying personalized informa-tion environments has been the survey questionnaire.\nRelevant questions vary in their granularity: some ask\nabout broad media types (e.g., radio, TV, newspapers,social media, etc.), while others offer preset or freeform\noptions to indicate interest in specific outlets. Such2 FREELON ET AL .\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nquestions can only capture a general sense of how people\nallocate their attention to media \u2014even when they ask\nusers to state which outlets or sites they visit most often,\nthey can reveal at most a minuscule sliver of their\nrespondents' media diets. Social media platforms presentan especially vexing challenge to this method: knowing\nthat Facebook is the United States' most frequently men-\ntioned \u201ctop site \u201d(Knight Foundation,\n2022 ) tells us little\nabout the content users see when they log on. The topical\ncomposition, political leaning, and amount of misinfor-\nmation (among many other variables) of a user's friendor follower network may vary widely depending on theaccounts therein and the individual-level account settings\nthe user chooses.\nInformation science scholars have long used the\ndiary method to investigate people's information behav-\niors. It entails instructing participants to manually\nrecord details of their media consumption activities ona regular basis using \u201cdiaries, \u201dwhich can incorporate\nmultiple media types. Carter and Mankoff (\n2005 ) wrote\nthat \u201c[t]he diary study is a method of understanding\nparticipant behavior and intent in situ, \u201din which \u201cpar-\nticipants control the timing and means of capture \u201d\n(p. 900, emphasis original). Their participants used anumber of audio, visual, and tangible objects as com-ponents of their media diaries. Carter and Mankoff\nnoted a need for \u201csituated annotation of captured\nevent[s] \u201d(p. 908) through specialized tools. More\nrecent studies have used online tools and mobile appli-\ncations to meet this need. Rieh et al. (\n2010 ) collected\nover 2400 diary entries from 333 respondents to studypeople's credibility assessment of everyday information\non the web, while Narayan et al. (\n2011 ) examined the\nphenomenon of information avoidance in everyday set-tings by qualitatively analyzing diary entries totaling468 participant days.\nScholars from a variety of disciplines have used diary\nmethods to study the consumption of political contentonline. Saltz et al. (\n2021 ) combined in-depth interviews,\ndiary, and co-design methods to investigate Americans'\nperceptions of their information environments and theirattitudes toward platforms' misinformation labels. Moe\nand Ytre-Arne (\n2022 ) argued that diaries enable a kind of\n\u201cinvestigation beyond the moment, and beyond single\nplatforms or providers \u201d(p. 44). For example, Gulyas et al.\n(2019 ) used diaries to study how audiences consume local\nnews online, whereas Mihelj et al. ( 2022 ) combined inter-\nviews with media diaries to study audience engagementwith COVID-19 news. The use of diaries goes beyond\nlearning about news consumption patterns, with Beckers\net al. (\n2021 ) focusing on political learning through the\nnews in their diary study.\nThe diary method's major limitations stem from\nhuman fallibility: it tends to under-capture events thatare infrequent or, alternatively, frequent but last only\nbriefly. Further, because of the labor involved, long-termdiary studies tend to have high attrition rates\n(Vandewater & Lee,\n2009 ). And a recent systematic\nreview found that self-reported media use correlates onlymoderately with automatically logged traces of digital\nactivity (Parry et al.,\n2021 ). Various user-centric methods\nof information environment research have emerged inrecent years to compensate for these limitations.\n3|USER-CENTRIC RESEARCH ON\nSOCIAL MEDIA ENVIRONMENTS\nSince its inception, the study of social media has been\ndominated by platform-centric approaches. Such studies\ncollect data matching certain publicly available criteria,\nusually one or more keywords or authorship by particularsets of users (e.g., all posts within a given timeframe\nauthored by members of Congress). While platform-\ncentric research has been effective in answering a broadrange of research questions, it can tell us little about what\nordinary users encounter when browsing social media.\nChristner et al. (\n2022 ) define user-centric methods as\nthose that \u201ctrac[e] the comprehensive media usage of an\nindividual on the client side \u201d(p. 80). In contrast with\nplatform-centric methods, user-centric approaches\nattempt to see as the user sees at greater resolutions thanare possible with surveys or diaries. Another difference is\nthat, since participants are essentially granting permis-\nsion for their data to be used in research, the researcherstypically obtain informed consent, which is infeasible for\nlarge-N platform-centric studies. User-centric methods\nmay use existing social media APIs, as ours does; scrap-ing techniques that mine cached browser data (Haim &Nienierza,\n2019 ; Menchen-Trevino, 2016 ); or download\nwhatever appears on participants' screens at regular\nintervals (e.g., Reeves et al., 2021 ). We might use a\nplatform-centric approach if we are concerned with how\nmuch misinformation or debunked content exists on\nsocial media (Farhall et al., 2019 ; Ng & Loke, 2021 ). But\nif we want to know how many people are exposed to or\nshare misinformation, we will need to use data on what\ncontent users have selected or viewed \u2014that is, a user-\ncentric approach (Grinberg et al., 2019 ; Guess et al., 2019 ;\nOsmundsen et al., 2021 ). We argue that user-centric per-\nspectives are more applicable to questions of informationquality because misinformation that no one consumes isof little relevance.\nUser-centric research on personalized information\nenvironments has addressed several key questions andtopics. One of the most widely studied areas incorporates\na cluster of theories including selective exposure, political\npolarization, and ideological echo chambers. These kindsFREELON ET AL . 3\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nof studies generally attempt to measure how politically\ndiverse participants' information environments are, basedon the longstanding normative fear that people will use\ndigital affordances to screen out ideologically challenging\ncontent (see, e.g., Sunstein,\n2018 ). Findings reveal such\necho chambers to be fairly rare \u2013most participants have\nideologically diverse news preferences (Bentley\net al., 2019 ; Bruns, 2019 ; Guess, 2021 ; Nelson &\nWebster, 2017 ). Moreover, news constitutes a relatively\nsmall proportion of most people's information diets\n(Cronin et al., 2022 ; Stier et al., 2022 ; Wojcieszak\net al., 2022 ). Research in the COVID-19 era has found\nthat the pandemic has increased attention to the news, as\npeople seek trustworthy scientific, medical, and policy\ninformation to help structure their daily lives (Altayet al.,\n2022 ; Nelson & Lewis, 2022 ). A related collection of\nstudies has focused on the transition of news audiences\nfrom desktop to mobile devices (Fedeli & Matsa, 2018 ;\nNelson & Lei, 2018 ), with mobile news access being asso-\nciated with increased ideological diversity (Yang\net al., 2020 ).\nMisinformation has been another major focus of user-\ncentric research since 2016, when the topic surged onto\nthe social science agenda (Grinberg et al., 2019 ;\nKucharski, 2016 ; Williamson, 2016 ). Existing studies have\nfound misinformation to be rare overall, but concentrated\namong political conservatives and older adults. In partic-\nular, such individuals have been observed to consumeand share misinformation at higher rates than others\n(Allen et al.,\n2020 ; Grinberg et al., 2019 ; Guess\net al., 2019 ). Misinformation shared on social media is\nlikely to be consistent with whatever ideology the sharing\nuser holds (Guess et al., 2021 ). Heavily polarized\nindividuals \u2014those who report strong dislike of political\noutgroup members \u2014have been observed to share misin-\nformation at high rates, with this effect being much\nstronger for Republicans than Democrats (Osmundsen\net al., 2021 ). The audiences of sites dedicated to produc-\ning false \u201cclickbait \u201dcontent have been characterized as\nsmall, disloyal, and chronically online (Nelson &\nTaneja, 2018 ). Conservatives have been shown to be more\nsusceptible to online misinformation specifically concern-\ning COVID-19 (Calvillo et al., 2020 ). But, perhaps coun-\nterintuitively, visiting low-credibility sites is notconsistently related to holding false beliefs (Weeks\net al.,\n2021 ).\n4|OUR USER-CENTRIC\nAPPROACH: PIEGRAPH\nExisting user-centric approaches break down into several\ncategories, each with its own advantages and disadvan-\ntages. Many user-centric researchers purchase data fromcommercial brokers such as Comscore and NetQuest,\nwhich reveal the hyperlinks participants click on. Thisapproach is effective in discovering how participants\nfocus on content of high interest but omits content they\nmay have seen in their social media feeds but not clickedon. Understanding such content is important, especially\ngiven the influence of headlines as sources of information\nabout the world beyond direct experience (Geer &Kahn,\n1993 ; Jan\u00e9t et al., 2022 ). Another technique, called\ndata donation, relies on participants' generosity in export-\ning their own social media data using tools provided bythe platform (Araujo et al.,\n2022 ; Halavais, 2019 ; Ohme\net al., 2023 ; Pfiffner & Friemel, 2023 ). But such donations\ntypically include only messages the participants create\nthemselves rather than those posted by accounts they fol-low. The Screenomics project is unique in using client-\nside software to take screenshots of participants' mobile\nand desktop devices every 5 seconds (Reeves et al.,\n2021 ).\nWhile this powerful technique can collect content from\nany app (including offline ones), the process of convert-\ning data-rich pages into flat images strips away muchcontent of empirical value, including hyperlinks and\nimage URLs.\nTo our knowledge, the approach we adopt with our\nPIEGraph system is unique in the universe of user-centric methods. It uses an opt-in X/Twitter API end-\npoint ( statuses/home_timeline ) that returns all content in\nour participants' feeds \u2014that is, messages by users they\nfollow. Thus, we can reconstruct each of our participants'\npersonalized X/Twitter environments for the entire study\nperiod, with no clicks or screen time required. Our sys-tem's major contribution is to capture complete, user-\nconstructed environments of X/Twitter for analysis, an\naccomplishment unprecedented at the current scale asfar as we are aware. One major difference between PIE-Graph and other user-centric methods is that we have no\nway of knowing how much of their timeline content our\nparticipants have actually viewed. However, we arguethat the content that populates X/Twitter users' timelines\nrepresents the kinds of content our participants are gen-\nerally interested in. In other words, we assume that par-ticipants who have followed (for example) many\nconservative/right-leaning accounts are probably inter-\nested in conservative content regardless of how often theyactually use X/Twitter. Our empirical results support this\nassumption.\n5|RESEARCH QUESTIONS\nOur research questions offer an initial exploration into\nthe data collected through PIEGraph, drawing on existing\nuser-centric research on news, politics, and misinforma-\ntion. We begin with basic questions about the amount of4 FREELON ET AL .\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\ndata across all PIEs (RQ1a and b), proceeding to the\nrespective distributions across participants (RQ2a \u2013c),\nchange over time (RQ3a \u2013c), and survey-derived predic-\ntors (RQ4a \u2013c) for political content, ideological skew, and\nfact quality. Coincidentally, our dataset covers theperiods of time preceding and following Elon Musk's pur-\nchase of X/Twitter, which allows us to analyze the extent\nto which our participants' PIEs changed in the Musk era.\n\u0081RQ1a: What is the total number of messages collected\nacross all personalized information environments?\n\u0081RQ1b: How does the number of messages change\nover time?\n\u0081RQ2a: What are the amount and distribution of politi-\ncal content across all our sample's users?\n\u0081RQ2b: What are the mean ideological skew and distri-\nbution of political content across all our sample's\nusers?\n\u0081RQ2c: What are the mean factual reporting skew and\ndistribution of political content across all our sample's\nusers?\n\u0081RQ3a: How does political content volume change\nover time?\n\u0081RQ3b: How does mean ideological skew change\nover time?\n\u0081RQ3c: How does mean fact quality change over time?\n\u0081RQ4a: Which survey characteristics predict individual\npolitical content volume?\n\u0081RQ4b: Which survey characteristics predict individual\nideological skew?\n\u0081RQ4c: Which survey characteristics predict individual\nfactual reporting skew?\n6|DATA AND METHODS\nWe collected our data using a software system of our own\ndesign called PIEGraph, which differs from the otheruser-centric systems described above both technically\nand conceptually. We began by contracting with the\nresearch firm Dynata to supply a panel of participantsdemographically matched to the US Census (see SI\nsection B in Data\nS1). Each participant was required to\nhold an active X/Twitter account to be eligible for thestudy. They each filled out a survey containing a battery\nof items inquiring about their demographic details, ideo-\nlogical and partisan commitments, media consumptionpreferences, and beliefs in various conspiracy theories(see SI section C in Data\nS1). They were then offered the\nopportunity to grant read-only access to their X/Twitter\ntimelines for the research team. Participants wereinformed that their X/Twitter data would be collected\nand analyzed anonymously. Participants remainedanonymous to us throughout the entire research process,\nas Dynata handled the compensation logistics and gener-ated unique alphanumeric IDs for each participant. Of\nthe 19,021 people who took our survey, 1020 (5.4%) chose\nto offer us access to their X/Twitter timelines. Partici-pants were solicited between December 2021 and May\n2022, and data collection occurred between December\n2021 and April 2023 and concluded on April 4, 2023,when PIEGraph's access to the X/Twitter API was\nrevoked.\nPIEGraph collected and stored complete timeline data\nfor each of these participants (i.e., posts by users they fol-low). Data were collected through X/Twitter's v2 API,\nmaking the process completely invisible to participants.\nInstructions for how to remove oneself from the projectwere posted to the project web site, and 94 participants\ndid so, leaving 926 valid participants at the time of data\nanalysis. Each participant's X/Twitter data was storedalongside their Dynata ID so that we could link it with\ntheir survey data. Participants who completed the survey\nand granted access to their X/Twitter timelines werecompensated $5.\nWe were especially interested in the hyperlinks popu-\nlating our participants' personalized information environ-ments, so we drew on data from the Media Bias FactCheck (MBFC;\nhttps://mediabiasfactcheck.com/ ) project,\nwhich measures the ideological leaning and fact quality\nof over 3400 English-language news and political sites.MBFC measures both ideology and factual reporting on\nunidimensional scales, with the former ranging from\nextreme left to extreme right, and the latter ranging from\u201cVery High \u201dto\u201cVery Low. \u201d\n1Its factual reporting scores\nrepresent subjective evaluations of the sites it indexes. To\nattain the highest score, a site must fulfill the followingcriteria: \u201calways [be] factual, sources to credible informa-\ntion, and makes immediate corrections to incorrect infor-\nmation, and has never failed a fact check in either news\nreporting or op-eds \u201d(Media Bias Fact Check, 2020, n.p.).\nA site that \u201crarely uses credible sources and is not trust-\nworthy for reliable information at all \u201d(Media Bias Fact\nCheck, 2019, n.p.) would receive MBFC's lowest factualreporting score. The site's scores correlate highly with\nalternative domain-based information quality metrics\n(Lin et al.,\n2022 ) and have been used in a number of stud-\nies (Aires et al., 2019 ; Chen et al., 2022 ; Ribeiro\net al., 2018 ). See SI section A in Data S1for additional\ndetails on how we collected and preprocessed the rawMBFC data.\nAfter our X/Twitter data had been collected, we used\na Python script to link each participant's timeline posts\nwith their survey data using their Dynata IDs. Weexcluded all participants whose posts contained fewer\nthan 20 MBFC-classified link instances (so called becauseFREELON ET AL . 5\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nmultiple instances of the same link counted toward the\ntotal), leaving us with a final sample size of n=790 par-\nticipants. For the link instances that appeared in these\nparticipants' PIEs, we calculated mean ideology and fac-tual reporting scores that we use as outcome variables inthe statistical models reported below.\n7|RESULTS\nTo answer RQ1a, we tallied the total number of unique\nposts across all PIEs, which came to 106,929,041.2Indi-\nvidual participants' PIE sizes varied widely \u2014from a mini-\nmum of 0 posts on some days to a maximum of 1,981posts on a single day \u2014depending primarily on how\nmany users each participant followed.\n3Figure 1displays\nthe total number of posts per month starting in May\n2022, the month participant recruitment concluded, andMarch 2023, the final complete month of data collection.\nA general decline begins from August to September,\ntapering off temporarily but resuming from Novemberthrough February with a brief rally from February to\nMarch. Addressing RQ1b, this analysis provides some\nevidence that our participants' PIEs have decreased insize since Elon Musk took control of X/Twitter at the end\nof October 2022. This could be the result of accounts clos-\ning, users going dormant on the platform and/ortrying out platform alternatives like Mastodon, orX/Twitter's spam/bot removal efforts. Other relevant fac-\ntors may include Twitter filing suit against Musk over the\nsale in July, as well as the leaked announcement aroundthe company's issues with information security in\nAugust. While it is outside the scope of this study todetermine which of these variables caused the decline in\nunique post volume, it is likely some combination\nthereof.\nTo answer our cluster of research questions about\nthe respective quantities and distributions of political,ideological, and fact quality content (RQ2a \u2013c), we\nbegin by operationalizing \u201cpolitical content \u201das\ndomains classified by MBFC. This rests on theassumption that the organization has attained compre-\nhensive coverage of most news and political sites that\nare widely known in the United States. Of all uniqueposts, 11,251,711 (10.5%) contained at least one\nMBFC-classified link. Figure\n2shows the distribution\nof MBFC classification proportions across participants,with participants' percentile positions comprising thex-axis. In other words, the chart represents two differ-\nent proportions: that of MBFC-classified links out of\nall of each participant's PIE links ( y-axis), and that of\neach participant ranked in descending order of their\ncorresponding y-axis proportion ( x-axis). It reveals, for\nexample, that participants at the 90th percentile areseeing PIEs in which 25% of links are classified as\n\u201cpolitical \u201dby our definition, and everyone below that\nrank is seeing <25% political links. Such contentdrops very quickly \u2014to about 50% by the 98th percen-\ntile, to 13% by the 75th, to 3% \u20134% by the 50th per-\ncentile (median). Moving to ideological leaning(RQ2b), Figure\n3displays participant percentiles by\nthe mean ideology scores of their MBFC-classified\nlinks. It reveals that our participants' timelines are\nmostly left-leaning, with roughly 20% having a right-of-center ideology mean, and the remaining 80% fall-\ning to the left of center. Finally, Figure\n4plots the\nFIGURE 1 Total unique posts over time.6 FREELON ET AL .\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nparticipant percentile against the individual mean fact\nquality of PIE links (RQ2c). Again, higher y-axis\nvalues correspond to greater degrees of mean fact\nquality. Encouragingly, only participants in the bottom\n3% see mean fact quality scores at or near the lowesttwo levels, Low and Very Low (corresponding to y-\naxis values of 0.2 and 0, respectively). About 90% ofparticipants see mean fact quality values between 0.4\n(Mixed) and 0.8 (High), while the top 6% of PIEs fea-ture mean scores between \u201cHigh \u201dand \u201cVery High \u201d\n(scored as 1).\nRQ3a \u2013c relate to how our three quantities of interest\nchange over time. Figure\n5answers RQ3a, plotting the\naverage monthly proportions of posts containing at least\nFIGURE 2 Distribution of political posts by participant percentile.\nFIGURE 3 Distribution of mean ideology scores by participant percentile.FREELON ET AL . 7\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\none MBFC-coded link out of all posts. Overall, the degree\nof longitudinal change is quite limited apart from a slightdecrease between August and November. Otherwise, theproportions tend to hover close to 0.1. Monthly mean\nideological leaning across all PIEs (Figure\n6; RQ3b)\nshows a similar pattern, with values remaining slightlybelow zero (left of center) after recruitment concludes in\nMay. The decreasing mean ideological leaning over timereflects the fact that more of our left-leaning participants\nentered our sample later during the recruitment process.Similarly, in Figure\n7we see that mean fact quality stabi-\nlizes after May slightly above the \u201cMostly Factual \u201dlevel\n(0.6), addressing RQ3c.\nFigure 8a\u2013canswers RQ4a \u2013c, which inquires about\nthe extent to which individual-level variables can predict\nPIE content. Starting with proportion of political\nFIGURE 4 Distribution of mean fact quality scores by participant percentile.\nFIGURE 5 Average monthly proportions of political posts.8 FREELON ET AL .\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n(MBFC-classified) content, panel 8a shows that the more\nparticipants reported using \u201cmainstream media \u201dand the\nolder they were, the more political content their PIEs\ncontained. Moving to ideological leaning (panel 8b), we\nsee that the more satisfied participants reported beingwith mainstream media, the less conservative content\ntheir feeds contained. In contrast, mainstream media usefrequency, conservative self-identification, strength of\nRepublican Party identification, age, and belief that theJanuary 6th insurrection was exaggerated all predicted\nmore conservative content. Finally, panel 8c reveals that\nfrequency of mainstream media consumption, frequencyof X/Twitter use, and belief that January 6 was exagger-\nated were all associated with lower mean fact quality\nFIGURE 6 Average monthly ideology scores.\nFIGURE 7 Average monthly fact quality scores.FREELON ET AL . 9\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nscores. However, unexpectedly, the belief that Jewish\npeople control the US government and media system pre-\ndicts higher mean fact quality.\n8|DISCUSSION\nOur analyses yield some results consistent with previous\nresearch along with a few surprises. The total number of\nunique posts we found lacks comparable figures in the\nliterature \u2014we cannot know whether it is a relatively\nlarge or small amount compared either to X/Twitter in\nthe past or to other platforms. We can say, however, that\nprevious user-centric studies have found that around 2% \u2013\n3% of content that participants view is political (Croninet al.,\n2022 ; Wojcieszak et al., 2022 ). The fact that over\n10% of our links were classified as politically relevant by\nMBFC indicates that people may be exposed to morepolitical content briefly in their feeds than direct website\nvisits would indicate. We also found that our sample's\nideological mean and distribution lean to the left, whichis consistent with what we know about Twitter's user\nbase (Freelon,\n2019 ; Wojcik & Hughes, 2019 ), and that\nlow-quality content is fairly rare (Allcott &Gentzkow,\n2017 ; Grinberg et al., 2019 ; Guess et al., 2019 ).\nWhile we did not foresee Elon Musk's purchase of\nX/Twitter occurring during our study period, it offeredan opportunity to observe how our participants' environ-ments changed pre- and post-acquisition. Post volume\nacross all timelines began decreasing before the purchase\nand continued after, suggesting that Musk probably wasnot the sole cause of the decrease, although his corporate\ndecisions and rhetoric may have contributed to it. Alonger study period would have helped us determine how\nanomalous the dip in volume starting in August 2022\nwas. Interestingly, none of the three main outcome vari-\nables (political content percentage, mean ideologicalleaning, and mean fact quality) manifested any recogniz-able difference after October 2022. Generally, each quan-\ntity attained stability after all participants had been\nrecruited at the end of May. It seems Musk's purchasedid not result in a marked shift to right-wing or low-\nquality political hyperlinks (although this may result\nfrom the leftward skew of our sample). This is somewhatinconsistent with non-peer-reviewed analyses showing\npost-acquisition increases in hate speech (Center for\nCountering Digital Hate,\n2022 ; Siddiqui & Merrill, 2023 )\nand misinformation (Carniel, 2023 ). More comprehensive\nresearch must be conducted to answer these questions\nconclusively. Broadly, our results highlight the impor-\ntance of extended study periods to document the influ-ence of major changes in platform policy and leadership\n(see, e.g., Cavusoglu et al.,\n2016 ).\nOur statistical models support previous findings link-\ning age, media use frequency, and belief in certain con-\nspiracy theories (but not others) to our outcome\nvariables. Older individuals have long been known toconsume more political and right-wing content than their\nyounger counterparts (Guess et al.,\n2019 ,2021 ;\nOsmundsen et al., 2021 ). However, contrary to these\nstudies, age here does not predict the presence oflow-quality links in information environments. Those\nwho state that they consume mainstream media content\nmore frequently have more political content, more right-wing content, and lower-quality content in their PIEs\nthan those who consume less mainstream content. This\nFIGURE 8 Regression models predicting proportion of MBFC-coded ( \u201cpolitical \u201d) posts, mean ideology, and mean fact quality.\nRegression coefficients are standardized. See SI section C in Data S1for the complete phrasing of these questions. SI section E in Data S1\npresents this data in tabular format.10 FREELON ET AL .\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nis consistent with research showing that people with\nhigher levels of political interest and knowledge may bemore likely to consume and spread misinformation if it\nhelps them defend their opinions (Valenzuela\net al.,\n2019 ). Further, our findings suggest that not all\nconspiracy theories are connected equally strongly to\nright-wing or low-quality sites. Of the four conspiracy\ntheories we included as predictors, only belief in thenotion that the mainstream media exaggerated\nthe January 6 insurrection to disparage Donald Trump\npredicted more right-wing PIE content. This belief alsopredicted lower fact quality, but the antisemitic beliefthat Jewish people control American media and politics\nactually predicted higher fact quality. This may be evi-\ndence of a monitorial impulse among conspiracy andmisinformation consumers \u2014although they dislike and\ndisbelieve what we understand as high-quality media\noutlets, they may consume their output nonetheless todecode and denounce what they perceive as lies\n(cf. Munyaka et al.,\n2022 ) or cite them misleadingly to\nsupport their baseless claims (Aaronovitch, 2010 ).\nOverall, this study offers a thoroughly moderate and\napolitical impression of our participants' X/Twitter infor-\nmation environments. News and politics constitute aminor concern, with 90% of PIE content devoted to othertopics (which we plan on exploring in future research).\nIdeologically, most timelines stick fairly close to the cen-\nter, with very few occupying what we might considerextreme echo chambers (see Bruns,\n2019 ; Guess\net al., 2018 ). In terms of fact quality, the aggregate mean\nhews close to the \u201cmostly factual \u201drating, and extremely\nlow- and high-quality content are both rare. And all these\ntrends hold steady for the better part of a year. Our statis-\ntical models reveal no major asymmetries for any of thedemographic variables we tested. Normatively, this couldbe cause for optimism, as it seems few have opted into\nsubstantial amounts of politically extreme or chronically\nlow-quality content. The statistical relationships wefound between ideological and party self-identification\non the survey and mean ideological leaning of timeline\ncontent suggests that our participants' X/Twitter follow-ing patterns index their offline political identities fairly\nclosely. In other words, it is reasonable to assume that\nthe content people choose to view on X/Twitter probablyresembles what they choose in other media contexts.\nYet our methodological reliance on hyperlinks means\nthat the empirical impression we offer here is incomplete.While lists of unreliable domains and MBFC-styledomain-based ideology and fact quality ratings have been\nused in many studies (Grinberg et al.,\n2019 ; Guess\net al., 2019 ,2021 ), it is possible that important political\ncontent is hiding somewhere beyond these methods' abil-\nity to capture. They account neither for politicaldiscussions that lack hyperlinks, for example, nor for\nvisual or audio content that may be political in nature. Amore comprehensive search for political content would\nbe a logical next step for PIE research, but the necessary\nmethods will be much more labor-intensive than cross-referencing a pre-classified list of web domains. Even if\nwe were to focus exclusively on text, we might need a\nlengthy dictionary of political terms or a supervisedmachine learning model trained on posts classified as\n\u201cpolitical. \u201dWhile hyperlink analysis offers a starting\npoint, future research should broaden its search parame-ters for political content.\nOur study has several other limitations that should be\nacknowledged. First, it is limited to X/Twitter, which\ndoes not represent anyone's complete PIE. Nevertheless,the ideological preferences that emerged in our partici-\npants' X/Twitter timelines were strongly correlated with\ntheir survey answers, suggesting that they are not limitedto X/Twitter. Second, we were not able to determine\nwhat our participants viewed or clicked on, unlike Com-\nscore and Netquest data which show clicks but not thebroader hyperlink universe from which they were drawn.\nIt would be ideal to integrate both types of data in a sin-\ngle study, but we are not aware of any tools capable ofdoing so. Third, while many of the potential participantswe solicited were willing to fill out our survey, only a\nsmall proportion thereof were willing to grant access to\ntheir X/Twitter feeds. While we did not investigate whatfactors might have driven potential participants' decisions\nto participate or not, existing research indicates that pri-\nvacy concerns, perceived relevance of the research, andincentive size may be relevant (Pfiffner & Friemel,\n2023 ;\nSilber et al., 2022 ). We further note that while our final\nsample matches the US Census fairly closely for the mostpart (see discussion in SI section B in Data\nS1), our find-\nings will not necessarily generalize to populations outside\nthe United States.\nAlthough PIEGraph offers one level of understanding\nof our participants' respective X/Twitter feeds, it does not\naddress the question of how X/Twitter's algorithms\nimpact said content. Specifically, because the API weused delivers all content from the set of users each partic-\nipant follows, individual choices to use X/Twitter's \u201cFor\nYou \u201dpage vs. its \u201cFollowing \u201dpage would not affect our\nfindings. Whether this should be considered a limitation\nis debatable \u2014obviously it would be worthwhile to be\nable to distinguish algorithmically-delivered content fromwhat users chose themselves, but X/Twitter has neveroffered this data to outside researchers. While X/Twitter's\nalgorithm curation is unlikely to affect the sets of users\nour participants follow (the present object of study), itprobably influences what they see while using the plat-\nform. Earlier studies have shown that right-leaning USFREELON ET AL . 11\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nmedia outlets were amplified more than left-leaning ones\nby X/Twitter's algorithm (Husz /C19ar et al., 2022 ), and that\nposts containing external links were considerably less\nprevalent in the algorithmic timelines than in the chro-\nnological ones (Bandy & Diakopoulos, 2021 ). Before the\nrecent changes to X/Twitter's API, algorithmic audit\napproaches based on synthetic or \u201csock puppet \u201daccounts\nwere used to collect algorithmic content (Bandy &Diakopoulos,\n2021 ; Bartley et al., 2021 ). However,\nX/Twitter's API policies severely limited the number of\naccounts from which they could collect data \u2014these two\nstudies each employed only eight accounts. Ideally, socialmedia platforms would include algorithmic metadata in\ntheir API output to help determine the effects of algo-\nrithms on the content of an individual's feed, but no APIof which we are currently aware does so.\nMore generally, user-centric approaches, while useful\nfor answering certain research questions, are not a meth-odological panacea. The process of interpreting digital\ntraces as indicators of human intention and attention\nrequires many assumptions, some of which will not holdtrue in all circumstances. For example, any system that\ntracks PIEs or interactive online behavior (clicks, shares,\netc.) can tell us what a user did or might have seen, butthey cannot definitively tell us why. We can say, in otherwords, that a particular piece of content appeared in a\nuser's environment or that they clicked on it (depending\non the system), but we cannot definitively know that theyviewed, read, agreed with, or interpreted it as we would\nhave. Moreover, systems like ours rely on data provided\nby platform companies, which is incomplete in the best-case scenario and to which access can be revoked at any\ntime (see discussion below). Other systems that do not\nrely on such data must use only what can be extractedfrom the webpage's source code or screenshots, whichmay exclude valuable data such as hyperlinks and com-\nments that load automatically as the user scrolls down.\nDespite these limitations, we maintain that user-centricapproaches in general, and ours in particular, can yield\nuseful insights about the content streams people encoun-\nter through social media.\nOur overall approach and findings are relevant to\nmultiple of the broader aspects of the information ecosys-\ntem, particularly government policy and the technologi-cal design choices of social media platforms. The First\nAmendment to the US Constitution broadly limits the\npower of American law to compel social media compa-nies to operate their platforms in any particular manner(Hooker,\n2019 ). Nevertheless, laws passed in Texas and\nFlorida in 2021 attempt to prevent platforms from ban-\nning or otherwise punishing accounts based on theirpolitical viewpoints. The Supreme Court has decided to\nrule on the constitutionality of such measures, butresearch like ours can help reveal the extent to which\n(1) platforms' terms-of-service enforcement and (2) so-called anti-censorship laws affect the visibility of affected\naccounts. In particular, our methods could have tracked\nthe prevalence of accounts known or suspected of spread-ing misinformation or hateful content over time and on a\nstate-by-state basis. Just as the current study notes\na marked decline in content volume after Musk's take-over of X/Twitter, it could have also documented content\nchanges following the implementation of state and fed-\neral laws as well as new platform policies, had the com-pany not decided to place data access beyond the reach ofmost researchers.\nBetween January 2021 and April 2023, X/Twitter\noffered academic researchers free access to 10 millionposts per month with the submission of a brief applica-\ntion. Elon Musk's decision to end this program and\ncharge $100 per month for the lowest tier of data access,which as of this writing offers only 10,000 posts per\nmonth, drastically limits both the kinds of studies that\ncan be rigorously conducted on X/Twitter as well as thepopulation of researchers that can conduct them. To put\nit plainly, while we paid nothing to X/Twitter to produce\nthe research described in this article, PIEGraph is no lon-ger usable without paying at least $5000 per month, andpossibly more. Other platforms have made different deci-\nsions with respect to data availability: while Reddit has\nsharply reduced the utility of its data by requiring short-lived API tokens (Pushshift-Support,\n2023 ), Meta has\nopened access to data about Facebook's public groups\nand pages via a partnership with the University of Michi-gan (Meta,\n2023 ). Social media data access in the\nUnited States is currently subject to the whims of com-\npany leadership, rendering the study of content quality \u2014\nalong with every other social media topic \u2014indefinitely\ntenuous. This, in turn, places a critical aspect of informa-\ntion ecosystem health at existential risk.\nThe importance of research on digital harms such as\nmisinformation has been recognized by the European\nUnion, which directs \u201cvery large online platforms \u201dto\nprovide data to independent researchers \u201cfor the sole pur-\npose of conducting research that contributes to the detec-\ntion, identification, and understanding of systemic risks\nin the Union \u201d(Digital Services Act, 2022 ). We believe a\nstrong case can be made that the research presented here\naddresses a specific \u201csystemic risk \u201das the EU defines it:\nlow-quality information. Therefore, in addition to thisstudy's inherent benefits, it stands as an example of theacknowledged empirical value of social media research\nthat currently lies beyond reach (at least for X/Twitter)\nunder the US's unregulated data access status quo. Thatvalue could be reclaimed with regulation similar to the\nEU's Digital Services Act, which imposes on platforms a12 FREELON ET AL .\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nvery different set of public-minded requirements than the\nFlorida and Texas laws discussed earlier. A crucial step inmoving toward solutions for digital risks, and a healthier\ninformation ecosystem more broadly, is mapping risk\nprevalence, and we believe clear government policy is thebest way to ensure access to social media data (both\nplatform- and user-centric) consistently across platforms.\nFUNDING INFORMATION\nThis research was funded by the John S. and James\nL. Knight Foundation and the William and Flora HewlettFoundation.\nENDNOTES\n1MBFC's ideology scale has 67 possible ordinal categories: /C033\nthrough /C01 for left of center and 1 through 33 for right of center,\nwith 0 lying exactly in the middle. We normalized these to rangebetween /C01 and 1. Its factual reporting scale has six possible ordi-\nnal categories \u2014Very High, High, Mostly Factual, Mixed, Low,\nand Very Low \u2014and we normalized these to range between\n0 and 1.\n2In this count, each unique post ID counts once, where uniquenessis considered across all PIEs. In other words, a single post appear-ing in 10 different participant timelines would count only once.\n3In these counts, uniqueness applies within PIEs, such that a sin-gle post ID would count once for each timeline in which itappeared.\nREFERENCES\nAaronovitch, D. (2010). Voodoo histories: The role of the conspiracy\ntheory in shaping modern history (1st ed.). Riverhead\nHardcover.\nAires, V. P., Nakamura, G., & Nakamura, E. F. (2019). A link-based\napproach to detect media bias in news websites. In Companion\nproceedings of the 2019 world wide web conference, pp. 742 \u2013\n745. https://doi.org/10.1145/3308560.3316460\nAllcott, H., & Gentzkow, M. (2017). Social media and fake news in\nthe 2016 election. Journal of Economic Perspectives ,31(2), 211 \u2013\n236. https://doi.org/10.1257/jep.31.2.211\nAllen, J., Howland, B., Mobius, M., Rothschild, D., & Watts, D. J.\n(2020). Evaluating the fake news problem at the scale of the\ninformation ecosystem. Science Advances ,6(14), eaay3539.\nhttps://doi.org/10.1126/sciadv.aay3539\nAltay, S., Nielsen, R. K., & Fletcher, R. (2022). Quantifying the\n\u201cinfodemic \u201d: People turned to trustworthy news outlets dur-\ning the 2020 coronavirus pandemic. Journal of Quantitative\nDescription: Digital Media ,2.https://doi.org/10.51685/jqd.\n2022.020\nAraujo, T., Ausloos, J., van Atteveldt, W., Loecherbach, F.,\nMoeller, J., Ohme, J., Trilling, D., van de Velde, B., DEVreese, C., & Welbers, K. (2022). OSD2F: An open-sourcedata donation framework. Computational Communication\nResearch ,4(2), 372 \u2013387.\nhttps://doi.org/10.5117/CCR2022.2.\n001.ARAU\nBandy, J., & Diakopoulos, N. (2021). More accounts, fewer links:\nHow algorithmic curation impacts media exposure in twittertimelines. In Proceedings of the ACM on human-computerinteraction, 5(CSCW1), pp. 78:1 \u201378:28.\nhttps://doi.org/10.1145/\n3449152\nBartley, N., Abeliuk, A., Ferrara, E., & Lerman, K. (2021). Auditing\nalgorithmic bias on twitter. In 13th ACM web science confer-ence 2021, pp. 65 \u201373.\nhttps://doi.org/10.1145/3447535.3462491\nBayer, J. B., Tri \u1ec7u, P., & Ellison, N. B. (2020). Social media ele-\nments, ecologies, and effects. Annual Review of Psychology ,\n71(1), 471 \u2013497. https://doi.org/10.1146/annurev-psych-010419-\n050944\nBeckers, K., Van Aelst, P., Verhoest, P., & d'Haenens, L. (2021).\nWhat do people learn from following the news? A diary studyon the influence of media use on knowledge of current news\nstories. European Journal of Communication ,36(3), 254 \u2013269.\nBentley, F., Quehl, K., Wirfs-Brock, J., & Bica, M. (2019). Under-\nstanding online news behaviors. In Proceedings of the 2019CHI conference on human factors in computing systems,pp. 1 \u201311.\nhttps://doi.org/10.1145/3290605.3300820\nBreuer, J., Kmetty, Z., Haim, M., & Stier, S. (2022). User-centric\napproaches for collecting Facebook data in the \u2018post-API age \u2019:\nExperiences from two studies and recommendations for future\nresearch. Information, Communication & Society , 26(14),\n2649\u20132668. https://doi.org/10.1080/1369118X.2022.2097015\nBruns, A. (2019). Are filter bubbles real? John Wiley & Sons.\nCalvillo, D. P., Ross, B. J., Garcia, R. J. B., Smelter, T. J., &\nRutchick, A. M. (2020). Political ideology predicts perceptions\nof the threat of COVID-19 (and susceptibility to fake news\nabout it). Social Psychological and Personality Science ,11, 1119 \u2013\n1128. https://doi.org/10.1177/1948550620940539\nCarniel, B. (2023). Misinformation superspreaders are thriving on\nMusk-owned Twitter. Health Feedback. https://healthfeedback.\norg/misinformation-superspread ers-thriving-on-musk-owned-\ntwitter/\nCarter, M. C., Cingel, D. P., Ruiz, J. B., & Wartella, E. (2023). Social\nmedia use in the context of the personal social media ecosystemframework. Journal of Communication ,73(1), 25 \u201337.\nhttps://\ndoi.org/10.1093/joc/jqac038\nCarter, S., & Mankoff, J. (2005). When participants do the captur-\ning: The role of media in diary studies. In Proceedings of the\nSIGCHI conference on human factors in computing systems,\npp. 899 \u2013908. https://doi.org/10.1145/1054972.1055098\nCavusoglu, H., Phan, T. Q., Cavusoglu, H., & Airoldi, E. M. (2016).\nAssessing the impact of granular privacy controls on contentsharing and disclosure on Facebook. Information Systems\nResearch ,27(4), 848 \u2013879.\nhttps://doi.org/10.1287/isre.2016.0672\nCenter for Countering Digital Hate. (2022). The Musk Bump:\nQuantifying the rise in hate speech under Elon Musk. https://\ncounterhate.com/blog/the-musk-bump-quantifying-the-rise-in-hate-speech-under-elon-musk/\nChen, M., Chu, X., & Subbalakshmi, K. P. (2022). MMCoVaR: Mul-\ntimodal COVID-19 vaccine focused data repository for fakenews detection and a baseline architecture for classification. In\nProceedings of the 2021 IEEE/ACM international conference\non advances in social networks analysis and mining, pp. 31 \u201338.\nhttps://doi.org/10.1145/3487351.3488346\nChristner, C., Urman, A., Adam, S., & Maier, M. (2022). Automated\ntracking approaches for studying online media use: A criticalreview and recommendations. Communication Methods and\nMeasures ,16(2), 79 \u201395.\nCronin, J., von Hohenberg, B. C., Gon\u00e7alves, J. F. F., Menchen-\nTrevino, E., & Wojcieszak, M. (2022). The (null) over-timeeffects of exposure to local news websites: Evidence from traceFREELON ET AL . 13\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\ndata. Journal of Information Technology & Politics ,20(4),\n407\u2013421. https://doi.org/10.1080/19331681.2022.2123878\nDigital Services Act. (2022). \u00a7Article 40, Section 4. https://www.eu-\ndigital-services-act.com/Digital_Services_Act_Article_40.html\nFarhall, K., Carson, A., Wright, S., Gibbons, A., & Lukamto, W.\n(2019). Political elites' use of fake news discourse across com-\nmunications platforms. International Journal of Communica-\ntion,13,4353\u20134375.\nFedeli, S., & Matsa, K. E. (2018). Use of mobile devices for news con-\ntinues to grow, outpacing desktops and laptops . Pew Research\nCenter.\nFreelon, D. (2019). Tweeting left, right, & center: How users and\nattention are distributed across twitter (pp. 1 \u201338). John S. &\nJames L. Knight Foundation.\nFreelon, D., & Wells, C. (2020). Disinformation as political commu-\nnication. Political Communication ,37(2), 145 \u2013156. https://doi.\norg/10.1080/10584609.2020.1723755\nGeer, J. G., & Kahn, K. F. (1993). Grabbing attention: An experi-\nmental investigation of headlines during campaigns. Political\nCommunication ,10(2), 175 \u2013191.\nGrinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B., &\nLazer, D. (2019). Fake news on twitter during the 2016U.S. presidential election. Science ,363(6425), 374 \u2013378.\nhttps://\ndoi.org/10.1126/science.aau2706\nGuess, A. M. (2021). (Almost) everything in moderation: New evi-\ndence on Americans' online media diets. American Journal of\nPolitical Science ,65(4), 1007 \u20131022. https://doi.org/10.1111/ajps.\n12589\nGuess, A. M., Aslett, K., Tucker, J., Bonneau, R., & Nagler, J.\n(2021). Cracking open the news feed: Exploring whatU.S. Facebook users see and share with large-scale platformdata. Journal of Quantitative Description: Digital Media ,1.\nhttps://doi.org/10.51685/jqd.2021.006\nGuess, A. M., Nagler, J., & Tucker, J. (2019). Less than you think:\nPrevalence and predictors of fake news dissemination on Face-book. Science Advances ,5(1), eaau4586.\nhttps://doi.org/10.\n1126/sciadv.aau4586\nGuess, A. M., Nyhan, B., Lyons, B., & Reifler, J. (2018). Avoiding\nthe echo chamber about echo chambers. Knight Foundation ,2,\n1\u201325.\nGulyas, A., O'Hara, S., & Eilenberg, J. (2019). Experiencing local\nnews online: Audience practices and perceptions. Journalism\nStudies ,20(13), 1846 \u20131863. https://doi.org/10.1080/1461670X.\n2018.1539345\nHaim, M., & Nienierza, A. (2019). Computational observation:\nChallenges and opportunities of automated observation withinalgorithmically curated media environments using a browserplug-in. Computational Communication Research ,1(1), 79 \u2013102.\nhttps://doi.org/10.5117/CCR2019.1.004.HAIM\nHalavais, A. (2019). Overcoming terms of service: A proposal for\nethical distributed research. Information, Communication &\nSociety ,22(11), 1567 \u20131581. https://doi.org/10.1080/1369118X.\n2019.1627386\nHooker, M. P. (2019). Censorship, Free Speech & Facebook: Apply-\ning the first amendment to social media platforms via the pub-lic function exception. Washington Journal of Law,\nTechnology & Arts ,15(1), 36 \u201373.\nHusz/C19ar, F., Ktena, S. I., O'Brien, C., Belli, L., Schlaikjer, A., &\nHardt, M. (2022). Algorithmic amplification of politics on twit-ter.Proceedings of the National Academy of Sciences ,119(1),\ne2025334119.\nhttps://doi.org/10.1073/pnas.2025334119Jan\u00e9t, K., Richards, O., & Landrum, A. R. (2022). Headline format\ninfluences evaluation of, but not engagement with, environ-mental news. Journalism Practice ,16(1), 35 \u201355.\nhttps://doi.org/\n10.1080/17512786.2020.1805794\nKnight Foundation. (2022). Media and democracy: Unpacking\nAmerica's complex views on the digital Public Square. https://\nknightfoundation.org/reports/media-and-democracy/\nKucharski, A. (2016). Study epidemiology of fake news. Nature ,\n540(7634), 7634. https://doi.org/10.1038/540525a\nLin, H., Lasser, J., Lewandowsky, S., Cole, R., Gully, A.,\nRand, D., & Pennycook, G. (2022). High level of agreementacross different news domain quality ratings. PsyArXiv .\nhttps://\ndoi.org/10.31234/osf.io/qy94s\nMenchen-Trevino, E. (2016). Web historian: Enabling multi-\nmethod and independent research with real-world web brows-ing history data. In iConference 2016 proceedings.\nhttp://hdl.\nhandle.net/2142/89388\nMeta. (2023). Meta content library and API jtransparency center.\nhttps://transparency.fb.com/researchtools/meta-content-\nlibrary/\nMihelj, S., Kondor, K., & \u02c7St\u02c7etka, V. (2022). Audience engage-\nment with COVID-19 news: The impact of lockdown andlive coverage, and the role of polarization. Journalism Stud-\nies,23(5\u20136), 569 \u2013587.\nhttps://doi.org/10.1080/1461670X.2021.\n1931410\nMoe, H., & Ytre-Arne, B. (2022). The democratic significance of\neveryday news use: Using diaries to understand public con-nection over time and beyond journalism. Digital Journal-\nism,10(1), 43 \u201361.\nhttps://doi.org/10.1080/21670811.2020.\n1850308\nMunyaka, I., Hargittai, E., & Redmiles, E. (2022). The misinforma-\ntion paradox: Older adults are cynical about news media, but\nengage with it anyway. Journal of Online Trust and Safety ,1(4),\n4.https://doi.org/10.54501/jots.v1i4.62\nNarayan, B., Case, D. O., & Edwards, S. L. (2011). The role of infor-\nmation avoidance in everyday-life information behaviors. Pro-\nceedings of the American Society for Information Science andTechnology ,48(1), 1 \u20139.\nhttps://doi.org/10.1002/meet.2011.\n14504801085\nNelson, J. L., & Lei, R. F. (2018). The effect of digital platforms on\nnews audience behavior. Digital Journalism ,6(5), 619 \u2013633.\nhttps://doi.org/10.1080/21670811.2017.1394202\nNelson, J. L., & Lewis, S. C. (2022). The structures that shape news\nconsumption: Evidence from the early period of the COVID-19\npandemic. Journalism ,23, 2495 \u20132512. https://doi.org/10.1177/\n14648849221095335\nNelson, J. L., & Taneja, H. (2018). The small, disloyal fake news\naudience: The role of audience availability in fake news con-sumption. New Media & Society ,20, 3720 \u20133737.\nhttps://doi.org/\n10.1177/1461444818758715\nNelson, J. L., & Webster, J. G. (2017). The myth of partisan selective\nexposure: A portrait of the online political news audience.\nSocial Media +Society ,3(3).https://doi.org/10.1177/2056305\n117729314\nNg, L. H. X., & Loke, J. Y. (2021). Analyzing public opinion and\nmisinformation in a COVID-19 telegram group chat. IEEE\nInternet Computing ,25(2), 84 \u201391.https://doi.org/10.1109/MIC.\n2020.3040516\nNorris, T. B., & Suomela, T. (2017). Information in the ecosystem:\nAgainst the \u201cinformation ecosystem \u201d.First Monday, 22 (9).\nhttps://doi.org/10.5210/fm.v22i9.684714 FREELON ET AL .\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nOhme, J., Araujo, T., Boeschoten, L., Freelon, D., Ram, N.,\nReeves, B. B., & Robinson, T. N. (2023). Digital trace data col-lection for social media effects research: APIs, data donation,and (screen) tracking. Communication Methods and Measures ,\n1\u201318.\nhttps://doi.org/10.1080/19312458.2023.2181319\nOsmundsen, M., Bor, A., Vahlstrup, P. B., Bechmann, A., &\nPetersen, M. B. (2021). Partisan polarization is the primary psy-\nchological motivation behind political fake news sharing ontwitter. American Political Science Review ,115(3), 999 \u20131015.\nhttps://doi.org/10.1017/S0003055421000290\nPariser, E. (2011). The filter bubble: What the internet is hiding from\nyou. Penguin.\nParry, D. A., Davidson, B. I., Sewall, C. J. R., Fisher, J. T.,\nMieczkowski, H., & Quintana, D. S. (2021). A systematic reviewand meta-analysis of discrepancies between logged and self-reported digital media use. Nature Human Behaviour ,5(11), 11.\nhttps://doi.org/10.1038/s41562-021-01117-5\nPfiffner, N., & Friemel, T. N. (2023). Leveraging data donations for\ncommunication research: Exploring drivers behind the willing-\nness to donate. Communication Methods and Measures ,17(3),\n227\u2013249. https://doi.org/10.1080/19312458.2023.2176474\nPushshift-Support. (2023). Pushshift live again and how moderators\ncan request Pushshift access [Reddit post]. R/Pushshift. www.\nreddit.com/r/pushshift/comments/14ei799/pushshift_live_again_and_how_moderators_can/\nReeves, B., Ram, N., Robinson, T. N., Cummings, J. J., Giles, C. L.,\nPan, J., Chiatti, A., Cho, M., Roehrick, K., Yang, X.,Gagneja, A., Brinberg, M., Muise, D., Lu, Y., Luo, M.,Fitzgerald, A., & Yeykelis, L. (2021). Screenomics: A frameworkto capture and analyze personal life experiences and the waysthat technology shapes them. Human \u2013Computer Interaction ,\n36(2), 150 \u2013201.\nhttps://doi.org/10.1080/07370024.2019.1578652\nRibeiro, F., Henrique, L., Benevenuto, F., Chakraborty, A.,\nKulshrestha, J., Babaei, M., & Gummadi, K. (2018). Media biasmonitor: Quantifying biases of social media news outlets atlarge-scale. Proceedings of the International AAAI Conference on\nWeb and Social Media ,12(1), 290 \u2013299.\nRieh, S. Y., Kim, Y.-M., Yang, J. Y., & St Jean, B. (2010). A diary\nstudy of credibility assessment in everyday life information\nactivities on the web: Preliminary findings. Proceedings of the\nAmerican Society for Information Science and Technology ,47(1),\n1\u201310.\nhttps://doi.org/10.1002/meet.14504701182\nSaltz, E., Leibowicz, C. R., & Wardle, C. (2021). Encounters with\nvisual misinformation and labels across platforms: An inter-\nview and diary study to inform ecosystem approaches to misin-\nformation interventions. In Extended abstracts of the 2021 CHIconference on human factors in computing systems, pp. 1 \u20136.\nhttps://doi.org/10.1145/3411763.3451807\nSiddiqui, F., & Merrill, J. B. (2023). Elon Musk's twitter pushes hate\nspeech, extremist content into \u2018for you \u2019pages. Washington\nPost. https://www.washingtonpost.com/technology/2023/03/\n30/elon-musk-twitter-hate-speech/\nSilber, H., Breuer, J., Beuthner, C., Gummer, T., Keusch, F.,\nSiegers, P., Stier, S., & Wei\u00df, B. (2022). Linking surveys and dig-ital trace data: Insights from two studies on determinants ofdata sharing behaviour. Journal of the Royal Statistical Society\nSeries A: Statistics in Society ,185(suppl 2), S387 \u2013S407.\nhttps://\ndoi.org/10.1111/rssa.12954\nStier, S., Mangold, F., Scharkow, M., & Breuer, J. (2022). Post post-\nbroadcast democracy? News exposure in the age of onlineintermediaries. American Political Science Review ,116(2), 768 \u2013\n774. https://doi.org/10.1017/S0003055421001222\nSunstein, C. (2018). #Republic: Divided democracy in the age of\nsocial media . Princeton University Press.\nThorson, K., & Wells, C. (2016). Curated flows: A framework for\nmapping media exposure in the digital age. Communication\nTheory ,26(3), 309 \u2013328. https://doi.org/10.1111/comt.12087\nValenzuela, S., Halpern, D., Katz, J. E., & Miranda, J. P. (2019). The\nparadox of participation versus misinformation: Social media,political engagement, and the spread of misinformation. Digital\nJournalism ,7(6), 802 \u2013823.\nhttps://doi.org/10.1080/21670811.\n2019.1623701\nVandewater, E. A., & Lee, S.-J. (2009). Measuring children's media\nuse in the digital age: Issues and challenges. American Behav-\nioral Scientist ,52, 1152 \u20131176. https://doi.org/10.1177/\n0002764209331539\nWalter, N., Cohen, J., Holbert, R. L., & Morag, Y. (2020). Fact-\nchecking: A meta-analysis of what works and for whom. Politi-\ncal Communication ,37(3), 350 \u2013375.\nWardle, C., & Derakhshan, H. (2017). Information disorder: Toward\nan interdisciplinary framework for research and policymaking(Vol. 27). Council of Europe Strasbourg.\nWeeks, B. E., Menchen-Trevino, E., Calabrese, C., Casas, A., &\nWojcieszak, M. (2021). Partisan media, untrustworthy newssites, and political misperceptions. New Media & Society ,\n25(10), 2644 \u20132662.\nhttps://doi.org/10.1177/14614448211033300\nWilliamson, P. (2016). Take the time and effort to correct misinfor-\nmation. Nature ,540(7632), 171. https://doi.org/10.1038/\n540171a\nWojcieszak, M., Clemm von Hohenberg, B., Casas, A., Menchen-\nTrevino, E., de Leeuw, S., Gon\u00e7alves, A., & Boon, M. (2022).Null effects of news exposure: A test of the (un)desirable effects\nof a \u2018news vacation \u2019and \u2018news binging \u2019.Humanities and Social\nSciences Communications ,9(1), 1 \u201310.\nhttps://doi.org/10.1057/\ns41599-022-01423-x\nWojcik, S., & Hughes, A. (2019). How twitter users compare to the\ngeneral public. Pew Research Center. https://www.\npewinternet.org/2019/04/24/sizing-up-twitter-users/\nYang, T., Maj /C19o-V/C19azquez, S., Nielsen, R. K., & Gonz /C19alez-Bail /C19on, S.\n(2020). Exposure to news grows less fragmented with anincrease in mobile access. Proceedings of the National Academy\nof Sciences ,117(46), 28678 \u201328683.\nhttps://doi.org/10.1073/pnas.\n2006089117\nSUPPORTING INFORMATION\nAdditional supporting information can be found online\nin the Supporting Information section at the end of this\narticle.\nHow to cite this article: Freelon, D., Pruden,\nM. L., Malmer, D., Wu, Q., Xia, Y., Johnson, D.,Chen, E., & Crist, A. (2024). What's in your PIE?\nUnderstanding the contents of personalized\ninformation environments with PIEGraph. Journal\nof the Association for Information Science andTechnology ,1\u201315.\nhttps://doi.org/10.1002/asi.24869FREELON ET AL . 15\n 23301643, 0, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24869 by Harvard University, Wiley Online Library on [28/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "What's in your PIE? Understanding the contents of personalized information environments with PIEGraph", "author": ["D Freelon", "ML Pruden", "D Malmer", "Q Wu"], "pub_year": "2024", "venue": "Journal of the \u2026", "abstract": "Social media have long been studied from platform\u2010centric perspectives, which entail sampling  messages based on criteria such as keywords and specific accounts. In contrast, user\u2010"}, "filled": false, "gsrank": 200, "pub_url": "https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.24869", "author_id": ["EjVT0LAAAAAJ", "s0moKm4AAAAJ", "K7f0V0kAAAAJ", "8m1Q-voAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:PFXyXu5OqFgJ:scholar.google.com/&output=cite&scirp=199&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D190%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=PFXyXu5OqFgJ&ei=J7WsaLrrFfnSieoPxKLpgQ0&json=", "num_citations": 2, "citedby_url": "/scholar?cites=6388442857127171388&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:PFXyXu5OqFgJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://qunfangwu.com/files/whats_freelon.pdf"}}, {"title": "Wikipedia Citations: Reproducible Citation Extraction from Multilingual Wikipedia", "year": "2024", "pdf_data": "Noname manuscript No.\n(will be inserted by the editor)\nWikipedia Citations: Reproducible Citation Extraction from\nMultilingual Wikipedia\nNatallia Kokash* \u00b7Giovanni Colavizza\nMay 2024\nAbstract Wikipedia is an essential component of the\nopen science ecosystem, yet it is poorly integrated with\nacademic open science initiatives. Wikipedia Citations\nis a project that focuses on extracting and releasing\ncomprehensive datasets of citations from Wikipedia. A\ntotal of 29.3 million citations were extracted from En-\nglish Wikipedia in May 2020. Following this one-off re-\nsearch project, we designed a reproducible pipeline that\ncan process any given Wikipedia dump in the cloud-\nbasedsettings.Todemonstrateitsusability,weextracted\n40.6 million citations in February 2023 and 44.7 million\ncitations in February 2024. Furthermore, we equipped\nthe pipeline with an adapted Wikipedia citation tem-\nplatetranslationmoduletoprocessmultilingualWikipedia\narticlesin15Europeanlanguagessothattheyareparsed\nandmappedintoagenericstructuredcitationtemplate.\nThis paper presents our open-source software pipeline\nto retrieve, classify, and disambiguate citations on de-\nmand from a given Wikipedia dump.\n1 Introduction\nAcitationis a reference to a source such as a book, ar-\nticle, web page, or other published material that helps\nauthors to support an argument, statement, fact, or ac-\nknowledge prior relevant discussions through their own\nintellectual work. Citations are an indispensable part\n*Corresponding author\nN. Kokash\nUniversity of Amsterdam, Kloveniersburgwal 48, 1012 CX Ams-\nterdam, The Netherlands E-mail: natallia.kokash@gmail.com\nG. Colavizza\nDepartment of Classical Philology and Italian Studies, University\nof Bologna, Italy E-mail: giovanni.colavizza@unibo.itof scientific publications as they help to trace the ori-\ngins and evolution of ideas. Academic researchers take\nwhat is currently known and create new knowledge. In\nthis process, citing sources is important for accumu-\nlating current state-of-the-art, emphasizing and getting\ncredit for novel contributions, avoiding repetitive argu-\nments and repetitive efforts, and helping anyone verify\nand validate the research results presented in any given\nscientific work.\nIn the era of information explosion, we realized that\nscientific knowledge is hard to locate and process unless\nit is collected in online databases with additional search\ncapabilities. Citation databases [4,28] are collections of\nreferenced papers, articles, books, and other materials\nentered into an online system in a structured and con-\nsistent way. All the information relating to a single doc-\nument (author, title, publication details, abstract, and\nperhaps the full text) makes up the entry record for\nthatdocument.Moreover,suchrecordsareconnectedto\nknowledge graphs that facilitate the search of relevant\nworks, measuring the impact of a paper, an author or\na journal, etc. General purpose citation databases such\nas Web of Science, Scopus, or Google Scholar cover a\nlarge number of recently published works. Field-specific\ndatabases such as PubMed and MathSciNet provide\nrecords on works from the most reputable journals in\nthe respected fields.\nCitations are increasingly used as performance in-\ndicators in research policy and within the research sys-\ntem. Yet, even with the presence of citation databases,\nthe majority of scientific knowledge from specialized\nsources remains unavailable to the general public. The\nmajority of people rely on search engines to find rele-\nvantinformation.OpenonlinesourcessuchasWikipedia\nplay a fundamental role in providing factual informa-\ntion on the Web. Wikipedia is one of the most vis-arXiv:2406.19291v1  [cs.DL]  27 Jun 2024\n2 N. Kokash, G. Colavizza\nited websites in the world and is perceived as a reliable\nsource of knowledge even for scientific research.\nIn the scientific world researchers and authors gen-\nerally follow well-established practices of citing rele-\nvant academic papers. Citations in other types of liter-\nature, and in particular, Wikipedia articles, may vary\nin quality and scope. The Wikipedia content quality,\nincluding the quality of its references, has been scru-\ntinized in a number of previous studies [29,47,48] and\nremains the subject of interest. However, any exten-\nsive study based on Wikipedia citations quickly be-\ncomes obsolete due to the constant evolution of its ar-\nticles. Considering a large volume of periodically up-\ndated pages and attributes, we released a Wikipedia\nCitations extraction pipeline that allows anyone inter-\nested to obtain a comprehensive dataset of citations ex-\ntracted from a chosen Wikipedia dump in a structured,\nunified and classified form. Our work is not limited to\nEnglish Wikipedia, the pipeline includes a translation\nstep that helps researchers and citation consolidation\nbodies to convert various citation templates from Wi-\nkicode into a common English-based data frame ready\nfor automated processing and analysis.\nA crucial question to ask to improve Wikipedia\u2019s\nverifiability standards, as well as to better understand\nits dominant role as a source of information, is the fol-\nlowing: What sources are cited in Wikipedia? Our work\nis a direct continuation of the effort to extract and ana-\nlyze a full set of English Wikipedia citations performed\nby Singh et al. [35]. The datasets extracted and clas-\nsified in this work were used in a number of follow-\nup projects. Unlike previous Wikipedia-based citation\nstudies, our goal was not to produce a one-time sta-\ntistical analysis of a given snapshot but to develop a\npipeline for any interested researcher to extract parsed\ncitations mapped into a common template and enable\nWikipedia citation integration with Open Science in-\nfrastructure.\nAnswering the question of what exactly is cited in\nWikipedia is challenging for a variety of reasons. First\nof all, editorial practices are not uniform, in partic-\nular across different language versions of Wikipedia.\nSecondly, while some citations contain stable identi-\nfiers (e.g., DOIs), others do not. As previous studies\nshow [26], and we confirm in this paper, only a low\nfraction of all citation templates, e.g., around 5% of\ncitations in English Wikipedia, include DOIs.\nWe expand upon previous work [35] by presenting\na codebase for collecting citations from various lan-\nguage editions of Wikipedia. The extracted citations\nare translated to fit a common English schema and are\ncategorized to identify reliable citation sources such as\nscientific journals, books, and news. We also present animproved lookup procedure to augment citations with-\nout identifiers if similar references are found in citation\ndatabases.\nThis article is organized as follows. We start by de-\nscribing our pipeline focusing on its three main steps:\ncitation template harmonization, classification, and ci-\ntation identifier lookup. We subsequently provide a de-\nscription of the published data sets and compare our\nfindingswithsimilarWikipediacitationevaluationmeth-\nods.\n2 Workflow overview\nGiven a Wikipedia dump, our pipeline [15] retrieves ci-\ntationsandharmonizescitationtemplates,i.e.,converts\ncitations into a format that matches the same schema.\nOur work adopts the approach first developed by Singh\net al. [35]. Figure 1 explains the technical representa-\ntion of Wikipedia citations. A citation in Wikipedia is\nan abbreviated alphanumeric expression in the form of\na Web link formatted as a subscript next to the rele-\nvant content of an article. The link directs a reader to\na References section where a full textual citation iden-\ntifying the source of the information is provided. The\ncitation text format depends on the essence of the cited\nresource (book, journal, web page, etc.) and is rendered\nautomatically from a citation template chosen by the\narticle author. A Wikipedia template is a page that is\nembedded into other pages to allow for the repetition\nof information. Citation templates are reusable pages\nspecifically defined to embed citations. They can be in-\ncluded in Wikicode (also known as Wikitext or Wiki\nmarkup), the markup language used towrite Wikipedia\npages, via double curly braces.\nFigure 2 shows the basic steps of the citation ex-\ntraction pipeline. XML Wikipedia dumps [7] provide\nraw Wikicode in which we locate all templates by ex-\ntracting the content of double curly braces {{...}}and\n\u27e8ref\u27e9tags. From these templates, we extract a tem-\nplate name and filter out templates unrelated to cita-\ntion data. Wikipedia citation templates cover a much\nwider range of sources than those common for scien-\ntific literature, e.g., citing speeches, interviews, pod-\ncasts, press releases, court decisions, etc. For English\nWikipedia,weprocessnearly30citationtemplatetypes[40]\nwhich we parse with the extended version of the Medi-\naWikiparser[22].Initially,theparseronlysupported17\ncitation templates; support for an additional 18 of the\nmost frequently used English templates was added [36].\nThe resulting uniform key-value data set can easily\nbe transformed into a tabular form for further process-\ning. In particular, extracted in such a manner datasets\nwith identifiers serve as input for our classification and\nWikipedia Citations: Reproducible Citation Extraction from Multilingual Wikipedia 3\nFig. 1: Wikipedia citation representation [35].\nlookup scripts, as well as are publicly released for any-\none to analyze.\nThe choice of a citation template by a Wikipedia ar-\nticle author cannot fully represent the nature of a cited\nsource. Many templates such as citation,cite web, or\ntheir analogues in other languages are very generic and\ncan be used to refer to scientific articles or arbitrary\nweb pages with potentially non-factual content. Many\nWikipediacontentqualityevaluationstudiesrelyonthe\ndistinctionbetweencitationstoscientificliteraturesuch\nas journal articles and conference proceedings or other\ncontent. To simplify such studies, we classify extracted\ncitations equipped with known identifiers or URLs, la-\nbeling them as \u201cjournal\u201d, \u201cbook\u201d, \u201cnews\u201d, or \u201cother\u201d. Fur-\nthermore, part of the citations which are likely to refer\nto books or journals (i.e., defined using corresponding\nWikipediatemplates)areaugmentedwithidentifierslo-\ncatedviaGoogleBooksandCrossrefAPIs.Thesesteps,\ncitation data classification andcitation data lookup , are\nshown in Figure 2 and discussed in more detail in the\nnext section.\nWikipedia is represented in 329 languages. Its au-\nthors can cite sources using either language-specific or\nEnglish templates. Our main effort in releasing multi-\nlingual datasets was to assemble lists of citation tem-\nplates for each language and convert relevant fields into\na common English template. We started with known ci-\ntationtemplatesperlanguage(typicallycoveringbooks,\njournals,webpages,andnews),and,insomecases,aug-\nmented these lists with additional frequently used tem-\nplates (films, links, Web archives, etc.) which we were\nable to locate via the XML reference tags vs usage fre-\nquency dictionaries.\nTo extract citation data from multilingual dumps,\nwe employed and extended a Wikipedia template trans-\nlation module [39] which is part of the Wikipedia plat-\nformsoftware.Itiswrittenin Luaandafter resolvingor\nsubstitutingdependenciesonotherbuilt-inmodules,we\nintegrated it into our Python-based pipeline using the\nLupa library [3]. Similarly to English dumps (see Fig-\nure 3), we extract all templates from Wikicode and se-\nlectthosethatrefertolanguage-specificorEnglishcita-\ntion templates. While English templates are parsed and\nmapped to uniform template key-value pairs directly,parsed language-specific templates are first translated\ninto English. We only substitute template keys and\nleave their values unchanged.\n3 Classification and lookup\nAlong with information such as author(s), date of pub-\nlication, title, and page numbers, citations may include\nunique identifiers depending on the type of work being\nreferred to. Citations of books often include an Inter-\nnational Standard Book Number (ISBN). Specific vol-\numes, articles, or other identifiable parts of a period-\nical, may have an associated Serial Item and Contri-\nbution Identifier (SICI) or an International Standard\nSerial Number (ISSN). Electronic documents may have\na Digital Object Identifier (DOI). Biomedical research\narticles may have a PubMed Identifier (PMID).\nSimilarly to [35], we label citations with available\nidentifiers:\n\u2013Every citation with a PMC or PMID was labeled as\na journal article.\n\u2013Every citation with a PMC, PMID, or DOI and us-\ning the citation template for journals and confer-\nences was labeled as a journal article.\n\u2013Every citation that had an ISBN is labelled as a\nbook.\n\u2013All citations with their URL top-level domain be-\nlonging to a given set of media news agencies are\nlabeled as news.\nConceptually, our classification method is similar to\nthe training set labeling for a neuron network classifier\npresented in [35]. While training a classifier is a valid\napproach for categorizing citation data, the prepara-\ntion of the training dataset might be compromised by\nselection bias [46]. More specifically, the method selects\ncitations with DOI, PMID, PMC, and ISBN identifiers\nand labels them as \u201cbook\u201d or \u201cjournal\u201d; it also selects ci-\ntations with top domains in URL that refer to a small\nlist of news portals and labels them as \u201cweb\u201d. Then\nthe classifier is trained and applied to label the whole\ndataset. However, this training dataset might not rep-\nresent a randomized sampling from the whole dataset\n4 N. Kokash, G. Colavizza\nCitation data extraction Citation data classi\ufb01cation Citation data lookup\n{author: 'V annevar Bush', title: 'As W e\nMay Think', publication_date: '1945'}\n{author: 'Norbert Wiener', title: 'The\nHuman Use of Human Beings',\npublication_date: '1950'}{author: 'V annevar Bush', title: 'As W e\nMay Think', publication_date: '1945',\nclass: 'journal_article' }\n{author: 'Norbert Wiener', title: 'The\nHuman Use of Human Beings',\npublication_date: '1950',\u00a0\nclass: 'book' }{author: 'V annevar Bush', title: 'As W e\nMay Think', publication_date:\n'1945',\u00a0class: 'journal_article',\nidenti\ufb01er:\u00a0\n'doi.org/10.1 145/227181.227186 '}\n{author: 'Norbert Wiener', title: 'The\nHuman Use of Human Beings',\npublication_date: '1950',\u00a0\nclass: 'book',\u00a0 identi\ufb01er:\u00a0\n'9781283140171 '}\nWikipedia articles\nFig. 2: Citation extraction process [35].\nFig. 3: Citation translation process.\nas variants of citations may not be included in the la-\nbeling process outlined above. Consequently, the high\nperformance observed on the testing part of this same\nlabeled dataset does not imply it will be performing\nequally well while labeling Wikipedia citations in gen-\neral. Therefore, we release for public use [17][18] only a\nconservatively labeled dataset with citations classified\nto\u201cbook\u201d,\u201cjournal\u201d,\u201cnews\u201d,and\u201cother\u201d categoriesusing\nthe aforementioned deterministic classifier. In the orig-\ninal work, the classifier was trained on a set of features:\ncitation text, citation statement (the text preceding a\ncitation in a Wikipedia article), Part of Speech (POS)\ntags, citation section, the order of the citation within\nthe article, and the total number of words in the ar-\nticle. Our feature importance analysis showed that all\nfeatures except for the citation text have a low impact\non the classification accuracy. Nevertheless, we main-\ntain the neural network-based classifier code as part of\nour codebase for maximum backward compatibility and\nfuture use.\nTo select citations that refer to the news, we col-\nlected news media links, 22646 unique website top do-\nmains,andsocialmediapagesubdomains,from4sources:\n1.mediabiasfactcheck.com : The resource provides 9\ncategories of media sources, 5 of which are news\nportals with or without political bias (least, left,\nright, left center, and right center bias), and 4 cate-\ngories with other sources (conspiracy-pseudo, ques-\ntionable, satire, and science). We omit 4 latter cate-\ngoriesastheyarenotsuitableforclassifyingWikipediacitationsas\u201cnews\u201d (while\u201cscience\u201d domainsaretrust-\nworthy, they list many journal websites - ideally, we\nwould like to label Wikipedia citations with such\nURLs as \u201cjournals\u201d.\n2.newspaperindex.com : A list of the most important\nonline newspapers and other news sources in all\ncountries in the world. All the newspapers focus on\ngeneralnews,politics,debate,andtheeconomy,and\nthey are free to access online.\n3.newsmedialists.com :Theresourcelistslinkstonews-\npapers,magazines,TV,andradiochannelspercoun-\ntry, as well as to their social network pages (Face-\nbook, Instagram, Twitter, Pinterest, and YouTube).\n4. Projects https://github.com/vegetable68/news_\ndomain_labeled and us-news-domains assembled\na list of 3976 and 5000 news domains, respectively.\nWe join top domains from the above resources into a\nset and classify a citation with a given URL as news if\nits top domain is in this set.\nA part of citations categorized as \u201cother\u201d due to the\nabsence of identifiers may still represent references to\nbooks or journals. Citation data lookup is a process of\nsearching for cited sources from Wikipedia in external\ncitation databases. In our pipeline, we perform such a\nsearch in Crossref [6] and Google Books [10] databases\nfor citation templates that imply a reference to a jour-\nnal or a book (i.e, \u201ccite book\u201d, \u201ccite journal\u201d, \u201ccite ency-\nclopedia\u201d, \u201ccite proceedings\u201d in English Wikipedia) and\naugments the extracted dataset with retrieved identi-\nfiers if a match is found. The aforementioned citation\nWikipedia Citations: Reproducible Citation Extraction from Multilingual Wikipedia 5\ndatabases return the best matches that may or may\nnot coincide with the requested titles. Hence, we only\nconsider the search successful if the returned title has\na small editing distance [23] from the requested title.\nThe lookup with follow-up validation based on edit-\ning distance is a relatively slow process. To mitigate\nthis step from potential failing (due to network issues,\ndenial of service, wrong return format, etc.) on millions\nof requests, we process each parquet file separately and\nprovide an option to save results in batches of a chosen\nsize.\n4 Wikipedia citation datasets\n4.1 English Wikipedia\nThe first release of the English Wikipedia Citations\ndataset from 2020 [36] contains 4.0M citations to schol-\narly publications with known identifiers \u2014 including\nDOI, PMC, PMID, and ISBN \u2014 and further equip\nan extra 261K citations with DOIs from Crossref. Ta-\nble 1 shows updated data for English Wikipedia, which\namounts for 5.0M citations to scientific journals and\nbooks in 2023 and 5.5M in 2024. These data were ob-\ntained using the same extraction pipeline and show the\ntrend in Wikipedia growth, i.e., roughly a 10% increase\nin the overall number of citations over a year.\nAsci score is defined as the ratio of academic as op-\nposedtonon-academicreferencesincludedinWikipedia\narticles [24]. Along with the numbers of citations refer-\nring to published books, journals, and news, the table\nshows the sci score as a fraction of journal citations ver-\nsusthewholenumberofextractedcitations.Inbrackets\nnext to the estimated sci score, 5%, we provide the es-\ntimation based on the presence of DOI by Lewoniewski\net al. [24], also performed on 2023 English Wikipedia\nand equal to 4.55%. As besides journals we also identify\nbookcitations,itmaybeinterestingtoseethatthefrac-\ntion of overall book and journal citations exceeds 12%\n(sci score 2).\nWith an effort to consolidate known lists of news\nmedia across the world, we were able to label a signifi-\ncantportionofWikipediacitationsas\u201cnews\u201d.Suchcita-\ntions generally can also be deemed trustworthy, assum-\ning the majority of news media adhere to journalistic\nvalues and ensure that the information they distribute\nis accurate, fair, and thorough. Rel score in Table 1\nshows the fraction of all three classes of trustworthy\ninformation sources.\nWhileweonlyused4indextypes(DOI,PMC,PMID,\nand ISBN) to classify citations, primarily motivated by\nthe question how 2023 and 2024 datasets differ from\n2020, the following index types are also commonly usedasannotationsinWikipediacitationtemplates:JSTOR,\nBIBCODE, ISMN, ISSN, SSRN, JFM, ARXIV, RFC,\nOL,LCCN,OCLC,ASIN,OSTI,MR,ZBL,USENETID,\nwhich potentially can be exploited for more thorough\nclassification.\nOurexperimentationwiththelookupprocessshowed\nthat a significant number of Wikipedia citations are\nnot recognized as scientific citations solely due to the\nabsence of identifiers in their templates. In particular,\n929.601 out of 2.445.913 English Wikipedia citations\n(38%) we searched for in citation databases were aug-\nmented with unique identifiers via Google Books and\nCrossref APIs and hence refer to either books or jour-\nnals. Together with these citations falling into journal\nand book classes, the Rel score 2 for English Wikipedia\nwould amount to 14.7%.\nPreviously,weattemptedtodisambiguatereferences\nextracted from published books in Humanities [19] us-\ning the same method and were able to retrieve iden-\ntifiers roughly for 38% of requested citations too. Al-\nthough the estimation of citation database coverage\nwas not the focus of any of these two projects, the fact\nthat the disambiguation of two very different and sub-\nstantial in size citation datasets yields similar outcomes\nin terms of the fraction of discovered entries, suggests\nthat these two citation databases collectively cover not\nmore than 40% of cited sources that are likely to re-\nfer to scientific publications. Previous estimations of\ncitation database coverage [27] showed a higher per-\ncentage, although numbers significantly differed across\nservices (Google Scholar, Microsoft Academic, Scopus,\nDimensions,etc.)andcoveragegapsinsomeareas,such\nas Physics and Humanities, were revealed. The likely\nexplanation lies in the fact that this study evaluated\ncoverage using a sample of citations taken from a seed\nsample of 2515 highly-cited documents listed in Google\nScholar\u2019sClassicPapers.Whilethisisaviableapproach\nfor estimating the relative coverage of aforementioned\ndatabases, it may not be a fair dataset for answering\nthe question of what overall percentage of cited scien-\ntific sources are present in existing citation databases.\nOur Wikipedia-based datasets provide interesting op-\nportunities for such studies.\n4.2 Multilingual Wikipedia\nInthecaseofmultilingualWikipedia,wemanuallyana-\nlyzed template frequency maps and selected which tem-\nplates to parse for each language. Table 2 shows the\nsizes of processed language dumps, the overall number\nof templates, the overall number of recognized citation\ntemplates, and the fraction of parsed citation templates\n6 N. Kokash, G. Colavizza\nTable 1: English Wikipedia citations\nYear Citations Journals Books News Scientific Reliable Sci score Sci score 2 Rel score\n202340.664.485 2.052.172 2.994.601 9.926.598 5.046.773 14.973.371 5.05 (4.55) 12.41 36.82\n202444.766.800 2.248.748 3.277.629 10.958.151 5.526.377 16.484.528 5.02 12.34 36.82\n+10.0% 9.6% +9.4% + 10.4% +9.5% +10.0% -0.03 -0.07 0\nTable 2: Multi-lingual Wikipedia citations\nLanguage SizeTemplates Citations %\nGerman 6.7GB 36.513.001 4.854.945 13.3\nFrench 5.9GB 88.758.180 9.552.768 10.8\nRussian 5.1GB 37.486.749 7.437.100 19.84\nSpanish 4.2GB 29.436.355 6.918.442 23.5\nItalian 3.6GB 39.272.469 5.545.082 14.12\nPolish 2.4GB 24.347.212 4.744.158 19.5\nPortuguese 2.2GB 22.579.438 4.775.025 21.15\nDutch 1.8GB 18.812.097 566.549 3.0\nSwedish 1.5GB 21.900.024 3.802.416 17.36\nCatalan 1.2GB 11.905.510 2.239.714 18.81\nFinnish 900.9MB 7.433.203 1.697.731 22.84\nTurkish 883.9MB 8.890.506 1.993.177 22.42\nNorwegian 763.7MB 7.502.679 796.500 10.62\nDanish 413.3MB 3.176.509 437.239 13.76\nversus the number of all template references, i.e., any\ninserts within \u27e8ref\u27e9and double curly braces {{...}}.\nTable 3 provides numbers related to citation clas-\nsification in 14 languages other than English. Observe\nthat our results correlate with the sci score reported\nin [24] (provided in brackets next to our score for con-\nvenience).Ourestimationtendstobeslightlyhigherfor\nmost languages. Apart from differences in the extrac-\ntion process, this can be explained by a recent recogni-\ntionoftheimportanceofuniqueidentifiersinWikipedia\ncitations. In contrast to our work, the authors do not\nmap various multilingual citation templates to a com-\nmon English data frame and estimate the number of\nscientific citations based on the simple presence of the\nDOI index in the citation text, which is the main focus\nof their work and is given for a full set of Wikipedia\nlanguages.\nAnalogously to sci score, we compute the fraction of\nreliable citation sources, rel score, which include refer-\nencestopublishedbooks,journals,andwebresourcesat\nreputable news agency domains versus the whole num-\nberofcitations.Catalan,Spanish,andFrenchWikipedia\nreceived higher scores on both such metrics.\nTable 4 presents counts for the five most common\nmulti-lingual Wikipedia citation templates. For each\nlanguage, native citation templates for journals, books,\nwebpages,andnewsareamongthoseusedmost.Often,\ntheirEnglishcounterpartsarewidelyexploited,i.e., cite\nwebtemplateappearsamongthemostcommoncitation\ntemplates in 9 out of 14 languages.Our released datasets [17,18] include the following\ncolumns:\n\u2013type of citation : Wikipedia template type used\nto define the citation, e.g., cite book .\n\u2013page title :titleoftheWikipediaarticlefromwhich\nthe citation was extracted.\n\u2013title:sourcetitle,e.g.,titleofthebook,newspaper\narticle, etc.\n\u2013url: link to the source, e.g., the web page where the\nnews article was published, description of the book\nat the publisher\u2019s website, online library web page,\netc.\n\u2013tld: top link domain extracted from the URL, e.g.,\nbbcforhttps://www.bbc.co.uk/ .\n\u2013authors: list of article or book authors, if available.\n\u2013ID list: list of publication identifiers mentioned in\nthe citation, e.g., DOI, ISBN, etc.\n\u2013citation : citation text as used in Wikipedia code\n\u2013actual label : \u201cbook\u201d, \u201cjournal\u201d, \u201cnews\u201d, or \u201cother\u201d\nlabel assigned based on the analysis of citation iden-\ntifiers or top link domain.\n\u2013acquired ID list :identifierslocatedbythelookup\nprocess for selected citation types.\nA much wider set of citation template properties\nis retrieved, parsed, and mapped into a uniform for-\nmat by the pipeline, e.g., publication date, publication\nplace, publisher name, issue, volume, pages, etc. These\nextended datasets can be shared on demand for the 15\nFebruary 2024 dumps discussed in this paper. The code\nis open-source and we provide detailed instructions on\nhow to run extraction, classification, and lookup scripts\nonGoogleCloudDataproc,afast,easy-to-use,andfully\nmanaged service for running Apache Spark, see docu-\nmentation in the project repository [15,16].\n5 Related work\nOpen science [11] is the movement to make scientific\nresearch and its dissemination accessible to all levels of\nsociety. Wikipedia is a popular open science platform\nthat over years has become an integral part of scholarly\ncommunication. Kousha and Thelwall [21] assessed the\nvalue of Wikipedia citations for academic articles and\nbooks,discoveringthat5%ofarticlesand33%ofmono-\ngraphs have at least one citation from Wikipedia. On\nWikipedia Citations: Reproducible Citation Extraction from Multilingual Wikipedia 7\nTable 3: Multi-lingual Wikipedia reliable sources\nLanguage Citations Books Journals News Scientific Reliable Sci score Sci score 2 Rel score\nGerman 4.854.945 320.179 105.542 901.091 425.721 1.326.812 2.17 (1.88) 8.77 27.33\nFrench 9.552.768 798.525 264.560 1.907.183 1.063.085 2.970.268 2.77 (1.84) 11.13 31.09\nRussian 7.178.389 420.828 130.470 1.370.665 551.298 1.921.963 1.82 (1.84) 7.68 25.84\nSpanish 6.918.442 522.910 213.767 1.699.396 736.677 2.436.073 3.09 (2.13) 10.65 35.21\nItalian 5.545.082 384.816 128.366 917.517 513.182 1.430.699 2.31 (2.52) 9.25 25.80\nPolish 4.744.158 463.783 95.988 513.006 559.771 1.072.777 2.11 (1.55) 11.80 22.61\nPortuguese 4.775.025 243.593 142.216 1.176.140 385.809 1.561.949 3.0 (2.61) 8.08 32.71\nDutch 566.549 27.074 12.706 114.110 39.780 133.890 2.24 (0.78) 7.02 23.63\nSwedish 3.802.416 112.748 155.740 869.662 268.488 1.138.150 4.1 (1.44) 7.06 29.93\nCatalan 2.239.714 261.779 105.125 423.241 366.904 790.145 4.7 (2.93) 16.38 35.28\nFinnish 1.697.731 209.556 12.068 286.420 221.624 508.044 0.71 (0.59) 13.05 29.92\nTurkish 1.993.177 85.079 56.202 339.122 141.281 480.403 2.82 (2.43) 7.09 24.10\nNorwegian 796.500 43.314 12.373 151.780 55.687 207.467 1.55 (1.68) 6.99 26.05\nDanish 437.239 23.303 7.522 70.760 30.825 101.585 1.72 (1.23) 7.05 23.23\nTable 4: Five most common multi-lingual Wikipedia citation templates\n(a) German\nTemplate Count\ninternetquelle 2.842.270\nliteratur 907.044\nwebarchiv 740.540\ncite web 284.434\ncite news 31.502(b) French\nTemplate Count\nlien web 5.976.136\nouvrage 1.502.055\narticle 1.266.557\ncitation 806.564\ncite report 386(c) Russian\nTemplate Count\ncite web 3.175.967\nwayback 690.092\n\u043a\u043d\u0438\u0433\u0430 546.076\n\u0441\u0442\u0430\u0442\u044c\u044f 278.650\ncite news 189.256(d) Spanish\nTemplate Count\ncita web 4.076.779\ncita libro 711.873\ncita noticia 514.859\ncite web 504.741\ncita publicaci\u00b4 on 494.706\n(e) Italian\nTemplate Count\ncita web 3.735.469\ncita libro 840.491\ncita news 476.739\ncita pubblicazione 318.901\ncita conferenza 1.648(f) Polish\nTemplate Count\ncytuj stron\u00b8 e 2530191\ncytuj 1351399\ncytuj ksi\u00b8 a\u02d9 zk\u00b8 e 624640\ncytuj pismo 237926\ncite web 2(g) Portuguese\nTemplate Count\ncitar web 3.446.748\ncitar livro 383.034\ncitar peri\u00b4 odico 291.663\nlink 211.959\ncitar jornal 199.418(h) Dutch\nTemplate Count\nciteer web 431.570\ncite web 57.149\nciteer boek 41.465\ncite news 11.489\nciteer journal 10.718\n(i) Swedish\nTemplate Count\nwebbref 3.176.499\nbokref 228.791\ntidskriftsref 169.625\nwayback 166.952\ncite web 31.073(j) Finnish\nTemplate Count\nverkkoviite 1.175.985\nkirjaviite 267.953\nwayback 152.801\nlehtiviite 66.079\ncite web 24.799(k) Turkish\nTemplate Count\nweb kayna\u02d8 g\u0131 1.264.127\nwebar\u00b8 siv 356.897\nhaber kayna\u02d8 g\u0131 149.095\nkitap kayna\u02d8 g\u0131 126.230\ndergi kayna\u02d8 g\u0131 94.657(l) Norwegian\nTemplate Count\nkilde www 374.391\nwayback 124.632\ncite web 116.100\nkilde bok 56.554\nkilde avis 43.818\n(m) Catalan\nTemplate Count\nref-web 1.185.404\nref-llibre 389.447\nref-publicaci\u00b4 o 265.016\nwebarchive 211.238\nref-not\u00b4 \u0131cia 118.187(n) Danish\nTemplate Count\ncite web 193.539\nwebarchive 109.005\ncite news 42.853\ncite book 25.639\nkilde 20.804\nthe other hand, Wikipedia helps popularize academic\nresearch. Various studies have shown that a high por-\ntion of citations to sources in Wikipedia refer to scien-\ntific or scholarly literature [30]. The literature cited in\nWikipedia has been found to correlate positively with a\njournal\u2019s popularity, its impact factor, and open access\npolicy [2,21,37]. A clear influence of Wikipedia on sci-\nentific research has been found [41], despite a generallack of reciprocity in acknowledging it as a source of\ninformation from the scientific literature [13,42]. Many\nefforts are dedicated to the task of expanding and im-\nproving Wikipedia\u2019s verifiability through citations to\nreliable sources [9,34]. Citations in Wikipedia are also\nuseful for users browsing low-quality or underdeveloped\narticles, as they allow them to look for information out-\nside of the platform [31].\n8 N. Kokash, G. Colavizza\nWikipedia is instrumental in providing access to sci-\nentific information and in fostering the public under-\nstandingofscience[12,43].Accordingto2017study[38],\nopen-access journals have a 47% higher chance of be-\ning cited in Wikipedia. Our datasets of classified cita-\ntionsmayassistresearchersinanalyzingtheimpactand\ndistribution of scientific knowledge via Wikipedia, e.g.,\nprovided a list of open-access journals, one can measure\ntheir portion among citations in various Wikipedia edi-\ntions.Duetotheimportanceofcitationsformanystud-\nies around Wikipedia as a crucial part of open science\ninfrastructure, there is an ongoing effort to produce full\nsets of Wikipedia citations. Zagovora et al. [47] present\na data set that contains individual revision histories\nof all Wikipedia references ever created in the English\nWikipedia until June 2019. Regardless of how com-\nplete the extracted set is, with millions of Wikipedia\nedits per year, no set can be considered final. Therefore\nwe wanted to equip researchers interested in Wikipedia\nstudies with an open and easy-to-use tool to collect ci-\ntations.\nEarly studies of what scientific and scholarly liter-\nature is cited in Wikipedia point to relatively low cov-\nerage, indicating that between 1 %and 5%of all pub-\nlished journal articles are cited in Wikipedia [32,47].\nThe authors of this study found a persistent increase of\nreferences equipped with some form of document iden-\ntifier over time, they underline how relying on refer-\nences with document identifiers is still not sufficient to\ncapture all relevant publications cited from Wikipedia.\nOur estimation of scientific citations is higher and con-\nsistent among multi-lingual dumps. We show that via\nthe augmentation of citations this percentage can be\nsignificantly higher as identifiers are often omitted in\ncitations. Moreover, even the look-up process does not\nsolve the problem of recognizing scientific references\ncompletely. While citation database coverage estima-\ntions differ across studies and domains [27,44], based\non our experience with English Wikipedia citation aug-\nmentationdiscussedinthispaper,lessthanhalfoflikely\nscientific citations (more precisely, 38%) are found via\nGoogle Books and Crossref search (which is remarkably\nclosetothefractionofdisambiguatedcitationsfromthe\nBrill\u2019s dataset [20] in our previous project).\nThe majority of Wikipedia citation studies are done\non its English version only. Lewoniewski et at al. anal-\nyses the use of common references in several Wikipedia\nlanguage editions: English, German, French, Russian,\nPolish, Ukrainian, and Belarussian [25]. Later the same\nauthors present descriptive analysis of DOI-containing\nreferences in 310 Wikipedia languages [24]. However,\nthese studies do not translate and align citations across\nlanguages.Thispartofourcontributionopensopportu-nitiesformorein-depthstudiesofmultilingualWikipedia\ncitation structures and trends.\nMaggio et al. [26] establish benchmarks for the rel-\native distribution and click rate of citations with DOI\nfrom English Wikipedia, with a focus on medical ci-\ntations. Arroyo-Machado et al. [2] construct an open\nknowledge graph for the large-scale analysis of the En-\nglish Wikipedia metrics. A complete relational dataset\nofEnglishWikipediaisbuilt.PooladianandBorrego[33]\nevaluate the impact of research papers by their expo-\nsure to a wider audience beyond the academic commu-\nnity. The paper evaluates the Wikipedia references to\npapers in Information Science published between 2001\nand 2010 and concludes that less than 3% of articles in\nthe sample were cited and that nearly one-third of the\nWikipedia citations link to open-access sources. Jemiel-\nniak et al. [14] determine the ranking of the most cited\njournalsbytheirrepresentationintheEnglish-language\nmedical pages of Wikipedia. Torres-Salinas et al. [43]\nstudyhowscientificknowledgeispresentedinWikipedia\narticles in the field of Humanities, finding out that the\ncitation average for Humanities articles in Wikipedia is\nlowerthanthegeneral.Colavizza[5]evaluatesWikipedia\u2019s\nrapid response to Covid-19 research observing that al-\nmost 2% from 160.000 scientific articles were already\ncited in Wikipedia in a few months after the start of the\npandemic. This supports our view that any snapshot-\nbased evaluation gets obsolete quickly and continuous\nintegrationofcitationsourceswithacademicinfrastruc-\nture is needed.\nNicholson et al. [29] analyzed scientific articles ref-\nerenced in the English Wikipedia to see how they had\nbeen cited in the scientific literature. The authors find\nthat around 3% of citations are used to provide sup-\nporting evidence, 0.35% to provide contradicting evi-\ndence, and 96.7% mention a study without indicating\nthat they provide supporting or contradicting evidence.\nZheng et al. [48] assess gender- and country-based bi-\nases in Wikipedia citation practices. The research has\nfound that publications by women are cited less than\nthose by men. Scholarly publications by authors affili-\nated with non-Anglosphere countries are also disadvan-\ntagedingettingcitedbyWikipedia.Thelevelofgender-\nor country-based inequalities varies by research field,\nand the gender-country intersectional bias is prominent\nin math-intensive STEM fields.\nAmongMediaWikiparserimplementationsthatwork\non XML dumps [7] only a few offer complete or near\ncomplete coverage and are regularly maintained [1]. Re-\ncently a Python library for parsing and mining meta-\ndata from the Wikipedia HTML dumps [8] has been\nmade available [45]. Besides using the HTML dumps,\nusers can also use the Wikipedia API to obtain the\nWikipedia Citations: Reproducible Citation Extraction from Multilingual Wikipedia 9\nHTML of a particular article from their title and parse\nthe HTML string with this library.\n6 Conclusion and Future Work\nThis contribution is a direct continuation of the previ-\nouseffort[35,24]bythecommunitytostudy,use,main-\ntain, and expand work on citations from Wikipedia.\nWepublishedtheWikipediaCitationsdatasetsin15\nlanguages. Citations in multilingual datasets are trans-\nlated and harmonized to match a common template.\nWe used persistent identifiers such as DOIs and IS-\nBNs and top domains extracted from URL references\nwhenever available to classify citations into four cate-\ngories: \u201cbooks\u201d, \u201cjournals\u201d, \u201cnews\u201d, and \u201cother\u201d. To iden-\ntify news references, we assembled a comprehensive list\nof news media agencies worldwide. Furthermore, we re-\nfined a mechanism to annotate citations without per-\nsistent identifiers such from Google Books and Crossref\ncitation databases.\nWe collected basic statistics about 2024 datasets\nand compared them with available data from previ-\nous years. Our results show that e.g., the number of\nWikipedia citations increased by 10% since 2023, and\nthe sci score of the majority of the processed language\neditions, i.e., the percentage of scientific journal cita-\ntions, has improved reaching 2.61% vs 1.88% previously\non average for the 15 languages we analyzed. We also\nrelease all our code to extend upon our work and up-\ndate the datasets in the future, equipped with detailed\ninstructions on how to run it reliably on Google Cloud\ninfrastructure. We believe that this code is easy to ex-\ntend for processing of other language editions by spec-\nifying language-specific citation template names and\nmapping their properties to English counterparts (\u201cau-\nthors\u201d, \u201ctitle\u201d, etc.), fostering cross-cultural research on\nWikipedia content.\nAcknowledges\nThis research was supported in part by the University of Ams-\nterdam Data Science Centre.\nReferences\n1. Alternative parsers (2024). URL https://www.mediawiki.\norg/wiki/Alternative_parsers\n2. Arroyo-Machado, W., Torres-Salinas, D., Costas, R.: Wik-\ninformetrics: Construction and description of an open\nWikipedia knowledge graph data set for informetric pur-\nposes. Quantitative Science Studies 3(4), 931\u2013952 (2022).\nDOI 10.1162/qss_a_00226. URL https://doi.org/10.\n1162/qss_a_002263. Behnel, S.: Lupa: Python wrapper around Lua and LuaJIT\n(2024). URL https://pypi.org/project/lupa/\n4. Castanha, Hj \u00f8rland, B., Gutierres, R.C., de Ara\u00b4 ujo, P.C.: Ci-\ntation indexing and indexes. Knowledge Organization 48(1),\n72\u2013101 (2021). DOI 10.5771/0943-7444-2021-1-72\n5. Colavizza, G.: COVID-19 research in Wikipedia. Quantita-\ntive Science Studies 1(4), 1349\u20131380 (2020). DOI 10.1162/\nqss_a_00080. URL https://doi.org/10.1162/qss_a_00080\n6. Crossref REST API Documentation (2016). URL https:\n//www.crossref.org/documentation/retrieve-metadata/\nrest-api/\n7. Dumps, W.: Wikimedia Downloads (2024). URL https://\ndumps.wikimedia.org/backup-index.html\n8. Dumps, W.: Wikimedia Enterprise HTML Dumps (2024).\nURL https://dumps.wikimedia.org/other/enterprise_\nhtml/\n9. Fetahu, B., Markert, K., Anand, A.: Fine Grained Citation\nSpan for References in Wikipedia. In: Proceedings of the\n2017 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1990\u20131999. Association for Computational\nLinguistics (2017). DOI 10.18653/v1/D17-1212\n10. Google Books APIs: Volume (2012). URL https:\n//developers.google.com/books/docs/v1/reference/\nvolumes\n11. Grand, A.: Open science. Journal of Science Communication\n14(2015). DOI 10.22323/2.14040302\n12. Heilman, J., Kemmann, E., Bonert, M., Chatterjee, A.,\nRagar, B., Beards, G., Iberri, D., Harvey, M., Thomas,\nB., Stomp, W., Martone, M., Lodge, D., Vondracek, A.,\nDe Wolff, J., Liber, C., Grover, S., Vickers, T., Mesko, B.,\nLaurent, M.: Wikipedia: A key tool for global public health\npromotion. Journal of medical Internet research 13, e14\n(2011). DOI 10.2196/jmir.1589\n13. Jemielniak, D., Aibar, E.: Bridging the gap between\nWikipedia and academia. Journal of the Associ-\nation for Information Science and Technology 67(7),\n1773\u20131776 (2016). DOI https://doi.org/10.1002/asi.\n23691. URL https://asistdl.onlinelibrary.wiley.com/\ndoi/abs/10.1002/asi.23691\n14. Jemielniak, D., Masukume, G., Wilamowski, M.: The most\ninfluential medical journals according to Wikipedia: Quan-\ntitative analysis. Journal of Medical Internet Research 21,\ne11429 (2019). DOI 10.2196/11429\n15. Kokash, N.: Extraction and classification of references from\nWikipedia articles (2024). URL https://github.com/\nalbatros13/wikicite\n16. Kokash, N.: Extraction and classification of references from\nWikipedia articles in multiple languages (2024). URL https:\n//github.com/albatros13/wikicite/tree/multilang\n17. Kokash, N., Colavizza, G.: A Comprehensive Dataset of\nClassified Citations with Identifiers from English Wikipedia\n(2024) (2024). URL https://zenodo.org/records/10782978\n18. Kokash,N.,Colavizza,G.:AComprehensiveDatasetofClas-\nsified Citations with Identifiers from Multilingual Wikipedia\n(2024) (2024). URL https://zenodo.org/records/11210434\n19. Kokash, N., Romanello, M., Suyver, E., Colavizza, G.: From\nbooks to knowledge graphs. Journal of Data Mining and\nDigital Humanities (2023)\n20. Kokash, N., Romanello, M., Suyver, E., Colavizza,\nG.: The Brill knowledge graph: A database of bibli-\nographic references and index terms extracted from\nbooks in Humanities and Social Sciences. Research\nData Journal for the Humanities and Social Sciences\npp. 1 \u2013 21 (2024). DOI 10.1163/24523666-bja10036.\nURL https://brill.com/view/journals/rdj/aop/\narticle-10.1163-24523666-bja10036/article-10.\n1163-24523666-bja10036.xml\n10 N. Kokash, G. Colavizza\n21. Kousha,K.,Thelwall,M.:AreWikipediacitationsimportant\nevidence of the impact of scholarly articles and books? Jour-\nnal of the Association for Information Science and Technol-\nogy68(3),762\u2013779(2017). DOIhttps://doi.org/10.1002/asi.\n23694. URL https://asistdl.onlinelibrary.wiley.com/\ndoi/abs/10.1002/asi.23694\n22. Kurtovic, B.: A Python parser for MediaWiki wikicode (mw-\nparserfromhell) (2024). URL https://github.com/earwig/\nmwparserfromhell\n23. Levenshtein, V.I.: Binary Codes Capable of Correcting Spu-\nrious Insertions and Deletions of Ones. Probl. Inf. Transm.\n1(1), 8\u2014-17 (1965)\n24. Lewoniewski, W., W\u00b8 ecel, K., Abramowicz, W.: Understand-\ning the use of scientific references in multilingual Wikipedia\nacross various topics. Procedia Computer Science 225, 3977\u2013\n3986 (2023). DOI https://doi.org/10.1016/j.procs.2023.\n10.393. URL https://www.sciencedirect.com/science/\narticle/pii/S187705092301551X . 27th Int. Conference on\nKnowledge Based and Intelligent Information and Engineer-\ning Sytems (KES 2023)\n25. Lewoniewski, W., W\u00b8 ecel, K., Abramowicz, W.: Analysis of\nReferences Across Wikipedia Languages, 756 edn., pp. 561\u2013\n573. Springer (2017). DOI 10.1007/978-3-319-67642-5_47\n26. Maggio, L.A., Willinsky, J.M., Steinberg, R.M., Mietchen,\nD., Wass, J.L., Dong, T.: Wikipedia as a gateway to biomed-\nical research: The relative distribution and use of citations\nin the English Wikipedia. PLOS ONE 12(12), 1\u201312 (2017).\nDOI 10.1371/journal.pone.0190046. URL https://doi.org/\n10.1371/journal.pone.0190046\n27. Mart \u00b4in-Mart\u00b4in, A., Thelwall, M., Orduna-Malea, E., Del-\ngado L\u00b4 opez-C\u00b4 ozar, E.: Google Scholar, Microsoft Academic,\nScopus, Dimensions, Web of Science, and OpenCitations\u2019\nCOCI: a multidisciplinary comparison of coverage via ci-\ntations. Scientometrics 126(1), 871\u2013906 (2021). DOI\n10.1007/s11192-020-03690-4. URL https://link.springer.\ncom/10.1007/s11192-020-03690-4\n28. Neuhaus, C., Daniel, H.D.: Data sources for performing ci-\ntation analysis: An overview. Journal of Documentation 64,\n193\u2013210 (2008). DOI 10.1108/00220410810858010\n29. Nicholson, J.M., Uppala, A., Sieber, M., Grabitz, P., Mor-\ndaunt, M., Rife, S.C.: Measuring the quality of scientific ref-\nerences in Wikipedia: an analysis of more than 115m cita-\ntions to over 800 000 scientific articles. The FEBS Journal\n288(14), 4242\u20134248 (2021). DOI https://doi.org/10.1111/\nfebs.15608. URL https://febs.onlinelibrary.wiley.com/\ndoi/abs/10.1111/febs.15608\n30. Nielsen, F., Mietchen, D., Willighagen, E.: Scholia, scien-\ntometrics and wikidata. In: The Semantic Web: ESWC\n2017 Satellite Events, LNCS, pp. 237\u2013259 (2017). DOI\n10.1007/978-3-319-70407-4_36\n31. Piccardi, T., Redi, M., Colavizza, G., West, R.: Quantifying\nengagementwithcitationsonWikipedia. In:WWW\u201920:Pro-\nceedings of The Web Conference 2020, pp. 2365\u20132376 (2020).\nDOI 10.1145/3366423.3380300\n32. Pooladian, A., Borrego, \u00b4A.: Methodological issues in measur-\ning citations in Wikipedia: a case study in library and infor-\nmation science. Scientometrics 113, 455 \u2013 464 (2017). URL\nhttps://api.semanticscholar.org/CorpusID:255007038\n33. Pooladian, A., Borrego, \u00b4A.: Methodological issues in measur-\ning citations in Wikipedia: a case study in Library and In-\nformation Science. Scientometrics 113(1), 455\u2013464 (2017).\nDOI 10.1007/s11192-017-2474-z. URL https://doi.org/\n10.1007/s11192-017-2474-z\n34. Redi, M., Fetahu, B., Morgan, J., Taraborelli, D.: Cita-\ntion needed: A taxonomy and algorithmic assessment of\nWikipedia\u2019s verifiability. In: WWW \u201919: The World WideWeb Conference, pp. 1567\u20131578 (2019). DOI 10.1145/\n3308558.3313618\n35. Singh, H., West, R., Colavizza, G.: Wikipedia citations: A\ncomprehensive data set of citations with identifiers extracted\nfrom English Wikipedia. Quantitative Science Studies 2, 1\u2013\n19 (2020). DOI 10.1162/qss_a_00105\n36. Singh, H., West, R., Colavizza, G.: Wikipedia Citations: A\ncomprehensive dataset of citations with identifiers extracted\nfrom English Wikipedia (2020)\n37. Sugimoto, C.R., Work, S., Larivi\u00b4 ere, V., Haustein, S.: Schol-\narly use of social media and altmetrics: A review of the liter-\nature. Journal of the Association for Information Science and\nTechnology 68(9), 2037\u20132062 (2017). DOI https://doi.org/\n10.1002/asi.23833. URL https://asistdl.onlinelibrary.\nwiley.com/doi/abs/10.1002/asi.23833\n38. Teplitskiy, M., Lu, G., Duede, E.: Amplifying the impact of\nopen access: Wikipedia and the diffusion of science. Journal\noftheAssociationforInformationScienceandTechnology 68\n(2015). URL https://api.semanticscholar.org/CorpusID:\n10220883\n39. The Wikimedia Foundation, I.: Module:CS1 translator\n(2024). URL https://en.wikFipedia.org/wiki/Module:\nCS1_translator\n40. The Wikimedia Foundation, I.: Wikipedia: Citation tem-\nplates (2024). URL https://en.wikipedia.org/wiki/\nWikipedia:Citation_templates\n41. Thompson,N.C.,Hanley,D.:ScienceisshapedbyWikipedia:\nEvidence from a randomized control trial. IRPN: Insti-\ntutional (2018). URL https://api.semanticscholar.org/\nCorpusID:30918097\n42. Tomaszewski, R., MacDonald, K.: A study of citations to\nWikipedia in scholarly publications. Science & Technol-\nogy Libraries 35, 1\u201316 (2016). DOI 10.1080/0194262X.2016.\n1206052\n43. Torres-Salinas, D., Romero-Fr\u00b4 \u0131as, E., Arroyo-Machado, W.:\nMapping the backbone of the Humanities through the eyes\nof Wikipedia. Journal of Informetrics 13(3), 793\u2013803 (2019).\nDOI 10.1016/j.joi.2019.07.002. URL https://ideas.repec.\norg/a/eee/infome/v13y2019i3p793-803.html\n44. Visser, M., van Eck, N.J., Waltman, L.: Large-scale compar-\nison of bibliographic data sources: Scopus, Web of Science,\nDimensions, Crossref, and Microsoft Academic. Quantita-\ntive Science Studies 2(1), 20\u201341 (2021). DOI 10.1162/qss_\na_00112. URL https://doi.org/10.1162/qss_a_00112\n45. Wikimedia Enterprise: A Python library for parsing and\nmining metadata from the Enterprise HTML Dumps (mw-\nparserfromhtml) (2024). URL https://pypi.org/project/\nmwparserfromhtml/\n46. Zadrozny, B.: Learning and evaluating classifiers under sam-\nple selection bias. In: Proc. of the 21st Int. Conf. on Machine\nLearning, ICML \u201904, p. 114. Association for Computing Ma-\nchinery, New York, NY, USA (2004). DOI 10.1145/1015330.\n1015425. URL https://doi.org/10.1145/1015330.1015425\n47. Zagovora, O., Ulloa, R., Weller, K., Fl\u00a8 ock, F.: \u201cI updated the\n\u27e8ref\u27e9: The evolution of references in the English Wikipedia\nand the implications for altmetrics. Quantitative Science\nStudies 3(1), 147\u2013173 (2022). DOI 10.1162/qss_a_00171.\nURL https://doi.org/10.1162/qss_a_00171\n48. Zheng, X., Chen, J., Yan, E., Ni, C.: Gender and country\nbiases in Wikipedia citations to scholarly publications. Jour-\nnal of the Association for Information Science and Technol-\nogy74(2),219\u2013233(2023). DOIhttps://doi.org/10.1002/asi.\n24723. URL https://asistdl.onlinelibrary.wiley.com/\ndoi/abs/10.1002/asi.24723", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Wikipedia Citations: Reproducible Citation Extraction from Multilingual Wikipedia", "author": ["N Kokash", "G Colavizza"], "pub_year": "2024", "venue": "arXiv preprint arXiv:2406.19291", "abstract": "Wikipedia is an essential component of the open science ecosystem, yet it is poorly integrated  with academic open science initiatives. Wikipedia Citations is a project that focuses on"}, "filled": false, "gsrank": 201, "pub_url": "https://arxiv.org/abs/2406.19291", "author_id": ["qKviINAAAAAJ", "LQEgr6UAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:LSfzVRNJzUUJ:scholar.google.com/&output=cite&scirp=200&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D200%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=LSfzVRNJzUUJ&ei=KbWsaMZzjoiJ6g_SwpG4CQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=5029756706237916973&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:LSfzVRNJzUUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2406.19291"}}]