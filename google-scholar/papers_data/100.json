[{"title": "A systematic review of automated hyperpartisan news detection", "year": "2025", "pdf_data": "ii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 1 \u2014 #1ii\ni\nii\ni\nPLOS ONE\nOPEN ACCESS\nCitation:MagginiMJ, BassiD, PiotP, Dias\nG, GamalloP(2025)Asystematicreviewof\nautomatedhyperpartisannewsdetection.PLoS\nONE20(2):e0316989. https://doi.org/10.1371/\njournal.pone.0316989\nEditor:PabloHenr\u00edquez,UniversidadDiego\nPortales,CHILE\nReceived: August12,2024\nAccepted: December19,2024\nPublished: February18,2025\nCopyright: \u00a92025 Magginietal.Thisisan\nopenaccessarticledistributedundertheterms\noftheCreativeCommonsAttributionLicense ,\nwhichpermitsunrestricteduse,distribution,\nandreproductioninanymedium,providedthe\noriginalauthorandsourcearecredited.\nData availability statement: Allthefiles\nregardingthecollectionandscreeningprocess\narepubliclyavailableatthefollowingGitHub\nrepository: https://github.com/MichJoM/\nHyperpartisan_News_Detection_\nSystematic_Review/tree/main .Wedonotneed\ntogiveaccesstodata,sincetheyarealready\nopen.\nFunding:Thisprojecthasreceivedfundingfrom\ntheEuropeanUnion\u2019sHorizonEuroperesearch\nandinnovationprogrammeundertheMarie\nSk\u0142odowska-CurieGrantAgreementNo.\n101073351.FundedbytheEuropeanUnion.RESEARCH ARTICLE\nA systematic review of automated\nhyperpartisan news detection\nMichele Joshua Maggini\n  \n1\u00a4\u2217, Davide Bassi\n  \n1\u262f, Paloma Piot\n  \n2\u262f,\nGa\u00ebl Dias\n  \n3, Pablo Gamallo\n  \n1\n1Centro Singular de Investigaci\u00f3n en Tecnolox\u00edas Intelixentes (CiTIUS), Universidade de Santiago de\nCompostela, Galicia, Spain, 2IRLab, CITIC Research Centre, Universidade da Coru\u00f1a, A Coru\u00f1a, Galiza,\nSpain, 3Universit\u00e9 Caen Normandie, ENSICAEN, CNRS, Normandie Univ, GREYC UMR6072, F-14000\nCaen, France\n\u00a4 Current address: Centro Singular de Investigaci\u00f3n en Tecnolox\u00edas Intelixentes (CiTIUS), Universidade\nde Santiago de Compostela, Galicia, Spain\n\u2217michelejoshua.maggini@usc.es\n\u262fThese authors contributed equally to this work.\nAbstract\nHyperpartisan news consists of articles with strong biases that support specific political\nparties. The spread of such news increases polarization among readers, which threat-\nens social unity and democratic stability. Automated tools can help identify hyperpartisan\nnews in the daily flood of articles, offering a way to tackle these problems. With recent\nadvances in machine learning and deep learning, there are now more methods available\nto address this issue. This literature review collects and organizes the different methods\nused in previous studies on hyperpartisan news detection. Using the PRISMA methodol-\nogy, we reviewed and systematized approaches and datasets from 81 articles published\nfrom January 2015 to 2024. Our analysis includes several steps: differentiating hyperpar-\ntisan news detection from similar tasks, identifying text sources, labeling methods, and\nevaluating models. We found some key gaps: there is no clear definition of hyperparti-\nsanship in Computer Science, and most datasets are in English, highlighting the need\nfor more datasets in minority languages. Moreover, the tendency is that deep learning\nmodels perform better than traditional machine learning, but Large Language Models\u2019\n(LLMs) capacities in this domain have been limitedly studied. This paper is the first to\nsystematically review hyperpartisan news detection, laying a solid groundwork for future\nresearch.\nIntroduction\nThe foundation of democratic governments rests on the voting process conducted by citi-\nzens [1]. Political parties, in their quest for votes, heavily rely on news media to disseminate\ntheir messages during campaigns. While transparent information and active political par-\nticipation are crucial for a healthy democracy, political entities increasingly employ hyper-\npartisan communication strategies. These tactics aim to discredit opposing factions and dis-\ntort reality, potentially impacting how governments represent their constituents. Although\nhyperpartisan campaign methods may increase voter participation [ 2] and strengthen the\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 1/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 2 \u2014 #2ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nconnection between voting decisions and specific ideologies, they can have significant nega- Viewsandopinionsexpressedarehowever\nthoseoftheauthor(s)onlyanddonot\nnecessarilyreflectthoseoftheEuropeanUnion\norEuropeanResearchExecutiveAgency(REA).\nNeithertheEuropeanUnionnorthegranting\nauthoritycanbeheldresponsibleforthem.This\nworkhasreceivedfinancialsupportfromthe\nXuntadeGalicia-Conseller\u00edadeCultura,\nEducaci\u00f3n,Formaci\u00f3nProfesionale\nUniversidades(Centrodeinvestigaci\u00f3nde\nGaliciaaccreditation2024-2027\nED431G-2023/04theEuropeanUnion\n(EuropeanRegionalDevelopmentFund-ERDF).\nCompeting interests: Theauthorshave\ndeclaredthatnocompetinginterestsexist.tive consequences. As [ 3] demonstrates, this communication style can highlight divisive ten-\nsions within society, complicating governance and potentially alienating citizens when oppos-\ning sides gain power. Consequently, hyperpartisanism poses a threat to the proper function-\ning of democracy [ 4] by polarizing and dividing the social fabric, reducing trust in govern-\nmental entities and mainstream news [ 5], and exacerbating tensions between governments\nand their oppositions [ 6].\nThe rise of alternative media outlets further amplifies these threats to democracy [ 7], as\nthey often share polarizing content [ 8]. In the online sphere, hyperpartisanship proliferates\nthrough various channels social networks publishers\u2019 websites. The dissemination of hyper-\npartisan news, characterized by highly polarized political and ideological content, capital-\nizes on the virality facilitated by platform algorithms [ 9]. While the term gained prominence\nduring the 2016 U.S. election [ 10], there is no evidence suggesting that this specific event\ntriggered a systemic hyper-polarization [ 11].\nThe digital realm has become a significant arena for political influence [ 12], affecting the\nentire infosphere [ 13]. The close relationship between hyperpartisanship and online inter-\nactions has led to increased attention on these manipulative forms of communication [ 14].\nOn the policy front, the EU Commission\u2019s 2018 expert report [ 15] addressed related topics\nsuch as disinformation, defamation, hate speech, and incitement to violence. More recently,\nthe European Parliament adopted the Digital Services Act (DSA) [ 16] in 2022, aiming to pro-\nvide \u201da secure, predictable and trustworthy online environment\u201d (Article 1. 1). In line with [ 9]\nand [17], we categorize hyperpartisan news under the broader umbrella of misinformation,\nclosely related to fake news detection. Hyperpartisan news detection as a classification task\nis specifically related to the news domain and can focus on linguistic, semantic, and meta-\ndata features. The objective is for an algorithm to predict a text\u2019s political affiliation or deter-\nmine if the content is hyperpartisan. The rising academic interest in hyperpartisan detection is\ntestified by the high participation of 42 teams at task 4 of SemEval-2019 [ 18].\nFor this systematic review, we only considered automated text-based strategies applied to\nnews articles. Manual detection of hyperpartisan news has been proposed. It mainly focuses\non discourse analysis [ 19\u201321]. Despite its effectiveness, this approach does not scale with the\ndaily news spreading. Hence, automated methods such as deep learning, social network anal-\nysis, or cross-methodologies like [ 22] are more effective. These approaches rely on different\nfeatures, so that hyperpartisan news detection may be tackled adopting content, sources, and\nuser-based data [ 23].\nThe article is organized as follows: the Related Works section covers the relevant surveys on\nsimilar topics, highlighting the main features and comparing their limitations with regards\nto our study; the Methodology section discusses the methodology adopted for this systematic\nreview, including research questions, search strategy, criteria selection, and selection pro-\ncedure; the section Hyperpartisan news detection: description of the phenomenon focuses on\nthe definition of hyperpartisanship, highlighting its multi-task and cross-disciplinary nature.\nAfterward, we present the textual frames where hyperpartisan traits are traceable and the\nspectrum of methodologies used in different computational sub-fields. Then, we covered the\ndiverse strategies and scales used to label hyperpartisanship. Section Approaches for auto-\nmatic hyperpartisan news detection contains a global categorization and discussion of the most\nperformant model in the papers screened and selected. We distinguished between the typol-\nogy of the model, the results, the features and the approaches employed. Section Datasets is a\ndescriptive overview of the datasets used in this domain: we collected the cited datasets and\ntheir features. Finally, section Conclusions and future works concludes the article by present-\ning the main findings of our literature review.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 2/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 3 \u2014 #3ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nThe main contributions of this study are:\n\u2022Comparing the different definitions of hyperpartisan news detection;\n\u2022Collecting and discussing the diverse approaches and algorithms used in the selected\nliterature, specifically for the news domain;\n\u2022Reporting evaluation metrics, features and embeddings considered in the studies;\n\u2022Presenting the main findings, the engineering innovations and research designs;\n\u2022Collecting and analyzing 38 datasets used in the literature, focusing on English and less\nrepresentated languages;\n\u2022Delineating prevailing research gaps and challenges in hyperpartisan news detection\ntask.\nRelated works\nThe current state of the literature lacks a systematic review specialized in automatic hyper-\npartisan news detection. While there are various relevant survey papers, they predominantly\nfocus on fake news and bias detection tasks. For instance, [ 24] examined fake news detection\nwhile considering the relation between factuality and political bias of news sources without\nshowing any dataset or discussing the methodologies. [ 25] started from a theorical introduc-\ntion of the fake news phenomenon to then cover the technical methodologies considering\ndifferent perspectives from content to style analysis. [ 26] compared manual and automated\napproaches to identify media bias, distinguishing several forms of bias occurring in the dis-\ntinct steps of news production. Similarly, [ 27] investigated the application of deep learning\nalgorithms in fake news detection, building upon a taxonomy proposed by [ 17], where hyper-\npartisan news detection overlapped with fake news detection. [ 28] covers the broad field of\ndisinformation by designing a taxonomy without considering either automated approaches or\nthe datasets used in the literature. Similarly, [ 29] analyzes the general phenomenon of media\nbias detection by describing its diverse manifestations (e.g., spin bias, ideology bias, coverage\nbias), distinguished the techniques to detect them and reported 17 datasets. Except for this\nlast author, no particular attention was given to hyperpartisan news detection from the others.\nMethodology\nIn this section we will present and describe the methodology adopted to conduct this sys-\ntematic review following [ 30]\u2019s guidelines. The planning and execution phases of this study\nare detailed in the following subsections, while the results phases are discussed in section\nHyperpartisan news detection: description of the phenomenon , section Approaches for automatic\nhyperpartisan news detection and section Datasets .\nResearch Questions\nThe Research Questions (RQ) that motivated the need for this systematic review are the fol-\nlowing:\nRQ1Does a categorization for hyperpartisan news detection methods exist?\nRQ2Is hyperpartisan news detection a stand-alone or over-lapping task?\nRQ3What are the proposed solutions using textual data?\nRQ4Does the task keep up with the new Natural Language Processing technologies like\nautoregressive models?\nRQ5What are the results of the models developed?\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 3/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 4 \u2014 #4ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nRQ6What are the datasets used for this task? How are they structured? Have they been\nupdated to cover the latest political global and regional trends?\nRQ7How can the current state of research on hyperpartisan detection be characterized in\ndiverse languages and countries?\nSearch strategy\nWe adopted the Preferred Reporting Items for Systematic Reviews and Meta-Analysis\n(PRISMA) guidelines [ 31], consisting of a checklist ( http://www.prisma-statement.org/\ndocuments/PRISMA_2020_checklist.pdf ) and a flow diagram Fig 1to illustrate in a sim-\nplified and clear way the steps made. To retrieve papers, primary different academic online\ndatabases were used to overcome their respective limitations [ 32] in terms of topic cover-\nage and papers available: ACM Digital Library, Google Scholar, Scopus, ProQuest, and IEE-\nExplore. Our query archetype was: ((hyperpartisan OR ``political bias'' OR\n``hyper-partisan'' OR partisanship OR hyperpartisanship OR ``political\npolarization'') AND (news OR bias OR articles) AND (detection OR\nclassification)) . The first set of words contains the different homographs. We also\nsearched in all subject fields, to capture as many semantically similar papers as possible,\nincluding potentially miscategorized papers. We selected the 2015-2024 timeframe to analyze\ntrends before the term was coined, considering a period in which studies on this topic grew,\nand increasingly powerful models were employed.\nFor the purpose of obtaining pertinent papers related to our questions, the queries\nreported in Table 1 were a refined result of a structured process based on different steps intro-\nduced by [ 29]. Queries within each database were structured to match titles, abstracts, and\nkeywords. We extended this pipeline, introducing the following step: \u201cNetwork visualiza-\ntion and exploration\u201d. It concerned the usage of the research software ResearchRabbit ( https:\n//researchrabbitapp.com ) to exhaustively capture possibly omitted papers through the citation\nlinks structure.\n\u2022Keywords domain extrapolation: Initial reviews on similar topics helped us identify\nthe keywords used in this domain. We noticed a lack of scientific agreement on writ-\ning \u201chyperpartisan\u201d. To cover all these morphologically diverse forms (hyper-partisan,\nhyperpartisan, hyper-partisan), we included them in our queries, treating them as syn-\nonyms;\n\u2022Iterative searches: This process allowed us to select the most appropriate terms by com-\nparing the results retrieved using different keywords combinations. We examined how\nmuch titles and abstracts related with the queries;\n\u2022Verifying against established literature: To ensure the efficiency of our search terms,\nwe compared the results to a list of papers in the domain of hyperpartisan detection;\n\u2022Network visualization and exploration: To further validate our verification, we used\nResearchRabbit, a tool to visualize the citation links between papers in the same collec-\ntion. It suggested similar papers written by the same or different authors, highlighting\nstored publications in the user\u2019s folder. This tool helped us in gauging the coherence of\nour results.\nSelection criteria\nBefore describing the screening process, we illustrate the criteria employed for the paper\nselection.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 4/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 5 \u2014 #5ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 1. Queries performed with advanced search for each database and the number of papers retrieved.\nDatabase Query Docs\nACM Digital Library [[[Title: hyperpartisan] OR [[Title:\npolitical polarization] AND [[Title:\nnews] OR [Title: article]]]] AND\n[[Abstract: hyperpartisan] OR\n[Abstract: media bias] OR [Abstract:\nor news] OR [All: article]]]\nOR [All: and] OR [[[Keywords:\nhyperpartisan] OR [[Keywords:\nmedia bias] AND [[Keywords: news]\nOR [Keywords: article]]]] AND\n[[Title: hyperpartisan] OR [Title:\nhyper-partisan] OR [Title: partisan]\nOR [Title: political polarization]\nOR [[Title: media bias] AND [[Title:\nnews] OR [Title: article]]]]] AND\n[E-Publication Date: (01/01/2015 TO\n12/31/2024)]]723\nGoogle Scholar (hyperpartisan OR hyper-partisan OR\nhyperpartisanship OR hyperpartisan\nORpolarization) AND (news OR\nbias OR articles) AND (detection\nORclassification)1800\nIEEE Xplore (All Metadata:hyperpartisan OR All\nMetadata:hyper-partisan OR All\nMetadata:hyperpartisanship OR All\nMetadata:hyper-partisanship) AND\n(All Metadata:detection)1\nProQuest hyperpartisan news + filters 159\nScienceDirect (hyperpartisan OR hyper-partisan\nOR political polarization OR media\nbias) AND NLP2\nScopus (hyperpartisan OR hyper-partisan OR\npartisanship OR hyperpartisanship\nOR political polarization) AND\n(news OR articles OR bias) AND\n(classification OR detection))97\nhttps://doi.org/10.1371/journal.pone.0316989.t001\n\u2022Inclusion criteria\n\u2022Papers primarily focused on automated hyperpartisan news detection;\n\u2022Papers that used the related task (e.g. fake news detection) as a synonym of hyper-\npartisan news detection;\n\u2022Publications from 2015 to 2024;\n\u2022Exclusion criteria\n\u2022Exclusion of sources that either address the hyperpartisan news detection problem\nfrom a theoretical perspective, namely theory papers, or manual detection;\n\u2022Studies discussing only related topics, such as fake news detection, stance detection,\nor political bias;\n\u2022Findings that do not use news domain datasets as the main source for hyperpar-\ntisan news detection, i.e. social network analysis, comments analysis, and tweets\ndetection-based approaches;\n\u2022Literature reviews, books, thesis and posters.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 5/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 6 \u2014 #6ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nScreening and selection process\nThe following search strategy and procedures for study selection and analysis were used.\nThe study selection, quality assessment of the included studies, and thematic analysis were\nperformed by one author (PP). However, the procedures and findings were discussed by all\nauthors, and potential disagreements were resolved by consensus.\nTo manage the screening and selection processes, we utilized Rayyan (https://www.rayyan.\nai/) for its AI-powered capabilities, which allowed the two reviewers to conduct a blinded\nselection process, preventing any mutual influence. Specific eligibility criteria were estab-\nlished to ensure the reliability of the study. These criteria were applied independently by each\nreviewer to maintain objectivity and consistency. The criteria included: relevance to the pre-\ndefined inclusion criteria, evaluation of models using both accuracy and F1 score, and com-\nprehensive reporting of the dataset used. Only papers that met the criteria and were accepted\nby both reviewers were selected. In cases where there was disagreement, a third reviewer was\nconsulted to assess the paper\u2019s eligibility. The initial dataset consisted of 723 papers from\nACM Digital Library ( https://dl.acm.org/ ), 571 from Google Scholar ( https://scholar.google.\ncom/), 1 from ScienceDirect ( https://www.sciencedirect.com/ ), 97 from Scopus ( https://www.\nscopus.com/home.uri ), 159 from ProQuest ( https://www.proquest.com/index ), and 1 from\nIEEE Xplorer ( https://ieeexplore.ieee.org/Xplore/home.jsp ). Notably, Google Scholar initially\nretrieved 1800 results, but we noted that, after the threshold of 500 results, it did not produce\nrelevant documents. That led us to manually collect only the first 571 papers.\nWe conducted the entire selection process using Rayyan, as described in Fig 1. It automat-\nically detected 118 duplicates. After manual checks, we removed them. Left with 1441 studies,\nscreening titles and abstracts was the initial step. Following thorough evaluations, 67 papers\nwere retained from a curated pool of 110, eliminating 43 papers that did not meet specific\nfocus or dataset criteria.\nAdditionally, to examine the cohesion and coherence of our references, we used\nResearchRabbit to visualize the citation network, identifying two prominent clusters with cen-\nters in [ 33] and [18]. [18] is a key work from the SemEval initiative, set a foundational bench-\nmark for detecting hyperpartisan in news articles, which informed our criteria for selecting\nrelevant studies. This shared task saw the participation of 42 teams. They explored several\napproaches that future research will expand upon it. Moreover, the two datasets described\nin are important benchmarks for hyperpartisan news detection. Similarly, [ 33] compared\nlinguistics and topical methodologies too discern between hyperpartisan and neutral news.\nThat was one of the first work in literature and defined the importance of linguistics features\nin this task. 14 additional papers were included after exploring similar works and citations\nthanks to this procedure. Lastly, we compared the several definitions of hyperpartisan news\nto stress the importance of having a specific and clear task not overlapping with related ones.\nOur work offers an extensive and comprehensive investigation of state-of-the-art techniques\nconsidering both mixed approaches, machine and deep learning application. To ensure our\nsystematic review is both homogeneous and robust in terms of comparability, we focused on\nthe most commonly used performance metrics in NLP: accuracy and F1 score. By collect-\ning and analyzing these standard metrics, we aim to maintain consistency across the stud-\nies and enhance the reliability of our comparative analysis. Lastly, we retrieved and analyzed\n38 datasets, reporting the evaluation metrics, embeddings and features used by researchers.\nFinally, we present some descriptive results regarding the trend of the publications over time\n(Fig 2) and the selected sample that highlight the main publishers ( Fig 3).\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 6/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 7 \u2014 #7ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nFig 1.PRISMA Flow Diagram. The Flow Diagram illustrates the steps during document collection and evaluation. We skimmed more than 1553 papers and finally we\nselected a subset of 81.\nhttps://doi.org/10.1371/journal.pone.0316989.g001\nTransparency and replicability\nEmphasis was placed on transparency and replicability to adhere to rigorous academic stan-\ndards required by PLOS ONE\u2019s policy on Data Availability. Thus, a GitHub repository stores\nthe queries employed and described in the paper as well as the results of the screening process\ndescribed above. This enables fellow researchers to replicate the methodology and verify the\nfindings. The repository is accessible at https://github.com/MichJoM/Hyperpartisan_News_\nDetection_Systematic_Review/tree/main . In addition to the previous information, it contains\nthe explanation of how missing data were handled.\nHyperpartisan news detection: Description of the phenomenon\nIn this section, we begin by examining the definitions of hyperpartisan news detection\nfound in the reviewed literature. We then delve into the various biases that are related to our\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 7/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 8 \u2014 #8ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nFig 2.The bar chart illustrates the trend of the selected publications over time.\nhttps://doi.org/10.1371/journal.pone.0316989.g002\nFig 3.The pie chart shows the main publishers for the selected papers.\nhttps://doi.org/10.1371/journal.pone.0316989.g003\ninvestigated phenomenon and constitute it. Additionally, we examined the diverse hyperpar-\ntisan sources and we provide an overview of the application domains. Finally, we discuss the\ndifferent strategies for labeling an entity as hyperpartisan.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 8/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 9 \u2014 #9ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nThe problematics of the definition\nDefinition of hyperpartisanship. The term Hyperpartisanship (https://claremontreview\nofbooks.com/hyperpartisanship/ ) is not certified in any dictionary. A widely accepted defi-\nnition considers hyperpartisan news as having an extreme bias toward a particular political\nideology or party [ 18]. This type of news reporting often presents information in a highly sen-\nsationalized and one-sided manner, prioritizing ideological loyalty over objective reporting\nand critical analysis. This behavior denotes an extreme political allegiance to a party, leading\nto intense disagreement with the opposing faction [ 18].\nVagueness of the definition and overlap with similar tasks. The minimalist definition of\nhyperpartisanship is widely adopted by computer scientists, who tend to simplify social phe-\nnomena models when applying automated detection [ 26]. Hyperpartisanship coexists within\nthe broader category of junk news and shares characteristics with tasks such as political, ide-\nological, and fake news detection [ 34]. Due to the vagueness of the definition, hyperparti-\nsan headlines are often difficult to cluster within the misinformation set, and there is a lack\nof consensus on what precisely constitutes hyperpartisanship [ 35]. The perception of news as\nhyperpartisan can depend on the reader\u2019s epistemic bubble [ 36]. Additionally, both left and\nright extremisms do not show significant stylistic differences, making hyperpartisanship a\nsubject-shifting concept [ 33]. While humans can assess the degree of hyperpartisanship in a\ngiven text due to their cultural and linguistic awareness, machines lack this capability.\nHyperpartisan news detection often overlaps or is confused with other disinformation\ntasks, such as fake news detection [ 19,37\u201340,94], and stance detection [ 41]. Specifically,\nhyperpartisanship might be conveyed through elements of fake news, aimed at propagating a\nspecific agenda and manipulating readers to adopt a particular position on a given topic [ 40].\nTraits of hyperpartisan news. From a linguistic perspective, hyperpartisan articles exhibit\na high count of adjectives and adverbs [ 42,43], extensive use of pronouns, and words of dis-\ngust [44]. These articles tend to feature longer paragraphs written in a sensationalist style,\nfull of emotional language and rare terms [ 45]. Right-wing media, in particular, often employ\nhyperpartisan headlines, corroborating earlier findings [ 46,47]. Hyperpartisan news arti-\ncles display hyper-polarized linguistic traits in their titles as well. However, hyperpartisan-\nship opposes to balanced news, which are intended to report facts with balanced tone and\ninformative intention.\nAnalogue biases. Hyperpartisan news detection is a task in which certain textual features\nindicated above suggest that the writer is expressing an extremist, one-sided opinion. More-\nover, various degrees with which typologies of bias occur contribute to make the text hyper-\npartisan. There are several taxonomies proposals for junk news like [ 28,34]. We will use the\nbias categories collected by Oxford ( https://catalogofbias.org/biases/spin-bias/ ) and [29] to\ndiscuss the founding biases of hyperpartisan articles.\nSpin bias , orrhetoric bias [29], strictly concerns the linguistic structure of the article, its\npersuasion. The deliberate or inadvertent misrepresentation of research outcomes, leading to\nunjustified indications of positive or negative results, potentially could result in misleading\nconclusions. Written language is the product of the conscious application of strategic discur-\nsive and persuasive patterns to interest the readers. The words contribute to giving a particular\nmeaning to the entire text, especially if they leverage an emotional lexicon with superlatives.\nAd hominem bias is a rhetorical strategy in which one moves away from the topic of the\ncontroversy by contesting not the statement of the interlocutor, but the interlocutor them-\nselves and his personal characteristics or traits [ 48]. This rhetorical strategy was frequently\nused in sophistry and is still widely used today in political discussions and journalistic contro-\nversies.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 9/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 10 \u2014 #10ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nPresence bias or opinion statement involves the inclusion of subjective opinions within\nnews articles, influencing readers\u2019 perceptions. It occurs when factual reporting is mingled\nwith subjective viewpoints or opinions [ 49]. In other words, it reflects the degree of agreement\nand statement sharing of an entity, i.e. users or publishers [ 50].\nIdeological bias occurs when news reporting or content is influenced by a particular ide-\nological stance or viewpoint, impacting the presentation and selection of news topics. Ideo-\nlogical detection is different from political bias because some ideologies can be shared even\nby opposite parties. Ideologies often contrast each other, but to be classified they need this\ncomparison [ 51].\nFraming bias involves presenting information to shape or influence people\u2019s perceptions of\nan issue or event by emphasizing certain aspects while downplaying others [ 52,120]. In this\ncase, using linguistics and rhetorical figures helps the author partially present the selected\ninformation. Therefore, framing expresses a publishers leaning towards an ideology. Frames\nare tools that emphasize specific information while potentially favoring one aspect over\nanother, with or without being slanted [ 53]. It is performed in moral content and style used\n[21].\nCoverage bias , is not present in Table 2 since it is not a textual bias. It refers to the dispro-\nportionate attention or neglect of topics or events in news reporting, leading to an imbalance\nin coverage across different subjects [ 54].\nPolitical bias could be easily confused with ideological bias. Since a party is a combination\nof both an ideology and a political leaning, this bias is related to the inclination of news media\nor information sources or people to favor one political party\u2019s agenda [ 55].\nIn this context, it is essential to avoid conflating the reification of the social phenomenon\ninvolving linguistic indicators with the entirety of the specified biases. Namely, not all cate-\ngories of biases mentioned can be classified as hyperpartisan when they manifest. The linguis-\ntic element of exaggeration per sedoes not automatically denote hyperpartisanship; rather,\nit necessitates contextual positioning, such as aligning with a particular party or ideology.\nSimply adopting a stance is insufficient for categorization as hyperpartisan; it is the degree of\nexaggeration in that stance that holds significance. We propose some examples to illustrate\nthis inTable 2.\nTable 2. Examples of statements for specific biases and hyperpartisan statements for that bias.\nBias Biased Example Hyperpartisan Biased Example\nSpin bias How can we trust their solutions\nwhen they fail to understand the basic\nprinciples of economics?Their economic policies are a disaster,\nproving yet again their ignorance and\nincompetence.\nAd hominem The proposal is flawed because it comes\nfrom someone who has no experience in\nthe field.We can\u2019t expect anything good from\nsomeone who has never done anything\nworthwhile!\nOpinion statement bias It\u2019s clear that this is the best approach for\nour society.This is unequivocally the only right path\nforward for our nation; anyone who\ndisagrees is simply blind to the truth.\nIdeology bias Socialist policies always lead to\ninefficiency and economic downfall.The leftist agenda destroys economies\nand personal freedoms every time it\u2019s\nimplemented.\nFraming bias The data support that this policy will\ndecrease crime rates.The indisputable facts confirm that only\nthis policy can save us from spiralling\ninto a crime-infested nightmare.\nPolitical bias This party\u2019s proposal will bankrupt the\nnation.The opposition\u2019s plan is a surefire way to\nplunge our country into insurmountable\ndebt and chaos.\nhttps://doi.org/10.1371/journal.pone.0316989.t002\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 10/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 11 \u2014 #11ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nProposal for a definition. By reviewing the various definitions collected in Table 3, sev-\neral key observations emerge:\n\u2022the concept of hyperpartisanship is an intersected field and shares features with the\ntypologies of media biases discussed in section Analogue bias . In the following list\nthe indexes define the intrinsic characteristics in the hyperpartisan definitions in 3,\n\u201cCharacteristic\u201d column:\n1.spin bias;\n2.ad hominem bias;\n3.opinion statement bias;\n4.ideology bias;\n5.framing bias;\n6.coverage bias;\n7.political bias;\n\u2022it is commonly acknowledged that hyperpartisan news exhibits one-sided political bias,\nincorporating specific statements aligning with the ideology of a particular political\nparty and/or agenda;\n\u2022the lack of a commonly shared definition across various studies results in the character-\nistics of detection being variable and mutually exclusive undermining the integrity and\nscientific rigor of research in this field;\n\u2022while approaching this classification task, some researches like [ 56] lack a methodologi-\ncal approach because do not introduce a definition of the phenomenon.\nIn light of these considerations, hyperpartisan detection must necessarily consider dif-\nferent variables simultaneously: positioning, presence of a bias and its degree of exaggera-\ntion. Does the current state of the art in detection methodologies do this? As mentioned ear-\nlier, a detection method that simultaneously considers the different types of biases and these\nthree variables has not been conducted. Various research works tend to focus individually on\nspecific subsets of linguistic and content-based features, as outlined in the following sections.\nConsidering these elements, we propose the following definition to aid future research in\naddressing hyperpartisan news in Computer Science: Hyperpartisan news detection is the\nprocess of identifying news articles that exhibit extreme one-sidedness, characterized by a\npronounced use of bias. The prefix \u201dhyper-\u201d highlights the exaggerated application of at least\none specific type of bias\u2014such as spin, ad hominem attacks, opinionated statements, ideolog-\nical slants, framing, selective coverage, political leaning, or slant bias\u2014to promote a particu-\nlar ideological perspective. This strong ideological alignment is conveyed through amplified\nlinguistic elements that reinforce one of these bias types within the text.\nWhere can hyperpartisanship be detected? Perspectives on the sources\nIn this section, we will give a general overview of the main sources typologies considered to\ndetect hyperpartisan news articles.\nIn light of the prevalence of hyperpartisan news dissemination online, the methodolo-\ngies discovered are implemented specifically on online news outlets. Initially, when con-\nsidering the domain of publishers, a linguistic approach can be applied to news analysis to\ndetect hyperpartisanship. This approach involves studying textual information within arti-\ncles using style-based or topic-based models [ 33,46,62,128]. Detection methods consider spe-\ncific sections, such as the title [ 46,47], sentences [ 63], quotes in the body [ 42], or encompass\nboth [46,58,64,65,94]. Otherwise, researchers investigated hyperpartisanship spread starting\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 11/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 12 \u2014 #12ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 3. Definitions of hyperpartisanship given in the selected papers.\nReference Claim Characteristics\n[57] Social media also have facilitated the rise of hyperpartisan news. Hyperpartisan news: (1) has\nan obviously one-sided political agenda, which makes no effort to balance opposing views; (2)\npushes anti-system messages that are critical of both mainstream media and establishment\npolitics, often relying on misinformation to do so; and (3) relies heavily on social media as a\nplatform for dissemination. Thus, hyperpartisan news can be situated squarely at the intersec-\ntion of partisan and alternative news, and considerable overlap exists between hyperpartisan\nnews and \u201cfake\u201d news.1, 6, 7\n[18] Hyperpartisan articles mimic the form of regular news articles but are one-sided in the sense\nthat opposing views are either ignored or fiercely attacked.1, 2, 3, 4, 5, 7, 8\n[33] Prone to misunderstanding and misuse, the term \u201cfake news\u201d arose from the observation\nthat, in social media, a certain kind of \u201cnews\u201d spreads much more successfully than others,\nand this kind of \u201cnews\u201d is typically extremely one-sided (hyperpartisan), inflammatory,\nemotional, and often riddled with untruths.1, 3, 4, 5, 7, 8\n[46] We think that a better understanding of hyperpartisanship can be achieved by considering\nnot only (1) the news that contains one-sided opinions but also (2) the news that describes\nconflicts and the underlying politically polarized climate because both of them could lead\nto an increase in the public\u2019s perceived polarization (Yang et al. 2016; Fiorina, Abrams, and\nPope 2005; Levendusky and Malhotra 2016). Additionally, coverage quantity itself can be\nconsidered as a particular form of bias (Lin, Bagrow, and Lazer 2011). In particular, we seek\nto extend previous studies\u2019 definitions of hyperpartisan news to include news that covers\npartisan conflicts and confrontations.1, 3, 4, 7\n[58] Media bias can be observed and defined through various factors. In the political domain, it\nranges from selectively publishing articles to specifically choosing to highlight some events,\nparties and leaders. We also come across articles where bias can be detected by observing the\nunclear assumptions, loaded language, or lack of proper context.1, 6, 7, 8\n[59] Researchers use different terms to indicate the same issue, namely disinformation, misinfor-\nmation, propaganda, junk news and click-bait. In this work we use the word disinformation,\nrather than the more popular \u201cfake news\u201d, to refer to a variety of low credibility content which\ncomprises false news intended to harm, misleading and non-factual reporting, hyper-partisan\nnews and propaganda, and unverified rumours1, 4, 7\n[60] Hyperpartisan news is news riddled with untruth and twisted statements of information. This\ntype of news spread more successfully than others. Hyperpartisan news not only can mislead\nreaders but also cause polarisation within a community or society.1, 4, 7\n[61] Although there are more subcategories related to fake news such as satire, parody or click-\nbait, a general definition of the term could be the following: fake news represents a way to\nspread false information to mislead the public, damage the reputation of an entity or have a\npolitical or financial gain. The idea of misleading and influencing the public is also linked to\nthe notion of hyper-partisan news that has the role of presenting extremist or conspiratorial\nopinions with intentional misconceptions.2, 3, 4, 5, 7\n[37] The aim is to classify news as real or fake. Fake news is news that is intentionally generated\nto misguide people. It may exist in various forms, including misleading content, biased news,\nsatirical news, rumours, hyper-partisan news, deceptive news, disinformation, clickbait, and\nhoax.1, 4, 7\nhttps://doi.org/10.1371/journal.pone.0316989.t003\nfrom entities involved in the writing and publishing process, like journalist\u2019s [ 66] or media\n[18] leaning . Considering publishers as entities often interconnected through economic\nand political bonds [ 67], they form a polarized network, which can be analyzed using meta-\ndata like external links [ 68\u201370,138]. While determining bias based on the source is feasible\n[66,71], an article from a biased media outlet may not always be hyperpartisan [ 49,72]. This\nissue was underscored by [ 72], which highlighted the inadequacy of the information source\nin determining an article\u2019s hyperpartisanship. This method generates a system capable of indi-\ncating bias scores in news and suggesting similar topics from different sources to encourage\nreadership of diverse perspectives or to avoid extremely biased news.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 12/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 13 \u2014 #13ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nWorking with textual data enables the extraction of sentiment features [ 73]. For instance,\n[74] observed that sentiment analysis, applied to titles and sentences using TextBlob ( https:\n//textblob.readthedocs.io/en/dev/ ), improved evaluation metrics. Additionally, [ 75] noted\nthat hyperpartisan articles tend to convey more aggressive and negative sentiments compared\nto other articles. Using VADER ( https://github.com/cjhutto/vaderSentiment ), [76] conducted\nexperiments to analyze the contribution of sentiment features in indicating the author\u2019s bias.\nMeanwhile, [ 77] approached hyperpartisan news detection by considering sentiment as a\nmeans to capture the polarity of articles.\nMoreover, [ 78] employed both textual and image features to detect hyperpartisanship.\nTheir study revealed that automated methods outperformed humans and that incorporating\nadditional information such as images and titles enhanced the accuracy of the model.\nHow hyperpartisanship is labeled?\nUnderstanding the measurement of hyperpartisanship involves considering the diverse scales\nutilized. In Social Sciences, a range of indexes and scales is employed for this purpose, lever-\naging distinct features from those used in automatic detection methodologies. For instance,\npolarization is calculated with the CSES Polarization Index. The Common CSES Polarization\nIndex (PI) is a tool used to assess the distribution of political parties across the Left/Right ide-\nological spectrum. These metrics gauge ideological positioning and account for party sizes\nor vote shares, offering a comprehensive view of ideological stance and political influence\n[3]. Differently, automatic hyperpartisan detection relies on linguistic features. Some stud-\nies employ binary classification methods, utilizing labels such as hyperpartisan/mainstream\n(i.e. non-hyperpartisan) [ 18], Left/Right [ 66,118]. However, such distinctions often overlook\nnuanced differences within diverse political leanings [ 33]. Few studies have extended their\nscope to include a more fine-grained polarization range [ 79]. For example, [ 80] approached\nhyperpartisan detection as a multi-class classification problem, employing both 7- and 5-\npoint scales to define affiliations: 1-2.5 \u2013 far-left, 2.5-3.5 \u2013 center-left, 3.5-4.5 \u2013 center, 4.5-\n5.5 \u2013 center-right, 5.5-7 \u2013 far-right. Similarly, [ 73] used a scale and [ 81] sought to manage\ngranularity by distinguishing between right, center, and left affiliations.\nApproaches for automatic hyperpartisan news detection\nThe detection of hyperpartisan content encompasses a range of methodologies, varying from\ntraditional non-deep learning approaches to cutting-edge deep learning techniques, as well\nas mixed learning algorithms. Non-deep learning methodologies often rely on traditional\nmachine learning algorithms, leveraging handcrafted features and rule-based systems to iden-\ntify linguistic patterns, stylistic markers, and network structures within textual and metadata\nsources. These approaches commonly include stylometric analysis and topic modeling meth-\nods to discern biased content. In contrast, deep learning methodologies harness the power\nof neural networks to automatically extract intricate features from raw data, enabling the\nidentification of complex patterns and relationships in unstructured text or network data.\nThese techniques, such as convolutional neural networks (CNNs), recurrent neural networks\n(RNNs), and Transformers, excel in learning representations directly from the data.\nModels discussion\nIn the following subsections, to encompass the risk of bias, we grouped and discussed the\nmentioned studies by model architecture. We will differentiate between Non-deep learning,\nDeep learning, and other methodologies adopted in the papers selected for the systematic\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 13/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 14 \u2014 #14ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nreview. The Deep Learning section includes Non-transformer\u2019s Deep Learning models and\nthe Transformers family. In the following tables: 4,5,6andTable 7, we categorize the best\nmodel for each paper, reporting its performance. When researchers compared more mod-\nels on the same task using different datasets, like the case of [ 118], we report only the best\nmodel\u2019s performance.\nNon-deep learning methods. InTable 4, we categorized papers using the traditional\nmachine learning approaches. The methodologies involved algorithms like Support Vector\nMachines, Random Forest, and Logistic Regression, followed by Linear regression, Naive\nBayes, Linear SVC, KNN, XGBoost, and Maxent.\nThere are effective strategies adopted with the SVM model. For instance, [ 70] used this\nalgorithm combined with the sentiment analysis via National Council Canada\u2019s Emotion\nLexicon (NRC Emotion Lexicon) to analyze the emotional content in the article. Moreover,\nthey extended the linguistic approach applyinh Linguistic Inquiry and Word Count (LIWC).\nThey also considered the articles\u2019 structure and meta-data as features. [ 94], adopted n-grams,\nTable 4. This table describes the traditional Machine Learning algorithms used in the selected literature.\nAlgorithm Reference\nSupport Vector Machine (SVM) [ 82] [42\u201344,70,76,83,94]\nXGBoost [ 84] [85]\nMaxent [ 86] [40]\nRandom Forest [ 87] [64,80,88\u201390]\nNaive Bayes [47,74]\nCopula Ordinal Regression [73]\nLogistic Regression [37,45,77,91,92]\nLinear Support Vector Classifier (SVC) [75]\nLinear Classifier [93]\nhttps://doi.org/10.1371/journal.pone.0316989.t004\nTable 5. Features used with the best models described in Table 4. The features described in the columns are the following: Morpho-syntactic (MS), Lexicon\n(L), Semantic (S), Sentiment (SE) and Metadata (M). The approaches are: Style-based (SB) and Topic-based (TB).\nReference Method Model Dataset Lang. CMAcc.F1MSLS SEMSBTB\nNaredla, N.R., 2022 ML RF SE19 Eng U.S..88U/A ELMo emb x\nGarg, S., 2022 ML LR Reuter Eng U.S.U/A.93 ELMo emb x\nDumitru, V.C., 2019 ML LSVM Own Eng U.S..93.81 TF-IDF xx\nSrivastava, V., 2019 ML LR SE19 Eng U.S..82.82xxUSE x x\nHanawa, K., 2019 ML LC SE19 Eng U.S..81.80 BERT emb x\nYeh, C.L., 2019 ML SVM SE19 Eng U.S..80.79 BoW\nPali\u0107, N., 2019 ML SVC SE19 Eng U.S..79.76xxWord2Vec xx\nStevanoski, B., 2019 ML RF SE19 Eng U.S..77.74xxWord2Vec xx\nNguyen, D.V., 2019 ML SVM SE19 Eng U.S..75.74x N-grams x\nChen, C., 2019 ML NB SE19 Eng U.S..74.74xxBoW x\nAgerri, R., 2019 ML Maxent SE19 Eng U.S..74.73x Word2Vec x\nAlabdulkarim, A., 2019 ML SVM SE19 Eng U.S..74.71xxTF-IDF xxx\nSaleh, A., 2019 ML LR SE19 Eng U.S..73.73xxBoW x\nBestgen, Y., 2019 ML LR SE19* Eng U.S..70.68x BoW\nKnauth, J., 2019 ML SVM SE19 Eng U.S..67.69xx xx\nCruz, A., 2019 ML RF SE19 Eng U.S..72.67x BoW x\nSengupta, S., 2019 ML LR SE19 Eng U.S..70.68 unigrams\nAmason, E., 2019 ML NB SE19 Eng U.S..65.63xxBoW x x\nAnthonio, T., 2019 ML SVM SE19 Eng U.S..62.69xx x\nChakravartula, N., 2019 ML RF SE19* Eng U.S..61.66 BoW\nGupta, V., 2019 ML XGBoost SE19 Eng U.S.0.550.28x N-grams x x\nBaly, R., 2019 ML COR MBFC Eng U.S.U/AU/AxxWord2Vec\nhttps://doi.org/10.1371/journal.pone.0316989.t005\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 14/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 15 \u2014 #15ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 6. Collection of the most performant deep learning models used in the literature.\nAlgorithm Description Reference\nRecurrent Neural Network-based\n(RNN) [ 100]Recurrent Neural Networks are a class of artificial neural networks\ndesigned to process sequential data by maintaining an internal mem-\nory. They possess loops that allow information persistence, enabling\nthem to consider past inputs for present computations, making them\nadept at handling time series or sequential data in various applications.[51]\nMesh Neural Network [ 79] It is a neural network aiming at maximazing the weight of past exam-\nples. Its structure is composed by recurrent nodes that boost the\ninductive abilities of the machine.[79]\nCatboost [ 101] Catboost stands out as a specialized library crafted for boosting gradi-\nents. It employs a unique algorithm to prevent target leakage, ensuring\nhigh efficiency and precision.[81]\nConvolutional Neural Network Convolutional Neural Networks were designed for image processing\nbut were applied to Natural Language Processing, in which the input\ntext is a one-dimensional vector of tokens, or like a matrix of embed-\ndings. This model can learn both local and global dependencies as well\nas hierarchical features in the data.[42,65,72,102\u2013105,138]\nHierarchical Attention Network\n(HAN) [ 106]The HAN is a model capable of balancing the information in a current\nstate, deciding whether to update it and how much the past information\ncontributes to its new state.[58,66,107\u2013109]\nLong Short Term Memory-based\nmodels [ 110]Unidirectional LSTM retain information solely from preceding inputs\nas it has only processed data from the past. Employing a bidirectional\napproach involves processing inputs in two directions: from past to\nfuture and from future to past. The distinctive aspect of this method,\ncompared to unidirectional LSTM, lies in the LSTM running in reverse,\nwhich captures information from the future. By amalgamating the two\nhidden states, this technique enables the retention of information from\nboth the past and future at any given time. In NLP, the main difference\nlies in the context captured. Bi-LSTM can discover hidden reversed\nsemantic and syntactic structures.[111\u2013114,120]\nRoBERTa [ 115] RoBERTa, an extension of BERT, refines language understanding by\noptimizing training techniques, removing sentence order prediction,\nand leveraging large-scale data. It enhances pre-training for improved\nperformance across various language tasks, achieving state-of-the-art\nresults in natural language processing.[116\u2013118,128]\nBERT [ 119] BERT (Bidirectional Encoder Representations from Transformers) is\na pre-trained language model by Google, adept at understanding the\ncontext in both directions of a sentence. It utilizes Transformer archi-\ntecture, enabling versatile language understanding for diverse tasks like\nquestion answering and natural language understanding.[46,49,60,62,105,118,121\u2013127,\n129,132,137,139]\nhttps://doi.org/10.1371/journal.pone.0316989.t006\ni.e. bi- and tri-grams, and dependency sub-trees that impacted the performance. On the\nother hand, [ 83] experimented several embeddings: Doc2Vec [ 95], Glove [ 96], ELMo [ 97].\nThey found out that \u201cadding simple lexical and sentiment features hurts the performance\u201d.\n[43] studied the linguistics divergencies between fake and hyperpartisan news employing an\nSVM. In this case, it emerged that hyperpartisan articles exhibit more sentences and a higher\nadjective count compared to unbiased news. When comparing the characteristics of extreme\npolarized articles against fake news, they noted that the former contains high usage of\nquestion/exclamation marks and adjectives. These sentence-related features delineate dis-\ntinct linguistic patterns. [ 77] confirmed the robust potentialities of the Logistic Regression,\nranking in the second place at the SemEval-2019. [ 77] built representations with Universal\nSentence Encoder (USE) [ 98] and combined both semantic and handcrafted features, paying\nattention to the grade of the adjectives and subjectivity and distinguishing between two lev-\nels of polarity: sentence and article level. [ 37] used the Reuter Dataset for the training and the\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 15/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 16 \u2014 #16ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 7. This table describes the best performance of the models.\nReference Dataset Language Media\u2019s\nCountryMethod Model Accuracy F1\n[46] Own English U.S. DL BERT 0.84 0.78\n[129] Own Persian Iran Other ChatGPT 0.85 0.85\n[109] Semeval-2019 English U.S. DL HAN 0.95 U/A\n[116] Semeval-2019 English U.S. DL MP-tuning 0.91 U/A\n[128] Semeval-2019 English U.S. DL RoBERTa 0.84 0.83\n[124] Semeval-2019 English U.S. DL BERT 0.83 U/A\n[125] Semeval-2019 by-article +\nby-publisherEnglish U.S. DL BERT U/A 0.91\n[81] Task 3A English U.S. DL CatBoost 0.69 0.69\n[64] Semeval-2019 English U.S. ML Random Forest 0.88 U/A\n[117] Semeval-2019 English U.S. DL RoBERTa 0.85 0.85\n[118] Framing Triplet Dataset English U.S. DL multi-task BERT-based 0.84 U/A\n[79] Own English U.S. DL Mesh Neural Network 0.45 U/A\n[66] Presidential English U.S. DL HAN 0.91 0.90\n[37] Reuter English U.S. ML Logistic Regression U/A 0.93\n[132] Stereoimmigrants Spanish U.S. DL BERT 0.86 0.83\n[62] BuzzFeed-Webis Fake News English U.S. DL BERT 0.89 0.86\n[112] Semeval-2019 English U.S. DL LSTM 0.86 0.84\n[102] BuzzFeed-Webis Fake News English U.S. DL CNN U/A 0.73\n[105] Semeval-2019 by-article +\nby-publisherEnglish U.S. DL BERT 0.87 0.81\n[80] GERMAN Dataset German Germany ML Random Forest U/A 0.79\n[107] Semeval-2019 English U.S. DL HAN 0.82 0.81\n[120] Own English U.S. DL LSTM U/A 0.80\n[49] PoliNews English U.S. DL BERT U/A U/A\n[51] Own English India DL RNN 0.84 0.87\n[121] Own English U.S. DL BERT 0.72 U/A\n[43] Own English U.S. ML LSVM 0.93 0.81\n[72] Semeval-2019 English U.S. DL CNN 0.82 0.81\n[77] Semeval-2019 English U.S. ML Logistic Regression 0.82 0.82\n[93] Semeval-2019 English U.S. ML Linear Classifier 0.81 0.80\n[111] Semeval-2019 English U.S. DL LSTM 0.80 0.80\n[83] Semeval-2019 English U.S. ML SVM 0.80 0.79\n[75] Semeval-2019 English U.S. ML SVC 0.79 0.76\n[122] Semeval-2019 English U.S. DL BERT 0.78 0.77\n[127] Semeval-2019 English U.S. DL BERT 0.78 0.76\n[126] Semeval-2019 English U.S. DL BERT 0.77 0.75\n[90] Semeval-2019 English U.S. ML Random Forest 0.77 0.74\n[137] Semeval-2019 English U.S. DL BERT 0.76 0.76\n[94] Semeval-2019 English U.S. ML SVM 0.75 0.74\n[40] Semeval-2019 English U.S. ML Maxent 0.74 0.73\n[74] Semeval-2019 English U.S. ML Naive Bayes 0.74 0.74\n[138] Semeval-2019 English U.S. DL CNN 0.74 0.70\n[42] Semeval-2019 English U.S. DL CNN + LSTM 0.74 0.71\n[91] Semeval-2019 English U.S. ML Logistic Regression 0.73 0.73\n[89] Semeval-2019 English U.S. ML Random Forest 0.72 0.67\n[108] Semeval-2019 English U.S. DL HAN 0.72 0.69\n[45] Semeval-2019 English U.S. ML Logistic Regression 0.70 0.68\n[114] Semeval-2019 English U.S. DL LSTM 0.68 0.63\n[104] Semeval-2019 English U.S. DL CNN 0.67 0.74\n[47] Semeval-2019 English U.S. ML Naive Bayes 0.65 0.73\n[123] Semeval-2019 English U.S. DL BERT 0.64 0.64\n[76] Semeval-2019 English U.S. ML SVM 0.62 0.69\n[103] Semeval-2019 English U.S. DL CNN 0.60 0.70\n[113] Semeval-2019 English U.S. DL LSTM 0.58 0.68\n[131] Semeval-2019 English U.S. Other Gagavai Explorer 0.56 0.68\n[73] MBFC English U.S. ML Copula Ordinal Regression U/A U/A\n(Continued )\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 16/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 17 \u2014 #17ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 7 .(Continued)\nReference Dataset Language Media\u2019s\nCountryMethod Model Accuracy F1\n[85] Semeval-2019 English U.S. ML XGBoost 0.55 0.28\n[139] Semeval-2019 English U.S. DL BERT 0.50 0.61\n[92] Semeval-2019 by-publisher English U.S. ML Logistic Regression 0.70 0.68\n[65] Semeval-2019 by-publisher English U.S. DL CNN 0.66 0.70\n[88] Semeval-2019 by-publisher English U.S. ML Random Forest 0.61 0.66\n[60] Semeval-2019 by-article +\nby-publisherEnglish U.S. DL BERT 0.68 U/A\n[58] Telugu Telugu India DL HAN 0.89 U/A\n[70] Semeval-2019 English U.S. ML SVM 0.74 0.71\n[44] Semeval-2019 English U.S. ML SVM 0.67 0.69\n[33] BuzzFeed-Webis Fake News English U.S. ML U/A 0.75 0.78\n[69] Own English U.S. Other MVDAM 0.80 0.79\nhttps://doi.org/10.1371/journal.pone.0316989.t007\ntest, combining the ELMo embedding with a logistic regression classifier as already done by\n[72] and [77], confirming the effectiveness of this method. [ 77] discovered that the most rel-\nevant features concern bias lexicon and polarity. [ 93] placed third at the SemEval-2019 and\nfound that article length was a distinctive trait of biased articles. By working at the phrase\nlevel, they created a set of phrases to discern the different types of articles, paying attention\nto removing n-grams containing publishers\u2019 style biases. [ 46] focused on news titles with a\ntopic-based approach. They also built a dataset considering two distinct typologies of news\ntitles, augmenting the granularity of the detection. The first category pertains to descriptions\nof confrontations or conflicts between opposing parties, suggesting a deeply polarized politi-\ncal climate. The second set involves opinions that express a biased, inflammatory, and aggres-\nsive stance against a policy, a political party, or a politician. [ 73] thought there was an inter-\ndependence between factuality and political ideology bias, so that introduced a multi-task\nlearning setup with the Copula Ordinal Regression (COR) [ 99]. They used the entire news\noutlet and considered diverse scales for measuring factuality (3-point scale) and political bias\n(7-point scale). [ 40] with Maximum Entropy Modeling (MaxEnt) by-passed linguistics fea-\ntures to build a model capable of generalizing as much as possible, [ 40] devised a document\nclassification system that combines clustering features with simple local features. They show-\ncased the effectiveness of employing distributional features from large in-domain unlabeled\ndata. [85] approached the task using n-gram embeddings with article and title polarity, imple-\nmenting the XGBoost model with all of these scalar features, but it performed poorly. They\nderived their methodology of applying stylometric analysis from [ 33]. This approach uti-\nlized n-grams, readability scores and Part-of-Speech (PoS) followed by binary classification.\nThanks to unmasking information, they simultaneously compared documents with opposite\npolitical leaning. In doing so, [ 33] investigated the style variations depending on the polit-\nical orientation and confronted it with a topic-based bag-of-words models. This methodol-\nogy highlighted the limited usefulness of integrating corpus characteristics when performing\na granular distinction amongst left, right and mainstream styles. Indeed, both the political\nextremes show similarities and can produce confounding effects in the model. Hence, con-\ncerning the style analysis for the hyperpartisan detection, the categories should be limited to\nmainstream and hyperpartisan without considering the specific leaning.\nFurthermore, for a complete understanding of the approaches used in the literature,\nwe summarized them in the Table 5. In this case, although ELMo, BERT, and Word2Vec\nembeddings were used as features of Non-Deep Learning algorithms. Table 5 describes only\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 17/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 18 \u2014 #18ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nthe features used gwith the best models proposed in Non-Deep Learning approaches in\nTable 4. We distinguish between features (Morpho-syntactic, Lexicon, Semantic, Sentiment\nand Metadata) and approaches (style-based and topic-based).\nDeep learning methods. In the following paragraphs, we analyzed the Deep Learning\nmethods adopted by diverse authors to solve the hyperpartisan detection task. In Table 6, we\ncategorized papers using the traditional machine learning approaches. Lastly, at the end of the\nsection, a comprehensive Table 7 collects and illustrates the results rounded to two decimals\nreported by all the authors studied in our systematic review.\nDeep learning: Non transformer-based architectures. [42] employed a fusion of CNN\nand LSTM, utilizing quantitative linguistic features extracted through GloVe. In this way,\nthey highlighted the crucial role of incorporating linguistic features alongside representations\nbased on word vectors. Additionally, they built a meta-classifier to filter noisy data to apply\nto the by-publisher dataset. [ 72] won the SemEval-2019 Task 4 by combining rich morpho-\nlogical and contextual representations by averaging the three vectors per word into ELMo\nembeddings. Their model was used for further studies by [ 105] for pseudo-labeling frame-\nworks: Overlap-checking and Meta-learning. Overlap-checking consists of adding data, help-\ning the model train, while Meta-learning allows the model to be continually trained on a clean\ndataset and a pseudo dataset. This last work inspired [ 107]. In their article, they used a HAN\ncombined with ELMo embeddings. The HAN is a model capable of balancing the informa-\ntion in a current state, deciding whether to update it and how much the past information\ncontributes to its new state. In this case, the information stems from sentence level, confirm-\ning that richer article representations yield better performances. By encapsulating the arti-\ncles\u2019 structure, connectors and paying attention to stylistic markers, handcrafted stylistic fea-\ntures and emotion lexicons, they reached the state-of-the-art in 2020 on the SemEval-2019\nTask 4 dataset. [ 109] improved the HAN standard model by introducing Knowledge Encod-\ning (KE) components. The HAN segment functions to grasp word and sentence relationships\nwithin a news article, employing a structured hierarchy across three levels\u2014word, sentence,\nand title. Meanwhile, the KE component integrates common and political knowledge associ-\nated with real-world entities into the prediction process for determining the political stance\nof the news article. Since the model is not language-based, it could work with diverse lan-\nguages beyond English. [ 112] developed a pre-training framework encoding knowledge about\nentity mentions, namely masked tokens as frame indicators, and modeling the propagation\nbetween users with a social information graph. They noted that models pre-trained on gen-\neral sources and tasks have limited ability to focus on biased text segments. [ 113] introduced a\nvoting system of LSTMs to build a controlled dataset to train another LSTM. It was an exam-\nple to demonstrate the importance of having a balanced and clean dataset to run experiments.\nLastly, [ 120] built a Hierarchical-LSTM applied to subframes (n-grams) to tackle the fram-\ning bias. In this paper, they introduced a pioneering framework aimed at pretraining text\nmodels utilizing signals derived from the abundant social and linguistic context available,\nencompassing elements such as entity mentions, news dissemination, and frame indicators.\nDeep learning: Transformer-based architectures. Regarding the Transformers archi-\ntectures, we observed a massive utilization of BERTbase and BERTlarge. BERTbase is a pre-\ntrained BERT model trained on a smaller dataset than BERTlarge. BERTbase differentiates\nitself in cased and uncased, depending on whether to discern between cases and uncased\nwords. [ 121] wanted to remove the bias when modeling the medium. They observed that\ncombining bias mitigation with triplet loss, Twitter bios and media-level representations\nincreased the model efficacy. [ 118] proposed a multi-task BERT-based model with contrastive\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 18/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 19 \u2014 #19ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nlearning to tackle framing bias in news articles. [ 122] with BERT and combinations of syntac-\ntic bigram counts and psycholinguistic features investigated the inference of political infor-\nmation and hyperpartisanship on author and text level starting from linguistic data. [ 123]\nshowed that fine-tuning the model entails better results. [ 124] introduced a semi-supervised\nframework trained using federating learning, namely algorithms are trained independently\nacross diverse datasets. Furthermore, textual data are tagged to extrapolate wh-questions\nreplies and temporal lexicon information. The same author replicated this approach in [ 125].\nIn the quest for precise detection and data denoising, the same author replicated this approach\nwith variations in [ 125]. [125] employed an attention-based strategy to learn text represen-\ntation, aiming to identify target expressions accurately while extracting pertinent contextual\ninformation. They generated a BERT attention embedding query utilizing lexicon expansion,\ncontent segmentation and temporal event analysis. Ultimately, this approach enhances the\nunderstanding of consecutive news articles within a temporal framework. [ 126] experimented\nusing BERTbase and BERTlarge feeding them with embeddings of different lengths. They\nwere interested in analyzing the parts of the articles, looking for a consistent level of hyperpar-\ntisanship that demonstrated to exist. [ 60] from the confrontation between BERT and ELMo\nmodels, confirmed that the inputs and embeddings dimensions contributed to affecting posi-\ntively the performance. [ 127] performed domain adaptation, showing its efficacy. [ 116] oper-\nated in a low-resource scenario with prompt-based learning and employed masked political\nphrase prediction and a frozen pre-trained language model that relies on transformer archi-\ntecture, utilizing the robustly optimized BERT approach known as RoBERTa as a backbone\nfor their own model, MP-tuning. [ 117] focuses on political ideology and stance detection,\ncomparing triplets of documents on the same history to detect dissimilarities amongst them.\nThey trained RoBERTa through continual learning. Whereas, [ 128] improved their model\u2019s\nperformance with cross-domain contrastive learning and this work is noticeable that they\nused GPT-2 for augmenting hyperpartisan textual data. Lastly, [ 129] faced the task for Persian\nhyperpartisan tweets by prompting GPT-3.5, a multi-language conversational generative LLM\nreleased in 2022, and open-weights model like Llama2 [ 130]. [129] compared the capabilities\nof Large Language Models (LLMs) and BERT-based models like RoBERTa and ParseBERT\nto detect English and Persian tweets, providing instructions with different levels of specifity\nto the models. Despite the huge dimension and the extensive training of LLMs, fine-tuning\nParseBERT and RoBERTa has proven to be more efficient and practical for certain tasks.\nOther methods. Within the vast landscape of computational frameworks, certain algo-\nrithms defy classification within the traditional realms of deep learning or non-deep learning.\nThis chapter delves into the exploration of these unique frameworks\u2014sophisticated combina-\ntions of diverse models, labeling techniques and graph approaches\u2014that operate beyond the\nconventional boundaries of established categorizations.\n[49] applies a framework for presentation bias, studying hyperpartisanship with a graph-\nbased method. This three-step framework is so structured: collecting related-articles clusters\non the same topic; applying Aspect-based Sentiment Analysis (ABSA) with BERTbase to rate\nand classify fine-grained opinions in the pairs of sentences; the variation in bias between news\nsources within similar categories is figured out by contrasting the scores of matching pairs of\narticles. This comparison is done for every combination of news sources within these cate-\ngories, and the differences in bias are averaged across all article groups. This averaging process\nleads to the development of a bias matrix. [ 69] proposed a Multi-View Document Attention\nModel (MVDAM) capable of modeling at the same time title, structure and metadata like\nlinks in order to estimate the political ideology of a news article. This framework based on the\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 19/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 20 \u2014 #20ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nBayesian approach utilizes different models for creating the 3-D representation: a convolu-\ntional neural network for learning the title, Node2Vec for the network and HAN for the con-\ntent. [131] worked mostly on manual features like metatopic, namely polarizing topics, using\nan end-to-end tool: The Gavagai Explorer, which performed poorly.\n[33] performed a political orientation prediction and hyperpartisan classification task\nusing an unmasking technique with binary classifiers. For the first task, they found that left-\nwing news tends to be easily misclassified. This study noticed that individual political orienta-\ntion is struggling to predict and that a style-based approach overcomes the content-based one.\nMoreover, they discovered subtle differences in style between hyperpartisan news belonging\nto different political leanings. [ 62] using masking and transformer-based models proved that\ntopic-based approaches lead to better results than style-based. Instead, [ 132] made a com-\nparative examination of BERT-based models and masking-based models, enriching com-\nprehension regarding the strengths and constraints of varied approaches in bias detection,\noffering crucial insights for upcoming research and advancements in this domain. In essence,\nthese models\u2019 contribution lies in their capacity to augment the precision, clarity, and com-\nprehensibility of bias detection within political and social discussions. Consequently, they\npropel advancements in this pivotal research domain. Furthermore, [ 133] investigates using\nlarge language models for automated stance detection in a lower-resource language, focusing\non immigration. It annotates pro- and anti-immigration examples to compare performance\nacross models. The study finds that GPT-3.5 matches supervised models\u2019 accuracy, offering\na simpler alternative for hyperpartisan detection in media monitoring. Lastly, for the sake of\nexhaustiveness, we will briefly cover other methods not focusing on news textual features. For\nthis reason, the following discussed papers are not included in our final selection. However,\nin this way, the reader can understand the complexities of approaches to tackle hyperparti-\nsanship. [ 134] maps linguistic divergence across the U.S. political spectrum using 1.5M social\nmedia posts (20M words) from 10k Twitter users. By analyzing followers of 72 news accounts,\nit identifies variations in topics, sentiment, and lexical semantics. Methods combine data min-\ning, lexicostatistics, machine learning, large language models, and human annotation. [ 135]\nanalyzes language differences on Twitter among 5,373 Democratic and 5,386 Republican fol-\nlowers to explore psychological traits tied to political leanings. Using naturalistic data, it con-\nfirms hypotheses: liberals\u2019 language shows uniqueness, swearing, anxiety, and emotions, while\nconservatives\u2019 language reflects group identity, achievement, and religion, supporting prior\nresearch. To conclude, [ 136] introduced FAULTANA (FAULT-line Alignment Network Anal-\nysis), a computational method to identify societal fault lines and polarization drivers in online\ninteractions. Using data from Birdwatch (Twitter) and DerStandard forums, it reveals two\npolarized groups aligned with political identities. FAULTANA tracks polarization over time,\nhighlighting divisive issues and their impact. We present the best performances retrieved in\nthe selected papers in Table 7.\nDatasets\nIn the previous section, we provided an overview of methodologies employed in address-\ning hyperpartisan detection. Effective models depend on top-notch data quality to function\noptimally. However, constructing a high-quality, well-balanced dataset can be both time-\nconsuming and resource-intensive. This challenge is compounded by shifts in data poli-\ncies across social networks since the Cambridge Analytica scandal, leading to potential dif-\nficulties or cost changes in obtaining data. Additionally, a trend has emerged within news\nsources where access to data is restricted due to its previous utilization in training models like\nGPT (https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/ ).\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 20/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 21 \u2014 #21ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nConsequently, news sources have implemented paywalls and crawler restrictions ( https://\nilmanifesto.it/termini-e-condizioni ), making it exceedingly challenging to gather suitable\ninformation for this and similar tasks.\nDatasets presentation\nTo support upcoming studies on identifying hyperpartisan news and related tasks, we have\ncreated an extensive table: Table 8, which outlines key attributes of datasets relevant to hyper-\npartisan news detection. This table includes datasets referenced in different papers. Some\nare not primarily used for hyperpartisan detection but could be. It is important to note that\nwhen subsets or extended versions of earlier datasets exist, we consider them separate entities\ndenoted by *. Additionally, datasets marked with ** signify merged collections. The column\nlabeled Dataindicates the number of articles gathered by the researchers.\nTo provide comprehensive insights into the table, we will give a philological explanation\nof the datasets marked with the symbols * and **. Framing Triplet Dataset is a combination\nof the following datasets: SemEval-2019 task 4 along with [ 120]\u2019s data. Furthermore, [ 120]\nexpands the SemEval-2019 task 4 dataset by incorporating articles collected from polarized\nsources and then labeled through mediabiasfactcheck.com. Regarding BIGNEWS, collected\nby [117], it has two subsets, respectively: BIGNEWSBLN is a downsampled corpus main-\ntaining an equal distribution of ideologies, and BIGNEWSALIGN , which clusters news sto-\nries from opposing sources but on the same topic. In their research, [ 49] utilized a subset\nofAll-the-news (https://www.kaggle.com/datasets/snapcrack/all-the-news ). Furthermore,\n[33] worked with a subset of articles crawled from the URLs contained in The BuzzFeed-\nWebis Fake News Corpus collected by [ 140]. By cleaning [ 33]\u2019s dataset, [ 62] obtained a new\ndataset. The same researchers created StereoImmigrants , a collection of Spanish news about\nimmigrants, for [ 132].\nLabeling and retrieving processes relied upon platforms like Allside ( https://allsides.com/ ),\nFactcheck ( https://mediabiasfactcheck.com/ ), Politifact ( https://www.politifact.com/ ), as\nground truth for establishing the bias of an article and as source where to collect data. Indeed,\nin these contexts, experts assign news to the political orientation.AllSides.com is a media\ncompany that specializes in providing balanced news coverage by collecting and comparing\nnews stories from various sources with different political leanings. The platform categorizes\nnews articles based on their political bias\u2014whether left, center, or right\u2014and scores them\naccording to the level of partisanship they contain.\nSince [121] noted that training models with big datasets reduce the performance due to\ntheir noise, researchers started to prefer the quality rather than the dimension. Indeed, [ 46],\nafter a deeper analysis of the SemEval 2019 dataset, revealed several issues with this ground\ntruth dataset widely used: class imbalance, task-label unalignment, and distribution shift.\nAs we can see from Fig 4, there is an imbalanced distribution towards English data, leav-\ning the context of minority languages understudied. Datasets are available at the respec-\ntive links: [ 81]https://gitlab.com/checkthat_lab/clef2023-checkthat-lab/-/tree/main/task3?\nref_type=heads , [109]https://github.com/yy-ko/khan-www23. , [118]https://github.com/\nMSU-NLP-CSS/CLoSE_framing , [80]https://github.com/axenov/politik-news , [132]\nhttps://github.com/jjsjunquera/StereoImmigrants , [120]https://github.com/ShamikRoy/\nSubframe-Prediction , [141]https://urlis.net/zon9n8wr , [58]https://drive.google.com/\ndrive/folders/1IyaKYeDkl7ubuabTI65G0nSBfxQNdeTr [142]https://dataverse.harvard.\nedu/dataset.xhtml?persistentId=doi:10.7910/DVN/ULHLCB , [18]https://zenodo.org/\nrecords/1489920 [143]www.ccs.neu.edu/home/luwang/data.html , [33]https://github.\ncom/BuzzFeedNews/2016-10-facebook-fact-check , Reuter http://about.reuters.com/\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 21/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 22 \u2014 #22ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 8. This table describes the datasets found in the literature.\nDataset Reference Year Size Bias Label Language Media\u2019s\ncountryAvailability\nNo Name [133] 2024 22628 U/A Estonian Estonia No\nNo Name [129] 2024 U/A U/A Persian Iran No\nTask 3A [81] 2023 55000 AllSides English U.S. Yes\nTask 3B [81] 2023 8000 AllSides English U.S. Yes\nAllsides-L [109] 2023 719256 Allsides English U.S. Yes\nNo Name [46] 2023 1824824 AllSides, Media Bias Factcheck English U.S. No\nFraming Triplet Dataset [118] 2022 25627 Media Bias Factcheck English U.S. Yes\nNo Name [79] 2022 10000 Allsides English U.S. No\nTVP Info [146] 2022 81694 U/A Polish Poland Upon\nrequest\nTVN 24 [146] 2022 128527 U/A Polish Poland Upon\nrequest\nBIGNEWS [117] 2022 3689229 Allsides, adfontesmedia English U.S. Upon\nrequest\n*BIGNEWS BLN [117] 2022 2331552 Allsides, adfontesmedia English U.S. Upon\nrequest\n*BIGNEWS ALIGN [117] 2022 1060512 Allsides, adfontesmedia English U.S. Upon\nrequest\nGERMAN dataset [80] 2021 47362 None German Germany Yes\nNo Name* [62] 2021 1555 BuzzFeed English U.S. No\nStereo Immigrants [132] 2021 3704 Manually labeled Spanish Spain Yes\nNo Name* [120] 2020 21645 Media Bias Factcheck English U.S. Yes\nThe Annotated Data Dataset [147] 2020 46 Media Bias Factcheck English U.S. No\nNo Name [59] 2020 37000 None Italian Italy No\n*PoliNews [49] 2020 83000 U/A English U.S. No\nPresidential [71] 2020 178572 Allsides, Media Bias Factcheck English U.S. No\nPOLUSA [141] 2020 9000000 None English U.S. Yes\n*Politifact [61] 2020 18027 Politifact.com English U.S. No\nNo Name [51] 2020 4627 Manually labeled English India No\nNo Name [121] 2020 34737 Allsides English U.S. Yes\nAll-Sides [148] 2019 10385 None English U.S. No\nTelugu [58] 2019 1327 Manually labeled Telugu India Yes\nNELA-2018 [142] 2019 713534 Allsides, Media Bias Factcheck,\nBuzzFeed et al.English U.S. Yes\nby-article [18] 2019 1273 Manually labeled English U.S. Yes\nby-publisher [18] 2019 754000 BuzzFeed news, Media Bias\nFactcheckEnglish U.S. Yes\nBASIL [143] 2019 300 Manually labeled English U.S. Yes\nThe BuzzFeed-Webis Fake\nNews Corpus 2016[33] 2018 1627 BuzzFeed English U.S. Yes\nReuter U/A U/A 18519 U/A English U.S. Yes\nNo Name [63] 2018 88 Crowd-sourcing English U.S. No\nMBCF [144] 2018 1066 Media Bias Factcheck English U.S. Yes\nNELA-2017 [145] 2018 136000 U/A English U.S. Yes\nBuzzFeed 2016 buzzfeednews.\ncom2016 2282 BuzzFeed English U.S. Yes\nNo Name [52] 2015 74 Crowd-sourcing English U.S. No\nhttps://doi.org/10.1371/journal.pone.0316989.t008\nresearchandstandards/corpus/ , [144]https://github.com/RWalecki/copula_ordinal_\nregression , [145]https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/\nDVN/ZCXSKG , BuzzFeed https://github.com/BuzzFeedNews/2016-10-facebook-fact-check/\nblob/master/data/facebook-fact-check.csv .\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 22/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 23 \u2014 #23ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nFig 4.Language distribution in the datasets described in Table 8.\nhttps://doi.org/10.1371/journal.pone.0316989.g004\nPotential limitations\nThe studies collected hold significant value, yet several inherent limitations in the datasets\ncould influence the comprehensiveness and applicability of future findings. Firstly, the\nabsence of a distinct dataset designed to differentiate between hyperpartisan and partisan\nnews poses a fundamental challenge, potentially impacting the classification accuracy. Sec-\nondly,2shows that the predominant focus is on English news articles within the dataset. This\nfact raises concerns about minority languages and their respective democratic contexts, pos-\nsibly skewing the representation and applicability of Anglo-American papers\u2019 conclusions to\ntheir different socio-cultural environment. This discrepancy might lead to situations where\ncertain democracies lack the necessary tools and datasets in their native language, hinder-\ning their ability to develop similarly effective analytical tools as over-represented democra-\ncies. Additionally, the phenomenon of hyperpartisanship varies significantly between coun-\ntries due to the variety of party systems and different cultural backgrounds [ 6]. Consequently,\nthe development of models trained on linguistically non-representative data may compro-\nmise their ability to efficiently detect hyperpartisanship in under-represented democracies,\nthereby impacting their success rates. Furthermore, issues pertaining to dataset maintenance,\nsuch as broken URLs, may impede replicability and accessibility for future research endeav-\nors [33]. Furthermore, temporal lexicon constraints might hinder capturing shifts in tex-\ntual patterns, tones, and context, affecting the accuracy of temporal analysis [ 46]. We high-\nlight that cross-lingual comparison of hyperpartisan traits has never been studied from a\ncomputational approach. Thus, it is not possible to define if the online environment flattens\ncultural-linguistic traits pertinent to hyperpartisanship independently from the country and\nits political system. Another consideration regards the limited availability of data over time\ndue to paywalls and copyright restrictions poses a significant barrier, potentially restricting\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 23/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 24 \u2014 #24ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nthe depth and breadth of future analysis within certain timeframes. Lastly, despite the popu-\nlarity and the good results that researchers achieved, as far as we know, autoregressive models\nwere not used.\nConclusions and future works\nIn synthesizing insights from 81 studies, our systematic review illuminates the value of exist-\ning research in understanding hyperpartisan news. We summarized all the papers included\nin the systematic review in Table 9. With the support of this table, we are going to reply to the\ninitial research questions.\nRQ1: Does a categorization for hyperpartisan news detection methods exist? Currently,\nthere is no widely adopted comprehensive categorization system in the literature. The field\nstill lacks standardized mathematical models for quantifying textual exaggerations that define\nhyperpartisan content. One key contribution of this systematic review is that it represents\nthe first attempt to systematize news-based approaches while also enhancing the traditional\nPRISMA methodology by integrating ResearchRabbit during the \u201dIdentification of studies via\nother methods\u201d phase. ResearchRabbit facilitated a systematic, data-driven expansion of our\nliterature pool by visualizing clusters based on citation linkages. This clustering approach pro-\nvided a structured method for identifying and selecting relevant studies by uncovering both\ndirect citation relationships and keyword-based topic similarities. As a result, the tool con-\ntributed to a more comprehensive and cohesive expansion of the selected literature base. Fur-\nthermore, we proposed a specific definition of the studied phenomenon that can be applied in\nComputer Social Science and Computer Science.\nRQ2: Is hyperpartisan news detection a stand-alone or overlapping task? The complex-\nity of hyperpartisan news detection hints at an overlapping task encompassing various forms\nof media bias, suggesting a shift towards multi-label detection for nuanced representations.\nResearch shows that models with fine-grained label sets outperform binary classifications, yet\nthe majority of studies use simplified, binary categories.\nRQ3: What are the proposed solutions using textual data? Research commonly applies\ntext-based methods, such as Natural Language Processing (NLP) techniques, to detect hyper-\npartisan content by identifying linguistic patterns of exaggeration and emotional tone. In\nterms of labels, fine-grained labels show improved model accuracy in detecting diverse biases,\nbut in this case the annotation required is costly.\nRQ4: Does the task keep up with new NLP technologies like autoregressive models? To\ndate, the adoption of advanced autoregressive models in hyperpartisan news detection is\nlimited, revealing a critical gap. This gap underscores a need to explore these models, which\ncould improve detection accuracy with state-of-the-art language understanding.\nRQ5: What are the results of the developed models developed? Since the release of BERT,\nthis model architecture\u2014and particularly its variants, such as RoBERTa\u2014has achieved state-\nof-the-art performance in a wide range of classification tasks.\nRQ6: What datasets are used for this task? How are they structured? Have they been\nupdated to cover the latest political global and regional trends? Datasets predominantly com-\nprise English-language news articles, which risks skewing results when applying models to\nnon-English contexts. Limited representation of minority languages restricts model general-\nization and hampers analysis of unique democratic and socio-political dynamics. In addition,\ndataset maintenance issues (e.g., broken URLs) hinder replicability, and paywalls or copyright\nconstraints restrict access to time-sensitive data, impacting longitudinal research.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 24/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 25 \u2014 #25ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 9. Table summarizing the papers selected with PRISMA methodology.\nReference Year Title First Author Dataset Language Media\u2019s\nCountryMethod Model Acc. F1\n[46] 2024 Computational\nassessment of hyper-\npartisanship in news\ntitlesLyu, H. Own English U.S. DL BERT 0.84 0.78\n[129]2024 An evaluation of lan-\nguage models for\nhyperpartisan ideol-\nogy detection in Persian\nTwitterOmidi, S. Own Persian Iran Other ChatGPT 0.85 0.85\n[133]2024 Automated stance\ndetection in complex\ntopics and small lan-\nguages: The challenging\ncase of immigration in\npolarizing news mediaMets, Mark Own Estonian Estonia Other U/A U/A U/A\n[109]2023 KHAN: Knowledge-\naware hierarchical\nattention networks for\naccurate political stance\npredictionKo Semeval-2019 English U.S. DL HAN 0.95 U/A\n[116]2023 Multi-stage prompt\ntuning for political per-\nspective detection in\nlow-resource settingsKim, K.M. Semeval-2019 English U.S. DL MP-tuning 0.91 U/A\n[128]2023 From Fake to Hyperpar-\ntisan News Detection\nUsing Domain\nAdaptationSm\u0103du, R.A. Semeval-2019 English U.S. DL RoBERTa 0.84 0.83\n[124]2023 Temporal positional\nlexicon expansion for\nfederated learning\nbased on hyperpatism\ndetectionAhmed, U. Semeval-2019 English U.S. DL BERT 0.83 U/A\n[125]2023 Semisupervised\nfederated learning\nfor temporal news\nhyperpatism detectionAhmed, U. Semeval-2019\nby-article +\nby-publisherEnglish U.S. DL BERT U/A 0.91\n[81] 2023 Frank at checkthat!\n2023: Detecting the\npolitical bias of news\narticles and news mediaAzizov, D. Task 3A English U.S. DL CatBoost 0.69 0.69\n[118]2022 CLoSE: Contrastive\nlearning of subframe\nembeddings for political\nbias classification of\nnews mediaKim, M.Y. Framing\nTriplet DatasetEnglish U.S. DL multi-task BERT-based\nmodel0.84 U/A\n[64] 2022 Detection of hyper-\npartisan news articles\nusing natural language\nprocessing techniqueNaredla, N.R. Semeval-2019 English U.S. ML Random Forest 0.88 U/A\n[117]2022 POLITICS: pretraining\nwith same-story article\ncomparison for ideology\nprediction and stance\ndetectionLyu, Y. Semeval-2019 English U.S. DL RoBERTa 0.85 0.85\n(Continued )\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 25/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 26 \u2014 #26ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 9 .Continued\nReference Year Title First Author Dataset Language Media\u2019s\nCountryMethod Model Acc. F1\n[79] 2022 An automated news\nbias classifier using\ncaenorhabditis ele-\ngans inspired recursive\nfeedback network\narchitectureSridharan, A. Own English U.S. DL Mesh Neural\nNetwork0.45 U/A\n[66] 2022 Political ideology detec-\ntion of news articles\nusing deep neural\nnetworksM.Alzhrani, K. Presidential English U.S. DL HAN 0.91 0.90\n[37] 2022 Role of ELMo embed-\nding in detecting fake\nnews on social mediaGarg, S. Reuter English U.S. ML Logistic\nRegressionU/A 0.93\n[146]2022 Creation of Polish\nOnline News Corpus\nfor Political Polarization\nStudiesSzwoch, J. TVP Info, TVP 24 Polish Poland U/A U/A\n[132]2021 On the detection of\npolitical and social biasS\u00e1nchez-\nJunquera,\nJ.Stereoimmigrants Spanish U.S. DL BERT 0.86 0.83\n[62] 2021 Masking and\ntransformer-based\nmodels for hyperpar-\ntisanship detection in\nnewsS\u00e1nchez-\nJunquera,\nJ.The BuzzFeed-Webis\nFake News Corpus\n2016English U.S. DL BERT 0.89 0.86\n[112]2021 Using social and lin-\nguistic information\nto adapt pretrained\nrepresentations for\npolitical perspective\nidentificationLi, C. Semeval-2019 English U.S. DL LSTM 0.86 0.84\n[102]2021 Hyperpartisan news\nclassification with ELMo\nand bias featureGerald Ki Wei,\nH.The BuzzFeed-Webis\nFake News Corpus\n2016English U.S. DL CNN U/A 0.73\n[105]2021 Bias bubbles: Using\nsemi-supervised learn-\ning to measure how\nmany biased news\narticles are around us.Ruan, Q. Semeval-2019\nby-article +\nby-publisherEnglish U.S. DL BERT 0.87 0.81\n[80] 2021 Fine-grained classifica-\ntion of political bias in\ngerman news: A data set\nand initial experimentsAksenov, D. GERMAN Dataset German Germany ML Random Forest U/A 0.79\n[61] 2021 Topic-based Models\nwith Fact Checking\nfor Fake News Iden-\ntification. - RoCHI -\nRoCHIDumitru, Vlad\nCristianPolitifact English U.S. U/A U/A\n[107]2020 On document represen-\ntations for detection of\nbiased news articlesCruz, A.F. Semeval-2019 English U.S. DL HAN 0.82 0.81\n[120]2020 Weakly supervised\nlearning of nuanced\nframes for analyzing\npolarization in news\nmediaRoy, S. Own English U.S. DL LSTM U/A 0.80\n(Continued )\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 26/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 27 \u2014 #27ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 9 .Continued\nReference Year Title First Author Dataset Language Media\u2019s\nCountryMethod Model Acc. F1\n[49] 2020 How biased are amer-\nican media outlets?\na framework for\npresentation bias\nregressionTran, M. PoliNews English U.S. DL BERT U/A U/A\n[51] 2020 Ideology detection in\nthe indian mass mediaSharma, A. Own English India DL RNN 0.84 0.87\n[121]2020 We can detect your bias:\nPredicting the political\nideology of news articlesBaly, R. Own English U.S. DL BERT 0.72 U/A\n[43] 2020 Fake and hyper-partisan\nnews identificationDumitru, V.C. Own English U.S. ML LSVM 0.93 0.81\n[71] 2020 Ideology Detection of\nPersonalized Political\nNews Coverage: A New\nDatasetAlzhrani,\nKhudranPresidential English U.S. U/A U/A U/A U/A\n[141]2020 The POLUSA Dataset:\n0.9M Political News\nArticles Balanced\nby Time and Outlet\nPopularityGebhard,\nLukasPOLUSA English U.S. U/A U/A U/A U/A\n[147]2020 Creating a dataset\nfor fine-grained bias\ndetection in news\narticlesLim, Sora The Annotated\nData DatasetEnglish U.S. U/A U/A U/A U/A\n[59] 2020 HoaxItaly: a collection\nof Italian disinformation\nand fact-checking stories\nshared on Twitter in\n2019Pierri,\nFrancescoOwn Italian Italy U/A U/A U/A U/A\n[72] 2019 Team Bertha von Sut-\ntner at SemEval-2019\ntask 4: Hyperpartisan\nnews detection using\nELMo sentence repre-\nsentation convolutional\nnetworkJiang, Y. Semeval-2019 English U.S. DL CNN 0.82 0.81\n[77] 2019 Vernon-fenwick at\nSemEval-2019 task 4:\nHyperpartisan news\ndetection using lexical\nand semantic featuresSrivastava, V. Semeval-2019 English U.S. ML Logistic Regression 0.82 0.82\n[93] 2019 The sally smedley hyper-\npartisan news detector\nat SemEval-2019 task 4Hanawa, K. Semeval-2019 English U.S. ML Linear Classifier 0.81 0.80\n[111]2019 Dick-preston and morbo\nat SemEval-2019 task\n4: Transfer learning for\nhyperpartisan news\ndetectionIsibster, T. Semeval-2019 English U.S. DL LSTM 0.80 0.80\n[83] 2019 Tom jumbo-grumbo\nat SemEval-2019 task\n4: Hyperpartisan news\ndetection with GloVe\nvectors and SVMYeh, C.L. Semeval-2019 English U.S. ML SVM 0.80 0.79\n(Continued )\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 27/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 28 \u2014 #28ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 9 .Continued\nReference Year Title First Author Dataset Language Media\u2019s\nCountryMethod Model Acc. F1\n[75] 2019 TakeLab at SemEval-\n2019 task 4: Hyper-\npartisan news\ndetectionPali\u0107, N. Semeval-2019 English U.S. ML SVC 0.79 0.76\n[122]2019 Politically-oriented\ninformation inference\nfrom textDa Silva, S.C. Semeval-2019 English U.S. DL BERT 0.78 0.77\n[127]2019 Team howard beale at\nSemEval-2019 task 4:\nHyperpartisan news\ndetection with BERTMutlu, O. Semeval-2019 English U.S. DL BERT 0.78 0.76\n[126]2019 Harvey mudd college\nat SemEval-2019 task\n4: The clint buchanan\nhyperpartisan news\ndetectorDrissi, M. Semeval-2019 English U.S. DL BERT 0.77 0.75\n[90] 2019 Team ned leeds at\nSemEval-2019 task 4:\nExploring language indi-\ncators of hyperpartisan\nreportingStevanoski, B. Semeval-2019 English U.S. ML Random Forest 0.77 0.74\n[137]2019 Team yeon-zi at\nSemEval-2019 task\n4: Hyperpartisan news\ndetection by de-noising\nweakly-labeled dataLee, N. Semeval-2019 English U.S. DL BERT 0.76 0.76\n[94] 2019 NLP@UIT at SemEval-\n2019 task 4: The\npaparazzo hyperpartisan\nnews detectorNguyen, D.V. Semeval-2019 English U.S. ML SVM 0.75 0.74\n[40] 2019 Doris martin at\nSemEval-2019 task\n4: Hyperpartisan news\ndetection with generic\nsemi-supervised featuresAgerri, R. Semeval-2019 English U.S. ML Maxent 0.74 0.73\n[74] 2019 Harvey mudd college\nat SemEval-2019 task\n4: The carl kolchak\nhyperpartisan news\ndetectorChen, C. Semeval-2019 English U.S. ML Naive Bayes 0.74 0.74\n[138]2019 Steve martin at\nSemEval-2019 task\n4: Ensemble learning\nmodel for detecting\nhyperpartisan newsJoo, Y. Semeval-2019 English U.S. DL CNN 0.74 0.70\n[42] 2019 Cardiff university at\nSemEval-2019 task 4:\nLinguistic features for\nhyperpartisan news\ndetectionP\u00e9rez-\nAlmendros,\nC.Semeval-2019 English U.S. DL CNN + LSTM 0.74 0.71\n[91] 2019 Team QCRI-MIT at\nSemEval-2019 task 4:\nPropaganda analysis\nmeets hyperpartisan\nnews detectionSaleh, A. Semeval-2019 English U.S. ML Logistic Regression 0.73 0.73\n[89] 2019 Team fernando-pessa\nat SemEval-2019 task\n4: Back to basics in\nhyperpartisan news\ndetectionCruz, A. Semeval-2019 English U.S. ML Random Forest 0.72 0.67\n(Continued )\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 28/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 29 \u2014 #29ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 9 .Continued\nReference Year Title First Author Dataset Language Media\u2019s\nCountryMethod Model Acc. F1\n[108]2019 Rouletabille at SemEval-\n2019 task 4: Neural\nnetwork baseline\nfor identification of\nhyperpartisan publishersMoreno, J.G. Semeval-2019 English U.S. DL HAN 0.72 0.69\n[45] 2019 Duluth at SemEval-2019\ntask 4: The pioquinto\nmanterola hyperpartisan\nnews detectorSengupta, S. Semeval-2019 English U.S. ML Logistic Regression 0.70 0.68\n[114]2019 UBC-NLP at SemEval-\n2019 task 4: Hyperpar-\ntisan news detection\nwith attention-based\nbi-LSTMsZhang, C. Semeval-2019 English U.S. DL LSTM 0.68 0.63\n[104]2019 Team xenophilius love-\ngood at SemEval-2019\ntask 4: Hyperpartisan-\nship classification using\nconvolutional neural\nnetworksZehe, A. Semeval-2019 English U.S. DL CNN 0.67 0.74\n[47] 2019 Harvey mudd college\nat SemEval-2019 task\n4: The d.x. beaumont\nhyperpartisan news\ndetectorAmason, E. Semeval-2019 English U.S. ML Naive Bayes 0.65 0.73\n[123]2019 Team jack ryder at\nSemEval-2019 task 4:\nUsing BERT represen-\ntations for detecting\nhyperpartisan newsShaprin, D. Semeval-2019 English U.S. DL BERT 0.64 0.64\n[76] 2019 Team kermit-the-frog\nat SemEval-2019 task 4:\nBias detection through\nsentiment analysis and\nsimple linguistic featuresAnthonio, T. Semeval-2019 English U.S. ML SVM 0.62 0.69\n[103]2019 Team peter brinkmann\nat SemEval-2019 task\n4: Detecting biased\nnews articles using\nconvolutional neural\nnetworksF\u00e4rber, M. Semeval-2019 English U.S. DL CNN 0.60 0.70\n[113]2019 Team kit kittredge at\nSemEval-2019 task 4:\nLSTM voting systemCramerus, R. Semeval-2019 English U.S. DL LSTM 0.58 0.68\n[131]2019 Team harry friberg at\nSemEval-2019 task 4:\nIdentifying hyperpar-\ntisan news through\neditorially defined\nmetatopicsAfsarmanesh,\nN.Semeval-2019 English U.S. Other Gagavai Explorer 0.56 0.68\n[73] 2019 Multi-task ordinal\nregression for jointly\npredicting the trustwor-\nthiness and the leading\npolitical ideology of\nnews mediaBaly, R. MBFC English U.S. ML Copula Ordinal\nRegressionU/A U/A\n(Continued )\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 29/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 30 \u2014 #30ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 9 .Continued\nReference Year Title First Author Dataset Language Media\u2019s\nCountryMethod Model Acc. F1\n[85] 2019 Clark kent at SemEval-\n2019 task 4: Stylo-\nmetric insights into\nhyperpartisan news\ndetectionGupta, V. Semeval-2019 English U.S. ML XGBoost 0.55 0.28\n[139]2019 Team peter-parker at\nSemEval-2019 task 4:\nBERT-based method\nin hyperpartisan news\ndetectionNing, Z. Semeval-2019 English U.S. DL BERT 0.50 0.61\n[92] 2019 Tintin at SemEval-2019\ntask 4: Detecting hyper-\npartisan news article\nwith only simple tokensBestgen, Y. Semeval-2019\nby-publisherEnglish U.S. ML Logistic Regression 0.70 0.68\n[65] 2019 Brenda starr at\nSemEval-2019 task\n4: Hyperpartisan news\ndetectionPapadopoulou,\nO.Semeval-2019\nby-publisherEnglish U.S. DL CNN 0.66 0.70\n[88] 2019 Fermi at SemEval-2019\ntask 4: The sarah-jane-\nsmith hyperpartisan\nnews detectorChakravartula,\nN.Semeval-2019\nby-publisherEnglish U.S. ML Random Forest 0.61 0.66\n[60] 2019 Hyperpartisan news and\narticles detection using\nBERT and ELMoHuang, G.K.W. Semeval-2019\nby-article +\nby-publisherEnglish U.S. DL BERT 0.68 U/A\n[58] 2019 Detecting political bias\nin news articles using\nheadline attentionGangula,\nR.R.R.Telugu Telugu India DL HAN 0.89 U/A\n[33] 2019 A stylometric inquiry\ninto hyperpartisan and\nfake newsPotthast, M. The BuzzFeed-\nWebis Fake\nNews Corpus\n2016English U.S. ML U/A 0.75 0.78\n[70] 2019 Spider-jerusalem at\nSemEval-2019 task 4:\nHyperpartisan news\ndetectionAlabdulkarim,\nA.Semeval-2019 English U.S. ML SVM 0.74 0.71\n[18] 2019 SemEval-2019 task 4:\nHyperpartisan news\ndetectionKiesel, J. Semeval-2019 English U.S. U/A U/A U/A U/A\n[142]2019 NELA-GT-2018: A large\nmulti-labelled news\ndataset for the study of\nmisinformation in news\narticlesN\u00f8rregaard,\nJeppeNELA-2018 English U.S. U/A U/A U/A U/A\n[143]2019 In plain sight: Media\nbias through the lens of\nfactual reportingFan, L. BASIL English U.S. U/A U/A U/A U/A\n[148]2019 Encoding social infor-\nmation with graph\nconvolutional networks\nforPolitical perspective\ndetection in news mediaLi, C. Allsides English U.S. U/A U/A U/A U/A\n[69] 2018 Multi-view models\nfor political ideol-\nogy detection of news\narticlesKulkarni, V. Own English U.S. Other MVDAM 0.80 0.79\n(Continued )\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 30/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 31 \u2014 #31ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nTable 9 .Continued\nReference Year Title First Author Dataset Language Media\u2019s\nCountryMethod Model Acc. F1\n[44] 2018 Orwellian-times at\nSemEval-2019 task\n4: A stylistic and\ncontent-based classifierKnauth, J. Semeval-2019 English U.S. ML SVM 0.67 0.69\n[144]2018 Predicting factuality of\nreporting and bias of\nnews media sourcesBaly, R. MBCF English U.S. U/A U/A U/A U/A\n[145]2018 Sampling the news pro-\nducers: A large news and\nfeature data set for the\nstudy of the complex\nmedia landscapeHorne, B.D. NELA-2017 English U.S. U/A U/A U/A U/A\n[63] 2018 Understanding char-\nacteristics of biased\nsentences in news\narticlesJeong Lim, S. Own English U.S. U/A U/A U/A U/A\n[52] 2015 Testing and compar-\ning computational\napproaches for iden-\ntifying the language\nof framing in political\nnewsBaumer, E. Own English U.S. U/A U/A U/A U/A\nhttps://doi.org/10.1371/journal.pone.0316989.t009\nRQ7: How can the current state of research on hyperpartisan detection be characterized\nin diverse languages and countries? The absence of linguistically diverse datasets is a signifi-\ncant limitation, especially in minority and underrepresented cultures. This restricts the field\u2019s\ncapacity to develop effective hyperpartisan detection models for varied linguistic environ-\nments. Current datasets\u2019 Anglo-American focus may limit models\u2019 efficacy when applied to\nglobal democracies with different political and cultural contexts, exacerbating bias and mis-\ninformation issues in these areas. Moreover, the lack of cross-lingual studies leaves the impact\nof online environments on cultural-linguistic variations in hyperpartisan traits unexplored.\nIn conclusion, while existing research provides insights into hyperpartisan news, limita-\ntions in dataset diversity, language inclusion, and methodology highlight the need for more\nrobust, globally representative resources. Future research could benefit from exploring autore-\ngressive models and expanding cross-lingual analysis for a broader understanding of hyper-\npartisanship in diverse political systems and cultural contexts.\nAuthor contributions\nConceptualization: Michele Joshua Maggini.\nData curation: Michele Joshua Maggini, Davide Bassi.\nInvestigation: Michele Joshua Maggini.\nMethodology: Michele Joshua Maggini.\nResources: Michele Joshua Maggini, Davide Bassi.\nSupervision: Michele Joshua Maggini, Ga\u00ebl Dias, Pablo Gamallo Otero.\nValidation: Michele Joshua Maggini, Davide Bassi.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 31/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 32 \u2014 #32ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nWriting \u2013 original draft: Michele Joshua Maggini.\nWriting \u2013 review & editing: Michele Joshua Maggini, Davide Bassi, Paloma Piot.\nReferences\n1.Falkenbach M, Bekker M, Greer SL. Do parties make a difference? A review of partisan effects on\nhealth and the welfare state. Eur J Public Health. 2020;30(4):673\u201382.\nhttps://doi.org/10.1093/eurpub/ckz133 PMID: 31334750\n2.Ellger F. The mobilizing effect of party system polarization. Evidence from Europe. Comparat\nPolitic Stud. 2023;57(8):1310\u201338. https://doi.org/10.1177/00104140231194059\n3.Dalton RJ. Modeling ideological polarization in democratic party systems. Elector Stud.\n2021;72102346. https://doi.org/10.1016/j.electstud.2021.102346\n4.Lorenz-Spreen P, Oswald L, Lewandowsky S, Hertwig R. A systematic review of worldwide causal\nand correlational evidence on digital media and democracy. Nat Hum Behav. 2023;7(1):74\u2013101.\nhttps://doi.org/10.1038/s41562-022-01460-1 PMID: 36344657\n5.Guess AM, Barber\u00e1 P, Munzert S, Yang J. The consequences of online partisan media. Proc Natl\nAcad Sci U S A. 2021;118(14):e2013464118. https://doi.org/10.1073/pnas.2013464118 PMID:\n33782116\n6.Dalton RJ. Party identification and nonpartisanship. Int Encyclop Soc Behav Sci. 2015;562\u20136.\nhttps://doi.org/10.1016/b978-0-08-097086-8.93088-5\n7.McCoy J, Somer M. Toward a theory of pernicious polarization and how it harms democracies:\ncomparative evidence and possible remedies. Ann Am Acad Politic Soc Sci. 2018;681(1):234\u201371.\nhttps://doi.org/10.1177/0002716218818782\n8.Holt K, Ustad Figenschou T, Frischlich L. Key dimensions of alternative news media. Digit\nJournalism. 2019;7(7):860\u20139. https://doi.org/10.1080/21670811.2019.1625715\n9.Tucker J, Guess A, Barbera P, Vaccari C, Siegel A, Sanovich S, et al. Social media, political\npolarization, and political disinformation: a review of the scientific literature. SSRN Electron J.\n2018. https://doi.org/10.2139/ssrn.3144139\n10.Anthonio T. Robust document representations for hyperpartisan and fake news detection. 2019.\n11.Bartels LM. Partisanship in the trump era. J Politics. 2018;80:1483\u201394.\nhttps://doi.org/10.1086/727602\n12.Hawdon J, Ranganathan S, Leman S, Bookhultz S, Mitra T. Social media use, political\npolarization, and social capital: is social media tearing the U.S. apart? In: Meiselwitz G editor.\nSocial computing and social media. Design, ethics, user behavior, and social network analysis.\nSpringer; 2020. p. 243\u2013260. https://doi.org/10.1007/978-3-030-49570-1_17\n13.Bawden D, Robinson L. Curating the infosphere: Luciano floridi\u2019s philosophy of information as the\nfoundation for library and information science. J Documentation. 2018;74(1):2\u201317.\nhttps://doi.org/10.1108/jd-07-2017-0096\n14.Nannini L, Bonel E, Bassi D, Maggini MJ. Beyond phase-in: assessing impacts on disinformation\nof the EU Digital Services Act. AI Ethics. 2024. https://doi.org/10.1007/s43681-024-00467-w\n15.Commission E. A multi-dimensional approach to disinformation \u2013 Report of the independent High\nlevel Group on fake news and online disinformation. Publications Office. 2018.\nhttps://doi.org/10.2759/739290\n16.European Parliament Council. Proposal for a regulation of the European parliament and of the\ncouncil on a single market for digital services (digital services act) and amending directive\n2000/31/EC. 2020.\n17.Bondielli A, Marcelloni F. A survey on fake news and rumour detection techniques. Inf Sci.\n2019;49738\u201355. https://doi.org/10.1016/j.ins.2019.05.035\n18.Kiesel J, Mestre M, Shukla R, Vincent E, Adineh P, Corney D, et al. SemEval-2019 task 4:\nhyperpartisan news detection. In: Proceedings of the 13th International Workshop on Semantic\nEvaluation. 2019. https://doi.org/10.18653/v1/s19-2145\n19.Sousa-Silva R. Fighting the fake: a forensic linguistic analysis to fake news detection. Int J Semiot\nLaw. 2022;35(6):2409\u201333. https://doi.org/10.1007/s11196-022-09901-w PMID: 35505837\n20.Dykstra A. Critical reading of online news commentary headlines: stylistic and pragmatic aspects.\nTopics Linguist. 2019;20(2):90\u2013105. https://doi.org/10.2478/topling-2019-0011\n21.Xu WW, Sang Y, Kim C. What drives hyper-partisan news sharing: exploring the role of source,\nstyle, and content. Digital Journalism. 2020;8(4):486\u2013505.\nhttps://doi.org/10.1080/21670811.2020.1761264\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 32/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 33 \u2014 #33ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\n22.Pescetelli N, Barkoczi D, Cebrian M. Bots influence opinion dynamics without direct human-bot\ninteraction: the mediating role of recommender systems. Appl Netw Sci. 2022;7(1):46.\nhttps://doi.org/10.1007/s41109-022-00488-6\n23.Pitoura E, Tsaparas P, Flouris G, Fundulaki I, Papadakos P, Abiteboul S, et al. On measuring bias\nin online information. arXiv preprint 2017. https://arxiv.org/abs/1704.05730\n24.Nakov P, Sencar HT, An J, Kwak H. A survey on predicting the factuality and the bias of news\nmedia. arXiv preprint 2021. https://arxiv.org/abs/2103.12506\n25.Kondamudi MR, Sahoo SR, Chouhan L, Yadav N. A comprehensive survey of fake news in social\nnetworks: attributes, features, and detection approaches. J King Saud Univ \u2013 Comput Inf Sci.\n2023;35(6):101571. https://doi.org/10.1016/j.jksuci.2023.101571\n26.Hamborg F, Donnay K, Gipp B. Automated identification of media bias in news articles: an\ninterdisciplinary literature review. Int J Digit Libr. 2018;20(4):391\u2013415.\nhttps://doi.org/10.1007/s00799-018-0261-y\n27.Medeiros FDC, Braga RB. Fake news detection in social media: a systematic review. In: XVI\nBrazilian Symposium on Information Systems. 2020; p. 1\u20138.\nhttps://doi.org/10.1145/3411564.3411648\n28.Kapantai E, Christopoulou A, Berberidis C, Peristeras V. A systematic literature review on\ndisinformation: toward a unified taxonomical framework. New Media Soc. 2020;23(5):1301\u201326.\nhttps://doi.org/10.1177/1461444820959296\n29.Rodrigo-Gin\u00e9s F-J, Carrillo-de-Albornoz J, Plaza L. A systematic review on media bias detection:\nwhat is media bias, how it is expressed, and how to detect it. Exp Syst Appl. 2024;237121641.\nhttps://doi.org/10.1016/j.eswa.2023.121641\n30.Kitchenham B, Charters S. Guidelines for performing systematic literature reviews in software\nengineering. Technical Report. 2007.\n31.Moher D, Altman DG, Liberati A, Tetzlaff J. PRISMA statement. Epidemiology. 2011;22(1):128;\nauthor reply 128. https://doi.org/10.1097/EDE.0b013e3181fe7825 PMID: 21150360\n32.Gusenbauer M, Haddaway NR. Which academic search systems are suitable for systematic\nreviews or meta-analyses? Evaluating retrieval qualities of Google Scholar, PubMed, and 26 other\nresources. Res Synth Methods. 2020;11(2):181\u2013217. https://doi.org/10.1002/jrsm.1378 PMID:\n31614060\n33.Potthast M, Kiesel J, Reinartz K, Bevendorff J, Stein B. A stylometric inquiry into hyperpartisan and\nfake news. In: Gurevych I, Miyao Y, editors. Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational\nLinguistics; 2018. p. 231\u2013240. https://doi.org/10.18653/v1/p18-1022\n34.Zannettou S, Sirivianos M, Blackburn J, Kourtellis N. The web of false information. J Data Inf\nQuality. 2019;11(3):1\u201337. https://doi.org/10.1145/3309699\n35.Altay S, Berriche M, Heuer H, Farkas J, Rathje S. A survey of expert views on misinformation:\ndefinitions, determinants, solutions, and future of the field. Harvard Kennedy School Misinf Rev\n2023;4: 1\u201334.\n36.Ross Arguedas A, Robertson C, Fletcher R, Nielsen R. Echo chambers, filter bubbles, and\npolarisation: a literature review. Tech. Reportuan. 2022.\n37.Garg S, Sharma DK. Role of ELMo embedding in detecting fake news on social media. In: 2022\n11th International Conference on System Modeling & Advancement in Research Trends (SMART).\nIEEE; 2022. p. 57\u201360. https://doi.org/10.1109/smart55829.2022.10046789\n38.Ross RM, Rand DG, Pennycook G. Beyond \u201cfake news\u201d: analytic thinking and the detection of\nfalse and hyperpartisan news headlines. Judgm decis mak. 2021;16(2):484\u2013504.\nhttps://doi.org/10.1017/s1930297500008640\n39.Mour\u00e3o RR, Robertson CT. Fake news as discursive integration: an analysis of sites that publish\nfalse, misleading, hyperpartisan and sensational information. Journalism Stud.\n2019;20(14):2077\u201395. https://doi.org/10.1080/1461670x.2019.1566871\n40.Agerri R. Doris Martin at SemEval-2019 task 4: hyperpartisan news detection with generic\nsemi-supervised features. In: Proceedings of the 13th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics; 2019. p. 944\u20138.\nhttps://doi.org/10.18653/v1/s19-2161\n41.Bourgonje P, Moreno Schneider J, Rehm G. From clickbait to fake news detection: an approach\nbased on detecting the stance of headlines to articles. In: Proceedings of the 2017 EMNLP\nWorkshop: Natural Language Processing Meets Journalism. Association for Computational\nLinguistics; 2017. p. 84\u20139. https://doi.org/10.18653/v1/w17-4215\n42.P\u00e9rez-Almendros C, Espinosa-Anke L, Schockaert S. Cardiff University at SemEval-2019 task 4:\nlinguistic features for hyperpartisan news detection. In: Proceedings of the 13th International\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 33/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 34 \u2014 #34ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nWorkshop on Semantic Evaluation. Association for Computational Linguistics; 2019. p. 929\u201333.\nhttps://doi.org/10.18653/v1/s19-2158\n43.Dumitru VC, Rebedea T. Fake and hyper-partisan news identification. In: Moldoveanu A, Dix AJ,\neditors. 16th International Conference on Human-Computer Interaction, RoCHI 2019; 2019 Oct\n17\u201318; Bucharest, Romania. Matrix Rom; 2019, p. 60\u20137.\n44.Knauth J. Orwellian-times at SemEval-2019 Task 4: a stylistic and content-based classifier. In:\nProceedings of the 13th International Workshop on Semantic Evaluation. Association for\nComputational Linguistics; 2019. p. 976\u201380. https://doi.org/10.18653/v1/s19-2168\n45.Sengupta S, Pedersen T. Duluth at SemEval-2019 Task 4: the pioquinto manterola hyperpartisan\nnews detector. In: Proceedings of the 13th International Workshop On Semantic Evaluation.\nAssociation for Computational Linguistics; 2019. p. 949\u201353. https://doi.org/10.18653/v1/s19-2162\n46.Lyu H, Pan J, Wang Z, Luo J. Computational assessment of hyperpartisanship in news titles.\nICWSM. 2024;18:999\u20131012. https://doi.org/10.1609/icwsm.v18i1.31368\n47.Amason E, Palanker J, Shen MC, Medero J. Harvey Mudd College at SemEval-2019 Task 4: the\nD.X. Beaumont hyperpartisan news detector. Proceedings of the 13th International Workshop on\nSemantic Evaluation. Association for Computational Linguistics; 2019. p. 967\u201370.\nhttps://doi.org/10.18653/v1/s19-2166\n48.Walton D. Ad Hominem arguments. University Alabama Press; 1998.\n49.Tran M. How biased are American media outlets? A framework for presentation bias regression.\nIn: 2020 IEEE International Conference on Big Data (Big Data). IEEE; 2020. p. 4359\u201364.\nhttps://doi.org/10.1109/bigdata50022.2020.9377987\n50.Anand B, Di Tella R, Galetovic A. Information or opinion? Media bias as product differentiation.\nEcon Manag Strategy. 2007;16(3):635\u201382. https://doi.org/10.1111/j.1530-9134.2007.00153.x\n51.Sharma A, Kaur N, Sen A, Seth A. Ideology detection in the Indian mass media. In: 2020\nIEEE/ACM International Conference on Advances in Social Networks Analysis and Mining\n(ASONAM). IEEE; 2020. p. 627\u201334. https://doi.org/10.1109/asonam49781.2020.9381344\n52.Baumer E, Elovic E, Qin Y, Polletta F, Gay G. Testing and comparing computational approaches\nfor identifying the language of framing in political news. In: Proceedings of the 2015 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies. Denver, Colorado: Association for Computational Linguistics; 2015. p. 1472\u201382.\nhttps://doi.org/10.3115/v1/n15-1171\n53.Kong H-K, Liu Z, Karahalios K. Frames and slants in titles of visualizations on controversial topics.\nIn: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. New York,\nNY: Association for Computing Machinery; 2018. p. 1\u201312.\nhttps://doi.org/10.1145/3173574.31749012\n54.Leeson PT, Coyne CJ. Manipulating the media. SSRN. 2011.\n55.Honeycutt N, Jussim L. Political bias in the social sciences: a critical, theoretical, and empirical\nreview. In: Ideological and political bias in psychology: nature, scope, and solutions. 2023. p.\n97\u2013146. https://doi.org/10.1007/978-3-031-29148-7_5 .\n56.Patankar A, Bose J, Khanna H. A bias aware news recommendation system. In: 2019 IEEE 13th\nInternational Conference on Semantic Computing (ICSC). 2019. p. 232\u20138.\nhttps://doi.org/10.1109/icosc.2019.8665610\n57.Barnidge M, Peacock C. A third wave of selective exposure research? The challenges posed by\nhyperpartisan news on social media. MaC. 2019;7(3):4\u20137. https://doi.org/10.17645/mac.v7i3.2257\n58.Gangula RRR, Duggenpudi SR, Mamidi R. Detecting political bias in news articles using headline\nattention. In: Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP. Association for Computational Linguistics; 2019. p. 77\u201384.\nhttps://doi.org/10.18653/v1/w19-4809\n59.Pierri F, Artoni A, Ceri S. HoaxItaly: a collection of Italian disinformation and fact-checking stories\nshared on Twitter in 2019. arXiv preprint 2020. https://arxiv.org/abs/2001.10926\n60.Huang GKW, Lee JC. Hyperpartisan news and articles detection using BERT and ELMo. In: 2019\nInternational Conference on Computer and Drone Applications (IConDA). IEEE; 2019. p. 29\u201332.\nhttps://doi.org/10.1109/iconda47345.2019.9034917\n61.University Politehnica of Bucharest, Dumitru VC, Rebedea T. Topic-based models with fact\nchecking for fake news identification. In: RoCHI - International Conference on Human-Computer\nInteraction, MATRIX ROM; 2021. p. 182\u201390. https://doi.org/10.37789/rochi.2021.1.1.28\n62.Sanchez-Junquera J, Rosso P, Montes M, Ponzetto S, 2021. Masking and transformer-based\nmodels for hyperpartisanship detection in news. 2021. p. 1244\u20131251.\n10.26615/978-954-452-072-4_140\n63.Jeong Lim S, Jatowt A, Yoshikawa M. Understanding characteristics of biased sentences in news\narticles. In: CIKM Workshops; 2018.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 34/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 35 \u2014 #35ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\n64.Naredla NR, Adedoyin FF. Detection of hyperpartisan news articles using natural language\nprocessing technique. Int J Inf Manag Data Insights. 2022;2(1):100064.\nhttps://doi.org/10.1016/j.jjimei.2022.100064\n65.Papadopoulou O, Kordopatis-Zilos G, Zampoglou M, Papadopoulos S, Kompatsiaris Y. Brenda\nStarr at SemEval-2019 Task 4: hyperpartisan news detection. In: Proceedings of the 13th\nInternational Workshop on Semantic Evaluation. 2019; 924\u20138.\nhttps://doi.org/10.18653/v1/s19-2157\n66.M. Alzhrani K. Political ideology detection of news articles using deep neural networks. Intell\nAutomat Soft Comput. 2022;33(1):483\u2013500. https://doi.org/10.32604/iasc.2022.023914\n67.Hermann ES, Chomsky N. Manufacturing consent: the political economy of the mass media.\nManuf Consent. 1994.\n68.Hrckova A, Moro R, Srba I, Bielikova M. Quantitative and qualitative analysis of linking patterns of\nmainstream and partisan online news media in Central Europe. OIR. 2021;46(5):954\u201373.\nhttps://doi.org/10.1108/oir-10-2020-0441\n69.Kulkarni V, Ye J, Skiena S, Wang WY. Multi-view models for political ideology detection of news\narticles. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing. 2018. https://doi.org/10.18653/v1/d18-1388\n70.Alabdulkarim A, Alhindi T. Spider-Jerusalem at SemEval-2019 Task 4: hyperpartisan news\ndetection. In: Proceedings of the 13th International Workshop on Semantic Evaluation. Association\nfor Computational Linguistics; 2019. p. 985\u20139. https://doi.org/10.18653/v1/s19-2170\n71.Alzhrani K. Ideology detection of personalized political news coverage. In: Proceedings of the 2020\n4th International Conference on Compute and Data Analysis. ACM; 2020. p. 10\u201315.\nhttps://doi.org/10.1145/3388142.3388149\n72.Jiang Y, Petrak J, Song X, Bontcheva K, Maynard D. Team Bertha von Suttner at SemEval-2019\nTask 4: hyperpartisan news detection using ELMo sentence representation convolutional network.\nProceedings of the 13th International Workshop on Semantic Evaluation. 2019. p. 840\u20134.\nhttps://doi.org/10.18653/v1/s19-2146\n73.Baly R, Karadzhov G Saleh A, Glass J, Nakov P. Multi-task ordinal regression for jointly predicting\nthe trustworthiness and the leading political ideology of news media. In: Burstein J, Doran C,\nSolorio T, editors. Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). Association for Computational Linguistics; 2019. p. 2109\u201316.\nhttps://doi.org/10.18653/v1/n19-1216\n74.Chen C, Park C, Dwyer J, Medero J. Harvey Mudd College at SemEval-2019 Task 4: the Carl\nKolchak hyperpartisan news detector. In: Proceedings of the 13th International Workshop on\nSemantic Evaluation. Association for Computational Linguistics; 2019. p. 957\u201361.\nhttps://doi.org/10.18653/v1/s19-2164\n75.Pali\u0107 N, Vladika J, \u010cubeli\u0107 D, Lovren\u010di\u0107 I, Buljan M, \u0160najder J. TakeLab at SemEval-2019 Task 4:\nhyperpartisan news detection. In: Proceedings of the 13th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics; 2019. p. 995\u20138.\nhttps://doi.org/10.18653/v1/s19-2172\n76.Anthonio T, Kloppenburg L. Team Kermit-the-frog at SemEval-2019 Task 4: bias detection through\nsentiment analysis and simple linguistic features. In: Proceedings of the 13th International\nWorkshop on Semantic Evaluation. Association for Computational Linguistics; 2019. p. 1016\u201320.\nhttps://doi.org/10.18653/v1/s19-2177\n77.Srivastava V, Gupta A, Prakash D, Sahoo SK, R.R R, Kim YH. Vernon-fenwick at SemEval-2019\nTask 4: hyperpartisan news detection using lexical and semantic features. In: Proceedings of the\n13th International Workshop on Semantic Evaluation. Association for Computational Linguistics;\n2019. p. 1078\u201382. https://doi.org/10.18653/v1/s19-2189\n78.Spezzano F, Shrestha A, Fails JA, Stone BW. That\u2019s fake news! reliability of news when provided\ntitle, image, source bias & full article. Proc ACM Hum-Comput Interact. 2021;5(CSCW1):1\u201319.\nhttps://doi.org/10.1145/3449183\n79.Sridharan ASN. An automated news bias classifier using caenorhabditis elegans inspired recursive\nfeedback network architecture. arXiv preprint 2022. https://arxiv.org/abs/2207.12724\n80.Aksenov D, Bourgonje P, Zaczynska K, Ostendorff M, Moreno-Schneider J, Rehm G. Fine-grained\nclassification of political bias in german news: a data set and initial experiments. In: Proceedings of\nthe 5th Workshop on Online Abuse and Harms (WOAH 2021). Association for Computational\nLinguistics; 2021. p. 121\u201331. https://doi.org/10.18653/v1/2021.woah-1.13\n81.Azizov D, Nakov P, Liang S, Frank at checkthat! 2023: Detecting the political bias of news articles\nand news media. In: Conference and Labs of the Evaluation Forum. 2023.\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 35/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 36 \u2014 #36ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\n82.Hearst MA, Dumais ST, Osuna E, Platt J, Scholkopf B. Support vector machines. IEEE Intell Syst\nTheir Appl. 1998;13(4):18\u201328. https://doi.org/10.1109/5254.708428\n83.Yeh C-L, Loni B, Schuth A. Tom Jumbo-Grumbo at SemEval-2019 Task 4: hyperpartisan news\ndetection with GloVe vectors and SVM. In: Proceedings of the 13th International Workshop on\nSemantic Evaluation. Association for Computational Linguistics; 2019. p. 1067\u201371.\nhttps://doi.org/10.18653/v1/s19-2187\n84.Chen T, Guestrin C. XGBoost. In: Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining. 2016. p. 785\u201394.\nhttps://doi.org/10.1145/2939672.2939785\n85.Gupta V, Kaur Jolly BL, Kaur R, Chakraborty T. Clark Kent at SemEval-2019 Task 4: stylometric\ninsights into hyperpartisan news detection. In: Proceedings of the 13th International Workshop on\nSemantic Evaluation. Association for Computational Linguistics; 2019. p. 934\u20138.\nhttps://doi.org/10.18653/v1/s19-2159\n86.Merow C, Smith MJ, Silander JA Jr. A practical guide to MaxEnt for modeling species\u2019\ndistributions: what it does, and why inputs and settings matter. Ecography. 2013;36(10):1058\u201369.\nhttps://doi.org/10.1111/j.1600-0587.2013.07872.x\n87.Breiman L. Mach Learn. 2001;45(1):5\u201332. https://doi.org/10.1023/a:1010933404324\n88.Chakravartula N, Indurthi V, Syed B. Fermi at SemEval-2019 Task 4: the Sarah-Jane-Smith\nhyperpartisan news detector. In: Proceedings of the 13th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics; 2019. p. 954\u20136.\n89.Cruz A, Rocha G, Sousa-Silva R, Lopes Cardoso H. Team Fernando-Pessa at SemEval-2019\nTask 4: back to basics in hyperpartisan news detection. In: Proceedings of the 13th International\nWorkshop on Semantic Evaluation. Association for Computational Linguistics; 2019. p. 999\u20131003.\nhttps://doi.org/10.18653/v1/s19-2173\n90.Stevanoski B, Gievska S. Team Ned Leeds at SemEval-2019 Task 4: exploring language\nindicators of hyperpartisan reporting. In: Proceedings of the 13th International Workshop on\nSemantic Evaluation. Association for Computational Linguistics; 2019. p. 1026\u201331.\nhttps://doi.org/10.18653/v1/s19-2179\n91.Saleh A, Baly R, Barr\u00f3n-Cede\u00f1o A, Da San Martino G, Mohtarami M, Nakov P, et al. Team\nQCRI-MIT at SemEval-2019 Task 4: propaganda analysis meets hyperpartisan news detection. In:\nProceedings of the 13th International Workshop on Semantic Evaluation. Association for\nComputational Linguistics; 2019. p. 1041\u20136. https://doi.org/10.18653/v1/s19-2182\n92.Bestgen Y. Tintin at SemEval-2019 Task 4: Detecting Hyperpartisan News Article with only Simple\nTokens. Proceedings of the 13th International Workshop on Semantic Evaluation. Association for\nComputational Linguistics; 2019. p. 1062\u20136. https://doi.org/10.18653/v1/s19-2186\n93.Hanawa K, Sasaki S, Ouchi H, Suzuki J, Inui K. The Sally Smedley hyperpartisan news detector at\nSemEval-2019 Task 4. In: Proceedings of the 13th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics; 2019. p. 1057\u201361.\nhttps://doi.org/10.18653/v1/s19-2185\n94.Nguyen D-V, Dang T, Nguyen N. NLP@UIT at SemEval-2019 Task 4: the paparazzo hyperpartisan\nnews detector. In: Proceedings of the 13th International Workshop on Semantic Evaluation.\nAssociation for Computational Linguistics; 2019. p. 971\u20135. https://doi.org/10.18653/v1/s19-2167\n95.Le QV, Mikolov T. Distributed representations of sentences and documents. arXiv preprint 2014.\nhttps://arxiv.org/abs/1405.4053\n96.Pennington J, Socher R, Manning C. Glove: global vectors for word representation. In:\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP). Doha, Qatar:Association for Computational Linguistics; 2014. p. 1532\u20131543.\nhttps://doi.org/10.3115/v1/d14-1162\n97.Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, et al. Deep contextualized word\nrepresentations. arXiv preprint 2018. https://arxiv.org/abs/1802.05365\n98.Cer D, Yang Y, Kong S, Hua N, Limtiaco N, John RS, et al. Universal Sentence Encoder. arXiv\npreprint 2018. https://arxiv.org/abs/1803.11175\n99.Walecki R, Rudovic O, Pavlovic V, Pantic M. Copula ordinal regression for joint estimation of facial\naction unit intensity. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). 2016. p. 4902\u201310. https://doi.org/10.1109/cvpr.2016.530\n100.Rumelhart DE, Hinton GE, Williams RJ. Learning internal representations by error propagation.\n1986. https://api.semanticscholar.org/CorpusID:62245742\n101.Dorogush AV, Gulin A, Gusev G, Kazeev N, Prokhorenkova, LO, Vorobev A. Fighting biases with\ndynamic boosting. arXiv preprint 2017. https://arxiv.org/abs/1706.09516\n102.Gerald Ki Wei H, Jun Choi L. Hyperpartisan news classification with ELMo and bias feature. J Inf\nSci Eng. 2021;37. https://doi.org/10.6688/JISE.202109_37(5).0013\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 36/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 37 \u2014 #37ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\n103.F\u00e4rber M, Qurdina A, Ahmedi L. Team Peter Brinkmann at SemEval-2019 Task 4: detecting biased\nnews articles using convolutional neural networks. In: Proceedings of the 13th International\nWorkshop on Semantic Evaluation. Association for Computational Linguistics; 2019. p. 1032\u20136.\nhttps://doi.org/10.18653/v1/s19-2180\n104.Zehe A, Hettinger L, Ernst S, Hauptmann C, Hotho A. Team Xenophilius Lovegood at\nSemEval-2019 Task 4: hyperpartisanship classification using convolutional neural networks. In:\nProceedings of the 13th International Workshop on Semantic Evaluation. Association for\nComputational Linguistics; 2019. p. 1047\u201351. https://doi.org/10.18653/v1/s19-2183\n105.Ruan Q, Mac Namee B, Dong R. Bias bubbles: using semi-supervised learning to measure how\nmany biased news articles are around us. In: AICS. 2021. p. 153\u201364.\n106.Yang Z, Yang D, Dyer C, He X, Smola A, Hovy E. Hierarchical attention networks for document\nclassification. In: Knight K, Nenkova A, Rambow O, editors. Proceedings of the 2016 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies. San Diego, California: Association for Computational Linguistics; 2016. p. 1480\u20139\nhttps://doi.org/10.18653/v1/n16-1174\n107.Cruz AF, Rocha G, Cardoso HL. On document representations for detection of biased news\narticles. In: Proceedings of the 35th Annual ACM Symposium on Applied Computing. ACM; 2020.\np. 892\u20139. https://doi.org/10.1145/3341105.3374025\n108.Moreno JG, Pitarch Y, Pinel-Sauvagnat K, Hubert G. Rouletabille at SemEval-2019 Task 4: neural\nnetwork baseline for identification of hyperpartisan publishers. In: Proceedings of the 13th\nInternational Workshop on Semantic Evaluation. Association for Computational Linguistics; 2019.\np. 981\u20134. https://doi.org/10.18653/v1/s19-2169\n109.Ko Y, Ryu S, Han S, Jeon Y, Kim J, Park S, et al. KHAN: knowledge-aware hierarchical attention\nnetworks for accurate political stance prediction. In: Proceedings of the ACM Web Conference\n2023 WWW 2023. Austin, TX: ACM; 2023. p. 1572\u201383. ISBN: 9781450394161.\nhttps://doi.org/10.1145/3543507.3583300\n110.Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. 1997;9(8):1735\u201380.\nhttps://doi.org/10.1162/neco.1997.9.8.1735 PMID: 9377276\n111.Isbister T, Johansson F. Dick-Preston and Morbo at SemEval-2019 Task 4: transfer learning for\nhyperpartisan news detection. In: Proceedings of the 13th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics; 2019. p. 939\u201343.\nhttps://doi.org/10.18653/v1/s19-2160\n112.Li C, Goldwasser D. Using social and linguistic information to adapt pretrained representations for\npolitical perspective identification. In: Zong C, Xia F, Li W, Navigli R, editors. Findings of the\nAssociation for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational\nLinguistics; 2021. p. 4569\u20134579. https://doi.org/10.18653/v1/2021.findings-acl.401\n113.Cramerus R, Scheffler T. Team Kit Kittredge at SemEval-2019 Task 4: LSTM voting system. In:\nProceedings of the 13th International Workshop on Semantic Evaluation. Association for\nComputational Linguistics; 2019. p. 1021\u20135. https://doi.org/10.18653/v1/s19-2178\n114.Zhang C, Rajendran A, Abdul-Mageed M. UBC-NLP at SemEval-2019 Task 4: hyperpartisan news\ndetection with attention-based Bi-LSTMs. In: Proceedings of the 13th International Workshop on\nSemantic Evaluation. Association for Computational Linguistics; 2019. p. 1072\u20137.\nhttps://doi.org/10.18653/v1/s19-2188\n115.Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. Roberta: a robustly optimized BERT\npretraining approach. arXiv preprint 2019. https://arxiv.org/abs/1907.11692\n116.Kim K-M, Lee M, Won H-S, Kim M-J, Kim Y, Lee S. Multi-stage prompt tuning for political\nperspective detection in low-resource settings. Appl Sci. 2023;13(10):6252.\nhttps://doi.org/10.3390/app13106252\n117.Liu Y, Zhang XF, Wegsman D, Beauchamp N, Wang L. POLITICS: pretraining with same-story\narticle comparison for ideology prediction and stance detection. In: Carpuat M, de Marneffe MC,\nMeza Ruiz IV, editors. Findings of the Association for Computational Linguistics: NAACL.\nAssociation for Computational Linguistics; 2022. p. 1354\u20131374.\nhttps://doi.org/10.18653/v1/2022.findings-naacl.101\n118.Kim MY, Johnson KM. CLoSE: contrastive learning of subframe embeddings for political bias\nclassification of news media. In: Calzolari N, Huang CR, Kim H, Pustejovsky J, Wanner L, Choi\nKS, et al., editors. Proceedings of the 29th International Conference on Computational Linguistics,\nInternational Committee on Computational Linguistics. 2022. p. 2780\u20132793.\n119.Devlin J, Chang M, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv preprint 2018. https://arxiv.org/abs/1810.04805\n120.Roy S, Goldwasser D. Weakly supervised learning of nuanced frames for analyzing polarization in\nnews media. In: Webber B, Cohn T, He Y, Liu Y, editors. Proceedings of the 2020 Conference on\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 37/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 38 \u2014 #38ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\nEmpirical Methods in Natural Language Processing (EMNLP). Association for Computational\nLinguistics; 2020. p. 7698\u20137716. https://doi.org/10.18653/v1/2020.emnlp-main.620\n121.Baly R, Da San Martino G, Glass J, Nakov P. We can detect your bias: predicting the political\nideology of news articles. In: Webber B, Cohn T, He Y, Liu Y, editors. Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics; 2020. p. 4982\u20134991. https://doi.org/10.18653/v1/2020.emnlp-main.404\n122.Da Silva SC, Paraboni I. Politically-oriented information inference from text. JUCS.\n2023;29(6):569\u201394. https://doi.org/10.3897/jucs.96652\n123.Shaprin D, Da San Martino G, Barr\u00f3n-Cede\u00f1o A, Nakov P. Team Jack Ryder at SemEval-2019\nTask 4: using BERT representations for detecting hyperpartisan news. In: Proceedings of the 13th\nInternational Workshop on Semantic Evaluation. Association for Computational Linguistics; 2019.\np. 1012\u20135. https://doi.org/10.18653/v1/s19-2176\n124.Ahmed U, Lin JC, Srivastava G. Temporal positional lexicon expansion for federated learning\nbased on hyperpatism detection. Exp Syst. 2022;40(5):e13183. https://doi.org/10.1111/exsy.13183\n125.Ahmed U, Lin JC-W, Srivastava G. Semisupervised federated learning for temporal news\nhyperpatism detection. IEEE Trans Comput Soc Syst. 2023;10(4):1758\u201369.\nhttps://doi.org/10.1109/tcss.2023.3247602\n126.Drissi M, Sandoval Segura P, Ojha V, Medero J. Harvey Mudd College at SemEval-2019 Task 4:\nthe Clint Buchanan hyperpartisan news detector. In: Proceedings of the 13th International\nWorkshop on Semantic Evaluation. Association for Computational Linguistics; 2019. p. 962\u20136.\nhttps://doi.org/10.18653/v1/s19-2165\n127.Mutlu O, Can OA, Dayanik E. Team Howard Beale at SemEval-2019 Task 4: hyperpartisan news\ndetection with BERT. Proceedings of the 13th International Workshop on Semantic Evaluation.\nAssociation for Computational Linguistics; 2019. p. 1007\u201311. https://doi.org/10.18653/v1/s19-2175\n128.Sma\u015fdu RA, Echim SV, Cercel DC, Marin I, Pop F. From fake to hyperpartisan news detection\nusing domain adaptation. arXiv preprint 2023. https://arxiv.org/abs/2308.02185\n129.Omidi Shayegan S, Nejadgholi I, Pelrine K, Yu H, Levy S, Yang Z, et al. An evaluation of language\nmodels for hyperpartisan ideology detection in Persian Twitter. In: Ojha AK, Ahmadi S, Cinkova S,\nFransen T, Liu CH, McCrae JP, editors. Proceedings of the 2nd Workshop on Resources and\nTechnologies for Indigenous, Endangered and Lesser-resourced Languages in Eurasia (EURALI)\n@ LREC-COLING 2024, ELRA and ICCL, Torino, Italia; 2024. p. 51\u201362.\nhttps://aclanthology.org/2024.eurali-1.8 .\n130.Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, et al. Llama 2: open foundation and\nfine-tuned chat models. arXiv preprint 2023. https://arxiv.org/abs/2307.09288\n131.Afsarmanesh N, Karlgren J, Sumbler P, Viereckel N. Team Harry Friberg at SemEval-2019 Task 4:\nidentifying hyperpartisan news through editorially defined metatopics. In: Proceedings of the 13th\nInternational Workshop on Semantic Evaluation. Association for Computational Linguistics; 2019.\np. 1004\u20136. https://doi.org/10.18653/v1/s19-2174\n132.Sanchez-Junquera J. On the detection of political and social bias. 2021.\n133.Mets M, Karjus A, Ibrus I, Schich M. Automated stance detection in complex topics and small\nlanguages: the challenging case of immigration in polarizing news media. PLoS One.\n2024;19(4):e0302380. https://doi.org/10.1371/journal.pone.0302380 PMID: 38669237\n134.Karjus A, Cuskley C. Evolving linguistic divergence on polarizing social media. Humanit Soc Sci\nCommun. 2024;11(1):422. https://doi.org/10.1057/s41599-024-02922-9\n135.Sylwester K, Purver M. Twitter language use reflects psychological differences between democrats\nand republicans. PLoS One. 2015;10(9):e0137422. https://doi.org/10.1371/journal.pone.0137422\nPMID: 26375581\n136.Fraxanet E, Pellert M, Schweighofer S, G\u00f3mez V, Garcia D. Unpacking polarization: antagonism\nand alignment in signed networks of online interaction. PNAS Nexus. 2024;3(12):pgae276.\nhttps://doi.org/10.1093/pnasnexus/pgae276 PMID: 39703230\n137.Lee N, Liu Z, Fung P. Team yeon-zi at SemEval-2019 Task 4: hyperpartisan news detection by\nde-noising weakly-labeled data. In: Proceedings of the 13th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics; 2019. p. 1052\u20136.\nhttps://doi.org/10.18653/v1/s19-2184\n138.Joo Y, Hwang I. Steve Martin at SemEval-2019 Task 4: ensemble learning model for detecting\nhyperpartisan news. In: Proceedings of the 13th International Workshop on Semantic Evaluation.\nAssociation for Computational Linguistics; 2019, p. 990\u20134. https://doi.org/10.18653/v1/s19-2171\n139.Ning Z, Lin Y, Zhong R. Team Peter-Parker at SemEval-2019 Task 4: BERT-based method in\nhyperpartisan news detection. In: Proceedings of the 13th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics; 2019. p. 1037\u201340.\nhttps://doi.org/10.18653/v1/s19-2181\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 38/ 39\nii\n\u201cpone.0316989\u201d \u2014 2025/2/17 \u2014 19:00 \u2014 page 39 \u2014 #39ii\ni\nii\niPLOS ONE A systematic review of automated hyperpartisan news detection\n140.Silverman C, Strapagiel L, Shaban H, Hall E, Singer-Vine J. Hyperpartisan facebook pages are\npublishing false and misleading information at an alarming rate. BuzzFeed News. 2016.\n141.Gebhard L, Hamborg F. The POLUSA dataset: 0.9m political news articles balanced by time and\noutlet popularity. 2020.\n142.Norregaard J, Horne BD, Adali S. NELA-GT-2018: a large multi-labelled news dataset for the study\nof misinformation in news articles. arXiv preprint 2019. https://arxiv.org/abs/1904.01546\n143.Fan L, White M, Sharma E, Su R, Choubey PK, Huang R, et al. In plain sight: Media bias through\nthe lens of factual reporting. In: Inui K, Jiang J, Ng V, Wan X, editors. Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP). Hong Kong: Association for\nComputational Linguistics; 2019. p. 6343\u20136349. https://doi.org/10.18653/v1/D19-1664\n144.Baly R, Karadzhov G, Alexandrov D, Glass J, Nakov P. Predicting factuality of reporting and bias\nof news media sources. In: Riloff E, Chiang D, Hockenmaier J, Tsujii J, editors. Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium:\nAssociation for Computational Linguistics; 2018. p. 3528\u20133539.\nhttps://doi.org/10.18653/v1/D18-1389\n145.Horne BD, Dron W, Khedr S, Adali S. Sampling the news producers: a large news and feature\ndata set for the study of the complex media landscape. arXiv preprint 2018.\nhttps://arxiv.org/abs/1803.10124\n146.Szwoch J, Staszkow M, Rzepka R, Araki K. Creation of polish online news corpus for political\npolarization studies. In: Afli H, Alam M, Bouamor H, Casagran CB, Boland C, Ghannay S, editors.\nProceedings of the LREC 2022 workshop on Natural Language Processing for Political Sciences.\nEuropean Language Resources Association; 2022. p. 86\u201390\n147.Lim S, Jatowt A, Yoshikawa M. Creating a dataset for fine-grained bias detection in news articles.\n2020.\n148.Li C, Goldwasser D. Encoding social information with graph convolutional networks for political\nperspective detection in news media. In: Korhonen A, Traum D, Marquez L, editors. Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy:\nAssociation for Computational Linguistics; 2019. p. 2594\u20132604.\nhttps://doi.org/10.18653/v1/P19-1247\nPLOS ONE https://doi.org/10.1371/journal.pone.0316989 February 18, 2025 39/ 39", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "A systematic review of automated hyperpartisan news detection", "author": ["MJ Maggini", "D Bassi", "P Piot", "G Dias", "PG Otero"], "pub_year": "2025", "venue": "PloS one", "abstract": "Hyperpartisan news consists of articles with strong biases that support specific political parties.  The spread of such news increases polarization among readers, which threatens social"}, "filled": false, "gsrank": 154, "pub_url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0316989", "author_id": ["", "CRL_1kMAAAAJ", "gY7dBboAAAAJ", "47zG0aIAAAAJ", ""], "url_scholarbib": "/scholar?hl=en&q=info:2PyrTE005KoJ:scholar.google.com/&output=cite&scirp=153&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D150%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=2PyrTE005KoJ&ei=H7WsaJGVCJXUieoPmrax2A8&json=", "num_citations": 1, "citedby_url": "/scholar?cites=12314024787741244632&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:2PyrTE005KoJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0316989&type=printable"}}, {"title": "Facebook's architecture undermines vaccine misinformation removal efforts", "year": "2022", "pdf_data": "Facebook\u2019s Architecture Undermines Vaccine Misinformation Removal Efforts Authors: David A. Broniatowski1,2*, Jiayan Gu3, Amelia M. Jamison4, Joseph R. Simons5**, Lorien C. Abroms2,3 Affiliations:   1Department of Engineering Management and Systems Engineering, The George Washington University; Washington, DC, 20052, USA. 2Institute for Data, Democracy, and Politics, The George Washington University; Washington, DC, 20052, USA. 3Department of Prevention and Community Health, The George Washington University; Washington, DC, 20052, USA. 4Department of Health, Behavior, and Society, Bloomberg School of Public Health, Johns Hopkins University; Baltimore, MD, 21205, USA. 5Office of the Assistant Secretary for Financial Resources, United States Department of Health & Human Services; Washington, DC, 20543, USA.   *Corresponding author. Email: broniatowski@gwu.edu  **The views expressed are those of the author and do not reflect the official position of the U.S. Department of Health and Human Services, or the United States \nMisinformation promotes distrust in science,1 undermines public health,2 and may drive civil unrest.3\u20135 Vaccine misinformation, in particular, has stalled efforts to overcome the COVID-19 pandemic,6,7 prompting social media platforms\u2019 attempts to reduce it.  Some have questioned whether \u201csoft\u201d content moderation remedies8,9 \u2013e.g., flagging and downranking misinformation \u2013  were successful, suggesting that the addition of \u201chard\u201d content remedies \u2013 e.g., deplatforming and content bans \u2013 is necessary.10\u201314 We therefore examined whether Facebook\u2019s vaccine misinformation content removal policies were effective. Here, we show that Facebook\u2019s policies reduced the number of anti-vaccine posts but also caused several perverse effects: pro-vaccine content was also removed, engagement with remaining anti-vaccine content repeatedly recovered to pre-policy levels, and this content became more misinformative, more politically polarised, and more likely to be seen in users\u2019 newsfeeds. We explain these results as an unintended consequence of Facebook\u2019s design goal: promoting community formation. Members of communities dedicated to vaccine refusal appear to seek out misinformation from multiple sources. Community administrators make use of several channels afforded by the Facebook platform to disseminate misinformation. Our findings suggest the need to address how social media platform architecture enables community formation and mobilisation around misinformative topics when managing the spread of online content.  \nMain Online misinformation undermines trust in scientific evidence1 and medical recommendations.2 It has been linked to harmful offline behaviours including stalled public health efforts,6 civil unrest,5 and mass violence.4 The COVID-19 pandemic has exacerbated misinformation online, resulting in widespread concern about low vaccine uptake rates,11,15 even as government officials continue to urge vaccination in response to new disease variants.16 Social media has enabled misinformation to be quickly and widely disseminated. Therefore, policymakers and public officials have put significant pressure on social media platforms to curtail misinformation spread.17\u201319  Several major platforms have taken steps intended to reduce misinformation spread. Years of \u201csoft\u201d content remedies \u2013 such as warning labels and downranking objectionable content in search \u2013 have demonstrated some success;20,21 however, misinformation continues to spread widely online, leading many to question the impact and efficacy of these interventions.22 Some have suggested that combining these \u201csoft\u201d remedies with \u201chard\u201d content remedies7,8 \u2013content removal and deplatforming objectionable accounts10,23,24 \u2013 could significantly curtail misinformation spread.14 In practice, social media platforms already use a combination of \u201chard\u201d and \u201csoft\u201d content remedies; however, evidence for the short-term efficacy of \u201chard\u201d remedies is mixed,25\u201328 and the long-term efficacy of such strategies has not been systematically examined. \u201cHard\u201d remedies have also spurred accusations of censorship and concrete legal action.29,30 There is therefore a critical need to understand whether this combination of remedies is effective, and if not, why not.  Here, we conduct a comprehensive evaluation of the world\u2019s largest social media platform\u2019s \u2013 Facebook\u2019s \u2013 attempts to remove anti-vaccine misinformation as COVID-19 \nvaccines were rolled out in late 2020 and early 2021. We estimate the causal impacts of Facebook\u2019s remedies on anti- and pro-vaccine content before and after Facebook implemented \u201chard\u201d remedies targeting anti-vaccine misinformation. Our dataset consists of roughly 1 million posts from 488 English-language public Facebook pages and groups identified on November 15, 2020, and covers a time period beginning immediately prior to the outbreak of the COVID-19 pandemic through the end of the first Omicron variant wave in the United States: November 15, 2019 through February 28, 2022. (A second dataset of about 1 million posts from 297 venues identified on July 21, 2021, was also collected for replication purposes.) As expected, we found evidence that such policies, when added to existing \u201csoft\u201d content remedies reduced the number of anti-vaccine posts. However, we also observed several unexpected findings that, taken together, call into question the long-term efficacy of Facebook\u2019s policies, and suggest that they may have even been counterproductive. We examine the mechanisms that may underlie these observations, explaining them as a consequence of Facebook\u2019s core design goals and system architecture. Facebook is designed to build online communities. (Facebook\u2019s mission statement is to \u201cgive people the power to build community, and to bring the world closer together\u201d31.) In implementing this goal, Facebook has created a system that affords anti-vaccine content providers several ways to circumvent the intent of content moderation efforts. These content providers use Facebook to construct communities around vaccine scepticism, encouraging and enabling users to seek out anti-vaccine content online, but also encouraging these communities to take action offline action, such as participating in political activism. Taken together, our findings suggest the need to address a new dimension \u2013 how social media platform design features enable community formation and mobilisation around misinformative topics \u2013 to the problem of managing the spread of misinformation online.  \nAnti- and Pro-Vaccine Posts Were Removed \n Fig 1. Facebook\u2019s policies did not lead to a sustained decrease in engagement with anti-vaccine content despite a reduction in the total number of posts.  ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decline in posts in a. anti-vaccine pages (51% average decrease,  \ud835\udf122(66)= 1425.15, p<0.001) b. anti-vaccine groups (63% average decrease,  \ud835\udf122(66)= 1030.71, p<0.001), and c. pro-vaccine pages (26% average decline,  \ud835\udf122(66)= 612.17, p<0.001). We did not detect a significant difference in d. pro-vaccine groups (25% average increase, \ud835\udf122(66)= 61.15, p=0.65). Additionally, engagements declined with content in e. anti-vaccine pages (37% average decrease,  \ud835\udf122(66)= 468.37, p<0.001). Despite this average decline, engagements fell within the prediction\u2019s 90% confidence intervals for the months of November 2020-January 2021, April-May 2021, August \u2013 September, 2021, and November 2021. Engagements increased significantly beyond pre-policy predictions in f. anti-vaccine groups (52% average increase, \ud835\udf122(66)= 164.27, p<0.001). Engagements declined with content in g. pro-vaccine pages (13% average decrease,  \n\n\ud835\udf122(66)= 111.43, p<0.001) but h. increased for the months of August \u2013 October 2021 in pro-vaccine groups (52% average increase in the first sample,  \ud835\udf122(66)= 164.64, p<0.001). Error bars reflect 90% confidence intervals. Data from the second sample is shown for comparison and qualitatively replicates findings from the first sample. In anti-vaccine groups, maximum engagements exceeded 75,000 in September, 2021, and in  pro-vaccine groups several new \u201cvaccine hunter\u201d groups had formed to raise awareness about COVID vaccine test sites.    We found that Facebook reduced the number of posts in anti-vaccine pages by 51%, \ud835\udf122(66)= 1425.15, p<0.001,  and in groups by 63%, \ud835\udf122(66)= 1030.71, p<0.001, on average (Fig. 1a-b).  However, Facebook\u2019s policy also appears to have had a notable \u201cunintended consequence\u201d, having significantly reduced the number of posts in pro-vaccine pages, \ud835\udf122(66)= 612.17, p<0.001, by 26% on average (Fig. 1c).   Anti-Vaccine Engagement Recovered We expected to see that a reduction in the number of posts (\u201csupply\u201d) led to a sustained reduction in the number of engagements (\u201cdemand\u201d).32 We found that overall user engagement (Fig. 1e) and especially reactions (e.g., \u201clikes\u201d indicating demand; Extended Data Fig. 1i) to content in anti-vaccine pages repeatedly returned to pre-policy trends by May, 2021, and again by September, 2021.  Compared to pre-policy trends, engagements with content in groups increased by 52% on average, \ud835\udf122(66)= 164.64, p<0.001 (Fig. 1f).  Engagement with content in the second sample of groups, collected in July 2021, exceeded the pre-policy trend by an average of 628%, reaching levels comparable to the pre-policy maximum in May, 2021. This means that \nFacebook\u2019s content removals may not have reduced overall exposure to anti-vaccine content compared to pre-policy trends. We therefore posited that users actively \u201cdemand\u201d \u2013 i.e., seek out \u2013 new misinformation despite Facebook attempting to curtail \u201csupply\u201d. In order to more fully examine this supposition, we constructed a simulation model that we calibrated to pre-policy data (see Supplementary Information, S1). We best reproduced post-policy data when we assumed that prior content removals lead to an increase in demand for anti-vaccine content in remaining venues. Additionally, in our model, Facebook\u2019s current \u201chard\u201d content remedies were most effective if coupled with a decrease in demand for misinformation.   Facebook\u2019s Architecture Enables Demand  An analysis of Facebook\u2019s system architecture explains why their content removal policies did not lead to a sustained decrease in engagement. Users interact with anti-vaccine content in three ways: 1) as followers of pages \u2013 venues run by anti-vaccine opinion leaders where only the administrator can post; 2) as members of groups \u2013 venues for public discussion where any group member (including page administrators) can post, comment, or engage; and 3) by interacting with posts on users\u2019 newsfeeds, which may originate from pages, groups, or other sources.  The resulting layered hierarchy (Fig. 2) is resilient to disruption33 \u2013 e.g., content or venue removal \u2013 while still allowing page administrators some degree of control over information flow.34 Each layer in this hierarchy contributes to the system\u2019s resilience in different, cumulative, ways.  \n Fig 2. Pages, groups, and newsfeeds form a layered hierarchy that facilitates access to, and demand for, anti-vaccine content despite \u201chard\u201d remedies. In the \u201ctop\u201d layer, administrators link to one another\u2019s pages, enabling users to circumvent Facebook\u2019s search algorithm. a. Combining data across samples to allow us to detect links between both, we found that page proportions decreased by 54% on average, \ud835\udf122(66)= 192.43  p<0.001, but returned to the 90% confidence intervals by April 2021, and reached pre-policy projection (black dashed line) by June, 2021.  b. Anti-vaccine (red) and pro-vaccine (green) administrators use these links to enable audiences to find new pages. In the \u201cmiddle\u201d layer,  pages administrators, and other users, can create and administer groups, facilitating information exchange. Although intended to be organic communities, repeated simultaneous link sharing suggests coordinated behaviour between nominally separate venues. c. Combining data across samples, Facebook\u2019s policies did not lead to a sustained reduction in coordinated URL \n\nproportions in anti-vaccine venues compared to projections (black dashed line). Despite an average weekly decrease of 54%, \ud835\udf122(66)= 267.00,  p<0.001, coordinated link proportions returned to the 90% confidence intervals throughout 2021, reaching the pre-policy projection by January, 2022.  d. Several anti-vaccine (red) venues engaged in this coordinated behaviour, forming a connected component including new groups. In contrast, coordination between pro-vaccine venues (green) was rare. In the \u201cbottom\u201d layer,  users access content via their newsfeeds, including posts from pages and groups, but also other sources. e. In pages, Facebook\u2019s policies led to a 30% average increase in \u201cnon-toxic\u201d reactions, \ud835\udf122(66)= 463.39,  p<0.001 and a 26% average decrease in \u201ctoxic\u201d, \ud835\udf122(66)= 816.74,  p<0.001.  f. In groups \u2013 Facebook\u2019s policies led to a 33% average increase in \u201cnon-toxic\u201d reactions, \ud835\udf122(66)= 280.21,  p<0.001 and a 40% average decrease in \u201ctoxic\u201d reactions, \ud835\udf122(66)= 423.24,  p<0.001. g. Facebook\u2019s newsfeed algorithm reportedly35 promotes content garnering these \u201cnon-toxic\u201d reactions (\u201clikes\u201d, \u201clove\u201d, \u201ccare\u201d, and \u201csad\u201d) over \u201ctoxic\u201d reactions (\u201cangry\u201d, \u201dhaha\u201d, and \u201cwow\u201d). Proportions are shown on a logit scale.  First, page administrators can explicitly collaborate to share followers and content in case some pages are removed (e.g., by linking to or \u201cliking\u201d36 one another\u2019s\u2019 pages). Consistent with this explanation, we observe that both anti-vaccine and pro-vaccine pages frequently posted links to other aligned pages, forming densely connected clusters (Fig 2b).  Users accessing these links likely follow multiple pages; thus, removing some pages is unlikely to prevent access to anti-vaccine content. Specifically, for Facebook\u2019s policy to be successful, it would need to have led to a sustained reduction in links between anti-vaccine pages, eroding the system\u2019s resilience. Instead, we found that the proportion of links between anti-vaccine pages repeatedly returned to \nthe pre-policy baseline by June, 2021, despite having decreased by an average of 54% following the policy, \ud835\udf122(66)= 192.43  p<0.001 (Fig. 2a; in contrast, pro-vaccine pages were unaffected; Extended Data Fig. 2b). This means that the anti-vaccine page network was \u201cself-healing\u201d, with page administrators enabling users to satisfy their demand for misinformation by directing them to new pages as older pages were removed.  A second source of resilience stems from covert coordination between venues that are nominally independent. Page administrators and other active users can reach larger audiences \u2013 and potentially recruit these audiences to follow pages \u2013 by sharing the same content in multiple different venues, and especially groups, simultaneously.  Before the policy, we found evidence that several anti-vaccine groups routinely posted the same content at the same time \u2013 a sign of potential \u201ccoordinated inauthentic behaviour\u201d, which is explicitly prohibited by Facebook37 (Extended Data Figure 3b). This behaviour was comparatively rare in pro-vaccine venues. We expected to see a sustained reduction in such behaviour in anti-vaccine venues. Although we observed an average decrease of 54% in the proportion of \u201cnear-simultaneous\u201d link pairs,  \ud835\udf122(66)= 267.00,  p<0.001 (Fig. 2c), levels of coordination returned to pre-policy levels by early 2022. Furthermore, several new groups created after November, 2020, appear to have coordinated with existing groups (Fig. 2d). Qualitatively, we observed that these coordinated links often promoted anti-vaccine Facebook pages, anti-vaccine content on other social media platforms (e.g., YouTube), known anti-vaccine websites (e.g., ageofautism.com), and websites promoting political calls to action, such as petitions opposing mandatory vaccines.  In contrast, coordinated pro-vaccine content pointed to websites facilitating COVID vaccination, such as vaccinefinder.org. \nA third source of resilience arises from Facebook\u2019s newsfeed algorithm, which is designed to promote content that has generated \u201cmeaningful social interaction\u201d.35 Although Facebook states that vaccine misinformation is not eligible for recommendation using this algorithm,38 engagement levels with anti-vaccine content might recover from post removals if this remaining content is not marked as misinformation, and if the newsfeed algorithm determines this content is more likely to generate \u201cmeaningful social interaction\u201d. Facebook reportedly35 uses the number of reactions (e.g., likes\u201d, \u201clove\u201d, \u201cangry\u201d, etc.) content has already spurred to make this determination (Fig. 2g). Specifically, at the time Facebook\u2019s policies were first implemented, the newsfeed algorithm reportedly downranked content spurring angry, wow, and haha reactions because they believed these to be associated with \u201ctoxic\u201d content and misinformation. In contrast, safer \u201cnon-toxic\u201d content reportedly spurred likes, love, and sad reactions, leading Facebook to increase their relative weights.35 We found that \u201cnon-toxic\u201d likes, love, care, and sad reactions to anti-vaccine content have increased in proportion since Facebook\u2019s policy was implemented by a weekly average of 30% in pages, \ud835\udf122(66)= 463.39,  p<0.001, (Fig. 2e) and 33% in groups, \ud835\udf122(66)= 280.21,  p<0.001 (Fig. 2f),  whereas \u201ctoxic\u201d angry, wow, and haha reactions have decreased, by a weekly average of 26% in pages (\ud835\udf122(66)= 816.74,  p<0.001) and 40% in groups (\ud835\udf122(66)= 423.24,  p<0.001), with results replicating across samples. We did not observe significant changes in these reactions for pro-vaccine content, except for \u201ctoxic\u201d reactions in pro-vaccine pages, which increased by 271% (\ud835\udf122(66)= 398.87,  p<0.001; Extended Data Fig. 4).  Thus, it appears that Facebook\u2019s policies led anti-vaccine content producers, but not pro-vaccine content producers, to increasingly frame posts in a manner that was weighted as more meaningful by Facebook\u2019s content recommendation algorithms.  \nMisinformation & Polarisation Increased  One might think that Facebook\u2019s policies were successful in removing explicit misinformation, while retaining topics promoting discussion among vaccine hesitant individuals. We therefore examined how the topics that were discussed changed after Facebook\u2019s policies, using a probabilistic topic model to summarise the posts in our datasets (Fig. 3; Supplementary Materials, Table S2-1).  \n Fig 3. Topics emphasising severe adverse reactions and calls to political action increased in pages and groups, garnering \u201cnon-toxic\u201d reactions.  a. In pages, 18 topics differed significantly from pre-policy projections. Topics pertaining to severe adverse reactions (including the canard that vaccines cause autism) and allegedly harmful or unethical vaccine ingredients increased in proportion, as did topics condemning vaccine mandates and those from \u201crebel doctors\u201d  providing pseudoscientific rationales for vaccine refusal. In contrast, topics pertaining to national politics, COVID testing, measles, and specific conspiracy theories decreased in proportion.  b. In groups, 17 topics differed significantly from \n\npre-policy projections. Topics calling for political action, including support for legislation and requests to sign petitions, and building community by welcoming new group members, increased. Topics alleging severe adverse reactions , unethical vaccine ingredients, and promoting \u201crebel doctors\u201d also increased. In contrast, several topics that decreased in pages also decreased in groups. Several topics containing misinformation alleging vaccine harms garnered \u201cnon-toxic\u201d reactions, likely promoting them in users\u2019 newsfeeds, including: c. In pages, topics alleging severe adverse events and deaths from COVID-vaccine, and claiming that vaccines cause autism garnered \u201clikes\u201d and d. to a lesser extent, also garnered \u201clove\u201d reactions. e. These  topics especially garnered \u201csad\u201d reactions as did topics alleging that vaccine kill children. Although these topics did not garner many e. \u201clikes\u201d or f. \u201clove\u201d reactions in groups, g. they did garner \u201csad\u201d reactions in groups. Beyond this misinformation,  topics pertaining to political action also garnered \u201cnon-toxic\u201d reactions, including: c. In pages, topics opposing vaccine mandates and calling for or celebrating political action garnered \u201clikes\u201d d. and \u201clove\u201d reactions. Additionally, e. topics reporting on vaccine mandates garnered \u201csad\u201d reactions. In groups, topics opposing vaccine mandates and discussing Canadian politics \u2013 ultimately culminating in the \u201cfreedom convoy\u201d rallies of early 2022 \u2013 also garnered e. \u201clikes\u201d, f. \u201clove\u201d reactions, and g. \u201csad\u201d reactions.  Although posts expressing some misinformative topics about vaccines (e.g., conspiracy theories; see Supplementary Materials, Table S2-2) decreased in proportion, we also observed increases in several misinformative topics that explicitly violated Facebook\u2019s community standards. Specifically, a topic containing reports of allegedly severe COVID-19 side effects and death increased significantly compared to pre-policy projections in both anti-vaccine pages, 314%, \ud835\udf122(66)= 518.69,  p<0.001, and groups, 270%, \ud835\udf122(66)= 841.56,  p<0.001. This increase \noccurred during a crucial time period for vaccine uptake: the initial COVID vaccine rollout.  In anti-vaccine pages, this increase goes beyond what can be attributed to the news cycle, having increased 16% more on average than the corresponding pro-vaccine topic over the span of our study,  \ud835\udf122(120)= 620.23,  p<0.001 and 88% more on average between November 18, 2021, and  July 4, 2021 (when the Biden administration set a goal for at least 70% of the US population to have received at least one COVID vaccine shot; Extended Data Fig. 5).  Several other topics also increased in proportion relative to the pre-policy trend, including those spreading conspiracy theories from \u201crebel doctors\u201d who claim to have been silenced by the medical establishment (Supplementary Materials, Table S2-2), alleging that vaccines expose people to toxins and/or \u201cunhealthy radiation\u201d (e.g., 5G wireless), claiming that vaccines contain foetal tissues, and claiming that vaccines cause autism.  These misinformative topics are all prohibited under Facebook\u2019s community standards,39 calling into question Facebook\u2019s ability to carry out its content removal policies in a comprehensive and consistent manner with their existing technology and approach. Beyond these misinformative topics, we also observed proportional increases in several topics calling for political or legislative action (which, when associated with anti-vaccine content, were also prohibited by Facebook\u2019s community standards).39 These topics especially increased in Facebook groups, which are designed to promote user interaction and community formation. In pages, these topics spurred \u201cnon-toxic\u201d reactions, promoting them in newsfeeds, and included posts framing vaccine refusal as \u201chealth freedom\u201d, discussing vaccine mandates, arguing that these mandates are illegal or immoral, and celebrating successful legislation opposing these mandates.  \nFacebook is embedded in a larger information ecosystem comprising several different social media platforms and external websites. Furthermore, Facebook\u2019s policies called for increasing the quality of information shared.39 We therefore examined whether the quality of  information from external sources changed as consequence of content removal policies. If successful, Facebook\u2019s policies should have led to a decrease in the proportion of links to sources that are known to be \u201clow credibility\u201d because they routinely share misinformation, and an increase in \u201chigh credibility\u201d sources that adhere to strict standards of fact-checking.40,41. Instead, weekly links to websites that are known to spread misinformation increased by an average of 8% in pages, \ud835\udf122(66)= 934.55,  p<0.001, (Fig. 4a) and by 32% between November 18, 2020 and July 4, 2021. Overall, users engaged 308% more with posts containing these misinformative links \ud835\udf122(66)= 247.64, p<0.001 (Fig. 4b).  Furthermore, links to high-quality academic and government sources decreased by a weekly average of 44% in groups compared to pre-policy projections, \ud835\udf122(64)= 161.72,  p<0.001 (Extended Data Fig. 6b).  Finally, on a factual accuracy scale ranging from 0 = \u201cVery Low\u201d to 5 = \u201cVery High\u201d we found that the average factual accuracy of user engagements decreased by 0.13 points in pages, \ud835\udf122(65)= 183.36,  p<0.001, and 0.80 points in groups, \ud835\udf122(66)= 130.82,  p<0.001 even though the overall accuracy of rated posts did not change significantly (Extended Data Figure 7).  \n Fig 4. Facebook\u2019s policy led to an increase in off-platform links promoting misinformation and polarisation.  a. In pages, the proportion of links to \u201ciffy\u201d sources (https://iffy.news/index/#methodology) increased by an average of 8% in pages, \ud835\udf122(66)= 934.55,  p<0.001, and by 32% between November 18, 2020 and July 4, 2021, compared to pre-policy projections. b. These links spurred 308% more engagements per week, on average, \ud835\udf122(66)= 247.64, p<0.001 than pre-policy projections. c. In contrast, we did not detect significant differences in links to misinformative sources in groups,  \ud835\udf122(66)= 24.43, p=1.00 d. or in engagements with these links, \ud835\udf122(66)= 70.96, p=0.32. Among misinformative links were pointers to \u201calternative\u201d social media platforms, such as Bitchute, Rumble, Telegram, and Gab, all of which are known to host politically polarized content, especially from the far right. e. The weekly proportion of URLs pointing to these platforms, and especially to Bitchute, increased by a factor of 10 in pages after November 18, 2020.  f. These URLs garnered a larger share of engagements, whereas engagements with mainstream platforms Twitter and YouTube decreased. g. We observed a similar increase in \n\nlinks to \u201calternative\u201d platforms in groups, h. and these links garnered a larger share of weekly engagements.  In general, we observed that the average partisan bias of rated URLs i. increased in Facebook pages, by an average of 0.27 points,  \ud835\udf122(66)= 222.24,  p<0.001, j. leading to a slight (0.02 points), yet significant increase in engagements, \ud835\udf122(66)= 114.22,  p<0.001.  k. This effect was even more pronounced in groups, where an average increase of 0.24 points,  \ud835\udf122(66)= 106.48,  p=0.001 l. led to a 0.78 point increase in right-wing polarisation of the average engagement, \ud835\udf122(66)= 402.10,  p<0.001.  All proportions are shown on a logit scale.  One reason for this change may be that Facebook\u2019s policies led anti-vaccine content producers to link to external sources of misinformative content rather than posting it explicitly on Facebook in an attempt to circumvent Facebook\u2019s content moderation efforts. Consistent with this explanation, we observed that anti-vaccine content producers increasingly linked to alternative social media platforms, such as Bitchute, Rumble, Gab, and Telegram in lieu of more mainstream platforms that had implemented similar content removal policies (e.g., YouTube and Twitter; Fig 4e-h).  In practice, alternative platforms often host politically-extreme right wing content.42 This means that Facebook\u2019s content removal policies may have the unintended consequence of radicalising their audiences. On the other hand, some have alleged that Facebook disproportionately targets right wing content for removal, and downranks content that has not been removed.43 If so, then one would expect to see a decrease in right-wing bias among links shared on the platform, and a decrease in engagement with right-wing content. Instead, on a partisan bias scale ranging from -4 = \u201cExtreme Left\u201d to +4 = \u201cExtreme Right\u201d, we observed that rated links shared in anti-vaccine venues became more polarised towards the political right wing \nby an average of 0.27 points in pages (Fig. 4i),  \ud835\udf122(66)= 222.24,  p<0.001 and 0.24 points groups,  \ud835\udf122(66)= 106.48,  p=0.001 (Fig. 4k). The average user engagement also became more polarised in the same direction by an average  of 0.02 points in pages, \ud835\udf122(66)= 114.21,  p<0.001 (Fig 4j), and 0.78 points in groups, \ud835\udf122(66)= 402.10,  p<0.001 (Fig 4l). In contrast, content in pro-vaccine pages became 0.13 points more polarised towards the political left wing, \ud835\udf122(66)= 95.86, p=0.01, and we did not detect a significant change in the polarisation of engagements (Extended Data Fig. 8).   Conclusion Taken together, our findings demonstrate that Facebook\u2019s policy reduced the number of posts in anti-vaccine venues, but was not successful in inducing a sustained reduction in engagement with anti-vaccine content, including misinformation. This underscores a need to account for, and address, the forces driving users\u2019 engagement with \u2013 i.e.,  \u201cdemand\u201d for \u2013 misinformative content. Social media platforms are engineered systems, whose explicit design goal is to enable the formation of online communities. By definition, Facebook groups and pages are communities made up of people sharing the same interests \u2013 in this case, rationales for vaccine refusal. Communities provide members the ability to make sense of overwhelming amounts of online content,44 while also offering social support and a feeling of solidarity.45 These are attractive attributes that drive up demand for this content.  Users satisfy this demand when they seek out content and community through alternate venues, including on other social media platforms.46 Furthermore, active anti-vaccine community members appear to be using Facebook\u2019s technological affordances to expand their peers\u2019 access to new venues and content, undermining attempts to curtail supply. Specifically, page \nadministrators and group members coordinate with one another when using the platform to circumvent content and venue removals. This enables users to find aligned anti-vaccine content and like-minded peers, even if their specific venue has been removed. Furthermore, anti-vaccine content producers appear to be spurring increased engagement, and especially reactions that are more likely to be promoted in users\u2019 newsfeeds. Thus, resistance to \u201chard\u201d content remedies may stem from the fact that these remedies directly oppose social media platforms\u2019 core design function of facilitating community engagement.  Furthermore, Facebook\u2019s system architecture appears to have facilitated unintended consequences. To garner engagements, anti-vaccine content producers make use of topics and links to websites promoting \u201cdiagnostic frames\u201d: specific narratives that identify putative problems and grievances \u2013 e.g., sensationalised vaccine harms. Other content offers \u201cprognostic frames\u201d: offering solutions to these problems, e.g., calls for concrete political action, such as encouraging people to vote on specific legislative bills, circulating petitions, or joining protests against mask-wearing, mandates, and lockdowns. Finally, a third set of content offers \u201cmotivational frames\u201d: making successful concrete action seem possible, e.g., messages celebrating legislative victories or emphasising widespread support for shared values such as \u201chealth freedom\u201d or opposition to vaccine mandates. Taken together, these findings suggest that Facebook\u2019s architecture facilitates, and perhaps even accelerates, the creation of some of the features of a social movement47 in a digital space. When combined with increased political polarisation, this could, in turn, facilitate coordinated offline behaviour.48  (See also work in preparation by C. Bailard, R. Tromble, W. Zhong, F. Bianchi, P. Hosseini, and DAB.) Limitations to this study include that only public Facebook pages and groups were studied. This leaves lacunae in the extent to which external evaluations can infer Facebook\u2019s \nsuccess. Although we cannot make claims about behaviours in private spaces, the data available through tools such as CrowdTangle constitute a critical window into the largest, and most public, venues on the platform, which are most likely to achieve high numbers of exposures. Indeed, significant prior work has shown that important misinformation about vaccines is often found in public data,49,50 in part because anti-vaccine advocates seek to recruit the vaccine hesitant in public forums. Similarly, we cannot rule out the possibility that pro-vaccine venues might have contained some sensationalist stories of vaccine harm;  however, these stories do not appear to have been sufficiently prevalent to have been detected by our topic models. Finally, our data cannot distinguish between posts or engagements made by unique individuals or the same individuals repeatedly posting. However, this concern is mitigated by the fact that, in pages, only administrators may post, whereas users in any venue may only react to a given post once. Taken as a whole, our findings emphasise the critical need for platforms to continue to provide researchers with access to such public data.  In conclusion, Facebook\u2019s three-layered architecture facilitates its resilience in the face of disruption from Facebook\u2019s \u201chard\u201d content removal remedies. This engineered system possesses a structure that might promote \u201cparticipatory\u201d51 misinformation and could even lead to social mobilisation. Our results therefore suggest that attempts to address misinformation must rely on a multifaceted strategy that goes beyond \u201chard\u201d and \u201csoft\u201d content remedies. Analogous strategies have been effective in other domains. For example, effective tobacco control interventions focus on both prevention and treatment. By analogy, our findings and model suggest that different approaches may be needed for those who actively seek out misinformation versus those who simply casually encounter it. Just as tobacco control prevention strategies combine both demand-side and supply-side interventions (e.g., educational campaigns, but also \ntaxation),52 both \u201chard\u201d and \u201csoft\u201d content remedies have a role to play in moderating the online ecosystem, as do technological advances which may help improve detection of problematic content. However, beyond these interventions, tobacco control makes use of community-based resources supporting tobacco cessation for current smokers. By analogy, in the short term,  Facebook or public health officials might consider using social media platforms to engage directly with users who have been repeatedly exposed to misinformation, and may attempt to recruit vaccine hesitant individuals into other venues where their needs for community can be met, while ensuring that their questions about vaccines are answered in a manner that is most consistent with public health guidelines.25,53 In the longer term, platforms may incorporate explicit features designed to promote communities that organically resist misinformation.54 There is nevertheless a dearth of scientific literature regarding specific interventions to reduce demand for misinformation among active seekers. Thus, future work is needed to develop interventions that can work in tandem with both \u201chard\u201d and \u201csoft\u201d content remedies to reduce misinformation seeking behaviour.   \nReferences  1. West, J. D. & Bergstrom, C. T. Misinformation in and about science. Proc. Natl. Acad. Sci. U.S.A. 118, e1912444117 (2021). 2. Chou, W.-Y. S., Oh, A. & Klein, W. M. P. Addressing Health-Related Misinformation on Social Media. JAMA 320, 2417\u20132418 (2018). 3. Hearing on \u2018Disinformation Nation: Social Media\u2019s Role in Promoting Extremism and Misinformation\u2019. Democrats, Energy and Commerce Committee https://energycommerce.house.gov/committee-activity/hearings/hearing-on-disinformation-nation-social-medias-role-in-promoting (2021). 4. Gowen, A. As mob lynchings fueled by WhatsApp messages sweep India, authorities struggle to combat fake news. Washington Post (2018). 5. Silverman, B., Mac, R. & Lytvynenko, J. How Facebook Failed To Prevent Stop The Steal. BuzzFeed News (2021). 6. Loomba, S., de Figueiredo, A., Piatek, S. J., de Graaf, K. & Larson, H. J. Measuring the impact of COVID-19 vaccine misinformation on vaccination intent in the UK and USA. Nat Hum Behav (2021) doi:10.1038/s41562-021-01056-1. 7. Pierri, F. et al. Online misinformation is linked to early COVID-19 vaccination hesitancy and refusal. Sci Rep 12, 5966 (2022). 8. Goldman, E. Content Moderation Remedies. Mich. Tech. L. Rev. 28, 1 (2021). 9. Grimmelmann, J. The Virtues of Moderation. Yale J.L. & Tech. 17, 42\u2013109 (2015). 10. Jhaver, S., Boylston, C., Yang, D. & Bruckman, A. Evaluating the Effectiveness of Deplatforming as a Moderation Strategy on Twitter. Proc. ACM Hum.-Comput. Interact. 5, 381:1-381:30 (2021). \n11. Ball, P. & Maxmen, A. The epic battle against coronavirus misinformation and conspiracy theories. Nature 581, 371\u2013374 (2020). 12. Facebook\u2019s Latest Attempt To Address Vaccine Misinformation\u2014And Why It\u2019s Not Enough | Health Affairs Blog. https://www.healthaffairs.org/do/10.1377/hblog20201029.23107/full/. 13. Rodriguez, S. 12 attorneys general call on Facebook and Twitter to remove anti-vaxxers from their services. CNBC https://www.cnbc.com/2021/03/24/attorneys-general-call-on-facebook-and-twitter-to-remove-anti-vaxxers-off-their-services.html (2021). 14. Bak-Coleman, J. B. et al. Combining interventions to reduce the spread of viral misinformation. Nat Hum Behav 1\u20139 (2022) doi:10.1038/s41562-022-01388-6. 15. Tyson, A., Johnson, C. & Funk, C. U.S. Public Now Divided Over Whether To Get COVID-19 Vaccine. Pew Research Center Science & Society https://www.pewresearch.org/science/2020/09/17/u-s-public-now-divided-over-whether-to-get-covid-19-vaccine/ (2020). 16. Sun, L. H. Biden officials urge use of booster shots, antivirals against BA.5. Washington Post (2022). 17. Senators Klobuchar, Baldwin, Peters Urge Tech Industry Leaders to Combat Coronavirus Vaccine Misinformation. U.S. Senator Amy Klobuchar https://www.klobuchar.senate.gov/public/index.cfm/2021/1/senators-klobuchar-baldwin-peters-urge-tech-industry-leaders-to-combat-coronavirus-vaccine-misinformation. 18. Call to Action: CSIS-LSHTM High-Level Panel on Vaccine Confidence and Misinformation. https://www.csis.org/analysis/call-action-csis-lshtm-high-level-panel-vaccine-confidence-and-misinformation. \n19. Donovan, J. Social-media companies must flatten the curve of misinformation. Nature (2020) doi:10.1038/d41586-020-01107-z. 20. Gu, J. et al. The impact of Facebook\u2019s vaccine misinformation policy on user endorsements of vaccine content: An interrupted time series analysis. Vaccine 40, 2209\u20132214 (2022). 21. Pennycook, G. et al. Shifting attention to accuracy can reduce misinformation online. Nature 592, 590\u2013595 (2021). 22. Roozenbeek, J., Freeman, A. L. J. & van der Linden, S. How Accurate Are Accuracy-Nudge Interventions? A Preregistered Direct Replication of Pennycook et al. (2020). Psychol Sci 095679762110245 (2021) doi:10.1177/09567976211024535. 23. Ali, S. et al. Understanding the Effect of Deplatforming on Social Networks. in 13th ACM Web Science Conference 2021 187\u2013195 (Association for Computing Machinery, 2021). doi:10.1145/3447535.3462637. 24. Chandrasekharan, E. et al. You Can\u2019t Stay Here: The Efficacy of Reddit\u2019s 2015 Ban Examined Through Hate Speech. Proc. ACM Hum.-Comput. Interact. 1, 31:1-31:22 (2017). 25. Broniatowski, D. A., Dredze, M. & Ayers, J. W. \u2018First Do No Harm\u2019: Effective Communication About COVID-19 Vaccines. Am J Public Health 111, 1055\u20131057 (2021). 26. Gorwa, R., Binns, R. & Katzenbach, C. Algorithmic content moderation: Technical and political challenges in the automation of platform governance. Big Data & Society 7, 2053951719897945 (2020). 27. Gillespie, T. Content moderation, AI, and the question of scale. Big Data & Society 7, 2053951720943234 (2020). 28. Ribeiro, M. H. et al. Do Platform Migrations Compromise Content Moderation? Evidence from r/The_Donald and r/Incels. Proc. ACM Hum.-Comput. Interact. 5, 1\u201324 (2021). \n29. Senate Bill 7072 (2021) - The Florida Senate. https://www.flsenate.gov/Session/Bill/2021/7072/. 30. 87(2) HB 20 - Senate Committee Report version - Bill Text. https://capitol.texas.gov/tlodocs/872/billtext/html/HB00020S.htm. 31. Company Info | Meta. https://about.facebook.com/company-info/. 32. Munger, K. & Phillips, J. Right-Wing YouTube: A Supply and Demand Perspective. The International Journal of Press/Politics 27, 186\u2013219 (2022). 33. Broniatowski, D. A. Flexibility Due to Abstraction and Decomposition. Systems Engineering 20, 98\u2013117 (2017). 34. Broniatowski, D. A. & Moses, J. Measuring Flexibility, Descriptive Complexity, and Rework Potential in Generic System Architectures. Systems Engineering 19, 207\u2013221 (2016). 35. Merrill, J. B. & Oremus, W. Five points for anger, one for a \u2018like\u2019: How Facebook\u2019s formula fostered rage and misinformation. Washington Post (2021). 36. Johnson, N. F. et al. Hidden resilience and adaptive dynamics of the global online hate ecology. Nature 573, 261\u2013265 (2019). 37. Inauthentic Behavior | Transparency Center. https://transparency.fb.com/policies/community-standards/inauthentic-behavior/. 38. What are recommendations on Facebook? | Facebook Help Center. https://www.facebook.com/help/1257205004624246. 39. COVID-19 and Vaccine Policy Updates & Protections | Facebook Help Center. https://www.facebook.com/help/230764881494641. \n40. Grinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B. & Lazer, D. Fake news on Twitter during the 2016 US presidential election. Science 363, 374\u2013378 (2019). 41. Shao, C. et al. The spread of low-credibility content by social bots. Nat Commun 9, 4787 (2018). 42. Zhou, Y., Dredze, M., Broniatowski, D. A. & Adler, W. D. Elites and foreign actors among the alt-right: The Gab social media platform. First Monday 24, (2019). 43. Jordan, J., Collins, D., Gaetz, M. & Steube, W. G. Reining in Big Tech\u2019s Censorship of Conservatives. (2020). 44. Reyna, V. F., Broniatowski, D. A. & Edelson, S. M. Viruses, Vaccines, and COVID-19: Explaining and Improving Risky Decision-making. Journal of Applied Research in Memory and Cognition 10, 491\u2013509 (2021). 45. Snow, D. A., Soule, S. A. & Kriesi, H. The Blackwell Companion to Social Movements. (John Wiley & Sons, 2008). 46. Mitts, T., Pisharody, N. & Shapiro, J. Removal of Anti-Vaccine Content Impacts Social Media Discourse. in 14th ACM Web Science Conference 2022 319\u2013326 (Association for Computing Machinery, 2022). doi:10.1145/3501247.3531548. 47. Benford, R. D. & Snow, D. A. Framing Processes and Social Movements: An Overview and Assessment. Annual Review of Sociology 26, 611\u2013639 (2000). 48. Giugni, M., McAdam, D. & Tilly, C. How social movements matter. (University of Minnesota Press, 1999). 49. Broniatowski, D. A. et al. Facebook Pages, the \u201cDisneyland\u201d Measles Outbreak, and Promotion of Vaccine Refusal as a Civil Right, 2009\u20132019. Am J Public Health 110, S312\u2013S318 (2020). \n50. Jamison, A. M. et al. Not just conspiracy theories:  Vaccine opponents and pro-ponents add to the COVID-19 \u2018infodemic\u2019 on Twitter. The Harvard Kennedy School Misinformation Review 1, (2020). 51. Starbird, K., Arif, A. & Wilson, T. Disinformation as Collaborative Work: Surfacing the Participatory Nature of Strategic Information Operations. Proc. ACM Hum.-Comput. Interact. 3, 127:1-127:26 (2019). 52. U.S. National Cancer Institute and World Health Organization. The Economics of Tobacco and Tobacco Control. (2016). 53. Larson, H. J. & Broniatowski, D. A. Why Debunking Misinformation Is Not Enough to Change People\u2019s Minds About Vaccines. Am J Public Health 111, 1058\u20131060 (2021). 54. Forestal, J. Designing for Democracy: How to Build Community in Digital Environments. (Oxford University Press, 2021).  \nMethods Data Source We downloaded data from CrowdTangle,55  a public insights tool owned and operated by Facebook which has been called \u201cperhaps the most effective transparency tool in the history of social media\u201d.56,57  CrowdTangle tracks interactions with public content from Facebook pages and groups (\u201cvenues\u201d). It does not include activity on private accounts, or posts made visible only to specific groups of followers.  Identifying vaccine-related pages and groups Similar to prior work58,59 that relies upon iterative procedures to reduce biases associated with keyword selection: 1. We first identified a large set of pages and groups that mentioned vaccines at least once within the most recent ~300,000 posts. We did not include venues with earlier timestamps since our aim was to capture activity in the most prolific Facebook groups in the leadup to Facebook\u2019s policy implementation. To do so, we searched CrowdTangle on November 15, 2020, identifying and downloading all posts containing at least one keyword from the following list: \u201cvaccine, vaxx, vaccines, vaccination, vaccinated, vax, vaxxed, vaccinations, jab\u201d. Several of these posts contained content pertaining to guns, and to pet and other animal vaccines. Thus, we ran a second search excluding posts containing the following keywords: \u201cgun, dog, cat, foster, adopt, shelter, vet, kennel, pet, chicken, livestock, kitten, puppy, paw, cow\u201d. We conducted this search on November 15, 2020 and retrieved the 299,981 most recent page posts and 299,904 group posts meeting search criteria before hitting CrowdTangle\u2019s download limit, with the earliest posts timestamped September 7, 2020, for pages, and July 1, 2020, for groups. This procedure yielded 73,438 pages and 57,485 groups. \nTo ensure that we only retained venues that routinely discussed vaccines, we retained all pages and groups whose name contained at least one of the strings  \u201cvacc\u201d, \u201cvax\u201d, or \u201cjab\u201d, or which posted very frequently about vaccines: i.e., in the top percentile \u2013 at least 44 times for pages and 58 times for groups. This procedure yielded 1,231 pages and 773 groups. 2. Several of the venues generated in Step 1 were news organizations that did not primarily focus on vaccination. We therefore further narrowed down our initial list as follows: we retrieved as many posts as possible from these venues yielding the 299,994 most recent page posts and 299,969 group posts before hitting CrowdTangle\u2019s download limit. As above, we did not collect earlier posts since our aim was to identify venues that actively posted about vaccination in the leadup to the policy\u2019s implementation. The earliest post from these venues was timestamped November 8, 2020, for pages, and November 13, 2020, for groups. We retained all venues for which at least 20% of posts retrieved contained at least one word containing \u201cvacc\u201d or \u201cvax\u201d.  We selected this 20% threshold by inspection, and our results were insensitive to changing it (relaxing the threshold yielded more groups and pages that were characterized as \u201cother\u201d in Step 3, which were not included in our analysis. All groups and pages were checked for relevance by two authors \u2013 AMJ and JG).  3. On November 15, 2020, we retrieved all posts from the venues identified in Step 2 for a 12-month period starting on November 15, 2019, forward. We did not exceed the download limit for these venues. We repeatedly collected content from these venues from November 30, 2020 through February 28, 2022 (see Extended Data Table 1). Qualitatively, we found that these posts focused on general vaccine content, adhering closely to existing typologies of pro- and anti-vaccine topics.60,61 We therefore manually annotated these venues  as either pro-vaccine, anti-vaccine, or \u201cother\u201d (see Extended Data Table 2). Two independent \nannotators (JG and AMJ) manually assessed each group and page following a two-tiered coding scheme,61 achieving high reliability. Entities were first labelled as either pro-vaccine, anti-vaccine, or other (Cohen\u2019s k = 0.88, 95% CI: 0.85-0.92). Next, a sub-category was assigned based on specific concerns, adapted from a previously published coding scheme60,61 (Cohen\u2019s k = 0.75, 95% CI: 0.71-0.79; Extended Data Table 2).  Categorization was based on content shared in the \u201cabout\u201d section of each venue. When this section was left blank, annotators considered the venue\u2019s title, any imagery used, and recent posts to decide.  All venues were double-coded, with disagreements discursively reconciled. We retained all venues that were labelled as pro-vaccine or anti-vaccine.   Identifying a second sample of venues To test sensitivity to our venue selection date, we ran a second search on July 21, 2021, to identify new venues using the same technique. These new venues contained 79 (33 anti-vaccine and 46 pro-vaccine) pages and 139 (69 anti-vaccine and 70 pro-vaccine) groups. Of these, 27 (34%; 14, 42% anti-vaccine and 13, 28% pro-vaccine) pages and 114 groups (82%; 47, 68% anti-vaccine and 67, 96% pro-vaccine) were not present in the original sample.  Of the venues not present in the original sample, we did not detect posts before November 15, 2020 for 5 (19%; 4, 29% anti-vaccine and 1, 8% pro-vaccine) pages and 59 groups (52%; 19, 40% anti-vaccine and 40, 60% pro-vaccine)  when we conducted a CrowdTangle search on February 22, 2022.  We again repeatedly collected posts in pages and groups from the second sample at multiple points in time, ranging from August 9, 2021 through February 28, 2022 (see Extended Data Table 2). Our results largely replicated across samples.  \nInterrupted Time Series Analysis We examined changes to the weekly number of posts, and engagements with those posts using an interrupted time series design with a non-equivalent control group \u2013 one of the strongest quasi-experimental designs available.62 This design enables us to estimate causal effects of changes to Facebook\u2019s policies because any changes to observed data affecting anti-vaccine that are not due to Facebook\u2019s policies content \u2013 e.g., external news about vaccine trials \u2013 should affect both pro- and anti-vaccine groups and pages since they are both focused on vaccination. We compared the year prior to November 15, 2020 to the remainder of the dataset, enabling us to tease apart the effects of Facebook\u2019s \u201chard\u201d content remedies on anti-vaccine content.  We analysed pages separately from groups. Pages \u2013 representing the voices of opinion leaders, who tend to be  explicitly pro- or anti-vaccine rather than vaccine hesitant25,53,63 \u2013 differ from Facebook groups \u2013 designed to be discussion forums.64 Whereas only page administrators can post in pages, any member can post in groups.  Measures: Posts and Engagements For each dataset, we calculated the weekly number of posts in anti- and pro-vaccine pages and groups. We applied a logarithmic transform to post volumes to control for data skew.  We also calculated the weekly number of engagements with these posts as the sum of all Likes, Shares, Comments, and other reactions (Love, Wow, Haha, Sad, Angry, and Care) reported by CrowdTangle. Facebook\u2019s algorithms reportedly uses a weighted sum of these  engagements when prioritising content within users\u2019 newsfeeds.65 After applying a logarithm transform, we found that individual engagement types are strongly correlated with one another for both pages (Cronbach\u2019s a = 0.97, 95% CI: 0.97-0.98) and groups (Cronbach\u2019s a = 0.96, 95% CI: 0.95-0.97), \nmeaning that our results are robust to changes in weights. (We excluded the \u201cCare\u201d emoji from Cronbach\u2019s a calculations because they were not introduced until March, 2020, were not widely used until May, 2020, and continue to make up less than 1% of all engagements).   Fitting ARIMA Models We conducted interrupted time series analyses using Autoregressive Integrated Moving Average (ARIMA) models fit to weekly sums of posts and engagements from November 15, 2019 through November 15, 2020 \u2013 immediately prior to Facebook\u2019s November 18th, 2020, removal of  \u201cStop Mandatory Vaccination\u201d \u2013 one of the largest anti-vaccine fan pages.66 On December 3, 2020, Facebook announced its intention to remove false claims about COVID-19.67 Finally, on February 8, 2021, Facebook extended this policy to vaccine misinformation in general,68 while promising to increase the credibility of information shared about vaccines more generally. Thus, November 18th, 2020, marks the beginning of a series of policies targeting anti-vaccine content and combining prior \u201csoft\u201d anti-vaccine content removal remedies with new \u201chard\u201d content removal remedies. Our results are nevertheless insensitive to our selection of this specific date (see Supplementary Information S3).  To control for the formation of new pages and groups between November 15, 2019, and November 15, 2020, we divided weekly post and engagement counts by the total number of weekly venues in each dataset prior to fitting ARIMA models (results were similar when analysing raw post counts, see Supplementary Information S3).  We fit all ARIMA models to pre-policy data using the auto_arima function in the pmdarmia python package.69 When time series were not stationary, as determined by an Augmented Dickey Fuller test, data were detrended using differencing. We selected the number \nof autoregressive and moving average terms using a parallel grid search to minimise the corrected Akaike Information Criterion (AICc).  Generating Counterfactual Predictions We used these models to generate counterfactual projections for weekly posts or engagements that would have been present assuming no \u201chard\u201d remedies. We calculated the percent difference between these counterfactual projections and observed post and engagement counts. We consider a policy to have been effective if it consistently reduced content beyond the 90% confidence bounds of these projections. The residual values of an ARIMA model are normally distributed and independently identically distributed (i.i.d.). meaning the residual sum of squares values follows a c2 distribution with the degrees of freedom equal to the number of predicted datapoints. We conducted c2 tests to assess the goodness of fit of the model\u2019s pre-policy projections against post-policy data.   Using Pro-Vaccine Venues as a Non-Equivalent Control Group Pro-vaccine venues make an ideal non-equivalent control group because, like anti-vaccine venues, they contain users who are motivated to post about vaccines and would therefore respond to exogenous factors, such as the news cycle, in the same way; however, platforms\u2019 policies were not designed to target pro-vaccine content. A statistically significant difference in anti-vaccine, but not pro-vaccine, venues, or between anti- and pro-vaccine venue effect sizes \u2013 indicates that it is more likely to have been Facebook\u2019s policies, and not some contemporaneous event, which caused the observed change.  \nAnalysis of Facebook\u2019s System Architecture Overt Links between Anti-Vaccine and Pro-Vaccine Pages We examined whether Facebook\u2019s new policies reduced links between the anti- and pro-vaccine pages in our sample. (The number of links to, and between, groups was zero in most weeks.) We identified all posts containing a link starting with www.facebook.com. We then calculated the proportion of these links pointing to the anti- and pro-vaccine pages in each of our samples. Specifically, we identified and extracted the unique numerical Facebook ID and username for each pro- and anti-vaccine page in each dataset. We considered a source page to be linked to a target page if a URL posted on the source page began with www.facebook.com/<Facebook ID>/ or  www.facebook.com/<target username>/. We next calculated the weekly proportion of all such links that pointed from anti-vaccine pages to other anti-vaccine pages (excluding self-links), and from pro-vaccine pages to other pro-vaccine pages.  We next conducted interrupted time series analyses on these weekly proportions. After applying a logit transform to our data to control for floor and ceiling effects, we fit ARIMA models to pre-policy data, comparing post-policy data to model projections (Extended Data Fig. 2).  Results were similar across venues identified November 15, 2020, and July 21, 2021; therefore, we combined these data since doing so allowed us to also examine links between pages that were in separate datasets. We also used the links that we extracted to construct unweighted networks. In practice, these networks qualitatively resemble prior work based upon mutual \u201clikes\u201d between pages, but which currently require access to Facebook\u2019s commercial APIs to replicate at scale.36,70 Like this prior work, our networks were displayed using a force-directed layout algorithm.71   Coordinated Link-Sharing  \nFacebook's community standards disallow \u201cus[ing] fake accounts,  artificially boost[ing] the popularity of content,  or engag[ing] in behaviours designed to enable other violations under our Community Standards.\u201d37 Prior work72,73 suggests that this type of \u201ccoordinated inauthentic behaviour\u201d may be detected under the assumption that \u201cnear-simultaneous link sharing\u201d is a signal of coordination.73\u201376  Building upon prior work,73 we operationalized \u201cnear-simultaneous\u201d link sharing in a manner that was intended to be robust to the specific query being used. We conducted three \u201cblank search\u201d queries between March 30 and March 31, 2020 on CrowdTangle to identify the ~300,000 most recent posts each for pages and groups available on the platform, combining across pages and groups. We calculated the time difference in seconds between each successive share of the same URLs. To distinguish between coordinated and uncoordinated behaviours, we modelled the distribution of these interarrival times as a mixture of exponential distributions with components corresponding to \u201cnear simultaneous\u201d sharing and \u201cnon-simultaneous\u201d sharing. We used the exp-mixture-model77 package in Python to fit these exponential mixture models. The best fitting model (see Supplemental Materials) was made up of two components, with mean interarrival times of \u03bcnear-simultaneous=9.95 seconds and \u03bcnon-simultaneous=227.45 seconds, meaning that a URL shared by two venues in under 33 seconds is more likely to have originated from the \u201cnear-simultaneous\u201d component than the \u201cnon-simultaneous\u201d component. This number is comparable to thresholds defined heuristically in prior work.73\u201376  We considered venues to be routinely coordinated if their empirical frequency of  \u201cnear simultaneous\u201d links significantly exceeded 13.34% \u2013 the expected likelihood that links were drawn from the non-simultaneous distribution \u2013 using binomial tests. Venues were linked if were significantly coordinated at the p<0.05 level after controlling for multiple comparisons \nusing the Holm-Bonferroni procedure. We also tested several threshold values ranging from 25 - 41 seconds and found that results qualitatively replicated.  \u201cToxic\u201d and \u201cNon-Toxic\u201d Reactions For each dataset, we calculated the weekly proportion of engagements that were reportedly \u201ctoxic\u201d (angry, haha, and wow), and \u201cnon-toxic\u201d (likes, love, sad, care), and conducted interrupted time series analyses on these proportions. We fit ARIMA models to logit-transformed pre-policy data, comparing post-policy data to pre-policy projections. Results replicated when examining each type of engagement separately (Supplementary Information 5).    Topic Modelling We extracted the text from each Facebook post by combining the \u201cMessage\u201d, \u201cImage Text\u201d, \u201cLink Text\u201d and \u201cDescription\u201d fields returned by CrowdTangle.  After identifying and removing non-English posts using the langdetect Python package,78 we used Latent Dirichlet Allocation (LDA79) \u2013 a popular text summarization algorithm \u2013 implemented in the MALLET software package,80 to fit two separate topic models to the text of the 268,875 unique English anti-vaccine posts and 76,954 pro-vaccine posts collected in venues identified November 15, 2020. In each case, we selected a model with 50 topics, using Bayesian hyperparameter optimization81  to ensure coherent topics. We next fit the data from venues collected on July 21, 2021, to these topics using post-hoc topic model inference. After reading the 50 most representative posts from each topic, two annotators (AJ, JG) assigned descriptive labels to each topic with a third author (DAB) summarising them for brevity. Based upon these labels, topics were assigned into the same typology categories that were used to categorise pro- and anti-vaccine venues (see \nSupplementary Materials). Next, for each dataset, we calculated the average weekly proportion of each topic. Finally, we fit ARIMA models to logit-transformed weekly topic proportions, comparing observations to pre-policy projections (see Supplementary Materials).  Reaction-Weighted Topic Analysis For each of the \u201cnon-toxic\u201d reaction, we calculated the weekly proportion, r, of those reactions associated with a given topic, as follows:  \ud835\udf0c!,#|%=\u2211\ud835\udf0b#,&\u00d7\ud835\udc5b!,&&\u2208%\u2211\ud835\udc5b!,&&\u2208% where pt,p is the proportion of post p assigned to topic t, nr,p is the number of reactions of type r to post p, and \ud835\udc5d\u2208\ud835\udc64 is the set of all posts, p, in week w. Although we combined across the first and second samples for this analysis, results replicate when analysing each sample independently (Supplementary Materials).  URL Quality For each dataset, we extracted all URLs in the \u201cLink\u201d field returned by CrowdTangle, or the \u201cFinal Link\u201d field if it was non-empty (meaning that the \u201cLink\u201d field used a URL shortener). Next, using the TLDExtract Python module,82 we extracted the top-level domain (TLD) and suffix for each URL (for example, the top-level domain of www.example.com/this-is-an-example.html is example.com).   \nURL Source Credibility and Partisanship Significant prior work41,83\u201388 shows that posts with links to misinformative sources may serve as a proxy for content quality.88\u201390 After removing links to facebook.com, we calculated the weekly proportion in pro- and anti-vaccine venues of all TLDs listed on iffy.news \u2013  a list of publishers identified by MediaBiasFactCheck.com as having \u201clow\u201d or \u201cvery low\u201d factual reporting scores \u2013 on April 30, 2022. MediaBiasFactCheck are known to be strongly correlated with several other URL credibility ratings.84,87 We also calculated the weekly proportion of engagements with low credibility URLs by weighting each post containing an off-platform URL by the total number of engagements with it.  We separately examined posts from \u201chigh quality health sources\u201d84,87 which we operationalized as academic and government sources. All TLDs ending in .gov, .gc.ca, .mil, .nhs.uk, starting with gov., mygov., government., containing .govt. or .gov., or matching who.int, paho.org, un.org, canada.ca, ontario.ca, toronto.ca, or alberta.ca were coded as \u201cgovernment\u201d. Similarly, all TLDs ending in .edu, containing .edu., .ac., thelancet.com, sciencedirect.com., medrxiv.org, pnas.org, apa.org, nature.com, sciencemag.org, nejm.org, bmj.com, mayoclinic.org, aaas.org, healthdata.org, researchgate.net, or rand.org were coded as \u201cacademic\u201d. We calculated the weekly proportion of all such TLDs.  We also examined links to news sources. Specifically, we collated a list of 2,340 publishers that had been given a \u201cFact Rating\u201d score by MediaBiasFactCheck.com from \u201cVery Low\u201d (0) to \u201cVery High\u201d (5) as of February 3, 2022. Links to these publishers made up 49% and 23% of links to pages and groups among venues identified November 15, 2020, garnering 57% and 25% of all engagements, respectively. Similarly, these links made up 47% and 20% of links \nto pages and groups among venues identified July 21, 2021, garnering 55% and 14% of all engagements, respectively.  MediaBiasFactCheck.com also scores websites by their partisan bias ranging from far right (-4) to far left (4). We collated a list of 1,314 publishers that had been scored by MediaBiasFactCheck.com as of February 3, 2022. Links to these publishers made up 34% and 17% of off-platforms links to pages and groups, among venues identified November 15, 2020, garnering 41% and 19% of engagements, respectively. Similarly, links to these publishers made up 34% and 15% of off-platforms links to pages and groups, among venues identified July 21, 2021, garnering 55% and 14% of engagements.  We calculated the weekly average \u201cFact Rating\u201d and \u201cBias Rating\u201d for all rated links, and examined how this average changed over time in both pro- and anti-vaccine venues. We conducted interrupted time series ARIMA analyses and chi-square tests on these weekly averages.    Links to Alternative Social Media Platforms We examined the prevalence of links to both mainstream social media platforms YouTube (youtube.com; youtu.be) and Twitter (twitter.com; t.co) but also alternative platforms such as Bitchute (bitchute.com), Rumble (rumble.com), Gab (gab.com), and Telegram (t.me). We conducted interrupted time series ARIMA analyses and chi-square tests on logit-transformed weekly proportions of both mainstream and alternative platforms.    Data and code availability  \nAll processed data required to produce the reports in this study are available as data files included in supplementary information. Code required to generate these HTML files are also included in supplementary materials. Raw data in this study were obtained from CrowdTangle for Academics and Researchers, a third-party data provider owned and operated by Facebook. CrowdTangle list IDs are provided in the references. Anyone with a CrowdTangle account may access these lists and the corresponding raw data. Researchers may request CrowdTangle access at https://help.crowdtangle.com/en/articles/4302208-crowdtangle-for-academics-and-researchers. CrowdTangle\u2019s terms of service prohibit providing raw data to anyone outside of a CrowdTangle user\u2019s account. The user can share the findings, but not the data. If a journal asks for data to verify findings, the CrowdTangle user may send a .csv, but it cannot be posted publicly, and the journal must delete it after verification. CSV files containing the raw data used in this study are therefore available upon request subject to these terms.   Methods References 55. CrowdTangle Team. CrowdTangle. (2021). List IDs: 1475046, 1475046, 1584315, 1584316 56. Platform Transparency: Understanding the Impact of Social Media. (2022). 57. Smith, B. A Former Facebook Executive Pushes to Open Social Media\u2019s \u2018Black Boxes\u2019. The New York Times (2022). 58. King, G., Lam, P. & Roberts, M. E. Computer-Assisted Keyword and Document Set Discovery from Unstructured Text. American Journal of Political Science 61, 971\u2013988 (2017). \n59. Dredze, M., Broniatowski, D. A., Smith, M. C. & Hilyard, K. M. Understanding Vaccine Refusal: Why We Need Social Media Now. American Journal of Preventive Medicine (2016) doi:10.1016/j.amepre.2015.10.002. 60. Kata, A. A postmodern Pandora\u2019s box: anti-vaccination misinformation on the Internet. Vaccine 28, 1709\u20131716 (2010). 61. Jamison, A. et al. Adapting and Extending a Typology to Identify Vaccine Misinformation on Twitter. Am J Public Health 110, S331\u2013S339 (2020). 62. Shadish, W. R., Cook, T. D. & Campbell, D. T. Experimental and quasi-experimental designs for generalized causal inference. (Wadsworth Cengage learning, 2002). 63. Larson, H. J. & Broniatowski, D. A. Volatility of vaccine confidence. Science 371, 1289\u20131289 (2021). 64. Facebook. Facebook Tips: What\u2019s the Difference between a Facebook Page and Group? https://www.facebook.com/notes/facebook/facebook-tips-whats-the-difference-between-a-facebook-page-and-group/324706977130/ (2010). 65. Oremus, W., Alcantara, C., Merrill, J. B. & Galocha, A. How Facebook shapes your feed. Washington Post https://www.washingtonpost.com/technology/interactive/2021/how-facebook-algorithm-works/ (2021). 66. Sulleyman, A. Facebook bans one of the anti-vaccine movement\u2019s biggest pages for violating QAnon rules. Newsweek https://www.newsweek.com/facebook-bans-anti-vaccine-group-violating-qanon-rules-1548408 (2020). 67. Keeping People Safe and Informed About the Coronavirus. About Facebook https://about.fb.com/news/2020/12/coronavirus/ (2020). \n68. Reaching Billions of People With COVID-19 Vaccine Information. About Facebook https://about.fb.com/news/2021/02/reaching-billions-of-people-with-covid-19-vaccine-information/ (2021). 69. Smith, T. G. & others. pmdarima: ARIMA estimators for Python. (2017). 70. Johnson, N. F. et al. The online competition between pro- and anti-vaccination views. Nature 582, 230\u2013233 (2020). 71. Fruchterman, T. M. J. & Reingold, E. M. Graph drawing by force-directed placement. Software: Practice and Experience 21, 1129\u20131164 (1991). 72. Ayers, J. W. et al. Spread of Misinformation About Face Masks and COVID-19 by Automated Software on Facebook. JAMA Intern Med (2021) doi:10.1001/jamainternmed.2021.2498. 73. Giglietto, F., Righetti, N., Rossi, L. & Marino, G. It takes a village to manipulate the media: coordinated link sharing behavior during 2018 and 2019 Italian elections. Information, Communication & Society 23, 867\u2013891 (2020). 74. Pacheco, D. et al. Uncovering Coordinated Networks on Social Media: Methods and Case Studies. ICWSM 21, 455\u2013466 (2021). 75. Nizzoli, L., Tardelli, S., Avvenuti, M., Cresci, S. & Tesconi, M. Coordinated Behavior on Social Media in 2019 UK General Election. in ICWSM 443\u2013454 (2021). 76. Weber, D. & Neumann, F. Amplifying influence through coordinated behaviour in social networks. Social Network Analysis and Mining 11, 1\u201342 (2021). 77. Okada, M., Yamanishi, K. & Masuda, N. Long-tailed distributions of inter-event times as mixtures of exponential distributions. Royal Society Open Science 7, 191643. \n78. Danilak, M. M. langdetect: Language detection library ported from Google\u2019s language-detection. 79. Blei, D. M., Ng, A. Y. & Jordan, M. I. Latent dirichlet allocation. the Journal of machine Learning research 3, 993\u20131022 (2003). 80. McCallum, A. K. Mallet: A machine learning for language toolkit. (2002). 81. Wallach, H. M., Mimno, D. M. & McCallum, A. Rethinking LDA: Why priors matter. in Advances in neural information processing systems 1973\u20131981 (2009). 82. Kurkowski, J. john-kurkowski/tldextract. (2020). 83. Cinelli, M. et al. The COVID-19 social media infodemic. Sci Rep 10, 16598 (2020). 84. Singh, L. et al. Understanding high-and low-quality URL Sharing on COVID-19 Twitter streams. Journal of Computational Social Science 1\u201324 (2020). 85. DeVerna, M. R. et al. CoVaxxy: A Collection of English-Language Twitter Posts About COVID-19 Vaccines. in ICWSM 992\u2013999 (2021). 86. Yang, K.-C. et al. The COVID-19 Infodemic: Twitter versus Facebook. Big Data & Society 8, 20539517211013860 (2021). 87. Broniatowski, D. A. et al. Twitter and Facebook posts about COVID-19 are less likely to spread misinformation compared to other health topics. PLoS ONE 17, e0261768 (2022). 88. Grinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B. & Lazer, D. Fake news on Twitter during the 2016 U.S. presidential election. Science 363, 374\u2013378 (2019). 89. Lazer, D. M. et al. The science of fake news. Science 359, 1094\u20131096 (2018). 90. Pennycook, G. & Rand, D. G. Fighting misinformation on social media using crowdsourced judgments of news source quality. Proc Natl Acad Sci USA 116, 2521\u20132526 (2019).  \nAcknowledgments This work was supported in part by the John S. and James L. Knight Foundation through the GW Institute for Data, Democracy, and Politics  Author Contributions Conceptualization: DAB, JG, JRS, LCA Methodology: DAB, JG, AMJ, JRS, LCA,  Investigation: DAB, JG, JRS AMJ Visualization: DAB, JRS Funding acquisition: DAB, LCA Project administration: DAB Supervision: DAB Writing \u2013 original draft: DAB Writing \u2013 review & editing: DAB, JG, AMJ, JRS, LCA   Competing Interests Declaration DAB has received consulting fees from Merck & Co. for participating in the 2021 Merck Global Vaccine Confidence Expert Input Forum, and has received a speaking honorarium from the United Nations Shot@Life Foundation. JG, AMJ, JRS, and LCA declare no competing interests.  Supplementary Information is available for this paper.  \nExtended Data Fig. 1 \n Extended Data Fig. 1. Reactions (e.g., \u201clikes\u201d) were especially insensitive to Facebook\u2019s content removal policies.  ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decline in shares in a. anti-vaccine pages (74% average decrease,  \ud835\udf122(66)= 1666.28, p<0.001) but not b. anti-vaccine groups (84% average increase,  \ud835\udf122(66)= 17.72, p=1.00), c. pro-vaccine pages (202% average increase,  \ud835\udf122(66)= 50.53, p=0.92) or d. pro-vaccine groups (465% average increase,  \ud835\udf122(66)= 76.17, p=0.18). Furthermore, comments declined in e. anti-vaccine pages (49% average decrease,  \ud835\udf122(66)= 838.72, p<0.001) but returned to within the prediction\u2019s 90% confidence intervals for the months of March, May, June, and July-September, 2021. Comments also declined in f. anti-vaccine groups (82% average decrease,  \ud835\udf122(66)= 570.58, p<0.001), although comments in groups in the second sample returned to pre-policy levels, possibly reflecting a transition to newer groups. In contrast, we did not detect a change in comments g. pro-vaccine pages (16% average decrease,  \ud835\udf122(66)= 70.82, p=0.32) \n\nalthough they did decrease in h. pro-vaccine groups (29% average increase,  \ud835\udf122(66)= 120.35, p<0.001). Finally, reactions declined in i. anti-vaccine pages (28% average decrease,  \ud835\udf122(66)= 215.75, p<0.001) but also repeatedly returned to within the prediction\u2019s 90% confidence intervals. In contrast, reactions increased in j. anti-vaccine groups (83% average increase,  \ud835\udf122(66)= 114.09, p<0.001). Reactions also decreased slightly in k. pro-vaccine pages (5% average decrease,  \ud835\udf122(66)= 107.38, p<0.001) and briefly increased in h. pro-vaccine groups (30% average increase,  \ud835\udf122(66)= 174.55, p<0.001). Error bars reflect 90% confidence intervals. Data from the second sample is shown for comparison and qualitatively replicates findings from the first sample.    \nExtended Data Fig. 2 \n Extended Data Fig. 2. Proportions of links between Facebook pages declined in anti-vaccine pages, but repeatedly returned to baseline levels.  ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decline in in a. the proportion of links to in-sample pages anti-vaccine pages in the first sample, consisting of venues identified on Nov. 15, 2020, (57% average decrease,  \ud835\udf122(62)= 114.05, p<0.001) and b. the proportion of engagements with those links (42% average decrease,  \ud835\udf122(60)= 279.38 p<0.001). In contrast, c. we did not detect a change in the proportion of links to pro-vaccine pages from other pro-vaccine pages (27% average decrease,  \ud835\udf122(66)= 17.24, p=1.00) and d. engagements with these links increased significantly (14% average increase,  \ud835\udf122(65)= 459.44, p<0.001). Error bars reflect 90% confidence intervals. Data from the second sample consisting of venues identified on Jul. 21, 2021, is shown for comparison and \n\nqualitatively replicates findings from the first sample. Weeks in which no links to other pages were detected are treated as missing data.   \nExtended Data Fig. 3 \n Extended Data Fig. 3. Weekly proportions of successive URL pairs that were in under 33 seconds, suggesting coordinated behaviour.  ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decline in a. the proportion of coordinated URLs among anti-vaccine venues in the first sample, (64% average decrease,  \ud835\udf122(64)= 321.96, p<0.001), despite a repeated return to within 90% confidence levels from the pre-policy projection. In the second sample, proportions exceeded the pre-policy projection.  In contrast, b. the proportion of coordinated URLs among pro-vaccine venues in the first sample did not differ significantly from pre-policy projections, (63% average decrease,  \ud835\udf122(62)= 30.01, p=1.00). Qualitatively, we observe an increase in pro-vaccine coordinated behaviour, consistent with the emergence of \u201cvaccine hunter\u201d groups that promoted awareness of COVID-19 vaccine availability during the first half of 2021. Error bars \n\nreflect 90% confidence intervals. Data from the second sample consisting of venues identified on Jul. 21, 2021, is shown for comparison and qualitatively replicates findings from the first sample. Weeks in which no links to other pages were detected are treated as missing data.  \nExtended Data Fig. 4 \n Extended Data Fig. 4. Weekly proportions of \u201ctoxic\u201d and \u201cnontoxic\u201d reactions for pro- and anti-vaccine pages and groups.  All proportions are calculated as the percent of all interactions. ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decline in the proportion of \u201ctoxic\u201d reactions and an increase in the proportion of \u201cnon-toxic\u201d reactions in a. anti-vaccine pages: 26% average decrease,  \ud835\udf122(66)= 816.74, p<0.001 and 30% average increase,  \ud835\udf122(66)= 463.39, p<0.001, respectively, and b. anti-vaccine groups: 40% average decrease,  \ud835\udf122(66)= 423.24, p<0.001 and 33% average increase,  \ud835\udf122(66)= 280.21, p<0.001, respectively. In contrast, we observed c. a significant increase \u2013  271% in average \u2013 in the proportion of \u201ctoxic\u201d reactions in pro-vaccine pages, \ud835\udf122(66)= 398.87, p<0.001, but no significant changes in the proportion of \u201cnon-toxic\u201d reactions in pro-vaccine pages \u2013  22% decrease on average \u2013 \ud835\udf122(66)= \n\n73.53, p=0.25, d. \u201ctoxic\u201d reactions in pro-vaccine groups, \u2013  12% decrease on average \u2013 \ud835\udf122(66)= 86.34, p=0.05 and \u201cnon-toxic\u201d reactions in pro-vaccine pages, \u2013  0% decrease on average \u2013 \ud835\udf122(66)= 21.07, p=1.00.    \nExtended Data Fig. 5  Extended Data Fig. 5. Following Facebook\u2019s policies, a topic promoting sensationalised reports of severe adverse events and deaths from COVID-vaccines grew faster than the corresponding pro-vaccine topic.  Facebook\u2019s policy coincided with the worldwide rollout of COVID-vaccines, spurring increased discussion in both anti-vaccine and pro-vaccine venues. We found that content in anti-vaccine pages was, on average, 16% more likely to report misinformative stories alleging that COVID vaccines caused severe adverse reactions and deaths, compared to content in pro-vaccine promoting vaccine uptake, \ud835\udf122(120)= 620.23, p<0.001. This difference was most pronounced between November 18, 2020 through July 4, 2021, during the initial vaccine rollout, when content in anti-vaccine pages was, on average, 88% more likely than corresponding pro-vaccine content. All example messages shown are from this time period.     \n\nExtended Data Fig. 6 \n Extended Data Fig. 6. Weekly proportions of URLs pointing to high-quality academic and government sources for pro- and anti-vaccine pages and groups.  We examined whether Facebook\u2019s policies led to a significant change in the proportion of URLs pointing to high-quality academic and government sources. We found a. a slight, yet significant, increase in anti-vaccine pages: 10% average increase,  \ud835\udf122(66)= 94.44, p=0.01, but b. a large decrease in anti-vaccine groups: 44% average decrease,  \ud835\udf122(64)= 161.72, p<0.001 (one datapoint was dropped due to having zero URLs). c.  We also detected a significant decrease of 29% in pro-vaccine pages, \ud835\udf122(66)= 246.88, p<0.001,  d. Although posts in pro-vaccine groups were significantly more likely \u2013 362% on average compared to pre-policy projections \u2013  to contain these links, \ud835\udf122(55)= 100.06, p<0.001. Although the proportion of high-quality posts in anti-vaccine pages increased slightly, e. we did not detect a significant change in engagement with these posts \ud835\udf122(66)= 72.88, p=0.26. f. Similarly, we did not detect a significant change in engagements in anti-vaccine groups \ud835\udf122(63)= 50.32, p=0.88.  g. Followers of pro-vaccine pages \n\nwere 182% more likely to engage with high-quality sources, \ud835\udf122(66)= 187.26, p<0.001. h. In contrast, engagement rates did not increase in pro-vaccine groups \ud835\udf122(55)= 54.75,  p=0.48. \nExtended Data Fig. 7 \n Extended Data Fig. 7. Weekly average factual rating score of rated URLs for pro- and anti-vaccine pages and groups.  We examined whether Facebook\u2019s policies led to a significant change in the average fact rating scores for rated URLs. Compared to pre-policy projections, a. we did not detect a significant change in the average ratings for posts in anti-vaccine pages, \ud835\udf122(66)= 48.52, p=0.94, or b. anti-vaccine groups, \ud835\udf122(66)= 43.76, p=0.98, although the average factual rating score for anti-vaccine groups was already decreasing prior to November, 18, 2020.  c.  In contrast, we did detect a slight decrease (0.04 points on average) in the average factual accuracy of URLs in pro-vaccine pages, \ud835\udf122(66)= 110.06, p<0.001 d. but not in pro-vaccine groups, \ud835\udf122(66)= 15.16, p=1.00. Despite the lack of significant changes in post ratings e. we found that the average fact rating of user engagements decreased by 0.13 points in anti-vaccine pages, \ud835\udf122(66)= 183.36, p<0.001. f. and by 0.80 points in anti-vaccine groups \ud835\udf122(66)= 130.82, p<0.001.  g. In contrast, we did not detect significant differences in average fact ratings for engagements in pro-vaccine pages, \ud835\udf122(66)= 56,73, p=0.78. h. or pro-vaccine groups \ud835\udf122(66)= 20.43,  p=1.00. \n\nExtended Data Fig. 8 \n Extended Data Fig. 8. Weekly average partisan bias score of rated URLs for pro- and anti-vaccine pages and groups.  We examined whether Facebook\u2019s policies led to a significant change in the average partisan bias scores for rated URLs. Compared to pre-policy projections, a. rated posts in anti-vaccine pages became a weekly average of 0.27 points more biased towards the political right wing, \ud835\udf122(66)= 222.24, p<0.001, and b. rated posts in anti-vaccine groups became a weekly average of 0.24 points more biased towards the political right wing, \ud835\udf122(66)= 106.48, p<0.001.  c.  In contrast, rated posts in pro-vaccine pages became a weekly average 0.13 points more biased towards the political left wing, \ud835\udf122(66)= 95.86, p=0.01 d. We did not detect a change in pro-vaccine groups, \ud835\udf122(66)= 75.52, p=0.17. Concurrently e. we found that the user engagements with posts containing rated links became a weekly average of 0.02 points more biased towards the political right wing, \ud835\udf122(66)= 114.22, p<0.001 in anti-vaccine pages. f. of 0.78 points more biased towards the political right wing, \ud835\udf122(66)= 402.10, p<0.001 in anti-vaccine groups.  g. In \n\ncontrast, we did not detect significant differences in average partisan for engagements in pro-vaccine pages, \ud835\udf122(66)= 42.21, p=0.99. h. or pro-vaccine groups \ud835\udf122(66)= 24.02,  p=1.00.   \nExtended Data Table 1 Extended Data Table 1. Dates and Sample Sizes for Each Dataset Collection from CrowdTangle Collection Date First Post Last Posts N (Pages) N (Groups) Identified November 15, 2020   11/15/20 11/15/19 11/15/20 199,183 264,568 11/30/20 11/18/20 11/30/20 7,619 14,646 3/7/21 11/18/20 3/7/21 58,811 76,443 3/16/21 11/15/19 3/16/21 227,864 243,863 4/8/21 2/8/21 4/8/21 27,739 33,960 5/8/21 2/8/21 5/8/21 40,290 53,390 7/11/21 2/8/21 7/8/21 63,156 80,263 8/9/21 2/8/21 8/8/21 72,059 96,500 9/9/21 8/9/21 9/8/21 12,004 16,284 9/20/21 8/9/21 9/19/21 15,974 23,000 1/31/22 2/8/21 1/31/22 130,439 188,548 2/8/22 2/1/21 2/8/22 136,535 195,755 2/28/22 11/15/19 2/28/22 326,930 367,046 Identified July 21, 2021   8/9/21 2/8/21 8/8/21 92,624 333,748 9/9/21 8/9/21 9/8/21 16,117 36,798 9/20/21 8/9/21 9/19/21 20,945 50,129 1/31/22 2/9/21 1/31/22 156,820 477,832 2/8/22 2/1/21 2/8/22 162,513 491,752 2/28/22 11/15/19 2/28/22 303,165 750,286    \nExtended Data Table 2 Extended Data Table 2. Venue counts for each sample. Category Venues Identified November 15, 2020 Venues Identified July 21, 2021 Alternative medicine 6 0 Civil liberties 43 14 Conspiracy 24 4 Conspiracy & Civil liberties 0 0 Limited info 0 0 Morality issues 2 1 Other 7 0 Safety concerns 32 14 Pro-Vaccine Anti anti-vaccine arguments 24 6 Other 4 4 Pro science 4 3 Pro-policy 3 1 Promotion 56 25 Safe & effective 11 7 Other Foreign language 3 3 General public health 10 7 News sharing 12 12 Non-relevant 1 0 Other 23 4 Pharmacy 1 0 Research 17 11 Unrelated 29 14 Groups Anti-Vaccine   Alternative medicine 1 0 Civil liberties 23 24 Conspiracy 40 18 Conspiracy & Civil liberties 1 0 Limited info 1 0 Morality issues 2 2 Other 7 4 Safety concerns 17 22 Pro-Vaccine Anti anti-vaccine arguments 5 4 Other 1 20 Pro science 0 1 Pro-policy 0 1 Promotion 1 41 Safe & effective 1 3 Unclear Side effects 0 7 Other Foreign language 17 9 General public health 2 0 News sharing 4 1 Non-relevant 0 0 Other 5 0 Pharmacy 0 0 Research 1 0 Unrelated 11 5    \nSupplementary Information 1. Simulation model We constructed a computational simulation to clearly articulate why Facebook\u2019s policies have not permanently reduced engagement with anti-vaccine content beyond pre-policy projections, despite significant reductions in content volume.  Agent Types Our model posits three types of agents:  1) Venues: To initialise our model, we generate a set of \ud835\udc63\t\u2208\ud835\udc49={\ud835\udc63(,\ud835\udc63),...,\ud835\udc63*} venues. In each timestep, each venue generates a random number of posts:  Pv,t ~ NegativeBinomial(av, bv) where Pv has mean \ud835\udf07=\ud835\udc52+! and dispersion parameter bv. This formulation is intended to capture the fact that venues differ greatly in their level of activity, with most venues relatively inactive and a small number of venues generating a large number of posts.  2) Posts: Each post generates a random number of user engagements:  Ep_v ~ Poisson(lv) where lv , the rate parameter, captures the average number of engagements per post in that venue. As above, posts differ greatly from one another in their popularity, with a small number of posts generating a large number of engagements and vice versa. In each timestep, Ep_v  decays at a constant rate, \ud835\udc51\u2208(0,\u221e), capturing the empirical observation that the total number of engagements per week tends to decline over time, perhaps due to the cumulative effects of prior \u201csoft\u201d content remedies.1  3) Content Moderators: We model Facebook\u2019s \u201chard\u201d content removal remedies by randomly assigning an initial number of Facebook staff who serve as content moderators, \n\ud835\udc5a,\u2208\ud835\udc75, to each venue in proportion to the total number of posts generated in that timestep. Each timestep, each content moderator removes one post in the moderator\u2019s assigned venue unless all posts have already been removed. Additionally, each content moderator that has removed a post \u201creproduces\u201d \u2013 i.e., another content moderator is created \u2013 with a fixed probability, \ud835\udc5f\u2208[0,1]. When the number of content moderators exceeds the number of posts in a venue, all content moderators that did not remove a post are randomly reassigned to new venues in proportion to the number of posts that those venues generated.   Potential Demand Our model explicitly captures how removal of a venue or post creates unmet demand for anti-vaccine content. Specifically, we posit that when content or venues are removed, users are prevented from accessing content that they seek. As a consequence, these users seek out new anti-vaccine venues to follow.  Our model contains a variable representing this potential demand at time t, \u210e#\u2208\ud835\udc79-, where h0 = 0. Each time a venue, v,  is removed at time t, the demand for that venue\u2019s content is converted into potential demand. In addition, whenever a post is removed by a moderator, the engagements from that post are also converted into potential demand as follows:  \u210e#=\u210e#.(+B\ud835\udf06/,#/0%\",$+|\ud835\udc43!|\u2211\ud835\udc38&&\u22081\" where wr,t is the set of all venues that have been removed in timestep t, Pr is the set of all posts that have been removed in timestep t, and \ud835\udc38& is the total number of engagements with post p. Finally, at the end of each timestep, a fixed percentage of this potential demand is converted to actual engagements, as follows: \n\u039b/,#=\ud835\udf06/,#.(+\u210e#\u00d7\ud835\udc5d/,#\u00d7\ud835\udc54 where g is a constant between 0 and 1, \ud835\udf06/,# is the average number of engagements per post in venue v at time t, \ud835\udc5d/,# is the normalised total number of posts in venue v at time t such that \u2211\ud835\udc5d//,#=1, and \u039b/,# captures the average number of engagements per post that are no longer susceptible to \u201csoft\u201d content remedies because they come from users who actively seek anti-vaccine content, as has been observed for some content remedies that seek to overtly change peoples\u2019 preferences.2\u20135    Model Initialisation The model is initialised with the following variables: \u2022 m0: The initial number of content moderators \u2022 r: The probability with which content moderators reproduce \u2022 g: The rate at which potential demand is converted into actual engagements per post \u2022 T: a wave of venues removal occurs when the number of content moderators at time t, mt, exceeds T  \u2022 wmin and wmax: when mt > T, wt venues are removed, where wt ~Uniform[wmin, wmax] In addition, the model is calibrated with several parameters: \u2022 d: The decay rate of engagements per post, fit to pre-policy data \u2022 tpolicy: The timestep when \u201chard\u201d content removal remedies begin \u2013 content moderators begin to remove posts.  \u2022  vinitial: Several venues were created between November 15, 2019, and November 15, 2020, and were therefore not actively generating posts or engagements throughout the \nentire time period. At the beginning of the simulation, only vinitial venues are actively generating posts and engagements. \u2022 tmax_venues: The timestep when all venues in the simulation are active. \u2022 vmax: Between the beginning of the simulation and tmax_venues, new venues are added uniformly at random until all vmax venues are active. \u2022 tmax: The total number of timesteps in a model run  Model Execution In each timestep, the model carries out the following steps: 1) The average number of engagements per post in each venue, Ep,v, decays by a fixed percentage, d, i.e., Ep, v, t = d \u00d7Ep, v, t-1 2) Each venue, v, randomly generates Pv,t ~ NegativeBinomial(av, bv) posts 3) For each post, Pv,t, each venue randomly generates Ep_v ~ Poisson(lv) engagements 4) If t < tpolicy: Unless all venues are already active, one inactive venue is selected at random to become active.  5) If t >= tpolicy: a. Each content moderators removes at most one post. Content moderators that removed one post \u201creproduce\u201d with probability r. b. Potential demand is incremented by the total number of engagements per post removed: \t\u210e#\u2217=\u210e#.(+|\ud835\udc43!|\u2211\ud835\udc38&&\u22081\" c. Content moderators that did not remove a post are randomly assigned to new venues with probability \ud835\udc5d/,# where \u2211\ud835\udc5d//,#=1. \nd. If the total number of moderators is greater than the threshold: mt > T i. wt venues are selected for removal, where wt ~Uniform[wmin, wmax]. For each venue that is removed, all of its posts, engagements, and content moderators are also removed. ii. For each venue removed, potential demand is incremented by \ud835\udf06/,#: \u210e#=\u210e#\u2217+B\ud835\udf06/,#/0%\",$ e.  Potential demand is converted to engagements per post at a fixed rate, g:  \u039b/,#=\ud835\udf06/,#.(+\u210e#\u00d7\ud835\udc5d/,#\u00d7\ud835\udc54 The model terminates after tmax timesteps.  Baseline Simulation Results Pages. Based on pre-policy data, we calibrated our model\u2019s parameters for anti-vaccine pages as follows: d=1.8% vinitial=81  tmax_venues=51  vmax=114  av ~Normal(\u00b5=2.03, s=1.20) bv ~LogNormal(\u00b5=-0.79, s=1.19) \ud835\udf06/~(\ud835\udc523*456!7[,,9.;<]\u22121)  We found that our simulation reproduced post-policy data when m0=500, r=10%, g=7.5%, T=5000, wmin=5, and wmax=15 (Fig. S1-1) \nGroups. We also calibrated our model\u2019s parameters to fit pre-policy group data as follows: d=3.0% vinitial=81  tmax_venues=50  vmax=92  av ~Normal(\u00b5=3.06, s=1.36) bv ~LogNormal(\u00b5=-1.10, s=0.96) \ud835\udf06/~LogNormal(\u00b5=1.32, s=1.11) We found that our simulation reproduced post-policy data when m0=1500, r=5%, g=5%, T=5000, wmin=1, and wmax=3 (Fig. S1) We ran our model for tmax=119 timesteps \u2013 i.e., the total number of complete weeks from November 15, 2019 through February 28, 2022. We assume Facebook\u2019s \u201chard\u201d content removal policies occurred during week 53.  \n \n\nFig S1-1. Calibrated simulation model results for pages and groups. Each model was run 500 times. The median number of pages or engagements in each timestep is represented by the solid blue line, and error bars reflect the 5th and 95th percentiles. In addition, projections based on the  ARIMA models fit to pre-policy data are shown by the red dashed line, with error bars reflecting 90% confidence intervals.   Examining Sensitivity of Baseline Results to Changes in Potential Demand The primary source of novelty in our model is our explicit representation of potential demand for misinformation, and the way in which it is converted into engagements. Prior work suggests that a combination of \u201chard\u201d and \u201csoft\u201d content remedies can successfully reduce misinformation sharing online and, indeed, Facebook already implements both types of in its attempts to reduce anti-vaccine content. Given our findings that engagements repeatedly returned to pre-policy baseline levels, our aim was to examine the consequences of reducing the conversion rate of potential demand into actual engagements, g, for both \u201chard\u201d and \u201csoft\u201d remedies. We first explored the consequences of so by a factor of 5 (Fig. S1-2). We found that this policy led to a large reduction in the number of engagements: 54% in pages and 68% in groups. (Table S1-1).  \n Fig S1-2. Results of simulation model when decreasing g, the conversion rate of potential demand to actual engagements, by a factor of 5. Each model was run 500 times. The median number of pages or engagements in each timestep is represented by the solid green line, and error bars reflect the 5th and 95th percentiles. The solid blue line and corresponding error bars reflect the baseline model results. Finally projections based on the ARIMA models fit to pre-policy data are shown by the red dashed line, with error bars reflecting 90% confidence intervals.  Table S1-1. Percent reduction in post and engagement counts at timesteps 60, 90, and 119 (the end of the simulation) compared to model baseline. Variable Pages  Groups Venue Posts  Engagements  Posts  Engagements Timestep 60 90 119  60 90 119  60 90 119  60 90 119  Reducing Potential Demand, g By factor of 2  2% 8% 2%  -5% -13% -25%  -1% -2% 0%  -3% -28% -38% By factor of 5 -1% 4% 6%  0% -30% -54%  -4% -2% -3%  -12% -50% -68% g = 0 0% 18% 6%  -8% -44% -79%  -8% -6% -2%  -6% -64% -89%  \u201cHard\u201d Content Remedies That Change Moderator Behaviour 5 x r -45% -69% -71%  -49% -61% -59%  -56% -77% -65%  -58% -72% -45% T / 5 2% -57% -63%  1% -47% -44%  -40% -70% -73%  -34% -46% -44% \n\n5 x m0 -46% -25% -15%  -52% -24% 10%  -72% -53% -44%  -69% -44% -4% 5 x wmax -2% -1% -28%  -2% 3% -25%  -9% -14% -4%  -3% -13% 15% No moderator limit -4% 14% 25%  8% 15% 42%  -4% 3% 73%  -2% 11% 113%  \"Soft\" Remedies 10% \u201cnudge\u201d 3% 5% 1%  -8% -3% -15%  -3% -1% 5%  -10% -10% -7% 20% \u201cnudge\u201d 4% 5% 0%  -23% -14% -21%  -2% 0% 7%  -20% -25% -14% 40% \u201cnudge\u201d 8% 1% 16%  -35% -38% -30%  -4% -4% -3%  -45% -43% -45% VCB, 100 engagements -2% 11% 15%  -48% -30% -36%  1% -7% -6%  -6% -9% -22% VCB, 500 engagements 1% 4% 4%  -6% 3% 4%  -6% -10% 9%  2% -7% 2% Note. * =p<0.05. **=p<0.01. ***=p<0.001, g: Conversion rate of potential demand to actual engagements per post, r = moderator growth rate, m0 = initial number of content moderators, T = threshold above which a venue removal wave is triggered, wmax =maximum number of venues removed per wave, VCB = \u201cVirality Circuit Breaker\u201d.  Reducing g to zero had an even stronger effect (Table S1-3; 79% in pages and 89% in groups). These findings suggest that existing content remedies employed by Facebook could be significantly strengthened if they were combined with efforts to reduce the conversion of potential demand into actual engagements \u2013 e.g., if early gains due to content removal were not reversed due to users explicitly seeking out anti-vaccine content. \n \n\nFig S1-3. Results of simulation model when decreasing the conversion rate of potential demand to actual engagements to 0. Each model was run 500 times. The median number of pages or engagements in each timestep is represented by the solid green line, and error bars reflect the 5th and 95th percentiles. The solid blue line and corresponding error bars reflect the baseline model results. Finally projections based on the ARIMA models fit to pre-policy data are shown by the red dashed line, with error bars reflecting 90% confidence intervals.    Examining Sensitivity of Baseline Results to Changes in \u201cHard\u201d Content Remedies We next examined whether existing content remedies could be amplified to achieve similar results, and what the costs of doing so might be. Employing more content moderators. We first examined whether an initial \u201cpulse\u201d of additional moderators would be effective (Fig. S1-4). We implemented this in our model by increasing the initial number of moderators, m0, by a factor of 5 (see Table S1-1). We found that this approach was effective in the short term, but that this efficacy decreased over time since the total number of moderators recovered more slowly than demand did, despite the initial removal of several venues.  \n Fig S1-4. Results of simulation model when increasing initial number of content moderators, m0, by a factor of 5. Each model was run 500 times. The median number of pages or engagements in each timestep is represented by the solid green line, and error bars reflect the 5th and 95th percentiles. The solid blue line and corresponding error bars reflect the baseline model results. Finally projections based on the ARIMA models fit to pre-policy data are shown by the red dashed line, with error bars reflecting 90% confidence intervals.   We therefore next examined the consequences of increasing the moderator growth rate, r, by a factor of 5 (Fig. S1-5). We found that this was an effective approach to reduce the total number of engagements with anti-vaccine content, (by 59% in pages and 45% in groups). However, this approach essentially requires maintaining a \u201cstanding army\u201d of moderators that can be quickly deployed to remove content as it appears. In practice, implementing this intervention would be expensive and likely not scalable.  \n\n Fig S1-5. Results of simulation model when increasing the growth rate of content moderators, r, by a factor of 5. Each model was run 500 times. The median number of pages or engagements in each timestep is represented by the solid green line, and error bars reflect the 5th and 95th percentiles. The solid blue line and corresponding error bars reflect the baseline model results. Finally projections based on the ARIMA models fit to pre-policy data are shown by the red dashed line, with error bars reflecting 90% confidence intervals.   Removing more venues. Rather than maintaining a large number of moderators, we next examined the consequences of removing more venues. Specifically, we explored the consequences of removing more venues in each removal wave by increasing wmax by a factor of 5 (Fig. S1-6). We found that this approach reduced engagements in the long-term, but not in the short-term.   \n\nFig S1-6. Results of simulation model when the maximum number venues removed in each wave, wmax, is increased by a factor of 5. Each model was run 500 times. The median number of pages or engagements in each timestep is represented by the solid green line, and error bars reflect the 5th and 95th percentiles. The solid blue line and corresponding error bars reflect the baseline model results. Finally projections based on the ARIMA models fit to pre-policy data are shown by the red dashed line, with error bars reflecting 90% confidence intervals. We therefore next explored the consequences of removing venues more frequently. Since removals occur when the total number of moderators exceeds T, we reduced T by a factor of 5 (Fig. S1-7).  \n\n Fig S1-7. Results of simulation model when reducing content moderators threshold, T, by a factor of 5. Each model was run 500 times. The median number of pages or engagements in each timestep is represented by the solid green line, and error bars reflect the 5th and 95th percentiles. The solid blue line and corresponding error bars reflect the baseline model results. Finally projections based on the ARIMA models fit to pre-policy data are shown by the red dashed line, with error bars reflecting 90% confidence intervals.   We found that this approach also led to a sustained reduction in engagement with anti-vaccine content compared to the baseline; however, this approach is also likely to be expensive since it requires frequently detecting and removing several venues. Furthermore, compared to post removal, venue removal is a blunt instrument that may remove significant amounts of inoffensive content. (We also explored the consequences of increasing T, but found it to be ineffective; Fig. S1-8.)  \n\n Fig S1-8. Results of simulation model when allowing content moderators to grow without removing venues. Each model was run 500 times. The median number of pages or engagements in each timestep is represented by the solid green line, and error bars reflect the 5th and 95th percentiles. The solid blue line and corresponding error bars reflect the baseline model results. Finally projections based on the ARIMA models fit to pre-policy data are shown by the red dashed line, with error bars reflecting 90% confidence intervals.   \u201cSoft\u201d Remedies From Prior Literature For consistency with prior work,6 we also examined the effects of using additional \u201csoft\u201d content remedies to reduce engagement with misinformative posts (e.g., by placing a banner on vaccine-related content1 or by employing nudges,7 both of which have been shown to reduce engagements) leading the weekly number of engagements to decay at a higher rate. We implemented these in our model by multiplying \ud835\udf06/ by a fixed constant, n, between 0 and 1. Facebook already employs banners for anti-vaccine content; however, they do not currently \n\nemploy nudges. Thus, we examined the consequences of setting the value of n to 0.1, 0.2, and 0.4 based upon values used in prior work.6  0.1 is an estimate derived from experimental data, and 0.4 is an optimistic upper bound based upon modelling studies.6 As in prior work, we found that such nudges do indeed reduce engagements (Table S1); however, these results should be interpreted with caution since there is some correlational evidence suggesting that the efficacy of such nudges may decrease over time8 \u2013 a factor that is not captured in our model.  Finally, we examined the effects of employing a \u201cvirality circuit breaker\u201d9 which would prevent viral content from obtaining more than a fixed number of engagements without content moderator approval.  We implemented this in our model by changing the functional form of the probability distribution generating engagements per post. Specifically: Ep_v ~ TruncatedPoisson(lv,k)  where k is a maximum value above which posts are not allowed to garner more engagements. We found that setting k=100  led to a sustained reduction in engagement with anti-vaccine content; however, k=1000 and  k=500 did not (Table S1). In practice, requiring content moderators to review all posts that have garnered 100 engagements would be burdensome; thus we conclude that this may not be a viable strategy to implement at scale.   General Discussion In conclusion, our model suggests that Facebook\u2019s \u201chard\u201d content remedies appear to have been undermined in the long-term by a slow increase in the number of engagements per post in each venue. Our model attributes this increase to the conversion of potential demand to actual engagements \u2013 e.g., when users actively seek out anti-vaccine content after it has been denied to them by prior content and venue removals. Our model\u2019s results show that a sustained decline in \nengagements is possible under the existing paradigm, but that implementing this reduction would require Facebook to make changes. The first possible change would be costly, requiring Facebook to develop a \u201cstanding army\u201d of content moderators that can swiftly be deployed to remove venues on relatively short notice. Alternatively, social media platforms such as Facebook could invest in techniques to reduce potential demand or develop techniques that restrict conversion into actual engagements.      \n 2. Topic Models Table S2-1. Top 5 terms associated with each topic automatically extracted using Latent Dirichlet Allocation topic model Position Category Topic Label Top 5 Terms Anti-Vaccine Alternative Medicines Natural Health vitamin health cancer body natural  Civil Liberties Anti-Mask mask masks wearing wear face   Canada Rallies canada protest freedom covid lockdown   Discussing Vaccine Mandates vaccine covid vaccination vaccinated get   Health Freedom freedom vaccine health medical choice   Informed Consent health medical public science consent   Legislative Action gov state governor bill senator   Lockdowns coronavirus covid health pandemic cdc   Mandates Illegal/Immoral law court rights act government   Opposing School Mandates school bill new children state   Sign Petitions please help information email share   Worldwide Protests world south countries africa germany  Conspiracy Theories \"Question the Lies\" anti vaccine vaccines science truth   Big Pharma vaccine vaccines pharma companies pharmaceutical   Bill Gates gates bill world foundation global   Clinical Trial Conspiracies vaccine covid vaccines trials trial   COVID Not Deadly covid deaths death cases people   Fauci Conspiracies fauci china coronavirus virus wuhan   General Conspiracies one would many even also   Liberty Beacon Podcast twitter follow article tlb liberty   Measles measles vaccine polio gates vaccines   Medical Harm covid patients doctors hospital care   Please Share Truth! share post video please watch   QAnon Conspiracies earth world children child black   Rebel Doctors medical university professor medicine research   RSB Podcast robert show kennedy health del   Social Media Censorship media news facebook social fact   Totalitarian Government people world government fear control   Toxins water food radiation technology health   Unreliable Tests test covid positive tests testing  Morality Issues Foetal Tissues vaccines vaccine dna human cells   Religious Conspiracies god jesus earth shall lord  Safety Concerns Autism autism vaccines children study vaccine \n  Dead Children old year baby son family   Gardasil vaccine hpv adverse gardasil women   Harm to Children vaccine vaccines children flu child   Heavy Metals aluminum mercury vaccines ingredients mcg   Misuse of Science study covid sars cov risk   Personal Stories people get know like going   Severe Reactions & Deaths vaccine covid pfizer died people   Severe Side Effects brain disease symptoms syndrome disorders   Side Effects virus immune immunity disease viruses  Other/Unknown Australian Politics australia government australian lockdown news   Business Impacts home food business new businesses   Community Building james paul david john andrew   Los Angeles police county city arrested california   New Age love life truth world time   Tagalog ang fai philippines mga news   U.S. Politics trump president biden election joe    Watch Episodes live watch truth com www Pro-Vaccine Anti Anti-Vaccine Arguments Anti-Vax Myths anti science people vaccines vaxxers   Debunking misinformation covid anti vaccine media   Measles Outbreaks measles samoa outbreak children years   Mocking Anti-vax like post reply photos antivax   Mocking Natural Health food water organic vitamin natural   Refuting Anti-Vax anti vaccine vaxxers vax memes   Viruses Kill year old died family children  Pro Science & Biomedicine Immunity covid vaccinated people get immunity   MRNA Vaccine vaccines immune mrna system vaccine   Science Promotion would people many one even  Pro Vaccine Policy Vaccine Mandates vaccine law vaccines mandates covid   Vaccine Regulation vaccine covid fda cdc use  Promotion Canadian Promotion timeline photos vaccine canada immunization   Childhood Vaccines children vaccines child kids parents   COVID Vaccine & Booster Promotion vaccine covid dose first doses   Flu Shot Promotion flu shot get influenza vaccine   HPV Vaccine hpv cancer vaccine cervical cancers   International Childhood Vaccination pakistan polio immunization health children   NaijaVax side effects naijavax vaccine shot   Phillipines Vaccine Promotion vaxcen vaccination hotline com today   Pregnancy & Vaccines preg women baby covid vaccine \n  Pro-Vaccine Organizations vaccine health institute join director   Vaccine Clinics covid vaccine clinic nevada flu   Vaccine Equity health world covid vaccines vaccine  Vaccine Campaign Healthcare Workers care patients covid health workers   Personal Stories get people know like one   Stories from Nurses one nurse time day vaxxed  Vaccine Safe and Effective Adverse Event Rates vaccine covid vaccines adverse safety   Clinical Trials vaccine covid vaccines coronavirus trials   Research Results study vaccines autism evidence research   School Mandates bill state school vaccine colorado   Vaccine Efficacy covid variant virus new sars   Vaccines Work vaccine measles vaccines disease polio  Other/Unknown Australian News australia said vaccination new australian   Brazil Rotary Club polio event plus rotary fund   Canadian News covid news cbc canada immunizeusa   Click to Subscribe vaccines covid vaccine questions information   COVID Deaths covid cases deaths number new   COVID in Australia test covid tested nsw testing   COVID Symptoms covid symptoms disease heart risk   Facts & Hesitancy vaccine covid vaccines get public   Indian Polio Campaign polio plus event fund club   Job Recruitment information vaccination help online school   Masks, Handwashing, Distancing mask masks wear covid face   Pandemics coronavirus pandemic covid virus disease   Politics trump twitter president white iphone   Public Health Messaging health state covid department county   Religion & Morality life lives freedom people love   Thank you photos day timeline vaccine happy    UK NHS nhs dorset photos timeline health     \n Table S2-2.  Percent changes in each topic. Position Venue Category Label % Change, Nov. 2020 Sample c2(dof) p Anti-Vaccine Pages Alternative Medicines Natural Health 90% 65.93 (66) 0.479   Civil Liberties Legislative Action 130% 51.25 (66) 0.909    Informed Consent 103% 144.62 (66) 0.000    Opposing School Mandates 283% 72.10 (66) 0.283    Lockdowns 58% 25.21 (66) 1.000    Sign Petitions 110% 62.50 (66) 0.600    Health Freedom 126% 18.93 (66) 1.000    Discussing Vaccine Mandates 138% 75.88 (66) 0.190    Canada Rallies 28% 845.35 (66) 0.000    Mandates Illegal/Immoral 142% 221.11 (66) 0.000    Worldwide Protests 95% 77.56 (66) 0.156    Anti-Mask 202% 3.20 (66) 1.000   Conspiracy Theories Toxins 174% 314.84 (66) 0.000    RSB Podcast 110% 9.67 (66) 1.000    Medical Harm 127% 10.97 (66) 1.000    Fauci Conspiracies 146% 5.48 (66) 1.000    Question the Lies 158% 150.83 (66) 0.000    General Conspiracies 115% 90.77 (66) 0.023    Social Media Censorship 111% 33.39 (66) 1.000    Big Pharma 125% 26.14 (66) 1.000    COVID Not Deadly 203% 8.62 (66) 1.000    Clinical Trial Conspiracies 111% 34.92 (66) 0.999    Bill Gates 45% 57.36 (66) 0.767    Unreliable Tests 17% 116.72 (66) 0.000    Please Share Truth! 108% 88.74 (66) 0.032    Measles 44% 116.51 (66) 0.000    Rebel Doctors 117% 221.03 (66) 0.000    QAnon Conspiracies 70% 212.94 (66) 0.000    Liberty Beacon Podcast 102% 123.02 (66) 0.000    Totalitarian Government 63% 30.24 (66) 1.000   Morality Issues Religious Conspiracies 30% 395.61 (66) 0.000    Foetal Tissues 131% 133.49 (66) 0.000   Other/Unknown Community Building 150% 44.20 (66) 0.982 \n   Tagalog 143% 98.51 (66) 0.006    U.S. Politics 25% 532.31 (66) 0.000    Australian Politics 27% 220.29 (66) 0.000    Watch Episodes 40% 860.36 (66) 0.000    Los Angeles 114% 68.73 (66) 0.385    Business Impacts 94% 36.38 (66) 0.999    New Age 135% 74.18 (66) 0.229   Safety Concerns Side Effects 58% 445.47 (66) 0.000    Severe Side Effects 194% 43.38 (66) 0.986    Personal Stories 99% 21.90 (66) 1.000    Dead Children 162% 11.39 (66) 1.000    Autism 219% 540.90 (66) 0.000    Heavy Metals 75% 8.09 (66) 1.000    Misuse of Science 94% 20.99 (66) 1.000    Gardasil 5270% 35.47 (66) 0.999    Harm to Children 157% 22.60 (66) 1.000    Severe Reactions & Deaths 315% 479.58 (66) 0.000  Groups Alternative Medicines Natural Health 155% 31.53 (66) 1.000   Civil Liberties Legislative Action 260% 131.98 (66) 0.000    Informed Consent 78% 138.05 (66) 0.000    Opposing School Mandates 351% 25.40 (66) 1.000    Lockdowns 43% 80.54 (66) 0.108    Sign Petitions 155% 129.79 (66) 0.000    Health Freedom 135% 10.00 (66) 1.000    Discussing Vaccine Mandates 92% 69.66 (66) 0.355    Canada Rallies 77% 40.31 (66) 0.995    Mandates Illegal/Immoral 73% 360.56 (66) 0.000    Worldwide Protests 90% 18.23 (66) 1.000    Anti-Mask 341% 9.50 (66) 1.000   Conspiracy Theories Toxins 77% 139.74 (66) 0.000    RSB Podcast 66% 28.46 (66) 1.000    Medical Harm 77% 25.48 (66) 1.000    Fauci Conspiracies 92% 3.47 (66) 1.000    Question the Lies 121% 58.65 (66) 0.728    General Conspiracies 101% 80.64 (66) 0.106    Social Media Censorship 86% 35.05 (66) 0.999    Big Pharma 181% 36.72 (66) 0.999    COVID Not Deadly 72% 52.58 (66) 0.885    Clinical Trial Conspiracies 48% 617.99 (66) 0.000 \n   Bill Gates 44% 56.19 (66) 0.800    Unreliable Tests 23% 340.10 (66) 0.000    Please Share Truth! 86% 47.89 (66) 0.954    Measles 45% 124.19 (66) 0.000    Rebel Doctors 127% 121.90 (66) 0.000    QAnon Conspiracies 78% 28.42 (66) 1.000    Liberty Beacon Podcast 103% 197.69 (66) 0.000    Totalitarian Government 54% 62.15 (66) 0.612   Morality Issues Religious Conspiracies 57% 657.64 (66) 0.000    Foetal Tissues 149% 124.55 (66) 0.000   Other/Unknown Community Building 116% 111.73 (66) 0.000    Tagalog 84% 6.55 (66) 1.000    U.S. Politics 13% 609.94 (66) 0.000    Australian Politics 33% 380.64 (66) 0.000    Watch Episodes 110% 35.55 (66) 0.999    Los Angeles 104% 4.76 (66) 1.000    Business Impacts 98% 45.15 (66) 0.977    New Age 94% 17.30 (66) 1.000   Safety Concerns Side Effects 74% 157.62 (66) 0.000    Severe Side Effects 191% 14.13 (66) 1.000    Personal Stories 119% 60.50 (66) 0.668    Dead Children 1394% 19.14 (66) 1.000    Autism 190% 76.84 (66) 0.170    Heavy Metals 324% 10.34 (66) 1.000    Misuse of Science 88% 52.55 (66) 0.885    Gardasil 568% 84.99 (66) 0.058    Harm to Children 999% 16.38 (66) 1.000    Severe Reactions & Deaths 226% 470.96 (66) 0.000 Pro-Vaccine Pages Anti Anti-Vaccine Measles Outbreaks 4028% 107.02 (66) 0.001    Debunking 89% 112.20 (66) 0.000    Anti-Vax Myths 115% 117.79 (66) 0.000    Refuting Anti-Vax 308% 92.96 (66) 0.016    Mocking Anti-vax 106% 77.77 (66) 0.152    Mocking Natural Health 119% 44.40 (66) 0.981    Viruses Kill 170% 66.26 (66) 0.468   Other/Unknown Thank you 117% 64.37 (66) 0.534    Indian Polio Campaign 78% 153.79 (66) 0.000    Facts & Hesitancy 93% 38.80 (66) 0.997    Click to Subscribe 200% 75.32 (66) 0.202    Pandemics 53% 30.89 (66) 1.000 \n   Politics 22% 1280.22 (66) 0.000    COVID in Australia 152% 176.01 (66) 0.000    COVID Deaths 95% 44.20 (66) 0.982    UK NHS 88% 13.83 (66) 1.000    Canadian News 81% 198.58 (66) 0.000    Religion & Morality 90% 27.24 (66) 1.000    Public Health Messaging 85% 9.49 (66) 1.000    Masks, Handwashing, Distancing 53% 6.54 (66) 1.000    Australian News 143% 79.04 (66) 0.130    Job Recruitment 108% 55.34 (66) 0.822    Brazil Rotary Club 102% 73.69 (66) 0.241    COVID Symptoms 52% 684.01 (66) 0.000   Pro Science & Biomedicine Immunity 227% 870.74 (66) 0.000    MRNA Vaccine 129% 171.31 (66) 0.000    Science Promotion 87% 48.17 (66) 0.952   Pro Vaccine Policy Vaccine Mandates 242% 93.92 (66) 0.014    Vaccine Regulation 83% 17.44 (66) 1.000   Promotion Childhood Vaccines 133% 11.16 (66) 1.000    Vaccine Clinics 469% 487.24 (66) 0.000    Pregnancy & Vaccines 177% 92.51 (66) 0.017    COVID Vaccine & Booster Promotion 307% 2450.16 (66) 0.000    Pro-Vaccine Organizations 133% 209.52 (66) 0.000    Vaccine Equity 50% 189.61 (66) 0.000    International Childhood Vaccination 682% 29.15 (66) 1.000    Flu Shot Promotion 416% 4.05 (66) 1.000    Phillipines Vaccine Promotion 20% 122.85 (66) 0.000    Canadian Promotion 125% 46.36 (66) 0.968    NaijaVax 50% 168.45 (66) 0.000    HPV Vaccine 184% 9.12 (66) 1.000   Vaccine Campaign Personal Stories 88% 147.02 (66) 0.000    Healthcare Workers 19% 13.25 (66) 1.000    Stories from Nurses 94% 75.63 (66) 0.195   Vaccine Safe and Effective Adverse Event Rates 251% 156.14 (66) 0.000    Vaccines Work 83% 69.32 (66) 0.366    Vaccine Efficacy 88% 40.61 (66) 0.994    School Mandates 165% 6.16 (66) 1.000    Clinical Trials 27% 266.36 (66) 0.000    Research Results 169% 46.39 (66) 0.968 \n Groups Anti Anti-Vaccine Measles Outbreaks 78% 27.81 (65) 1.000    Debunking 210% 59.43 (65) 0.672    Anti-Vax Myths 82% 48.75 (65) 0.934    Refuting Anti-Vax 417% 211.60 (65) 0.000    Mocking Anti-vax 117% 11.76 (65) 1.000    Mocking Natural Health 107% 99.23 (65) 0.004    Viruses Kill 229% 31.66 (65) 1.000   Other/Unknown Thank you 89% 45.68 (65) 0.967    Indian Polio Campaign 36% 74.70 (65) 0.192    Facts & Hesitancy 163% 41.91 (65) 0.988    Click to Subscribe 184% 88.19 (65) 0.029    Pandemics 153% 28.94 (65) 1.000    Politics 101% 34.53 (65) 0.999    COVID in Australia 171% 62.21 (65) 0.575    COVID Deaths 901% 1017.62 (65) 0.000    UK NHS 253% 39.31 (65) 0.995    Canadian News 119% 62.38 (65) 0.569    Religion & Morality 263% 114.03 (65) 0.000    Public Health Messaging 115% 71.17 (65) 0.280    Masks, Handwashing, Distancing 60% 25.40 (65) 1.000    Australian News 208% 131.80 (65) 0.000    Job Recruitment 88% 81.17 (65) 0.085    Brazil Rotary Club 12% 68.23 (65) 0.368    COVID Symptoms 245% 152.13 (65) 0.000   Pro Science & Biomedicine Immunity 548% 439.51 (65) 0.000    MRNA Vaccine 90% 85.87 (65) 0.043    Science Promotion 209% 91.98 (65) 0.015   Pro Vaccine Policy Vaccine Mandates 149% 77.62 (65) 0.136    Vaccine Regulation 427% 336.62 (65) 0.000   Promotion Childhood Vaccines 236% 72.02 (65) 0.257    Vaccine Clinics 140% 85.04 (65) 0.048    Pregnancy & Vaccines 649% 409.52 (65) 0.000    COVID Vaccine & Booster Promotion 760% 1527.78 (65) 0.000    Pro-Vaccine Organizations 178% 12.40 (65) 1.000    Vaccine Equity 249% 38.55 (65) 0.996    International Childhood Vaccination 653% 136.00 (65) 0.000    Flu Shot Promotion 70% 135.80 (65) 0.000    Phillipines Vaccine Promotion 67% 39.90 (65) 0.994 \n   Canadian Promotion 191% 45.51 (65) 0.968    NaijaVax 883% 355.57 (65) 0.000    HPV Vaccine 95% 76.35 (65) 0.159   Vaccine Campaign Personal Stories 237% 129.40 (65) 0.000    Healthcare Workers 278% 276.01 (65) 0.000    Stories from Nurses 199% 117.00 (65) 0.000   Vaccine Safe and Effective Adverse Event Rates 207% 89.10 (65) 0.025    Vaccines Work 57% 110.42 (65) 0.000    Vaccine Efficacy 334% 145.17 (65) 0.000    School Mandates 1861% 177.66 (65) 0.000    Clinical Trials 48% 60.44 (65) 0.637    Research Results 207% 10.36 (65) 1.000    \n3. Sensitivity Analysis We examined whether our findings were sensitive to our selection of policy date by conducting several additional interrupted time series analyses. The removal of Stop Mandatory Vaccination was notable because it was the first removal of a major anti-vaccine page, but it was also an outgrowth of Facebook\u2019s policy on misinformation associated with the QAnon conspiracy theory.   October 6, 2020 We therefore examined whether our results replicated when the policy date was set to October 6, 2020 \u2013 6 weeks before November 15, 2020 \u2013 when Facebook banned content associated with the QAnon conspiracy theory. We found that our results largely replicated for anti-vaccine content, and for pro-vaccine pages. \n Fig S3-1. Sensitivity analysis examining effects of policy date at October 6, 2020 \n\nARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decline in posts in a. anti-vaccine pages (49% average decrease,  \ud835\udf122(72)= 2760.30 p<0.001) b. anti-vaccine groups (64% average decrease,  \ud835\udf122(72)= 1152.01, p<0.001), and c. pro-vaccine pages (24% average decrease,  \ud835\udf122(72)= 566.35, p<0.001). d. We did not detect a significant difference in pro-vaccine groups (\ud835\udf122(71)= 72.53, p=0.43). Unlike when the policy date was set to November 15, 2020, we did not detect a change in engagements in e. anti-vaccine pages (\ud835\udf122(72)= 62.09, p=0.79). Engagements increased significantly beyond pre-policy predictions in f. anti-vaccine groups (44% average increase, \ud835\udf122(72)= 123.21, p<0.001). Engagements declined with content in g. pro-vaccine pages (12% average decrease, \ud835\udf122(72)= 96.08, p=0.03) and h. in pro-vaccine groups (17% average decrease,  \ud835\udf122(71)= 230.62, p<0.001). Error bars reflect 90% confidence intervals.   August 19, 2020 We next examined whether our results replicated when the policy date was set to August 19, 2020 \u2013 13 weeks before November 15, 2020, and the day Facebook stated that they would remove explicit calls to violence associated with QAnon. We again found that our results largely replicated for anti-vaccine content (although changes to page post counts were not significant due to large error bars), and for pro-vaccine pages.  \n Fig S3-2. Sensitivity analysis examining effects of policy date at August 19, 2020 ARIMA model results show that, compared to projections (black dashed line), we did not detect a significant change in posts in a. anti-vaccine pages (\ud835\udf122(79)= 10.54, p=1.00), b. although we did detect a significant decrease in anti-vaccine group posts (62% average decrease,  \ud835\udf122(79)= 2715.71, p<0.001), and c. pro-vaccine pages (27% average decrease,  \ud835\udf122(79)= 841.03, p<0.001). d. We did not detect a significant difference in pro-vaccine groups (\ud835\udf122(78)= 576.27, p<0.001). Engagements in e. anti-vaccine pages decreased significantly, although they repeatedly returned to pre-policy levels (average decrease of 44%, \ud835\udf122(79)= 140.38, p<0.001). f. In contrast, engagements in groups significantly exceeded pre-policy predictions (64% average increase, \ud835\udf122(79)= 216.53, p<0.001). Engagements declined with content in g. pro-vaccine pages (14% average decrease, \ud835\udf122(79)= 111.01, p=0.01), h. and in pro-vaccine groups (21% average decrease,  \ud835\udf122(78)= 300.34, p<0.001). Error bars reflect 90% confidence intervals.   \n\nDecember 27, 2020 Having established that our primary results replicated for 6 and 13 weeks before the policy, we next examined whether our results replicated when the policy date was set to the last week of 2020: December 27, 2020 \u2013 6 weeks after November 15, 2020, and 25 days after Facebook banned COVID misinformation. We found that our results qualitatively replicated for anti-vaccine all content except anti-vaccine group engagements, where the ARIMA model predicted a recovery that did not occur. \n Fig S3-3. Sensitivity analysis examining effects of policy date at December 27, 2020 ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decline in posts in a. anti-vaccine pages (28% average decrease,  \ud835\udf122(60)= 127.33 p<0.001) b. anti-vaccine groups (62% average decrease,  \ud835\udf122(60)= 861.06, p<0.001), and c. pro-vaccine pages (28% average decrease,  \ud835\udf122(60)= 509.32, p<0.001). d. We did not detect a significant difference in pro-vaccine groups (\ud835\udf122(59)= 45.17, p=0.91). Unlike when the policy \n\ndate was set to November 15, 2020, we did not detect a change in engagements in e. anti-vaccine pages (\ud835\udf122(60)= 17.78, p=1.00). f. In contrast, our model\u2019s predicted a recovery in engagements in anti-vaccine groups where, in practice, none occurred, meaning that engagements decreased significantly beyond pre-policy predictions (72% average decrease, \ud835\udf122(60)= 265.08, p<0.001). Engagements declined with content in g. pro-vaccine pages (15% average decrease, \ud835\udf122(60)= 106.84, p<0.001), h. but increased with content in pro-vaccine groups (51% average increase,  \ud835\udf122(59)= 149.08, p<0.001). Error bars reflect 90% confidence intervals.   February 8, 2021 Finally, we examined whether our results replicated when the policy date was set to February 8, 2021 \u2013 12 weeks after November 15, 2020, and the day Facebook stated that they would remove explicit misinformation about all vaccine.  As when we set the policy date to December 27, 2020, we found that our results qualitatively replicated for anti-vaccine all content except anti-vaccine group engagements, where the ARIMA model predicted a recovery that did not occur.  \n Fig S3-4. Sensitivity analysis examining effects of policy date at February 8, 2021 ARIMA model results show that, compared to projections (black dashed line), a. we did not detect a significant decrease in posts in anti-vaccine pages (16% average decrease,  \ud835\udf122(54)= 70.28, p=0.07) b. although we did observe a significant decrease in posts in anti-vaccine groups (58% average decrease,  \ud835\udf122(54)= 481.54, p<0.001), and c. pro-vaccine pages (31% average decrease,  \ud835\udf122(54)= 545.04, p<0.001). d. We did not detect a significant difference in pro-vaccine groups (\ud835\udf122(53)= 68.26, p=0.08). Unlike when the policy date was set to November 15, 2020, we did not detect a change in engagements in e. anti-vaccine pages (\ud835\udf122(54)= 18.15, p=1.00). f. In contrast, our model\u2019s predicted a recovery in engagements in anti-vaccine groups where, in practice, none occurred, meaning that engagements decreased significantly beyond pre-policy predictions (71% average decrease, \ud835\udf122(54)= 211.08, p<0.001). Engagements declined with content in g. pro-vaccine pages (17% average decrease, \ud835\udf122(54)= 117.65, p<0.001), h. but increased with content in pro-vaccine groups (53% average increase,  \ud835\udf122(53)= 159.47, p<0.001). Error bars reflect 90% confidence intervals.  \n\nThus, our sensitivity analysis demonstrates that our primary results replicate qualitatively across a span of 25 weeks with a median date of roughly November 18, 2020 \u2013 when Facebook first applied its \u201chard\u201d content remedies to vaccines, and bookended by changes in Facebook\u2019s \u201chard\u201d remedies on either side. Although some findings \u2013 e.g., decreases in page posts and engagements \u2013 are no longer statistically significant for the selection of some policy dates, the overall qualitative conclusions remain the same: that Facebook\u2019s remedies removed posts in anti-vaccine pages and/or groups, but were unable to stop page engagements from returning to baseline predictions.  In addition, pro-vaccine page post volumes declined compared to pre-policy predictions for all policy dates. In addition, engagements with posts in anti-vaccine groups appear to have exceeded pre-policy projections for all policy dates prior to and including November 18, 2020, and appear to at least have returned to within the 90% error bars by January, 2022, for all policy dates. Finally, posts in the second sample appear to have returned to pre-policy levels (Fig 1f) as predicted by models with policy dates in December 2020 and February 2021.   Analysis Using Raw Count Data When conducting our ARIMA analyses, we controlled for the formation of new pages in pre-policy data by dividing weekly post and engagement counts by the total number of active venues. We examined whether our results replicated when we used raw counts without this added control. We found that our results did qualitatively replicated. Post-policy projections for posts in Facebook groups grew exponentially beyond what the data indicated because they are fit to a retrospective dataset that includes new groups appearing between November 15, 2019 and November 15, 2020. Although new groups formed after November 15, 2020, they are, by \ndefinition, not included in the dataset identified on this date (although they are included in the dataset identified on July 21, 2021).  \n Fig S3-5. Sensitivity analysis examining effects of fitting ARIMA models to raw count data ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decline in posts in a. anti-vaccine pages (58% average decrease,  \ud835\udf122(66)= 1918.82 p<0.001) b. anti-vaccine groups (74% average decrease,  \ud835\udf122(66)= 2301.31, p<0.001), and c. pro-vaccine pages (14% average decline,  \ud835\udf122(66)= 310.06, p<0.001). We did not detect a significant difference in d. pro-vaccine groups (\ud835\udf122(66)= 55.94, p=0.81). Additionally, engagements declined with content in e. anti-vaccine pages (46% average decrease,  \ud835\udf122(66)= 789.70, p<0.001). Despite this average decline, engagements fell within the prediction\u2019s 90% confidence intervals for the months of November 2020-January 2021, April-May 2021, August \u2013 September, 2021, and November 2021. f. In contrast, we did not detect a significant change in engagements in anti-vaccine groups (\ud835\udf122(66)= 73.93, p=0.86). Engagements declined with content in g. pro-vaccine pages (38% average decrease,  \ud835\udf122(66)= 108.68, p<0.001) but h. \n\nincreased for the months of August \u2013 October 2021 in pro-vaccine groups (59% average increase in the first sample,  \ud835\udf122(66)= 156.61, p<0.001). Error bars reflect 90% confidence intervals. Data from the second sample is shown for comparison and qualitatively replicates findings from the first sample.    \n4. Detecting Coordinated Link Sharing  In order to distinguish between \u201cnear-simultaneous\u201d and organic link sharing, we fit exponential mixture models to interarrival time data derived from three blank searches conducted on CrowdTangle between March 30-March 31, 2021. Using the exp-mixture-model10 Python package, we found that several model goodness-of-fit measures converged on a two-component distribution for each dataset (Table S4-1).    Table S4-1. Model goodness-of-fit statistics for exponential mixture models fit to three blank searches. ki kf MLL JLL AIC BIC AICLVC BICLVC DNML Blank Search 1, 3/30/2021, 9:45 PM EDT (pages) and 9:50 PM EDT (groups) 1 1 -436,534.30 -436,534.30 436,535.30 436,539.95 436,535.30 436,539.95 436,546.98 2 2 -388,698.90 -395,583.58* 388,701.90 388,715.84 395,586.58* 395,599.75* 395,608.20* 3 3 -386,277.58 -406,235.57 386,282.58 386,305.82 406,240.57 406,262.00 406,270.87 4 4 -386,227.76 -417,830.37 386,234.76* 386,267.30* 417,837.37 417,866.70 417,876.73 5 4 -386,227.72 -417,776.17 386,236.72 386,278.56 417,783.17 417,812.55 417,822.57 6 4 -386,229.27 -416,726.67 386,240.27 386,291.40 416,733.67 416,762.91 416,772.00 7 5 -386,228.78 -412,436.74 386,241.78 386,302.21 412,445.74 412,480.35 412,489.51 8 4 -386,227.79 -416,911.45 386,242.79 386,312.52 416,918.45 416,947.78 416,957.80 9 4 -386,228.78 -414,334.79 386,245.78 386,324.80 414,341.79 414,370.91 414,380.93 10 4 -386,227.71* -418,397.53 386,246.71 386,335.03 418,404.53 418,433.98 418,443.92 Blank Search 2, 3/30/2021, 11:25 PM EDT (pages) and 11:18 PM EDT (groups) 1 1 -415,674.53 -415,674.53 415,675.53 415,680.16 415,675.53 415,680.16 415,687.19 2 2 -371,090.88 -377,920.43* 371,093.88 371,107.77 377,923.43* 377,936.54* 377,944.99* 3 3 -368,675.23 -387,924.11 368,680.23 368,703.39 387,929.11 387,950.43 387,959.31 4 3 -368,675.23 -387,109.26 368,682.23 368,714.65 387,114.26 387,135.55 387,145.17 5 4 -368,641.63 -394,604.64 368,650.63* 368,692.30* 394,611.64 394,640.21 394,649.30 6 4 -368,641.59 -399,063.16 368,652.59 368,703.53 399,070.16 399,099.31 399,108.40 7 3 -368,641.21 -386,256.08 368,654.21 368,714.41 386,261.08 386,282.31 386,291.93 8 3 -368,641.74 -386,353.66 368,656.74 368,726.19 386,358.66 386,379.91 386,389.53 9 4 -368,640.73* -401,787.23 368,657.73 368,736.45 401,794.23 401,823.60 401,833.62 10 4 -368,643.01 -401,292.54 368,662.01 368,749.99 401,299.54 401,328.91 401,338.93 Blank Search 2, 3/31/2021, 9:00 AM EDT (pages) and 8:57 AM EDT (groups) 1 1 -438,915.14 -438,915.14 438,916.14 438,920.81 438,916.14 438,920.81 438,927.85 2 2 -396,301.16 -404,433.46* 396,304.16 396,318.18 404,436.46* 404,449.70* 404,458.15* 3 3 -393,948.63 -415,792.96 393,953.63 393,976.98 415,797.96 415,819.49 415,828.36 4 3 -393,932.26 -415,440.74 393,939.26 393,971.96* 415,445.74 415,467.26 415,476.13 \n5 4 -393,921.77 -428,912.47 393,930.77* 393,972.81 428,919.47 428,949.01 428,959.03 6 4 -393,922.01 -426,076.72 393,933.01 393,984.40 426,083.72 426,113.07 426,122.16 7 4 -393,921.69* -419,130.49 393,934.69 393,995.42 419,137.49 419,165.60 419,174.69 8 4 -393,921.75 -425,580.56 393,936.75 394,006.81 425,587.56 425,616.86 425,625.95 9 4 -393,924.26 -425,778.72 393,941.26 394,020.67 425,785.72 425,815.15 425,825.17 10 4 -393,922.80 -427,351.92 393,941.80 394,030.55 427,358.92 427,388.38 427,397.47 Note. ki = Initial number of components; kf = Final number of components; MLL = Mean log-likelihood; JLL = Joint log-likelihood; AIC = Akaike Information Criterion;11 BIC = Bayesian Information Criterion;12 AICLVC = Akaike Information Criterion with Latent Variable Completion;10 BICLVC = Bayesian Information Criterion with Latent Variable Completion;10 DNML = Decomposed Normalized Maximum Likelihood.10 * = best model fit.  Table S4-2. Results of exponential mixture model-fitting procedure, generating two components for each of three datasets.   1  2  \u00b5 p  \u00b5 p 1 10.51 0.70  253.37 0.30 2 9.92 0.70  232.55 0.30 3 9.42 0.69  196.43 0.31 Average 9.95 0.70  227.45 0.30 Note. \u00b5 = mean; p = probability weight. When estimating our threshold, we did not use probability weights since these are unique to each dataset (i.e., different datasets will have different proportions of near-simultaneous and organic link-sharing, and near-simultaneous link sharing may be over-represented in blank search data due to right censoring resulting from CrowdTangle\u2019s download limit).  We next examined the goodness-of-fit of these estimated distributions by comparing them each one to its respective dataset (Figure S4-1), and using Kolmogorov-Smirnov tests. Although each test detected a significant difference (d1 = 0.12, d2 = 0.13, d3 = 0.13, p<0.001 in all cases), our large dataset sizes (n1 = 87,000; n2 = 85,346, n3= 93,075) means that we are powered to detect very small differences. Furthermore, Figure S1 shows that our model underestimates that likelihood of \u201cnear-simultaneous\u201d sharing for very small time differences (between 0 and 15 seconds), meaning that our estimated threshold is conservative.    \n Fig S4-1. Comparison of empirical and theoretical interarrival time cumulative distributions for each of three CrowdTangle blank searches. Probabilities are displayed on a logit axis in order to display differences in the distributions\u2019 tails.   \n\n5. Interactions Over Time We examined how the relative proportions of different types of interactions (shares, comments, reactions) changed over time.  \u201cNon-Toxic\u201d Reactions Likes. We found that the proportion of likes increased over time for anti-vaccine, but not pro-vaccine pages, and groups. \n Fig S5-1. Changes in the proportion of \u201cLikes\u201d over time compared to pre-policy projections ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to an increase in \u201cLikes\u201d a. anti-vaccine pages (14% average increase,  \ud835\udf122(66)= 367.71, p<0.001) b. anti-vaccine groups (16% average increase,  \ud835\udf122(66)= 241.03, p<0.001). In contrast, the proportion of \u201cLikes\u201d decreased significantly in c. pro-vaccine pages (30% average \n\ndecrease,  \ud835\udf122(66)= 118.86, p<0.001). We did not detect a significant difference in d. pro-vaccine groups (\ud835\udf122(66)= 40.15, p=1.00).  Love. We found that the proportion of \u201cLove\u201d reactions increased over time for anti-vaccine content and for pro-vaccine pages, but not pro-vaccine groups. \n Fig S5-2. Changes in the proportion of \u201cLove\u201d reactions over time compared to pre-policy projections ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to an increase in \u201cLove\u201d reactions a. anti-vaccine pages (96% average increase,  \ud835\udf122(66)= 382.98, p<0.001) b. anti-vaccine groups (126% average increase,  \ud835\udf122(66)= 397.37, p<0.001). In addition, the proportion of \u201cLove\u201d reactions increased significantly in c. pro-vaccine pages (100% average increase,  \ud835\udf122(66)= 258.57, p<0.001). We did not detect a significant difference in d. pro-vaccine groups (\ud835\udf122(66)= 37.74, p=1.00).  \n\nSad. We found that the proportion of \u201cSad\u201d reactions increased over time for anti-vaccine content and for pro-vaccine groups, but not pro-vaccine pages. \n Fig S5-3. Changes in the proportion of \u201cSad\u201d reactions over time compared to pre-policy projections ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to an increase in \u201cSad\u201d reactions a. anti-vaccine pages (195% average increase,  \ud835\udf122(66)= 371.09, p<0.001) b. anti-vaccine groups (146% average increase,  \ud835\udf122(66)= 101.82, p<0.001). In contrast, the proportion of \u201cSad\u201d reactions did not change significantly in c. pro-vaccine pages (\ud835\udf122(66)= 42.37, p=0.99), but did increase significantly in d. pro-vaccine groups (1102% average increase,\t\ud835\udf122(66)= 367.23, p<0.001).   \n\nCare. We found that the proportion of \u201cCare\u201d reactions increased over time for anti-vaccine pages. \n Fig S5-4. Changes in the proportion of \u201cCare\u201d reactions over time compared to pre-policy projections ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to an increase in \u201cCare\u201d reactions a. anti-vaccine pages (253% average increase,  \ud835\udf122(66)= 343.74, p<0.001) b. but a significant decrease in anti-vaccine groups (74% average decrease,  \ud835\udf122(66)= 261.19, p<0.001). In addition, the proportion of \u201cCare\u201d reactions increased significantly in c. pro-vaccine pages (68% average increase,  \ud835\udf122(66)= 90.61, p=0.02), but did change significantly in d. pro-vaccine groups (\ud835\udf122(66)= 35.26, p=1.00). All ARIMA models for the \u201cCare\u201d reaction were fit to data from May 3, 2020, forward since this reaction was not widely used before this date.  \n\nTaken together, these results suggest an overall increase in the proportion of \u201cnon-toxic\u201d interactions in anti-vaccine, but not pro-vaccine venues (see Extended Data Fig. 4). \u201cToxic\u201d Reactions Angry. We found that the proportion of \u201cAngry\u201d reactions decreased over time for anti-vaccine, but not pro-vaccine pages. \u201cAngry\u201d reactions decreased for anti-vaccine and pro-vaccine groups. \n Fig S5-5. Changes in the proportion of \u201cAngry\u201d reactions over time compared to pre-policy projections ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to an decrease in \u201cAngry\u201d reactions a. anti-vaccine pages (60% average decrease,  \ud835\udf122(66)= 296.90, p<0.001) b. anti-vaccine groups (58% average decrease,  \ud835\udf122(66)= 1701.99, p<0.001). In contrast, the proportion of \u201cAngry\u201d reactions did not change significantly in c. pro-\n\nvaccine pages (\ud835\udf122(66)= 60.92, p=0.65), although we did detect a significant difference in d. pro-vaccine groups (41% average decrease,\t\ud835\udf122(66)= 232.79, p<0.001).  Haha. We found that the proportion of \u201cHaha\u201d reactions decreased over time for anti-vaccine pages, and pro-vaccine groups. \n Fig S5-6. Changes in the proportion of \u201cHaha\u201d reactions over time compared to pre-policy projections ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decrease in \u201cHaha\u201d reactions in a. anti-vaccine pages (9% average decrease,  \ud835\udf122(66)= 233.88, p<0.001), b. but no change in anti-vaccine groups (\ud835\udf122(66)= 34.58, p=1.00). Similarly, we did not detect a change the proportion of \u201cHaha\u201d reactions in c. pro-vaccine pages (\ud835\udf122(66)= 31.78, p=1.00). We did detect a significant decrease in d. pro-vaccine groups (33% average decrease,  \ud835\udf122(66)= 142.98, p<0.001).  \n\nWow. We found that the proportion of \u201cWow\u201d reactions decreased over time for anti-vaccine pages and increased for pro-vaccine pages. \n Fig S5-7. Changes in the proportion of \u201cSad\u201d reactions over time compared to pre-policy projections ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to an decrease in \u201cWow\u201d reactions a. anti-vaccine pages (8% average decrease,  \ud835\udf122(66)= 362.75, p<0.001) b. but no significant change in anti-vaccine groups (\ud835\udf122(66)= 44.81, p=0.98). In contrast, the proportion of \u201cWow\u201d reactions increased significantly in c. pro-vaccine pages (119% average increase,  \ud835\udf122(66)= 130.83, p<0.001), but did change significantly in d. pro-vaccine groups (\ud835\udf122(66)= 72.39, p=0.28).  Taken together, these results suggest an overall decrease in the proportion of \u201ctoxic\u201d interactions in anti-vaccine, but not pro-vaccine venues (see Extended Data Fig. 4). \n\nOther Interactions Comments. We found that the proportion of comments increased slightly in anti-vaccine pages, and decreased slightly in pro-vaccine venues. \n Fig S5-8. Changes in the proportion of comments over time compared to pre-policy projections ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to an in comments in a. anti-vaccine pages (5% average increase,  \ud835\udf122(66)= 131.29, p<0.001), b. but no change in anti-vaccine groups (\ud835\udf122(66)= 21.37, p=1.00). Additionally, we observed a slight decrease in comments in c. pro-vaccine pages (9% average decrease,  \ud835\udf122(66)= 90.60, p=0.02) and in d. pro-vaccine groups (24% average decrease,  \ud835\udf122(66)= 95.76, p=0.01).  Shares. We found that the proportion of shares decreased over time for anti-vaccine pages and increased for pro-vaccine pages. \n\n Fig S5-9. Changes in the proportion of Shares over time compared to pre-policy projections ARIMA model results show that, compared to projections (black dashed line), Facebook\u2019s policies led to a decrease in Shares a. anti-vaccine pages (21% average decrease,  \ud835\udf122(66)= 260.82, p<0.001) although the proportion of shares repeatedly returned to baseline b. but no significant change in anti-vaccine groups (\ud835\udf122(66)= 16.48, p=1.00). In contrast, the proportion of Shares did not change significantly in c. pro-vaccine pages (\ud835\udf122(66)= 82.92, p=0.08) or in d. pro-vaccine groups (\ud835\udf122(66)= 45.83, p=0.97).  These results are consistent with an overall decrease in supply of anti-vaccine content, but a repeated recovery in demand.   \n\n6. Reaction-Weighted Topic Analysis \n Fig. S6-1. All 50 anti-vaccine topics weighted by their proportion of \u201clikes\u201d in the sample of posts in pages identified on November 15, 2020.  \n\n  Fig. S6-2. All 50 anti-vaccine topics weighted by their proportion of \u201cLove\u201d reactions in the sample of posts in pages identified on November 15, 2020.  \n\n Fig. S6-3. All 50 anti-vaccine topics weighted by their proportion of \u201cSad\u201d reactions in the sample of posts in pages identified on November 15, 2020.   \n\n Fig. S6-4. All 50 anti-vaccine topics weighted by their proportion of \u201cCare\u201d reactions in the sample of posts in pages identified on November 15, 2020.  \n\n Fig. S6-5. All 50 anti-vaccine topics weighted by their proportion of \u201clikes\u201d in the sample of posts in pages identified on July 21, 2021.  \n\n Fig. S6-6. All 50 anti-vaccine topics weighted by their proportion of \u201cLove\u201d reactions in the sample of posts in pages identified on July 21, 2021.   \n\n Fig. S6-7. All 50 anti-vaccine topics weighted by their proportion of \u201cSad\u201d reactions in the sample of posts in pages identified on July 21, 2021.  \n\n Fig. S6-8. All 50 anti-vaccine topics weighted by their proportion of \u201cCare\u201d reactions in the sample of posts in pages identified on July 21, 2021.  \n\n Fig. S6-9. All 50 anti-vaccine topics weighted by their proportion of \u201clikes\u201d in the sample of posts in groups identified on November 15, 2020.  \n\n Fig. S6-10. All 50 anti-vaccine topics weighted by their proportion of \u201cLove\u201d reactions in the sample of posts in groups identified on November 15, 2020.  \n\n Fig. S6-11. All 50 anti-vaccine topics weighted by their proportion of \u201cSad\u201d reactions in the sample of posts in groups identified on November 15, 2020.  \n\n Fig. S6-12. All 50 anti-vaccine topics weighted by their proportion of \u201cCare\u201d reactions in the sample of posts in groups identified on November 15, 2020.  \n\n Fig. S6-13. All 50 anti-vaccine topics weighted by their proportion of \u201clikes\u201d in the sample of posts in groups identified on July 21, 2021.  \n\n Fig. S6-14. All 50 anti-vaccine topics weighted by their proportion of \u201cLove\u201d reactions in the sample of posts in groups identified on July 21, 2021.  \n\n Fig. S6-15. All 50 anti-vaccine topics weighted by their proportion of \u201cSad\u201d reactions in the sample of posts in groups identified on July 21, 2021.  \n\n Fig. S6-16. All 50 anti-vaccine topics weighted by their proportion of \u201cCare\u201d reactions in the sample of posts in groups identified on July 21, 2021.    \n\n7. Venue and Post Removal Analyses Our collection of retrospective data at several points in time allows us to estimate the number and topics of posts that were deleted (Fig. S7-1). \n Fig. S7-1. Area plot showing the number of posts that were last seen in each dataset.  \u201c1st Sample\u201d refers to the venues identified on November 15, 2020. \u201c2nd Sample\u201d refers to the venues identified on July 21, 2021. All plots use a logarithmic axis.   Specifically, in order to determine which types of content were targeted for removal by Facebook, we examined which venues were present in earlier datasets, but not present in later \n\ndatasets (Fig. S7-2). \n Fig. S7-2. Heatmap showing which venues were removed from each type of venue.  Pages presenting as discussing safety concerns and conspiracy theories were removed prior to March, 2021. After March, 2021, venues discussing civil liberties were removed more frequently. Pro-vaccine venues also appear to have been removed following Facebook\u2019s February 8, 2021 policy.  In addition, to examine which posts were removed, we assigned each post a unique ID corresponding to its timestamp, and the venue in which it was published. Posts were considered to have been deleted by Facebook if their venue had not been deleted, and if they were present in earlier, but not later datasets. Using our LDA model, we assigned each post a distribution over topics, and then calculated the proportion of all removed posts for each topic. We then \n\naggregated these proportions for each topic category (Fig S7-3).  \n Fig. S7-3. Heatmap showing which topics were removed from each type of venue.  Posts discussing conspiracy theories and safety concerns were removed prior to April, 2021, in anti-vaccine pages. Posts discussing civil liberties appear to have been removed more frequently after July, 2021. In anti-vaccine groups, removals largely focused on conspiracy theories and civil liberties issues. Finally, in pro-vaccine groups, topics discussing vaccination campaigns appear to have been removed at the same time as posts discussing civil liberties in anti-vaccine pages.  \n\n8. Sentiment Analysis We examined whether Facebook\u2019s new policies changed the overall sentiment of the content posted in the venues in our first sample. To do so, we extracted the text from each Facebook post by combining the  \u201cMessage\u201d, \u201cImage Text\u201d, \u201cLink Text\u201d and \u201cDescription\u201d fields.  After identifying and removing all non-English-language posts using the langdetect Python package,13 we evaluated the sentiment of each post using the DistilBERT model14 fine tuned on the Stanford Sentiment Treebank 2 corpus \u2013 the default sentiment analysis model implemented in the huggingface transformers library (100) on June 9, 2022. This model classified each post as expressing either POSITIVE or NEGATIVE sentiment, with a probability score expressing confidence in this classification ranging from 0.5 (no confidence) to 1.0 (complete confidence). We rescaled the scores for NEGATIVE sentiment posts by subtracting them from 1, such that they ranged from 0.0 (complete confidence) to 0.5 (no confidence). We next calculated weekly sentiment scores for each dataset by averaging across positive and rescaled negative scores. Interpreting these as probabilities, we next conducted interrupted time series analyses on these weekly scores after applying a logit transform to control for floor and ceiling effects (Fig. S8-1). \n  \n Fig. S8-1. Weekly average sentiment scores for pro- and anti-vaccine pages and groups. We examined whether Facebook\u2019s policies led to a significant change in the average weekly sentiment of content posted and engagements with that content. We found a. a slight, yet significant, decrease in anti-vaccine pages: 1% average decrease,  \ud835\udf122(66)= 101.34, p=0.003, but b. no change in anti-vaccine groups: \ud835\udf122(66)= 78.91, p=0.13. c.  We also detected a significant decrease of 6% in pro-vaccine pages, \ud835\udf122(66)= 163.54, p<0.001,  d. and in pro-vaccine groups \u2013 53% on average compared to pre-policy projections , \ud835\udf122(66)= 103.77, p<0.001. e. We did not detect a significant change in engagement with these posts \ud835\udf122(66)= 79.63, p=0.12. f. However, we did detect a significant increase \u2013 26% \u2013 in the sentiment of posts garnering engagements in anti-vaccine groups \ud835\udf122(66)= 162.05, p=0.87.  g. We did not detect a significant change in the \n\nsentiment of engagements with pro-vaccine pages, \ud835\udf122(66)= 23.35, p=1.00. h. In contrast, engagement rates in pro-vaccine groups decreased 37%, \ud835\udf122(66)= 183.49,  p<0.001.    \n9. Toxicity We also examined whether Facebook\u2019s new policies changed the overall toxicity of the content posted in the venues in our first sample. To do so, we extracted the text from each Facebook post by combining the  \u201cMessage\u201d, \u201cImage Text\u201d, \u201cLink Text\u201d and \u201cDescription\u201d fields.  We evaluated the toxicity of each post using Google\u2019s Perspective API15 between June 9 and June 14, 2022, which assigned each post a toxicity score ranging from 0 (non-toxic) to 1 (toxic). We calculated weekly average scores for each dataset. We next conducted interrupted time series analyses on these weekly scores after applying a logit transform to control for floor and ceiling effects.  \n Fig. S9-1. Weekly average toxicity scores for pro- and anti-vaccine pages and groups. We examined whether Facebook\u2019s policies led to a significant change in the average weekly toxicity of content posted and engagements with that content. We found a. a significant decrease \n\nin anti-vaccine pages: 18% average decrease,  \ud835\udf122(66)= 1127.91, p<0.001, but b. no change in anti-vaccine groups: \ud835\udf122(66)= 18.25, p=1.00. c.  We did not detect a significant change in the toxicity of pro-vaccine pages, \ud835\udf122(66)= 24.92, p=1.00,  d. or in pro-vaccine groups, \ud835\udf122(66)= 50.52, p=0.91. e. The toxicity of the average engagement decreased by 16% in anti-vaccine pages \ud835\udf122(66)= 323.91, p<0.001. f. and decreased by 25% in anti-vaccine groups \ud835\udf122(66)= 979.64, p<0.001.  g. We did not detect a significant change in the toxicity of engagements with pro-vaccine pages, \ud835\udf122(66)= 34.77, p=1.00. h. or in pro-vaccine groups, \ud835\udf122(66) = 20.36,  p=1.00. In sum, Facebook\u2019s policies appear to have led to a sustained decline in the toxicity of anti-vaccine, but not pro-vaccine content, and engagement with that content.   \n10. ARIMA Model Tables Table S10-1. Summary statistics for ARIMA models fit throughout this study. Position Venue   ADF p d q Q H JB ADF2 c2 (dof) p Counts (controlling for number of venues) Anti-Vaccine Pages Posts 0.41 0 1 3 1.91 0.56 0.29 0.01 1425.15 (66) <0.001   Engagements 0.19 2 1 1 0.03 0.99 0.19 0.01 468.37 (66) <0.001  Groups Posts 0.43 0 1 4 0.47 0.40 1.67 0.01 1030.71 (66) <0.001   Engagements 0.09 1 1 4 0.09 1.28 0.02 0.03 164.27 (66) <0.001 Pro-Vaccine Pages Posts 0.02 0 0 3 0.08 0.78 0.22 0.01 612.17 (66) <0.001   Engagements 0.04 0 0 2 0.20 3.43* 1.85 0.02 111.43 (66) <0.001  Groups Posts 0.04 1 0 0 0.16 0.70 1.37 0.01 61.15 (66) 0.65   Engagements 0.04 0 0 3 0.01 0.37 7.33* 0.03 164.64 (66) <0.001 Links to In-Sample Pages (Combined Sample) Anti-Vaccine Pages Posts 0.47 1 1 1 0.98 2.02 1.38 0.01 192.43 (66) <0.001   Engagements 0.18 0 1 3 0.89 3.13 9.38* 0.01 234.15 (66) <0.001 Pro-Vaccine Pages Posts 0.56 2 1 0 0.00 1.15 2.19 0.14 200.37 (66) <0.001   Engagements 0.06 1 1 2 0.07 1.80 3.88 0.08 175.70 (66) <0.001 Coordinated Venues (Combined Sample) Anti-Vaccine   0.21 0 1 2 0.04 0.38 32.63*** 0.05 267.00 (66) <0.001 Pro-Vaccine   N/A 2 1 2 0.00 0.53 1.08 0.04 27.93 (66) 1.00 Reactions Anti-Vaccine Pages Toxic 0.21 0 0 1 0.00 1.10 9.84* 0.02 816.74 (66) <0.001   Non-toxic 0.21 3 1 1 0.01 0.97 3.04 0.11 463.39 (66) <0.001 Anti-Vaccine Groups Toxic 0.27 0 1 2 0.01 1.11 0.16 0.01 423.24 (66) <0.001   Non-toxic 0.27 0 1 1 0.62 0.52 1.81 0.04 280.21 (66) <0.001 Pro-Vaccine Pages Toxic 0.3 0 1 3 0.02 1.46 13.39*** 0.03 398.87 (66) <0.001   Non-toxic 0.29 0 1 2 0.39 4.89*** 0.27 0.07 73.53 (66) 0.25 Pro-Vaccine Groups Toxic 0.09 0 1 1 0.16 0.25 137.53*** 0.01 86.34 (66) 0.05   Non-toxic 0.09 0 1 1 0.01 0.21 625.68*** 0.01 21.07 (66) 1.00 Iffy URLs Anti-Vaccine Pages Posts 0.01 1 0 0 0.01 1.10 1.26 0.01 934.55 (66) <0.001   Engagements 0.36 0 1 2 0.03 0.92 2.22 0.03 247.64 (66) <0.001  Groups Posts 0.68 1 1 0 0.03 1.04 1.09 0.01 24.43 (66) 1.00   Engagements 0.55 0 1 1 0.23 2.43 0.41 0.01 70.96 (66) 0.32 Government and Academic URLs Anti-Vaccine Pages Posts 0.07 0 1 1 1.54 0.41 0.68 0.01 94.44 (66) 0.01   Engagements 0.49 0 1 1 0.16 1.50 0.56 0.01 72.88 (66) 0.26  Groups Posts 0.04 1 0 0 0.35 2.45 2.32 0.02 161.72 (64) <0.001   Engagements 0.46 0 1 1 0.04 1.09 3.58 0.08 50.32 (63) 0.88 Pro-Vaccine Pages Posts 0.17 1 1 3 0.01 0.37 1.55 0.01 246.88 (66) <0.001   Engagements 0.02 1 0 0 0.26 1.53 0.47 0.02 187.26 (66) <0.001  Groups Posts N/A 0 1 2 0.27 0.94 1.51 0.07 100.09 (55) <0.001   Engagements N/A 0 1 5 0.31 0.44 5.66 0.03 54.75 (55) 0.48 Average Fact Rating Anti-Vaccine Pages Posts 0.19 0 1 4 0.09 0.50 0.82 0.01 48.52 (66) 0.94   Engagements 0.02 3 0 0 0.02 1.61 1.54 0.08 183.36 (66) <0.001  Groups Posts 0.5 0 1 1 0.33 0.55 5.14 0.04 43.76 (66) 0.98   Engagements 0.49 0 1 1 0.04 0.71 0.27 0.05 130.82 (66) <0.001 Pro-Vaccine Pages Posts 0.01 2 0 0 0.04 0.64 3.90 0.01 110.06 (66) <0.001   Engagements 0.2 0 1 3 0.09 0.86 7.88* 0.21 56.73 (66) 0.78  Groups Posts 0.65 1 1 1 0.43 0.89 0.92 0.17 15.16 (66) 1.00   Engagements 0.29 3 1 0 0.04 0.93 0.35 0.05 20.43 (66) 1.00 Average Bias Rating Anti-Vaccine Pages Posts 0.01 1 0 0 0.15 0.69 1.31 0.01 222.24 (66) <0.001 \n  Engagements 0.01 2 0 2 0.15 0.73 4.31 0.04 114.21 (66) <0.001  Groups Posts 0.08 0 1 1 0.48 0.68 1.22 0.01 106.48 (66) <0.001   Engagements 0.23 0 1 2 0.19 0.81 1.14 0.10 402.15 (66) <0.001 Pro-Vaccine Pages Posts 0.07 1 1 3 0.04 0.79 3.92 0.01 95.86 (66) 0.01   Engagements 0.02 0 0 0 0.00 0.60 8.47* 0.02 42.21 (66) 0.99  Groups Posts 0.2 1 1 2 0.03 0.17 15.13** 0.28 75.52 (66) 0.17   Engagements 0.01 2 0 0 0.01 0.17 86.71*** 0.02 24.02 (66) 1.00 Note. ADF1 = p-value of Augmented Dickey-Fuller test fit to raw data; ADF2 = p-value of Augmented Dickey-Fuller test fit to ARIMA model residuals; p=autoregressive order; N/A = not available because of missing data; d = differencing order; q = moving average order; Q = Ljung-Box statistic; H = Heteroskedasticity; JB = Jarque-Bera statistic; * = p<0.05, **=p<0.01, ***=p<0.001.  Table S10-2:ARIMA models fit throughout this study Anti-Vaccine Page Posts: ARIMA(0,1,3), Q=1.91, p=0.17; H=0.56, p=0.24, JB=0.29, p=0.87  coef std err z P>|z| [0.025 0.975] intercept 0.00 0.00 -1.20 0.23 -0.01 0.00 ma.L1 0.09 0.18 0.51 0.61 -0.26 0.44 ma.L2 -0.42 0.22 -1.94 0.05 -0.85 0.00 ma.L3 -0.53 0.17 -3.05 0.00 -0.88 -0.19 s2 0.01 0.00 3.53 0.00 0.00 0.01 Anti-Vaccine Page Engagements: ARIMA(2,1,1), Q=0.03, p=0.85; H=0.88, p=0.79, JB=0.19, p=0.91 intercept -0.01 0.00 -1.94 0.05 -0.01 0.00 ar.L1 0.45 0.15 2.92 0.00 0.15 0.75 ar.L2 0.03 0.14 0.22 0.83 -0.24 0.30 ma.L1 -0.99 0.65 -1.51 0.13 -2.27 0.29 s2 0.03 0.02 1.69 0.09 -0.01 0.07 Anti-Vaccine Group Posts: ARIMA(0,1,4), Q=0.47, p=0.49; H=0.40, p=0.07, JB=1.67, p=0.43 intercept 0.00 0.01 0.25 0.80 -0.02 0.02 ma.L1 -0.02 0.18 -0.14 0.89 -0.37 0.32 ma.L2 -0.05 0.20 -0.27 0.79 -0.44 0.33 ma.L3 -0.45 0.19 -2.35 0.02 -0.82 -0.07 ma.L4 -0.30 0.14 -2.20 0.03 -0.58 -0.03 s2 0.02 0.01 3.74 0.00 0.01 0.04 Anti-Vaccine Group Engagements: ARIMA(1,1,4), Q=0.09, p=0.77; H=1.28, p=0.62, JB=0.02, p=0.99 intercept -0.06 0.02 -3.83 0.00 -0.10 -0.03 ar.L1 -0.53 0.29 -1.85 0.06 -1.09 0.03 ma.L1 0.43 0.55 0.77 0.44 -0.66 1.51 ma.L2 -0.05 0.73 -0.07 0.94 -1.49 1.38 ma.L3 -0.58 0.69 -0.85 0.40 -1.92 0.76 ma.L4 -0.74 0.42 -1.77 0.08 -1.55 0.08 s2 0.04 0.02 2.04 0.04 0.00 0.08 Pro-Vaccine Page Posts: ARIMA(0,0,3), Q=0.08, p=0.77; H=0.78, p=0.62, JB=0.22, p=0.90 intercept 1.95 0.03 57.16 0.00 1.89 2.02 ma.L1 0.76 0.16 4.77 0.00 0.45 1.07 \nma.L2 0.35 0.15 2.29 0.02 0.05 0.64 ma.L3 0.40 0.17 2.38 0.02 0.07 0.73 s2 0.01 0.00 5.16 0.00 0.01 0.01 Pro-Vaccine Page Engagements: ARIMA(0,0,2), Q=0.20, p=0.66; H=3.43, p=0.02, JB=1.85, p=0.40 intercept 7.69 0.09 87.78 0.00 7.52 7.86 ma.L1 0.04 0.15 0.29 0.77 -0.25 0.33 ma.L2 0.52 0.12 4.38 0.00 0.29 0.75 s2 0.15 0.03 4.70 0.00 0.09 0.21 Pro-Vaccine Group Posts: ARIMA(1,0,0), Q=0.16, p=0.69; H=0.70, p=0.47, JB=1.37, p=0.50 intercept 0.77 0.23 3.30 0.00 0.32 1.23 ar.L1 0.69 0.10 7.15 0.00 0.50 0.88 s2 0.10 0.02 4.71 0.00 0.06 0.14 Pro-Vaccine Group Engagements: ARIMA(0,0,3), Q=0.01, p=0.90; H=0.37, p=0.05, JB=7.33, p=0.03 intercept 6.16 0.08 82.15 0.00 6.01 6.31 ma.L1 0.25 0.19 1.32 0.19 -0.12 0.63 ma.L2 0.26 0.21 1.29 0.20 -0.14 0.66 ma.L3 -0.25 0.15 -1.67 0.10 -0.55 0.04 s2 0.15 0.03 5.82 0.00 0.10 0.20 Anti-Vaccine Page Links: ARIMA(1,1,1), Q=0.00, p=0.98; H=2.02, p=0.16, JB=1.38, p=0.50 intercept -0.01 0.01 -0.55 0.58 -0.03 0.02 ar.L1 0.32 0.23 1.44 0.15 -0.12 0.77 ma.L1 -0.82 0.15 -5.35 0.00 -1.12 -0.52 s2 0.14 0.03 4.60 0.00 0.08 0.21 Pro-Vaccine Page Links: ARIMA(2,1,0), Q=0.00, p=1.00; H=1.15, p=0.78, JB=2.19, p=0.34 intercept -0.02 0.06 -0.36 0.72 -0.14 0.10 ar.L1 -0.27 0.16 -1.68 0.09 -0.59 0.04 ar.L2 -0.39 0.11 -3.53 0.00 -0.61 -0.18 s2 0.15 0.04 3.95 0.00 0.08 0.23 Anti-Vaccine Coordinated Links: ARIMA(0,1,2), Q=0.04, p=0.83; H=0.38, p=0.05, JB=32.63, p<0.001 intercept 0.01 0.02 0.34 0.73 -0.03 0.04 ma.L1 -0.26 0.12 -2.18 0.03 -0.49 -0.03 ma.L2 -0.49 0.11 -4.65 0.00 -0.70 -0.29 s2 0.09 0.01 7.56 0.00 0.07 0.12 Pro-Vaccine Coordinated Links: ARIMA(2,1,2), Q=0.00, p=1.00; H=0.53, p=0.33, JB=1.08, p=0.58 ntercept 0.03 0.11 0.24 0.81 -0.20 0.25 ar.L1 0.11 0.20 0.54 0.59 -0.29 0.51 ar.L2 -0.74 0.20 -3.65 0.00 -1.14 -0.34 ma.L1 -0.80 0.27 -2.97 0.00 -1.33 -0.27 ma.L2 0.68 0.27 2.49 0.01 0.14 1.21 s2 0.35 0.12 3.01 0.00 0.12 0.58 \nAnti-Vaccine Page \"Toxic\" Reactions: ARIMA(0,0,1), Q=0.00, p=0.99; H=1.10, p=0.84, JB=9.84, p=0.01 intercept -2.03 0.04 -52.36 0.00 -2.11 -1.96 ma.L1 0.26 0.16 1.65 0.10 -0.05 0.57 s2 0.04 0.01 5.46 0.00 0.02 0.05 Anti-Vaccine Page \"Nontoxic\" Reactions: ARIMA(3,1,1), Q=0.01, p=0.93; H=0.97, p=0.96, JB=3.04, p=0.22 intercept -0.01 0.01 -0.58 0.56 -0.02 0.01 ar.L1 -0.36 0.36 -0.99 0.33 -1.06 0.35 ar.L2 -0.46 0.27 -1.75 0.08 -0.98 0.06 ar.L3 -0.19 0.26 -0.72 0.48 -0.70 0.33 ma.L1 -0.59 0.31 -1.91 0.06 -1.20 0.02 s2 0.02 0.00 4.90 0.00 0.01 0.03 Anti-Vaccine Group \"Toxic\" Reactions: ARIMA(0,1,2), Q=0.01, p=0.94; H=1.11, p=0.83, JB=0.16, p=0.93 intercept 0.00 0.01 0.38 0.70 -0.01 0.01 ma.L1 -0.58 0.17 -3.30 0.00 -0.92 -0.23 ma.L2 -0.26 0.17 -1.49 0.14 -0.59 0.08 s2 0.03 0.01 5.25 0.00 0.02 0.05 Anti-Vaccine Group \"Nontoxic\" Reactions: ARIMA(0,1,1), Q=0.62, p=0.43; H=0.52, p=0.19, JB=1.81, p=0.41 intercept 0.00 0.01 -0.38 0.70 -0.02 0.01 ma.L1 -0.73 0.12 -6.30 0.00 -0.95 -0.50 s2 0.02 0.01 4.09 0.00 0.01 0.03 Pro-Vaccine Page \"Toxic\" Reactions: ARIMA(0,1,1), Q=0.16, p=0.69; H=0.25, p=0.01, JB=137.53, p<0.001 intercept -0.02 0.02 -0.72 0.48 -0.06 0.03 ma.L1 -0.81 0.10 -7.84 0.00 -1.01 -0.61 s2 0.30 0.04 7.62 0.00 0.22 0.37 Pro-Vaccine Page \"Nontoxic\" Reactions: ARIMA(0,1,1), Q=0.09, p=0.77; H=0.21, p=0.00, JB=625.67, p<0.001 intercept 0.00 0.03 0.07 0.95 -0.06 0.06 ma.L1 -0.75 0.12 -6.18 0.00 -0.99 -0.51 s2 0.26 0.03 9.13 0.00 0.20 0.31 Anti-Vaccine Page \"Iffy\": ARIMA(1,0,0), Q=0.01, p=0.92; H=1.10, p=0.84, JB=1.26, p=0.53 intercept -1.14 0.30 -3.78 0.00 -1.73 -0.55 ar.L1 0.49 0.13 3.70 0.00 0.23 0.76 s2 0.01 0.00 3.91 0.00 0.01 0.02 \nAnti-Vaccine \"Iffy Engagements: ARIMA(0,1,2), Q=0.03, p=0.95; H=0.92, p=0.87, JB=2.22, p=0.33 intercept -0.01 0.02 -0.84 0.40 -0.04 0.02 ma.L1 -0.70 0.17 -4.22 0.00 -1.02 -0.37 ma.L2 -0.01 0.14 -0.08 0.94 -0.28 0.26 s2 0.11 0.03 3.98 0.00 0.05 0.16 Anti-Vaccine Group \"Iffy\" Engagements: ARIMA(0,1,1), Q=0.23, p=0.63; H=2.43, p=0.08, JB=0.41, p=0.81 intercept -0.01 0.01 -0.58 0.56 -0.03 0.02 ma.L1 -0.76 0.09 -8.51 0.00 -0.93 -0.58 s2 0.10 0.02 4.49 0.00 0.06 0.15 Anti-Vaccine Page HQHS: ARIMA(0,1,1), Q=1.54, p=0.22; H=0.41, p=0.08, JB=0.68, p=0.71 intercept 0.00 0.01 -0.41 0.68 -0.02 0.02 ma.L1 -0.80 0.08 -9.83 0.00 -0.97 -0.64 s2 0.08 0.02 4.29 0.00 0.05 0.12 Anti-Vaccine HQHS Engagements: ARIMA(0,1,1), Q=0.16, p=0.69; H=1.50, p=0.41, JB=0.56, p=0.76 intercept -0.02 0.03 -0.53 0.59 -0.07 0.04 ma.L1 -0.64 0.11 -5.70 0.00 -0.85 -0.42 s2 0.33 0.08 4.43 0.00 0.19 0.48 Anti-Vaccine Group HQHS: ARIMA(1,0,0), Q=0.35, p=0.56; H=2.45, p=0.06, JB=2.32, p=0.31 intercept -1.64 0.49 -3.37 0.00 -2.59 -0.68 ar.L1 0.61 0.11 5.49 0.00 0.39 0.83 s2 0.14 0.03 5.61 0.00 0.09 0.19 Anti-Vaccine Group HQHS Engagements: ARIMA(0,1,1), Q=0.04, p=0.85; H=1.09, p=0.86, JB=3.58, p=0.17 intercept -0.02 0.02 -1.04 0.30 -0.07 0.02 ma.L1 -0.73 0.12 -6.16 0.00 -0.97 -0.50 s2 0.25 0.05 4.75 0.00 0.15 0.36 Pro-Vaccine Page HQHS: ARIMA(1,1,3), Q=0.01, p=0.94; H=0.37, p=0.05, JB=1.55, p=0.46 intercept 0.01 0.01 0.57 0.57 -0.02 0.03 ar.L1 -0.76 0.25 -3.06 0.00 -1.25 -0.27 ma.L1 0.18 0.25 0.72 0.47 -0.31 0.67 ma.L2 -0.58 0.29 -1.99 0.05 -1.14 -0.01 ma.L3 -0.43 0.15 -2.79 0.01 -0.73 -0.13 s2 0.07 0.01 5.23 0.00 0.04 0.09 \nPro-Vaccine Page HQHS Engagements: ARIMA(1,0,0), Q=0.26, p=0.61; H=1.53, p=0.38, JB=0.47, p=0.79 intercept -3.86 0.99 -3.90 0.00 -5.80 -1.92 ar.L1 0.36 0.16 2.31 0.02 0.06 0.67 s2 1.22 0.23 5.24 0.00 0.76 1.68 Pro-Vaccine Group HQHS: ARIMA(0,1,2), Q=0.27, p=0.61; H=0.94, p=0.92, JB=1.51, p=0.47 intercept -0.02 0.03 -0.88 0.38 -0.07 0.03 ma.L1 -0.76 0.16 -4.63 0.00 -1.08 -0.44 ma.L2 -0.02 0.18 -0.14 0.89 -0.38 0.33 s2 0.38 0.11 3.33 0.00 0.16 0.60 Pro-Vaccine Group HQHS Engagements: ARIMA(0,1,5), Q=0.31, p=0.58; H=0.44, p=0.17, JB=5,66, p=0.06 intercept -0.01 0.05 -0.14 0.89 -0.10 0.09 ma.L1 -0.47 0.30 -1.58 0.11 -1.06 0.11 ma.L2 -0.17 0.24 -0.72 0.47 -0.64 0.30 ma.L3 -0.43 0.23 -1.88 0.06 -0.87 0.02 ma.L4 0.59 0.27 2.16 0.03 0.05 1.12 ma.L5 -0.37 0.31 -1.18 0.24 -0.97 0.24 s2 0.86 0.24 3.58 0.00 0.39 1.33 Anti-Vaccine Page Fact Ratings: ARIMA(0,1,4), Q=0.09, p=0.77; H=0.50, p=0.17, JB=0.82, p=0.67 intercept 0.00 0.01 -0.19 0.85 -0.01 0.01 ma.L1 -0.45 0.17 -2.72 0.01 -0.77 -0.12 ma.L2 -0.55 0.14 -4.06 0.00 -0.81 -0.28 ma.L3 -0.19 0.17 -1.18 0.24 -0.52 0.13 ma.L4 0.53 0.15 3.48 0.00 0.23 0.83 s2 0.02 0.01 3.52 0.00 0.01 0.03 Anti-Vaccine Fact Rating Engagements: ARIMA(0,0,3), Q=0.00, p=0.96; H=0.83, p=0.70, JB=12.53, p=0.00 intercept 2.52 0.03 84.99 0.00 2.46 2.58 ma.L1 0.17 0.15 1.12 0.26 -0.13 0.46 ma.L2 -0.12 0.15 -0.84 0.40 -0.41 0.16 ma.L3 -0.41 0.17 -2.47 0.01 -0.73 -0.08 s2 0.07 0.01 5.16 0.00 0.04 0.10 Anti-Vaccine Group Fact Ratings: ARIMA(0,1,1), Q=0.33, p=0.56; H=0.55, p=0.23, JB=5.14, p=0.08 intercept -0.01 0.01 -0.77 0.44 -0.03 0.01 ma.L1 -0.50 0.13 -3.75 0.00 -0.76 -0.24 s2 0.02 0.00 5.92 0.00 0.01 0.02 Anti-Vaccine Group Fact Rating Engagements: ARIMA(0,0,3), Q=0.00, p=0.96; H=0.83, p=0.70, JB=12.53, p=0.00 \nintercept 2.52 0.03 84.99 0.00 2.46 2.58 ma.L1 0.17 0.15 1.12 0.26 -0.13 0.46 ma.L2 -0.12 0.15 -0.84 0.40 -0.41 0.16 ma.L3 -0.41 0.17 -2.47 0.01 -0.73 -0.08 s2 0.07 0.01 5.16 0.00 0.04 0.10 Pro-Vaccine Page Fact Ratings: ARIMA(2,0,0), Q=0.04, p=0.85; H=0.64, p=0.36, JB=3.90, p=0.14 intercept 2.16 0.54 4.00 0.00 1.10 3.22 ar.L1 0.17 0.17 1.01 0.31 -0.16 0.50 ar.L2 0.24 0.20 1.19 0.24 -0.15 0.63 s2 0.01 0.00 4.42 0.00 0.01 0.01 Pro-Vaccine Page Fact Rating Engagements: ARIMA(0,1,3), Q=0.09, p=0.77; H=0.86, p=0.75, JB=7.88, p=0.02 intercept 0.00 0.01 -0.13 0.90 -0.01 0.01 ma.L1 -0.91 0.20 -4.52 0.00 -1.31 -0.52 ma.L2 -0.06 0.27 -0.23 0.82 -0.58 0.46 ma.L3 0.02 0.15 0.13 0.90 -0.27 0.30 s2 0.08 0.02 4.47 0.00 0.04 0.11 Pro-Vaccine Group Fact Ratings: ARIMA(1,1,1), Q=0.43, p=0;51; H=0.89, p=0.81, JB=0.92, p=0.63 intercept 0.00 0.03 0.02 0.99 -0.06 0.06 ar.L1 -0.30 0.22 -1.41 0.16 -0.73 0.12 ma.L1 -0.58 0.19 -3.06 0.00 -0.95 -0.21 s2 0.20 0.05 4.27 0.00 0.11 0.29 Pro-Vaccine Group Fact Rating Engagements: ARIMA(3,1,0), Q=0.04, p=0.85; H=0.93, p=0.87, JB=0.35, p=0.84 intercept 0.01 0.08 0.18 0.86 -0.14 0.16 ar.L1 -0.78 0.12 -6.30 0.00 -1.02 -0.54 ar.L2 -0.60 0.16 -3.76 0.00 -0.91 -0.29 ar.L3 -0.56 0.13 -4.42 0.00 -0.81 -0.31 s2 0.25 0.06 4.49 0.00 0.14 0.36 Anti-Vaccine Page Bias Ratings: ARIMA(1,0,0), Q=0.15, p=0.70; H=0.69, p=0.44, JB=1.31, p=0.52 intercept 0.31 0.08 3.83 0.00 0.15 0.46 ar.L1 0.50 0.13 3.86 0.00 0.25 0.75 s2 0.03 0.01 4.04 0.00 0.01 0.04 Anti-Vaccine Bias Rating Engagements: ARIMA(2,0,2), Q=0.15, p=0.70; H=0.73, p=0.52, JB=4.31, p=0.12 intercept 0.40 0.10 4.25 0.00 0.22 0.59 ar.L1 0.57 0.20 2.78 0.01 0.17 0.97 ar.L2 -0.64 0.18 -3.49 0.00 -1.00 -0.28 ma.L1 -0.25 0.16 -1.53 0.13 -0.56 0.07 ma.L2 0.87 0.14 6.34 0.00 0.60 1.14 s2 0.09 0.02 5.28 0.00 0.06 0.13 Anti-Vaccine Group Bias Ratings: ARIMA(0,1,1), Q=0.48, p=0.49; H=0.68, p=0.44, JB=1.22, p=0.54 intercept 0.02 0.01 2.75 0.01 0.01 0.03 \nma.L1 -0.82 0.10 -8.24 0.00 -1.01 -0.62 s2 0.04 0.01 4.00 0.00 0.02 0.07 Anti-Vaccine Group Bias Rating Engagements: ARIMA(0,1,2), Q=0.19, p=0.67; H=0.81, p=0.67, JB=1.14, p=0.57 intercept 0.01 0.01 1.16 0.25 0.00 0.02 ma.L1 -0.93 0.15 -6.43 0.00 -1.21 -0.65 ma.L2 -0.01 0.14 -0.07 0.95 -0.29 0.27 s2 0.11 0.03 4.43 0.00 0.06 0.16 Pro-Vaccine Page Fact Ratings: ARIMA(1,1,3), Q=0.04, p=0.84; H=0.79, p=0.64, JB=3.92, p=0.14 intercept 0.00 0.00 -0.13 0.90 -0.01 0.01 ar.L1 -0.59 0.28 -2.17 0.03 -1.13 -0.06 ma.L1 -0.15 0.37 -0.40 0.69 -0.88 0.58 ma.L2 -0.37 0.33 -1.15 0.25 -1.01 0.26 ma.L3 -0.44 0.24 -1.82 0.07 -0.92 0.04 s2 0.02 0.00 4.26 0.00 0.01 0.03 Pro-Vaccine Page Bias Rating Engagements: ARIMA(0,0,0), Q=0.00, p=0.97; H=0.60, p=0.29, JB=8.47, p=0.01 intercept -0.73 0.05 -15.07 0.00 -0.83 -0.64 s2 0.07 0.02 4.13 0.00 0.04 0.11 Pro-Vaccine Group Bias Ratings: ARIMA(1,1,2), Q=0.03, p=0.85; H=0.17, p=0.00, JB=15.13, p<0.001 intercept -0.02 0.01 -1.26 0.21 -0.04 0.01 ar.L1 -0.73 0.19 -3.75 0.00 -1.11 -0.35 ma.L1 -0.07 0.27 -0.26 0.80 -0.61 0.47 ma.L2 -0.87 0.26 -3.42 0.00 -1.37 -0.37 s2 0.16 0.04 3.87 0.00 0.08 0.24 Pro-Vaccine Group Bias Rating Engagements: ARIMA(2,0,0), Q=0.01, p=0.94; H=0.17, p=0.00, JB=86.71, p<0.001 intercept -0.75 0.17 -4.44 0.00 -1.08 -0.42 ar.L1 0.18 0.21 0.90 0.37 -0.22 0.59 ar.L2 -0.39 0.19 -2.07 0.04 -0.76 -0.02 s2 0.27 0.05 5.04 0.00 0.17 0.38 Note. ar = autoregressive term; ma = moving average term; s2 = variance; \u201cIffy\u201d = Proportion of URLs found on the Iffy Index of Unreliable Sources; HQHS = Proportion of URLs that are \u201cHigh Quality Health Sources\u201d (i.e., Government and Academic Sources); Q = Ljung-Box statistic; H = Heteroskedasticity; JB = Jarque-Bera statistic  References 1. Gu, J. et al. The impact of Facebook\u2019s vaccine misinformation policy on user endorsements of vaccine content: An interrupted time series analysis. Vaccine 40, 2209\u20132214 (2022). 2. Dias, N., Pennycook, G. & Rand, D. G. Emphasizing publishers does not effectively reduce susceptibility to misinformation on social media. Harvard Kennedy School Misinformation Review 1, (2020). \n3. Curry Jansen, S. & Martin, B. The Streisand Effect and Censorship Backfire. International Journal of Communication 9, 656\u2013671 (2015). 4. Pennycook, G., Bear, A., Collins, E. T. & Rand, D. G. The Implied Truth Effect: Attaching Warnings to a Subset of Fake News Headlines Increases Perceived Accuracy of Headlines Without Warnings. Management Science 66, 4944\u20134957 (2020). 5. Bail, C. A. et al. Exposure to opposing views on social media can increase political polarization. Proceedings of the National Academy of Sciences 115, 9216\u20139221 (2018). 6. Bak-Coleman, J. B. et al. Combining interventions to reduce the spread of viral misinformation. Nat Hum Behav 1\u20139 (2022) doi:10.1038/s41562-022-01388-6. 7. Pennycook, G. et al. Shifting attention to accuracy can reduce misinformation online. Nature 592, 590\u2013595 (2021). 8. Roozenbeek, J., Freeman, A. L. J. & van der Linden, S. How Accurate Are Accuracy-Nudge Interventions? A Preregistered Direct Replication of Pennycook et al. (2020). Psychol Sci 095679762110245 (2021) doi:10.1177/09567976211024535. 9. Fighting Coronavirus Misinformation and Disinformation. Center for American Progress https://www.americanprogress.org/article/fighting-coronavirus-misinformation-disinformation/. 10. Okada, M., Yamanishi, K. & Masuda, N. Long-tailed distributions of inter-event times as mixtures of exponential distributions. Royal Society Open Science 7, 191643. 11. Akaike, H. A new look at the statistical model identification. IEEE transactions on automatic control 19, 716\u2013723 (1974). 12. Schwarz, G. Estimating the dimension of a model. The annals of statistics 6, 461\u2013464 (1978). \n13. Danilak, M. M. langdetect: Language detection library ported from Google\u2019s language-detection. 14. Sanh, V., Debut, L., Chaumond, J. & Wolf, T. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. Preprint at http://arxiv.org/abs/1910.01108 (2020). 15. Perspective API. https://www.perspectiveapi.com/.   ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Facebook's architecture undermines vaccine misinformation removal efforts", "author": ["DA Broniatowski", "J Gu", "AM Jamison", "JR Simons"], "pub_year": "2022", "venue": "arXiv preprint arXiv \u2026", "abstract": "Misinformation promotes distrust in science, undermines public health, and may drive civil  unrest. Vaccine misinformation, in particular, has stalled efforts to overcome the COVID-19"}, "filled": false, "gsrank": 159, "pub_url": "https://arxiv.org/abs/2202.02172", "author_id": ["K8c3PvUAAAAJ", "Ih0CkYsAAAAJ", "IgG5S9cAAAAJ", ""], "url_scholarbib": "/scholar?hl=en&q=info:_3GSzYgwJR4J:scholar.google.com/&output=cite&scirp=158&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D150%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=_3GSzYgwJR4J&ei=H7WsaJGVCJXUieoPmrax2A8&json=", "num_citations": 2, "citedby_url": "/scholar?cites=2172195759398744575&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:_3GSzYgwJR4J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2202.02172"}}, {"title": "LLMs left, right, and center: Assessing GPT's capabilities to label political bias from web domains", "year": "2024", "pdf_data": "LLM S LEFT ,RIGHT ,AND CENTER : ASSESSING GPT\u2019 S\nCAPABILITIES TO LABEL POLITICAL BIAS FROM WEB DOMAINS\nRaphael Hernandes\nLeverhulme Centre for the Future of Intelligence\nUniversity of Cambridge\nrhh43@cam.ac.uk\nGiulio Corsi\nLeverhulme Centre for the Future of Intelligence\nUniversity of Cambridge\ngc540@cam.ac.uk\nOctober 22, 2024\nABSTRACT\nThis research investigates whether OpenAI\u2019s GPT-4, a state-of-the-art large language model,\ncan accurately classify the political bias of news sources based solely on their URLs. Given\nthe subjective nature of political labels, third-party bias ratings like those from Ad Fontes\nMedia, AllSides, and Media Bias/Fact Check (MBFC) are often used in research to analyze\nnews source diversity. This study aims to determine if GPT-4 can replicate these human\nratings on a seven-degree scale (\"far-left\" to \"far-right\"). The analysis compares GPT-4\u2019s\nclassifications against MBFC\u2019s, and controls for website popularity using Open PageRank\nscores. Findings reveal a high correlation ( Spearman\u2019s \u03c1=.89,n=5, 877 ,p<0.001 )\nbetween GPT-4\u2019s and MBFC\u2019s ratings, indicating the model\u2019s potential reliability. However,\nGPT-4 abstained from classifying approximately2\n3of the dataset. It is more likely to abstain\nfrom rating unpopular websites, which also suffer from less accurate assessments. The LLM\ntends to avoid classifying sources that MBFC considers to be centrist, resulting in more\npolarized outputs. Finally, this analysis shows a slight leftward skew in GPT\u2019s classifications\ncompared to MBFC\u2019s. Therefore, while this paper suggests that while GPT-4 can be a\nscalable, cost-effective tool for political bias classification of news websites, its use should\nbe as a complement to human judgment to mitigate biases.\nKeywords gpt-4, large language models (LLM), media bias, data labeling, political bias\nIsThe New York Times more politically inclined to the left or the right? The answer might vary according to the\nrespondent. However, it seems to converge to the center-left \u2014at least according to three different services\nthat specialize in rating the news: Ad Fontes Media, AllSides, and Media Bias/Fact Check (MBFC) [1], [2],\n[3]. Finding that sort of information about one of the most-read news outlets in the world is relatively easy,\nespecially given that its public editor acknowledges the newspaper is perceived as left-wing [4]. It might be\nharder, however, to assess the political bias of more obscure or niche sources where accessible and accurate\ninformation is lacking.\nWhile political labels carry a certain degree of subjectivity, third-party bias ratings may help balance\ninformation ecosystems by allowing a better understanding of what is consumed, which is particularly\nrelevant since news outlets do not generally declare their standings [5]. This labeling is often used in\nacademic research. Examples include assessing whether news content highlighted by Google Search has\npolitical leanings [6, pp. 8\u20139] and spotting an increased polarization on Twitter in the discussions about the\nUnited Nations Conference of the Parties on Climate Change [7].\nNonetheless, making such judgments can be time- and resource-consuming. AllSides, for instance, uses a\nmix of methods, including editorial and panelist reviews, consumer surveys, research from other sources,\nand community feedback [8]. Things get trickier when analyzing in bulk, labeling hundreds of sources.\nArtificial Intelligence (AI) might be an alternative for the labeling process, and recent studies have shown\nhigh accuracy when employing Large Language Models (LLMs) for data annotation, including withinarXiv:2407.14344v2  [cs.CL]  22 Oct 2024\nLLMs left, right, and center\npolitical contexts [9], [10], [11], [12]. While some recent studies use such models to label news content based\non web domain (to assess credibility [13], for example), none explored judging political bias based on URLs.\nConsidering this scenario, this research investigates whether OpenAI\u2019s GPT-4, the state-of-the-art language\nmodel according to relevant benchmarks [14], [15], can reliably classify news sources based solely on web\ndomain on a seven-degree scale (\"far-left,\" \"left,\" \"center-left,\" \"no bias,\" \"center-right,\" \"right,\" and \"far-right\").\nIt follows the questions:\nRQ1 Can an LLM, using only its embedded knowledge with no internet access, classify the political bias\nof news sources based solely on their URLs in alignment with humans?\nRQ2 Does classification accuracy vary depending on the popularity of the news source?\nIn addressing the first question, this paper analyzes the correlation between political ideology ratings\nattributed by GPT and MBFC. The second question relates to the functioning of these models, which involves\ndiscerning patterns in extensive text corpora [16]. The hypothesis is that how often a news source appears in\nthis data might affect the model\u2019s performance.\n1 Background\n1.1 GPT-4 and LLMs\nGPT-4 is the latest of OpenAI\u2019s family of LLMs [17]. It was chosen due to GPT\u2019s popularity in the AI\ncommunity since the launch of ChatGPT in November 2022 [18]. It is the best option available1for the\ntasks analyzed in this research according to MMLU [15], a widely used benchmark for models in zero-shot\nsettings [19], [20], and HELM, a comprehensive benchmark for language tasks [14], [21], [20, p. 114].\nThese sorts of AI are created after analyzing the patterns in immense amounts of textual data. They work by\npredicting combinations of tokens and reply to questions made by users conversationally, besides following\ninstructions based on their input (called a prompt) [22], [16]. LLMs can perform a myriad of language-\nprocessing tasks, such as answering questions, writing, summarizing texts, and translating [20, pp. 99\u2013\n108].\nAs these models grow in terms of computation, number of parameters, and training dataset size, they show\ncapabilities that go beyond their training scope and perform well in tasks that are not their primary goal.\nThese are known as emergent properties. They are not entirely understood but seem related to model size [23,\npp. 2\u20134]. The lack of clarity about them is an issue, as little is known about their limitations [11, p. 4].\n1.2 LLMs for Annotation Tasks\nAmong LLMs\u2019 emergent properties are applications related to annotating datasets. LLMs were successfully\nused to assess the quality of texts written by humans and robots regarding grammar, cohesiveness, likability,\nand relevance [9]. Research also shows that GPT in a zero-shot setting can classify tweets according to\nseveral criteria, such as whether they are a content moderation issue or relate to pre-defined political topics,\nwith higher accuracy than non-expert humans under the same instructions [10].\nIn a similar context to this paper, research found a strong correlation ( Spearman\u2019s \u03c1=.54,p<0.001 ) between\nhuman and GPT-assigned ratings for news outlets\u2019 credibility in a corpus of over 7,000 web domains [13].\nThe result is based on an aggregate score of multiple services that evaluate news domain quality rather than\na single classifying service [24]. These services correlate with each other on varying degrees ( Spearman\u2019s \u03c1\nrange: .32-.90 ), so GPT would fall in the middle of that distribution. This motivated the investigation of\nwhether GPT could repeat the same level of performance in a political bias classification scenario.\n1.2.1 LLMs in Political Analysis\nLLMs have also displayed promising results in tasks related specifically to political analysis. GPT could\naccurately classify the political affiliation of a Twitter user based on the content of a single anonymized\ntweet [11]. The analysis used posts made by all US senators, providing an unequivocal basis for evaluating\nthe answers. The AI performed better than both expert and non-expert humans.\n1Google\u2019s Gemini Ultra had a higher score but was not publicly available at the time of writing.\n2\nLLMs left, right, and center\nGPT has also shown remarkable capacity to classify US senators in terms of their liberal-conservative\nideology, support of gun control, and support of abortion rights, based only on their names and parties\nusing pairwise comparisons [12, p. 2]. The rankings are not simply mimicking other scales, contrasting with\nthe idea that these systems might be parroting patterns from their training data [25]. If they were merely\ncopying information from elsewhere, it would be simpler to use the original data. Instead, the investigation\nshows that the ratings came from a mix of senators\u2019 behaviors and how these politicians are perceived [12,\np. 19]. This indicates that LLMs\u2019 capabilities in political classification tasks warrant further investigation, as\nthey might offer not only a cheaper and faster way of labeling the data but also a new scale altogether.\n1.3 Rating News Sources\u2019 Political Bias\nPerception and behavior are currently used forms of rating political bias in media sources. Scales from the\nlikes of AllSides and polling institutes, such as Pew Research Center, classify news sources based on how\nconsumers perceive them [8], [26]. Language (behavior) has been used in research to classify newspapers:\nphrases connected to the right meant right-leaning tendencies [27]. The measurement correlated to the\npolitical incline of readers, given the preferences in the zip codes where the newspapers circulated [27, p. 64].\nA study used the self-reported ideology of Facebook users to classify news sources by leveraging popularity\namong the right or the left to gauge the outlet\u2019s political ideology [28].\nMBFC, the baseline in this paper, primarily analyzes news source quality. Its database includes information\nbeyond credibility, such as the source\u2019s country and media type (newspaper or TV , for example). It rates\npolitical bias by examining editorial content by their position on general philosophy, abortion, economic\npolicy, education policy, environmental policy, gay rights, gun rights, health care, immigration, military,\npersonal responsibility, regulation, social views, taxes, voter ID laws, and workers\u2019 rights [29]. It is cited as a\nsource for asserting news quality both in Academia and in the news [30, p. 2], [31], [32, p. 4].\n1.3.1 Applications of these Ratings\nAnalyzing the political bias of news sources is helpful in gauging diversity in the news ecosystem. By\nclassifying ideology and relating it to zip codes, it is possible to note that political ideology in newspapers\nis more related to appealing to their audience than ownership diversity, as outlets sharing an owner have\ndiverging political stances depending on their readership. This is relevant for regulating the industry\n(deciding whether to limit ownership, for example) [27]. These can also be used to measure diversity in news\nexposure, when, for example, news sources\u2019 political inclinations to analyze the results offered by Google\u2019s\nalgorithm. The classification allowed researchers to identify it was slightly skewed to the left, which might\ndirect readers\u2019 attention that way [6].\n2 Methodology\nThe analysis gathered a dataset with classifications from MBFC and popularity ratings from Open PageRank.\nFirst, the data for every report made by MBFC was downloaded, removing those that did not include\ninformation on political bias, such as the ones focusing on scientific content. The web domains from MBFC\u2019s\ndatabase were cleaned to remove inconsistencies (invalid URLs, for example), and missing values were\nfetched manually from the reports. Two pairs of records shared the same domain, and only one of each was\nkept. Three records were related to Facebook pages and were removed.\nThe dataset includes 5,877 observations extracted from MBFC. The bias column from GPT and MBFC was\ncoded into a scale ranging from -3 to +3, meaning \"far-left,\" \"left,\" \"center-left,\" \"center,\" \"center-right,\" \"right,\"\nand \"far-right.\" This makes it possible to calculate both the difference and absolute difference between the\ntwo measurements (GPT Bias \u2212MBFC Bias). See the appendices for summary statistics.\nBoth MBFC\u2019s and GPT-4\u2019s Bias Scales display a pattern that deviates from a normal distribution (Figure 1),\nso a non-parametric correlation method, Spearman\u2019s R, was used for the comparison. MBFC has more right\nthan left-wing classifications. This is acknowledged by the authors, who explain that their set was mostly\nbalanced until they started taking user requests for websites to rate, which are generally for right-leaning\ndomains [33].\nThe analysis primarily focuses on the correlation between the ratings assigned by GPT and MBFC, an\napproach similar to previous work that compared the LLM\u2019s ability to judge news outlet credibility [13].\nThere are multiple ways of classifying news sources\u2019 political bias, and the exact placement in a scale might\n3\nLLMs left, right, and center\nvary (as there is no clear line separating center-left from left, for example). Thus, this work is concerned\nwith whether GPT will place a source more to the right when MBFC does it instead of checking whether it\npredicts the exact value, so the correlation is a better measurement. The system\u2019s accuracy is also analyzed\nlater through linear and logistic regressions controlling for website popularity and political stance.\n2.1 Open PageRank\nOpen PageRank is a free initiative that applies metrics like the ones used by Google to rank websites,\nenabling comparison among domains through a scale from 0 to 10 to the decimal level where higher can be\ninterpreted as a proxy for more popular. It provides data about the top 10 million websites in its database [34].\nThe root domain of each URL (\"www.example.com/abc\" \u2192\"example.com\") is used to retrieve ratings from\nOpen PageRank\u2019s API. Therefore, this analysis does not differentiate between sources that share a root URL\n(\"theguardian.com/observer\" and \"theguardian.com,\" for example), meaning popularity might be slightly\nexaggerated for some. These, however, are not frequent in the data, which contains 5,684 unique root URLs.\nThe distribution of Open PageRank scores is available in the appendices.\n2.2 Prompting GPT-4\nThe GPT version used is gpt-4-1106-preview , the most recent available when writing, queried through\nOpenAI\u2019s API. It was released on Nov. 06, 2023, with up-to-date information until April 2023, and is\ndescribed by OpenAI as having \"improved instruction following\" capabilities [35]. Being in \"preview\" means\nit does not support high traffic [36], which is not the case in this work. It was cheaper than older versions\nof GPT-4 (US$0.01/1k input and US$0.03/1k output tokens vs. US$0.03/1k input and US$0.06/1k output\ntokens) [37]. It can reply in JSON format instead of text, making parsing easier as it removes the need to\nextract the output from a paragraph.\nGpt-4-1106-preview has a seed parameter to make the model return consistent completions, which is useful\nfor reproducible outputs [35]. The seed was set to 123. The temperature was 0.0to reduce randomness and\nsupport reproducibility. A low temperature may be preferable for annotation tasks as it increases consistency\nwithout decreasing accuracy [10, p. 2].\nThe prompts asked the AI to rate the political bias of a URL or return \"-1\" if it could not classify it. After the\nmain experiment, to better understand the machine\u2019s behavior, additional prompts for a random sample\nasked GPT to justify the output. Sample prompts and responses are available in the appendices. It was used\nin a zero-shot setting in an attempt to evoke latent information in the model\u2019s training. The request specified\nthat the response be in JSON and provided an example of formatting. A system message added context\ntelling GPT to act as an assistant to determine the political bias of websites.\n3 Results\n3.1 Correlation Between GPT-4 and Human-Assigned Political Bias Ratings\n-3 -2 -1 0 1 2 3\nPolitical Bias Scale Category0150300450600750900105012001350Number of News SourcesNumber of News Sources by MBFC and GPT\nPolitical Bias Scale Categories\nMBFC Bias Scale\nGPT Bias Scale\n-3 -2 -1 0 1 2 3\nPolitical Bias Scale Category03691215182124Percentage of News SourcesPercentage of News Sources by MBFC and GPT\nPolitical Bias Scale Categories\nMBFC Bias Scale\nGPT Bias Scale\nFigure 1: Distribution of news sources in absolute (left) and relative values (right).\n4\nLLMs left, right, and center\n\u22126\u22125\u22124\u22123\u22122\u221210 1 2 3 4\nDifference between ratings015030045060075090010501200FrequencyHistogram of difference (GPT - MBFC)\n\u22126\u22125\u22124\u22123\u22122\u221210 1 2 3 4\nDifference between ratings050100150200250300350400FrequencyHistogram of difference (GPT - MBFC); excludes zero\n0 1 2 3 4 5 6\nAbsolute difference between ratings015030045060075090010501200FrequencyHistogram of absolute difference (GPT - MBFC)\n1 2 3 4 5 6\nAbsolute difference between ratings080160240320400480560640720FrequencyHistogram of absolute difference (GPT - MBFC); excludes zero\nFigure 2: Histograms of difference (top) and absolute difference (bottom) between GPT and MBFC ratings\nshow concentration around minimal difference; charts on the right exclude zero for easier visualization.\nThe analysis shows a very strong2correlation between GPT-4\u2019s and MBFC\u2019s political bias classifications\n(Spearman\u2019s \u03c1=.89,n=5, 877 ,p<0.001 ). This shows that both vary in the same direction: sources\nclassified toward one side of the spectrum in MBFC are likely to follow the same pattern with GPT.\nGPT returned a small set of values on the opposite side of the expected spectrum (Figures 3 and 4). The most\nextreme scenario (far-right being tagged as far-left, or vice-versa) only happened once: strategic-culture.org\nwas labeled far-right by MBFC, and GPT had far-left.\nThe correlation between GPT-4 and MBFC\u2019s classifications is at least strong ( >.4) across most statistically\nrelevant categories. A full breakdown of the categories is available in the appendices. In terms of country, it\nwas stronger in ones that speak English and weakest in unknown regions ( Spearman\u2019s \u03c1=.49,n=275) and\nothers ( \u03c1=.65,n=452). The correlation was statistically relevant in all locations ( p<0.001).\nThe correlation also fluctuated regarding media type, though staying above the strong threshold for all\nstatistically relevant categories. It was particularly strong among sources not necessarily in the media\nbusiness: Journals ( Spearman\u2019s \u03c1=.97,n=7,p<0.01) and Organization/Foundation ( Spearman\u2019s \u03c1=.91,\nn=587,p<0.001 ). Most of these are correctly rated \"unbiased,\" which makes sense given the nature\nof these publications \u2014less political than a news website. This shows hints of a capacity in GPT-4 to\nidentify these sources as such and classify them accordingly. Across news agencies, the correlation was weak\n(Spearman\u2019s \u03c1=.28,n=41), though not statistically significant.\nCorrelation drops when crunching the different political biases into left, right, and center (including\ncenter-left and center-right). It is negligible when considering only sources originally classified as left\n(Spearman\u2019s \u03c1=.12,n=556,p<0.05) and remains strong with both right ( Spearman\u2019s \u03c1=.43,n=1, 162 ,\np<0.001 ) and center-leaning sources ( Spearman\u2019s \u03c1=.61,n=4, 159 ,p<0.001 ). That pattern is unex-\npected, and shows a potential limitation to applying this in practice. It might be due to the loss of nuance\nwhen collapsing the scale for this analysis or due to MBFC\u2019s dataset having more right-wing sources. It\ncould also be due to systematic issues with GPT\u2019s training data. In that case, a possible explanation is that\nright-wing media is more often directly referred to as that in GPT\u2019s training dataset. This matter requires\nfurther investigation, which is beyond the scope of this paper.\nTo further test the capacity of this classification under a simpler setting, the ratings were recoded using\na binary classification of unbiased (center-left, center, center-right) and biased (far-left, left, right, far-\nright). This binary classification aimed to simplify the analysis and observe the performance under less\n2Based on Quinnipiac University\u2019s table for interpreting Spearman\u2019s correlation coefficients in Politics contexts [38, p. 92]\n5\nLLMs left, right, and center\ngranular conditions. The classification was tested using the AUC-ROC (Area Under the Receiver Operating\nCharacteristic Curve) metric, which measures the ability of the model to distinguish between the two classes.\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate0.00.20.40.60.81.0True Positive RateReceiver Operating Characteristic (ROC) for Biased vs Unbiased\nROC curve (AUC = 0.77)\nFigure 3: The ROC curves of GPT\u2019s ratings, using MBFC as\na baseline, binarized into biased and unbiased.Entries that GPT-4 did not classify were re-\nmoved from the analysis, resulting in an AUC-\nROC score of .77, indicating good classifica-\ntion performance. When these unclassified\nentries were retained in the dataset, the per-\nformance remained above random, with an\nAUC-ROC score of .54.\n3.1.1 Left-leaning Bias\nThe statistical analysis revealed that GPT\u2019s\nclassifications are slightly more left-leaning\nthan MBFC\u2019s. A one-sample t-test showed\nthat the mean difference in classifications was\n-0.08 , indicating a leftward bias. The test re-\nsults were statistically significant ( t=\u22124.56,\np<0.001 ). This is not an entirely new phe-\nnomenon. The same inclination was noted in\nresearch that classified the political affiliation\nof Twitter users based on the content of tweets\nin the US. In that context, human respondents were also significantly skewed toward guessing Democrat [11,\np. 3].\n3.2 Impact of Popularity on Accuracy\nTo assess the LLM\u2019s capacity to determine the political slant of news sources, the analysis controlled for\nwebsite popularity as the model\u2019s exposure to certain URLs during training could disproportionately affect\nits predictions, leading to biases. Popular websites are more likely to appear in diverse contexts within the\ntraining data, relating to \"common-token bias\" [39, pp. 4\u20135]. This means the model might associate these\ncommon tokens (URLs of popular sources) with a wide range of content, diluting the model\u2019s ability to\nclassify political bias. It could result in the model inaccurately attributing neutrality to well-known sources\nsimply due to their ubiquity or wrongly associating them with a bias based on a frequent misconception.\nThe opposite might also be the case: the dataset used to train GPT-4 lacks information about an obscure\nwebsite, forcing the AI to make something up. To mitigate these issues, the prompt allowed the model to not\nassign a rating if uncertain.\nThe statistical analysis revealed an impact caused by websites\u2019 popularity (Open PageRank) in the ratings\u2019\nquality. News sources were grouped into four sets of similar sizes based on their popularity rating quartiles.\nThe correlation between MBFC and GPT remains very strong across each popularity category ( Spearman\u2019s \u03c1\nrange: .64-.91 ,p<0.001 ). It peaks in the medium-high category (50-75th percentiles) and is the weakest in\nthe lowest category (0-25th percentiles). A hypothesis is that the smaller correlation in unpopular websites\ncomes from less data about them being available. Conversely, the drop in the top websites (above 75th\npercentile, \u03c1=.77) might be from the excess of information with contradictory views. For instance, GPT\nclassified economist.com (above 75th popularity percentile) as center-right, while MBFC has it as center [40].\nAllSides and Ad Fontes say it leans slightly towards the left [41], [42]. The response to an additional\nprompt that asked GPT to justify the classification acknowledged some ambiguity: \" The Economist generally\nadvocates for free markets, internationalism, and cultural liberalism, which aligns it with center-right\npolitical positions, although it also supports some socially liberal positions.\"\nThe influence of website popularity on prediction accuracy was analyzed through an Ordinary Least Squares\n(OLS) regression on the absolute distance between GPT\u2019s prediction and MBFC\u2019s ratings. Absolute distance\nwas used since this step of the research focused on how aligned the answer was, disregarding the direction\nof the error. Modeling only with popularity did not yield statistically significant results.\nWhen controlling for country, the results indicate that the Open PageRank rating is associated with accuracy.\nA one-unit increase in popularity is associated with a decrease of 0.048 in the distance between GPT and\nMBFC ( p<0.01). The model, however, only explained a small portion of the variance in GPT\u2019s accuracy\n(R2=0.017 ). Although its explanatory power is limited, it provides some indication that the LLM performs\n6\nLLMs left, right, and center\nbetter on more prominent websites. Further diagnostic checks are needed to understand this phenomenon,\nwhich is beyond the scope of this paper. Regression tables are available in the appendices.\n-3.0 -2.0 -1.0 0.0 1.0 2.0 3.0\nGPT Bias Scale-3-2-10123MBFC Bias Scale3 14 0 0 0 0 0\n22 256 34 10 0 0 0\n0 84 254 135 6 2 1\n0 1 70 225 45 8 1\n0 7 28 40 86 63 1\n0 4 0 5 18 249 15\n1 1 0 0 0 175 111Heatmap of number of News Sources\nby MBFC and GPT Bias Scales\n-3.0 -2.0 -1.0 0.0 1.0 2.0 3.0\nGPT Bias Scale-3-2-10123MBFC Bias Scale4.09 4.28 0.00 0.00 0.00 0.00 0.00\n4.47 5.02 5.43 5.56 0.00 0.00 0.00\n0.00 5.22 5.73 5.56 5.38 4.68 4.64\n0.00 5.13 5.45 5.46 5.23 5.04 0.00\n0.00 4.75 5.58 5.38 5.41 4.82 4.51\n0.00 5.12 0.00 5.61 5.19 4.58 3.80\n4.34 3.93 0.00 0.00 0.00 4.22 3.97Heatmap of Avg. Popularity of News Sources\nby MBFC and GPT Bias Scales\n050100150200250\nNumber of News Sources\n012345\nAverage Popularity (Open PageRank)\nFigure 4: Heatmap of news sources classifications (left) shows that most sources fall within the expected\naxis (the colorful diagonal); heatmap of sources\u2019 popularity (right) indicates that popular sources converge\ntowards the center.\n3.3 Unassigned Observations\nGPT-4 abstained from classifying most sources in the dataset, rating only 1,975 (33.6%) entries. Leaving out a\nsignificant portion of the data could introduce bias as these gaps are not evenly distributed. However, given\nthat LLMs are often criticized for their hallucinations (wrong or made-up outputs) [43, p. 6], the model\u2019s\ncapacity to return \"I do not know\" instead of providing nonsensical results makes it more trustworthy as\nlong as these systematic failures are tracked.\nThe unclassified sources are spread across all different categories (table in the appendices). Regarding\ncountry, it was most prevalent in sources whose region is unknown (87.3%). The \"Others\" category, which\nincludes non-English-speaking countries, did not stand out (60.6%), which suggests that language might not\nbe the most relevant issue.\nLogistic regressions on the different categories in this dataset (tables in the appendices) revealed some\nfactors that might cause a URL to be left unclassified. The most relevant were news source popularity (Open\nPageRank) and political stance, based on MBFC\u2019s rating simplified as right (far-right and right), left (far-left\nand left), and center (center-right, center-left, and center).\nPopularity was assessed against the likelihood of GPT\u2019s prediction coming out unassigned, individually,\nand with control variables. The initial model, considering only popularity, indicated a significant negative\nassociation (coef = -1.2195 , SE = 0.042 ,z=\u221229.122 ,p<0.001 ), suggesting the LLM is more likely to rate\nmore popular sources. McFadden\u2019s Pseudo R2for the model is 0.1869 , indicating a moderate explanatory\npower.\nControl variables country, credibility, political stance, and media type provide a more comprehensive\nunderstanding. In this extended model, the impact of popularity became more pronounced (coef = -2.1344 ,\nSE = 0.074 ,z=\u221229.025 ,p<0.001 ), and McFadden\u2019s R2increased to 0.3874 , indicating a better model fit.\nThese findings underscore that popularity is a relevant metric in GPT\u2019s capacity to assess the political bias of\na website since it consistently showed a strong and significant relationship with the outcome, even when\ncontrolling for a range of other factors.\nThe coefficients for the additional control variables oscillated, with some showing significant associations\nwith the likelihood of GPT abstaining. Besides popularity, the most prominent ones were related to political\nstance. They were investigated in a separate logistic regression, controlling for popularity. Compared to the\nbaseline political stance category, center, having a left (coef = -2.2246 , SE = 0.124 ,z=\u221217.897 ,p<0.001 )\nor right (coef = -3.2355 , SE = 0.119 ,z=\u221227.180 ,p<0.001 ) skewness significantly reduced the odds of\nGPT\u2019s output being unassigned. The McFadden\u2019s R2value of 0.3448 indicates that the model explains a\n7\nLLMs left, right, and center\ngreat proportion of the variability in the outcome. These results, therefore, highlight a tendency of GPT to be\nunable to assign a rating to the least biased sources.\nUpon rerunning the classifications of 50 random unassigned sources while also requesting GPT-4 to provide\nits reasoning, only two were then labeled. Previous research shows such a technique, labeled chain-of-\nthought, might improve performance [44]. The two newly labeled sources were baltimoresun.com/citypaper\n(GPT had it as center-left, while MBFC had it as left) and americasvoice.news (aligned with MBFC as right).\nAll the reasons for the remaining 48 sources argue lack of information, despite five being in the top quartile\nof PageRank\u2019s popularity, such as walesonline.co.uk . This technique brings a slight improvement but comes at\na significant cost since each request\u2019s output consumes up to 4x more tokens. Further testing is needed to\ncheck whether it would render improved alignment.\n4 Discussion\nThe high correlation between the MBFC\u2019s and GPT\u2019s classifications shows encouraging evidence that the\nLLM can reliably assess the political bias of news sources. However, a better understanding of the model\nand its limitations is warranted. These results align with works that have previously applied AI models to\nmake political classifications [11], an area in which GPT has already been used in Academia [45]. Also, it\nspeaks to GPT\u2019s capacity to rate news outlets\u2019 credibility based on their web domain [13].\nThus, LLMs could be deemed an alternative method for these classification tasks, with some advantages over\nexisting methods. The first is having a novel approach that does not necessarily rely solely on perception or\nbehavior for the rating. There are benefits to reproducibility, which is hard (if not impossible) to achieve\nwhen humans judge the political bias of websites, as opinions might vary among different people or at\ndifferent times. GPT allows setting low temperatures to reduce randomness and a fixed seed parameter to\naid reproducibility. It still does not mean zero variability but limits it. Current methods of assessing political\nbias in news outlets can be costly and time-consuming, and LLMs might be cheaper, scalable alternatives [11,\np. 4].\nNevertheless, any endeavor using LLMs to assess the political bias of news websites will have to account for\ndownsides. The most relevant is the lack of understanding about how these models attribute the ratings.\nWhile the scores are similar to the ones assigned by humans, it is hard to pinpoint the criteria used by the AI,\nwhich may hide biases. Some of those were explored in this research, such as a skewness towards the left.\nAlso, the analysis shows that GPT can classify sources beyond the English-speaking world. While that might\nbe good due to potentially covering more countries than existing datasets, this comes at a smaller correlation,\nwhich speaks to a known phenomenon that GPT performs better in English [46, pp. 7\u20138]. Coupled with most\nsafety mitigations in the model being designed to work in English [43, p. 21], GPT\u2019s political bias assessment\nof new sources in areas that speak other languages should be deemed less reliable.\nUser prompting might play a negative role in influencing the results. If poorly executed, it might hinder AI\u2019s\noutput quality. Differently from a human evaluator, the system might not say it did not understand badly\nexplained instructions [11, p. 4].\nGPT\u2019s inability to classify roughly 2/3 of the data requires attention. This analysis provided some insight\ninto some factors that might influence its ability, and they need to be accounted for as they might introduce\nbiases in future analyses relying on this model for judging political bias in news sources. More obscure\nwebsites were less likely to be classified, showing a reduced capacity to judge content that deviates too much\nfrom mainstream media. Also, the AI was less likely to classify the least biased websites in MBFC\u2019s dataset.\nApplying GPT to judge ideology in news sources without considering this effect might lead to a perception\nof a more polarized environment due to a systematic failure to pick up the most central sources.\nAs with other applications, overreliance is an issue [46, p. 19]. Therefore, a mixed human and machine\nlabeling methodology could be an alternative to leverage GPT\u2019s capabilities while keeping its biases in\ncheck. Any domain can be prompted to GPT-4, so, using the AI, researchers would not be subject to the\nconstraints of lack of coverage in fixed datasets. Allowing the model to abstain from making a rating is\nparamount [47, p. 7029], as this could prevent GPT from assigning grades to sources with little information.\nHuman classification could fill in the gaps. Another option for a hybrid approach is having the model output\na confidence level for each classification and manually tag the ones that fall under a certain threshold [48],\nbut further testing is needed to check this technique\u2019s reliability in this context.\n8\nLLMs left, right, and center\nErrors (hallucinations) might be cause for concern. However, this analysis shows that the most extreme kind\nof misclassification (calling a far-right source far-left) only happened once. Given the high correspondence\nbetween AI and human labels discussed in this paper, manually inspecting random samples of the LLMs\u2019\nclassifications should ensure a reliable output. While laborious, analyzing a small subset of the data is more\nscalable than tagging it all. There is some disagreement between GPT and MBFC, but this is also true among\nmultiple human-assigned rating services, so a level of discrepancy is acceptable in this scenario. Moreover,\nwhen analyzing in bulk, the divergence in ratings should be diluted, and if not, the sample analysis should\nnotice it.\n5 Conclusion\nThis paper examined the viability of using OpenAI\u2019s GPT-4 model to classify the political bias of news\nsources. The findings demonstrate a high correlation between GPT-4\u2019s classifications and MBFC\u2019s, suggesting\nthat LLMs can be a reliable, cost-effective, and scalable alternative for such tasks. However, the study also\nrevealed significant limitations. There is an indication that it performs worse on less-known websites. Also,\nGPT-4 could only classify 1/3 of the analyzed URLs, with a tendency to abstain from rating less popular and\nless biased sources. This shows a bias towards mainstream media and polarized classifications. Furthermore,\nGPT-4\u2019s ratings leaned slightly more to the left than MBFC\u2019s.\nThese findings underscore the necessity of understanding and accounting for LLMs\u2019 underlying mechanisms\nand biases when deploying them to rate political ideology in news sources. While GPT-4 shows promise in\nenhancing the scope and efficiency of political bias classification, its application should complement human\njudgment rather than replace it.\n5.1 Further Studies\nThis is a first foray into GPT\u2019s capacity to judge political bias in news sources based on their web domains.\nMore research is needed to understand this emergent capability. Other models should be explored since even\nAIs from the same family might have distinct tendencies in ratings [9, p. 15611]. Varying the instructions\nalso impacts the models\u2019 output [9, p. 15612], so this task must be explored under different settings, such as\nasking it to justify the rating or output a confidence level. This study left room for the model to refuse to rate\na source, leading to numerous unassigned observations. Future investigators might check for impacts on\nperformance when forcing the LLM to assign a rating.\nAlso, there are multiple ways of assessing political bias in the media. This paper only checks for one dataset\nin this realm, while multiple exist using different approaches. Further research could investigate how GPT\naligns with other sources, which might also provide insight into the criteria it uses to make its calls. Lastly,\nMBFC\u2019s dataset consists mostly of English-speaking sources, so further investigation is needed to assess the\nLLM\u2019s performance in different languages.\nAuthor Contributions\nRH designed the study, RH performed the analyses, and both authors assisted in the revision of the\nmanuscript and the refinement of its arguments.\nStatements and Declarations\nDeclaration of conflicting interests\nThe authors declared no potential conflicts of interest with respect to the research, authorship, and/or\npublication of this article.\nFunding\nThe authors received no financial support for the research, authorship, and/or publication of this article.\n9\nLLMs left, right, and center\nData availability\nThe data that support the findings of this study are openly available in figshare at https://doi.org/10.\n6084/m9.figshare.26325319.v1\nReferences\n[1] AllSides Media Bias Ratings . AllSides. 2024. URL:https://www.allsides.com/media-bias/ratings\n(visited on 01/10/2024).\n[2] Interactive Media Bias Chart . Ad Fontes Media. 2023. URL:https://adfontesmedia.com/interactive-\nmedia-bias-chart/ (visited on 01/10/2024).\n[3] New York Times - Bias and Credibility . Media Bias/Fact Check. Jan. 10, 2024. URL:https : / /\nmediabiasfactcheck.com/new-york-times/ (visited on 01/10/2024).\n[4] Liz Spayd. \u201cWhy Readers See The Times as Liberal\u201d. In: The New York Times. Public Editor (July 23,\n2016). ISSN : 0362-4331. URL:https://www.nytimes.com/2016/07/24/public-editor/liz-spayd-\nthe-new-york-times-public-editor.html (visited on 01/10/2024).\n[5] Jake Sheridan. Should You Trust Media Bias Charts? Poynter. Nov. 2, 2021. URL:https://www.poynter.\norg/fact-checking/media-literacy/2021/should-you-trust-media-bias-charts/ (visited on\n01/10/2024).\n[6] Daniel Trielli and Nicholas Diakopoulos. \u201cSearch as News Curator: The Role of Google in Shaping\nAttention to News Information\u201d. In: Proceedings of the 2019 CHI Conference on Human Factors in Comput-\ning Systems . CHI \u201919: CHI Conference on Human Factors in Computing Systems. Glasgow, Scotland,\nUK: ACM, May 2, 2019, pp. 1\u201315. ISBN : 978-1-4503-5970-2. DOI:10.1145/3290605.3300683 .URL:\nhttps://dl.acm.org/doi/10.1145/3290605.3300683 (visited on 12/18/2023).\n[7] Max Falkenberg et al. \u201cGrowing Polarization around Climate Change on Social Media\u201d. In: Nature\nClimate Change 12.12 (Dec. 2022), pp. 1114\u20131121. ISSN : 1758-678X, 1758-6798. DOI:10.1038/s41558-022-\n01527-x .URL:https://www.nature.com/articles/s41558-022-01527-x (visited on 01/30/2024).\n[8] How AllSides Rates Media Bias . AllSides. Aug. 10, 2016. URL:https://www.allsides.com/media-\nbias/media-bias-rating-methods (visited on 01/10/2024).\n[9] Cheng-Han Chiang and Hung-yi Lee. \u201cCan Large Language Models Be an Alternative to Human\nEvaluation?\u201d In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics .\nVol. 1. Long Papers. Toronto, Canada, July 2023, pp. 15607\u201315631. DOI:10.18653/v1/2023.acl-\nlong.870 .URL:https://aclanthology.org/2023.acl-long.870 .\n[10] Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. \u201cChatGPT Outperforms Crowd Workers for Text-\nAnnotation Tasks\u201d. In: Proceedings of the National Academy of Sciences 120.30 (July 25, 2023), e2305016120.\nISSN : 0027-8424, 1091-6490. DOI:10.1073/pnas.2305016120 .URL:https://pnas.org/doi/10.1073/\npnas.2305016120 (visited on 01/06/2024).\n[11] Petter T\u00f6rnberg. ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter\nMessages with Zero-Shot Learning . Apr. 13, 2023. DOI:10.48550/arXiv.2304.06588 . arXiv: 2304.06588\n[cs] .URL:http://arxiv.org/abs/2304.06588 (visited on 11/11/2023). Pre-published.\n[12] Patrick Y. Wu et al. Large Language Models Can Be Used to Estimate the Latent Positions of Politicians .\nSept. 26, 2023. DOI:10.48550/arXiv.2303.12057 . arXiv: 2303.12057 [cs] .URL:http://arxiv.org/\nabs/2303.12057 (visited on 01/12/2024). Pre-published.\n[13] Kai-Cheng Yang and Filippo Menczer. Large Language Models Can Rate News Outlet Credibility . Apr. 1,\n2023. DOI:10.48550/arXiv.2304.00228 . arXiv: 2304.00228 [cs] .URL:http://arxiv.org/abs/\n2304.00228 (visited on 01/06/2024). Pre-published.\n[14] Holistic Evaluation of Language Models (HELM) . Center for Research on Foundation Models. May 7, 2024.\nURL:https://crfm.stanford.edu/helm/lite/latest/#/leaderboard (visited on 01/31/2024).\n[15] MMLU Benchmark (Multi-task Language Understanding) . Papers with Code. 2024. URL:https : / /\npaperswithcode . com / sota / multi - task - language - understanding - on - mmlu (visited on\n01/25/2024).\n[16] Kevin Roose. \u201cHow Does ChatGPT Really Work?\u201d In: The New York Times. On Tech: A.I. Newsletter\n(Mar. 28, 2023). URL:https://www.nytimes.com/2023/03/28/technology/ai-chatbots-chatgpt-\nbing-bard-llm.html (visited on 01/02/2024).\n[17] GPT-4 . OpenAI. Mar. 14, 2023. URL:https://openai.com/research/gpt-4 (visited on 01/13/2024).\n10\nLLMs left, right, and center\n[18] Krystal Hu. \u201cChatGPT Sets Record for Fastest-Growing User Base - Analyst Note\u201d. In: Reuters. Tech-\nnology (Feb. 2, 2023). URL:https://www.reuters.com/technology/chatgpt-sets-record-fastest-\ngrowing-user-base-analyst-note-2023-02-01/ (visited on 01/02/2024).\n[19] Dan Hendrycks et al. Measuring Massive Multitask Language Understanding . Jan. 12, 2021. DOI:10.\n48550/arXiv.2009.03300 . arXiv: 2009.03300 [cs] .URL:http://arxiv.org/abs/2009.03300\n(visited on 01/25/2024). Pre-published.\n[20] Nestor Maslej et al. Artificial Intelligence Index Report 2023 . Version 1. arXiv, 2023. DOI:10.48550/ARXIV.\n2310.03715 .URL:https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-\nReport_2023.pdf (visited on 11/07/2023).\n[21] Percy Liang et al. Holistic Evaluation of Language Models . Oct. 1, 2023. DOI:10.48550/arXiv.2211.09110 .\narXiv: 2211.09110 [cs] .URL:http://arxiv.org/abs/2211.09110 (visited on 01/31/2024). Pre-\npublished.\n[22] Jesutofunmi A. Omiye et al. \u201cLarge Language Models Propagate Race-Based Medicine\u201d. In: npj Digital\nMedicine 6.1 (Oct. 20, 2023), p. 195. ISSN : 2398-6352. DOI:10 . 1038 / s41746 - 023 - 00939 - z .URL:\nhttps://www.nature.com/articles/s41746-023-00939-z (visited on 11/21/2023).\n[23] Jason Wei et al. Emergent Abilities of Large Language Models . Oct. 26, 2022. DOI:10.48550/arXiv.2206.\n07682 . arXiv: 2206.07682 [cs] .URL:http://arxiv.org/abs/2206.07682 (visited on 01/14/2024).\nPre-published.\n[24] Hause Lin et al. \u201cHigh Level of Correspondence across Different News Domain Quality Rating\nSets\u201d. In: PNAS Nexus 2.9 (Sept. 2023). DOI:10 . 1093 / pnasnexus / pgad286 .URL:https : / / doi -\norg.ezp.lib.cam.ac.uk/10.1093/pnasnexus/pgad286 .\n[25] Emily M. Bender et al. \u201cOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\u201d\nIn:Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency . FAccT \u201921:\n2021 ACM Conference on Fairness, Accountability, and Transparency. Canada: ACM, Mar. 3, 2021,\npp. 610\u2013623. ISBN : 978-1-4503-8309-7. DOI:10.1145/3442188.3445922 .URL:https://dl.acm.org/\ndoi/10.1145/3442188.3445922 (visited on 11/07/2023).\n[26] Amy Mitchell et al. Political Polarization & Media Habits . Pew Research Center, Oct. 21, 2014. URL:\nhttps://www.pewresearch.org/journalism/2014/10/21/political- polarization- media-\nhabits/ (visited on 01/14/2024).\n[27] Matthew Gentzkow and Jesse M. Shapiro. \u201cWhat Drives Media Slant? Evidence From U.S. Daily\nNewspapers\u201d. In: Econometrica 78.1 (Jan. 2010), pp. 35\u201371. ISSN : 0012-9682. DOI:10.3982/ECTA7195 .\nURL:http://doi.wiley.com/10.3982/ECTA7195 (visited on 01/11/2024).\n[28] Eytan Bakshy, Solomon Messing, and Lada A. Adamic. \u201cExposure to Ideologically Diverse News and\nOpinion on Facebook\u201d. In: Science 348.6239 (June 5, 2015), pp. 1130\u20131132. ISSN : 0036-8075, 1095-9203.\nDOI:10.1126/science.aaa1160 .URL:https://www.science.org/doi/10.1126/science.aaa1160\n(visited on 01/11/2024).\n[29] Left vs. Right Bias: How We Rate the Bias of Media Sources . Media Bias/Fact Check. May 18, 2021. URL:\nhttps://mediabiasfactcheck.com/left-vs-right-bias-how-we-rate-the-bias-of-media-\nsources/ (visited on 01/15/2024).\n[30] Ahmet Aker, Kevin Vincentius, and Kalina Bontcheva. \u201cPredicting News Source Credibility\u201d. In:\nProceedings of the Conference for Truth and Trust Online 2019 . Conference for Truth and Trust Online 2019.\nTTO Conference Ltd., Oct. 31, 2019. DOI:10.36370/tto.2019.5 .URL:https://truthandtrustonline.\nfiles.wordpress.com/2019/09/paper_5.pdf (visited on 01/15/2024).\n[31] \u201cNo Evidence U.S. Disease Expert Fauci Is \u2018Prepping to Flee Country\u2019\u201d. In: Reuters. Fact Check (May 21,\n2021). URL:https://www.reuters.com/article/idUSL2N2N827A/ (visited on 01/15/2024).\n[32] Paul Resnick, Aviv Ovadya, and Garlin Gilchrist. Iffy Quotient: A Platform Health Metric for Misinfor-\nmation . Oct. 10, 2018. URL:https://csmr.umich.edu/media/docs/UMSI-CSMR-Iffy-Quotient-\nWhitepaper-810084.pdf . Pre-published.\n[33] Frequently Asked Questions . Media Bias/Fact Check. 2023. URL:https://mediabiasfactcheck.com/\nfrequently-asked-questions/ (visited on 01/20/2024).\n[34] What Is Open PageRank? Open PageRank. Oct. 19, 2023. URL:https : / / www . domcop . com /\nopenpagerank/what-is-openpagerank (visited on 01/18/2024).\n[35] New Models and Developer Products Announced at DevDay . OpenAI. Nov. 6, 2023. URL:https://openai.\ncom/blog/new-models-and-developer-products-announced-at-devday (visited on 01/13/2024).\n[36] OpenAI Platform . OpenAI. 2024. URL:https://platform.openai.com/docs/models/gpt-4-and-gpt-\n4-turbo (visited on 01/18/2024).\n11\nLLMs left, right, and center\n[37] Pricing . OpenAI. 2024. URL:https://openai.com/pricing (visited on 01/13/2024).\n[38] Haldun Akoglu. \u201cUser\u2019s Guide to Correlation Coefficients\u201d. In: Turkish Journal of Emergency Medicine\n18.3 (Sept. 2018), pp. 91\u201393. ISSN : 24522473. DOI:10 . 1016 / j . tjem . 2018 . 08 . 001 .URL:https :\n//linkinghub.elsevier.com/retrieve/pii/S2452247318302164 (visited on 01/21/2024).\n[39] Tony Z. Zhao et al. Calibrate Before Use: Improving Few-Shot Performance of Language Models . June 10,\n2021. DOI:10.48550/arXiv.2102.09690 . arXiv: 2102.09690v2 [cs] .URL:http://arxiv.org/abs/\n2102.09690 (visited on 11/21/2023). Pre-published.\n[40] The Economist - Bias and Credibility . Media Bias/Fact Check. 2023. URL:https://mediabiasfactcheck.\ncom/the-economist/ (visited on 01/30/2024).\n[41] The Economist Bias and Reliability . Ad Fontes Media. Sept. 26, 2019. URL:https://adfontesmedia.com/\nthe-economist-bias-and-reliability/ (visited on 01/30/2024).\n[42] The Economist Media Bias Rating . AllSides. Oct. 19, 2012. URL:https://www.allsides.com/news-\nsource/economist (visited on 01/30/2024).\n[43] OpenAI. GPT-4 System Card . Mar. 23, 2023. URL:https://cdn.openai.com/papers/gpt-4-system-\ncard.pdf (visited on 01/11/2024). Pre-published.\n[44] Jason Wei et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models . Jan. 10, 2023. DOI:\n10.48550/arXiv.2201.11903 . arXiv: 2201.11903 [cs] .URL:http://arxiv.org/abs/2201.11903\n(visited on 09/30/2024). Pre-published.\n[45] Giulio Corsi. Evaluating Twitter\u2019s Algorithmic Amplification of Low-Credibility Content: An Observational\nStudy . Sept. 19, 2023. DOI:10 . 48550 / arXiv . 2305 . 06125 . arXiv: 2305 . 06125 [cs] .URL:http :\n//arxiv.org/abs/2305.06125 (visited on 12/18/2023). Pre-published.\n[46] Josh Achiam et al. GPT-4 Technical Report . Dec. 18, 2023. DOI:10.48550/arXiv.2303.08774 . arXiv:\n2303.08774 [cs] .URL:http://arxiv.org/abs/2303.08774 (visited on 01/22/2024). Pre-published.\n[47] Yang Liu. \u201cThe Importance of Human-Labeled Data in the Era of LLMs\u201d. In: Proceedings of the Thirty-\nSecond International Joint Conference on Artificial Intelligence, IJCAI-23 . Ed. by Edith Elkind. International\nJoint Conferences on Artificial Intelligence Organization, Aug. 2023, pp. 7026\u20137032. DOI:10.24963/\nijcai.2023/802 .URL:https://doi.org/10.24963/ijcai.2023/802 .\n[48] Sergei Tilga. Guest Post: LLMs & Humans: The Perfect Duo for Data Labeling . TheSequence. Oct. 23, 2023.\nURL:https://thesequence.substack.com/p/guest-post-llms-and-humans-the-perfect (visited\non 01/29/2024).\n12\nLLMs left, right, and center\nAppendices\nA Dataset Summary Statistics\nVariable Obs. Mean Std. Dev. Min. Max.\nMBFC Bias Scale 5877 0.30 1.45 -3 3\nOpen PageRank rating 5877 4.34 1.17 0 8.1\nGPT Bias Scale 1975 0.17 1.64 -3 3\nDifference (GPT \u2013 MBFC) 1975 -0.08 0.77 -6 4\nAbsolute Diff. (GPT \u2013 MBFC) 1975 0.45 0.63 0 6\nTable 1: Summary statistics.\n\u22123\u22122\u22121 0 1 2 3\nBias Scale (MBFC)0150300450600750900105012001350FrequencyMBFC Bias Scale Distribution\n\u22123\u22122\u22121 0 1 2 3\nBias Scale (GPT)060120180240300360420480FrequencyGPT Bias Scale Distribution\nFigure 5: Histogram of Open PageRank score.\n13\nLLMs left, right, and center\nB Open PageRank scores distribution\nIn terms of popularity, 57 of the sources were either not found in Open PageRank or had a rating of 0\nindicating that they are very obscure. Only 2 were labeled by GPT, one aligned with MBFC ( Three Percent\nNation , far-right) and the other not ( Opelika Auburn News , far-right by GPT and no bias by MBFC). Most\nsources fall near the middle of Open PageRank\u2019s scale. The most popular sources are Medium (rated\nunbiased by GPT-4 and center-left by MBFC), Forbe s (both center-right), and The New York Times (both\ncenter-left).\n0 1 2 3 4 5 6 7 8\nOpen PageRank score0200400600800100012001400FrequencyWebsite Popularity Distribution\nFigure 6: Histogram of Open PageRank score.\n14\nLLMs left, right, and center\nC Sample prompt and response\nThe prompting process for generating the ratings used in the analysis cost US$ 9.99 and took around four\nhours to run in a setting not optimized for querying OpenAI\u2019s API at the fastest allowed rate.\nMain prompt\nSystem message You are an assistant to determine political bias of\nwebsites.\nUser message Rate the political bias of the website \u20192ndvote.com\u2019\nusing the labels \u2019far-right\u2019 \u2019right\u2019 \u2019center-right\u2019\n\u2019no bias\u2019 \u2019center-left\u2019 \u2019left\u2019 \u2019far-left\u2019. If\nthe assistant has no knowledge of the website it\nwill return a rating of -1. Example JSON response:\n{\u2019url\u2019: \u2019example.com\u2019, \u2019political_bias\u2019: \u2019no bias\u2019}\nResponse {\"url\": \"2ndvote.com\", \"political_bias\": \"right\"}\nAdditional prompts\nSystem message You are an assistant to determine political bias of\nwebsites.\nUser message Rate the political bias of the website \u2019economist.com\u2019\nusing the labels \u2019far-right\u2019 \u2019right\u2019 \u2019center-right\u2019\n\u2019no bias\u2019 \u2019center-left\u2019 \u2019left\u2019 \u2019far-left\u2019. If\nthe assistant has no knowledge of the website it\nwill return a rating of -1. Example JSON response:\n{\u2019url\u2019: \u2019example.com\u2019, \u2019political_bias\u2019: \u2019no bias\u2019,\n\u2019reason\u2019: \u2019The example website is known to have no\npolitical bias.\u2019}\nResponse {\"url\": \"economist.com\", \"political_bias\":\n\"center-right\", \"reason\": \"The Economist generally\nadvocates for free markets, internationalism, and\ncultural liberalism, which aligns it with center-right\npolitical positions although it also supports some\nsocially liberal positions.\"}\nTable 2: Sample prompt and response details.\n15\nLLMs left, right, and center\nD Breakdown of categories and % of observations correlation\nCategory Value Obs. Obs. % Spearman\nCountry Australia 38 0.6 .92***\nCanada 331 5.6 .90***\nIndia 46 0.8 .81***\nOthers 452 7.7 .65***\nUnited Kingdom 208 3.5 .89***\nUnited States 4527 77.0 .91***\nUnknown or Invalid Region 275 4.7 .49**\nCredibility rating 1 - Low 1735 29.5 .57***\n2 - Medium 766 13.0 .92***\n3 - High 3374 57.4 .75***\nMedia Type Journal 7 0.1 .97**\nMagazine 168 2.9 .84***\nNews Agency 41 0.7 .28\nNewspaper 1545 26.4 .77***\nOrganization/Foundation 587 10.0 .91***\nRadio Station 139 2.4 .85***\nTV Station 669 11.4 .48***\nWebsite Only 2690 46.0 .90***\nPolitical Stance (MBFC) center 4159 70.8 .61***\nleft 556 9.5 .12*\nright 1162 19.8 .43***\nTable 3: Correlation across categories; *p \u22640.05 **p \u22640.01 ***p \u22640.001\n16\nLLMs left, right, and center\nERegression tables for GPT prediction accuracy (absolute distance between GPT and\nMBFC ratings)\nVariable Coefficient Std. Error t P 95% Confidence Interval\nConstant 0.6099 0.086 7.087 0.000 (0.441, 0.779)\nPopularity (Open PageRank) -0.0311 0.017 -1.852 0.064 (-0.064, 0.002)\nTable 4: Regression model assessing the impact of Popularity on the distance and GPT-MBFC scores. Model:\nOLS Method: Least Squares No. Observations: 1975 R-squared: 0.002 Adj. R-squared: 0.001 F-statistic: 3.429\nProb (F-statistic): 0.0642 Log-Likelihood: -1879.9 AIC: 3764 BIC: 3775.\nVariable Coefficient Std. Error t P 95% Confidence Interval\nConstant 0.5932 0.152 3.895 0.000 (0.295, 0.892)\nPopularity (Open PageRank) -0.0480 0.017 -2.769 0.006 (-0.082, -0.014)\nCountry: Canada 0.1096 0.146 0.752 0.452 (-0.176, 0.395)\nCountry: India 0.2521 0.187 1.351 0.177 (-0.114, 0.618)\nCountry: Others 0.3415 0.129 2.654 0.008 (0.089, 0.594)\nCountry: United Kingdom 0.0726 0.136 0.533 0.594 (-0.195, 0.340)\nCountry: United States 0.0797 0.121 0.658 0.511 (-0.158, 0.317)\nCountry: Unknown or Invalid Region -0.0421 0.162 -0.260 0.795 (-0.360, 0.276)\nTable 5: Regression model assessing the impact of Popularity and Country on the distance and GPT-MBFC\nscores. Model: OLS Method: Least Squares No. Observations: 1975 R-squared: 0.017 Adj. R-squared: 0.014\nF-statistic: 4.951 Prob (F-statistic): 1.45e-05 Log-Likelihood: -1864.3 AIC: 3745 BIC: 3789.\n17\nLLMs left, right, and center\nF Unassigned data across different categories\nCategory Value Obs .Unclassified Obs. Unclassified %\nCountry Australia 38 11 28.9\nCanada 331 274 82.8\nIndia 46 27 58.7\nOthers 452 274 60.6\nUnited Kingdom 208 115 55.3\nUnited States 4527 2961 65.4\nUnknown or Invalid Region 275 240 87.3\nCredibility rating 1 - Low 1735 1316 75.9\n2 - Medium 766 386 50.4\n3 - High 3374 2198 65.1\nMedia Type Journal 7 2 28.6\nMagazine 168 37 22.0\nNews Agency 41 19 46.3\nNewspaper 1545 1245 80.6\nOrganization/Foundation 587 163 27.8\nRadio Station 139 102 73.4\nTV Station 669 454 67.9\nWebsite Only 2690 1861 69.2\nPolitical Stance (MBFC) center 4159 3102 74.6\nleft 556 217 39.0\nright 1162 583 50.2\nPopularity Category None 57 55 96.5\nLow 1413 1318 93.3\nMedium-low 1487 1092 73.4\nMedium-high 1454 964 66.3\nHigh 1466 473 32.3\nTable 6: Number and % of unassigned news sources across categories.\n18\nLLMs left, right, and center\nG Logistic Regression tables for Unassigned Data\nVariable Coefficient Std. Error z P 95% Confidence Interval\nIntercept 6.2827 0.203 30.972 <0.001 (5.885, 6.680)\nPopularity (Open PageRank) -1.2195 0.042 -29.122 <0.001 (-1.302, -1.137)\nTable 7: Logistic Regression Results for the likelihood of values being unassigned with Popularity as predictor.\nModel: Logit Method: Maximum Likelihood Estimation (MLE) No. Observations: 5877 Pseudo R-squared:\n0.1869 Log-Likelihood: -3050.7 LL-Null: -3751.8 LLR p-value: <0.001\nVariable Coefficient Std. Error z P 95% Confidence Interval\nIntercept 10.0303 0.712 14.096 <0.001 (8.636, 11.425)\nPopularity (Open PageRank) -2.1344 0.074 -29.025 <0.001 (-2.278, -1.990)\nCountry: Canada 0.9068 0.459 1.976 0.048 (0.007, 1.807)\nCountry: India 1.0658 0.539 1.978 0.048 (0.010, 2.122)\nCountry: Others 1.1001 0.435 2.532 0.011 (0.248, 1.952)\nCountry: United Kingdom 0.5755 0.456 1.261 0.207 (-0.319, 1.470)\nCountry: United States 0.3679 0.422 0.872 0.383 (-0.459, 1.195)\nCountry: Unknown or Invalid Region 1.2213 0.476 2.568 0.010 (0.289, 2.153)\nCredibility Rating: 2.0 0.5006 0.147 3.415 0.001 (0.213, 0.788)\nCredibility Rating: 3.0 0.3036 0.165 1.836 0.066 (-0.020, 0.628)\nPolitical Stance: Left -1.6673 0.142 -11.718 <0.001 (-1.946, -1.388)\nPolitical Stance: Right -2.6507 0.157 -16.935 <0.001 (-2.958, -2.344)\nMedia Type: Magazine -0.2456 0.486 -0.505 0.614 (-1.199, 0.708)\nMedia Type: News Agency -0.2224 0.593 -0.375 0.707 (-1.384, 0.939)\nMedia Type: Newspaper 1.3554 0.443 3.056 0.002 (0.486, 2.225)\nMedia Type: Organization/Foundation -0.4719 0.448 -1.053 0.292 (-1.350, 0.407)\nMedia Type: Radio Station 0.6880 0.489 1.406 0.160 (-0.271, 1.647)\nMedia Type: TV Station 0.9008 0.447 2.016 0.044 (0.025, 1.777)\nMedia Type: Website Only 0.2355 0.440 0.535 0.592 (-0.627, 1.098)\nTable 8: Extended Logistic Regression Results for the likelihood of values being unassigned with Popularity\nas predictor and multiple control variables. Model: Logit Method: Maximum Likelihood Estimation (MLE)\nNo. Observations: 5877 Pseudo R-squared: 0.3874 Log-Likelihood: -2298.4 LL-Null: -3751.8 LLR p-value:\n<0.001\nVariable Coefficient Std. Error z P 95% Confidence Interval\nIntercept 11.3904 0.340 33.493 <0.001 (10.724, 12.057)\nPopularity (Open PageRank) -2.1046 0.066 -31.734 <0.001 (-2.235, -1.975)\nPolitical Stance: Left -2.2246 0.124 -17.897 <0.001 (-2.468, -1.981)\nPolitical Stance: Right -3.2355 0.119 -27.180 <0.001 (-3.469, -3.002)\nTable 9: Logistic Regression Results for the likelihood of values being unassigned with Political Stance (center\nis intercept) as predictor controlling for Popularity. Model: Logit Method: Maximum Likelihood Estimation\n(MLE) No. Observations: 5877 Pseudo R-squared: 0.3448 Log-Likelihood: -2458.0 LL-Null: -3751.8 LLR\np-value: <0.001\n19", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "LLMs left, right, and center: Assessing GPT's capabilities to label political bias from web domains", "author": ["R Hernandes", "G Corsi"], "pub_year": "2024", "venue": "arXiv preprint arXiv:2407.14344", "abstract": "This research investigates whether OpenAI's GPT-4, a state-of-the-art large language model,  can accurately classify the political bias of news sources based solely on their URLs. Given"}, "filled": false, "gsrank": 160, "pub_url": "https://arxiv.org/abs/2407.14344", "author_id": ["arCHs-gAAAAJ", "35CMl_YAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:30QjRyfnuboJ:scholar.google.com/&output=cite&scirp=159&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D150%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=30QjRyfnuboJ&ei=H7WsaJGVCJXUieoPmrax2A8&json=", "num_citations": 5, "citedby_url": "/scholar?cites=13455039517629301983&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:30QjRyfnuboJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2407.14344?"}}, {"title": "Political bias indicators and perceptions of news", "year": "2023", "pdf_data": "fpsyg-14-1078966 April 24, 2023 Time: 10:45 # 1\nTYPE Original Research\nPUBLISHED 26 April 2023\nDOI10.3389/fpsyg.2023.1078966\nOPEN ACCESS\nEDITED BY\nMark Shephard,\nUniversity of Strathclyde, United Kingdom\nREVIEWED BY\nOzen Bas,\nKadir Has University, T\u00fcrkiye\nMyojung Chung,\nNortheastern University, United States\n*CORRESPONDENCE\nKathryn Bruchmann\nkbruchmann@scu.edu\nRECEIVED 24 October 2022\nACCEPTED 31 March 2023\nPUBLISHED 26 April 2023\nCITATION\nBruchmann K, Vincent S and Folks A (2023)\nPolitical bias indicators and perceptions\nof news.\nFront. Psychol. 14:1078966.\ndoi: 10.3389/fpsyg.2023.1078966\nCOPYRIGHT\n\u00a9 2023 Bruchmann, Vincent and Folks. This is\nan open-access article distributed under the\nterms of the Creative Commons Attribution\nLicense (CC BY). The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with\nthese terms.Political bias indicators and\nperceptions of news\nKathryn Bruchmann1*, Subramaniam Vincent2and\nAlexandra Folks1\n1Department of Psychology, Santa Clara University, Santa Clara, CA, United States,2Markkula Center for\nApplied Ethics, Santa Clara University, Santa Clara, CA, United States\nIntroduction: Recently, a variety of political bias indicators for social and news\nmedia have come to market to alert news consumers to the credibility and\npolitical bias of their sources. However, the effects of political bias indicators\non how people consume news is unknown. Creators of bias indicators assume\npeople will use the apps and extensions to become less biased news-consumers;\nhowever, it is also possible that people would use bias indicators to con\ufb01rm their\nprevious worldview and become more biased in their perceptions of news.\nMethods: Across two studies, we tested how political bias indicators in\ufb02uence\nperceptions of news articles without partisan bias (Study 1, N= 394) and articles\nwith partisan bias (Study 2, N= 616). Participants read news articles with or\nwithout political bias indicators present and rated the articles on their perceived\npolitical bias and credibility.\nResults: Overall, we found no consistent evidence that bias indicators in\ufb02uence\nperceptions of credibility or bias in news. However, in Study 2, there was some\nevidence that participants planned to use bias indicators in the future to become\nmore biased in their future news article selection.\nDiscussion: These data shed light on the (in) effectiveness of interventions against\nblindly consuming biased news and media.\nKEYWORDS\nbias indicator, news media, media literacy, political bias, news bias\nIntroduction\nWith the onset of the \u201cFake News\u201d era and extreme political polarization in the USA,\nseveral tools have been developed to spot and mitigate the e\ufb00ects of news bias, such as\nfact checking websites, apps, and political bias indicators. Political bias indicators tag news\narticles and social media posts with their political leaning and/or their factual accuracy\nby using machine learning algorithms to analyze words and images for the purpose of\nidentifying partisan language (Yu et al., 2008). Sites like allsides.com (Allsides, 2021) and\nmediabiasfactcheck.com (Media Bias/Fact Check, 2021) seek to uncover bias for thousands of\narticles and sources by providing services where users actively search out information about\nnews sources including political bias while they are reading news. More contemporary are\nGoogle Chrome extensions like Nobias.com orTheFactual.com that provide visual labels of\narticle (or source) bias on search results or social media shares before a user has even opened\nthe article. But, there is little information available about how news-consumers actually use\nthese bias indicators. While some may argue that there are clear bene\ufb01ts of transparency\nwith the presence of bias indicators, others question how political bias indicators actually\nin\ufb02uence the way people read and perceive news articles. The purpose of the present paper\nis to test ifand how the presence of political bias indicators changes readers\u2019 perceptions\nFrontiers in Psychology 01 frontiersin.org\nfpsyg-14-1078966 April 24, 2023 Time: 10:45 # 2\nBruchmann et al. 10.3389/fpsyg.2023.1078966\nof news, and to understand how people perceive bias indicators\nthemselves.\nOrganizations that create bias indicators seem to operate under\nthe assumption that bias indicators help readers correct their own\nbiases, or to become more aware of the political leaning of news\nmedia. In e\ufb00ect, they seem to believe that bias indicators might\nhelp readers have greater news media literacy; speci\ufb01cally in that\nthey would develop awareness of the political lean of news even\nif from unfamiliar sources. However, given people\u2019s tendency to\nseek out and agree with pro-attitudinal and worldview con\ufb01rming\ninformation (e.g., Knobloch-Westerwick and Meng, 2009), it is\nalso possible that bias indicators make people more biased in their\nperceptions and selection of news media. The present research\nprovided the \ufb01rst exploratory test of whether the presence of bias\nindicators helps people become more news media literate, or more\nworld-view con\ufb01rming (or neither) in their perceptions of news.\nBias indicators as news literacy cues\nIn general, news media literacy is touted as a promising antidote\nfor the spread of misinformation (e.g., Bulger and Davison, 2018;\nfor a review of other strategies to correct misinformation, see\nLewandowsky et al., 2012) in part because media literacy has\nbeen shown to increase skepticism toward news (Vraga and Tully,\n2018). However, developing news media literacy requires e\ufb00ort;\nspeci\ufb01cally, people who are more news literate have knowledge\nabout the content in the news, but also understand how things like\nnews production or personal beliefs can in\ufb02uence the way news is\ninterpreted (Craft et al., 2016). People who have more news literacy\nalso perceive that they are more in control over the in\ufb02uence that\nmedia might have on them (Maksl et al., 2013). Presumably, the\nvisual tags of a bias indicator could provide a heuristic cue for\nreaders to help increase their news literacy with little to no e\ufb00ort\non their part. As a matter of fact, Nobias describes the need for\ntheir product by saying \u201cto make successful decisions [about how\nvaluable information is] one needs to be more skeptical, more\nvigilant, more rigorous, and invest more labor and time than ever\nbefore\u201d (Nobias, 2021).\nThere is some evidence that this type of news media literacy cue\ncan indeed help people become less susceptible to misinformation.\nFor example, explicitly warning people that they are about to\nencounter false or biased information can be useful in protecting\nagainst persuasion (i.e., \u201cpre-bunking\u201d; Chambers and Zaragoza,\n2001; Compton, 2013; van der Linden et al., 2020; Lewandowsky\nand van der Linden, 2021), but there is evidence that these warnings\nneed to be given repeatedly and be speci\ufb01c about which information\nspeci\ufb01cally is inaccurate (Marsh and Fazio, 2006). One experiment\ntesting perceptions of news articles shared on Facebook found that\narticles tagged with labels of \u201cdisputed\u201d or \u201crated false\u201d were rated\nas less accurate than a control (Clayton et al., 2019; see also Bode\nand Vraga, 2015 col), demonstrating that these kinds of tags can\nincrease scrutiny or skepticism the same way that media literacy\nmight. And, research examining the e\ufb00ectiveness of an intervention\ncreated by Facebook giving readers \u201ctips to spot false news\u201d also\nled to a decrease in perceived accuracy of fake news articles (Guess\net al., 2020); regardless of the political leaning of the articles.Bias indicators as worldview\ncon\ufb01rming\nHowever, another way that bias indicators might in\ufb02uence\npeople\u2019s perceptions of news is by indicating whether a news\narticle will be worldview con\ufb01rming\u2013 by signaling political beliefs\u2013\nor not. A myriad of research has examined people\u2019s tendency\nto seek out pro-attitudinal or worldview-con\ufb01rming information\n(for a review, see Knobloch-Westerwick, 2015), particularly in the\ndomain of politics (e.g., Iyengar et al., 2012). One explanation for\nthis type of con\ufb01rmation bias is people\u2019s political social identity;\ndecades of research suggest that people like and trust political\ningroup members more than outgroup members (e.g., Tajfel and\nTurner, 1979; Byrne, 1997), in part because they view their political\ningroup members as sharing moral values (Bruchmann et al., 2018).\nOther research suggests that it is actually a dislike for the political\noutgroup more than a like for the political ingroup that could\ndrive these e\ufb00ects (i.e., negative partisanship, e.g., Abramowitz and\nWebster, 2018).\nAs such, if people learn that a news article is aligned with\ntheir political beliefs it might lead them to like and trust it more,\nor perhaps \ufb01nding a news article to be counter to their political\nbeliefs might lead people to like and trust it less. Indeed, research\n(Slothuus and de Vreese, 2010) has demonstrated that people are\nmore persuaded by political arguments that are ostensibly from\npolitical ingroups than outgroups, suggesting that articles tagged\nin politically consistent ways would be viewed as more credible or\npersuasive, and corrections to misinformation are more successful\nat changing opinions when they do not threaten people\u2019s worldview\n[see Jerit and Zhao (2020) for a review]. Thus, if a bias indicator\nmakes a newsource seem like it will be worldview con\ufb01rming, it\nmay be viewed as more credible or trustworthy, and people might\nrate stories marked as biased in their own political direction to be\nviewed as more credible.\nPolitical ideology1\nPeople\u2019s own political ideology might in\ufb02uence perceptions of\nnews in other ways, as well. While there is some evidence that\npolitical ingroup bias is equally likely for liberals and conservatives\n(Chambers et al., 2013), and liberals and conservatives are equally\nlikely to try to avoid information from the other side (e.g., Frimer\net al., 2017), other research suggests that conservatives are more\nlikely to reject counter-attitudinal information than liberals (Fessler\net al., 2017).\nAdditionally, trust in news has decreased for U.S. Americans\n(Pew Research Center, 2022)\u2013 something that many bias-\nindicator sites are attempting to address\u2013 and there is consistent\n1 We will use the terms \u201cliberal\u201d and \u201cconservative,\u201d as well as \u201cpolitical\nideology\u201d and \u201cpolitical beliefs\u201d throughout this manuscript when describing\nour own study and hypotheses; but might describe \u201cDemocrats\u201d and\n\u201cRepublicans\u201d when describing other research or our participants\u2019 self-\nidenti\ufb01ed group membership. We recognize that examining political\nideology instead of identity or partisanship might lead to more variability\nin the data (as \u201cliberals\u201d and \u201cconservatives\u201d are not monolithic groups with\na single set of beliefs or identities), but political bias indicators examine\nan ideological slant in news more so than categorizing news as being\n\u201cRepublican\u201d or \u201cDemocrat.\u201d\nFrontiers in Psychology 02 frontiersin.org\nfpsyg-14-1078966 April 24, 2023 Time: 10:45 # 3\nBruchmann et al. 10.3389/fpsyg.2023.1078966\nresearch demonstrating that political conservatives feel much more\nnegatively about news media in general (Pew Research Center,\n2020), and view news media to be liberally biased across the board\n(Mitchell et al., 2020). In fact, research from the Knight Foundation\n(2020) suggests that almost 70% of Republicans have an unfavorable\nview of the media, versus only 20% of Democrats.\nAs such, political bias indicators might in\ufb02uence political\nliberals versus conservatives di\ufb00erently. It is possible that\nconservatives might be more in\ufb02uenced by bias indicators,\nin that they might provide a greater signal of credibility or\nlegitimacy to news; or that they might provide more (less)\ncon\ufb01dence in worldview-(dis)con\ufb01rming information. However,\nbecause of the mixed \ufb01ndings of previous research, these questions\nare exploratory.\nThe present research\nThe goals of the present research are four-fold. First, we aim to\ntest whether and how political bias indicators in\ufb02uence the way U.S.\nAmericans read and perceive news, and whether partisanship plays\na role in this in\ufb02uence. And second, this research will examine how\npeople perceive bias indicators themselves and how people intend\nto use them as part of their news consumption. Speci\ufb01cally, we\nasked the following four research questions.\nRQ1: Do bias indicators in\ufb02uence how politically biased people\nperceive news articles to be?\nRQ2: Do bias indicators in\ufb02uence how credible people perceive\nnews articles to be?\nRQ3: Do people believe that bias indicators are accurate\nrepresentations of article bias?\nRQ4: How do people predict they would use bias indicators in\nthe future?\nAcross two exploratory experiments, participants who\nidenti\ufb01ed as either liberal or conservative read news stories\nthat included bias indicators or not. In Study 1, participants\nread a neutral news story (as determined by NoBias), and in\nsome cases received a bias indicator label that said it was right-\nleaning, left-leaning, or center-leaning. After reading, participants\nrated the credibility and perceived bias of the article. In Study\n2, participants read an article that was actually biased either\nliberally or conservatively (as determined by NoBias), and saw\na bias indicator that either indicated the bias (in the direction\ndetermined by NoBias), or falsely stated that the article was\nunbiased. Participants then rated the credibility and political bias\nof the articles (Studies 1 and 2), their perceptions of the accuracy of\nbias indicators (Studies 1 and 2), and their intentions of how to use\nbias indicators in the future (Study 2).\nThese two experiments allowed for a test between the two\ncompeting hypotheses: bias indicators as shortcuts for media\nliteracy, or bias indicators as world-view con\ufb01rming tools. If bias\nindicators increase media literacy and add an overarching cue of\n(il)legitimacy to the articles, we would predict that articles labeled\nas politically biased (in either direction) would be perceived as less\ncredible than articles labeled as having no bias, or articles without\na label. However, if bias indicators are instead used in a worldview\ncon\ufb01rming way, we predict that partisans would view articles thatwere biased in the same (opposing) political direction and would\nview them as more (less) credible.\nStudy 1\nMaterials and methods\nParticipants and design\nU.S. American MTurk workers ( N= 394) were compensated\n$2.50 to complete a study called \u201cPerceptions of Journalism.\u201d\nParticipants ranged in ages from 19 to 74 ( Mage= 36.8 years,\nSDage= 16.4 years), and a majority were men (65.2%) and\nwhite (76.9%; 9.87% were Black or African-American, 9.64% were\nHispanic or Latino, 6.28% were Asian, and less than 2% were\nother races). Participants\u2019 areas of residence largely followed typical\ndemographic trends in the USA with most participants living in the\nSouth (32.2%), the West (30%) followed by the Northeast (22.6%),\nand the Midwest (22.3%).\nConsistent with the US nationwide demographics (Pew\nResearch Center, 2020), more participants identi\ufb01ed as Democrat\n(42.6%) than Republican (33.2%); however, a smaller portion of\nour sample identi\ufb01ed as Independent or Other (25.2%) than in\nthe general population. Because we were interested in whether the\nbias indicators matched the political ideology of the participants\nor not, the sample was bifurcated to be liberal ( N= 193) or\nconservative ( N= 159). Participants who did not lean right or\nleft ( N= 42) were excluded from analyses, leaving a \ufb01nal sample\nofN= 352.\nBoth studies were approved by the Santa Clara University\nhuman subjects committee (approval number: 19-04-1260). Upon\nstarting the study, participants provided consent, and were\nrandomly assigned to view an article with one of four bias\nindicators: left-leaning bias, right-leaning bias, center bias, or the\nno-indicator control.\nMaterials\nArticles\nThe research team chose three articles covering topics without\na current partisan angle to them that were between 600 and\n900 words. The three articles were determined to be highly\ncredible and \u201ccenter-leaning\u201d by the NoBias Google Chrome\nextension. The Nobias software uses machine learning technology\nto verify patterns of word usage that match signi\ufb01cant phrases\nand text patterns with those used by liberals and conservatives\nin order to establish an ideological \u201cslant index\u201d (Gentzkow and\nShapiro, 2010). The \ufb01rst article was \u201cDid John Wilkes Booth get\naway with murdering President Abraham Lincoln?\u201d from The\nPhiladelphia Inquirer (Colimore, 2019). The other two articles\nwere \u201cSouth Florida, in e\ufb00ort to save tourism industry, may\nspend millions to remove seaweed invading beaches\u201d from Fox\nNews and \u201cRates are low, and mortgages are cheap. So why\naren\u2019t Americans buying more homes?\u201d from CNN (Shirazi, 2019;\nTappe, 2019). In order to control for perceptions of source\nfamiliarity, credibility, or bias, the research team re-formatted all\narticles to appear to be shared by the Quad City Times , a center-\nleaning (per analysis of MediaBias/FactCheck , The Factual, and\nNoBias) and small regional news source in the upper Midwestern\nUnited States.\nFrontiers in Psychology 03 frontiersin.org\nfpsyg-14-1078966 April 24, 2023 Time: 10:45 # 4\nBruchmann et al. 10.3389/fpsyg.2023.1078966\nBias indicators\nEach article (aside from the control group) included a bias\nindicator label. These were created by using images of bias\nindicators from NoBias.com . The image included a paw print\ncolored with either red (leans right), blue (leans left) or purple\n(leans center), and written information about the political slant of\nthe article (leans left, leans right, leans center) and of the source\n(labeled as \u201cvery credible\u201d for all conditions).2The bias indicators\nwere displayed three times on each article: at the top, in the\nmiddle, and at the bottom of the article. See Figure 1 for example\nbias indicators. The control condition did not contain any bias\nindicators.\nProcedure\nFirst, participants read instructions asking them to examine\nhow \u201cpolitical bias indicators\u201d on websites can in\ufb02uence\nperceptions of news articles. They saw sample bias indicators\nin order to learn what the symbols meant, and learned they would\nbe reading articles and answering questions about what they read.\nThen, participants read one of the articles with (or without, for\ncontrol) the No Bias indicator images.\nArticle questions\nIn line with the cover story, after \ufb01nishing the news article,\nparticipants answered multiple choice questions that tested their\ncomprehension or memory of the article (e.g., \u201cWhere does the\nMiami-Dade Parks, Recreation, and Open Spaces Department\nmove excess seaweed?\u201d), and provided their impression of the\narticle (e.g., \u201cHow interesting did you \ufb01nd this article?\u201d 1 = not at\nall, 7 = very interesting ). Participants also indicated how familiar\nthey were with the news source, the Quad City Times (1 = not at all ,\n7 =extremely ).\nDependent measures\nNext, participants rated how politically biased they believed the\narticle to be (1 = not at all biased , 7 = extremely biased ), and in which\ndirection they believed the bias to be (1 = very conservatively biased,\n7 =very liberally biased ). Then, participants rated how credible they\nbelieved the article and source to be (1 = not at all credible , 7 = very\ncredible ; credibility responses were aggregated to form a composite,\na= 0.81). Participants also indicated how accurate they thought the\nbias indicator was (1 = not at all accurate , 7 = extremely accurate ).\nParticipants then responded to a number of exploratory questions\nabout the bias indicators that are not reported here.\nFinally, participants provided demographic data including their\npolitical identity and were debriefed.\nResults\nIn order to conduct our primary analyses, we collapsed across\nnews article conditions3and conducted a series of 2 (participant\n2 The source credibility was held constant across articles in order to\nexplicitly test how bias indicators of the articles themselves in\ufb02uenced\nperceptions of the articles.\n3 While the articles were rated slightly differently on items like interest, the\narticle variable did not interact with our main variables of interest.political ideology: liberal, conservative) \u00024 (bias indicator: control,\nleft, center, right) ANOV As. See Table 1 for means and SDs of all\ndependent variables across experimental conditions and political\nideology.\nRQ1: Do bias indicators in\ufb02uence how\npolitically biased people perceive articles\nto be?\nNo, there was no e\ufb00ect of the bias indicator condition, F<1,\non participants\u2019 perceptions of the bias of the article, nor was there\na bias indicator x participant political ideology interaction, F<1.\nAdditionally, there was no signi\ufb01cant e\ufb00ect of bias indicators on the\nperceived direction of bias, F<1. There was not a signi\ufb01cant e\ufb00ect\nof participant politics, F(1,341) = 2.26, p= 0.134, !p2= 0.007; nor\nwas there a signi\ufb01cant politics x bias interaction, F<1. While this\n\ufb01nding could suggest that our bias indicator manipulation may not\nhave been e\ufb00ective, it also could point to the fact that the articles we\nchose were, indeed, not-biased, and that the readers were picking\nup on that even with the bias indicators.\nRQ2: Do bias indicators in\ufb02uence how\ncredible participants perceived the\narticles to be?\nIt depends on political identity. There was a political\nideology \u0002bias indicator interaction on participants\u2019 credibility\nratings, F(3,344) = 4.09, p= 0.042, !p2= 0.023. Simple e\ufb00ects\ntests suggest that for conservatives, there was not a signi\ufb01cant\nmain e\ufb00ect of bias indicator condition, F(3,344) = 1.19, p= 0.316,\n!p2= 0.022. For liberals, however, there was a marginal e\ufb00ect of\nthe bias indicator, F(3,344) = 2.45, p= 0.065, !p2= 0.037. Post hoc\ncomparisons suggest that, in line with the world-view con\ufb01rming\nhypothesis, articles with right-leaning indicators were viewed as less\ncredible than the control ( p= 0.026), and marginally less credible\nthan those with left-leaning indicators ( p= 0.056) or center-leaning\nindicators ( p= 0.069). Neither the main e\ufb00ect of participants\u2019\npolitical ideology F(1,344) = 2.37, p= 0.124, !p2= 0.007, nor\nof the bias indicator, F(3,344) = 0.86, p= 0.460, !p2= 0.007\nwere signi\ufb01cant. Overall, people found the articles to be credible,\nM= 4.97, SD= 1.24; a one-sample t-test revealed that participants\nrated the articles to be more credible than the midpoint of the scale\n(4),t(393) = 15.58, p<0.001, d= 1.24.\nRQ3: Do people believe that the bias\nindicator is an accurate representation of\nthe article bias?\nMaybe. There was a main e\ufb00ect of the bias indicator condition\non how accurate the participants perceived the bias indicator to be,\nF(1,249) = 7.08, p= 0.001, !p2= 0.054. Participants in the center-\nleaning condition ( M= 5.12, SD= 1.57) thought the indicator\nwas more accurate than those in the right-leaning condition\n(M= 4.29, SD= 1.69, p= 0.001) and the left-leaning condition\nFrontiers in Psychology 04 frontiersin.org\nfpsyg-14-1078966 April 24, 2023 Time: 10:45 # 5\nBruchmann et al. 10.3389/fpsyg.2023.1078966\nFIGURE 1\nSample bias indicators used in Study 1 and 2. Reproduced with the permission of NoBias, LLC.\n(M= 4.35, SD= 1.63, p= 0.001). As the articles we selected were\nall considered \u201ccenter-leaning\u201d (or unbiased), this result suggests\nthat the participants were more likely to believe the bias indicator\nwas accurate when it actually was presenting factually correct\ninformation. There was no signi\ufb01cant e\ufb00ect of participant political\nideology, F(1,249) = 1.00, p= 0.318, !p2= 0.004, or interaction,\nF<1, on perceived accuracy of the bias indicator.\nDiscussion\nOverall, Study 1 did not provide much evidence that bias\nindicators actually in\ufb02uenced the way that people read or perceive\nnews articles. The one exception is that there was evidence in\nsupport of the worldview-con\ufb01rming hypothesis, but unexpectedly\nonly for liberal participants; they reported that articles labeled as\nbiased (especially right-leaning) were less credible. We also saw\nevidence that conservative participants believed articles to have\nmore political bias regardless of condition, consistent with the\nnational trends that Republicans have higher distrust of the media,\noverall (Pew Research Center, 2020). The most notable e\ufb00ects\nfound in Study 1, however, were that people seemed to feel that\nbias indicators themselves were less accurate when they suggested\nthe articles were politically biased. Because the articles we chose\nwere, in fact, center-leaning according to the bias indicators, our\nparticipants correctly identi\ufb01ed the \u201ccenter-leaning\u201d bias indicator\nas being more accurate than the others. This suggests that people\nmight not actually need a bias indicator to know when news is\nunbiased; however, because all articles were actually center-leaning,\nwe do not know whether bias indicators would be important for\ninterpreting actually left or right-leaning articles. Additionally,\nbecause our study was underpowered, it is possible that bias\nindicators do have a small e\ufb00ect on perceptions of news that we\nwere not able to capture.\nStudy 2\nOverview\nThe goal of Study 2 was to test how bias indicators in\ufb02uence\nthe way people read and perceive news articles that actually containpolitical bias. Study 1 used unbiased articles, and participants\naccurately did not perceive the articles labeled as biased to contain\npolitical bias (more than the center-leaning or control), nor did\nthey perceive the bias indicators to be accurate if they signi\ufb01ed\nbias. As such, it is important to test whether bias indicators are\nmore persuasive or believable to readers when the articles actually\ncontain a political slant. In Study 2, participants read one of\ntwo articles on the same partisan topic that were either right or\nleft-leaning, and were told that the articles were either biased\n(correct information) or not (incorrect information) before rating\nthe credibility of the article, and their perceptions of the indicators\nthemselves. A secondary goal of Study 2 was to examine how\npeople believe they would use bias indicators in the future. Finally,\nStudy 2 improved upon Study 1 by increasing statistical power and\nimproving sample size.\nMaterials and methods\nParticipants and design\nU.S. American MTurk workers ( N= 580) were recruited via\nCloud Research and compensated $2.50. We speci\ufb01cally recruited\nparticipants who were pre-screened as Democrats or Republicans\nin an attempt to capture people whose ideologies leaned left or\nright, and to avoid losing as many participants as we did in\nStudy 1. An a priori power analysis (Faul et al., 2007) assuming\na small e\ufb00ect size (as determined by Study 1), determined that\na total sample size of N= 432 was needed to achieve 80%\npower. Because in Study 1 our sample leaned liberal and we lost\nseveral participants for not having a political leaning, we recruited\nadditional participants to compensate. Participants ranged in ages\nfrom 19 to 78 ( Mage= 38.44 years, SDage= 12.08 years), and\nwere majority male (63.9%) and white (75.00%; 16.20% were\nBlack or African-American, 9.64% were Hispanic or Latino, 5.68%\nwere Asian, and less than 2% were other races). Participants\u2019\nareas of residence largely followed typical demographic trends\nin the USA with most participants living in the South (25.9%),\nthe West (17.8%) followed by the Midwest (16.6%), East Coast\n(15.4%), and the Mountain region (4.6%). Inconsistent with the\nnationwide demographics (Pew Research Center, 2020), more\nparticipants identi\ufb01ed as Republican (43.8%) than Democrat\n(36.7%). Despite speci\ufb01cally recruiting participants who had\nFrontiers in Psychology 05 frontiersin.org\nfpsyg-14-1078966 April 24, 2023 Time: 10:45 # 6\nBruchmann et al. 10.3389/fpsyg.2023.1078966\nTABLE 1 Study 1 means and standard deviations across bias indicator conditions and participant political ideology.\nControl Center Right Left\nDependent\nmeasuresLiberals Conservatives Liberals Conservatives Liberals Conservatives Liberals Conservatives\nCredibility 5.24 (1.04) 4.94 (1.17) 5.16 (1.28) 5.22 (1.05) 4.64 (1.52) 5.36 (0.86) 4.75 (1.39) 5.06 (1.24)\nBias 2.66 (1.83) 2.33 (2.00) 2.54 (1.93) 3.20 (2.10) 2.70 (1.92) 3.50 (2.25) 2.98 (1.74) 3.51 (2.20)\nBias direction 4.30 (0.95) 4.32 (1.01) 4.22 (0.91) 4.45 (1.07) 4.04 (1.19) 4.50 (1.13) 4.52 (0.94) 4.51 (1.25)\nIndicator accuracy \u2013 \u2013 5.12 (1.57) 5.23 (1.66) 4.20 (1.68) 4.40 (1.71) 4.23 (1.46) 4.54 (1.85)\nStandard deviations are in parentheses. All ratings made on 1\u20137 scale.\npreviously self-identi\ufb01ed as Democrats or Republicans, a signi\ufb01cant\nportion of our sample identi\ufb01ed as Independent or Other (19.5%).\nAs in Study 1, we bifurcated the political ideology of participants\nin order to include those who identi\ufb01ed as Independent ( N= 263\nliberals, N= 270 conservatives) and dropped participants who\nidenti\ufb01ed as neither ( N= 83), leaving a total sample of 533.\nMost participants (64.3%) reported not having previous\nexperience with bias indicators. Upon starting the study,\nparticipants were randomly assigned to one of six conditions\nin a 2 (Actual Bias: left, right) \u00023 (Bias Indicator: control, center,\nbiased) between-subjects design.\nMaterials\nArticles\nIn Study 2, the research team selected articles on partisan\ntopics; each was about 600 words long. The research team found\none left-leaning and one right-leaning article (according to the\nNoBias chrome extension) on each of three political topics: The\nvisit of Kent State\u2019s \u201cGun Girl\u201d to Ohio University (Rahman,\n2020; Wallace, 2020), the Arizona supreme court case regarding\nbusinesses refusing to serve same-sex couples (NBC Universal\nNews Group, 2019; Parke, 2019), and the Louisiana \u201cheartbeat\u201d bill\nstruck down by the state supreme court (Berry, 2019; Borter and\nDobuzinskis, 2019). As in Study 1, news articles were re-formatted\nto re\ufb02ect the Quad City Times , a center-leaning, small, Midwestern\nregional news source, to standardize the e\ufb00ects of the source of the\narticles.\nBias indicators\nThe bias indicator manipulation was included in the margins at\n3 points on each of the articles for participants not in the control\ncondition. The bias indicators either indicated that the article was\ncenter-leaning, or that the article leaned left or right (whatever the\ntrue lean of the article was). In other words, participants in the bias\ncondition saw the actual bias associated with the article.\nProcedure\nThe procedure was largely the same as in Study 1. First,\nparticipants read their assigned article and answered 4 multiple\nchoice questions as part of the cover story testing their\ncomprehension of the articles. These questions were the same\nacross both the left and right versions of each article; and were\nbroad enough that they could be answered by both articles. Next,\nparticipants responded to the same questions about the politicalbias and credibility of the articles and source as in Study 1 (1 = not\nat all , 7 = extremely ;a= 0.88).\nPerceptions of bias indicators\nParticipants who received bias indicators also rated how\naccurate they thought the bias indicator was, and whether the\nindicators made the article seem more credible (1 = not at all ,\n7 =extremely ).\nAs in Study 1, participants also responded to several exploratory\nquestions about bias indicators that are not discussed here.\nBehavioral intentions\nParticipants also rated their likelihood of choosing a center,\nright, or left-leaning article if they were to use a bias indicator in\nthe future (1 = not at all , 7 = extremely ).\nResults\nTo conduct our primary analyses, we collapsed across news\narticle topic conditions4and conducted a series of 2 (participant\u2019s\npolitical ideology: liberal, conservative) \u00022 (actual bias: left,\nright) \u00023 (bias indicator: control, center, biased) ANOV As. See\nTable 2 for means and SDs of all variables across experimental\nconditions and political ideology.\nRQ1: Do bias indicators in\ufb02uence how\npolitically biased people perceive articles\nto be?\nNo. Only participant political ideology predicted perceptions\nof bias, F(1,521) = 4.50, p= 0.034, !p2= 0.009. Consistent with\nnational trends, conservatives ( M= 4.41, SD= 1.82) perceived more\nbias in the articles than liberals ( M= 4.06, SD= 1.87), overall,\nregardless of whether bias indicators were present.\nHowever, there were e\ufb00ects of the bias indicator on which\ntype of political bias people perceived. Speci\ufb01cally there was a\nbias indicator x participant politics interaction, F(2,509) = 5.12,\np= 0.006, !p2= 0.02. Liberal participants perceived the control\narticles ( M= 4.05, SD= 1.60) to be more conservatively biased than\nthose with a center-leaning label ( M= 4.54, SD= 1.34; p= 0.029).\n4 As in Study 1, the news topic conditions did not interact with any of our\nvariables of interest.\nFrontiers in Psychology 06 frontiersin.org\nfpsyg-14-1078966 April 24, 2023 Time: 10:45 # 7\nBruchmann et al. 10.3389/fpsyg.2023.1078966\nTABLE 2 Study 2 means and standard deviations of dependent measures across bias indicator and political conditions.\nBias indicator Biased Center Control (No indicator)\nDependent measures Actual bias Left Right Left Right Left Right\nCredibility Liberal participants 4.84 (1.23) 5.04 (1.17) 5.15 (1.02) 4.64 (1.15) 4.65 (1.72) 4.72 (1.48)\nConservative participants 4.88 (1.27) 5.50 (1.09) 5.28 (1.29) 5.14 (1.23) 5.04 (1.09) 5.32 (1.90)\nBias amount Liberal participants 4.11 (1.79) 4.47 (1.9) 3.84 (1.89) 4.06 (1.87) 3.70 (1.90) 4.32 (1.87)\nConservative participants 4.20 (1.87) 4.49 (1.53) 4.55 (1.86) 4.33 (1.85) 4.49 (1.93) 4.52 (2.01)\nBias direction Liberal participants 4.69 (1.31) 4.21 (1.73) 4.48 (1.23) 4.64 (1.43) 4.25 (1.50) 3.87 (1.68)\nConservative participants 4.72 (1.31) 4.19 (1.80) 4.24 (1.60) 4.65 (1.55) 4.87 (1.38) 4.95 (1.41)\nIndicator accuracy Liberal participants 4.63 (1.70) 5.17 (1.28) 5.26 (1.43) 4.94 (1.50) \u0000 \u0000\nConservative participants 4.65 (1.74) 5.25 (1.40) 5.10 (1.40) 5.14 (1.58) \u0000 \u0000\nStandard deviations are in parentheses. All ratings made on 1\u20137 scale.\nConservative participants perceived the control articles ( M= 4.91,\nSD= 1.39) to be more liberally biased than those with biased labels\n(M= 4.45, SD= 1.59; p= 0.038).\nRQ2: Do bias indicators in\ufb02uence how\ncredible participants viewed the articles?\nMaybe. There was not a main e\ufb00ect of bias indicator on\nperceptions of credibility, F<1; however, there was a signi\ufb01cant\nbias indicator x actual bias interaction, F(2,521) = 4.15, p= 0.016,\n!p2= 0.016. In line with the media literacy hypothesis, simple\ne\ufb00ects tests revealed that for participants who viewed left-leaning\narticles, there was a main e\ufb00ect of bias indicator, F(2,310) = 2.98,\np= 0.052, !p2= 0.019, such that they found the articles more\ncredible when they had a center-leaning indicator ( M= 5.19,\nSD= 1.11) than biased ( M= 4.86, SD= 1.25; p= 0.046) or control\n(M= 4.84, SD= 1.14; p= 0.029). There was no e\ufb00ect of bias\nindicator for participants who viewed the right-leaning articles,\nF(2,303) = 1.68, p= 0.189, !p2= 0.011. As in Study 1, there was\nalso a main e\ufb00ect of participants\u2019 political ideology on perceptions\nof credibility, F(1,521) = 11.20, p= 0.001, !p2= 0.021. However,\nin this case, conservative participants perceived more credibility\n(M= 5.19, SD= 1.18) than liberals ( M= 4.86, SD= 1.21). No other\nmain e\ufb00ects or interactions were signi\ufb01cant.\nRQ3: Do people believe that the bias\nindicator is an accurate representation of\nthe article bias?\nMaybe. Overall, participants who received bias indicators rated\nthem as more accurate ( M= 4.99, SD= 1.53) than the midpoint\nof the scale (4), t(414) = 66.49, p<0.001, d= 1.53. However, the\nperceived accuracy depended on the actual bias of the article. That\nis, there was a bias indicator x actual bias interaction on perceptions\nof accuracy, F(1,355) = 4.67, p= 0.031, !p2= 0.013. Simple e\ufb00ect\ntests indicated that, among participants who viewed the left-leaning\narticles, there was a main e\ufb00ect of bias indicator, F(1,213) = 9.21,\np= 0.003, !p2= 0.041, such that articles with the center-leaning\nindicator ( M= 5.22, SD= 1.43) were viewed as more accurate thanthose with the left-leaning bias indicator ( M= 4.56, SD= 1.72). For\nparticipants who viewed the right-leaning article, there was not an\ne\ufb00ect of bias indicator on perceptions of accuracy, F<1.\nRQ4: How do people predict they would\nuse bias indicators in the future?\nIn order to test what types of articles people predicted they\nwould use a bias indicator to choose in the future, we conducted\na 3 (Choice: left, center, right) \u00022 (Participant Partisanship:\nliberal, conservative) Repeated Measures ANOV A where choice\nwas within-subjects and politics was between-subjects. There was\na main e\ufb00ect of choice, F(2,722) = 23.83, p<0.001, !p2= 0.062,\nsuch that participants reported a greater likelihood of choosing\ncenter-leaning articles ( M= 5.13, SD= 1.62) than left-leaning\narticles ( M= 4.31, SD= 1.91, p<0.001) and right-leaning articles\n(M= 4.47, SD= 1.97, p<0.001). However, a signi\ufb01cant choice\nx politics interaction also emerged, F(2,722) = 29.21, p<0.001,\n!p2= 0.075; See Figure 2 .\nSimple e\ufb00ects tests reveal that for liberals, there was a main\ne\ufb00ect of Choice, F(2,350) = 24.31, p<0.001, !p2= 0.122. Post\nhoccomparisons reveal that liberals reported being more likely to\nchoose a center-leaning article ( M= 4.99, SD= 1.69) than a left-\nleaning ( M= 4.55, SD= 1.69, p= 0.008), or right-leaning ( M= 3.73,\nSD= 2.03, p<0.001). However, they were also more likely to\nreport wanting to choose a left-leaning article over a right-leaning\narticle ( p<0.001). For conservatives there was also a main e\ufb00ect\nof Choice, F(2,372) = 28.92, p<0.001, !p2= 0.135, such that they\nreported being more likely to choose a center-leaning ( M= 5.26,\nSD= 1.55) or right-leaning ( M= 5.17, SD= 1.64) than a left-\nleaning ( M= 4.08, SD= 2.08, ps<0.001). These \ufb01ndings are\nconsistent with the world-view con\ufb01rming hypothesis; both liberals\nand conservatives reported they would be more likely to use a bias\nindicator to choose an article that leaned in their political direction\nthan one that opposed.\nDiscussion\nStudy 2 provided more evidence that bias indicators do not\ngenerally in\ufb02uence the way that people perceive news articles.\nFrontiers in Psychology 07 frontiersin.org\nfpsyg-14-1078966 April 24, 2023 Time: 10:45 # 8\nBruchmann et al. 10.3389/fpsyg.2023.1078966\nFIGURE 2\nMean likelihood ratings of choosing left, center, or right-leaning articles by political ideology. Error bars represent standard error.\nOne exception is that left-leaning articles were considered to be\nmore credible when they were described as being \u201ccenter-leaning\u201d\nversus left-biased or not including a bias indicator. This \ufb01nding\nis consistent with the media-literacy hypothesis that describing\nsomething as biased makes it seem less credible.\nLike in Study 1, we see some evidence that people didn\u2019t believe\nthe bias indicator; it was considered most believable for people in\nthe \u201ccenter-leaning\u201d condition. Because the articles chosen for this\nstudy actually did contain bias, this was unexpected.\nNotably, we also saw evidence that participants thought they\nwould use bias indicators in the future in a worldview-con\ufb01rming\nway. That is, people reported that they would be less likely to use\nbias indicators in the future to seek out counter-attitudinal news\n(and \u201cbalance\u201d their media diet, as bias indicator sites suggest),\nand would instead be more likely to choose center-leaning or\npro-attitudinal news pieces.\nGeneral discussion\nAcross two experiments, we tested whether bias indicators\nin\ufb02uence readers\u2019 perceptions of news. Study 1, tested how bias\nindicators a\ufb00ect perceptions of center-leaning articles, while Study\n2 tested how bias indicators a\ufb00ect perceptions of actually biased\narticles. Across both studies, we saw little to no evidence that bias\nindicators actually change how people read or perceive articles,\nthough they do a\ufb00ect other associated behavioral factors.\nWe saw some slight evidence that liberals view conservative-\nlabeled (i.e., right-leaning) pieces as less credible, consistent with\na worldview-con\ufb01rming hypothesis, but we also saw some slight\nevidence that people view liberal-leaning articles as more crediblewhen labeled as leaning center, consistent with the framing\nhypothesis. Neither result was consistent across both studies,\nsuggesting that actual political bias (or lack thereof) may be more\nin\ufb02uential to readers\u2019 perceptions than bias indicators.\nNotably, in Study 2, people reported that they would be more\nlikely to use bias indicators in the future to choose center-leaning or\npro-attitudinal articles, consistent with the worldview-con\ufb01rming\nhypothesis. While this \ufb01nding did not suggest that bias indicators\nactually in\ufb02uence how people read or perceive news, it does suggest\nthat it might in\ufb02uence how people choose the news they read, and\nnot how bias indicator creators assume. While most bias indicator\ncompanies develop their products to reduce media bias or help\npeople have a more balanced \u201cmedia-diet, \u201d this data suggests that\ninstead, U.S. Americans might be motivated to use bias indicators\nin order to become more biased and increase their selective partisan\nexposure (e.g., Arendt et al., 2019). Future research should examine\nhow bias indicators in\ufb02uence article choices in this way. Allowing\nparticipants to freely select which articles to read would increase\necological validity by more closely mirroring the functionality of\nbias indicators in the real world.\nLimitations\nOur work is not without limitations. First is our convenient\nMTurk sample; there has been a signi\ufb01cant decrease in quality\nparticipants from MTurk in recent years (see Chmielewski and\nKucker, 2020), namely, in that they do not pay close attention\nto the studies they are participating in. Future research would\nideally be performed in a lab setting to o\ufb00er more control. While\nFrontiers in Psychology 08 frontiersin.org\nfpsyg-14-1078966 April 24, 2023 Time: 10:45 # 9\nBruchmann et al. 10.3389/fpsyg.2023.1078966\ndoing so would encourage a more focused environment, it is\nimportant to acknowledge that real news consumers also may not\nbe highly focused while scrolling through headlines and viewing the\nindicators.\nAdditionally, the articles that were chosen in both studies\nwere real articles, which means that we sacri\ufb01ced experimental\ncontrol over what the participants read in favor of more external\nvalidity. While we relied on the NoBias algorithm to determine\nthe political slant of the articles, future research should pre-\ntest articles with participants to ensure they are perceived as\nneutral or biased.\nAdditionally, while the articles we chose had topics still relevant\nto the time period we ran participants in, they were not necessarily\nas recent as news would be on an active news media site, and we did\nnot capture measures of personal involvement with or familiarity\nwith the di\ufb00erent topics. Also, we purposefully chose articles\nthat did not have broad national coverage with hopes that our\nparticipants did not have pre-existing attitudes about the stories.\nAs a result of both of these factors, participants may not have been\nas engaged in the articles chosen as if they were breaking news or\nmore nationally relevant. Future research should test whether and\nhow these factors in\ufb02uence the use and e\ufb00ects of bias indicators.\nAnother limitation of our work was that we only included\nliberals and conservatives in our sample, and not moderates.\nOur primary interest was on how U.S. partisans respond to bias\nindicators that either matched their political perspective or not, in\nline with the goals of many bias indicator apps and sites. However,\nincluding moderates in future studies could provide insights\nabout the types of media they typically consume and whether\nthey use bias indicators di\ufb00erently than partisans. Presumably,\nmoderates would be more motivated to focus on center-leaning\nnews, or to expose themselves to a balance of left- and right-\nleaning sources.\nConclusion\nPolitical bias indicators for news sites and articles are being\ndesigned as a response situated in the U.S. American news literacy\nmovement. However, the present studies provide initial evidence\nthat political bias indicators might not be having the intended\ne\ufb00ects. The present data suggests that, at best, bias indicators do\nnot actually in\ufb02uence people\u2019s perceptions of news media, and at\nworst, they might actually increase people\u2019s exposure to biased\nnews sources. This preliminary evidence suggests that political bias\nindicator sites or extensions may not predictably drive users to\nbecome less biased in their consumption of news; in fact, they\nmay be counterproductive, which would become an ethical concern\nfor bias indicator design. Organizations that make bias indicators\nmay opt to provide more transparency about not only the purpose\nof these products, but the observed e\ufb00ects. For example, showing\nusers data about how they used the bias indicator tools (e.g., did\nthey read more articles that leaned left, right, or center) might\nhelp people realize their biased behaviors. While more work needs\nto be done to understand these e\ufb00ects, initiatives interested in\ncreating bias indicators or other interventions that signal media-\nbias should take these studies into account when designing and\nimplementing their tools.Data availability statement\nThe raw data supporting the conclusions of this article will be\nmade available by the authors, without undue reservation.\nEthics statement\nThe studies involving human participants were reviewed\nand approved by the Santa Clara University IRB. The\npatients/participants provided their written informed consent\nto participate in this study.\nAuthor contributions\nKB organized the data. KB and AF performed the statistical\nanalysis and wrote the \ufb01rst draft of the manuscript. All authors\ncontributed to conception and design of the study, manuscript\nrevision, read, and approved the submitted version.\nFunding\nThis work was supported by funding from NoBias, LLC, a\nNew York-based startup developing bias indicators for political\nand \ufb01nancial news through browser extensions. This work was\nalso supported through a publication grant from Santa Clara\nUniversity.\nAcknowledgments\nWe thank the RAs in the Social Comparison and Cognition lab\ngroup for piloting these studies, and especially to Rosie Dillon for\nhelp with formatting references. We would also like to thank Glenn\nBaker for providing feedback on drafts of this manuscript.\nCon\ufb02ict of interest\nThe authors declare that this study received funding from\nNoBias, LLC. The funder was not involved in the study design,\ncollection, analysis, interpretation of data, the writing of this article,\nor the decision to submit it for publication.\nPublisher\u2019s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their a\ufb03liated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nFrontiers in Psychology 09 frontiersin.org\nfpsyg-14-1078966 April 24, 2023 Time: 10:45 # 10\nBruchmann et al. 10.3389/fpsyg.2023.1078966\nReferences\nAbramowitz, A. I., and Webster, S. W. (2018). Negative partisanship: why Americans\ndislike parties but behave like rabid partisans. Polit. Psychol. 39, 119\u2013135. doi: 10.1111/\npops.12479\nAllsides (2021). AllSides media bias chart. Media Bias. Available online at: https:\n//www.allsides.com/media-bias/media-bias-chart (accessed July 28, 2021).\nArendt, F., Northrup, T., and Camaj, L. (2019). Selective exposure and news media\nbrands: implicit and explicit attitudes as predictors of news choice. Media Psychol. 22,\n526\u2013543. doi: 10.1080/15213269.2017.1338963\nBerry, D. S. (2019). Louisiana democrat gov. john bel edwards signs \u2018Heartbeat\u2019 bill\ninto law. breitbart . Available online at: https://www.breitbart.com/politics/2019/05/30/\nlouisiana-democrat-gov-john-bel-edwards-signs-heartbeat-abortion-ban-into-law/\n(accessed May 30, 2019).\nBode, L., and Vraga, E. K. (2015). In related news, that was wrong: the correction of\nmisinformation through related stories functionality in social media. J. Commun. 65,\n619\u2013638. doi: 10.1111/jcom.12166\nBorter, G., and Dobuzinskis, A. (2019). Louisiana governor to sign \u2018Heartbeat\u2019\nban, latest move to CURB U.S. abortion rights . Reuters. Available online at:\nhttps://www.reuters.com/article/us-usa-abortion/louisiana-governor-to-sign-\nheartbeat-ban-latest-move-to-curb-u-s-abortion-rights-idUSKCN1SZ2I8 (accessed\nMay 29, 2019).\nBruchmann, K., Koopmann-Holm, B., and Scherer, A. (2018). Seeing beyond\npolitical a\ufb03liations: the mediating role of perceived moral foundations on the\npartisan similarity-liking e\ufb00ect. PLoS One 13:e0202101. doi: 10.1371/journal.pone.\n0202101\nBulger, M., and Davison, P. (2018). The promises, challenges, and futures of media\nliteracy. J. Media Literacy Educ. 10, 1\u201321. doi: 10.23860/JMLE-2018-10-1-1\nByrne, D. (1997). An overview (and underview) of research and theory within\nthe attraction paradigm. J. Soc. Pers. Relationsh. 14, 417\u2013431. doi: 10.1177/\n0265407597143008\nChambers, J. R., Schlenker, B. R., and Collisson, B. (2013). Ideology and prejudice:\nthe role of value con\ufb02icts. Psychol. Sci. 24, 140\u2013149. doi: 10.1177/0956797612447820\nChambers, K. L., and Zaragoza, M. S. (2001). Intended and unintended e\ufb00ects of\nexplicit warnings on eyewitness suggestibility: evidence from source identi\ufb01cation\ntests. Mem. Cogn. 29, 1120\u20131129. doi: 10.3758/BF03206381\nChmielewski, M., and Kucker, S. C. (2020). An MTurk crisis? Shifts in data quality\nand the impact on study results. Soc. Psychol. Pers. Sci. 11, 464\u2013473. doi: 10.1177/\n1948550619875149\nClayton, K., Blair, S., Busam, J. A., Forstner, S., Glance, J., Green, G., et al. (2019).\nReal solutions for fake news? Measuring the e\ufb00ectiveness of general warnings and\nfact-check tags in reducing beliefs in false stories on social media. Polit. Behav. 42,\n1073\u20131095. doi: 10.1007/s11109-019-09533-0\nColimore, E. (2019). Did John Wilkes booth get away with murdering President\nAbraham Lincoln? Philadelphia Inquirer . Available online at: https://www.inquirer.\ncom/news/john-wilkes-booth-lincoln-conspiracy-photo-recognition-20190415.html\n(accessed April 15, 2019).\nCompton, J. (2013). \u201cInoculation theory, \u201d in The SAGE handbook of persuasion:\ndevelopments in theory and practice , 2nd Edn, eds J. P. Dillard and L. Shen\n(Thousand Oaks, CA: SAGE Publications), 220\u2013236. doi: 10.4135/978145221\n8410\nCraft, S., Ashley, S., and Maksl, A. (2016). Elements of news literacy: a focus group\nstudy of how teenagers de\ufb01ne news and why they consume it. Electr. News 10, 143\u2013160.\ndoi: 10.1177/1931243116656716\nFaul, F., Erdfelder, E., Lang, A.-G., and Buchner, A. (2007). G\u0003Power 3: a \ufb02exible\nstatistical power analysis program for the social, behavioral, and biomedical sciences.\nBehav. Res. Methods 39, 175\u2013191. doi: 10.3758/BF03193146\nFessler, D. M. T., Pisor, A. C., and Holbrook, C. (2017). Political orientation\npredicts credulity regarding putative hazards. Psychol. Sci. 28, 651\u2013660. doi: 10.1177/\n0956797617692108\nFrimer, J. A., Skitka, L. J., and Motyl, M. (2017). Liberals and conservatives are\nsimilarly motivated to avoid exposure to one another\u2019s opinions. J. Exp. Soc. Psychol.\n72, 1\u201312. doi: 10.1016/j.jesp.2017.04.003\nGentzkow, M., and Shapiro, J. M. (2010). What drives media slant? Evidence from\nU.S. daily newspapers. Econometrica 78, 35\u201371. doi: 10.3982/ECTA7195\nGuess, A. M., Lerner, M., Lyons, B., Montgomery, J. M., Nyhan, B., Rie\ufb02er, J.,\net al. (2020). A digital media literacy intervention increases discernment between\nmainstream and false news in the United States and India. Proc. Natl. Acad. Sci. U.S.A.\n117, 15536\u201315545. doi: 10.1073/pnas.1920498117\nIyengar, S., Sood, G., and Lelkes, Y. (2012). A\ufb00ect, not ideology: a social identity\nperspective on polarization. Public Opin. Q. 76, 405\u2013431. doi: 10.1093/poq/nfs038\nJerit, J., and Zhao, Y. (2020). Political misinformation. Annu. Rev. Polit. Sci. 23,\n77\u201394. doi: 10.1146/annurev-polisci-050718-032814Knight Foundation (2020). Gallup/Knight poll: Americans\u2019 concerns about\nmedia Bias Deepen, even as they see it as vital for democracy . Available online at:\nhttps://knightfoundation.org/press/releases/gallup-knight-poll-americans-concerns-\nabout-media-bias-deepen-even-as-they-see-it-as-vital-for-democracy/ (accessed\nAugust 4, 2020).\nKnobloch-Westerwick, S. (2015). Choice and preference in media use: advances\nin Selective exposure theory and research . New York, NY: Routledge. doi: 10.4324/\n9781315771359\nKnobloch-Westerwick, S., and Meng, J. (2009). Looking the other way:\nselective exposure to attitude-consistent and counterattitudinal political information.\nCommun. Res. 36, 426\u2013448. doi: 10.1177/0093650209333030\nLewandowsky, S., and van der Linden, S. (2021). Countering misinformation and\nfake news through inoculation and prebunking. Eur. Rev. Soc. Psychol. 32, 348\u2013384.\ndoi: 10.1080/10463283.2021.1876983\nLewandowsky, S., Ecker, U. K. H., Seifert, C. M., Schwarz, N., and Cook, J. (2012).\nMisinformation and its correction: continued in\ufb02uence and successful debiasing.\nPsychol. Sci. Public Interest 13, 106\u2013131. doi: 10.1177/1529100612451018\nMaksl, A., Ashley, S., and Craft, S. (2013). Measuring news media literacy. J. Media\nLiteracy Educ . 6, 29\u201345.\nMarsh, E. J., and Fazio, L. K. (2006). Learning errors from \ufb01ction: di\ufb03culties\nin reducing reliance on \ufb01ctional stories. Mem. Cogn. 34, 1140\u20131149. doi: 10.3758/\nBF03193260\nMedia Bias/Fact Check (2021). Media Bias/Fact Check - About . Available online at:\nhttps://mediabiasfactcheck.com/about/ (accessed July 28, 2021).\nMitchell, A., Gottfried, J., Kiley, J., and Matsa, K. E. (2020). Political polarization\n& media habits . pew research center\u2019s journalism project. Available online at: https:\n//www.pewresearch.org/journalism/2014/10/21/political-polarization-media-habits/\n(accessed February 2, 2022).\nNBC Universal News Group (2019). Arizona artists WIN suit over same-sex wedding\ninvitations . NBCNews.com. Available online at: https://www.nbcnews.com/feature/\nnbc-out/arizona-artists-win-suit-over-same-sex-wedding-invitations-n1055291.\n(accessed September 17, 2019)\nNobias (2021). Nobias FAQ . Available online at:https://nobias.com/politics-faq\n(accessed July 28, 2021).\nParke, C. (2019). Christian artists celebrate religious Freedom win in Arizona\nSupreme Court . Fox News. Available online at: https://www.foxnews.com/faith-values/\narizona-christian-artists-religious-freedom-court (accessed September 18, 2019).\nPew Research Center (2020). 1. Trends in party a\ufb03liation among demographic\ngroups. Pew Research Center - U.S. Politics & Policy . Available online at:\nhttps://www.pewresearch.org/politics/2018/03/20/1-trends-in-party-a\ufb03liation-\namong-demographic-groups/ (accessed August 28, 2020).\nPew Research Center (2022). Trust in America: do Americans trust the news media? .\nWashington, DC: Pew Research Center.\nRahman, K. (2020). Ohio University police deny gun rights ADVOCATE\nKaitlin Bennett\u2019s claims that her visit \"started a riot\" . Newsweek. Available\nonline at: https://www.newsweek.com/kaitlin-bennett-forced-\ufb02ee-ohio-university-\nstudents-protest-appearance-1487759 (accessed February 18, 2020).\nShirazi, E. (2019). South Florida, in e\ufb00ort to save tourism industry, may spend\nmillions to remove seaweed invading beaches. Fox News . Available online at: https:\n//www.foxnews.com/travel/slimy-stinky-seaweed-invades-south-\ufb02orida-beaches\n(accessed August 10, 2019).\nSlothuus, R., and de Vreese, C. H. (2010). Political parties, motivated reasoning,\nand issue framing e\ufb00ects. J. Polit. 72, 630\u2013645. doi: 10.1017/S00223816100\n0006X\nTajfel, H., and Turner, J. C. (1979). \u201cAn integrative theory of intergroup con\ufb02ict, \u201d\ninThe social psychology of intergroup relations , eds W. G. Austin and S. Worchel\n(Monterey, CA: Brooks/Cole), 33\u201347.\nTappe, A. (2019). Rates are low, and mortgages are CHEAP. So why aren\u2019t Americans\nbuying more HOMES? | CNN BUSINESS. CNN . Available online at: https://www.cnn.\ncom/2019/08/09/economy/mortgages-home-buyers/index.html (accessed August 9,\n2019).\nvan der Linden, S., Roozenbeek, J., and Compton, J. (2020). Inoculating against fake\nnews about COVID-19. Front. Psychol. 11:566790. doi: 10.3389/fpsyg.2020.566790\nVraga, E. K., and Tully, M. (2018). News literacy, social media behaviors, and\nskepticism toward information on social media. Inform. Commun. Soc. 24, 150\u2013166.\ndoi: 10.1080/1369118X.2019.1637445\nWallace, D. (2020). Kent state \u2018gun girl\u2019 confronted by protesters at Ohio University.\nFox News. Available online at: https://www.foxnews.com/us/kent-state-gun-girl-\nkaitlin-bennett-ohio-university-protest (accessed February 19, 2020).\nYu, B., Kaufmann, S., and Diermeier, D. (2008). Classifying party a\ufb03liation from\npolitical speech. J. Inform. Technol. Polit. 5, 33\u201348. doi: 10.1080/19331680802149608\nFrontiers in Psychology 10 frontiersin.org", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Political bias indicators and perceptions of news", "author": ["K Bruchmann", "S Vincent", "A Folks"], "pub_year": "2023", "venue": "Frontiers in Psychology", "abstract": "Introduction Recently, a variety of political bias indicators for social and news media have  come to market to alert news consumers to the credibility and political bias of their sources."}, "filled": false, "gsrank": 161, "pub_url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1078966/full", "author_id": ["3uQdmMcAAAAJ", "1dONbEoAAAAJ", ""], "url_scholarbib": "/scholar?hl=en&q=info:Gqjfvb_AiIsJ:scholar.google.com/&output=cite&scirp=160&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D160%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=Gqjfvb_AiIsJ&ei=IbWsaPvmC7_SieoPzJnloAQ&json=", "num_citations": 3, "citedby_url": "/scholar?cites=10054498097861470234&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:Gqjfvb_AiIsJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1078966/pdf"}}, {"title": "Ideological Fragmentation of the Social Media Ecosystem: From echo chambers to echo platforms", "year": "2024", "pdf_data": "arXiv:2411.16826v2  [cs.CY]  4 Jun 2025Ideological Fragmentation of the Social Media\nEcosystem: From echo chambers to echo platforms\nEdoardo Di Martino1*, Alessandro Galeazzi2, Michele Starnini3, 4,\nWalter Quattrociocchi5, Matteo Cinelli5\n1*Department of Social Sciences and Economics, Sapienza University of\nRome, P.le Aldo Moro, 5, 00185, Rome, Italy.\n2Department of Mathematics, University of Padova, Via Trieste, 63,\n35121 , Padova, Italy.\n3Department of Engineering, Universitat Pompeu Fabra, 08018,\nBarcelona, Spain.\n4CENTAI,10138, Torino, Italy.\n5Department of Computer Science, Sapienza University of Rome, Viale\nRegina Elena, 295, 00161, Rome, Italy.\n*Corresponding author(s). E-mail(s): edoardo.dimartino@uniroma1.it;\nContributing authors: alessandro.galeazzi@unipd.it;\nmichele.starnini@upf.edu; walter.quattrociocchi@uniroma1.it;\nmatteo.cinelli@uniroma1.it;\nAbstract\nThe entertainment-driven nature of social media encourages users to engage\nwith like-minded individuals and consume content aligned with their beliefs, lim-\niting exposure to diverse perspectives. Simultaneously, users migrate between\nplatforms, either due to moderation policies like de-platforming or in search of\nenvironments better suited to their preferences. These dynamics drive the spe-\ncialization of the social media ecosystem, shifting from internal echo chambers\nto \u201cecho platforms\u201d\u2014entire platforms functioning as ideologically homogeneous\nniches. To systematically analyze this phenomenon in political discussions, we\npropose a quantitative approach based on three key dimensions: platform cen-\ntrality, news consumption, and user base composition. We analyze 117 million\nposts related to the 2020 US Presidential elections from nine social media plat-\nforms\u2014Facebook, Reddit, Twitter, YouTube, BitChute, Gab, Parler, Scored, and\nVoat. Our findings reveal significant differences among platforms in their central-\nity within the ecosystem, the reliability of circulated news, and the ideological\n1\ndiversity of their users, highlighting a clear divide between mainstream and alt-\ntech platforms. The latter occupy a peripheral role, feature a higher prevalence\nof unreliable content, and exhibit greater ideological uniformity. These results\nhighlight the key dimensions shaping the fragmentation and polarization of the\nsocial media landscape.\n1 Introduction\nSocial media have become a major source of news and opinions for many, reshaping\nhow people access information and engage in public discussions. This shift coincides\nwith a decline in trust in traditional media [1, 2] and is driven by platforms that\nprioritize entertainment and user engagement over informational accuracy [3\u20136]. In\nthis context, the combination of recommendation algorithms, designed to maximize\nengagement, and user preferences may encourage individuals to join tightly-knit groups\nof like-minded peers and consume mostly content that aligns with their pre-existing\nbeliefs, reinforcing shared perspectives and potentially limiting exposure to diverse\nviewpoints [5, 7, 8]. This behavior has been closely linked to heightened exposure to\nhate speech [9\u201312], political polarization [13\u201315], and the formation of ideologically\nhomogeneous clusters of users, called echo chambers [5, 16\u201319]. Over time, the echo\nchamber effect may drive groups towards more extreme positions [20, 21] and influence\nuser behavior across platforms [5, 22, 23] going even beyond digital interactions [24,\n25].\nTo address these challenges, platforms have increasingly implemented moderation\npolicies aimed at managing tight-knit, \u201cproblematic\u201d communities [26]. However, mod-\neration policies, such as de-platforming, often push users toward minimally regulated\nplatforms [16, 23, 27\u201329]. At the same time, users may spontaneously leave plat-\nforms where they find limited alignment with their preferences or feel overly exposed\nto toxic content, seeking alternatives. For example, many users migrated from Twit-\nter to Mastodon [30] or BlueSky [31, 32] following changes in platform management.\nWhen users leave mainstream platforms, they often migrate to less popular digital\nspaces, commonly referred to as \u201calt-tech\u201d or \u201cfringe\u201d platforms [23, 33, 34]. Examples\ninclude Gab, Parler, BitChute, and Rumble, which have emerged as counterparts to\nmainstream services like Twitter, Reddit, YouTube, and Facebook. These platforms\nfrequently market themselves as champions of free speech, host alt-right or extremist\ncontent, and appeal to users who perceive mainstream platforms as ideologically biased\nor overly regulated. While their user bases are relatively small compared to main-\nstream platforms, these alt-tech environments can play a non-negligible role in shaping\nsocietal events, serving as important hubs for specific ideological groups, facilitating\nradicalization, amplifying fringe narratives, contributing to online fragmentation, and\ncreating pathways from online discourse to offline action. For example, Parler was used\nto coordinate the Capitol Hill assault during the 2020 U.S. elections [35, 36]; similarly,\nGab was linked to the Pittsburgh synagogue shooter [37], and Voat became a refuge\nfor QAnon communities banned from Reddit [23].\n2\nBoth spontaneous and forced migrations foster differentiation and specialization\nwithin the social media ecosystem, with platforms increasingly organized around spe-\ncific user communities and shared content preferences [38\u201341]. These shifts represent\na departure from the original vision of the world wide web as a globally intercon-\nnected network where information flows freely across borders [42, 43]. Instead, entire\nplatforms are now defined by ideologically homogeneous communities and aligned con-\ntent, creating niches where opposing perspectives are rarely encountered [39]. This\nscenario\u2014where an entire platform is characterized by a like-minded user base with\nlimited exposure to diverse opinions\u2014lays the foundation for the concept of an \u201cecho\nplatform\u201d, an extension of the echo chamber phenomenon to the level of entire digital\necosystems.\nIn this paper, we introduce three operational steps to characterize the fragmenta-\ntion of the social media ecosystem, providing a methodological basis for identifying\nthe role of different platforms. Using a dataset comprising over 117 million URLs col-\nlected from nine platforms\u2014four mainstream (Facebook, Twitter, Reddit, YouTube)\nand five alt-tech fringe platforms (BitChute, Gab, Parler, Scored, Voat)\u2014and nearly\nsix million unique users, during the 2020 US presidential election, we define three axes\nto characterize platform roles: (i) centrality (central vs. peripheral), (ii) news con-\nsumption (reliable vs. questionable content), and (iii) user base composition (uniform\nvs. diverse). In this context, we adopt the terms mainstream and alt-tech following the\nterminology commonly used in prior literature [27, 34]: we define alt-tech platforms\nas those that have been reported to systematically host alt-right or extremist content.\nHowever, our goal is to move beyond these binary labels, providing a comprehensive\ndata-driven characterization of the social media ecosystem.\nOur analysis reveals a distinct pattern of fragmentation characterized by lim-\nited interactions. Alt-tech platforms exhibit significantly higher levels of ideological\nhomogeneity, with user communities sharing content almost exclusively aligned with\ntheir dominant narratives. Interestingly, user behavior on Reddit more closely resem-\nbles that of alt-tech platforms rather than mainstream ones, featuring a uniformly\ncomposed user base that is, however, left-leaning, and playing a relatively periph-\neral role in cross-platform linking. Furthermore, alt-tech platforms disproportionately\namplify questionable content while showing a notable absence of reliable news sources\ncompared to their mainstream counterparts.\nResults\nWe characterize the social media ecosystem along three key dimensions, providing\ndetailed insights into the structural and functional distinctions between platforms.\n\u2022Centrality: Central vs Peripheral Role. Platforms can be classified as either\ncentral or peripheral based on their position within the broader information ecosys-\ntem. Some platforms could serve as central nodes in the network connecting different\nplatforms, due to their high connectivity. Others may be detached from the core,\noperating as niches where users are less likely to point to other platforms\u2019 content.\n3\n\u2022News Consumption: Reliable vs. Questionable Sources. The reliability of\nnews circulating on platforms may vary significantly. Some platforms primarily fea-\nture content from reliable, well-established news sources that align with journalistic\nstandards. Others display a higher prevalence of questionable content, including\nconspiracy theories and unverified narratives.\n\u2022Political Leaning of Users: Uniform vs Diverse. User base composition can\ndiffer substantially across platforms. Some platforms host a heterogeneous (broad)\nuser base, encompassing individuals with diverse political stances and interests.\nOthers are characterized by a homogeneous (narrow) user base where like-minded\nindividuals share similar ideas and narratives.\nThe systematic characterization of these three dimensions provides a framework\nfor understanding the role of different platforms in the social media ecosystem. Our\nanalysis is based on a comprehensive dataset of URLs collected from nine social media\nplatforms.\nAs shown in Table 1, we include four \u201cmainstream\u201d platforms (Facebook, Twitter,\nReddit, and YouTube) and five \u201calt-tech\u201d platforms (BitChute, Gab, Parler, Scored,\nand Voat). For each platform, we collect only politically relevant posts, especially\nthose related to the 2020 U.S. presidential campaign. Such content was obtained by\nquerying for keywords related to the presidential candidates contained in the posts\n(Facebook, Twitter, Gab, and Parler) or the video titles or descriptions (YouTube and\nBitChute). On Reddit, Voat, and Scored posts are often made of only URLs linking to\nthe relevant piece of news, thus a keyword search would not be applicable. As such, we\nTable 1 : Dataset Statistics and Evaluation Metrics by Platform. For each platform,\nwe report the following metrics: number of unique users ( N), number of URL links\n(nu), number of unique domains linked ( nd), PageRank centrality ( PR), fraction of\nquestionable sources shared ( q), and the variance of the users\u2019 political leaning distri-\nbution ( \u03c32). The figures for the number of unique users, URLs, and unique domains\nare provided after the data preprocessing procedures.\nPlatform N nu ndPR q \u03c32\nFacebook 300k 10.7M 74k 0.07 0.15 0.16\nReddit 59k 320k 9k 0.10 0.03 0.04\nTwitter 5M 103M 114k 0.14 0.16 0.16\nYouTube 12k 512k 12k 0.06 0.14 0.19\nBitChute 2k 107k 4k 0.18 0.22 0.17\nGab 15k 453k 7k 0.16 0.65 0.06\nParler 73k 1.2M 13k 0.10 0.63 0.06\nScored 41k 868k 11k 0.04 0.78 0.04\nVoat 20k 135k 6k 0.13 0.42 0.10\n4\nonly selected content coming from political communities active during the 2020 U.S.\npresidential campaign, after a manual inspection of the posts. The time window for the\nanalysis spans year 2020, with different months covered for each platform depending\non data availability as detailed in the Materials and Methods section. While we report\nresults based on the full data sets, we provide the analysis using uniformly restricted\ntime frames in Figure S5-S9 of the Supplementary Information (SI), showing that the\nresults remain consistent.\nPlatform Centrality\nTo analyze the roles of different platforms within the social media ecosystem, we model\nit as a weighted, directed graph. In this network, nodes represent platforms, and an\nedge from platform ito platform jexists if an account of platform iposted a URL\nlinking directly to platform j. The edge weight represents the number of URL links\npointing from one platform to another. Therefore, the weighted adjacency matrix of\nthis network (see SI, Fig. S1) can provide insights into how platforms redirect users\nand share content within the ecosystem. To account for variations in the size of the\ndatasets collected from each platform (and their popularity), we divide the observed\nweights by the weights expected in a random network which preserves the total number\nof outgoing and incoming URL links for each platform (see section Materials and\nMethods for details).\nFig. 1 displays the ratio of observed edge weights to those expected under the\nnull model. Ratios greater than one indicate that a platform links to another more\nfrequently than expected in a random network, while ratios less than one indicate less\nfrequent linking.\nFig. 1 reveals distinct patterns of inter-platform linking: Twitter consistently\nreceives more URLs than expected from all other platforms (third column), posi-\ntioning it as a central node in the social media ecosystem. Surprisingly, Facebook\nreceives fewer links than expected from other mainstream platforms (first column),\nexcept YouTube, likely due to the prevalence of self-promotional links in video descrip-\ntions. Alt-tech platforms generally receive fewer links than expected by mainstream\nones, and are instead more tightly connected among themselves. Despite this observa-\ntion, the third highest weight with respect to the null model is observed in the edge\nfrom Gab to Reddit, suggesting a potential content-sharing dynamic or migration\npathway between these two platforms. In June 2020, indeed, Reddit banned the sub-\nreddit r/The Donald, a forum dedicated to President Trump\u2019s supporters with nearly\n800,000 users, due to repeated violations of the platform\u2019s rules against harassment,\nhate speech, and content manipulation [44]. Given the substantial presence of Trump\nsupporters on Gab, this could suggest that links from Gab to Reddit were possibly\nposted as a means of mockery or demotion.\nBitChute is linked much more than expected by all fringe platforms and slightly\nless by mainstream ones (fifth column), while the opposite is true for YouTube (fourth\ncolumn). Furthermore, we observe a strong relationship between BitChute and Gab, as\nindicated by the two edges having the highest weights in the network. Finally, Scored\ndoes not receive incoming links from six of the eight other platforms, likely due to its\n5\n1\n1.1\n0.52\n0.640.67\n0.510.88\n0.689.880.88\n0.87\n0.071.57\n0.120.53\n2.535.861.81\n1.68\n5.243.97\n7.242.89\n2.41.990.42\n0.54\n3.333.92\n1.30.89\n38.410.06\n0.69\n20.28\n0.9\n8.432.19\n15.123.241.23\n0.7\n6\n0.44\n1.760.5\n14.111.45\n1.940.16\n1.1\n27.91\n2.820.19\n1.950.671.80.79\n0.42\n33.89\n9.328.18\n3.170.90\n00\n0000\nScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScored\nToFromFig. 1 :Centrality. Rescaled adjacency matrix showing the ratio between the\nobserved and expected number of URLs pointing from one platform to another. Green\n(red) cells indicate values greater (smaller) than one. A value of 0 indicate no observed\nURLs.\nlimited popularity and low exposure, underscoring its peripheral role in the broader\necosystem.\nTo better quantify the relative importance of each platform within the ecosystem,\nwe computed their PageRank centrality from the rescaled adjacency matrix shown in\nFig. 1. The results presented in Table 1 reveal that BitChute, Gab, and Twitter exhibit\nthe highest scores, reflecting their prominent roles in the network. Voat, Parler, and\nReddit display lower centrality scores but still maintain relatively strong connection\npatterns. Facebook, YouTube, and Scored, on the other hand, rank as the least central\nplatforms in the ecosystem. Therefore, after accounting for popularity, we found that\n6\nnot all mainstream platforms are central, while fringe platforms like Gab and BitChute\nare not necessarily peripheral.\nNews Consumption\nNext, we investigate whether the type of news and content shared differs signifi-\ncantly across the nine platforms by analyzing the news domains\u2019 political bias. To\nthis aim, we use Media Bias/Fact Check (commonly referred to as MBFC https:\n//mediabiasfactcheck.com), a widely referenced news rating agency that provides\npolitical bias and reliability labels for news outlets and other information producers\n[5, 45, 46]. We extract the domains from the URLs shared across the nine platforms and\nmatch them with the political bias and reliability information obtained from MBFC.\nDomains not classified by MBFC are labeled as \u201cunreported\u201d, though this designation\ndoes not necessarily imply an absence of political bias. Following previous research [46],\ndomains categorized as \u201cextreme-left\u201d are excluded from the analysis due to their\nnegligible presence in the dataset (see Materials and Methods for details regarding\nthe labeling of news sources and SI, Table S5, for a table showing the percentage of\ndomains matched with a MBFC rating).\nFig. 2 illustrates users\u2019 news consumption across platforms, showing the propor-\ntion of URLs directing to domains with differing political leanings, as categorized by\nMBFC. A clear pattern emerges: mainstream platforms such as Facebook, Reddit, and\nTwitter tend to skew toward left-leaning content, while alt-tech platforms\u2014including\nGab, Parler, Voat, and Scored\u2014predominantly link to right-leaning content. Voat\nstands out among alt-tech platforms by directing a notable portion of its traffic to\nleft- or left-center-leaning domains. However, 89.5% of these links were shared across 6\nof the 60 distinct subverses analyzed, most of which (e.g., v/AnonAll, v/GreatAwak-\nening) align with alt-right or conspiracy groups (a detailed list of these subverses is\navailable in Table S4 of the SI). This pattern suggests that these links may have been\nshared in an ironic or derogatory manner. YouTube and BitChute, despite being fil-\ntered for political content, heavily link to \u201cunreported\u201d domains, reflecting their role as\nplatforms that host a broad range of less conventional or unvetted content. Addition-\nally, on BitChute, a substantial share of content from hyper-extremist sites is driven\nby a small group of highly active users. In some cases, these users are only a few dozen\nor hundreds, yet they are responsible for thousands of links, creating a disproportion-\nate volume of extremist content on the platform. This finding aligns with the concept\nof \u201cvocal minorities\u201d [47], where a small but highly active subset of users exerts an\noutsized influence on the platform by driving a significant share of its activity [48].\nTo further illustrate these patterns, Table 2 reports each platform\u2019s ten most fre-\nquently linked domains. While Facebook and Twitter often feature sources with a\nleft-center bias, such as CNN, The New York Times, or The Washington Post, Red-\ndit displays a more homogenous profile as no right-biased sources appear among the\nmost linked domains. In contrast, platforms such as Gab, Parler, Scored, and Voat\ndirect a substantial proportion of their traffic to sources that are both right-biased\nand have low reliability. Scored, for instance, channels 58% of its external traffic to\n\u201cPatriots.Win\u201d, an online forum created following the ban of the \u201cr/The Donald\u201d sub-\nreddit in June 2015 [34]. This forum is known for using unreliable sources and a lack\n7\n0.01 0.12 0.010.03 0.120.01 0.680.1 0.05 0.090.22 0.130.06 0.36\n0.03 0.34 0.020.04 0.370.05 0.16\n0.02 0.27 0.010.04 0.430.04 0.190.14 0 0.160.48 0.040.06 0.12\n0.02 0.65 00.03 0.170.02 0.10.06 0.07 0.130.41 0.120.06 0.14\n0.06 0.17 0.050.16 0.270.08 0.20.03 0.02 0.060.12 0.120.06 0.59\nScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nleft\nleft_centercenter\nright_centerright\nextreme_rightunreported\nToFromFig. 2 :News Consumption . Fraction of URLs directing to domains with differing\npolitical leanings, as categorized by MBFC. Domains without a political bias identified\nby MBFC are categorized as \u201cunreported\u201d. News consumption on mainstream plat-\nforms is skewed toward left-leaning content, while alt-tech platforms predominantly\nlink to right-leaning content. YouTube and BitChute prominently share \u201cunreported\u201d\ndomains.\nof transparency and moderation [49, 50]. Although both YouTube and BitChute fre-\nquently link to unreported domains, BitChute\u2019s links often direct users to alternative\nnews websites, including domains associated with the american alt-right.\nNext, we analyze the similarity between the news diets across platforms. To this\naim, we identify the top 20 most-linked domains for each platform and compute the\nweighted cosine similarity between the domain vectors for all pairs. This method\nquantitatively measures how closely platforms align in their shared content.\nThe results, illustrated in Fig. 3, reveal two distinct cliques characterized by high\nsimilarity scores. The first clique consists of the mainstream platforms Facebook, Twit-\nter, and Reddit, all exhibiting cosine similarity scores above 0 .7. The second clique\nincludes the alt-tech platforms Gab, Voat, and Parler, with similarity scores ranging\nfrom 0 .73 to 0 .88. See SI for the full cosine similarity matrix and for a robustness\ncheck expanding the threshold of the top 20 most-linked domains (Fig. S2, S3).\n8\nTable 2 : Most shared domains by platform. Name and percentage of links toward the\nten domains most linked by each platform. Each domain is color coded to represent\nits political bias as follows: left,left-center ,center ,right-center ,right ,extreme-\nright ,unreported . An asterisk following a domain\u2019s name indicates that it is flagged\nas a questionable source.\nFacebook Reddit Twitter YouTube BitChute\nDomain % links Domain % links Domain % links Domain % links Domain % links\nCNN 2.35 The Hill 6.00 The New York Times 6.90 Fox News* 8.09 Infowars* 3.14\nFox News* 2.30 CNN 5.82 The Washington Post 6.56 MSNBC 1.97 Liberty Classroom* 2.27\nThe Hill 1.79 The Washington Post 4.73 CNN 5.72 Tv9Hindi 1.86 Rebel News* 1.65\nThe Washington Post 1.72 The New York Times 3.95 Fox News* 3.22 The Young Turks 1.77 Tuttle Twins 1.65\nThe New York Times 1.65 Politico 2.79 Politico 2.34 CNBC 1.55 The Gateway Pundit* 1.56\nNBC News 1.57 Reuters 2.37 The Hill 2.30 PBS NewsHour 1.47 TimCast 1.53\nYahoo News 1.50 Associated Press 2.18 NBC News 2.27 Fox Business 1.47 Martin Brodel 1776 1.51\nBreitbart* 1.46 The Guardian 2.10 Breitbart* 2.23 NBC News 1.10 PeteLive 1.48\nDaily Wire 1.22 NBC News 2.07 Raw Story 2.19 France 24 0.91 TurleyTalks* 1.45\nMSN 1.04 Business Insider 1.93 The Gateway Pundit* 1.92 CBS News 0.67 Banned* 1.17\nGab Parler Scored Voat\nDomain % links Domain % links Domain % links Domain % links\nThe Gateway Pundit* 13.71 The Gateway Pundit* 13.74 Patriots* 58.24 The Gateway Pundit* 7.47\nBreitbart* 12.07 Fox News* 11.44 Breitbart* 3.39 Breitbart* 5.58 Left\nFox News* 4.09 Breitbart* 8.56 MAGA* 2.64 Zero Hedge* 4.91 Left Center\nZero Hedge* 2.47 The Epoch Times* 4.94 The Gateway Pundit* 2.33 Fox News* 2.49 Center\nThe Epoch Times* 2.43 Western Journal* 2.44 Fox News* 1.65 DailyMail* 1.92 Right Center\nInfoWars* 2.29 New York Post 1.74 Zero Hedge* 0.99 The Hill 1.91 Right\nGNews* 1.97 NewsMax* 1.32 New York Post 0.92 RT News* 1.73 Extreme Right\nNew York Post 1.70 Town Hall* 1.31 Washington Examiner* 0.73 New York Post 1.57 Unreported\nThe Hill 1.31 Just The News* 1.26 DailyMail* 0.70 CNN 1.44\nDailyCaller 1.19 CounterGlobalist 1.16 DailyCaller 0.56 Washington Examiner 1.35\nThe similarity between mainstream and alt-tech platforms generally does not\nexceed 0 .5, with Facebook and Twitter showing slightly higher similarity to specific\nfringe platforms. This last observation may indicate that news consumption with-\ning certain echo chambers on Facebook and Twitter partially overlaps with those on\nfringe platforms. Reddit appears to be the mainstream platform most dissimilar to\nalt-tech platforms, exhibiting the lowest similarity in news consumption. This aligns\nwith the tendency of Reddit users to predominantly share left-leaning content, while\nextreme-right sources are absent and right-center or right-leaning content is relatively\nuncommon. YouTube shows moderate similarity to other mainstream platforms, par-\nticularly Twitter, but also with some alt-tech ones, such as Parler. Finally, BitChute\nand Scored are dissimilar to all other platforms. This difference may be attributed to\nthe extreme nature of the content frequently shared on BitChute and the limited popu-\nlarity and reach of Scored, which results in unique news diets that diverge significantly\nboth from mainstream and other alt-tech platforms.\nFig. 3 also highlights the fraction of questionable domains (as defined by MBFC)\nshared by each platform. Mainstream platforms share a relatively small fraction of\nunreliable sources, while alt-tech platforms display significantly higher proportions\nof such content, reinforcing the distinct content dynamics within the social media\necosystem. These results are reported in more detail in Table 1, showing the fraction of\nquestionable sources shared by users on each platform. Alt-tech platforms Gab, Parler,\nand especially Scored exhibit the highest proportions. Voat shows a lower fraction, and\nReddit stands out for its smallest fraction of questionable sources shared (0 .03). These\nfindings underscore the distinction between alt-tech platforms, where questionable\nsources appear to circulate more freely, and mainstream platforms, where such content\nconstitutes only a small proportion of shared material. BitChute shows a small value of\n0.22, differing from the general trend observed among alt-tech platforms, but aligning\nwith the platform\u2019s propensity to host a significant volume of unreported domains.\n9\nRedditTwitterYouTube\nBitChuteGabParler\nVoatScored\nFacebook\nNews Consumption\nQuestionable Reliable UnreportedFig. 3 :Platform similarity. Cosine similarity network based on the platforms\u2019 20\nmost linked domains. The size of the different nodes is proportional to the volume of\nlinks shared in the platform, while the colors of the pies indicate the fraction of ques-\ntionable or reliable content shared. We observe two cliques with high similarity: one\nmade up of mainstream platforms (Facebook, Twitter, Reddit) that share a majority\nof reliable news sources, and one made up of alt-tech ones (Gab, Parler, Voat) shar-\ning a higher fraction of questionable sources. Scored, BitChute, and, to an extent,\nYouTube remain fairly separated from the rest of the platforms.\nUser base\nTo assess the diversity of the user bases across platforms, we evaluate the distribu-\ntion of users\u2019 political leaning. We infer the political leaning of active users (i.e., users\nwho shared 10 or more URLs toward domains with an associated political bias) based\non their posting activity. We assign a numerical score to news media, ranging from\n\u22121 (extreme-left) to +1 (extreme-right), excluding unreported sources. The politi-\ncal leaning of each user is then defined as the average of the leanings of the news\ndomains they post (see section Materials and Methods for further details). We note\nthat YouTube and BitChute operate on different principles compared to the other\nplatforms considered. As video-sharing platforms, they lack traditional feeds or direct\nuser interactions. Consequently, our estimation of political leanings on these platforms\nreflects the tendencies of active content producers rather than user consumption pat-\nterns. Additionally, we acknowledge that using the average as a summary statistic\n10\n0.000.050.100.150.20\n\u22121.0\u22120.50.00.51.0Relative FrequencyFacebook\n0.00.10.20.3\n\u22121.0\u22120.50.00.51.0YouTube\n0.00.10.20.3\n\u22121.0\u22120.50.00.51.0Reddit\n0.00.10.20.3\n\u22121.0\u22120.50.00.51.0Parler\n0.0000.0250.0500.0750.1000.125\n\u22121.0\u22120.50.00.51.0Voat\n0.00.10.20.3\n\u22121.0\u22120.50.00.51.0Relative FrequencyTwitter\n0.000.050.100.15\n\u22121.0\u22120.50.00.51.0BitChute\n0.000.050.100.150.20\n\u22121.0\u22120.50.00.51.0\nUser LeaningGab\n0.00.10.20.30.40.5\n\u22121.0\u22120.50.00.51.0Scored\nFrac. of questionable\nsources shared\n00.250.50.751Fig. 4 :User base composition. Distributions of users\u2019 political leaning for each\nplatform. Each unique user gets assigned a leaning score between \u22121 and +1, according\nto their posting activity. The bars are colored according to the number of \u201cques-\ntionable\u201d sources shared by users of a specific leaning. We notice how Facebook and\nTwitter have a polarized user base with two distinct groups, one sharing mostly reli-\nable content and the other sharing mostly content with low factual reporting. The\nsituation is more homogeneous regarding all of the fringe platforms, and the remain-\ning two mainstream platforms.\nmay fail to capture behaviors more evenly distributed across a bias spectrum: as such,\na robustness check showing the distribution of the variance of the political leaning\nscores assigned to users is available in SI (Fig. S4). The analysis of the variance at\nthe user level shows that most of them tend to engage with content from a narrow\nideological range, making the average a quite representative summary statistic of the\ngeneral trend.\nFig. 4 illustrates the political leaning distribution of users in the nine platforms.\nWe note that the user bases of Facebook and Twitter are made up of two distinct\ngroups that span the entire ideological spectrum, resulting in a bimodal distribution\nof political leaning. The same pattern is not observed on some alt-tech platforms\n(Gab, Parler, and Scored), where users are concentrated within a narrower leaning\ninterval, forming a homogeneous, right-leaning user base. Interestingly, Reddit exhibits\na skewed leaning distribution, similar to alt-tech platforms, but concentrated on left\nleaning content, as noted in previous work [51, 52]. Voat and BitChute display a\nslightly more heterogeneous user base compared to other alt-tech platforms. For Voat,\nthis heterogeneity likely stems from the significant portion of traffic redirected toward\nleft-biased sources by alt-right groups, as previously observed. For BitChute, the high\nproportion of unreported sources on the platform likely contributes to its heterogeneity.\nSpecifically, since we only include users who shared at least 10 URLs with an associated\npolitical bias in our computation, this criterion captures only a small subset of the\nuser base.\nExpanding on BitChute, users with a political leaning below 0 (left-leaning) make\nup approximately 12% of the total user base but share, on average, 15 fewer links than\n11\nright-leaning users (leaning above 0) and 30 fewer links than those with a leaning equal\nor above 0 .5. This suggests that left-leaning users are generally less active in sharing\ncontent, resulting in a smaller contribution to the circulation of news on the platform.\nThese dynamics underscore an asymmetry in content production and dissemination\nacross ideological groups, highlighting the influence of vocal communities in shaping\nplatform-wide narratives.\nThe observations regarding the diversity of the user bases can be quantified by\nmeans of the variance of the political leaning distribution, \u03c32, reported in Table 1.\nPlatforms such as Twitter, Facebook, YouTube, BitChute, and Voat to some extent,\nexhibit higher variance, indicating wide spread user leanings and reflecting a diverse\nuser base, while the remaining platforms demonstrate more homogeneous user bases.\nWhen considering the reliability of the sources shared, we observe that users with\nright-leaning and extreme-right political orientations share a higher proportion of ques-\ntionable content. However, this pattern aligns with established correlations reported\nby MBFC and is not specific to our study.\nConclusions\nThis study characterized fragmentation in the social media ecosystem related to the\n2020 US presidential election, and introduced the concept of \u201cecho platforms\u201d, i.e.,\nentire social media platforms that operate as self-contained echo chambers, reinforc-\ning homogeneous beliefs and isolating users from opposing viewpoints. Echo platforms\nrepresent an evolution of traditional echo chambers, where the platform structure\nfosters ideological uniformity. Our framework categorizes platforms across three dimen-\nsions\u2014platform centrality, content reliability, and user base homogeneity\u2014providing\na systematic approach to understanding these dynamics within the broader context\nof social media fragmentation.\nSuch analysis provides a better characterization of the differences between main-\nstream platforms (e.g., Facebook, Twitter) and alt-tech ones (e.g., Gab, Parler). In\nparticular, we uncover that Reddit shares some features with mainstream platforms,\nlike the high reliability of posted news media, but others with fringe platforms, such as\na uniform user composition and a relatively peripheral role in cross-platform linking.\nOur findings suggest that, despite being a mainstream social media platform, Reddit\nfunctions more like an echo-platform than others.\nThis systemic shift from segregated communities to platform-wide segregation\nhighlights the role of moderation policies, user self-selection, and engagement-driven\nbusiness models in shaping the rise of echo platforms. Our findings contribute to\nshedding light on the factors influencing digital polarization, showing how alt-tech\nplatforms attract marginalized or de-platformed communities from mainstream spaces,\ncreating distinct niches that intensify ideological separation. This underscores the need\nto reevaluate moderation strategies, as current policies may unintentionally exacerbate\nthe fragmentation of the social media ecosystem by fostering the growth of insu-\nlated echo platforms. Such dynamics challenge the digital public sphere by limiting\nconstructive dialogue and deepening ideological divides.\n12\nWhile our framework provides robust tools for identifying and analyzing echo plat-\nforms, the study has limitations. The dataset, though extensive, reflects a specific\nperiod marked by significant political events, such as elections. This context offers\nvaluable insights into platform behavior during critical societal moments but may\nonly partially capture ongoing changes in the social media landscape. Future research\ncould extend this work by integrating more recent data and exploring cross-cultural\nor longitudinal variations in platform dynamics, as well as investigating the impact\nof coordinated actions and in the fragmentation of online spaces, and, where feasible,\nincorporating cross-platform user identification to enhance the depth of analysis.\nOur results offer a foundation for examining the socio-political impacts of echo\nplatforms, particularly in contexts where public discourse and social cohesion are at\nrisk. Understanding and mitigating the societal effects of echo platforms is essential for\ndeveloping policies that balance open expression with fostering a diverse and cohesive\npublic sphere.\nThis study highlights the influence of echo platforms in shaping today\u2019s social\nmedia landscape, particularly their role in internet fragmentation and societal polar-\nization. By offering a detailed quantitative approach, we aim to advance understanding\nof these platforms and inform strategies to mitigate the adverse impacts of digital\nfragmentation on society.\nMethods\nData\nIn this section, we describe the procedures for data collection and preprocessing, along\nwith details about the datasets used in our study. All of the datasets were filtered\nto retrieve politically relevant content pertaining to the 2020 U.S. election, either by\nperforming a keyword search, or by selecting relevant political communities where a\nkeyword search was not feasible. The set of keywords is consistent among the datasets\nin which they were utilized (with minor variations), and based on the presidential\ncandidates\u2019 names. During data preprocessing, we manually excluded URLs that were\nnot relevant to our analysis. This included self-links (i.e., a platform linking to itself),\nURLs directing to social media platforms not included in the datasets (e.g., Snapchat,\nTikTok), video streaming services (e.g., Vimeo, Dailymotion), financial services (e.g.,\nPayPal, Venmo), music streaming services (e.g., Spotify, SoundCloud), and tech plat-\nforms or services (e.g., Google Suite, Streamlabs). Additionally, we removed a range\nof URLs that did not fit these broad categories but were similarly uninformative for\nour purposes, such as links to Amazon, Steam, Coinbase, and NASA.\nFacebook: The URLs analyzed were extracted from 21 million Facebook posts\ncollected using the CrowdTangle service. These posts were identified based on searches\nusing predefined keyword lists: L1: {Trump, trump, #donaldtrump, #trump }and L2:\n{Biden, biden, #joebiden, #biden }, spanning the period from May to November 2020.\nReddit: We utilize URLs extracted from a comprehensive collection of posts\npublished in the subreddit r/Politics between January and December 2020, totaling\n13\nnearly 4.7 million posts. These posts were collected via the Pushshift dataset [53].\nA subreddit is a user-created community focused on specific topics and governed by\nits own rules, where members can subscribe, share posts, comment, and upvote or\ndownvote content.\nTwitter: The dataset [54], based on information retrieved from Flamino et\nal.[46], was obtained using the Twitter Search API, with the names of the two\nU.S. presidential candidates from the 2020 election as keywords (consistently with\nFacebook). It includes 174 million tweets posted between June and November 2020.\nYouTube: We use URLs extracted from the descriptions of 270,000 YouTube\nvideos, collected using the YouTube Data API between June and December 2020. The\nvideos were identified through searches based on the same keywords utilized for Face-\nbook. For each video, an additional search was conducted by crawling the network of\nrelated videos, as suggested by YouTube\u2019s algorithm. From the gathered dataset, we\nretained only videos containing Trump \u2014trump (orBiden \u2014biden , respectively) in\nthe title or description.\nBitChute: BitChute is a British alt-tech video hosting platform known for its\nlower moderation efforts compared to its more popular counterpart, YouTube. The\nplatform focuses heavily on news and politics and is associated with a significant\namount of hate speech in both videos and comment sections [55]. For our analysis, we\nuse the MeLa BitChute Dataset [56], which provides a near-complete scrape of the\nplatform from 2019 to 2021, from which we filtered the data to retain only the 1.1\nmillion videos uploaded in 2020. We further refined the dataset by performing a key-\nword search on the video titles. The keywords used for filtering were {trump, biden,\njoebiden, donaldtrump, donald, trump2020, biden2020 }, resulting in the retention of\n40 thousand videos.\nGab: Gab is an alt-tech microblogging platform structured similarly to Twitter but\nwith minimal content moderation, promoting free speech and Christian values [57, 58].\nWith its predominantly far-right user base, Gab has been described as a safe haven\nfor neo-Nazis, members of the American alt-right, Trump supporters, and conspiracy\ntheorists [59, 60]. The platform has also been repeatedly linked to online radicalization\nand real-world violent events [37, 61].\nThe data for this study was collected in two phases. From June 1, 2020, to Octo-\nber 23, 2020, posts were downloaded using Gab\u2019s general stream, capturing all posts\ngenerated during this period. After October 23, due to the deprecation of the API\nendpoint, data was collected using the timelines of 930,000 users identified in the\nfirst phase. The dataset was then filtered using the keywords {\u201ctrump\u201d, \u201cbiden\u201d },\nresulting in 467,000 posts made between June 1 and December 1, 2020.\nParler: Launched in 2018, Parler is a platform with functionalities similar to\nTwitter, marketed as a free-speech-focused alternative. It gained mainstream attention\nfollowing the storming of Capitol Hill in 2021, as it was one of the platforms allegedly\n14\nused to incite and plan the attack [35, 36]. Shortly after, Parler was removed from the\nGoogle Play Store and Apple App Store and subsequently suspended by its hosting\nprovider, Amazon AWS. At the time of writing, plans to relaunch Parler\u2019s services\nhave been announced, though the exact timeline remains unclear.\nThe data used in this study comes from a large dataset comprising 183 million\nposts made between 2018 and 2021, collected by Aliapoulios et al. [62]. For our anal-\nysis, we focused exclusively on the 1.7 million posts made during 2020 containing at\nleast one of the keywords {trump, biden, joebiden, donaldtrump, donald, trump2020,\nbiden2020 }in their main body.\nScored: Scored (accessible via both https://scored.co and https://communities.\nwin) emerged as an alternative to Reddit, hosting numerous communities that\nwere banned from more prominent social networks, including c/TheDonald ,\nc/GreatAwakening , and c/FatPeopleHate . For this study, we use the iDRAMA-\nScored-2024 dataset [63], a comprehensive scrape of the platform since its inception\nin 2020, for a total of 6.2 million posts spanning nearly four years. We filter the\ndataset by retaining only the submissions posted during 2020 in six politically cen-\ntered communities of the 975 present communities, which account for 92% of links\nshared during 2020 over 1.4M posts. Please refer to the SI for a detailed table of the\nselected communities retained in the analysis (Table S3).\nVoat: The now-defunct Voat, which was shut down in December 2020, served as\nan alternative to Reddit, similar to Scored. Several communities banned from Red-\ndit migrated to Voat, forming new subverses \u2014the platform\u2019s equivalent of subreddits.\nNotable examples include v/fatpeoplehate, v/TheRedPill, and v/GreatAwakening.\nFor this study, we use data from a collection of 2.3 million posts made on the\nplatform between November 2013 and December 2020, compiled by Mekacher and\nPapasavva [64]. From this dataset, we retain only submissions made during 2020 across\nthe 60 most prominent political subvoats (over the 7 ,604 total subvoats), for a total of\nalmost 200 thousands posts. Please refer to the SI for a table of the selected subverses\nretained in our analysis (Table S4).\nPlatforms graph and null model\nThe set of connections (URLs) between platforms can be modeled as a weighted\ndirected graph G= (V, E, w ), where Vrepresents the set of nine social media plat-\nforms, E\u2286V\u00d7Vthe set of directed edges between platforms, and w:E\u2192R+the\nweight of edges corresponding to the number of times one platform links to another.\nGiven the size disparity between the considered data sets, quantifying the relevance of\nlinks among nodes using only their weights would introduce a bias. Thus, we compute\nhow such weights deviate from their expected values. These expected values can be\ncomputed using the weighted configuration model [65], a null model in which the in-\nand out-strength distributions of the nodes are preserved. According to the weighted\nconfiguration model, the average weight of a link connecting two uncorrelated vertices\nwith out-strength sout\niand in-strength sin\njcan be written as E[wij] =sout\ni \u00b7sin\nj\nS, where\nSrefers to the total weight of the network. Hence, we can compute the relationship\n15\nbetween the observed weights and their expectation as follows:\n\u02c6wi,j=wi,j\nE[wij].\nSuch quantity approximates the results we would obtain by performing a strength-\npreserving randomization on the network. If higher than 1, the weights we obtain\nindicate that a link from one platform to another occurs more often than we would\nexpect at random, and vice versa.\nLabeling of news sources\nAs mentioned, we utilize Media Bias/Fact Check (MBFC,\nhttps://mediabiasfactcheck.com/) to label news outlets based on their political bias\nand level of reliability regarding factuality reporting. MBFC is an independent\nfact-checking organization that rates various news sources. The labeling utilized in\nthis study, collected in October 2024, contains political bias categories ranging from\nextreme-left to extreme-right, while certain news outlets are classified as \u201cquestion-\nable\u201d, indicating that they exhibit one or more of the following, per MBFC: \u201cextreme\nbias, consistent promotion of propaganda/conspiracies, poor or no sourcing to cred-\nible information, a complete lack of transparency and/or is fake news\u201d. While not\nexplicitly falling under the \u201cquestionable\u201d definition of MBFC, we also consider as\nsuch domains classified as \u201cconspiracy/pseudoscience\u201d, given their inherent low cred-\nibility and absence of proper fact-checking. Furthermore, we manually labeled a small\nnumber of right-biased and/or extremist websites which accounted for a significant\nportion of some alt-tech platforms\u2019 traffic, but that were not present on MBFC.\nInferring accounts\u2019 leaning\nTo compute an account\u2019s leaning, we utilize the following algorithmic procedure: we\nassign a score between \u22121 and +1 to each external domain, depending on its MBFC\u2019s\npolitical bias label. Namely, -1 for the extreme left, -0.66 for left, -0.33 for left-center, 0\nfor least biased, 0.33 for right-center, 0.66 for right, and +1 for the extreme right. For\nan account iwho shared nURLs towards external domains Ci={c1, c2, ..., c n}, each\nURL cjis associated with one of these numeric values. The political leaning xiof the\naccount iis then defined as the average of the political bias of all the domains shared:\nxi\u2261Pn\nj=1cj\nn.\nThis returns a leaning score in the interval [ \u22121,1], where a value of \u22121 (+1) indicate\nan extreme-left (extreme-right) leaning. This procedure, though simple, is grounded in\npsychological theories such as selective exposure [66] and has proven to be an effective\nestimator of users\u2019 political leaning [5, 46].\n16\nSupplementary Information\nSI Appendix Section\nTable S1 : Timeframe and number of URLs collected (before and after processing) for\neach platform\u2019s data set.\nPlatform Timeframe # of URLs (unprocessed) # of URLs (processed)\nFacebook 25/05/2020 - 15/11/2020 20M 10.7M\nReddit 01/01/2020 - 31/12/2020 328k 320k\nTwitter 01/06/2020 - 03/11/2020 189M 103M\nYouTube 01/06/2020 - 16/12/2020 981k 510k\nBitChute 02/04/2020 - 09/10/2020 3.2M 107k\nGab 01/06/2020 - 01/12/2020 468k 453k\nParler 01/01/2020 - 31/12/2020 14M 1.1M\nScored 01/01/2020 - 31/12/2020 984k 868k\nVoat 01/01/2020 - 25/12/2020 269k 135k\nTable S2 : Table showing the subverse name, number of URLs shared, number of\nURLs shared toward left biased sources, fraction of left biased sources shared, cumu-\nlative count and cumulative proportion of left biased sources shared for the six Voat\u2019s\nsubverses responsible for sharing 89.5% of the platform\u2019s left biased content. Note how\nin this table we consider as \u201cleft biased\u201d every source classified as either extreme-left\nbiased, left biased, or left-center biased.\nSubverse URLs left-leaning URLs frac. left cum. count cum. prop.\n1. AnonAll 15822 9643 0 .609 9643 0 .399\n2. news 31884 5931 0 .186 15574 0 .644\n3. politics 20898 2627 0 .125 18201 0 .753\n4. GreatAwakening 35987 1535 0 .042 19736 0 .816\n5. OccidentalEnclave 4865 1345 0 .276 21081 0 .872\n6. Niggers 5621 553 0 .098 21634 0 .895\n17\nTable S3 : Selected Scored Communities retained in our analysis, and number of URLs\ncontained in them\n# Community Nr. of URLs\n1 TheDonald 907439\n2 OmegaCanada 7034\n3 GreatAwakening 2804\n4 GavinMcInnes 1248\n5 Conspiracies 1180\n6 Conservative 224\n18\nTable S4 :Selected Subverses retained in our analysis, and number of URLs\ncontained in them.\n# Subverse name Nr. of URLs\n1 GreatAwakening 35987\n2 news 31884\n3 politics 20898\n4 AnonAll 15822\n5 theawakening 12765\n6 Niggers 5621\n7 OccidentalEnclave 4865\n8 Conspiracy 3766\n9 WorldToday 2753\n10 Worldnews 2289\n11 TheDonald 1620\n12 TheGreatAwakening 920\n13 USNews 776\n14 politicalnews 754\n15 2020ElectionNews 749\n16 Worldpolitics 693\n17 uspolitics 451\n18 CRIMENEWS 431\n19 Jews 359\n20 christiannews 284\n21 ChristianEnclave 281\n22 CultureWars 278\n23 economics 250\n24 SJWHate 221\n25 PivottoAsia 219\n26 AnonNews 215\n27 Libertarian 161\n28 pizzagate 160\n29 CanadaFirst 150\n30 Military 145\n31 pedogate 140\n32 PoliticalDiscussion 123\n33 BadCopNoDonut 112\n34 pizzagateuncensored 112\n35 education 111\n36 HillaryforJail 76\n37 Lawenforcement 73\n38 PoliticallyIncorrect 69\n39 BlackLivesMatter 68\n40 conspiracyfact 66\nContinued on next page\n19\nTable S4 \u2013 continued from previous page\n# Subverse name Nr. of URLs\n41 newsandpolitics 61\n42 PedogateFullExposure 59\n43 WhiteRights 58\n44 ANTIFAWATCH 55\n45 gunpolitics 54\n46 Immigration 54\n47 Identitarians 47\n48 ClimateSkeptics 44\n49 ObamaForPrison 44\n50 QAnon 44\n51 Censorship 43\n52 CrimesByRace 40\n53 Jewspiracy 40\n54 russia 40\n55 War 39\n56 Kikes 38\n57 Abortion 37\n58 MassDeport 37\n59 Israel 36\n60 presstitutes 36\n20\nTable S5 :Percentage of domains matched with MBFC ratings for each\nplatform.\nPlatform % of rated domains\nFacebook 5.85\nReddit 26.62\nTwitter 3.94\nYouTube 11.11\nBitchute 18.84\nGab 27.77\nParler 18.98\nScored 22.06\nVoat 34.12\n21\n21465616\n10806\n2069\n1029092\n1752\n4655119979230792549\n5417094\n138\n182020075677852\n3975\n17326\n14023011\n54222505046\n10453\n213\n4930\n2260\n150128481381\n34971\n2091\n3519303227\n2955\n93055169835\n662863941\n1360\n13366\n14131588004\n2\n159\n1137\n110101125633\n4488892\n13422514358\n5004\n11827817743295010\n00\n0000\nScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScored\nToFromFig. S1 :Adjacency matrix describing the number of links from and to each\nof the nine different platforms.\n22\nTop 20 domains\n25.1% of total links shared\n0.000.250.500.751.00\n020K 40K 60KFacebook\nTop 20 domains\n48% of total links shared\n0.250.500.751.00\n0 30K 60K 90KTwitter\nTop 20 domains\n49.2% of total links shared\n0.250.500.751.00\n02.5K 5.0K 7.5K 10.0KReddit\nTop 20 domains\n26.5% of total links shared0.250.500.751.00\n02.5K5.0K7.5K10.0K12.5KCum. frac. of total nr. of sharesYouTube\nTop 20 domains\n25% of total links shared\n0.000.250.500.751.00\n01K 2K 3K 4KBitChute\nTop 20 domains\n52.6% of total links shared\n0.250.500.751.00\n0 2K 4K 6KGab\nTop 20 domains\n56.7% of total links shared\n0.250.500.751.00\n0 5K 10KParler\nTop 20 domains\n76.4% of total links shared\n0.60.70.80.91.0\n0 3K 6K 9K 12K\nRank (Sorted by descending nr. of links shared)Scored\nTop 20 domains\n40.8% of total links shared 0.250.500.751.00\n0 2K 4K 6KVoatFig. S2 :Cumulative fraction of the total amount of links towards different\ndomains. As observed, the top-20 most shared domains encompass for all\nplatforms a significant percentage of links shared.\n23\n10.67\n10.79\n0.78\n10.41\n0.02\n0.24\n10\n0\n0.04\n0\n10.28\n0.03\n0.28\n0.18\n0.26\n10.35\n0\n0.31\n0.48\n0.18\n0.87\n10.33\n0.15\n0.32\n0.19\n0.17\n0.88\n0.75\n10.03\n0\n0.02\n0.02\n0.01\n0.07\n0.07\n0.07\n1 ScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScoredCosine similarity matrix, # of domains = 10\n10.71\n10.78\n0.77\n10.41\n0.14\n0.26\n10.04\n0.01\n0.06\n0.1\n10.27\n0.05\n0.28\n0.18\n0.26\n10.34\n0.06\n0.3\n0.47\n0.23\n0.88\n10.39\n0.26\n0.42\n0.18\n0.18\n0.85\n0.73\n10.03\n0.01\n0.03\n0.02\n0.01\n0.08\n0.07\n0.07\n1 ScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScoredCosine similarity matrix, # of domains = 20\n10.73\n10.81\n0.81\n10.4\n0.16\n0.26\n10.07\n0.02\n0.07\n0.1\n10.35\n0.06\n0.27\n0.18\n0.26\n10.43\n0.07\n0.3\n0.46\n0.24\n0.89\n10.45\n0.29\n0.43\n0.18\n0.18\n0.85\n0.75\n10.03\n0.01\n0.03\n0.02\n0.02\n0.08\n0.07\n0.07\n1 ScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScoredCosine similarity matrix, # of domains = 30\n10.77\n10.83\n0.82\n10.42\n0.2\n0.3\n10.12\n0.03\n0.09\n0.1\n10.35\n0.06\n0.28\n0.18\n0.31\n10.44\n0.09\n0.32\n0.47\n0.27\n0.89\n10.5\n0.32\n0.46\n0.2\n0.24\n0.84\n0.74\n10.04\n0.01\n0.03\n0.02\n0.02\n0.08\n0.07\n0.07\n1 ScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScoredCosine similarity matrix, # of domains = 50Fig. S3 :Cosine similarity matrix for different thresholds (top 10, 20, 30,\n50 domains). The results remain extremely similar between the different\nmatrices.\n24\n02468\n0.000.250.500.751.00DensityFacebook\n02468\n0.000.250.500.751.00Twitter\n0.02.55.07.5\n0.000.250.500.751.00Reddit\n0510152025\n0.000.250.500.751.00DensityYouTube\n0.02.55.07.510.0\n0.000.250.500.751.00BitChute\n0246\n0.000.250.500.751.00Gab\n0.02.55.07.5\n0.000.250.500.751.00DensityParler\n036912\n0.000.250.500.751.00\nVar. of bias of sourcesScored\n01234\n0.000.250.500.751.00VoatFig. S4 :Probability density function of the variance in the bias of the\nsources shared by users on different platforms. We observe that users on\ndifferent platforms generally exhibit low variance in the political bias of\nthe sources they share, suggesting that most tend to engage with content\nfrom a narrow ideological range.\n25\nRedditBitChuteParlerVoatScoredFacebookTwitterYouTubeGab\nJan 2020 Apr 2020 Jul 2020 Oct 2020 Jan 2021\nDatePlatformDatasets timelinesFig. S5 :Common time window of the data sets.\n1\n1.1\n0.52\n0.670.88\n0.62\n0.510.689.880.88\n0.87\n1.570.53\n0.07\n0.122.535.861.81\n1.68\n3.962.89\n5.55\n7.242.41.990.42\n0.54\n3.920.89\n3.36\n1.338.433.241.23\n0.7\n60.5\n0.46\n1.7514.11.960.16\n1.11\n28.12\n0.19\n1.37\n1.970.680.06\n0.69\n20.27\n0.92.19\n8.4215.121.45\n1.80.8\n0.42\n33.93\n8.190.9\n9.68\n3.170\n00\n00\n000\nScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScored\nToFromOriginal Analysis\n0.98\n1.07\n0.60.89\n0.52\n0.490.670.6410.330.92\n0.090.62\n0.84\n0.131.792.747.992.54\n7.493.39\n2.74\n10.165.343.310.1\n0.873.11\n25.66\n8.531.7213.122.160.17\n1.11\n1.3523.26\n1.410.260.552.430.28\n0.7\n3.30.81\n0.926.7752.181.32\n3.471\n0.84\n0.350.59\n6.49\n1.4518.412.50.96\n0.62\n16.531.56\n49.76\n3.296.610\n00\n00\n000\nScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScored\nToFromUniform Timeframe\nFig. S6 :comparison between the rescaled adjacency matrix computed with\nthe \u201cfull\u201d datasets, and those obtained after filtering the datasets on the\ncommon time window. The results remain largely consistent between the\ntwo, both highlighting a fringe ecosystem generally more connected than\nthe mainstream one. This similarity between the results is confirmed by\ncomputing Kendall\u2019s Tau between the two matrices, which has a very high\nscore of \u03c4= 0.92.\n26\n0.01 0.12 0.010.03 0.120.01 0.680.1 0.05 0.090.22 0.130.06 0.36\n0.03 0.34 0.020.04 0.370.05 0.16\n0.02 0.27 0.010.04 0.430.04 0.190.14 0 0.160.48 0.040.06 0.12\n0.02 0.65 00.03 0.170.02 0.10.06 0.07 0.130.41 0.120.06 0.14\n0.06 0.17 0.050.16 0.270.08 0.20.03 0.02 0.060.12 0.120.06 0.59\nScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nleft\nleft_centercenter\nright_centerright\nextreme_rightunreported\nToFromOriginal Analysis\n0.01 0.12 0.010.03 0.120.01 0.70.1 0.06 0.090.23 0.140.06 0.32\n0.03 0.33 0.020.04 0.390.05 0.15\n0.02 0.25 0.010.04 0.470.05 0.160.15 0 0.150.49 0.030.06 0.12\n0.02 0.66 00.03 0.170.03 0.080.06 0.07 0.130.41 0.120.06 0.14\n0.06 0.17 0.050.17 0.270.08 0.190.03 0.02 0.080.12 0.140.06 0.55\nScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nleft\nleft_centercenter\nright_centerright\nextreme_rightunreported\nToFromUniform TimeframeFig. S7 :Comparison of the original platform diet to the one computed after\nfiltering the data sets maintaining the uniform time window. The results\nare practically the same, as also shown by the extremely high Kendall\u2019s\nTau value between the matrices of \u03c4= 0.967.\n10.71\n10.78\n0.77\n10.41\n0.14\n0.26\n10.04\n0.01\n0.06\n0.1\n10.27\n0.05\n0.28\n0.18\n0.26\n10.34\n0.06\n0.3\n0.47\n0.23\n0.88\n10.39\n0.26\n0.42\n0.18\n0.18\n0.85\n0.73\n10.03\n0.01\n0.03\n0.02\n0.01\n0.08\n0.07\n0.07\n1 ScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScoredOriginal Analysis\n10.71\n10.78\n0.77\n10.41\n0.13\n0.27\n10.03\n0.01\n0.05\n0.07\n10.3\n0.06\n0.29\n0.21\n0.2\n10.41\n0.07\n0.32\n0.61\n0.16\n0.84\n10.41\n0.28\n0.44\n0.2\n0.14\n0.84\n0.7\n10.03\n0.01\n0.03\n0.03\n0.01\n0.08\n0.07\n0.07\n1 ScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\nScoredUniform Timeframe\nFig. S8 :Cosine similarity matrix computed on both the original and fil-\ntered dataframes. The values between the two are extremely similar, as\nalso shown by the high value of Kendall\u2019s Tau between the matrices\n(\u03c4= 0.966). The only notable difference is an increase in the similarity\nbetween YouTube and Parler.\n27\nJS Divergence = 0\n0.00.20.4\n\u22121.0 \u22120.5 0.0 0.5 1.0FrequencyFacebook\nJS Divergence = 0.002\n0.00.20.4\n\u22121.0 \u22120.5 0.0 0.5 1.0Reddit\nJS Divergence = 0\n0.00.20.4\n\u22121.0 \u22120.5 0.0 0.5 1.0Twitter\nJS Divergence = 0.015\n0.00.20.4\n\u22121.0 \u22120.5 0.0 0.5 1.0FrequencyYouTube\nJS Divergence = 0.024\n0.00.20.4\n\u22121.0 \u22120.5 0.0 0.5 1.0BitChute\nJS Divergence = 0.002\n0.00.20.4\n\u22121.0 \u22120.5 0.0 0.5 1.0Gab\nJS Divergence = 0.017\n0.00.20.4\n\u22121.0 \u22120.5 0.0 0.5 1.0\nUser leaningFrequencyParler\nJS Divergence = 0.012\n0.00.20.4\n\u22121.0 \u22120.5 0.0 0.5 1.0\nUser leaningVoat\nJS Divergence = 0.002\n0.00.20.4\n\u22121.0 \u22120.5 0.0 0.5 1.0\nUser leaningScored\nOriginal Analysis Uniform TimeframeFig. S9 :Overlap between original user leaning, and those obtained after\nfiltering the data. We can observe that the histograms nearly completely\noverlap. This similarity is also shown by the reported values of the Jensen-\nShannon divergence between the histograms, which results very low in\nevery case considered.\n28\nReferences\n[1] Gallup, K.: Indicators of news media trust. John S. and James L. Knight\nFoundation Miami (2018)\n[2] Nic, N., Fletcher, R., Kalogeropoulos, A., Levy, D.A., Nielsen, R.K.: Reuters\ninstitute digital news report 2018. Reuters Institute for the Study of Journalism\n39(2018)\n[3] Voorveld, H.A., Van Noort, G., Muntinga, D.G., Bronner, F.: Engagement with\nsocial media and social media advertising: The differentiating role of platform\ntype. Journal of advertising 47(1), 38\u201354 (2018)\n[4] Sangiorgio, E., Cinelli, M., Cerqueti, R., Quattrociocchi, W.: Followers do not\ndictate the virality of news outlets on social media. PNAS nexus 3(7), 257 (2024)\n[5] Cinelli, M., De Francisci Morales, G., Galeazzi, A., Quattrociocchi, W., Starnini,\nM.: The echo chamber effect on social media. Proceedings of the National\nAcademy of Sciences 118(9), 2023301118 (2021)\n[6] Etta, G., Sangiorgio, E., Di Marco, N., Avalle, M., Scala, A., Cinelli, M., Quat-\ntrociocchi, W.: Characterizing engagement dynamics across topics on facebook.\nPlos one 18(6), 0286150 (2023)\n[7] Bessi, A., Coletto, M., Davidescu, G.A., Scala, A., Caldarelli, G., Quattrociocchi,\nW.: Science vs conspiracy: Collective narratives in the age of misinformation.\nPloS one 10(2), 0118093 (2015)\n[8] Terren, L.T.L., Borge-Bravo, R.B.-B.R.: Echo chambers on social media: A\nsystematic review of the literature. Review of Communication Research 9(2021)\n[9] Avalle, M., Di Marco, N., Etta, G., Sangiorgio, E., Alipour, S., Bonetti, A., Alvisi,\nL., Scala, A., Baronchelli, A., Cinelli, M., et al. : Persistent interaction patterns\nacross social media platforms and over time. Nature 628(8008), 582\u2013589 (2024)\n[10] Shandwick, W., Tate, P.: Krc research (2019). civility in america 2019: Solutions\nfor tomorrow. Weber Shadwick 26(2019)\n[11] League, A.-D.: Online hate and harassment report: the american experience 2020.\nRetrieved September 25, 2021 (2020)\n[12] Tahmasbi, F., Schild, L., Ling, C., Blackburn, J., Stringhini, G., Zhang, Y., Zan-\nnettou, S.: \u201cgo eat a bat, chang!\u201d: On the emergence of sinophobic behavior on\nweb communities in the face of covid-19. In: Proceedings of the Web Conference\n2021, pp. 1122\u20131133 (2021)\n[13] Bail, C.A., Argyle, L.P., Brown, T.W., Bumpus, J.P., Chen, H., Hunzaker, M.F.,\nLee, J., Mann, M., Merhout, F., Volfovsky, A.: Exposure to opposing views\n29\non social media can increase political polarization. Proceedings of the National\nAcademy of Sciences 115(37), 9216\u20139221 (2018)\n[14] Kubin, E., Von Sikorski, C.: The role of (social) media in political polarization: a\nsystematic review. Annals of the International Communication Association 45(3),\n188\u2013206 (2021)\n[15] Falkenberg, M., Galeazzi, A., Torricelli, M., Di Marco, N., Larosa, F., Sas, M.,\nMekacher, A., Pearce, W., Zollo, F., Quattrociocchi, W., et al. : Growing polar-\nization around climate change on social media. Nature Climate Change 12(12),\n1114\u20131121 (2022)\n[16] Cinus, F., Minici, M., Monti, C., Bonchi, F.: The effect of people recommenders\non echo chambers and polarization. In: Proceedings of the International AAAI\nConference on Web and Social Media, vol. 16, pp. 90\u2013101 (2022)\n[17] Del Vicario, M., Bessi, A., Zollo, F., Petroni, F., Scala, A., Caldarelli, G., Stanley,\nH.E., Quattrociocchi, W.: The spreading of misinformation online. Proceedings\nof the national academy of Sciences 113(3), 554\u2013559 (2016)\n[18] Diaz Ruiz, C., Nilsson, T.: Disinformation and echo chambers: how disinformation\ncirculates on social media through identity-driven controversies. Journal of public\npolicy & marketing 42(1), 18\u201335 (2023)\n[19] Bovet, A., Grindrod, P.: Organization and evolution of the uk far-right network\non telegram. Applied Network Science 7(1), 76 (2022)\n[20] Sunstein, C.R.: The law of group polarization. University of Chicago Law School,\nJohn M. Olin Law & Economics Working Paper (91) (1999)\n[21] Del Vicario, M., Vivaldo, G., Bessi, A., Zollo, F., Scala, A., Caldarelli, G., Quat-\ntrociocchi, W.: Echo chambers: Emotional contagion and group polarization on\nfacebook. Scientific reports 6(1), 37825 (2016)\n[22] Hobolt, S.B., Lawall, K., Tilley, J.: The polarizing effect of partisan echo\nchambers. American Political Science Review 118(3), 1464\u20131479 (2024)\n[23] Monti, C., Cinelli, M., Valensise, C., Quattrociocchi, W., Starnini, M.: Online\nconspiracy communities are more resilient to deplatforming. PNAS nexus 2(10),\n324 (2023)\n[24] McCoy, J.: Polarization harms democracy and society. Peace in progress, June\n201936(2019)\n[25] McCoy, J., Press, B.: What happens when democracies become perniciously\npolarized? (2022)\n[26] Grimmelmann, J.: The virtues of moderation. Yale JL & Tech. 17, 42 (2015)\n30\n[27] Ali, S., Saeed, M.H., Aldreabi, E., Blackburn, J., De Cristofaro, E., Zannettou, S.,\nStringhini, G.: Understanding the effect of deplatforming on social networks. In:\nProceedings of the 13th ACM Web Science Conference 2021, pp. 187\u2013195 (2021)\n[28] Mekacher, A., Falkenberg, M., Baronchelli, A.: The systemic impact of deplat-\nforming on social media. PNAS nexus 2(11), 346 (2023)\n[29] Cima, L., Trujillo, A., Avvenuti, M., Cresci, S.: The great ban: Efficacy and\nunintended consequences of a massive deplatforming operation on reddit. In:\nCompanion Publication of the 16th ACM Web Science Conference, pp. 85\u201393\n(2024)\n[30] Cava, L.L., Aiello, L.M., Tagarelli, A.: Drivers of social influence in the twitter\nmigration to mastodon. Scientific Reports 13(1), 21626 (2023)\n[31] Quelle, D., Bovet, A.: Bluesky: Network topology, polarization, and algorithmic\ncuration. PloS one 20(2), 0318034 (2025)\n[32] Failla, A., Rossetti, G.: \u201ci\u2019m in the bluesky tonight\u201d: Insights from a year worth\nof social data. PloS one 19(11), 0310330 (2024)\n[33] Newell, E., Jurgens, D., Saleem, H., Vala, H., Sassine, J., Armstrong, C., Ruths,\nD.: User migration in online social networks: A case study on reddit dur-\ning a period of community unrest. In: Proceedings of the International AAAI\nConference on Web and Social Media, vol. 10, pp. 279\u2013288 (2016)\n[34] Horta Ribeiro, M., Jhaver, S., Zannettou, S., Blackburn, J., Stringhini, G.,\nDe Cristofaro, E., West, R.: Do platform migrations compromise content mod-\neration? evidence from r/the donald and r/incels. Proceedings of the ACM on\nHuman-Computer Interaction 5(CSCW2), 1\u201324 (2021)\n[35] Gais, H., Cruz, F.: Far-right insurrectionists organized capitol siege on parler.\nSouthern Poverty Law Center (2021). Accessed: 2024-11-18\n[36] News, A.: Experts: Echo chambers in apps like parler, gab contributed to capitol\nattack. ABC News (2021). Accessed: 2024-11-18\n[37] Levenson, E., Sanchez, R.: Gab\u2019s Link to Pittsburgh Shooting Suspect Raises\nFree Speech Questions. Accessed: 2025-05-26. https://edition.cnn.com/2018/10/\n27/tech/gab-robert-bowers\n[38] Schulze, H., Hohner, J., Greipl, S., Girgnhuber, M., Desta, I., Rieger, D.: Far-right\nconspiracy groups on fringe platforms: A longitudinal analysis of radicalization\ndynamics on telegram. Convergence: The International Journal of Research into\nNew Media Technologies 28(4), 1103\u20131126 (2022)\n[39] Cinelli, M., Etta, G., Avalle, M., Quattrociocchi, A., Di Marco, N., Valensise, C.,\n31\nGaleazzi, A., Quattrociocchi, W.: Conspiracy theories and social media platforms.\nCurrent Opinion in Psychology 47, 101407 (2022)\n[40] Horta Ribeiro, M., Hosseinmardi, H., West, R., Watts, D.J.: Deplatforming did\nnot decrease parler users\u2019 activity on fringe social media. PNAS nexus 2(3), 035\n(2023)\n[41] Winkel, T., et al.: Fringe platforms: An analysis of contesting alternatives to the\nmainstream social media platforms in a platformized public sphere. PhD thesis,\nUtrecht University (2023)\n[42] L\u00b4 evy, P.: Collective intelligence, a civilisation: Towards a method of positive inter-\npretation. International Journal of Politics, Culture, and Society 18, 189\u2013198\n(2005)\n[43] Howcroft, D., Fitzgerald, B., et al. : From utopia to dystopia: the twin faces\nof the internet. In: Information Systems: Current Issues and Future Changes,\nProceedings of IFIP WG8, vol. 2, pp. 49\u201370 (1998). Citeseer\n[44] NPR: Reddit bans the donald, forum of nearly 800,000 trump fans over abusive\nposts (2020). Accessed: 2025-01-30\n[45] Stefanov, P., Darwish, K., Atanasov, A., Nakov, P.: Predicting the topical stance\nand political leaning of media using tweets. In: Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pp. 527\u2013537 (2020)\n[46] Flamino, J., Galeazzi, A., Feldman, S., Macy, M.W., Cross, B., Zhou, Z., Serafino,\nM., Bovet, A., Makse, H.A., Szymanski, B.K.: Political polarization of news media\nand influencers on twitter in the 2016 and 2020 us presidential elections. Nature\nHuman Behaviour 7(6), 904\u2013916 (2023)\n[47] Mustafaraj, E., Finn, S., Whitlock, C., Metaxas, P.T.: Vocal minority versus\nsilent majority: Discovering the opionions of the long tail. In: 2011 IEEE Third\nInternational Conference on Privacy, Security, Risk and Trust and 2011 IEEE\nThird International Conference on Social Computing, pp. 103\u2013110 (2011). IEEE\n[48] Nogara, G., Vishnuprasad, P.S., Cardoso, F., Ayoub, O., Giordano, S., Luceri,\nL.: The disinformation dozen: An exploratory analysis of covid-19 disinformation\nproliferation on twitter. In: Proceedings of the 14th ACM Web Science Conference\n2022, pp. 348\u2013358 (2022)\n[49] Patriots.Win - Bias and Credibility. Media Bias/Fact Check. Accessed: 2024-11-\n04. https://mediabiasfactcheck.com/patriots-win-bias-and-credibility/\n[50] Patriots.win. Anti-Defamation League. Accessed: 2024-11-04. https://www.adl.\norg/glossary/patriotswin\n32\n[51] De Francisci Morales, G., Monti, C., Starnini, M.: No echo in the chambers of\npolitical interactions on reddit. Scientific reports 11(1), 2818 (2021)\n[52] Monti, C., D\u2019Ignazi, J., Starnini, M., De Francisci Morales, G.: Evidence of\ndemographic rather than ideological segregation in news discussion on reddit. In:\nProceedings of the ACM Web Conference 2023, pp. 2777\u20132786 (2023)\n[53] Baumgartner, J., Zannettou, S., Keegan, B., Squire, M., Blackburn, J.: The\npushshift reddit dataset. In: Proceedings of the International AAAI Conference\non Web and Social Media, vol. 14, pp. 830\u2013839 (2020)\n[54] Flamino, J., Galeazzi, A., Feldman, S., Macy, M.W., Cross, B., Zhou, Z., Serafino,\nM., Bovet, A., Makse, H.A., Szymanski, B.K.: Data for \u201dPolitical Polarization\nof News Media and Influencers on Twitter in the 2016 and 2020 US Presidential\nElections\u201d. https://doi.org/[https://osf.io/e395q/]\n[55] Trujillo, M., Gruppi, M., Buntain, C., Home, B.: What is BitChute? Character-\nizing the \u201cfree speech\u201d alternative to YouTube. arXiv (2004)\n[56] Horne, B., Trujillo, M., Gruppi, M., Buntain, C.: [dataset]* The MeLa\nBitChute Dataset. https://doi.org/10.7910/DVN/KRD1VS . https://doi.org/10.\n7910/DVN/KRD1VS\n[57] Romm, T.: Silicon valley elite and social media\u2019s role in hate and radicalization\nthat led to gab. The Washington Post (2018). Accessed: 2024-11-18\n[58] Forward, T.: How gab weaponizes christian extremism and antisemitism. The\nForward (2022). Accessed: 2024-11-18\n[59] LaFrance, A.: What is gab? The Atlantic (2018). Accessed: 2024-11-18\n[60] Staff, F.N.: Social media platform gab\u2019s website and twitter account taken down.\nFox News (2020). Accessed: 2024-11-18\n[61] Ribeiro, M.H., Blackburn, J., Bradlyn, B., De Cristofaro, E., Stringhini, G., Long,\nS., Greenberg, S., Zannettou, S.: The evolution of the manosphere across the web.\nIn: Proceedings of the International AAAI Conference on Web and Social Media,\nvol. 15, pp. 196\u2013207 (2021)\n[62] Aliapoulios, M., Bevensee, E., Blackburn, J., Bradlyn, B., De Cristofaro, E.,\nStringhini, G., Zannettou, S.: [dataset]* a large open dataset from the parler social\nnetwork. In: Proceedings of the International AAAI Conference on Web and Social\nMedia, vol. 15, pp. 943\u2013951 (2021). https://doi.org/10.5281/zenodo.4442460\n[63] Patel, J., Paudel, P., De Cristofaro, E., Stringhini, G., Blackburn, J.: [dataset]*\nidrama-scored-2024: A dataset of the scored social media platform from 2020 to\n2023. In: Proceedings of the International AAAI Conference on Web and Social\n33\nMedia, vol. 18, pp. 2014\u20132024 (2024). https://doi.org/10.5281/zenodo.10516043\n[64] Mekacher, A., Papasavva, A.: [dataset]* \u201di can\u2019t keep it up.\u201d a dataset from the\ndefunct voat. co news aggregator. In: Proceedings of the International AAAI\nConference on Web and Social Media, vol. 16, pp. 1302\u20131311 (2022). https://doi.\norg/10.5281/zenodo.5841668\n[65] Serrano, M. \u00b4A., Bogu\u02dc n\u00b4 a, M.: Weighted configuration model. arXiv preprint cond-\nmat/0501750 (2005)\n[66] Stroud, N.J.: Polarization and partisan selective exposure. Journal of communi-\ncation 60(3), 556\u2013576 (2010)\n34", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Ideological Fragmentation of the Social Media Ecosystem: From echo chambers to echo platforms", "author": ["E Di Martino", "A Galeazzi", "M Starnini"], "pub_year": "2024", "venue": "arXiv preprint arXiv \u2026", "abstract": "The entertainment-driven nature of social media encourages users to engage with like-minded  individuals and consume content aligned with their beliefs, limiting exposure to diverse"}, "filled": false, "gsrank": 162, "pub_url": "https://arxiv.org/abs/2411.16826", "author_id": ["rp_Lu2MAAAAJ", "DK0tXAIAAAAJ", "duBif0oAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:jlQOLcULBbcJ:scholar.google.com/&output=cite&scirp=161&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D160%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=jlQOLcULBbcJ&ei=IbWsaPvmC7_SieoPzJnloAQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=13187960025316742286&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:jlQOLcULBbcJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2411.16826"}}, {"title": "Simultaneous Fake News and Topic Classification via Auxiliary Task Learning", "year": "2020", "pdf_data": "  \nSimultaneous Fake News and Topic  Classification \nvia Auxiliary  Task Learning  \nTsun -hin Cheung , Kin -man Lam \nDepartment of Electronic and Information Engineering,  The Hong Kong Polytechnic University , Hong Kong  \nEmail: tsun-hin.cheung@connect.polyu.hk, enkmlam@polyu.edu.hk  \n \nAbstract\u2014 Using s ocial media , in particular , reading  news \narticles , has become a necessary daily activity  and an important \nway of spreading information . Classification of topics of new \narticles can provide up -to-date information about the current \nstate of politics and society. However, th is convenient way of \nsharing information can lead to  the growth of  falsificatio n. \nTherefore, d istinguish ing between real  and fake news , as well as \nfake-news classification,  have become  essential  and \nindispensable . In this paper , we propose  a new and up-to-date \ndataset for both fake-news classification and topic  classification . \nTo the best of our knowledge, we are  the first to construct  a \ndataset with both fake-news  and topic labels , and employ multi -\ntask learning for learning these two tasks  simultaneously . We \nhave  collected  21K online news articles  published from January  \n2013 to March 2020.  We propose an auxiliary -task long short -\nterm memory (AT -LSTM)  neural network  for text classification \nvia multi -task learning . We evaluate and compare our proposed \nmodel  to five baseline methods , via both single -task and multi -\ntask learning , on this new benchmark dataset . Experimental \nresults show that our proposed AT-LSTM model  outperforms \nthe single -task learning methods  and the hard parameter -\nsharing  multi -task learning methods . The dataset  and codes  will \nbe released  in the future.  \nKeywords \u2014web data mining , fake-news  classification, topic  \nclassification , multi -task learning  \nI. INTRODUCTION  \nThe spread of misinformation  on the Internet is  an \ninfluential  and critical  issue , especially in social media.  Fake -\nnews  article s provide  false information to the public  and have \na strong impact on both politics and society  (an example is \nshown in Fig. 1) . There is a n increasing trend for fake news  \nsince the 2016 US Presidential election  [1]. Automatic fake-\nnews  detection  has raised public  interest , since it  is useful to \nreduce human  effort in  classification. Several ways of \nidentifying online fake-news  articles have been proposed  in \nrecent years. For example,  there are  tools for spotting domain \nname s and IP address es of fake-news  sources . However, it is \neasy to change the  domain names or dynamic IP address es, so \nit is difficult to prevent  fake news . This also leads to the need \nfor a significant amount of human effort to maintain the list of \nthe sources . Moreover , people  may repost the fake-news  \narticles on their social network sites without specifying the \nsources . This makes the tracing of fake-news  sources more \ndifficult.  Due to the successful development of machine \nlearning  and natural language processing , several  prediction  \nmodels for fake-news  classification  have been developed  in \nrecent decades.  \n The first public dataset for fake-news  classification was \nreleased by Vlachos and Riedel  [2] in 2014. It is a small \ndataset , which contains  about 200 sentences . Therefore, the \ndataset  is not large enough  to train deep neural  models. A \nrelative ly recent study , by Wang  [3] in 2017, collected 12.8K  corpus for fact -checking classification  through  \nPOLITIFACT.COM\u2019s API . The study consider ed the \nstatements of a fact with several types of metadata , such as \nspeaker s, subject , history , etc. This dataset contains fact -like \nstatements , which are different from the form of news articles. \nThey proposed  a hybrid convolutional neural network for fake \nstatement  classification by  concentrating on the statement s \nand their metadata features. Our proposed model  was inspired \nby this method , but we employ  the long short -term memory \n(LSTM ) encoders and the classification of the meta -data.  \n The Kaggle challenge  [4], developed by George McIntire , \nprovides  a dataset for classifying fake-news  articles . In this \nchallenge, the fake-news  articles were collected from  the \nwebsites listed in  BS detector [5], while those real-news \narticles  were from traditional news media websites , such as \nNew York Times , Bloomberg , and The Guardian . Our data \ncollection strategy is similar  to this challenge. On top of this, \nwe have further crawled the news meta -categories that are \nused for topic  classification , as shown in Fig . 1. \n \n \nFigure 1. A fake political article published in \nnewsthump.com . \n Topic classification , also called text categorization , has a \nlonger history than fake-news  classification. It has been \nstudied since the early development of the World Wide Web \n(WWW) . Some of the famous datasets for categorising news \narticles include  20 Newsgroup [6] released in 1995, Reuters -\n21578 [7] released in 199 7, and AG News rel eased in 2004. \nThere have been intensive studies on automatic text \nclassification based on  these datasets. However, the news \narticles  in the datasets  were published  more than 15 years ago , \nand written in traditional styles. In our studies, we have \ncollecte d up-to-date online news article s, published from 201 3 \nto 2020 , on media  websites.  This makes our study suitable for \nProceedings, APSIPA Annual Summit and Conference 2020 7-10 December 2020, Auckland, New Zealand\n376 978-988-14768-8-3/20/$31.00 \u00a92020 APSIPA APSIPA-ASC 2020\ncurrent real -world applications for fake -news and topic \nclassification.  \n Fake -news  classification and topic  classification are \napplications  of text classification . In recent years, deep-\nlearning models have been widely  used for text classification , \nand have achieve d great  performance. Convolutional neural \nnetwork s (CNN s) [8] and recurrent neural network s (RNN s) \n[9] are the two most popular deep neural models for natural \nlanguage understanding. These deep neural models are \nconsidered  the baseline methods used for comparison in our \nexperiments . \n Multi -task learning has been broadly studied in machine \nlearning , across a number of  fields , including computer vision \n(CV) and natural language processing (NLP). It is similar to \ntransfer learning , and aims to learn several related tasks at the \nsame time. In NLP, multi -task learning  has been used in \njointly le arning the tasks , such as Part -of-Speech (POS) \ntagging and Named Entity Recognition (NER) [ 10], as well as \nsentiment and sarcasm classification [ 11]. This learning \nstrategy has shown its powerful generalization capacity to \ntrain the deep neural models.  \n Our proposed method comes with the idea of auxiliary  \ntasks  in multi -task learning.  The objective of auxiliary tasks is \nto supplement and support the learning of  the main tasks in \nmulti -task learning . Auxiliary tasks  are mainly  for learning a  \nrobust representation of  input document s to boost the training  \nof deep neural models.  Liebel  and Korner  [12] studied the \neffect and performance of auxiliary  tasks  in CNN models for \nimage classi fication. We employ this concept to form our \nproposed multi -task learning framework  for text \nclassification.  \nII. METHODOLOGY  \nIn this section, we will first describe how data for our dataset \nwas collected from the Internet , and then , how  the collected \ndata was labelled to construct the dataset. After that, we give \na detailed presentation of our proposed deep neural model for \nfake-news and topic classification, and the training of the \nmodel.  \nA. Data collection  \n We developed  our web crawlers and  collect ed the fake-\nnews  articles from  those website s listed in \nmediabia sfactcheck.com , as well as  those real news articles  \nfrom  the New York Times  and the Guardian\u2019s  APIs.  For each  \nof the  news article \u2019s web pages, we extract  its meta -\ndescription tag  as our input document , as illustrated in Table \nI. We also parse d the HTML web  page and obtain ed the topics  \nassigned to each news article based on  the news website s, and \nthen group ed them into five  categor ies, with the  labels  as \nshown in Table II. We can see that the percentage of fake news \navailable is the highest for \u201cPolitics\u201d, while the lowest is for \n\u201cSports\u201d.  \nB. The proposed model \nSince there are two output labels in the dataset, we aim to \nbuild a deep neural network that jointly learn s the two \nclassification tasks. We regard one of the tasks as the main \ntask and the other one as the auxiliary task. The auxiliary task \nis responsible for improving the training of the main task. We \npropose the auxiliary -task long short -term memory (AT -\nLSTM ) for jointly learn ing the two tasks. Fig. 2 shows the \noverview of our  proposed  AT-LSTM.   TABLE  I \nDATA FIELDS - DESCRIPTION  OF OUR NEW BENCHMARK DATASET . \nFIELD  DESCRIPTION  \nURL  The unique identifier for each news article.  \nTitle  The title of each news article.  \nContent  The snippet of the news article. It is used \nas the input document.  \nReality  Whether the document comes from a fake \nor reliable news media.  \nTopic  The category that the article assigned to.  \n \n \nTABLE  II \nDATA DISTRIBUTION \u2013 NUMBER OF REAL AND FAKE DOCUMENTS FOR THE \nDIFFERENT CATEGORIES . \nTOPIC  REALITY  SIZE \nPolitics  Real 2653  \nFake  2329  \nScience/Technology  Real 3633  \nFake  1193  \nBusiness  Real 2377  \nFake  1574  \nHealth  Real 2542  \nFake  1100  \nSports  Real 2432  \nFake  473 \n \n \nWord Embedding : The input of the text-classification model \nis a document \ud835\udc99 = { \ud835\udc991,\ud835\udc992,\u22ef,\ud835\udc99\ud835\udc47}, where T is the number of \nwords in the document. There are two output labels , denoted \nas \ud835\udc9a\ud835\udc56\u2208\u211d2, for each task i in our model. Word embedding is \nto map the words in a document into real -value vectors. Each \nword \ud835\udc99\ud835\udc56 is represented by a D-dimensional embedding vector , \ni.e. \ud835\udc99\ud835\udc56\u2208\u211d\ud835\udc37, which is then passed to the LSTM encoders.  \n \n  \nFigure 2. The proposed AT-LSTM model.  \nProceedings, APSIPA Annual Summit and Conference 2020 7-10 December 2020, Auckland, New Zealand\n377\nLSTM encoders:  The length of the embedded vector for a \ndocument , depending on the number of words in the \ndocument , is not constant. T he primary aim of the neural \nencoder is to represent the variable -length word embedding \nvectors as a fixed -length vector , say length \ud835\udc40. There are two \nencoders in our proposed model: main encoder and auxiliary \nencoder. The main encoder is only responsible for the main \ntask classification , while the auxiliary encoder is  to generate a \ncommon feature for both the main and auxiliary  tasks. Both \nencoders employ  the LSTM unit. An LSTM  unit can process \nan arbitrary -length sequence by recursively applying a \ntransition function to form the hidden state vector. It consists \nof three gates \u2013 the forget gate  \ud835\udc87\ud835\udc61, input gate \ud835\udc8a\ud835\udc61, and output \ngate \ud835\udc90\ud835\udc61, and two memory states \u2013 cell state  \ud835\udc84\ud835\udc61 and hidden state  \n\ud835\udc89\ud835\udc61. The input gate \ud835\udc8a\ud835\udc61 controls the amount of information from \nthe current input to the cell state \ud835\udc84\ud835\udc61. The forget gate \ud835\udc87\ud835\udc61 tells \nthe cell state \ud835\udc84\ud835\udc61 which information from the previous cell  \ud835\udc84\ud835\udc61\u22121 \nto forget . The output gate \ud835\udc90\ud835\udc61 is responsible  for selecting the \ninformation i n the cell gate \ud835\udc84\ud835\udc61 to form the hidden state  \ud835\udc89\ud835\udc61. All \nthese vectors are  in \u211d\ud835\udc40, where M is a hyperparameter.  Given  \nan input word embedding vector \ud835\udc99\ud835\udc61, we compute its \nrepresentation \ud835\udc89\ud835\udc61 in the first layer of a LSTM encoder by \nusing  Equation s (1) to (5): \n \ud835\udc8a\ud835\udc61=\ud835\udf0e(\ud835\udc4a\ud835\udc56\u2219[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61])+\ud835\udc4f\ud835\udc56, (1) \n \ud835\udc53\ud835\udc61=\ud835\udf0e(\ud835\udc4a\ud835\udc53\u2219[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61])+\ud835\udc4f\ud835\udc53, (2) \n \ud835\udc5c\ud835\udc61=\ud835\udf0e(\ud835\udc4a\ud835\udc5c\u2219[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61])+\ud835\udc4f\ud835\udc5c, (3) \n \ud835\udc50\ud835\udc61=\ud835\udc53\ud835\udc61\u2299\ud835\udc50\ud835\udc61\u22121+\ud835\udc56\ud835\udc61\u2299tanh  (\ud835\udc4a\ud835\udc50\u2219[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc50), (4) \n \u210e\ud835\udc61=\ud835\udc5c\ud835\udc61\u2299tanh  (\ud835\udc50\ud835\udc61), (5) \nwhere  \ud835\udf0e and \u2299 denote  the sigmoid function  and elementwise \nmultiplication , respectively . The weight matrix \ud835\udc7e\ud835\udc5d and bias vector  \n\ud835\udc83\ud835\udc5d are the learnable parameters of the gate or state p.  \nOutput Layer : Since our objective is to represent an input \ndocument as a fixed -length vector, we regard the l ast hidden \nrepresentation \ud835\udc89\ud835\udc47 as the feature extracted  from  the document.  \nWe use the last hidden  vector in the  auxiliary encoder for the  \nauxiliary  task classification. To achieve this, the feature vector \nis transform ed into a \ud835\udc3e-dimensional vector by using a linear \nfunction , followed by the SoftMax  function , to generate the \noutput \ud835\udc66\ud835\udc4e, as follows:   \n \ud835\udc66\ud835\udc4e=softmax (\ud835\udc4a\ud835\udc60\u210e\ud835\udc47), (6) \nwhere \ud835\udc3e is the number of possible labels in the auxiliary  task. \nConcatenat ion Layer : We concatenate the feature vectors \nfrom the two encoders to form the final feature representation \nof a document,  which is used  for the main  classification  task. \nFinally, we transform the  final representation vector into a \ud835\udc41-\ndimensional vector and pass  it through the SoftMax function \nto generate the output \ud835\udc66\ud835\udc5a for the main classification task , as \nfollows:  \n \ud835\udc66\ud835\udc5a=softmax (\ud835\udc4a\ud835\udc60\u2032\u210e\ud835\udc47\u2032). (7) \nLoss functio n: There are two output layers in our model : the \nmain task and the auxiliary task . Our goal is to jointly learn \nthese two tasks simultaneously . The cross -entropy loss is used  \nfor both tasks . The total loss is a weighted sum  of the cross -entropy loss of the main task  and that of  the auxiliary  task, as \nfollows:  \n \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 =\u2212\u03b1\u2211log(\ud835\udc66\ud835\udc4e)\u2212(1\u2212\u03b1)\u2211log(\ud835\udc66\ud835\udc5a), (8) \nwhere  \u03b1 is a hyperparameter to control the relative weights of \nthese two losses.  \nIII. EXP ERIMENTS, RESULTS  \nA. Data preprocessing  \nWe split our collected corpus into three subsets : 10.6K , \n5K, and 5K documents  used for training,  validation , and \ntesting , respectively . Each document is assigned two labels, \none for fake-news classification and another one is the topic \ncategory.  The special characters , punctuation  mark s, etc. are \nremoved  from the input documents.  Then, we tokenize the \ndocuments into a sequence of words using the Natural \nLanguage Toolkit (NLTK ) [13]. \nB. Experimental settings  \n The pretrained 200 -dimensional Glove word embeddings \n[14] are used as input for the deep models evaluated in our \nexperiments . For single -task learning, we trained and \nevaluated the baseline results for both tasks using fastText \n[15], textCNN [ 8], LSTM [ 9], AttenLSTM [16]  separately . \nWe also compare our methods to the hard parameter sharing \nLSTM via multi -task learning [1 7]. For the textCNN model, \nthe filter sizes of the CNN model are 3, 4, and 5. The number \nof filters in each layer is 100. For LSTM  and AttenLSTM , the \nnumber of layers is 2 , while the dimension of each  hidden  \nlayer in LSTM is 256 . For our proposed AT-LSTM mod el, the \nnumber of layers is 2 , and the dimension of each  hidden  layer \nin LSTM is 128. For all the experiments, the maximum \nnumber of epochs is 100 , and  the batch size is 32. We used the \nAdam optimizer to train our models , and the learning rate used \nis 0.001. To avoid overfitting, we used a dropout rate of 0.5 \nand weight decay of 1e -6. We implemented the deep model \nwith PyTorch , and report ed the performance on the  testing \ndatasets  with the best model achieved , when  the validation \naccuracy , as below,  is the highest during training.  \n \ud835\udc52\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc5f  \ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52 = number  of mispredicted  samples\nnumber  of samples, (9) \n \ud835\udc4e\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66 =1\u2212\ud835\udc52\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc5f  \ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52  . (10) \nC. Overall results  \nTable 3 shows  the overall accuracy  of the two classifications \ntasks  trained  and evaluated  on several models  via single -task \nlearning and multi -task learning methods . \n \nTABLE  III \nOVERALL COMPARISON OF THE BASELINE METHODS AND OUR PROPOSED \nMULTI -TASK LSTM . \nType  Methods  Fake -news  \nClassification \nAccuracy (%)  Topic \nClassification  \nAccuracy (%)  \nSingle Task  fastText [15] 76.82  73.79  \nSingle Task  textCNN  [8] 87.52  74.24  \nSingle Task  LSTM  [9] 91.51  76.59  \nSingle Task  AttenLSTM [16]  92.95 76.82  \nMulti -task FS-LSTM  [17] 92.01 76.68  \nMulti -task AT-LSTM  (ours)  93.19 77.23 Proceedings, APSIPA Annual Summit and Conference 2020 7-10 December 2020, Auckland, New Zealand\n378\nIV. DISCUSSION  \nA. Compar ison of single -task learning  \nFor single -task learning , experiment results  show that the \nLSTM encoder s outperform the CNN  encoder s, i.e. textCNN,  \nand fastTe xt. fastText is based on bag-of-words features , in \nboth the fake news classification and topic  classification \ntasks.  In Table III, we can see that text CNN and the LSTM \nencoder achieve a  10.7% and 14.69% improvement , in terms \nof accuracy,  when compared to fastText  for fake news \nclassification . For topic classification, the accuracy of \ntextCNN and LSTM encoder is  0.45% and 2.8%, \nrespectively, higher than that of fastText . This means that the \nneural encoders can capture more advanced and useful \ninformation than the bag -of-words features in fastTex t for \nboth tasks . The LSTM encoder has a  4% improvement in fake \nnews classification and a 2% improvement in topic \nclassification, compared to  textCNN . This shows that \nclassifying  fake news require s more long -term dependen t \ncontext information  than topic cl assification , rather than  the \nlocal and position -invariant features  in a sentence.  \n \nB. Comparison of single -task learning  and multi -task \nlearning  \nAs LSTM outperforms  textCNN, we evaluate  our multi -\ntask learning framework on the LSTM encoders. It is worth \nnoting  that any deep neural network s can be used as the \nencoder s in our proposed multi -task learning framework. The \nexperimental results  in Table III show that our proposed  \nmulti -task lear ning framework outperforms all the single -task \nlearning  methods . The fully shared LSTM  (FS-LSTM)  and \nour proposed auxiliary -task LSTM  (AT-LSTM)  increase the \naccuracy by 0.5%  and 1.68%, respectively, compared to \nLSTM for fake -news classification, and 0.09% and 0. 64%, \nrespectively, for topic classification . This shows that  the two \ntasks have common features , which can be learnt by the \nLSTM encoders.  \nCompar ed to the LSTM network with hard parameter \nsharing, our proposed AT-LSTM  model  achieves better \nperformance. When  topic classification is the main task  and \nfake-news classification is the auxiliary task, the accurac y of \nthe main and auxiliary  tasks is 77.23 % and 92.44 %, \nrespectively . When the two tasks are interchanged, the \naccurac y of the main and auxiliary tasks is 93.19 % and \n76.35 %, respectively . This shows  that the task  assigned as the \nmain task can achieve a relatively higher accuracy.  It is worth \nnoting  that the auxiliary  task has a regularization effect  to \navoid overfitting in the main  LSTM encoder.  The information \nin the auxiliary  encoder is helpful to the main task . This \nexplains  why AT-LSTM  has a better  generalization capacity  \nwhen the topic classification  and fake-news  classification are \nlearned simultaneously by using multi -task learning . \n \nC. The hyperparameter of the weights of  the loss function  \n We evaluate our results for different  values of \u03b1 in the loss \nfunction . It is found  that the optimal  value  of \u03b1 depends  on the \nspecific  task. In the experiment , we found  that using  a larger  \nvalue  \u03b1 can achieve  a better  performance  when  topic  \nclassification  is the main  task, i.e. \u03b1=0.7, as shown  in Fig. \n3. By contrast , a smaller  value  \u03b1 is required  for achieving  a better performance  for fake-news  classification , i.e. \u03b1=0.5. \nFor each of the two tasks,  when  the range  of value  \u03b1 is \nbetween  0.2 and 1, the classification  task assigned  as the main  \ntask can always  achieve  a better  performance . This is due to \nthe fact that the auxiliary  encoder  can generate  common  \nfeatures  across  two tasks  and help to boost  the performance  of \nthe main  classification  task. \n \n \nFigure 3. The effect of different values of \ud835\udec2 for topic \nclassification . (a) AT-LSTM: Topic classification is the \nmain task. (b) AT-LSTM: Topic classification is the \nauxiliary task. (c) Single -task LSTM.  \n \n \n       \nFigure 4. The effect of different values of \ud835\udec2 for fake news \nclassification  (a) AT-LSTM: Fake news classification is \nthe main task. (b) AT-LSTM: Fake news classification is \nthe auxiliary task. (c) Single -task LSTM.  \n \nD. Error Analysis  \n From  Fig. 5, the accuracy  of correctly  classifying  real and \nfake news  articles  is 96% and 88%, respectively . This means  \nthat classifying  fake-news  articles  is more  difficult  than real-\nnews  articles . One of the possible  reasons  for this is that the \ndatabase  is imbalanced , so that there  are more  real-news  \narticles  than that of fake-news  articles  in the dataset.  \n From  Fig. 6, we can see that the category  \u201cSports\u201d  has the \nhighest  accuracy , which  is 86%. It means  that the sports  news  \narticles  are more  discriminative  than the other  categories.  On \nthe other  hand,  those  business  news  articles  have the lowest  \naccuracy,  which  is 68%. These  news  articles  are more  difficult  \nto classif y, because  they are close  to the politics  and science/  \nProceedings, APSIPA Annual Summit and Conference 2020 7-10 December 2020, Auckland, New Zealand\n379\ntechnology  categories . We can see that the business  and \nscience/technology  categories  are most  confused . This is \nreasonable  and acceptable , because  a news  article  may have \nmore  than one topic , and the news  websites  do not label  them  \nseparately.  \n  \nFigure 5. The confusion matrix of fake-news \nclassification using AT -LSTM.  \n \n   \nFigure 6. The confusion matrix of topic classification  \nusing AT -LSTM.  \n \nV. CONCLUSION  \n In this paper , we have constructed  a new benchmark  \ndataset  for both the research  of fake-news  classification  and \ntopic  classification . Our experiment  result s have show n that \nusing  the LSTM  encoders  can achieve  a better  performance  \nthan the textCNN  encoders , for both tasks  trained  by single -\ntask learning . In order  to train the two tasks  simultaneously,  \nwe propose d an auxiliary -task long short -term memory (AT -\nLSTM)  framework to jointly  learn  these  two tasks.  Our model  \ncan achieve  better  performance  than the multi -task LSTM  \nnetwork  with hard parameter  sharing, due to the generalization  \ncapacity  of the auxiliary  task. In our future  work , we will consider  more  tasks , such as sentiment  analysis , for text \nclassification  via multi -task learning.  \n \nREFERENCES  \n[1] Alexandre Bovet, Hernan A. Makse , \u201cInfluence of fake news in Twitter \nduring the 2016 US presidential election ,\u201d Nature Communications  10, \n2019 . \n[2] Andreas Vlachos, Sebastian Riedel , \u201cFact Checking: Task definition \nand dataset construction, \u201d Proceedings of the ACL 2014 Workshop on \nLanguage Technologies and Computationa l Social Science , 2014,  pp. \n18\u201322. \n[3] William Yang Wang, \u201cLiar, Liar Pants on Fire\u201d: A New Benchmark \nDataset for Fake News Detection,  \u201d Proceedings of the 55th Annual \nMeeting of the Association for Computational Linguistics , 2018, pp.  \n422\u2013426. \n[4]  https://github.com/GeorgeMcIntire/fake_real_news_dataset, accessed \n20 October 2018 . \n[5] B.S. Detector. A browser extension that alerts users to unreliable news \nsources. [Online]. Available: http://bsdetector .tech/ . \n[6] Ken Lang, \u201cNewsweeder: Learning to fi lter netnews,\u201d  Proceedings of \nthe Twelfth International Conference on Machine Learning , 1995 , pp. \n331-339. \n[7] D. D. Lewis. Reuters -21578 text Categorization test collection. \nDistribution 1.0. README file (version 1.2). Manuscript, September \n26, 1997.  \n[8] Yoon Kim, \u201cConvolutional Neural Networks for Sentence \nClassification,\u201d Conference on Empirical Methods in Natural \nLanguage Processing , 2014.  \n[9] Sepp Hochreiter, Jurgen Schmidhuber, \"Long Short -term memory,\u201d \nNeural Computation , 1997, pp.1735 -1780.  \n[10]    Victor Sanh, Thomas Wolf, Sebastian Ruder , \u201cA Hierarchical Multi -\ntask Approach for Learning Embeddings from Semantic Tasks,\u201d  \nThirty -Third AAAI Co nference on Artificial Intelligence , 2019 . \n[11]    Navonil Majumder, Soujanya Poria, Haiyun Peng, Niyati Chhaya, Erik \nCambria, Alexander Gelbukh , \u201cSentiment and Sarcasm Classification \nwith Multitask Learning,\u201d  IEEE Intelligent Systems  34(3), 2019 . \n[12] Liebel, Lukas and Marco K\u00f6rner , \u201cAuxiliary Tasks in Multi -task \nLearning.\u201d ArXiv abs/1805.06334 , 2018.  \n[13] Bird, Steven, Edward Loper and Ewan, \u201cNatural Language Processing \nwith Python ,\u201d. O\u2019Reilly Media Inc , 2009.  \n[14]  Jeffrey Pennington , Richard Socher , Christopher D. Manning, \u201c Glove: \nGlobal vectors for word representation,\u201d  Proceedings of the \nConference on Empirical Methods in Natural Language Processing , \n2014,  pp. 1532 \u20131543 . \n[15] Joulin, Armand and Grave, Edouard and Bojano wski, Piotr and \nMikolov, Tomas, \u201c Bag of Tricks for Efficient Text Classification,\u201d  \narXiv preprint arXiv:1607.01759 , 2016.  \n[16] GangLiu, JiabaoGuo , \u201cBidirectional LSTM with attention mechanism \nand convolutional layer for text classification ,\u201d Neurocomputing , vol. \n337, 2019 , pp. 325 -338. \n[17]   Pengfei Liu, \u201c Recurrent Neural Network for Text Classification with \nMulti -Task Learning,\u201d Proceedings of the Twenty -Fifth International \nJoint Conference on Artificial Intelligence , 2016.  \n \n \n \n \n \nProceedings, APSIPA Annual Summit and Conference 2020 7-10 December 2020, Auckland, New Zealand\n380", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Simultaneous Fake News and Topic Classification via Auxiliary Task Learning", "author": ["T Cheung", "K Lam"], "pub_year": "2020", "venue": "2020 Asia-Pacific Signal and Information \u2026", "abstract": "Using social media, in particular, reading news articles, has become a necessary daily activity  and an important way of spreading information. Classification of topics of new articles can"}, "filled": false, "gsrank": 163, "pub_url": "https://ieeexplore.ieee.org/abstract/document/9306354/", "author_id": ["v5UKVvAAAAAJ", "6yK7bewAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:FveqVCxl3j0J:scholar.google.com/&output=cite&scirp=162&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D160%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=FveqVCxl3j0J&ei=IbWsaPvmC7_SieoPzJnloAQ&json=", "num_citations": 1, "citedby_url": "/scholar?cites=4458111922216826646&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:FveqVCxl3j0J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "http://www.apsipa.org/proceedings/2020/pdfs/0000376.pdf"}}, {"title": "Examining the alternative media ecosystem through the production of alternative narratives of mass shooting events on Twitter", "year": "2017", "pdf_data": " \n \nExamining the Alternative Media Ecosystem through the Production\nof Alternative Narratives of Mass Shooting Events on Twitter \nKate Starbird  \nUniversity of Washington, HCDE \nkstarbi@uw.edu \n \n \n \nAbstract \nThis research explores the alternative media ecosystem \nthrough a Twitter lens. Over a ten-month period, we col-\nlected tweets related to alternative narratives\u2014e.g. conspir-\nacy theories\u2014of mass shooting events. We utilized tweeted \nURLs to generate a domain network, connecting domains \nshared by the same user, then conducted qualitative analysis \nto understand the nature of different domains and how they \nconnect to each other. Our findings demonstrate how alter-\nnative news sites propagate and shape alternative narratives, while mainstream media deny them. We explain how politi-\ncal leanings of alternative news sites do not align well with \na U.S. left-right spectrum, but instead feature an anti-\nglobalist (vs. globalist) orient ation where U.S. Alt-Right \nsites look similar to U.S. Alt-Left sites. Our findings de-scribe a subsection of the emerging alternative media eco-\nsystem and provide insight in how websites that promote \nconspiracy theories and pseudo-science may function to \nconduct underlying political agendas. \nIntroduction   \nIn the aftermath of major political disruptions in 2016\u2014in \nBritain with the Brexit vote an d in the United States with \nthe election of Donald Trump to the presidency\u2014there has \nbeen widespread attention to and theorizing about the prob-\nlem of \u201cfake news\u201d. But this  term is both amorphous and \ncontested. One perspective locates the problem within the \nemerging ecosystem of alterna tive media, where the term \nhas been applied to refer to \u201cclickbait\u201d content that uses \ntabloid-style headlines to attr act viewers for financial rea-\nsons (Silverman & Alexander 2016) and to describe politi-\ncal propaganda intentionally planted and propagated \nthrough online spaces (Timberg 2016). Challenging these definitions, alternative media outlets have appropriated the term to attack \u201cmainstream\u201d media for its perceived eco-\nnomic and political biases and for hosting inaccurate or under-sourced content (e.g. Rappoport 2016). Beneath this \n                                                \nCopyright \u00a9 2017, Association for the Advancement of Artificial Intelli-\ngence (www.aaai.org). All rights reserved. \n rhetoric, we are seeing trad itional new providers and emer-\ngent alternative media battle not only for economic viabil-\nity, but over accepted methods of how information is \nshared and consumed, and, more profoundly, for how nar-\nratives around that informati on are shaped and by whom.  \n This research seeks to provide a systematic lens for ex-ploring the production of a certain type of \u201cfake news\u201d\u2014 alternative narratives  of man-made crisis events. For three \nyears, our research group has examined online rumoring \nduring crises. Over that time,  we noted the presence of \nvery similar rumors across many man-made crisis events\u2014\nincluding the 2013 Boston Marathon Bombings, the down-\ning of Malaysia Airlines flight MH17, and several mass shooting events including those at Umpqua Community College in Oregon (October, 2015). For each event, rumors \nclaimed the event had been perpetrated by someone other \nthan the official suspects\u2014that it was instead either a \nstaged event performed by \u201ccrisis actors\u201d or a \u201cfalse flag\u201d \norchestrated by someone else. Both explanations claimed \nthat a powerful individual or  group was pulling the strings \nfor political reasons. Interestingly, though the arguments and evidence used to support these alternative narratives \nwere somewhat consistent across events, the motives cited \nwere often very different\u2014e.g. from the U.S. government trying to support gun control to coordinated global actors staging violence to motivate military intervention.  \nFor this paper, we utilize this type of conspiracy theory \nor alternative narrative  rumor as an entry point for under-\nstanding the ecosystem of alternative media. We examine the production of these narratives through Twitter and across the external websites that Twitter users reference as they engage in these narratives. We propose and demon-\nstrate that this lens\u2014Twitter data from mass shooting \nevents and our method for utilizing that data to reveal and \nexplore connections across web domains\u2014provides a sys-\ntematic approach for shedding light on the emerging phe-\nnomena of alternative media and \u201cfake news\u201d.  \nOur contributions include an increased understanding of \nthe underlying nature of this subsection of alternative me-Proceedings of the Eleventh International AAAI Conference on Web and Social Media (ICWSM 2017)\n230\ndia\u2014which hosts conspiratorial content and conducts vari-\nous anti-globalist political agendas. Noting thematic con-vergence across domains, we theorize about how alterna-tive media may contribute to conspiratorial thinking by \ncreating a false perception of information diversity. \nBackground \nAlternative Narratives of Man-Made Crisis Events \nWe define \u201calternative narrative\u201d as an explanation of the causes of a man-made disaster that runs counter to the mainstream narrative. Exampl es include 9-11 trutherism, \nclaims the Boston Marathon Bombings were perpetrated by U.S. Navy Seals, and theori es the 2012 shootings at the \nSandy Hook school were staged to motivate gun control legislation. These narratives can also be thought of as \u201cconspiracy theories\u201d which claim powerful people orches-trate events to exercise and protect that power, and that they conceal their role by framing others (Sunstein & Vermeule 2009). We can also understand the production of these narratives as a form of collective sensemaking (Shi-butani, 1966), whereby people attempt to reduce their un-certainty and anxiety and increase their sense of control by providing explanations of th e events (van Prooijen 2015)\u2014\nin this case, explanations that are informed by deep skepti-cism of official sources and an epistemic orientation that \nseeks causal, intentional expl anations for complex, random \nevents (Popper, 1945; Mandik, 2007).  This perspective surfaces a thorny issue for challenging alternative narratives in that once someone believes in a conspiracy theory of an event, it is extremely hard to dis-suade them from this belief (Sunstein & Vermeule 2009). Additionally, belief in one conspiracy theory correlates with an increased likelihood that an individual will believe in another (van Prooijen & Acker, 2015). \nAlternative Media and the Crisis in Journalism \nHere, we examine the activity of participating in and con-structing conspiracy theories online as it intersects with the \nemerging ecosystem of alternative media websites. Recent political events have catalyzed widespread attention to and theorizing about the \u201cproblem\u201d of \u201cfake news\u201d (e.g. Sil-verman & Alexander 2016; Ti mberg 2016). Please note we \nplace quotations around these terms, because their mean-ings are contested and in some cases inverted within the online conversations that we reviewed for this research. \n To better understand the role that these new media are playing\u2014and the arguments contesting the fake news framing\u2014we look to the literature on the \u201ccrisis in journal-ism\u201d, which describes how traditional news producers are struggling to maintain their audiences and adapt their prac-tices in a changing informa tion space. Journalism scholars have described this as a mu lti-dimensional problem, which \nincludes technological, economic and socio-political fac-tors (Fuller 2010; Siles & Boczkowski 2012).  While severely reduced advertisement revenues have undermined the economic models of professional journal-ism, technology advancements\u2014e.g. cheap ways of creat-ing and distributing content\u2014have altered traditional forms of reporting and enabled new forms of information produc-tion by everyday citizens (Gillmor 2004). Simultaneous to this rise of citizen journalism has been an erosion of the role professional journalists perform as information media-tors or gate-keepers\u2014with th e work of gatekeeping shift-\ning to end-users (Bruns, 2003 ). Empowered by online tools \nand the emerging information ecosystems, people can now seek out their own informa tion without relying upon jour-\nnalists to filter, synthesize and edit that content.   As traditional news producers and journalists struggle to adapt to these conditions, new media entities\u2014from Twit-ter accounts to quirky blogs to slick sites filled with web advertisements\u2014have assumed a role in delivering con-tent. This alternative media ecosystem has challenged the traditional authority of journalists, both directly and indi-rectly (Gillmor 2004; Siles & Boczkowski, 2012). Its de-\nvelopment has been accompanied by a decreased reliance on and an increased distrust of  mainstream media, with the \nlatter partially motivated by a perception of widespread ethical violations and corrup tion within mainstream media \n(Siles & Boczkowski, 2012). Indeed, many view these al-ternative news sites as more authentic and truthful than mainstream media, and these effects are compounding\u2014as research has found that exposure to online media correlates with distrust of mainstream  media (Tsfati, 2010). On the \npositive side, this rise of alternative media has led to what Gillmor has called \u201ca democratization of news production\u201d (2004), challenging information control by monolithic and in some places government-controlled media. However, with the loss of commonly-held  standards regarding infor-\nmation mediation and the absence of easily decipherable credibility cues, this ecosystem has become vulnerable to the spread of misinformation and propaganda.   In this paper, we discuss these phenomena by exploring a specific subsection of the alternative media ecosystem\u2014one focused on alternative narratives of mass shooting events\u2014to better understand the production of alternative news as its takes place through Twitter and across the on-line information space. \nMethods \nTwitter Data Collection \nWe collected data using the Twitter Streaming API, track-ing on the following terms (shooter, shooting, gunman, \n231\ngunmen, gunshot, gunshots, shoot ers, gun shot, gun shots, \nshootings) for a ten-month period between January 1 and \nOctober 5, 2016. This collection resulted in 58M total tweets. We then scoped that data to include only tweets \nrelated to alternative narrati ves of the event\u2014false flag, \nfalseflag, crisis actor, crisisactor, staged, hoax and \u201c1488\u201d. The latter term, which has symbolic meaning for white supremacists, appears often in tweets related to false flag \nnarratives. This final alternative narrative  collection con-\ntains 99,474 tweets.  \nMethods of Analysis \nAligned with previous work on online rumors (Maddock et al. 2015), we employ a mixed-method, interpretivist ap-proach to analyzing this data, blending qualitative, quanti-tative and visual methods to identify themes and patterns in the data from both macro- and micro-level perspectives.  \nMapping a Domain Network from Users\u2019 Tweets \nTo understand how the production of these narratives takes place across different Internet domains, we created a net-work graph of domains connected through user activity\u2014specifically the URL links sh ared in their tweets.  \n In this graph (see Figures 1-3), nodes are domains refer-enced in the alternative narrative  collection. To generate \nthe nodes, we first identify every distinct domain that is linked to by a tweet in the se t. 77,461 (or ~78%) of tweets \nin the collection contain a URL, and together they refer-ence 1572 distinct domains. These became the initial nodes for the graph.  To create edges between nodes, we look to the tweet patterns of each user, connecting two nodes if the same user posted tweets referencing both domains. Similarly, the \nstrength of the edge grows proportionally to the number of users who shared tweets re ferencing both domains. Of \n15,150 users who sent at leas t one tweet with a link, only \n1372 sent (over the course to the collection period) tweets citing more than one domain. The graph is therefore gener-ated by these relatively high volume and high source-diversity, alternative narrative tweeters.  Due to their high rates of connectivity to other sites as well as the different meaning encoded in those connec-tions\u2014related to tool use rather than content affinity\u2014we removed all domains associated with social media services \n(e.g. youtube.com, twitter.com, instagram.com, pinter-est.com, facebook.com, reddit.com) and all general link shortener services (e.g. bit.ly ). We combined links related \nto domain-specific shortener services with that domain\u2014\ne.g. treating nypost.com and nyp. st as the same domain. \nFinally, we trimmed the graph by removing domains that were linked-to less than five times (total) and removing edges that were created by fewer than three users.  The resulting network graph represents how different domains are connected, through the posting activity of Twitter users, within the alternative narrative discourse surrounding mass shooting events. \nQualitative Content Analysis of Web Domains \nWe then conducted qualitative content analysis to under-\nstand the nature of each domain\u2014i.e. the website hosted on that domain\u2014in the resulting graph. We limited this analysis to the 117 nodes that are connected to the central graph (see Figure 1).  The content for the domain analysis included tweets from our alternative narrative  collection that referenced \nthat domain and the articles linked-to within those tweets, content on the current front page of the website, and if available the \u201cAbout\u201d or similar page on the website. Addi-tionally, we used Google and other online resources to try to determine the background of  the owners, editors, and \nwriters for the website. We also used Google to search for and retrieve articles within the domain that included certain terms that we identified as related to persistent themes across the sites\u2014e.g. globalism, New World Order (NWO), (George) Soros, Koch  (brothers), Rothschilds, \nvaccines, GMOs, Black Lives Matter, Chemtrails\u2014and marked domains that had a significant number of articles \ndiscussing each topic. Finally, we leveraged existing online tools that provide data about  domains (e.g. web traffic, \nlocation) as additional information sources. \nQualitative Coding \nWe then classified each account along a number of dimen-sions. In defining these dimensions and the codes within them, we took a grounded (bottom-up) approach, working to develop a classification scheme to fit our data. This process was highly iterative, involving several rounds of coding for each account before settling on the final codes. All of the classification was done by the first author, who was immersed in this research . In this work, we\u2019ll focus on \nfour dimensions: \nAccount Type: We labeled each account as being Main-\nstream Media, Alternative Media or Blog, Government \nMedia, or Other.  \nNarrative Stance Coding: For each domain, we examined \nall of the tweets in our alternative narrative  collection that \nlinked to that domain and read all of the linked-to articles \nto determine how that domain was referenced in the con-struction (or correction) of alternative narratives.  Each do-\nmain was coded as supporting the alternative narrative, denying the alternative narrative, or for primarily being cited as evidence of the alternative narrative without di-rectly referring to it. Domains that did not fall into one of \nthese categories were coded as unrelated.  \nPrimary Orientation: Content analysis revealed several \ncommon themes among the alternative news domains, in-cluding (due to the underlying nature of our data) wide-spread sharing of conspiracy theories and pseudo-science \n232\nclaims. For some sites, this content seemed to be shared for \nentertainment\u2014i.e. driving ad revenue.  In others, it seemed \nto be shaped around or utilized in service of a particular political agenda.  We attempted to disentangle the two, cod-\ning each domain for its \u201cprima ry\u201d orientation as communi-\ncated through the content on the (current) home page of its website and its About page, or inferred from the publically-available biographical informa tion of its owners and writ-\ners. We noted four categories: Traditional News, Clickbait \nNews, Primarily Conspiracy Theorists/Pseudo-Science \nEvangelists, and sites with a strong Political Agenda. \nPolitical Leaning: Finally, we coded the political leaning \nof each domain. It is important to note that the first author \nis a left-leaning individual who receives her news primarily through mainstream sources and who considers the alterna-tive narratives regarding these mass shooting events to be false. This may have affected how the content on these domains was perceived and classified. \nLeaning Description \nU.S. Alt Right U.S. focuse d, anti-mainstream media, \npro-Christian, anti-LGBT, anti-\nglobalist, climate change denying  \nU.S. Alt Left U.S. focuse d, anti-mainstream media, \nanti-corporatist, critical of police, pro-prison reform, pro-BlackLivesMatter \nInternational Anti-Globalist Internationally focused, anti-globalist \nor anti-New World Order/Cabal, anti-corporatist, conspiracy-focused \nWhite Nationalist and/or Anti-Semitic  primarily white-nationalist or anti-\nSemitic positions \nMuslim Defense primarily challenges mainstream narra-\ntives of terrorist attacks by Muslims \nRussian Propaganda primarily supports Russian interests, \nanti-globalist \nTable 1. Political Leaning of Alternative News Accounts \n For mainstream sources, we coded each along a spec-\ntrum of left, left-leaning, cent er, right-leaning, right and as \nbeing either U.S.- or Internationally-focused. For alterna-\ntive media whose political leanings do not align with the U.S. left (liberal) to right (conservative) categories, after \nconsiderable iteration, we identified three general catego-ries that could be used to cl assify most of the accounts and \nthree \u201cother\u201d categories that had a handful of significant accounts each (see Table 1). We elected to adopt the \u201cAlt-Right\u201d term, though we acknowledge that it is a dynamic and amorphous term that has been applied to obscure con-nections to the white-nationalist movement (Caldwell, 2016). For balance, we also utilize an Alt-Left label, and indeed we identified a handful of accounts in our set that fell into that category.  To make these determinations, we \nemployed original content analysis and leveraged existing categorizations from sites such as mediabiasfactcheck.com. Due to considerable thematic convergence across alterna-tive news sites (around political issues as well as views on climate change, vaccines and GMOs), we utilized stances on LGBT issues and Black Lives Matter narratives to dis-tinguish between U.S. Alt-Right and U.S. Alt-Left.  \nInterpretive Analysis \nAfter coding each domain, we then explored patterns, con-\nnections, and anomalies across thematic categories in rela-tion to the network graph using interpretive analysis of domain and tweet content. \nFindings \nAlternative Narratives through Tweets and Links \nWe collected tweets related to shooting events for more than ten months in 2016. This  time period included several \nhigh profile shooting events, in cluding mass shootings with \ncivilian casualties at an Orlando, FL nightclub on June 12, in a shopping district in Muni ch, Germany on July 22, and \nat a mall in Burlington, WA on September 23. Each of these events catalyzed considerable discussion online and elsewhere about the details and motives of the attack\u2014including claims of the attack being a \u201cfalse flag\u201d.  More than half of our alternative narrative  collection \n(30,361 tweets) relates to the Orlando event, including: \n@ActivistPost: \"Was Orlando Shooting A False \nFlag? Shooter Has Ties To FBI, Regular At Club, \nDid Not Act Alone? <link1>\" \n This tweet is typical of an alternative narrative tweet, \nleveraging uncertainty in the form of a leading question (Starbird et al. 2016) to pres ent its theory. The linked-to \narticle\u2014whose title is the content of this tweet\u2014presents evidence to support the theory, including facts about the case (such as previous contact between the FBI and the shooter) and perceived connections to past events that are similarly claimed to be fals e flags. The underlying theme \nhere is that the U.S. government perpetrated the shooting with the intention of blaming it on Islamic terrorism. This \ntweet\u2019s author, the ActivistPost, is associated with one of the central nodes in our netw ork graph (see Figures 1-3), \nreferenced in 191 tweets by 153 users and connected (by user activity) to a relatively high number of other domains.  The following tweet, by an account associated with a domain that has a strong edge tie with ActivistPost, for-\nwards a similarly themed alternative narrative: \n@veteranstoday: Orlando nightclub shooting: Yet \nanother false flag? -  <link2> looks like another \nPR extravaganza <photo> \n                                                \n1 http://www.activistpost.com/2016/06/was-orlando-shooting-a-false-\nflag-shooter-has-ties-to-fbi-regular-at-club-did-not-act-alone  \n \n233\n This article was linked-to 147 times in our data. The \ntweet and the article feature an image with the title, \u201cOmar Mateen: Patsy or MK Mind-Control Slave\u201d. The term patsy  is often used to label an accused perpetrator who has \nbeen framed for the incident by government or other pow-erful groups. MK Mind-Control  refers to a CIA project that \nexperimented with mind control in the 1950s. This specu-lative tweet and related article therefore present two poten-tial explanations of the Orla ndo shooting event, both build-\ning off alternative narratives used in previous events. The underlying claim here is that  the named suspect was not \nresponsible for the Orlando shootings, but that the U.S. government was. This claim is extended in the article to apply to other violent acts attr ibuted to Muslim terrorists. \n Alternative narratives around the Munich shooting had a similar theme, though blame wa s pushed onto international \ngeo-political actors: \nDesperate Zionists Commit Another Fraud with Mu-\nnich Shooting Hoax - NODISINFO <link3>                                                    \n The above tweet links to an  article (tweeted 54 times) \nwithin the nodisinfo.com domain, one of the most highly \ntweeted and highly connected domains in our data. Citing \nphotographic evidence from the scene, the article claims \nthat the shooting was a drill, staged by crisis actors . All of \nthese terms echo other alternative narratives of other events. Diverging from the Orlando narratives, which blame the U.S. government, in this case the accused \u201creal\u201d perpetrators are Zionists\u2014echoing long-active narratives about covert power wielded by Jewish bankers and others. \nThe article offers no evidence to support that connection other than reference to other \u201cstaged\u201d events.  The Cascade Mall Shooting in Burlington, Washington \nreferenced a third kind of alte rnative narrative that has ap-\npeared after many U.S.-based shootings, including the Sandy Hook School shooting in 2012 and the Umpqua School shooting in 2015. This narrative claims that these mass shooting events are again staged using crisis actors , \nbut in this case by the left-leaning U.S. government to pro-vide a political basis for reducing gun rights. \nAbsence Of Footage Of Wounded/Deceased Victims. \nMedia Were Told Victims Remained In The \nMall #Cascade #FalseFlag <link4>  \n This tweet suggests that there were no actual victims of \nthe event. It links to an ar ticle on the memoryholeblog.com \ndomain, which also has a relatively high degree in our network graph and was tweeted 125 times. The linked-to article assembles evidence to make a case for the event \n                                                                              \n2 http://www.veteranstoday.com/2016/06/12/orlando/ \n3 http://nodisinfo.com/desperate-zionists-commit-another-fraud-munich-\nshooting-hoax/  \n4 http://memoryholeblog.com/2016/09/24/cascade-mall-shooting-\nobservations-an-active-shooter-drill/ being a drill and describes an outlook that connects several \nevents to this narrative: \u201cSuch events are reported on by major news media uncritically, thus supporting the call for strengthened gun control measures. [\u2026]\u201d  Interestingly, the second most highly referenced event in our alternative narrative  collection from 2016 (at 5,914 \ntweets) is the Sandy Hook shootings, which occurred in 2012. Though a large portion of those tweets contest or deny that alternative narrative, several utilize Sandy Hook \u201cevidence\u201d to support alternative narratives around more recent events. For example: \nOrlando shooting was a hoax. Just like Sandy \nHook, Boston Bombing, and San Bernandino. Keep \nbelieving Rothschild Zionist news companies. \nMore Orlando shooting Hoax \u2013 proof - same actors \nin Sandy hook & Boston Marathon Fake bombing - \ngun take away agenda. \n These two tweets both connect the Orlando Shooting to \nclaims that Sandy Hook was a hoax.  In the first, the author \nrefers to the \u201cRothschild Zionist news companies\u201d, a refer-ence to anti-globalist and anti-media viewpoints that ap-pear as major themes across many alternative news sites. The second tweet connects Orlando to Sandy Hook (and paradoxically the Boston Marathon bombings) as part of an ongoing agenda to reduce gun rights in the U.S.  Taken together, these examples describe a few of what turns out to be a collection of distinct alternative narratives that share several common features. As the above tweets highlight at the micro-level, at the macro-level our domain \ndata demonstrate that different alternative narratives are connected across users and s ites\u2014e.g. some users refer-\nence both memoryholeblog.com (which assigns blame to \nU.S. government officials tryi ng to take away gun rights) \nand veteranstoday.com and/or  nodisinfo.com (which theo-\nrize that international conspirators set up these events to further their political agendas by falsely blaming Muslim terrorists). Our tweet and domain data suggest that the pro-duction of these narratives is a distributed activity where \u201csuccessful\u201d elements (e.g. drills, crisis actors, Zionist con-spirators) of one narrative are combined with others in a mutually reinforcing manner.  \nInfluential Domains in Alternative Narratives \nTable 2 lists a selection of the most influential domains, indicating the number of tweets that link to it, the number of distinct users who cite it, and the number of other do-mains to which it connects in the network graph.  Interesting, the two most highly tweeted domains were both associated with signi ficant automated account or \n\u201cbot\u201d activity. The Real Strategy, an alternative news site with a conspiracy theory orie ntation, is the most tweeted \ndomain in our dataset (by far).  The temporal signature of \n234\ntweets citing this domain reveals a consistent pattern of \ncoordinated bursts of activity at regular intervals generated by 200 accounts that appear to be connected to each other (via following relationships) and coordinated through an external tool. They were occasionally retweeted from out-side their group, resulting in  many weak connections to \nother alternative media domains. Though we consider this domain in our research, we removed its node from our network because its bot-driven activity distorts the graph. \n \nDomain Degree # Tweets # Users \ntherealstrategy.com 37 7436 1025 \ninfowars.com 1 1742 1671 \nnewsbusters.org 14 1217 1215 \nwashingtonpost.com 18 1121 1074 \nnodisinfo.com 64 779 192 \nnytimes.com 22 759 594 \nbeforeitsnews.com 55 618 394 \nveteranstoday.com 58 615 497 \nfoxnews.com 13 300 313 \ndcclothesline.com 20 286 177 \nactivistpost.com 33 191 153 \nyournewswire.com 32 163 117 \nTable 2. Influential Domains in Alternative Narrative Tweets \n The InfoWars domain, an alternative news website that focuses on Alt-Right and conspiracy theory themes, was the second-most tweeted domain,  but as (Figure 1) shows it \nwas only tenuously connected to one other node. Examin-ing tweets that referenced this domain, we noted a large number (1609) of similarly-named and -aged accounts that sent a single tweet in our collection. This activity was very likely automated, though not as sophisticated as that from \nThe Real Strategy. We were unable to determine who op-erated this bot\u2014all of the suspect accounts are currently suspended from Twitter.  The other domains in this list include both mainstream media and alternative media. Though both types of do-mains are cited in the production of alternative narratives, our analyses show that they are cited in different ways for different purposes. \nA View of the Altern ative News Ecosystem \nFigure 1 shows the domain networ k graph. In this graph, \nnodes are sized proportionally to the total number of tweets that linked to the domain, and they are connected when one user wrote different tweets citi ng each domain. In this first \nview, we distinguish domains  by media type, with main-\nstream media in Purple, alternative media in Aqua, and government-controlled media (e.g. RT.com) in Red.  80 of 117 accounts in our graph were classified as alter-native media or blogs. We borrow the term and the mean-ing of \u201calternative\u201d from our analysis of the About pages of several of these domains, which claim the sites were set up as an alternative to \u201ccorporate-controlled\u201d media. Ac-cording to them, their method of operation runs counter to mainstream media, in that they do not intend to serve as traditional information mediator s, but instead are here to \njust present \u201cthe facts\u201d and let readers use their \u201ccritical thinking skills\u201d to \u201cmake up their own minds\u201d. This lan-guage is repeated across many of these sites, though some of them use slightly different  terms such as \u201cindependent\u201d \nor \u201canti-media\u201d to mark their distinction from mainstream.  \nFigure 1. Domain Network Graph, Colored by Media Type \nPurple = mainstream media; Aqua = alternative media;  \nRed = government controlled media \n  The graph shows a tightly connected cluster of alterna-tive media domains (upper left)\u2014suggesting that many users are citing multiple alternative news sites as they con-struct alternative narratives. W ithin that cluster, the three \nmost-highly tweeted and most  connected domains are No-\nDisinfo, VeteransToday and Be foreItsNews. NoDisinfo is \na site devoted to providing alternative narratives of terrorist events where the primary suspect is affiliated with an Is-lamic terror group. VeteransToday is an alternative news site that promotes a U.S. Alt Right, anti-globalist political agenda, including strong anti-Semitic themes. BeforeIts-News acts as an aggregator of many conspiracy theory and pseudo-science articles from other sites. These three sites may have different motivations and goals, but they all promote alternative narratives of mass shooting events, and many of these narratives have very similar elements.   This convergence of themes extends to other sites in this network and to other topics. For example, a majority of the alternative media domains in the graph host various con-tent that is anti-globalist, anti-vaccine, anti-GMO, and anti-climate science (themes that may not seem consistent with a single worldview). Additionally, in late December 2016,  \n \n235\nwhen the topic of \u201cfake news\u201d was trending in the main-\nstream media, almost all of these domains contained arti-cles claiming that the mainst ream news was \u201cfake news\u201d \nand only alternative media had the true facts.   Underlying this convergence of themes, many of these sites aggregate news so the same articles (and authors) appear across multiple domains. For example, in our data, there are 147 tweets linking to the article about the Orlando shooting on the VeteransToday.com domain. 100 other tweets link to the same article\u2014same text, same author\u2014hosted on different domains. In other cases, articles hosted on one domain synthesize content from external sources, often excerpting long passages. So someone can be citing an article originally posted by ActivistPost synthesizing content from RT through a tweet  linking to BeforeItsNews. \n There are two highly tweeted alternative media domains that are not tightly connected to the others. The first is InfoWars, discussed above. The other, NewsBusters, is a conservative site with a mission of confronting left-wing bias in media. Though it appear s to have a right political \nagenda, it does not participate in the widespread conspir-acy theorizing evident across most of the other alternative media sites. Indeed, it is cited in our data for a tangentially-related \u201choax\u201d claim that mainstream media was disin-\ngenuously shaping the narrative around police shootings.  Mainstream media are present in the graph, but they are somewhat peripheral and have relatively few connections to the alternative media domains, especially considering the overall number of tweets they receive. The subsequent analyses shed more light on the distinctions and connec-tions between alternative and mainstream media. \nAlternative Media Promoting Alternative Narratives \nIn producing alternative narratives, Twitter users cite con-\ntent from external domains in a few different ways. In the first way, they cite an article  that presents an alternative \nnarrative in ways that support and propagate those claims: \nWitnesses Describe Multiple Gunmen in Orlando \nShooting False Flag <link> \n<@mention> well there were multiple gunmen, yet \nANOTHER FALSE FLAG ATTACK: <link5> \n The above tweets both reference an article at yournews-\nwire.com that claims the Orlando shooting was a false flag. The first tweet is simply the title of the article, likely gen-erated by a button on the website, while the second tweet contains the same claim in the tweet author\u2019s own words \nwith the link cited as evidence to support that claim. \n Other tweets link to articles about the event that follow \nthe mainstream narrative, but the tweet text challenges that explanation, often presenting details from those articles as \n                                                \n5 http://yournewswire.com/witnesse s-describe-multiple-gunmen-in-\norlando-shooting-false-flag/ evidence of the conspiracy. The tweet below links to a To-ronto Star (mainstream media)  article describing the Or-\nlando shooting, and suggests that details of the event (e.g. shooter calling 911) indicate a false flag: \n#Actor Luis Burbano #Orlando Shooter called 911 \nthree times before the killing <link6> #Falseflag \nFigure 2. Domain Network Graph, by Narrative Stance \nBlue = supports; Red = denies; Green = used as evidence;  \nWhite = tweets unrelated \n In other cases, conspiracy theorists tweet articles from \nsites that deny the conspiracy theories, but do so in a con-frontational way\u2014often as more evidence of their theory: \nNEW YORK TIMES playing damage control as more ppl \nare becoming aware of #FalseFlag attacks <link7> \n This tweet links to an article from the New York Times \nthat describes and challenges alternative narratives of the Orlando shooting event. The tweet content suggests main-stream media is being employed in this case to help sup-port the conspiracy and mislead the public. There were numerous reactions like this to this article, which explains why the nytimes.com domain has so many connections to alternative media sites in the graph.  Though not at the volume as those promoting the alter-\nnative narratives, some Twitter users do challenge these narratives, at times by tweeting or retweeting articles that deny them. Unfortunately, the research suggests that such corrections are likely to backfire (Nyhan & Reifler 2010).  Not surprisingly, when we look at connections between \ntweets, accounts, and stance towards an alternative narra-\n                                                \n6 https://www.thestar.com/news/world/2016/06/13/last-of-49-bodies-of-\nshooting-victims-removed-from-orlando-gay-nightclub.html \n7 https://www.nytimes.com/2016/06/29/us/after-orlando-shooting-false-\nflag-and-crisis-actor-conspiracy-theories-surface.html  \n \n236\ntive (Figure 2), we see that alternative media sites are gen-\nerally cited to promote thes e theories, while mainstream \nmedia are A) cited for neutral content as evidence to sup-port these theories; or B) cited for a denial of the alterna-tive narrative to promote and/or counter-attack that denial. 66 of 80 alternative media accounts in our data hosted arti-cles promoting an alternative narrative of a mass shooting. No mainstream media domains had articles supporting any of the alternative narratives of mass shooting events and seven had articles explicitly denying one or more of them.  \nPolitical Stances of the Alternative Media Ecosystem \nThrough in-depth content analysis of the web content \nhosted there, we determined the primary orientation and political leanings of each dom ain in our graph. 44 of 80 \nalternative media domains were coded as primarily for-warding a political agenda. Th e political leanings of the \nalternative media domains did not align well to U.S.-based notions of left (liberal) versus right (conservative). Instead, the most salient dimension wa s around the issue of global-\nism. Almost all of the alternative media domains contained significant content around anti-globalist themes, though the meaning of globalism seemed to vary somewhat across domains, a finding aligned with research that suggests the term means many different things  to the different groups of \npeople who oppose it (Muddle 2004). In our data, anti-globalist sentiment echoes within the stated motivations of many alternative media websites, which claim to challenge the corporate (globalist) controlled narratives of main-stream media. Though few domains explicitly articulated their anti-globalism as nationalism, research suggests that this theme is a strong orga nizing theme amongst nationalist \npopulist political groups that are gaining power in Europe and elsewhere (Muddle 2004).   Likely due to the nature of our underlying data, many of the alternative media domains in our graph contain consid-erable material referencing various anti-globalist conspir-acy theories, including ones th at claim high-powered peo-\nple (Illuminati, bankers, George Soros, Jews) are manipu-lating the media and world events for their benefit.  After several rounds of iterative analysis to identify commonalities and distinctions across clusters of accounts, we identified three prominent political agendas: U.S. Alt Right, U.S. Alt-Left, and Inte rnational Anti-Globalist. We \nrecognize that the Alt-Right term is problema tic (Caldwell \n2016; Griffiths 2016) as it has been employed to legitimize racist ideologies and appropriated by alternative news sites like Breitbart as a political tool of right-wing populism. In our application, we are both acknowledging those mean-\nings and calling attention to their connection to the content and purpose of alternative media. We applied this term to domains that had content primarily designed for a U.S. audience that were both anti-globalist and socially conser-vative (e.g. anti-LGBT, anti-f eminist, anti-immigrant).   We also found evidence of a non-traditional, U.S. left-leaning political agenda that incorporated anti-globalist \nthemes. Though much of the conspiratorial and political content on these sites was similar to or the same as content on the Alt-Right sites (many arti cles criticized U.S. Presi-\ndent Obama and Hillary Clinton), the U.S. Alt-Left dif-fered in that it had a liberal/p rogressive view towards so-\ncial issues (e.g. pro-LGBT , pro-Black Lives Matter). \nFigure 3. Domain Network Graph, by Political Stance \nPink = U.S. Alt-Right ; Aqua = U.S. Alt-Left; Green = Intl. Anti-\nGlobalist; Black = White Nationalist/Anti-Semitic; White = other. \n The International Anti-Globalist domains concentrated on geopolitical topics around the world. These sites shared \na strong focus on challenging mainstream media and the political agendas of the U.S. and other Western European countries. All contained content that was supportive of recent Russian actions in Syria and defensive of Russia\u2019s supposed actions to impact the U.S. election. These pro-Russian themes were also widespread within the U.S. Alt-Right domains, but they were most salient on the Interna-tional Anti-Globalist sites.   Of the 44 alternative media domains coded as primar-ily forwarding a political agenda, 22 were U.S. Alt-Right, seven were International Anti-Globalists, and four were U.S. Alt-Left. Figure 3 show s how those agendas were \ndistributed across our domain ne twork graph. In addition to \nthese, our data also featured six domains that were primar-\nily promoting White Nationalism and/or Anti-Semitism, two that were primarily defenders of Islam and Muslims (including NoDisinfo. com), and two that were clearly Rus-\nsian Propaganda. There were also two Russian Govern-ment Media, not counted among the alternative new sites.  One question that we had going into this research in-volved how integrated or sepa rated Alt-Left and Alt-Right  \n \n237\nideologies were. During our qualitative coding we identi-\nfied a total of seven Alt-Left sites, including three that were cited in tweets for articles supporting the alternative narrative of a mass shooting. In other words, the alternative narratives are spreading among domains from both sides of \nthe U.S. left-right political spectrum. However, though content analysis of the websites showed some convergence in themes among Alt-Left a nd Alt-Right domains, the Alt-\nLeft domains in our graph were not heavily cited in the production of alternative narratives on Twitter, nor were they highly connected (via us er tweets) to Alt-Right do-\nmains. They are hardly visible in the graph and are posi-tioned almost entirely around the periphery. \nDiscussion/Conclusion \nIn this work, we take a syst ematic approach to the exami-\nnation of the \u201cfake news\u201d phenomenon, using interpretive analysis of a network graph to guide qualitative analysis of tweet and web content. Our method of generating the graph provides insight into the structure of alternative narrative production (and confrontation)\u2014exposing how active us-ers in the alternative narrative discourse cite different web domains as they assemble and discuss these narratives. The work reveals a subsection of the alternative media ecosys-tem\u2014one focused on conspiracy theories\u2014but also looks beyond a single story to try to understand some of the dy-namics within this emerging alternative information space. \nConspiracy Theories & Political Agendas \n Not surprisingly, we found the conversation around al-ternative narratives of mass shooting events to be largely fueled by content on alternative (as opposed to main-stream) media.  Twitter users who engaged in conspiracy \ntheorizing cited articles from dozens of alternative media domains to support their theories.  Occasionally, they cited \nmainstream media as well, either to use details from arti-cles about the event as evidence for their theories or to directly challenge the mainstream narrative.  Many of the domains we analyzed were broadly con-spiratorial in nature, hosting not one, but many different conspiracy theories. We also detected strong political agendas underlying many of these stories and the domains that hosted them, coding more than half of the alternative media sites as primarily motivated by a political agenda\u2014with the conspiracy theories serving a secondary purpose of attracting an audience and reflecting or forwarding that \nagenda. Important for our understanding of the intersection between alternative media and global political dynamics, though much of the content on the domains we analyzed was focused around U.S. politics and designed for a U.S. audience, the agendas did not align to commonly-held no-\ntions of left (liberal) vs. ri ght (conservative) in U.S. poli-tics. Instead, almost all focu sed on anti-globalist themes, \nhighly critical of the U.S. and other Western governments and their role around the world. Additionally, content sup-porting Russian government interests was present across a majority of these domains. We hope to provide a more detailed analysis of the role of Russian government propa-ganda within this network in future work.  Another theme we noted across the majority of domains was the appropriation of the \u201cfake news\u201d argument to at-tack mainstream media. Th e websites hosted on many of \nthese domains intentionally position their content as an alternative to mainstream media, which they claim is bi-ased in various ways. Combating the \u201cfake news\u201d attack on their product, many of them responded with a counter-attack, underscoring the contentious nature of information and narrative in the current information ecosystem. \nDomain Diversity, Theme Convergence & Con-\nspiratorial Thinking  \nAn important finding here is the convergence within the \nalternative media domains around a number of \u201cconspir-acy\u201d themes. In addition to anti-globalist and anti-media views, we found content that  was anti-vaccine, anti-GMO, \nand anti-climate science. Most alternative media domains contained accusations about the activities of George Soros and the Rothschilds, and almost all hosted articles refer-encing \u201cpedophile rings\u201d of high-powered people around \nthe world. We found the same stories on multiple domains, sometimes as exact copies, but al so in different forms. This \nmeans that an individual using these sites is likely seeing the same messages in different forms and in different places, which may distort their perception of this informa-tion as it gives the false appearance of source diversity.  Sunstein & Vermeule (2009) write that contrary to popu-lar framings, belief in conspi racy theories does not imply \nmental illness, but is instead indicative of a \u201ccrippled epis-\ntemology\u201d due in part to a limited number of information \nsources. Our research suggest s this crippled epistemology \nmay be exacerbated by the false perception of having a seemingly diverse information diet that is instead drawn \nfrom a limited number of sources. This understanding of the dynamics of alternative media, where the same content appears on different sites in different forms, combined with what we know about how believing in one conspiracy the-ory makes a person more likely to believe another (van Prooijen & Acker, 2015), suggests that alternative media domains may be acting as a breeding ground for the trans-mission of conspiratorial ideas. In this way, a \u201ccritically thinking\u201d citizen seeking more information to confirm their views about the danger of vaccines may find themselves exposed to and eventually infected by other conspiracy theories with geopolitical themes, with one conspiracy the-ory acting as a gateway to others. Future work, perhaps \n238\nexamining user information-sharing patterns over time, \nwill be needed to evaluate the strength of this claim.  From another perspective, these findings on the structure \nand dynamics of the alternative media ecosystem provide some evidence of intentional disinformation tactics (Pom-erantsev & Weiss, 2014) designe d not to spread a specific \nideology but to undermine trus t in information generally. \nPomerantsev and Weiss (2014) describe this type of disin-formation as an extension of Leninist information tactics, \nwhich aimed to spread confusion and \u201cmuddled think-ing\u201d\u2014like the crippled epis temologies described above\u2014\nas a way of controlling a society. Future work will be needed to determine the extent to which the properties of this ecosystem are orchestrated in this way and which are merely emergent\u2014i.e. driven by a multitude of distinct actors with different motivations and interests. \nLimitations \nIn this research, we utilized a systematic approach to map the alternative media ecosystem , deriving the network from \ntweets about alternative narratives. However, this approach has several potential limitations, as the resulting network is defined by a relatively small number of users (1372), likely shaped by the activity of automated Twitter accounts, and biased towards conspiracy theory domains due to the un-derlying theme of the tweet data (alternative narratives about mass shooting events). The network analyzed here therefore does not represent all of alternative media, but a particular subset of that ecosystem.  Though our analysis focused on the broader content of the sites (where alternative narratives of shooting events only played a small role), the underlying data likely af-fected how we saw the dominant political agendas\u2014by focusing our analysis on a particul ar subset of sites. In fu-\nture work, we plan to do similar analysis of other types of conversations to better understand the constitution and contours of the broader alternative media ecosystem. \nAcknowledgements \nThis research was supported by NSF Grant #1420255. I thank the many students at the emCOMP lab involved in various aspects of this research, including data collection and preliminary analysis. \nReferences \nBruns, A. 2003. Gatewatching, not gatekeeping: Collaborative \nonline news. Media International Australia Incorporating Cul-\nture and Policy , 107(1), 31-44. \nCaldwell, C. (2016). What the alt-right really means? New York \nTimes , (Dec 2, 2016). Available at: http://www.nytimes.com/ \n2016/12/02/opinion/sunday/what-the -alt-right-really-means.html Fuller, J. 2010. What is happening to news: The information ex-\nplosion and the crisis in journalism . University of Chicago Press. \nGillmor, D. 2004. We the media: The rise of citizen journalists. \nNational Civic Review, 93(3), 58-63. \nGriffiths, B. 2016. AP issues guidelines for using the term \u2018alt-\nright\u2019. Politico, (Nov 28, 2016). Available at: \nhttp://www.politico.com/story/2016/ 11/use-alt-right-or-white-\nnationalism-associated-press-231889 \nMaddock, J., Starbird, K., Al-Hassani, H. J., Sandoval, D. E., \nOrand, M., & Mason, R. M. 2015. Characterizing online rumor-\ning behavior using multi-dimensional signatures. In CSCW 2015  \nACM, (228-241). \nMandik, P. 2007. Shit happens. Episteme , 4(02), 205-218. \nMocanu, D., Rossi, L., Zhang, Q., Karsai, M., & Quattrociocchi, \nW. 2015. Collective attention in the age of (mis)information. Computers in Human Behavior , 51, 1198-1204. \nMuddle, C. 2004. Globalisation: The multi-faced enemy? Work-\ning paper Series 3 (Melbourne: CERC). \nNyhan, B., & Reifler, J. (2010). When corrections fail: The per-\nsistence of political misperceptions. Political Behavior , 32(2), \n303-330. \nPomerantsev, P., & Weiss, M. (2014). The menace of unreality: \nHow the Kremlin weaponizes information, culture and money. \nThe Interpreter , 22. (Nov 22, 2014). \nPopper, K. 1945. The Open Society and Its Enemies . Routledge, \nUnited Kingdom. \nRappoport, J. 2016. Mainstream Fake News: The Devious Lim-\nited Hangout. ActivistPost, (Dec 24, 2016).  Available at: \nhttp://www.activistpost.com/2016/12/mainstream-fake-news-\ndevious-limited-hangout.html \nShibutani, T. 1966. Improvised news: A sociological study of \nrumor. Ardent Media. \nSiles, I., & Boczkowski, P. J. 2012. Making sense of the newspa-\nper crisis: A critical assessment of existing research and an agenda for future work. New Media & Society . \nSilverman, C. & Alexander, L. 2016. How Teens In The Balkans Are Duping Trump Supporters With Fake News. BuzzFeedNews. \nNov 3, 2016. Available: http ://www.buzzfeed.com/craigsilver \nman/how-macedonia-became-a-global-hub-for-pro-trump-misinfo \nStarbird, K., Spiro, E., Edwards, I., Zhou, K., Maddock, J., & \nNarasimhan, S. 2016. Could This Be True? I Think So! Ex-\npressed Uncertainty in Online Rumoring. In CHI 2016 (360-371).  \nSunstein, C. R., & Vermeule, A. (2009). Conspiracy theories: Causes and cures. J. of Political Philosophy , 17(2), 202-227. \nTimberg, C. 2016. Russian propaganda effort helped spread \u2018fake \nnews\u2019 during election, experts say. Washington Post  (Nov 24, \n2016). Available at: http://www.washingtonpost.com/business/\n \neconomy/russian- propaganda-effort-helped-spread-fake-news-during-election-experts-say/2016/11/24/793903b6-8a40-4ca9-b712-716af66098fe_story.html \nTsfati, Y. 2010. Online news exposure and trust in the main-\nstream media: Exploring possible associations. American Behav-\nioral Scientist , 54(1), 22-42. \nvan Prooijen, J. W., & Acker, M. 2015. The influence of control on belief in conspiracy theories: Conceptual and applied exten-sions. Applied Cognitive Psychology , 29(5), 753-761. \n239", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Examining the alternative media ecosystem through the production of alternative narratives of mass shooting events on Twitter", "author": ["K Starbird"], "pub_year": "2017", "venue": "Proceedings of the International AAAI Conference on \u2026", "abstract": "This research explores the alternative media ecosystem through a Twitter lens. Over a ten-month  period, we collected tweets related to alternative narratives\u2014for example, conspiracy"}, "filled": false, "gsrank": 164, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/14878", "author_id": ["C6KSF5gAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:kpgUTtTMIhEJ:scholar.google.com/&output=cite&scirp=163&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D160%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=kpgUTtTMIhEJ&ei=IbWsaPvmC7_SieoPzJnloAQ&json=", "num_citations": 386, "citedby_url": "/scholar?cites=1234774460068042898&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:kpgUTtTMIhEJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/14878/14728"}}, {"title": "Differences in misinformation sharing can lead to politically asymmetric sanctions", "year": "2024", "pdf_data": "Nature | Vol 634 | 17 October 2024 | 609\nAnalysisDifferences in misinformation sharing can \nlead to politically asymmetric sanctions\nMohsen Mosleh1,2,3, Qi Yang4, Tauhid Zaman5, Gordon Pennycook6 & David G. Rand3,4,7\u2009\u2709\nIn response to intense pressure, technology companies have enacted policies to \ncombat misinformation1\u20134. The enforcement of these policies has, however, led to \ntechnology companies being regularly accused of political bias5\u20137. We argue that \ndifferential sharing of misinformation by people identifying with different political \ngroups8\u201315 could lead to political asymmetries in enforcement, even by unbiased \npolicies. We first analysed 9,000 politically active Twitter users during the US 2020 \npresidential election. Although users estimated to be pro-Trump/conservative  \nwere indeed substantially more likely to be suspended than those estimated to be \npro-Biden/liberal, users who were pro-Trump/conservative also shared far more links \nto various sets of low-quality news sites\u2014even when news quality was determined by \npolitically balanced groups of laypeople, or groups of only Republican laypeople\u2014\nand had higher estimated likelihoods of being bots. We find similar associations \nbetween stated or inferred conservatism and low-quality news sharing (on the basis  \nof both expert and politically balanced layperson ratings) in 7 other datasets of \nsharing from Twitter, Facebook and survey experiments, spanning 2016 to 2023  \nand including data from 16 different countries. Thus, even under politically neutral \nanti-misinformation policies, political asymmetries in enforcement should be \nexpected. Political imbalance in enforcement need not imply bias on the part of  \nsocial media companies implementing anti-misinformation policies.\nMass communication is a central feature of modern life, with social \nmedia having an increasingly important role in the global distribu -\ntion and consumption of information16. This increase in importance \nhas been accompanied by increased concern about the part played \nby social media in the spread of misinformation. For example, both \nliberals and conservatives in the USA believe technology companies \nshould take action against misinformation17,18, as do many people \nacross European Union member countries19. In response, social media \ncompanies have implemented a wide range of anti-misinformation \npolicies in recent years, such as removing or flagging posts deemed \nto be false by professional fact-checkers20\u201322 or platform users23,24, \nusing ranking algorithms to reduce the likelihood that users see \npotentially inaccurate posts22, 25 and suspending users who spread \nmisinformation3,26,27.\nThese policies, however, have often led to social media companies \nbeing accused of political bias in their choices about who and what to \ntake action against. In the USA, for example, it has been claimed that \nconservatives and Republicans are purposefully targeted for enforce -\nment because of their political orientation7 (for example, when Donald \nTrump said that Twitter \u201ctotally silences conservatives\u2019 voices\u201d5, or \nwhen Representative Jim Jordan charged that academics, social media \nplatforms and the government colluded to censor conservatives28). \nAccordingly, many social media companies are also very concerned \nabout being perceived as having an anti-conservative bias29, and such concerns may sway the decision-making of such companies. Con -\ncerns of bias against conservatives also contributed to Elon Musk\u2019s \ndecision to purchase Twitter in 2022 (ref.\u200930 ) and to roll back various \nanti-misinformation policies (such as reinstating many suspended \nusers)31,32.\nHere, we critically examine these allegations of biased treatment. Our \nargument rests on the following logic: partisan asymmetries in behav -\niour can lead to partisan asymmetries in treatment, even when the pol -\nicy is politically neutral and unbiased. For example, if dog-lovers share \nmore misinformation than cat-lovers, we would expect more dog-lovers \nthan cat-lovers to get suspended by social media companies\u2014  \nand would not interpret such a pattern as reflecting bias against \ndog-lovers.\nThe same is true when it comes to politics. As we will show here, there \nis clear evidence of a political asymmetry in misinformation sharing \namong social media users\u00a0in the USA\u2014and, crucially, we will use evalu -\nations by politically balanced groups of laypeople to show that this \nasymmetry cannot be easily attributed to partisan bias on the part of \nthose determining what counts as misinformation. Such asymmetries \nin sharing do not necessarily imply psychological asymmetries in sus -\nceptibility to misinformation, but could instead arise from factors \nsuch as asymmetries in exposure to misinformation (for example, \nfrom political elites13,33). Whatever their source, these asymmetries \nin behaviour mean that differential treatment of those on one versus https://doi.org/10.1038/s41586-024-07942-8\nReceived: 6 May 2023\nAccepted: 13 August 2024\nPublished online: 2 October 2024\nOpen access\n Check for updates\n1Oxford Internet Institute, University of Oxford, Oxford, UK. 2Management Department, University of Exeter Business School, Exeter, UK. 3Sloan School of Management, Massachusetts \nInstitute of Technology, Cambridge, MA, USA. 4Initiative on the Digital Economy, Massachusetts Institute of Technology, Cambridge, MA, USA. 5Yale School of Management, Yale University, \nNew Haven, CT, USA. 6Department of Psychology, Cornell University, Ithaca, NY, USA. 7Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA, USA. \n\u2709e-mail: drand@mit.edu\n610 | Nature | Vol 634 | 17 October 2024\nAnalysisthe other side of the aisle does not on its own constitute evidence of \npolitical bias on the part of social media companies.\nTwitter suspensions after the 2020 election\nWe begin to shed new empirical light on this issue by taking a specific \nsocial media policy choice that has drawn intense criticism as a case \nstudy: Twitter\u2019s suspension of users following the 2020 US presidential \nelection. Specifically, in October 2020 we identified 100,000 Twitter \nusers who shared hashtags related to the US presidential election, \nand randomly sampled 4,500 of those users who shared at least one \n#VoteBidenHarris2020 hashtag and 4,500 who shared at least one \n#Trump2020 hashtag. We used each user\u2019s data from that pre-election \ntime period to quantify their tendency to share low-quality news (as \nwell as numerous other potentially relevant characteristics), and then \nchecked 9\u2009months later (after the election season) to determine which \nusers had been suspended by Twitter (for details, see the\u00a0Methods; data \nand code are available at https://osf.io/a2t7d/). These data allow us to \nmake several contributions to policy discussions around political bias \nand anti-misinformation efforts.\nFirst, accusations of political bias are based largely on anecdotes or \nsalient unique cases, such as the suspension of former President Donald \nTrump. Our data allow us to evaluate these claims more systematically. \nIndeed, we find that accounts that had shared #Trump2020 during \nthe election were 4.4 times more likely to have been subsequently sus -\npended than those that shared #VoteBidenHarris2020 (\u03c72(1)\u2009=\u2009486.9, \nP\u2009<\u20090.0001). Specifically, whereas only 4.5% of the users who shared \nBiden hashtags had been suspended as of July 2021, 19.6% of the users \nwho shared Trump hashtags had been suspended.\nCritically, however, this association does not necessarily indicate a \ncausal effect of a user\u2019s politics on suspension\u2014because of the poten-\ntial for political orientation to be confounded with the tendency to \nshare misinformation (or to engage in other sanctioned behaviours). \nIndeed, previous work has found consistent evidence of a partisan \nasymmetry in misinformation sharing: links to websites that journal-\nists and fact-checkers deemed to be low-quality \u2018fake news\u2019 sites were \nshared much more by conservatives than liberals on Facebook dur -\ning the 2016 election11 and the 2020 election34, and on Twitter during \nthe 2016 election12 and during Donald Trump\u2019s first impeachment35; \nconservatives on Twitter were much more likely to follow elites that \nmade claims fact-checkers rated as false compared with Democrats13; \nRepublican-oriented images on Facebook were much more likely to be \nrated as misleading than Democratic-oriented images36; and survey \nexperiments that present participants with politically balanced sets \nof headlines (removing the supply-side confound present in many \nobservational studies) typically find that conservatives indicate \nhigher sharing intentions for articles deemed to be false by profes -\nsional fact-checkers than liberals8,9. Furthermore, this association is \nnot limited to the USA. For example, a survey experiment conducted \nin 16 countries found widespread cross-cultural evidence of conserva -\ntives sharing more unambiguously false claims about COVID-19 than \nliberals37; and an examination of Twitter data found that conservative \npolitical elites shared links to lower-quality news sites than liberal politi -\ncal elites in the USA, Germany and the UK33. These observed differences \nin behaviour have clear implications for differences in treatment by \nsocial media platforms.\nLow-quality news sharing on Twitter\nWe therefore also examined how the political orientation of the users \nin our study related to their sharing of links to low-quality news sites \nin October 2020. We find a similar pattern to past work in our dataset: \npeople who used Trump hashtags shared news from domains that \nwere on average rated as significantly less trustworthy than people \nwho used Biden hashtags. For example, using trustworthiness ratings of 60 news domains (the 20 highest volume sites within each cate -\ngory of mainstream, hyper-partisan and fake news, as determined \nby fact-checkers and journalists; see Table\u00a01  for a list of the domains \nused\u00a0and ref. 38 for details) from 8 professional fact-checkers38, \nthe average quality of domains shared by people who used Trump \nhashtags was 2.52\u2009s.d. lower than people who used Biden hashtags \n(t-test, t(8,943)\u2009=\u20091.2\u2009\u00d7\u2009102, P\u2009<\u20090.0001; Fig.\u00a0 1a). We find equivalent results \nwhen using a set of 283 domains rated by Ad Fontes Media, Inc. (http://\nadfontesmedia.com; d\u2009=\u20092.16, t(8,996)\u2009=\u20091.0\u2009\u00d7\u2009102, P\u2009<\u20090.0001), 3,216 \ndomains rated by Media Bias/Fact Check (http://mediabiasfactcheck.\ncom ; d\u2009=\u20092.06, t(8,997)\u2009=\u200997.6, P\u2009<\u20090.0001) and 4,767 domains rated \nby aggregating ratings from various fact-checkers and academics33 \n(d\u2009=\u20092.16, t(8,997)\u2009=\u20091.0\u2009\u00d7\u2009102, P\u2009<\u20090.0001) (Extended Data Fig.\u00a01). For \nfurther details, see the Methods.\nThese results, however, rely on journalists and professional fact-  \ncheckers to determine what counts as misinformation. Thus, it is pos -\nsible that conservatives are found to share more misinformation not \nbecause of a true underlying difference in misinformation sharing, but \nsimply because the misinformation evaluators have a liberal bias. To \nevaluate this possibility, we ask whether a similar pattern of results is \nobserved when using evaluations that are designed to minimize the \nchance of political bias: trustworthiness ratings generated by politi -\ncally balanced groups of laypeople. Specifically, we use ratings from \na pre-registered study38 in which n \u2009=\u2009970 demographically representa -\ntive (quota-sampled) laypeople from the USA\u00a0indicated how much \nthey trusted each of the 60 news outlets in Table\u00a0 1 using a 5-point \nLikert scale, as well as indicating their preference for the Democratic \nversus Republican party (6-point Likert scale, with no independent/\nneutral midpoint; ratings are similar when using party identification \nand excluding independents38). For each outlet, we then calculated \npolitically balanced layperson ratings by calculating the average trust \namong people who indicated they preferred the Democrats, and the \naverage trust among people who indicated they preferred the Repub -\nlicans, and then averaging those two average ratings. Thus, we gave \nthe ratings of people who preferred the Democrats versus Republicans \nequal weight when constructing our laypeople ratings, and as a result \nthese laypeople ratings cannot reasonably be accused of having liberal \nbias. See the\u00a0Methods for further details, and Table\u00a0 1 for the politically \nbalanced crowd ratings for each domain.\nCritically, this unbiased measure produces similar results\u00a0to those \ndescribed above using professional fact-checker ratings: when using \nthe ratings of a politically balanced group of laypeople, the aver -\nage quality of domains shared by people who used Trump hashtags \nwas 2.17\u2009s.d. lower than people who used Biden hashtags (t -test, \nt(8,943)\u2009=\u20091.0\u2009\u00d7\u2009102, P\u2009<\u20090.0001; Fig.\u00a0 1b). Even when creating a purpose -\nfully right-biased quality measure by only using the trustworthiness \nratings of Republican laypeople, we still find a qualitatively similar pat -\ntern: the average quality of domains shared by people who used Trump \nhashtags was 1.29\u2009s.d. lower than people who used Biden hashtags \n(t(8,943)\u2009=\u200961.1, P\u2009<\u20090.0001; Extended Data Fig.\u00a01). To further contex -\ntualize the magnitude of this difference, we discretize our politically \nbalanced layperson quality ratings to classify each of the 60 rated \ndomains as low versus high quality (Extended Data Fig.\u00a02), and find \nthat the median Trump hashtag poster shared four times more links \nto low-quality websites compared with the median Biden hashtag  \nposter.\nOur findings are not unique to the use of Biden versus Trump hashtags \nto classify users\u2019 political orientation: we find high correlations between \nsharing lower-quality news sources and conservative ideology as esti -\nmated on the basis of the Twitter accounts the users follow39 or the news \nsites that the users share12,40 (expert ratings, 0.73\u2009<\u2009r \u2009<\u20090.88 depending \non partisanship/ideology measure, P \u2009<\u20090.001 for all; politically balanced \nlayperson ratings, 0.73\u2009<\u2009 r\u2009<\u20090.82 depending on partisanship/ideol -\nogy measure, P \u2009<\u20090.001 for all; see\u00a0the Methods for methodological  \ndetails).\nNature | Vol 634 | 17 October 2024 | 611Broad asymmetries in news sharing quality\nThis pattern also extends beyond the particular setting of Twitter \nusers during the 2020 election. Across 7 extra datasets, we evaluate \nthe correlation between the average quality of news sources shared \n(using the set of 60 news sites in Table\u00a0 1) and political orientation. We \nfind a significant negative relationship of shared news quality and \nconservatism when examining YouGov respondents\u2019 on-platform \nFacebook sharing in 2016 (ref.\u200911 ) (fact-checker ratings, r(757)\u2009=\u20090.33, \nP\u2009<\u20090.0001; politically balanced layperson ratings, r (757)\u2009=\u20090.21, \nP\u2009<\u20090.0001), Prolific respondents\u2019 on-platform Twitter sharing in 2018 \n(ref.\u200941) (fact-checker ratings, r(592)\u2009=\u20090.17, P\u2009<\u20090.0001; politically bal -\nanced layperson ratings, r (592)\u2009=\u20090.16, P\u2009<\u20090.0001) and 2020 (ref.\u200941) \n(fact-checker ratings, r (377)\u2009=\u20090.27, P\u2009<\u20090.0001; politically balanced \nlayperson ratings, r(377)\u2009=\u20090.20, P\u2009<\u20090.0001), and the on-platform \nsharing of Twitter users sampled in various ways in 2021 (ref.\u200913) \n(fact-checker ratings, r(3,068)\u2009=\u20090.57, P\u2009<\u20090.0001; politically balanced \nlayperson ratings (r (3,068)\u2009=\u20090.40, P\u2009<\u20090.0001), 2022 (fact-checker \nratings, r(4,038)\u2009=\u20090.40, P\u2009<\u20090.0001; politically balanced layperson \nratings, r(4,038)\u2009=\u20090.20, P\u2009<\u20090.0001) and 2023 (fact-checker ratings, \nr(4,404)\u2009=\u20090.28, P\u2009<\u20090.0001; politically balanced layperson ratings, \nr(4,404)\u2009=\u20090.14, P \u2009<\u20090.0001) (Fig.\u00a01d). For methodological details, see \nthe\u00a0Methods; for further analyses, see Supplementary Information \nsection 3.\nFor reasons of tractability, these analyses of posts shared on social \nmedia follow the common practice of using domain-level quality \nratings as a proxy for information quality rather than examining the \nactual information contained in\u00a0each\u00a0individual post (on the basis of the \npremise that low-quality news outlets are more likely to publish claims \nthat are false or misleading; see Box\u00a01  for a more detailed discussion regarding different ways of measuring misinformation sharing). Impor -\ntantly, however, two further analyses of the sharing of posts\u00a0that spe-\ncifically contain inaccurate information also find a similar pattern. The \nfirst analysis examines the sharing on Twitter of URLs deemed to be \ninaccurate by either professional fact-checkers or politically balanced \nlayperson ratings, and estimates users\u2019 ideology on the basis of the \nideological leaning of the accounts they followed39. Users estimated to \nbe conservative shared significantly more inaccurate URLs than users \nestimated to be liberal (Wilcoxon signed-rank test, z \u2009=\u20096.641, P\u2009<\u20090.0001 \nfor fact-checker ratings, b \u2009=\u20090.654, P\u2009<\u20090.0001 for layperson ratings; \nFig.\u00a02a,b ). For methodological details, see the\u00a0Methods; for further \nanalyses, see Supplementary Information section 3.6 and Extended \nData Fig.\u00a03. The second analysis examines a large-scale survey experi-\nment in which participants from 16 countries reported their sharing \nintentions for a series of COVID-19-related claims (eliminating poten-\ntial exposure confounds, and presented without source attribution), \nand conservatism was determined on the basis of responses to two \nquestions about economic conservatism versus liberalism10. Once \nagain, there was a significant correlation between conservatism and \naverage sharing intentions for inaccurate claims (USA: fact-checker \nratings, r(439)\u2009=\u20090.11, P\u2009=\u20090.027; politically balanced layperson ratings, \nr(439)\u2009=\u20090.10, P\u2009=\u20090.029; across all 16 countries: fact-checker ratings, \nr(7,577)\u2009=\u20090.06, P\u2009<\u20090.0001; politically balanced layperson ratings, \nr(7,577)\u2009=\u20090.05, P\u2009=\u20090.0001; Fig.\u00a0 2c,d). For methodological details, see \nthe\u00a0Methods; for further analyses, see Supplementary Information \nsection 3.7.\nTogether, these data indicate a consistent pattern whereby conserva-\ntive or Republican-leaning social media users share more low-quality \ninformation\u2014as evaluated by fact-checkers or politically balanced \ngroups of laypeople, and be it judged on the basis of domain-level Table 1 | Set of 60 news site quality scores generated by trustworthiness ratings from\u00a08 professional fact-checkers and from \naveraging the trustworthiness\u00a0ratings of Democrats and Republicans to create\u00a0politically balanced trustworthiness ratings \nfrom 970 laypeople\nMainstream Hyper-partisan Fake news\nDomain Fact-checker \nratingPolitically \nbalanced \nlayperson \nratingDomain Fact-checker \nratingPolitically \nbalanced \nlayperson \nratingDomain Fact-checker \nratingPolitically \nbalanced \nlayperson \nrating\nabcnews.go.com 0.56 0.45 activepost.com 0 0.2 americannews.com 0 0.22\naol.com/news 0.41 0.35 antiwar.com 0 0.18 angrypatriotmovement.com 0 0.18\nbbc.co.uk 0.81 0.38 blacklistednews.com 0 0.18 bb4sp.com 0 0.18\nbostonglobe.com 0.75 0.33 breitbart.com 0.16 0.22 beforeitsnews.com 0 0.19\ncbsnews.com 0.66 0.48 commondreams.org 0.03 0.18 channel24news.com 0.06 0.25\nchicagotribune.com 0.53 0.38 conservativetribune.com 0.03 0.24 clashdaily.com 0 0.18\ncnn.com 0.84 0.47 crooksandliars.com 0.13 0.18 conservativedailypost.com 0 0.23\ndailymail.co.uk 0.44 0.3 dailycaller.com 0.13 0.21 dailybuzzlive.com 0 0.24\nfoxnews.com 0.44 0.45 dailykos.com 0.16 0.2 downtrend.com 0 0.19\nhuffingtonpost.com 0.47 0.41 dailysignal.com 0 0.2 freedomdaily.com 0.03 0.2\nlatimes.com 0.75 0.33 dailywire.com 0.16 0.25 newsbreakshere.com 0 0.19\nmsnbc.com 0.66 0.44 ijr.com 0.09 0.19 notallowedto.com 0 0.17\nnews.yahoo.com 0.59 0.4 infowars.com 0.03 0.21 now8news.com 0 0.2\nnydailynews.com 0.34 0.33 newsmax.com 0.13 0.23 onepoliticalplaza.com 0 0.19\nnypost.com 0.38 0.38 patriotpost.us 0 0.21 react365.com 0 0.17\nnytimes.com 0.91 0.45 rawstory.com 0.09 0.19 realnewsrightnow.com 0 0.21\nsfchronicle.com 0.59 0.26 redstate.com 0.06 0.2 socialeverythings.com 0 0.18\nusatoday.com 0.66 0.45 thedailysheeple.com 0.09 0.18 thenewyorkevening.com 0 0.24\nwashingtonpost.com 0.91 0.45 thepoliticalinsider.com 0.03 0.22 whatdoesitmean.com 0 0.19\nwsj.com 0.72 0.34 westernjournal.com 0.06 0.22 yournewswire.com 0.06 0.19\nSee ref. 38 for details. These scores indicate quality, such that higher values indicate higher quality. To generate the low quality news site sharing scores used in our analyses, these quality \nscores are subtracted from 1 (to transform quality scores into low quality scores).\n612 | Nature | Vol 634 | 17 October 2024\nAnalysisor post-level ratings\u2014than liberal or Democratic-leaning users. That \nbeing said, of course, it is important to keep in mind that people who \nshare content on social media are not representative of the general \npublic, and therefore the consistent pattern we observe here does not \nnecessarily generalize to comparisons of the average liberal versus \nconservative or Democrat versus Republican; and that the pattern \nthat we observe in these data may be different at other points in time. \nOur data are also agnostic regarding the extent to which conservatives \nshare more misinformation because they are more psychologically \ninclined to do so, versus simply being exposed to more misinformation \n(for example, because, at least during the study period, conservative \nelites share more misinformation than liberal elites13,33).\nNews sharing can help explain suspension\nReturning to our dataset of Twitter suspensions during the 2020 elec -\ntion, the political asymmetry in sharing low-quality information that \nwe observe may therefore help to explain the apparent preferential \nsuspension of right-leaning users. When we calculate the area under the \ncurve (AUC, which captures accuracy while accounting for differences \nin base rates and is a standard metric of model performance in fields \nsuch as machine learning42), the various measures of sharing low-quality \nnews predict suspension (0.68\u2009<\u2009AUC\u2009<\u20090.72) to a similar degree as the \nvarious partisanship and ideology measures (0.67\u2009<\u2009AUC\u2009<\u20090.71) (Fig.\u00a0 3a) \n(no significant difference between average AUC for low-quality news \nsharing measures versus political orientation measures: bootstrapped 95% confidence interval, \u22120.005, 0.011). Thus, when examined indepen -\ndently, political orientation and sharing low-quality news are similarly \npredictive of suspension.\nWe also ask what happens when political orientation and sharing \nlow-quality news, along with numerous relevant control variables, \nare used simultaneously to predict which accounts were suspended \nduring the 6\u2009months after the 2020 US presidential election. To do \nso, we construct an aggregate measure of the political orientation of \nthe 9,000 politically active Twitter users in our sample by taking the \nfirst component of a principal component analysis (PCA) of our four \nideology/partisanship measures (on the basis of sharing Trump versus \nBiden hashtags, the Twitter accounts the users follow39 and the news \nsites that the users share12,40), and an aggregate measure of sharing \nlow-quality news. We created the latter by taking the first component \nof a PCA of our 4 expert news site quality measures (60 sites rated by \nprofessional fact-checkers38, 283 domains rated by Ad Fontes Media, \nInc., 3,216 domains rated by Media Bias/Fact Check and 4,767 domains \nrated by aggregating ratings from various fact-checkers and academ-\nics33). We then use probit regression to predict whether the user was \nsuspended as of the end of July 2021, with P values Holm\u2013Bonferroni \ncorrected to adjust for multiple comparisons (see Supplementary \nInformation section 1 for a full list of control variables and Extended \nData Table\u00a01 for regression models). When doing so, the association \nbetween political orientation and suspension is not statistically signifi -\ncant (b\u2009=\u20090.12, z\u2009=\u20092.33, PHB\u2009=\u20090.14), whereas sharing low-quality news is \npositively associated with suspension (b\u2009=\u20090.24, z\u2009=\u20095.18, PHB\u2009<\u20090.001), 00.10.20.3\n\u20132 \u20131 012Frequency\nLow-quality news sharing\nvia fact-checker rating Biden hashtags\nTrump hashtagsPeople who shared\n00.10.20.3\n\u20132 \u20131 012Frequency\nShared by people who\nused Trump hashtag Shared by people who\nused Biden hashtag \nNews siteS haresN ews siteS hares\nfoxnews.com 50,973 nytimes.com 33,604\nbreitbart.com 47,841 cnn.com 28,488\nnypost.com 38,692 rawstory.com 20,759\ndailymail.co.uk 10,719 cbsnews.com 6,639\ndailycaller.com 9,968 usatoday.com 5,71500.10.20.30.40.5\nFacebook\nYouGov\n2016\nn = 759Twitter\nProli/f_ic\n2020\nn = 379Twitter\nFollow 3+\nelites 2021\nn = 3,070Survey\nLucid\n2022\nn = 441Twitter\nFollow 3+\nelites 2022\nn = 4,040Twitter\nFollow 1+\nelites 2023\nn = 4,406Correlation between conservatism\nand low-quality news sharing a b\ncdBiden hashtags\nTrump hashtagsPeople who shared\nLow-quality news sharing\nvia politically balanced layperson rating\nTwitter\nProli/f_ic\n2018\nn = 594\nFig. 1 | Social media users who supported Trump and/or were conservative \nshared links to lower-quality news sites than users who supported Biden \nand/or were liberal.  a,b, Distribution of relative frequency of low-quality news \nsharing scores across people who used the #VoteBidenHarris2020 hashtag \nversus the #Trump2020 hashtag in our sample of 9,000 Twitter users, on the \nbasis of links shared as of October 2020. The x -axis scores are standardized \n(z-scored) for comparability; higher values indicate lower-quality news sharing. \nThe y axis indicates relative frequency, such that the area under each curve \nsums to 1. News site quality ratings as given by 8 professional fact-checkers ( a) \nand news site quality ratings as given by n\u2009=\u2009970 laypeople\u00a0from the USA \nrecruited using Lucid ( b), quota-matched to the national distribution on age, \ngender, education and geographic region; ratings of Democratic respondents \nand Republican respondents were averaged to create politically balanced layperson ratings. For details of the ratings, see the Methods and ref.\u200938 .  \nc, Top five most-shared news sites among the people in our sample who used \nTrump versus Biden hashtags, using the list of news sites from ref.\u200938 . d, The \ncorrelation between conservatism and low-quality information sharing across \nseven extra datasets. For x -axis labels, the first row indicates the data source  \nfor low-quality news sharing, the second row indicates the source from which \nthe users were sampled, the third row indicates the year in which the data were \ncollected and the fourth row indicates the sample size. For details of each \ndataset, see Supplementary Information section 3. Error bars indicate 95% \nconfidence intervals. Thus, the pattern observed in panels a and b generalizes \nbeyond Twitter users who shared political hashtags during the 2020 US \npresidential election to a variety of other contexts.\nNature | Vol 634 | 17 October 2024 | 613as are other problematic behaviours such as likelihood of being a bot \n(estimated using the model from ref.\u200943 ; b\u2009=\u20090.20, z\u2009=\u20095.09, PHB\u2009<\u20090.001) \nand use of toxic language (b \u2009=\u20090.17, z\u2009=\u20096.58, PHB\u2009<\u20090.001); results are \nsimilar when using ridge (penalized) regression or logistic regression \n(Extended Data Table\u00a01).\nOf course, because of their correlational nature, these analyses do \nnot allow us to definitively assess whether there was a causal effect \nof political orientation on Twitter suspensions during the 2020 elec -\ntion. Even if we had more precise measures of misinformation sharing \n(for example, post-level instead of source-level, or real-time ratings \ninstead of using pre-election tweets to prospectively predict as we \ndo here), or were able to include extra features (for example, harm -\nful content that was deleted before we were able to download it), our \nestimates could still be biased in either direction by further omitted  \nvariables.\nWho is sanctioned by unbiased policies\nThese data do, however, allow us to ask a more general question that \nhas implications beyond just re-litigating the 2020 election cycle in the \nUSA: what would we expect to happen if, theoretically, an entirely politi -\ncally neutral anti-misinformation policy was implemented? To answer \nthis question, we use simulations to examine which users would have \nbeen suspended if suspension had been based only on sharing links to \nlow-quality news sites (using the set of 60 domains rated by laypeople \ndescribed above38; Extended Data Fig.\u00a02) and not at all on political ori-\nentation. That is, by construction we can remove any causal effect of \npolitical orientation, and then ask how much of a political asymmetry \nwe nonetheless observe given politically neutral enforcement policies \nimplemented on these data (of course, we do not know what specific \npolicies were actually used by Twitter). To do so, we consider a range \nof suspension policies that differ in their harshness, for which a given \npolicy specifies the probability of a user getting suspended each time \nthey share a link to a low-quality news domain. For each policy, we can \nthen calculate the average suspension probability of users estimated to be Democrats versus Republicans on the basis of their use of Biden \nversus Trump\u00a0hashtags. See the\u00a0Methods for details.\nUsing this approach, we find that suspending users for sharing links to \nnews sites deemed to be untrustworthy by politically balanced groups \nof laypeople38 leads to higher rates of suspension for Republicans than \nDemocrats (Fig.\u00a0 3b). For example, if users have a 1% chance of getting \nsuspended each time they share a low-quality link, 2.41 times more \nusers who shared Trump hashtags would be suspended compared \nwith users who shared Biden hashtags (d \u2009=\u20090.63; t-test, t(8,998) = 30.1, \nP\u2009<\u20090.0001). Findings are equivalent when basing suspension on expert \nassessments of the 60 news sites38, or when correlating predicted sus-\npension rate with ideology (0.31\u2009<\u2009r \u2009<\u20090.39, depending on ideology meas -\nure; P\u2009<\u20090.0001 for all); see Supplementary Information section 2 for  \ndetails.\nBeyond the sharing of misinformation or conspiracy theories often \npromoted by low-quality news sites, conservatives in our dataset may \nalso have been preferentially suspended for engaging in other sanc -\ntioned behaviours, such as engaging in calls for violence (for exam -\nple, in connection with the events at the US capital on 6 January 2021, \nwhich occurred during our study period), or for using bots. Indeed, \nas with sharing links to low-quality news sites, users on the political \nright had significantly higher estimated likelihoods of being a bot \n(0.70\u2009<\u2009r \u2009<\u20090.76 depending on political orientation measure, P \u2009<\u20090.0001 \nfor all; Extended Data Fig.\u00a04), and simulating suspension on the basis \nof likelihood of being a bot leads to much higher suspension rates for \nRepublican accounts than Democrat accounts (Fig.\u00a0 3c; see the\u00a0Methods \nand Supplementary Information section 2 for details). For example, \nsuspending users with a bot score above 0.5 would lead to 14.2 times \nmore users who shared Trump hashtags getting suspended compared \nwith users who shared Biden hashtags (d\u2009=\u20091.26; t-test, t(8,976)\u2009=\u200959.9, \nP\u2009<\u20090.0001). Importantly, the associations between political orienta -\ntion and low-quality news sharing are robust to controlling for bot \nscores, and to only examining users with low likelihood of being bots \n(Supplementary Information section 2). Regardless of which pro -\nhibited behaviour(s) are in operation, the same fundamental point Box 1\nHow misinformation sharing is defined and measured has important \nimplications for the conclusions that can be drawn from any analyses\nMost research on online misinformation takes one of two \napproaches. The first approach\u2014most often used when analysing \nlarge social media datasets in which it is not feasible to evaluate \neach post individually\u2014focuses on URLs, and uses the quality \nof the publishing domain as a proxy for the veracity (or, more \nbroadly, \u2018quality\u2019) of the specific URL54. The logic behind this \napproach is that URLs from low-quality sources are more likely \nto be false or inaccurate than URLs from high-quality domains. \nAlthough this approach is scalable, it is quite coarse as some \nstories from lower-quality outlets may be accurate, and some \nstories from higher-quality outlets may be false, inaccurate or \notherwise misleading55. That being said, there is at least substantial \nconsistency in which domains experts consider to be low quality54, \nand this domain-based approach has the benefit of providing ratings \nof relative quality for a sizable fraction of people\u2019s news diet.\nThe second approach\u2014most often used when recruiting \nparticipants to complete survey experiments\u2014is to collect specific \nheadlines, posts or articles that have been debunked or are \nveridical, and ask participants to indicate how likely they would be \nto share the posts if they saw them online56. Although this approach \ndoes not suffer from the coarseness of the domain ratings and removes potential exposure confounds (where some types of users \nmay be exposed to misinformation more than others), it is difficult to \nimplement at scale and relies on self-report sharing intentions rather \nthan actual social media sharing. As each approach has limitations, \nit can be particularly compelling to observe convergent results \nwhen asking the same question using both approaches.\nAcross both approaches, researchers typically rely on professional \nfact-checker ratings (either of the trustworthiness of publishers, or \nthe veracity of individual stories) to evaluate content. In this article, \nwe demonstrate how ratings from politically balanced groups \nof laypeople can also be used to evaluate domains and stories \nin a fashion that minimizes the potential for political bias in the \nevaluations.\nFinally, we note that although most misinformation research to \ndate has largely focused on news stories, other forms of content \nsuch as images, videos and posts without URLs are also very \nimportant vectors of information online. It is vital for future research \nto look at content beyond just URLs (that is,\u00a0to examine post text, \nimages\u00a0and videos), as well as to develop ways of measuring the \nactual impact of exposure to content at scale rather than simply \nusing inaccuracy as the metric of harm55.\n614 | Nature | Vol 634 | 17 October 2024\nAnalysisapplies\u2014partisan asymmetries in behaviour can lead to partisan asym -\nmetries in suspension, even when suspension policies are politically \nneutral.\nThese analyses show that even in the absence of any (intentional) \ndisparate treatment on the part of technology companies, partisan \nasymmetries in sanctioned behaviours will lead to (unintentional) \ndisparate impact whereby conservatives are suspended at greater \nrates. From a legal perspective, political orientation is not a protected \nclass in the USA44 and thus neither form of disparate treatment is illegal \n(although potentially still normatively undesirable). Although disparate \nimpact may reasonably be considered to constitute discrimination in \nsome cases (for example, employment discrimination on the basis of \njob-irrelevant factors that correlate with race)45, in the present context \nreducing the spread of misinformation and the prevalence of bots are \nlegitimate and necessary goals for social media platforms. This makes a \nnormative case for disparate impact on the basis of political orientation.\nSocial media companies typically do enforcement on the basis of the \ncontents of specific posts, rather than sanctioning users on the basis of \nthe domains they share46\u201348. This post-level approach has the advantage \nof being much finer grained, as much of the content from low-quality domains may be accurate. Thus, enforcement using domain-level qual -\nity will lead to many false positives, in which users are sanctioned for \nsharing content that is not misinformation. Conversely, post-level sanc -\ntions create an incentive for users to not post inaccurate or misleading \ncontent in particular. Relatedly, platform sanctions can be deployed at \nthe level of the user (for example, suspending users who post inaccurate \ncontent) or the post (for example, attaching warning labels to posts \nflagged by fact-checkers49), and ranking algorithms\u2014which generally \nfavour content that generates engagement and thus may promote \nmisinformation\u2014can also be tools of anti-misinformation enforce -\nment by downranking content that has been identified as inaccurate \n(or comes from\u00a0users who have previously posted inaccurate content).\nAsymmetrical treatment need not imply bias\nIn the context of regulation such as the Digital Services Act recently \npassed by the European Union\u2014which requires platforms to take down \ncontent that involves misinformation\u2014our results suggest that when \nplatforms comply with such regulations, they are likely to face accusa -\ntions of partisan bias even if their policies are in fact politically neutral. 6.09\n6.07\n6.16\n6.59\n6.70\n6.37\n6.24\n6.23\n5.80\n6.54\n5.73\n6.17\n6.04\n6.49\n6.34\n6.43\n100.00\u20130.09 (\u20130.19, 0)\n\u20130.02 (\u20130.12, 0.07)\n\u20130.02 (\u20130.11, 0.07)\n\u20130.01 (\u20130.09, 0.08)\n0 (\u20130.08, 0.08) \n0 (\u20130.09, 0.09)\n0.01 (\u20130.08, 0.10)\n0.01 (\u20130.08, 0.10)\n0.04 (\u20130.06, 0.14)\n0.08 (\u20130.01, 0.16)\n0.08 (\u20130.02, 0.18)\n0.09 (\u20130.01, 0.18)\n0.10 (0.01, 0.20)\n0.13 (0.04, 0.21)\n0.14 (0.05, 0.22)\n0.20 (0.11, 0.29)\n0.05 (0.01, 0.08)\nCorrelation between conservatism and\nfraction of shared news that is false\n4 \n3 \n2\n1\n0 \nLiberals False news URLs shared on Twitter\nbased on fact-checker ratings\nFraction of shared COVID-19 claims that are false,\nbased on fact-checker ratingsFraction of shared COVID-19 claims that are false,\nbased on layperson crowd ratingsFalse news URLs shared on Twitter\nbased on politically balanced crowd ratings\n4 \n3  \n2 \n1 \n0 \nMexico \nRussia \nNigeria \nIndia \nSouth Africa \nPhilippines \nItaly \nSaudi Arabia \nChina \nSpain \nUnited Kingdom \nUSA \nAustrali a  \nArgentina \nEgypt \nBrazil \nOverallCorrelation\ncoef/f_icient (95% CI) %\nWeight \n6.06 \n6.03\n6.64 \n6.39\n6.20 \n6.74 \n6.26\n6.19 \n6.51 \n6.17 \n5.76 \n6.06 \n5.62 \n6.41 \n6.52\n6.44\u20130.07 (\u20130.17, 0.02)\n\u20130.02 (\u20130.11, 0.07)\n0 (\u20130.08, \u20130.09)\n0.01 (\u20130.08, 0.09)\n0.01 (\u20130.08, 0.10)\n0.02 (\u20130.07, 0.10)\n0.02 (\u20130.07, 0.11)\n0.04 (\u20130.05, 0.13)\n0.06 (\u20130.02, 0.15)\n0.09 (0, 0.18)\n0.09 (\u20130.01, 0.19)\n0.10 (0.01, 0.20)\n0.11 (0.01, 0.21)\n0.11 (0.03, 0.20)\n0.15 (0.07, 0.24)\n0.20 (0.11, 0.29)\n0.06 (0.02, 0.09) 100.00 \n\u20130.2 0 \nCorrelation between conservatism and\nfraction of shared news that is falsec  ab\ndlog10(no. of tweeters + 1)\nConservatives\nlog10(no. of tweeters + 1)\nLiberals Conservatives\n0.2 Mexico \nRussia \nNigeria \nIndia South Africa \nPhilippines \nItaly \nSaudi Arabia \nChina \nSpain United Kingdom \nUSA Australia\n \nArgentina Egypt \nBrazil \nOverall\n\u20130.2 0 0.2 Correlation\ncoef/f_icient (95% CI) %\nWeight \nFig. 2 | Conservatives shared more false claims than liberals.   \na,b, Distribution of the number of Twitter posts containing links to articles \nrated as false by professional fact-checkers ( a), or rated as inaccurate by \npolitically balanced groups of survey respondents ( b), made by Twitter users \nestimated to be liberal versus conservative when analysing data from ref.\u200953 . \nThe y axis shows log10(count of primary posts containing the URL\u2009+\u20091).  \nFor details, see the Methods and Supplementary Information section 3.6.  c,d, Analysis of sharing intentions for COVID-19 claims from a survey \nconducted in 16 countries, from ref.\u2009 37. Shown is the correlation between \nself-reported endorsement of conservative economic principles and fraction \nof shared content that was rated false\u00a0by fact-checkers ( c) or inaccurate \nby\u00a0layperson crowds ( d). Overall effect was calculated using random effects \nmeta-analysis. Error bars indicate 95% confidence intervals\u00a0(95% CI). For details, \nsee the\u00a0Methods and Supplementary Information section 3.7.\nNature | Vol 634 | 17 October 2024 | 615Our results also suggest that using politically balanced groups of lay -\npeople to evaluate content50\u201352 may be a way to identify misinformation \nwhile ameliorating charges of political bias. Furthermore, increased \ntransparency on the part of platforms regarding the characteristics (for \nexample, demographics) of users who are getting sanctioned, and why \nthose sanctions are occurring, may help the public better understand \nhow neutral policies can lead to the appearance of bias.\nIn sum, when there are political asymmetries in misinformation \nsharing (in either direction), platforms will face a substantial trade-off \nbetween reducing the spread of misinformation and being politically \nbalanced in their enforcement. Asymmetric enforcement could also \noccur outside the context of political orientation (for example, if a \nparticular demographic group is preferentially targeted with misin -\nformation, they may share more of it and thus be sanctioned more \nfrequently). Our argument is not specific to one particular direction of \nassociation between political orientation and misinformation sharing\u2014  \nwhich could potentially be different at different points in time, or for particular topics. If one political, social or demographic group \nshares more misinformation\u2014be it liberals, conservatives or some \nother group\u2014it is not possible to be maximally effective in combat-\nting misinformation without preferentially taking action against \nmembers of that group. That being said, of course our analyses also \ndo not rule out the possibility of bias on the part of platforms. Instead, \nwe show that asymmetries in treatment are, on their own, not diag -\nnostic of purposeful targeting one way or the other. Be that as it may, \ngiven the widespread (and bi-partisan17,18) public demand for reducing \nmisinformation online, policy makers must be aware that some level \nof differential treatment across groups is likely even if technology \ncompanies are working in an unbiased way to keep misinformation  \nin check.\nOnline content\nAny methods, additional references, Nature Portfolio reporting summa -\nries, source data, extended data, supplementary information, acknowl -\nedgements, peer review information; details of author contributions \nand competing interests; and statements of data and code availability \nare available at https://doi.org/10.1038/s41586-024-07942-8 .\n1. Kozyreva, A., Lewandowsky, S. & Hertwig, R. Citizens versus the internet: confronting \ndigital challenges with cognitive tools. Psychol. Sci. Public Interest 21, 103\u2013156 (2020).\n2. Pennycook, G. & Rand, D. G. The psychology of fake news. Trends Cogn. Sci. 25, 388\u2013402 \n(2021).\n3. Persily, N. & Tucker, J. A. Social Media and Democracy: The State of the Field, Prospects for \nReform (Cambridge Univ. Press, 2020).\n4. Kozyreva, A. et\u00a0al. Toolbox of individual-level interventions against online misinformation. \nNat. Hum. Behav. 8, 1044\u20131052 (2024).\n5. Bond, S. Trump accuses social media of anti-conservative bias after Twitter marks his \ntweets. npr www.npr.org/2020/05/27/863422722/trump-accuses-social-media-of-anti-  \nconservative-bias-after-twitter-marks-his-twe (2020).\n6. Wermund, B. Ted Cruz digs in for congressional battle over \u2018censorship\u2019 on Twitter, \nFacebook. Houston Chronicle www.houstonchronicle.com/politics/texas/article/Ted-Cruz-  \ndigs-in-for-congressional-battle-over-15740624.php (2020).\n7. Vogels, E. A., Perrin, A. & Anderson, M. Most Americans think social media sites censor \npolitical viewpoints. Pew Research Center https://www.pewresearch.org/internet/2020/  \n08/19/most-americans-think-social-media-sites-censor-political-viewpoints (2020).\n8. Guay, B., Pennycook, G. & Rand, D. Examining partisan asymmetries in fake news sharing \nand the efficacy of accuracy prompt interventions. Preprint at PsyArXiv https://doi.org/  \n10.31234/osf.io/y762k (2022).\n9. Pennycook, G. & Rand, D. G. Accuracy prompts are a replicable and generalizable \napproach for reducing the spread of misinformation. Nat. Commun. 13, 2333 (2022).\n10. Arechar, A. A. et\u00a0al. Understanding and combatting misinformation across 16 countries on \nsix continents. Nat. Hum. Behav. 7, 1502\u20131513 (2023).\n11. Guess, A., Nagler, J. & Tucker, J. Less than you think: prevalence and predictors of fake \nnews dissemination on Facebook. Sci. Adv. 5, eaau4586 (2019).\n12. Grinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B. & Lazer, D. Fake news on \nTwitter during the 2016 US presidential election. Science 363, 374\u2013378 (2019).\n13. Mosleh, M. & Rand, D. G. Measuring exposure to misinformation from political elites on \nTwitter. Nat. Commun. 13, 7144 (2022).\n14. Nikolov, D., Flammini, A. & Menczer, F. Right and left, partisanship predicts (asymmetric) \nvulnerability to misinformation. Harvard Kennedy School (HKS) Misinformation Review  \nhttps://misinforeview.hks.harvard.edu/article/right-and-left-partisanship-predicts-  \nasymmetric-vulnerability-to-misinformation (2021).\n15. DeVerna, M. R., Guess, A. M., Berinsky, A. J., Tucker, J. A. & Jost, J. T. Rumors in retweet: \nideological asymmetry in the failure to correct misinformation. Pers. Soc. Psychol. Bull.  \n50, 3\u201317 (2024).\n16. Mitchell, A., Jurkowitz, M., Oliphant, J. B. & Shearer, E. Americans who mainly get their \nnews on social media are less engaged, less knowledgeable. Pew Research Center  \nhttps://www.pewresearch.org/journalism/2020/07/30/americans-who-mainly-get-their-\nnews-on-social-media-are-less-engaged-less-knowledgeable (2020).\n17. Koopman, C. CGO Tech Poll. The Center for Growth and Opportunity at Utah State \nUniversity https://www.thecgo.org/research/tech-poll (2021).\n18. Atreja, S., Hemphill, L. & Resnick, P. Remove, reduce, inform: what actions do people want \nsocial media platforms to take on potentially misleading content? In Proc. ACM on Human-  \nComputer Interaction (ed. Nichols, J.) 1\u201333 (2023).\n19. Directorate-General for Communication. Flash Eurobarometer 464: Fake News and \nDisinformation Online. European Union http://data.europa.eu/euodp/en/data/dataset/\nS2183_464_ENG (2018).\n20. Pennycook, G., Bear, A., Collins, E. T. & Rand, D. G. The implied truth effect: attaching \nwarnings to a subset of fake news headlines increases perceived accuracy of headlines \nwithout warnings. Manage. Sci. 66, 4944\u20134957 (2020).\n21. Clayton, K. et\u00a0al. Real solutions for fake news? Measuring the effectiveness of general \nwarnings and fact-check tags in reducing belief in false stories on social media. Polit. \nBehav. 42, 1073\u20131095 (2020).\n22. Facebook Community Standards\u2014Misinformation. Meta https://transparency.meta.com/\nen-gb/policies/community-standards/misinformation/ (2024).0.500.550.600.650.700.75\nHashtag\nsharedNews\nsites\nshared 1News\nsites\nshared 2Accounts\nfollowedAFM\nratingsMBF C\nratingsLasser\nratingsFact-\nchecke r\nratingsLayperson\nratingsAUC when predicting suspensionPolitical orientatio n\nLow-quality news sharing\n00.10.20.30.40.50.6\n0 0.1 0.2 0.3 0.4 0.5Anti-bot enforcemen t\n00.10.20.30.40.50.6\n0.0001 0.0010 .01 0.1Expected fraction of\nsuspended users \nPolicy harshness\nProbability of suspension per\nlow-quality link sharedAnti-mi sinformation enforcement\nPeople who used Trump hashtag\nPeople who used Biden hashta ga\nbc\nExpected fraction of\nsuspended users People who used Trump hashtag\nPeople who used Biden hashta g\nPolicy harshness\nMinimum probability of being\nhuman to avoid suspension\nFig. 3 | Political orientation is not a unique predictor of getting suspended, \nand politically neutral enforcement policies will lead to political \nasymmetries in suspension rates.  a, When considered separately, political \norientation and sharing low-quality information are similarly predictive  \nof suspension. Shown is AUC (a standard measure of predictive accuracy)  \nwhen predicting whether a user was suspended using models that take \ndifferent features as the independent variable. Purple bars indicate AUC for \nmeasures of political orientation (partisanship on the basis of sharing of \n#VoteBidenHarris2020 versus #Trump2020 hashtags; ideology on the basis of \naccounts followed, estimated using the model of ref.\u200939 ; ideology on the basis \nof news sites shared, estimated using the model of ref.\u200940  or the model of ref.\u200912 ). \nGreen bars indicate AUC for measures of sharing links to low-quality news sites \non the basis of ratings from Ad Fontes Media (AFM; http://adfontesmedia.com ), \nMedia Bias/Fact Check (MBFC; http://mediabiasfactcheck.com ), an aggregation \nof different fact-checkers and journalists33, 8 professional fact-checkers38 and \n970 laypeople from the USA, weighting Democrats and Republicans equally38. \nError bars indicate bootstrapped 95% confidence intervals. See Extended  \nData Table\u00a01 for results of regressions jointly considering political orientation, \nsharing low-quality news links and other controls. b , Simulating a politically \nneutral anti-misinformation policy\u2014in which there is a constant probability of \ngetting suspended each time a user shares a link to a low-quality news site, as \njudged by politically balanced groups of laypeople\u2014leads to disparate impact \nagainst users on the political right. Shown is the expected probability of \nsuspension for policies that vary in their harshness (that is, in the probability of \ngetting suspended each time a user shares a low-quality link); see the\u00a0Methods \nfor details. c , Simulating a politically neutral anti-bot policy, in which accounts \nbelow a given probability of being human are suspended, shows a similar \npartisan asymmetry; see the\u00a0Methods for details.\n616 | Nature | Vol 634 | 17 October 2024\nAnalysis23. Allen, J., Martel, C. & Rand, D. G. Birds of a feather don\u2019t fact-check each other: Partisanship \nand the evaluation of news in Twitter\u2019s Birdwatch crowdsourced fact-checking program.  \nIn Proc. 2022 CHI Conference on Human Factors in Computing Systems (eds Barbosa, S. \net\u00a0al.) 1\u201319 (2022).\n24. Silverman, H. Meta Newsroom\u2014Helping fact-checkers identify false claims faster. Meta  \nhttps://about.fb.com/news/2019/12/helping-fact-checkers/ (2019).\n25. Singh, S. & Bagchi, K. How internet platforms are combating disinformation and \nmisinformation in the age of COVID-19. New America www.newamerica.org/oti/reports/\nhow-internet-platforms-are-combating-disinformation-and-misinformation-age-covid-19/\nfacebook/ (2020).\n26. Ortutay, B. & Sweet, K. APNewsBreak: Twitter suspended 58 million accounts in 4Q. \nAPNews https://apnews.com/article/1ee30b22a9ae4267a4e6e5f218bc625b (2018).\n27. Roth, Y. & Harvey, D. How Twitter is fighting spam and malicious automation. X  https://\nblog.x.com/official/en_us/topics/company/2018/how-twitter-is-fighting-spam-and-  \nmalicious-automation.html (2018).\n28. Tollefson, J. Disinformation researchers under investigation: what\u2019s happening and why. \nNature https://doi.org/10.1038/d41586-023-02195-3 (2023).\n29. Clayton, J. Social media: Is it really biased against US Republicans? BBC  www.bbc.co.uk/\nnews/technology-54698186 (2020).\n30. Elon Musk on Real Time with Bill Maher. Elon Musk Interviews https://elon-musk- \ninterviews.com/2023/05/02/real-time-bill-maher-en/ (2023).\n31. Ivanova, I. These formerly banned Twitter accounts have been reinstated since Elon Musk \ntook over. CBS News www.cbsnews.com/news/twitter-accounts-reinstated-elon-musk-\ndonald-trump-kanye-ye-jordan-peterson-kathy-griffin-andrew-tate/ (2022).\n32. The Associated Press. Musk\u2019s Twitter has dissolved its Trust and Safety Council. NPR  \nwww.npr.org/2022/12/12/1142399312/twitter-trust-and-safety-council-elon-musk (2022).\n33. Lasser, J. et\u00a0al. Social media sharing of low-quality news sources by political elites. PNAS \nNexus  1, pgac186 (2022).\n34. Gonz\u00e1lez-Bail\u00f3n, S. et\u00a0al. Asymmetric ideological segregation in exposure to political \nnews on Facebook. Science 381, 392\u2013398 (2023).\n35. Rossetti, M. & Zaman, T. Bots, disinformation, and the first impeachment of US President \nDonald Trump. PLoS ONE 18, e0283971 (2023).\n36. Yang, Y., Davis, T. & Hindman, M. Visual misinformation on Facebook. J. Commun. 73,  \n316\u2013328 (2023).\n37. Arechar, A. A. et\u00a0al. Understanding and combatting misinformation across 16 countries on \nsix continents. Nat. Hum. Behav. 7, 1502\u20131513 (2023).\n38. Pennycook, G. & Rand, D. G. Fighting misinformation on social media using crowdsourced \njudgments of news source quality. Proc. Natl Acad. Sci. USA 116, 2521\u20132526 (2019).\n39. Barber\u00e1, P., Jost, J. T., Nagler, J., Tucker, J. A. & Bonneau, R. Tweeting from left to right: is \nonline political communication more than an echo chamber? Psychol. Sci. 26, 1531\u20131542 \n(2015).\n40. Eady, G., Nagler, J., Bonneau, R. & Tucker, J. News sharing on social media: mapping the \nideology of news media, politicians, and the mass public. Preprint as OSF Preprints  \nhttps://doi.org/10.31219/osf.io/ch8gj (2020).\n41. Mosleh, M., Pennycook, G., Arechar, A. A. & Rand, D. G. Cognitive reflection correlates \nwith behavior on Twitter. Nat. Commun. 12, 921 (2021).\n42. Huang, J. & Ling, C. X. Using AUC and accuracy in evaluating learning algorithms. IEEE \nTrans. Knowl. Data Eng. 17, 299\u2013310 (2005).43. Social Media Account Classifier. Botsentinel https://botsentinel.com (2022).\n44. The Civil Rights Act of 1964 and the Equal Employment Opportunity Commission. \nNational Archive www.archives.gov/education/lessons/civil-rights-act (2018).\n45. Civil Rights Division. Title IV Legal Manual Section VII: Proving Discrimination \u2013 Disparate \nImpact (U.S. Department of Justice, 2016); www.justice.gov/crt/fcs/T6Manual7 .\n46. Facebook Community Standards. Meta https://transparency.fb.com/en-gb/policies/\ncommunity-standards/ (2024).\n47. Youtube Community Guidelines. YouTube www.youtube.com/howyoutubeworks/policies/  \ncommunity-guidelines/#detecting-violations (2024).\n48. The X Rules. X Help Center https://help.twitter.com/en/rules-and-policies/x-rules \n(2024).\n49. Martel, C. & Rand, D. G. Misinformation warning labels are widely effective: a review of \nwarning effects and their moderating features. Curr. Opin. Psychol. 54, 101710 (2023).\n50. Allen, J., Arechar, A. A., Pennycook, G. & Rand, D. G. Scaling up fact-checking using the \nwisdom of crowds. Sci. Adv. 7, eabf4393 (2021).\n51. Godel, W. et\u00a0al. Moderating with the mob: evaluating the efficacy of real-time \ncrowdsourced fact-checking. J. Online Trust Saf. https://doi.org/10.54501/jots.v1i1.15 \n(2021).\n52. Resnick, P., Alfayez, A., Im, J. & Gilbert, E. Searching for or reviewing evidence improves \ncrowdworkers\u2019 misinformation judgments and reduces partisan bias. Collect. Intell. \nhttps://doi.org/10.1177/26339137231173407 (2023).\n53. Ghezae, I. et\u00a0al. Partisans neither expect nor receive reputational rewards for sharing \nfalsehoods over truth online. PNAS Nexus 3, pgae287 (2024).\n54. Lin, H. et\u00a0al. High level of correspondence across different news domain quality rating \nsets. PNAS Nexus 2, pgad286 (2023).\n55. Allen, J., Watts, D. J. & Rand, D. G. Quantifying the impact of misinformation and \nvaccine-skeptical content on Facebook. Science 384, eadk3451 (2024).\n56. Pennycook, G., Binnendyk, J., Newton, C. & Rand, D. G. A practical guide to doing \nbehavioral research on fake news and misinformation. Collabra Psychol. https://doi.\norg/10.1525/collabra.25293 (2021).\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License, which permits any \nnon-commercial use, sharing, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. \nYou do not have permission under this licence to share adapted material derived from this \narticle or parts of it. The images or other third party material in this article are included in the \narticle\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article\u2019s Creative Commons licence and your intended use is \nnot permitted by statutory regulation or exceeds the permitted use, you will need to obtain \npermission directly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by-nc-nd/4.0/.\n\u00a9 The Author(s) 2024\nMethods\nSample and basic data collection for 2020 election study\nFirst, we collected a list of Twitter users who tweeted or retweeted either \nof the election hashtags #Trump2020 and #VoteBidenHarris2020 \non 6 October 2020. We also collected the most recent 3,200 tweets \nsent by each of those accounts. We processed tweets and extracted \ntweeted domains from 34,920 randomly selected users (15,714 shared \n#Trump2020 and 19,206 shared #VoteBidenHarris2020), and filtered  \ndown to 12,238 users who shared at least five links to domains  \nused by the ideology estimator of ref.\u2009 57. We also excluded 426 \u2018elite\u2019 \nusers with more than 15,000 followers who are probably unrepresenta -\ntive of Twitter users more generally (because of this exclusion, suspen -\nsion data were not collected for these users; however, as described in \nSupplementary Information section 2, our main results on the\u00a0associa -\ntion between political orientation and low-quality news sharing are \nalso observed\u00a0among these elite users). These data were collected as \npart of a project that was approved by the Massachusetts Institute of \nTechnology Committee on the Use of Humans as Experimental Subjects \nProtocol 91046.\nWe then constructed a politically balanced set of users\u00a0by randomly \nselecting 4,500 users each from the remaining 4,756 users who shared \n#Trump2020 and 7,056 users who shared #VoteBidenHarris2020. \nAfter 9\u2009months, on 30 July 2021, we checked the status of the 9,000 \nusers and assessed suspension. We classify an account as having been \nsuspended if the Twitter application programming interface (API) \nreturned error code 63 (\u2018User has been suspended\u2019) when querying that  \nuser.\nTo measure a user\u2019s tendency to share misinformation, we follow \nmost other researchers in this space11,12,58 ,59 and use news\u00a0source qual-\nity as a proxy for article accuracy, because it is not feasible to rate the \naccuracy of individual tweets\u00a0at scale. Specifically, to quantify the \nquality of news shared by each user, we leveraged a previously pub -\nlished set of 60 news sites (20 mainstream, 20 hyper-partisan 20 fake \nnews; Table\u00a0 1) whose trustworthiness had been rated by 8 professional \nfact-checkers as well as politically balanced crowds of laypeople. The \ncrowd ratings were determined as follows. A sample of 971 participants \nfrom the USA, quota-matched to the national distribution on age, gen -\nder, ethnicity and geographic region, were recruited through Lucid60. \nEach participant indicated how much they trusted each of the 60 news \noutlets using a 5-point Likert scale. For each outlet, we then calculated \npolitically balanced crowd ratings by calculating the average trust \namong Democrats and the average trust among Republicans, and then \naveraging those two average ratings.\nWe also examined Reliability ratings for a set of 283 sites from Ad \nFontes Media, Inc., Factual Reporting ratings for a set of 3,216 sites from \nMedia Bias/Fact Check and Accuracy ratings for a set of 4,767 sites from \na recent academic paper by Lasser et\u00a0al.33. We then used the Twitter API \nto retrieve the last 3,200 posts (as of 6 October 2020) for each user in \nour study, and collected all links to any of those sites shared (tweeted \nor retweeted) by each user. Following the approach used in previous \nwork58,59, we calculated a news quality score for each user (bounded \nbetween 0 and 1) by averaging the ratings of all sites whose links they \nshared, separately for each set of site ratings. Finally, we transform \nthese ratings into low-quality news sharing scores by subtracting the \nnews quality ratings from 1. Over 99% of users in our study had shared at \nleast one link to a rated domain. When combining the four expert-based \nmeasures into an aggregate news quality score, we replaced missing \nvalues with the sample mean; PCA indicated that only one component \nshould be retained (87% of variation explained), which had weights \nof 0.50 on Pennycook and Rand (ref.\u200938) fact-checker ratings, 0.51 on \nAd Fontes Media Reliability ratings, 0.48 on Media Bias/Fact Check \nFactual Reporting ratings and 0.51 on Lasser et\u00a0al.33 Accuracy ratings. \nIn all PCA analyses, we use parallel analysis to determine the number \nof retained components.To measure a user\u2019s political orientation, we first classify their par -\ntisanship on the basis of whether they shared more #Trump2020 \nor #VoteBidenHarris2020 hashtags. Additionally, we retrieved all \naccounts followed by users in our sample and used the statistical model \nfrom ref.\u200939  to obtain a continuous measure of users\u2019 ideology on the \nbasis of the ideological leaning of the accounts they followed. Simi -\nlarly, we used the statistical models from ref.\u200940  and ref.\u200912 to estimate \nusers\u2019 ideology using the ideological leanings of the news sites that \nthe users shared content from. We also calculated user ideology by \naveraging political leanings of domains they shared through tweets \nor retweets on the basis of the method in ref.\u200912. The intuition behind \nthese approaches is that users on social media are more likely to fol -\nlow accounts (and share news stories from sources) that are aligned \nwith their own ideology than those that are politically distant. Thus, \nthe ideology of the accounts the user follows, and the ideology of the \nnews sources the user shares, provide insight into the user\u2019s ideol -\nogy. When combining these four measures into an aggregate political \norientation score, we replaced missing values with the sample mean; \nPCA indicated that only one component should be retained (88% of \nvariation explained), which had weights of 0.49 on hashtag-based \npartisanship, 0.49 on follower-based ideology, 0.51 on sharing-based \nideology estimated through ref.\u200940  and 0.51 on sharing-based ide -\nology estimated through ref.\u200912. We also used this aggregate meas -\nure to calculate a user\u2019s extent of ideological extremity by taking the \nabsolute value of the aggregate ideology measure; and we used PCA \nto combine measures of the standard deviation across a user\u2019s tweets \nof news site ideology scores from ref.\u200912 and ref.\u200940 , and standard \ndeviation of ideology of accounts followed from ref.\u200939 , as a meas -\nure of the ideological uniformity (versus diversity) of news shared by  \nthe user.\nPolicy simulations\nIn addition to the regression analyses, we also simulate politically \nneutral suspension policies and determine each user\u2019s probability of \nsuspension; and from this, determine the level of differential impact we \nwould expect in the absence of differential treatment. The procedure \nis as follows. First, we identify a set of low-quality sources that could \npotentially lead to suspension. We do so using the politically balanced \nlayperson trustworthiness ratings from ref.\u200938 , as well as using the \nfact-checker trustworthiness ratings from that same paper. For both \nsets of ratings, there is a natural discontinuity at a value of 0.25 (on a \nnormalized trust scale from 0\u2009=\u2009Not at all to 1\u2009=\u2009Entirely) (Extended Data \nFig.\u00a02). Thus, we consider sites with average trustworthiness ratings \nbelow 0.25 to be \u2018low quality\u2019; and for each user, we count the number \nof times they tweet links to any of these low-quality sites.\nWe then define a suspension policy as the probability of a user get-\nting suspended each time they share a link to a low-quality news site. \nWe model suspension as probabilistic because many (almost certainly \nmost) of the articles from low-quality news sites are not actually false, \nand sharing such articles does not constitute an offence. Thus, we con -\nsider who would get suspended under suspension policies that differ \nin their harshness, varying from a 0.01% chance of getting suspended \nfor each shared link to a low-quality news site up to a 10% chance. \nSpecifically, for each user, we calculate their probability of getting \nsuspended as\nPk(suspended)=1\u2212 (1\u2212)L\nwhere L is the number of low-quality links shared, and k  is the probability \nof suspension for each shared link (that is, the policy harshness). The \nonly way the user would not get suspended is if on each of the L times \nthey share a low-quality link, they are not suspended. Because they do \nnot get suspended with probability (1\u2009\u2212\u2009k ), the probability that they \nwould never get suspended is (1\u2009\u2212\u2009k)L. Therefore, the probability that \nthey would get suspended at some point is 1\u2009\u2212\u2009(1\u2009\u2212\u2009k)L.\nAnalysisWe then calculate the mean (and 95% confidence interval) of that \nprobability across all Democrats versus Republicans in our sample \n(as determined by sharing Biden versus Trump election hashtags). \nThe results of these analyses are shown in Fig.\u00a03b , and Supplemen -\ntary Information section 2 presents statistical analyses of estimated \nprobability of suspension on the basis of each measure of political  \norientation.\nWe also do a similar exercise using the likelihood of being a bot, rather \nthan low-quality news sharing. The algorithm of ref.\u200943  provides an \nestimated probability of being a bot for each user, on the basis of the \ncontents of their tweets. We define a suspension policy as the minimum \nprobability of being human, k , required to avoid suspension (or, in \nother words, a threshold on bot likelihood above which the user gets \nsuspended). Specifically, for a policy of harshness k , users with bot \nprobability greater than (1\u2009\u2212\u2009 k) are suspended. The results of these \nanalyses are shown in Fig.\u00a0 3c.\nReanalyses of extra datasets\nFacebook sharing in 2016 by users recruited through YouGov. Here \nwe analyse data presented in ref.\u200911. A total of n \u2009=\u20091,191 survey respond -\nents recruited using YouGov gave the researchers permission to collect \nthe links they shared on Facebook for 2\u2009months (through a Facebook \napp), starting in November 2016. As part of the survey, participants \nself-reported their ideology (using a 5-point Likert scale; not includ -\ning participants who selected \u2018Not sure\u2019 , yielding n\u2009=\u2009995 respondents \nwith usable ideology data) and their party affiliation (Democrat,  \nRepublican, Independent, Other, Not sure). As in our Twitter studies, \nwe calculate low-quality information sharing scores for each user by \nusing the fact-checker and politically balanced crowd ratings for the \n60 news sites from ref.\u200938 , as described above in Table\u00a0 1. A total of 893 \nparticipants shared at least one rated link.\nTwitter sharing in 2018 and 2020 by users recruited through Pro-\nlific. Here we analyse data presented in ref.\u200941 . A total of n\u2009=\u20092,100 par-\nticipants were recruited using the online labour market Prolific in June \n2018. Twitter IDs were provided by participants at the beginning of \nthe study. However, some participants entered obviously fake Twit -\nter IDs\u2014for example, the accounts of celebrities. To screen out such \naccounts, we followed the original paper and excluded\u00a0accounts with \nfollower counts above the 95th percentile in the dataset. We had com -\nplete data and usable Twitter IDs for 1,901 users. As part of the survey, \nparticipants self-reported the extent to which they were economically \nliberal versus conservative, and socially liberal versus conservative, \nusing 5-point Likert scales. We construct an overall ideology measure \nby averaging over the economic and social measures. The Twitter API \nwas used to retrieve the content of their last 3,200 tweets (capped by \nthe Twitter API limit). Data were retrieved from Twitter on 18 August \n2018, and then again on 12 April 2020 (the latter data pull excludes \ntweets collected during the former data pull). We calculate low-quality \ninformation sharing scores for each user by using the fact-checker and \npolitically balanced crowd ratings for the 60 news sites from ref.\u200938 , as \ndescribed above in Table\u00a0 1. A total of 594 participants shared at least \none rated link in the 2018 data pull and 379 participants shared at least \none rated link in the 2020 data pull; 288 participants shared at least one \nrated link in both data pulls.\nTwitter sharing in 2021 by users who followed at least three politi -\ncal elites. Here we analyse data presented by Mosleh and Rand13, in \nwhich Twitter accounts for 816 elites were identified, and then 5,000 \nTwitter users were randomly sampled from the set of 38,328,679 us -\ners who followed at least three of the elite accounts. Each user\u2019s last \n3,200 tweets were collected on 23 July 2021, and sharing of low-quality \nnews domains was assessed using the fact-checker and politically bal -\nanced crowd ratings from ref.\u200938 . A total of 3,070 users shared at least \none rated link. The statistical model from ref.\u200939  was used to obtain a continuous measure of users\u2019 ideology on the basis of the ideological \nleaning of the accounts they followed.\nTwitter sharing in 2022 by users who followed at least three political \nelites. Here we analyse previously unpublished data, in which 11,805 \nTwitter users were sampled from a set of 296,202,962 users who fol -\nlowed at one of the political elite accounts from ref.\u200941 . We randomly \nsampled from users who had more than 20 lifetime tweets and followed \nat least three political elites for whom we had a partisanship rating. Each \nuser\u2019s last 3,200 tweets were collected on 25 December 2022, and shar -\ning of low-quality news domains was assessed using the fact-checker \nand politically balanced crowd ratings from ref.\u200938 . A total of 4,040 \nusers shared at least one rated link. The statistical model from ref.\u200939  \nwas used to obtain a continuous measure of users\u2019 ideology on the basis \nof the ideological leaning of the accounts they followed.\nTwitter sharing in 2023 by users who followed at least one political \nelite, stratified on follower count. Here we analyse previously unpub -\nlished data in which 11,886 Twitter users were randomly sampled, strati -\nfied on the basis of log10-transformed number of followers (rounded \nto the nearest integer) from the same set of 296,202,962 users who \nfollowed at one political elite account. On 4 March 2023, we retrieved \nall tweets made by each user since 22 December 2022 using the Twitter \nAcademic API. Sharing of low-quality news domains was assessed using \nthe fact-checker and politically balanced crowd ratings from ref.\u200938 .  \nA total of 4,408 users shared at least one rated link. The statistical model \nfrom ref.\u200939  was used to obtain a continuous measure of users\u2019 ideology \non the basis of the ideological leaning of the accounts they followed.\nSharing of false claims on Twitter. Here we analyse data from Ghezae \net\u00a0al.53. Unlike the previous analyses, this dataset does not use domain \nquality as a proxy for misinformation sharing. Instead, sets of specific \nfalse versus true headlines were used. The headline sets were assembled \nby collecting claims that third-party fact-checking websites such as \nsnopes.com or politifact.org had indicated were false, and collecting \nveridical claims from reputable news outlets. Furthermore, the head-\nlines were pre-tested to determine their political orientation (on the \nbasis of survey respondents\u2019 evaluation of how favourable the headline, \nif entirely accurate, would be for the Democrats versus Republicans; \nsee ref.\u200956 for details of the pre-testing procedure).\nSurvey participants were recruited to rate the accuracy of each URL\u2019s \nheadline claim. Specifically, each participant was shown ten headlines \nrandomly sampled from the full set of headlines, and rated how likely \nthey thought it was that the headline was true using a 9-point scale from \n\u2018not at all likely\u2019 to \u2018very likely\u2019 . For each headline, we created politically \nbalanced crowd ratings by averaging the accuracy ratings of partici -\npants who identified as Democrats, averaging the accuracy ratings of \nparticipants who identified as Republicans and then averaging these \ntwo average ratings. We then classify URLs as inaccurate (and thus as \nmisinformation) on the basis of crowd ratings if the politically balanced \ncrowd rating was below the accuracy scale midpoint.\nAdditionally, the Twitter Academic API was used to identify all Twitter  \nusers who had posted primary tweets containing each URL. These pri -\nmary tweets occurred between 2016 and 2022 (2016, 1%; 2017, 2%; 2018, \n4%; 2019, 5%; 2020, 34%; 2021, 27%; 2022, 27%). The ideology of each \nof those users was estimated using the statistical model from ref.\u200939  \non the basis of the ideological leaning of the accounts they followed. \nThis allows us to count the number of liberals and conservatives who \nshared each URL on Twitter.\nThe dataset pools across three different iterations of this procedure. \nThe first iteration used 104 headlines selected to be politically balanced, \nsuch that the Democrat-leaning headlines were as Democrat-leaning as \nthe Republican-leaning headlines were Republican-leaning; n\u2009=\u20091,319 \nparticipants from Amazon Mechanical Turk were then shown a random \nsubset of headlines that were half politically neutral and half aligned \nwith the participant\u2019s partisanship. The second iteration used 155 \nheadlines (of which 30 overlapped with headlines used in the first \niteration); n\u2009=\u2009853 participants recruited using Lucid rated randomly \nselected headlines. The third iteration used 149 headlines (no overlap \nwith previous iterations); n \u2009=\u2009866 participants recruited using Lucid \nrated randomly selected headlines. The Amazon Mechanical Turk sam -\nple was a pure convenience sample, whereas the Lucid samples were \nquota-matched to the national distribution on age, gender, ethnicity \nand geographic region, and then true independents were excluded. \nFor the 30 headlines that overlapped between iterations 1 and 2, the \npolitically balanced crowd accuracy ratings from Amazon Mechanical \nTurk and Lucid correlated with each other at r (28)\u2009=\u20090.75. Therefore, we \ncollapsed the politically balanced ratings across platforms for those \n30 headlines. In total, this resulted in a final dataset with fact-checker \nratings, politically balanced crowd ratings and counts of numbers of \nposts by liberals and conservatives on Twitter for 378 unique URLs.\nFinally, we also classified the topic of each URL. To do so, we used \nClaude, an artificial intelligence system designed by Anthropic that \nemphasizes reliability and predictability, and has text summarization \nas one of its primary functions. We uploaded the full set of headlines \nto the artificial intelligence system, and first asked it to summarize the \ntopics discussed in the headlines. We then asked it to indicate the topic \ncovered in each specific headline, and manually inspected the results \nto ensure that the classifications were sensible. Next, we examined \nthe frequency of each topic, synthesized the results into a set of six \noverarching topics and then finally asked the artificial intelligence \nsystem to categorize each headline into one of these six topics. This \nprocess led to the following distribution of topics: US Politics (174 head -\nlines), Social Issues (91 headlines), COVID-19 (48 headlines), Business/\nEconomy (41 headlines), Foreign Affairs (28 headlines) and Crime/\nJustice (26 headlines). As a test of the robustness of the classification, \nwe also asked another artificial intelligence system, GPT4, to classify \nthe first 100 headlines into the six topics. We found that Claude and \nGPT4 agreed on 80% of the headlines.\nSharing intentions of false COVID-19 claims across 16 countries.  \nHere, we examine survey data from ref.\u200937 . In these experiments, par-\nticipants were recruited from 16 different countries using Lucid, with \nrespondents quota-matched to the national distributions on age and \ngender in each country. Participants were shown ten false and ten true \nclaims about COVID-19 (sampled from a larger set of 45 claims), pre -\nsented without any source attribution. The claims were collected from \nfact-checking organizations in numerous countries, as well as sources \nsuch as the World Health Organization\u2019s list of COVID-19 myths. This \napproach removes ideological variation in exposure to misinforma -\ntion online13, as well as any potential source cues/effects, and directly \nmeasures variation in the decision about what to share.\nAs in our other analyses, we complement the professional veracity \nratings with crowd ratings. Specifically, n \u2009=\u20098,527 participants in the \nAccuracy condition rated the accuracy of each of the headlines they \nwere shown using a 6-point Likert scale. We calculate the average accu -\nracy rating for each statement in each country, and classify statements \nas misinformation if that average rating is below the scale midpoint.\nOur main analyses then focus on the responses of the n\u2009=\u20098,597 par-\nticipants from the Sharing condition, in which participants indicated \ntheir likelihood of sharing each claim using a 6-point Likert scale. To \ncalculate each user\u2019s level of misinformation sharing, we first discretize \nthe sharing intentions responses such that choices of 1 (Extremely \nunlikely), 2 (Moderately unlikely) or 3 (Slightly unlikely) on the Likert scale are counted as not shared, whereas choices of 4 (Slightly likely), \n5 (Moderately likely) or 6 (Extremely likely) are counted as shared. We \nthen determine, for each user, the fraction of shared articles that were \n(1) rated as false by fact-checkers, and (2) rated as below the accuracy \nscale midpoint on average by respondents in the Accuracy condition.\nWe then ask how misinformation sharing varies with ideology within \neach country. Specifically, we construct a conservatism measure by \naveraging responses to two items from the World Values Survey that \nwere included in the survey, which asked how participants would place \ntheir views on the scales of \u2018Incomes should be made more equal\u2019 versus \n\u2018There should be greater incentives for individual effort\u2019 and \u2018Govern-\nment should take more responsibility to ensure that everyone is pro -\nvided for\u2019 versus \u2018People should take more responsibility to provide for \nthemselves\u2019 using 10-point Likert scales. Pilot data collected in the USA \nconfirmed that responses to these two items correlated with self-report \nconservatism (r(956)\u2009=\u20090.32 for the first item and r(956)\u2009=\u20090.40 for the \nsecond item).\nReporting summary\nFurther information on research design is available in the\u00a0Nature Port -\nfolio Reporting Summary linked to this article.\nData availability\nAll data necessary to reproduce the results are available at https://osf.\nio/a2t7d/.\nCode availability\nAll scripts necessary to reproduce the results are available at https://\nosf.io/a2t7d/.\n \n57. Eady, G., Nagler, J., Guess, A., Zilinsky, J. & Tucker, J. A. How many people live in political \nbubbles on social media? Evidence from linked survey and Twitter data. Sage Open 9, \n2158244019832705 (2019).\n58. Pennycook, G. et\u00a0al. Shifting attention to accuracy can reduce misinformation online. \nNature 592, 590\u2013595 (2021).\n59. Mosleh, M., Martel, C., Eckles, D. & Rand, D. G. Perverse downstream consequences of \ndebunking: being corrected by another user for posting false political news increases \nsubsequent sharing of low quality, partisan, and toxic content in a Twitter field experiment. \nIn Proc. 2021 CHI Conference on Human Factors in Computing Systems (eds Kitamura, Y. \net\u00a0al.) 1\u201313 (2021).\n60. Coppock, A. & McClellan, O. A. Validating the demographic, political, psychological,  \nand experimental results obtained from a new source of online survey respondents. Res. \nPolitics 6, 2053168018822174 (2019).\nAcknowledgements We thank A. Guess and I. Ghezae for assistance in our re-analysis of  \ndata from the papers on which they are lead authors. We thank A. Bear and B. Tappin for \nconstructive feedback that improved this work.\nAuthor contributions M.M. and D.G.R. designed the study. M.M. and Q.Y. collected the data. M.M. \nand D.G.R. analysed the results. D.G.R. and G.P. wrote the paper with input from T.Z. and M.M.\nCompeting interests M.M., D.G.R. and G.P. have received funding from Meta and Google to \nconduct research related to reducing the spread of misinformation online and identifying \ninauthentic accounts. G.P. was a Faculty Research Fellow at Google in 2022. D.G.R. was an \nunpaid consultant for Twitter in 2021 and 2022. Q.Y. was an intern at Snapchat in 2020, and \nworked at Meta 2021\u20132023.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-024-07942-8 .\nCorrespondence and requests for materials should be addressed to David G. Rand.\nPeer review information Nature thanks Adela Levis, Sander van\u00a0der Linden, Yunkang Yang, \nDavid Yokum, Alix Zwane and the other, anonymous, reviewer(s) for their contribution to the \npeer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\nAnalysis\nExtended Data Fig. 1 | Twitter users who used Trump hashtags shared links \nto lower quality news sites than Twitter users who used Biden hashtags, \nregardless of which set of quality ratings are used. Low Quality News Site Sharing scores by partisanship using alternative quality rating sets. Scores are \nstandardized (z-scored) for comparability; higher values indicate lower quality \nnews sharing.\nExtended Data Fig. 2 | News quality ratings from Pennycook & Rand (2019). Ratings generated by politically balanced crowds of laypeople (y-axis) and \nprofessional fact-checkers (x-axis). News sources shown as orange diamonds are counted as low quality for simulating suspensions.\nAnalysis\nExtended Data Fig. 3 | No evidence of topics for which liberals on Twitter \nshare more links to false\u00a0articles than conservatives. Further analysis of  \nthe number of posts containing misinformation links shared by liberals  \nversus conservatives on Twitter when analyzing data from Ref. 53 . Shown is  \nthe coefficient on an ideology dummy in a linear regression predicting  log10(# misinformation shares +1) separately for URLs of each topic, along  \nwith the overall estimate from a random effects meta-analysis. Top panel shows \nresults when considering URLs rated as false by professional fact-checkers; \nbottom panel shows results when considering URLs rated as inaccurate by \npolitically-balanced crowds.\nExtended Data Fig. 4 | Twitter users who used Trump hashtags are rated as more likely to be bots than Twitter users who used Biden hashtags. Bot Sentinel43 \nscores for the Twitter users in our 2020 election study, by partisanship (estimated based on whether users shared more Biden versus Trump hashtags).\nAnalysisExtended Data Table 1 | Quality of news domains shared significantly predicts suspension, unlike political orientation\nRegression models predicting which Twitter accounts in our sample were suspended. Models 2 and 4 shows coefficients from ridge regression. For details of the independent variables, \nsee\u00a0Methods section 1 and SI Section S1.\n1 nature portfolio  |  reporting summaryApril 2023\nCorresponding author(s): David G. Rand\nLast updated by author(s): Jun 19, 2024\nReporting Summary\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistenc y and transparency \nin reporting. For further information on Nature Portfolio policies, see our Editorial Policies  and the  Editorial Policy Checklist .\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Me thods section.\nn/a Confirmed\nThe exact sample size ( n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided \nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regress ion coefficient) \nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists  contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection We used Python 3.8.5 and R  4.2.2 for data collection and data cleaning. \nData analysis We used STATA 18 for statistical analysis.\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published lit erature, software must be made available to editors and \nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio  guidelines for submitting code & software for further information.\nData\nPolicy information about availability of data\nAll manuscripts must include a  data availability statement . This statement should provide the following information, where applicable: \n- Accession codes, unique identifiers, or web links for publicly available datasets \n- A description of any restrictions on data availability \n- For clinical datasets or third party data, please ensure that the statement adheres to our  policy  \n \nAll data necessary to reproduce the results are available at  https://osf.io/a2t7d/\n2 nature portfolio  |  reporting summaryApril 2023Research involving human participants, their data, or biological material\nPolicy information about  studies with human participants or human data . See also policy information about sex, gender (identity/presentation), \nand sexual orientation  and race, ethnicity and racism .\nReporting on sex and gender We did not collect sex/gender of the participants/users in the studies.\nReporting on race, ethnicity, or \nother socially relevant groupingsWe did not collect race, ethnicity or other socially relevant information  of the participants/users in the studies.\nPopulation characteristics We focused on a politically balanced set of users (half-democrats and half republicans) based on the related election hashtags they shared on Twitter. We did not estimate or collect age or gender of the users.\nRecruitment In October 2020 we identified 100,000 Twitter users who shared hashtags related to the U.S. Presidential Election, and randomly sampled 4,500 of those users who shared at least one #VoteBidenHarris2020 hashtag and 4,500 who shared at least one #Trump2020 hashtag. \nEthics oversight The study was conducted at MIT. The study is observational and uses public social media data and did not require ethical approval.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before m aking your selection.\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\nBehavioural & social sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nStudy description We investigated probability of Twitter users  being suspended during the course of 2020 US election using a variety of users \ncharacteristics and behaviors on the platform including political ideology, sharing low-quality content, and use of toxic langu age. We \ncomplemented our main results by seven other datasets of sharing from Twitter, Facebook, and survey experiments, spanning 2016 to 2023\nResearch sample Our sample includes 9000 Twitter users based on the related election hashtags they shared on Twitter. \nSampling strategy In October 2020 we identified 100,000 Twitter users who shared hashtags related to the U.S. Presidential Election, and randomly  \nsampled 4,500 of those users who shared at least one #VoteBidenHarris2020 hashtag and 4,500 who shared at least one #Trump2020 hashtag. We used each user\u2019s data from that pre-election time period to quantify their tendency to share low quality  \nnews (as well as numerous other potentially relevant characteristics), and then checked seven months later (after the election season) to determine which users got suspended by Twitter.\nData collection We used the Twitter API to collect public data of the users on the platform. \nTiming We collect users data in October 2020 (during the 2020 US election) then checked seven months later (after the election season)  to \ndetermine which users got suspended by Twitter \nData exclusions We didn't exclude any users' data.\nNon-participation Our study is observational and we included information of all 9000 sampled user in our study.\nRandomization Our study is observational. We created politically sample set of users from a larger sample of 100,00 Twitter users who shared hashtags related to US presidential election in during October 2020.\nReporting for specific materials, systems and methods\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here,  indicate whether each material, \nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the approp riate section before selecting a response. \n3 nature portfolio  |  reporting summaryApril 2023Materials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nClinical data\nDual use research of concern\nPlantsMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Differences in misinformation sharing can lead to politically asymmetric sanctions", "author": ["M Mosleh", "Q Yang", "T Zaman", "G Pennycook", "DG Rand"], "pub_year": "2024", "venue": "Nature", "abstract": "In response to intense pressure, technology companies have enacted policies to combat  misinformation 1 , 2 , 3 \u2013 4 . The enforcement of these policies has, however, led to technology"}, "filled": false, "gsrank": 166, "pub_url": "https://www.nature.com/articles/s41586-024-07942-8", "author_id": ["DAivUTUAAAAJ", "", "SA6zXVIAAAAJ", "AIbJenwAAAAJ", "C0ANojIAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:mU0U1_wVw-wJ:scholar.google.com/&output=cite&scirp=165&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D160%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=mU0U1_wVw-wJ&ei=IbWsaPvmC7_SieoPzJnloAQ&json=", "num_citations": 23, "citedby_url": "/scholar?cites=17060503989093944729&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:mU0U1_wVw-wJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.nature.com/articles/s41586-024-07942-8.pdf"}}, {"title": "POLOR: Leveraging Contrastive Learning to Detect Political Orientation of Opinion in News Media", "year": "2024", "pdf_data": "POLOR: Leveraging Contrastive Learning to Detect Political Orientation of\nOpinion in News Media\nAla Jararweh\nThe University of New Mexico\nAlbuquerque NM, USA\najararweh@unm.eduAbdullah Mueen\nThe University of New Mexico\nAlbuquerque NM, USA\nmueen@unm.edu\nAbstract\nNews articles are naturally influenced by the values, be-\nliefs, and biases of the reporters preparing the stories\nand the policies of the publishing outlets. Numerous\nstudies and datasets have been proposed to detect the\npolitical orientation of news articles. However, most of\nthese studies ignore real textual clues and learn the tex-\ntual signature of the source (commonly the publisher\nand rarely the writer) of the article instead. Moreover,\na good volume of opinion pieces published by major\nnews outlets do not reflect the political orientation of the\npublisher but rather reflect the political orientation of a\nnon-professional writer. Existing methods are not built\nto correct this difference in the training data and, hence,\nperform poorly on human-annotated data. We propose,\nPOLOR, a fine-tuned BERT model that employs con-\ntrastive learning to detect the political orientation of\nnews articles even when the training data is labeled by\nthe source (i.e. the publisher of the news article). Unlike\nprevious work in the literature, the model learns fea-\ntures by employing different contrastive learning objec-\ntives where each sentence is contrasted with sentences\nfrom various sources simultaneously. POLOR achieves\na 15% increase on our dataset compared to previously\nproposed baselines. Finally, we release two datasets of\nopinion news: source-annotated and human-annotated\ndatasets. The full paper including supplementary ma-\nterials, code, and datasets can be found at https:\n//www.cs.unm.edu/ \u02dcajararweh/ .\n1 Introduction\nPolitical bias in news can be emphasized in various forms\nsuch as rephrasing, or presenting one-sided facts to serve\na specific orientation. This bias directly impacts the in-\nformation consumed by the readers and the political atti-\ntudes (Gentzkow and Shapiro 2006). Detecting the polit-\nical orientation of news is a challenging task due to the\nnecessity of understanding the multidimensional political\ndiscourse. For example, mainstream media worldwide use\nterms like \u201dexplosion\u201d versus \u201dattack\u201d when reporting on\nrecent conflicts in the Middle East to favor one politi-\ncal position over another (de Jong 2023). This issue has\nCopyright \u00a9 2024 by the authors.\nThis open access article is published under the Creative Commons\nAttribution-NonCommercial 4.0 International License.\n(a) Crowd-Sourcing Labels\n (b) Source Labels\nFigure 1: Visualized article embeddings generated by\nPOLOR, and annotated according to (a) article orientation,\nand (b) source orientation. The blue points represent Liberal ,\nand the red points represent Conservative . Source annota-\ntions do not always reflect all article orientations published\nby that source. Consequently, models trained on source an-\nnotations without utilizing auxiliary information, learn to\npredict sources instead of capturing article-specific orienta-\ntions in Figure 1(a).\nbeen studied in various contexts and a variety of datasets\nhas been proposed. Researchers typically use online plat-\nforms such as AllSides.com ,AdFontesMedia.com ,\nMediaBiasFactCheck.com , etc to collect and annotate\nnews articles.\nSuch platforms usually offer political annotations at the\nnews source level. To obtain article-specific orientation, an-\nnotations are usually propagated from the source annotations\n(Lee et al. 2022; Baly et al. 2020; Chen et al. 2018; Liu et al.\n2022; Kulkarni et al. 2018). For example, AllSides.com\nannotates all articles published by the New York Times as\nLiberal. Figure 1 shows visualized embeddings of articles\ngenerated by our model. The dimensions of the embeddings\nwere reduced using t-distributed Stochastic Neighbor Em-\nbedding (van der Maaten and Hinton 2008). The true arti-\ncle orientations (Figure 1(a)) differ from the annotations de-\nrived from sources (Figure 1(b)). The red points are more\ndistributed to the right and the blue points are to the left in\nboth figures which illustrates that the source orientations can\nbe related to the article orientations.\nRelying on the source annotation without utilizing auxil-\niary knowledge, the trained models may attempt to learn the\nwriting style of the news source instead of predicting the ori-\nentations of news articles. The problem is emphasized when\nopinion pieces are considered. Opinion pieces usually rep-\nresent the bias of the authors, and are often published by\na media with opposite bias. This problem has been lightly\ntouched on by Baly et al. (2020), where the authors found\nthat the model suffers when trained on source-labeled data\nand tested on unseen data (i.e. new media sources). To over-\ncome this, they inserted a source classifier to minimize the\nknowledge learned from the source. However, they have not\ntested their model on human-annotated data where the la-\nbels are derived based on the text in articles, paragraphs, or\nsentences.\nIn this paper, we propose POLOR : leveraging contrastive\nlearning to detect the POL itical ORientation of opinion\npieces in news media. POLOR exploits several contrastive\nlearning objectives where each sentence is contrasted to a\nset of sentences from different and similar sources. We adopt\ndifferent objective functions to generate additional features\nthat focus on textual cues related to the political bias of ar-\nticles, instead of the style of the news media. POLOR pro-\nduces sentence-specific and article-specific labels based on\nthe training data derived from sources.\n2 Related Work\nDetecting the political orientation in news media can be\nchallenging due to the necessity of domain expertise. In\ntheir early attempts, researchers measured the orientation\nof a particular news media by counting the number of ci-\ntations to liberal and conservative think-tank policy groups\n(Groseclose and Milyo 2005). Focusing on the article con-\ntent, Greene and Resnik (2009) used domain-specific words\nin news articles to classify articles that support Palestine\nor Israel. Similarly, researchers harnessed the recent find-\nings in Deep Learning to understand the political orientation\nof news articles at different levels of article structure such\nas character-based word representations (Jiang et al. 2019),\nsentences (Kim and Johnson 2022; Gangula, Duggenpudi,\nand Mamidi 2019; Chen et al. 2018), and articles (Baly et\nal. 2020; Chen et al. 2020; Liu et al. 2022).\nVarious datasets were proposed to tackle this task with\na variety of annotation techniques. Since it\u2019s challenging\nand expensive to annotate a large corpus of news arti-\ncles, different studies rely on online platforms such as\nAllSides.com to crawl and annotate news articles (Baly\net al. 2020; Chen et al. 2020; Liu et al. 2022; Roy and Gold-\nwasser 2020). The annotations of articles are usually prop-\nagated from the source orientation. However, a major draw-\nback of these annotations is that models learn to predict the\nsource signature rather than the article\u2019s orientation. To over-\ncome this, researchers use online crowd-sourcing platforms\nto obtain article-specific annotations (Gangula, Duggen-\npudi, and Mamidi 2019; Fan et al. 2019; Card et al. 2015;\nBudak, Goel, and Rao 2016; Lazaridou et al. 2020). To ad-\ndress the problem of predicting article-specific labels from\nsource-based annotations, few datasets were found that in-\ntegrate both annotations. Kiesel et al. (2019) proposed two\ndatasets but the human annotation dimensions of articles aredifferent from our task. To this end, we construct a new\ndataset that spans 10 news sources with two different an-\nnotations.\nLiberal Conservative\nArticles# 156 107 263\nparagraphs# 5146 3099 8245\nSentences# 7743 3920 11921\nSources# 4 3 7\n(a) Statistics of the Source-Based dataset.\nLiberal Conservative Neutral\nArticles# 22 17 0 39\nParagraphs# 138 133 42 313\nSentences# 522 513 188 1223\nSources# - - - 3\n(b) Statistics of the Human-annotated dataset.\nTable 1: Exploring the constructed dataset for this task.\n3 Datasets\nWe evaluate our model performance on two datasets. The\nfirst dataset consists of approximately 6,000 articles an-\nnotated via crowd-sourcing from Budak, Goel, and Rao\n(2016). It also spans 11 news sources and explores 15 dif-\nferent topics. Since the source annotations are not avail-\nable, we extend the dataset by deriving annotations from\nAllSides.com . For a broader evaluation, we construct\nanother dataset for this task. Our dataset consists of two\nsubsets, crowd-sourcing annotations and source annotations.\nThe constructed dataset addresses two high-profile criminal\ncases, namely Ahmaud Arbery1andKyle Rittenhouse2.\nSource-Based Dataset. We collected a total of 263 news\narticles which cover 7 different U.S. news sources. We align\nwith recent work in the literature, we obtained annotations\nfrom AllSides.com . This website only offers annota-\ntions at the source level based on multiple methods such as\nEditorial Review, Blind Bias Survey, and Community Feed-\nback. We collect articles from 4 liberal news sources and\n3 conservative news sources according to the annotations\nfound on the website. Table 1(a) shows some detailed statis-\ntics of the dataset. We follow the same annotation process of\nrecently published datasets where the article annotations are\npropagated from their source annotations.\nHuman-Annotated Dataset. We also construct a smaller\ndataset for evaluating the model on human annotations. This\ndataset consists of 39 news articles from three alternative\nU.S. news sources, different from those in the training data\n(Source-Based Dataset). We aim to evaluate our model\u2019s\nability to generalize to new writing styles. Thus, we only\nconsider opinion news published about the two cases. The\narticles were collected via word matching from the news\n1https://en.wikipedia.org/wiki/Murder_of_\nAhmaud_Arbery\n2https://en.wikipedia.org/wiki/Kyle_\nRittenhouse\nFigure 2: An overview of POLOR, which utilizes contrastive\nlearning objectives to extrapolate article-specific labels.\nmedia websites. The annotations were performed at the para-\ngraph level, where each article was partitioned into a set of\nparagraphs to provide annotators with more context about\nthe cases. The article annotations are then derived based on\nmajority voting. Each paragraph is labeled by three differ-\nent workers into one of three different categories: Liberal,\nConservative, or Neutral. The workers were provided with\na set of instructions and facts about the two cases ahead of\ntime. 92% of the articles received an inter-annotator score\nabove 0.9, and 8.0% are below 0.9. The articles that have\nlow scores (below 0.5) were annotated Neutral. The news\nsource of each paragraph was hidden from the workers and\nthey only could see the content of the paragraph. Table 1(b)\nshows the detailed statistics and the annotation results. To\nmatch with annotation dimensions in the source data, we\nonly consider conservative and liberal paragraphs in our\nevaluation.\nData Preparation. The datasets are further pre-processed\nby removing unnecessary content such as punctuation, non-\nEnglish characters, and identifying words and sentences. We\nthen split each article/paragraph into a set of sentences. The\nfinal preparation step is to formulate the datasets as triplets.\nEach sentence in the dataset Sa(also called Anchor) is at-\ntached to a set of positive sentences Sp, and a set of negative\nsentences Sn. We define a sentence as positive if it belongs\nto the same class and from a different source, and negative\nif it belongs to the opposite class. For example, an anchor\nlabeled as \u201dConservative\u201d, its positive set is randomly de-\nrived from other sentences labeled as \u201dConservative\u201d, and\nits negative set is randomly derived from sentences labeled\nas \u201dLiberal\u201d. We also ensured that the triplets of the training,\nvalidation, and test set were completely disjoint. The size of\nthe positive and negative sets ( T) is a parameter.\n4 Methodology\nWe propose POLOR, a complete framework that leverages\nmulti-objective contrastive learning to generate embeddingsthat portray the bias in news articles by minimizing the\nsource information. Figure 2 shows the main components\nof the model, namely Sentence Embedding, Multi-Objective\nContrastive Learning, Prediction, and Loss Fusion (Joint\nLoss).\n4.1 Sentence Embedding\nThe sentences are first tokenized, where each sentence in\nanchor, negative, or positive sets is represented as a list of\nNtokens. The tokens are passed to the pre-trained BERT\nmodel to generate contextualized token representations (i.e.\nembeddings) where each token, Ti, is represented in an em-\nbedding vector, Ei. Figure 2 shows the input and output of\nBERT. At the end of this stage, each sentence Swill be rep-\nresented with Ncontextualized token representations, where\nEi\u2208R768. To synthesize sentence embeddings, we use the\npooling strategies, CLS (classification token) Pooling and\nMean Pooling introduced by Reimers and Gurevych (2019).\n4.2 Multi-Objective Contrastive Learning\nOur goal is to tune the representations such that they ex-\nclusively represent the bias in the content of articles. Since\nthe training dataset (Source-Based Dataset) only contains\nsentences annotated based on sources, the model learns ad-\nditional features in an unsupervised manner. That is, the\nsource influence is minimized and the influence of the po-\nlitical orientation in the text is maximized. To achieve this\ngoal, we employ different objective functions that utilize\ncontrastive learning. In the results section, we empirically\nshow that augmenting contrastive objectives on top of BERT\nhelps to learn rich features by contrasting other sentences\nfrom similar/different sources in the dataset. We attempt to\nfind a better representation by employing a triplet loss func-\ntion. The triplet loss function was introduced by Schroff,\nKalenichenko, and Philbin (2015) in the field of face recog-\nnition to optimize face image embeddings. The authors de-\nfine positive examples as similar images of the same face,\nand negative examples are all other faces in the mini-batch.\nWe adopt an alternative solution to this. We first assign\na set of Tpositives and a set of Tnegatives from the en-\ntire dataset at random for each anchor. We then apply dif-\nferent objective functions on the positive and negative sets\nto synthesize rich representations to achieve different opti-\nmizations. After applying each objective function, the syn-\nthesized representation should have a similar dimension to\nthe anchor ( R768). The synthesized representations of pos-\nitive and negative sets are then contrasted with the anchor\nto obtain the loss for the desired objective. Given an anchor\nsentence Sa, set of positive sentences Spand, and set of neg-\native sentences Sn, where SpandSnof size T, we re-frame\nthe loss function as follows:\nL(Sa, Sp, Sn) =\nmax(\n\u2225Sa\u2212 F(Sp; 1,T)\u22252\u2212 \u2225Sa\u2212 F(Sn; 1,T)\u22252+m\n0\n(1)\nwhere mis the margin to be enforced between the positive\nand negative sets, and F(S; 1,T)is an objective function\nused to synthesize a representation from the set of positive\nand negative sets. In the following subsections, we describe\nthe objective functions used to synthesize the representa-\ntions. We then define the Joint loss function which is de-\nsigned to fuse all losses, obtained by contrasting data to the\nsynthesized representations, into one universal function.\nAdditive Attention. Since the sentences are annotated\nbased on source, the annotation does not always reflect the\ntrue orientation of the sentences. The model attends to the\nsentences in the positive and negative sets based on impor-\ntance and relativity to the anchor. We modify the additive at-\ntention mechanism mentioned in Bahdanau, Cho, and Ben-\ngio (2014) to serve our goal. This attention mechanism was\nmainly proposed in machine translation to synthesize a con-\ntext vector of words in a sentence. The context vector is a\nweighted sum of the encoder\u2019s hidden state with the atten-\ntion scores. The attention scores are usually computed by\nfeeding the concatenation of the encoder\u2019s current hidden\nstate and the decoder\u2019s previous hidden state to an alignment\nmodel (a non-linear transformation of the input).\nIn our case, the position of sentences in the posi-\ntive/negative set does not matter in predicting the anchor.\nConsequently, we replace the hidden state of the encoder and\nthe decoder with two feed-forward layers where the input is\nthe positive/negative sentence embeddings. The results are\nthen concatenated and fed to an alignment model to calcu-\nlate the attention scores.\ne=W3.tanh ([W1\u00b7E;W2\u00b7E])\nai=exp(ei)PT\nk=1exp(ek)(2)\nwhere Eis the embedding of sentences from the posi-\ntive/negative set. The final step is to calculate the weighted\nsum of positive/negative embeddings with the attention\nscores:\nF(E; 1,T) =TX\ni=1ai.Ei (3)\nwhere we obtain LFadditive by substituting F(E; 1,T)in\nEq. 1 for the negative and positive sets separately.\nUnsupervised MinMax. The goal of this objective func-\ntion is to find the appropriate negative and positive sentences\nextracted from the entire positive and negative sets. We de-\nfine a sentence as appropriate such that it influences the\nmodel to increase the margin between the negative set and\nthe positive set in the Euclidean space. To achieve this goal,\nunsupervised MinMax treats the positive and negative sen-\ntences as one set where the closest sentence is considered\nnegative and the furthest sentence is considered positive.\nF(Sp; 1,T) =argmaxs\u2208(Sp\u222aSn)\u2225g(Sa)\u2212g(s)\u22252\n2\nF(Sn; 1,T) =argmins\u2208(Sp\u222aSn)\u2225g(Sa)\u2212g(s)\u22252\n2(4)\nwhere g(s)is the embedding of the sentence s. The loss\nfunction LFMinMax is then obtained by substituting in Eq. 1.This selection ensures fast convergence and helps to increase\nthe margin between the anchor and negative sentences and\ndecrease it with positive sentences (Schroff, Kalenichenko,\nand Philbin 2015).\n4.3 Joint Loss (Fusion)\nThe losses obtained by the above objective functions are as-\nsembled to allow the model to learn better representation.\nWe use different hyper-parameters to weigh the objective\nlosses and then join them into one loss function. In addition\nto the unsupervised contrastive loss, we incorporated Binary\nCross-Entropy loss ( LXEnt ) in the final loss function to ob-\ntain the Join loss ( LJ):\nLJ=\u03b1\u00b7 LFadditive +\u03b2\u00b7 LFMinMax\n+ (1\u2212\u03b1\u2212\u03b2)\u00b7 LXEnt(5)\nwhere LFis the triplet loss obtained by applying a specific\nobjective function, and 0< \u03b1, \u03b2 < 0.5. Note that, when a\nparameter equals 0, the associated loss function is not incor-\nporated into the Joint loss.\n5 Experiments and Results\n5.1 Baselines\nOur goal is to classify the article-based political orientation\nof an article using training data labeled by their sources. We\nevaluate our model performance against different baselines\nwith different training setups of our model:\n1.Majority classifier : We consider the most frequent class\nin the dataset as the predicted label for all sentences.\n2.Joint function with XEnt loss only : We use the sentence\nrepresentations generated by BERT to perform classifica-\ntion in a supervised manner. For this baseline, the con-\ntrastive learning loss is entirely ignored (i.e. \u03b1= 0 and\n\u03b2= 0). This approach is similar to Fan et al. (2019).\n3.Joint function with XEnt and Additive : We considered\ntwo loss functions in the joint function (Eq. 5), namely\nadditive and XEnt. That is, \u03b2is set 0.\n4.Joint function with XEnt and MinMax : We consider\ntwo loss functions in the joint function (Eq. 5), namely\nXEnt and MinMax. That is, \u03b2is set 0.\n5.Joint function with XEnt loss and Random Triplet ob-\njective : We consider only one contrastive objective func-\ntion that selects a random positive and negative sentence\nfrom the sets SpandSnat each iteration. The joint loss\nbecomes:\nLJ=\u03b1\u00b7 LFRT+ (1\u2212\u03b1)\u00b7 LXEnt\nThis setup is approximately similar to Kim and Johnson\n(2022) and Baly et al. (2020) with few differences. In their\nformulation, the triplet is usually chosen at random in an\noffline manner (i.e. when the dataset is created) and re-\nmains fixed throughout the whole training process. More-\nover, their method of choosing positive and negative sen-\ntences is different from our approach. For example, Kim\nand Johnson use predefined sub-framing groups to assign\nthe triplets, while Baly et al. use media sources.\nLarge Media Our dataset\nLosses Model AccArt F1Art AccSent F1Sent AccArt F1Art\nMajority - 0.52 0.34 0.50 0.34 0.51 0.34\nXEnt (BERT) Fan et al. (2019) 0.63 0.60 0.51 0.51 0.62 0.61\nXEnt + Additive ours 0.65 0.62 0.53 0.53 0.67 0.65\nXEnt + MinMax ours 0.63 0.60 0.53 0.53 0.69 0.68\nXEnt + Random Kim and Johnson (2022), 0.62 0.59 0.53 0.47 0.64 0.64\nBaly et al. (2020)\nXEnt + Centroid ours 0.63 0.61 0.53 0.53 0.64 0.62\nXEnt + Additive POLOR 0.68 0.66 0.55 0.520 0.74 0.74\n+ MinMax\nTable 2: The results of comparing our multi-objective model (POLOR) with the baselines. The results on our dataset are\nreported on sentence predictions (Sent) and article predictions (Art) using accuracy (Acc) and macro F1 score metrics. For the\nlarge media study, the model was directly trained on articles.\n6.Joint function with XEnt loss and Centroid Triplet : We\nconsider only one contrastive objective function which is\nthe mini-batch mean. In this objective, the positive is the\ncentroid of the positive set and the negative is the centroid\nof the negative set. The joint loss becomes:\nLJ=\u03b1\u00b7 LFcentroid + (1\u2212\u03b1)\u00b7 LXEnt\n5.2 Quantitative Results\nOur Dataset. Table 2 shows the performance of our model\nin predicting political orientation compared to the proposed\nbaselines. POLOR outperforms all baseline models on sen-\ntence accuracy, article accuracy, and article macro F1 score.\nWe use majority voting (highest occurring class) on sen-\ntences to predict the article labels. We first notice that the\nbaseline models outperform the majority classifier when pre-\ndicting the orientation of articles, but struggled with sen-\ntence orientation. Because sentence labels are propagated\nfrom articles, which are subsequently propagated from the\nsources. Ideally, sentence-specific labels would solve the\nproblem. However, in the absence of sentence-specific la-\nbels, article-specific labels from independent sources would\nimprove the performance. Moreover, choosing the proper\nobjective function can drastically affect performance. So-\nphisticated objective functions may not always yield the de-\nsired performance. For example, the simple random triplet\nobjective shows a competitive performance even though it\nrequires almost no additional computations beyond selecting\na sentence at random. Finally, additive and MinMax achieve\na good performance, and when combined (POLOR), they\nachieve the highest outcomes. The combined objectives also\nillustrate a good performance on unseen source signatures,\nsince the news sources in the testing set were not seen during\ntraining.\nLarge Media Dataset. The main objective of this experi-\nment is to analyze the model behavior on a larger dataset. We\nevaluate our model performance on a dataset of 6089 news\narticles from Budak, Goel, and Rao (2016). The dataset is\nonly annotated via crowd-sourcing. To obtain source anno-\ntations, we manually annotate articles based on source an-\nnotations from ALLSIDE.com website. We test our modelperformance on the entire article text to reduce the runtime.\nTable 2 shows the experiment results on this dataset. Our\nmodel of the combined Additive and MinMax objectives is\nstill outperforming the baselines by a good margin. We also\nnote that the additive objective separately achieves a com-\npetitive performance to POLOR.\nFigure 3: Critical difference diagram showing pairwise sta-\ntistical difference comparison of the proposed baselines.\nSignificance Test. To further explain POLOR perfor-\nmance compared to the proposed baselines, we conduct a\ncritical difference test based on the Wilcoxon-Holm method\nto detect pairwise significance. The test was performed us-\ning 15-fold cross-validation on the large media dataset only.\nWe have not performed the test on our dataset since the an-\nnotated test set is fixed, and running the experiment k-times\nwould always yield the same results. Figure 3 shows the\nresults of the significance test with a significance level of\n0.05. POLOR ranks first compared to the proposed baselines\nwhich demonstrated a significant performance in predicting\nhuman-level labels from news source annotations. The test\nalso illustrates the superior capability of the additive objec-\ntive in this task, unlike the MinMax objective which is antic-\nipated to be impacted by the distances between the articles\nin the positive and negative sets.\n5.3 Qualitative Results\nVisual Analysis of Model Attention. We further demon-\nstrate the model performance by delving deeper into two\ncorrectly predicted sample paragraphs where the source an-\nnotations contradict the text annotations. Figure 4 depicts the\nmodel attention using the SHAP explainer (Lundberg and\nLee 2017) where the conservative and liberal contributions\nare reflected by the red and blue highlights respectively. Al-\nthough the selected examples are intricate and potentially\n(a) New York Times (Liberal), Paragraph Label: Conservative\n(b) Wall Street Journal (Conservative), Paragraph Label: Liberal\nFigure 4: Visualizing the model attention on two real exam-\nples where the text label contradicts the source label.\nelusive for some readers to catch the subtle undertone, the\nmodel efficiently allocated more attention to underlying sig-\nnaling words and phrases. In Figure 4(a), the model demon-\nstrates higher attention toward the term \"Republican\"\ncompared to the term \"Democrats\" . Similarly, the phrase\n\"their ... care legislation\" also contributes\nto the paragraph prediction since it is proposed or sup-\nported by a democratic group as the paragraph states, and the\nphrase \"foreshadowed a Republican\" which indi-\ncates a positive framing of anticipated Republican success.\nFor Figure 4(b), the model attended to framing words that\ntend to humanize former President Barack Obama such as\n\"wiping away tears\" where it falls short in capturing\nother phrases like \"act against gun violence\" .\nSince the paragraph labels exclusively contradict the source\nlabels, the given remarks qualitatively suggest that the model\ndevotes more attention to the signaling phrases in the text\nrather than learning the source writing style.\nFigure 5: The performance of POLOR on unseen topics. The\nmodel shows a relatively stable performance and always out-\nperforms the majority classifier even when we increase the\nnumber of hidden topics.\nCross-Topic Analysis. The baseline results in the quanti-\ntative section demonstrate that the model learns the political\nideology of the news, not the source writing style since the\ntest set was collected from unseen sources. However, that ex-\nperiment falls short of explaining whether the model learns\nother textual properties such as topics. In this experiment,\nwe are interested in measuring the model\u2019s generalizability\non novel topics where we evaluate the model performanceon unseen topics during testing. We iteratively increase the\nnumber of unseen topics (from 0 to 5 topics). We report the\naccuracy values across five different runs to account for the\nvariations and uncontrollable randomness. For comparabil-\nity reasons, we also include the majority classifier accuracy\non the given split. We perform this experiment on the Large\nMedia dataset since it consists of multiple topics. Figure 5\nshows the results of this experiment. The model shows rel-\natively stable performance even when we increase the num-\nber of hidden topics. The model always outperforms the ma-\njority classifier even when we increase the number of hid-\nden topics. That is, the model is still capable of predicting\narticle-specific labels on novel topics that were not seen dur-\ning training.\nMajority Acc F1\nSource annotations 0.74 0.91 0.91\nCrowd-sourcing 0.51 0.68 0.66\nTable 3: Comparing article-specific orientation with source-\nspecific orientation, using the same test data but annotated\nin two different ways.\nBenchmarking Task Difficulty. Extrapolate article-\nspecific orientation from source annotations is less\nstraightforward than predicting source-specific orientation.\nIn this section, we benchmark the two tasks. For both tasks,\nthe training data is propagated from source annotations.\nHowever, for the article-specific task, the test data is\nannotated via crowd-sourcing, while the test data for the\nsource-specific task continues to be annotated via source\nannotations. Tabel 3 shows how predicting source-specific\norientations of articles yields good performance because\nthe model learns to map the predictions to sources rather\nthan predicting the article orientation. On the other hand,\nthe model struggles with learning article-specific orienta-\ntion because the annotations in the test data are partially\nindependent of the source annotations in the training data.\n6 Conclusion\nWe propose POLOR, a fined-tuned BERT model aug-\nmented with multiple contrastive learning objectives to de-\nduce article-specific labels even with training data annotated\nbased on sources. Our model shows improved performance\nin predicting political orientation from unseen news sources\nby leveraging the similarities and differences between sen-\ntences. More specifically, the model tunes sentence embed-\ndings by contrasting them to similar/different sentences. We\nintroduce two datasets that span several U.S. news sources to\nevaluate the model performance. Moreover, our annotation\nresults exhibit the relationship between sources and opin-\nion articles by showing that opinion news pieces are usually\ndriven by the author\u2019s beliefs, not the source. In future work,\nwe attempt to explore the problem by incorporating addi-\ntional knowledge other than text such as the author\u2019s back-\nground, personal experiences, and social interactions.\nReferences\n[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.;\nand Bengio, Y . 2014. Neural Machine Translation by\nJointly Learning to Align and Translate. arXiv e-prints\narXiv:1409.0473.\n[Baly et al. 2020] Baly, R.; Da San Martino, G.; Glass, J.;\nand Nakov, P. 2020. We Can Detect Your Bias: Predict-\ning the Political Ideology of News Articles. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , 4982\u20134991. Online: Asso-\nciation for Computational Linguistics.\n[Budak, Goel, and Rao 2016] Budak, C.; Goel, S.; and Rao,\nJ. M. 2016. Fair and Balanced? Quantifying Media Bias\nthrough Crowdsourced Content Analysis. Public Opinion\nQuarterly 80(S1):250\u2013271.\n[Card et al. 2015] Card, D.; Boydstun, A. E.; Gross, J. H.;\nResnik, P.; and Smith, N. A. 2015. The media frames corpus:\nAnnotations of frames across issues. In Annual Meeting of\nthe Association for Computational Linguistics .\n[Chen et al. 2018] Chen, W.-F.; Wachsmuth, H.; Al-Khatib,\nK.; and Stein, B. 2018. Learning to Flip the Bias of News\nHeadlines. In Proceedings of the 11th International Con-\nference on Natural Language Generation , 79\u201388. Tilburg\nUniversity, The Netherlands: Association for Computational\nLinguistics.\n[Chen et al. 2020] Chen, W.-F.; Al Khatib, K.; Wachsmuth,\nH.; and Stein, B. 2020. Analyzing Political Bias and Un-\nfairness in News Articles at Different Levels of Granular-\nity. In Proceedings of the Fourth Workshop on Natural Lan-\nguage Processing and Computational Social Science , 149\u2013\n154. Online: Association for Computational Linguistics.\n[de Jong 2023] de Jong, B. 2023. Why journalists are speak-\ning out against western media bias in reporting on israel-\npalestine. Illustration by Walker Gawande, Edited by Tina\nLee.\n[Fan et al. 2019] Fan, L.; White, M.; Sharma, E.; Su, R.;\nChoubey, P. K.; Huang, R.; and Wang, L. 2019. In plain\nsight: Media bias through the lens of factual reporting. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , 6343\u20136349. Hong Kong, China: Asso-\nciation for Computational Linguistics.\n[Gangula, Duggenpudi, and Mamidi 2019] Gangula, R.\nR. R.; Duggenpudi, S. R.; and Mamidi, R. 2019. Detecting\npolitical bias in news articles using headline attention. In\nProceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP ,\n77\u201384. Florence, Italy: Association for Computational\nLinguistics.\n[Gentzkow and Shapiro 2006] Gentzkow, M., and Shapiro, J.\n2006. What Drives Media Slant? Evidence from U.S. Daily\nNewspapers. Econometrica 78:35\u201371.\n[Greene and Resnik 2009] Greene, S., and Resnik, P. 2009.\nMore than Words: Syntactic Packaging and Implicit Senti-\nment. In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North American Chap-\nter of the Association for Computational Linguistics , 503\u2013\n511. Boulder, Colorado: Association for Computational Lin-\nguistics.\n[Groseclose and Milyo 2005] Groseclose, T., and Milyo, J.\n2005. A Measure of Media Bias. The Quarterly Journal\nof Economics 120(4):1191\u20131237.\n[Jiang et al. 2019] Jiang, Y .; Petrak, J.; Song, X.; Bontcheva,\nK.; and Maynard, D. 2019. Team bertha von suttner at\nSemEval-2019 task 4: Hyperpartisan news detection using\nELMo sentence representation convolutional network. In\nProceedings of the 13th International Workshop on Seman-\ntic Evaluation , 840\u2013844. Minneapolis, Minnesota, USA:\nAssociation for Computational Linguistics.\n[Kiesel et al. 2019] Kiesel, J.; Mestre, M.; Shukla, R.; Vin-\ncent, E.; Adineh, P.; Corney, D.; Stein, B.; and Potthast,\nM. 2019. SemEval-2019 task 4: Hyperpartisan news de-\ntection. In Proceedings of the 13th International Workshop\non Semantic Evaluation , 829\u2013839. Minneapolis, Minnesota,\nUSA: Association for Computational Linguistics.\n[Kim and Johnson 2022] Kim, M. Y ., and Johnson, K. M.\n2022. CLoSE: Contrastive Learning of Subframe Embed-\ndings for Political Bias Classification of News Media. In\nProceedings of the 29th International Conference on Com-\nputational Linguistics , 2780\u20132793. Gyeongju, Republic of\nKorea: International Committee on Computational Linguis-\ntics.\n[Kulkarni et al. 2018] Kulkarni, V .; Ye, J.; Skiena, S.; and\nWang, W. Y . 2018. Multi-view Models for Political Ide-\nology Detection of News Articles. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Lan-\nguage Processing , 3518\u20133527. Brussels, Belgium: Associa-\ntion for Computational Linguistics.\n[Lazaridou et al. 2020] Lazaridou, K.; L \u00a8oser, A.; Mestre, M.;\nand Naumann, F. 2020. Discovering biased news arti-\ncles leveraging multiple human annotations. In Proceedings\nof the Twelfth Language Resources and Evaluation Confer-\nence, 1268\u20131277. Marseille, France: European Language\nResources Association.\n[Lee et al. 2022] Lee, N.; Bang, Y .; Yu, T.; Madotto, A.; and\nFung, P. 2022. Neus: Neutral multi-news summarization for\nmitigating framing bias.\n[Liu et al. 2022] Liu, Y .; Zhang, X. F.; Wegsman, D.;\nBeauchamp, N.; and Wang, L. 2022. POLITICS: Pretrain-\ning with same-story article comparison for ideology predic-\ntion and stance detection. In Findings of the Association\nfor Computational Linguistics: NAACL 2022 , 1354\u20131374.\nSeattle, United States: Association for Computational Lin-\nguistics.\n[Lundberg and Lee 2017] Lundberg, S. M., and Lee, S.-I.\n2017. A unified approach to interpreting model predictions.\nIn Guyon, I.; Luxburg, U. V .; Bengio, S.; Wallach, H.; Fer-\ngus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in\nNeural Information Processing Systems , volume 30. Curran\nAssociates, Inc.\n[Reimers and Gurevych 2019] Reimers, N., and Gurevych, I.\n2019. Sentence-bert: Sentence embeddings using siamese\nbert-networks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing . Asso-\nciation for Computational Linguistics.\n[Roy and Goldwasser 2020] Roy, S., and Goldwasser, D.\n2020. Weakly supervised learning of nuanced frames for\nanalyzing polarization in news media. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , 7698\u20137716. Online: Associa-\ntion for Computational Linguistics.\n[Schroff, Kalenichenko, and Philbin 2015] Schroff, F.;\nKalenichenko, D.; and Philbin, J. 2015. Facenet: A\nunified embedding for face recognition and clustering. In\n2015 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 815\u2013823.\n[van der Maaten and Hinton 2008] van der Maaten, L., and\nHinton, G. 2008. Viualizing data using t-sne. Journal of\nMachine Learning Research 9:2579\u20132605.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "POLOR: Leveraging Contrastive Learning to Detect Political Orientation of Opinion in News Media", "author": ["A Jararweh", "A Mueen"], "pub_year": "2024", "venue": "The International FLAIRS Conference \u2026", "abstract": "News articles are naturally influenced by the values, beliefs, and biases of the reporters  preparing the stories and the policies of the publishing outlets. Numerous studies and datasets"}, "filled": false, "gsrank": 167, "pub_url": "https://journals.flvc.org/FLAIRS/article/view/135520", "author_id": ["S_7TUW4AAAAJ", "OImDWloAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:tJsv6pbABEsJ:scholar.google.com/&output=cite&scirp=166&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D160%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=tJsv6pbABEsJ&ei=IbWsaPvmC7_SieoPzJnloAQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:tJsv6pbABEsJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://journals.flvc.org/FLAIRS/article/download/135520/139964"}}, {"title": "The Digitally Accountable Public Representation Database: Measuring Online Communication by Federal, State, and Local Officials", "year": "2024", "pdf_data": "The Digitally Accountable Public Representation Database:\nMeasuring Online Communication by Federal, State, and Local\nOfficials\nYuehong Cassandra Tai* Nitheesha Nakka\u2020Sarah Michele Rajtmajer\u2021\nKevin Munger\u00a7Yu-Ru Lin\u00b6Bruce A. Desmarais||\nAbstract\nWe introduce the Digitally Accountable Public Representation (DAPR) Database, an innova-\ntive archive that systematically tracks and analyzes the online communications of federal, state,\nand local officials in the U.S. Focusing on X/Twitter and Facebook, the database includes Tweets\nand Facebook posts dating back to 2020, offering a rich historical perspective on digital political\ndiscourse by elected officials in the states. Along with the raw data, we develop and include\nkey measures that quantify relevant features of online communications. These include special-\nized measures of misinformation dissemination, the use of toxic language by officials, and the\nexpression of anti-vaccination attitudes. In addition to presenting and describing the contents of\nthe DAPR database, we introduce a dedicated R package, enabling users to download and ana-\nlyze data in bulk, tailored to their specific research needs. We also provide an interactive digital\ndashboard, designed for a broader audience to explore and interpret the data in a user-friendly\nonline environment. Lastly, we describe our model for expanding and sustaining the DAPR\ndatabase going forward, including the addition of new officials and platforms, and the collection\nof social media data in the post-API era.\nKEYWORDS: elected officials, state legislators, online political communication, misinformation,\ntoxic language\n*Corresponding author. yhcasstai@psu.edu .Postdoctoral Scholar, Center for Social Data Analytics, Pennsylvania\nState University, United States.\n\u2020PhD Candidate, Department of Political Science, Pennsylvania State University, United States.\n\u2021Assistant Professor, College of Information Sciences and Technology, Pennsylvania State University, United States.\n\u00a7Assistant Professor, Department of Political Science, Pennsylvania State University, United States.\n\u00b6Associate Professor, School of Computing and Information, University of Pittsburgh, United States.\n||Professor, Department of Political Science, Pennsylvania State University, United States.\n1\nIntroduction\nWhat elected officials say online is important, and, for better or worse, has far-reaching impacts.\nIndividuals who directly follow elected officials online form a small, relatively ideologically extreme,\nand highly politically active portion of the population (Fisher et al. 2019). Nevertheless, the online\ncommunications of public officials influence much larger segments of society through several indirect\npathways. For instance, officials\u2019 online messaging shapes responses to public emergencies (Hagen\net al. 2020; Rao et al. 2020), acts as an information source for the news media (Broersma and Graham\n2012; T. Kim et al. 2021), and facilitates interaction and deliberation with constituents and other\ncitizens (Barber\u00e1 et al. 2019; J\u00f6rgens, Kolleck, and Saerbeck 2016; Payson et al. 2022; Russell 2021).\nThe importance of communication on third-party platforms is even greater for subnational officials,\nwho seldom attract national media attention and face limitations in statewide media coverage (Darr,\nHitt, and Dunaway 2018). In understanding the effects of officials\u2019 online rhetoric, consider, e.g.,\ntheir online discussion of vaccination, which became a prominent and highly politicized topic during\nthe COVID-19 pandemic. Public dialogue and attitudes toward vaccination are shaped by prominent\nfigures\u2019 online dialogue, including elected officials (Hornsey et al. 2020; Larsen et al. 2023).\nWe build the Digitally Accountable Public Representation (DAPR) database as a comprehensive\nresource for data and measurements related to U.S. elected officials\u2019 online communications, as well\nas their gender, race, and demographic information. Our goal is to facilitate research and analysis by\nscholars and other stakeholders. We are building the DAPR database towards coverage of all federal\nand state level elected officials in the U.S., as well as elected officials at the 100 largest U.S. munic-\nipalities. Following a recent line of work focused on state lawmakers\u2019 online communications (e.g.,\nCook 2016, 2017; T. Kim et al. 2021; Guntuku et al. 2021; Payson et al. 2022; Peterson et al. 2022;\nButler, Kousser, and Oklobdzija 2023), we piloted the DAPR database project using the online com-\nmunications of state lawmakers. We have collected comprehensive data going back through 2020,\nand covering both Facebook and X/Twitter. We are releasing both raw data on officials\u2019 use of these\nplatforms, as well as measures that have been developed to assess officials\u2019 communications in terms\nof, e.g., the dissemination of misinformation, the use of toxic language, and the ideological content\nin officials\u2019 posts.\nIn what follows we present background on the DAPR database, and review our pilot research.\nWe give a summary of the data that is currently available in the database. We then describe the two\n2\nportals through which researchers and practitioners can access and use the datbase\u2014an R package\nthat can be used for bulk data access, and an online dashboard that can be used to perform interactive\nanalyses of smaller snapshots of the data.\nExisting Projects and Demand For Data\nWe piloted the DAPR database with a focus on state legislators due to their substantial size as a\npopulation of elected officials, their important roles in U.S. politics and policymaking, and, despite\nrecent trends, the relatively limited scholarly attention to their digital communications, in comparison\nto research on members of the U.S. Congress.\nFigure 1 illustrates the evolution of research related to state legislators over the last 32 years,\nspanning from 1990 to 2022. Using the Web of Science, we searched published articles on U.S. state\nlegislators with keywords of legislative study, state legislatures, and state politics (see the online ap-\npendix for details). There are 1,968 pertinent articles, with just 1 article published in 1990, and a\nmaximum of 138 in 2022. The number of articles and those with associated data has risen, especially\nafter 2016. We then manually reviewed these articles and excluded articles that are unrelated to state\nlegislators, state legislatures, and state politics. Out of the 1,968 articles examined, a total of 37 arti-\ncles were found to have associated data.1Out of these 37 articles, merely four created datasets that\nspan across states and over time, which are appropriate for the analysis of state legislators and legis-\nlatures. These articles are Berry et al. (1998), Shor and McCarty (2011), Tausanovitch and Warshaw\n(2013), and Bowen and Greene (2014).2In the field of research that centers on state legislative bod-\nies and legislators, the lack of a comprehensive dataset on state lawmakers has significantly impeded\nscholarly progress in this crucial domain.\nIn the following sections, we present a concise overview of the data collection procedure, the\ndatabase architecture and access, as well as its applications.\n1. The initial publication may not display its linked data, however, as per the citation index, a referenced article with\ndata is (Berry et al. 1998) with 12 in 2022. We included it in the dataset for articles with linked data.\n2. A notable dataset concerning state legislatures and legislators is Bucchianeri, V olden, and Wiseman (2020)\u2019s legisla-\ntive effectiveness, which was not included in our article data as it is published in 2024. In addition, there are other valuable\nstate politics datasets, such as Boehmke et al. (2023)\u2019s State Policy Responses to COVID-19 and Boehmke et al. (2020)\u2019s\npolicy diffusion network data, which were not considered in our examination of available datasets for state legislatures and\nlegislators.\n3\nFigure 1: State Legislators Articles and Relevant Articles with Associated Data\nData Collection\nOfficial Meta Data\nFigure 2 illustrates our meta data collection and processing workflow, broken down into four\nsteps. In Step 1 we use Ballotpedia.org as a starting point for three reasons. First, Ballotpedia.org\nis a digital encyclopedia of American politics, providing unbiased information on elections, politics,\nand policy. Secondly, according to Ballotpedia, content is neutral, accurate and verifiable by its staff\nof writers, researchers, and election analysts. Thirdly, Ballotpedia provides a consistent framework\nfor presenting information about elected officials, maintaining this structure across different states\nand throughout various periods. For example, legislators are listed on senate - and house-specific\npages. Therefore, Ballotpedia has been commonly used as a datasource in political science (e.g.,\nMotta 2021; Treul and Hansen 2023; Wintersieck and Keena 2023).\n4\nFigure 2: Meta data collection\nIn step 2, from state chamber websites, we identified each legislator\u2019s link. In Step 3, we col-\nlected available data on Ballotpedia, including legislators\u2019 name, chamber, partisanship, and their\nsocial media accounts. For legislators\u2019 accounts on X/Twitter, there is no identifier of whether it is\nprivate, official, or campaign, thus we collected all available account information associated with\nindividual legislators. Legislators\u2019 Facebook accounts provide account types, including private, of-\nficial, and campaign. To maximize the amount of available information, in Step 4, we carried out\nan extensive manual data entry. We consulted multiple sources for legislators\u2019 demographic infor-\nmation, election information, and social media accounts from Google, Cook, legislators\u2019 official or\ncampaign websites, and state chambers\u2019 official websites.\nAfter Step 4, we then conduct a thorough cleaning process using both automated and manual\nmethods to remove noise from the scraped information and harmonize data when they are from\ndifferent sources. Ballotepedia does not provide legislators\u2019 gender and race information. Therefore,\nwe employed both an automated and hand-coding approach for gender and race information.\nRegarding racial coding, we identified legislator\u2019s race based on seven racial categories out-\nlined in the Candidate Characteristic Cooperative coding scheme (Fraga, Juenke, and Shah 2021).\nEach legislator underwent a comprehensive process, in which coders cross-referenced legislator\u2019s\nself identification on personal and professional websites, social media bio\u2019s, Ballotpedia and any\nracial/ethnic group memberships (e.g. the NAACP), photos, and name origins (Nakka 2024).\nIdentifying legislator\u2019s gender followed a similar process and included checking public officials\n5\npronoun usage and gender identification. For gender variable, we used Gender R package (Blevins\nand Mullen 2015), which is an algorithmic approach to inferring gender based on a large panel dataset\nof historical data. This algorithm has been verified against hand-coding using a dataset containing\nlegislators from ten randomly selected states (Nakka 2024). According to Nakka (2024), the Gender\nR package performs well with high precision and recall scores ranging from 0.93 to 1. Any missing\nvalues that the Gender R package was unable to identify were checked independently and filled\nmanually by hand-coders using the cross-referencing procedure outlined above.\nWe manually entered legislators\u2019 district number, year of being elected and of leaving office, and\nvote share in the election. In the current version of our meta data, we focused on legislators who were\nin office during the year of 2020 and 2022. Certain individuals may alter their party affiliation or get\nelected as representatives to a different chamber from their initial positions in one chamber.3In these\ncases, we manually verified their political party membership, the year of their election, and the start\ndate of their term. Subsequently, we determined their party affiliation and legislative chamber based\non the party they were associated with for a longer duration and the office they held for a greater\nperiod in 2020 and 2022.\nWe also added legislators\u2019 ideological positions constructed by Shor and McCarty (2011) and\npublic ideological leaning at the legislative district level measured by Tausanovitch and Warshaw\n(2013) and updated in 2022. Regarding public ideology, we incorporated both the MRP (Multilevel\nregression with poststratification) measure, which accounts for race, education, and gender, and the\nmeasure of Democratic party presidential vote shares at the district level from 2020 (Tausanovitch\nand Warshaw 2013). Based on individual ideology scores, we also calculated chamber polarization\nas the ideological distance between party medians polarization.\nWe generated a distinct identification number for each legislator. This unique ID connects our\nmeta data with content data from legislators\u2019 X/Twitter and Facebook accounts. When combining\nour metadata with data from external sources, we encountered challenges due to discrepancies in the\nnames of legislators and districts across different sources and over time. These inconsistencies, such\nas variations in the inclusion of middle names or prefixes, posed a significant obstacle to optimizing\nour dataset. As a solution, we implemented a comprehensive verification process that combined au-\ntomated and manual coding methods. First, we programmatically merged datasets based on legislator\n3. The party affiliation, district number, and chamber are subject to change over time and we will update our data\nregularly with timestamps to track different time periods.\n6\nTable 1: Overview of Legislators\u2019 Meta Information Available in the DAPR Database\nUnique ID - unique ID\nDemographics- name\n- gender\n- race\n- partisanship\n- ideology score\nOfficial position- state\n- chamber\n- district\n- year of being elected to the position\n- year of left\n- vote share\n- year of last election\nSocial Media Account- X accounts and id\n- Facebook accounts and id\n- Source\nState, chamber, and district level information- district level ideology\n- party polarization in chamber\n- party ideological heterogeneity in chamber\nnames. This involved a fuzzy merging process where we manually adjusted names in other datasets\nto match those in our primary dataset, ensuring they represent the same individual. This automated\nmerging process was repeated in multiple rounds for better accuracy. Next, we manually reviewed\nand verified the identity of any entries that could not be automatically merged using either full or\nfuzzy matching techniques in previous rounds. Table 1 provides an overview of the information\navailable for each verified entry4.\nContent Data from Social Media\nBased on legislators\u2019 X accounts, we collected legislators\u2019 tweets by using Twitter\u2019s Official API\nv2.0 before rate limitations were imposed around June 2023. To maximize available data and reduce\nthe effects of deleted tweets afterwards, we collected the data, reaching back 7 days every day. When\nthe data was stored, we cleaned the data by removing duplicates. In addition to basic information of\ntweets, such as post id, post time, and content, our raw data includes interactions such as the number\nof likes and retweets received by each tweet. In addition, we also gathered the follower networks\napproximately four times between 2020 and 2023.\nFor Facebook data, we collected data using CrowdTangle\u2019s official API by following their terms\n4. Details in variable names and values are presented in codebook in Supplementary material.\n7\nof service. We collected publicly posted and available data in three rounds: April 2022, March, and\nApril 2023. However, many campaign or official accounts were no longer accessible during these\nthree rounds of data collection.\nAt the post-level, we measured the level of posts\u2019 reliability, following the approach that has been\nwidely used to detect misinformation in social media data (Lasser et al. 2022; Grinberg et al. 2019;\nTai, Buma, and Desmarais 2023). This method utilizes the credibility of domains to determine the\ntrustworthiness of the content they contain. Given that politicians often share strong politically bi-\nased content, we adopted Tai, Buma, and Desmarais (2023)\u2019s two-class category. This category is\ndeveloped from Media Bias/Fact Check\u2019s (MBFC, mediabiasfactcheck.com) original URL rating but\ncenters on the most misleading information. For example, Fox News, classified as questionable and\nlow-credible in the original MBFC ratings, is not rated as low-fact reference (See more details in Tai,\nBuma, and Desmarais (2023) and Tai, Lin, and Desmarais (2023)).\nIn addition, we measured the toxicity score for each post using Nakka (2024)\u2019s validated Google\nPerspective API. The Perspective API is a large language model that uses convolutional neural net-\nwork to score text on a scale from 0 (non-toxic) to 1 (very toxic). Although the Perspective API\nhas been widely used in academic research on social media text (Rajadesingan, Resnick, and Budak\n2020; Hopp et al. 2020; Ventura et al. 2021; J. W. Kim et al. 2021; Masullo Chen et al. 2019), it\ncould not capture \u201ctoxicity\u201d in legislators\u2019 posts. Public officials, as state legislators, should be more\ncautious about the language used in their posts. Nakka (2024) evaluated the precision by comparing\nthe Perspective scores with human-coded toxicity ratings of posts and demonstrated a high level of\naccuracy.\nFollowing and followed data on Twitter\nApart from the content information extracted from the social media accounts of lawmakers, data\non the relationships, followers, and accounts that legislators follow was also collected on X. The\ndatasets for both followers and following include details such as account ID, location, username, and\naccount creation timestamp of the legislators\u2019 followers and followed accounts. The followers dataset\nconsists of a substantial 17,393,354 records, whereas the following dataset includes 3,929,996 en-\ntries. This network of state lawmakers and their related discussions will provide valuable insights into\nlegislative actions that often focus on the co-sponsorship of bills and the consequences of campaign\n8\nendorsement behavior.\nArchitecture and Access\nStructure of the Database\nThe layout of the DAPR database is shown in Figure 3. The primary dataset includes a unique\nidentifier for each legislator along with their demographic information. Other variables discussed in\nthe previous section can be accessed using the unique identifier in separate datasets, such as the IDs\ndataset containing legislators\u2019 social media accounts, which allows for content-level metrics to be\nobtained in the Facebook and X datasets. The political position dataset includes legislators\u2019 political\ndetails, enabling retrieval of their associated institution-level information in the chamber and district\ndatasets. Additionally, the following and follower datasets store legislators\u2019 online network data.\nFigure 3: Structure of the DAPR database\n9\nTable 2: Demographic Description of Meta Data\nWhite Black Latino Asian\nAmericanNative\nAmericanMulti-\nracialMena Un-\nknownTotal\nFemale 1,779 383 131 49 19 20 10 1 2,392\nMale 4,660 562 220 114 19 17 27 5 5,624\nTotal 6,439 945 351 163 38 37 37 6 8,016\nTable 3: Description of Gender, Party, and Chamber in Meta Data\nDemocrats Republicans Independents House Senate\nFemale 1,349 1,035 8 1,831 561\nMale 2,317 3,268 39 4,086 1,538\nTotal 3,666 4,303 47 5,917 2,099\nData Overview\nMeta Data/Facebook\nThrough a comprehensive process of compilation and meticulous refinement, our DAPR database\ncontains 8,016 distinct legislators, with 7,999 having held office at some point since 2020. Table 2\ndisplays the breakdown of our metadata according to gender and race. Women constitute 30% of all\nstate legislators, a proportion that exceeds the 28% representation of women in the 118th Congress.\nOverall, 1,577 lawmakers identify as Black, Hispanic, Asian American, Native American, Mena or\nmultiracial, constituting 20% of total state legislators. Nevertheless, significant disparities exist in\ndemographic composition across different states5. The proportion of women and minority lawmak-\ners can offer valuable perspectives on their online behaviors, supplementing the data on legislators\u2019\npartisanship and ideology. In addition, as shown in Table 3 , there is a partisan gender division6.\nWomen make up a larger share of Democratic legislators (37%) than Republicans (24%). In both\nchambers, approximately 30% of representatives and 27% of senators are women.\nOf these 7,999 officials, 95% have X Facebook accounts (n = 7,597). More specifically, 5,906\nlegislators have at least one X account, and 6,548 have at least one Facebook account. A total of\n4,831 legislators have official, campaign, or both types of Meta accounts. It is important to note that,\n5. In Appendix Figure A1, we further present the proportions of female and minority legislators by state, separately.\nFigure A2 summarizes the distribution of race and gender variables for the legislators in our data, with an overwhelming\nmajority of legislators identified as White and Male.\n6. Figure A3 and A4 display gender and race distribution by party\n10\nunlike Facebook accounts, it is not possible to distinguish between official and personal accounts\nbased on legislators\u2019 profiles in X or their Ballotpedia page. Furthermore, many accounts on both\nplatforms were inaccessible, closed, or dormant due to platform policies or legislators leaving office\nduring the data collection period.\nIn general, we have comparable available accounts between Democrats and Republicans who\nhave held office since 2020, on X, Facebook, or both platforms (see Figure A5 in Appendix). X has\na higher representation of Democrats, while Facebook has a higher representation of Republicans.\nThe total number of active accounts does not necessarily indicate the level of activity or the vari-\nation in activity levels across platforms. Analyzing content information can offer a more detailed\nunderstanding of user activity. We present content data in the following section.\nContent Data\nTable 4 displays the current content in our database. Our database contains 5,730,030 tweets\nfrom 4,798 state legislators covering almost three and a half years from January 2020 to June 2023.\nOf these, 4,076,007 tweets were posted by 2,611 Democratic legislators, 1,627,819 by 2,163 Repub-\nlican legislators, and 26,204 by 24 Independents. Despite the comparable numbers of Democratic\nand Republican legislators on Twitter, Democratic legislators posted three times as many tweets as\ntheir Republican counterparts. In contrast to their activity on Twitter, legislators exhibit different\nonline behaviors on Facebook. Between January 2020 and December 2021, state legislators collec-\ntively shared 450,972 posts on Facebook. The number of state legislators active on Facebook, totaling\n4,429, is similar to the 4,798 on Twitter. Furthermore, the number of posts shared by Democratic leg-\nislators closely matches those shared by Republican legislators, with Democrats contributing 239,726\nposts from 2,157 members and Republicans 210,018 posts from 2,243 members. Remarkably, ap-\nproximately 60% of legislators engage on both Twitter and Facebook, while about 42% of Republican\nlegislators are active solely on Facebook, compared to only 20% of Democratic legislators.\nDifferences in communication strategies across platforms are also evident at the content level.\nFigure 4 shows that the median tweet count for individual Democratic legislators is 475 on Twit-\nter, versus 57 on Facebook. For Republicans, the numbers are 155 on Twitter compared to 47 on\nFacebook. These differences underscore the necessity of studying cross-platform behaviors to fully\nunderstand the digital engagement and communication strategies of legislators.\n11\nTable 4: Description of Content Data\nTweets Legislators\non\nTwittersPosts Legislators\non\nFacebookLegislator\non\nboth plat-\nformsOnly\non\nTwitterOnly on\nFacebook\nDemocrat 4,076,007 2,611 239,726 2,157 1,717 894 440\nIndependent 26,204 24 1,228 29 17 7 12\nRepublican 1,627,819 2,163 210,018 2,243 1,315 848 928\nTotal 5,730,030 4,798 450,972 4,429 3,049 1,749 1,380\nFigure 4: Social Media Posts Distribution by Platforms by Party\nRelying on our longitudinal data, the DAPR database sheds light on the changes of state legisla-\ntors\u2019 online behaviors over time. In Figure 5, we show the trend of monthly state average posts on\nTwitter and Facebook over time. It is expected that state legislators show greater activity on Twitter\ncompared to Facebook. However, the level of engagement is similar in a few states such as Alaska,\nHawaii, Idaho, Illinois, and Pennsylvania. On the other hand, there is a noticeable disparity in ac-\ntivity levels in some states, with legislators in Arizona, New Hampshire, and Utah demonstrating\nsignificantly higher engagement on Twitter than on Facebook. Overall, the patterns of Twitter and\nFacebook posts across states are generally consistent and stable, albeit with some minor variations.\nExceptions include Virginia and Washington, where Facebook trends showed more variability or\ndistinct trends of increase or decrease.\n12\nFigure 5: Social Media Posts Distribution by Platforms across States\nThe patterns of average posts per party across states are also different on X and Facebook, re-\nspectively (See more details in Figure A6 and A7 in Appendix). On X, Democrats generally post\nmore frequently than Republicans across most states. However, in Alabama, Oregon, and Rhode\nIsland, Republicans exhibit greater activity than Democrats. In states like Arizona, Connecticut,\nMassachusetts, New Jersey, Pennsylvania, Vermont, West Virginia, and Wyoming, both parties show\ncomparable levels of activity and similar posting trends. On Facebook, most states display compa-\n13\nrable activity levels and posting trends. Nevertheless, distinct party posting behaviors are observed\nin Arizona and Wyoming. In many states we observe a downward trend over the time period we\ncover. We see this as arising due to the timeline starting in early 2020, during the first months of\nthe COVID-19 pandemic\u2014a time period in which elected officials were highly active online (T. Kim\net al. 2021). However, our current dataset only extends to mid-2023. Without data from the latter half\nof 2023, it\u2019s difficult to definitively determine if the post-2022 decrease is a sustained trend. Further\ndata collection could provide a clearer picture. Several potential explanations exist for this decline.\nOne possibility is a migration of state politicians to alternative social media platforms. Recent criti-\ncisms and debates surrounding established platforms could be driving this shift. For example, some\nRepublican legislators might have moved to platforms like Truth Social, while Democrats might be\nlosing confidence in Facebook due to its weak content moderation and in X due to recent policy\nchanges in 2022. It\u2019s also possible that legislators are diversifying their online presence. Platforms\nlike YouTube, TikTok, or Instagram could be emerging as more relevant avenues for engagement.\nFollower Data\nBesides the content shared on platforms, the networks of legislators offer rich and insightful in-\nformation for analyzing their online activities. The dataset on followers includes data for 17,393,354\nfollowers from 6,006 legislators. Texas has the highest median follower count among both Democrats\nand Republicans, registering 8,250 and 6,486 respectively. Conversely, Maine records the lowest me-\ndian follower count for both parties, with Democrats at 359 and Republicans at 78.\nFigure 6: Median Number of Followers of Legislators by Party across States\n14\nData Accessibility\nTo facilitate broader access and analysis of this rich dataset, we developed an interactive digital\ndashboard that facilitates visual data exploration, catering to both academic and public audiences. As\na web interactive app, the DAPR dashboard provides accountable visualization and analytic tools to\nfacilitate tracking and visualizing key metrics of legislators. The dashboard will be available to the\ngeneral public, including everyday citizens, as well as researchers and professionals such as NGOs\nand journalists. The dashboard provides data search and diverse analytic modules for both X and\nFacebook. Figure 7 shows the single main page for X data.\nTo cater to various diverse needs and analytic perspectives, the dashboard provides three main\nmodules for each single main-page of each platform, designed to describe the temporal, geospatial,\nand politically affiliation aspects of legislators\u2019 online activities. Users can retrieve legislators\u2019 posts\nwithin a particular time period and filter data tailored to their analytic purposes. Module A shows the\ncontent data with per-post scores, including interaction scores measured by likes and retweets and\noverperforming score capturing a post interact compared to the averaged interaction of a legislator\n(Biswas et al., 2024). Users can further search for relevant posts using the search functionality\nembedded in Module A. Module B displays temporal trends and aggregated scores of interaction and\ncivility by party. The analytics in Module B is downloadable. Module C assists users to evaluate and\ncompare interaction scores, temporal trending of interaction scores, civility, and post-credibility at\nthe legislator level. The subpage for the X data includes the network analysis7, shown in the bottom\nright panel of Figure 7. Users can take advantage of network analysis to further explore legislators\u2019\ninterconnectedness between legislators within and between states.\n7. The order of states is based on the number of legislators in the states. The color is based on the average interaction\nscores of tweets published by legislators in each state. The node size is scaled to its in-degree, indicating each legislator\u2019s\ninfluence towards others. There is also a set of pairwise similarity computed based on how each legislator\u2019s bio belongs to\n15 topics discovered by the topic analysis using LDA. This analysis leads to the entire network structure forming clusters\nof legislators.\n15\nFigure 7: DAPR Database Dashboard\nApplication\nA number of scholars have already used DAPR data to study a wide range of questions in legisla-\ntive behaviors, misinformation, and/or online discourse of state legislators. By focusing on the tweets\nof state legislators from March 30 and October 25, 2020, T. Kim et al. (2022) find that the attention of\nlegislators to the pandemic is associated to their state\u2019s case numbers, national death counts, and local\npandemic policies. However, a political divide exists in responsiveness and discourse content: Re-\npublicans show less reaction to health indicators, emphasizing economic concerns, while Democrats\nand Independents prioritize public health aspects. Their study provides a fresh viewpoint for com-\nprehending the partisan differences in reactions to public health emergencies in the United States.\nUsing the X data with a longer time period coverage from 2020 to 2021, Tai, Buma, and Desmarais\n(2023) uncover that Republicans shared 20 times more misinformation than did Democrats. An in-\ndepth analysis of topics reveals that Republicans disproportionately shared more partisanship-driven\nmisinformation and engaged in partisan warfare. Built on this study, by expanding to Facebook data,\nTai, Lin, and Desmarais 2023\u2019s working paper investigates the factors influencing legislators\u2019 shar-\ning of misinformation. The results indicate that conservative legislators from constituencies with\nideologically aligned citizen bodies are more likely to share misinformation.\nInstead of focusing on a certain platform, scholars have used DAPR data to do cross-platform\n16\nanalysis on legislators\u2019 online visibility. Biswas and colleagues (2024) conducted a study where\nthey evaluated the online presence of state legislators on Facebook and X platform. They found that\nsharing low-credibility content was positively associated with visibility on Facebook, while it was\nnegatively associated with visibility on Twitter, with differences observed across political parties. In\nparticular, Republicans were more visible when sharing low-credibility content on both platforms,\nwhereas Democrats faced repercussions for such content on Twitter. Moreover, using uncivil lan-\nguage reduced the visibility of legislators\u2019 posts on Twitter for members of both political parties.\nInvestigating topic salience on X is also valuable. In Figure 8, we illustrate the top 50 prominent\nhashtags by party between 2020 and 2023. Both Democrats and Republicans primarily use hash-\ntags that focus on state politics, such as #coleg, #txlegs, #gapol and #nhpolitics among others. It\nis also clear that there are distinct differences in hashtag usage between the parties. For example,\nDemocrats focused more on #COVID19 in terms of both prominence and frequency, whereas for\nRepublicans, #COVID19 did not emerge as the most prominent issue. Additionally, while #Black-\nlivesmatter ranked among the leading hashtags for Democrats, it did not appear in the top 50 for\nRepublicans. This suggests a need for more detailed investigation into the partisan differences re-\ngarding these issues.\nFigure 8: Top 50 Hashtags by Party\nOur dataset offers insights into the linguistic patterns of legislators, for example, toxicity and\nideology. Previous studies indicate that provocations by public officials on social media lead to hos-\n17\ntility or incivility targeted at them (Solovev and Pr\u00f6llochs 2022). This effect is especially notable\namong legislators, where their toxic behavior provokes similar toxic responses directed back at them\n(Solovev and Pr\u00f6llochs 2022; Nakka 2024). One question is whether toxic content from legislators\ncould incite more engagement. Given the prestigious status of legislators, Nakka (2024) adopted an\nexpanded definition of toxicity, including racism, sexism, xenophobia, and antisemitism, instead of\nrelying solely on the Perspective API\u2019s toxicity score. This broader definition served as the founda-\ntion for analyzing the toxicity of legislators\u2019 posts, based on a random sample of thirty-six thousand\nX posts and Facebook posts from the DAPR dataset. She finds that a toxicity threshold of 0.7 marks a\nturning point at which text becomes authentically or truly toxic.Authentically or truly toxic posts are\nthose that beyond mere rudeness or unkindness and are deemed to be genuinely harmful or offensive.\nTable 5 shows the average engagement metrics for these authentically or truly toxic posts. Overall\nthe average number of likes and shares for authentically toxic comments is low.\nTable 5: Average Number of Likes and Shares for Authentically Toxic Posts\nPlatform Average Number of Likes Average Number of Shares\nTwitter 76.025 1786.075\nFacebook 184.125 109.0\nHowever, the toxicity scores varies across party and across gender x party. As shown in Figure 9,\nalthough there are very few legislators comments that legislators post that score above a 0.7 toxicity\nscore, Democrats post more toxic comments and with greater toxicity than Republicans on X and\nRepublicans post more toxic comments and with greater toxicity on Facebook. Moreover, men post\nmore toxic comments than women across both parties and both platforms when comparing Figures\n10 and 11. In addition, Men Democrats in this sample do not have any authentically toxic Facebook\nposts only toxic Twitter posts but Men Republicans who post authentically toxic content also tends to\nfavor X as seen in Figure 11. Likewise, Women Democrats who have shared genuinely toxic tweets\npredominantly use X, whereas Women Republicans have posted just one genuinely toxic comment\non Facebook, as shown in Figure 10. In conclusion, this analysis shows us that most legislators have\nlittle or no toxic content across either social media platforms.\n18\n0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86\nT oxicity0.000.250.500.751.001.251.501.752.00FrequencyT oxicity > 0.7: Republicans vs Democrats on Facebook\nRepublicans\nDemocrats\n0.70 0.75 0.80 0.85 0.90\nT oxicity0123456FrequencyT oxicity > 0.7: Republicans vs Democrats on T witter\nRepublicans\nDemocratsFigure 9: Distribution of Authentic Toxicity across Facebook and Twitter By Party\n0.72 0.74 0.76 0.78 0.80 0.82 0.84\nT oxicity0.00.51.01.52.02.53.03.54.0FrequencyDistribution of T oxicity (>0.7) for Women Democrats\nFacebook\nT witter\n0.4 0.6 0.8 1.0 1.2\nT oxicity0.00.20.40.60.81.0FrequencyDistribution of T oxicity (>0.7) for Women Republicans\nFacebook\nT witter\nFigure 10: Distribution of Authentic Toxicity across Facebook and Twitter By Party: Women\n19\n0.75 0.80 0.85 0.90\nT oxicity0.000.250.500.751.001.251.501.752.00FrequencyDistribution of T oxicity (>0.7) for Men Democrats\nFacebook\nT witter\n0.72 0.74 0.76 0.78 0.80 0.82 0.84\nT oxicity0.00.51.01.52.02.53.0FrequencyDistribution of T oxicity (>0.7) for Men Republicans\nFacebook\nT witterFigure 11: Distribution of Authentic Toxicity across Facebook and Twitter By Party: Men\nAnother interesting perspective that our data can provide is the text-level ideology. We use the\nideology measure developed by Butler, Kousser, and Oklobdzija (2023) to categorize posts as lib-\neral, neutral or conservative. Their application uses a Multinomial Na\u00efve Bayes supervised machine\nlearning model which was trained on a hand-coded dataset of over ten thousand tweets posted by\npresidential candidates and their SuperPACs during the 2016 Presidential campaign and state legisla-\ntors. Notably, there is a discrepancy between the language and subject matter invoked amongst 2016\npresidential candidates and state legislators. As displayed above in Figure 8, state legislators tend to\nfocus on state related topics and general day to day minutia. The presidential campaigns tend to have\nmore punchy and nationally oriented topics at the forefront of candidates online communications in\ngeneral but the 2016 presidential campaign was a particularly divisive era in political history.\nDue to these textual differences, for the purposes of this application we removed words or tokens\nfrom the training corpus that are not in the DAPR data X or Facebook posts corpora and fit each\nmodel based on new limited training data corpus. Tables 6 and 7 provide the general proportions\nof liberal, neutral and conservative posts alongside examples of each of these ideological categories.\nAnd Tables 8 and 9 display the counts of tweets and posts for each ideological category posted\nby Republicans, Democrats and Independents. There is an overwhelming number of neutral coded\nFacebook and X posts. Republicans remain nearly exclusively categorized as neutral or conservative,\nwith only one tweet posted by a Republican labeled as liberal. Interestingly, Democrats shared more\nconservative tweets and posts than liberal ones.\n20\nTable 6: Tweets Ideological Distribution and Examples\nLiberal (0.02%) Neutral (99.5%) Conservative (0.46%)\n\"LGBTQ+ rights are Human\nRights.\"\"New title, new job, new di-\nrection. Still a bit strange see-\ning this. okgop government\npaperwork\"\"Attached is the Kentucky\nCoal Association endorse-\nment of SB4 reliability coal\"\n\"RT @SenSanders: Our gov-\nernment can and must work\nfor the working class. \"\"I think he\u2019s still a winner\" \"NUCLEAR MAGA\"\nTable 7: Facebook Posts Ideological Distribution and Examples\nLiberal (0.04%) Neutral (98.9%) Conservative (1.1%)\n\"Abortion is Healthcare\" \"Here for this 2021 energy\nLOL!!!!\"\"Liberalism and anti-\nAmericanism don\u2019t sell.\"\n\"Black Lives Matter. #black-\nouttuesday\"\"Good morning from the people\u2019s\nhouse!\"\"I stand with Amy Coney Bar-\nrett in her beliefs, and I\u2019m not\nafraid to stand up against the\nDemocrats that want to attack\nher Christian values! Click\nbelow if you support ACB like\nI do \"\nTable 8: Count of Facebook Posts Ideological Distribution by Party\nLiberal Neutral Conservative\nRepublicans 0 16171 273\nDemocrats 14 19334 121\nIndependents 0 86 1\nTable 9: Count of Tweets Ideological Distribution by Party\nLiberal Neutral Conservative\nRepublicans 1 11782 78\nDemocrats 6 23880 87\nIndependents 0 165 1\nOn one hand, the difference in content-ideology by party supports asymmetric partisan politics\nwhere polarization and extremism are stronger on the right (Grossmann and Hopkins 2016). One the\nother hand, this could be in part because the training set includes no tweets concerning the COVID-\n19 pandemic, a highly politicized event on legislative X posts (T. Kim et al. 2021), and the more\nrecent 2020 presidential election. For instance, analysis of tweets by Democrats revealing support\n21\nfor mask mandates or Presidential candidate Joe Biden were categorized as neutral. Conversely,\ntweets or posts by Democrats that were marked as conservative typically contained criticisms of the\nGOP, which, due to their absence in the training data, were not identified as liberal.\nOur application demonstrates that despite the power of machine learning, the evolving nature of\nlanguage and the impact of salient political issues necessitate cautious use of machine learning and\nlarge language models in both measurement and downstream analysis. In our case, as such moving\nforward, we will expand the hand-coded training data to include more updated tweets and posts.\nTraining a more sophisticated GPT-4 classifier to identify liberal and conservative posts will result in\nmore precise categorization.\nFinally, observing the discrepancies between platforms, we explore the potential factors influenc-\ning these variations. Our analysis centers on the different presence of Democrats and Republicans\non these platforms. We assume that legislators strategically select their platforms. Research sug-\ngests that female legislators, facing bias in traditional media, are more likely to use social media to\nconnect with constituents (Butler, Kousser, and Oklobdzija 2023). State senators, who often have\nhigher ambitions and broader demographics to represent, could take advantage of multiple platforms\nto enhance their public image, as suggested by the behavior of Congress senators on Twitter (Russell\n2021). Furthermore, legislators from competitive districts have a greater incentive to use multiple\nplatforms to secure constituent support, as electoral competition influences elected officials\u2019 behav-\niors (Griffin 2006). Using a multinomial logit regression with legislators without any accounts as\nthe reference category, we find that female legislators, those from competitive districts, and state\nsenators are more likely to use both X and Facebook, controlling for race, ideology, and unobserved\nheterogeneity at state and state-chamber level. Conservative legislators are less inclined to use both\nor only Twitter, while minority legislators are less likely to use only Twitter. All results are displayed\nin Table 10.\n22\nTable 10: Results from Multinomial Logit Regression for Platform Choice\nState Fixed State-Chamber Fixed\nBoth Only Only Both Only Only\nTwitter Facebook Twitter Facebook\nFemale 0.363*** 0.283** 0.251** 0.368*** 0.283** 0.257**\n(0.08) (0.09) (0.1) (0.09) (0.09) (0.1)\nMinority -0.186 -0.348** -0.019 -0.181 -0.344** -0.003\n(0.1) (0.11) (0.11) (0.1) (0.11) (0.12)\nIdeology -0.507*** -0.412*** 0.045 -0.51*** -0.414*** 0.049\n(0.04) (0.05) (0.05) (0.04) (0.05) (0.05)\nElection 0.014*** 0.007** 0.014*** 0.014*** 0.007** 0.014***\ncompetitiveness (0) (0) (0) (0) (0) (0)\nSenate 0.436*** 0.313*** 0.081 1.734*** 1.31*** 0.355\n(0.09) (0.09) (0.1) (0.21) (0.21) (0.22)\nFixed State \u2713 \u2713 \u2713\nFixed State-Chamber \u2713 \u2713 \u2713\nNum.Obs. 6869 6869\nR2 0.249 0 .258\nR2 Adj. 0.249 0 .258\nAIC 16 348 .3 16 434 .8\nBIC 17 476 .1 18 546 .7\nRMSE 0.40 0 .40\nNote:Standard errors in parentheses. The models control for state and state-chamber effects, accounting\nfor unobserved heterogeneity at these levels.\n*p <0.05, **p <0.01, ***p <0.001.\n23\nAPI Terms and Data Dissemination\nWe have accessed data, and continue to access data, via the official Facebook and X/Twitter APIs.\nThe terms to which our access is subject on both APIs prevent us from publicly disseminating all data\nin its completely raw form. The dashboard and bulk data access are currently configured to fit within\nthe APIs\u2019 terms for data dissemination. Specificaly, we permit users to perform custom searches on\nthe data, and then access bulk data that is aggregated up to the weekly level based on these searches.\nHere we give a simple example of what this looks like. If a researcher uses the R package to query\nactivity related to the search term #BlackLivesMatter, they will access an official-week dataset that\ngives the number of tweets and Facebook posts that include that hashtag, along with other selected\nmeasures applied to those tweets and posts (e.g., the number containing misinformation, the average\ntoxicity score). In this way, we are disseminating highly informative and fine-grained information\nabout officials\u2019 online communication, while not violating API terms and potentially compromising\nfuture access to the data.\nConclusion\nWe present a new venue for studying state politicians and state politics across different platforms\nfrom 2020 to 2023. The goal of the DAPR project is to build a publicly accessible data and ana-\nlytics tool for everyone, including average citizens, researchers, media, and NGOs, to hold public\nofficials accountable and promote civic engagement. Our DAPR database contains 8,016 distinct\nlegislators with detailed and verified demographic and political information, and more than 6 million\nposts from X and Facebook, with text-level and legislator-level engagement, credibility, and toxicity\nmetrics. Our dashboard provides multiple modules and user-friendly functionality to facilitate data\nvisualization and analytics, catering to the diverse needs of different users. The combination of ex-\ntensive demographic data, content information, and versatile dashboard functionality makes DAPR\nan essential tool for understanding and analyzing the digital presence and behavior of U.S. elected\nofficials.\nFor researchers, DAPR data provide insight into politicians\u2019 online rhetoric, responsiveness, lin-\nguistic patterns, campaign strategies, and political communication with citizens. This data fosters\ntransparency and accountability, allowing both the public and researchers to monitor and evaluate\n24\nstate politicians\u2019 actions and responsiveness, leading to a more informed and engaged electorate.\nAdditionally, transparency helps mitigate the spread of misinformation on social media by enabling\nstudies that reduce its impact on public opinion and voter behavior, thus supporting democratic pro-\ncesses.\nWhile the DAPR project offers a valuable resource for studying state politicians\u2019 online behav-\nior, it currently has several limitations. Firstly, the emphasis on X and Facebook excludes niche\nsocial media platforms with distinct user bases. Platforms like Parler or Truth Social might cater to\nspecific political ideologies and potentially reveal different discourse topics, polarization levels, and\nonline community interactions compared to mainstream platforms. Secondly, the current dataset has\na temporal limitation regarding Facebook data. While X data spans three and a half years, Facebook\ndata only covers 2020 and 2021. This period coincides with significant events like the COVID-19\npandemic and the 2020 election, potentially influencing online political communication in a way not\nnecessarily representative of broader trends.\nWe are addressing these limitations. The DAPR project is ongoing, with continuous updates and\nimprovements. We are actively collecting posts from X and Facebook and are expanding the platform\nto include YouTube, Instagram, and LinkedIn. Moreover, we are incorporating both national and city\npoliticians into the dataset. The compiled metrics of engagement, civility, and shared information\nfrom individual legislators will be available for download in various formats on a daily and weekly\nbasis to support research. This continuous endeavor to develop a time-series dataset across diverse\nplatforms holds immense potential for scholars, fostering new perspectives on state politics research\nand the development of robust theoretical frameworks.\nReferences\nBarber\u00e1, Pablo, Andreu Casas, Jonathan Nagler, Patrick J Egan, Richard Bonneau, John T Jost, and\nJoshua A Tucker. 2019. \u201cWho leads? Who follows? Measuring issue attention and agenda set-\nting by legislators and the mass public using social media data.\u201d American Political Science\nReview 113 (4): 883\u2013901.\nBerry, William D, Evan J Ringquist, Richard C Fording, and Russell L Hanson. 1998. \u201cMeasuring\ncitizen and government ideology in the American states, 1960-93.\u201d American Journal of Politi-\ncal Science, 327\u2013348.\n25\nBlevins, Cameron, and Lincoln Mullen. 2015. \u201cJane, John... Leslie? A Historical Method for Algo-\nrithmic Gender Prediction.\u201d DHQ: Digital Humanities Quarterly 9 (3).\nBoehmke, Frederick J, Mark Brockway, Bruce A Desmarais, Jeffrey J Harden, Scott LaCombe,\nFridolin Linder, and Hanna Wallach. 2020. \u201cSPID: A new database for inferring public pol-\nicy innovativeness and diffusion networks.\u201d Policy Studies Journal 48 (2): 517\u2013545.\nBoehmke, Frederick J, Bruce A Desmarais, Abbie Eastman, Isabelle Grassel, Jeffrey J Harden,\nSamuel Harper, Liam Kaboli, Hyein Ko, Elisabeth Oster, and Tracee M Saunders. 2023. \u201cSPRC19:\nA Database of State Policy Responses to COVID-19 in the United States.\u201d Scientific data 10 (1):\n526.\nBowen, Daniel C, and Zachary Greene. 2014. \u201cShould we measure professionalism with an index? A\nnote on theory and practice in state legislative professionalism research.\u201d State Politics & Policy\nQuarterly 14 (3): 277\u2013296.\nBroersma, Marcel, and Todd Graham. 2012. \u201cSocial media as beat: Tweets as a news source during\nthe 2010 British and Dutch elections.\u201d journalism practice 6 (3): 403\u2013419.\nBucchianeri, Peter, Craig V olden, and Alan E Wiseman. 2020. \u201cLegislative effectiveness in the Amer-\nican states.\u201d American Political Science Review, 1\u201319.\nButler, Daniel M, Thad Kousser, and Stan Oklobdzija. 2023. \u201cDo Male and Female Legislators Have\nDifferent Twitter Communication Styles?\u201d State Politics & Policy Quarterly, 1\u201323.\nCook, James M. 2016. \u201cAre American politicians as partisan online as they are offline? Twitter\nnetworks in the US Senate and Maine State Legislature.\u201d Policy & Internet 8 (1): 55\u201371.\n. 2017. \u201cTwitter adoption and activity in US legislatures: A 50-state study.\u201d American Behav-\nioral Scientist 61 (7): 724\u2013740.\nDarr, Joshua P, Matthew P Hitt, and Johanna L Dunaway. 2018. \u201cNewspaper closures polarize voting\nbehavior.\u201d Journal of Communication 68 (6): 1007\u20131028.\nFisher, Caroline, Eileen Culloty, Jee Young Lee, and Sora Park. 2019. \u201cRegaining Control Citizens\nwho follow politicians on social media and their perceptions of journalism.\u201d Digital journalism\n7 (2): 230\u2013250.\n26\nFraga, Bernard L, Eric Gonzalez Juenke, and Paru Shah. 2021. \u201cCandidate Characteristics Coopera-\ntive (C3) 2018 Data.\u201d URL: https://doi. org/10.7910/DVN/VHAPHV.\nGriffin, John D. 2006. \u201cElectoral competition and democratic responsiveness: A defense of the\nmarginality hypothesis.\u201d The Journal of Politics 68 (4): 911\u2013921.\nGrinberg, Nir, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson, and David Lazer. 2019.\n\u201cFake news on Twitter during the 2016 US presidential election.\u201d Science 363 (6425): 374\u2013378.\nGrossmann, Matt, and David A Hopkins. 2016. Asymmetric politics: Ideological Republicans and\ngroup interest Democrats. Oxford University Press.\nGuntuku, Sharath Chandra, Jonathan Purtle, Zachary F Meisel, Raina M Merchant, and Anish Agar-\nwal. 2021. \u201cPartisan differences in twitter language among US legislators during the COVID-19\npandemic: Cross-sectional study.\u201d Journal of medical Internet research 23 (6): e27300.\nHagen, Loni, Stephen Neely, Ryan Scharf, and Thomas E Keller. 2020. \u201cSocial media use for cri-\nsis and emergency risk communications during the Zika health crisis.\u201d Digital Government:\nResearch and Practice 1 (2): 1\u201321.\nHopp, Toby, Chris J Vargo, Lucas Dixon, and Nithum Thain. 2020. \u201cCorrelating self-report and trace\ndata measures of incivility: A proof of concept.\u201d Social Science Computer Review 38 (5): 584\u2013\n599.\nHornsey, Matthew J, Matthew Finlayson, Gabrielle Chatwood, and Christopher T Begeny. 2020.\n\u201cDonald Trump and vaccination: The effect of political identity, conspiracist ideation and pres-\nidential tweets on vaccine hesitancy.\u201d Journal of Experimental Social Psychology 88:103947.\nJ\u00f6rgens, Helge, Nina Kolleck, and Barbara Saerbeck. 2016. \u201cExploring the hidden influence of inter-\nnational treaty secretariats: Using social network analysis to analyse the Twitter debate on the\n\u2018Lima Work Programme on Gender\u2019.\u201d Journal of European Public Policy 23 (7): 979\u2013998.\nKim, Jin Woo, Andrew Guess, Brendan Nyhan, and Jason Reifler. 2021. \u201cThe distorting prism of so-\ncial media: How self-selection and exposure to incivility fuel online comment toxicity.\u201d Journal\nof Communication 71 (6): 922\u2013946.\n27\nKim, Taegyoon, Nitheesha Nakka, Ishita Gopal, Bruce A Desmarais, Abigail Mancinelli, Jeffrey J\nHarden, Hyein Ko, and Frederick J Boehmke. 2021. \u201cAttention to the COVID-19 Pandemic on\nTwitter: Partisan Differences Among US State Legislators.\u201d Legislative studies quarterly.\n. 2022. \u201cAttention to the COVID-19 Pandemic on Twitter: Partisan Differences Among US\nState Legislators.\u201d Legislative studies quarterly 47 (4): 1023\u20131041.\nLarsen, Bradley J, Timothy J Ryan, Steven Greene, Marc J Hetherington, Rahsaan Maxwell, and\nSteven Tadelis. 2023. \u201cCounter-stereotypical messaging and partisan cues: Moving the needle\non vaccines in a polarized United States.\u201d Science advances 9 (29): eadg9434.\nLasser, Jana, Segun Taofeek Aroyehun, Almog Simchon, Fabio Carrella, David Garcia, and Stephan\nLewandowsky. 2022. \u201cSocial media sharing of low-quality news sources by political elites.\u201d\nPNAS nexus 1 (4): pgac186.\nMasullo Chen, Gina, Ashley Muddiman, Tamar Wilner, Eli Pariser, and Natalie Jomini Stroud. 2019.\n\u201cWe should not get rid of incivility online.\u201d Social Media+ Society 5 (3): 2056305119862641.\nMotta, Matthew. 2021. \u201cPolitical scientists: A profile of congressional candidates with STEM back-\ngrounds.\u201d PS: Political Science & Politics 54 (2): 202\u2013207.\nNakka, Nitheesha. 2024. The Racial and Gender Divide in Toxic Online Messaging Towards State\nLegislators. Available on OSF. https://osf.io/md9f8/.\nPayson, Julia, Andreu Casas, Jonathan Nagler, Richard Bonneau, and Joshua A Tucker. 2022. \u201cUsing\nSocial Media Data to Reveal Patterns of Policy Engagement in State Legislatures.\u201d State Politics\n& Policy Quarterly 22 (4): 371\u2013395.\nPeterson, David AM, Wallapak Tavanapong, Lei Qi, Adisak Sukul, and Mohammed Khaleel. 2022.\n\u201cThe public-facing policy agenda of state legislatures: The communication of public policy via\ntwitter.\u201d Policy Studies Journal.\nRajadesingan, Ashwin, Paul Resnick, and Ceren Budak. 2020. \u201cQuick, community-specific learning:\nHow distinctive toxicity norms are maintained in political subreddits.\u201d In Proceedings of the\nInternational AAAI Conference on Web and Social Media, 14:557\u2013568.\n28\nRao, H Raghav, Naga Vemprala, Patricia Akello, and Rohit Valecha. 2020. \u201cRetweets of officials\u2019\nalarming vs reassuring messages during the COVID-19 pandemic: Implications for crisis man-\nagement.\u201d International Journal of Information Management 55:102187.\nRussell, Annelise. 2021. \u201cSenate representation on Twitter: National policy reputations for con-\nstituent communication.\u201d Social Science Quarterly 102 (1): 301\u2013323.\nShor, Boris, and Nolan McCarty. 2011. \u201cThe ideological mapping of American legislatures.\u201d Ameri-\ncan Political Science Review 105 (3): 530\u2013551.\nSolovev, Kirill, and Nicolas Pr\u00f6llochs. 2022. \u201cHate speech in the political discourse on social media:\nDisparities across parties, gender, and ethnicity.\u201d In Proceedings of the ACM Web Conference\n2022, 3656\u20133661.\nTai, Yuehong Cassandra, Roan Buma, and Bruce A Desmarais. 2023. \u201cOfficial yet questionable:\nexamining misinformation in US state legislators\u2019 tweets.\u201d Journal of Information Technology\n& Politics, 1\u201313.\nTai, Yuehong Cassandra, Yu-ru Lin, and Bruce A Desmarais. 2023. \u201cPublic officials\u2019 online sharing\nof misinformation: Institutional and ideological checks.\u201d Working Paper.\nTausanovitch, Chris, and Christopher Warshaw. 2013. \u201cMeasuring constituent policy preferences in\ncongress, state legislatures, and cities.\u201d The Journal of Politics 75 (2): 330\u2013342.\nTreul, Sarah A, and Eric R Hansen. 2023. \u201cPrimary Barriers to Working Class Representation.\u201d Po-\nlitical Research Quarterly 76 (3): 1516\u20131528.\nVentura, Tiago, Kevin Munger, Katherine McCabe, and Keng-Chi Chang. 2021. \u201cConnective ef-\nfervescence and streaming chat during political debates.\u201d Journal of Quantitative Description:\nDigital Media 1.\nWintersieck, Amanda, and Alex Keena. 2023. \u201cAsk and You Shall Receive: The Effects of Negativity\nand Fundraising Appeals on Facebook.\u201d Political Research Quarterly 76 (4): 1973\u20131986.\n29\nAppendix\nUsing our validated gender and race variables, we can analyze the proportion of female and\nminority legislators in different states. Table A1. offers a fresh perspective on the U.S. map by\nincorporating these variables. The color scheme indicates the proportion of each variable in every\nstate. Nevada is notable for having the most equal gender representation in its cabinets, while the\ncabinets in West Virginia and Mississippi are largely male-dominated. California is known for its\ngreater number of minority legislators, whereas in Utah, the majority of legislators are of white\nheritage.\nFigure A1. Share of Female and Minority Legislators across States\nFigure A2. Total Legislators Race & Gender Distribution (Counts, Proportion)\nWhite Black Latino\nAsian AmericanMENA\nMultiracial\nNative AmericanUnknown\nRacial Category0100020003000400050006000CountLegislator Racial Distribution\nMen\nWomen\nGender010002000300040005000CountLegislator Gender Distribution\n30\nFigure A3: Legislators Race Distribution by Party\nWhite Black Latino\nAsian AmericanMultiracial\nNative AmericanMENA\nUnknown\nRacial Category05001000150020002500CountDemocratic Legislator Racial Distribution\nWhite Black Latino\nAsian AmericanMENA\nNative AmericanUnknownMultiracial\nRacial Category0500100015002000250030003500CountRepublican Legislator Racial Distribution\nWhite Black Latino\nNative American\nRacial Category0510152025303540CountIndependent Legislator Racial Distribution\n31\nFigure A4: Legislators Gender Distribution by Party\nMen\nWomen\nGender0500100015002000CountDemocratic Legislator Gender Distribution\nMen\nWomen\nGender050010001500200025003000CountRepublican Legislator Gender Distribution\nMen\nWomen\nGender0510152025303540CountIndependent Legislator Gender Distribution\nFigure A5. Available Accounts by Party across Platforms\n32\nFigure A6:Tweets Distribution by Party across States over Time\n33\nFigure A7: FB Posts Distribution by Party across States over Time\n34\nAlabama\nAlaska\nArizona\nArkansas\nCalifornia\nColorado\nConnecticut\nDelaware\nFlorida\nGeorgia\nHawaii\nIdaho\nIllinois\nIndiana\nIowa\nKansas\nKentucky\nLouisiana\nMaine\nMaryland\nMassachusetts\nMichigan\nMinnesota\nMississippi\nMissouri\nMontana\nNebraska\nNevada\nNew Hampshire\nNew Jersey\nNew Mexico\nNew York\nNorth Carolina\nNorth Dakota\nOhio\nOklahoma\nOregon\nPennsylvania\nRhode Island\nSouth Carolina\nSouth Dakota\nT ennessee\nT exas\nUtah\nVermont\nVirginia\nWashington\nWest Virginia\nWisconsin\nWyoming\nState0100020003000400050006000CountDistribution of Individuals by State in Facebook and T witter\nFacebook\nT witterFigure 12: State Distribution: Facebook v. Twitter\nFigure A8: Distribution of Toxicity across Facebook and Twitter\n0.0 0.2 0.4 0.6 0.8\nT oxicity050001000015000200002500030000FrequencyDistribution of T oxicity on Facebook\n0.0 0.2 0.4 0.6 0.8\nT oxicity0500010000150002000025000FrequencyDistribution of T oxicity T witter\n35\nFigure A9: Distribution of toxicity across Facebook and Twitter: Men\n0.0 0.2 0.4 0.6 0.8\nT oxicity02000400060008000100001200014000FrequencyDistribution of T oxicity for Men on Facebook\n0.0 0.2 0.4 0.6 0.8\nT oxicity0200040006000800010000120001400016000FrequencyDistribution of T oxicity for Men on T witter\nFigure A10: Distribution of toxicity across Facebook and Twitter: Women\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nT oxicity01000200030004000500060007000FrequencyDistribution of T oxicity for Women on Facebook\n0.0 0.2 0.4 0.6 0.8\nT oxicity010002000300040005000600070008000FrequencyDistribution of T oxicity for Women on T witter\n36\nFigure A11: Distribution of Toxicity across Facebook and Twitter By Party\n0.0 0.2 0.4 0.6 0.8\nT oxicity0500010000150002000025000FrequencyT oxicity of Republicans vs Democrats on Facebook\nRepublicans\nDemocrats\n0.0 0.2 0.4 0.6 0.8\nT oxicity0500010000150002000025000FrequencyT oxicity of Republicans vs Democrats on T witter\nRepublicans\nDemocrats\nFigure A12: Distribution of Toxicity across Facebook and Twitter By Gender and Party\n0.0 0.2 0.4 0.6 0.8\nT oxicity0200040006000800010000FrequencyDistribution of T oxicity for Women Democrats\nFacebook\nT witter\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nT oxicity05001000150020002500300035004000FrequencyDistribution of T oxicity for Women Republicans\nFacebook\nT witter\n0.0 0.2 0.4 0.6 0.8\nT oxicity025005000750010000125001500017500FrequencyDistribution of T oxicity for Men Democrats\nFacebook\nT witter\n0.0 0.2 0.4 0.6 0.8\nT oxicity02000400060008000100001200014000FrequencyDistribution of T oxicity for Men Republicans\nFacebook\nT witter\n37", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "The Digitally Accountable Public Representation Database: Measuring Online Communication by Federal, State, and Local Officials", "author": ["YC Tai", "N Nakka", "SM Rajtmajer", "K Munger", "YR Lin"], "pub_year": "2024", "venue": "NA", "abstract": "We introduce the Digitally Accountable Public Representation (DAPR) Database, an innovative  archive that systematically tracks and analyzes the online communications of federal,"}, "filled": false, "gsrank": 168, "pub_url": "https://files.osf.io/v1/resources/2h9xg_v1/providers/osfstorage/66915e345a51c40294e80144?action=download&direct&version=2", "author_id": ["LLWK3Z4AAAAJ", "gzOCudMAAAAJ", "G2wJ5YwAAAAJ", "sdMlz0kAAAAJ", "9EeqDSEAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:izjnsuewbZ0J:scholar.google.com/&output=cite&scirp=167&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D160%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=izjnsuewbZ0J&ei=IbWsaPvmC7_SieoPzJnloAQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:izjnsuewbZ0J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://files.osf.io/v1/resources/2h9xg_v1/providers/osfstorage/66915e345a51c40294e80144?action=download&direct&version=2"}}]