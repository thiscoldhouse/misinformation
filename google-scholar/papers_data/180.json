[{"title": "News Source Citing Patterns in AI Search Systems", "year": "2025", "pdf_data": "News Source Citing Patterns in AI Search Systems\nKai-Cheng Yang\nNortheastern University, Boston, MA, USA\nyang3kc@gmail.com\nAbstract\nAI-powered search systems are emerging as new information\ngatekeepers, fundamentally transforming how users access\nnews and information. Despite their growing influence, the\ncitation patterns of these systems remain poorly understood.\nWe address this gap by analyzing data from the AI Search\nArena, a head-to-head evaluation platform for AI search sys-\ntems. The dataset comprises over 24,000 conversations and\n65,000 responses from models across three major providers:\nOpenAI, Perplexity, and Google. Among the over 366,000\ncitations embedded in these responses, 9% reference news\nsources. We find that while models from different providers\ncite distinct news sources, they exhibit shared patterns in ci-\ntation behavior. News citations concentrate heavily among a\nsmall number of outlets and display a pronounced liberal bias,\nthough low-credibility sources are rarely cited. User prefer-\nence analysis reveals that neither the political leaning nor the\nquality of cited news sources significantly influences user sat-\nisfaction. These findings reveal significant challenges in cur-\nrent AI search systems and have important implications for\ntheir design and governance.\nCode \u2014 https://github.com/yang3kc/ai search arena\nIntroduction\nAI-powered search systems are transforming how users ac-\ncess and consume information online. Unlike traditional\nweb search engines that return ranked lists of web pages,\nAI search systems synthesize information from multiple\nsources and present coherent, conversational responses aug-\nmented with citations to supporting evidence (Xiong et al.\n2024). Studies suggest that these systems lower the infor-\nmation access barrier (Wu et al. 2020) and enable users\nto perform complex tasks more quickly (Spatharioti et al.\n2023; Suri et al. 2024). Products such as ChatGPT with web\nsearch, Perplexity AI, and Google\u2019s AI search features have\ngained millions of users, demonstrating the potential of AI\nsearch systems to become mainstream.\nScholars have long conceptualized gatekeeping as the\nprocess of selecting and highlighting certain information\nwhile filtering out other content (Barzilai-Nahon 2008;\nCopyright \u00a9 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.Shoemaker and V os 2009). Originally applied to human ed-\nitors and journalists, this concept has been extended to algo-\nrithmic systems that filter and prioritize information flows at\nscale (Van Dalen 2023). AI search engines represent a new\nevolution of this phenomenon: rather than simply retrieving\ndocuments, they actively synthesize information and fore-\nground specific sources. As these systems increasingly serve\nas primary gateways to information, they exercise unprece-\ndented control over which outlets reach users, effectively\nfunctioning as the new gatekeepers of the digital informa-\ntion ecosystem (Kuai et al. 2025).\nHowever, this gatekeeping role raises critical questions\nabout which sources these systems prioritize and how their\ncitation patterns shape public discourse (Memon and West\n2024). These concerns are not new since audits of traditional\nsearch engines and news aggregators have shown that cited\nnews sources concentrate among a relatively small number\nof popular outlets and exhibit liberal bias (Trielli and Di-\nakopoulos 2019; Fischer, Jaidka, and Lelkes 2020). Built\nupon traditional information retrieval systems, AI search\nsystems may amplify these issues through their complex\nand opaque mechanisms that can embed systematic biases\nin source selection (Yang and Menczer 2025).\nRecent audits of popular AI search engines confirm these\nconcerns. Using simulated user queries, researchers have\nshown that tools like Microsoft Copilot tend to favor main-\nstream news outlets (Brantner, Karlsson, and Kuai 2025).\nThese systems have also been found to surface more low-\ncredibility sources than Google News and demonstrate left-\nleaning bias (Li and Sinnamon 2024; Jaidka and Furniture-\nwala 2025). Given the contentious nature of the contempo-\nrary media landscape, AI systems\u2019 tendency to favor sources\nwith particular characteristics could contribute to informa-\ntion bubbles and exacerbate information disparities.\nWhile this emerging evidence provides valuable insights,\ncritical gaps remain. First, previous audits of AI search sys-\ntems rely primarily on simulated user queries, which may\nnot accurately reflect real-world usage patterns. Second,\nthese audits typically examine only one or a few AI search\nsystems, limiting their representativeness and preventing the\nidentification of cross-model patterns. Third, these studies\nlack user preference data, making it difficult to determine\nwhich factors drive user satisfaction with search results.\nTo address these gaps, we conduct a large-scale empiricalarXiv:2507.05301v1  [cs.IR]  7 Jul 2025\nanalysis of news citation patterns in AI search systems us-\ning the AI Search Arena dataset. The dataset contains over\n366,000 citations from conversations between users and 12\nAI search models from three major providers: OpenAI, Per-\nplexity, and Google. The dataset is collected through a head-\nto-head evaluation setup where users are presented with re-\nsponses from different AI search models for the same query\nand asked to choose the better response. This dataset en-\nables us to examine the citation patterns of AI search sys-\ntems in real-world usage scenarios. Specifically, we focus\non two key research questions:\n\u2022RQ1 : Do different AI search systems exhibit systematic\npatterns in their news citation behavior, and if so, what\nfactors are associated with these patterns?\n\u2022RQ2 : How do the characteristics of cited news sources\nrelate to user preferences for AI search responses?\nOur analysis reveals the following key findings. First, AI\nsearch models from the same provider exhibit similar cita-\ntion patterns, while differences between providers are more\npronounced. Second, all models demonstrate significant ci-\ntation concentration, with a small number of news outlets\naccounting for a disproportionate share of citations. Third,\nwe observe consistent left-leaning political bias across all\nAI search systems, despite their general preference for high-\nquality sources. Notably, however, user preferences for AI\nsearch responses show no significant association with either\nthe political orientation or quality of cited news sources.\nOur findings confirm the gatekeeping role of AI search\nsystems and reveal potential issues in their news-citing be-\nhaviors. These findings have important implications for the\ndesign of AI search systems and their role in shaping public\naccess to information.\nRelated Work\nSources Cited by Traditional Search Engines\nAI search systems are typically implemented as a combi-\nnation of large language models (LLMs) and web search\nengines (Xiong et al. 2024). Therefore, audits of tradi-\ntional search engines and news aggregators are particularly\nrelevant to our study. Previous research reveals a consis-\ntent pattern: cited news concentrates heavily on a relatively\nsmall number of popular outlets in traditional search en-\ngines (Trielli and Diakopoulos 2019; Fischer, Jaidka, and\nLelkes 2020). This concentration persists regardless of user\nidentities and personalization settings (Trielli and Diakopou-\nlos 2022). While low-credibility sources do appear in search\nresults, they remain relatively rare (Robertson et al. 2023).\nRegarding political bias, research from organizations like\nAllSides1and academic studies (Robertson et al. 2018,\n2023) suggest that Google tends to display more left-leaning\nsources in search results and top stories. This tendency ap-\npears consistent across users with different political leanings\nand personalization settings. However, user queries them-\nselves vary in their political orientation, leading to results\n1https://www.allsides.com/blog/google-news-shows-strong-\npolitical-bias-allsides-analysiswith different degrees of political tendencies (Robertson\net al. 2018; Tong 2025). In this study, we examine whether\nsimilar patterns emerge in AI search systems.\nSources Cited by AI Search Systems\nThe rise of AI search systems has drawn significant attention\nfrom the academic community, with many auditing studies\nemerging in recent years. A common approach involves cre-\nating queries covering various topics and then examining re-\nsponses from different AI search tools.\nFor instance, Brantner, Karlsson, and Kuai (2025) focus\non Microsoft Copilot during the run-up to Taiwan\u2019s 2024\npresidential election to examine what content it surfaces\nand how accurately it cites sources. The authors also test\nthe same queries in different languages to assess how the\nmodel responds across linguistic contexts. Their results sug-\ngest that Copilot relies heavily on professional news out-\nlets and skews heavily toward UK- and US-based English\nsources across all languages. While deliberate disinforma-\ntion is rare, they find that many professional news citations\nare mis-summarized or misattributed, and Bing links are of-\nten broken, making misinformation a byproduct of summary\nand technical errors.\nSimilarly, Li and Sinnamon (2024) audit three AI search\nsystems\u2014ChatGPT, Bing Chat, and Perplexity\u2014by collect-\ning over 1,000 answers to 48 public-interest queries. The\nauthors analyze the cited sources and show that they are\ndominated by US commercial and news sites with scant aca-\ndemic or governmental material. They warn that these pat-\nterns may amplify confirmation bias and commercial/geo-\ngraphic skew, recommending stricter source-quality filters\nand greater transparency in generative AI search.\nJaidka and Furniturewala (2025) adopt a comparative\napproach by sending queries to an AI search tool and\nGoogle News. The authors use over 80,000 queries across\n11 election-year countries covering various topics. Unlike\nother studies, the AI search tool being tested combines Ope-\nnAI\u2019s GPT-4o model with a news API, giving the authors\nbetter control over the underlying mechanisms. They show\nthat the AI search tool surfaces significantly less relevant\nand more low-quality content while being only slightly less\nleft-leaning overall than Google News, with the quality gap\nwidest for right-party queries and deeper result pages.\nIn their qualitative study, Narayanan Venkit et al. (2025)\nrecruit 21 expert users to compare AI search systems to\ntraditional search engines. The experts commonly identify\nproblems in the AI search systems, such as misattribution\nand misrepresentation of cited sources, missing citations for\ncertain claims, and a lack of transparency in source selec-\ntion. In particular, these experts express general distrust to-\nward certain sources cited by AI search systems, especially\nforums, blogs, and opinion pieces.\nThese studies provide valuable insights into the citation\npatterns of AI search systems. In this study, we contribute\nto this line of research by examining a large-scale dataset\nfrom real-world interactions between users and AI search\nsystems and covering a wide range of AI search models from\ndifferent providers.\nData and Methods\nAI Search Arena Dataset\nOur analysis is based on the AI Search Arena\ndataset (Miroyan et al. 2025), which contains 24,069\nconversations between users and AI search systems.2These\nconversations were collected from March to May 2025,\nspanning seven weeks. The dataset captures real-world\nusage patterns through a platform where users can interact\nwith different AI search models and compare their responses\nin head-to-head evaluations. Each conversation includes\nthe user\u2019s query and responses from two AI models. Some\nconversations include multiple turns of interaction. Approx-\nimately half of the conversations are rated by users, who\nselect a winner or declare a tie. Some conversations also\ninclude information about the language of the conversation\nand the country of the user.\nWe extracted 366,087 citations (URLs) from responses\ngenerated by 12 AI search models from three providers:\nOpenAI, Perplexity, and Google. The models analyzed in-\nclude variations with different capabilities and settings. For\neach citation, we extract the domain (e.g., nytimes.com for\nThe New York Times) for further analysis. Summary statis-\ntics of the dataset are provided in the Appendix.\nNews Outlet Identification\nThe 366,087 citations link to 83,533 unique domains. To un-\nderstand the news citation patterns, we identify the news out-\nlets leveraging a comprehensive list of news domains (Yang\n2025). This list is compiled from multiple sources, including\nNewsGuard,3MBFC (Media Bias/Fact Check),4ABYZ,5\nand Media Cloud.6The list also incorporates news domains\nfrom datasets curated by academics, including Robertson\net al. (2018), Cronin et al. (2023), Le Qu \u00b4er\u00b4e, Chiang, and\nNaaman (2022), Fischer, Jaidka, and Lelkes (2020), and\nHorne et al. (2022). We identify 1,795 (2.1%) unique do-\nmains as news outlets, which account for 32,865 (9.0%) of\nall citations. In the Appendix, we also provide the percent-\nage of citations for other domain types, such as social media\nand technology, to provide additional context.\nNews Outlet Political Leaning\nWe leverage the DomainDemo dataset to obtain political\nleaning scores for news outlets (Yang et al. 2025). This\ndataset is derived from a panel of over 1.5 million Twitter\nusers whose accounts are matched to their voter registra-\ntion records, linking users\u2019 source-sharing behaviors to their\npolitical affiliations. Based on this information, the authors\ngenerate an audience-based metric that quantifies the politi-\ncal leaning of news outlets for over 129,000 domains.\nThe resulting political leaning scores range from -1 to +1,\nwith -1 indicating sources predominantly shared by Demo-\ncratic users and +1 indicating sources predominantly shared\n2The dataset can be downloaded at\nhttps://huggingface.co/datasets/lmarena-ai/search-arena-24k\n3https://www.newsguardtech.com\n4https://mediabiasfactcheck.com\n5http://www.abyznewslinks.com\n6https://mediacloud.orgby Republican users. The authors validated this metric by\ncomparing it with existing datasets from academic studies,\nsuch as Bakshy, Messing, and Adamic (2015), Eady et al.\n(2025), Buntain et al. (2023), and organizations, such as\nMBFC and AllSides, finding high levels of agreement.\nMerging the ratings from the DomainDemo dataset with\nour analytical sample, we obtain political leaning scores for\n1,646 (91.7%) unique domains, which account for 98.6%\nof all news citations. We use thresholds of -0.33 and 0.33 to\nclassify news domains as \u201cleft-leaning,\u201d \u201ccenter,\u201d and \u201cright-\nleaning.\u201d Domains without political leaning scores are la-\nbeled as \u201cunknown political leaning.\u201d\nNews Outlet Quality\nWe adopt the news source quality ratings generated by Lin\net al. (2023). These ratings are compiled from multiple\nsources, including NewsGuard, MBFC, Iffy index of un-\nreliable sources,7and two lists generated by professional\nfact-checkers and researchers (Pennycook and Rand 2019;\nLasser et al. 2022). The authors aggregate these ratings\nto produce a comprehensive list covering 11,520 websites.\nThe final ratings are obtained through principal component\n(PCA) analysis, and the scores are normalized to range from\n0 to 1, with higher scores indicating higher quality.\nMerging the ratings from Lin et al. (2023) with our an-\nalytical sample leads to quality ratings for 1,283 (71.5%)\nunique domains, which account for 80.8% of all news cita-\ntions. We use a threshold of 0.5 to classify news domains as\neither \u201chigh-quality\u201d or \u201clow-quality.\u201d News domains with-\nout quality rating scores are labeled as \u201cunknown quality.\u201d\nCitation Concentration Measure\nTo measure the concentration of citations across unique do-\nmains, we use the Gini index G, which is calculated as:\nG=1\n2U2\u00afxUX\ni=1UX\nj=1|xi\u2212xj|, (1)\nwhere Uis the number of unique domains, xiis the num-\nber of citations to the i-th domain, and \u00afx=PU\ni=1xi/Uis\nthe average number of citations per domain. The Gini index\nranges from 0 to 1, where 0 indicates perfect equality, and 1\nindicates perfect inequality.\nUser Question Characteristics\nTo understand the association between news citation pat-\nterns and user questions, we extract rich linguistic features\nfrom the user questions. We calculate sentence embeddings\nusing SentenceTransformers with the \u201call-MiniLM-L6-v2\u201d\nmodel (Reimers and Gurevych 2019), which capture the se-\nmantic characteristics of the questions.\nSince embedding vectors are cryptic, we also include\nmore interpretable features. First, we classify the questions\nby their intent. The intent taxonomy is provided by Miroyan\net al. (2025) and includes the following categories: \u201cfactual\n7https://iffy.news\nlookup,\u201d \u201cinformation synthesis,\u201d \u201canalysis,\u201d \u201crecommenda-\ntion,\u201d \u201cexplanation,\u201d \u201ccreative generation,\u201d \u201cguidance,\u201d \u201ctext\nprocessing,\u201d and \u201cother.\u201d\nSecond, we perform topic modeling on the questions\nusing BERTopic (Grootendorst 2022). We follow recom-\nmended best practices by using the \u201call-MiniLM-L6-v2\u201d\nmodel to generate sentence embeddings, UMAP to reduce\ndimensions, and HDBSCAN to cluster topics. This pro-\ncedure yields 10 topics, and we manually generate labels\nsuch as \u201cAI models and technology,\u201d \u201cnews updates,\u201d and\n\u201csports and entertainment\u201d for them. Since individual ques-\ntions might be associated with multiple topics, we calculate\nthe topic distribution for each question as its topic features.\nThis distribution captures the similarity of the question to\neach topic. The full list of representative keywords of each\ntopic and their labels are provided in the Appendix.\nRegression Analysis\nTo understand the factors associated with news citation\npatterns, we conduct regression analyses using each user\nquestion-AI response pair as the unit of analysis. We focus\non a subset of the dataset that meets two criteria. First, we in-\nclude only questions labeled as English to ensure consistent\nlinguistic feature extraction. Second, we restrict our analy-\nsis to responses containing at least one news citation, as our\nprimary interest lies in understanding news citation patterns.\nThese criteria yield 9,098 observations.\nThe dependent variables for our linear regression mod-\nels are the percentages of left-leaning, right-leaning, high-\nquality, and low-quality news citations among all news ci-\ntations in each response (rather than among all citations).\nThis approach allows us to examine the composition of news\nsources when AI models choose to cite news outlets.\nOur regression models include the following independent\nvariables. At the conversation level, we include the total\nnumber of conversation turns and the country/region of the\nclient. We create dummy variables for the 10 most frequent\ncountries/regions in the dataset and code all other coun-\ntries/regions as \u201cother.\u201d Conversations with missing coun-\ntry/region information are coded as \u201cunknown.\u201d The US\nserves as the reference category.\nAt the response level, we include model family dummy\nvariables, with Perplexity as the reference category. We also\ninclude the turn number for each response, the number of\ncitations, and the percentage of news citations among all ci-\ntations. Additionally, we include the word count of the re-\nsponse, which is log-transformed and converted to Z-scores\nto address distributional skewness.\nAt the question level, we include the word count of\neach question, which is log-transformed and converted to\nZ-scores. We apply PCA to reduce the aforementioned sen-\ntence embedding with 384 dimensions to 10 principal com-\nponents and include them in the regression models as control\nvariables. Intent features are encoded as dummy variables\nwith \u201crecommendation\u201d serving as the reference category.\nTopic distribution features are standardized to Z-scores and\nincorporated into the regression models.\n0% 5% 10% 15% 20%\nPercentage of news citationsPerplexityGoogleOpenAI\n7.7%7.9%19.1%Figure 1: News citation rates among all citations across\nmodel families.\nUser Preference Analysis\nTo understand the relationship between news citation pat-\nterns and user preferences, we focus on conversations where\nboth responses cite at least one news source. We also exclude\nconversations with no user judgment or with a tie, leading to\n1,534 head-to-head comparisons.\nWe employ the Bradley-Terry model to estimate the asso-\nciation between different features and user choices (Bradley\nand Terry 1952; Chiang et al. 2024). The model assumes that\nthe probability of preferring response A over B depends on\nthe difference in their latent quality scores, which are mod-\neled as linear combinations of observed features. We esti-\nmate coefficients using maximum likelihood estimation and\nassess 95% confidence intervals and statistical significance\nthrough bootstrap resampling with 1,000 replications.\nWe calculate the proportion of news citations among all\ncitations as the control variable. We then calculate the pro-\nportions of left-leaning, center, and right-leaning news ci-\ntations, and the proportions of high-quality and low-quality\nnews citations among news citations to test their association\nwith user preferences. To control for additional confounding\nfactors, we include response word count and total citation\ncount, both of which have been identified as key determi-\nnants of user preferences (Miroyan et al. 2025). Since user\njudgments are provided at the conversation level, we average\nall response-level variables across turns when a conversa-\ntion involves multiple rounds of exchanges to produce final\nconversation-level variables. The difference in style features\nbetween the two AI models is transformed into Z-scores be-\nfore being fed into the Bradley-Terry model.\nResults\nBasic News Citations Characteristics\nOur analysis reveals that models within the same family ex-\nhibit similar citation patterns. Specifically, models from the\nsame family tend to cite similar proportions of news sources\namong all citations. A detailed breakdown by individual\nmodel is provided in the Appendix, whereas Figure 1 dis-\nplays the news citation rates across model families. We find\nthat OpenAI models consistently cite news sources at higher\nrates than their counterparts, while Google and Perplexity\nmodels show comparable citation frequencies.\nThe similarity extends beyond citation rates to the spe-\ncific news domains cited. Our cosine similarity analysis of\nnews citations (detailed in the Appendix) shows that mod-\nels within the same family achieve similarity scores ranging\n0% 20% 40% 60% 80% 100%\nCumulative percentage of unique domains0%20%40%60%80%100%Cumulative percentage of news citations\nOpenAI (G=0.83)\nPerplexity (G=0.77)\nGoogle (G=0.69)Figure 2: News citation Lorenz curves across model fami-\nlies. The diagonal line represents perfect equality. The Gini\ncoefficients are annotated in the legend.\nfrom 0.82 to 0.99. In contrast, cross-family similarities are\nsubstantially lower (ranging from 0.11 to 0.58). These find-\nings support our decision to aggregate subsequent analyses\nat the model family level, focusing on inter-family differ-\nences rather than intra-family variations.\nFrequent News Sources and Concentration\nTable 1 presents the top 20 most frequently cited news\nsources by model family, illustrating the distinct citation\npreferences across different AI systems. Each model family\nexhibits clear preferences for specific domains. For instance,\nOpenAI models most frequently cite reuters.com , Google\nmodels favor indiatimes.com , and Perplexity models pre-\nferbbc.com . While some domains like apnews.com appear\nacross multiple families, the overall patterns align with our\nsimilarity analysis showing strong inter-family differences.\nTo further characterize these differences, we apply the log-\nodds ratio informative Dirichlet prior method (Monroe, Co-\nlaresi, and Quinn 2008) in the Appendix to identify overrep-\nresented news sources for each model family. This approach\ndown-weights commonly cited domains and isolates sources\nthat are uniquely preferred by each model family.\nTable 1 also reveals a highly concentrated citation pat-\ntern. For example, OpenAI models cite reuters.com andap-\nnews.com at rates of 22.8% and 12.2% of all citations, re-\nspectively. Consequently, the top 20 most frequent news\nsources account for 67.3% of all citations for OpenAI mod-\nels. While Google and Perplexity models show less severe\nconcentration, their top 20 sources still represent 31.9% and\n28.5% of all citations, respectively.\nTo characterize the concentration of news citations across\nall news domains, we plot the Lorenz curves of the news\ncitation rates across model families in Figure 2. We also cal-\nculate the Gini coefficients for each model family. We can\nsee that OpenAI models have the highest level of inequality\n(G=0.83) in their citation patterns, followed by Perplexity\nmodels (G=0.77) and Google models (G=0.69).\nOpenAI14.5% 85.2% 0.3%\n(a)Perplexity23.6% 75.2% 1.2%\n(b)\n-1 -0.5 0 0.5 1\nPolitical leaning scoreGoogle26.3% 72.8% 0.8%\n(c)3.8% 96.2%\n(d)\n7.8% 92.2%\n(e)\n00.20.40.60.81.0\nQuality score10.3% 89.7%\n(f)Figure 3: Distribution of political leaning and quality scores\nof news sources across model families. The dashed lines in-\ndicate the thresholds for political leaning and quality scores\ndescribed in the main text. The percentages of sources in dif-\nferent categories are annotated in the figure.\nComparing the results in Table 1 with the Gini coefficients\nreveals an apparent inconsistency. While Google models\nshow higher concentration among their top 20 news sources\nthan Perplexity models (31.9% vs. 28.5%), Perplexity mod-\nels exhibit higher overall inequality (G=0.77) than Google\nmodels (G=0.69). This pattern suggests that Google mod-\nels\u2019 concentration is driven by heavy reliance on their most\nfrequently cited sources, whereas Perplexity models display\nmore dispersed citation patterns across a broader range of\ndomains. A key contributing factor is that Perplexity models\ncite substantially more unique news sources (1,430) com-\npared to Google (881) and OpenAI (707) models. This long\ntail of domains cited only once or twice contributes to the\nhigher Gini coefficients observed for Perplexity models, de-\nspite their lower concentration among top sources.\nPolitical Leaning and Quality of News Sources\nTo assess the political leaning of news sources cited by\ndifferent model families, we examine the distribution of\nsource political leaning scores in Figure 3. Kolmogorov-\nSmirnov tests reveal that all pairwise distributions differ sig-\nnificantly ( p < 0.001) even after correcting for multiple\ncomparisons. Applying the -0.33 and 0.33 thresholds, we\nclassify the news domains in the citations into three cate-\ngories: left-leaning, center, and right-leaning. We find that\nOpenAI models demonstrate the highest proportion of cen-\nter sources (85.2%), compared to Google (75.2%) and Per-\nplexity (72.8%) models. For left-leaning sources, Google\nmodels show the highest citation rate at 26.3%, followed by\nPerplexity models at 23.6% and OpenAI models at 14.5%.\nRight-leaning sources represent a minimal fraction across\nall model families: 1.2% for Perplexity, 0.8% for Google,\nand 0.3% for OpenAI models. Despite these variations, all\nTable 1: Top 20 most frequent news sources by the model family with political leaning and quality. Political leaning\n(columns \u201cL\u201d): L=left-leaning, C=center, R=right-leaning, U=unknown. Quality (columns \u201cQ\u201d): H=high-quality, L=low-\nquality, U=unknown.\nOpenAI Google Perplexity\nDomain % L Q Domain % L Q Domain % L Q\nreuters.com 22.8 C H indiatimes.com 4.3 C L bbc.com 3.2 C H\napnews.com 12.2 C H forbes.com 2.5 C H yahoo.com 2.7 C H\nft.com 7.0 C H aljazeera.com 2.5 L H 163.com 2.0 C U\naxios.com 6.7 C H cbsnews.com 2.0 C H sohu.com 2.0 C L\ntime.com 3.9 C H apnews.com 2.0 C H nytimes.com 1.7 C H\nelpais.com 2.5 L H techradar.com 1.9 C H cnn.com 1.6 C H\nas.com 2.4 C U healthline.com 1.7 C H reuters.com 1.5 C H\ntheatlantic.com 1.5 L H 163.com 1.5 C U sina.com.cn 1.5 L U\ntechradar.com 1.1 C H cbr.com 1.4 C U espn.com 1.4 C H\nlemonde.fr 0.8 L H yahoo.com 1.3 C H forbes.com 1.3 C H\nsohu.com 0.8 C L screenrant.com 1.3 C U rbc.ru 1.3 L U\nbbc.com 0.8 C H zdnet.com 1.2 C H cbsnews.com 1.2 C H\nknowyourmeme.com 0.7 L H sina.com.cn 1.1 L U indiatimes.com 1.0 C L\nindiatimes.com 0.7 C L pcmag.com 1.1 C H apnews.com 0.9 C H\ntheguardian.com 0.6 C H hindustantimes.com 1.1 C H techradar.com 0.9 C H\nforbes.com 0.6 C H medicalnewstoday.com 1.0 C H screenrant.com 0.9 C U\nrbc.ru 0.6 L U livemint.com 1.0 C U techcrunch.com 0.9 C H\nhealthline.com 0.5 C H pbs.org 1.0 L H businessinsider.com 0.9 C H\nusnews.com 0.5 C H thehindu.com 1.0 L H pbs.org 0.8 L H\nbusinessinsider.com 0.4 C H cnet.com 0.9 C H cnbc.com 0.8 C H\nTotal 67.3 Total 31.9 Total 28.5\nthree model families exhibit a pronounced left-leaning bias\nin their news source citations.\nSimilarly, we show the distribution of source quality\nscores in Figure 3 as well. Kolmogorov-Smirnov tests reveal\nthat all pairwise distributions differ significantly ( p <0.001)\neven after correcting for multiple comparisons. Applying\na threshold of 0.5, OpenAI models demonstrate the high-\nest proportion of high-quality sources (96.2%), followed by\nGoogle models (92.2%) and Perplexity models (89.7%).\nTable 1 also displays the quality and political leaning cat-\negories for the top 20 most frequently cited news sources by\nmodel family. These domains are predominantly center and\nleft-leaning sources and are overwhelmingly high-quality,\nconsistent with the findings in Figure 3. The same pattern is\nobserved for the overrepresented news sources as well (see\nthe Appendix for details).\nRegression Analysis\nThe patterns observed in Figure 3 may be confounded by\nvarious factors, such as the nature of users\u2019 questions. To\nisolate the effects of different factors, we conduct linear\nregression analyses. As described in the Data and Meth-\nods section, we regress the percentage of left-leaning, right-\nleaning, high-quality, and low-quality news citations among\nall news citations in each response on various conversation-\nlevel, question-level, and response-level variables. Figure 4\npresents the regression coefficients for selected variables,\nwith complete results available in the Appendix. Since thedependent variables are percentages, the coefficients repre-\nsent the percentage point change in the dependent variable\nfor a one-unit increase in the independent variable.\nThe intercepts in the figures represent the baseline per-\ncentages of different types of news citations when all other\nvariables are set to zero. The intercept for left-leaning news\ncitations is 18.5%, substantially higher than the 2.5% for\nright-leaning news, confirming the persistent left-leaning\ntendency in news citations. Similarly, the intercept for high-\nquality news citations (69.9%) far exceeds that for low-\nquality news (5.7%), demonstrating a clear preference for\nhigh-quality news sources.\nExamining model family differences, we find that com-\npared to Perplexity models, Google models cite significantly\nmore left-leaning and low-quality news sources while citing\nfewer right-leaning and high-quality sources. Conversely,\nOpenAI models cite fewer left-leaning sources but more\nhigh-quality sources relative to Perplexity. These regression\nresults confirm the patterns observed in Figure 3, demon-\nstrating that the inter-family differences persist even after\ncontrolling for potential confounding factors. Moreover, the\ncoefficients for the model family variables are relatively\nsmall compared to the intercepts, suggesting that the ten-\ndency to cite left-leaning and high-quality sources is consis-\ntent across all model families.\nRegarding question intent, questions in the \u201cguidance\u201d\ncategory are associated with decreased citations to both\nleft-leaning and high-quality news sources. Additionally,\n-10 0 10 20T opic: comics and gamesT opic: music and lyricsT opic: online content and bookT opic: fictional character battleT opic: biography and personal storiesT opic: sports and entertainmentT opic: news updatesT opic: diet, nutrients, and healthT opic: stock prices and marketT opic: AI models and technologyIntent: text processingIntent: otherIntent: info synthesisIntent: guidanceIntent: factual lookupIntent: explanationIntent: creative generationIntent: analysisModel family: OpenaiModel family: GoogleIntercept\n(a)% left-leaning news\n-5 0 5\nCoefficient\n(b)% right-leaning news\n-20 020406080\n(c)% high-quality news\n-10 -5 0 5 10\n(d)% low-quality newsFigure 4: Coefficients for news source citation patterns in linear regression models. Each subfigure corresponds to a different\ndependent variable. Only coefficients for selected variables (intercepts, model family, question intent, and question topic) are\ndisplayed. Solid dots represent coefficients that are statistically significant at the 0.05 level, while hollow dots represent non-\nsignificant coefficients. Error bars indicate 95% confidence intervals. Coefficients represent the percentage point change in the\ndependent variable for a one-unit increase in the independent variable. The model family and intent variables are binary indica-\ntors, with Perplexity and \u201crecommendation\u201d serving as reference categories. The topic variables represent the topic distribution\nof each question and are standardized to Z-scores.\n\u201ccreative generation\u201d questions show decreased citations to\nright-leaning news sources, while \u201ctext processing\u201d ques-\ntions exhibit increased citations to right-leaning sources.\nThe patterns for the question topics are more complex.\nIt is worth noting that these variables represent the topic\ndistribution converted to Z-scores, so the unit of change is\none standard deviation. We see no statistically significant\nassociations of these variables with left-leaning and low-\nquality news citations. However, for right-leaning news cita-\ntions and high-quality news citations, there are some signifi-\ncant associations. For instance, the topic \u201conline content and\nbook\u201d is associated with decreased citations to right-leaning\nnews but increased citations to high-quality news.\nFor clarity, we omit coefficients for several variables from\nthe figures, as they fall outside our primary focus or are dif-\nficult to interpret. Nevertheless, some of these omitted vari-\nables remain statistically significant. For example, certain\nclient country/region variables show significant associations\nwith specific news citation patterns. Complete regression re-\nsults are available in the Appendix.\nUser Preference Analysis\nTo assess the user preference for different news citation fea-\ntures, we show the Bradley-Terry model coefficients in Fig-\nure 5. We estimate different model specifications by gradu-\nally adding news citation features to the base model.The base model (Figure 5(a)) includes only control vari-\nables: response word count and citation count. Previous\nstudies have identified these as the most important factors\ndriving user satisfaction (Miroyan et al. 2025). When esti-\nmated on the complete dataset, both coefficients are positive\nand significant, confirming prior findings. However, in our\nfocused analysis of conversations where both AI models cite\nat least one news source in each response, the significance of\nthe citation count disappears, as shown in the figure.\nWe then add the proportion of news citations to the base\nmodel (Figure 5(b)). The coefficient for this variable is not\nstatistically significant, indicating that users do not exhibit a\nstrong preference for the proportion of news citations after\ncontrolling for response length and citation count. Finally,\nwe add various news citation features in terms of political\nleaning and quality to the base model (Figure 5(c)). None of\nthe coefficients are statistically significant, suggesting that\nusers do not have strong preferences for news citations based\non political leaning or quality ratings. The primary factors\nassociated with user preference remain response length.\nDiscussion\nNews Citation Patterns and User Preference\nOur analysis provides a comprehensive overview of news\ncitation patterns across AI search models from three major\nproviders using real-world user interactions. Our findings\nCitation countResponse word count\n(a)\nNews proportionCitation countResponse word count\n(b)\n-0.2 00.20.40.60.8\nCoefficientProportion low-quality newsProportion high-quality newsProportion center-leaning newsProportion right-leaning newsProportion left-leaning newsCitation countResponse word count\n(c)Figure 5: Bradley-Terry model coefficients for user prefer-\nence analysis. (a) The base model with only the control vari-\nables. (b) The model with control variables and proportion\nof news citations. (c) The model with control variables and\nvarious news citation features. The dots indicate the point\nestimates and the error bars indicate the 95% confidence in-\ntervals. The solid dots indicate statistically significant coef-\nficients at the 0.05 level, while the hollow dots indicate non-\nsignificant coefficients.\ncontribute to the growing literature on AI search systems\u2019\npotentially disruptive impact on the news ecosystem (Hagar\nand Diakopoulos 2025).\nWe reveal rich and varied citation behaviors that address\nourRQ1 . We find that models within the same family tend\nto cite similar news sources, even with different configura-\ntions. At the same time, inter-family differences are more\npronounced, with different model families relying on sub-\nstantially different news sources. Despite these differences,\nconsistent citation patterns emerge across all model families.\nWe show that news citations are highly concentrated\namong a small number of well-established outlets\u2014a pat-\ntern consistent with previous audits of traditional search en-\ngines (Trielli and Diakopoulos 2019; Fischer, Jaidka, and\nLelkes 2020). This concentration is most pronounced in\nOpenAI models, where the top 20 news sources account for\n67.3% of all citations, while Google and Perplexity models\nshow less severe but still substantial concentration.\nAnother striking finding is the pronounced liberal bias in\nnews citations, with left-leaning and center sources compris-\ning over 98% of all citations across model families. This pat-\ntern aligns with findings from audits of traditional search en-\ngines (Robertson et al. 2018, 2023). Regarding source qual-\nity, all model families demonstrate a clear preference for\nhigh-quality outlets, though with notable disparities across\nproviders. In particular, OpenAI models cite high-quality\nsources at a rate of 96.2%, compared to 92.2% for Google\nmodels and 89.7% for Perplexity models.\nOur regression analysis confirms the robustness of thesepatterns by controlling for various confounding factors. The\ncoefficients for different question categories reveal that AI\nsearch systems adapt their news citation behavior to question\ntypes, mirroring patterns observed in traditional search en-\ngines (Robertson et al. 2018, 2023). Surprisingly, however,\nthe results contradict our expectation that AI search systems\nwould cite more politically charged sources when address-\ning politics-related questions. Most user questions in our\ndataset focus on topics that are not inherently political, such\nas AI and technology or sports and entertainment. While the\n\u201cnews updates\u201d category represents the most likely context\nfor political news citations, its coefficient lacks statistical\nsignificance. These findings suggest that the observed cita-\ntion patterns stem primarily from the intrinsic characteristics\nof the AI systems themselves rather than from the nature of\nuser questions.\nOur analysis on the user preference addresses our RQ2\nand confirms findings from previous studies showing that\nusers tend to prefer longer responses in AI search sys-\ntems (Li and Aral 2025). However, neither the proportion of\nnews citations nor the characteristics of cited news sources\nare associated with user preference with statistical signifi-\ncance. This finding aligns with the human subject experi-\nment by Li and Aral (2025), which demonstrates that adding\ncitations increases users\u2019 trust in responses regardless of ci-\ntation validity or relevance.\nImplications\nOur results confirm that AI search systems already func-\ntion as powerful algorithmic gatekeepers, with a small num-\nber of news outlets capturing the lion\u2019s share of citations.\nThe extreme inequality we observe, particularly for Ope-\nnAI models, reveals a winner-take-all dynamic where es-\ntablished brands secure ever-greater visibility while smaller\nor marginalized outlets struggle to reach audiences through\nAI-mediated channels, echoing concerns about knowledge\nsealing in AI search systems (Lindemann 2024). Since our\nBradley-Terry analysis demonstrates that users prioritize re-\nsponse length over the specific outlets cited, the gatekeep-\ning power of these systems remains largely unchallenged by\nend-users.\nFrom a democratic perspective, source concentration un-\ndermines the diversity of ideas and perspectives available\nthrough AI search as gatekeepers (Shoemaker and V os\n2009). This problem is compounded by our finding that con-\ncentration occurs alongside significant political bias, mean-\ning users encounter not only fewer sources but sources that\nlean heavily in one political direction. The systematic nature\nof both concentration and political bias is particularly trou-\nbling, as these patterns appear consistently across all three\nAI search providers examined. This consistency suggests the\nissue transcends individual system architectures and likely\nreflects broader patterns in training data, retrieval mecha-\nnisms, or optimization objectives.\nThe variation in concentration patterns and source quality\nacross model families creates a situation where users\u2019 access\nto information depends heavily on their choice of AI sys-\ntem. For instance, while OpenAI users benefit from citations\nto higher-quality news sources, they also encounter much\ngreater source concentration compared to users of Google\nor Perplexity systems. This disparity may create a new form\nof digital divide, where the breadth and quality of informa-\ntion access becomes contingent on platform choice or the\nability to afford more expensive products.\nOur findings on user preference suggest that users may not\nclosely examine the specific sources cited in responses, pro-\nvided that citations are present. These results reinforce con-\ncerns that AI search systems, by delivering pre-summarized\nresponses, may diminish users\u2019 motivation and capacity\nto explore the broader context surrounding answers and\nto critically assess the credibility and relevance of cited\nsources (Shah and Bender 2022; Lindemann 2024).\nFuture Research\nLike many other studies on AI search systems, our study\nfaces some limitations, and the findings should be inter-\npreted accordingly. Nevertheless, these limitations indicate\ncritical directions for future research.\nFirst, as an observational study of these complex black-\nbox systems, we cannot pinpoint the exact causes of the\nobserved news citation patterns. AI search systems typi-\ncally comprise multiple components, including large lan-\nguage models and information retrieval systems (Xiong\net al. 2024). The mechanisms by which LLMs interpret\nuser queries, retrieve relevant information, and select ci-\ntation sources could all contribute to the citation patterns.\nTherefore, further research is needed to decompose these\nsystem components and understand how they individually\nand collectively influence the final outputs. We also call for\ngreater industry transparency regarding system architectures\nand component interactions, as this is essential for enabling\npublic understanding and evaluation of these systems.\nSecond, our findings underscore the need for a more sys-\ntematic analysis of AI search systems as they rapidly evolve.\nThe present study represents a snapshot from a specific time\nperiod (March-May 2025) and may not reflect future system\nperformance. Longitudinal studies tracking changes in ci-\ntation patterns over time would help determine whether ob-\nserved biases remain stable or shift with system updates. Our\nfinding that different platforms exhibit drastically different\nnews citation patterns highlights the importance of includ-\ning additional providers in future analyses. It would also be\nvaluable to examine how citation patterns vary across differ-\nent query types, languages, and contexts beyond our current\nscope. Such systematic analysis would not only deepen our\nunderstanding of AI search systems\u2019 behavior but also en-\nable users to make more informed decisions when selecting\namong these systems.\nThird, our analysis focuses exclusively on news citations,\nwhich represent only 9% of all citations in the dataset. The\nremaining 91% of citations deserve equal scrutiny in future\nresearch. For instance, 10% of citations point to social me-\ndia platforms. Unlike traditional news, this user-generated\ncontent presents greater regulatory challenges and creates\nsignificant difficulties for both human evaluators and AI sys-\ntems attempting to assess credibility. Therefore, social me-\ndia may serve as particularly problematic vectors for parti-\nsan or low-quality information (Shu et al. 2017). Future re-search should, therefore, examine citation patterns across all\nsource types to develop a comprehensive understanding of\nhow AI search systems curate and prioritize information.\nOur findings also raise fundamental questions about the\ndesign and governance of AI search systems. Given the dif-\nficulty of quantifying output quality in AI search systems,\nhow can we design evaluation metrics that satisfy both user\npreferences and the public interest? This challenge is partic-\nularly acute given our finding that these two objectives might\nnot be aligned. The remedy may require novel approaches\nto training, retrieval, and ranking that explicitly account for\npolitical diversity and source credibility. Regulatory frame-\nworks may also be needed to ensure that AI search systems\nserve the public interest by providing balanced and reliable\ninformation access.\nReferences\nBakshy, E.; Messing, S.; and Adamic, L. A. 2015. Expo-\nsure to ideologically diverse news and opinion on Facebook.\nScience , 348(6239): 1130\u20131132.\nBarzilai-Nahon, K. 2008. Toward a theory of network gate-\nkeeping: A framework for exploring information control.\nJournal of the American Society for Information Science and\nTechnology , 59(9): 1493\u20131512.\nBradley, R. A.; and Terry, M. E. 1952. Rank analysis of\nincomplete block designs: I. The method of paired compar-\nisons. Biometrika , 39(3/4): 324\u2013345.\nBrantner, C.; Karlsson, M.; and Kuai, J. 2025. Sourcing be-\nhavior and the role of news media in AI-powered search en-\ngines in the digital media ecosystem: Comparing political\nnews retrieval across five languages. Telecommunications\nPolicy , 49(5): 102952.\nBuntain, C.; Bonneau, R.; Nagler, J.; and Tucker, J. A.\n2023. Measuring the Ideology of Audiences for Web Links\nand Domains Using Differentially Private Engagement Data.\nProceedings of the International AAAI Conference on Web\nand Social Media , 17(1): 72\u201383.\nChiang, W.-L.; Zheng, L.; Sheng, Y .; Angelopoulos, A. N.;\nLi, T.; Li, D.; Zhu, B.; Zhang, H.; Jordan, M. I.; Gonzalez,\nJ. E.; and Stoica, I. 2024. Chatbot arena: an open platform\nfor evaluating LLMs by human preference. In Proceedings\nof the 41st International Conference on Machine Learning ,\nICML\u201924. JMLR.org.\nCronin, J.; Clemm von Hohenberg, B.; Gonc \u00b8alves, J. F. F.;\nMenchen-Trevino, E.; and Wojcieszak, M. 2023. The (null)\nover-time effects of exposure to local news websites: Evi-\ndence from trace data. Journal of Information Technology &\nPolitics , 1\u201315.\nEady, G.; Bonneau, R.; Tucker, J. A.; and Nagler, J. 2025.\nNews Sharing on Social Media: Mapping the Ideology of\nNews Media, Politicians, and the Mass Public. Political\nAnalysis , 33(2): 73\u201390.\nFischer, S.; Jaidka, K.; and Lelkes, Y . 2020. Auditing local\nnews presence on Google News. Nature Human Behaviour ,\n4(12): 1236\u20131244.\nGrootendorst, M. 2020. KeyBERT: Minimal keyword ex-\ntraction with BERT.\nGrootendorst, M. 2022. BERTopic: Neural topic modeling\nwith a class-based TF-IDF procedure. arXiv:2203.05794.\nHagar, N.; and Diakopoulos, N. 2025. AI Overviews, Chat-\nbots, and News Traffic: The Story So Far. https://generative-\nai-newsroom.com/ai-overviews-chatbots-and-news-traffic-\nthe-story-so-far-c010b3bf53cb (Accessed: 2025-07-01).\nHorne, B. D.; Gruppi, M.; Joseph, K.; Green, J.; Wihbey,\nJ. P.; and Adal\u0131, S. 2022. NELA-Local: A Dataset of US\nLocal News Articles for the Study of County-Level News\nEcosystems. In Proceedings of the International AAAI Con-\nference on Web and Social Media , volume 16, 1275\u20131284.\nJaidka, K.; and Furniturewala, S. 2025. The News with Chat-\nGPT: An Audit and Survey Experiment on the Effects of\nGPT-Enabled News Search on User Attitudes.\nKuai, J.; Brantner, C.; Karlsson, M.; Couvering, E. V .; and\nRomano, S. 2025. AI chatbot accountability in the age of al-\ngorithmic gatekeeping: Comparing generative search engine\npolitical information retrieval across five languages. New\nMedia & Society , 0(0): 14614448251321162.\nLasser, J.; Aroyehun, S. T.; Simchon, A.; Carrella, F.; Gar-\ncia, D.; and Lewandowsky, S. 2022. Social media sharing\nby political elites: An asymmetric American exceptionalism.\narXiv:2207.06313.\nLe Qu \u00b4er\u00b4e, M. A.; Chiang, T.-W.; and Naaman, M. 2022. Un-\nderstanding Local News Social Coverage and Engagement\nat Scale during the COVID-19 Pandemic. In Proceedings of\nthe International AAAI Conference on Web and Social Me-\ndia, volume 16, 560\u2013572.\nLi, A.; and Sinnamon, L. 2024. Generative AI Search En-\ngines as Arbiters of Public Knowledge: An Audit of Bias\nand Authority. Proceedings of the Association for Informa-\ntion Science and Technology , 61(1): 205\u2013217.\nLi, H.; and Aral, S. 2025. Human Trust in AI Search: A\nLarge-Scale Experiment. arXiv:2504.06435.\nLin, H.; Lasser, J.; Lewandowsky, S.; Cole, R.; Gully, A.;\nRand, D. G.; and Pennycook, G. 2023. High level of corre-\nspondence across different news domain quality rating sets.\nPNAS nexus , 2(9): pgad286.\nLindemann, N. F. 2024. Chatbots, search engines, and the\nsealing of knowledges. AI & SOCIETY , 1\u201314.\nMemon, S. A.; and West, J. D. 2024. Search Engines Post-\nChatGPT: How Generative Artificial Intelligence Could\nMake Search Less Reliable. arXiv:2402.11707.\nMiroyan, M.; Wu, T.-H.; King, L.; Li, T.; Pan, J.; Hu, X.;\nChiang, W.-L.; Angelopoulos, A. N.; Darrell, T.; Norouzi,\nN.; and Gonzalez, J. E. 2025. Search Arena: Analyzing\nSearch-Augmented LLMs. arXiv:2506.05334.\nMonroe, B. L.; Colaresi, M. P.; and Quinn, K. M. 2008.\nFightin\u2019words: Lexical feature selection and evaluation for\nidentifying the content of political conflict. Political Analy-\nsis, 16(4): 372\u2013403.\nNarayanan Venkit, P.; Laban, P.; Zhou, Y .; Mao, Y .; and Wu,\nC.-S. 2025. Search Engines in the AI Era: A Qualitative Un-\nderstanding to the False Promise of Factual and Verifiable\nSource-Cited Responses in LLM-based Search. In Proceed-\nings of the 2025 ACM Conference on Fairness, Accountabil-\nity, and Transparency , FAccT \u201925, 1325\u20131340. New York,NY , USA: Association for Computing Machinery. ISBN\n9798400714825.\nPennycook, G.; and Rand, D. G. 2019. Fighting misinfor-\nmation on social media using crowdsourced judgments of\nnews source quality. Proceedings of the National Academy\nof Sciences , 116(7): 2521\u20132526.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT:\nSentence Embeddings using Siamese BERT-Networks.\narXiv:1908.10084.\nRobertson, R. E.; Green, J.; Ruck, D. J.; Ognyanova, K.;\nWilson, C.; and Lazer, D. 2023. Users choose to engage\nwith more partisan news than they are exposed to on Google\nSearch. Nature , 618(7964): 342\u2013348.\nRobertson, R. E.; Jiang, S.; Joseph, K.; Friedland, L.; Lazer,\nD.; and Wilson, C. 2018. Auditing Partisan Audience Bias\nwithin Google Search. Proceedings of the ACM on Human-\nComputer Interaction , 2(CSCW).\nShah, C.; and Bender, E. M. 2022. Situating Search. In\nProceedings of the 2022 Conference on Human Information\nInteraction and Retrieval , CHIIR \u201922, 221\u2013232. New York,\nNY , USA: Association for Computing Machinery. ISBN\n9781450391863.\nShoemaker, P. J.; and V os, T. 2009. Gatekeeping theory .\nRoutledge.\nShu, K.; Sliva, A.; Wang, S.; Tang, J.; and Liu, H. 2017.\nFake News Detection on Social Media: A Data Mining Per-\nspective. SIGKDD Explor. Newsl. , 19(1): 22\u201336.\nSpatharioti, S. E.; Rothschild, D. M.; Goldstein, D. G.; and\nHofman, J. M. 2023. Comparing Traditional and LLM-\nbased Search for Consumer Choice: A Randomized Experi-\nment. arXiv:2307.03744.\nSuri, S.; Counts, S.; Wang, L.; Chen, C.; Wan, M.; Safavi, T.;\nNeville, J.; Shah, C.; White, R. W.; Andersen, R.; Buscher,\nG.; Manivannan, S.; Rangan, N.; and Yang, L. 2024. The\nUse of Generative Search Engines for Knowledge Work and\nComplex Tasks. arXiv:2404.04268.\nTong, C. 2025. Unite or divide? Biased search queries and\nGoogle Search results in polarized politics. In Proceedings\nof the 17th ACM Web Science Conference 2025 , Websci \u201925,\n158\u2013168. New York, NY , USA: Association for Computing\nMachinery. ISBN 9798400714832.\nTrielli, D.; and Diakopoulos, N. 2019. Search as News Cu-\nrator: The Role of Google in Shaping Attention to News\nInformation. In Proceedings of the 2019 CHI Conference\non Human Factors in Computing Systems , CHI \u201919, 1\u201315.\nNew York, NY , USA: Association for Computing Machin-\nery. ISBN 9781450359702.\nTrielli, D.; and Diakopoulos, N. 2022. Partisan search be-\nhavior and Google results in the 2018 U.S. midterm elec-\ntions. Information, Communication & Society , 25(1): 145\u2013\n161.\nVan Dalen, A. 2023. Algorithmic gatekeeping for profes-\nsional communicators: Power, trust, and legitimacy . Taylor\n& Francis.\nWu, Z.; Sanderson, M.; Cambazoglu, B. B.; Croft, W. B.;\nand Scholer, F. 2020. Providing Direct Answers in Search\nResults: A Study of User Behavior. In Proceedings of\nthe 29th ACM International Conference on Information\n& Knowledge Management , CIKM \u201920, 1635\u20131644. New\nYork, NY , USA: Association for Computing Machinery.\nISBN 9781450368599.\nXiong, H.; Bian, J.; Li, Y .; Li, X.; Du, M.; Wang, S.; Yin,\nD.; and Helal, S. 2024. When Search Engine Services Meet\nLarge Language Models: Visions and Challenges. IEEE\nTransactions on Services Computing , 17(6): 4558\u20134577.\nYang, K.-C. 2025. A list of news domains. Zenodo.\nYang, K.-C.; Goel, P.; Quintana-Math \u00b4e, A.; Horgan, L.;\nMcCabe, S. D.; Grinberg, N.; Joseph, K.; and Lazer, D.\n2025. DomainDemo: a dataset of domain-sharing ac-\ntivities among different demographic groups on Twitter.\narXiv:2501.09035.\nYang, K.-C.; and Menczer, F. 2025. Accuracy and Political\nBias of News Source Credibility Ratings by Large Language\nModels. In Proceedings of the 17th ACM Web Science Con-\nference , Websci \u201925, 127\u2013137. New York, NY , USA: Asso-\nciation for Computing Machinery.Appendix\nSummary Statistics\n0% 2% 4% 6% 8% 10%\nPercentage of total citationsCommunity blogsSearch enginesAcademicT echnologyOtherGovernment & educationEncyclopediaNewsSocial media\n1.3%1.4%1.7%3.2%3.4%4.2%4.8%9.0%10.0%\nFigure 6: Distribution of citations across domain types. The\n\u201cUnclassified\u201d category is excluded from the figure but ac-\ncounts for 60.1% of all citations.\nIn addition to news domains, we also identify other domain\ntypes from the dataset. We identify government and edu-\ncational domains based on their domain suffixes and clas-\nsify them as \u201cgovernment & education.\u201d We also manually\nidentify domains of academic journals (e.g., science.org )\nand research repositories (e.g., arxiv.org ), classifying them\nas \u201cacademic.\u201d Finally, we manually classify the 200 most\nfrequently cited domains into the following categories: \u201cso-\ncial media,\u201d \u201cencyclopedia,\u201d \u201ctechnology,\u201d \u201csearch engines,\u201d\n\u201ccommunity blogs,\u201d and \u201cother.\u201d These 200 domains ac-\ncount for 32.6% of all citations. All remaining domains are\nlabeled as \u201cunclassified.\u201d In Figure 6, we visualize the dis-\ntribution of citations across domain types.\nIn Table 2, we report the summary statistics for the models\nin our dataset. Note that the model gemini-2.5-pro-exp-03-\n25-wo-search is not a search model. Although it is included\nin the raw dataset, we exclude it from our analysis.\nSome models in the table may share the same base LLM\nbut differ in configuration parameters. For instance, gpt-4o-\nsearch-preview andgpt-4o-search-preview-high both use the\ngpt-4o base model, with the latter configured for a higher\nsearch context size. Since models within the same family\nexhibit similar patterns in our analysis, we aggregate their\nstatistics for most reported results. For detailed model spec-\nifications and configurations, see Miroyan et al. (2025).\nIn Table 2, we also show the news citation rates across\nmodels. We can see that the news citation rates are simi-\nlar across models within the same family. However, OpenAI\nmodels tend to cite more news sources in their responses.\nModel News Citation Similarity\nFigure 7 presents a cosine similarity heatmap of cited news\ndomains across models. The heatmap reveals strong intra-\nfamily clustering, with models within each provider family\nTable 2: Summary statistics of different models in our dataset. The table presents the provider, model name, number of re-\nsponses, total citations, news citations, and the percentage of news citations among all citations in the raw dataset. The model\nmarked with an asterisk is not a search model and so we exclude it from our analysis.\nProvider Model Responses Citations News Citations News %\nOpenAI gpt-4o-mini-search-preview 3,945 7,126 1,374 19.3%\nOpenAI gpt-4o-search-preview 5,789 9,775 1,985 20.3%\nOpenAI gpt-4o-search-preview-high 5,747 10,782 1,996 18.5%\nOpenAI gpt-4o-search-preview-high-loc 6,995 12,354 2,288 18.5%\nPerplexity sonar 4,513 38,266 2,794 7.3%\nPerplexity sonar-pro 3,911 37,016 2,602 7.0%\nPerplexity sonar-pro-high 6,768 72,644 5,167 7.1%\nPerplexity sonar-reasoning-pro-high 6,455 62,235 5,662 9.1%\nPerplexity sonar-reasoning 5,941 47,497 3,601 7.6%\nGoogle gemini-2.0-flash-grounding 5,185 17,060 1,324 7.8%\nGoogle gemini-2.5-flash-preview-04-17-grounding 3,359 23,886 1,583 6.6%\nGoogle gemini-2.5-pro-exp-03-25-grounding 5,920 27,446 2,489 9.1%\nGoogle gemini-2.5-pro-exp-03-25-wo-search* 1,240 0 0 0.0%\nTotal 65,768 366,087 32,865 9.0%\nexhibiting highly similar news domain citation patterns. Ex-\namining inter-family differences, we observe that OpenAI\nmodels demonstrate distinct citation patterns compared to\nGoogle and Perplexity models, with average cosine similar-\nity scores below 0.33. In contrast, Google and Perplexity\nmodels show moderate similarity to each other, achieving\naverage cosine similarity scores of around 0.53.\nOverrepresented News Domains\nTo identify overrepresented domains for a given model fam-\nily when compared to other model families, we leverage the\nlog-odds ratio informative Dirichlet prior method, originally\ndeveloped to compare text corpora (Monroe, Colaresi, and\nQuinn 2008). This Bayesian framework assumes the fre-\nquencies of different news domains follow certain multino-\nmial distributions and incorporates the background informa-\ntion of the typical frequency of each news domain into the\nmodel through Dirichlet priors.\nThe log-odds ratio \u03b4\u03b1\u2212\u03b2\ndof domain dbetween model fam-\nily\u03b1and model family \u03b2can be estimated by:\n\u03b4\u03b1\u2212\u03b2\nd= log\u0012n\u03b1\nd+nb\nd\nN\u03b1+Nb\u2212(n\u03b1\nd+nb\nd)\u0013\n\u2212 (2)\nlog \nn\u03b2\nd+nb\nd\nN\u03b2+Nb\u2212(n\u03b2\nd+nb\nd)!\n, (3)\nwhere n\u03b1\nd(n\u03b2\nd) is the number of citations to domain din\nmodel family \u03b1(\u03b2),nb\ndis the number of citations to domain\ndin the background corpus, N\u03b1(N\u03b2) is the total number of\ncitations in model family \u03b1(\u03b2), and Nbis the total number\nof citations in the background corpus.The variance and Z-score of \u03b4\u03b1\u2212\u03b2\ndcan be estimated as\n\u03c32(\u03b4\u03b1\u2212\u03b2\nd)\u22481\nn\u03b1\nd+nb\nd+1\nn\u03b2\nd+nb\nd,Z\u03b1\u2212\u03b2\nd=\u03b4\u03b1\u2212\u03b2\ndq\n\u03c32(\u03b4\u03b1\u2212\u03b2)\nd.\n(4)\nPositive values of Z\u03b1\u2212\u03b2\ndmeans domain dis over-represented\nin the cited domains of model family \u03b1when compared to\nmodel family \u03b2, and the threshold 1.96 can be used to esti-\nmate statistical significance with p <0.05.\nFor each model family, we compare its citation patterns\nagainst the combined patterns of the other two families. We\nuse the aggregated news domain citation frequencies across\nall three model families as the background corpus. Table 3\npresents the top 20 overrepresented news domains for each\nmodel family, along with their corresponding log-odds ra-\ntio Z-scores. All reported Z-scores exceed 1.96, indicating\nstatistical significance.\nWhen comparing these overrepresented domains with the\nmost frequently cited sources from the main analysis, we\nobserve that domains commonly cited by multiple model\nfamilies are filtered out, revealing more distinctive prefer-\nences for each model family. We find that some most fre-\nquently cited domains also exhibit high log-odds ratio Z-\nscores. For example, OpenAI models disproportionately cite\nreuters.com andapnews.com , Google models show a strong\npreference for indiatimes.com , and Perplexity models favor\nbbc.com .\nWe also include the political leaning and quality cate-\ngories of the news domains in the table. We can see that\nthese overrepresented domains are still overwhelmingly left-\nleaning and high-quality, comfirming the robustness of our\nmain findings.\nTable 3: Top 20 overrepresented news sources by model family, ranked by their log-odds ratio Z-scores. Political lean-\ning (columns \u201cL\u201d): L=left-leaning, C=center, R=right-leaning, U=unknown. Quality (columns \u201cQ\u201d): H=high-quality, L=low-\nquality, U=unknown.\nOpenAI Google Perplexity\nDomain Z-score L Q Domain Z-score L Q Domain Z-score L Q\nreuters.com 34.09 C H indiatimes.com 8.38 C L bbc.com 9.67 C H\napnews.com 22.22 C H livemint.com 6.62 C U nytimes.com 8.41 C H\nft.com 19.73 C H aljazeera.com 6.51 L H yahoo.com 7.70 C H\naxios.com 18.43 C H thehindu.com 6.17 L H espn.com 6.47 C H\ntime.com 11.43 C H dailymail.co.uk 5.31 C L cnn.com 6.07 C H\nas.com 10.63 C U independent.co.uk 3.79 C H cnbc.com 5.83 C H\ntheatlantic.com 8.64 L H history.com 3.58 C H sohu.com 5.80 C L\nelpais.com 7.67 L H ndtv.com 3.58 C H 163.com 5.77 C U\ntheguardian.com 6.03 C H hindustantimes.com 3.49 C H economictimes.com 5.53 C U\nbiomedcentral.com 4.64 L H forbes.com 3.45 C H techcrunch.com 5.42 C H\nlemonde.fr 4.59 L H healthline.com 3.37 C H nbcnews.com 5.15 C H\nasahi.com 2.66 L H cbr.com 3.20 C U usatoday.com 4.91 C H\nknowyourmeme.com 2.58 L H medicalnewstoda... 2.86 C H rbc.ru 4.79 L U\nipsnews.net 2.58 L U cbsnews.com 2.78 C H accuweather.com 4.71 C H\nmakeuseof.com 2.49 C H reliefweb.int 2.71 L U globo.com 4.70 L U\nscrippsnews.com 2.38 U U infobae.com 2.70 C H npr.org 4.69 C H\nwindowscentral.com 2.36 C U fool.com 2.63 C H dw.com 4.55 C H\npitchfork.com 2.11 L U timesofisrael.com 2.60 C H wsj.com 4.36 C H\nabc.net.au 2.11 C H dexerto.com 2.59 C U euronews.com 4.10 C H\nbizjournals.com 2.07 C H techradar.com 2.59 C H sina.com.cn 3.82 L U\nTable 4: User question topics discovered through topic modeling by BERTopic. Keywords represent the most relevant terms\nextracted using the KeyBERT algorithm. Topic labels provide human-readable descriptions of the discovered topics.\nTopic Keywords Label\n0 ai, ai model, models, model, google, API, search, web, data, code AI models and technology\n1 stock price, stock market, stock, stocks, market, investor, markets, in-\nvest, trading, investmentstock prices and market\n2 healthy, fat, diabetes, supplements, vegan, foods, health, eat, diseases,\ndiseasediet, nutrients, and health\n3 news latest, latest news, news today, today news, news, world news,\nlatest, tell latest, recent, todaynews updates\n4 matches, final, tournament, South America, teams, won, match, win-\nners, champions, continentsports and entertainment\n5 software engineer, engineer, graduated, career, actor, Bucharest, con-\ntact, mastodon, software, universitybiography and personal stories\n6 battle, battles, combat, strongest, vs, abilities, weapon, characters,\nweapons, mainfictional character battle analysis\n7 book, chapter, chapters, summary, summary latest, authors, wrote, read,\nold testament, testamentonline content and book\n8 lyrics, song, testimony, plead, songs, son, minor, rap, artist, tune music and lyrics\n9 tails, sonic, villain, spider, murderer, killed, evil, chaos, tyrant, nick-\nnamecomics and games\ngpt-4o-mini-search-previewgpt-4o-search-preview\ngpt-4o-search-preview-high\ngpt-4o-search-preview-high-locsonar\nsonar-pro\nsonar-pro-highsonar-reasoning\nsonar-reasoning-pro-highgemini-2.0-flash-grounding\ngemini-2.5-flash-preview-04-17-groundinggemini-2.5-pro-exp-03-25-groundinggpt-4o-mini-search-preview\ngpt-4o-search-preview\ngpt-4o-search-preview-high\ngpt-4o-search-preview-high-loc\nsonar\nsonar-pro\nsonar-pro-high\nsonar-reasoning\nsonar-reasoning-pro-high\ngemini-2.0-flash-grounding\ngemini-2.5-flash-preview-04-17-grounding\ngemini-2.5-pro-exp-03-25-grounding\n0.00.20.40.60.81.0\nFigure 7: Cosine similarity heatmap of cited news domains across models.\nQuestion Topics\nIn Table 4, we present the full list of question topics discov-\nered through topic modeling and their corresponding key-\nwords and labels. The keywords are identified using Key-\nBERT algorithm (Grootendorst 2020).\nRegression Analysis Results\nIn Table 5, we present the full linear regression analysis re-\nsults.\nTable 5: Regression coefficients for news source citation patterns. Coefficients show the relationship between features and news\nsource citation patterns. Statistical significance: *** p <0.001, **p <0.01, *p <0.05. Sample size: 9,098 observations.\nFeature % left-leaning news % right-leaning news % high quality news % low quality news\nIntercept 18.53*** 2.46*** 69.93*** 5.69***\nModel family: Google 3.95*** -0.92** -4.97*** 2.26***\nModel family: Openai -2.76** -0.37 2.31* -0.73\nNumber of citations -0.19* -0.04 0.04 -0.08\nNews sources percentage -0.06*** -0.02*** 0.16*** -0.03***\nResponse length (words, log Z-score) -0.23 0.75*** 1.41 0.13\nTotal turns -0.07 -0.02 -0.18 0.10\nTurn number 0.98** -0.05 0.47 -0.11\nQuestion length (words, log Z-score) -0.42 0.23 -0.10 0.32\nClient country/region: BR -0.08 2.30* -6.89* -1.03\nClient country/region: CA 3.36* -1.07 3.71* -1.02\nClient country/region: CN 7.53* 1.61 -0.29 3.29*\nClient country/region: DE 3.05 -0.28 4.25* -0.62\nClient country/region: GB 0.12 -0.04 1.96 -0.18\nClient country/region: HK -4.25 -1.02 7.08* -0.25\nClient country/region: IN 1.40 -0.56 -6.65*** 9.78***\nClient country/region: MX 10.29*** 5.54*** 5.58* 1.05\nClient country/region: other 1.73 -0.52 -0.11 -0.29\nClient country/region: RU 10.21*** -0.89 -0.46 0.27\nClient country/region: unknown 6.15*** 0.32 -2.37* -0.43\nEmbedding: PC 0 -5.69*** -0.70*** 2.25*** -0.90***\nEmbedding: PC 1 0.80 0.10 4.91*** 0.83***\nEmbedding: PC 10 1.07** -0.41*** -3.60*** 0.40*\nEmbedding: PC 11 -0.08 -0.11 1.81*** -0.13\nEmbedding: PC 12 0.08 0.40** 0.82* 0.14\nEmbedding: PC 13 -0.61 0.30* -0.89* 0.50**\nEmbedding: PC 14 2.60*** 0.12 0.55 -0.04\nEmbedding: PC 15 1.00** -0.08 0.41 -0.63***\nEmbedding: PC 16 0.26 0.34** 0.36 -0.08\nEmbedding: PC 17 0.07 0.58*** -2.09*** -0.82***\nEmbedding: PC 18 0.02 0.20 0.24 -0.35*\nEmbedding: PC 19 0.73* -1.40*** 1.71*** -0.61***\nEmbedding: PC 2 -1.89*** -0.02 -2.37*** -0.07\nEmbedding: PC 3 -5.16*** -0.54*** -2.54*** 0.10\nEmbedding: PC 4 -0.74 -0.09 -3.45*** -0.81***\nEmbedding: PC 5 1.05** -0.25* 0.93* -0.02\nEmbedding: PC 6 0.80* 0.32** -4.05*** 0.01\nEmbedding: PC 7 -0.76* -0.31* 0.67 -0.72***\nEmbedding: PC 8 -0.47 0.21 -1.11** -0.54**\nEmbedding: PC 9 -1.68*** -0.40** 0.50 0.24\nIntent: analysis -1.99 0.58 -1.08 -0.39\nIntent: creative generation -0.02 -2.26*** -1.75 -1.80\nIntent: explanation -0.27 -0.35 -1.39 0.16\nIntent: factual lookup 0.69 0.20 -2.85 -0.61\nIntent: guidance -6.49*** -1.07 -6.42** -1.30\nIntent: info synthesis -1.70 0.89 -2.37 -1.41\nIntent: other -1.24 0.22 2.84 -1.17\nIntent: text processing -0.77 3.68** -0.56 -2.13\nTopic: AI models and technology -0.58 0.02 2.42*** -0.34\nTopic: stock prices and market 0.37 0.11 -1.21*** 0.15\nTopic: diet, nutrients, and health 0.11 0.04 0.90* -0.09\nTopic: news updates -0.43 -0.14 -0.06 -0.16\nTopic: sports and entertainment -0.30 0.17 -0.05 0.26\nTopic: biography and personal stories 0.14 0.22* 0.01 0.24\nTopic: fictional character battle analysis 0.23 -0.12 -2.17*** 0.00\nTopic: online content and book 0.38 -0.78*** 1.64*** -0.42\nTopic: music and lyrics 0.05 0.27 -1.82*** 0.02\nTopic: comics and games 0.04 -0.20 -0.47 0.20\nR20.12 0.06 0.13 0.05", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "News Source Citing Patterns in AI Search Systems", "author": ["KC Yang"], "pub_year": "2025", "venue": "arXiv preprint arXiv:2507.05301", "abstract": "AI-powered search systems are emerging as new information gatekeepers, fundamentally  transforming how users access news and information. Despite their growing influence, the"}, "filled": false, "gsrank": 292, "pub_url": "https://arxiv.org/abs/2507.05301", "author_id": ["tqb96X8AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:CWzjN9wyDDkJ:scholar.google.com/&output=cite&scirp=291&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D290%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=CWzjN9wyDDkJ&ei=OLWsaJm1KI6IieoP0sKRuAk&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:CWzjN9wyDDkJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2507.05301?"}}, {"title": "Monitoring misinformation related interventions by Facebook, Twitter and YouTube: methods and illustration", "year": "2022", "pdf_data": "Working paper, 2022 -1, April 2022Monitoring misinformation related interventions by Facebook,\nTwitter and YouTube: methods and illustration.\nShaden Shabayek\u2217, H\u0013 elo\u007f \u0010se Th\u0013 ero\u2020, Dana Almanla\u2021, Emmanuel Vincent\u00a7\nApril 29, 2022\nAbstract\nThere is growing pressure for mainstream platforms, such as Facebook, Twitter or YouTube,\nto \fght misinformation by moderating the content that spreads on their site. We investigated\nthe interventions of the platforms by collecting social media data via APIs and scraping. These\ninterventions can be classi\fed into three broad categories: (i) temporary or permanent suspen-\nsion of users, (ii) displaying \rags and information panels, and (iii) reducing the visibility of\nsome content. We provide examples illustrating how researchers can monitor the interventions\nwithin each of the three categories for each platform. Finally, we discuss the restrictions to\naccess data and the lack of transparency regarding misinformation related interventions, and\nhow to help the academic community, NGOs and data journalists to successfully study online\nmisinformation.\n1 Introduction\nSection 230 in the United States Communications Decency Act provides immunity for website\nplatforms against the content created by users. Similar regulations exist in the European Union\nvia the E-commerce Directive (2000) in articles 12 and 15.1Nevertheless, there is growing pressure\nfor mainstream social media platforms, such as Facebook, Twitter or YouTube, to moderate the\navailable content. In particular, platforms take explicit actions when content is in violation of local\nlaws in di\u000berent jurisdictions, such as laws regarding defamation of a racial nature, dissemination\nof symbols from unconstitutional organizations, privacy protection, digital security, electoral laws.\nFor example, Facebook reports having implemented a total of 64 :7 thousand content restrictions\nbased on local law across all countries in 2020.2\nFurthermore, mainstream platforms are increasingly engaging in editorial-like tasks by imple-\nmenting targeted policies to insure that each platform's rules are not violated. Community guide-\nlines of Facebook, Twitter and YouTube can be summarized in a handful of categories, regarding\n\u2217Corresponding author: shaden.shabayek@sciencespo.fr, Sciences Po, m\u0013 edialab.\n\u2020thero.heloise@gmail.com, Sciences Po, m\u0013 edialab.\n\u2021dana.almanla@cri-paris.org, CRI.\n\u00a7emmanuel.vincent@sciencespo.fr, Sciences Po, m\u0013 edialab.\n1See section 4 in Bayer (2019) [1] for a comprehensive overview of the mentioned articles.\n2See Facebook Transparency Center, Content restrictions based on Local Law:\ntransparency.fb.com/data/contentrestrictions. We summed the count of content restrictions over all countries\nreported in the table, for H1 and H2 of the year 2020.\n1\nWorking paper, 2022 -1, April 2022safety, privacy and authenticity; which include sub-categories such as violence, terrorism, child sex-\nual exploitation, abuse, harassment, hateful conduct, suicide or self-harm, illegal or regulated goods\nand services, platform manipulation and spam (see Appendix 6.1 for references). While speci\fc to\neach platform, the previously cited categories correspond in most cases to well de\fned concepts that\nfall into legal frameworks in many countries, unlike misinformation (see Fathaigh et al. (2021) [3]\nfor a discussion for the case of the EU). The intricacies of constructing a legal framework for misin-\nformation arises from the di\u000eculty of identifying and qualifying a piece of online content as false or\nmisleading, among an overwhelming quantity of daily produced content, without infringing existing\nlaws.3In particular, a number of recent studies point towards the idea that misinformation is a\nsmall subset of the total supply of information on online social networking platforms (e.g. Grinberg\net al. (2019) [5] and Broniatowski et al. (2020) [2]). Yet, this seemingly small subset is generating\ngreat concern in traditional media and in society in a broader sense.4\nHence, in the present article, we focus on mainstream platforms' policies and interventions\nregarding content with low credibility or false information, commonly referred to as Fake News (see\nLazer et al. (2018) [7]). The misinformation phenomenon is still ill-de\fned, as it encompasses several\ncombined features such as spreading inaccurate, false or misleading information, with or without the\nintention of in\ruencing or manipulating a target pool of audience. The growth of social networking\nplatforms over the last decade in terms of number of users worldwide and volume of content,\nhas modi\fed the information ecosystem in terms of production of information and its mediation.\nMany users can now produce and share content which includes news related information, without\nhaving to abide by strict editorial processes that ensure accuracy of information and reliability\nof sources. In particular, false or inaccurate content produced and shared on social networking\nplatforms concerning the political life or public health may have a potentially harmful impact on\nthe society when it goes viral. This gave rise to a set of heterogenous policies across mainstream\nplatforms, mainly based on fact-checking but also content moderators and users' reporting. For\nexample, Facebook has a substantial partnership program with Fact-checking partners certi\fed by\nthe non-partisan International Fact-Checking Network. The company uses a number of signals and\nmachine learning models to predict misinformation and surface it to fact-checkers.5Twitter seems\nto have a di\u000berent approach where they focus on providing context rather than fact-checking6and\nthe platform is testing a new system based on the wisdom of the crowds to tackle misinformation (see\nTwitter Birdwatch). As for YouTube, this platform utilizes the schema.org ClaimReview markup,\nwhere fact-checking articles created by eligible publishers can appear on information panels (see\nAppendix 6.1 for references).\nDuring the COVID-19 global health pandemic, platforms have upgraded their guidelines to\ninclude a set of rules to tackle the propagation of potentially harmful content (see Appendix 6.1\nfor references). Those policies are enforced via existing actions used by the platforms to tackle\n3See A guide to anti-misinformation actions around the world on the website of Poynter Institute.\n4For example see the February 2020 speech of the Director General of the WHO at the Munich Security Con-\nference, where he says \\But we're not just \fghting an epidemic; we're \fghting an infodemic.\" Furthermore, the\nEuropean commission recognizes the spread of online disinformation as a problem and has put together in 2018 a\nCode of practice on Disinformation, which is a set of self-regulatory standards to \fght disinformation.\n5See the section Frequently asked questions: `How does Facebook use technology to detect potential misinforma-\ntion?\"\n6To the best of our knowledge, Twitter does not have a page which summarizes its fact-checking strategy. The\nTwitter Safety Team tweeted on June 3, 2020 the following: \\We heard: 1. Twitter shouldn't determine the\ntruthfulness of Tweets 2. Twitter should provide context to help people make up their own minds in cases where\nthe substance of a Tweet is disputed. Hence, our focus is on providing context, not fact-checking.\" Tweet ID\n1267986503721988096.\n2\nWorking paper, 2022 -1, April 2022other rules' violations, such as: labelling content to provide more context or indicate falsehood,\npublishing a list of terms or topics that will be \ragged, suspending accounts, implementing strike\nsystems, reducing the visibility of content, etc. As each platform is a private company, those\npolicies are generally not coordinated and are implemented in di\u000berent ways across platforms. Such\ntargeted policies show the willingness of mainstream platforms to enhance the quality of the online\nconversation, but also sheds light on the lack of speci\fc policies to tackle misinformation in general.\nThe 2019 report of the Facebook Data Transparency Advisory Group (DTAG) states that \\ DTAG\nwas not tasked with evaluating any of the following: (...) Facebook's policies with respect to \\fake\nnews\" or misinformation, as neither of these categories were counted as violations within the \frst\ntwo versions of the Community Standards Enforcement Report \". In particular, policies regarding\nmisinformation are generally not part of the set of platform rules or community guidelines (as of\nJuly 2021).\nMisinformation related interventions by mainstream platforms are hard to monitor, study or\nverify by third parties (e.g. academic community, data journalists, NGOs), as in many cases mis-\nleading or false content does not qualify as a violation of a given platform's rules and to date, it does\nnot explicitly appear as a separate category in available transparency reports. This makes the study\nof online misinformation, the assessment of the impact of platforms' actions to tackle misinforma-\ntion and their relevancy a burdensome task for the academic community. Hence, in the present\narticle we explain how to verify mainstream platforms' current actions regarding content with low\ncredibility or false information with data mining. We do so by providing a series of examples for\ndi\u000berent interventions and platforms. We chose to focus on three platforms: Facebook, Twitter and\nYouTube. Both Facebook and YouTube are in the top three most popular social media platforms\nin terms of number of users.7We further choose Twitter because it is a social networking platform\nwith the most news-focused users, according to the Pew Research center (2019) [6]. To collect data\nfrom these three platforms, we either used the APIs (Application Programming Interfaces), or web\nscraping, i.e. retro-engineering the HMTL code of a web page to extract meaningful data, see Table\n1 for a summary. Minet [18], a webmining tool developed by the SciencesPo m\u0013 edialab, was often\nused, and we scripted our own data mining code when it was necessary.\nApplication Programming Interface (API) Web Scrapping\nCrowdTangle API and Buzzsumo API Code created for this article\nTwitter API V2 minet tw scrape\nYouTube API V3 Code created for this article\nTable 1: Summary of ressources used for data collection.\nMore speci\fcally, in this article, we survey a number of common policies against misinformation\nused by Facebook, Twitter and YouTube. We classi\fed those common policies into three broad\ncategories: ( i) temporary or permanent suspension of users, ( ii) informing users with \rags and\nnotices, and ( iii) reducing the visibility of some content. We compile in Table 2, a list of links that\nredirect to the policies, regulations and transparency centers of Facebook, Twitter and YouTube\nthat we discuss throughout the article. Furthermore, the examples provided to illustrate how to\nmonitor a given policy were picked out of a list of domain names with several failed fact-checks. To\nbe more precise, some domain names with failed fact-checks were picked because a given platform\n7See for example the ranking of the most popular social networks as of April 2021 on Statista:\nhttps://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/.\n3\nWorking paper, 2022 -1, April 2022communicated about an intervention or because an intervention (e.g. suspension) was announced\non the social media accounts linked to a given domain name. Finally, we discuss how an increased\ne\u000bort of transparency regarding speci\fc content can help the community of researchers study and\nassess the impact of platforms' policies regarding misinformation.\n2 Temporary & permanent suspension\nMainstream social media platforms may suspend the account of a speci\fc user when they deem\nthat the platforms' rules have been violated. Account suspension can be temporary or permanent.\nWhen the suspension is temporary the user is prohibited for a limited period of time from posting\ncontent on their account, but content created prior to suspension remains available to the user and\ntheir followers. However, when the suspension is permanent, in most cases, followers or subscribers\nno longer have access to the content prior to the suspension and the user can no longer use the\naccount to create new content. In what follows, we focus on the implementation of this policy by\nFacebook, Twitter and YouTube, and we provide simple examples to illustrate.\n2.1 Facebook\nWhen an account is permanently suspended by Facebook, it disappears from the platform. That is,\nthe data can no longer be scrapped and it also disappears from the CrowdTangle API.8Facebook\npublishes on monthly basis a coordinated inauthentic behavior report, where it informs how many\npersonal accounts, pages or groups were deleted and to which deceptive network they may have\nbelonged to.9But as long as external persons do not have access to historical data of deleted\naccounts, these reports cannot be veri\fed by the academic community, NGOs and journalists.\nFigure 1: Number of Facebook posts published each day by the Facebook page Donald J. Trump between January\n1, 2020 and June 15, 2021. The data corresponds to 6 083 posts retrieved from the CrowdTangle API using the posts\nendpoint.\nFacebook can also apply a temporary suspension, and in this case the data can often be collected\nand analyzed. For example, Donald Trump's o\u000ecial Facebook page has been suspended following\nthe Capitol attack on January 6, 2021.10Nevertheless the page's data is still present in the Crowd-\nTangle API. Hence, we collected the 6 083 posts it had published between January 1, 2020 and\n8CrowdTangle is a public insights tool owned and operated by Facebook, that exclusively tracks public content\nfrom Facebook public groups and pages.\n9See the April 2021 report as an example.\n10See https://www.facebook.com/zuck/posts/10112681480907401\n4\nWorking paper, 2022 -1, April 2022June 15, 2021 using the posts endpoint.11The minet Python library [18] was used to collect the\ndata. We can verify on Figure 1 that the Donald J. Trump page has not published any content\nsince January 6, 2021, and that this behavior is not consistent with the page's previous activity:\nan average of 16 posts were published each day on Facebook before the suspension.\n2.2 Twitter\nTwitter has implemented a strike system as part of their Civic Integrity Policy and their COVID-19\nmisleading information policy (see Table 2 in Appendix 6.1). Violations of both policies can entail\nstrikes, where two strikes lead to a 12-hour account lock and \fve or more strikes lead to permanent\nsuspension from the platform. A list of notable Twitter temporary and permanent suspensions can\nbe found on Wikipedia.12The 12-hour account lock is hard to observe in the data, especially for\nusers who do not have an over the clock regular tweeting activity.13In this section, we provide one\nexample of a temporary suspension of a Twitter account, that seems to be the result of a manual\ndecision concerning a Tweet which violated the rules.\nThe Twitter account @ LifeSite of the website lifesitenews.com has been suspended for at least\ntwo periods of time: from end of 2019 until fall 2020 for 308 days, then again since January 2021 for\nhaving violated Twitter Rules14. In particular, this website has several failed fact-checks concerning\nthe published articles, according to Media Bias Fact Check and open.feedback.org.15. We collected\nthe activity (Tweets, Replies, Quotes, Retweets) on their Twitter account via the Twitter API,\nusing the historical search endpoint. We then plotted the number of Tweets, Retweets, Quotes and\nReplies per day, as shown in the left panel of Figure 2. The two periods of temporary suspension are\nclearly observed in the data as the user(s) of the account were not allowed to use the functionalities\nof the Twitter Platform.\nFigure 2: Left Panel: number of Tweets (including Retweets, Replies, Quotes) per day of the Twitter account\n@Lifesite linked to the website lifesitenews.com from January, 2019 until April 2021. Right Panel: number of\nTweets (including Retweets, Replies, Quotes) per day that have shared a lifesitenews.com URL link from January,\n2019 until April 2021.\nTo further assess the impact of this double temporary suspension, we collect via the Twitter API\nv2, all the Tweets that have shared during the same period a url link containing lifesitenews.com.\n11See the endpoint documentation for more details: https://github.com/CrowdTangle/API/wiki/Posts.\n12See https://en.wikipedia.org/wiki/Twitter suspensions.\n13The following tool https://makeadverbsgreatagain.org/allegedly/ provides an overview of the daily and hourly\ntweeting activity, including repetition of Tweets for any given Twitter account.\n14See Lifesitenews's article discussing the reason for the suspension: https://www.lifesitenews.com/news/lifesite-\nis-dumping-twitter-and-so-should-you. Twitter rules can be found at: https://help.twitter.com/en/rules-and-\npolicies/twitter-rules.\n15See https://mediabiasfactcheck.com/life-site-news/ and https://open.feedback.org/media/CM.\n5\nWorking paper, 2022 -1, April 2022The right panel of Figure 2, shows that during both periods of temporary suspension, other users still\nshared lifesitenews.com links and that the level was only slightly below the tweeting and retweeting\nlevels prior to the \frst temporary suspension. More speci\fcally, there was an average of 960 Tweets\n(including Retweets, Replies, Quotes) per day over the \frst temporary suspension period of 308\ndays from December 9, 2019 until October 12, 2020, against an average of 977 Tweets (including\nRetweets, Replies, Quotes) per day during the exact same period one year earlier.\nFinally, suspending a Twitter account is an intervention which aims at penalizing users, in order\nto make them respect the Twitter rules. In the case of Twitter accounts linked to domain names,\nthe activity of the account usually consists in spreading on social media articles published on their\nwebsite. The right panel of Figure 2 points towards the limitations of suspending the Twitter\naccount @ LifeSite , because the articles published on lifesitenews.com were still being actively\nshared by other Twitter users.\n2.3 YouTube\nIn this section, we turn to the suspension policy of YouTube. When a YouTube channel publishes\na video that violates the community guidelines for the \frst time, it will usually receive a warning.\nBut for the second violation, the channel receives its \frst strike. A strike means that users are not\nallowed to post content on their YouTube channel for one week. A strike remains associated to a\nchannel for 90 days, and if a second strike occurs in the same 90-day period as the \frst strike, the\nsuspension will then last for two weeks. A third strike results in the termination of the channel.16\nTo illustrate the implementation of this policy, we use the following YouTube channels as two\nexamples of one-week suspensions: One America news Network and Tony Heller.\nThe website of One America News Network has a \\low\" factual reporting score according to the\nwebsite Media Bias Fact Check and several inaccurate articles according to open.feedback.org.17\nAccording to NBC news (2020) [4], the YouTube channel received a \frst strike on November 24,\n2020 for the promotion of a false cure for COVID-19. We collected the number of video uploaded\nand the number of view of this channel using the YouTube API v3, between November 2020 and\nJanuary 2021. For the video counts, we used the playlists endpoint to retrieve the videos uploaded\nwith their publishing date and for the view count we used the IDs of the videos we had from the\nplaylists, then via the videos endpoint we retrieved the view counts on June 2021.\nFigure 3: Left Panel: Number of YouTube videos uploaded each day by the youtube channel One America news\nNetwork November 1, 2020 and January 1, 2021. Right Panel: accumulated view counts for videos. The metrics\ncorrespond to the videos' publishing date and the data is retrieved from the YouTube API with the playlists and\nvideos endpoints.\n16https://support.google.com/youtube/answer/2802032.\n17See mediabiasfactcheck.com/one-america-news-network/ and https://open.feedback.org/media/5M.\n6\nWorking paper, 2022 -1, April 2022Figure 3 shows the daily number of videos uploaded by the channel, and its number of views.\nWe can clearly see the suspension period in the data, as the number of published video is down\nto zero for one week after the strike on November 24, 2020. Because no videos were published\nduring this period, the number of views were also nul. When comparing one month before and\nafter the suspension, the view count decreased by -73%, indicating that the videos uploaded after\nthe suspension were less popular. Furthermore, the activity of the channel was close to zero after\nMarch 2021, and OANN's Twitter account has announced their migration from YouTube to Rumble\non March 17, 2021 (see the left panel of Figure 4).\nFigure 4: Left Panel: Tweet announcing OANN moving to Rumble, Twitter ID 1372238828425998336. Right Panel:\nTony Heller's tweet after getting temporally suspended from YouTube.\nTony Heller writes regularly blog posts on the website realclimatescience.com , which also has\na \\low\" factual reporting score according to the website Media Bias Fact Check.18He tweeted\non September 29, 2020 about his YouTube channel being `shut down' because of a video about\nan anti-covid-lockdown doctor getting arrested (see the right panel of Figure 4). We applied the\nsame methods as in the previous example to collect data. Again a one-week suspension period from\nSeptember 29 until October 5 can be observed clearly in the daily number of uploaded videos and\nof views (see Figure 5). Comparing one month before and after the suspension, this channel also\nwitnessed a drop of view counts by \u000070%.\nFigure 5: Left Panel: Number of YouTube videos uploaded each day by the YouTube channel Tony Heller between\nSeptember 1, 2020 and November 15, 2020. Right Panel: accumulated view counts for videos uploaded by the same\nYouTube channel. The date corresponds to the videos' publishing date.\nAs most YouTube channels upload videos on a weekly or even a monthly basis, a one-week\nsuspension does not appear very restrictive at \frst sight. But we observed that the intervention\n18See mediabiasfactcheck.com/real-climate-science/\n7\nWorking paper, 2022 -1, April 2022was followed by a reduction in the number of view for the two channels investigated, with one\nchannel even stopping its YouTube activity in the following months. Although more research is\nneeded, it is possible that the one-week suspension following a strike might reduce the reach of\nactive YouTube channels.\n3 Flags, Notices and information panels\nMainstream platforms such as Facebook, Twitter and YouTube can resort to providing more context\nto users regarding a speci\fc post, Tweet or video. This type of intervention takes di\u000berent formats\naccording to the platform and also has di\u000berent denominations. Facebook for example refers to\nthis intervention by mentioning \\\rags\", while Twitter refers to \\notices\" and \\interstitials\" and\nYouTube uses the term \\information panels\". In this section, we explain the speci\fcs of this\nintervention for each platform and illustrate with examples.\n3.1 Facebook\nTo the best of our knowledge, two types of \rags can currently be displayed on Facebook posts: ( i)\ninformation banners which do not refute the message in the post and provide a link which redirects\nto an authoritative source, such as \\Visit the COVID-19 Information Centre for vaccine resources\"\nand ( ii) fact-check \rags that assigns a \\rating\" for a text or a link in a given post, such as \\False\ninformation Checked by independent fact-checkers\" (see Figure 6). The fact-check \rags can display\none of the following ratings : \\False\", \\Partly false\", \\Missing context\", \\False headline\", \\Altered\nmedia\", \\Opinion\", \\Satire\", \\Not eligible\" and \\True\".19\nFigure 6: Examples of Facebook posts having shared a link fact-checked as False by one of Facebook's partners.\nScreenshots taken on July 8, 2021.\nNo information or available \felds regarding the Facebook \rags can be found on Buzzsumo or\nCrowdTangle, the two APIs we use to access Facebook data. The only way to verify Facebook's\n\ragging policy is thus via scraping publicly available content.\nWe \frst searched for all the Facebook posts having shared a speci\fc link20rated as `False' by\nScience Feedback, a fact-checking organization which partners with Facebook. To do so, we used\nthe `search' endpoint in CrowdTangle API. Twenty Facebook posts were collected this way, and\nwe scripted an adapted scraping code to verify whether they contained an information banner, a\nfact-check \rag, both or none. Three posts were unavailable, and thus could not be categorized\nby the scraper. As Science Feedback has informed Facebook that they have assigned the rating\n\\False\" for this link, we expected that all these posts would contain a fact-check \rag, but we had\nno expectations for the information banner.\n19See https://www.facebook.com/business/help/341102040382165.\n20Click here to access the link and its fact-check by Health Feedback.\n8\nWorking paper, 2022 -1, April 2022\nFigure 7: Count of di\u000berent \rags for 17 Facebook posts who have shared the exact same link, fact-checked False\nby one of Facebook's partners.\nSurprisingly only 11 posts out of the remaining 17 had a \\False information\" \rag (see Figure 7\nand left panel of Figure 6 for an example). We observed that the \ragged posts were also the ones in\nwhich the false link was expanded (i.e., a banner was visible with an image of the link and that can\nbe clicked on, see the middle and right panels of Figure 6 for examples). As the `False information'\n\rag is applied on the link banner, and not on the link itself, a user is thus able to share a \\False\"\nlink on Facebook without the \\False\" \rag if the link is not expanded.\nWe also observed that most of the posts sharing this \\False\" link (13 out of 17, Figure 7) had\nan information banner saying \\Visit the COVID-19 Information Centre for vaccine resources\" (see\nthe \frst two examples of Figure 6). We could not identify why the banner was applied on some\nposts and not on others (we saw no clear di\u000berence in the post messages for example). It should be\nnoted that some posts displayed both the information and the fact-check banners, as in the middle\npanel of Figure 6.\n3.2 Twitter\nAlongside other social networking platforms, when the content of a Tweet violates the Twitter rules,\na notice can be added to provide more context according to Twitter's Help Center. At the Tweet\nlevel, notices take the form of a label or an interstitial. Labels are context speci\fc (e.g. COVID-19\nor presidential elections) and redirect users to a webpage to get more context, for example \\Get the\nfacts about COVID-19\" (see right panel of Figure 8). Interstitials are presented as a greyed box on\ntop of a Tweet, which indicates sensitive content, violations of Twitter rules, withheld Tweets for\nviolation of local laws or even Tweets from suspended accounts, for example \\The following media\nincludes potentially sensitive content\" (see left panel of Figure 9). At the account level, notices can\nalso indicate whether an account has been temporarily or permanently suspended.\nTo the best of our knowledge, when using the Twitter API v2, there is no \feld which indicates\nwhether a Tweet contains a notice or not; while the interstitial \\possibly sensitive\" and \\withheld\"\nare both Tweet \felds that can be recovered21from the Twitter API v2. Hence, in order to investigate\nthe presence of notices of all types, we resorted to scrape publicly available Tweets, using minet [18].\n21See https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet.\n9\nWorking paper, 2022 -1, April 2022\nFigure 8: Two Tweets sharing the same link marked as \\False\" by Science Feedback, screenshots taken on June\n20, 2021. Left Panel: Tweet without an information notice. Right Panel: Tweet containing an information notice.\nFigure 9: Left Panel: Tweet containing a URL link hidden behind an interstitial. Right Panel: when clicking on\nview, to view the content hidden behind the interstitial in the left panel. Screenshots taken on July 1, 2021.\nThis tool was recently enriched upon our request in order to capture whether a Tweet contains a\nnotice or not, via the \\minet twitter scrape\" command.\nIn this section, we take a deeper look at how notices and interstitials are introduced by Twitter,\nto add context to potentially inaccurate or misleading content. To that end, we gathered a set of\n3 094 links redirecting to articles which were marked as \\False\" between April 2019 and February\n2021 by Science Feedback, a fact-checking organization verifying the credibility of science-related\nviral information. As a second step, we collected on June 30, 2021 all the Tweets that have shared\nat least one link out of the set of 3 094 links marked as False . This data collection resulted in\n323 938 Tweets, excluding Retweets. Only 28 Tweets contained the notice \\Get the facts about\nCOVID-19\" , 5 Tweets contained the notice \\Learn about US 2020 election security e\u000borts\" and only\n1 had the following notice \\This claim about election fraud is disputed\" . Furthermore, we noticed\nthat the labeling rule might not be applied uniformly on a given set of Tweets sharing the exact\nsame link, among the set of collected Tweets. To give an example, 657 Tweets had shared the exact\n10\nWorking paper, 2022 -1, April 2022same link redirecting to a video on Bitchute, entitled Important information on coronavirus 5G\nKung Flu . Among those 657 Tweets only 3 contained the notice \\Get the facts about COVID-19\"\n(see Figure 8). This points towards the non-automation of the Tweet labelling process and that it\nmight be that these 3 Tweets were the only ones reported by other users.\nWe further examine the placement of interstitials that indicate a possibly sensitive content. We\n\fnd that only 9 344 (2 :97%) Tweets out of 323 938 containing a link marked as False, have an\ninterstitial \\potentially sensitive content\" . To check whether the interstitial \\potentially sensitive\ncontent\" is automated or not, we pick out one Tweet having shared a link22marked as \\False\", which\nalso contains the interstitial potentially sensitive content (see Figure 9). Among the 32 Tweets that\nhave shared the exact same link, only 5 tweets had the interstitial \\potentially sensitive content\".\nAgain this points towards the non-automation of interstitials placement based on a speci\fc URL.\nTo date, no data is available to verify whether some interstitials or notices were placed based on:\nTweet reporting by other users, or Twitter moderators, or algorithmic rules. Furthermore, notice\nthat the appearance of interstitials may depend on the settings of a user's account and country or\nlanguage speci\fc regulations. In particular, in the settings of Twitter account, one can deactivate\nthe display of interstitials for sensitive content , by ticking the box Display media that may contain\nsensitive content in the section content you see .\nFigure 10: Summary of the number of Tweets among the set of 323 938 Tweets containing a link marked as False,\nby Science Feedback, which contain a notice or a Possibly sensitive interstitial or none.\nFinally, the 9 344 Tweets containing the interstitial \\potentially sensitive content\" widely out-\nnumber the 34 tweets which contain a notice (see Figure 10). We noticed that among the Tweets\nwhich contain the interstitial \\potentially sensitive content\" , there are Tweets which contain links\nredirecting to misleading articles about COVID-19 and yet they do not contain the label \\Get\nthe facts about COVID-19\" . We also noticed that the 9 344 Tweets which contain the intersti-\ntial\\potentially sensitive content\" do not necessarily contain graphic violent content23but rather\n22The link marked as False redirects to the article 5G Technology and induction of coronavirus in skin cells { US\nNational Library of Medicine (what David Icke has been saying for months) and can be found here.\n23The Twitter Help center explains the placement of interstitials of possibly sensitive content as follows: \\We may\nplace some forms of sensitive media like adult content or graphic violence behind an interstitial advising viewers to\n11\nWorking paper, 2022 -1, April 2022links that redirect to articles marked as \\False\" by Science Feedback. When navigating through\nthe Twitter Help Center, we found no explanation for this large discrepancy between using the\ninterstitial \\potentially sensitive content\" and notices which provide more context to users.24\n3.3 YouTube\nYouTube provide information panel at the top of search results or under a video related to topics\nprone to misinformation. Information panels provide more context on a controversial matter, with\na link to an independent, third-party partner's website such as Wikipedia or the World Health\nOrganization. Importantly, YouTube states that these panels exist regardless of the point of view\nexpressed in a given video. YouTube also speci\fes that information panels may not be available in\nall countries/regions and languages.25\nTo further study the assignment of information panels to videos, we compiled a list of 170\nYouTube videos, fact-checked by Science Feedback between April 2019 and March 2021 and marked\nas containing inaccurate information. We collected the information panels, when present, under\neach video by scraping the content of the web page. Three types of information panels were found\nregarding COVID-19, COVID-19 Vaccine, and climate change, as shown in Figure 11.\nFigure 11: Examples of YouTube information panels displayed under Youtube videos respectively about: COVID-\n19, COVID-19 Vaccine and climate change. The videos were accessed on September 2, 2021.\nAlmost half (46 %) of the videos containing misinformation were shown without any information\npanel (Figure 12). For the rest of videos, the most frequent information panel concerned COVID-19\n(42 %), followed by COVID-19 vaccine (9 %). It is possible that YouTube is currently focusing\nits e\u000borts on the current `hot' controversial topics with a potentially wide public impact, leaving\nroom for misinformation related to other topics to spread, but more research is needed to formally\ncompare misinformation videos about COVID-19 and about other subjects.\nbe aware that they will see sensitive media if they click through\".\n24Twitter Safety account announced in a Tweet (see Tweet ID : 1379515615954620418) on April 6, 2021 that their\nteam \\will begin deploying automated tools to build on (their) e\u000borts to label Tweets that may contain misleading\ninformation around COVID-19 vaccinations\".\n25https://support.google.com/youtube/answer/9004474\n12\nWorking paper, 2022 -1, April 2022\nFigure 12: Summary of the number of YouTube videos which contain an information panel, among the set of videos\nmarked as false by Science Feedback.\nFigure 13: Screenshots showing two duplicates of the same video, taken on July 21, 2021. Left panel: a YouTube\nvideo about COVID-19 without an information panel. Right panel: a duplicate of the same video uploaded under a\ndi\u000berent title and with a `COVID-19 vaccine' information panel.\nWe noticed that the same video was sometimes uploaded multiple times and thus appeared on\ndi\u000berent YouTube links. One YouTube link might contain an information panel while its duplicates\ndid not (see Figure 13 for an example). In addition, we noticed that some keywords in the video titles\nwere often associated with an information panel, such as `COVID-19', `coronavirus', `pandemic' and\n`testing', while some notational variants like `C O V I D 19' or `Cv19' were not associated with a\npanel. Therefore, we suspect that YouTube is automatically adding information panels based on the\npresence of certain keywords in the video title (and maybe in the video description), independently\nof the content of the video. Note that in Figure 13, the video with `C.O.V.I.D.19' in its title had an\ninformation panel, hinting that YouTube might adapt its list of suspect keywords as new notational\nvariants for COVID-19 appear, or that some information panels might be added manually (by\ncontent regulators or when a video is \ragged by many users).\n13\nWorking paper, 2022 -1, April 20224 Reducing the visibility\nMainstream social media platforms can reduce the visibility of content created or shared by users,\nwhenever they violate the platforms' rules. The implementation of this policy varies across platforms\nand is not easy to verify ex-post. In what follows we provide indirect methods to explore how\nreducing the visibility of content works on Facebook, Twitter and YouTube.\n4.1 Facebook\nTo tackle misinformation, Facebook can reduce the spread of misleading content through their\nbuilt-in ranking system. More speci\fcally, Facebook ranks each post and/or ad by assigning to\nit a relevancy score, where a high score leads to a high likelihood of the post and/or the ad to\nappear on a user's newsfeed. Doing so, Facebook can make a post or a whole account less visible\nby decreasing the relevancy score of its content; this is precisely the reduce measure (see Facebook\nNewsroom (2018) [8]). This measure can be veri\fed by looking at the number of views (reach) of a\npost, but this metric is not available via the APIs used to access Facebook data: CrowdTangle or\nBuzzSumo. Hence we can indirectly investigate the reduce measure by looking at the engagement\nmetrics (likes, comments, shares) related to a given post; which are available on CrowdTangle and\nBuzzSumo. If a post reaches less users because it has a lower ranking, then it is less likely to receive\nlikes, comments and shares, relative to a post with a higher ranking.\nTo illustrate, we investigate the case of the website Infowars . This website appears in the\nMisinformation Directory of FactCheck.org, among other websites who have posted deceptive con-\ntent26. Furthermore, the factual reporting of Infowars has been rated very low by the website\nMedia Bias Fact Check and it has several failed fact-checks reported by open.feedback.org.27\nOn May 2, 2019, Facebook announced they would prohibit users from sharing Infowars con-\ntent unless, they are explicitly condemning the material.28To verify the measure, we used the\n\\/posts/search\" endpoint29of the CrowdTangle API, to collect 37 242 Facebook public posts that\nhad shared a URL link containing \\infowars.com\", published between January 1, 2019 and Decem-\nber 31, 2020.30\nThe number of public posts sharing an Infowars link remained globally stable throughout\n2019 (see Figure 14 left panel). Thus the measure announced by Facebook does not seem to\nhave prevented users from sharing an Infowars link. Nevertheless, a clear drop in engagement was\nobserved on May 2, 2019 (see Figure 14 right panel). The number of reactions, shares and comments\nper post have decreased respectively by \u000094%,\u000096% and \u000093% when comparing the two months\nbefore and after May 2, 2019. This suggests the measure taken by Facebook in May 2019, is not a\nban per se, but rather a reduce measure. This is because users could still post Infowars links, but\nthese posts generated less engagement. It should be noted that the engagement metrics increased\n26See https://www.factcheck.org/2017/07/websites-post-fake-satirical-stories/.\n27See https://mediabiasfactcheck.com/infowars-alex-jones/ and open.feedback.org\nhttps://open.feedback.org/media/B9.\n28See the article in Wired by Martineau (2020) [9] and see the section So what happened with InfoWars? They\nwere up on Friday and now they are down? about.fb.com/news/2018/08/enforcing-our-community-standards/.\n29see the documentation for more details: github.com/CrowdTangle/API/wiki/Search.\n30We found in the collected data some Facebook posts that did not directly share an Infowars link (but rather\na YouTube or Facebook video containing an Infowars link in its description), thus we excluded such posts from our\ndata to keep only the 27 721 posts directly sharing an Infowars link.\n14\nWorking paper, 2022 -1, April 2022\nFigure 14: Public Facebook posts sharing an Infowars link in 2019 and 2020 collected from the CrowdTangle API.\nThe red line marks the date of May 2, 2019, when Facebook has announced the ban regarding Infowars. Left panel:\nNumber of daily posts. Right panel: Engagement metrics: average number of reactions, shares and comments per\npost.\nagain by the end of 2019 / beginning of 2020, suggesting that the reduce measure may have been\nlifted a few months after its implementation.\nFurthermore, Facebook has also announced on December 20, 2019 that \\The BL is now banned\nfrom Facebook\".31The Beauty of life (thebl.com/) is a US-based media company operated by\nThe Epoch Times that shares pro-Trump views and conspiracy theories such as QAnon.32To\nverify Facebook's ban, we \frst tested whether we could post a Facebook message containing a url\nfrom thebl.com. This turned out to be impossible. But such manual veri\fcation cannot inform\nus whether the ban applies indeed to all Facebook users and accounts (as we used only our own\npersonal accounts), nor when it has started. To further investigate this policy, we collected data\nfrom the BuzzSumo API33. We used the \\/search/articles\" endpoint to collect the engagement\nmetrics of the 13 634 articles crawled from the thebl:com website between January 1, 2019 and\nJune 15, 2021.\nFigure 15: Articles from The Beauty of Life website (thebl.com) published between January 1, 2019 and June 15,\n2021 and gathered from the Buzzsumo API. Left panel: Facebook engagement metrics (average number of reactions,\nshares and comments per article). Right panel: Number of articles published per day. The red line marks the date\nof December 1, 2019.\nThe number of Facebook reactions, shares and comments dropped to zero for TheBL's articles\npublished after December 1, 2019 (see Figure 15 left panel), indicating the start of the ban. We\nnote that although the ban was communicated in an article published on December 20, 2019, it\n31https://about.fb.com/news/2019/12/removing-coordinated-inauthentic-behavior-from-georgia-vietnam-and-the-us/\n32See wikipedia article.\n33BuzzSumo is a commercial content database that tracks the volume of user interactions with internet content\non Facebook, Twitter, and other social media platforms.\n15\nWorking paper, 2022 -1, April 2022seems to have actually started on December 1, 2019. The communication around the ban appeared\nto have discouraged The Beauty of Life to proceed with their activity. Indeed the number of articles\nthey published daily was around 50 until December 20, 2019, when it decreased drastically to reach\naround 5 to 10 articles published per day (see Figure 15 right panel).\nUsing BuzzSumo data, we ascertained that links from thebl.com were not shared on Facebook\nanymore. The ban started on December 1, 2019, and appeared to be still enforced in June 2021.\n4.2 Twitter\nTwitter can take action against a Tweet which violates the Twitter rules, by limiting its visibility34\non users' timelines and in search results. To illustrate we provide an example for the website\nglobalresearch:ca , which has several failed fact-checks according to Media Bias Fact Check, a\nwebsite which provides a measure of factual reporting of several media outlets based on available\nfact-checks.35\nFigure 16: Screenshots taken on June 14, 2021. Left Panel: screenshot that shows that no results can be found\nwhen searching for articles linked to the domain name globalresearch:ca via the Twitter search box. Right Panel:\nexample of a Tweet posted on June 14 containing the domain name globalresearch.ca .\nWhen a user searches via the twitter search-box for this domain name, no results appear as shown\nin the screenshot in the left panel of Figure 16, taken on June 14, 2021. To further investigate the\npossible implementation of a reduced visibility measure, we searched via the Twitter API v2 for\nTweets, excluding Retweets, containing the query globalresearch.ca from April 15, 2021 until August\n15, 2021. As shown in the left panel in Figure 17, we \fnd a strictly positive number of Tweets\ncontaining the domain name globalresearch.ca after the date on which the screenshot was taken\n(left panel of Figure 16). Hence, the visibility of Tweets containing this domain name has been\nreduced because users can no longer access Tweets containing the domain name globalresearch.ca via\nthe search box. Nevertheless users are not restrained from posting Tweets containing this domain\nname (see right panel of Figure 16). Furthermore, those collected Tweets have strictly positive\nengagement metrics as shown in the right panel of Figure 17. Hence, the users who post Tweets\nincluding articles from the globalresearch.ca website receive Tweet level engagement from their own\nfollowers.\nFinally, the website globalresearch:ca is linked to the Twitter account @ CRG CRM , which was\nsuspended from Twitter on May 25, 2021 as shown in a message about the account's suspension\n34See the paragraph Limiting Tweet visibility : https://help.twitter.com/en/rules-and-policies/enforcement-\noptions.\n35See https://mediabiasfactcheck.com/global-research/.\n16\nWorking paper, 2022 -1, April 2022\nFigure 17: Left Panel: daily number of Tweets, excluding Retweets, containing the query globalresearch.ca from\nJanuary 1, 2021 until June 30, 2021. Right Panel: engagement metrics of Tweets containing the query globalre-\nsearch.ca from January 1, 2021 until June 30, 2021. Data collected via the Twitter API v2 on August 17, 2021.\n(see the right panel in Figure 18). To the best of our knowledge, no o\u000ecial communication by\nTwitter has announced the suspension nor the exact date at which it was implemented. Hence the\naccount may have gotten suspended before that date. Furthermore when a user attempts to click\non a link which contains globalresearch.ca in a Tweet, a warning message appears and indicates that\nthe link may be unsafe (see the left panel in Figure 18). For the case of this Twitter account, it is\nlikely that a mix of interventions was used: reducing the visibility via the search box, suspending\nthe related Twitter account and returning a warning message when users click on a link containing\nthis domain name.\nFigure 18: Screenshots taken on June 14, 2021. Left Panel: warning message that appears when a user attempts\nto click on a link which contains the domain name globalresearch.ca . Right Panel: account suspension message of\n@CRG CRM linked to globalresearch.ca .\n4.3 YouTube\nYouTube can reduce the visibility of certain videos via its recommendation system, namely by\nrecommending content from authoritative sources. The platform usually recommends content to\nthe users based on the watch history and searched queries in google and YouTube; which can\nboth be cleared via account settings. A recommendation system based on previous preferences\nhas a downside, namely when a user repeatedly watches videos that might be misleading or that\npromote conspiracy theories and gets recommended similar videos. YouTube states that they set\n17\nWorking paper, 2022 -1, April 2022out to prevent their systems from serving up content that could misinform users in a harmful way,\nparticularly in domains that rely on veracity, such as science, medicine, news, or historical events.\nYouTube identi\fes misinformation by using external human evaluators and experts to examine if\na given video is promoting misleading information or conspiracy theories, and then use machine\nlearning systems to discard misinformation from their recommendations.36\nTo examine this e\u000bect, we designed an experiment that simulates a user's behavior on YouTube.\nWe used a list of 45 videos marked as False by Science Feedback and classi\fed under the category\n\\health\". The experiment starts with one video marked as \\False\" from our list of videos and a\nclean watch history. For each video, our automated program collects around3720 recommendations\nand clicks on the top 10 videos in sequential order. After that, for every video in the set of the top\n10 recommendations, we collect their own top 20 recommended videos. In addition, to simulate\na user's behavior, the automated program watches each video for a few minutes38. Doing so, we\ncollected 10 549 YouTube recommended video entries, among which 3 895 were unique. To analyse\nthe collected data, we look for misinformation: ( i) at the channel level and ( ii) at the video level.\nThe 3 895 unique videos came from 1 562 di\u000berent channels, and we counted the number of\ntimes every channel got recommended. Figure 19 shows the top 10 recommended channels, whose\nvideos represent 30% of the recommendations. Fox News and Sky News Australia, which both have\na \\mixed\" factual reporting level according to the website Media Bias Fact Check and have several\nfailed fact-checks,39are at the \frst and third place respectively. PragerU is also among the list of\nthe 10 most recommended channels, with a \\low\" level of factual reporting according to the website\nMedia Bias Fact check.\nFigure 19: Top 10 channels recommended by YouTube when we simulated a user that starts by watching a video\nfact-checked as False and then follows the top recommendations.\nWe then ranked the videos based on the number of times they got recommended, and asked\nScience Feedback to fact-check the top 40 recommended videos (those videos were recommended\n1 735 times, which represents 16% of the recommendations). Among the 40 examined videos,\n36See the page \\How does YouTube address misinformation?\"\n37When we visit a video, we get a number of recommendations that vary between 18 videos and 22 videos.\n38If the video duration is ten minutes or less, the automated program watches the whole video. If the video\nduration is strictly higher than ten minutes, then the automated program watches only \fve minutes.\n39See https://mediabiasfactcheck.com/fox-news-bias/ and https://mediabiasfactcheck.com/sky-news-australia/.\n18\nWorking paper, 2022 -1, April 2022four videos were marked as misleading and they were recommended 132 times out of the 1 735\nrecommendations (that is 7 :7% of the recommendations). Four other videos were marked as hyper-\npartisan and they were recommended 184 times (10 :7%).\nAs we collected the recommendations of misinformation videos, we expected to \fnd a high\nproportion of videos of questionable content. But only 18 :4% of the recommendations corresponded\ntomisleading orhyper-partisan videos. This relatively small percentage hints towards that idea that\nYouTube is probably reducing the visibility of videos containing misinformation. However, when we\ninvestigated misinformation at the channel level, we found that videos belonging to channels that\nare not highly authoritative were being often recommended. It appears that the recommendation\nsystem of YouTube is reducing the visibility at the video level but not necessarily at the channel\nlevel.\n5 Discussion\nIn this article we have assessed three broad categories of interventions to tackle misinformation\nby Facebook, Twitter and YouTube: ( i) suspension of accounts, ( ii) introduction of \rags, la-\nbels and information panels and ( iii) reducing the visibility of content.40Furthermore, we have\nprovided a methodology that can be useful for third-party monitoring. The code needed to\ncollect the data and create the \fgures is available on the following public GitHub repository:\ngithub.com/medialab/webclim misinformation related interventions, and we hope it can be a use-\nful ressource for researchers, NGOs and journalists addressing online misinformation.\nMainstream platforms such as Facebook, Twitter and Youtube are increasingly working to-\nwards higher data transparency. We invite the reader to see links that redirect to the respective\ntransparency centers in Appendix 6.1. Yet the present research sheds light on the restrictions to\naccess data and its limits, which impede the academic community from studying successfully online\nmisinformation and assessing the impact of platforms' interventions regarding this phenomenon.\nHence, few comments are in order regarding the limitations to access data, the lack of transparency\nvis-\u0012 a-vis misinformation related interventions and the lack of interventions designed speci\fcally for\nmisinformation.\nA need for more complete data. When Facebook, Twitter or YouTube intervene by sus-\npending an account, the data can no longer be accessed via scraping or via the o\u000ecial APIs (Twitter,\nYouTube, CrowdTangle) for the purposes of scienti\fc research. This hampers the investigation of\nhow platforms moderate content. To further illustrate this point, the original list of YouTube\nvideos with failed fact-checks (summarized in Figure 12), had over 200 YouTube videos available\nin March 2021. However, by June 2021, thirty videos had disappeared from the YouTube API for\npolicy violation. Similarly, in a related research project, Th\u0013 ero and Vincent (2020) [23] searched\non CrowdTangle for 94 public accounts sharing speci\fc content associated with misinformation in\nNovember 2020. They tried to collect the posts from those 94 pages in January 2021, to only\ndiscover that 11 pages had disappeared from the CrowdTangle API. Researchers' work could be\nsimpli\fed, if metadata for content posted by removed accounts could still be accessible via the\nAPIs.\nAlthough it is currently impossible to verify permanent deletions using available social media\ndata, their e\u000bects can be observed using an adapted methodology. Following the Capitol riots\n40For a detailed list of interventions of multiple platforms, we invite the reader to navigate through the following\nairtable compiled by Slatz and Leibowicz (2021) [21].\n19\nWorking paper, 2022 -1, April 2022in January 2021, Twitter has announced the suspension of more than 70 000 accounts related to\nQAnon.41A journalist has recently investigated the e\u000bects of this measure by counting the number\nof times hashtags related to the QAnon movement have been used over time, such as #QAnon or\n#WWG1WGA. A drastic reduction in hashtag use was found when comparing the \frst quarter\nof 2021 with the \frst quarter of 2020.42Thus, counting speci\fc hashtag use over time allows to\nindirectly observe permanent suspensions on Twitter and there is a need for similar methodologies\nto be experimented on data from other platforms. However, a point of concern is that data can also\ndisappear because of other factors independent of platforms' content moderation. To provide an\nexample, McMinn et al. (2012) [11] collected 120 million Tweets related to general topics. The same\npublished set of Tweet IDs was then used by Mazoyer et al. (2020) [10], to run the same collection\nin 2018. Mazoyer et al. (2020) [10] were only able to retrieve 67 millions of Tweets (55%). The\nauthors argue that this drop is probably due to Tweets having been removed by users and Twitter\naccounts being shut by the users themselves. As data can disappear from platforms over time, it is\nhard to distinguish whether platforms or users are the ones responsible for data removal. Hence in\nthe special case of Twitter accounts related to QAnon, it is hard to identify how much of the drop\nin QAnon related hashtag use is due to the Twitter intervention or to accounts migrating to other\nplatforms out of fear of having their accounts suspended by the platform or simply a drop in using\nthe hashtag for other unidenti\fed factors.\nAside from the disappearance of data due to content moderation, some essential metrics for\nthe study of misinformation are not accessible via main APIs. In particular, the reach of posts on\nFacebook, which is the number of people who actually see a given post, cannot be recovered from\nthe CrowdTangle API.43Hence, researchers can only build proxies for this metric based on available\ndata (see section 4.1 for an example). Furthermore, to the best of our knowledge, \felds related to\n\rags, notices and information panels are not available on the o\u000ecial APIs, with the exception of the\nwithheld and possibly sensitive content interstitials in the Twitter API v2. For the purposes of the\npresent research we thus had to scrape this information from publicly available content on Facebook,\nTwitter and YouTube. Again, this disrupts the monitoring of the platforms' interventions and the\nassessment of their impact (Pasquetto, Swire-Thompson, Amazeen et al. (2020) [14]).\nA last concern regards the growing restrictions to access APIs (Perriam et al.(2019) [17]). In\nApril 2018, likely as a response to the Cambridge Analytica scandal, Facebook severely limited\naccess to their o\u000ecial APIs44and CrowdTangle is today a reliable source for researchers to access\nFacebook data. However, it should be noted that CrowdTangle has been recently used by jour-\nnalists to show that Facebook is disproportionally favoring highly partisan information45and a\ngeneral suspicion is growing that its access might be shut down in a near future to journalists and\nresearchers.46Regarding Twitter and YouTube, it is still possible and relatively easy to access their\n41https://blog.twitter.com/en us/topics/company/2021/protecting{the-conversation-following-the-riots-in-\nwashington{.\n42See https://twitter.com/Shayan86/status/1394742784683298818.\n43See https://help.crowdtangle.com/en/articles/4558716-understanding-and-citing-crowdtangle-data.\n44See https://about.fb.com/news/2018/04/restricting-data-access/ and http://thepoliticsofsystems.net/2018/08/\n/facebooks-app-review-and-how-independent-research-just-got-a-lot-harder/.\n45See the news article https://www.economist.com/graphic-detail/2020/09/10/facebook-o\u000bers-a-distorted-view-\nof-american-news and the Tweets from https://twitter.com/facebookstop10. The last link is a Twitter account\nshowing each day the ten most shared link on Facebook, most of them coming from extreme right and misinformation\nsources. Facebook has reacted to this here: https://about.fb.com/news/2020/11/what-do-people-actually-see-on-\nfacebook-in-the-us/.\n46See the news articles https://www.nytimes.com/2021/07/14/technology/facebook-data.html and\nhttps://www.bloomberg.com/news/articles/2021-08-03/facebook-disables-accounts-tied-to-nyu-research-project.\n20\nWorking paper, 2022 -1, April 2022o\u000ecial APIs.\nA need for more transparency. Mainstream platforms generally lack transparency regarding\ntheir misinformation related interventions. The broad lines of policies regarding misinformation\nare communicated via blog posts and are not typically part of the set of rules or community\nguidelines of mainstream platforms (see Appendix 6.1). Yet, when navigating through the categories\nof policy areas, to the best of our knowledge, misinformation is absent from the data available on the\ntransparency centers of Facebook, Twitter and YouTube. Platforms' o\u000ecial communication about\nmisinformation interventions is scarce and the academic community, NGOs and data journalists,\nusually discover interventions related to misinformation via monitoring social media accounts related\nto domain names with several failed fact-checks or via articles in News outlets. For example, while\nconducting the present research, it was brought to our attention that the Twitter account related\nto the domain name globalresearch:ca was suspended. But to the best of our knowledge, there was\nno o\u000ecial communication by the Twitter Safety team to explain the reasons of this suspension (see\nsection 4.2).\nWe are aware that full transparency regarding operational aspects of misinformation related\ninterventions can back\fre. This is because the aimed users might \fnd ways to circumvent the\ninterventions. We noticed that some YouTube channels are trying to avoid the likely automated\nCOVID-19 information panels, by using notational variants such as \\C.O.V.I.D\" or \\C O V I D\".\nSimilarly, we came across notational variants on Twitter of the domain name o\u000b-guardian.org47such\naso\u000b-guardian. org ,o\u000b-guardian .org ,o\u000b-guardian./org ,o\u000b-guardian*org .48Furthermore there is\nlittle information about how algorithms are practically used as a tool for content moderation; since\nthey can be used to reduce the visibility of some content or even prevent problematic content from\nbeing recommended. Hence researchers have to resort to proxy measures and simulated experiments\nto have a better understanding the impact of algorithmic design. Based on a sample of 23 YouTube\nstudies, Yesilada and Lewandowsky (2022) [25] \fnd that YouTube recommender system could lead\nusers to problematic content, but highlight the limitations of this conclusion since there is an\nincomplete understanding of the recommendation system and models built might not re\rect the\nactual functioning of YouTube recommendation algorithm. Hence, a balance is needed between:\n(i) fully transparent communication about misinformation related interventions so that both users\nand researchers can understand and investigate how the platforms are regulating content, and ( ii)\nnot explicitly giving too much information which would allow the policies to be bypassed.\nIn certain cases, the platform interventions were not motivated by content moderation to limit\nmisinformation, but rather by other rule violations. For example, the o\u000ecial reason for the suspen-\nsion of Donald Trump's Twitter account was not misinformation49but the violation of Twitter's\nGlori\fcation of Violence Policy, with his last Tweets encouraging the Capitol's riot.50Similarly,\nwhen Facebook removed four pages related to the InfoWars website, the cited reason was repeated\nviolations of Community Standards: \\While much of the discussion around Infowars has been re-\nlated to false news, which is a serious issue that we are working to address [...], none of the violations\n47Here is an example of a failed fact-check of one of the articles of the website o\u000b-guardian.org :\npolitifact.com/factchecks/2020/jul/07/blog-posting/covid-19-tests-are-not-scienti\fcally-meaningless/\n48For this case, we failed to understand precisely what kind of intervention Twitter users were circumventing,\nsince they could share in a Tweet an article redirecting to the website o\u000b-guardian.org with the correct notation and\nonly a warning message would appear when one clicks on it.\n49Although Donald Trump may have spread many misinformation during his governance, see for example:\nhttp://allianceforscience.cornell.edu/blog/2020/10/what-drove-the-covid-misinformation-infodemic/.\n50See https://blog.twitter.com/en us/topics/company/2020/suspension.\n21\nWorking paper, 2022 -1, April 2022that spurred today's removals were related to this\".51Regarding The Beauty of Life website, its\nban was o\u000ecially due to coordinated inauthentic behavior,52which includes using fake accounts\nthat misrepresent one's identity or using methods to arti\fcially boost the popularity of content.\nAccording to Facebook, coordinated inauthentic behavior is a distinct phenomenon from disin-\nformation, as \\most of the content shared by coordinated manipulation campaigns isn't probably\nfalse\".53Nevertheless, both misinformation and coordinated inauthentic behavior were attested for\nthe Beauty of Life, and reported to Facebook by the fact-checking organization Snopes.54In the\nabove examples, the compagnies' communication was very clear on the reason of the interventions\n(although one can suspect that the o\u000ecial reason given can depart from the real motivation behind\nan intervention). However, very often no o\u000ecial communication on behalf of the platforms can\nbe found for a speci\fc interventions observed in collected data. Because misinformation is often\nassociated with creating fake accounts to speed its spread, and incitation to violence, it is some-\ntimes unclear to distinguish between interventions due to misinformation and interventions due to\nother guideline violations, and more transparent communication on platforms' actions would help\nmonitor the interventions speci\fcally related to misinformation.\nA need for more consistent and adapted policies. Misinformation on the web is a problem\nthat is increasingly discussed since the 2016 American elections and in the years that followed,\nplatforms may have decided on a case-by-case basis which actions to enforce. Infowars and The\nBeauty of Life domains have both several failed fact-checks55and have both violated Facebook's\ncommunity guidelines (incitation to violence and coordinated unauthentic behavior). Yet one's\ndistribution was reduced for only half of a year, while the other one's links cannot be shared\nanymore on Facebook since the end of 2019, and it is not clear why such di\u000berent enforcements\nwere applied to these comparable websites. It is also possible that platforms usually follow speci\fc\nrules on how to \fght misinformation, but that sometimes the sudden media coverage of speci\fc\nevents leads them to take exceptional actions, such as with the Capitol riots and Trump's suspension\nfrom Facebook, Twitter and YouTube. We would argue that policies should be clearly stated and\ncarefully designed, rather than implementing interventions on a case-by-case basis and in response\nto the latest news events.\nFurthermore, it is likely that some interventions used to tackle online misinformation correspond\nto interventions that were designed for earlier policy areas. To illustrate, YouTube explicitly states\n56that: \\Several policies in our Community Guidelines are directly applicable to misinformation\",\nsuch as the deceptive practices, impersonation and hate speech policies. Hence interventions such\nas content deletion or account suspension, might be e\u000ecient for policy areas for which they were\nspeci\fcally designed, such as the policy area Adult Nudity & Sexual Activity . Nevertheless, tackling\nonline misinformation might need adapted interventions. Namely, account suspensions or content\ndeletion motivated by the spread of inaccurate information can be perceived as censorship by the\ntargeted users. In particular, such interventions can generate platform migrations (see Rogers\n(2020) [19]) towards new alternative social media platforms (e.g. Telegram, Gab, Minds).\nFinally, the means to tackle online misinformation and understanding the phenomenon itself,\n51See the article https://about.fb.com/news/2018/08/enforcing-our-community-standards/.\n52See https://about.fb.com/news/2019/12/removing-coordinated-inauthentic-behavior-from-georgia-vietnam-\nand-the-us/.\n53See: https://about.fb.com/news/2019/10/inauthentic-behavior-policy-update/.\n54See https://www.snopes.com/news/2019/11/12/bl-fake-pro\fles/, https://www.snopes.com/news/2019/11/12/bl-\nfake-pro\fles/ and https://www.snopes.com/news/2019/12/13/facebook-bl-cib/.\n55See https://mediabiasfactcheck.com/infowars-alex-jones/ and https://mediabiasfactcheck.com/the-bl/.\n56See What policies exist to \fght misinformation on Youtube? (Last visited September 2021).\n22\nWorking paper, 2022 -1, April 2022is an active area of research. Developing pertinent interventions against misinformation depends\non understanding the interventions currently used by platforms (e.g. for labeling, see Morrow\net al. (2021) [12]) along with the psychology of interaction with social media (Pennycook and\nRand (2021) [16]). Adding fact-checking labels to controversial content is certainly the intervention\nwhose e\u000bectiveness was the most studied so far. It was shown that credibility indicators do reduce\nmisperceptions and sharing, although their impact varied vastly depending on the presentation\nof the warnings (see Yaqub (2020) [24] and Sharevski (2021) [22]). Moreover, a fact-checking\nlabel can lead to unpredicted consequences depending on the perception of the person reading it.\nOn Facebook, Twitter and YouTube, fact-checking messages are currently applied only to some\ncontent, while the rest of the content on a given platform remains naturally unlabelled. Such\nsparse application of warnings can lead to an \\implied truth\" e\u000bect, where users may assume\nthat content without warnings have actually been veri\fed and shown to be true (Pennycook et\nal. (2020) [15]). Using a di\u000berent approach, Mosleh et al. (2021) [13] identi\fed Twitter users\nsharing false news and replied to their false Tweets with links redirecting to fact-checking websites.\nThey observed a decrease in the quality and accuracy of the content subsequently shared by the\ncorrected user. These research papers highlight the necessity to investigate potential unintended\ne\u000bects of well-meaning interventions. Furthermore, misinformation related interventions can be\npoorly misunderstood. Based on survey data, Saltz et al. (2021) [20] show that 40% of the survey\nrespondents (wrongly) believed that content is mostly or all fact-checked. To date, research about\nmisinformation related interventions by platforms is still scarce and addressing the speci\fc needs in\nterms of data for auditing and monitoring tasks can greatly help in improving our understanding of\nthese interventions and study their impact. The methods presented in this paper can help to monitor\nnot only the platforms' actions against misinformation, but also their subsequent consequences in\nreal-life settings.\nReferences\n[1] Judith Bayer. Between anarchy and censorship public discourse and the duties of social media.\nCEPS Paper in Liberty and Security in Europe , (2019-03), May 2019.\n[2] David A. Broniatowski, Daniel Kerchner, Fouzia Farooq, Xiaolei Huang, Amelia M. Jamison,\nMark Dredze, Sandra Crouse Quinn, and John W Ayers. Twitter and facebook posts about\ncovid-19 are less likely to spread misinformation compared to other health topics. PLoS ONE ,\n17(1), 2022.\n[3] Ronan O Fathaigh, Natali Helberger, and Naomi Appelman. The perils of legally de\fning\ndisinformation. Internet Policy Review , 10(4), 2021.\n[4] Ahiza Garc\u0013 \u0010a-Hodges. Youtube suspends oann for violating its covid-19 policy.\nNBCnews, https://www.nbcnews.com/news/all/youtube-suspends-oann-violating-its-covid-19-\npolicy-n1248845 , November 2020.\n[5] Nir Grinberg, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson, and David Lazer.\nFake news on twitter during the 2016 u.s. presidential election. Science , 263(274), 2019.\n[6] Adam Hughes and Stefan Wojicik. 10 facts about americans and twitter. Pew Research Center ,\n2019.\n23\nWorking paper, 2022 -1, April 2022[7] David Lazer, Matthew Baum, Yochai Benkler, Adam Berinsky, Kelly Greenhill, Filippo\nMenczer, Miriam Metzger, Brendan Nyhan, Gordon Pennycook, David Rothschild, Michael\nSchudson, Steven Sloman, Cass Sunstein, Emily Thorson, Duncan Watts, and Jonathan Zit-\ntrain. The science of fake news. Science , 359(6380), 2018.\n[8] Tessa Lyons. The three-part recipe for cleaning up your news feed. Facebook Newsroom\nhttps://about.fb.com/news/2018/05/inside-feed-reduce-remove-inform/ , May 2018.\n[9] Paris Martineau. Facebook bans alex jones, other extremists - but not as planned. Wired\nhttps://www.wired.com/story/facebook-bans-alex-jones-extremists/ , February 2019.\n[10] B\u0013 eatrice Mazoyer, Julia Cag\u0013 e, Nicolas Herv\u0013 e, and C\u0013 eline Hudelot. A french corpus for event\ndetection on twitter. In Proceedings of the 12th Language Resources and Evaluation Conference ,\npages 6220{6227, 2020.\n[11] Andrew J McMinn, Yashar Moshfeghi, and Joemon M Jose. Building a large-scale corpus for\nevaluating event detection on twitter. In Proceedings of the 22nd ACM international conference\non Information & Knowledge Management , pages 409{418, 2013.\n[12] Garret Morrow, Briony Swire-Thompson, Jessica Polny, Matthew Kopec, and John Whihbey.\nThe emerging science of content labeling: Contextualizing social media content moderation.\nmimeo , 2021.\n[13] Mohsen Mosleh, Cameron Martel, Dean Eckles, and David Rand. Perverse downstream conse-\nquences of debunking: Being corrected by another user for posting false political news increases\nsubsequent sharing of low quality, partisan, and toxic content in a twitter \feld experiment.\nInproceedings of the 2021 CHI Conference on Human Factors in Computing Systems , pages\n1{13, 2021.\n[14] Irene V. Pasquetto, Briony Swire-Thompson, Michelle A. Amazeen, Fabr\u0013 \u0010cio Benevenuto, Na-\ndia M. Brashier, Robert M. Bond, Lia C. Bozarth, Ceren Budak, Ullrich K. H. Ecker, Lisa K.\nFazio, Emilio Ferrara, Andrew J. Flanagin, Alessandro Flammini, Deen Freelon, Nir Grinberg,\nRalph Hertwig, Kathleen Hall Jamieson, Kenneth Joseph, Jason J. Jones, R. Kelly Garrett,\nDaniel Kreiss, Shannon McGregor, Jasmine McNealy, Drew Margolin, Alice Marwick, FiIippo\nMenczer, Miriam J. Metzger, Seungahn Nah, Stephan Lewandowsky, Philipp Lorenz-Spreen,\nPablo Ortellado, Gordon Pennycook, Ethan Porter, David G. Rand, Ronald E. Robertson,\nFrancesca Tripodi, Soroush Vosoughi, Chris Vargo, Onur Varol, Brian E. Weeks, John Wih-\nbey, Thomas J. Wood, and Kai-Cheng Yang. Tackling misinformation: What researchers could\ndo with social media data. Harvard Kennedy School Misinformation Review , 1(8), December\n2020.\n[15] Gordon Pennycook, Adam Bear, Evan T Collins, and David G Rand. The implied truth e\u000bect:\nAttaching warnings to a subset of fake news headlines increases perceived accuracy of headlines\nwithout warnings. Management Science , 66(11):4944{4957, 2020.\n[16] Gordon Pennycook and David G Rand. The psychology of fake news. Trends in cognitive\nsciences , 2021.\n[17] Jessamy Perriam, Andreas Birkbak, and Andy Freeman. Digital methods in a post-api envi-\nronment. International Journal of Social Research Methodology , 2019.\n24\nWorking paper, 2022 -1, April 2022[18] Guillaume Plique, Pauline Breteau, Jules Farjas, H\u0013 elo\u007f \u0010se Th\u0013 ero, and Jean Descamps. Minet,\na webmining cli tool and library for python zenodo. http://doi.org/10.5281/zenodo.4564399.\n2019.\n[19] Richard Rogers. Deplatforming: Following extreme internet celebrities to telegram and al-\nternative social media deplatforming: Following extreme internet celebrities to telegram and\nalternative social media deplatforming: Following extreme internet celebreties to telegram and\nalternative social media. European Journal of Communication , 35(3):213{229, 2020.\n[20] E. Saltz, S. Barari, C.R. Leibowicz, and C. Wardle. Misinformation interventions are common,\ndivisive, and poorly understood. Harvard Kennedy School (HKS) Misinformation Review , 2(5),\n2021.\n[21] Emily Saltz and Claire Leibowicz. Shadow bans, fact-checks, info hubs: The\nbig guide to how platforms are handling misinformation in 2021. Nieman-\nLab https://www.niemanlab.org/2021/06/shadow-bans-fact-checks-info-hubs-the-big-guide-to-\nhow-platforms-are-handling-misinformation-in-2021/ , June 2021.\n[22] Filipo Sharevski, Raniem Alsaadi, Peter Jachim, and Emma Pieroni. Misinformation warning\nlabels: Twitter's soft moderation e\u000bects on covid-19 vaccine belief echoes. arXiv preprint\narXiv:2104.00779 , 2021.\n[23] H\u0013 elo\u007f \u0010se Th\u0013 ero and Emmanuel Vincent. Investigating facebook's interventions against accounts\nthat repeatedly share misinformation. Information Processing and Management , 59(2), 2022.\n[24] Waheeb Yaqub, Otari Kakhidze, Morgan L Brockman, Nasir Memon, and Sameer Patil. E\u000bects\nof credibility indicators on social media news sharing intent. In Proceedings of the 2020 chi\nconference on human factors in computing systems , pages 1{14, 2020.\n[25] Muhsin Yesilada and Stephan Lewandowsky. Systematic review: Youtube recommendations\nand problematic content. Internet Policy Review , 11(1), 2022.\n6 Appendix\n6.1 Quick access to community guidelines and policies\n25\nWorking paper, 2022 -1, April 2022Rules\nfacebook.com/communitystandards/recentupdates/\nhelp.twitter.com/en/rules-and-policies/twitter-rules\nyoutube.com/intl/en us/howyoutubeworks/policies/community-guidelines/\nRules\nenforcement\ntransparency.fb.com/data/community-standards-enforcement/\ntransparency.twitter.com/en/reports/rules-enforcement.html\ntransparencyreport.google.com/youtube-policy/\nTransparency\ncenter\ntransparency.fb.com/data/\nhttps://law.yale.edu/yls-today/news/facebook-data-transparency-advisory-group-releases-\fnal-report\ntransparency.twitter.com/en/reports.html\ntransparencyreport.google.com/?hl=en\nPolicy\nregarding\nCovid-19\nhttps://www.facebook.com/help/230764881494641/\nhelp.twitter.com/en/rules-and-policies/medical-misinformation-policy\nblog.twitter.com/en us/topics/company/2021/updates-to-our-work-on-covid-19-vaccine-misinformation\nblog.twitter.com/en us/topics/company/2020/covid-19\nsupport.google.com/youtube/answer/9891785\nFact-checking\npolicy\nfacebook.com/journalismproject/programs/third-party-fact-checking/how-it-works\nx\nsupport.google.com/youtube/answer/9229632\nFighting\nmisinformation\nfacebook.com/formedia/blog/working-to-stop-misinformation-and-false-news\nabout.fb.com/news/2018/05/hard-questions-false-news/\nyoutube.com/intl/en us/howyoutubeworks/our-commitments/\fghting-misinformation/#policies\nblog.youtube/inside-youtube/the-four-rs-of-responsibility-raise-and-reduce/\nStrike System\nSee in the following link the section What is the number of strikes a person or\nPage has to get to before you ban them?\nabout.fb.com/news/2018/08/enforcing-our-community-standards/\nhttps://transparency.fb.com/en-gb/enforcement/taking-action/counting-strikes/ (updated July 29,2021)\nSee in the following links the section: Account locks and permanent suspension\nhelp.twitter.com/en/rules-and-policies/election-integrity-policy\nhelp.twitter.com/en/rules-and-policies/medical-misinformation-policy\nhttps://support.google.com/youtube/answer/2802032?hl=en. Accessed 21 6 2021\nAccount suspension\n https://help.twitter.com/en/managing-your-account/suspended-twitter-accounts\nFlags, Notice\nand Information Panels\nhttps://www.facebook.com/business/help/341102040382165\nhttps://help.twitter.com/en/rules-and-policies/notices-on-twitter\nsupport.google.com/youtube/answer/9004474?hl=en\nTable 2: Summary of ressources, last accessed on July 5, 2021.\nWorking paper, 2022 -1, April 2022Acknowledgments\nWe thank our colleagues at Sciences Po m\u0013 edialab for their feedback and suggestions.\nFunding\nThis research was supported by the Make Our Planet Great Again' French state aid managed by the\nAgence Nationale de la Recherche under the 'Investissements d'avenir' program with the reference\nANR-19-MPGA-0005.\nCompeting interests\nWe have no con\ricts of interest to disclose.\nEthics\nThe data collection and processing complied with the EU General Data Protection Regulation\n(GDPR).\n27", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Monitoring misinformation related interventions by Facebook, Twitter and YouTube: methods and illustration", "author": ["S Shabayek", "H Th\u00e9ro", "D Almanla", "E Vincent"], "pub_year": "2022", "venue": "NA", "abstract": "There is growing pressure for mainstream platforms, such as Facebook, Twitter or YouTube,  to fight misinformation by moderating the content that spreads on their site. We investigated"}, "filled": false, "gsrank": 293, "pub_url": "https://hal.science/hal-03662191/", "author_id": ["0GMsKBsAAAAJ", "", "", ""], "url_scholarbib": "/scholar?hl=en&q=info:fntiyc9jJ3sJ:scholar.google.com/&output=cite&scirp=292&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D290%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=fntiyc9jJ3sJ&ei=OLWsaJm1KI6IieoP0sKRuAk&json=", "num_citations": 2, "citedby_url": "/scholar?cites=8874171334844906366&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:fntiyc9jJ3sJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://hal.science/hal-03662191/file/misinformation_interventions_2021_Shabayek_Thero_Almanla_Vincent.pdf"}}, {"title": "The green web: Evaluating online news communities and their environmentalism", "year": "2019", "pdf_data": "THE GREEN WEB: EVALUATING ONLINE NEWS COMMUNITIES AND THEIR \nENVIRONMENTALISM  \n \nBy \n \nStian Himberg Roussell  \n \n \nA Thesis  Presented to  \nThe Faculty of Humboldt State University  \nIn Partial Fulfillment  of the Requirements for the Degree  \nMaster of Arts in Social Science: Environment and Community  \n \nCommittee Membership  \nDr. Anthony Silvaggio , Committee Chair  \nDr. Nikola Hobbel , Committee Member  \nDr. John Meyer , Committee Member  \nDr. Mark Baker , Program Graduate Coordinator  \n \nJuly 2019  \n \nii \n ABSTRACT  \nTHE GR EEN WEB: EVALUATING ONLINE NEWS COMMUNITIES AND THEIR \nENVIRONMENTALISM  \n \nStian  Himberg  Roussell  \n \n In the past news and media were disseminated through channels that were \ndifficult for consumers to communicate with. Now a new form of news dissemination is \ntaking place on the internet where communication between consumers and producers \nexists. C rowdsourced social media sites , where users elect and vote on articles and \ncomments to represent a topic, are creating new forms of news disseminatio n which \nheavily alters media content and community discussions. This study was conducted in \norder to better understand how crowdsourced social media platforms affect users\u2019 \nperception of environmental topics, issues, and problems.  It focuse s on the crowdso urced \nsocial media platform Reddit evaluating  the top 100 articles within the subreddit titled \nr/environment. The methods used in this study include secondary data analysis, archival \nresearch of online forums, and social network analysis. Most  articles ana lyzed discussed \nthe topic of politics, including President Trump and his administration \u2019s environmental \npolicies, as well as a lack of transparency related to environmental policies  in general . \nMany of  the comments  responded to these articles by discussing  politics . Findings also \nincluded a perception that monetary and social capital must be present for individuals to \nfacilitate widescale, meaningful environmental change. Issues such as ecological changes \n \niii \n only accounted for 2 percent of overall crowdsourced  articles. Overall this study found an \nencouraging message in the articles and comments indicating support for global \nenvironmental activism. More research on how communities interpret environmental \nnews is  needed to broaden our understanding of peoples\u2019 i nterpretations of environmental \npolicies and their representation in crowdsourced and legacy media.   \n \niv \n ACKNOWLEDGEMENTS  \nI would like to thank my parents , Dr. John Roussell and Dr. Cathrine Himberg for \ntheir endless support and unconditional encouragement th roughout my life, but especially \nthroughout my college experience.  I would like to thank my committee chair, mentor, and \nfriend Anthony Silvaggio for years of guidance at Humboldt State University where I \nlearned my passions and interests with regards to S ociology and especially \nEnvironmental Sociology.  I would like to thank my grandparents and my aunt, Mommor, \nHumme, and Tante Lise for their support and love throughout the creation and writing of \nmy thesis. A special thanks to my best friend and business partner Jonny Trimboli for \nallowing me to drive his car to defend my thesis in person. Finally, I\u2019d like to thank my \ncohort and all of my friends I\u2019ve made throughout my college career who have \nencouraged me to pursue my passions no matter how difficult th e road gets.  \n  \n \nv \n TABLE OF CONTENTS  \n \nABSTRACT  ................................ ................................ ................................ ........................  ii \nACKNOWLEDGEMENTS  ................................ ................................ ...............................  iv \nLIST OF TABLES  ................................ ................................ ................................ ...........  viii \nLIST OF FIGURES  ................................ ................................ ................................ ...........  ix \n1.0 INTRODUCTION  ................................ ................................ ................................ ........  1 \n2.0: LITERATURE REVIEW  ................................ ................................ ..........................  10 \n2.1 Communication and Changing Media Landscapes  ................................ ................  10 \n2.2 Social Media as a News Dissemination Tool  ................................ .........................  15 \n2.3 Reddit: An Influential Crowdsourced Online Community  ................................ ..... 19 \n2.4 Conclusion: The Gap  ................................ ................................ ..............................  25 \n3.0 METHODOLOGY AND THEORY ................................ ................................ ...........  26 \n3.1 Theoretical Frameworks for Coding Environmental Journalism  ...........................  28 \n3.1.1 Conservation and wildlife management  ................................ ...........................  29 \n3.1.2 Wildlife initiatives  ................................ ................................ ...........................  30 \n3.1.3 Reforming of society environmentalism  ................................ ..........................  31 \n3.1.4 Eco -philosophies  ................................ ................................ ..............................  33 \n4.0 METHOds  ................................ ................................ ................................ ...................  35 \n4.1 Study Site  ................................ ................................ ................................ ................  35 \n4.2 Social Network Analysis  ................................ ................................ ........................  36 \n4.3 Content Analysis  ................................ ................................ ................................ ..... 37 \n4.4 Coding Process  ................................ ................................ ................................ ....... 38 \n \nvi \n 5.0 RESULTS  ................................ ................................ ................................ ...................  40 \n5.1 Environmental Arti cles ................................ ................................ ...........................  41 \n5.2 Social Network Analysis  ................................ ................................ ........................  43 \n5.3 Timeline of Articles  ................................ ................................ ................................  45 \n5.4 Comments on Articles  ................................ ................................ ............................  46 \n5.5 Politics  ................................ ................................ ................................ ....................  48 \n5.6 Comments within Political Articles:  ................................ ................................ ....... 53 \n5.7 Management of Energy Resources  ................................ ................................ .........  56 \n5.7.1 Fossil fuels  ................................ ................................ ................................ ....... 57 \n5.7.2 Policies  ................................ ................................ ................................ .............  58 \n5.7.3 Renewables a nd consumption  ................................ ................................ ..........  59 \n5.8 Comments on Management of Energy Resources  ................................ ..................  60 \n5.9 Environmental Movements  ................................ ................................ .....................  62 \n5.10 Environmental Movements Comments  ................................ ................................  64 \n5.11 Wildlife Preservation  ................................ ................................ ............................  66 \n5.11.1 Endangered species  ................................ ................................ ........................  67 \n5.11.2 Policies  ................................ ................................ ................................ ...........  68 \n5.12 Comments on Wildlife Preservation:  ................................ ................................ .... 69 \n5.13 Environmental Pollution  ................................ ................................ .......................  70 \n5.13.1 Human  ................................ ................................ ................................ ............  70 \n5.13.2 Water  ................................ ................................ ................................ ..............  71 \n5.13.3 Air  ................................ ................................ ................................ ..................  72 \n5.14 Comments on Envir onmental Pollution  ................................ ................................  72 \n \nvii \n 5.15 Media Related  ................................ ................................ ................................ ....... 74 \n5.16 Comments on the Media  ................................ ................................ .......................  75 \n5.17 E cological Shifts  ................................ ................................ ................................ ... 76 \n5.18 Ecological Shifts Comments  ................................ ................................ .................  76 \n6.0 DISCUSSION  ................................ ................................ ................................ .............  78 \n6.1 Social Network Analysis  ................................ ................................ ........................  78 \n6.2 Politics, Policies, Transparency and the Trump Effect  ................................ ...........  80 \n6.3 Management of Energy Resources, Dominant Voices, and Policies  ......................  83 \n6.4 Enviro nmental Movements  ................................ ................................ .....................  85 \n6.5 Wildlife Preservation  ................................ ................................ ..............................  87 \n6.6 Environmental Pollution  ................................ ................................ .........................  88 \n6.7 Me dia Related  ................................ ................................ ................................ .........  89 \n6.8 Ecological Shifts  ................................ ................................ ................................ ..... 89 \n7.0 CONCLUSION AND RECOMMENDATIONS  ................................ .......................  91 \n7.1 Limitations and Recommendations  ................................ ................................ ........  94 \nREFERENCES  ................................ ................................ ................................ .................  96 \nAPPENDIX  ................................ ................................ ................................ .....................  100 \n \n \n \n  \n \nviii \n LIST OF TABLES  \nTable 1: Political Articles within r/environment  ................................ ............................  100 \nTable 2: Management of Energy Resources Articles in r/environment  ..........................  109 \nTable 3: Environmental Movement Articles in r/environment  ................................ ....... 115 \nTable 4: Wildlife Preservation Articles in r/environment ................................ ...............  120 \nTable 5: Environmental Pollution Articles in r/environment  ................................ .........  123 \nTable 6: Media Related Articles in r/environment  ................................ .........................  126 \nTable 7: Ecological Shift Articles in r/environment  ................................ .......................  127 \n \n  \n \nix \n LIST O F FIGURES  \nFigure 1: The Scope of The Literature Review  ................................ ................................ .. 1 \nFigure 2: Word cloud of top 100 articles found in r/environment  ................................ .... 40 \nFigure 3: A Pie Chart of Environmental Articles by Environmental Type  ......................  42 \nFigure 4: A Social Network Ana lysis of the Top 100 Environmental Articles in \nr/environment  ................................ ................................ ................................ ....................  43 \nFigure 5: A Timeline of the top 100 articles in r/environment  ................................ .........  45 \nFigure 6 : Bar graph of political co mments within political articles  ................................ . 53 \nFigure 7: Bar graph of average comment type in Management of Energy Resources  ..... 60 \nFigure 8: Bar graph o f comments within environmental movement section  ....................  65 \nFigure 9: Bar graph of wildlife preservation comments on average  ................................ . 69 \nFigure 1 0: Bar graph of environment al pollution comments by average  .........................  73 \nFigure 11: Bar graph of comments on media related articles  ................................ ...........  75 \n \n1 \n \n 1.0 INTRODUCTION  \n\u201cIt\u2019s amazing that the amount of news that happens in the world every day always just \nexactly fits the newspaper\u201d  -Jerry Seinfeld  \n In the past, news and media were disseminated through channels that were \ndifficult for consumers to communicate with. Media like radio, newspapers, and \ntelevision all broadcast information with few forms of communication existing for users \nto discuss information with each othe r. In the pa st communication with broadcast media \nexisted, like  letters to the editor, however with crowdsourced media those comments exist \nin an online space instantaneously for other consumers to view  and interpret . These \nbroadcast forms of media are heavily influen tial in informing society about topical issues \n(Hansen 2018).  \nToday, in the age of the internet, a new form of media interaction exists between \nproducers and consumers  of media . For example, s ocial media sites are increasingly \nbeing used to deliver and re ceive news media. Social media platforms also implement \nadditional channels of discussion in the form of messaging boards, where articles offer \nconsumers the ability to respond to various news stories. This in effect creates dialogue \nbetween news consumers  as well as with news producers. In this thesis I will examine \nhow crowdsourced environmental news articles have shaped online discourse and \nrhetoric regarding environmental issues. To achieve this goal, I focus on the social media \nwebsite of Reddit, due t o the nature of its articles and comments being crowdsourced. By \n2 \n \n evaluating crowdsourced articles and comments on Reddit, this study will assess  how, \nwhere, and when environmental rhetoric exists within online social media sites.   \n Many websites exist tha t utilize messaging boards, where users can comment on \nissues or topics and reply to one another creating online forms of dialogue.  For example,  \nwebsites largely utilized by online users, like You Tube have a comment section on each \nvideo creating a messagi ng board within each piece of content.  Legacy media outlets, \nones born out of traditional media, also use messaging boards and have adapted to \ninternet culture. For example,  Fox News or CNN have social media pages on websites \nlike Facebook or You Tube where  they post interactive content.  \nLarge corporations that did not make their fortune from  the commodity of news \nhave  also introduced themselves into the space of news dissemination. Apple and Google, \ntwo of the largest technolog y companies in the world, hav e their own online news they \ncreate for users. Although these companies as well as legacy media outlets utilize \nmessaging boards, alternative forms of media (particularly social media) introduce a new \nconcept regarding news dissemination called crowdsourci ng articles. Crowdsourced \narticles are chosen by consumers, not media conglomerates. Articles are effectively \nbrought forward by consumers and their importance and relevance is voted on by other \nforum users.  This phenomenon offers new insight s into how ne ws is consumed. This \ncultural shift in news consumption  has major e ffects on how information is portrayed and \nperceived within society .  \nReddit, a social media website created in 2005, utilizes crowdsourcing as its \nprimary way to deliver news articles and comments. Reddit is the fifth most visited \n3 \n \n website in the United States and the thirteenth most visited website in the world (Alexa \nInter net 2019). Fifty -Five percent of Reddit\u2019s users are from the United States. This \nmeans topics within the website will have more of an American perspective as compared \nto other sites. Within Reddit there are subreddits, forums that are devoted to various \nspecific topics, and these subreddits crowdsource their articles and comments. A \ncrowdsourced article begins with an individual user posting an article that has a linked \nonline source. The user who posted the article, then retitles the article within the \nsubreddit, explaining what exists within the primary source.  On the website Reddit this is \nhow every article gets disseminated within online forums.  This allows users to be semi -\nproducers of articles within subreddits. Users that are consuming news on the sub reddit \ncan look through articles (and comments) and \u201cupvote\u201d or \u201cdownvote\u201d them.  \nAn upvote means a user likes the article and thinks it is worthwhile for the \ncommunity to consider and read. A downvote theoretically entails the opposite, that a \nuser sees t he article as not relating to the overall subreddit topic  or that they did not like \nthe discourse within the article . Because the meaning of upvotes and downvotes are \nsubjective, individuals interpret how they utilize these tools within online communities.  \nThe amount of upvotes are tallied and displayed right next to the article headline. When a \ngroup of users upvote an article enough, it becomes \u201chot\u201d. Articles that are \u201chot\u201d then are \ndisplayed on the front page of the subreddit. Additionally, users can so rt articles by \u201ctop\u201d \nof a certain date, or \u201ctop of all time\u201d displaying the most upvoted articles that have ever \nexisted within a subreddit. This sorting is important because it can quickly show the most \n4 \n \n impactful, or upvoted, articles within a subreddit. This in effect shows the most \nrepresentative articles of a subreddit community.  \nSocial media and online news sources like Reddit, allow for consumers to interact \nwith their articles and the information they find. Users can comment on articles, and these \ncomments can be \u201cupvoted\u201d or \u201cdownvoted\u201d just like the articles. This social media \nformat creates a crowdsourced discussion forum, or messaging board, in which articles \nand comments come from users as opposed to media conglomerates. Social media sites, \nlike Reddit, are increasingly being used to deliver and receive news media  (Holcomb, \nGottfried and Mitchell  2013).  \nThe structure of Reddit fosters the development of separate communities \ndedicated to specific topics, called \u201csubreddits\u201d. Subreddits are found b y utilizing the \nURLs (Uniform Resource Locator) ability to add r/ to the end of the main website\u2019s URL. \nThis feature allows the community to create whatever type of subreddit they desire and \nallows individual users to subscribe to any community in which th ey choose to \nparticipate. Reddit users can also create their own subreddits if they do not see one that \nfits the specific space they want to participate within. With a general Reddit account, \nindividuals can upload articles from the world wide web and peer s can upvote the article, \nmaking it popular and widely seen within the subreddit. The subreddit \u201cr/environment\u201d, \nwhich is the focus of this study, has 500,000 unique subscribers, comparable to the \npopulation of the city of Sacramento, California.  \nThe webs ite Reddit is heavily used as a source for political news (Holcomb, \nGottfried, and Mitchell 2013) and its ability to crowdsource articles and comments has \n5 \n \n aided in drastically changing dissemination of news media. When articles and comments \nare crowdsource d, the content that is chosen by users becomes news itself. Individuals \nchoose and vote on articles deemed important to a topic, illustrating the types of articles \nthe community accepts as representing them.  \nThe subreddit of r/environment discusses issues  and topics relating to the \nenvironment. Environmental issues exist as topics within news outlets. Although past \nstudies have evaluated how legacy news outlets represent environmental issues, little \nresearch has been conducted assessing crowdsourced media\u2019 s portrayal of the \nenvironment. With this study, crowdsourced media is being evaluated to see how \nenvironmental discourse exists between consumers when articles and comments are self -\nselected by a community. Because environmental news has historic roots in  legacy \nmedia, it is important studies emerge evaluating how new forms of media are altering the \nportrayal of environmental topics.  \nThis leads to the first research question of this study which asks, \u201cWhat is the \nmost dominant environmental discourse on Reddit?\u201d. Articles with more \u201cupvotes\u201d are \nmore influential in their accessibility and how they are perceived within the subreddit.  I n \naddition, comments are crowdsourced within messaging forums associated with each \nindividual article. By evaluating the subreddit r/environment, we can assess how \nr/environment users discuss environmental issues . This study\u2019s second research question, \n\u201cWh at patterns can we establish from the comments in these articles?\u201d aims to see what \npatterns emerge between articles and comments  regarding environmental rhetoric . Do \ndifferent article types elicit different or similar comments within Reddit\u2019s messaging \n6 \n \n boards?  What is being discussed on various types of environmental articles?  The third \nresearch question of this study simply asks, \u201cWhat news sources are being \ncrowdsourced?\u201d This question shows what sources users are crowdsourcing and what \nsources users upv ote or crowdsource more. This illustrates what sources the community \nof r/environment deems credible with portraying environmental topics.  \nBy studying crowdsourced forums on a website like Reddit, community input has \nthe potential to be evaluated, not jus t the portrayal of news  by legacy news outlets. Thi s \nallows studies like mine to  utilize content analysis in an entirely new way, where online \nforums resemble a town hall meeting as opposed to legacy news outlets that broadcast \nmessages with little to no u ser feedback that can be evaluated  (Hansen 2018) . Due to its \nsocial media roots, Reddit itself offers a platform for individuals to contribute, discuss, \nand organize ideas within the public sphere , not just be utilized for internet culture and \ncontent with out political meaning (Vago 2016) . So how can an online social media site \nwith a majority of its content revolving around internet culture  without political meaning , \ndiscuss and propose political and social change with widespread issues like the \nenvironmen t? Due to Reddit  users\u2019  nature of discussing a widespread arrangement of \ntopics self -selected by users, everything from meaningless memes to environmental \npolitical action have the potentiality to be popular and discussed.  \nThis thesis also examines how s ocial activism a major aspect to consider within \ncrowdsourced forms of media dissemination. In 2007, Reddit users, became involved in a \nGreenpeace campaign established to protect humpback whales from international  \ncommercial whaling. This campaign tracked a minke whale within the South Pacific \n7 \n \n Ocean as a part of a larger effort  to raise awareness about the international issue of \nwhaling. To generate public interest , Greenpeace  created an online poll asking \nparticipants within the Greenpeace discussion board o n Reddit to name the whale. The \ncampaign quickly evolved into an internet \u201cmeme\u201d as Reddit users organized to vote for \nthe name \u201cMister Splashy Pants\u201d which eventually won the competition. The publicity \nfrom this Greenpeace campaign had an immense impact on the industry and garnered \nwide scale public visibility . The Japanese government announc ed a moratorium \n(temporary prohibition) on its humpback whale hunt (Vago, 2016).  The Greenpeace \ncampaign illustrates how online discussion forums ha ve the potential to harbor, create, \nand disseminate social and political change.  \nCrowdsourced journalism also has the potential to impact society negatively. \nGuzman  argues that journalists, Reddit users and people on Twitter all share the blame \nfor rushed and inaccurate information pointing to the false reporting of  the culprit within \nthe Boston bombing massacre that occurred in 2013 (2013). False  personal information \nwas shared with  police officers  regard ing who the culprits were, ultimately affecting how \nlaw enforcement and the public reacted to the tragedy. Although cyber -vigilantes failed \nin this instance to detect and find the terrorists, their efforts offer interesting insight into \nhow crowdsourced online forums could help police enforcement and detect ives in future \ninvestigations (Nhan 2015).  The Boston bombing displays how users within online \ncommunities instantaneously can burden ongoing investigations. This form of social \nactivism can be harmful in some cases (Nhan 2015).  \n8 \n \n Social media communities  are interactive and have the potential to affect society  \nlike the Greenpeace campaign or the Boston bombings.  They are rarely seen as viable, \ncohesive communities that hold desirable  academic data  (Matthews 2015) . As younger \ngenerations begin to obtain news from these sources rather than traditional spaces \n(Gottfried and Shearer 2017), such as mass media through television, it is imperative to \nresearch these online communities as well as how their information networks emerge and \ndisperse. The third research q uestion of this study, \u201cWhat news sources are being \ncrowdsourced\u201d, evaluates patterns regarding sources for various article types. First, we \nmust identify the news source, then patterns regarding what environmental topics are \ndiscussed and disseminated by various sources emerge. This shows patterns regarding \nnews sources and environmental topics the subreddit self -selects to represent the \nr/environment community. It also illustrates what outside sources are the most influential \nwithin the r/environment subr eddit.  \nThis study will evaluate the top 100 \u201cupvoted\u201d (liked by) articles within the \nsubreddit r/environment, as well as analyze the top 2 comments of each article to gain a \nbetter understanding of how this specific online community produces and interpret s \nvarious environmental content. Chapter 2 will discuss the literature regarding \ncrowdsourced journalism and its differences from legacy news outlets. In addition, the \nmethodology for this study will be explained. Chapter 3 will lay out all the results for  the \nstudy. Chapter 4 will be the discussion of the thesis. Chapter 5 will discuss  this study\u2019s \nlimitations, establish conclusions, and provide an avenue for potential future research.  \n9 \n \n This research aims to study one specific online messaging board (r/env ironment), \nilluminating the relationship between consumers and news producers, analyzing how \ncrowdsourced online forums understand their participation in the environmental news \nportrayed to them . A large amount of information for academic researchers exist s within \nvarious online sources, but the data has yet to be mined (Matthews 2015). This is relevant \nbecause issues like the environment need to be studied within the context of \ncrowdsourced online communities.  \n  \n10 \n \n 2.0: LITERATURE REVIEW  \nThis thesis draws from three distinct bodies of literature. For the purpose of this \nliterature review, first I will review the changing landscape of media, and how legacy \nmedia differs in structure to online \ncrowdsourced messaging boards. \nSecond, I will di scuss how social media \nhas become a tool to disseminate news \nand activism within its online \ncommunities. Lastly, I will discuss how \nReddit has been used as a crowdsourced \nwebsite that enables activism and user interactions. Little research has been conduct ed \nevaluating online crowdsourced social media platforms and their portrayal of \nenvironmental news stories (ICA 2018).  \n2.1 Communication and Changing Media Landscapes  \n Scholars have pointed to the fact that new generations are increasingly relying on \nonline sources for their political news (Gottfried and Shearer 2017).  Forty -three percent \nof Americans obtain political news primarily online, only seven percent below the group \nof Americans who primarily watch cable news (Gottfried and Shearer 2017). Sixty -six \npercent of Americans obtain at least some of their news from social media sources, up \nFigure 1: The Scope of The Literature Review  \n11 \n \n five percent from 2016 (Gottfried and Shearer 2017). This data illustrates  the immense \npower social media has over news dissemination  to citizens within society.  \n This transformation from broadcast journalism to participatory media illustrates \nhow society is changing the definition and conception of media. Seethaler and Beaufort \n(2017) state alternative forms of broadcast journalism are emerging that encourage \nparticip ative communication between consumers and producers. These scholars did a \ncomparative analysis of Austrian broadcast journalism  and how these organizations have \nevolved over time. Seethaler and Beaufort also evaluated the societal perceptions \nregarding bro adcast media.  \nThey found  that several conceptions of broadcast journalism exist, and that \njournalism is evolving from one -directional communication to participatory channels in \nwhich individuals become more aware and in control of their social environment  \n(Seethaler and Beaufort 2017). This is due to the interactive nature of social media as \nopposed to one -directional broadcasted forms of media. Additionally, these scholars \nfound that younger generations value participatory forms of media at a higher rate as \ncompared to older generations (Seethaler and Beaufort 2017). This has led  to broadcast \njournalism (or legacy news media) adapting to societal expectations, largely driven by the \nperceptions of younger generations (Seethaler and Beaufort 2017). As news c onsum er \ndemographics and societal expectations change, legacy news outlets are also adapting to \nthe new online participatory journalistic era.  \n Not only are social media sites dedicated to portrayal of news topics influencing \nsociety and users  into partic ipating in the news dissemination process , but they are also \n12 \n \n influenc ing legacy media news outlets  to evolve and incorporate some form of \nparticipatory crowdsourced online forums within their content (Wasike 2010) . Legacy \nnews outlets are news systems that  were created before the internet existed. In a 2010 \nresearch study evaluating editors of legacy news outlets, George Washington Universit y \nfound that 89 percent of legacy news media editors said they utilized social media blogs \nfor research on news topics  (Wasike 2010). This illustrates how social media news \nforums influence legacy news media outlets. Legacy news outlets aren\u2019t only \nincorporating online forums, but are also utilizing social media sites like twitter, \nYouTube, and Facebook for news content. The importance of social media sites in news \ndissemination directly impacts users and crowdsourced content, however these social \nmedia spaces also directly impact the news content of legacy news outlets.  \nIn addition to legacy news media outlets adapting t o the online news era, societal \nroles for journalists are also changing. Scholars Firmstone and Coleman (2014) used \nLeeds City Council in England as a case study, where legacy forms of media were \ncompared with new modes of communication regarding interacti ons with citizens.  \nFirmstone and Coleman used semi -structured interviews to ask 20 elected politicians and \nsenior journalists about the changing media landscape.  They found that the move towards \ndigital modes of engagement are changing perceptions regardin g the professional role of \njournalists and how they dissem inate  information from  government agencies  to the \ngeneral public (Firmstone and Coleman 2014). In one interview a veteran journalist \nstated that due to understaffed news organizations, news content is produced often \nwithout checks or challenges. In other words,  as society craves faster news dissemination \n13 \n \n news outlets are pressured to provide faster content which affects the quality of news \ncontent being produced. This phenomenon  was directly related to the speed at which \nonline news outlets produce content after large societal narrative s emerge  in the public \ndomain . These scholars help delineate how the roles of journalists are changing as legacy \nmedia adapts and evolves with online media. Additionall y, Firmstone and Coleman \nassociate four distinct typologies of citizen journalism; producers, contributors, sources, \nand participants (2014). All these typologies are apparent in modern day crowdsourced \njournalism, including websites like Reddit.  \nTrust is  an important topic for citizens and news consumers when discussing news \noutlets and news dissemination. Scholars like Daniller, Allen, Tallevi, and Mutz (2017) \nall discuss how to measure  societal  trust of news sources  within a changing media \nenvironment.  Using data from the General Social Survey from 1972, the study assesses  \nmedia users\u2019 level of trust regarding various large -scale news media outlets. The scholars \ndiscussing the GSS were able to find that users view sources as more trustworthy  when \nthey are asked about specific news organizations  as opposed to when they are asked \ngenerically about news media (Daniller, Allen, Tallevi, and Mutz 2017). As news media \nchanges and is continually disseminated through social media channels, specific s ources \nare shared  and disseminated through social media sites which could lead to individuals \nvaluing these sources as more trustworthy as opposed to if they just conceptually thought \nof crowdsourced journalism without participating within it  (ICA 2018).  In other words, \nindividuals that participate in online news forums might evaluate the content as more \ntrustworthy according to Daniller, Allen, Tallevi, and Mutz (2017).  \n14 \n \n  As trust has become a central issue for citizens discussing news dissemination, \nSchola r Rodny -Gumede (2017) conducted a survey within South Africa that asked \njournalists and media advocacy groups where changes could occur so that the public \nsphere was less fragmented  regarding news topics . Her findings showed that journalists \nand media advo cacy groups believed that in order to widen the public sphere regarding \nmedia, more synergies needed to be created between traditional news media and new, \nonline social media platforms (Rodny -Gumede 2017). These synergies proposed are \nimportant for societa l cohesion regarding news interpretation.  In South Africa \nspecifically, a large portion of news consumers are cut -off from mainstream media \n(Rodny -Gumede 2017).  Often seen as two separate media entities, traditional legacy \nmedia and crowdsourced media  could influence different demographics within society  \nregarding similar news content . When utilized in synergy these united platforms have the \npotential to create content that the public sphere engages with  holistically, not just those \nwith access to mainstrea m media.  Additionally, these platforms create a more diverse set \nof debates (Rodny -Gumede 2017). This scholar illustrates the societal need for combined \nforms of legacy and new social media so that media articles and discussions represent its \nusers and con sumers as clearly as possible.  She states this is how the public can be \nencouraged to engage with news content (Rodny -Gumede 2017).  \n The landscape of media is changing with regards to environmental news \ndissemination. One scholar Detweiler (1992) evaluated  environmental rhetoric and \narticles discussed on the legacy media outlet Time magazine.  Time magazine was chosen \ndue to its perceived widescale societal impact and outreach.  Initial coding categories \n15 \n \n taken from Detweiler were air quality, water quality, h uman caused environmental harm, \nenvironmental additives, management of energy producing resources, wildlife or \nwilderness conservation, environmental movements, ecological shifts, politics, and the \nmedia itself (Detweiler 1992). Although these categories  are widescale, Detweiler \nutilized broad categories and subcategories due to the immense amount of different \nenvironmental articles within his study site (Detweiler 1992). Detweiler evaluated the \nchange in Time magazine articles from the years 1991 to 1992 r egarding environmental \ntopics. His findings state that  there was a 22 percent increase in portrayal of \nenvironmental social movements (Detweiler 1992). Detweiler also found that there was a \n13 percent decrease in portrayal of energy resource -based articles  (1992). His findings \nillustrate how quickly news outlets can change regarding their rhetoric and specific news \ntopics.  \n2.2 Social Media as a News Dissemination Tool  \nSocial Media is a powerful new platform that connects users in various ways. \nSome social media users utilize their platforms to discuss, debate, and protest news media \nthrough sharing news sources. This creates a vast ideological landscape in which \nactivism, crowdsourced journalism, and user generated public discussion forums are ripe \nwith col lectable data (Matthews 2015). This type of media and news dissemination \ncontrasts with  legacy forms of broadcast journalism, while still relying on the content \nproduced  in legacy media sources.  \n16 \n \n The massive use of social media is changing how activists communicate with one \nanother. In a case study, Thomas Poell (2014), evaluated the social media efforts of the \nToronto Community Mobilization Network, which created and facilitated the protest \nagains t the 2010 Toronto G -20 Summit. Poell (2014) found that the network encouraged \nprotestors to post about the activism and protests on social media sites like Twitter, \nYoutube, and Flickr. These posts all shared a hashtag (used on social media websites to \nidentify messages on a specific topic) phrased as #g20report. This study illustrates how \nsocial media accelerates the communication between activists and creates a public visual \ndemonstration of activism (Poell 2014).  \nAs activists utilize corporate social m edia, their data becomes in jeopardy of \nlosing control and ownership over their crowdsourced communication networks  (Poell \n2014) . This is due to the corporate ownership of intellectual property that exists on \nwebsites. Individuals\u2019 ideas and discussions oc curring on corporate social media \nplatforms belong to the social media site rather than its users, allowing social media sites \nthe ability to sell various forms of users\u2019 metadata. We must acknowledge the \nhierarchical power of how content is produced in on line communities so that we \nunderstand the neoliberal structure that still affects these spaces (Allan 2018).  \nThe digital ethics of online content are ignored due to the widescale amount of \ncontent that exists within these online spaces (Allan 2018).  In addition to activists \npotentially losing control over their communication networks  due to the public ignorance \nregarding digital ethics , social media  sites have the potential to  share and disseminate \nfalse news because of  their crowdsourced nature (Allan 20 18). Allan  criticizes social \n17 \n \n media and its dissemination process stating that the platform can easily spread \nmisinformation due to false news and sources being shared by individuals rather than \ncreated by news organizations. Because social media encompasse s a large -scale platform \nin which messages on a variety of topics are shared and discussed, the digital ethics \nregarding information become secondary (Allan 2018). According to Allan (2018) the \npolitical economy of communication channels are somewhat invis ible within social \nmedia spaces as opposed to traditional legacy forms of media. This is important to \nconsider due to the fact all digital forms of media are also embedded within an \noverarching neoliberal agenda, in which corporate and class interests domi nate these \ninstitutions\u2019 main agendas (Allan 2018).  \nAlthough social media could democratize channels of news distribution (ICA \n2018), scholars like Suciu (2018) state that the judgement that journalists are free because \nof crowdsourced journalism is naive  and short -sighted. The new forms of media \ndistribution available online offer new networks of news dissemination, however these \nnetworks are still neoliberal in nature. Additionally, corporate social media can apply \nalgorithms to how news sources and arti cles are shared (Suciu 2018). Because these \nalgorithms are not transparent to the public and online users, questions emerge regarding \nwhat content is seen by individuals on social media websites (Suciu 2018).  \nDespite social media \u2019s neoliberal nature, fund amental differences exist between \nlegacy news outlets and online crowdsourced communities. Crowdsourced articles, \narticles elevated into social media sites\u2019 influential platforms via upvotes, have the \npotential to be just as influential  in shaping public o pinion  as past forms of legacy media. \n18 \n \n Stephens and Jarvis (2016) evaluated online crowdsourced articles to see if their partisan \nlanguage was more or less prevalent than front page newspaper headlines. Their study \ndrew 302 \u201cfront page\u201d articles from 8 week s in relation to the 2012 presidential \ncampaign. The y found  that online audiences participating in crowdsourced journalism \nselect top articles that utilized language that is more partisan when compared to \ntraditional newspaper headlines (Stephens and Jarvi s 2016). In most cases online versions \nof articles featured shorter article headlines than their newspaper counterparts. In \naddition, these online headlines were less sensationalistic (Stephens and Jarvis 2016). \nThese findings illustrate how online crowdso urced forums have the potential to select and \nportray articles that establish preconceived notions already held within an online \ncommunity, effectively creating an ideological echo chamber.  \nScholar Sebastian Valenzuela (2017) used a mixed methods approach  to \nunderstand the generic frames individuals were using when sharing news sources on the \nsocial media sites Facebook and Twitter. He found that when articles have a morality \nframe they are shared at an increased rate (Valenzuela 2017). He also found that when an \narticle utilized a conflict frame or emphasized economic consequences they were shared \nless (Valenzuela 2017). These findings show that crowdsourced social media article \nauthors value different behavioral frameworks when posting as compared to trad itional \nnews journalists. This gap between social media posters and journalists can lead to \ndifferences regarding popular and shared content (Valenzuela 2017).  \n Utilizing a unique case study, scholars Greenwood and Thomson (2017) \nevaluated how social medi a users on Instagram (a mobile social media platform) react to \n19 \n \n different types of photos posted by news organizations or photographers. Their research \nattempted to understand the consumptive patterns of individuals that used social media \nregularly. Using Q methodology, these scholars had 30 participants rank various photos \nposted on Instagram. Q methodology utilizes a ranking system in which participants \norder content presented by a researcher on a scale from least liked to most liked. They \nfound  that three distinct group types emerged that liked different types of photos, feature \nlovers, newshounds and optimists (Greenwood and Thomson 2017). As online sources \ncontinual produce content, individuals select where they consume content as well as \nwhich co ntent they want to consume. This demonstrates further how social media users \ncan stay within ideological bubbles regarding content  (Greenwood and Thomson 2017) .  \n2.3 Reddit: An Influential Crowdsourced Online Community  \nScholars in the field of journalism d emonstrate the increase in social influence \nsocial media sites like Reddit have over p olitical news. Social media sites can tailor news \nmedia and content in a more digestible way for readers because it is crowdsourced \n(Wasike 2010). When individuals are ut ilizing social media to discuss news media topics, \nthey create online communities based on these  various  topics. Wasike notes s ome users \nare creating their own cyber newsrooms (2010). Reddit is a perfect example of a social \nmedia site that utilizes crowdso urced content as its product.  \n Scholars suggest when new voices emerge within Reddit online discussion \nforums, new ideas emerge. An article produced  by the International Communication \nAssociation  analyzed community posts and discussions on Reddit from the  years 2008 to \n20 \n \n 2017.  The ICA found  that new contributors to online discussion forums do not adhere to \nlinguistic norms found on Reddit, rather they contribute their own (International \nCommunication Association 2018). This article has major implications wit h regards to \nunderstanding the concept of \u201cvoice\u201d within online crowdsourced communities (ICA \n2018). How an individual contributes to a crowdsourced community is an important \nconcept to understand, because most of the content seen within online crowdsource d \ncommunities is produced by individuals not large corporations.  \n If the concept of a voice is to be fully represented, it must be evaluated as a social \npractice, not just a physiological phenomenon (ICA 2018).  Scholars state that the \nconcept of \u201cvoice\u201d is less a matter of meaning and more a matter of representation within \nthe public sphere (ICA 2018). This demonstrates how crowdsourced forums on websites \nlike Reddit, help create the voices individuals use to discuss issues on online forums. \nScholars also  discuss the constraints that individuals\u2019 influences have on the collective \nvoice. According to Muchnik, Aral, and Taylor (2013) comment ratings (like upvotes on \nReddit) do not purely reflect how different comments appeal to users but are rather a \ncombina tion of judgments of the merit of a comments and the social influence that has \naffected the commenter in past comment ratings. In other words, as a commenter \ncontinual comments and is receiving feedback from peers within an online space, how \nthey tailor co mments changes to reflect the community feedback. Linguistic and \nideological norms are then established within online crowdsourced discussion forums \n(ICA 2018). The paper \u201c The Voices of Reddit: Exploring the Effects of New Entrants on \nThe Content of Online  Discussions Conferences\u201d which appeared in the International \n21 \n \n Communication Association conference in 2018, help illustrate how crowdsourced online \nforums operate like a public forum in which different \u201cvoices\u201d are distributed into a \nperceived public spher e.  These voices produce the content on online news outlets. In \norder to study content produced on these websites  it is imperative to study how \nindividuals interpret and produce their ideas and thoughts . The International \nCommunication Association does an excellent job illustrating this vital importance.  \nMitchell and Lim\u2019s (2018) article \u201cToo Crowded for Crowdsourced Journalism: \nReddit Portability, and Citizen Participation in the Syrian Crisis\u201d, evaluated the subreddit \ndedicated to the Syrian civil war. T hese scholars analyzed how crowdsourced articles \nwere portraying the Syrian civil war within the subreddit r/SyrianCivilWar.  Mitchell and \nLim wanted to find out how crowdsourced online communities discussed and \ndisseminated information as compared to legac y media outlets.  Their findings stated that \nusers and observers deemed the subreddit had often provided more extensive information \nand faster coverage to related issues as compared to legacy news media outlets (Mitchell \nand Lim 2018).  This illustrates how  online crowdsourced communities could be more \nefficient at disseminating news as opposed to past forms of legacy media  like television \nnews shows or newspapers .  \nCrowdsourced journalism offers an interesting alternative to legacy news media \noutlets and ha s the potential to provide better coverage of issues (Mitchell and Lim \n2018). To study  the differences in news dissemination between crowdsourced online \ncommunities and legacy news outlets , Mitchell and Lim evaluated the most popular \narticles of all time w ithin the subreddit  r/SyrianCivilWar . \u201cA corpus of threads was built \n22 \n \n from the subreddit, sorting by \u2018most popular\u2019 of all time\u201d (Mitchell and Lim 2018). By \nevaluating the top upvoted articles of all time, a discourse analysis occurred utilizing the \nmost in fluential sources within the subreddit.  \nThe top upvoted sources within the subreddit help illustrate the sources the \ncommunity utilizes and ideally represents the subreddit as a whole (Mitchell and Lim \n2018). Additionally, all subreddits have a small desc ription called an \u201cabout us page\u201d \ndetailing the content that is supposed to exist within the subreddit. Within \nr/SyrianCivilWar\u2019s about us page, \u2018r/SyrianCivilWar is described as \u201cnews, analysis, \ndiscussion and investigative journalism of the conflict in S yria along with the regional \nand global ramifications\u201d (Mitchell and Lim 2018). This description establishes a serious \ntone with regards to the subreddit. By implementing and studying the nuances and \nintricacies associated with the subreddit\u2019s conceptual p age, Mitchell and Lim can more \neasily discern how the community of r/SyrianCivilWar  sees itself and how it is different \nin its rhetoric compared to  news stories produced by neoliberal legacy news outlets .  \nMitchell and Lim (2018) found that by replacing me ta-narratives within legacy \nnews media outlets, social media sites offer a new way to synthesize information. \u201cAs \nexemplified by the case of r/SyrianCivilWar, such communities can also facilitate citizen \ncrowdsourced journalism projects to potentially chal lenge legacy media narratives\u201d \n(Mitchell and Lim 2018). With monopolization and manipulation of large -scale \ndiscourses, and legitimizing large meta -narratives while undermining others, the news \nmedia can affect public perceptions on the issues and events i t covers (Chomsky & \nHerman, 2002; Hackett & Zhao, 1998). By allowing individuals to contribute and vote on \n23 \n \n topics they find of interest, legacy news outlets could be questioned and re -evaluated by \ncrowdsourced journalism on websites like Reddit. Alternativ ely, legacy news outlets have \nthe potential and financial resources to recreate their informational distribution networks, \ncreating their own forms of crowdsourced online forums, illustrating the power of \nneoliberal media and agenda -setting institutions wi thin society.  \nOther scholars have written about the implications of Reddit on public affairs \n(ICA 2018). The paper \u201cSource, Please? A Content Analysis of Links Posted in \nDiscussions of Public Affairs on Reddit\u201d was submitted to the International \nCommunica tion Association conference of 2018. This paper discussed how citizen \njournalism and crowdsourced online forums fulfill a new role where consumers can \ninteract with content produced. T raditional forms of media have failed at producing these \nnew roles for n ews consumers  (ICA 2018). The scholar also stated that little research has \nbeen done to investigate the sources citizens use within crowdsourced online forums \n(ICA 2018). The study conducted utilized content analysis and evaluated  the sources \nindividual co ntributors used within  the subreddit of r/Seattle. The findings stated that \nusers relied more heavily on alternative media sources and government sources as \nopposed to legacy media  when discussing environmental issues occurring within the city \nof Seattle  (ICA 2018). Crowdsourced online forums tend to use a wider range of sources \ndisassociated with legacy news outlets. In this case online news forums rely less heavily \non legacy news sources. Legacy news sources help create the agenda but are not a major \nfocus within sourced literature on Reddit (ICA 2018).  \n24 \n \n The problems legacy news outlets have won\u2019t be solved with social news sites; \nhowever, they offer a new valuable piece of news dissemination that may help \ndemocratize journalism (ICA 2018). In the findings  of the ICA study evaluating the \nsubreddit of r/Seattle , the agenda on Reddit and within the subreddit of r/Seattle were \nseen as limited to discussing issues that only impacted the lives of Reddit users, and \nignored large scale news like environmental issu es (ICA 2018). This is an important \naspect to consider when evaluating online crowdsourced forums. Crowdsourced articles \nhave the potential to dra w from a plethora of sources, however the way these articles are \ndisseminated vary vastly from legacy news out lets. This can be a strength and a weakness \nwith regards to the goal of democratizing news dissemination.  \nIn the study titled \u201cFreedom from the Press?\u201d Suran and Kilgo (2017) evaluate \nhow Reddit produces a public sphere in which discussion of various topics emerge. Suran \nand Kilgo see these crowdsourced websites as an alternative to legacy media outlets \nregarding the production of content. They assess the crowdsourced website\u2019s  ability to \ncreate c rowdsourced journalism  and discuss how Reddit can produce news content more \ndemocratically as compared to legacy news outlets.  Suran and Kilgo also illustrat e the \nlack of academic content that exists assessing  social media sites like Reddit. Their study \nstates that although Reddit has gained the attention of popular news media, academic \nstudies have not given the website careful consideration and attention when it comes to \nholding desirable academic content (Suran and Kilgo 2017). While there is research th at \nanalyzes how social media creates public spheres, \u201cthere is a gap when it comes to \nstudying the types of content shared on social news sites\u201d (Suran and Kilgo 2017, p. \n25 \n \n 1036). As social media presence increases, it is important academic studies evaluate this \nform of media, not just legacy media outlets  due to the changing nature of news \ndissemination .  \n2.4 Conclusion: The Gap  \n In the first part of the literature review, I discussed how consumers of news media \nare changing news distribution channels with t heir continual use of social media for news \ninformation (Gottfried and Shearer 2017). Journalists are also changing and evolving to \nthe evolution of digital media. Social media itself has exacerbated this change \nimplementing elements of democracy and activ ism into news dissemination (Poell 2014). \nThis adaptation to shifts in digital media dissemination isn\u2019t only empowering citizens \nand online news media users. Neoliberal agendas and algorithms still affect how news is \ndelivered and curated for users.   \nLittle academic research has been conducted evaluating news dissemination on \nsocial media sites like Reddit (ICA 2018).  Plenty of content exists in these spaces it just \nneeds to be mined for by academics (Matthews 2015).  In addition, past studies  conducted \nby the ICA evaluating the subreddit of r/Seattle,  have discerned that environmental news \nstories on applicable subreddits are potentially ignored (ICA 2018), justifying a study \ndedicated to specifically understanding how crowdsourced articles on social media  sites \nlike Reddit, present and discuss environmental news.  \n  \n26 \n \n 3.0 METHODOLOGY AND THEO RY \nThe methodology for this study centers itself on content analysis and social \nnetwork analysis. Both use and organize readily available data, as opposed creating  new \ndata, like the methodology of  surveying. Although both are different in producing in \nevaluating content  they also rely heavily on proper coding . In the case of social network \nanalysis graphing, techniques to accurately display their data are imperative to p roducing \na proper network (Lasswell 1948).  \nContent analysis simply put evaluates readily available data. Within the 20th \ncentury, when media became an integral part of societal influence, content analysis has \nthe ability to deeply analyze rhetoric from media and dissect media logic. \"Who says \nwhat,  to whom, why, to what extent and with what effect?\" (Lasswell 1948).  This is an \nimportant part of analyzing a crowdsourced form of journalism on the website Reddit, \ndue to the amount of discussions and content that exists within the platform.  \nWhen discus sing studies that evaluate online crowdsourced news forums, it is \nimportant to acknowledge the concept of the \u201cSpiral of Silence;\u201d if an opinion is seen as \nbeing held by a minority, individuals are less likely to discuss and comment on these \nopinions (Hans en 2018). This concept is important to note due to the fact many public \nforums champion popular rhetoric regarding a topic. Online crowdsourced forums have \nthe potential to exacerbate the \u201cSpiral of Silence\u201d which is important to consider in \nstudies evalua ting messaging forums.  \n27 \n \n Although the \u201cSpiral of Silence\u201d can affect what topics are discussed based on \npopularity and societal acceptance, the fact something is not within public discourse \noffers interesting analysis, as if it were. In other words, topics that are brought forward in \nforms of media tell us just as much about a media site as the topics media does not cover. \nThe content that does exist within online Reddit forums are seen by many users, which in \neffect represents the ideas and values of the su breddit. \u201cIf men define situations as real, \nthey are real in their consequences\u201d (Thomas 1928). Symbolic interactionism illustrates \nhow a failed interpretation of a concept does not diminish the power and influence that \nideology holds over society, because  this concept is still being internalized by individuals \n(Thomas 1928). In other words, fake articles may be presented to the public, but they are \nstill internalized as real, creating and influencing real world ideologies.  \nUsing cultivation theory , where p rolonged use of a media establishes a connection \nbetween an individuals\u2019 media use and their conception of the world,  we can examine a \nlinkage between consumption of  online media and how individuals view the nature of \nthe surrounding world (Hansen 2018). The conception of media can be evaluated as a \nnetwork in which the press, politicians, and the public influence each other (Hansen \n2018). This network enables various hierarchies of public influence depending on the \ntype of media being represented. For exa mple, television news media is heavily \ninfluenced by large corporate media structures and politicians, however due to its \nbroadcast nature, public interaction is limited compared to crowdsourced online news \nforums. The media itself offers insight and refle ction into public institutions, due to it \noften being the most influential source of public opinion (Hansen 2018). Now that there \n28 \n \n is a transformation in use from legacy media outlets to crowdsourced online forums, \npublic opinion is heavily representative o f individuals participating within the forum, \nmuch like a town hall as opposed to a press conference.  Utilizing this theory  illustrates \nhow crowdsourced online spaces have the potential to democratize news dissemination \ndue to how individuals produce the crowdsourced communities\u2019 content themselves as \nopposed to paid reporters from news corporations.  \n3.1 Theoretical Frameworks for Coding Environmental Journalism  \nThe way in which comments and articles are coded play an integral role in \ndeveloping a proper content analysis study. For this study applying coding processes that \ninvolve the media and environmentalism are an important key to understanding the data \nthat exists within the study. With multiple studies that have already evaluated political \ncontent on  Reddit, as well as Brulle (2003) and Detweiler\u2019s (1992) studies on \nenvironmental movements and journalism, a coding process (which I will describe in the \nMethods section) offers a way to decipher the large amount of data collected.  \nScholars like Brulle ( 2003) and Detweiler (1992) illustrate excellent ways to code \nfor environmental rhetoric within media content. Their coding processes can be applied \nto studies that wish to evaluate crowdsourced forums on social media sites like Reddit. \nDetweiler\u2019s study wh ich evaluates Time magazine and their portrayal of environmental \nissues offers insight into how environmental rhetoric is portrayed within legacy media \narticles. It also illustrates how powerful legacy news media outlets are in disseminating \ninformation.  \n29 \n \n Understanding how media operates is an additional consideration studies must \nconsider when evaluating online crowdsourced journalism. Utilizing the theory of media \nframing, a branch of Goffman\u2019s frame analysis, I can explore how media portrays various \ntopics when evaluating crowdsourced article headlines. Media framing enables studies \nevaluating crowdsourced journalism to acknowledge the plethora of issues and ways \nindividuals, and media, represent various forms of content. This is important to note for \nany study evaluating forms of news media and dissemination because it sets the context \nfor news outlets and social media sites.  \n Often seen as a uniform movement, the environmental movement is widely \ndiverse in actions, beliefs and attitudes (Brulle 2003). N ot all environmentalism is the \nsame; in fact, most movements have varied specific goals set within the larger frame of \nthe environmental movement. This variation is crucial to understanding how \nenvironmental identities emerge within political spaces, envir onmental organizations, and \nnews outlets. These ideological areas all play a heavy role in my case study of the \nr/environment subreddit. For my study I outline four major discourses of environmental \nmovements drawing from Brulle\u2019s (2003) article entitled \u201c U.S. environmental \nmovements\u201d. Moreover, Detweiler\u2019s (1992) thesis evaluating the New York Times and \nTime magazine's portrayal of environmental issues and topics from 1991 and 1992 will \nbe instrumental within this study\u2019s coding process.  \n3.1.1 Conservation and wildlife management  \n The first type of environmental movement involves evaluating and managing \nenvironmental resources. This discourse acknowledges the fact human society is \n30 \n \n dependent on natural resources. Conservation and wildlife managem ent must exist so that \nexcess resources are not drawn from nature, heavily placing responsibility on human \nsystems to understand the power they hold by extracting resources for society. By \nconserving natural resources, those resources are not left alone, b ut instead managed by \nvarious societal systems that interact with them. Conservation and Wildlife Management \ngroups received approximately 25 percent of the total budget allocated to various \nenvironmental organizations (Brulle 2003).  \nThis form of environm entalism is the second most funded behind preservation \n(which will be discussed later), asserting itself as a dominant ideology within the \nenvironmental movement. Due to the amount of money this form of environmentalism \nhas access to, I expect it to be qui te present within the environment subreddit. Detweiler\u2019s \ncoding process includes management of energy producing resources, air quality, water \nquality, human caused environmental harm, and environmental additives which all play \nan integral role within the m anagement of resources discussion. I expect this category to \nbe highly represented within my study.   \n3.1.2 Wildlife initiatives  \n The second movement type entitled wildlife initiatives involves the philosophy of \npreservation. This movement sees nature not as a machine but as its own intact organism. \nHuman actions can impair the ability of natural systems to maintain themselves or to \nevolve further (Brulle, 2003). Preservation as an environmental discourse is represented \nin 50 percent of environmental moveme nts income (Brulle, 2003). This is the highest \nfunded form of environmentalism that exists, possibly due to the large amount of costs \n31 \n \n preserving lands entails. Preserving areas is expensive due to the monetary loss that exists \nfor industries and government s. Preservation also calls into question our human value \nsystem.  \nValue extends beyond monetary gains to also include preservation of wild lands \nand wildlife. This departure from an anthropocentric value system realizes the needs of \nfuture generations of h umans; nature and wildlands are diminishing making them \nexponentially more valuable in the future. Detweiler (1992) also has coded for this \nmovement utilizing wildlife and wilderness conservation and preservation, although \ndocumented within their respectiv e studies it will be fascinating to apply these coding \nprocesses to new forms of crowdsourced journalism within the r/environment subreddit.  \n3.1.3 Reforming of society environmentalism  \n The third environmental movement, which contains the most environment al \nsubtypes, is entitled reforming society environmentalism (Brulle 2003). Within this \nmovement, individuals believe that we must reform society and human political systems \nin order to truly establish an effective form of environmentalism that is not behol den to \nmonetary and political systems. Reform  of environmentalism, a subcategory, establishes \nthe notion that natural systems are the basis of all organic existence, including humans. In \nother words, humankind's survival is linked to ecosystem survival. Pr oper use of natural \nsciences may help guide the relationship between humanity and the environment.  \nThe next subtype in reform of environmentalism is anti -globalization. The anti -\nglobalization green movement prides themselves on the premise that all humans  and their \ncommunities deserve to live in an equitable just and environmentally sustainable world. \n32 \n \n Global capitalism and political/economic forces have exacerbated inequality leading to \napolitical \u201cdemocratic\u201d societies that speak on behalf of monetary val ues rather that \ncitizen values. These interlocking coercive powers need to be eliminated for any true \nform of environmentalism to emerge.  \nSimilarly, environmental justice calls for this inequality to be eliminated, however \nthis subtype goes further to say  that local communities must be empowered for social \nchange to occur on a wide scale basis. Reform of Environmentalism only takes up 2 \npercent of the total environmental movement budget (Brulle 2003). Perhaps this is due to \nthe fact these groups call into question societal systems and advocate direct major change \nregarding equity within society. Despite its low level of funding, I expect this form of \nenvironmentalism to be highly present within the subreddit due to the social justice \nnature of the internet and crowdsourced article forums (Poell 2014).  \n Continuing with the discourse of societal reformation includes environmental \nhealth. This subtype claims no technology should be used unless proven harmless to the \nenvironment. As a part of the ecosystem ours elves, human health is based on our \ninteractions with toxic ecosystems. The subtype of ecofeminist environmentalism claims \nandrocentric concepts, values and institutions are the core of society and that in order for \ntrue environmentalism to occur these ins titutions, values and ideologies must be \neradicated (Brulle 2003). The last subtype of societal reformation involves animal rights. \nThe main premise of this subtype involves the concept that life has a right to develop \naccording to its own character and th at humanity has no right to infringe upon these \nrights of other animals (Brulle 2003).  \n33 \n \n 3.1.4 Eco-philosophies  \nThe last environmental discourse involves eco -philosophies. Based on the notion \nof changing societal belief systems, these forms of environmental ism illicit a form of \nspiritualism in which nature is enjoyed, respected, and deemed in a sense holy. Eco -\nspiritualism a subtype of eco -philosophies claims that religious beliefs need to develop \nand embody this eco -centric ethic so that our foundational mo ral belief system is \ningrained with nature, much like many indigenous cultures. The other subtype within \neco-philosophies is deep ecology. This subtype claims all life on earth has intrinsic value \nseparate from how humans perceive this value system. Presen tly humans hold an \nanthropocentric view on how society should operate and work with natural systems.  \nTo affect this anthropocentric ideology, society must decrease human intervention \nand increase wilderness areas. How we view nature must fundamentally cha nge at the \nbase level of our humanistic culture. These are the least funded environmental groups that \naccount for less than half of a percent (Brulle 2003). This might be due to the fact they \naim to change attitudes and beliefs; however, these ideologies a nd frameworks are \ncontrary to our modern form of capitalism in which human individuality and success is \nparamount over the environment and eco -spiritualism. I do not expect this form of \nenvironmentalism to be discussed much on the subreddit, however when t his form \nenvironmentalism emerges, I believe it will be quite profound and provoke discussion \nand thought within the various forums on the environment subreddit.  \n All these discourses offer different ways to interpret nature and human \ninteractions with na ture. Many of these environmental movements exist on online \n34 \n \n forums, and it is important to the purpose of this study to discern the differences due to \nthe fact this study evaluates what types of environmental discourse emerge within the \nenvironment subredd it. Contradictory values may exist within the data and types of \nenvironmentalism, despite the fact all self -deem themselves environmentalists. It is \nimportant to note to see where cohesion and disconnect exist. Additionally, by \nunderstanding the funding th at exists behind each movement, we can see what forms of \nenvironmentalism crowdsourced news users are advocating or rejecting. Brulle (2003) \nand Detweiler\u2019s (1992) studies offer my study ways in which I am able to code, interpret, \nand discuss data I find w ithin discussions relating to the environment on Reddit.  \n35 \n \n  4.0 METHO DS \nThe purpose of this study aims to illustrate how online communities accept, \ninterpret and communicate with various forms of environmental journalism . This study \nuses a mixed methods approach  including  secondary data analysis, archival research of \nonline forums, and a social network analysis within the social media website Reddit. The \nstudy site will be the subreddit (discussion  board within the website) r/environment due \nto its discussion of environmental issues.  \n4.1 Study Site  \nBy utilizing the top followed sources within the subreddit r/environment, this \nstudy identifies  r/environment\u2019s most influential pieces of content. All studies evaluated \nwithin the literature review  that analyze reddit utilize  top posts within a certain subreddit \nto better understand influential content . By gathering the top posts within the subreddit, \narticles and comments that are the most influential a re evaluated and coded (Suran and \nKilgo 2017). Reddit as a website and study site enables the collection of data due to its \ninteractive interface. Sorting by the top posts and comments within a subreddit is \naccessible, which is not common in many social me dia sites.  \nDates will be collected, illustrating  the timeframe top articles and posts were \nproduced (Suran and Kilgo 2017). This will create a time -based representation of when \npeople use and share within the subreddit of r/environment. The depth of data covered in \nmy project will help establish wide scale correlations I can evaluate and gain knowledge \n36 \n \n  from  regarding portrayal and communal dissemination of environmental topics . All data \nwill be collected by early February 2019.  \n4.2 Social Network Analysis  \nI have created a social network analysis so that a macro -level ideological map \nexists that shows the various types of environmentalism found within the subreddit. A \nsocial network analysis evaluates and describes communication patterns and could \npotentially identify community structures as well as the most influential users within a \nforum (Moessner 2018). The nodes of this map will be r/environment articles and the \ncolor of nodes will be the various types of environmental journalism portrayed (util izing \ncoding from Brulle and Detweiler). The edges (lines connecting nodes) within the social \nnetwork analysis will be the primary sources of articles. Usually this method is utilized in \na corporate setting with regards to consumptive patterns, however it is extremely \napplicable in understanding how news sources are disseminated within crowdsourced \nnews communities. This method will  represent the sources that are being crowdsourced \nby r/environment\u2019s community. The amount of upvotes the articles receive wil l represent \nthe node size, illustrating an article perceived level of influence.  \nWithin reddit anyone can post an article, but that article must have a link to an \nadditional source or webpage detailing further information based on the article headline. \nBy understanding the shared sources of the community, the purpose of this study will be \nto gain a better insight into where most sources come from on the internet. This social \n37 \n \n  network analysis will provide the best overall insight into all the data collected , detailing \nmy study within an ideological map.  \n4.3 Content Analysis  \nI have evaluated, assessed, and coded article titles and top comments to see how \ncommunities react to news articles a s well as  see whether various communities respond \nwith similar or diff erent types of environmental rhetoric to the articles that are \ncrowdsourced. These comments are quantitatively organized utilizing themes from Brulle \n(2003) and Detweiler (1992). Quotes have been  recorded with each environmental article \nso that future quot es may be used to accurately represent articles and comments.  \nBecause all news sources fall within the subreddit of r/environment, the social \nmedia subreddit can also be evaluated to see the community\u2019s macro -level environmental \nphilosophies (based on dif ferent forms of news sources). I have compiled the dates \nassociated with articles so that discussions and forums have a timeframe in which they \noccurred. This illustrates when various types of articles emerge within forums and how \nthey relate to larger soc io-cultural phenomena. The upvotes for both articles and \ncomments will be documented so that the scope and scale of comments and articles can \nbe analyzed.  \nData was categorized using Detweiler (1992) and Brulle\u2019s (2003) categorization \nof discourses. When r epresenting the data, categories emerged that are representative of \nthese codes for articles. These will be different sections within the data. Comments were \nanalyzed with averages for the overall study, showcasing what types of comments \n38 \n \n  different articles  create. This will provide interesting context for each of the article types. \nFor example, articles dealing with the political realm of the environment will measure \nwhether political comments also come from these article types. Bar graphs will be created \nto illustrate this phenomenon.  \n4.4 Coding Process  \nAs stated above, 100 of the top \u201cliked\u201d articles within the subreddit r/environment \nwere evaluated. The subreddit itself has approximately 500,000 subscribers. The top 100 \narticles will be collected and dis cussed. Within these top 100 articles 2 of the top \n\u201cupvoted\u201d comments will be coded for and discussed to see how the community is \ninterpreting and projecting various types of environmental information.  \nWhile that data was being collected, the amount of up votes, dates the posts were \ncreated, and the main sources were coded for (Suran and Kilgo 2017).  This process \nemulates past studies that evaluated subreddit communities  such as Suran and Kilgo \n(2017), and the ICA (2018) Reddit studies . These codes help il lustrate the social network \nanalysis (which I have discussed above). By coding for Reddit\u2019s macro -data influence \nand scope will be accounted and coded for, not just content. This benefits a cademic  \nstudies that wants to evaluate large scale information netw orks and their societal \ninfluences.  \nLeadership development suggests that successful leaders move beyond \ndeveloping a personal identity to also develop a strong sense of collective identity (Ibarra \n2009). By evaluating the most \u201cupvoted\u201d comments within th e r/environment subreddit, \n39 \n \n  we can democratically discern leaders based on their rhetoric and its reception from \nindividuals within the community. This is a form of democracy in which the best or most \nliked ideas and concepts rise to the top. The process of  liking and commenting in a \nsubreddit resembles a town hall meeting, however the space itself holds more \nmeasurable, quantitative data.  \nWithin the 100 articles and 200 comments, the purpose of this study is to code for \nthe various discourses of environment alism that are portrayed and discussed  in order to \nbetter understand how environmental news is disseminated in crowdsourced social media \nspaces . These codes are heavily influenced by Scott Detweiler\u2019s (1992) thesis studying \nmedia portrayal of environmental  issues and topics. Initial coding categories will be air \nquality, water quality, human caused environmental harm, environmental additives, \nmanagement of energy producing resources, wildlife or wilderness conservation, \nenvironmental movements, ecological s hifts, politics, and the media itself. These coding \ncategories were refined while the coding processes took place and applies to both coded \narticles and comments. Brulle\u2019s (2003) environmental discourses provide background \ninformation for these coding cate gories and illustrates how various individuals interpret \nenvironmental issues and content within society. These coding mechanisms combine to \nrepresent the data as well as possible.  \n \n40 \n \n  5.0 RESULTS  \n \nFigure 2: Word cloud of top 100 articles found in r/environment in the shape of the \nReddit Logo (Word Frequency > 3)  \n \n A word cloud was produced in order to allow the data to represent itself before I \ninterpret and convey its meaning. The face and antenna in the middle is the logo of \nReddit and the words around the logo represent the most used words in the top 100 article \ntitles found in r/environment. The variance in color does not represent meaning, it is only \napparent to create eligibility of words within the f igure. The larger the word, the more \nfrequently it appears within article headline titles produced by users who posted outside \nsources to r/environment.  \n  \n\n41 \n \n  5.1 Environmental Articles  \n Within the top 100 upvoted articles in the subreddit r/environment, many various \nforms of environmental articles emerged. The subreddit\u2019s description defines itself as a \nsubreddit for \u201cCurrent news, information and issues related to the environment\u201d (Reddit \n2019) . The most prevalent accounted for article category within the stu dy related to \npolitics. 42 out of 100 of r/environment articles discussed the various facets and problems \nfacing the political side of the environment. Many of these articles specifically related to \nthe Trump administration's ability to address environment al problems. These articles are \nillustrated throughout this results section with the color red.   \n The next largest section within the environmental subreddit was management of \nenergy producing resources. 19 out of the top 100 articles within the environme nt \nsubreddit discussed how management of energy resources occurs and being implemented \nin society. These articles also consider how energy resources might be changed so that \nthey are more environmentally friendly and sustainable. The color chosen to repres ent \nthese articles within the result section is blue.  \nThe next largest section were articles concerned with Environmental movements \nand groups relating environmental messages. This section held 12 out of the top 100 \narticles and are represented using the color gray. Wildlife preservation was the next \nlargest section with 10 out of the top 100 articles within r/environment. These articles use \nthe color green in various figures throughout the results section. Environmental pollution \nis accounted for in 10 ou t of the top 100 articles and are represented with the color \n42 \n \n  orange. Next, media related issues, found in 5 out of the top 100 articles, is represented \nwith the color yellow. Lastly, the topic of ecological shifts was only included in 2 out of \nthe top 100 articles within the environmental subreddit and are represented with the color \nblack.  \n With the exception of the section of articles concerned with politics, topics are \nsomewhat diverse and evenly distributed. Various subtopics also emerged within these \nsections creating greater discourse regarding their respective areas of topics as well as \nenvironmentalism. These subtopics will be discussed here in the results section.  \n \nFigure 3: A Pie Chart of Environmental Articles by Environm ental Type  \n\n43 \n \n   \nFigure 4: A Social Network Analysis of the Top 100 Environmental Articles in \nr/environment  \n \n5.2 Social Network Analysis  \nThis social network analysis was created to illustrate and implement an \nideological map of differe nt types of content posted within the subreddit of \n\n44 \n \n  r/environment. Interesting findings emerge within the social network analysis of \nenvironmental articles. Sixty -six percent of articles did not share a common source; this \nconcept is illustrated by nodes th at do not have an edge or are not attached via a grey line. \nThose nodes that do have a shared edge essentially share a common source for \ninformation; thirty -three percent of articles had at least one shared source. Ten articles \nwere sourced from the news o utlet The Guardian all of which were diverse in themes. \nThree articles related to wildlife preservation, three related to management of resources, \nthree were political articles, and one discussed ecological shifts . These linkages are \nlabeled on the Social network analysis with an A. The Washington Post  was the second \nmost popular media source, accounting for seven out of the 100 articles. Four of these \narticles directly related to politics while one related to mana gement of resources, one \nrelated to ecological shifts (global changes in various ecologies) and one related to \nenvironmental pollution. These articles are labeled within the social network analysis \nwith a B.  \nSix articles were self -sourced within Reddit an d half of these articles related to \nr/environment\u2019s activism, where users call to different forms of political action, within \nthe subcategory of environmental movements. Two of the articles addressed Politics, and \none addressed the media portrayal of the e nvironment. These articles are labeled with a C \non the social network analysis. Thinkprogress.org  had six articles which were also \ndiverse in their representation of issues. Two were political articles, two discussed \nwildlife preservation, one discussed en vironmental pollution, and one discussed \nmanagement of energy resources. These article linkages are labeled with a D on the \n45 \n \n  social network analysis. Business Insider had five sources, three of which were related to \npolitics, while two related to environmen tal social movements. These article linkages are \nlabeled with an E. The online news source The Hill accounted for four articles all relating \nto politics. These linkages are labeled with an F on the social network analysis. The last \nmajor shared source was the news outlet The Independent . This source was linked to four \narticles and had two articles addressing management of resources, one related to politics, \nand one discussing environmental movements. These linkages are labeled with a G on the \nsocial network  analysis.  \n5.3 Timeline of Articles  \n \nFigure 5: A Timeline of the top 100 articles in r/environment  \n\n46 \n \n  The timeline of top articles within r/environment illustrates when top articles \noccur chronologically. All the top 100 articles occurred after the date of 7/1/2016. \nMoreover, a major increase occurred right after the election of Donald Trump on \n11/8/2016 (only two top articles were posted before his election). Interestingly, within the \ngraph there is a cluster of political  articles right around the 2018 November elections. All \nthese dates display how r/environment reflects and reacts to American politics, despite \nenvironmental issues and news existing within a global phenomenon. The one outlier \narticle, discussing Bill Gate s and his funding for renewable energy, towers over other \narticles at nearly 50,000 upvotes.  \nThe number of subscribers for the subreddit r/environment changed after the \nTrump election as well. The day Trump was elected there were only 155,000 subscribers \n(redditmetrics.com 2019). By the same date the following year the number of subscribers \nincreased to approximately 377,000 showing that an extraordinary amount of users that \nstarted participating within the subreddit after the election of Donald Trump. As o f May \n2019, there are 550,000 subscribers illustrating that the trend to participate in the \nsubreddit is continually increasing  as the Trump presidency continues  (redditmetrics.com \n2019).  \n5.4 Comments on Articles  \n The top two comments of each environmental  article were also coded to gain a \nbetter understanding of the community\u2019s reaction to these articles as well as to \nunderstand how they manifest ideals of environmentalism within the environment \n47 \n \n  subreddit. Out of 200 comments, 126 were comments providing a dditional information, \n38 were a call to action in some sort of way, 17 were jokes, and 19 were questions asking \nfor additional information. These codes are important for further dissection of concepts \nand understanding common ideals regarding actions and information the community \nagrees upon.  \nThe next coded item within the comment section discusses environmental \nrhetoric within comment texts. Ninety -nine out of 200 comments directly related to the \npolitics of the environment. This was an increase from the  representation within the \narticles themselves; 42 percent of articles discussed politics while 49.5 percent of \ncomments discussed politics. The next largest theme in the comments was economy and \nglobalization which contained 24 out of 200 articles. Reform  of environmentalism \naccounted for 21 out of the 200 articles showcasing its importance as well within the \ncomment section. Reform  of environmentalism comments were categorized based on \nthese comments discussing how social movements of the environment in s ociety could \nbe potentially reformed.  \nThemes of environmental justice, conservation, and animals and food production \nmaintained surprisingly low frequencies during coding, all represented in just four \ncomments each. Lastly wildlife preservation was appare nt in only two comments. These \noverall frequencies help illustrate the community\u2019s overall concerns regarding our \nenvironment and potential environmental solutions to the complex problems that exist. \nIn the upcoming sections in this chapter, which layout a ll of the article types in the \nstudy, I will dissect the environmental articles that elicited different types of comments, \n48 \n \n  additionally evaluating and comparing, averages associated with each article \nsubcategory.  \n5.5 Politics  \n Forty -two out of the top 100 articles within the environment subreddit dealt with \npolitics. Within this theme, three major subcategories emerged. The subcategories of \nthese articles include, transparency, policy, and simply put Trump. 10 out of the 42 \narticles with in this section addressed transparency. Policies were addressed by 13 articles, \nand articles relating to solely Trump and his rhetoric accounted for 19 out of the 42 \narticles. Articles about Trump are the most popular and seems to be the most influential \nwithin the subreddit of the environment.  \nTransparency:  \nDiscussion of transparency revolved mostly around the Trump administration's \nability to affect environmental policy and discredit research and documents regarding \nclimate change. 10 out of the 42 artic les dealing with politics discussed transparency. \nThese articles also discussed how the Trump administration is attempting to defund the \nEPA and alter its main goals. Articles like, \u201c Leaked memo shows EPA told employees to \nlie about climate science\u201d (Reddi t 2017), question the integrity of the government and its \nability to affect the various sectors and their main mission statements. It also showcases \nthe divide between political parties regarding environmental issues, especially with \nregards to transparenc y.  \n49 \n \n  Another notable headline reads, \u201cTrump officials erase \u2018shocking\u2019 amount of \nclimate data from yet another website: You paid for U.S. Geological Survey climate data, \nbut the White House is making it disappear\u201d (Reddit 2017). Not only do articles like th is \nconvey the concept of how the Trump administration is altering major institutions \nregarding climate science, but they also illustrate the ability and power of a government \nto alter research and knowledge within a desired sector. Articles like this may a ffect how \npeople perceive power within the United States Government. In the article titled \u201cI am an \nArctic researcher. Donald Trump is deleting my citations\u201d (Reddit 2017). It sounds like \nDonald Trump himself is deleting the articles, creating disdain for the man himself rather \nthan his policies or administration. This contributes to a major concept within my paper \nwhich I will discuss further in detail; the concept of the \u201cTrump effect\u201d. Many of these \narticles discussed Trump himself and his beliefs rather  than his policies or \nadministrations ability to affect transparency of major institutions with regards to \nenvironmental policies.  \nAlthough three distinct categories emerged regarding politics, many themes \noverlapped including transparency and the Trump a dministration. The main distinction in \ncategorization is that some article headlines dealt more with transparency as a driving \ntheme as opposed to distaste for Trump\u2019s environmental policies. Many articles \ndiscussing transparency, also discussed the Trump administration\u2019s lack of transparency \nregarding the environment however, not all articles regarding transparency solely \ndiscussed the administration and its policies. The article title, \u201cWoman Dragged Out of \nWest Virginia House Hearing For Listing Oil and Gas Contributions to Members\u201d \n50 \n \n  (Reddit 2018), entailed a more general discussion on government corruption regarding \nalliances to major industrial powers. Transparency articles like this one call into question \nthe government's motives, as major industrial co rporations affect policies regarding \nmacro -level issues like the environment.  \nArticles that address the political aspects of transparency are important in \nevaluating how vital information is disseminated and distributed through political realms \nlike the l egislative, the judicial and the executive branch. Additionally, they hold \nadministrations accountable regarding their various platforms that discredit, hide, and \nalter information regarding issues like the environment.  \nPolicies:  \nPolicies regarding the en vironment were extremely prevalent within \nr/environment, and specifically in political articles. This may have something to do with \nmany of the top articles existing within the election year. Once again, many of these \npolicies overlapped with the Trump adm inistration's change in discourse regarding the \nenvironment compared to past administrations and governments. 13 out of the 42 \npolitical articles addressed various environmental policies. \u201c Trump's America First \nEnergy Plan Actually Leaves America Behind - \u2018there\u2019s not a word about the clean \nenergy revolution, a boom in wind, solar, and energy efficiency that is creating millions \nof jobs, saving billions of dollars, and even saving lives by cutting pollution\u2019\" (Reddit \n2017). This article took Trump\u2019s slogan of Make America Again and reformed it to state \nthat his administration\u2019s environmental policies leave America behind.  \n51 \n \n  An article headline like this may be an interesting attempt from its author to \nbridge political parties regarding the environmental issue . It may also have the opposite \neffect; this article also had 19900 upvotes, making it the most liked article within the \npolicy section of political articles. It seems the online environmental community of \nr/environment would support the green new deal rat her than just revitalizing past forms \nof energy production. Another article titled \u201cNew Poll Shows Basically Everyone Likes \nAlexandria Ocasio -Cortez's Green New Deal\u201d (Reddit 2018) illustrates policies the \nr/environment community sees as bi -partisan and po pular amongst the general \npopulation.  \nTwo articles from the sample discussed the Paris climate agreement and how \nTrump is planning to pull out of the deal. The popularity of articles like, \u201cTrump has \nreportedly decided to withdraw from the Paris climate d eal\u201d (Reddit 2017) elucidates that \nthe subreddit of the environment values and sees importance to remain in the Paris \nclimate deal. The two articles dealing with this direct topic of pulling out of the \nagreement totaled approximately 38,700 upvotes illustr ating the deemed importance of \nthe topic. Another article on the topic discussed potential motives for Trump\u2019s \nadministration decision to pull out of the deal; \u201cThe Republicans who urged Trump to \npull out of Paris deal are big oil darlings: Twenty -two sena tors wrote a letter to the \npresident when he was said to be on the fence about backing out. They received more \nthan $10m from oil, gas and coal companies the past three election cycles\u201d (Reddit 2017). \nThis article essentially investigates how lobbyists wor k within congress, but also puts \n52 \n \n  blame on Republicans for reinforcing the notion to pull out of the deal. This enlightens \nreaders to how environmental topics fit within narratives regarding party politics.  \nTrump:  \nThe last section within the politics narrat ives of the data examines what I am \ncalling \u201cthe Trump effect.\u201d Throughout the data analyzed above, Trump is already the \nmost recurring theme regarding the environment and environmental articles within \nr/environment. This theme is so prevalent I felt it ne cessary to devote an entire section \ndiscussing Trump within the politics section of my data. 19 out of the 42 articles relating \nto the politics of the environment discussed Trump and, in some cases, his controversial \nremarks. This categorization was made d ue to many articles discussing how dangerous \nTrump\u2019s presidency has the potential to be. These were the main focus of Trump related \narticles. The article, \u201cTrump\u2019s presidency is dangerous for the planet\u201d (Reddit 2017), \nestablishes the fact the subreddit do es not seem to support President Trump, and that, as \nan individual, he is a threat to the environment and potential environmental policies. \n\u201cDonald Trump\u2019s stance on global warming is \u2018sociopathic, paranoid and malevolent\u2019, \nworld -leading economist says\u201d (R eddit 2017), an article which received 15,900 upvotes, \ndoesn\u2019t discuss much other than Trump's perceived incompetence. It will be interesting to \nsee the types of comments regarding Trump articles to see if they continue to discuss his \nindividual incompeten ce or whether they attack his various policies and transparency \nissues.  \nArticles such as, \u201cDem lawmaker: Trump 'tweeting like a child who hates science \nclass'\u201d (Reddit 2017), again lack critical political substance regarding the environment, \n53 \n \n  however, the articles does showcase the Trump effect, and how he individually is \nperce ived to be a danger to the environment. \u201cFrance lures U.S. scientists with anti -\nTrump climate grants. Several U.S. -based climate scientists are about to win multi -year, \nall-expenses -paid grants to relocate to France\u201d (Reddit 2017), discusses France\u2019s attem pt \nto create an anti -trump policy which in effect transfers discussions from environmental \nissues to anti -Trump issues. The Trump effect has an immense effect over environmental \ndiscussion, and it will be interesting to see how commenters react to these va rious \npolitical articles.  \n5.6 Comments within Political Articles:  \n Various  types of  top upvoted  comments emerged on the political articles detailed \nabove. 55 out of 84 comments in the  political article section dealt directly with politics. \nEconomy and globalization was the second largest comment theme represented in 11 out \nof 84 comments. Reform  of environmentalism was mentioned within seven comments. \nDiscussions of individualism were prevalent within three articles, Media was mentioned \nin two, and wildlife \nmanagement in one. \nDifferent types of \ncomments also \nemerged; 55 of 84 \ncomments related to \nsupplying additional \n Figure 6: Bar graph of political comments within political \narticles  \n54 \n \n  information, while 19 discussed a call to action in some sort of way. S ix comments asked \nquestions while four were meant as a joke.  \nThe comments that dealt with politics directly related to articles discussed within \nthe political realm. Many comments that relayed information directly discussed the issues \nrevolving around Tru mp and his administration. \u201cI think Trump just doesn't care about \nthe environment, wildlife or anything along that line\u201d (Reddit 2018), helps illustrate the \n\u201cTrump effect\u201d that has taken over social media and social media portrayal of the \nenvironment withi n r/environment.  \nFive comments that were a direct call to action discussed transparency related \nissues in various ways. One comment stated \u201cBy forcing companies to reveal all \npayments, it is harder to disguise such payments and when a company does so, it is \nclearly illegal as opposed to being in the grey area. Another advantage of the \ntransparency law would reveal how different countries act as tax havens making \nexemptions for large companies and get them to channel their profits through those \ncountries\u201d ( Reddit 2017). Another comment stated \u201cIf we ever get through this era of \nTrump, I feel like laws need to be passed against blatantly lying to the public. I know \nthings like laws and morals don't seem to faze this man but for goodness sake we need to \nstart somewhere\u201d (2017).  \nA last example stated \u201cI will never understand why the US has no unified federal \nombudsman laws. They're not uncommon in other countries, and the US clearly needs \nthem\u201d (Reddit 2018). Transparency seems to be the number one call to acti on within the \npolitical comment section. This trend mirrors the trends of article titles relating to the \n55 \n \n  political realm of the environment. The top comments within the community seems to \nview these issues as directly relating to change that can be impleme nted right away \nwithin our political system.  \nComments discussing politics within political articles are above the study\u2019s \noverall average  meaning political articles elicit political discussions higher than average  \n(Figure 6 ). This makes sense due to the similar rhetoric espoused in both the articles and \nthe comments. Additionally, the only other comment category that was higher than  the \noverall average of the study was  the comment section that discussed  the economy and \ngloba lization ( Figure 6 ). This shows the importance of both topics within the subreddit \ncommunity of r/environment.  \nEconomy and Globalization was the second largest comment type within the \npolitical category, encompassing 11 out of 84 comments. Many comments d iscussed \nissues relating to society ignoring the renewable energy industry. \u201cBy just focusing in \ncoal and oil, America will be left behind economically from competition\u201d (Reddit 2017). \nOther comments critiqued the profit motive within capitalism due to its  inherent ability to \nvalue cost effectiveness over the environment and human health. \u201cthe same man that \nfought the phasing out of asbestos (a substance proved to very prone to cause cancer) \nbecause it was cheap\u201d (Reddit 2017). One commenter did an excellen t job summarizing \nthis section within the comments. \u201cI don't understand how any of this is conservative or \nrepublican at all. This isn't even an ideological thing, it's just about money\u201d (Reddit \n2017). It seems commenters relating political issues to the g lobal economy want to \ndiscuss the economic influences within politics as opposed to just politics itself. The topic \n56 \n \n  of economy and globalization was represented above average within the political articles \nsection ( Figure 6 ).  \nRhetoric around reform of environmentalism appeared in seven comments. A few \nof these top comments were upset with the other comments relating to politics. \u201cI'm \ntalking to my brother about climate change and just every single post is donald did this \ndonald [sic] did that. I'd just like  some interesting facts about what's going on. But it's just \nr/politics at this point\u201d (Reddit 2017). This comment was one of two within my study that \nwere upset with r/environment\u2019s hyper focused rhetoric on Trump, further illustrating the \n\u201cTrump effect\u201d.  Another comment within the section discussed danger with terms like \n\u201cbelief in science\u201d. \u201cWe have to stop talking about \u2018belief of science\u2019. This is not \nsomething you choose like a religion. The correct way that we should be saying is is \n\u2018We\u2019ve got people  in charge of important shit who don\u2019t understand science\u2019\u201d (Reddit \n2017). Although comments regarding reforming environmentalism did not deal with \nreforming environmental movements within the political articles section, they did discuss \nways in which envi ronmentalists or those who are part of the environment subreddit can \nalter discussions relating to these issues. Reform  of environmentalism, or the call to \nchange current environmental social movements, was underrepresented compared to the \noverall average within political articles.  \n5.7 Management of Energy Resources  \n Nineteen out of the top 100 articles within the environment subreddit dealt with \nhow management of energy resources is harvested and distributed. Discussions also \n57 \n \n  talked about how these proces ses can change for the people\u2019s benefit. Within this section \nof articles, 4 major subtopics emerged. Eight out of the 19 articles dealt with specifically \nthe topic of fossil fuels and 7 out of the 19 articles discussed various policies regarding \nmanagement  of energy resources. These articles discussed various ways the government \nis implementing policies that help form how management of energy resources exist, \npolitically. Finally, four out of the 19 articles discussed renewables and consumption \nwithin socie ty, offering potential solutions and possible ways to conduct these proposed \nsolutions.  \n5.7.1 Fossil fuels \n Within the subcategory of fossil fuels, articles discussed the coal industry\u2019s ability \nto effectively alter discourse regarding their industry. In addition, articles discussed how \nsome CEOs and higher ups within the industry admitted contradictory facts regarding \ntheir industry such as the claim that clean coal does not exist. Articles like, \u201cCoal CEO \nadmits that \u2018clean coal\u2019 is a myth\u201d (Reddit 2017) , show how the subreddit discusses \nvarious issues relating to fossil fuels and the complexities involved with the fossil fuel \nindustry. \u201cClean coal\u2019 doesn\u2019t exist\u201d (Reddit 2017). It seems the subreddit itself discredits \nthe ability for the coal industry to  continue to thrive if transformations within the energy \nsector need to occur. \u201cTrump\u2019s promise to bring back coal jobs is worse than a con | \ntelling those communities, in effect: The best hope they have, and that their children \nhave, is to be trapped in a  dying industry that will poison them\u201d (Reddit 2017). The \nauthor of this article headline illustrates the belief in coal destroying lives rather than \n58 \n \n  enriching them. Due to Trump\u2019s discourse regarding coal, this once again establishes the \nnotion that the e nvironment (and industrial energy) fits within a political narrative.  \n5.7.2 Policies  \n The policies section of articles relating to management of resources discusses the \nvarious ways governments are attempting to help or diminish the fossil fuel and other  \nenvironmentally costly industries. 7 out of the 19 management of resources articles \nwithin the subreddit related to various policies regarding the industry. \u201cTrump's decision \nto allow plastic bottle sales in national parks slammed: \u2018The Corporate Agenda i s King\u2019\" \n(Reddit 2017). Two other articles discussed a similar issue regarding the bottled water \nindustry. This discourse shows that the subreddit is not only critical of the fossil fuel \nindustry, but also of other industries that are deemed environmentall y harmful.  \n Environmental policies that are deemed positive also emerge within the subreddit, \nhowever none within this section dealt with positive American policies. \u201cChina reassigns \n60,000 soldiers to plant trees in bid to fight pollution. Area to be pla nted by the end of the \nyear is roughly the size of Ireland\" (Reddit 2018). It seems that the reddit article\u2019s author \nsees reallocating war resources for environmentally friendly policies as positive and \ninspirational. Another article, \u201cNorway Is First Nati on to Ban All Palm Oil Based Biofuel \nto Prevent Rainforest Destruction\u201d (Reddit 2018), helps illustrate how individual \ncountries could affect global policies. Norway may be an exception to the rule due to \ntheir immense amount of wealth and dedication to gl obal environmental problems. In \naddition, little discussion revolves around developing nations and their environmental \npolicies.  \n59 \n \n  5.7.3 Renewables and consumption  \nThe last two subcategories in this section related to investment in renewables and \nvarious ways we consume resources individually and as a society. 2 out of 19 of these \narticles dealt with renewables while 2 out of the 19 dealt with consumption. \u201cBill Gate s \nthinks the 1% should foot the bill for renewable energy, and he's offering the first $2B\u201d \n(Reddit 2017). This article is the most upvoted article in the subreddits history and \ntherefore gives us some insight into what the subreddit deems the most valuabl e form of \nenvironmental actions are. \u201cElon Musk: \u2018We know we\u2019ll run out of dead dinosaurs to \nmine for fuel & have to use sustainable energy eventually, so why not go renewable now \n& avoid increasing risk of climate catastrophe? Betting that science is wron g & oil \ncompanies are right is the dumbest experiment in history by far\u2019\u201d (Reddit 2018). \nInterestingly, both articles relating to renewables used excerpts from billionaires that are \nleading voices within their industry. This may lead to interesting discuss ions within the \ncomments.  \nThe two articles relating to consumption discussed ways consumption can be \naltered to help the environment. \u201cPresident Obama Thinks We Should Eat Less Meat to \nHelp Combat Climate Change\u201d (Reddit 2017). Another article discussed h ow eating less \nmeat can benefit the environment, entitled, \u201cWhy eating less meat is the best thing you \ncan do for the planet in 2019\u201d (Reddit 2018). Both of the consumption articles within this \ndiscussion talk about the meat industry and its harmful enviro nmental effects with \nrelation to factory farming although this call to action does not state to cut out meat, but \nto simply eat less of it. This article has the potential to lead to interesting comments.  \n60 \n \n  5.8 Comments on Management of Energy Resources  \n There were various comment topics found within Management of Resources \narticles. Thirty -eight comments were evaluated within the 19 articles that were coded. 13 \nof these comments related \ndirectly to the political \nrealm. Ten of these \ncomments related to the \neconomy and \nglobalization. Four \ndiscussed conservation, \nwhile three discussed \nindividualism. Only one comment discussed environmental justice and reform of \nenviro nmentalism (one comment each).  \nOut of the 13 comments relating to the political realm, many comments discussed \nthe various ways in which the energy industry is embedded within the political system; \n\u201cClean Coal is and always has been a political tool for th e conservatives to put their \npawns at ease with funding a deprecated industry. I think it is disappointing how effective \nit has been\u201d (Reddit 2017). Many discuss the current Trump administration's ability to \nignore climate science in the name of politics. \u201cWait trump supports big business not the \npeople? What a surprise that no one saw coming\u201d (Reddit 2017).  \u201cThese people want to \n\"devalue\" the country they are now being appointed in charge of\u201d (Reddit 2017). Political \nFigure 7: Bar graph of average comment type in \nManagement of Energy Resources  \n61 \n \n  comments were underrepresented on aver age within the Management of Resources \narticles.  \nTen of these comments related to the economy and globalization. Once again, \ncomments within this topic related directly to the barriers that exist with regards to \nchanging our energy systems to renewable so urced energy. Additionally, commenters \naddressed the barriers in relation to the government\u2019s ability to be influenced by lobbyists \nwithin the fossil fuel industry. \u201cOil companies even know that they're wrong. Renewables \nare the future, so they invest mone y in renewables. But at the same time, they want to \nkeep making money today, so they lobby Congress to say that climate change is a myth\u201d \n(Reddit 2018). There seems to be a perceived urgency and fight going on between \nscience and large fossil fuel industri es. One comment said, \u201cThis is now a fossil fuel \nindustry fighting for its life as the world scientific community warns us that it must die or \nwe will. The gloves are off\u201d (Reddit 2019). The economy and globalization comments \nwithin the management of resou rces section were overrepresented compared to the \naverages within the study (double the overall average).  \nFour comments in the Management of Resources section discussed conservation. \nTwo of these comments were discussing the energy related to bitcoin and its immense \npower produced within the production process. \u201cIt's too bad that it doesn't do something \nuseful while wasting energy like machine learning or data training\u201d (Reddit 2018). \nAnother comment concentrated on the specifics, describing the kilowattag e used by \ncomputer machines producing bitcoin.  \n62 \n \n  \u201cAll coal -fired plants should be equipped with carbon scrubbing technology. But \nthis technology should not be used to prop up the dying fossil fuel industry as is being \ndone here by Trump. It should be used t o mitigate the effects of coal energy while our \ncivilization is transitioning to fully sustainable and non -polluting sources of energy\u201d \n(Reddit 2017).  \n It seems conservation is a key issue, however commenters on r/environment \nseem to understand the urgency  and need to transition to renewable forms of energy. \nConservation was overrepresented compared to the averages of the study.  \nDifferent types of rhetoric were also accounted for. Out of 38 comments, 29 \nadded additional information to the topic. Only four were a call to action. All the call to \naction comments related to how we could conserve various types of energy including \nnatural resources.  Three of the comments were jokes, while two were questions.  \n5.9 Environmental Movements  \n12 out of the top 100 art icles within the environment subreddit engaged with \nenvironmental movements. I have broken down these articles further because a call -to-\naction existed itself within the subreddit as opposed to traditional forms of social \nmovements. Three out of the 12 art icles within this section discussed an environmental \nproblem and a way for the subreddit to act. This demonstrates how the environment \nsubreddit itself may be perceived as an environmental group or movement. Nine out of \nthe 12 articles discussed movements happening outside of the subreddit in the real world.  \nThe articles pertaining to movements occurring across the globe discussed \nindividuals coming together to protest or motivate a certain environmental point of view. \nSix out of these 9 articles discussed  movements that directly related to holding the Trump \nadministration accountable. \u201cTrump has scientists mad enough to march on Earth Day. \n63 \n \n  Simply telling the truth has become a political act. By marching for truth, scientists are \nnot being political - they are merely doing their job\u201d (Reddit 2017). Additionally, articles \ndiscussed various lawsuits emerging to combat the Trump administration. \u201cThe Trump \nAdministration Just Went To Court To Stop Kids From Suing Over Climate Change. \nTwenty -one young people are suing the US government for contributing to climate \nchange in violation of their constitutional rights\u201d (Reddit 2017).  \nOther articles discussed various movements happening across the globe. One \narticle discussed a major movement happening within the Belgi an youth. \u201c35,000 Belgian \nHigh School students skipped school to urge the Belgian government to adopt solid and \neffective climate change policies. Third week in a row of skipping school (first week: \n3000 students, second week: 12 500 students, now 35 000 s tudents) #YouthForClimate \n(Reddit 2019). Some other articles discussed veterans reporting to standing rock to fight \nthe implementation of a pipeline. \u201cU.S. Army veterans are planning a 'deployment' to \nStanding Rock to protest the Dakota Access Pipeline: \u201cT his country is repressing our \npeople. We need to do the things that we actually said we\u2019re going to do when we took \nthe oath to defend the Constitution from enemies foreign and domestic\u201d (Reddit 2016). \nArticles like this one once again discuss the dimensio ns of appropriating war related \nissues into environmental ones. War rhetoric is now being used to discuss environmental \nissues, illuminating an interesting theme.  \n Three out of the 12 articles relating to Environmental movements discussed ways \nin which th e subreddit itself could adopt change with regards to various environmental \npolicies. \u201cLet's get Trump to accept climate change\u201d (Reddit 2016), an article on the \n64 \n \n  subreddit, attached various ways individuals could contact Trump\u2019s administration to \ndiscuss t he serious impacts of ignoring climate change. This article was written on \nNovember 8th 2016, the very day Trump became the president elect. Another post \nemerged only three days later discussing how to bring about more change. \u201cTrump is \nasking us how to ma ke America great again...It's our chance to tell him how important \nthe issue of climate change is to us!\u201d (Reddit 2016). The link this article posted involved \na .gov website in which individuals could voice their opinion or \u201ctheir story\u201d so that their \nvoices could be heard by government officials. The last post relating to reddit itself \nappropriating change involves the U.S. bill that would dismantle the EPA (Environmental \nProtection Agency). \u201cUS House Representative Matt Gaetz is sponsoring Bill H.R. 861 - \nWhich would terminate the EPA. Here is his contact information if you don't want to see \nunmitigated pollution, toxic dumping and the rape of our public lands in the US -\u201d \n(Reddit 2017). All these self -posts that call to action may lead to various interest ing \nphenomena occurring within the comment sections.  \n5.10 Environmental Movements Comments  \nI coded twenty -four comments within the environmental movements section. 12 \nof these comments related directly to politics. Six related to reforming environmentalism . \nTwo articles  discussed issues regarding environmental justice. Only one comment \ndiscussed the media or the economy & globalization (one comment each).  Within \nEnvironmental Movement articles, 12 comments discussed politics within Environmental \nMovement ar ticles. One Comment read, \u201cThey have managed to get the attention of \n65 \n \n  international press, organizations and European political parties which helps to build up \nthe pressure on Belgian politicians\u201d (Reddit 2019).  \nMany of these comments \ndiscuss the influential \nimpact environmental \nmovements have on the \npolitical process. \nDiscussions also \nelucidated why politics \nand policies were such an \nimportant piece to the environmental movem ent conversation. \u201cIt forces (safety) \nregulations on companies that could make some more profit if they weren't required not \nto poison people\u201d (Reddit 2017). Political comments were represented about on average \nwith the overall average of the study.  \nSix c omments related to reforming environmentalism. One comment discussed \nhow by just focusing on Republicans and Donald Trump users within r/environment are \nignoring the larger picture regarding the environmental movements\u2019 influence within \npolitics. \u201cI wonder  if people knew about this when the kids started the process during the \nObama administration? I'm sure most people will see this and think \u2018great! Regular \npeople are fighting against the Trump administration\u2019 when in fact this has been going on \nfor the pas t few years\u201d (Reddit 2017).  \nFigure 8: A bar graph of comments within environmental \nmovement section  \n66 \n \n  Other comments discussed how important it is to appropriate change individually \ninstead of waiting on the government. \u201cI dunno...seems like everyone is sitting around \nwaiting for the government to come save us all. What we need  is people to stand up and \ntake the bull by the horns\u201d (Reddit 2016). Another comment showcased how the rhetoric \nwithin environmental movements needs to change so that Conservatives could hop onto \nan ecological platform. \u201cWhy not switch how we talk to them  about climate change and \ntalk their language. Instead of pushing a better planet talk about economic growth, \neconomic opportunities, and jobs created from clean energy\u201d (Reddit 2016). Reforming \nEnvironmentalism was overrepresented on average within the en vironmental movements \nsection of the study. This makes sense due to the shared rhetoric type.  \nWith other coding processes, 14 comments discussed additional information while \nseven were a call to action of some sort. Two comments were questions and one \ncomment was a joke.  \n5.11 Wildlife Preservation  \n Another major interest of the environmental subreddit focuses on wildlife lands \nand wildlife preservation. Ten out of the 100 articles within the subreddit were related to \nprotecting various aspects of wildlife . Four out of the 10 articles related to endangered \nspecies and various aspects regarding them. Three out of the 10 articles dealt with \nprotection of wildlife lands and ways to help conduct change regarding protecting these \nwildlands. Lastly three out of t he 10 articles discussed policies that were or are being \ncreated affecting aspects of wildlife preservation.  \n67 \n \n  5.11.1 Endangered species  \n Four out of the 10 articles relating to wildlife preservation discussed endangered \nspecies. All four of these articles d iscussed the Trump administration\u2019s mishandling of \nissues surrounding endangered species and the endangered species act; \u201cNow Trump's \nGoing After the Bumblebees: The administration just delayed endangered status for a \nbumblebee species that's on the brink of extinction\u201d (Reddit 2017). More articles surfaced \ndiscussing Trump\u2019s administration and their ability to ignore endangered species; \u201cTrump \nAdministration Denies Endangered Protections For 25 Species. \u2018You couldn't ask for a \nclearer sign that the Trump a dministration puts corporate profits ahead of protecting \nendangered species\u2019\u201d (Reddit 2017).  \nQuestions were also raised concerning Trump\u2019s promised policy measures; \n\u201cTrump's 'beautiful wall' threatens 111 endangered species\u201d (Reddit 2017). The \ncombination of threats endangered species and the Trump administration\u2019s laws and \npolicies regarding these endangered species will foster an interesting discussion within \nthe comment sections.  \n Three out of the 10 articles within this section related to wild lands and  various \nways they are protected. \u201cMan Postpones Retirement to Save Reefs After He \nAccidentally Discovers How to Make Coral Grow 40 Times Faster\u201d (Reddit 2018). \nArticles like these are the few optimistic examples in an ideological landscape where \nmost arti cles are pessimistic. In the article, \u201cSwiss Businessman is Contributing $1 \nBillion Towards Protecting 30% of the Planet\u201d (Reddit 2018), power is in the hands of \nrich individuals that hold social and monetary capital.  \n68 \n \n  Not all articles regarding protection  were positive, the article, \u201cDonald Trump\u2019s \nEarth Day Statement Is Shameful: Trump released an Earth Day statement touting his \ncommitment to protecting the environment, despite doing the exact opposite in the first \nfew months of his administration\u201d (Reddi t 2017), shows how Trump\u2019s statement \nregarding protecting the environment on earth day was received by the individual writing \nthe article. It will be interesting to see if the comments mirror this sentiment regarding \nTrump.  \n5.11.2 Policies  \nThree out of th e 10 articles discussing wildlife preservation discussed the various \npolicies regarding preservation, \u201cAfter protests & public outcry, GOP bill to sell off 3.3M \nacres of public lands has been withdrawn\u201d (Reddit 2017). This articles discusses the \npower of e nvironmental movements and rhetoric to change bills and laws that affect wild \nlands. The article, \u201cFury after China reverses 25 -year-old tiger bone and rhino horn ban\u201d \n(Reddit 2018), takes issue with China\u2019s reversal on wildlife protection laws that exist \nsomewhat universally across the globe. It will be interesting to see how commenters \nrespond to this news. The last article related to Trump\u2019s response to California wildfires. \n\u201cTrump Blames \u2018Bad Environmental Laws\u2019 for California Wildfires, Says \u2018Must\u2019 Cut  \nDown Trees\u201d (Reddit 2018). With Trump\u2019s critique of environmental laws within \nCalifornia, I suspect diverse opinions will emerge regarding this subject matter.  \n  \n69 \n \n  5.12 Comments on Wildlife Preservation:  \n Twenty comments were coded within the ten articles o n Wildlife Preservation. \nTwelve of these comments directly discussed politics, four discussed reformation of \nenvironmentalism, two discussed the media, and one related to the economy and \nglobalization.  \n The political comments that dealt with Wildlife Pres ervation discussed the Trump \nadministration\u2019s stances on wildlife as well as how Republicans exacerbate the \nexploitation of the environment. \u201cI get the strange feeling that President Trump doesn't \nreally care about the environment or those things living in , on, or around it\u201d (Reddit \n2018). Another comment stated, \u201cRepublicans be like: \u2018The environment is a liberal \nconspiracy\u2019\" (Reddit 2018). Political comments were overrepresented on average \ncompared to the overall average of the study.  \n Four Comments \ndiscussed reforming \nenvironmentalism. \n\u201cWe need to keep \nvigilant to watch what \nhe is planning with the \nEndangered Species \nAct, but let's not make \nup stuff\u201d (Reddit \nFigure 9: Bar gr aph of wildlife preservation comments on \naverage  \n70 \n \n  2017). It seems the Trump ef fect has taken over the article headlines however there is \nsome push back from commenters discussing how change can be implemented within \nenvironmental discourse, while resisting focus on Trump. Despite the Trump effect, a \nfew comments discussed different organizations that are helping to retain and preserve \nwildlands. \u201cthere\u2019s an organization called rainforesttrust.org They save acres of rainforest \nthrough land purchase. Check it out\u201d (Reddit 2018). Reform of environmentalism was \noverrepresented within art icles discussing Wildlife Preservation.  \nWith additional coding nine of these comments went further into detail regarding \ninformation. Five comments were a call to action of some sort. Four of the comments \nasked questions to prove a point or to gain further information. Two of the comments \nwere j okes.  \n5.13 Environmental Pollution  \nEnvironmental pollution is a prevalent issue when it comes to discussing \nenvironmental problems. Ten out of the 100 top articles within the environment subreddit \nrelated to a form of environmental pollution. Five out of those 10 articles dealt with \nharms explicitly caused by human creations, such as pesticides and toxic pollution. Three \nout of the 10 articles discussed humans polluting waterways. And lastly two out of the 10 \narticles discussed air pollution.  \n5.13.1 Human  \n Five out of the 10 articles related to human harm to the environment via pesticides \nand mismanagement of resources. \u201cTrump\u2019s Legacy: Damaged Brains, This is what a \n71 \n \n  common pesticide does to a child\u2019s brain. -- Studies show that it damages the brain and \nreduces I.Q.s while causing tremors among children. Now the Trump administration is \nembracing it, overturning a planned ban that had been in the works for years\u201d (Reddit \n2017). Once again,  we see a Trump narrative within a section relating to environmental \npollution. \u201cThanks to Republicans, Pesticide Companies Are Now Free to Kill All the \nEndangered Species They Want - \u201cIt\u2019s like kicking them when they\u2019re down\u201d (Reddit \n2018). Conversely one  article discussed positive change with regards to pesticides. \n\u201cFrance is the first country to ban all five bee -killing pesticides\u201d (Reddit 2018). With \nregards to pesticides one article discussed Monsanto and its ability to affect scientific \nresearch and s way government policy creating a lack of transparency in government with \nregards to vital health information. \u201cMonsanto 'bullied scientists' and hid weedkiller \ncancer risk, lawyer tells court\u201d (Reddit 2018). I believe the combination of health and \nlack of transparency will lead to interesting comment discussions.  \n5.13.2 Water  \n Water related issues accounted for three out of the 10 articles relating to \nenvironmental pollution. \u201cThe EPA Is Beginning To Roll Back An Obama -Era Rule \nLimiting How Much Toxic Wast e Power Plants Release In Water. | so beholden to big \nbusiness that they are willing to let power plants continue to dump lead, mercury, \nchromium and other dangerous chemicals into our water supply\u201d (Reddit 2017). These \narticles hold political merit reiter ating the notion that bi -partisanship does not exist within \nthe political environmental realm. \u201cTrump caps off a long day by letting coal companies \ndump waste into streams\u201d (Reddit 2017). All the articles that discussed water within these \n72 \n \n  sections related directly to politics and transparency. \u201cPoison once flowed in America's \nwaters. With Trump, it might again\u201d (Reddit 2017).  \n5.13.3 Air \nTwo out of the 10 articles dealing with environmental pollution discussed air \npollution. One article discussed California and its air pollution problem. \u201c\u2018This is a stupid \npolicy\u2019: Emissions become latest front in California -Trump war - \u2018motorists will pay more \nat the pump, get worse gas mileage and breathe dirtier air. California will fight this \nstupidity in every conceivable  way possible\u2019\u201d (Reddit 2018). Another discussed the \npolitical implications of transitioning former policies to new ones; \u201cOne day after getting \nsued by 15 states, the Environmental Protection Agency on Wednesday reversed course \non its plans to delay imple mentation of Obama -era rules intended to reduce emissions of \nsmog -causing air pollutants\u201d (Reddit 2017).  \n5.14 Comments on Environmental Pollution  \nTwenty comments were coded for within 10 articles relating to environmental \npollution. Seven comments related  to politics. Three comments discussed the media and \nits portrayal of environmental pollution problems. One comment discussed environmental \njustice. One comment related to the economy and globalization. One comment discussed \nreformation of environmental mo vements and one comment also discussed wildlife \nmanagement. Seven comments relating to politics discussed the various ways politics and \npolicies have changed overtime to accommodate environmental pollution. One user \nwrote, \u201cDid you know the amount of vehic le pollutants in the air in the LA basin is down \n73 \n \n  98% since the 60s/70s, despite gasoline usage being up 3x? Purely due to CA clean air \nlaws. Which we will not give up on. California will win this fight\u201d (Reddit 2018).  \nComments like this depict the issue of environmental pollution as a fight between \npolicies and industrial waste. Politically themed comments also discussed the major \ninfluence money has \non politics and \npolitical decision \nmaking within the \nTrump \nAdministration. \u201cI'm \nsure it's just \ncoincidence that he \ngave a shitload of \nmoney to Trump and now his company can destroy the environment again. Such \nabsolutely disgusting human beings\u201d (Reddit 2018). Political comments were \nunderrep resented on average compared to the overall average of the study.  \nOne comment read, \u201cWe don't give a damn about our planet. Here's hoping we \ncan fix our shit in time\u201d (Reddit 2018). Another comment wrote, \u201cHow much toxic heavy \nmetal do you want in your wa ter supply? Um.. None\u201d (Reddit 2017). The overall feeling \nof pessimism is rampant within the Environmental Pollution section. These types of \ncomments are way overrepresented within this section compared to the overall averages \nof the study.  \nFigure 10: Bar graph of environment al pollution comments by \naverage  \n74 \n \n  Three comments  discussed the media\u2019s portrayal of issues regarding \nenvironmental pollution. \u201cReddit\u2019s ad section told me the weed killer was not a threat. \nInteresting. Proof that you can advertise any nonsense these days\u201d (Reddit 2018). \nComments like these help question  problems with large scale social media companies, \nlike Reddit, portraying issues related to the environment. Discussion of media within the \nenvironmental pollution section was overrepresented compared to the overall average of \nthe study.  \nThe 20 comments within environmental pollution were also coded for rhetoric \ntype. 10 of these comments discussed information in further detail. Five comments made \na joke of some sort. Four comments asked questions relating to the topic.  \n5.15 Media Related  \nFive out of the 100 articles within the environment subreddit dealt with media and \nthe ways media has succeeded or failed in representing environmental issues. One article \nthat critiqued the media was titled, \u201cThe media has essentially stopped covering climate \nchange\u201d (Re ddit 2017). Other articles also discussed various outlets winning in debates \nwith other outlets regarding climate science. \u201cFox News host Tucker Carlson bit off more \nthan he could chew when he tried to go head to head with \u2018Science guy\u2019 Bill Nye over \nclima te change\u201d (Reddit 2017). Another article title reads, \u201cClimate Deniers Debunked: \nWeather Channel Brutally Owns Breitbart\u201d (Reddit 2017). These articles once again \nbring the notion of politics into the equation. It will be interesting to see how individual s \n75 \n \n  in the comment section respond to these various types of rhetoric (Ones that attack right \nwing outlets versus ones that attack all media outlets).  \n5.16 Comments on the Media  \n Out of five articles, I coded 10 comments. Seven comments discussed media even \nfurther. Two comments discussed reformation of environmentalism. One comment \ndiscussed individualism.  \n The seven comments that discussed the media illustrated the immense amount of \npower the media had in detailing issues revolving around the environment. \u201cGod, these \nfucking headlines. No \none was owned, nothing \nwas brutal. Someone \njust took the time to \nexpla in the truth using \nfacts, and asked others \nto do the same\u201d (Reddit \n2016). Another \ncomment noted, \u201cNo one \ncares about an ad drenched Gizmodo page saying we should check out the YouTube \nvideo\u201d (Reddit 2016). Another commenter discussed his own thesis regardi ng \nenvironmental news writing, \u201cI researched this as part of my thesis and had to conclude \nthat climate change coverage between 2010 and 2015 was roughly the same. The trend \nFigure 11: Bar graph of comments on media related articles  \n76 \n \n  shown here looks scarier than it is. It focuses on the US and only on television c overage. \nNewspapers tend to cover more climate change than television and they haven't stopped \ncovering climate change\u201d (Reddit 2017). Comments like this showcase how different \nindividuals browse and comment within these online communities.  \n Seven of thes e comments discussed information with further detail. One comment \nwas a joke. One comment asked a question. One comment was a call to action.  \n5.17 Ecological Shifts  \nFinally, two out of the 100 articles within the environment subreddit dealt with \necologica l shifts and the immense power they have on our human society; \u201cEarth has lost \n10% of its wilderness since 1992. At current rates it will all disappear in 50 years\u201d \n(Reddit 2017). Another article was produced showcasing the immense power of climate \nchange.  \u201cThe World Has Barely 10 Years To Get Climate Change Under Control, U.N. \nScientists Say\u201d (Reddit 2018). Although the sampling produced few articles within this \nsection, a lack of content at times can explain phenomena unseen if they were plentiful \nwithin the subreddit.  \n5.18 Ecological Shifts Comments  \nFour comments were evaluated within articles about ecological shifts. Two \ncomments discussed animals and food production. One comment discussed \nindividualism. Two comments discussed information in further det ail. One comment was \n77 \n \n  a call to action. One comment was a joke.  Due to the low amount of frequencies \nregarding ecological shifts, no graph was created.   \n78 \n \n  6.0 DISCUSSION  \n6.1 Social Network Analysis  \n The two largest nodes  (circles)  related to Politics and Ma nagement of energy \nresources. The nodes create an ideological map of the content shared within \nr/environment. The larger a node, the more upvotes that article received. The color of \nnodes were determined by the type of environmental content the article tit le elicited. \nThese seem to be the topics that gain the most traction and discussion within the \nr/environment community. Edges provided an interesting phenomenon. Edges illustrate \nshared sources. When a source was used by multiple articles, an edge (or line ) was \ncreated to illustrate that shared source. Only 33 articles had a shared source illustrating \nthat 67 article sources only appeared once. This shows that the  r/environment subreddit \nutilize s sources from a wide variety of online sources. A greater numb er of sources does \nnot necessarily mean better information comes from these sources. People who prefer \nsocial media outlets for news information do not self -expose themselves to high -\ncredibility sources (Pearson and Knobloch 2018). Social media users repea tedly use \nobscure news outlets. This reemerged within my social network analysis, entailing a \npossibility of these sources espousing low -credibility information.  \nLegacy media newspaper outlets that are now primarily online, were heavily \nsourced. The Guard ian was the most sourced website with 10 articles. The Guardian is a \nnews company from the United Kingdom that dates to the early 19th century. The website \n79 \n \n  itself measures high for factual reporting and has self -identified in various articles as \nbeing a le ft-wing publication (mediabiasfactcheck.com). The second most common \nsource was The Washington Post with 7 articles. Recently purchased by Amazon CEO \nJeff Bezos for 250 million dollars, The Washington Post is more trusted by liberal readers \nthan conservati ves according to a 2014 pew research poll. Although these are major \nonline news outlets, their messages take on new meaning when individuals contribute and \nrename them within the r/environment subreddit. According to mediabiasfactcheck.com, \nthe top two new s sources within the top articles in r/environment, self -identified and were \nmore heavily trusted by liberal individuals  (2019) . This leads to the conclusion \nr/environment prefers liberal news outlets to conservative outlets. Both the Washington \nPost and T he Guardian used credible sources (mediabiasfactcheck.com). Reddit itself and \nthinkprogress.org had 6 sources shared each. This also illustrates the community\u2019s usage \nof non -profit organizations not just commercial websites that were discussing \nenvironment al topics. ThinkProgress itself is an organization  that was conceived to \ndiscuss and evaluate  American politics. This website is a project of the Center for \nAmerican Progress Action Fund which is a public policy research organization founded \nby John Podest a (Chairman of Hillary Clinton\u2019s 2016 Presidential Campaign). Although \nthis group has left wing ties, they state that they are editorial independent \n(mediabiasfactcheck.com).  \n  \n80 \n \n  6.2 Politics, Policies, Transparency and the Trump Effect  \n With 42 percent of articles and 49.5 percent share of comments, politics is the \nnumber one issue the r/environment community discusses. This creates and represents the \nsubreddits ideology and identity. Political issues were brought forward, and they can be \ncategor ized in various ways. Policies from individuals portraying their ideals on the \ngovernment was a prevalent topic within the subreddit. Policies also conveyed (some that \ndid harm as well as some perceived to do good) an important political topic the \ncommunit y discussed. 13 out of the 42 articles related to policies. The most prevalent \narticle type within policies related to the Trump administration and its movement towards \nbacking out of previous environmental policies. These illustrate the innate issue with \ngovernment, however (not to the community\u2019s fault) also fail to interrogate and put \nforward policies that may remedy the perceived environmental crisis. This hyper -focused \nnarrative on the Trump administration depicts the obsession the r/environment \ncommun ity has with political parties and political outcomes from these parties. Within \nthe online news era, individuals are prone to confirmation biases and utilize selective \nexposure to reinforce political ideologies and tendencies (Westerwick, Johnson, \nKnobloc h-Westerwick 2017). Due to r/environment\u2019s use of liberal article sources, and \nthe Trump effect, potential conservative users may be turned off to the subreddit and its \nrhetoric. It\u2019s already difficult for certain social media sites portraying news to gain  a \nwider audience (Mitchell and Lim 2018).   \n81 \n \n  The most prevalent policy prescription people had involved critiquing the \ngovernment\u2019s transparency regarding environmental issues. In fact, many policies people \nintroduced dealt with the transparency of governm ent. It seems in order to protect the \nenvironment individuals in r/environment required transparency and honesty from \ninstitutions that represented the environment within the government. Within the political \narticle section,  a heavily above average politic al response was elicited from comments. \nThis reinstates the community\u2019s perceived importance regarding politics within \nenvironmental policies. The r/environment subreddit sees the importance of transparency \nwithin government, and that policies that enforce  transparency, like ombudsmen laws, are \ndeemed functional ways to instate policy prescriptions.  \nTen out of 42 articles within the political section discussed transparency within \ngovernment as well as the suppression of citizens\u2019 expressions regarding the environment \nand environmental policies. A main concept regarding transparency discussed the major \ninfluence industrial powerhouses and money have within our government and their policy \ndescriptions. Once again, this section had the focal point of the Trump  Administration \nand its lack of transparency. One article titled \u201cI am an Arctic researcher. Donald Trump \nis deleting my citations\u201d (Reddit 2017) illustrates the distrust the r/environment \ncommunity has for not only the Trump Administration but also Trump himself.  \nOut of the 100 top articles within the r/environment subreddit, all their dates fell \nbetween November 8th, 2016 and February 2019. November 8th, 2016 was the day \nTrump was elected into office. It seems r/environment had a significant boost in usa ge \nbecause of the election of Trump. It also seems that popular article topics began to \n82 \n \n  discuss Trump and his perceived lack of competence relating to environmental issues. I t \nis not clear  whether this means more people identify as environmentalist or whet her \npeople are trying to find additional reasons to dislike Trump.  \nTrump himself became a topic within the political section of the study. Despite \nmost articles relating to the Trump administration, their lack of transparency, and their \nenvironmental poli cy removal, personal attacks against Trump were also the focal point \nof many articles. 19 out of 42 of the articles within the politics section (19 percent of the \noverall articles) were attacks on his beliefs personally rather than attacks on his \nadministr ation\u2019s negligence regarding environmental topics. For example, one article \ntitled \u201c Dem lawmaker: Trump 'tweeting like a child who hates science class'\u201d (Reddit \n2017) illustrates how the crowdsourced r/environment subreddit doesn\u2019t necessarily need \nto be o n topic with regards to environmental issues.  \nThis is a major example of how the Trump Effect, can alter discourse on the \nr/environment subreddit. It relates when issues related to the environment (or other \necological political issues) become secondary to  the perceived lack of competence and \ndistaste for a political individual, like Trump, who holds political power. It creates a \npower dynamic in which content relating to an issue is not the focal point of discussion. \nThis can be extremely dangerous to disc ussion revolving around an issue like the \nenvironment because it alters topics and discussion points. It also creates a form of echo \nchamber where it\u2019s easy to state a comment that everyone within the community agrees \nwith that holds little substance but w ill still be liked heavily. The Trump effect is the \n83 \n \n  main reason 42 articles relate to politics while 10 relate to environmental pollution and 2 \nrelate to ecological shifts.  \n6.3 Management of Energy Resources, Dominant Voices, and Policies  \n 19 out of 100 of the articles dealt with Management of Energy Resources. 8 out of \nthe articles within the 19 discussed industries and their resources. A few major voices \nemerged within this section. Bill Gates and his declaration to fund the industry of \nrenewables was the largest upvoted article in the r/environment community by a wide \nmargin. In addition, this section featured Elon Musk and he was quoted discussing how \nfossil fuels are a diminishing resource within the globe. The voices of wealthy and \ninfluential individuals were largely present within this management of energy resources \nsection. Some comments were critical of these voices stating that wealth inequality and \nthe capacity for capitalism to coagulate wealth contributes to many of the ecologica l \nharms we experience on earth today.  \n 8 out of the 19 articles discussed industries, their existence, and their ability to \nmanipulate markets and governments to constantly reinstate their perpetual existence, \nindividually and as an industry. The subreddi t also had issues with companies and \nindustries primarily in the fossil fuel industry and their lack of transparency. Lack of \ntransparency seems to be a key component of the r/environment subreddits\u2019 main focal \npoint of change within the government and lar ge-scale corporations. It is deemed to be \nthe major roadblock in financial and political sectors where facing the implementation of \nenvironmental change to combat environmental harm is secondary to profit and power. \n84 \n \n  Users have stated that when a lack of tr ansparency exists within the government and \nindustries it\u2019s difficult to implement proper policies when these policies may be \nperceived to be based on false premises. The subreddit seems to understand this when \ntransparency is explicitly discussed. When ot her issues are brought forward top \ncomments do not stray away from overarching macro -level policy subscriptions.  \n When discussing policy prescriptions within the management of energy resources \narticles the subreddit discussed policies they deemed negative  and positive towards the \noverall health of the environment. Negative policies dealt with policies that championed \ncorporate agendas and the free market over environmental health. Positive policies were \npolicies that directly allocated resources towards im proving the environment like China\u2019s \ntree plantation policy, or policies that banned industrial practices like Norway\u2019s ban on \npalm oil-based  biofuel to prevent rainforest destruction. No positive policies discussed \ndealt with American policies, only negat ive ones. The Trump Effect was also apparent \nwithin these policies due to the Trump Administration\u2019s crack down on environmental \npolicies. The policies discussed relating to America dealt with the Trump \nadministration\u2019s lift on the water bottle ban within National Parks. Two articles dealt with \nthese policies.  \n Comments within the Management of resources section dealt with a variety of \nissues. Discussing politics was underrepresented compared to the overall average. \nDiscussion of industry and globalization  was overrepresented showcasing that comments \nfell in line with topics. Conservation was also overrepresented which allows us to see that \nr/environment may see conservation as a way to combat environmental problems \n85 \n \n  apparent within society. Individualism wa s also overrepresented compared to the average \nwhich shows that individual consumption is an important way for individuals to combat \nenvironmental problems within their daily lives. Individual consumption mostly \ndiscussed cutting down meat consumption due to the meat industry and the intense \namount of resources required to produce factory farmed meat.   \n6.4 Environmental Movements  \n Twelve out of the 100 articles studied within r/environment dealt with \nenvironmental movements. Nine out of these 12 articles d iscussed movements and \ndemonstrations occurring outside of the internet and in the real world. Comments and \narticle titles were proud and supportive of these social movements regarding the \nenvironment. It seems environmental movements are an important part  of \nr/environment\u2019s action and reaction towards societal pressures and movements. Three out \nof the 12 articles dealing with environmental movements were the subreddits own call to \naction. These articles had links towards government websites where individua ls could \ncall or email representatives that were attempting to establish diminish environmental \nlaws and regulations.  \nAll three  of these social activist comment threads dealt with the Trump \nAdministration and its stance on environmental issues. One also i ncluded a petition in \nwhich individuals could sign in support of climate change. One petition aimed at the \nTrump administration\u2019s initial stances on the environment, discussed how the public \ncares about climate change. This petition occurred just days afte r he was initially elected. \n86 \n \n  This shows how fast social movements and perceived social urgency can contribute to \nfast forms of social activism. I believe further content analysis and ethnography studies \ncould be conducted by individuals who would like to st udy social movements within the \ninternet due to the sheer amount of content that exists. Social activist websites are \nsprouting and emerging especially with the help of crowdsourced journalism on social \nmedia.  \n Comments within the Environmental Movements section held fascinating \ndiscussions. Comments discussing politics were equal to the overall average showing that \npolitical and social movements regarding the environment are an important topic within \nthe realm of environmental movements. Calls to reform  of environmentalism, and its \nportrayal of environmental issues was 2.5 times the overall average. This shows that \ndiscussion is not only occurring regarding implementing social change, but that \nindividuals are also critiquing social movements and how they r epresent themselves and \ntheir ideologies. I believe the confidentiality aspect of the internet perhaps allows more \nindividuals to be hyper -critical of a movement they are a part of, and in effect can help \ncreate a more equitable, representative form of a s ocial movement.  \nDiscussion of the economy and globalization was way less than the overall \naverage, which may show where individuals within the r/environment community believe \nreal change can be created and that according to the r/environment subreddit is within \npolitical systems and individual behavior. Discussion of media was also below average \nwhich is fascinating due to the immense power media holds with portrayal or ignorance \nof social movements across the globe.  \n87 \n \n  6.5 Wildlife Preservation  \n 10 out of t he 100 articles studied within r/environment dealt with wildlife \npreservation. This section is one of the more heavily ecologically based subsections. \nEven though preservation was the main threshold within the coding process, 5 out of 10 \nof the articles cr itiqued Trump\u2019s administration and its policies regarding endangered \nspecies and preservation of wildlands. The other half of the articles dealt with funding \nand policies regarding protecting and preserving wildlands across the globe. It seems \nalthough sma ll in its share of article size, many individuals within the r/environment \ncommunity support preservation as an environmental policy and ethos.  \n Comments discussed a variety of topics. Politics was discussed above the overall \naverage enabling the concept that preservation has a largely political component to it. In \nother words, subreddit users grappled with the concept of who appropriates what land to \nbe protected and why? It seems many comments blame Republicans and perceived right -\nwing policies for the l ack of action regarding environmental issues. Reform of \nenvironmentalism was also largely overrepresented within the comment section \nshowcasing that those discussing preservation believe it to be an important component to \nenvironmentalism and environmental  rhetoric within the social movement field. \nDiscussion of the economy and globalization was below the overall average.  This might \nbe due to the fact a few articles championed large donors and individuals who practiced \nphilanthropy towards protecting wildl ands and endangered species. Additionally, due to \n88 \n \n  the capitalist nature of resource exploitative industries and processes, the harm \nexacerbated towards wildlands was not discussed as extensively within this section.   \n6.6 Environmental Pollution  \n With 10 o ut of the 100 studied articles, environmental pollution was an important \ntopic within r/environment. Once again, most articles related to the Trump administration \nand Republicans protecting big business over the environment showcasing the immense \nimpact th e Trump effect has had on this community. All the articles dealt with \nenvironmental harms like water, air and pesticide pollution. All the articles also dealt \nwith political processes that defended or protected these industries and their practices. \nThis le d to interesting discussion within the comment section.  \n Comments underrepresented politics when compared to the overall average which \nis fascinating due to the immense amount of political systems that were discussed within \nthe articles. It makes sense th at individuals within r/environment were very pessimistic \nabout processes that contribute to environmental pollution. Discussion of media was also \ntwice the average amount perhaps illustrating their lack of coverage on many issues \nregarding environmental p ollution like the Flint Water crisis. Also comments regarding \nthe media were critical of the website reddit itself and its advertisement money from large \nscale industries that are culprits within the pollution realm. Economy and globalization \nwas a comment  topic well below the overall average within the study which is fascinating \ndue to the culprits within these article case studies being capitalist industries exploiting \nnature to the point where environmental harm and pollution occurs.  \n89 \n \n  6.7 Media Related  \n Five articles within the study discussed media portrayal of environmental \nproblems. It seemed one article covered the overall sentiment quite well. The article titled \n\u201cThe Media has essentially stopped covering climate change\u201d illustrates the overall \nsenti ment individuals within the community felt towards media outlets and their \ndiscussion of environmental problems. Other articles also discussed right -wing news \noutlets and their bias against properly covering environmental issues. Comments within \nthe media related section acted as a content -based echo chamber in which media itself \nwas discussed 70 percent of the time. I believe distaste for the media\u2019s portrayal of \nenvironmental and social issues could perhaps be a bridge between identity politics and \nright -wing and left -wing individuals. It seems individuals agree money and \nadvertisements within media outlets alter the rhetoric and discussions media outlets are \nallowed or not allowed to discuss. Due to the fact only 33 percent of articles within \nr/environmen t shared a source, it seems messaging boards and online discussion threads \nare places where traditional media outlets can be combated through individuals openly \ndiscussing issues without a similar form of censorship.  \n6.8 Ecological Shifts  \n Ecological shift s are defined as large persistent changes in the structure and \nfunction of an ecological system. Only two articles discussed ecological shifts occurring \nwithin our world. The lack of coverage states more than the coverage itself. It seems \n90 \n \n  there is a consen sus within r/environment that society is already facing an ecological \ncrisis. The main coverage of politics and political systems shows how the community \ndeems change should be appropriated. This might be due to the immense power held by \ngovernment or the fact that government is an institution that is supposed to \ndemocratically listen to its subjects.  \n The lack of coverage on ecological shifts illustrates the theory of \u201cthe spiral of \nsilence\u201d, coined by Elisabeth Noelle Neumann. This theory states individu als have a fear \nof isolating themselves ideologically when discussing certain topics. Because politics \nwere discussed more heavily as compared to other topics, it seems other topics like \necological shifts were discussed less. This is partially due to the i nterpretation that other \nindividuals would rather discuss environmental politics as opposed to ecological shifts \nand issues.  \n  \n91 \n \n  7.0 CONCLUSION  AND  RECOMMENDATIONS  \nNews media is changing within society. The process in which news is \ndisseminated is also changing. Media users on social media sites like Reddit are \ncrowdsourcing articles which in turn is creating new media content based on how liked or \nupvoted certain articles are within a social media space. Users can choose what articles \nrise to the top, w hich affects news dissemination and consumption. Crowdsourcing \narticles is the process websites like Reddit utilize to display information. The subreddit \nwithin Reddit dedicated to environmental issues (r/environment) crowdsources articles \nand allows comme nters to discuss large scale issues within the website.  \nThis study attempted to understand how the subreddit of r/environment self -\nidentified its environmental tendencies. By utilizing content analysis, the top one hundred \narticles (and top two hundred com ments) were coded for. When coded the overall \nconception of r/environment\u2019s form of environmentalism emerges. Politics were \ndiscussed the most, while the economy and globalization were discussed the second most. \nEnvironmental movements, environmental pollu tion, and wildlife preservation were all \nclose to the third most discussed topics. Ecological shifts were discussed the least within \nthe subreddit. The subreddit r/environment\u2019s environmentalism is heavily based within \npolitics and the politics of the envi ronment. This helps illustrate how individuals who \npost on crowdsourced communities see politics as the number one environmental issue \nthat needs to be discussed.  \n92 \n \n   The subreddit r/environment heavily used liberal based news media sources \nwithin their arti cles. The top three article sources were all left leaning in their rhetoric. \nAdditionally, the subreddit had 42 out of 100 articles dedicated to discussing politics and \npolitical systems regarding the environment. Trump and the Trump administration was \nthe main focus  of 19 of these articles while also being a major theme in all other sections \nwithin the study. If the subreddit r/environment wants to spread its message outside of its \nmain users in order to gain new users and followers , it should focus on pol itical issues \npresented, like transparency, as opposed to individuals and political parties (Mitchell and \nLim 2018). This would help r/environment\u2019s appeal to conservatives and independents \nwho are ideologically on the fence regarding environmental issues.   \n The subreddit\u2019s focus on political systems affects the overall environmental \nmessage that was portrayed within r/environment. When Trump and his administration \nbecame a topic area, content and discussions regarding the environment fell second to \ndiscuss ions of Trump. This was frustrating as a researcher because it affected the \nperceived content I was going to evaluate. When politics were discussed, policies and \nsystems were evaluated by individuals. When topics like policies and transparency \nemerged, use rs discussed actual environmental issues as opposed to just posting ad \nhominins against Trump. The subreddit\u2019s environmental content would greatly benefit \nfrom avoiding the topic of Trump.  \n Politics were heavily discussed within r/environment and other topics were \nsecondary within their scale. Industrial powerhouses and corporate entities were \ndiscussed within the management of energy resources section, illustrating their deemed \n93 \n \n  importance re garding environmental issues. Once again, the topic of transparency was \nhighly prevalent within this section illustrating how important the topic of transparency is \nto the r/environment subreddit. The discussion of transparency revolved related to politics  \nand industrial practices. Perhaps as transparency increases within these sections of \nsociety different environmental topics will emerge discussing other practical issues. \nDiscussion of transparency and politics as a whole elicited discussions within the \ncomment section that focused on the effects of environmental politics.  \n A major finding regarding this study revolves around how the r/environment \ncommunity discusses issues regarding politics and the global economy  within certain \ntimeframes . Because of th e Trump presidency, this study had more political discussions \nspecifically looking at Trump and his lack of environmental awareness as opposed to \nenvironmental policies or solutions.  The most popular articles all dealt with convenient \npolitical topics like  the Trump presidency. Monetary investment in green energies were \ncelebrated by commenters within r/environment. R ich elites and governments  were \nperceived to have more influential power when it came to implementing environmental \nchange on a global societa l scale.  Small scale individual contributions and efforts from \npoorer, less influential citizens were not seen as being as viable. It seems social, \nmonetary, and intellectual capital are all intensely influential voices within the most \npopular articles in the r/environment community. In addition, Reddit itself exists on the \ninternet which can only be accessed by a certain amount of people in the world, creating \na population of users that are not representative of the world. These power dynamics of \n94 \n \n  dominant voices plays out in the landscape of the r/environment community and is \nimportant to be aware of when evaluating online crowdsourced communities in general.  \n Within this study\u2019s findings, the Trump administration seems to be a key area of \nfocus for the r/e nvironment community. This illustrates how Americanized the discussion \nof environmental politics is within the r/environment community. Readers and users who \nlook to social media sites that crowdsource articles should be leery of content provided \nand shoul d question the bias and intentions of article posters and commenters. Just \nbecause individuals who are not being paid to provide sources are creating content, unlike \nlegacy media structures, does not mean they are inherently unbiased. Caution in \ndissecting  information is an important ability for readers to utilize.  \n7.1 Limitations and Recommendations  \n There are numerous limitations  that exist within this study. Only one coder was \nutilized within the coding process which could represent a bias issue. To com bat a biased \ncoder all coding processes were done four times to make sure topics were coded \nconsistently. In addition, the criteria for categorization of concepts were abstracted from a \n1992 study that evaluated Time Magazine\u2019s portrayal of environmental t opics. This had \nthe potential to limit the ideological interpretations of content within r/environment. \nBecause the most upvoted articles were evaluated no timeframe was explicitly evaluated \nallowing articles and viewers with upvotes to allocate their deem ed importance. This \nallowed cultural phenomena\u2019s like Trump\u2019s election to affect the data timeline (more \nupvoted articles within the subreddit around this election). This affects the content being \n95 \n \n  displayed within the study. Perhaps future studies aimed at  understanding crowdsourced \narticles within a timeframe could evaluate a dataset with fixed dates as opposed to top \nupvoted articles. This would allow researchers to evaluate cultural phenomenon and the \nreaction r/environment exhibits. This study evaluated  top sourced and upvoted articles \ndue to the importance they represented within the community , therefore a timeframe \ncreated itself from the dataset .  \n Future studies that evaluate crowdsourced social media sites like Reddit could \nbenefit from comparing th eir news coverage regarding topics like the environment with \nlegacy news outlets. In addition, new forms of online media that are apart of legacy \nmedia structures can also be evaluated for discourse and content. Other academic studies \ncould also benefit fr om comparing different crowdsourced social media sites and their \nportrayal of topics like the environment. A simple way to do this would be to compare \ncertain subreddits and their discussions and perceptions of similar cultural topics that \nemerge. This wou ld help illustrate the overall championed and self -selected content users \nare liking or upvoting on different media platforms.  \n Not many academic studies have evaluated crowdsourced online news \ncommunities. This means, there is ripe content available for academic studies that wish to \nevaluate changing media landscapes and new forms of news dissemination. This growing \nfield should utilize content analysis and internet -based forms of ethnography, due to the \namount of readily available information that exists  within crowdsourced online \ncommunities.  \n \n96 \n \n  REFERENCES  \nAllan, K. 2018. Critical Political Economic Approaches to the Study of Language and  \nNeoliberalism. Journal of Sociolinguistics , Vol. 22, Issue 4, p.454 -469. \n \nBrulle , R. 2003. Environmental Discourse and Social Movement Organizations: A  \nHistorical and Rhetorical Perspective on the Development of U.S. Environmental  \nOrganizations. Sociological Inquiry , Vol. 66, Issue 1.  \n \nChomsky, N. and Herman, E. 2002. Manufacturing  Consent: The Political Economy of  \nthe Mass Media . Pantheon Books.  \n \nDaniller, A. Allen, D. Tallevi, A. and Mutz, D. 2017. Measuring Trust in the Press in a  \nChanging Media Environment. Communication Methods and Measures , Vol. 11,  \nIssue 1, p.76 -85. \n \nDetwe iler, S. 1992. A Content Analysis of Environmental Reporting in Time and the  \nNew York Times, 1991 and 1992. Detweiler.us/thesis.html  \n \nFirmstone, J. 2016. Mapping Changes in Local News. Journalism Practice , Vol. 10,  \nIssue 7, p.928 -938. \n \nFirmstone , J. and Coleman, S. 2014. The Changing Role of the Local News Media in  \nEnabling Citizens to Engage in Local Democracies. Journalism Practice , Vol. 8,  \nIssue 5, p.596 -606. \n \nGottfried, J. and Shearer, E. 2017. News Use Across Social Media Platforms 2017. P ew  \nResearch Center.  \n \nGreenwood, K. and Thomson, T. 2017. Beyond Framing . Journalism Practice , Vol. 11,  \nIssue 5, p.625 -644. \n \nGreenwood, K. and Thomson, T. 2017. I \u201cLike\u201d That: Exploring the Characteristics That  \nPromote Social Media Engagement with News P hotographs. Visual  \nCommunication Quarterly , Vol. 24, Issue 4, p.203 -218. \n \nGuzman, M. 2013. After Boston, Still Learning. Quill  Vol. 101, Issue 3, p.22 -25. \n \nHackett, R. and Zhao, Y. 1998. Sustaining Democracy?: Journalism and the Politics of  \nObjectivity . Garamond Press.  \n \n97 \n \n  Halpern, D. Katz, J. and Valenzuela, S. 2017. We Face, I Tweet: How Different Social  \nMedia Influence Political Participation through Collective and Internal Efficacy.  \nJournal of Communication , Vol. 22, Issue 6, p.320 -336. \n \nHansen, Ralph E. 2018. Mass Communication: Living in a Media World . Sage  \nPublishing.  \n \nHilbert, M. Vasquez, J. Halpern, D. and Valenzuela, S. 2017. One Step, Two Step,  \nNetwork Step? Complementary Perspectives on Communication Flows in  \nTwittered Citizen P rotests. Social Science Computer Review , Vol. 35, Issue 4,  \np.444 -461. \n \nHolcomb, J. Gottfried, J. and Mitchell, A. 2013. News Use Across Social Media  \nPlatforms. Pew Research Center.  \n \nIbarra, K. 2009. The Promotion of Public Interest Through New Policy Ini tiatives for  \nPublic Television: The cases of France and Spain. Interactions: Studies in  \nCommunication and Culture . Vol. 1, Issue 2, p.267 -282. \n \nInternational Communication Association. Source, Please? A Content Analysis of Links  \nPosted in Discussions of  Public Affairs on Reddit. 2018. 519130 Internet  \nPublishing and Broadcasting and Web Search Portals, p.1 -18. \n \nInternational Communication Association. The Voices of Reddit: Exploring the Effects  \nof New Entrants on The Content of Online Discussions. 2018.  519130 Internet  \nPublishing and Broadcasting and Web Search Portals p.1 -25. \n \nLasswell, H. 1948. The Analysis of Political Behavior: An Empirical Approach . The Yale  \nLaw Journal Company.  \n \nMatthews, J. 2015. To Establish the Effects of Branding on the Purchasing Behavior of  \nConsumers aged 18 -25 in the UK Clothing Industry. Cardiff Metropolitan \nUniversity.  \n \nMediabiasfactcheck.com, Retrieved in 2019.  \n \nMitchell, S. and Lim, M. 2018. Too Crowde d for Crowdsourced Journalism: Reddit,  \nPortability, and Citizen Participation in the Syrian Crisis. Carleton University.  \n \n \nMoessner, M. 2018. Analyzing Big Data in Social Media: Text and Network Analyses of  \nan Eating Disorder Forum. Int J Eat Disord , Vol . 51, Issue 7, p.656 -667. \n \n98 \n \n  Muchnik, L. Aral, S. and Taylor, SJ. 2013. Social Influence Bias: A Randomized  \nExperiment. Science , Vol. 341, Issue 6146, p.647 -651. \n \nNhan, J. Huey, L. and Broll, R. 2015. Digilantism: An analysis of Crowdsourcing and the  \nBosto n Marathon Bombings. The British Journal of Criminology , Volume 57,  \nIssue 2, p. 341 -361. \n \nPearson, G. and Knobloch, S. 2018. Perusing Pages and Skimming Screens: Exploring  \nDiffering Patterns of Selective Exposure to Hard News and Professional Sources  \nin Online and Print News. New Media and Society , Vol. 20, Issue 10, p.3580 - \n3596.  \n \nPoell, T. 2014. Connecting Activists and Journalists. Journalism Studies , Vol. 16, Issue 5,  \np719 -733. \n \nRodny -Gumede, Y. 2017. Social Media and the Re -affirmation of the Role of Journalism:  \nA Cursory Discussion of the Potential for Widening the Public Sphere in a  \nPostcolonial Society. Interactions: Studies in Communication and Culture , Vol. 8,  \nIssue 2/3, p.169 -187. \n \nSeethaler, J. and Beaufort, M. 2014. Community Media and Broadcast Journalism in  \nAustria: Legal and Funding Provisions as Indicators for the Perception of the  \nMedia\u2019s Societal Roles. Radio Journal: International Studies in Broadcast and  \nAudio Med ia Vol. 15, Issue 2, p.173 -194. \n \nStephens, M. and Jarvis, S. 2016. The Partisan Affect of News Seekers vs. Gatekeepers:  \nLinguistic Differences in Online vs. Front -Page News in Campaign 2012.  \nCommunication Research Reports , Vol. 33, Issue 3, p.275 -280. \n \nSuciu, I. 2018. Credibility and Freedom of Choice in Social Media in Relation with  \nTraditional Media. Journal of Media Re search , Vol. 11, Issue 3, p.24 -34. \n \nSuran, M. and Kilgo, K. 2017. Freedom from the Press? Journalism Studies , Vol. 18,  \nIssue 8, p.1035 -1051.  \n \nThomas, W. and Thomas, D. 1928. The Child in America: Behavior Problems and  \nPrograms . New York: Knopf, p.571 -572. \n \nVago, M. 2016. A decade before Boaty Mcboatface, the internet came together to name a  \nwhale. AV Club.  \n \nValenzuela, S. Correa, T. and Gil de Zuniga H Gil. 2018. Ties, Likes, and Tweets: Using  \nStrong and Weak Ties to Explain Differences in Protest Participation Across  \n99 \n \n  Facebook and Twitter Use. Political Communication , Vol. 35, Issue 1, p.117 -134. \n \nValenzuel a, S. Pina, M. and Ramirez, J. 2017.  Behavioral Effects of Framing on Social  \nMedia Users: How Conflict, Economic, Human Interest, and Morality Frames  \nDrive News Sharing. Journal of Communication , Vol. 67, Issue 5, p.803 -826. \n \nWasike, B. 2013. Framing Ne ws in 140 Characters: How Social Media Editors Frame the  \nNews and Interact with Audiences via Twitter. Global Media Journal : Canadian \nEdition. Vol. 6 Issue 1, p.5 -23. \n \nWasike, B. 2016. The Significant Other: A Longitudinal Analysis of Significant Samples  \nin Journalism Research 2000 -2014. International Journal of Communication , Vol.  \n10, p.2744 -2765.  \n \nWasike, B. 2018. Preaching to the Choir? An Analysis of Newspaper Readability vis -\u00e0- \nvis Public Literacy. Journalism , Vol. 19 Issue 11, p.1570 -1587.  \n \nWesterw ick, A. Johnson, B. Knoblock -Westerwick, S. 2017. Confirmation Biases in  \nSelective Exposure to Political Online Information: Source Bias vs. Content Bias.  \nCommunication Monographs , Vol. 84, Issue 3, p.343 -364.  \n \n \n  \n100 \n \n  APPENDI X \nTable 1: Political Articles within r/environment  \n In order to be categorized as a political article, the main focus of the article \nheadline has to deal with a policy, political topic, or political person. This means the \neffects of a polic y may be discussed or just how a political individual or organization is \nattempting to suppress information regarding the environment.  \nArticle Title  Sources  Date  Upvotes  # Of Comments  \nTrump signs his first significant \nbill \u2014 killing a transparency rule \nfor oil companies  www.vox.com  2/14/17  21400  1100  \nTrump's America First Energy \nPlan Actually Leaves America \nBehind - \"there\u2019s not a word about \nthe clean energy revolution, a \nboom in wind, solar, and energy \nefficiency that is creating millions \nof jobs, saving billions of dollars, \nand even saving lives by cutting \npollution.\"  cleantechnica.com  1/31/17  19900  1100  \nU.S. is the only nation in the \nworld not in Paris deal after Syria \nsigns on.  thehill.com  11/7/17  19600  1700  \n101 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nTrump has reportedly decided to \nwithdraw from the Paris climate \ndeal businessinsider.com  5/31/17  19100  3200  \nBernie Sanders questions EPA \nnominee Wheeler \"Is Climate \nChange a hoax?\"  Video of Sanders \nquestioning  1/17/19  18700  1700  \nIceland elects 41 -year-old \nenvironmentalist as prime minister  inhabitat.com/  12/5/17  18200  543 \nThousands of Pages Of Emails \nFrom Trump\u2019s New EPA Chief \nHave Been Released  buzzfeednews.com  2/23/17  17700  1300  \nAl Gore offers to work with \nTrump on clim ate change  time.com/  11/12/16  17700  1400  \nTrump\u2019s presidency is dangerous \nfor the planet  local.sltrib.com/  11/5/17  17500  1100  \nHarrison Ford Goes After Trump \nTeam While Accepting \nEnvironmental Honor: \"We\u2019ve got \npeople in charge of important shit \nwho don\u2019t believe in science\"  www.hollywoodreporter.\ncom/  11/4/17  17100  576 \nDonald Trump\u2019s stance on global \nwarming is \u2018sociopathic, paranoid www.independent.co  6/8/17  15900  702 \n102 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nand malevolent\u2019, world -leading \neconomist says  \nTrump's Hiring Freeze Will \nDevastate Our National Parks: Our \novercrowded National Parks need \nto hire 8,000 temporary employees \nto make sure you can go camping \nthis summer.  motherboard.vice.com/  1/26/17  15600  1300  \nThe world shrugs at Trump as \nglobal climate meeting begins in \nBonn. \u201cTrump is utterly isolated, \nand U.S. positions and negotiators \nwill be largely ignored as a result. \nThis has become a life -and-death \nissue f or most countries,\u201d  washingtonpost.com/  11/6/17  14200  1100  \nTrump administration sued over \nclimate change 'censorship' | NGO \nsuit claims US agencies are \nillegally withholding information \nabout the suppression of climate \nscience in public communications  climatechangenews.com  5/25/16  14000  166 \nDem lawmaker: Trump 'tweeting \nlike a child who hates science thehill.com  12/28/17  12500  471 \n103 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nclass'  \n2 states sue over Trump \nadministration suspending a rule \nthat would save taxpayers $330 \nmillion - \u201cPresident Trump should \nput the health of the American \npeople over the profits of private \ncompanies.\u201d  thinkprogress.org  7/6/17  12200  294 \nLeaked memo shows EPA told \nemployees to lie about climate \nscience  shareblue.com  3/29/17  11900  548 \nWoman Dragged Out of West \nVirginia House Hearing for \nListing Oil and Gas Contributions \nto Members  commondreams.org  2/11/18  11800  335 \nFrance lures U.S. scientists with \nanti-Trump climate grants. Several \nU.S.-based climate scientists are \nabout to win multi -year, all -\nexpenses -paid grants to relocate to \nFrance.  cbsnews.com  12/11/17  11700  521 \nCrime Against Humanity' and \n'International Embarrassment': commondreams.org  11/27/18  11400  496 \n104 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nTrump Refuses to 'Believe' \nClimate Report - His \nadministration just released a \nmajor report detailing far -reaching \nimpacts of the climate crisis. His \nresponse? \"I don't believe it.\"  \nSchwarzenegger mocks Trump on \ncoal, asks if he'll bring back \nBlockbuster next  cnn.com  6/28/18  11400  441 \nEditorial: Trump says, \u2018I have won \nawards on environmental \nprotection\u2019 \u2014 EnviroNews says, \n\u2018Bullshit!\u2019  environews.tv  4/9/17  11100  263 \nTrump officials erase \u2018shocking\u2019 \namount of climate data from yet \nanother website: You paid for U.S. \nGeological Survey climate data, \nbut the White House is making it \ndisappear.  thinkprogress.org  9/18/17  10900  451 \n\"I'm not going to stand back and \njust let evil win.\" CEO to sue \nTrump over land grab.  shareblue.com  12/4/17  10000  369 \nOn Capitol Hill, EPA chief gets an washingtonpost.com  6/28/17  9700  388 \n105 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nearful about Trump\u2019s \u2018downright \noffensive\u2019 budget plan. \"These \ncuts aren\u2019t an intent to rein in \nspending, they are an intentional \nstep to  undermine science and \nignore environmental and public \nhealth realities.\u201d  \nTrump official gets laughed at \nwhen promoting fossil fuels at \nCOP24  video of COP24  12/12/18  9700  725 \nLeaked Trump Administration \nMemo: Keep Public in Dark \nAbout How Endangered Species \nDecisions Are Made  ecowatch.com  10/22/18  9600  248 \n\"\"But six months before people \nwere sickened by the \ncontaminated romaine, Pres ident \nDonald Trump\u2019s FDA \u2013 \nresponding to pressure from the \nfarm industry and Trump\u2019s order \nto eliminate regulations \u2013 shelved \nthe water -testing rules for at least \nfour years.\"  wired.com  11/26/18  9400  393 \n106 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nThe Republicans  who urged \nTrump to pull out of Paris deal are \nbig oil darlings: Twenty -two \nsenators wrote a letter to the \npresident when he was said to be \non the fence about backing out. \nThey received more than $10m \nfrom oil, gas and coal companies \nthe past three electio n cycles  theguardian.com  6/1/17  9300  293 \nI am an Arctic researcher. Donald \nTrump is deleting my citations  theguardian.com  3/28/17  9000  350 \nA Trump team member just \ncompared climate science to the \nflat-Earth theory  washingtonpost.com  12/15/16  8700  714 \nTrump: Climate 'will change back \nagain'  video from HBO  11/5/18  8700  888 \nMark Zuckerberg says Donald \nTrump\u2019s decision on the Paris \nagreement \u2018puts our children\u2019s \nfuture at risk\u2019  recode.net  6/2/17  8700  558 \nTrump is deleting climate change, \none site at a time - The \nadministration has taken a hatchet theguardian.com  5/14/17  8600  353 \n107 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nto climate change language a cross \ngovernment websites. Here are \nseveral of the more egregious \nexamples.  \nTrump's Secretary of Defense says \nclimate change is real, and a \nnational security threat - \"a \nposition that appears at odds with \nthe views of the president who \nappointed him and many in the \nadministration in which he \nserves.\"  businessinsider.com  3/15/17  8500  337 \nUS government takes animal -\nwelfare data offline - The US \nDepartment of Agriculture will no \nlonger make lab inspection results \nand violations publicly available, \nciting privacy concerns.  nature.com  2/4/17  8100  263 \nDonald Trump\u2019s War on Scientists \nHas Had One Big Side Effect - \nMore than a dozen Democratic \ncandidates with scientific \nbackgrounds are running for motherjones.com  7/31/17  7900  256 \n108 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nCongress.  \nTrump on climate change: \u2018People \nlike myself, we have very high \nlevels of intelligence but we\u2019re not \nnecessarily such believers.\u2019 | \n\u201cHow can one possibly respond to \nthis?\u201d Dessler said when reached \nby email, calling the president\u2019s \ncomments \u201cidiotic\u201d  washingtonpost.com  11/28/18  7600  375 \nThe Pentagon is ignoring Trump's \nmandate to treat climate change as \na hoax  businessinsider.com  9/13/17  7400  251 \nWeather society to Perry: You \nlack 'fundamental understanding' \nof climate science  thehill.com  6/22/17  7400  287 \nTrump\u2019s proposed fee hike will \ncreate class system at national \nparks  thehill.com  11/13/17  7300  533 \nNew Poll Shows Basically \nEveryone Likes Alexandria \nOcasio -Cortez's Green New Deal  earther.gizmodo.com  12/17/18  7300  657 \n \n109 \n \n  Table 2: Management of Energy Resources Articles in r/environment  \n In order to be categorized within the management of energy resources section, \narticles must discuss a corporate or private entity that is affecting the environment. This \ncould be a wealthy CEO donating to an envi ronmental group or discussion of large -scale  \nindustrial effects on the environment. Policies that regulate these private entities would \nfall under the political section due to the main issue representing policies and their effects \non private industry.  \nArticle Title  Sources  Date  Upvotes  # Of Comments  \nBill Gates thinks the 1% \nshould foot the bill for \nrenewable energy, and he's \noffering the first $2B.  www.emphasisms.com  10/1/17  48300  1500  \nCoal CEO admits that \n\u2018clean coal\u2019 is a myth  reneweconomy.com.  7/17/17  17200  628 \nTrump's decision to allow \nplastic bottle sales in \nnational parks slammed: \n\"The Corporate Agenda is \nKing\"  www.theguardian.com  8/21/17  16400  692 \n\u2018Clean coal\u2019 doesn\u2019t exist.  thinkprogress.org/  1/30/17  15800  529 \nA ban on bottled water in \n23 national parks \nprevented up to 2m plastic www.theguardian.com/  9/26/ 17 14800  780 \n110 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nbottles from being used \nand discarded every year: \nDespite that, the Trump \nadministration reversed \nthe bottled water ban just \nthree months later, a \ndecision that horrified \nconservationists and \npleased the bottled water \nindustry.  \nPhotographer Says He \nLost His Job After \nLeaking Pictures Of  Rick \nPerry And Coal CEO - \n\u2018The photographs show... \nMurray handing Perry a \nfour-page confidential \n\"action plan\" for reviving \nthe country's struggling \ncoal industry... it mirrors \npolicy later pushed by the \nTrump administration.\u2019  npr.org  1/18/18  13900  459 \nElon Musk: \u201cWe know \nwe\u2019ll run out of dead cnbc.com  12/26/18  13200  473 \n111 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \ndinosaurs to mine for fuel \n& have to use sustainable \nenergy eventually, so why \nnot go renewable now & \navoid increasing risk of \nclimate catastrophe? \nBetting that science is \nwrong & oil companies \nare right is the dumbest \nexperiment in history by \nfar\u201d \nBitcoin\u2019s energy usage is \nhuge \u2013 we can't afford to \nignore it | The \ncryptocurrency uses as \nmuch CO2 a year as 1m \ntransatlantic flights. We \nneed to take it seriously as \na climate threat  theguardian.com  2/27/18  11700  1000  \nPresident Obama Thinks \nWe Should Eat Less Meat \nto Help Combat Climate \nChange  onegreenplanet.org  5/11/17  11700  1700  \nTrump\u2019s promise to bring washingtonpost.com  5/3/17  11200  601 \n112 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nback coal jobs is worse \nthan a con | telling those \ncommunities, in effect: \nThe best hope they have, \nand that their children \nhave, is to be trapped in a \ndying industry that will  \npoison them.  \nExxon\u2019s Pro -Fracking \nCEO Is Suing to Stop \nFracking Near His \nMansion (2014)  thenation.com  12/13/16  10100  270 \nNo Park Rangers or Food \nInspections \u2013 But \nGovernment Reopens for \nOil and Gas: a handful of \nbureaucrats were among \nthose back at work \napproving drilling \napplications for the oil and \ngas sector \u2013 a move that \nsome say is illegal and \npossibly even criminal.  usnews.com  1/14/19  10100  257 \nEach day a cruise ship independent.co.uk  1/29/19  9900  291 \n113 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nemits as much pollution as \na million cars  \nEPA chief: \u2018True \nenvironmentalism is using \nnatural resources that God \nhas blessed us with\u2019  thinkprogress.org  10/27/17  8800  799 \nNestle Faces Backlash \nOver Collecting Water \nFrom  Southern California. \nThe Forest Service is now \nreviewing Nestle\u2019s permit \nfor the first time in 30 \nyears.  losangeles.cbslocal.com  5/10/17  8700  365 \nThe Koch Brothers Are \nBehind a Plot to Open Up \nthe Grand Canyon \nWatershed to Toxic \nUranium Mining - Last \nyear, Arizona Rep. Ra\u00fal \nGrijalva introduced a bill \nto protect the greater \nGrand Canyon as a \nnational monument. But it \nhas stalled.  altnet.org  8/3/16  8700  357 \n114 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nWhy ea ting less meat is \nthe best thing you can do \nfor the planet in 2019  theguardian.com  12/25/18  8600  2100  \nChina reassigns 60,000 \nsoldiers to plant trees in \nbid to fight pollution. \n\"Area to be planted by the \nend of the  year is roughly \nthe size of Ireland.\"  independent.co.uk  2/15/18  8500  305 \nNorway Is First Nation to \nBan All Palm Oil Based \nBiofuel to Prevent \nRainforest Destruction  livekindly.co  12/11/18  8000  101 \n \n  \n115 \n \n  Table 3: Environmental Movement Articles in r/environment  \n In order to be categorized as an environmental movement, article headlines must \ndiscuss a certain e nvironmental movement or elicit environmental advocacy from reddit \nusers themselves.  \nArticle Title  Sources  Date  Upvotes  # Of Comments  \n35 000 Belgian High School \nstudents skipped school to \nurge the Belgian government \nto adopt solid and effective \nclimate change policies. \nThird week in a row of \nskipping school (first week: \n3000 students, second week: \n12 500 students, now 35 000 \nstudents) #YouthForClimate  Video of protest  1/24/19  24900  746 \nFrance is offering US \nscientists 4 -year grants to \nmove to the country and do \nresearch on climate change \nvia a website called Make \nOur Planet Great Again.  businessinsider.in/  1/17/18  22400  1200  \nLet's get Trump to accept \nclimate change  Self Post  11/8/16  22200  939 \n116 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nTrump is asking us how to \nmake America great \nagain...It's our chance to tell \nhim how important the issue \nof climate change is to us!  ptt.gov/yourstory/  11/11/16  20000  855 \nTrump has scientists mad \nenough to march on Earth \nDay. Simply telling the truth \nhas become a political act. \nBy marching for truth, \nscientists are not being \npolitical - they are merely \ndoing their job.  www.philly.com/  4/6/17  19600  1500  \nDonald Trump urged to \nditch his climate change \ndenial by 630 major firms \nwho warn it 'puts American \nprosperity at risk'  .independent.co.uk/  1/10/17  16600  596 \n117 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nU.S. Army veterans are \nplanning a 'deployment' to \nStanding Rock to protest the \nDakota Access Pipeline: \n\u201cThis country is repressing \nour people. We need to do \nthe things that we actually \nsaid we\u2019re going to do when \nwe took the oath to defend \nthe Constitution from \nenemies foreign and \ndomestic.\u201d  businessinsider.com  11/22/16  16500 1100  \nUS House Representative \nMatt Gaetz  is sponsoring \nBill H.R. 861 - Which would \nterminate the EPA. Here is \nhis contact information if \nyou don't want to see \nunmitigated pollution, toxic \ndumping and the rape of our \npublic lands in the US - Reddit thread  3/6/17  11000  455 \n118 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nThere's Now a \nWhistleblower Hotline for \nScientists Working Under \nthe Trump Admin - A \npolitical advocacy group, \n314, set up the hotline with a \nlaw firm to help scientists \nraise alarms if needed.  board.vice.com  11/2/17  10700  120 \nLeonardo DiCaprio: \"We \nAre the Last Generation \nThat Has a Chance to Stop \nClimate Change\" - The 3rd \ngala of the actor's \nfoundations sets new \nfundraising record, raising \nnearly $45 million.  alternet.org  7/27/16  10500  757 \nThe U.S. Supreme Court on \nMonday rejected a bid by \nPresident Donald Trump\u2019s \nadministration to put the \nbrakes on a lawsuit filed by \nyoung activists who have \naccused the U.S. \ngovernment of ignoring the in.reuters.com  7/30/18  9700  226 \n119 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nperils of climate change.  \nThe Trump Administration \nJust Went To  Court To Stop \nKids From Suing Over \nClimate Change. Twenty -\none young people are suing \nthe US government for \ncontributing to climate \nchange in violation of their \nconstitutional rights.  buzzfeed.com  12/12/17  8500  141 \n \n  \n120 \n \n  Table 4: Wildlife Preservation Articles in r/environment  \n In order to be categorized as an article that discusses wildlife preservation, the \nmain focus  of the article headline has to deal with either preserving wildlife areas or \npreserving and protecting endangered species that are threatened across the globe.  \nArticle Title  Sources  Date  Upvotes  # Of Comments  \nDonald Trump\u2019s Earth Day \nStatement Is Shameful: Trump \nreleased an Earth Day statement \ntouting his commitment to \nprotecting the environment, \ndespite doing the exact opposite \nin the first few months of his \nadministration.  huffingtonpost  4/22/17  18200  985 \nNow Trump's Going After the \nBumblebees: The administration \njust delayed endangered status \nfor a bumblebee species that's on \nthe brink of extinction.  motherjones.com  2/11/17  12200  372 \nTrump's 'beautiful wall' threatens \n111 endangered species  theecologist.org  2/20/17  11100  785 \nTrump Blames \u2018Bad \nEnvironmental Laws\u2019 for \nCalifornia Wildfires, Says \u2018Must\u2019 \nCut Down Trees  mediaite.com  8/5/18  10900  1100  \n121 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nFury after China reverses 25 -\nyear-old tiger bone and rhino \nhorn ban  mirror.co.uk  10/29/18  10600  384 \nAfter protests & public outcry, \nGOP bill to sell off 3.3M acres of \npublic lands has been withdrawn  thinkprogress.org  2/2/17  10500  138 \nMan Postpones Retirement to \nSave Reefs After He \nAccidentally Discovers How to \nMake Coral Grow 40 Times \nFaster  goodnewsnetwork  12/3/18  10100  225 \nTrump Administration Denies \nEndangered Protections For 25 \nSpecies. \"You couldn' t ask for a \nclearer sign that the Trump \nadministration puts corporate \nprofits ahead of protecting \nendangered species.\"  newsy.com  10/6/17  9900  228 \nTrump administration is taking \nsteps to remove a threatened lynx \nfrom the  endangered -species list: \nThe Trump administration \nannounced that it\u2019s moving to washingtonpost  1/13/18  7800  327 \n122 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nstrike the Canada lynx from the \nendangered -species list, despite \nan assessment the species will die \nout in its northern range without \nprotection.  \nSwiss Businessman is \nContributing $1 Billion Towards \nProtecting 30% of the Planet  goodnewsnetwork  11/8/18  7600  248 \n \n  \n123 \n \n  Table 5: Environmental Pollution Articles in r/environment  \n In order to be categorized as an article dealing with environmental pollution, \narticle headlines had to encompass some form of environmental pollution. Common \nenvironmental pollu tion topics include water pollution, air pollution, and pollution that \naffects humans and wildlife directly.  \nArticle Title  Sources  Date  Upvotes  # Of Comments  \nTrump caps off a long day by \nletting coal companies dump \nwaste into streams.  newrepublic.com/  2/17/17  20300  1100  \nFrance is the first country to \nban all five bee -killing \npesticides.  www.inquisitr.com/  11/21/18  14100  166 \n\u2018This is a stupid policy\u2019: \nEmissions become latest front \nin California -Trump war - \n\"motorists will pay more at the \npump, get wors e gas mileage \nand breathe dirtier air. \nCalifornia will fight this \nstupidity in every conceivable \nway possible.\u201d  politico.com  8/3/18  11500  828 \nThe EPA Is Beginning To  Roll \nBack An Obama -Era Rule \nLimiting How Much Toxic buzzfeed.com  8/16/17  11000  422 \n124 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nWaste Power Plants Release In \nWater. | \"... so beholden to big \nbusiness that they are willing \nto let power plants continue to \ndump lead, mercury, chromium \nand other dangerous chemicals \ninto our water sup ply,\u201d  \nTrump\u2019s Legacy: Damaged \nBrains, This is what a common \npesticide does to a child\u2019s \nbrain. -- Studies show that it \ndamages the brain and reduces \nI.Q.s while causing tremors \namong ch ildren. Now the \nTrump administration is \nembracing it, overturning a \nplanned ban that had been in \nthe works for years.  nytimes.com  10/29/17  10200  632 \nThanks to Republicans, \nPesticide Companies Are Now \nFree to Kill All the Endangered \nSpecies They Want - \u201cIt\u2019s like \nkicking them when they\u2019re \ndown.\u201d  motherjones.com  7/26/18  9800  577 \n125 \n \n  Article Title  Sources  Date  Upvotes  # Of Comments  \nMonsanto 'bullied scientists' \nand hid weedkiller  cancer risk, \nlawyer tells court  theguardian.com  7/10/18  9000  589 \nPoison once flowed in \nAmerica's waters. With Trump, \nit might again  theguardian.com  8/14/17  8200  409 \nOne day after getting sued by \n15 states, the Environmental \nProtection Agency on \nWednesday reversed course on \nits plans to delay \nimplementation of Obama -era \nrules intended to reduce \nemissions of smog -causing air \npollutants.  thinkprogress.org  8/3/17  7900  171 \nA cloud of radioactive \npollution over Europe in recent \nweeks indicates that an \naccident has happened in a \nnuclear facility in Russia or \nKazakhstan in the last week of \nSeptember, F rench nuclear \nsafety institute IRSN said  reuters.com  11/9/17  7500  355 \n \n126 \n \n  Table 6: Media Related Articles in r/environment  \n In order to be categorized as a media related article, article headlines  had to \ndiscuss some form of media portrayal of the environment. This could vary from popular \ncultural portrayal and news portrayal of environmental concepts.  \nArticle Title  Link to Article  Date  Upvotes  # Of Comments  \n\u201cSouth Park\u201d apologizes to Al \nGore and admits it was wrong \nabout global warming  www.salon.com/  11/9/18  21700  2800  \nFox News host Tucker Carlson \nbit off more than he could chew \nwhen he tried to go head to head \nwith \u2018Science guy\u2019 Bill Nye over \nclimate change.  rawstory.com  2/28/17  9600  1400  \nClimate Deniers Debunked: \nWeather Channel Brutally Owns \nBreitbart  gizmodo.com  12/7/16  9200  766 \nClimate change, Banksy style  Embedded video  10/10/18  8200  107 \nThe media has essentially \nstopped covering climate change.  newrepublic.com  3/24/17  7600  244 \n \n  \n127 \n \n  Table 7: Ecological Shift Articles in r/environment  \n In order to be categorized as an ecological shift, article headlines had to discuss \nshifts within various ecosystems across the globe.  \nArticle Title  Link to Article  Date  Upvotes  # Of Comments  \nEarth has lost 10% of its \nwilderness since 1992. At \ncurrent rates it will all disappear \nin 50 years.  theguardian.com  12/21/17  9900  674 \nThe World Has Barely 10 Years \nTo Get Climate Change Under \nControl, U.N. Scientists Say  washingtonpost.com  10/7/18  8100  990 \n ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "The green web: Evaluating online news communities and their environmentalism", "author": ["SH Roussell"], "pub_year": "2019", "venue": "NA", "abstract": "In the past news and media were disseminated through channels that were difficult for  consumers to communicate with. Now a new form of news dissemination is taking place on the"}, "filled": false, "gsrank": 295, "pub_url": "https://digitalcommons.humboldt.edu/etd/320/", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:h6LfgwrM3CwJ:scholar.google.com/&output=cite&scirp=294&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D290%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=h6LfgwrM3CwJ&ei=OLWsaJm1KI6IieoP0sKRuAk&json=", "num_citations": 1, "citedby_url": "/scholar?cites=3232682978079384199&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:h6LfgwrM3CwJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://digitalcommons.humboldt.edu/cgi/viewcontent.cgi?article=1339&context=etd"}}, {"title": "SemEval-2019 task 4: Hyperpartisan news detection", "year": "2019", "pdf_data": "Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019) , pages 829\u2013839\nMinneapolis, Minnesota, USA, June 6\u20137, 2019. \u00a92019 Association for Computational Linguistics829SemEval-2019 Task 4: Hyperpartisan News Detection\nJohannes Kiesel1Maria Mestre2;3Rishabh Shukla2Emmanuel Vincent2\nPayam Adineh1David Corney4Benno Stein1Martin Potthast5\n1Bauhaus-Universit\u00e4t Weimar\n<first >.<last>@uni-weimar.de2Factmata Ltd.\n<first >.<last>@factmata.com\n3mariarmestre@gmail.com\n4dpacorney@gmail.com5Leipzig University\nmartin.potthast@uni-leipzig.de\nAbstract\nHyperpartisan news is news that takes an ex-\ntreme left-wing or right-wing standpoint. If\none is able to reliably compute this meta in-\nformation, news articles may be automatically\ntagged, this way encouraging or discouraging\nreaders to consume the text. It is an open ques-\ntion how successfully hyperpartisan news de-\ntection can be automated, and the goal of this\nSemEval task was to shed light on the state of\nthe art. We developed new resources for this\npurpose, including a manually labeled dataset\nwith 1,273 articles, and a second dataset with\n754,000 articles, labeled via distant supervi-\nsion. The interest of the research community\nin our task exceeded all our expectations: The\ndatasets were downloaded about 1,000 times,\n322 teams registered, of which 184 con\ufb01gured\na virtual machine on our shared task cloud ser-\nvice TIRA, of which in turn 42 teams sub-\nmitted a valid run. The best team achieved\nan accuracy of 0.822 on a balanced sample\n(yes : no hyperpartisan) drawn from the manu-\nally tagged corpus; an ensemble of the submit-\nted systems increased the accuracy by 0.048.\n1 Introduction\nYellow journalism has established itself in so-\ncial media, nowadays often linked to phenomena\nlike clickbait, fake news, and hyperpartisan news.\nClickbait has been its \ufb01rst \u201csuccess story\u201d (Potthast\net al., 2016): When the viral spreading of pieces of\ninformation was \ufb01rst observed in social networks,\nsome investigated how to manufacture such events\nfor pro\ufb01t. Unlike for \u201cnatural\u201d viral content, how-\never, readers had to be directed to a web page\ncontaining the to-be-spread information alongside\npaid-for advertising, so that only teasers and not\nthe information itself could be shared. Then, to\nmaximize their virality, data-driven optimization\nrevealed that teaser messages which induce curios-ity, or any other kind of strong emotion, spread best.\nThe many forms of such teasers that have emerged\nsince are collectively called clickbait. New pub-\nlishing houses arose around viral content, which\nbrought clickbait into the mainstream. Traditional\nnews publishers, struggling for their share of the\nattention market that is a social network, adopted\nclickbait into their toolbox, too, despite its violation\nof journalistic codes of ethics.\nThe content spread using clickbait used to be\nmostly harmless trivia\u2014entertainment and distrac-\ntion to some, spam to others\u2014, but in the wake of\nthe 2016 United States presidential election, \u201cfake\nnews\u201d came to widespread public attention. While\ncertainly not a new phenomenon in yellow journal-\nism, its viral success on social media was a surprise\nto many. Part of this success was then attributed\nto so-called hyperpartisan news publishers (Bhatt\net al., 2018), which report strongly in favor of one\npolitical position and in \ufb01erce disagreement with\nits opponents. Clinging to hyperpartisanship often\nentails stretching the truth, if not breaking it with\nfake news, whose highly emotional content makes\nthem spread exceptionally fast, like clickbait.\nGiven the hype surrounding fake news, activists,\nindustry, and research are now paying a lot of at-\ntention to mitigating the problem, such as trying\nto check facts in news items. Clickbait and hyper-\npartisan news, however, have been less studied. In\nprevious work, we sought to help close this gap\nfrom both ends: for clickbait detection (Potthast\net al., 2016), part of our group created a large-scale\nevaluation dataset (Potthast et al., 2018b) and set\nup an ongoing competition for the best detection\napproach (Potthast et al., 2018a). For hyperpartisan\nnews detection (Potthast et al., 2018c), we teamed\nup to follow a similar approach that led to the Hy-\nperpartisan News Detection task at SemEval-2019.\nThis paper reports on the results of this task.\n8302 Task De\ufb01nition\nWe de\ufb01ne hyperpartisan news detection as follows:\nGiven the text and markup of an online\nnews article, decide whether the article\nis hyperpartisan or not.\nHyperpartisan articles mimic the form of regular\nnews articles, but are one-sided in the sense that\nopposing views are either ignored or \ufb01ercely at-\ntacked. We deliberately disregard the distinction\nbetween left and right, since previous work has\nfound that, in hyperpartisan form, both are more\nsimilar to each other in terms of style than either\nare to the mainstream (Potthast et al., 2018c). The\nchallenge of this task is to unveil the mimicking\nand to detect the hyperpartisan language, which\nmay be distinguishable from regular news at the\nlevels of style, syntax, semantics, and pragmatics.\n3 Data\nOur focus is on news articles published online, and\nwe provide two datasets with this task. One has\n1,273 articles, each labeled manually, while the sec-\nond, larger dataset of 754,000 articles is labeled in\na semi-automated manner via distant supervision at\nthe publisher level. These datasets are further split\ninto public and private sets. We released the public\nset for the model training, tuning, and evaluation,1\nwhile the unreleased private set is used to enable\nblind, cloud-based evaluation.\nAs online news articles are published mainly\nin the HTML format, both datasets use a uni\ufb01ed\nHTML-like format (see Figure 1). We restricted the\nmarkup for the article content to paragraphs ( <p>),\nlinks ( <a>), and quotes ( <q>). We distinguished\ninternal links to the other pages of the same do-\nmain, from which we removed the href-attribute\nvalue to avoid classi\ufb01ers \ufb01tting to them; and links\ntoexternal domains, for which we kept the attribute.\nAn XML schema that exactly speci\ufb01es the format\nis distributed along the datasets.\n3.1 Dataset Annotated By Article\nWe gathered a crowdsourced dataset of 1,273 arti-\ncles, each labeled manually by 3 annotators (Vin-\ncent and Mestre, 2018). These articles were pub-\nlished by active hyperpartisan and mainstream web-\nsites and were all assured to contain political news.\nAnnotators were asked to rate each article\u2019s bias on\nthe following 5-point Likert scale:\n1https://doi.org/10.5281/zenodo.14899201. No hyperpartisan content\n2. Mostly unbiased, non-hyperpartisan content\n3. Not Sure\n4. Fair amount of hyperpartisan content\n5. Extreme hyperpartisan content\nWe removed all articles from the dataset with\nlow agreement score and the aggregated rating of\n\u201cnot sure\u201d (see Vincent and Mestre for more de-\ntails). We then binarized the labels to hyperpar-\ntisan (average rating of 4 or 5) and not (average\nrating of 1 or 2). The \ufb01nal by-article set achieved\nan inter-annotator agreement of 0.5 Krippendorff\u2019s\nalpha. Of the remaining 1,273 articles, 645 were\npublished as a training dataset, whereas the other\n628 (50% hyperpartisan and 50% not) were kept\nprivate for the evaluation. To ensure that classi\ufb01ers\ncould not pro\ufb01t from over\ufb01tting to publisher style,\nwe made sure there was no overlap between the\npublishers of the articles between these two sets.\n3.2 Dataset Annotated By Publisher\nTo allow for methods that require huge amounts of\ntraining data, we compiled a dataset of 754,000 arti-\ncles, each labeled as per the bias of their respective\npublisher. To create this dataset, we cross-checked\ntwo publicly available news publisher bias lists\ncompiled by media professionals from BuzzFeed\nnews2and Media Bias Fact Check.3The former\nwas created by BuzzFeed journalists as a basis for a\nnews article, whereas the latter is Media Bias Fact\nCheck\u2019s main product. While both lists contain sev-\neral hundred news publishers, they disagree only\nfor nine, which we removed from our dataset.\nWe then crawled, archived, and post-processed\nthe articles available on the publishers\u2019 web sites\nand Facebook feeds. We archived all articles us-\ning a specialized tool (Kiesel et al., 2018) that re-\nmoves pop-overs and similar things preventing the\narticle content from being loaded. After \ufb01ltering\nout publishers that did not mainly publish politi-\ncal articles or had no political section to which we\ncould restrict our crawl, we were left with 383 pub-\nlishers. For each of the publishers\u2019 web sites we\nwrote a content-wrapper to extract the article con-\ntent and relevant meta data from the HTML DOM.\nWe then removed all articles that were too short\nto contain news,4that are not written in English,\n2https://github.com/BuzzFeedNews/\n2017-08-partisan-sites-and-facebook-pages\n3https://mediabiasfactcheck.com\n4Based on manual inspection of a hundred short articles, we\nset the threshold to 40 words.\n831<article id =\"0182515\" published-at =\"2007-01-22\" title =\"They\u2019re crumbling\">\n<p>What a pleasant surprise to see Jacques Leslie, a journalist and real expert on\ndams, with a long < a href =\"http://www.nytimes.com/2007/01/22/opinion/22leslie.2.html\n?ex=1327122000&amp;amp;en=42caf99f05e4cba8&amp;amp;ei=5090&amp;amp;partner=\nrssuserland&amp;amp;emc=rss\" type =\"external\">op-ed</ a> on the hallowed pages of the\nNew York Times. Leslie, author of < a href =\"\" type =\"internal\">Deep Water: The Epic\nStruggle Over Dams, Displaced People and the Environment</ a>, highlights the threat\nposed by poorly maintained and increasingly failing dams around the country:</ p>\n<p>Unlike, say, waterways and sanitation plants, a majority of dams - 56 percent of\nthose inventoried - are privately owned, which is one reason dams are among the\ncountry\u2019s most dangerous structures. Many private owners can\u2019t afford to repair\naging dams; some owners go so far as to resist paying by tying up official repair\ndemands in court or campaigning to weaken state dam safety laws.</ p>\n<p>Kinda makes you want to find out what is upstream.</ p> </ article >\nFigure 1: Example of a non-hyperpartisan article in our dataset. An archived version of the original article is\navailable at https://web.archive.org/web/20121006194050/https://grist.org/article/remember-the-dams/.\nor that contain obvious encoding errors. The \ufb01nal\ndataset consisted of 754,000 articles, split into a\npublic training set (600,000 articles), a public vali-\ndation set (150,000 articles) and a non-public test\nset (4,000 articles). Like for the by-article dataset,\nwe ensured that there is no overlap of publishers\nbetween the sets. Each set consists of 50% arti-\ncles from non-hyperpartisan publishers and 50%\narticles from hyperpartisan publishers, the latter\nagain being 50% from left-wing and 50% from\nright-wing publishers.\n4 Fairness and Reproducibility\nIn this shared task, we asked participants to submit\ntheir software instead of just its run output. The\nsubmissions were executed at our site on the test\ndata, enabling us to keep the test data entirely se-\ncret. This has two important advantages over tradi-\ntional shared task setups: \ufb01rst, software submission\ngives rise to blind evaluation; and second, it max-\nimizes the replicability and the reproducibility of\neach participant\u2019s approach. To facilitate software\nsubmission and to render it feasible in terms of\nwork overhead and \ufb02exibility for both participants\nand organizers, we employ the TIRA Integrated\nResearch Architecture (Potthast et al., 2019).\nA shortcoming of traditional shared task setups\nis that typically the test data are shared with partici-\npants, albeit without ground truth. Although partic-\nipants in shared tasks generally exercise integrity\nand do not analyze the test data other than running\ntheir software on it, we have experienced cases to\nthe contrary. Such problems particularly arise in\nshared tasks where the stakes are higher than usual;\nwhen monetary incentives are offered or winning\nresults in high visibility. A partial workaround is\nto share the test data only very close to the \ufb01nal\nsubmission deadline, minimizing analysis oppor-tunities. But if sharing the test data is impossi-\nble for reasons of sensibility and proprietariness,\nor because the ground truth can be easily reverse-\nengineered, a traditional shared task cannot be held.\nAnother shortcoming of traditional shared tasks\n(and many computer science publications in gen-\neral) is their lack of reproducibility. Although shar-\ning the software underlying experiments as well as\nthe trained models is easy, and although it would\ngreatly aid reproducibility, this is still rare. Typ-\nically, all that remains after a shared task are the\npapers and datasets published. Given that shared\ntasks often establish a benchmark for the task in\nquestion, acting normative for future evaluations,\nthis outcome is far from optimal and comparably\nwasteful. All of the above can be signi\ufb01cantly im-\nproved upon by asking participants not to submit\ntheir software\u2019s run output, but the software itself.\nHowever, this entails a signi\ufb01cant work overhead\nfor organizers, especially for larger tasks.\nIn order to mitigate the work overhead, we em-\nploy TIRA. In a nutshell, TIRA implements eval-\nuation as a service in the form of a cloud-based\nevaluation platform. Participants deploy their soft-\nware into virtual machines hosted at TIRA\u2019s cloud,\nand then remotely control the machines and the\nsoftware within, executing it on the test data. The\ntest data are available only within the cloud, and\nmade accessible on demand so that participants can-\nnot access it directly. At execution time, the virtual\nmachine is disconnected from the internet, copied,\nand only the copy gets access to the test data. Once\nthe automatically executed software terminates, its\nrun output is saved and the virtual machine copy\nis destroyed so as to prevent data leaks. This way,\nall submitted pieces of software can be archived in\nworking condition, and be re-evaluated at a later\ntime, even on new datasets.\n8325 Participating Systems\nThis task attracted a very diverse and interesting\nset of solutions from the participating teams. The\nteams employed very different sets of features,\na wide variety of classi\ufb01ers, and also employed\nthe large by-publisher dataset in different ways.\nAround half of the submissions used hand-crafted\nfeatures. In the following, we give an overview\nof the submitted approaches. For a more readable\nand condensed form, we only use the team names\nhere, which were chosen from \ufb01ctional journalistic\ncharacters or entities (see Table 1 for references).\n5.1 Features\nThe teams that participated in this task employed\na variety of features, including standard word n-\ngrams (also unigrams, i.e., bag-of-words), word\nembeddings, stylometric features, HTML features\nlike the target of hyperlinks, and a meta data feature\nin the form of the publication date.\nN-Grams Most teams that used hand-crafted fea-\ntures also included word n-grams: Pioquinto Man-\nterola and Tintin used them as their only features.\nCharacter and part-of-speech n-grams were, for\nexample, used by Paparazzo.\nWord embeddings Many teams integrated word\nembeddings into their approach. Frequently used\nwere Word2Vec, fastText, and GloVe. Noticeably,\nTom Jumbo Grumbo relied exclusively on them.\nBertha von Suttner relied on ELMo embeddings\n(Peters et al., 2018), which have the advantage of\nmodeling polysemy. Where the aforementioned\nword embeddings all rely on neural networks, Doris\nMartin employed a document representation based\non word clusters as part of their approach.\nBERT (Devlin et al., 2018), which jointly con-\nditions on both left and right context in all layers,\nis a rather new technique that was used by several\nteams. Peter Parker directly applied a freely avail-\nable pre-trained BERT model to the task, whereas\nHoward Beale and Clint Buchanan trained their\nown BERT models on the by-publisher dataset and\nthen performed \ufb01ne-tuning on the by-article dataset.\nDespite the \ufb01ne-tuning, Howard Beale reported\nover\ufb01tting issues for this strategy. Going one step\nfurther, Jack Ryder and Yeon Zi integrated BERT\nin their neural network architectures.\nStylometry Many teams used stylometric fea-\ntures including punctuation and article structure(Steve Martin, Spider Jerusalem, Fernando Pessa,\nNed Leeds, Carl Kolchak, Orwellian Times), read-\nability scores (Ned Leeds, Pistachon, Steve Martin,\nOrwellian Times, D X Beaumont), or psycholin-\nguistic lexicons (Ned Leeds, Spider Jerusalem,\nSteve Martin, Pistachon). Borat Sagdiyev em-\nployed a self-compiled list of trigger words that\ncontains mostly profanities. They noticed that such\nwords are used more often in hyperpartisan articles.\nEmotionality Several teams used sentiment and\nemotion features, either based on libraries (Borat\nSagdiyev, Steve Martin, Carl Kolchak) or lexicons\n(Spider Jerusalem, D X Beaumont). Notably, Ker-\nmit the Frog uses sentiment detection only. Vernon\nFenwick and D X Beaumont used subjectivity and\npolarity metrics as features.\nNamed entities Borat Sagdiyev used named en-\ntity types as features. In preliminary tests only\nthe type of \u201cnationalities or religious and political\ngroups\u201d was found to be predictive.\nQuotations A few teams treated quotations sep-\narately. Whereas Spider Jerusalem and Borat\nSagdiyev created separate features from quotations,\nthe Ankh Morpork Times \ufb01ltered them out for not\nnecessarily representing the views of the author.\nHyperlinks Only few teams considered hyper-\nlinks. Both Borat Sagdiyev and Steve Martin used\nexternal lists of partisan web pages to count how\noften an article links to partisan and non-partisan\npages. They assume that articles tend to link other\narticles on the same side of the political spectrum.\nPublication date Based on the conjecture that\nmonths around American elections could see more\nhyperpartisan activity, Borat Sagdiyev used the pub-\nlication month and year as separate features.\n5.2 Classi\ufb01ers\nWhile many different classi\ufb01ers were used overall,\nneural networks were the most frequent, which\nmirrors the current trend in text classi\ufb01cation.\nThe most popular type of neural networks among\nthe participants were convolutional ones (CNNs),\nwhich employ convolving \ufb01lters over neighboring\nwords. Many teams cited the architecture by Kim\n(2014). Xenophilius Lovegood added a second\nlayer to their CNN in order to encode more in-\nformation about the articles, using both available\nand custom-learned embeddings. While Pioquinto\nManterola experimented with a CNN, it suffered\n833from over\ufb01tting and was thus not used for the \ufb01nal\nsubmission. Peter Brinkmann built a submission\nusing available embeddings. Brenda Starr com-\nbined a CNN with a sentence-level bidirectional\nrecurrent neural network and an attention mecha-\nnism to a complex architecture. A similar approach\nwas employed by the Ankh Morpork Times. An\nensemble of three CNN-based models was used by\nBertha von Suttner. Steve Martin used a character\nbigram CNN as part of their approach.\nNext to CNNs, long short term memory net-\nworks (LSTM) were employed by Kit Kittredge\nand Miles Clarkson. The latter extended the net-\nwork with an attention model. Moreover, Joseph\nRouletabille used the hierarchical attention network\nof Yang et al. (2016).\nBesides neural networks, a wide variety of classi-\n\ufb01ers were used. A few teams opted for SVMs (e.g.,\nthe Orwellian Times), others for random forests\n(e.g., Fernando Pessa), linear models (e.g., Pista-\nchon), the Naive Bayes model (e.g., Carl Kolchak),\nXGBOOST (Clark Kent), Maxent (Doris Martin),\nand rule-based models (Harry Friberg). Morbo\nused ULMFit (Howard and Ruder, 2018) to adapt\na language model pre-trained on Wikipedia articles\nto the articles and classes of this task.\n5.3 Usage of the By-publisher Dataset\nThe submitted systems can also be distinguished by\nwhether and how they used the large, distantly-su-\npervised by-publisher dataset. Though much larger\nthan the by-article set, its labels are noisy, whereas\nthe opposite holds for the by-article dataset. One of\nthe key challenges faced by many teams was how\nto train a powerful expressive model on the smaller\ndataset without over\ufb01tting. Most teams made use\nof the larger dataset in some form or another. A\nchallenge faced by some of the teams was that the\ntest split of the by-article dataset was balanced be-\ntween classes, whereas the corresponding training\ndataset was not.\nSeveral systems trained the whole or part of their\nsystem on the by-publisher dataset. Some extracted\nfeatures like n-grams (e.g., Sally Smedley), word\nclusters (Doris Martin), or neural network word\nembeddings (e.g., Clint Buchanan). Others used\nthe larger dataset to perform hyperparameter search\n(e.g., Miles Clarkson). Many teams trained their\nmodels using the by-publisher dataset only (Pista-\nchon, Joseph Rouletabille, Xenophilius Lovegood,\nPeter Brinkmann, and Kit Kittredge).To reduce the noise in the distantly-supervised\ndata, some teams used only a subset of it. Yeon Zi,\nBorat Sagdiyev and the Anhk Morpork Times \ufb01tted\na model on the by-article dataset and ran it on the\nby-publisher one: the articles of the by-publisher\ndataset that were misclassi\ufb01ed by this model, were\npresumed to be noisy and \ufb01ltered out.\n6 Results\nA total of 42 teams completed the task, representing\nmore than twenty countries between them, includ-\ning India, China, the USA, Japan, Vietnam, and\nmany European countries. Table 1 shows the accu-\nracy, precision, recall, and F 1score for each team,\nsorted by accuracy. This task used accuracy as the\nmain metric to represent a \ufb01ltering scenario. The\naccuracy scores ranged from 0.462 up to 0.822.\nThe results show a range of trade-offs be-\ntween precision and recall and the resulting F 1\nscores. The highest F 1was 0.821 with a precision\nof 0.815 and a recall of 0.828; the highest precision\nwas 0.883 with a recall of 0.672 (F 1: 0.763); and\nthe highest recall was 0.971 with a relatively low\nprecision of 0.542 (F 1: 0.696).\n6.1 Methods Used by the Top Teams\nWhile the winning team, Bertha von Suttner, used\ndeep learning (sentence-level embeddings and a\nconvolutional neural network) the second-placed\nteam, Vernon Fenwick, took a different approach\nand combined sentence embeddings with more\ndomain-speci\ufb01c features and a linear model. Out\nof the top \ufb01ve teams, only two used \u201cpure\u201d deep\nlearning models of neural networks without any\ndomain-speci\ufb01c, hand-crafted features, showing no\nsingle method has a clear advantage over others.\nBertha von Suttner used a model based on ELMo\nembeddings (Peters et al., 2018) and trained on\nthe by-article dataset. After minimal preprocess-\ning, a pre-trained ELMo was applied onto each\ntoken of each sentence, and then averaged, to\nobtain average sentence embeddings. The sen-\ntence embeddings were later passed through a\nCNN, batch-normalized, followed by a dense layer\nand a sigmoid function to obtain the \ufb01nal prob-\nabilities. The \ufb01nal model was an ensemble of\nthe 3 best-performing models of a 10-fold cross-\nvalidation. The authors tried to include the by-\npublisher dataset, but found in their preliminary\ntests no approach to pro\ufb01t from the large data.\n834Submission By-article dataset By-publisher dataset\nTeam name Authors Code Rank Acc. Prec. Recall F 1 Rank Acc. Prec. Recall F 1\nBertha von Suttner Jiang et al. \u1f517 10.822 0.871 0.755 0.809 8 0.643 0.616 0.762 0.681\nVernon Fenwick Srivastava et al. 2 0.820 0.815 0.828 0.821\nSally Smedley Hanawa et al. 3 0.809 0.823 0.787 0.805 11 0.625 0.640 0.571 0.603\nTom Jumbo Grumbo Yeh et al. \u1f517 4 0.806 0.858 0.732 0.790 13 0.619 0.592 0.762 0.667\nDick Preston Isbister and Johansson 5 0.803 0.793 0.818 0.806 27 0.514 0.520 0.352 0.420\nBorat Sagdiyev Pali \u00b4c et al. 6 0.791 0.883 0.672 0.763 19 0.592 0.644 0.412 0.502\nMorbo Isbister and Johansson 7 0.790 0.772 0.822 0.796 16 0.601 0.587 0.679 0.630\nHoward Beale Mutlu et al. 8 0.783 0.837 0.704 0.765 9 0.641 0.606 0.806 0.692\nNed Leeds Stevanoski and Gievska 9 0.775 0.865 0.653 0.744 22 0.573 0.546 0.857 0.667\nClint Buchanan Drissi et al. \u1f517 10 0.771 0.832 0.678 0.747\nYeon Zi Lee et al. 11 0.758 0.744 0.787 0.765 5 0.663 0.635 0.766 0.694\nTony Vincenzo Staykovski 12 0.750 0.764 0.723 0.743\nPaparazzo Nguyen et al. \u1f517 13 0.747 0.754 0.732 0.743 24 0.530 0.530 0.541 0.535\nSteve Martin Joo and Hwang 14 0.745 0.853 0.592 0.699 18 0.597 0.625 0.483 0.545\nEddie Brock \u02d8Sajatovi \u00b4c et al. 15 0.744 0.782 0.675 0.725 10 0.631 0.681 0.491 0.571\nAnkh Morpork Times Almendros et al. 16 0.742 0.811 0.631 0.710 21 0.588 0.646 0.389 0.486\nSpider Jerusalem Alabdulkarim and Alhindi \u1f517 17 0.742 0.814 0.627 0.709\nCarl Kolchak Chen et al. 18 0.739 0.729 0.761 0.745\nDoris Martin Agerri \u1f517 19 0.737 0.754 0.704 0.728\nPistachon Saleh et al. 20 0.729 0.724 0.742 0.733 15 0.608 0.638 0.499 0.560\nJoseph Rouletabille Moreno et al. 21 0.725 0.788 0.615 0.691 2 0.680 0.640 0.827 0.721\nFernando Pessa Cruz et al. \u1f517 22 0.717 0.806 0.570 0.668 17 0.600 0.585 0.681 0.630\nPioquinto Manterola Sengupta and Pedersen \u1f517 23 0.704 0.741 0.627 0.679\nMiles Clarkson Zhang et al. 24 0.683 0.745 0.557 0.638 6 0.652 0.612 0.832 0.705\nXenophilius Lovegood Zehe et al. 25 0.675 0.619 0.914 0.738 4 0.663 0.632 0.781 0.699\nOrwellian Times Knauth 26 0.672 0.654 0.729 0.690 23 0.537 0.530 0.658 0.587\nTintin Bestgen 27 0.656 0.642 0.707 0.673 1 0.706 0.742 0.632 0.683\nD X Beaumont Amason et al. 28 0.653 0.597 0.939 0.730\nJack Ryder Shaprin et al. 29 0.646 0.646 0.646 0.646 7 0.645 0.600 0.869 0.710\nKermit the Frog Anthonio and Kloppenburg 30 0.621 0.582 0.860 0.694 20 0.589 0.575 0.681 0.623\nBilly Batson Kreutz et al. 31 0.615 0.568 0.962 0.714\nPeter Brinkmann F\u00e4rber et al. \u1f517 32 0.602 0.560 0.955 0.706 28 0.497 0.496 0.344 0.406\nAnson Bryson Stiff and Medero 33 0.592 0.720 0.303 0.426\nSarah Jane Smith Chakravartula et al. 34 0.591 0.554 0.933 0.695 14 0.612 0.586 0.765 0.664\nKit Kittredge Cramerus and Schef\ufb02er 35 0.578 0.547 0.908 0.683\nBrenda Starr Papadopoulou et al. 36 0.575 0.542 0.971 0.696 3 0.664 0.627 0.807 0.706\nHarry Friberg Afsarmanesh et al. 37 0.565 0.537 0.949 0.686\nRobin Scherbatsky Marx and Akut 38 0.551 0.542 0.662 0.596 25 0.524 0.822 0.062 0.116\nClark Kent Gupta et al. \u1f517 39 0.548 0.683 0.178 0.283 26 0.519 0.565 0.170 0.261\nMurphy Brown Sen and Jiang 40 0.529 0.518 0.822 0.635 12 0.623 0.615 0.659 0.636\nPeter Parker Ning et al. 41 0.503 0.502 0.771 0.608\nJohn King Bansal et al. 42 0.462 0.460 0.443 0.451\nTable 1: For each team and dataset, the performance of the submission that reached the highest accuracy is shown.\nIf a team published their code, the \u1f517links to the respective repository. We forked all repositories for archival.6\nThe second and third best teams used linear mod-\nels as their main predictor and embeddings as fea-\ntures, training on the by-article dataset only. Ver-\nnon Fenwick extracted sentence embeddings with\nthe Universal Sentence Encoder (USE) (Cer et al.,\n2018), while Sally Smedley used BERT to gener-\nate contextual embeddings. Both teams also em-\nployed hand-crafted, domain-speci\ufb01c features. Ver-\nnon Fenwick extracted article-level and sentence-\nlevel polarity, bias, and subjectivity, among others,\nwhile Sally Smedley used the by-publisher dataset\nto extract key discriminative phrases, which they\nlater looked up in the training data.\n6https://github.com/hyperpartisan-news-challenge6.2 Overall Insights\nThe results reveal several insights into the suitabil-\nity of different features and approaches for the task\nof hyperpartisan news detection.\nWord-embeddings have been reported to be\na very ef\ufb01cient feature by many teams. Tom\nJumbo Grumbo achieved an accuracy of 0.806\nwith GloVe embeddings and a classi\ufb01er trained\non the by-article dataset. The application of a pre-\ntrained BERT model by Peter Parker performed\nvery poorly (acc. 0.503). However, the same BERT\nembeddings were used for great effect by Sally\nSmedley, using techniques like word-dropout and\ninformative phrase identi\ufb01cation (acc. 0.809).\n835Also standard word n-grams were found to be\nsuitable for the task, though not as strong as em-\nbeddings. While n-grams where used in several\nwell-performing approaches, Pioquinto Manterola\nreached an accuracy of 0.704 with unigrams alone.\nSeveral teams reported an increase in accuracy\nthrough sentiment or similar features (e.g., Borat\nSagdiyev). Kermit the Frog used sentiment detec-\ntion alone to reach an accuracy of 0.621.\nBesides textual features, a few teams also an-\nalyzed HTML and article meta-features. Borat\nSagdiyev performed a detailed analysis in this re-\ngard, which helped them to achieve the highest\nprecision of all teams. For example, they found\nthat both the publication date and the number of\nlinks to known hyperpartisan pages could each im-\nprove the overall accuracy by about 0.01 to 0.02.\nOf the top teams, only Sally Smedley used the\nby-publisher dataset, and only to select n-grams.\nBased on the reports of several teams, the utiliza-\ntion of this dataset thus seems more dif\ufb01cult than\nwe expected. We conjecture that this is due to the\nmis-classi\ufb01cation of what should be the most in-\nformative articles: non-hyperpartisan articles from\nmainly hyperpartisan publishers, and hyperpartisan\narticles from non-hyperpartisan publishers. These\narticles are especially suited to distinguish features\nthat identify hyperpartisanship from features that\nidentify publisher style. While we assumed that\nthe advantages of big data would outweigh this\ndrawback, the results suggest that it might be more\nworthwhile to put effort in larger datasets where\neach article is annotated separately. Still, some\nteams managed to use the by-publisher dataset as a\nlarge dataset of in-domain texts. For example, Clint\nBuchanan reported that pre-training embeddings\non the by-publisher dataset increased the accuracy\nof their system on the by-article dataset.\nMoreover, the ranking of teams for the two test\ndatasets is quite different. Bertha von Suttner, who\nranked \ufb01rst for by-article, reached only rank eight\nfor the by-publisher dataset. Conversely, Tintin,\nwho optimized for by-publisher, ranked \ufb01rst there\nbut only 27th for the by-article dataset. This dis-\ncrepancy highlights the unexpected large differ-\nences between the datasets.\n7 Meta-Classi\ufb01cation Task\nInspired by successes of meta classi\ufb01ers in past\nSemEval tasks (e.g., Hagen et al. (2015)), we en-\nabled and encouraged participants to devise meta\nVernon Fenwick\nBertha von Suttner Borat Sagdiyev\nyes no yes noyes no\nHoward Beale\nyes no\nNed Leeds\nyes no13 193 2 160 17 26\n22 5\n10 22 36Figure 2: Meta-classi\ufb01cation decision tree J48-M10\nlearned on the predictions of the submitted systems (hy-\nperpartisan: yes or no; by-article dataset). The numbers\nshow the training class-distribution at the leafs.\nclassi\ufb01ers that learn from the classi\ufb01cations of the\nsubmitted approaches. For this meta-classi\ufb01cation\ntask, we split the test datasets further into new train-\ning (66%) and test sets (33%). We again made sure\nthat there are an equal amount of non-hyperpartisan\nand hyperpartisan articles, as well as an equal share\nof left-wing and right-wing articles within the hy-\nperpartisan sets. Furthermore, we again assured\nthat no publisher had articles in both the training\nand the test sets. An instance in these datasets\ncorresponds to the classi\ufb01cations (hyperpartisan or\nnot) of the best-performing software of each team\n(42 classi\ufb01cations for the by-article dataset and\n30 for the by-publisher one) of one article from the\noriginal test data.\nWe provide two simple classi\ufb01cation systems for\nbaselines, majority voting and an out-of-the-box de-\ncision tree, which both outperform the best single\nsubmitted software and which were both outper-\nformed by the meta-classi\ufb01ers submitted. Majority\nvoting refers to a system that outputs the classi-\n\ufb01cation (hyperpartisan or not) that the most base\nclassi\ufb01ers selected. As it does not learn a deci-\nsion boundary, it is\u2014strictly speaking\u2014not a meta\nclassi\ufb01er. For the decision tree, we used the J48\nimplementation of WEKA (Frank et al., 2016). We\ntested two variants: standard settings ( J48-M2 ) and\nrestricting leaf nodes to contain at least 10 articles\n(J48-M10 ) to force a simpler decision tree. Simpler\ntrees often generalize better to unseen data.\nFigure 2 shows the J48-M10 tree for the by-\narticle dataset. For every leaf of the tree, more\nthan 75% of the corresponding training articles\nare from the same class. This shows that even\nwith as few as 5 decision nodes, the training set\n836Team or system name Acc. Prec. Recall F 1\nFernando Pessa 0.899 0.895 0.904 0.900\nSpider Jerusalem 0.899 0.903 0.894 0.899\nMajority V ote 0.885 0.892 0.875 0.883\nJ48-M10 0.880 0.916 0.837 0.874\nJ48-M2 0.856 0.863 0.846 0.854\nBertha von Suttner alone 0.851 0.901 0.788 0.841\nTable 2: Accuracy, precision, recall, and F 1-measure\nfor the by-article meta learning test dataset.\ncould be \ufb01tted reasonably well. The meta clas-\nsi\ufb01er was thus able to use the submitted systems\nas predictive and distinct features, which shows\nthat some submitted systems performed well on\nsome articles where other systems did not and vice\nversa. Even more, the 5 systems employed by the\nmeta-classi\ufb01er are all within the top 10 systems\nof the task, which shows that there is considerable\nvariation even among the top performers. This is\nreasonable, given the variety of approaches used.\nIn addition to our approaches, two teams submit-\nted their own classi\ufb01ers in the short time span they\nhad. Fernando Pessa used a random forest classi-\n\ufb01er trained on the single predictions as well as the\naverage vote. Spider Jerusalem used a weighted\nmajority voting algorithm, where they weighted\neach single prediction by the precision of the re-\nspective classi\ufb01er on the training set.\nTable 2 shows the performance of the approaches\non the meta learning test dataset. Note that the best\nsingle system, Bertha von Suttner, reaches an in-\ncreased accuracy of 0.851 on the meta learning test\nset. This is due to variations in the small dataset.\nStill, all ensemble approaches reach a higher ac-\ncuracy. The majority voting approach reaches an\naccuracy of 0.885, and thus outperforms the J48\nclassi\ufb01ers. This is somewhat surprising, but shows\nthat there is a lot to gain by integrating also the\nsystems that performed less well\u2014team Fernando\nPessa came to a similar insight in their paper (Cruz\net al., 2019). The approaches of the two partici-\npants performed very similar, despite their method-\nological differences, and outperformed the majority\nvote. They managed to achieve an accuracy 0.048\npoints above Bertha von Suttner and therefore a\nconsiderable increase in performance.\nWe also repeated the experiments for the by-\npublisher dataset, but could not produce decisive\nresults there, yet. We assume that this is due to\nmost teams focusing on the other dataset and both\ndatasets being more different than expected.8 Conclusion\nThis paper reports on the setup, participation, re-\nsults, and insights gained from the \ufb01rst task in hy-\nperpartisan news detection, hosted as Task 4 at\nSemEval-2019. We detailed the construction of\nboth a manually annotated dataset of 1,273 arti-\ncles as well as a large dataset of 754,000 articles,\ncompiled using distant supervision. Moreover, it\nprovides a systematic overview of the 34 papers\nsubmitted by the participants, insights gathered\nfrom single teams, by comparing their approaches,\nand by an ad-hoc meta classi\ufb01cation.\nThrough the use of TIRA (Potthast et al., 2019),\nwe were able to establish a blind evaluation setup,\nso that future approaches can be compared on same\ngrounds. For this, we continue to accept new\napproaches in ongoing submissions.7Moreover,\nthrough the use of TIRA we can directly evalu-\nate the submitted approaches on new datasets for\nhyperpartisan news detection, provided they are\nformatted like the datasets presented here.\nVery promising results were achieved during the\ntask, with accuracy values above 80% on a bal-\nanced test set\u2014and even up to 90% using meta\nclassi\ufb01cation on all submissions. Like in many\nother NLP tasks, word embeddings could be used\nto great effect, but hand-crafted features also per-\nformed well. The differences between the two em-\nployed datasets were larger than anticipated, which\nsuggests a focus on by-article annotations in the\nfuture. A larger dataset of this kind will probably\nassist in improving the accuracy of future models\neven beyond the already very good level.\nIt thus seems that hyperpartisan news detection is\nalready suf\ufb01ciently developed to take the next step\nand demand human-understandable explanations\nfrom the approaches. The most obvious use cases\nof hyperpartisan news detectors are for \ufb01ltering ar-\nticles, which always requires a careful handling to\navoid unwarranted censorship. Especially in the\ncurrent political climate, it therefore seems neces-\nsary that hyperpartisanship detectors not only reach\na high accuracy, but also reveal their reasoning.\nAcknowledgements\nOur thanks go out to all participating teams; your\ncontributions made this task a success. We hope\nwe have been able to do your work justice, and\nare looking forward to doing so in the future. Our\nspecial thanks go out to the SemEval organizers for\nproviding perfect organizational support.\n7https://webis.de/events/semeval-19/\n837References\nNazanin Afsarmanesh, Jussi Karlgren, Peter Sumbler,\nand Nina Viereckel. 2019. Team Harry Friberg at\nSemEval-2019 Task 4: Identifying Hyperpartisan\nNews through Editorially De\ufb01ned Metatopics. In\nProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.\nRodrigo Agerri. 2019. Doris Martin at SemEval-2019\nTask 4: Hyperpartisan News Detection with Generic\nSemi-supervised Featuresl. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nAmal Alabdulkarim and Tariq Alhindi. 2019.\nSpider-Jerusalem at SemEval-2019 Task 4:\nHyperpartisan News Detection. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nCarla Perez Almendros, Luis Espinosa Anke, and\nSteven Schockaert. 2019. Cardiff University at\nSemEval-2019 Task 4: Linguistic Features for\nHyperpartisan News Detection. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nEvan Amason, Jake Palanker, Mary Clare Shen, and\nJulie Medero. 2019. Harvey Mudd College at\nSemEval-2019 Task 4: The D.X. Beaumont\nHyperpartisan News Detector. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nTalita Anthonio and Lennart Kloppenburg. 2019.\nTeam Kermit-the-frog at SemEval-2019 Task 4: Bias\nDetection Through Sentiment Analysis and Simple\nLinguistic Features. In Proceedings of The 13th\nInternational Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nYves Bestgen. 2019. Tintin at SemEval-2019 Task 4:\nDetecting Hyperpartisan News Article with only\nSimple Tokens. In Proceedings of The 13th\nInternational Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nShweta Bhatt, Sagar Joglekar, Shehar Bano, and\nNishanth Sastry. 2018. Illuminating the ecosystem of\npartisan websites. In Proceedings of the 27th\nInternational Conference on World Wide Web\nCompanion , WWW \u201918 Companion. International\nWorld Wide Web Conferences Steering Committee.\nDaniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\n2018. Universal Sentence Encoder.\nNikhil Chakravartula, Vijayasaradhi Indurthi, and\nBakhtiyar Syed. 2019. Fermi at SemEval-2019 Task 4:\nThe sarah-jane-smith Hyperpartisan News Detector.\nInProceedings of The 13th International Workshop onSemantic Evaluation . Association for Computational\nLinguistics.\nCelena Chen, Celine Park, Jason Dwyer, and Julie\nMedero. 2019. Harvey Mudd College at\nSemEval-2019 Task 4: The Carl Kolchak\nHyperpartisan News Detector. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nRebekah Cramerus and Tatjana Schef\ufb02er. 2019. Team\nKit Kittredge at SemEval-2019 Task 4. In Proceedings\nof The 13th International Workshop on Semantic\nEvaluation . Association for Computational\nLinguistics.\nAndr\u00e9 Cruz, Gil Rocha, Rui Sousa-Silva, and\nHenrique Lopes Cardoso. 2019. Team Fernando-Pessa\nat SemEval-2019 Task 4: Back to Basics in\nHyperpartisan News Detection. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of Deep\nBidirectional Transformers for Language\nUnderstanding. CoRR , abs/1810.04805.\nMehdi Drissi, Pedro Sandoval Segura, Vivaswat Ojha,\nand Julie Medero. 2019. Harvey Mudd College at\nSemEval-2019 Task 4: The Clint Buchanan\nHyperpartisan News Detector. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nMichael F\u00e4rber, Agon Qurdina, and Lule Ahmedi.\n2019. Team Peter Brinkmann at SemEval-2019 Task 4:\nDetecting Biased News Articles Using Convolutional\nNeural Networks. In Proceedings of The 13th\nInternational Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nEibe Frank, Mark A. Hall, and Ian H. Witten. 2016.\nData Mining: Practical Machine Learning Tools and\nTechniques , 4th edition, chapter The WEKA\nWorkbench. Morgan Kaufmann Publishers Inc., San\nFrancisco, CA, USA.\nViresh Gupta, Baani Leen Kaur Jolly, Ramneek Kaur,\nand Tanmoy Chakraborty. 2019. Clark Kent at\nSemEval-2019 Task 4: Stylometric Insights into\nHyperpartisan News Detection. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nMatthias Hagen, Martin Potthast, Michel B\u00fcchner, and\nBenno Stein. 2015. Webis: An Ensemble for Twitter\nSentiment Detection. In 9th International Workshop\non Semantic Evaluation (SemEval 2015) , pages\n582\u2013589. Association for Computational Linguistics.\nKazuaki Hanawa, Shota Sasaki, Hiroki Ouchi, Jun\nSuzuki, and Kentaro Inui. 2019. The Sally Smedley\nHyperpartisan News Detector at SemEval-2019 Task 4.\nInProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.\n838Jeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classi\ufb01cation.\nCoRR , abs/1801.06146.\nTim Isbister and Fredrik Johansson. 2019.\nDick-Preston and Morbo at SemEval-2019 Task 4:\nTransfer Learning for Hyperpartisan News Detection.\nInProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.\nYe Jiang, Johann Petrak, Xingyi Song, Kalina\nBontcheva, and Diana Maynard. 2019. Team Bertha\nvon Suttner at SemEval-2019 Task 4: Hyperpartisan\nNews Detection using ELMo Sentence Representation\nConvolutional Network. In Proceedings of The 13th\nInternational Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nYoungjun Joo and Inchon Hwang. 2019. Steve Martin\nat SemEval-2019 Task 4: Ensemble Learning Model\nfor Detecting Hyperpartisan News. In Proceedings of\nThe 13th International Workshop on Semantic\nEvaluation . Association for Computational\nLinguistics.\nJohannes Kiesel, Florian Kneist, Milad Alshomary,\nBenno Stein, Matthias Hagen, and Martin Potthast.\n2018. Reproducible Web Corpora: Interactive\nArchiving with Automatic Quality Assessment.\nJournal of Data and Information Quality (JDIQ) ,\n10(4):17:1\u201317:25.\nYoon Kim. 2014. Convolutional Neural Networks for\nSentence Classi\ufb01cation. Proceedings of the 2014\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP) .\nJ\u00fcrgen Knauth. 2019. Orwellian-times at\nSemEval-2019 Task 4: A Stylistic and Content-based\nClassi\ufb01er. In Proceedings of The 13th International\nWorkshop on Semantic Evaluation . Association for\nComputational Linguistics.\nNayeon Lee, Zihan Liu, and Pascale Fung. 2019.\nTeam yeon-zi at SemEval-2019 Task 4: Hyperpartisan\nNews Detection by De-noising Weakly-labeled Data.\nInProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.\nJose G. Moreno, Yoann Pitarch, Karen\nPinel-Sauvagnat, and Gilles Hubert. 2019.\nRouletabille at SemEval-2019 Task 4: Neural Network\nBaseline for Identi\ufb01cation of Hyperpartisan Publishers.\nInProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.\nOsman Mutlu, Ozan Arkan Can, and Erenay Dayanik.\n2019. Team Howard Beale at SemEval-2019 Task 4:\nHyperpartisan News Detection with BERT. In\nProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.Duc-Vu Nguyen, Thin Dang, and Ngan Nguyen. 2019.\nNLP@UIT at SemEval-2019 Task 4: The Paparazzo\nHyperpartisan News Detector. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nZhiyuan Ning, Yuanzhen Lin, and Ruichao Zhong.\n2019. Team Peter-Parker at SemEval-2019 Task 4:\nBERT-Based Method in Hyperpartisan News\nDetection. In Proceedings of The 13th International\nWorkshop on Semantic Evaluation . Association for\nComputational Linguistics.\nNiko Pali \u00b4c, Juraj Vladika, Dominik Cubeli \u00b4c, Ivan\nLovrencic, and Jan Snajder. 2019. TakeLab at\nSemEval-2019 Task 4: Hyperpartisan News Detection.\nInProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.\nOlga Papadopoulou, Giorgos Kordopatis-Zilos,\nMarkos Zampoglou, Symeon Papadopoulos, and\nYiannis Kompatsiaris. 2019. Brenda Starr at\nSemEval-2019 Task 4: Hyperpartisan News Detection.\nInProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word\nRepresentations. Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) .\nMartin Potthast, Tim Gollub, Matthias Hagen, and\nBenno Stein. 2018a. The Clickbait Challenge 2017:\nTowards a Regression Model for Clickbait Strength.\nCoRR , abs/1812.10847.\nMartin Potthast, Tim Gollub, Kristof Komlossy,\nSebastian Schuster, Matti Wiegmann, Erika Patricia\nGarces Fernandez, Matthias Hagen, and Benno Stein.\n2018b. Crowdsourcing a Large Corpus of Clickbait on\nTwitter. In 27th International Conference on\nComputational Linguistics (COLING 2018) , pages\n1498\u20131507. The COLING 2018 Organizing\nCommittee.\nMartin Potthast, Tim Gollub, Matti Wiegmann, and\nBenno Stein. 2019. TIRA Integrated Research\nArchitecture. In Nicola Ferro and Carol Peters, editors,\nInformation Retrieval Evaluation in a Changing World\n- Lessons Learned from 20 Years of CLEF . Springer.\nMartin Potthast, Johannes Kiesel, Kevin Reinartz,\nJanek Bevendorff, and Benno Stein. 2018c. A\nStylometric Inquiry into Hyperpartisan and Fake\nNews. In 56th Annual Meeting of the Association for\nComputational Linguistics (ACL 2018) , pages\n231\u2013240. Association for Computational Linguistics.\nMartin Potthast, Sebastian K\u00f6psel, Benno Stein, and\nMatthias Hagen. 2016. Clickbait Detection. In\n839Advances in Information Retrieval. 38th European\nConference on IR Research (ECIR 2016) , volume\n9626 of Lecture Notes in Computer Science , pages\n810\u2013817, Berlin Heidelberg New York. Springer.\nAbdelrhman Saleh, Ramy Baly, Alberto\nBarr\u00f3n-Cede\u00f1o, Giovanni Da San Martino, Mitra\nMohtarami, Preslav Nakov, and James Glass. 2019.\nTeam QCRI-MIT at SemEval-2019 Task 4:\nPropaganda Analysis Meets Hyperpartisan News\nDetection. In Proceedings of The 13th International\nWorkshop on Semantic Evaluation . Association for\nComputational Linguistics.\nSaptarshi Sengupta and Ted Pedersen. 2019. Duluth at\nSemEval-2019 Task 4: The Pioquinto Manterola\nHyperpartisan News Detector. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nDaniel Shaprin, Giovanni Da San Martino, Alberto\nBarr\u00f3n-Cede\u00f1o, and Preslav Nakov. 2019. Team Jack\nRyder at SemEval-2019 Task 4: Using BERT\nRepresentations for Detecting Hyperpartisan News. In\nProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.\nVertika Srivastava, Ankita Gupta, Divya Prakash,\nSudeep Sahoo, Rohit R. R., and Yeon Hyang Kim.\n2019. Vernon-fenwick at SemEval-2019 Task 4:\nHyperpartisan News Detection using Lexical and\nSemantic Features. In Proceedings of The 13th\nInternational Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nBozhidar Stevanoski and Sonja Gievska. 2019. Team\nNed Leeds at SemEval-2019 Task 4: Exploring\nLanguage Indicators of Hyperpartisan Reporting. In\nProceedings of The 13th International Workshop on\nSemantic Evaluation . Association for Computational\nLinguistics.\nEmmanuel Vincent and Maria Mestre. 2018.\nCrowdsourced Measure of News Articles Bias:\nAssessing Contributors\u2019 Reliability. In Proceedings of\nthe 1st Workshop on Subjectivity, Ambiguity and\nDisagreement (SAD) in Crowdsourcing , pages 1\u201310.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlexander J. Smola, and Eduard H. Hovy. 2016.\nHierarchical Attention Networks for Document\nClassi\ufb01cation. In NAACL HLT 2016, The 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 1480\u20131489.\nChia-Lun Yeh, Babak Loni, and Anne Schuth. 2019.\nTom Jumbo-Grumbo at SemEval-2019 Task 4:\nHyperpartisan News Detection with GloVe vectors and\nSVM. In Proceedings of The 13th International\nWorkshop on Semantic Evaluation . Association for\nComputational Linguistics.Albin Zehe, Lena Hettinger, Stefan Ernst, Christian\nHauptmann, and Andreas Hotho. 2019. Team\nXenophilius Lovegood at SemEval-2019 Task 4:\nHyperpartisanship Classi\ufb01cation using Convolutional\nNeural Networks. In Proceedings of The 13th\nInternational Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.\nChiyu Zhang, Arun Rajendran, and Muhammad\nAbdul-Mageed. 2019. UBC-NLP at SemEval-2019\nTask 4: Hyperpartisan News Detection With\nAttention-Based Bi-LSTMs. In Proceedings of The\n13th International Workshop on Semantic Evaluation .\nAssociation for Computational Linguistics.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "SemEval-2019 task 4: Hyperpartisan news detection", "author": ["J Kiesel", "M Mestre", "R Shukla", "E Vincent"], "pub_year": "2019", "venue": "Proceedings of the \u2026", "abstract": "Hyperpartisan news is news that takes an extreme left-wing or right-wing standpoint. If one  is able to reliably compute this meta information, news articles may be automatically tagged,"}, "filled": false, "gsrank": 296, "pub_url": "https://aclanthology.org/S19-2145/", "author_id": ["u3zSmJQAAAAJ", "1dh-y6QAAAAJ", "NMJsRBEAAAAJ", ""], "url_scholarbib": "/scholar?hl=en&q=info:uGOdky3bOYMJ:scholar.google.com/&output=cite&scirp=295&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D290%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=uGOdky3bOYMJ&ei=OLWsaJm1KI6IieoP0sKRuAk&json=", "num_citations": 302, "citedby_url": "/scholar?cites=9455829881437643704&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:uGOdky3bOYMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aclanthology.org/S19-2145.pdf"}}, {"title": "The depiction of Orania in print media (2013-2022): a quantitative analysis using Natural Language Processing (NLP)", "year": "2023", "pdf_data": "1 PBTHE DEPICTION OF ORANIA IN \nPRINT MEDIA (2013-2022): \nA QUANTITATIVE ANALYSIS \nUSING NATURAL LANGUAGE \nPROCESSING (NLP)\nABSTRACT\nThe current article investigates the depiction of the town of \nOrania in print media. Being an exclusive Afrikaner town, \nthis town is controversial and is often seen as a remnant \nof apartheid, leading residents of the town to form the \nperception that the media treats them unfairly. Using Natural \nLanguage Processing (NLP) techniques, namely a lexicon-\nbased sentiment analysis classification and a machine-\nlearning political bias classification, it is shown that the \nmajority of news reports and opinion pieces on this town \nexhibit minimal political bias, and publications on this town \nare evenly distributed between left and right political bias. \nIn addition, while the majority of news reports and opinion \npieces published on this town are neutral, more publications \nare positive than negative. However, differences in the \ndepiction of this town based on the language of publications \nare also discussed, with English publications more negative \nand Afrikaans publications more positive, and the majority of \npublications on this town are in Afrikaans. Overall, the study \nfinds that while some individual publications present Orania \nin a negative light, in general, the media reports on this town \nin a balanced way.\nKeywords: media studies, media fairness, journalism, \nOrania, machine learning, political bias classification, \nsentiment analysis, South African media\nINTRODUCTION\nGiven South Africa\u2019s post-1994 attempt to integrate different \nethnicities, and government pressure to promote English-\nonly spaces (such as in education), Orania\u2019s emphasis \non preserving the traditional Afrikaner identity is inevitably \ncontroversial. Although other exclusively Afrikaner communities \nsuch as Kleinfontein have also come under criticism, it is \nOrania in particular that is often presented in the media as \nthe bastion of racism. Orania is seen from this point of view \nas a remnant of apartheid, where apartheid is synonymous Dr Burgert Senekal\nDepartment of Computer \nScience and Informatics, \nUniversity of the Free \nState, Bloemfontein, \nSouth Africa  \nEmail : burgertsenekal@\nyahoo.co.uk  \nORCID: https://orcid.\norg/0000-0002-1385-\n9258\nDOI: https://doi.\norg/10.38140/xxx\nISSN 2415-0525 (Online)\nCommunitas 2023 28: 1-19\nDate submitted:  \n27 April 2023\nDate accepted:  \n15 June 2023\nDate published: \nxxx\n\u00a9 Creative Commons With \nAttribution (CC-BY)\n\n3 2\nSenekalwith racism. Seldon (2014) notes that Orania has become \u201ca by-word for racism \nand the nostalgic recreation of a time when they [Afrikaners] held power over others\u201d, \nand Davis (2020) remarks, \u201c[Orania] has become a byword for unchecked racism; a \nmetonym for all unreconstructed white South Africans and problematic race relations\u201d. \nAs Orania is associated with racism, the town receives a great deal of attention \nfrom the media, and Roets (2019), then acting Chief Executive Officer of the Orania \nMovement, stated that as many as three international news teams visit Orania per \nweek. The Orania Movement mostly oversees media liaison, taking journalists on tours \nand arranging interviews. Some journalists, such as Davis (2020), attempt to portray \nwhat they experience as an unusual town in a nuanced way but, as she writes, there \nis a perception among many residents of Orania that journalists treat them unfairly. \nAccording to Roets (2019), the town has received negative publicity for the past \n30 years, while Delvecki and Greiner (2014) aver, \u201cthe town has endured negative \npress and misunderstandings from its beginning\u201d.\n  The present article investigates the nature and extent of print media coverage of \nOrania using Natural Language Processing (NLP) techniques, and more specifically, \nsentiment analysis and political bias classification. The purpose is to determine \nwhether this controversial community is treated fairly by the media, or if news reports \nshow bias and a tendency to frame this community in a negative light. Note that the \ncurrent article focuses on the depiction of Orania in the media and does not argue for \nor against this community.\nThe article is structured as follows. First, some key terms used in the current article \nare discussed. Thereafter, background is provided on current trends in terms of \npeople\u2019s trust in the media, as well as text analysis approaches that attempt to make \nnews reports more transparent. This is followed by a discussion of Orania\u2019s troubled \nhistory with the media. Then, data gathering and analysis are discussed, followed by \na presentation and discussion of the findings. The article concludes with summary \nremarks and suggestions for further research.\nA NOTE ON TERMINOLOGY\nSome terms used in the current study warrant explanation, and therefore the current \nsection briefly discusses the terms \u201cAfrikaner\u201d and \u201cpolitical leaning\u201d.\nAfrikaner\nIn the simplest definition, the term \u201cAfrikaner\u201d refers to Afrikaans-speaking people of \npredominantly European descent. Hollfelder et al. (2020) for example found that on \naverage 95.3% of the Afrikaner\u2019s ancestors are of European origin, with the remaining \n4.7% people brought to the Cape as slaves in colonial times (3.4%) and from local \nKhoe-San people (1.3%). This European genetic heritage means that the Afrikaner \nis generally considered to be white, and Steyn (1984) for instance argues that the \ndefinition of the Afrikaner offered by Abel Coetzee is the most accurate: \u201c\u2019n Blanke \nAfrikaanssprekende inwoner van Suid-Afrika\u201d [A white Afrikaans-speaking resident of \nSouth Africa].\n3 2\nThe depiction of Orania in print media (2013-2022)In addition to language and genetic heritage, traditional Afrikaners are often Christian \nand conservative. Finlayson (2019) found that the typical characteristics attributed to \nAfrikaners can be summarised as follows: a common genetic ancestry, adherence to \nProtestant Christianity, use of the Afrikaans language, and the adoption of a distinct \nset of social values often referred to as conservatism (see also Vestergaard (2001)). \nThese facets of Afrikaner identity are also found in the comprehensive definition \noffered by Orania\u2019s founder, Prof. Carel Boshoff (2012), from the constitution of the \nAfrikaner Vryheidstigting [ Afrikaner Freedom Foundation , or Avstig]:\n\u2019n Kultuur-politieke gemeenskap van Afrikaanssprekende Suid-Afrikaners van \noorwegend Europese afkoms wat aan die hand van \u2019n gedeelde verlede en \ngedeelde verwagtings van die toekoms \u2019n selfbewuste bestaan voer, wat as \u2019n volk \nerken word en oor soewereiniteit in \u2019n eie staat beskik het, wat steeds aan taal \nen kultuur herken word en wat die reg het om sy identiteit te handhaaf en om nie \naan gedwonge inlywing uitgelewer te word nie. [A cultural-political community of \nAfrikaans-speaking South Africans of predominantly European descent who lead a \nself-conscious existence on the basis of a shared past and shared expectations of \nthe future, who are recognised as a people and who possessed sovereignty in their \nown state, who are still recognised by language and culture and have the right to \nmaintain their identity and not to be relinquished to forced incorporation .]\nOrania aims to preserve traditional Afrikaner culture, and therefore generally follows \nthe definition of the Afrikaner used by Boshoff (2012). \nLeft and right political leanings\n\u201cPolitical leaning\u201d as used in the current study refers to the traditional left/right divide, \nwhere left-wing ideals encompass liberal concepts such as individual freedom, \nequality, fraternity, human rights, progress, reform, and internationalism, whereas \nright-wing ideals encompass conservative concepts such as authority, hierarchy, \norder, duty, tradition, reaction, and nationalism (Heywood 2015). In this sense, Orania \nis generally regarded as a right-wing community because of its emphasis on traditional \nvalues and ethnicity. Residents also mostly support the conservative Freedom Front \nPlus (FF+), and leaders in the community, including Boshoff and his sons, Carel (IV) \nand Wynand, have worked for the FF+, while there are no formal ties with the more \nliberal Democratic Alliance (DA).\nRELIABILITY AND OBJECTIVITY OF MAINSTREAM MEDIA\nIn the contemporary world, mainstream media and science have been increasingly \nquestioned by the general public. Kavanagh and Rich (2018) refer to \u201ctruth decay\u201d in \nthe contemporary world, which they argue is described by four characteristics:\n \u2666increasing disagreement about facts and analytical interpretations of facts \nand data;\n \u2666a blurring of the line between opinion and fact;\n \u2666the increasing relative volume, and resulting influence, of opinion and personal \nexperience over fact; and\n \u2666declining trust in formerly respected sources of factual information.\n5 4\nSenekalMainstream media have been primary targets of this \u201cdeclining trust in formerly \nrespected sources of factual information\u201d, with both television news and print media \n(nowadays available online) affected (Kavanagh & Rich 2018). Mainstream media is \noften accused of furthering political agendas, biased reporting, manipulating the public, \nobscuring the truth, and even fabricating stories. While the decline in trust in science \nis less marked than for the media, trust in science has also eroded (Kavanagh & Rich \n2018). Kavanagh and Rich (2018) state that the decline in trust in these institutions is \npartly due to unethical practices of some of these institutions, such as falsifying data or \nmanipulating the peer review process in terms of scientific publications, or biased and \ninaccurate reporting by some media outlets. The resulting lack of trust in established \nproviders of information \u201cleaves people searching for new sources of credible and \nobjective information and increases uncertainty about basic facts, data, and analysis \nas people turn to new entities, not all of them trustworthy, to fill this vacuum\u201d (Kavanagh \n& Rich 2018). These alternative sources of information are often not comparably \nreliable. Toepfl et al. (2022) for instance found that mainstream media usually refrain \nfrom spreading conspiracy theories, but niche and special interest media play a key \nrole in spreading conspiracy theories, along with the Russian state-sponsored news \nchannel, Sputnik. Xu et al. (2022) in turn found that mainstream media outlets such \nas CNN and BBC were the fifth and third most reliable news sources respectively out \nof the 13 news agencies in their study, compared to Russia\u2019s RT (formerly Russia \nToday) being the third most unreliable news source. Nevertheless, as people lose \ntrust in Western mainstream media, they turn to alternative media such as Sputnik, \nRT, or other alternative news sources. This turn to alternative and unreliable sources \nof information contributes to the spread of misinformation and conspiracy theories, for \ninstance in reference to the COVID-19 pandemic (Johnson & Marcellino 2021; Van \nMulukom et al. 2022).\nOne way of regaining trust in mainstream media is by reducing bias, or at least making \nbias explicit. Bias can be defined as \u201cprejudice in favor of or against a particular thing, \nperson, or group in comparison to another, usually in an unjust manner\u201d (Raza et al. \n2022). Bias can be based on various factors, including gender, race, ethnicity, age, \ndisability, mental health, religion, and political ideology (Raza et al. 2022). Political bias \nis a particularly important manifestation of bias, as it may influence voter behaviour \nand perceptions of both domestic and foreign policy, and it also perpetuates and even \nexacerbates existing divisions in society. \nGiven that political bias in the media is such a crucial factor in determining the \ncredibility of the source, various websites focus on highlighting the bias inherent \nin different news sources. One of the most well-known of these is Media Bias/Fact \nCheck (https://mediabiasfactcheck.com/), a website that provides information about \nthe bias and credibility of news sources by country, including South Africa. Similarly, \nBiasly (www.biasly.com) provides bias rankings of news sources around the world, \nincluding some South African sources. In addition, Biasly provides a tool (www.biasly.\ncom) created using Artificial Intelligence (AI) that allows a user to determine the bias \nof a particular online publication, similar to the tool provided by The Bipartisan Press \n(www.thebipartisanpress.com).\n5 4\nThe depiction of Orania in print media (2013-2022)As Iyyer et al. (2014) argue, it is impractical and expensive to detect ideological bias \nin political texts manually, especially in the era of big data. Therefore, various studies \nhave investigated the feasibility of using machine learning to classify political bias. \nIyyer et al. (2014) for instance used Recursive Neural Networks (RNNs) combined \nwith Word2Vec (Le & Mikolov 2014) to train a model to classify news reports in terms \nof political bias. Chun et al. (2019) used Support Vector Machines (SVM) (Cortes \n& Vapnik 1995), Convolutional Neural Networks (CNN) (LeCun & Bengio 1998), \nWord2Vec (Le & Mikolov 2014) and BERT (Devlin et al. 2018) to train a model with \nup to 89% accuracy. Wang (2019) describes the tool used by The Bipartisan Press, \nwhich was trained using BERT (Devlin et al. 2018), and later upgraded using the \nRoBERTa model (Liu et al. 2019). Raza et al. (2022) developed Dbias, a Python library \nto identify bias in news reports, also using BERT (Devlin et al. 2018), RoBERTa (Liu \net al. 2019) and DestilBERT (Sanh et al. 2019). The bias classification tool by The \nBipartisan Press (Wang 2019) is used in the current study.\nApart from political bias, another way of evaluating the way the news depicts topics is \nthrough sentiment analysis. The media tends to focus on negative stories (Soroka et \nal. 2015), and Haselmayer and Jenny (2017) showed how sentiment analysis could \nbe used to study how the media reports on German parliamentary elections, while \nVan Atteveld et al. (2008) studied the news coverage of Dutch elections. Garvey and \nMaskal (2020) used the Google Cloud Natural Language API Sentiment Analysis \ntool to study how the media depicts AI and found that the media frames AI more \npositively than commonly thought. Taking a lexicon-based approach, Suryadi (2021) \nstudied how news about COVID-19 was framed in the Indonesian media. Given the \ncontroversial nature of Orania, and perceptions that Orania receives negative media \ncoverage, investigating whether this community is depicted in a positive or negative \nway will shed valuable light on whether the media depicts Orania in a balanced way.\nBACKGROUND TO ORANIA\u2019S RELATIONSHIP WITH THE MEDIA\nEven before Orania was purchased in 1991, the media portrayed Afrikaners who \ncampaigned for a volkstaat (Afrikaner homeland) negatively. Messina (1989) for \nexample refers to the Oranjewerkers (Orange Workers, one of the pro-volkstaat \ninstitutions during the 1980s) as \u201cdie lemoenmannetjies en -vrouetjies van Morgenzon\u201d \n[the little orange men and women of Morgenzon ] whose \u201cdorp-staat gedagtes en die \nbeweegrede daarvoor [...] [is] \u2019n duidelike teken van onchristelikheid\u201d [town-state \nthoughts and the motivation for them [...] [are] a clear sign of unchristian attitudes]. \nBoshoff (2012), a leading figure in the Oranjewerkers and later a founder of Avstig \nand Orania, also notes negative media coverage of volkstaat movements, \u201cVir die \nmedia en politieke kommentators was dit onmoontlik om die gedagtegang en groei \nvan hierdie Afrikanervryheidsbeweging anders te interpreteer as om dit as verregs, \nkonserwatief en die handhawing van apartheid te stigmatiseer\u201d [For the media and \npolitical commentators it was impossible to interpret the thinking and growth of this \nAfrikaner freedom movement other than to stigmatise it as far right, conservative and \nfor the maintenance of apartheid ].\n7 6\nSenekalWith the founding of Orania in 1991, its difficult relationship with the media began \n(Senekal 2021). The Afrikaans newspaper Die Burger called Orania a \u201crassistiese \nlugkasteel\u201d [a racist castle in the sky] (Anonymous 1991b) in 1991. Even before \nOrania\u2019s official opening on 13 April 1991, the first settlers had already developed an \naversion to journalists, and one of them, Johan Moolman, said, \u201cHulle had nie eers die \npit om met ons te kom praat nie, maar hardloop na die Kleurlinge toe om dan agterna \nhier in die dorp te kom rondsluip\u201d [They didn\u2019t even have the guts to come and talk \nto us, but ran to the Coloureds and then sneak around here in town] (Anonymous \n1991b). Moolman was referring to the eviction of Coloured inhabitants prior to the \nopening of the town, as is described in detail in Cavanagh (2013) and Senekal (2021).\nAfter former President Nelson Mandela\u2019s visit in 1995, Sello (1995) wrote about the \n\u201cracists\u201d of Orania in City Press, while Mkhondo (2001) called Orania an \u201cethnic and \nracist enclave\u201d and suggested Oranians \u201c[can] go to hell\u201d. McNally (2010) states his \nview equally clearly: \u201cI found myself splitting people into two categories: true racist and \nracist from horrific crime incidents.\u201ds According to McNally (2010), Afrikaners moved \nto Orania \u201cto be racist in peace\u201d, Orania is a \u201cbio-dome of apartheid\u201d and the \u201clast \nblemish of the old South Africa\u201d. He also refers to Boshoff\u2019s son and then president of \nthe Orania Movement, Carel Boshoff (IV), as an \u201cintellectual racist\u201d.\nAlleged racism remains a recurring theme in some reporting on Orania, and more than \na decade after McNally (2010), Thamm (2021) refers to Orania as a cult, and states,\nSingle men live in single quarters. Single mothers who arrive with children are \nimmediately set to work and are \u2018rehabilitated\u2019 by the Kaalvoet Vrouens \u2013 the \nbarefoot women. It is these matriarchs who will set you on the path to Christianity, \nthe restoration of your dignity and your Afrikaner pioneer identity. Arbeit macht frei. \nLook it up.\nHer use of German terms from the Nazi era in her report, such as Lebensraum  and \nHeimat, along with the above motto from Auschwitz, suggests that Orania has a Nazi \nconnection, although this suggestion is never explicitly stated.\nFalse reporting on Orania by major news organisations is usually followed by formal \ncomplaints by the Orania Movement. In May 2019, a news team from eNCA visited \nOrania, and after the report, the newsreader, Vuyo Mvoko, said that black people are \nonly welcome in Orania as domestic workers and gardeners \u2013 which is something that \nwas not said by the reporters (Roets 2019). The Orania Movement lodged a complaint \nwith the Broadcasting Complaints Commission of South Africa (BCCSA) and the case \nconcluded in Orania\u2019s favour (Roets 2019; Strydom 2019).\nWhile false reporting by major news outlets is usually addressed by the Orania \nMovement through official complaints, other false claims \u2013 often on social media \u2013 are \nusually left to supporters of Orania or the general public to address. In 2022, a user \nshared a \u201cnews report\u201d on Twitter (now the social media platform X) which claimed that \na 95-year-old woman had given birth in Orania after a \u201cdelayed pregnancy\u201d of more \nthan four decades, but users quickly spotted that the report was fake (Chloerii 2022).\n7 6\nThe depiction of Orania in print media (2013-2022)In response to negative reporting, especially from foreign journalists, Roets (2019) \nposted a notice at Stokkiesdraai Shopping Centre in Orania that reads, \u201cAttention all \nwhite journalists from Europe. Please leave your prejudice at the entrance.\u201d The notice \nwas translated into Dutch, German and French.\nThe negative view of Orania has also been accompanied throughout its history with \nthreats that the town will be demolished and destroyed. Even before Orania\u2019s opening \nin 1991, Die Transvaler (Anonymous 1991a) reported that Oranians received threats, \nfor example that they would be \u201celiminated\u201d. Opperman (2016) also states that in the \nearly 2000s, Orania received threats that people wanted to destroy the town.\nIn today\u2019s milieu, such threats are found, among other places, on social media, and in \nparticular on Twitter (X), as discussed in Kotz\u00e9 and Senekal (2018). Kotz\u00e9 and Senekal \n(2018) for instance cite tweets such as, \u201cHow about we accidentally get our hands on \nsome grenades and start genocide? Orania is a good place to start\u201d and \u201cThat Orania \nshit place must be burnt down we want them back in Europe or Australia\u201d. In this study \nit was found that Orania is mostly depicted in a negative light on X, and that Orania is \nmostly mentioned on this social media platform in the context of racism.\nDespite the perception of negative publicity, and the examples discussed above, news \noutlets often report on Orania\u2019s successes and development. Criticism of Orania is to \nbe expected, but a cursory glance at news reports on Orania shows that the severely \nnegative examples discussed above do not constitute the totality of media reports on \nthe town. For instance, Wyngaard (2018) and Snyman (2013) write about Orania in \npositive terms. To determine whether this controversial community is treated fairly by \nthe media, one must also take the numerous positive reports into account, as is done \nin the current study.\nMETHODS\nData gathering\nOrania\u2019s archive of newspaper publications was used as data in the current study. This \ncollection includes news reports, letters to news outlets and opinion pieces that mention \nOrania and constitute the most extensive news collection on Orania that is currently \navailable. The collection covers a wide range of publication outlets, although more than \n50% are from Netwerk24. This collection was previously digitised (Senekal & Kotz\u00e9 \n2018). During the digitisation process, efforts were made to include the best available \ncopy, and therefore only publications that could not be found in a digital format were \nscanned, while others were stored in Portable Document Format (PDF) straight from \nweb publications (Senekal & Kotz\u00e9 2018). This means that for the latter publications, \nno correction of Optical Character Recognition (OCR) errors is necessary, while the \nformer would require extensive efforts to correct OCR errors and produce a perfect \ntext for processing purposes. For this reason, only clippings published since 2013 are \nused in the current study, as this is the date when publications became available in a \ndigital format. The author has permanent access to the archive for research purposes. \n9 8\nSenekalBecause the NLP techniques applied below were designed for analysing text written in \nEnglish, Afrikaans publications were first translated to English using Google Translate. \nWhile Google Translate\u2019s outputs were not expected to be perfect, Aiken (2019) and \nBenjamin (2019) have shown that translations between Afrikaans and English are \nsome of the most accurate translations produced by this service.\nNote that the current study focuses on print media, and television news reports are \nnot included. This means that many foreign reports will not be included in the analysis \nbelow. In addition, some news outlets such as The New York Times did not publish \nanything on Orania in the period under investigation, and while 82 publication platforms \nare included in the total collection, only 59 published on Orania between 2013 and \n2022. While the collection includes publications in all languages, for the period under \ninvestigation, only Afrikaans and English publications are available.\nSentiment analysis\nSentiment analysis is an NLP technique that involves classifying a text as positive, \nneutral, or negative. Two popular approaches to sentiment classification are lexicon-\nbased and machine learning approaches. In the case of Orania, Kotz\u00e9 and Senekal \n(2018) used a lexicon-based approach. In the current study, the lexicon-based \nsentiment analysis method developed by Levallois (2013) was used, which has \nbeen demonstrated to be one of the most effective techniques for sentiment analysis \n(Ribeiro et al. 2016). The output is simply a classification between positive, negative, \nand neutral, without providing an indication of the level of sentiment that occurs in the \ntext. This method was applied to Orania\u2019s news collection from 2013 to 2022 using \nthe online set of tools available at https://nocodefunctions.com/, which include the \nnecessary text preprocessing steps such as lemmatisation, the removal of stopwords, \nand the removal of special characters.\nAs Orania is generally associated with the Afrikaner and Afrikaans, it was expected that \nAfrikaans news outlets would more often depict Orania in a positive light than English \nnews outlets, although as the background section showed, negative publications on \nOrania occur in Afrikaans as well as English news sources. In addition, as Orania is \nconsidered to be conservative and right wing, it was expected that right biased reports \nwould present Orania in a positive light, while left biased reports were expected to \ndepict Orania in a negative light. \nPolitical bias classification\nPolitical bias classification using machine learning involves employing a machine \nlearning model to classify whether a given text exhibits left or right political bias. \nFor political bias classification, the tool developed by The Bipartisan Press (www.\nthebipartisanpress.com), as described by Wang (2019), was used. This tool produces \nan output stating if the text is left or right biased and offers the level of bias (minimal, \nmoderate, strong, or extreme), but it does not classify texts as neutral. A minimum of \n100 words is necessary for an accurate classification using this model.\nWhile this model\u2019s accuracy has not been formally tested in a South African context, \nthe concept of right and left bias is not specific to the political climate in the United \n9 8\nThe depiction of Orania in print media (2013-2022)States where the tool was developed, and therefore the model was expected to also \nclassify texts accurately in a South African setting. Nevertheless, spot checks were \ncarried out, and the model for instance classified the election manifesto of the FF+, \nas published by News24 (Anonymous 2014), as moderate right, and Afrikaans singer \nSteve Hofmeyr\u2019s views on the Afrikaner (Hofmeyr 2013) as strong right, while the \ncriticism of Orania by Thamm (2021) was classified as moderate left, which in all \ncases agree with the author\u2019s own assessment.\nAs Orania is considered to be conservative, traditional and right wing, it was expected \nthat right biased reports would depict Orania in more favourable terms, as opposed \nto left biased publications. The four examples above illustrate what was expected: \nHofmeyr has performed in Orania, and Boshoff, as well as his sons, Wynand and \nCarel (IV), have worked for the FF+. If Hofmeyr or the FF+ refer to Orania, they are \ngenerally expected to do so in positive terms. On the other hand, Thamm is negative \nabout Orania, and from her publication and the classification using the tool by The \nBipartisan Press, it is clear that she writes from a left perspective. In other words, bias \nand sentiment were expected to show some agreement. In addition, as the Afrikaner \nis generally considered to be a more conservative ethnic group (Vestergaard 2001; \nFinlayson 2019), it was expected that Afrikaans publications would more often be \nbiased right in order to cater to an Afrikaans audience.\nRESULTS\nOverview of the dataset\nFigure 1 provides an overview of the dataset. Figure 1A shows the publication dates of \ntexts, and as the graph shows, there is no discernible pattern that would suggest either \na general increase or a decrease in the attention that Orania receives in the media. \nThe high number of publications in 2019 is related to controversy in this year around \nBoshoff (IV) (at the time the president of the Orania Movement), and the amount of \nattention Orania received around that year\u2019s general election.\nFigure 1B shows the publication platforms where more than two documents in this \ndataset were published for the period 2013 to 2022, and it can be seen here that while \na wide variety of publication platforms are included, the vast majority of documents \nwere published by Netwerk24 and Maroela Media. This distribution across platforms \nalready suggests that the amount of attention paid to Orania is not equally distributed \nbetween Afrikaans and English news outlets.\nFigure 1C shows the language distribution of documents, with 75% Afrikaans and 25% \nEnglish. This is important as Orania receives vastly more attention in Afrikaans news \npublications than in English, which could affect the way that Orania is depicted. \nFigure 1D shows the average number of words per text for Afrikaans and English \ndocuments, with English texts being slightly longer. The highest number of words per \ntext in this dataset is 7 628 words and the lowest number 95 words, and together with \nthe averages shown in Figure 1D, most texts exceed the minimum requirement of 100 \nwords for political bias classification using the model by Wang (2019).\n11 10\nSenekal\nFIGURE 1 : A SUMMARY OF THE DATASET\nSentiment analysis\nFigure 2 shows the results of the sentiment analysis. Figure 2A shows that the largest \nsegment of documents (38%) was classified as neutral, followed by positive (36%) \nand negative (27%). Despite some very negative outliers as noted in the background \nsection, publications in the media about Orania are therefore generally neutral or \npositive, with negative publications forming the minority of publications and only \nconstituting slightly more than a quarter of publications. \n11 10\nThe depiction of Orania in print media (2013-2022)Figure 2B shows sentiment over time, and it can be seen here that there is a general \npositive trend from 2015 onwards, with positive publications in 2021 exceeding those \nfrom the other categories. Negative sentiment however fluctuates between years, \nwithout any discernible pattern. \nFigures 2C and 2D show the sentiment distribution by language, and it can be seen \nhere that Afrikaans publications are more often positive (41%), followed by neutral \n(35%), with the minority of publications classified as negative (25%). This is in sharp \ncontrast with English publications, which tend to more often be neutral (45%), followed \nby negative (34%), with the smallest category being positive (21%). As noted earlier, \nthis is to be expected, given Orania\u2019s emphasis on Afrikaans, which may lead to \nAfrikaans media outlets being more positively disposed towards this community.\nFigure 2E visualises the relationship between language and sentiment for both \nlanguages, with the thickness of ties indicating higher percentages. Neutral publications \nare split almost evenly between Afrikaans and English, but negative publications are \nmore often produced in English, and positive publications are more often produced \nin Afrikaans. These three figures (2C, 2D and 2E) show that publications in Afrikaans \nmedia take a more positive stance towards Orania, while English media report more \nnegatively on this community.\n13 12\nSenekal\nFIGURE 2 : SENTIMENT ANALYSIS RESULTS\nPolitical bias classification\nFigure 3 provides the results of the political bias classification. Figure 3A shows \nthat political bias is almost evenly distributed through this dataset, but with a slight \npreference for right biased publications. \nFigure 3B shows that the majority of publications were classified as exhibiting \nminimal bias, both for left and right bias. Note that this model does not classify texts \nas neutral, but if it did, many of the minimal bias classifications could have been \nneutral classifications. When it comes to moderate bias, more left leaning publications \n13 12\nThe depiction of Orania in print media (2013-2022)exhibited moderate bias than is the case for right bias, with 7.37% of publications \nshowing moderate left bias, compared with 5.78% of right leaning publications \nshowing moderate bias. Strong bias is rare in this dataset, but even when it does \noccur, it is more often left leaning (1.58% of publications) than right leaning (1.2% of \npublications). No publication was classified as exhibiting extreme bias. \nFigure 3C shows that the even distribution in Figure 3A applies to time segments as \nwell, with some years showing more right bias and others showing more left bias. \nThere is no discernible pattern with regards to publication bias over time: publications \nthat mention Orania are not becoming either more or less left or right biased. \nThe same holds for language (Figure 3D): Afrikaans and English publications are more \nor less evenly divided between right and left bias, with English publications showing \na slightly greater right bias (58.73% of English publications) than Afrikaans publications \n(51.06% of Afrikaans publications). While this is a surprising finding, since Afrikaans \npublications were expected to be more right biased than English publications, the \ndifference between the languages remains small.\nFigure 3E shows political bias by sentiment. Contrary to expectations, right biased \npublications did not overwhelmingly depict Orania in a positive light, nor did left biased \npublications present Orania in a negative light. Rather, sentiment is fairly evenly spread \nbetween left and right bias. Indeed, negative sentiment is more often expressed in \nright biased publications than in left biased publications, with 41.05% of negative \nreports classified as left biased, and 58.95% of negative publications classified as \nright biased.\n15 14\nSenekal\nFIGURE 3 : THE RESULTS OF THE POLITICAL BIAS CLASSIFICATION\nDISCUSSION\nFrom the above, it is clear that Orania is not generally represented in a negative light \nin the media. While the largest segment of publications was classified as neutral, \npositive publications constitute the second largest segment. However, as expected, \nAfrikaans publications on Orania tend to be more positive, while English publications \nare often negative. \n15 14\nThe depiction of Orania in print media (2013-2022)It is also clear that strongly biased depictions of Orania are the exception rather than \nthe rule. The majority of documents in the current study showed only minimal bias, \nwhich could be considered close to neutral. In addition, the distribution of bias is more \nor less equal, with right biased reports occurring only slightly more frequently (53%) \nthan left biased reports (47%). Contrary to expectations, bias is also fairly evenly \ndistributed across Afrikaans and English publications, with English publications even \nshowing a slightly greater tendency to be right biased than Afrikaans publications. \nAlso contrary to expectations, right biased publications did not overwhelmingly report \non Orania in positive terms, but even showed a slight tendency to report negatively on \nthis community. Left biased reports, on the other hand, showed a surprising tendency \nto report positively on Orania. \nThe above shows that media reporting on Orania is far more balanced than the \nperception would suggest. While positive reporting on Orania in Afrikaans media \nwas expected, a large percentage of English publications also depicted Orania in a \npositive light (although negative publications were more common). Left or right bias is \nalso not an accurate predictor of the sentiment of a publication, with both presenting \nOrania in a positive as well as a negative light. Overall, while some journalists \nhave shown prejudice towards this community and there have been cases where \nfacts were distorted, in general, standards of professionalism seem to apply to this \ncontroversial community.\nAn important question that neither sentiment analysis nor bias classification answers \nis why recent print media reports on Orania tend to be more positive. One possible \nexplanation is that in light of the South African government\u2019s failures, for example \nin terms of electricity provision, service delivery and crime, Orania\u2019s successes are \nincreasingly recognised. It may also be that Orania\u2019s self-help attitude is increasingly \nseen as a solution to government failures. To determine why Orania is increasingly \ndepicted in a positive light, a thematic analysis of news reports will have to be \nconducted, which was not attempted here.\nAs mentioned before, the current study\u2019s focus on print media meant that television \nnews reports were not included. This is an important limitation, since the mentioned \ndistortion of facts by eNCA was a television news piece, and other television news \nreports could also depict Orania in more biased ways. \nCONCLUSION\nThis study investigated the depiction of Orania using two NLP methods, namely \nsentiment analysis and bias classification. It was shown that the depiction of Orania \nis generally balanced, with positive and negative reports in similar percentages (with \nslightly more positive than negative reports), and only a few publications showing more \nthan minimal bias, while bias itself is also roughly evenly distributed. In general, the \nmedia seems to approach this controversial community in a balanced way, although \nsome extreme examples were also discussed. It should also be noted that interest \nin this community comes mostly from Afrikaans news outlets, although interest is not \nconfined to Afrikaans news outlets.\n17 16\nSenekalIf mainstream media is to retain its role as a provider of trustworthy information, \neliminating bias and reporting on topics in a balanced way will be important. The \ncurrent study has shown that, apart from a small number of exceptions, the media has \ngenerally reported on this controversial community in a balanced way.\nWhile Orania is one controversial South African topic, similar techniques could be \napplied to study how the media depicts other controversial South African topics. \nHaselmayer and Jenny (2017) argue that these text analysis techniques open up a \nwide variety of research questions, among which the study of public opinion, media \nframing and political polarisation are areas where sentiment analysis might be useful \nat the nexus of communication science and political science.\nREFERENCES\nAiken, M. 2019. An updated evaluation of Google Translate accuracy. Studies in Linguistics \nand Literature  3(3): 253. DOI: 10.22158/sll.v3n3p253.\nAnonymous. 1991a. Blankes glo gedreig. Die Transvaler , 3 April: 2.\nAnonymous. 1991b. Orania \u2019n rassistiese lugkasteel. Die Burger , 12 February: 14.\nAnonymous. 2014. Manifesto FF+. Available at: https://www.news24.com/Elections/MANI \nFESTO-FF-20140311 [Accessed: 25 May 2018].\nBenjamin, M. 2019. Empirical evaluation of Google Translate across 107 languages . \nAvailable at: https://www.teachyoubackwards.com/empirical-evaluation/ [Accessed: \n25 June 2021].\nBoshoff, C. 2012. Dis nou ek . Pretoria: LAPA Uitgewers.\nCavanagh, E. 2013. The history of dispossession at Orania and the politics of land \nrestitution in South Africa. Journal of Southern African Studies 39(2): 391-407. DOI: \n10.1080/03057070.2013.795811.\nChloerii. 2022. 95 year old lady allegedly gives birth after being impregnated 45 years \nago. Available at: https://opera.news/za/en/parenting/4d075565e76f1de1528bc9a  \n46f2fa9f6 [Accessed: 28 April 2022].\nChun, S., Holowczak, R., Dharan, K., Wang, R., Basu, S. & Geller, J. 2019. Detecting \npolitical bias trolls in Twitter data. Proceedings of the 15th International Conference \non Web Information Systems and Technologies , SCITEPRESS - Science and \nTechnology Publications: 334-342. DOI: 10.5220/0008350303340342.\nCortes, C. & Vapnik, V. 1995. Support-vector networks. Machine Learning  20(3): 273-297. \nDOI: 10.1007/BF00994018.\nDavis, R. 2020. \u2018Everyone in Orania is woke\u2019: A journey to SA\u2019s most notorious town. \nAvailable at: https://www.dailymaverick.co.za/article/2020-01-21-everyone-in-orania  \n-is-woke-a-journey-to-sas-most-notorious-town/ [Accessed: 21 January 2020].\nDelvecki, A. & Greiner, A. 2014. Circling of the wagons? A look at Orania, South Africa. \nFocus on Geography  57(4): 164-173. DOI: 10.1111/foge.12042.\nDevlin, J., Chang, M.W., Lee, K. & Toutanova, K. 2018. Bert: Pre-training of deep bidirectional \ntransformers for language understanding. arXiv preprint arXiv:1810.04805 .\n17 16\nThe depiction of Orania in print media (2013-2022)Finlayson, K. 2019. \u2018ik ben een afrikaander\u2019: Redrawing Afrikaner ethnic boundaries in New \nZealand. Sites: A Journal of Social Anthropology and Cultural Studies 16(2). DOI: \n10.11157/sites-id436.\nGarvey, C. & Maskal, C. 2020. Sentiment analysis of the news media on artificial intelligence \ndoes not support claims of negative bias against artificial intelligence. Omics: A \njournal of Integrative Biology  24(5): 286-299. DOI: 10.1089/omi.2019.0078.\nHaselmayer, M. & Jenny, M. 2017. Sentiment analysis of political communication: Combining \na dictionary approach with crowdcoding. Quality & Quantity 51(6): 2623-2646. DOI: \n10.1007/s11135-016-0412-4.\nHeywood, A. 2015. Key concepts in politics and international relations. London: Springer. \nhttps://doi.org/10.1007/978-1-137-49477-1\nHofmeyr, S. 2013. Wie is die Afrikaner? Steve lewer repliek. Available at: https://\nmaroelamedia.co.za/debat/meningsvormers/wie-is-die-afrikaner-steve-lewer-\nrepliek/ [Accessed: 28 September 2020].\nHollfelder, N., Erasmus, J.C., Hammaren, R., Vicente, M., Jakobsson, M., Greeff, J.M. & \nSchlebusch, C.M. 2020. Patterns of African and Asian admixture in the Afrikaner \npopulation of South Africa. BMC Biology  18(1): 16. DOI: 10.1186/s12915-020-0746-\n1.\nIyyer, M., Enns, P., Boyd-Graber, J. & Resnik, P. 2014. Political ideology detection \nusing recursive neural networks. Proceedings of the 52nd Annual Meeting of the \nAssociation for Computational Linguistics (Volume 1: Long Papers). Stroudsburg, \nPA, USA: Association for Computational Linguistics: 1113-1122. DOI: 10.3115/v1/\nP14-1105.\nJohnson, C. & Marcellino, W. 2021. Bad actors in news reporting: Tracking news \nmanipulation by state actors. Santa Monica: RAND Corporation. DOI: 10.7249/\nRRA112-21.\nKavanagh, J. & Rich, M. 2018. Truth decay: An initial exploration of the diminishing role of \nfacts and analysis in American public life. Santa Monica: RAND Corporation. DOI: \n10.7249/RR2314.\nKotz\u00e9, E. & Senekal, B.A. 2018. Employing sentiment analysis for gauging perceptions \nof minorities in multicultural societies: An analysis of Twitter feeds on the Afrikaner \ncommunity of Orania in South Africa. The Journal for Transdisciplinary Research in \nSouthern Africa 14(1): a564. DOI: 10.4102/td.v14i1.564.\nLeCun, Y. & Bengio, Y. 1998. Convolutional Networks for images, speech, and time \nseries. In: Arbib, M.A. (ed.). The Handbook of Brain Theory and Neural Networks. \nCambridge, MA: MIT Press.\nLevallois, C. 2013. Umigon: Sentiment analysis for tweets based on terms lists and \nheuristics. International Workshop on Semantic Evaluation . 7th International \nWorkshop on Semantic Evaluation .\nLe, Q. & Mikolov, T. 2014. Distributed representations of sentences and documents. \nInternational conference on machine learning : 1188.\n19 18\nSenekalLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. \n& Stoyanov, V. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. \narXiv. DOI: 10.48550/arxiv.1907.11692.\nMcNally, P. 2010. Orania tourism: Come gawk at the racists. Available at: http://\nthoughtleader.co.za/paulmcnally/2010/02/01/orania-tourism-come-gawk-at-the-\nracists/ [Accessed: 12 October 2017].\nMessina, E.A. 1989. Ossewatrekke: Mites rondom Groot Trek-feeste. Kronos: Journal of \nCape History : 30-41.\nMkhondo, R. 2001. It\u2019s a pipe dream for the bittereinders. The Citizen , 11 April: 12.\nOpperman, M. 2016. Interview conducted by Sebastiaan Biehl. Orania Archive.\nRaza, S., Reji, D.J. & Ding, C. 2022. Dbias: Detecting biases and ensuring fairness in news \narticles. International Journal of Data Science and Analytics: 1-21. DOI: 10.1007/\ns41060-022-00359-4.\nRibeiro, F.N., Ara\u00fajo, M., Gon\u00e7alves, P., Andr\u00e9 Gon\u00e7alves, M. & Benevenuto, F. 2016. \nSentiBench - a benchmark comparison of state-of-the-practice sentiment analysis \nmethods. EPJ Data Science 5(1): 23. DOI: 10.1140/epjds/s13688-016-0085-1.\nRoets, S. 2019. Orania \u2013 Die waarheid sal see\u0308vier. Available at: https://maroelamedia.co.za/\ndebat/meningsvormers/orania-die-waarheid-sal-seevier/ [Accessed: 13 November \n2019].\nSanh, V., Debut, L., Chaumond, J. & Wolf, T. 2019. DistilBERT, a distilled version of BERT: \nsmaller, faster, cheaper and lighter. arXiv. DOI: 10.48550/arxiv.1910.01108.\nSeldon, S.R. 2014. Orania and the reinvention of Afrikanerdom. Unpublished doctoral \nthesis. University of Edinburgh, Scotland, United Kingdom.\nSello, S. 1995. Good manners from deluded folk in Orania. City Press, 20 August: 17.\nSenekal, B.A. & Kotz\u00e9, E. 2018. Die ontwikkeling van \u2019n koste-effektiewe en \nbyderwetse multimedia digitale argief by EPOG in Orania. LitNet Akademies \nGeesteswetenskappe  15(3): 239-275.\nSenekal, B.A. 2021. Die eerste 40 jaar van Orania. Tydskrif vir Geesteswetenskappe 61(2): \n526-550. DOI: 10.17159/2224-7912/2021/v61n2a8.\nSnyman, D. 2013. Van alle kante: Toe lees Madiba vir tant Betsie. Available at: https://\nwww.netwerk24.com/Stemme/Van-Alle-Kante-Toe-lees-Madiba-vir-tant-Betsie-  \n20130627 [Accessed: 5 July 2021].\nSoroka, S., Young, L. & Balmas, M. 2015. Bad news or mad news? Sentiment scoring of \nnegativity, fear, and anger in news content. The Annals of the American Academy \nof Political and Social Science 659(1): 108-121. DOI: 10.1177/0002716215569217.\nSteyn, J.C. 1984. Nuwe bedeling: Identiteitsverandering. Aambeeld  12(2): 20.\nStrydom, N. 2019. eNCA moet jammer s\u00ea vir Orania. Available at: https://maroelamedia.\nco.za/nuus/sa-nuus/enca-moet-jammer-se-vir...fcba5-6a99042137-105219033  \n&mc_cid=6a99042137&mc_eid=3d37c4e855 [Accessed: 5 July 2021].\n19 18\nThe depiction of Orania in print media (2013-2022)Suryadi, D. 2021. Does it make you sad? A lexicon-based sentiment analysis on COVID-19 \nnews tweets. IOP Conference Series: Materials Science and Engineering  1077(1): \n012042. DOI: 10.1088/1757-899X/1077/1/012042.\nThamm, M. 2021. Above us only sky: A road trip through South Africa\u2019s heartland. Available \nat: https://www.dailymaverick.co.za/article/2021-10-12-above-us-only-sky-a-road-\ntrip-through-south-africas-heartland/ [Accessed: 12 October 2021].\nToepfl, F., Kravets, D., Ryzhova, A. & Beseler, A. 2022. Who are the plotters behind the \npandemic? Comparing Covid-19 conspiracy theories in Google search results \nacross five key target countries of Russia\u2019s foreign communication. Information, \nCommunication & Society : 1-19. DOI: 10.1080/1369118X.2022.2065213.\nVan Atteveldt, W., Kleinnijenhuis, J., Ruigrok, N. & Schlobach, S. 2008. Good news or bad \nnews? Conducting sentiment analysis on Dutch text to distinguish between positive \nand negative relations. Journal of Information Technology & Politics 5(1): 73-94. \nDOI: 10.1080/19331680802154145.\nVan Mulukom, V., Pummerer, L.J., Alper, S., Bai, H., \u010cavojov\u00e1, V., Farias, J., Kay, C.S., \nLazarevic, L.B., Lobato, E.J.C., Marinthe, G., Pavela Banai, I., \u0160rol, J. & \u017de\u017eelj, \nI. 2022. Antecedents and consequences of COVID-19 conspiracy beliefs: A \nsystematic review. Social Science & Medicine 301: 114912. DOI: 10.1016/j.\nsocscimed.2022.114912.\nVestergaard, M. 2001. Who\u2019s got the map? The negotiation of Afrikaner identities in post-\napartheid South Africa. Daedalus  130(1): 19\u201344.\nWang, W. 2019. Calculating political bias and fighting partisanship with AI. Available at: \nhttps://www.thebipartisanpress.com/politics/calculating-political-bias-and-fighting-\npartisanship-with-ai/ [Accessed: 16 March 2023].\nWyngaard, B. 2018. Orania, koeksisters en ek. Available at: https://www.litnet.co.za/orania-\nkoeksisters-en-ek/ [Accessed: 5 July 2021].\nXu, M., Luo, Z., Xu, H. & Wang, B. 2022. Media bias and factors affecting the impartiality of \nnews agencies during COVID-19. Behavioral Sciences (Basel, Switzerland)  12(9). \nDOI: 10.3390/bs12090313.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "The depiction of Orania in print media (2013-2022): a quantitative analysis using Natural Language Processing (NLP)", "author": ["B Senekal"], "pub_year": "2023", "venue": "Communitas", "abstract": "The current article investigates the depiction of the town of Orania in print media. Being an  exclusive Afrikaner town, this town is controversial and is often seen as a remnant of apartheid"}, "filled": false, "gsrank": 297, "pub_url": "https://www.scielo.org.za/scielo.php?pid=S2415-05252023000100001&script=sci_arttext", "author_id": ["UD1iHp4AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:epu87Pb1bCEJ:scholar.google.com/&output=cite&scirp=296&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D290%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=epu87Pb1bCEJ&ei=OLWsaJm1KI6IieoP0sKRuAk&json=", "num_citations": 1, "citedby_url": "/scholar?cites=2408570341618916218&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:epu87Pb1bCEJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.scielo.org.za/pdf/comm/v28/01.pdf"}}, {"title": "College Student's Distrust in Hard News and Exposure to Fake News During the COVID-19 Pandemic.", "year": "2022", "pdf_data": "5DOI: 10.24193/jmr.42.1\nPublished First Online: March 2022College Student\u2019s Distrust in Hard News\nand Exposure to Fake NewsDuring the COVID-19 Pandemic\nChrysalis L. WRIGHT\n(corresponding author)\nDepartment of PsychologyUniversity of Central Florida, United States of AmericaE-mail: Chrysalis.Wright@ucf.edu.\nKwame GATLIN\nUniversity of Central Florida, United States of America\nRenee RIVERA\nUniversity of Central Florida, United States of America\nAbstract . This study was an experimental design that examined the effects of hard \nnews and fake news related to the COVID-19 pandemic on participants levels of COVID-19 related knowledge, attitudes, anxiety, and intent to engage in protective measures to prevent the spread of the virus. W e also examined sociodemographic fac-tors (race, age, biological sex, political affiliation, RW A, social class) that were hy-pothesized to be directly related to COVID-19 related outcomes. Participants included 327 college students from a large southeastern public university in the United States who were primed with either fake news, hard news, or no news prior to completing an online questionnaire. W e found differences in COVID-19 related outcomes based on experimental condition, but not in the predicted direction. Participants in the fake news condition had higher levels of COVID-19 knowledge, more positive attitudes related to the pandemic, and reported a higher intent to engage in protective meas-ures. Participants in the hard news condition reported lower levels of COVID-19 \nJournal of Media Research,\nVol. 15, Issue 1(42) / 2022, pp. 5\u201328\n\n6knowledge, an increase in anxiety, and less intent to engage in protective meas-\nures. W e also found a direct effect on COVID-19 related outcomes based on so-ciodemographic factors, particularly political orientation and RW A. Results are discussed specific to the college student population and should be helpful to those \ninvolved in policy making regarding social media, fake news, public health, and \nCOVID-19 health recommendations for this population.\nKeywords:  Fake news; Hard news; COVID-19; Coronavirus; Distrust in \nmedia.\nAs the world continues to confront the COVID-19 pandemic, a renewed emphasis \nhas been placed on knowledge and fact dissemination to address the novel information that is constantly being published. While much of this information is credible knowl-edge originating from medical professionals and scientific research, information from less credible sources that is not based on science is also prevalent. The concept of fake news became a major issue during the 2016 American Presidential election and has since grown to encompass other facets of life, such as public health. With the emergence of the novel coronavirus and the role the Internet has played in the pandemic, the level and scope of fake news has increased. This expansion of the reach of fake news has begun to produce sizable effects on the public health sector as many Americans no longer know what to believe. In addition to fake news, there is also concern regarding a general dis-trust in the media profession, government, and mainstream news as consumers search for answers to the questions they have regarding the pandemic. Hard news media specifi-cally frames the information that is presented to consumers. This includes news broad-casts, political information, as well as debates (Estrada et al. , 2016). Because the majority \nof consumers rely on hard news to get up to date and current information, the framing of hard news can impact the opinions and attitudes of consumers (Estrada et al. , 2016; \nMcKeever et al. , 2012; Steinberg, 2004).\nThe current study is unique in that we examined both fake news and hard news re-\nlated to the COVID-19 pandemic. W e specifically focused on the effects of both types of COVID-19 related information among college students considering reports of this popu-lation not taking the pandemic seriously and refusing the engage in preventative measures to help curb the spread of the virus, which may have put older people at an increased risk of infection (Scouten, 2020). Our results should be beneficial to those involved in policy making regarding social media, fake news, public health, and COVID-19 health recom-mendations for this population.\n7Distrust in Hard News\nThe way in which hard news frames, or presents, information to consumers is gener-\nally biased along partisan lines ( Newman et al. , 2018; Robison & Mullinix, 2016). Hard \nnews framing has been documented for a variety of news topics, including the MeT oo movement (Cuklanz, 2020), supreme court decisions (Clawson et al. , 2003), climate \nchange (Harrison et al. , 2020), immigration (Fryberg et al. , 2012; Timberlake & Williams, \n2012) as well as health-related information, such as health disparities (LaPoe et al. , 2021; \nNagler et al. , 2016), diet and its impact on health (Morris et al. , 2016), the Flint water crisis \n(Congdon et al. , 2020), and the current pandemic (Alshahrani, 2021; Chung et al. , 2021; \nHubner, 2021; Lukacovic, 2020; Myers, 2021; Nwakpu et al. , 2020; Ogola, 2020). While \nsome research has found no effect of media framing on consumer opinions and behaviors (Myers, 2021), other research has found that news media framing can have a direct impact on consumers, altering opinions, leading to attitude polarization, and even impacting be-haviors ( Han & Yzer, 2020). \nThe framing in hard news may be part of the reason why there is a rising, widespread \ndistrust in mainstream news  media ( Citrin & Stoker, 2018; Lee & Hosam, 2020; Newman et al. , 2018; Reese, 2019). In fact, three out of four Americans report being skeptical about \nthe information presented to them via hard news (Lee & Hosam, 2020).  Additionally, the increase of alternative news sources (e.g., social media) has been associated with an in-creased tendency to question information that is presented to consumers via hard news (Fletcher & Park, 2017; T sfati & Capella, 2003). The changing landscape of social media use and news consumption is directly related to the growing distrust in mainstream media (Flanagin & Metzger, 2017). Research has documented four dimensions of media trust. These include journalists covering relevant topics, focusing on facts, presenting accurate information, and providing clear commentary on the topic (Kohring & Matthes, 2007). Media framing and bias in reporting is in direct opposition to these key aspects of media trust, likely resulting in distrust. \nFurthermore, the current social, political, and economic climate can directly impact \ntrust in mainstream news media (Blobaum, 2016; T sfati & Ariely, 2014), alongside soci-odemographic characteristics of consumers (T sfati & Cohen, 2012).  Generally, sociode-mographic characteristics of interest tend to be race and ethnicity, age, biological sex, and socioeconomic indicators (Freimuth et al. , 2014; Lewis, 1981; Wilkins, 1995).  Political \norientation also plays a key role in that Democrats generally report higher levels of media trust compared to Republicans ( Jones, 2004; Newman et al. , 2018; T sfati & Cohen, 2012). \nAlongside political orientation is right-wing authoritarianism (R W A), which has recently increased in the U.S. (Eckhardt, 1991; Federico et al. , 2011; Havercroft & Murphy, 2018). \nR W A is related to an \u201cus versus them\u201d mindset (Lockwood, 2018; McCarthy, 2019). R W A is also associated with ideals related to the inevitability of social inequality, striving for su-periority over others, resistance to change, and conservatism, as well as authoritarian sub-mission and aggression ( Altemeyer, 1981; 1998; Duckitt, 2001; Jost et al. , 2003; Sibley & \nDuckitt, 2008; W eber & Federico, 2007). \n8Additionally, direct involvement with the topics and issues being covered by hard news, \nsuch as the COVID-19 pandemic, generally reduces consumers trust in media coverage (Gunther & Lasorsa, 1986). This growing distrust of hard news could lead to negative ef-fects among consumers, such as a decrease in trusting or believing science (Pechar et al. , \n2018), increased political polarization (Ladd, 2005, 2010), and changing aspects of what is considered truth, fact, and reality (Ariely, 2015; Gronke & Cook, 2007; Hanitzch et al. , \n2018).  Distrust in hard news can also lead to a lack of cooperation regarding recommenda-tions made by the media (Freimuth et al. , 2014; T sfati & Cohen, 2012), such as preventa-\ntive measures to help reduce the spread of COVID-19 during the pandemic. Essentially, if consumers do not trust hard news and view hard news as fake, they then turn to other avenues to seek news information, such as the internet and social media. This notion is consistent with the findings of Park and colleagues (2020), who found that an increase in social media use for consuming news information was related to an increase in distrust for hard news and that this relationship was notable across the globe. \nBelieving Fake News \nAs consumers turn more to social media for news information, it is important to un-\nderstand the risks involved in obtaining news information from less than credible sources. With this enters issues related to fake news. For instance, fake news can travel faster and spread farther when compared to hard news (Ball, 2018). Fake news is often shared among consumers unknowingly and can lead to a reliance on falsehoods and a general distrust in facts and science (Jang & Kim, 2018; Rapp & Salovich, 2018). Fake news is generally nega-tively charged and emotional, increasing the likelihood that consumers will remember the false information (Porter et al. , 2008). Fake news can have a direct effect on the behavior \nof consumers in that consumers are unaware of the intent at behavioral manipulation, its source, content, and impact on their behavior (Bastick, 2021).\nThe recent research into fake news as its own topic of study has been intertwined with \nresearch on the Internet, as this channel of information distribution has given many people a larger platform for spreading uncorroborated information. Research on rumor sharing also provides insight into this realm as it analyzes many of the same concepts. For instance, health rumors are generally considered to be unverified information, lacking a secure standard of supporting evidence. These rumors often spread because they resolve feelings of uncertainty during health crises or alert people to supposed threats. Moreover, people may feel they are helping the populous by disseminating this information in the interest of warning others about the supposed danger (Zhang et al. , 2015). \nSome individuals, in the interest of neutrality, share information they do not necessarily \ntrust with their online social networks (Chua et al. , 2016). It is this relationship with social \nmedia that is said to be the most significant factor in the rise of fake news during the previ-ous decade. Research on inattention has shown that people are more likely to be distracted from accuracy by the more fundamental aspects of social media, such as social validation, \n9reinforcement, and content where accuracy is irrelevant (e.g., animal photos). This may \nlead consumers to habituate to a lower level of accuracy consideration when engaging with social media content (Pennycook et al. , 2020). \nFake news surrounding health usually attempts to provide the public with knowledge \nabout certain conditions or diseases, but it can also influence health behaviors among indi-viduals, especially if it is negative or fear based (Chua & Banerjee, 2018). During the SARS epidemic in China, various rumors concerning the disease spread uncontrollably as the government was initially slow to release information regarding the virus. Researchers T ai and Sun (2011) found a direct correlation between infection rate and rumor incidences, as the higher levels of virus infections created a more favorable rumor environment. They concluded that this environment was also caused by the delayed distribution of informa-tion from the government as people attempted to make sense of the public health crisis that was unfolding before them. \nAllington and colleagues (2020) found a positive correlation between holding COVID-19 \nconspiracy beliefs and the use of social media, stating that COVID-19 conspiracy beliefs are more likely to be held by younger people and that older people may be more likely to reject these conspiracy beliefs because they engage with broadcast media more often. This may be one reason why the emerging adult population and college students more generally have not taken the COVID-19 pandemic seriously, refusing to follow preventative measures recom-mended by the CDC to help curb the spread of the virus (Scouten, 2020). \nCollege Students and News Consumption\nCollege aged consumers are avid social media users who obtain information related \nto current events and news from social media platforms more than hard news sources (Evanson & Sponsel, 2019; Pew Research Center, 2019; Silveira & Amaral, 2018). During the COVID-19 pandemic, college age consumers were most likely to obtain news informa-tion from Reddit, Instagram, T witter, Y ouTube, LinkedIn, and Facebook (Pew Research Center, 2021). This is problematic considering that those who obtain their news from so-cial media are not as likely to be aware or up to date with current events and are more likely to come across fake news (Allcott & Gentzkow, 2017; Pew Research Center, 2020). Some research has indicated that fake news may have a stronger effect on consumers compared to hard news and may be difficult for some consumers (e.g., college students) to accurately identify (Herrero-Diz et al. , 2019; Lazer et al. , 2020; Leeder, 2019; Wright et al. , 2019). \nCollege aged students have struggled to differentiate the accuracy of news sources, often \nviewing fake news content as more accurate than hard news and falling for fake news when they see it (Herrero-Diz et al. , 2019; Wineburg et al. , 2016). Additionally, some research \nhas suggested that female college students are more likely to believe fake news is accurate compared to male college students ( Herrero-Diz et al. , 2019). This is important to con-\nsider as younger consumers may lack the critical thinking abilities to effectively evaluate the information they encounter via social media (Auberry, 2018; Flanagin & Metzger, 2008; \n10Stanford History Education Group, 2016). Other studies, however, have found that col-\nlege  students do have the ability to critically evaluate the information they see online and to be able to differentiate between hard and fake news (Leeder, 2019). For instance, Silva and colleagues (2018) found that college students tend to evaluate the news they encounter by examining the sources referenced in the news information, their own previous experiences, as well as their own biased judgment. Evanson and Sponsel (2019) reported that younger consumers examined the textual aspects of online articles in order to determine its accuracy and reliability. \nOne of the main concerns regarding fake news is that the information cannot be instant-\nly verified and discredited by reputable sources (Ozturk et al. , 2015). However, Ozturk and \ncolleagues (2015) found that presenting counter information from reputable sources and other consumers along with the original rumor decreased the likelihood of it being shared. In contrast, Chua and Banerjee (2018) found that the presence of counter-rumors may not curb the effects of rumors, as counter-rumors lowered the intention to trust the rumors, but not necessarily the desire to share them. Comparatively, Zhang and colleagues (2015) found that an effective method to hinder the effects of fake news might be to place labels on fake news via social media and explaining the indicators of false information to the con-sumer. This is exactly what some social media giants did when they began putting labels on false information that was being shared via their platform (Guzman, 2020). \nIn line with this reasoning, during the Avian flu outbreak of 2004, the W orld Health \nOrganization\u2019s W estern Pacific Regional office conducted a rumor surveillance initiative. They were able to identify 40 rumors and verify nine of them as true. With this program, they were able to quickly inform the public regarding what information was true or false, preventing confusion and costly responses (Samaan et al. , 2005). Researchers Liu and \nHuang (2020) also emphasized the importance of personal fact-checking, since false infor-mation is more quickly discredited using this method and people who actively fact check information are less likely to fall victim to fake news. This is important because, when evaluating online health information, the heuristic cues people use vary depending on the subject matter and, because anyone can publish anything on the Internet, the reliability of these cues are reduced (Eastin, 2001). \nThe Current Study\n The current study was an experimental design that examined the effects of hard news \nand fake news related to the COVID-19 pandemic on participants levels of COVID-19 re-lated knowledge, attitudes, anxiety, and intent to engage in protective measures to prevent the spread of the virus. In our study, participants were assigned to one of three experimental conditions where they were primed with either fake news, hard news, or no news related to the COVID-19 pandemic. After viewing the prime (or no prime), participants answered a questionnaire regarding their COVID-19 knowledge, attitudes, and anxiety as well as their intent to engage in protective measures. W e also looked at common sociodemographic\n11variables ( i.e., race, age, biological sex, political affiliation, R W A, social class) and how these \nfactors related to the outcomes examined in the study. W e also specifically examined college students, considering that this population reportedly did not take the pandemic seriously and has refused to engage in preventative measures to help curb the spread of the virus (Scouten, 2020). The current study had two main hypotheses: \nHypothesis 1: W e hypothesized that participants would report differences in COVID-19 \nrelated outcomes based on experimental condition, with the fake news condition being as-sociated with negative COVID-19 related outcomes and the hard news condition being associated with positive COVID-19 related outcomes.\nHypothesis 2: W e hypothesized that the sociodemographic variables examined  (i.e., \nrace, age, biological sex, political affiliation, R W A, social class) would play a key role in the COVID-19 related outcomes. For instance, based on previous research, we hypothesized that younger participants would be more susceptible to COVID-19 related fake news com-pared to older participants (Allington et al. , 2020). W e also hypothesized that female partic-\nipants would be more susceptible to COVID-19 related fake news compared to male par-ticipants (Herrero-Diz et al. , 2019) and that participants who identified as Republican and \nthose with increased levels of R W A would be more susceptible to COVID-19 related fake news compared to participants from other political backgrounds (Jones, 2004; Newman et \nal., 2018; T sfati & Cohen, 2012). \nMethod\nParticipants and procedure\n Our participants included 327 college students from a large southeastern public re-\nsearch university. Participants were recruited for our study through their undergraduate psychology courses. Participants received class credit for completing the roughly 63-min online questionnaire.  Most participants were female ( n = 195, 59.6%) and identified as \nWhite ( n = 174, 53.2%), with remaining participants identifying as Hispanic ( n = 66, \n20.2%), African American ( n = 44, 12.5%), Asian or Pacific Islander ( n = 28, 8.6%) and \n\u201cother\u201d ( n = 18, 5.5%). All participants were between the ages of 18 and 57, with a M age \nof 20.75 ( SD = 5.04). Participants reported their political affiliation as undecided ( n = 129, \n39.4%), Democrat ( n = 125, 38.2%), and Republican ( n = 73, 22.3%). \n Participants were assigned to one of three experimental conditions where they were \nprimed with either fake news (condition 1), hard news (condition 2), or no news informa-tion (condition 3) related to COVID-19. Participants, however, were not asked to com-ment on, rate, or evaluate the news information they were primed with to ensure that they were not aware of the potential primed influence on their attitudes in this study (Higgins, 1996; Loersch & Payne, 2014). \nParticipants in condition 1 ( n = 134; 58 males and 76 females; 46.3% White, 20.1% \nHispanic, 14.2% African American, 10.4% Asian or Pacific Islander, 8.9% \u201cother\u201d; mean \n12age was 21.11, SD = 5.87; 38% Democrat, 36.6% undecided, 25.4% Republican) were \npresented with fake news about COVID-19. News information was obtained from The W aking Times, Global Research, and The W ashington Standard. Both The W aking Times and Global Research have been deemed as fake news, posting conspiracy beliefs and pseu-doscience, with extremely low factual reporting by Media Bias/Fact Check (2020a; 2021a). Additionally, Health Guard, a temporary service provided by News Guard, has rated The W ashington Standard as 25/100 for repeatedly publishing false content. This includes the use of deceptive headlines, limited differentiation of news, opinion, and presentation of information. Health Guard also found a lack of correcting errors in reporting (2021).\nParticipants in condition 2 ( n = 83; 31 males and 52 females; 60.2% White, 16.9% \nAfrican American, 14.5% Hispanic, 7.2% Asian or Pacific Islander, 8.4% \u201cother\u201d; mean age was 20.41, SD = 4.38; 41% Democrat, 38.6% undecided, 20.5% Republican) were pre-\nsented with hard news related to COVID-19 from CNN, the CDC, and the Associated Press. CNN generally has a left leaning reporting bias with mixed factual reporting (Media Bias/Fact Check 2021b). The Associated Press is labeled as least biased by Media Bias/Fact Check. This indicated that The Associated Press contains minimal bias, very few loaded words, is factual, and uses sourced information (2021c). According to the detailed report, the Associated Press has high credibility and very high factual reporting. Additionally, ac-cording to Media Bias/Fact Check, the CDC has very high factual, scientific reporting (2020b). \nParticipants in condition 3 ( n = 110; 43 males and 67; 56.4% White, 24.5% Hispanic, \n7.3% African American, 7.3% Asian or Pacific Islander, 4.5% \u201cother\u201d; mean age was 20.56,  \nSD = 4.29; 43.6% undecided, 36.4% Democrat, 20% Republican) were not presented with \nany news related to COVID-19. After exposure to the news information, participants an-swered items related to their attitudes and behaviors related to COVID-19 in an online questionnaire. \nMeasures\nCOVID-19 Knowledge . Participants answered a total of 23 items (Al-Hanawi et al. , \n2020) to assess their current knowledge related to COVID-19. Sample items included \u201cThe main clinical symptoms of COVID-19 are fever, fatigue, dry cough, myalgia, and shortness of breath\u201d and \u201cNot all people with COVID-19 have severe cases.\u201d Items were scored on a 4-point Likert scale with 1 being strongly disagree to 4 strongly agree. Four items were \nreverse scored. Items were averaged to derive at a total score for COVID-19 knowledge that was used in analysis. Alpha reliability for the current study was .58.\nCOVID-19 Attitudes. Participants answered six items about their attitudes related \nto COVID-19 (Al-Hanawi et al. , 2020). Sample items included \u201cIt is important to keep \ndistance from others, to avoid spreading COVID-19\u201d and \u201cCOVID-19 will eventually be successfully controlled.\u201d Items were scored on a 4-point Likert scale with 1 being strongly \ndisagree to 4 strongly agree. Items were averaged to derive at a total score for COVID-19 \nattitudes that was used in analysis. Alpha reliability for the current study was .71. \n13COVID-19 Protective Measures. Participants answered six questions, modi-\nfied from Al-Hanawi and colleagues (2020), related to their current practices related to COVID-19. Sample items included \u201cI plan to go to a crowded place\u201d and \u201cI plan to wear a mask when leaving my home.\u201d Items were scored on a 4-point Likert scale with 1 being strongly disagree to 4 strongly agree. Items were averaged to derive at a total score for pro-\ntective measures related to COVID-19 that was used in analysis. Alpha reliability for the current study was .85. \nCOVID-19 Anxiety. Participants answered 18 questions, modified from Roy and col-\nleagues (2020), to assess their current anxiety related to COVID-19. Sample items included \u201cI avoid social contact\u201d and \u201cI talk to my friends about COVID-19.\u201d Items were scored on a 4-point Likert scale with 1 being strongly disagree to 4 strongly agree. Items were averaged \nto derive at a total score for COVID-19 anxiety that was used in analysis. Alpha reliability for the current study was .91. \nRight-Wing Authoritarianism. R W A was assessed using the short version of the \nR W A scale (Rattazzi et al. , 2007). The scale includes ten items that are scored on a 5-point \nLikert scale with 1 being strongly disagree  and 5 being strongly agree . A sample item is \u201cOur \ncountry desperately needs a mighty leader who will do what has to be done to destroy the radical new ways and sinfulness that are ruining us.\u201d Items were summed to derive at a total score that was used in analyses. Alpha reliability for the scale was .93.\nSocial Class.  Social class was measured as a continuous variable by using measures \nof parental education, income, and occupation and self-reported social class identity (see Rubin & Wright, 2017).  Items were converted to z scores (\u03b1 = .80) and then averaged to \nderive at a total mean score that was used in analyses. \nDemographic Questionnaire. Participants were asked four questions related to their \nage, race and ethnicity, biological sex, and political party affiliation. Due to sample size limitations, race and ethnicity was dummy coded as 1 = White and 0 = non-White. \nAnalysis Plan\nW e initially assessed the reliability of scales, distributional characteristics, missing data, \nand intercorrelations of measures. Because missing data were found to be minimal (< 5%), a simple mean substitution method was used (Kline, 2005), which allows for complete case analyses, maintains the statistical power of tests, and considers the reason for missing data (T wala, 2009). This method is a good representation of the original data as long as the miss-ing data are less than 20% (Downey & King, 1998).\nAnalyses relevant to the our hypotheses included: (a) descriptive statistics regarding \nparticipants knowledge, attitudes, anxiety, and intent to engage in protective measures to prevent the spread of the virus; (b) intercorrelations of study variables; (c) a multivariate analysis of covariance (MANCOV A) to determine if there were significant differences in participants knowledge, attitudes, anxiety, and intent to engage in protective measures based on experimental condition while controlling for sociodemographic variables (e.g., race, age, biological sex, political affiliation, R W A, social class); and (d) linear regression \n14analyses to determine factors related to participants\u2019 susceptibility to believing news con-\ntent and COVID-19 knowledge, attitudes, anxiety, and intent to engage in protective measures (e.g., race, age, biological sex, political affiliation, R W A, social class).\nResults\nParticipants COVID-19 Knowledge, Attitudes,\nProtective Measures, and Anxiety\nDescriptive statistics for participants\u2019 knowledge, attitudes, anxiety, and intent to en-\ngage in protective measures related to COVID-19 can be found in T able 1. Overall, partici-pants reported moderate levels of COVID-19 related knowledge, attitudes, and protective measures and reduced levels of COVID-19 anxiety.  \nTable 1.  Participants COVID-19 Knowledge, Attitude, Protective Measures, and Anxiety\nMS D\nCOVID-19 Knowledge 3.29 .33COVID-19 Attitudes 3.36 .49COVID-19 Protective Measures 3.33 .70\nCOVID-19 Anxiety 2.57 .62\nIntercorrelations of Outcome Variables\nIntercorrelations of participant knowledge, attitudes, anxiety, and intent to engage in \nprotective measures related to COVID-19, R W A, race and ethnicity, age, political affilia-tion, biological sex, and social class can be found in T able 2. All of the outcome variables were positively correlated with each other, which was to be expected. Results also indicated that R W A was associated with decreased COVID-19 knowledge ( r = -.13, p = .05) and \ndecreased protective measures related to COVID-19 ( r = -.17, p = .01). There were also \nsignificant findings related to political orientation, in addition to the political orientations being negatively correlated with each other ( r = -.41, p = .01). Being a Republican was \nnegatively correlated with all of the outcome variables. Being a Republican was positively correlated with being White ( r = .25, p = .01) and increased levels of R W A ( r = .21, p = .01). \nBeing a Democrat was positively correlated with all of the outcome variables as well as be-ing older ( r = .16, p = .01). Being a Democrat was negatively correlated with increased levels \nof R W A ( r = -.36, p = .01) and identifying as White ( r = -.24, p = .01). \nSocial class was found to negatively correlate with COVID-19 knowledge ( r = -.19, p = \n.05), protective measures related to COVID-19 ( r = -.13, p = .05), and COVID-19 anxiety \n(r = -.12, p = .05). Social class was also significantly correlated with participant race, in that \nWhite participants were more likely to have a higher social class compared to participants from other racial and ethnic backgrounds ( r = .12, p = .05). Social class was also signifi-\n15cantly correlated with political orientation. Republicans were more likely to have a higher \nsocial class ( r = .16, p = .01) compared to Democrats ( r = -.24, p = .01). Social class was also \nnegatively correlated with age ( r = -.17, p = .01).\nParticipant age was found to be positively correlated with both protective measures re-\nlated to COVID-19 ( r = .12, p = .05) and COVID-19 anxiety ( r = .17, p = .01). There were \nalso significant correlations related to biological sex in that female participant had higher rates of COVID-19 knowledge ( r = .23, p = .01), higher COVID-19 attitudes ( r = .23,\np = .05), more intent to engage in protective measures related to COVID-19 ( r = .39,\np = .01), and higher levels of COVID-19 anxiety ( r = .41, p = .01). Male participants were \nmore likely to have increased levels of R W A ( r = -.12, p = .05) as well as be Republican\n(r = -.19, p = .01). Female participants were more likely to be Democrat ( r = .24, p = .01). \nTable 2.  Intercorrelations of Study Variables\n1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11.\n1. COVID-19 Knowledge .60**.59**.31**-.13*-.02 .06 -.19**.22**.23**-.19*\n2. COVID-19 Attitudes .56**.40**.07 -.05 .06 -.22**.19**.23**-.10\n3. COVID-19 Protective \nMeasures.63**-.17**-.09 .12*-.32**.30**.39**-.13*\n4. COVID-19 Anxiety .01 -.04 .17**-.35**.34**.41**-.12*\n5. RWA .01 -.01 .21**-.36**-.12*.05\n6. White -.01 .25**-.24**-.08 .12*\n7. Age -.08 .16**.04 -.17**\n8. Republican -.41**-.19**.16**\n9. Democrat .24**-.24**\n10. Biological Sex -.05\n11. Social Class\n* p < .05, ** p < .01\nExperimental Condition\nA multivariate analysis of covariance (MANCOV A) was conducted to determine if \nthere were significant differences in participants\u2019  knowledge, attitudes, anxiety, and intent to engage in protective measures related to COVID-19 based on experimental condition (i.e., fake news, hard news, no news), while controlling for sociodemographic variables (i.e., race, age, biological sex, political affiliation, R W A, social class). Our results indicat-ed significant differences based on experimental conditions for COVID-19 knowledge, F (2, 313) = 3.53, p = .03, \u03b7\n2 = .02, and protective measures related to COVID-19,\nF (2, 313) = 3.42, p = .03, \u03b72 = .02, but not COVID-19 attitudes, F (2, 313) = .54, p = .58, \nor COVID-19 anxiety, F (2, 313) = 1.26, p = .28. Specifically, we found that participants ex-\nposed to hard news had lower levels of COVID-19 knowledge, lower levels of COVID-19 \nanxiety, and more negative attitudes related to COVID-19 compared to those exposed to \n16fake news and no news. W e also found that participants exposed to fake news reported \nmore intent to engage in protective measures related to COVID-19 when compared to those exposed to hard news and no news. Descriptive statistics based on experimental con-dition can be found in T able 3.\nTable 3.  Experimental Condition\nMS D\nCOVID-19 KnowledgeFake News 3.36 .29\nHard News 3.27 .38No News 3.27 .33\nCOVID-19 AttitudesFake News 3.39 .49\nHard News 3.32 .50\nNo News 3.35 .49\nCOVID-19 Protective MeasuresFake News 3.40 .70\nHard News 3.29 .71No News 3.27 .71\nCOVID-19 AnxietyFake News 2.61 .60\nHard News 2.50 .65No News 2.58 .63\nW e also found significant differences for COVID-19 knowledge based on biological \nsex, F (1, 313) = 16.07, p = .00, \u03b72 = .05, R W A, F (1, 313) = 5.25, p = .02, \u03b72 = .02, and \nsocial class, F (1, 313) = 4.26, p = .04, \u03b72 = .01. Female participants reported higher levels \nof COVID-19 knowledge ( M = 3.36, SD = .27) compared to male participants ( M = 3.20,\nSD = .38). COVID-19 knowledge also decreased along with participants social class\n(r = -.13, p = .02) and R W A ( r = -.13, p = .02). \nSignificant difference for COVID-19 related attitudes were found for biological sex,\nF (1, 313) = 17.95, p = .00, \u03b72 = .05, and political orientation, F (1, 313) = 3.49, p = .06,\n\u03b72 = .01. Female participants reported higher levels of COVID-19 related attitudes\n(M = 3.45, SD = .34) compared to male participants ( M = 3.22, SD = .63). Democrat par-\nticipants reported the highest levels of COVID-19 related attitudes ( M = 3.47, SD = .30)\ncompared to Republican ( M = 3.16, SD = .62) and undecided participants ( M = 3.36,\nSD = .53).\nFor intent to engage in protective measures related to COVID-19, we found significant \ndifferences based on biological sex, F (1, 313) = 50.73, p = .00, \u03b72 = .14, political orienta-\ntion, F (1, 313) = 7.44, p = .007, \u03b72 = .02, and R W A, F (1, 313) = 7.09, p = .008, \u03b72 = .02.\nW e found that female participants had higher scores ( M = 3.55, SD = .52) compared to \nmale participants ( M = 3.00, SD = .80). Democrat participants reported higher scores\n(M = 3.60, SD = .48) compared to Republican ( M = 2.91, SD = .87) and undecided partic-\nipants ( M = 3.30, SD = .66). W e also found that the intent to engage in protective measures \nrelated to COVID-19 were negatively correlated with participants R W A ( r = -.17, p = .003). \n17Significant differences for COVID-19 anxiety were found for participant age,\nF (1, 313) = 7.39, p = .007, \u03b72 = .02, biological sex, F (1, 313) = 66.32, p = .00, \u03b72 = .18, and \npolitical orientation, F (1, 313) = 9.98, p = .002, \u03b72 = .03. Results indicated that female \nparticipants reported higher levels of COVID-19 anxiety ( M = 2.78, SD = .54) compared \nto male participants ( M = 2.26, SD = .61). Democrat participants also reported higher \nlevels ( M = 2.84, SD = .50) compared to Republican ( M = 2.16, SD = .65) and undecided \nparticipants ( M = 2.55, SD = .58). Results also indicated that COVID-19 anxiety was posi-\ntively correlated with age ( r = .17, p = .002). \n Factors Related to Participants Attitudes \nLinear regression analyses were conducted to determine the sociodemographic factors \n(i.e., race, age, biological sex, political affiliation, R W A, social class) that may be associated with the COVID-19 related outcomes based on experimental condition (i.e., fake news, hard news, no news). Considering the results of the MANCOV A, this was conducted for COVID-19 knowledge and protective measures related to COVID-19. Results can be found in T able 4. \nFor participants who were exposed to fake news, F (7, 125) = 2.18, p = .04, R\n2 = .11, \nand no news, F (7, 102) = 3.59, p = .002, R2 = .20, the regression model was significant for \nCOVID-19 knowledge. For participants exposed to fake news, biological sex was the only significant predictor. In this group, male participants reported less COVID-19 knowledge (M = 3.26, SD = .31) compared to female participants ( M = 3.39, SD = .26). For par-\nticipants in the no news experimental condition, biological sex and political orientation were significant predictors. Female participants had higher levels of COVID-19 knowledge(M = 3.36, SD = .25) compared to male participants ( M = 3.14, SD = .40). Democrat \nparticipants also reported higher levels of COVID-19 knowledge ( M = 3.41, SD = .24) \ncompared to participants with other political orientations. \nTable 4.  Regression Analyses for COVID-19 Knowledge  \nFake News Hard News No News\nWhite .08 .07 -.01\nAge .01 .04 -.04\nBiological Sex .19**.13 .27*\nRepublican .02 -.20 -.18#\nDemocrat .13 -.03 .20#\nRWA -.08 -.04 -.04\nSocial Class -.15 -.16 .07\n* p < .01, ** p < .05, # p < .10\nFor participants who were exposed to fake news, F (7, 123) = 6.08, p = .001, R2 = .26, \nhard news, F (7, 75) = 7.90, p = .001, R2 = .42, and no news, F (7, 102) = 5.08, p = .001, \nR2 = .26, the regression model was significant for intent to engage in protective measures \n18related to COVID-19. For participants exposed to fake news, biological sex was the only \nsignificant predictor for protective measures for COVID-19, with female participants having higher scores ( M = 3.63, SD = .52) compared to male participants ( M = 3.10,\nSD = .79). For those exposed to hard news, participant race, biological sex, political orienta-\ntion, and R W A were significant predictors. Here, non-White participants reported high-er scores for protective measures related to COVID-19 ( M = 3.32, SD = .73) compared \nto White participants ( M = 3.26, SD = .68). Female participants reported higher scores \n(M = 3.53, SD = .54) compared to male participants ( M = 2.91, SD = .79). Additionally, \nDemocrat participants reported higher scores ( M = 3.52, SD = .56) compared to those who \nwere Republican ( M = 2.62, SD = .84) or undecided ( M = 3.42, SD = .54). There was also \na negative correlation with protective measures related to COVID-19 and R W A ( r = -.34,\np = .002). For participants in the no news experimental condition, biological sex and po-\nlitical orientation were significant predictors. For participants exposed to no news, fe-male participants reported higher scores ( M = 3.49, SD = .52) compared to male partici-\npants ( M = 2.93, SD = .82). Additionally, Democrat participants reported higher levels\n(M = 3.55, SD = .56) compared to those who were Republican ( M = 2.90, SD = .88) or \nundecided ( M = 3.21, SD = .65). Results can be found in T able 5.\nTable 5.  Regression Analyses for Protective Measures for COVID-19\nFake News Hard News No News\nWhite -.13 .18#.07\nAge .10 .09 -.09Biological Sex .32\n*.30*.36*\nRepublican -.08 -.43*-.22**\nDemocrat .15 -.03 .07\nRWA .04 -.20**-.15\nSocial Class -.12 -.02 -.01\n* p < .01, ** p < .05, # p < .10\nDiscussion\nW e examined the effects of COVID-19 hard news and fake news on college students \nCOVID-19 knowledge, attitudes, and anxiety as well as their intent to engage in pro-tective measures to help prevent the spread of the virus using an experimental design. Specifically, we hypothesized that there would be differences in our COVID-19 related outcomes based on experimental condition, with exposure to hard news being related to positive COVID-19 related outcomes and exposure to fake news being related to negative COVID-19 related outcomes. W e controlled for sociodemographic variables (i.e., race, age, biological sex, political affiliation, R W A, social class) in an attempt to isolate the effects of \n19both hard and fake news. Finally, we also hypothesized that the sociodemographic variables \nexamined would have a direct impact on COVID-19 outcomes. \nGenerally, participants in this study reported moderate levels of COVID-19 related \nknowledge, attitudes, and intent to engage in protective measures to prevent the spread of the virus. Participants also reported reduced levels of COVID-19 anxiety. Considering that we focused on college students who may not have necessarily taken the pandemic se-\nriously (Scouten, 2020), these results were not surprising. Our results also indicated that the COVID-19 related outcomes examined were all significantly related to one another. Additionally, many of the sociodemographic variables examined in this study were signifi-cantly correlated with each other as well as our outcome variables. \nHard and Fake News\n Our hypothesis that there would be differences in COVID-19 related outcomes based \non experimental condition was supported by the data, although not in the direction that was predicted.  For instance, we found that participants exposed to fake news had higher levels of COVID-19 knowledge, more positive attitudes related to the pandemic, and re-ported a higher intent to engage in protective measures to help prevent the spread of the virus compared to participants in the other conditions (hard news, no news). While this is not exactly what was expected, it may be that the college students examined in our study have the ability to critically evaluate information that is presented to them, in this case fake news, and are able to identify fake news when they encounter it (Leeder, 2019), reduc-ing or even eliminating the potential negative effects. While not examined in this study, it could be that the college students examined here actively engaged in fact checking when encountering fake news in our study (Liu & Huang, 2020). Future research should further examine fake news and college student\u2019s ability to identify fake news, engage in fact check-ing and countering the information, and the ability to resist the effects of fake news. \n W e also found that participants primed with hard news related to COVID-19 reported \nlower levels of COVID-19 knowledge than those primed with fake news (but comparable to the control condition). Participants in this group also reported less intent to engage in protective measures for preventing the spread of COVID-19 and an increase in COVID-19 anxiety. Participants increased COVID-19 anxiety could be related to the framing effects of hard news, which may have put an emphasis on negative or fear-based reporting (Han & Yzer, 2020; Newman et al. , 2018; Robison & Mullinix, 2016). Even so, media fram-\ning appears to have a direct effect on distrust in hard news (Citrin & Stoker, 2018; Lee & Hosam, 2020; Newman et al. , 2018; Reese, 2019), which also appears to be evident here. \nThis potential distrust in hard news among our participants could be related to obtain-ing news information from alternative sources, such as social media (Flanagin & Metzger, 2017; Fletcher & Park, 2017; Park et al. , 2020; T sfati & Capella, 2003). Specific sociode-\nmographic factors of participants could also play a role here, such as political orientation and R W A (Freimuth et al. , 2014; Jones, 2004; Lewis, 1981; Newman et al. , 2018; T sfati \n20& Cohen, 2012; Wilkins, 1995). It could also be related to our participants direct involve-\nment and personal experiences during the COVID-19 pandemic, reducing their trust in how hard news covers what is taking place (Gunther & Lasorsa, 1986). Essentially, if par-ticipants have varying experiences compared to what is reported via hard news, they may view such reporting as false, discrediting hard news, and leading to a growing distrust in hard news media. This distrust in hard news then is related to reduced cooperation regard-ing recommendations made by the media (Freimuth et al. , 2014; T sfati & Cohen, 2012), \nwhich is evident in our participants lower levels of COVID-19 knowledge and less intent to engage in protective measures recommended by hard news for preventing the spread of COVID-19. \nDue to the experimental nature of the current study, we can theorize that exposure to \neither hard or fake news caused the COVID-19 related outcomes examined here. However, our results also highlight the important role of sociodemographic factors related to these outcomes, which may indicate that news exposure works in conjunction with already exist-ing factors. \nSociodemographic Factors\nW e found a direct impact of several sociodemographic factors on the COVID-19 re-\nlated outcomes examined in this study. For instance, biological sex was directly related to COVID-19 knowledge, attitudes, anxiety, and intent to engage in protective measures to prevent the spread of the virus. Female participants had higher scores in all of these areas compared to male participants. Age was also an important factor in that older participants reported higher levels of COVID-19 anxiety compared to younger participants. This alone could help explain why college students generally did not take the pandemic as seriously as other population groups and resisted engaging in efforts to reduce the spread of the virus (Scouten, 2020). For social class, we found that COVID-19 knowledge decreased with so-cial class. Both political orientation and R W A were also strong predictors. W e found that as R W A increased, COVID-19 knowledge as well as participants intent to engage in protective measures to curb the spread of the virus decreased. Also, Democrat participants reported higher levels of COVID-19 attitudes, had higher rates of COVID-19 anxiety, and reported more intent to engage in protective measures to help prevent the spread of the virus com-pared to Republican and undecided participants. These results could directly be related to political orientation and media trust. Historically, Democrats report higher levels of media trust compared to Republicans (Jones, 2004; Newman et al. , 2018; T sfati & Cohen, 2012), \nwhich appears to be a main factor impacting our results related to COVID-19 outcomes regardless of news condition (i.e., fake news, hard news, no news). \nW e also looked at these specific sociodemographic factors (i.e., race, age, biological sex, \npolitical affiliation, R W A, social class) to determine how they related to exposure to fake news, hard news, and no news for COVID-19 knowledge and protective measures to re-duce the spread of the virus. W e specifically examined these outcomes considering the dif-\n21ferences based on experimental condition (i.e., fake news, hard news, no news) that we \nfound for these outcomes. \nW e found that for participants exposed to fake news, biological sex was the only signifi-\ncant sociodemographic predictor that was related to their COVID-19 knowledge and pre-ventative measures related to COVID-19. Here, female participants exposed to fake news had more knowledge and a higher intent to engage in preventative measures to prevent the \nspread of the virus compared to male participants. This could indicate a differentiation based on biological sex related to susceptibility to fake news, but further research is needed in this area. \nFor those in our hard news condition, several of the sociodemographic predictors were \nsignificant, specifically being White, biological sex, political orientation, and R W A. Here, White participants as well as female participants reported more intent to engage in protec-tive measures after being exposed to hard news, compared to the other participants. Also, participants who were Republican or had higher levels of R W A were less likely to express intent to engage in protective measures after being exposed to hard news. These results em-phasize the relationship between political orientation (i.e., Republican) and media distrust (Jones, 2004; Newman et al. , 2018; T sfati & Cohen, 2012). \nInterestingly, we found significant sociodemographic predictors in the no news con-\ndition as well. Both biological sex and political orientation were associated with partici-pants knowledge related to COVID-19. Female participants had higher knowledge com-pared to male participants. Also, Democrat participants had higher knowledge related to COVID-19 compared to Republican participants. W e found a similar trend for the intent to engage in preventative measures in that females were more likely to express this intent and Republican participants were less likely. These results highlight the importance of these sociodemographic factors in relation to COVID-19 outcomes. Our results may indi-cate that exposure to hard or fake news, by itself, is not likely to have more of an impact on consumer attitudes related to COVID-19 than these sociodemographic predictors.\nLimitations and Implications for Future Research\nOur results should be considered in light of the limitations of our study. In our study, \nparticipants who were not exposed to any news information reported similar knowledge, attitudes, and protective measures as participants exposed to hard news. They also reported similar levels of anxiety compared to those exposed to fake news. This may indicate that our control condition held similar attitudes related to COVID-19 but did not have those attitudes primed in our study (see Ramasubramanian, 2007). \nAlso, the effect of news information on COVID-19 related knowledge and protective \nmeasures was significant but rather small (.02), which is common in experimental designs (Cohen, 1998; Field, 2009). Even so, even small effects can have real-world implications (McCartney & Rosenthal, 2000), such as those seen during the COVID-19 pandemic. Additionally, due to the small sample size in our study, some of our results were marginally \n22significant. Having a larger sample size for each condition would likely bring those results \nto a significant level, which is why they were still included and discussed.\nIt should be noted that the priming effect in our study may have had a short-term (i.e., \nduration of the study), rather than a long-term, effect on the participant attitudes exam-ined (W entura & Rothermund, 2014). This is evident due to the strong predictor of the sociodemographic variables examined here, indicating that these factors play a larger role in COVID-19 attitudes than news exposure itself. This also may mean that exposure to hard or fake news, by itself, is not likely to have more of an impact on consumer attitudes related to COVID-19 than these predictors. \nFinally, we focused specifically on the college student population, hindering the gener-\nalizability of our results to outside of that population. Our study was conducted online, due to operational changes at our institution resulting from the COVID-19 pandemic. Also, the alpha reliability for our COVID-19 knowledge was smaller than desired in our current study (Katz & Hass, 1988). Further research in this area should consider assess-ing participants attitudes prior to media exposure and consider that generally consumers engage in media self-selection based on their personal histories, interests, experiences, opin-ions, and attitudes. \nReferences\n1. Al-Hanawi, M., Angawi, K., Alshareff, N., Qattan, A., Helmy, H., Abudawood, Y., Alqurashi, \nM., Kattan, W ., Kadasah, N., Chirwa, G., & Alsharqi, O. (2020). Knowledge, attitude and practice toward COVID-19 among the public in the Kingdom of Saudi Arabia: A cross-sectional study, Frontiers in Public Health. https://doi.org/10.3389/fpubh.2020.00217\n2. Allington, D., Duffy, B., W essely, S., Dhavan, N., & Rubin, J. (2020). Health-protective \nbehaviour, social media usage and conspiracy belief during the covid-19 public health emergency. Psychological Medicine, 1 . https://doi.org/10.1017/S003329172000224X\n3. Alshahrani, A. (2021). A frame analysis of the language used by eight US media to describe \nthe role of China and Chinese in spreading COVID-19 during late January to early June 2020. Journal of Language and Linguistic Studies, 17 , 1129\u20131140.\n4. Allcott, H., & Gentzkow, M. (2017). Social media and fake news in the 2016 election. Journal of \nEconomic Perspectives, 31 (2), 211\u2013236. https://doi.org/10.1257/jep.31.2.211\n5. Altemeyer, B. (1981). Right-wing authoritarianism . Winnipeg: University of Manitoba Press.\n6. Altemeyer, B. (1998). The other \u201cauthoritarian personality.\u201d Advances in Experimental Social \nPsychology, 30,  47\u201392.\n7. Ariely, G. (2015). T rusting the press and political trust: A conditional relationship. Journal of \nElections, Public Opinion and Parties, 25 (3), 351\u2013367 http://doi.org/10.1080/17457289.2014.9\n97739\n8. Auberry, K. (2018). Increasing students\u2019 ability to identify fake news through information literacy education and content management systems. Reference Librarian, 59 (4), 179\u2013187. \nhttp://doi.org/10.1080/02763877.2018.1489935\n239. Ball, P. (2018, March 08). \u2018News\u2019 spreads faster and more widely when it\u2019s false. Nature. https://\ndoi.org/10.1038/d41586-018-02934-x\n10. Bastick, Z. (2021). W ould you notice if fake news changed your behavior? An experiment on the unconscious effects of disinformation. Computers in Human Behavior, 116 . https://doi.\norg/10.1016/j.chb.2020.106633\n11. Blobaum, B. (2016). Key factors in the process of trust. On the analysis of trust under digital conditions. In B. Blobaum (Ed.), Trust and communication in a digitized world  (pp. 3\u201325). \nNew Y ork, NY: Springer Berlin Heidelberg.\n12. Chua, A. Y. K., & Banerjee, S. (2018). Intentions to trust and share online health rumors: An experiment with medical professionals. Computers in Human Behavior, 87 , 1\u20139. https://doi.\norg/10.1016/j.chb.2018.05.021\n13. Chua A. Y. K., Banerjee, S., Guan, A. H., Xian, L. J., & Peng, P. (2016). Intention to trust and share health-related online rumors: Studying the role of risk propensity. 2016 SAI Computing \nConference (SAI), 1136\u20131139. http://doi.org/10.1109/SAI.2016.7556120\n14. Chung, A. Y., Jo, H., Lee, J., & Y ang, F. (2021). COVID-19 and the political framing of China, \nnationalism, and borders in the U.S. and South Korean news media. Sociological Perspectives, 1.  \nhttps://doi.org/10.1177/07311214211005484\n15. Citrin, J., & Stoker, L. (2018). Political trust in a cynical age. Annual Review of Political Science, \n21(1), 49\u201370. http://doi.org/10.1146/annurev-polisci-050316-092550\n16. Clawson, R. A., Strine IV , H., & W altenburg, E. (2003). Framing supreme court decisions: The \nmainstream versus the black press. Journal of Black Studies, 33 (6), 784\u2013800.\n17. Cohen, J. (1988). Statistical power analysis for the behavioral sciences  (2nd ed.). Erlbaum.\n18. Congdon Jr., M., Quang Ngo, & Y oung, E. (2020). Environmental injustice: Examining how \nthe New Y ork Times frames the Flint water crisis. Ohio Communication Journal, 58,  12\u201325.\n19. Cuklanz, L. (2020). Problematic news framing of #MeT oo. The Communication Review, 23 (4), \n251\u2013272. https://doi.org/10.1080/10714421.2020.1829302\n20. Downey, R. G., & King, C. (1998). Missing data in Likert ratings: A comparison of replace-ment methods. Journal of General Psychology, 125 , 175\u2013191. https://doi.org/10.1080/00221\n309809595542\n21. Duckitt, J. (2001). A dual-process cognitive-motivational theory of ideology and prejudice. Advances in Experimental Social Psychology, 33,  41\u2013113. https://doi.org/10.1016/S0065-260\n1(01)80004-6.\n22. Eastin, M. S. (2001). Credibility assessments of online health information: The effects of source expertise and knowledge of content. Journal of Computer-Mediated Communication, 6(4).  \n23. Eckhardt, W . (1991). Authoritarianism. Political Psychology, 21 (1), 97\u2013124.\n24. Estrada, E. P., Ebert, K., & Lore, M. H. (2016). Apathy and antipathy: Media coverage of \nrestrictive immigration legislation and the maintenance of symbolic boundaries. Sociological \nForum, 31 (3), 555\u2013576. 12262 https://doi.org/10.1111/socf\n25. Evanson, C., & Sponsel, J. (2019). From syndication to misinformation: How undergraduate \nstudents engage with and evaluate digital news. Communications in Information Literacy, 13,  \n228\u2013250.\n26. Federico, C., Fisher, E., & Deason, G. (2011). Expertise and the ideological consequences of the authoritarian predisposition. The Public Opinion Quarterly, 75 (4), 686\u2013708. https://doi.\norg/10.1093/poq/nfr026\n2427. Field, A. (2009). Discovering statistics using SPSS  (3rd ed.). SAGE Publications.\n28. Flanagin, A. J., & Metzger, M. J. (2008). Digital Media and Y outh: Unparalleled Opportunity \nand Unprecedented Responsibility. In M. J. Metzger & A. J. Flanagin  (Eds.)  Digital Media, \nYouth and Credibility , (pp. 5\u201328). Cambridge, MA: The MIT Press.\n29. Flanagin, A., & Metzger, M. J. (2017). Digital media and perceptions of source credibility in \npolitical communication. In K. Kenski & K. H. Jamieson (Eds.), The Oxford handbook of political \ncommunication  (pp. 417\u2013436). http://doi.org/10.1093/oxfordhb/9780199793471.013.65\n30. Fletcher, R., & Park, S. (2017). The impact of trust in the news media on online news \nconsumption and participation. Digital Journalism, 5 (10), 1281\u20131299 doi:10.1080/2167081\n1.2017.1279979.\n31. Freimuth, V ., Musa, D., Hilyard, K., Quinn, S., & Kim, K. (2014). T rust during the early stages of the 2009 H1N1 pandemic. Journal of Health Communication, 19 (3), 321\u2013339. https://doi.\norg/10.1080/10810730.2013.811323\n32. Fryberg, S. A., Stephens, N. M., Covarrubias, R., Markus, H. R., Carter, E. D., Laiduc, G. A., & Salido, A. J. (2012). How the media frames the immigration debate: The critical role of location and politics. Analyses of Social Issues and Public Policy (Policy), 12 (1), 96\u2013112. https://\ndoi.org/10.1111/j.1530-2415.2011.01259.x\n33. Gronke, P., & Cook, T. E. (2007). Disdaining the media: The American Public\u2019s changing \nattitudes toward the news. Political Communication, 24 (3), 259\u2013281. http://doi.org/10.108\n0/10584600701471591\n34. Gunther, A.C., & Lasorsa, D.L. (1986). Issue importance and perceptions of a hostile media. Journalism Quarterly, 63, 844\u2013848.\n35. Guzman, J. (2020, May 12). Facebook put \u2018warning labels\u2019 on 50 million pieces of misleading \ncoronavirus information in April. Retrieved from https://thehill.com/changing-america/well-being/longevity/497428-facebook-put-warning-labels-on-50-million-pieces-of\n36. Han, J., & Yzer, M. (2020). Media-induced misperception further divides public opinion: A test of self-categorization theory of attitude polarization. Journal of Media Psychology: Theories, \nMethods, and Applications, 32( 2), 70\u201381. https://doi.org/10.1027/1864-1105/a000259\n37. Hanitzsch, T., V an Dalen, A., & Steindl, N. (2018). Caught in the nexus: A comparative and \nlongitudinal analysis of public trust in the press. The International Journal of Press/Politics, \n23(1), 3\u201323. http://doi.org/10.1177/1940161217740695\n38. Harrison, S., Macmillan, A., & Rudd, C. (2020). Framing climate change and health: New \nZealand\u2019s online news media. Health Promotion International, 35 (6), 1320\u20131330. https://doi.\norg/10.1093/heapro/daz130\n39. Havercroft, J., & Murphy, J. (2018). Is the T ea Party Libertarian, authoritarian, or something else? Social Science Quarterly, 99 (3), 1021. https://doi.org/10.1111/ssqu.12495\n40. Health Guard (2021). The W ashington Standard. Retrieved from https://api.newsguardtech.\ncom/label/health/thewashingtonstandard.com?cid=5bb10e79-1f3d-49d2-ab6e-4c4c5f710859\n41. Herrero-Diz, P., Conde-Jim\u00e9nez, J., T apia-Frade, A., & V arona-Aramburu, D. (2019). The cred-ibility of online news: an evaluation of the information by university students / La credibili-\ndad de las noticias en Internet: una evaluaci\u00f3n de la informaci\u00f3n por estudiantes universitarios. \nCultura y Educaci\u00f3n, 31 (2), 407\u2013435. https://doi.org/10.1080/11356405.2019.1601937\n42. Higgins, E. T. (1996). Knowledge activation: Accessibility, applicability, and salience. In E. T. \nHiggins & A. W . Kruglanski (Eds.), Social psychology: Handbook of basic principles  (pp.133\u2013168). \nGuilford.\n2543. Hubner, A. (2021). How did we get here? A framing and source analysis of early COVID-19 \nmedia coverage . Communication Research Reports, 38 (2), 112\u2013120. https://doi.org/10.1080/0\n8824096.2021.1894112\n44. Jang, S. M., & Kim, J. K. (2018). Third person effects of fake news: Fake news regulation and media literacy interventions. Computers in Human Behavior, 80,  295\u2013302. https://doi.\norg/10.1016/j.chb.2017.11.034.\n45. Jones, D.A. (2004). Why Americans don\u2019t trust the media: A preliminary analysis. The \nInternational Journal of Press/Politics, 9 (2), 60-77.\n46. Jost, J. T., Glaser, J., Kruglanski, A. W ., & Sulloway, F. J. (2003). Political conservatism as \nmotivated social cognition. Psychological Bulletin, 129,  339\u2013375. https://doi.org/10.1037/003\n3-2909.129.3.339.\n47. Katz, I., & Hass, R. G. (1988). Racial ambivalence and American value conflict: Correlational and priming studies of dual cognitive structures. Journal of Personality and Social Psychology, \n55, 893-905.\n48. Kline, R. B. (2005). Principles and practice of structural equation modeling . New Y ork: The \nGuildford Press.\n49. Kohring, M., & Matthes, J. (2007). T rust in news media: Development and validation of a multidimensional scale. Communication Research, 34 (2), 231\u2013252. http://doi.org/10.1177/00\n93650206298071\n50. Ladd, J. (2005, April) Attitudes toward the news media and voting behavior. Paper presented at the annual meeting of the Midwest Political Science Association, Chicago, Illinois.\n51. Ladd, J. M. (2010). The role of media distrust in partisan voting. Political Behavior, 32 (4), 567\u2013\n585. http://doi.org/10.1007/s11109-010-9123-z\n52. LaPoe, V . L., Carter Olson, C. S., Azocar, C. L., LaPoe, B. R., Hazarika, B., & Jain, P. (2021). A comparative analysis of health news in indigenous and mainstream media. Health \nCommunication.  https://doi.org/10.1080/10410236.2021.1945179\n53. Lazer, D., Baum, M., Ognyanova, K., & V olpe, J. D. (2020). The state of the nation: A 50-state \nCOVID-19 survey Report #1. https://doi.org/10.31219/osf.io/arwh3\n54. Lee, T., & Hosam, C. (2020). Fake news Is real: The significance and sources of disbelief in mainstream media in T rump\u2019s America. Sociological Forum. https://doi.org/10.1111/socf.12603\n55. Leeder, C. (2019). How college students evaluate and share \u201cfake news\u201d stories. Library and \nInformation Science Research, 41 (3). https://doi.org/10.1016/j.lisr.2019.100967\n56. Lewis, I. A. (1981). The Media: Los Angeles Times Poll No. 46 . Los Angeles: Times-Mirror Co. \n57. Lockwood, M. (2018). Right-wing populism and the climate change agenda: Exploring the \nlinkages. Environmental Politics, 27 (4), 712\u2013732. https://doi.org/10.1080/09644016.2018.14\n58411\n58. Loersch, C., & Payne, B. K. (2014). Situated inference and the what, who, and where of priming. Social Cognition, 32 (Supplement), 137\u2013151. https://doi.org/10.1521/soco.2014.32.supp.137\n59. Liu, P. L., & Huang, L. V. (2020). Digital fake news about covid-19 and the third-person effect: \nExamining the channel differences and negative emotional outcomes. Cyberpsychology, Behavior, \nand Social Networking.  https://doi.org/10.1089/cyber.2020.0363\n60. Lukacovic, M. (2020). \u201cW ars\u201d on COVID-19 in Slovakia, Russia, and the United States: \nSecuritized framing and reframing of political and media communication around the pandemic. Frontiers in Communication, 5.  https://doi.org/10.3389/fcomm.2020.583406\n2661. McCarthy, J. (2019). Authoritarianism, populism, and the environment: Comparative \nexperiences, insights, and perspectives. Annals of the American Association of Geographers, 109( 2), \n301\u2013313. https://doi.org/10.1080/24694452.2018.1554393\n62. McCartney, K., & Rosenthal, R. (2000). Effect size, practical importance, and social policy for children. Child Development, 71 (1), 173\u2013180. https://doi.org/doi:https://doi.org/10.111\n1/1467-8624.00131\n63. McKeever, B. W ., Riffe, D., & Carpentier, F. D. (2012). Perceived hostile media bias, presumed media influence, and opinions about immigrants and immigration. Southern Communication \nJournal, 77 (5), 420\u2013437. https://doi.org/10.1080/1041794X.2012.691602\n64. Media Bias/Fact Check (2020a). W aking Times. Retrieved from https://mediabiasfactcheck.\ncom/the-waking-times/Media Bias/Fact Check (2020b). \n65. Center for Disease Control. Retrieved from https://mediabiasfactcheck.com/centers-disease-control-prevention-cdc/\n66. Media Bias/Fact Check (2021a). Global Research. Retrieved from https://mediabiasfactcheck.com/global-research/\n67. Media Bias/Fact Check (2021b). CNN. Retrieved from https://mediabiasfactcheck.com/left/\ncnn-bias/\n68. Media Bias/Fact Check (2021c). Associated Press. Retrieved from https://mediabiasfactcheck.\ncom/associated-press/\n69. Morris, C., Helliwell, R., & Raman, S. (2016). Framing the agricultural use of antibiotics and antimicrobial resistance in UK national newspapers and the farming press.  Journal of Rural \nStudies, 45 , 43\u201353 https://doi.org/10.1016/j.jrurstud.2016.03.003\n70. Myers, C. D. (2021). No effect of partisan framing on opinions about the COVID-19 pandemic. \nJournal of Elections, Public Opinion & Parties, 31,  132\u2013144. https://doi.org/10.1080/17457289.\n2021.1924747\n71. Nagler, R. H., Bigman, C. A., Ramanadhan, S., Ramamurthi, D., & Viswanath, K. (2016). Prevalence and framing of health disparities in local print news: Implications for multilevel interventions to address cancer inequalities. Cancer Epidemiology, Biomarkers & Prevention, \n25(4), 603\u2013612.\n72. Newman, N., Fletcher, R., Kalogeropoulos, A., Levy, D. A. L., & Nielsen, R. K. (2018). Digital \nnews report 2018 . Oxford, England: Reuters Institute for the Study of Journalism.\n73. Nwakpu, E. S., Ezema, V . O., & Ogbodo, N. (2020). Nigeria media framing of coronavirus \npandemic and audience response. Health Promotion Perspectives, 10 (3), 192\u2013199. https://doi.\norg/10.34172/hpp.2020.32\n74. Ogola, G. (2020). Africa and the Covid-19 Information Framing Crisis. Media and \nCommunication, 8. https://doi.org/10.17645/mac.v8i2.3223\n75. Ozturk, P., Li, H., & Sakamoto Y. (2015). Combating rumor spread on social media: The \neffectiveness of refutation and warning. 48th Hawaii International Conference on System \nSciences, Kauai, HI , 2406-2414. https://doi.org/10.1109/HICSS.2015.288\n76. Park, S., Fisher, C., Flew, T., & Dulleck, U. (2020). Global mistrust in news: The impact of \nsocial media on trust. JMM: The International Journal on Media Management, 22 (2), 83\u201396. \nhttps://doi.org/10.1080/14241277.2020.1799794\n77. Pechar, E., Bernauer, T., & Mayer, F. (2018). Beyond political ideology: The impact of attitudes towards government and corporations on trust in science. Science Communication, 40 (3), 291\u2013\n313. https://doi.org/10.1177/1075547018763970\n2778. Pennycook, G., McPhetres, J., Zhang, Y., Lu, J. G., & Rand, D. G. (2020). Fighting COVID-19 \nmisinformation on social media: Experimental evidence for a scalable accuracy-nudge intervention. Psychological Science, 31 (7), 770\u2013780. https://doi.org/10.1177/0956797620939054\n79. Pew Research Center. (2019, April 23). The role of social media in news. Retrieved from https://\nwww.journalism.org/2019/04/23/the-role-of-social-media-in-news/\n80. Pew Research Center (2020, July 30). In addition to lower awareness of current events, social media news users hear more about unproven claims. Retrieved from https://www.journalism.org/2020/07/30/in-addition-to-lower-awareness-of-current-events-social-media-news-users-hear-more-about-some-unproven-claims/\n81. Pew Research Center (2021, January 12). News use across social media platforms in 2020. Retrieved from https://www.journalism.org/2021/01/12/news-use-across-social-media-platfor\nms-in-2020/\n82. Porter, S., T aylor, K., & ten Brinke, L. (2008). Memory for media: Investigation of false memories \nfor negatively and positively charged public events. Memory, 16 (6), 658\u2013666.\n83. Ramasubramanian, S. (2007). Media-based strategies to reduce racial stereotypes activated \nby news stories . Journalism & Mass Communication Quarterly, 84 (2), 249\u2013264. https://doi.\norg/10.1177/107769900708400204\n84. Rapp, D., & Salovich, N. (2018). Can\u2019t we just disregard fake news? The consequences of \nexposure to inaccurate information. Policy Insights from the Behavioral and Brain Sciences, 5,  \n232\u2013239. https://doi.org/10.1177%2F2372732218785193\n85. Rattazzi, A., Bobbio, A., & Canova, L. (2007). A short version of the Right-Wing Authoritarianism (R W A) scale. Personality and Individual Differences, 43 , 1223\u20131234.\n86. Reese, S. D. (2019). The threat to the journalistic institution. Journalism, 20 (1), 202\u2013205. \nhttp://doi.org/10/gfrk28\n87. Robison, J., & Mullinix, K. J. (2016). Elite polarization and public opinion: How polarization is communicated and its effects. Political Communication, 33 (2), 261\u2013282. https://doi.org/10.\n1080/10584609.2015.1055526\n88. Roy, D., T ripathy, S., Kar, S. K., Sharma, N., V erma, S. K., & Kaushal, V . (2020). Study of knowledge, attitude, anxiety & perceived mental healthcare need in Indian population during COVID-19 pandemic. Asian Journal of Psychiatry, 51 . https://doi.org/10.1016/j.\najp.2020.102083\n89. Rubin, M., & Wright, C. L. (2017). Time and money explain social class differences in students\u2019 social integration at university. Studies in Higher Education, 42 (2), 315\u2013330. https://doi.org/10\n.1080/03075079.2015.1045481\n90. Samaan, G., Patel, M., Olowokure, B., Roces, M., & Oshitani, H. (2005). Rumor surveillance and Avian Influenza H5N1. Emerging Infectious Diseases, 11 (3), 463\u2013466. https://dx.doi.\norg/10.3201%2Feid1103.040657\n91. Scouten, T. (2020, August 27). College officials warn of consequences to students who do not follow COVID-19 regulations. Retrieved from https://miami.cbslocal.com/2020/08/27/college-students-covid-restrictions/\n92. Sibley, C. G., & Duckitt, J. (2008). Personality and prejudice: A meta-analysis and theoretical review. Personality and Social Psychology Review, 12 , 248\u2013279. https://doi.org/\n10.1177/1088868308319226.\n93. Silva, E., Green, J., & W alker, C. (2018). Source evaluation behaviours of first-year university \nstudents. Journal of Information Literacy, 12 (2), 24\u201343. https://doi.org/10.11645/12.2.2512\n2894. Silveira, P. & Amaral, I. (2018). Jovens e Pr\u00e1ticas de Acesso e de Consumo de Not\u00edcias nos Media \nSociais. Estudos em Comunica\u00e7\u00e3o, 1 (26), 261\u2013280.\n95. Stanford History Education Group (2016). Evaluating information: The cornerstone of civic \nonline reasoning. Robert R. McCormick Foundation. 1\u201329. Retrieved from https://stacks.stanford.edu/file/druid:fv751yt5934/SHEG%20Evaluating%20Information%20Online.Pdf\n96. Steinberg, S. L. (2004). Undocumented immigrants or illegal aliens? Southwestern media portrayals of Latino immigrants. Humboldt Journal of Social Relations, 28 (1), 109\u2013133.\n97. T ai, Z., & Sun, T. (2011). The rumoring of SARS during the 2003 epidemic in China. Sociology \nof Health & Illness, 33 (5), 677\u2013693. https://doi.org/10.1111/j.1467-9566.2011.01329.x\n98. Timberlake, J. M., & Williams, R. H. (2012). Stereotypes of U. S. immigrants from four global\nregions. Social Science Quarterly, 93 (4), 867\u2013890. http://doi.org/10.1111/j.1540-6237.2012.0\n0860.x\n99. T sfati, Y., & Ariely, G. (2014). Individual and contextual correlates of trust in media across 44countries. Communication Research, 41 (6), 760\u2013782. http://doi.org/10.1177/0093650213485\n972\n100. T sfati, Y., & Cappella, J. N. (2003). Do people watch what they do not trust? Exploring the association between news media skepticism and exposure.  Communication Research, 30 (5), \n504\u2013529. http://doi.org/10.1177/0093650203253371\n101. T sfati, Y., & Cohen, J. (2012). Perceptions of media and media effects: The third-person effect, trust in media, and hostile media perceptions. In The international encyclopedia of media \nstudies. http://doi.org/10.1002/9781444361506.wbiems995\n102. T wala, B. (2009). An empirical comparison for techniques for handling incomplete data \nusing decision trees. Applied Artificial Intelligence, 23,  373\u2013405. https://doi.org/10.1080/08\n839510902872223\n103. W eber, C., & Federico, C. M. (2007). Interpersonal attachment and patterns of ideological belief. Political Psychology, 28,  389\u2013416. https://doi.org/10.1111/j.1467-9221.2007.00579.x\n104. W entura, D., & Rothermund, K. (2014). Priming is not priming is not priming. Social \nCognition, 32( Supplement), 47\u201367.\n105. Wilkins, K. G. (1995). Gender, news media exposure, and political cynicism: Public opinion of \nHong Kong\u2019s future transition. International Journal of Public Opinion Research, 7 (3), 253\u2013\n269. https://doi.org/10.1093/ijpor/7.3.253\n106. Wineburg, S., McGrew, S., Breakstone, J., & Ortega, T. (2016). Evaluating information: The cornerstone of civic online reasoning. Stanford Digital Repository.  Retrieved from http://purl.\nstanford.edu/fv751yt5934.\n107. Wright, C. L., DeFrancesco, T., Hamilton, C., & Machado, L. (2019). The influence of media portrayals of immigration and refugees on consumer attitudes: An experimental design. Howard Journal of Communications,  1\u201323. https://doi.org/10.1080/10646175.2019.1649762\n108. Zhang, Z., Zhang, Z. & Li, H. (2015). Predictors of the authenticity of Internet health rumors. \nHealth Information and Libraries Journal, 32,  195-205. http://doi.org/10.1111/hir.12115", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "College Student's Distrust in Hard News and Exposure to Fake News During the COVID-19 Pandemic.", "author": ["CL Wright", "K Gatlin", "R Rivera"], "pub_year": "2022", "venue": "Journal of Media Research", "abstract": "This study was an experimental design that examined the effects of hard news and fake news  related to the COVID-19 pandemic on participants levels ofCOVID-19 related knowledge,"}, "filled": false, "gsrank": 298, "pub_url": "https://www.ceeol.com/search/article-detail?id=1031754", "author_id": ["", "", ""], "url_scholarbib": "/scholar?hl=en&q=info:bYxAIEeTvpoJ:scholar.google.com/&output=cite&scirp=297&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D290%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=bYxAIEeTvpoJ&ei=OLWsaJm1KI6IieoP0sKRuAk&json=", "num_citations": 1, "citedby_url": "/scholar?cites=11150511661108989037&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:bYxAIEeTvpoJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.mrjournal.ro/docs/R2/42jmr1.pdf"}}, {"title": "Political elites in the attention economy: Visibility over civility and credibility?", "year": "2025", "pdf_data": "Political Elites in the Attention Economy: Visibility Over Civility and Credibility?\nAhana Biswas1, Yu-Ru Lin1*, Yuehong Cassandra Tai2, Bruce A. Desmarais2\n1University of Pittsburgh\n2Pennsylvania State University\nahana.biswas@pitt.edu, yurulin@pitt.edu, yhcasstai@psu.edu, bdesmarais@psu.edu\nAbstract\nElected officials have privileged roles in public communica-\ntion. In contrast to national politicians, whose posting content\nis more likely to be closely scrutinized by a robust ecosystem\nof nationally focused media outlets, sub-national politicians\nare more likely to openly disseminate harmful content with\nlimited media scrutiny. In this paper, we analyze the factors\nthat explain the online visibility of over 6.5K unique state leg-\nislators in the US and how their visibility might be impacted\nby posting low-credibility or uncivil content. We conducted a\nstudy of posting on Twitter and Facebook (FB) during 2020-\n21 to analyze how legislators engage with users on these plat-\nforms. The results indicate that distributing content with low-\ncredibility information attracts greater attention from users on\nFB and Twitter for Republicans. Conversely, posting content\nthat is considered uncivil on Twitter receives less attention.\nA noticeable scarcity of posts containing uncivil content was\nobserved on FB, which may be attributed to the different com-\nmunication patterns of legislators on these platforms. In most\ncases, the effect is more pronounced among the most ideolog-\nically extreme legislators. Our research explores the influence\nexerted by state legislators on online political conversations,\nwith Twitter and FB serving as case studies. Furthermore, it\nsheds light on the differences in the conduct of political ac-\ntors on these platforms. This study contributes to a better un-\nderstanding of the role that political figures play in shaping\nonline political discourse.\n1 Introduction\nSocial media channels are effective for communication due\nto the ease of information dissemination irrespective of the\nquality of the information. Politicians also leverage social\nmedia due to the wide reach of these platforms. While pub-\nlic officials can promote constructive dialogue, they can also\nspread harmful content online. Government officials are of-\nten subjected to less stringent content moderation rules (Pel-\nletier et al. 2021), and have higher followings than average\nusers (Grant et al. 2010) which makes it easier for them to\nendorse and propagate harmful content online.\nPolitical communication online is often geared toward the\ntarget audience and platform characteristics (Kreiss 2016;\nEnli and Skogerb\u00f8 2013; Stier et al. 2020; Kelm 2020). For\n*Corresponding author.\nCopyright \u00a9 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.instance, Stier et al. (2020) found that social media messages\nof both candidates and their audiences focused on distinct\ntopics compared to the general audience during 2013 Ger-\nman federal elections. More broadly, politicians often tailor\ntheir content to achieve maximum political gains, and prior\nstudies have shown that social media plays a significant role\nin shaping political agendas, influencing public opinion, and\npotentially affecting electoral outcomes (Kreiss 2016; Bovet\net al. 2019; Boulianne and Larsson 2023).\nTo understand how politicians use social media to sway\npublic opinion, it is critical to examine what makes them vis-\nible online. In this study, we use the term visibility to refer to\nthe level of public engagement or interactions individuals re-\nceive on social media, which is often utilized to gauge their\noutreach and influence on these platforms (Eberl et al. 2020)\n. Party membership, demographic information (such as state,\nrace, and gender), and platform-level characteristics (such as\nfollower count, posting activity, and content style) are poten-\ntial contributors. We are particularly interested in determin-\ning if politicians\u2019 visibility is increased by the dissemina-\ntion of harmful content, which includes toxic and uncivil\nlanguage, as well as untrustworthy or non-credible infor-\nmation. Both uncivil and low-credibility content have been\nassociated with a decline in the quality of democratic dis-\ncourse (Goovaerts et al. 2020; Bennett et al. 2018). We ask,\ndoes posting uncivil and low-credibility content increase the\nvisibility of politicians? The answer to this question is criti-\ncal as prior research has linked harmful content online to vi-\nolent offline incidents, increased affective polarization, dis-\ntrust in institutions, and so on (Johnson 2018; Serrano-Puche\n2021; Coe et al. 2014). Harmful content originating from or\nendorsed by politicians may further exacerbate these nega-\ntive outcomes due to their larger audience base and higher\ntrustability owing to partisan preferences.\nThis work focuses on how US state legislators cultivate\nand exert their influence online. In contrast to national politi-\ncians, who are closely scrutinized by media outlets (Kyriaki-\ndou et al. 2021), sub-national politicians are more likely to\ndisseminate harmful content with limited scrutiny (Mihai-\nlidis et al. 2021). State legislators are responsible for laws\nacross all policy areas within state jurisdiction, making their\nrole crucial in the U.S. political system. Given the limited\nmedia coverage of state legislators (Squire et al. 2019), these\nsocial media platforms serve as important mediums for com-\nProceedings of the Nineteenth International AAAI Conference on Web and Social Media (ICWSM 2025)\n241\nmunicating their ideological and political positions to their\nvoters.\nWe study the dynamics of legislators\u2019 visibility, examin-\ning the different factors that influenced the attention they re-\nceived on Twitter and Facebook (FB) during the two-year\nperiod spanning 2020-2021. We focus on these two years\nowing to the surge of harmful content online due to signifi-\ncant events such as the US Presidential elections, COVID-\n19, Capitol Riots, and BLM protest movements (Ferrara\net al. 2020; Cuan-Baltazar et al. 2020; Toraman et al. 2022).\nStudying the visibility dynamics over a time period has cer-\ntain challenges. Apart from individual attributes (e.g., post-\ning frequency, party, demographics) or volume of harmful\ncontent posted, the politician\u2019s visibility may also vary by\ntime (e.g., during elections) or due to the particular topics\nthey post. We tackle these challenges in our study, our main\ncontributions are as follows :\n\u2022Factors Associated with Visibility. We present a large-\nscale, longitudinal study on political elites\u2019 online visibil-\nity in the US by comparing differences in their platform\nvisibility based on party, socio-demographic factors, and\nposting activity (RQ1, RQ2; See Section 3) . Republi-\ncans and men tend to have a higher level of visibility on\nFB, while Democrats tend to have higher visibility on\nTwitter. Posting uncivil content on Twitter and similarly\nlow-credibility content on FB is also correlated with their\nplatform visibility. Legislators\u2019 visibility on posting low-\ncredibility content, however, varies by party on Twitter.\nOur thorough analysis of legislators who post on both\nplatforms reveals notable platform differences associated\nwith their social media activities.\n\u2022Methodological Contribution. We conduct a causal in-\nference study to examine how legislators\u2019 social media\nposts affect their visibility, particularly when the content\nis uncivil or less credible (RQ3; See Section 3) . To en-\nsure that our findings are not influenced by potential con-\nfounding factors, such as temporal and topical correla-\ntions that are common in dynamic text data and can bias\nthe results, we leverage deep learning of potential out-\ncome and matching techniques. Our analytical method\nhelps disambiguate the effect of posting activities.\n\u2022Impact of Harmful Content. The results, based on ob-\nservational data using causal inference (RQ3; See Sec-\ntion 3) , have revealed significant and novel patterns. Our\nstudy found that posting uncivil content on Twitter led to\na decrease in visibility. It was observed that Republicans\nposting low-credibility content on both platforms have an\nincreased visibility, while Democrats posting the same\nhave lower visibility. The effect is more pronounced for\nideologically extreme legislators in most cases. Overall,\nour findings contribute to the understanding of politi-\ncians\u2019 online visibility, shedding light on cross-platform\ndifferences and partisan asymmetries.\n2 Related Work\nPolitical Elites\u2019 Online Behaviors. Social media are used\nby politicians for both broadcasting as well as having dia-\nlogue with voters. The effect of Twitter and Facebook useon election campaigns has been studied extensively (Kreiss\n2016; Jungherr 2016; Boulianne and Larsson 2023; Sahly\net al. 2019). Kreiss (2016) looked at how Twitter was used\nby political party staffers to shape the perspectives of jour-\nnalists and influence dedicated voters. V oters engaging in\npolitical discussion online have demonstrated increased in-\nterest and engagement in political affairs (Bode et al. 2016).\nSocial media, thus, serves as a powerful tool for politicians\nto influence public opinion and/or convey their stance re-\ngarding several important issues. Despite a large body of\nwork on political communication on social media, there is\nno clear understanding of which factors influence the on-\nline visibility of legislators, especially, how politicians post-\ningharmful content is viewed by the audience. Our research\naims to close this gap by examining the impact of uncivil\nandlow-credibility content on legislators\u2019 visibility by per-\nforming a cross-platform study\u2014after accounting for sev-\neral confounding factors related to their personal attributes,\ntemporal and topical variations.\nMisinformation and Virality. There exists a large body\nof literature characterizing the diffusion of low-credibility\ncontent online (V osoughi et al. 2018; Friggeri et al. 2014;\nZollo et al. 2015). V osoughi et al. (2018) found that false-\nhoods spread significantly faster, and reached a broader au-\ndience as compared to true news on Twitter. Friggeri et al.\n(2014) found that rumor cascades on FB tend to penetrate\ndeeper into the social network compared to general reshare\ncascades. Prior works suggest that the sentiment towards\nmisinformation is primarily negative which could be respon-\nsible for the variations seen in the diffusion (V osoughi et al.\n2018; Zollo et al. 2015).\nA significant body of research has examined the impact of\nmisinformation on the 2016 and 2020 US presidential elec-\ntions (Bovet et al. 2019; Pennycook and Rand 2021). Prior\nresearch has also shown that online misinformation tends to\nbe directed more frequently toward conservative users (Rao\net al. 2022; Yang et al. 2023) making them more likely to en-\ngage in misinformation. Misinformation may have a higher\nreach on social media platforms which could be leveraged\nby politicians to gain visibility, and the extent may vary\nacross ideologies. However, the question of how misinfor-\nmation originating from public figures is reacted by audi-\nences is less explored. In this work, we aim to illuminate\nthe impact of posting low-credibility content on legislators\u2019\nvisibility.\nThe Attention Economy and Toxic or Controversial Con-\ntent. There is no clear consensus on how uncivil content\nspreads on online platforms (Shmargad et al. 2022; Gervais\n2015). Prior works have studied the nature of incivility in on-\nline political communication suggesting that engaging in un-\ncivil discourse may have certain benefits for politicians such\nas political opinion polarization (Bodrunova and Blekanov\n2021) or empowerment by voicing criticism against author-\nities (Bodrunova et al. 2021). Uncivil content was found to\nbe associated with emotionally loaded language which gen-\nerated strong responses from the audiences (Mutz 2007). Ir-\nrespective of the kind of response, this may lead to higher\nvisibility. Coe et al. (2014) found that uncivil comments\n242\non news websites received more negative reactions. Thus,\neven though engaging in uncivil discourse may have cer-\ntain political benefits, it is unclear how that is perceived by\naudiences\u2014a gap that we address in this study.\nConfounding with Textual Data. Causal inference with\ntext data is particularly challenging since the assumptions of\ncausal inference (positivity, conditional ignorability, consis-\ntency) may not hold when confounding, treatment or out-\ncomes are encoded in text (Feder et al. 2022). For instance,\nposts on certain topics may be more likely to contain mis-\ninformation and also receive higher engagement from the\naudience. Prior works on extracting confounding from text\nhave utilized unsupervised dimensionality reduction meth-\nods (Roberts et al. 2020; Sridhar and Getoor 2019). Re-\ncent works leverage neural networks to automatically ex-\ntract features especially when the confounders in text are\nnot explicitly known (Koch et al. 2021). To address this\nproblem, some works have added transformer layers for text\nprocessing to TARNet or Dragonnet (Veitch et al. 2020;\nPryzant et al. 2020). We have extended the state-of-the-art\ntechniques to address the confounding factors that are com-\nmonly seen in dynamic social media content, such as textual\nand temporal correlations due to similar topics, events, and\npersonal attributes. To generate content representations, we\nleverage contextual RoBERTa embeddings (Liu et al. 2019)\nwith other post attributes. We then utilize a fine-tuned Drag-\nonnet model to produce content embeddings that isolate the\nconfounding factors.\n3 Study Design\nIt is crucial to understand how politicians develop influence\nthrough online media and factors associated with the\ninfluence. This work focuses on state legislators specifically\nsince they may be more likely than national-level politicians\nto disseminate harmful content owing to limited scrutiny.\nWe ask the following research questions (RQs):\nRQ1. How does the legislators\u2019 visibility, as measured by\nthe attention they receive, vary based on party affiliation,\nand individual attributes such as gender, ethnicity, state rep-\nresentation, and social media activity?\nRQ2. What attributes of legislators and their posts are asso-\nciated with their visibility?\nRQ3. How does low-credibility or incivility impact the\nvisibility of legislators\u2019 posts?\nIn RQ1 we analyze whether the attention received by leg-\nislators varies based on party affiliation, basic demograph-\nics, and posting activity. RQ2 examines what characteristics\nof legislators are most strongly correlated with their online\nvisibility change. RQ1 and RQ2 characterize how visibility\nvaries by legislators\u2019 attributes and provide an understand-\ning of factors that may potentially impact their visibility dy-\nnamics at the account level. To examine the impact of post-\ning harmful content, RQ3 investigates how low-credibility\nand incivility impacts the attention received by individual\nposts. More specifically, we study whether low-credibility\nor incivility increases or decreases a post\u2019s visibility wherevisibility is measured in terms of expected interaction rate.\nSince the user attention on social media platforms may vary\nby legislators\u2019 attributes (RQ1), and there may be other fac-\ntors associated with the visibility (RQ2), in RQ3 we estimate\nthe impact of incivility or low-credibility by controlling for\nthese variables as well as temporal and topical variations.\n3.1 Datasets\nWe collect Twitter and FB posts from all US state represen-\ntatives and senators who held office during 2020-2021 (i.e.,\neach legislator has been in office at least for a certain time\nbetween 2020 and 2021) . We focus on Twitter and FB due\nto the vast amounts1of content produced by legislators on\nthese platforms. Of the 8,028 legislators, 5,712 (64%) leg-\nislators, comprising 2,943 (61%) Democrats, 2,740 (48.2%)\nRepublicans, and 29 Independents had at least one Twitter\naccount (Kim et al. 2022). For FB, 5,1472(64.1%) legis-\nlators, comprising 2,215 (48.2%) Democrats, 2,515 (56.0%)\nRepublicans, and 418 other party members, had either an of-\nficial or a campaign account. We collect all their Twitter and\nFB posts during 2020-21. It is important to note that many\nof these accounts were either dormant or inaccessible during\nour data collection period3.\nTwitter. Our Twitter dataset4comprises around 4M tweets\nposted by 3,551 (44.2%) US state legislators during 2020-\n2021. The coverage of state legislators ranges from 29.4 to\n96.5%, with a mean of 71.6%. This suggests that our dataset\ncomprises a representative legislator population across\nmost US states. We only analyze tweets by Democrats and\nRepublicans due to the insufficient number of posts from\nIndependent legislators (N=29). We calculate the interaction\na tweet receives as the sum of likes, retweets, replies, and\nquotes. Table 1 shows the number of legislators, posts, and\nmedian5interactions on post. Democrats are more active\nand receive higher engagement on posts as compared to\nRepublicans. The comparatively higher volume of posts by\nDemocrats indicates that Twitter is a more preferred com-\nmunication platform for them compared to Republicans. We\nalso construct the intra-legislator follower network, where a\ndirected edge from legislator A\u2192Bindicates Afollows B.\nFB.We collect all FB posts6by US state legislators during\n2020-21. This yields over 493K posts from 5,147 (64.1%)\nlegislators. The coverage of state legislators ranges from\n23.5 to 80.6%, with a mean of 57.7%. We similarly focus\nonly on posts from Republicans and Democrats due to a\nsmall number of posts from other party members. We use the\ninteraction metric returned by the CrowdTangle API which\nis the sum of all reactions (\u2018Likes\u2019, \u2018Love\u2019, \u2018Wow\u2019, \u2018Haha\u2019,\n\u2018Sad\u2019, \u2018Angry\u2019, \u2018Care\u2019), comments, and shares for a post.\n1https://www.pewresearch.org/internet/2020/07/16/congress-\nsoars-to-new-heights-on-social-media/\n2FB account information was crawled from Ballotpedia\n3See Appendix for further details on data collection\n4Collected using Twitter API v2 before March 2023\n5The interactions received on posts have a skewed distribution,\nso we report the median instead of mean.\n6Collected using CrowdTangle API\n243\nTwitter FB\nparty #users #tweets Int #users #posts Int\nDem 1677 2.25M 8.0 2211 224K 58.0\nRep 1412 889K 6.0 2501 218K 101.0\nTable 1: Descriptive statistics for Twitter and FB datasets\nshowing the number of legislators, posts, and median inter-\nactions received per post by party.\nTable 1 shows the number of legislators, posts, and median\ninteractions on post. The posting frequency is similar for\nDemocrats and Republicans on FB unlike on Twitter. Inter-\nestingly, Republicans receive almost double the interaction\nas compared to Democrats, suggesting that Republicans may\nhave a larger audience base and hence a larger reach on FB.\nWe do not use the follower count for FB data since some\nof the legislator accounts are official accounts while others\nare Pages, due to which the follower counts across these dif-\nferent account types vary widely. We are unable to analyze\nFB\u2019s network data due to the lack of access.\n3.2 Individuals\u2019 Attributes\nWe characterize legislators based on their platform pres-\nence and individual-level characteristics. For Twitter-based\nattributes, we include their post count, follower count, and\nin-degree centrality in the intra-legislator follower network.\nThe post count serves as a proxy for measuring how ac-\ntively the legislators use the platform and follower count\nindicates the size of their audience base. Both post and fol-\nlower counts are likely to have an impact on online visibil-\nity (Hasan et al. 2022). Legislator\u2019s position in their peer net-\nwork may also impact their visibility (larger follower base,\ngreater potential for content virality, or seniority). To gauge\nthe influence of the legislators among their peers concern-\ning the immediate connections, we leverage the in-degree\ncentrality measure. For FB-based attributes, we include the\npost count. The individual-level attributes include party af-\nfiliation (Republican vs. Democratic), state, gender (Men\nvs. Women), ethnicity (White vs. Non-White), and ideology\nscores. We leverage the ideology scores constructed by Shor\nand McCarty (2011). Around 99.7% (N=3074) and 70.2%7\n(N=3306) of legislators are mapped8to their attributes on\nTwitter and FB, and only those legislators with attributes are\nanalyzed in our study . The final dataset comprises around\n62.2% and 53.8% White, and 68.2% and 67.7% men on\nTwitter and FB respectively, indicating that men and White\nlegislators outnumber women and Non-Whites respectively.\nThere are 2,131 (26.5%) overlapping users (OL) (i.e., legis-\nlators having accounts on both Twitter and FB) across Twit-\nter and FB. Figure 8 in Appendix shows the breakdown of\nethnicity and gender by party and for OL. Republicans have\nfewer women users compared to Democrats for both Twitter\nand FB and both platforms have more Republican men. The\nrepresentation of Non-White Republicans is higher on FB\n7See Data Collection section in Appendix\n8Ethnicity and gender are mapped using Ballotpedia. Binary\ngenders are used due to insufficient data about non-binary genders.than on Twitter.\n4 Measuring Posts\u2019 Civility, Credibility, and\nLegislators\u2019 Visibility\n4.1 Assessing Posts\u2019 Civility\nWe assess the civility of a post based on the toxicity of their\nlanguage which is a common practice in literature (Frimer\net al. 2023; Kim et al. 2021). Incivility in the context of\npolitical speech is commonly associated with rudeness ac-\ncording to the study by Stryker et al. (2016). We follow the\ndefinition of toxic language provided by Google Perspec-\ntive9:\u201crude, disrespectful, or unreasonable comment that\nis likely to make someone leave discussion\u201d. Based on this\ndefinition, it would be reasonable to assume that the toxicity\nclassifier is able to identify the markers of political incivil-\nity. We determine the level of toxicity using the \u201coriginal\u201d\nmodel10from Detoxify11(Hanu and Unitary team 2020). We\nchoose the cutoff for toxicity score as 0.82 based on manual\nannotation (see Appendix), i.e., posts having a score above\n0.82 are considered uncivil12. This yields 24,242 (0.8%) and\n277 (See Appendix) uncivil posts on Twitter and FB.\nTable 2 shows the number of legislators posting uncivil\ncontent, posts, and median interactions received, by party.\nThis may be attributed to politicians\u2019 different communica-\ntion styles across these platforms as observed in prior re-\nsearch, suggesting FB is used more for broadcasting pur-\nposes compared to Twitter which is leveraged more for hav-\ning dialogue (Enli and Skogerb\u00f8 2013). For our analysis on\nuncivil content in the following sections, we only focus on\nTwitter owing to the small number of uncivil posts on FB.\nAround 47.1% of Democrats and 32.6% Republican legisla-\ntors post uncivil content on Twitter. Democrats post almost\ndouble the number of uncivil content compared to Republi-\ncans (on average) on Twitter. This could be due to the higher\ninteraction received on such posts by Democratic legisla-\ntors. The interaction on uncivil content is higher compared\nto the baseline interaction (in Table 1) for both parties, with\na larger difference for Democrats.\nFigure 1A shows the rate of uncivil posts across years by\nparty and platform. More uncivil content was posted dur-\ning 2020 on Twitter, with Democrats having a higher rate\nof posting uncivil content. Figure 2A shows the rate of un-\ncivil posts across states by party and by platform. Interest-\ningly, we find that Republicans posted more uncivil content\non FB compared to Democrats across almost all the states,\nbut state-wise differences exist for Twitter.\n4.2 Assessing Posts\u2019 Credibility\nWe identify low-credibility content based on the credibility\nof URL in the post, which is a common practice in liter-\n9https://perspectiveapi.com/how-it-works/\n10The \u201coriginal\u201d model had the best performance when evalu-\nated against our manual annotation labels compared to the other\nDetoxify models, namely, \u201cdebiased\u201d and \u201cmultilingual\u201d.\n11We choose Detoxify over Google Perspective API since\nDetoxify has better or comparable performance (Arhin et al. 2021)\nand is faster\n12See Appendix for examples of uncivil posts\n244\nTwitter FB\nparty #users #tweets (%) Int #users #posts (%) Int\nDem 782 18111 (0.9%) 14.0 55 89 (0.1%) 131\nRep 461 6131 (0.7%) 7.0 54 188 (0.1%) 120\nTable 2: Number of uncivil posts, legislators posting, and\nmedian interactions received per post on uncivil content on\nTwitter and FB, by party.\nparty Rep Dem\nFB Twitter\n2020 2021 2020 20210.000.250.500.75\nyearA. I\nncivility (%)\nFB Twitter\n2020 2021 2020 20210.00\n.51.01.5\nyearB. Low\n-Credibility (%)\nFigure 1: Percentage of uncivil and low-credibility posts\nacross years, by party, by platform. Republicans have a\nhigher rate of posting low-credibility content on both plat-\nforms and across years. The yearly posting rates of uncivil\ncontent are comparable across parties.\nature (Lasser et al. 2022). It is important to note that this\nmethod of identifying low-credibility posts does not dis-\ncriminate between posts endorsing or debunking such in-\nformation. In this study, we are interested in looking at the\nvisibility of both, i.e., posts promoting or debunking low-\ncredibility information since both types of posts are expos-\ning the audience to harmful information. Prior research has\nshown that exposure to misinformation has an impact on be-\nlieving and subsequent sharing of such information (Halpern\net al. 2019). Thus, irrespective of the author\u2019s intent, the au-\ndience may be susceptible to believing and/or sharing such\nlow-credibility content. Moreover, the URL sharing patterns\nare similar for Democrats and Republicans on both Twitter\nand FB13, so it is unlikely that any biases are introduced in\nthe study due to our method of using URLs to identify low-\ncredibility posts. We use the low-fact URL domain refer-\nences provided by Tai et al. (2023) to identify low-credibility\nposts. Tai et al. (2023) refined Media Bias/Fact Check\u2019s14\n(MBFC) original ratings to include URLs that contain mis-\nleading information and not just politically biased ones. This\nrestrictive framing may undermine the true scale of misin-\nformation on these platforms but it offers a higher preci-\nsion (vs. recall) in identifying low-credibility content. This\nyields 6,848 (0.2%) and 4,141 (1.0%) low-credibility posts\non Twitter and FB respectively suggesting that legislators\npost more low-credibility content on FB.\n13Around 19.2% posts by Republicans and 17.4% by Democrats\ncontain URLs on Twitter, and 14.6% and 18.3% posts by Republi-\ncans and Democrats on FB contain URLs.\n14https://mediabiasfactcheck.com/\n(%)1    2   3  4 5Twitter\n0 5       10B. Low-Credibility\nFB\n0Dem Rep\n(%)party\n0.5 1.0 1.5 2.0Twitter\n1    2   3  4      0.0FB\n0A. Incivility\nILIA\nRIMIINIDHI\nWIVTVATXPANJLAKYKSFLAZALAK\nUTTNSDSCOKNYNVNEMTMSMEMAGADECTCAAR\nWYWVWAOROHNMNHNDNCMOMNMDCO\nILIA\nRIMIINIDHI\nVTVATXPANJLAKYKSFLAZALAK\nWIUTTNSDSCOKNYNVNEMTMSMEMAGADECTCAAR\nWAOROHNMNHNDNCMOMNMDCO\nWYWVFigure 2: Percentage of uncivil and low-credibility posts\nfrom states, by party, by platform. Republicans have a\nhigher rate of posting low-credibility content across all\nstates, on both platforms. Moreover, Republican legislators\nfrom most states have a higher rate of posting uncivil con-\ntent on FB. State and party-wise differences exist for posting\nrates of uncivil content on Twitter. Note that the x-axis scale\nhas been adjusted to better visualize party differences.\nTable 3 shows the number of legislators posting low-\ncredibility content, posts, and median interactions received,\nby party. Around 5.2% of Democrats and 36.7% Republi-\ncan legislators post low-credibility on FB. On Twitter, only\na handful of accounts are responsible for spreading low-\ncredibility content across both parties (1.1% for Democrats\nand 2.8% for Republicans). Republicans post more low-\ncredibility content compared to Democrats on both plat-\nforms. Similar to uncivil content, posts containing low-fact\nURLs receive higher interaction except for Democrats on\nTwitter. The median interaction for low-credibility content\nis almost three times on Twitter and double on FB for Re-\npublicans compared to their baseline interaction15.\nFigure 1B shows the rate of low-credibility posts across\nyears by party and platform. The prevalence was higher dur-\ning 2021 on both platforms and it was mainly driven by Re-\npublicans. Figure 2B shows the rate of low-credibility posts\nacross states by party and by platform. Low-credibility con-\ntent is driven by Republicans across all the states, with the\nhighest rate from Arizona, on both Twitter and FB. The low-\n15For Twitter, the median interaction on posts with and with-\nout URLs are 9.0 and 7.0 respectively whereas for FB, the median\ninteractions are 62.0 and 81.0, i.e., there is no clear pattern as to\nwhether having a URL increases or decreases the interaction of\nposts, suggesting that the results in Table 3 could potentially be\ndue to the low-credibility of URLs.\n245\nTwitter FB\nparty #users #tweets (%) Int #users #posts (%) Int\nDem 19 188 (0.0%) 4.0 83 114 (0.1%) 78.5\nRep 40 6660 (0.8%) 20.0 567 4027 (2.6%) 239.0\nTable 3: Number of posts containing low-fact URLs, legis-\nlators posting, and median interactions received per post on\nlow-credibility content on Twitter and FB, by party.\ncredibility information from Arizona is mostly related to the\n2020 US Presidential elections. In particular, we find that\nsome Republican legislators frequently shared posts from\nunreliable information outlets, especially during Arizona\u2019s\naudit of the 2020 election, which contributed to a significant\namount of low-credibility posts from Arizona.\nThe differences in interactions received by low-credibility\nand uncivil posts may be attributed to the differences in post\ntopics, timing, or attributes of authors\u2014we address this in\nthe following sections.\n4.3 Measuring Legislator\u2019s Visibility\nSocial media is being increasingly used as a tool by politi-\ncians to enhance their visibility and outreach to the public\n(Bahramirad 2022). The measure of a legislator\u2019s visibility\nis typically based on the interactions received on their posts.\nOur approach to measuring visibility is inspired by the met-\nrics used on Twitter and Facebook to calculate a post\u2019s ex-\npected engagement or virality potential (Twitter-team 2024;\nCrowdtangle 2024). Our objective is to measure the overall\noutreach of a post or legislator on the platform. Therefore,\nwe consider all the interactions received on a post or by leg-\nislators instead of focusing on a single type of engagement\nsuch as \u201cLikes.\u201d It\u2019s important to note that the visibility met-\nrics are platform-specific and may not be comparable across\nplatforms even if they share the same names. For instance,\naudiences may engage with \u201cLike\u201d feature on Twitter dif-\nferently than \u201cLike\u201d on Facebook due to different interface\ndesigns (see details in Appendix). However, our visibility\nmetrics are designed to capture the overall level of engage-\nment a post or legislator receives on a specific platform.\nMoreover, we do not distinguish between positive and\nnegative reactions received from the audience (e.g., \u2018love\u2019\nvs. \u2018angry\u2019 on FB). We are concerned with the visibility of\nthe legislators and both positive and negative reactions con-\ntribute to their overall outreach on the platform. As shown in\nTables 2 and 3, the interactions received on low-credibility\nand uncivil content are noticeably different from the over-\nall interactions received by legislators, suggesting that audi-\nences\u2019 reactions differ based on content (or other related fac-\ntors). Thus, using interactions as a proxy to measure legisla-\ntors\u2019 online visibility allows us to capture these differences\nand get insights into factors contributing to their visibility.\nThe visibility can be measured at both account and post\nlevel. For post level, the visibility of ithpost by legislator u,\nvuiis simply the interaction received on that post. Further-\nmore, we examine how visibility changes in relation to other\nvariables. Instead of measuring aggregated interactions over\nthe audience of their posts, we measure the legislator\u2019s vis-ibility ( V) by interaction rate, i.e., the number of interac-\ntions per post or audience size. We consider three different\nways to adjust the quantity of the interaction rate, since plat-\nform reach tends to be correlated with the number of follow-\ners (Hasan et al. 2022), resulting in the following dependent\nvariables (DVs): (1) interaction normalized by post count\n(VIP), (2) interaction normalized by follower count (VIF),\nand (3) interaction normalized by follower and post count\n(VIP,F). Therefore, the visibility of legislator u,Vu, is given\nby aggregating the visibility of all u\u2019s posts normalized by\npost and/or follower count.\n5 Methods\nIn this section, we describe the methods used to answer our\nRQs. In addition to the legislator behavior on Twitter and\nFB, we analyze the overlapping legislators (OL) to under-\nstand whether the differences observed across these plat-\nforms are due to different legislator populations or different\naudience/platform characteristics on Twitter and FB.\n5.1 Analyzing Legislators\u2019 Visibility\nIn RQ1, we analyze whether legislators\u2019 visibility varies\nbased on demographics, party, and posting activity. For at-\ntention disparity based on posting activity, we compare visi-\nbility of legislators having above the median number of posts\nwith those posting less than or equal to median16. Using\nMann-Whitney U test, we find differences in the platform\nvisibility of legislators based on party, gender, ethnicity, and\nposting frequency. We incorporate additional DV variants,\nnamely, 25th, 50th, and 75th percentile of the legislator\u2019s\npost interactions together with the mean interaction, i.e.,\nVIPto ensure robustness of our results. For states-wise dif-\nferences, we visualize the mean visibility across states.\nFor RQ2, we study the factors significantly correlated\nwith the platform visibility of legislators. In addition to the\nindividuals\u2019 attributes (Section 3.2), we measure the associ-\nation between low-credibility and uncivil post volumes and\ntheir visibility. Since low-credibility content is targeted more\ntowards conservative users (Rao et al. 2022; Yang et al.\n2023), we also consider the interaction between party and\nlow-credibility content posted. Additionally, the legislator\u2019s\nvisibility may be influenced by the visibility of their peers\nif their peers (re)post similar content often. To measure this\nnetwork visibility, we use the median of the visibilities of\nlegislator\u2019s in-degree neighbors in the intra-legislator fol-\nlower network. Unfortunately, we are unable to account for\nthe network effects on FB. We use the following model,\nVu=\u03b20+\u03b2PParty u+\u03b2GGender u+\u03b2EEthnicity u+\n\u03b2NPosts u+\u03b2FFollowers u+\u03b2CCentrality u+\n\u03b2SState u+\u03b2TUncivil u+\u03b2MLowCredible u+\n\u03b2NetNVu+\u03b2eParty u\u2217LowCredible u\n(1)\nwhere Vuis the visibility of legislator u,Uncivil u\nand LowCredible u are the count of uncivil and\n16We chose median as the threshold due to the skewed posting\nactivity distribution.\n246\nlow-credibility posts, Party u, Gender u, Ethnicity uand\nState uare dummy variables, Posts uis the post count,\nFollowers uis the follower count17(Twitter specific),\nCentrality uis the indegree centrality in intra-legislator fol-\nlower network (Twitter specific), and NVuis the network\nvisibility. To address the correlated errors across and within\nstates, we incorporate a random effect on the state variable.\nThe effect of each factor is estimated using a linear mixed ef-\nfects regression model18with standardized continuous vari-\nables. To ensure robust results for the analysis of RQ1 and\nRQ2, we conduct separate analyses for the years 2020 and\n2021, to determine if the visibility trends observed were con-\nsistent across both years19. Moreover, we analyze the topi-\ncal20distributions (e.g., COVID-19, BLM, elections) for our\ndataset and find that our data is not skewed towards any par-\nticular topic, suggesting limited bias due to specific topic(s).\n5.2 Analyzing the Impact of Low-credibility or\nUncivil Content\nMeasuring Outcome. Posting low-credibility and uncivil\ncontent may have an impact on how politicians are perceived\nonline. In particular, we analyze whether the presence of\nincivility or low-fact URLs increases/decreases their posts\u2019\nvisibility. To characterize the change in visibility, we ana-\nlyze the engagement on a post considering the post author\u2019s\nexpected visibility. Our metric is inspired by the overper-\nforming metric used at CrowdTangle (Crowdtangle 2024).\nThe overperforming score for post iis calculated as follows,\nOuip=vuip\nbup+thres p(2)\nwhere buis the median interaction for legislator u\u2019s posts\non the platform in previous w-days and a threshold (thres)\nfor the minimum number of interactions on a post to be con-\nsidered as overperforming, with pdenoting platform. The\ntermbupis used to adjust the outcome with respect to the\nlevel of expected interactions with the post authors on plat-\nform p. We choose the thres = 10 for Twitter (i.e., a post\nmust have at least 10 interactions to be considered overper-\nforming on Twitter) and 100for FB. We estimate the ideal\nwindow w, based on legislators\u2019 daily posting rates on these\nplatforms (see Appendix for thres ,westimation). To get\na reliable estimate of the overperforming metric, we choose\nw= 14 day rolling window. This allows us to calculate the\noverperforming score over a sufficient number of posts per\nlegislator, while also accounting for the temporal variation.\nFigure 3 shows the post overperforming scores on Twit-\nter and FB21. The distribution of overperforming scores are\n17Only included for DV not normalized by follower count\n18To satisfy assumptions of linear regression, all variables are\nsuitably transformed to be close to normal distributions (See Ap-\npendix). Ideology score is dropped since it is correlated with party.\n19Our findings revealed that the trends were similar for both\nyears. Therefore, we have reported the overall results for the en-\ntire study period.\n20Identified using keywords.\n21Our scores for FB posts are highly correlated (Spearman\n102\n1011040.000.250.500.751.00A. T witter\n101\n1011030.000.250.500.751.00B. FBproportionRep\nDemFigure 3: ECDF plots of overperforming score distribu-\ntions by party, by platform. Distributions are similar for\nTwitter but posts by Republicans overperform more on FB.\nsimilar for Republicans and Democrats on Twitter. On FB,\nposts by Republican legislators overperform more compared\nto posts by Democrats. This suggests that posts by Repub-\nlican legislators have a higher tendency to be viral on FB\ncompared to Democrats. For estimating the causal impact,\nwe are interested in analyzing whether a post overperforms\nor not due to its incivility or low-credibility, hence we bina-\nrize the overperforming score (outcome) at cutoff 1, i.e., a\npost is overperforming if it has a score >1.\nEstimating Causal Impact. It is necessary to control for\ntheconfounders that affect both the treatment andoutcome\nvariables to differentiate between spurious correlation and\ncausation. One of the possible confounders is the topic\u2014\nposts on certain topics (e.g., COVID-19) may be more likely\nto contain misinformation, and these topics may get higher\nvisibility. Another possible confounder could be the tone\u2014\nposts having certain tones (e.g., assertive) may be more\nlikely to contain uncivil language, and also more likely to\ngenerate stronger responses from the audience. Apart from\ntextual content, other confounding variables can include leg-\nislators\u2019 personal traits and the timing of their posts (e.g.,\nduring elections there might be a rise in harmful content as\nwell as an increase in legislators\u2019 visibility).\nWe control for the individual\u2019s attributes mentioned in\nSection 3.2 (excluding ideological scores), post content, and\ntime, i.e., count of days since the content was posted, start-\ning from 2020-01-01. To control for the content, we leverage\nembeddings generated by the pre-trained RoBERTa model.\nFor low-credibility content, we also include the URL head-\nlines along with post content because we assume that both\nthe text and news headlines are visible to the viewers. The\ntextual embeddings22are concatenated with individual\u2019s at-\ntributes, and time variable to get the final embedding of each\npost. The confounders we choose to control for are based on\nprior literature (Hasan et al. 2022; Sahly et al. 2019) and\ntheir feasibility of being measured. Apart from these con-\nfounders, there could also be certain other confounders (e.g.,\nexternal events, algorithmic promotion effects, effect of ads)\nwhich we are unable to measure and thus account for in this\nrho=0.997, p < 0.001) with the overperforming score returned\nby the CrowdTangle API, suggesting that our method successfully\nidentifies posts that are overperforming.\n22Only posts having a minimum of 10 words are considered for\nthis part of the analysis. This accounts for 5,405 (78.9%) low-\ncredibility and 15,883 (65.5%) uncivil posts on Twitter, and 2,427\n(58.6%) low-credibility posts on FB.\n247\nstudy. Our causal effect estimation has two steps: potential\noutcome prediction and matching.\nPotential outcome prediction. The confounding in our case\nis time-varying and encoded in complex textual data, so we\nleverage the non-parametric nature of neural networks to\nlearn deconfounded, low-dimensional representations of the\nhigh-dimensional data (Koch et al. 2021). We leverage the\nDragonnet23model proposed by Shi et al. (2019) which uses\nfeed-forward neural networks to learn balanced24represen-\ntations of the data such that each head models a separate po-\ntential outcome, a third propensity head predicts the propen-\nsity (\u03c0 ) of being treated and a free nudge parameter \u03f5(see\nAppendix for model description).\nWe fine-tune the Dragonnet model25by adding more hid-\nden layers and using cross-entropy loss. The treatment vari-\nables in our case are low-credibility and incivility respec-\ntively. We use a 5-fold cross-validation setting for training,\nwith a 1:1 ratio of treated vs. non-treated random samples\n(see Appendix for model performance). For low-credibility\nposts, we select corresponding non-treated posts that con-\ntain at least one URL and similarly include the URL head-\nlines to minimize the confounding from text (e.g., presence\nof URL). Figure 4 shows the effectiveness of our decon-\nfounding for incivility on Twitter.\nA. Before Deconfounding\nRep\nDem\nuncivil\ncivil\nB. After Deconfounding\nRep\nDem\nuncivil\ncivil\nFigure 4: Effectiveness of deconfounding for uncivil vs.\ncivil tweets. (A) shows the T-SNE space of content embed-\ndings for civil vs. uncivil tweets by party. (B) shows the\nrepresentation of the deconfounded embeddings returned by\nDragonnet. After deconfounding, the representation space\nfor treated and control covariate distributions (party in this\nexample) can not be distinguished.\nMatching. For Conditional Average Treatment Effect\n(CATE) estimation with Dragonnet predictions, we find\nthat the covariates are not balanced after propensity score\nreweighting. To improve balance, we further use matching.\nWe match the treated and untreated subjects based on the\ndeconfounded Dragonnet embeddings (see Appendix). The\ncovariate balance for matching is shown in Figure 5. All the\ncovariates are balanced for Twitter and FB low-credibility\n23Dragonnet is chosen over other deep learning models for\ncausal inference (S-learner, T-learner, TARNet) due to its \u201cTar-\ngeted Regularization\u201d procedure which allows for statistical guar-\nantees (Koch et al. 2021)\n24Balancing is a treatment adjustment strategy that forces the\ntreated and non-treated covariate distributions closer to deconfound\nthe treatment from outcome (Johansson et al. 2016)\n25Models trained on a single NVIDIA A100 40GB PCIe GPU\nA B C\n1.5 1.0 0.5\nStandardized Differences0.0FB non-credible\ndaysstate\nparty\n#postsgender\nethnicity\n2.0 1.5\nbefore1.0\nafter0.5\nStandardized Differences0.0Twitter non-credible\nmatchingdaysstate\nparty\n#postsgender\nethnicity\ncentrality\n#follower\n0.4 0.2 0.0\nStandardized DifferencesTwitter uncivil\ndaysstate\nparty\n#postsgender\nethnicity\ncentrality\n#followerFigure 5: Covariate balance after matching. All covari-\nates, except for party, and state in Twitter uncivil model, are\nbalanced (i.e., absolute standardized difference <0.1) after\nmatching on deconfounded embeddings.\nmodels. For Twitter incivility, all the covariates except for\nstate and party are balanced after matching. The final CATE\nis calculated based on the matched samples as follows,\nCATE =1\nN\u2032N\u2032X\n(Yi(1)\u2212Yi\u2032(0)) (3)\nwhere Y(T)is the outcome for treatment TandN\u2032is\nthe number of matched samples. We estimate the CATE\nfor Democrats and Republicans separately to study potential\nasymmetries in receptivity across their audiences. Further-\nmore, we look at the CATE for ideologically extreme leg-\nislators to understand whether audiences engage differently\nwith legislators at the extreme. We consider legislators hav-\ning top 25% conservative and top 25% liberal ideological\nscores as Extreme Republicans and Extreme Democrats.\nFurthermore, our analysis ensures that outliers, a common\noccurrence in social networks, do not significantly impact\nour results. (See the Appendix for more details.)\n6 Results\n6.1 RQ1: Legislators\u2019 Visibility by Party, Gender,\nEthnicity, Posting Frequency, State\nTable 4 shows the effect sizes for the Mann-Whitney U test.\nWe only report the results for VIPhere, the results for 25th,\n50th, and 75th percentile are added in the Appendix along\nwith 95% CI for Table 4. Overall, the visibility of legisla-\ntors differs significantly based on party, gender, and post-\ning frequency on both platforms and also for ethnicity on\nTwitter. Interestingly, Democrats and women appear to have\nhigher visibility on Twitter but Republicans and men have\nhigher visibility on FB (p < 0.001). White legislators also\nreceive more attention on Twitter. On both platforms, leg-\nislators with higher posting activity have higher visibility.\nFigure 6 shows the mean VIPacross US states for Twitter\nand FB. the visibility of legislators also differs based on their\nstate representation. The most visible state on Twitter is New\nMexico and Mississippi on FB. The posting rate of legisla-\ntors is the second highest for New Mexico on Twitter which\ncould be a possible explanation for the high visibility26.\n26For instance on FB, Mississippi Republican senator Chris Mc-\nDaniel has a remarkably high engagement which dominates the vis-\n248\nOverlapping (OL)\nIVs Twitter FB Twitter FB\nParty 0.239\u2217\u2217\u2217-0.299\u2217\u2217\u22170.189\u2217\u2217\u2217-0.347\u2217\u2217\u2217\n(Rep vs. Dem)\nGender 0.076\u2217\u2217\u2217-0.089\u2217\u2217\u22170.009 -0.128\u2217\u2217\u2217\n(Men vs. Women)\nEthnicity -0.067\u2217\u2217-0.031 -0.023 -0.062\n(White vs. Non-White)\nPosting Freq. 0.457\u2217\u2217\u22170.435\u2217\u2217\u22170.368\u2217\u2217\u22170.418\u2217\u2217\u2217\n(\u2264vs.>median)\n.p <0.1,\u2217p <0.05,\u2217\u2217p <0.01,\u2217\u2217\u2217p <0.001\nTable 4: RQ1 Effect sizes\nWe also look at the visibility of overlapping users (OL)\non these platforms. Similar to prior results, Republicans and\nmen receive higher engagement on FB and Democrats on\nTwitter. However, we do not find any significant difference\nacross gender and ethnicity on Twitter for OL. Our results\nsuggest that there exists cross-platform differences in how\naudiences engage with political content.\n1.00 0.75 0.50 0.25 0.00Visibility\n   (FB) 1.00 0.75 0.50 0.250.00Visibility\n(Twitter)\nFigure 6: Mean visibility of legislators from states. The\nvisibility (VIP) (normalized between 0-1) of legislators dif-\nfer based on their state representation and platform.\n6.2 RQ2: Factors Related to Visibility\nIn RQ2 we look at factors related to legislators\u2019 platform\nvisibility. The results for all DVs are similar but we only\nreport results for VIPin Table 5 (See Appendix for 95%\nCI). We are unable to estimate the interaction effect be-\ntween party and low-credibility posts for FB due to insuf-\nficient low-credibility posts from Democrats. Party, gender,\nand post frequency are significantly correlated with visibil-\nity after controlling for other variables on both platforms.\nRepublicans and men are more likely to garner higher visi-\nbility on FB whereas the opposite is true for Twitter. Higher\nposting activity is also related to higher visibility on both\nplatforms. On Twitter, the number of followers and central-\nity in the intra-legislator network are not correlated with the\nlegislators\u2019 visibility. Interestingly, there is a positive rela-\ntion between legislators\u2019 network visibility and visibility.\nAs shown in Table 5, the volume of low-credibility posts\nis positively associated with legislators\u2019 visibility on FB\nibility term for the state.Overlapping (OL)\nIVs Twitter FB Twitter FB\nParty [Rep] -0.133\u2217\u22170.423\u2217\u2217\u2217-0.142\u22170.512\u2217\u2217\u2217\nGender [Men] -0.078\u22170.090\u2217\u2217-0.011 0.113\u2217\u2217\nEthnicity [White] 0.020 0.015 0.020 0.040\n#posts 0.401\u2217\u2217\u22170.477\u2217\u2217\u22170.409\u2217\u2217\u22170.499\u2217\u2217\u2217\n#followers -0.028 - -0.028 -\nCentrality -0.020 - -0.029 -\nNetwork visibility 0.040\u22170.027 -\n#Low-Credible -0.268\u2217\u22170.079\u2217\u2217\u2217-0.335\u2217-0.037\u2217\n#Uncivil 0.148\u2217\u2217\u2217- 0.152\u2217\u2217\u2217-\nParty [Rep] x\n#Low-Credible 0.299\u2217\u2217- 0.351\u2217\u2217-\nR20.289 0.382 0.291 0.386\n.p <0.1,\u2217p <0.05,\u2217\u2217p <0.01,\u2217\u2217\u2217p <0.001\nTable 5: RQ2 Regression Results\n(p <0.001) but has an opposite effect (p < 0.01) on Twitter.\nHowever, visibility is positively correlated with the interac-\ntion between party and low-credibility posts on Twitter, i.e.,\none standard deviation increase in low-credibility posts by\nRepublicans is associated with a 0.299 standard deviation\nincrease in visibility . Thus, posting low-credibility content\nis related to higher visibility for only Republicans, otherwise\nit has a negative association on Twitter. Posting more uncivil\ncontent also increases the visibility of legislators on Twitter\n(p <0.001). These results suggest that posting harmful con-\ntent is associated with legislators\u2019 platform visibility.\nWe find similar results for OL, i.e., men and Republicans\nrelate to higher visibility on FB, and Democrats on Twit-\nter, again suggesting the cross-platform differences. Posting\nuncivil content on Twitter is positively associated with in-\ncreased visibility, while posting low-credibility content is\nnegatively associated with it. Moreover, visibility is posi-\ntively associated with the interaction term between party and\nlow-credibility posts. Interestingly however, posting low-\ncredibility content is related to decreased visibility for the\nOL on FB similar to Twitter. Therefore, legislators who\npost content on both platforms have different communica-\ntion strategies in comparison to the general legislator popu-\nlations on those platforms which may be attributed to audi-\nence preferences across platforms.\nThus, posting harmful content is related to the visibility of\nthe legislators. But the observed correlation may be a spuri-\nousone due to confounders, such as content in similar top-\nics. Next, we analyze whether incivility or low-credibility of\na post impacts its visibility.\n6.3 RQ3: Observed Causal Impact of Incivility\nand Low-credibility on Visibility\nFigure 7 shows the CATE estimates and 95% CI after match-\ning. CATE is the expected change in the overperformance\nof a post if it contains low-credibility or incivility. For\nFB, we find that Republican legislators receive higher atten-\ntion on posting low-credibility content. Similar results hold\nwhen we look at Extreme and OL Republicans. Interest-\n249\nFB non-credible Twitter non-credible Twitter uncivil\n-0.50 -0.25 0.00 0.25 0.50 -0.50 -0.25 0.00 0.25 0.50 -0.50 -0.25 0.00 0.25 0.50Extreme DemDemDem (OL)Extreme RepRepRep (OL)\nCATEFigure 7: Observed causal impact of low-credibility and\nincivility on legislators\u2019 content visibility. After control-\nling for confounders, low-credibility has a positive impact\non content visibility for Republicans on both platforms, but a\nnegative effect for Democrats on FB. Incivility significantly\nhinders content visibility for all subgroups on Twitter.\ningly, the visibility of Democrats decreases when they post\nlow-credibility content on FB. There are no effects for Ex-\ntreme and OL Democrat subgroups.\nFor Twitter, similar to FB, Republicans, including their\nOL and Extreme subgroups receive higher visibility on post-\ning low-credibility content. The effect size is higher for Ex-\ntreme Republicans, i.e., the more conservative a legislator\nis the higher attention they receive on posting misinforma-\ntion. For Democrats, however, we do not find any significant\neffects. The difference in outcomes for Democrats across\nTwitter and FB could either be due to their distinct behav-\niors (e.g., content) and/or audience preferences across these\nplatforms. The average number of posts containing low-\ncredibility content by Democratic legislators is higher on\nTwitter than on FB as shown in Table 3. This may suggest\nthat Democrats post less low-credibility content on FB since\ntheir visibility is penalized and/or vice-versa.\nThe cross-platform analysis results suggest that there are\nasymmetries in how the Republican and Democratic party\naudiences engage with low-credibility content online and\nthese asymmetries hold across platforms within and/or be-\ntween parties. The former are more likely to engage with\nmisinformation as shown by our results.\nFor uncivil content on Twitter, we find that both\nDemocrats and Republicans, including their subgroups, re-\nceive lower engagement on posts containing uncivil lan-\nguage. The negative effects are higher for Democratic legis-\nlators compared to Republicans. The effects are also higher\nfor Extreme legislators from both parties when compared to\ntheir party baselines. This implies that audiences irrespective\nof their partisan preferences engage less with uncivil content\nposted by legislators on Twitter and the legislators towards\nthe extreme political spectra are penalized more.\n7 Discussion\nWe analyzed the factors influencing the visibility dynam-\nics of US state legislators by conducting a cross-platform\nanalysis across Twitter and FB to understand different audi-\nence behaviors across these platforms. We showed that leg-\nislators\u2019 visibility varies based on their demographics, party,and posting frequency. Democrats have higher visibility on\nTwitter whereas Republicans have higher visibility on FB.\nMoreover, the regression analysis showed that a strong cor-\nrelation exists between party and visibility, i.e., Democrats\nare associated with higher engagement on Twitter and Re-\npublicans on FB. These results also hold for the overlapping\nlegislators, suggesting that audiences across these platforms\nengage with political content differently. Posting harmful\ncontent is also associated with legislators\u2019 visibility. Low-\ncredibility content is related to increased visibility on FB,\nbut decreased visibility on Twitter. Taking the effect of party\ninto account, low-credibility posting correlates with higher\nvisibility for Republicans. Uncivil posting is also correlated\nwith higher account-level visibility on Twitter.\nWe further analyzed whether the increased visibility is\ndue to posting harmful content after controlling for con-\nfounding factors such as demographics, party, topics, and\ntime. Low-credibility garners more attention for Repub-\nlicans on both platforms. However for Democrats, low-\ncredibility reduces their content visibility on Twitter. These\nresults highlight the partisan asymmetries in how low-\ncredibility content receives attention online. Existing works\nhave shown population asymmetries (Rao et al. 2022); our\nstudy further reveals attention disparity due to political ac-\ntors\u2019 party affiliation. Higher online visibility provides a\ngreater opportunity to influence public opinion (e.g., by\ngaining followers, impacting ranking algorithms). Politi-\ncians may post higher low-credibility content for political\ngains which may have implications for platform moderation.\nUnlike low-credibility, incivility decreases the visibility\nof legislators\u2019 posts on Twitter for both Republicans and\nDemocrats. The negative effects are more pronounced for\nDemocrats compared to Republicans as well as for Ex-\ntreme legislators, suggesting that audiences prefer to engage\nless with uncivil content irrespective of partisan preferences.\nPrior research has shown that uncivil content receives more\nnegative reactions (Coe et al. 2014) owing to its emotionally\ncharged language. The lower visibility may be attributed to\nthe lack of expressing negative sentiments on Twitter, but\nfurther research is needed to confirm this. Our results high-\nlight the cross-platform differences as well as asymmetries\nin how Democratic and Republican party audiences engage\nwith political content online which is aligned with previous\nliterature (Kelm 2020; Sahly et al. 2019) .\nWe show that harmful content is associated with legisla-\ntors\u2019 online visibility. Such observed associations may be\nspurious, and other factors may contribute to their visibil-\nity. For instance, posting uncivil content has a positive as-\nsociation with visibility on Twitter but incivility has a neg-\native impact on content visibility after controlling for con-\nfounders. Other factors may include the topics of their posts\nor simply the post timing. External factors (e.g., offline cam-\npaigns, media presence) can also contribute to their online\nvisibility which is out of scope for this study. Moreover,\nthere may be spillover effects from legislators\u2019 network vis-\nibility as hinted in our RQ2 results. Nevertheless, this study\nsheds light on some of the factors influencing legislators\u2019\noverall as well as content visibility, but more research is\nneeded to fully understand their online visibility dynamics.\n250\nLimitations and Future Work. Our study has certain\nlimitations. We only look at the years 2020 and 2021\u2014\nthe generalizability to other periods remains uncertain. Our\nmethod of identifying low-credibility content was conserva-\ntive which could have led to certain biases in our sampling.\nWe demonstrated the feasibility of addressing the represen-\ntation biases in Section 5; however, it is uncertain whether\nwe were able to fully correct for them. Furthermore, we only\nidentified uncivil and low-credibility posts based on the tex-\ntual content but do not consider other forms of content (e.g.,\nimages) which may also contain harmful information.\nWe only looked at the visibility based on total interac-\ntions received on posts without discriminating between pos-\nitive and negative reactions. Future work can study the im-\npact of harmful content on positive and negative visibility\nseparately to get a more nuanced understanding of public\nreceptivity. It would also be interesting to look at the impact\nof posting harmful content on different types of engagement\n(e.g., Likes vs. Retweets). Another possible extension could\nbe adapting our causal study design for continuous treatment\n(e.g., how visibility is affected by the degree of incivility).\nAcknowledgements\nThe authors would like to acknowledge support from\nNSF #2318461, AFOSR, and Pitt Cyber Institute\u2019s PCAG\nawards. The research was partly supported by Pitt\u2019s CRC re-\nsources (RRID:SCR 022735 through NIH #S10OD028483).\nAny opinions, findings, and conclusions or recommenda-\ntions expressed in this material do not necessarily reflect the\nviews of the funding sources.\nReferences\nArhin, K.; et al. 2021. Ground-Truth, Whose Truth?\u2013\nExamining the Challenges with Annotating Toxic Text\nDatasets. arXiv preprint arXiv:2112.03529.\nBahramirad, S. 2022. Virtual forums for public accountabil-\nity: How internet and communication technologies are influ-\nencing citizen interactions with a local government. CJAS,\n39(3): 313\u2013327.\nBennett, W. L.; et al. 2018. The disinformation order: Dis-\nruptive communication and the decline of democratic insti-\ntutions. European journal of communication, 33(2).\nBode, L.; et al. 2016. Politics in 140 characters or less:\nCampaign communication, network interaction, and politi-\ncal participation on Twitter. Journal of Political Marketing,\n15(4): 311\u2013332.\nBodrunova, S. S.; and Blekanov, I. S. 2021. A self-critical\npublic: Cumulation of opinion on Belarusian oppositional\nYouTube before the 2020 protests. Social Media+ Society,\n7(4): 20563051211063464.\nBodrunova, S. S.; et al. 2021. Constructive aggression? Mul-\ntiple roles of aggressive content in political discourse on\nRussian YouTube. Media and Communication, 9: 181\u2013194.\nBoulianne, S.; and Larsson, A. O. 2023. Engagement with\ncandidate posts on Twitter, Instagram, and Facebook during\nthe 2019 election. New Media & Society, 25(1): 119\u2013140.\nBovet, A.; et al. 2019. Influence of fake news in Twitterduring the 2016 US presidential election. Nature communi-\ncations, 10(1): 7.\nCoe, K.; et al. 2014. Online and uncivil? Patterns and deter-\nminants of incivility in newspaper website comments. Jour-\nnal of communication, 64(4): 658\u2013679.\nCrowdtangle. 2024. How do you calculate overperforming\nscores? https://help.crowdtangle.com/en/articles/2013937-\nhow-do-you-calculate-overperforming-scores. Accessed:\n2023-12-11.\nCuan-Baltazar, J. Y .; et al. 2020. Misinformation of COVID-\n19 on the internet: infodemiology study. JMIR public health\nand surveillance, 6(2): e18444.\nEberl, J.-M.; et al. 2020. What\u2019s in a post? How sentiment\nand issue salience affect users\u2019 emotional reactions on Face-\nbook. Journal of Information Technology & Politics, 17(1).\nEnli, G. S.; and Skogerb\u00f8, E. 2013. Personalized campaigns\nin party-centred politics: Twitter and Facebook as arenas for\npolitical communication. Information, communication & so-\nciety, 16(5): 757\u2013774.\nFeder, A.; et al. 2022. Causal inference in natural lan-\nguage processing: Estimation, prediction, interpretation and\nbeyond. Transactions of the ACL, 10: 1138\u20131158.\nFerrara, E.; et al. 2020. Characterizing social media manip-\nulation in the 2020 US presidential election. First Monday.\nFriggeri, A.; et al. 2014. Rumor cascades. In ICWSM, vol-\nume 8, 101\u2013110.\nFrimer, J. A.; et al. 2023. Incivility is rising among Ameri-\ncan politicians on Twitter. SPPS, 14(2): 259\u2013269.\nGervais, B. T. 2015. Incivility online: Affective and be-\nhavioral reactions to uncivil political posts in a web-based\nexperiment. Journal of Information Technology & Politics,\n12(2): 167\u2013185.\nGoovaerts, I.; et al. 2020. Uncivil communication and sim-\nplistic argumentation: Decreasing political trust, increasing\npersuasive power? Political Communication, 37(6).\nGrant, W. J.; et al. 2010. Digital dialogue? Australian politi-\ncians\u2019 use of the social network tool Twitter. Australian jour-\nnal of political science, 45(4): 579\u2013604.\nHalpern, D.; et al. 2019. From belief in conspiracy theories\nto trust in others: Which factors influence exposure, believ-\ning and sharing fake news. In SCSM 2019. Springer.\nHanu, L.; and Unitary team. 2020. Detoxify. Github.\nhttps://github.com/unitaryai/detoxify. Accessed: 2025-04-\n10.\nHasan, R.; et al. 2022. The Impact of Viral Posts on Visibil-\nity and Behavior of Professionals: A Longitudinal Study of\nScientists on Twitter. In ICWSM, volume 16, 323\u2013334.\nHua, Y .; et al. 2020. Characterizing twitter users who engage\nin adversarial interactions against political candidates. In\nCHI 2020, 1\u201313.\nJohansson, F.; et al. 2016. Learning representations for\ncounterfactual inference. In ICML, 3020\u20133029. PMLR.\nJohnson, J. 2018. The self-radicalization of white\nmen:\u201cFake news\u201d and the affective networking of paranoia.\nCommunication Culture & Critique, 11(1): 100\u2013115.\nJungherr, A. 2016. Twitter use in election campaigns: A sys-\ntematic literature review. Journal of information technology\n& politics, 13(1): 72\u201391.\nKelm, O. 2020. Why do politicians use Facebook and Twit-\n251\nter the way they do? The influence of perceived audience\nexpectations. SCM Studies in Communication and Media,\n9(1): 8\u201334.\nKim, J. W.; et al. 2021. The distorting prism of social me-\ndia: How self-selection and exposure to incivility fuel online\ncomment toxicity. Journal of Communication, 71(6).\nKim, T.; et al. 2022. Attention to the COVID-19 Pandemic\non Twitter: Partisan Differences Among US State Legisla-\ntors. Legislative studies quarterly, 47(4): 1023\u20131041.\nKoch, B.; et al. 2021. Deep Learning for Causal Inference.\nKreiss, D. 2016. Seizing the moment: The presidential cam-\npaigns\u2019 use of Twitter during the 2012 electoral cycle. New\nmedia & society, 18(8): 1473\u20131490.\nKyriakidou, M.; et al. 2021. Journalistic responses to mis-\ninformation. The Routledge Companion to Media Disinfor-\nmation and Populism, 529\u2013537.\nLasser, J.; et al. 2022. Social media sharing of low-quality\nnews sources by political elites. PNAS nexus, 1(4): pgac186.\nLiu, Y .; et al. 2019. Roberta: A robustly optimized bert pre-\ntraining approach. arXiv preprint arXiv:1907.11692.\nMihailidis, P.; et al. 2021. The cost of disbelief: Fracturing\nnews ecosystems in an age of rampant media cynicism. ABS,\n65(4): 616\u2013631.\nMutz, D. C. 2007. Effects of \u201cin-your-face\u201d television dis-\ncourse on perceptions of a legitimate opposition. APSR,\n101(4): 621\u2013635.\nPelletier, M. J.; et al. 2021. Fexit: The effect of political\nand promotional communication from friends and family on\nFacebook exiting intentions. Journal of Business Research,\n122: 321\u2013334.\nPennycook, G.; and Rand, D. G. 2021. Examining false be-\nliefs about voter fraud in the wake of the 2020 Presidential\nElection. The HKS Misinformation Review.\nPryzant, R.; et al. 2020. Causal effects of linguistic proper-\nties. arXiv preprint arXiv:2010.12919.\nRao, A.; et al. 2022. Partisan asymmetries in exposure to\nmisinformation. Scientific Reports, 12(1): 15671.\nRoberts, M. E.; et al. 2020. Adjusting for confounding with\ntext matching. AJPS, 64(4): 887\u2013903.\nSahly, A.; et al. 2019. Social media for political campaigns:\nAn examination of Trump\u2019s and Clinton\u2019s frame building\nand its effect on audience engagement. Social Media+ So-\nciety, 5(2): 2056305119855141.\nSerrano-Puche, J. 2021. Digital disinformation and emo-\ntions: exploring the social risks of affective polarization. In-\nternational Review of Sociology, 31(2): 231\u2013245.\nShi, C.; et al. 2019. Adapting neural networks for the esti-\nmation of treatment effects. NeurIPS, 32.\nShmargad, Y .; et al. 2022. Social norms and the dynamics\nof online incivility. Social Science Computer Review, 40(3).\nShor, B.; and McCarty, N. 2011. The ideological mapping\nof American legislatures. APSR, 105(3): 530\u2013551.\nSquire, P.; et al. 2019. State legislatures today: Politics un-\nder the domes. Rowman & Littlefield.\nSridhar, D.; and Getoor, L. 2019. Estimating causal effects\nof tone in online debates. arXiv preprint arXiv:1906.04177.\nStier, S.; et al. 2020. Election campaigning on social media:\nPoliticians, audiences, and the mediation of political com-\nmunication on Facebook and Twitter. In Studying PoliticsAcross Media, 50\u201374. Routledge.\nStryker, R.; et al. 2016. What is political incivility? Com-\nmunication monographs, 83(4): 535\u2013556.\nTai, Y . C.; et al. 2023. Official yet questionable: examining\nmisinformation in US state legislators\u2019 tweets. Journal of\nInformation Technology & Politics, 1\u201313.\nToraman, C.; et al. 2022. BlackLivesMatter 2020: An anal-\nysis of deleted and suspended users in Twitter. In WebSci\n2022, 290\u2013295.\nTwitter-team. 2024. the-algorithm. https://github.com/\ntwitter/the-algorithm?tab=readme-ov-file. Accessed: 2025-\n04-10.\nVeitch, V .; et al. 2020. Adapting text embeddings for causal\ninference. In UAI, 919\u2013928. PMLR.\nV osoughi, S.; et al. 2018. The spread of true and false news\nonline. science, 359(6380): 1146\u20131151.\nYang, Y .; et al. 2023. Visual misinformation on Facebook.\nJournal of Communication, jqac051.\nZollo, F.; et al. 2015. Emotional dynamics in the age of\nmisinformation. PloS one, 10(9): e0138740.\n252\nPaper Checklist\n1. For most authors...\n(a) Would answering this research question advance sci-\nence without violating social contracts, such as violat-\ning privacy norms, perpetuating unfair profiling, exac-\nerbating the socio-economic divide, or implying disre-\nspect to societies or cultures? Yes.\n(b) Do your main claims in the abstract and introduction\naccurately reflect the paper\u2019s contributions and scope?\nYes. See Section 6.\n(c) Do you clarify how the proposed methodological ap-\nproach is appropriate for the claims made? Yes. See\nSection 4 and 5.\n(d) Do you clarify what are possible artifacts in the data\nused, given population-specific distributions? Yes. See\nSection 3.\n(e) Did you describe the limitations of your work? Yes.\nSee Section 7.\n(f) Did you discuss any potential negative societal im-\npacts of your work? Yes. See Section 8.\n(g) Did you discuss any potential misuse of your work?\nYes. We discuss potential mis-intepretation of our con-\nclusions in Section 8.\n(h) Did you describe steps taken to prevent or mitigate po-\ntential negative outcomes of the research, such as data\nand model documentation, data anonymization, re-\nsponsible release, access control, and the reproducibil-\nity of findings? Yes. We will release the dataset in the\nfuture.\n(i) Have you read the ethics review guidelines and en-\nsured that your paper conforms to them? Yes.\n2. Additionally, if your study involves hypotheses testing...\n(a) Did you clearly state the assumptions underlying all\ntheoretical results? Yes. See Section 2.\n(b) Have you provided justifications for all theoretical re-\nsults? Yes. See Sections 6 and 7.\n(c) Did you discuss competing hypotheses or theories that\nmight challenge or complement your theoretical re-\nsults? Yes. See Section 2.\n(d) Have you considered alternative mechanisms or ex-\nplanations that might account for the same outcomes\nobserved in your study? Yes. We conduct robustness\ncheck to consolidate our findings. See Appendix\n(e) Did you address potential biases or limitations in your\ntheoretical framework? Yes. See Sections 7 and 8.\n(f) Have you related your theoretical results to the exist-\ning literature in social science? Yes. We discuss our\ntheoretical foundations in Section 3.\n(g) Did you discuss the implications of your theoretical\nresults for policy, practice, or further research in the\nsocial science domain? Yes. We discuss the implica-\ntions in Section 6 and 7.\n3. Additionally, if you are including theoretical proofs...\n(a) Did you state the full set of assumptions of all theoret-\nical results? N/A.(b) Did you include complete proofs of all theoretical re-\nsults? N/A.\n4. Additionally, if you ran machine learning experiments...\n(a) Did you include the code, data, and instructions\nneeded to reproduce the main experimental results (ei-\nther in the supplemental material or as a URL)? We\nwill release them later for the anonymous review.\n(b) Did you specify all the training details (e.g., data splits,\nhyperparameters, how they were chosen)? Yes. See\nSection 5.\n(c) Did you report error bars (e.g., with respect to the ran-\ndom seed after running experiments multiple times)?\nYes. We reported error bars whenever possible.\n(d) Did you include the total amount of compute and the\ntype of resources used (e.g., type of GPUs, internal\ncluster, or cloud provider)? Yes.\n(e) Do you justify how the proposed evaluation is suffi-\ncient and appropriate to the claims made? Yes. See\nSection 5.\n(f) Do you discuss what is \u201cthe cost\u201c of misclassification\nand fault (in)tolerance? Yes. See Section 8.\n5. Additionally, if you are using existing assets (e.g., code,\ndata, models) or curating/releasing new assets, without\ncompromising anonymity...\n(a) If your work uses existing assets, did you cite the cre-\nators? Yes. See Section 3.\n(b) Did you mention the license of the assets? N/A.\n(c) Did you include any new assets in the supplemental\nmaterial or as a URL? No.\n(d) Did you discuss whether and how consent was ob-\ntained from people whose data you\u2019re using/curating?\nYes. See Section 8.\n(e) Did you discuss whether the data you are using/curat-\ning contains personally identifiable information or of-\nfensive content? Yes. We conform to the social media\npolicies. See Section 8.\n(f) If you are curating or releasing new datasets, did you\ndiscuss how you intend to make your datasets FAIR?\nNo. We need to conform to social media platform poli-\ncies sharing our curated data. That means, we can only\nshare the public post IDs, without the data content it-\nself.\n(g) If you are curating or releasing new datasets, did you\ncreate a Datasheet for the Dataset? We will do our best\nreleasing the data without breaching the social media\nplatforms\u2019 policies.\n6. Additionally, if you used crowdsourcing or conducted\nresearch with human subjects, without compromising\nanonymity...\n(a) Did you include the full text of instructions given to\nparticipants and screenshots? N/A.\n(b) Did you describe any potential participant risks, with\nmentions of Institutional Review Board (IRB) ap-\nprovals? N/A.\n253\n(c) Did you include the estimated hourly wage paid to\nparticipants and the total amount spent on participant\ncompensation? N/A.\n(d) Did you discuss how data is stored, shared, and dei-\ndentified? N/A.\nEthics Statement\nData. We collect data from two sources, Twitter and FB.\nFor Twitter, the data was collected using Twitter\u2019s Official\nAPI v2.0 before rate limitations were imposed (i.e., March\n2023). For FB, we collect data using CrowdTangle\u2019s official\nAPI by following their terms of service. All the data are pub-\nlicly posted and available for viewing without restrictions.\nWe ensure that the Dragonnet model can effectively de-\nconfound the covariate representation space for treated and\nnon-treated samples based on our qualitative analysis and\nperformance metrics. The classification errors from Drag-\nonnet model are less likely to affect our results since we do\nnot use the model predictions to calculate CATE. However,\nmisclassification may still impact the deconfounding qual-\nity, resulting in confounding from textual content that we\ncannot measure, unlike other covariates. Our study suggests\nthat public figures sharing harmful content on social media\nhas significant consequences. We showed that when low-\ncredibility content is posted by public figures, the combi-\nnation of user behavior (interacting with the posts) and plat-\nform mechanisms (e.g., feed recommendation algorithms)\ncan result in increased visibility for such content. Our find-\nings should not be interpreted as an encouragement to spread\nsuch low-credibility content; instead, they should serve as a\nwarning that there may be incentives for political or elite ac-\ntors to do so. Moreover, our study has focused on the behav-\nior of US subnational politicians on two specific platforms\u2014\nTwitter and FB. The results may not be generalizable to\nother platforms and social media users including the ac-\ntivities of political opinion leaders and media elites from\nother countries, or even US national politicians, due to sev-\neral factors\u2014media scrutiny, platform moderation rules, and\npublic perception to name some. More research is needed to\nunderstand whether our results generalize to other settings.\n254\nAppendix\nData Collection: For Twitter data, we employed a com-\nprehensive approach, drawing from various sources, includ-\ning existing datasets (Kim et al. 2022), and conducting\nsearches on Google, Twitter, Wikipedia, legislators\u2019 official\nwebsites, campaign sites, and Ballotpedia, to compile the\naccounts and demographic information of state legislators.\nThis meticulous strategy facilitated the manual identifica-\ntion and verification of legislators\u2019 Twitter accounts. Sub-\nsequently, we refined our dataset to only include legislators\nwho served between 2020 and 2021, determined by their\ntenure in office. It is important to acknowledge that the com-\npleteness of our data was affected by factors such as inactive\nor inaccessible accounts after legislators left office.\nOur initial approach to collecting FB data involved gath-\nering posts from accounts bearing names indicative of be-\nlonging to state legislators. Subsequent verification against\ninformation from Ballotpedia allowed us to filter out non-\nlegislator accounts. To address data gaps, we conducted\nthree successive rounds of data recollection in April 2022,\nMarch 2023, and April 2023. The successive rounds allowed\nus to capture posts from accounts previously overlooked.\nHowever, similar to Twitter data, numerous accounts had\nbecome inaccessible, largely due to campaign or official ac-\ncounts no longer being active.\nDespite these efforts, we encountered a significant chal-\nlenge with FB data collection concerning accounts that were\nnot listed on Ballotpedia. Although we attempted to iden-\ntify missing accounts using keyword searches, achieving a\nperfect match with the legislator information on Ballotpedia\nwas difficult. This limitation resulted in a higher rate of mis-\nmatch in the mapped attributes of FB accounts, i.e., around\n70.2% of legislators could be mapped to their attributes for\nFB. Table 7 shows the statistics of our FB dataset after map-\nping the legislators to their attributes. The trends are similar\nto that in Table 2 which suggests that no or minimal biases\nwere introduced during our mapping process.\nFigure 8 shows the breakdown of ethnicity and gender by\nparty and for OL.\n32% 30%17% 23%\n29% 25%25% 21%\n30% 30%17% 23%Twitter FB OLethnicity\n0 0 0WhiteNon-White\n36% 32%\n13% 21%41% 27%\n13% 19%35% 32%\n12% 21%gender\n0 1K 2K 3K 4K 0 1K 2K 3K 4K 0 1K 2K 3K 4KWomenMen\nDem Rep\nFigure 8: Ethnicity and gender by party, platform, and for\noverlapping users. Our dataset has higher representation of\nmen and White legislators on both platforms.\nAssessing Post\u2019s Civility. For a study like ours it is hard\nto interpret the continuous toxicity scores returned by the\nDetoxify model. So we follow the common practice in lit-\nerature to convert the toxicity scores to binary based on a\nthreshold (Hua et al. 2020). It is important to have a thresh-old for the toxicity for our particular dataset because un-\ncivil language may evolve over different topics and time.\nTo estimate an ideal cutoff for the toxicity score, we man-\nually annotate a sample of 300 posts as uncivil or civil us-\ning stratified sampling such that more samples are included\nat higher toxicity scores. Three annotators labeled all 300\nsamples based on the aforementioned definition of toxic lan-\nguage. Since Cohen\u2019s Kappa suffers from imbalanced label\ndistributions, we compute the pairwise agreement scores for\nthe percentage of total samples agreed upon by the annota-\ntors, which ranges between 66.3-85.3%. The final labels are\ndecided according to the majority vote. Based on the ROC\n(AUC =0.81), we choose the cutoff for toxicity as 0.82,\ni.e., posts having a score above 0.82 are considered uncivil.\nThis is similar to previous works using Detoxify or Perspec-\ntive API which have a threshold between 0.5-0.9 (Hua et al.\n2020), though we acknowledge that our threshold is more\ntowards the conservative side which is done to ensure that\nuncivil posts are selected with high precision. Table 6 shows\nexamples of uncivil posts by legislators on Twitter and FB.\nThe number of uncivil posts on FB27is much lesser com-\npared to Twitter which shows that the political communica-\ntion styles are different on two platforms. This finding also\nresonates with previous studies which have suggested that\nFB is used more for broadcasting purposes whereas Twitter\nis used more for direct communication (Enli and Skogerb\u00f8\n2013). Based on this, it is reasonable that there are very less\nuncivil posts on FB since the language used on FB tends\nto be more formal. We further verify this by measuring the\nreadability scores of author\u2019s posts on these two platforms\nusing Flesch\u2013Kincaid grade level. The median readability of\nlegislators is 11.04 (i.e., the text is written at a level suitable\nfor someone who has completed the 11th grade in the US\neducation system) on FB and 9.55 on Twitter. This shows\nthat the language used on FB is indeed more formal and po-\ntentially the reason why it is more civil.\nMeasuring Visibility. The visibility metric is designed to\ncapture the overall engagement on the platforms and hence\nour metric includes all/most of the elements used to calculate\nengagement on the respective platforms. We further analyze\nthe contribution of individual visibility elements on each\nplatform. On Twitter, Likes contribute 2.3% and Retweets\ncontribute 97.7% to the overall interactions. Reply and quote\nconsist of a small percentage of the interactions. On FB how-\never, Likes contribute 51.8%, Shares contribute 17.0% and\nComments contribute 13.2% to the overall interactions. This\nsuggests that audiences engage differently with content on\nTwitter and FB, e.g., retweeting is most dominant form of\nengagement on Twitter whereas Liking for FB. Moreover,\nthe interpretation of individual elements may also be dif-\nferent across platforms, for instance, audiences may engage\nwith Like on Twitter differently than Like on FB simply due\nto different icons, therefore a direct comparison of individ-\nual engagement metrics may not be justified. So, by only\nincluding individual elements, the visibility metric may not\nbe able to capture the level of engagement on these plat-\n27The number of uncivil posts is still low at other cutoffs, for\ne.g., cutoff = 0.1 yields around 2,690 uncivil posts\n255\nplatform party text\nFB Rep While millions of Americans have yet to receive their stimulus checks, a new study reveals that $4.38 billion of\nthe new round will go right into the pockets of illegal immigrants. Another dumb idea and stupid stupid policy.\nWhat is wrong with these people?\nFB Dem \u201cI didn\u2019t think it would be this ridiculous. It\u2019s embarrassing to be a state senator at this point, \u201d Paul Boyer said\nof partisan recount. Yes it does make you look like idiots. Wasting time & resources on #TrumpsBigLie\nTwitter Rep We are at the start of one of the LARGEST recessions in American history, which will DESTROY many lives,\nand people are still in favor of partial lockdownshow stupid could you possibly be! Bunch of damn fools.\nTwitter Dem You are a blithering idiot. Who gives a shit about the VP . Vote for Trump and thousands upon thousands will\ndie.\nTable 6: Examples of Uncivil Posts on Twitter and FB, by party\nFB\nparty #users #tweets Int\nDem 1588 171K 61.0\nRep 1718 152K 114.0\nTable 7: Descriptive statistics for FB dataset after mapping,\nshowing the number of legislators, posts, and median inter-\nactions received per post by party.\nOL\nIVs Twitter [CI] FB [CI] Twitter [CI] FB [CI]\nParty 0.239 -0.299 0.189 -0.347\n[0.202, 0.277] [-0.337, -0.259] [0.141, 0.240] [-0.397, -0.298]\nGender 0.076 -0.089 0.009 -0.128\n[0.033, 0.117] [-0.132, -0.046] [-0.044, 0.059] [-0.182, -0.072]\nEthnicity -0.067 -0.031 -0.023 -0.062\n[-0.108, -0.027] [-0.079, 0.015] [-0.073, 0.029] [-0.128, 0.001]\nPosting 0.457 0.435 0.368 0.418\nFreq. [0.427, 0.492] [0.398, 0.470] [0.323, 0.414] [0.370, 0.461]\nTable 8: 95% CI for Table 4\nforms, making the interpretation of the metric harder for a\ncross-platform study.\nWe further examine the possibility of the visibility metric\nbeing dominated by a single element by testing if the under-\nlying distributions are similar for the total interactions and\nthe most dominant element. According to our KS tests, for\nboth Likes on FB and Retweets on Twitter, we find that the\ndistributions are significantly different (p-value<0.05) com-\npared to the total interactions, suggesting that other elements\nalso contribute to the overall engagement on both platforms\nand hence need to be included in the visibility measure.\nDV Transformation for RQ2. To satisfy assumptions of\nlinear regression in RQ2, we transform all variables to be\nclose to normal distributions using the \u201cbestNormalize\u201d R\npackage. For Twitter, we transform variables as follows:\nVIP(Yeo-Johnson), #posts (Box Cox), #Misinfo (sqrt),\n#Uncivil (sqrt), Network visibility (Center+scale), #follow-\ners (Box Cox), Centrality (None). For FB, we transform\nvariables as follows: VIP(Yeo-Johnson), #posts (Box Cox),\n#Misinfo (sqrt).IVs 25th50th75th\nParty 0.120\u2217\u2217\u22170.160\u2217\u2217\u22170.160\u2217\u2217\u2217\nGender 0.016 0.036 .0.060\u2217\u2217\nEthnicity 0.004 -0.027 -0.054\u2217\u2217\nPosting Freq. 0.255\u2217\u2217\u22170.354\u2217\u2217\u22170.386\u2217\u2217\u2217\nTable 9: Robustness analysis of RQ1 results for 25th, 50th,\nand 75th percentile visibility on FB\nIVs 25th50th75th\nParty -0.293\u2217\u2217\u2217-0.300\u2217\u2217\u2217-0.291\u2217\u2217\u2217\nGender -0.078\u2217\u2217\u2217-0.088\u2217\u2217\u2217-0.088\u2217\u2217\u2217\nEthnicity -0.048 -0.047 . -0.032\nPosting Freq. 0.376\u2217\u2217\u22170.402\u2217\u2217\u22170.424\u2217\u2217\u2217\nTable 10: Robustness analysis of RQ1 results for 25th, 50th,\nand 75th percentile visibility on Twitter\nRQ1 Tables. Table 8 shows the 95% CI for Table 4. Ta-\nbles 10 and 9 show the results for 25th, 50th, and 75th\npercentile visibility for Twitter and FB respectively.\nRQ2 Tables. Table 12 shows the 95% CI for Table 5.\nBenchmarks for thres ,w.Figure 9 shows the ECDF\nplots for daily mean interactions received by legislators on\nTwitter and FB, by party. For Twitter and FB, the medians\nare close to 10 and 100 respectively. So we select thres =\n10for Twitter and thres = 100 for FB. We do not have dif-\nferent thres across parties since medians are similar across\nparties on both platforms.\n1001021041060.00.20.40.60.81.0A. T witter\nparty\nRep\nDem\n1011031050.00.20.40.60.81.0B. FB\nparty\nRep\nDem\ndaily average interactions received by legislatorsproportion\nFigure 9: ECDF plots for daily mean interactions received\nby legislators on Twitter and FB, by party.\nFigure 10 shows the ECDF plots for daily number of posts\nby legislators on Twitter and FB, by party. Again the median\nposting rates are similar for Republicans and Democrats on\n256\n1001011020.00.20.40.60.81.0A. T witter\nparty\nRep\nDem\n1001011020.00.20.40.60.81.0B. FB\nparty\nRep\nDem\ndaily #posts by legislatorsproportionFigure 10: ECDF plots for daily number of posts by legisla-\ntors on Twitter and FB, by party.\nAUC Macro F1\noverall Extreme overall Extreme\nTwitter uncivil 0.74 0.72 0.69 0.66\nTwitter non-credible 0.73 0.74 0.66 0.68\nFB non-credible 0.80 0.88 0.74 0.82\nTable 11: Dragonnet performance\nboth Twitter and FB. The median daily post count on Twitter\nis 2 and 1 on FB. To ensure that we have a sufficient num-\nber of posts per legislator to compute the overperforming\nscores and simultaneously account for temporal variation in\nour data, we select w= 14 for both Twitter and FB.\nDragonnet Model Description. Dragonnet uses feed-\nforward neural networks to learn balanced representations\nof the data such that each head models a separate poten-\ntial outcome, a third propensity head predicts the propen-\nsity (\u03c0 ) of being treated, and a free nudge parameter \u03f5. The\n\u03c0and\u03f5parameters are used to re-weight the outcomes to\nprovide lower biased estimates of the Conditional Average\nTreatment Effect (CATE). The error gradients from the two\noutcome modeling heads are propagated back to the shared\nrepresentation layers of the Dragonnet model to learn the\ncovariate representation, i.e., \u03d5(X). The representation lay-\ners learn a balanced representation of the data since the\nmodel objective is to predict both outcomes and each out-\ncome modeling head learns a function of the transformed co-\nvariate representation, i.e., Y(T) =h(\u03d5(x), T ). The CATE\nfrom Dragonnet predictions is estimated as follows,\nCATE =1\nNNX\ni(Y\u2217\ni(1)\u2212Y\u2217\ni(0)) (4)\nwhere,\nY\u2217\ni=\u02c6Yi+ (Ti\n\u03c0(\u03d5(X i),1)\u22121\u2212Ti\n\u03c0(\u03d5(X i),0))\u00d7\u03f5 (5)\nwhere \u02c6Y(1)and\u02c6Y(0)are predictions returned by the\ntwo outcome modeling heads respectively, \u03c0is the predicted\npropensity of a sample being treated, and sample size N.\nDragonnet Model Performance. Figure 11 shows the\nROC curves for Twitter incivility, Twitter low-credibility\nand FB low-credibility Dragonnet models. The AUC and\nMacro F1-scores28are reported in Table 11.\n28F1-scores reported at optimal cutoff\n0.0 0.2 0.4 0.6 0.8 1.0\nFPR0.00.20.40.60.81.0TPRT witter uncivil\nOverall (AUC: 0.74)\nHeadO= 0 (AUC: 0.78)\nHeadO= 1 (AUC: 0.69)\n0.0 0.2 0.4 0.6 0.8 1.0\nFPR0.00.20.40.60.81.0TPRT witter non-credible\nOverall (AUC: 0.73)\nHeadO= 0 (AUC: 0.67)\nHeadO= 1 (AUC: 0.77)\n0.0 0.2 0.4 0.6 0.8 1.0\nFPR0.00.20.40.60.81.0TPRFB non-credible\nOverall (AUC: 0.80)\nHeadO= 0 (AUC: 0.75)\nHeadO= 1 (AUC: 0.87)Figure 11: ROC curves showing Dragonnet performance of\noverall as well as two outcome modeling heads, for Twitter\nincivility, Twitter and FB low-credibility respectively.\nOL\nIVs Twitter FB Twitter FB\nParty [Rep] -0.133 0.423 -0.142 0.512\n[-0.219, -0.046] [0.359, 0.487] [-0.252, -0.029] [0.431, 0.594]\nGender [Men] -0.078 0.090 -0.011 0.113\n[-0.149, -0.007] [0.029, 0.152] [-0.101, 0.079] [0.033, 0.194]\nEthnicity [White] 0.020 0.015 0.020 0.040\n[-0.061, 0.100] [-0.056, 0.086] [-0.079, 0.118] [-0.054, 0.133]\n#posts 0.401 0.477 0.409 0.499\n[0.345, 0.455] [0.443, 0.510] [0.337, 0.481] [0.459, 0.538]\n#followers -0.028 - -0.028 -\n[-0.076, 0.021] [-0.091, 0.034]\nCentrality -0.020 - -0.029 -\n[-0.060, 0.019] [-0.079, 0.021]\nNetwork visibility 0.040 0.027 -\n[0.002, 0.080] [-0.020, 0.076]\n#Low-Credible -0.268 0.079 -0.335 -0.037\n[-0.466, -0.070] [0.047, 0.110] [-0.601, -0.069][-0.074, 0.000]\n#Uncivil 0.148 - 0.152 -\n[0.101, 0.196] [0.092, 0.213]\nParty [Rep] x 0.299 - 0.351 -\n#Low-Credible [0.103, 0.496] [0.087, 0.614]\nR20.289 0.382 0.291 0.386\nTable 12: 95% CI for Table 5\nCovariate Balance for Matching. We employ 1:1 match-\ning such that each treated sample is matched to one un-\ntreated sample. We use Nearest Neighbour matching based\non Euclidean distance between the deconfounded tweet em-\nbeddings. We find matches for 9677 (64.0%) uncivil tweets,\n3957 (73.5%) low-credibility tweets, and 1583 (61.1%) low-\ncredibility FB posts using a distance cutoff of 0.1 to maxi-\nmize the covariate overlap. This gives us balanced represen-\ntations of the observed covariates across the treated and un-\ntreated samples as shown in Figure 6. We compute the stan-\ndardized differences for each of the covariates before and\nafter matching. A score between -0.1-0.1 indicates balance.\nEffect of Outliers Outliers are common in social net-\nworks, but their impact on analysis results and conclusions\ncan vary. In our dataset, outliers may exist in terms of post-\ning volume and engagement received due to the scale-free\ndistributions (refer to Fig 12). However, we have taken mea-\nsures to ensure these outliers do not significantly impact our\nresults.\nFor RQ1, we used a non-parametric statistical test, the\nMann-Whitney U test, to compare distributions, which is\n257\nrobust to outliers. For RQ2, we used non-linear transforma-\ntions on study variables to minimize the impact of outliers\nin the regression analysis.\nIn RQ3, we matched accounts with similar characteris-\ntics in the de-confounded embedding space, such as simi-\nlar posting rates. This process either discarded outliers if no\nadequate match was found or retained them if an adequate\nmatch was identified. This matching step ensured the bal-\nance of the covariates before running the estimation of the\nConditional Average Treatment Effect (CATE), further re-\nducing the impact of outliers.\n1001011021031040.00.20.40.60.81.0ProportionA. T witter posting volume\nparty\nRepublican\nDemocratic\n101\n1001011021031040.00.20.40.60.81.0ProportionB. Visibility on T witter\nparty\nRepublican\nDemocratic\nFigure 12: Scale-free distributions for (A) Posting volumes\nand (B) Visibility of legislators on Twitter. The distributions\nare also similar for FB.\n258", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Political elites in the attention economy: Visibility over civility and credibility?", "author": ["A Biswas", "YR Lin", "YC Tai", "BA Desmarais"], "pub_year": "2025", "venue": "Proceedings of the \u2026", "abstract": "Elected officials have privileged roles in public communication. In contrast to national politicians,  whose posting content is more likely to be closely scrutinized by a robust ecosystem of"}, "filled": false, "gsrank": 302, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/35814", "author_id": ["a1StKdgAAAAJ", "9EeqDSEAAAAJ", "LLWK3Z4AAAAJ", "fRM8IN4AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:1GYlreeQGSEJ:scholar.google.com/&output=cite&scirp=301&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D300%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=1GYlreeQGSEJ&ei=OrWsaI6ND8DZieoPqdqh8QU&json=", "num_citations": 4, "citedby_url": "/scholar?cites=2385096802386142932&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:1GYlreeQGSEJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/35814/37968"}}, {"title": "Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval", "year": "2025", "pdf_data": "arXiv:2506.17878v1  [cs.AI]  22 Jun 2025JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nTowards Robust Fact-Checking: A Multi-Agent System with\nAdvanced Evidence Retrieval\nTam Trinh\u2217\u2020, Manh Nguyen\u2021, Truong-Son Hy\u2020\n\u2217National Economics University, Vietnam\n\u2020The University of Alabama at Birmingham, United States\n\u2021Deakin University, Australia\nAbstract \u2014The rapid spread of misinformation in the digital\nera poses significant challenges to public discourse, necessitating\nrobust and scalable fact-checking solutions. Traditional human-\nled fact-checking methods, while credible, struggle with the\nvolume and velocity of online content, prompting the integration\nof automated systems powered by Large Language Models\n(LLMs). However, existing automated approaches often face\nlimitations, such as handling complex claims, ensuring source\ncredibility, and maintaining transparency. This paper proposes\na novel multi-agent system for automated fact-checking that\nenhances accuracy, efficiency, and explainability. The system\ncomprises four specialized agents: an Input Ingestion Agent\nfor claim decomposition, a Query Generation Agent for for-\nmulating targeted subqueries, an Evidence Retrieval Agent for\nsourcing credible evidence, and a Verdict Prediction Agent\nfor synthesizing veracity judgments with human-interpretable\nexplanations. Evaluated on benchmark datasets (FEVEROUS,\nHOVER, SciFact), the proposed system achieves a 12.3% im-\nprovement in Macro F1-score over baseline methods. The system\neffectively decomposes complex claims, retrieves reliable evidence\nfrom trusted sources, and generates transparent explanations for\nverification decisions. Our approach contributes to the growing\nfield of automated fact-checking by providing a more accurate,\nefficient, and transparent verification methodology that aligns\nwith human fact-checking practices while maintaining scalability\nfor real-world applications. Our source code is available at\nhttps://github.com/HySonLab/FactAgent.\nIndex Terms \u2014Fact-checking, Misinformation Detection, Large\nLanguage Models, Multi-agent Systems, Explainable AI\nI. I NTRODUCTION\nThe rapid proliferation of misinformation on digital plat-\nforms poses a significant threat to the integrity of information\nand public discourse. As false or misleading content spreads\nat unprecedented speed, the demand for effective and scal-\nable fact-checking solutions has become increasingly urgent\n[1]. Although traditional fact-checking performed by human\nexperts and specialized organizations has played a vital role,\nit is still insufficient to keep up with the sheer volume and\nvelocity of misinformation in the modern media landscape [2].\nMisinformation, defined as false information presented as\nfactual regardless of intent, has far-reaching consequences\nin the political, health, and economic domains. In political\nsettings, it can distort democratic processes and exacerbate\nsocial polarization [3]. During public health crises such as\nthe COVID-19 pandemic, misinformation has contributed to\nvaccine hesitancy and the spread of harmful behaviors [4].\nCorresponding author: thy@uab.eduIn economic contexts, unverified claims have been linked to\nmarket volatility and misguided consumer decisions [5].\nTo address these issues at scale, recent efforts have turned\nto automated fact-checking systems, bolstered by advances in\nLLMs [6]. These models exhibit impressive natural language\nunderstanding and generation capabilities, enabling them to\nprocess and evaluate claims on a previously unattainable scale.\nHowever, deploying LLMs for fact-checking also introduces\nnew challenges. LLMs are prone to hallucinations, generating\nplausible but incorrect information and often operate as black\nboxes, making it difficult to interpret or verify their output\n[7]. Furthermore, LLMs trained in static corpora may lack\naccess to up-to-date or domain-specific knowledge required\nfor accurate verification [8].\nTo mitigate these limitations, researchers have explored hy-\nbrid strategies that integrate LLMs with retrieval mechanisms\nand structured reasoning pipelines. Notable approaches include\nretrieval-augmented generation [9], chain-of-thought prompt-\ning [10], and tool-based reasoning frameworks [11]. Despite\nthese advancements, current systems often fall short when han-\ndling complex, multi-faceted claims, filtering non-verifiable\nstatements, and ensuring the credibility and completeness of\nretrieved evidence. Many models also rely on shallow web\nsnippets rather than full-document analysis, limiting the depth\nand reliability of their verifications [12].\nIn this paper, we propose a novel multi-agent framework\nfor automated fact-checking that integrates program-guided\nreasoning, high-quality evidence retrieval, and interpretable\nverdict generation. Our system is composed of four specialized\nagents (Figure 1):\n\u2022Input Ingestion Agent: Decomposes complex claims\ninto sub-claims and identifies their verifiability.\n\u2022Query Generation Agent: Constructs targeted search\nqueries to retrieve relevant information.\n\u2022Evidence Retrieval Agent: Sources of credible informa-\ntion from trusted knowledge bases and documents.\n\u2022Verdict Prediction Agent: Synthesizes retrieved evi-\ndence to assess the validity of the claim and generate\nexplanations.\nThe key contributions of this work are as follows.\n1) We introduce a structured multi-agent architecture that\nimproves fact-checking precision through decomposable\nreasoning and modular information processing.\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nLLM Response\nVeracity ResultKnowledge Source\nWeb\nSearchMulti-agent System\nOrchestratorInput Ingestion\nAgent\nQuery Generation\nAgentEvidence Seeking\nAgentVerdict Prediction\nAgent\nFig. 1: Overview of the multi-agent system. The system\ncomprises an Orchestrator that manages communication and\nworkflow between specialized agents, including Input Inges-\ntion Agent, Query Generation Agent, Evidence Seeking Agent,\nand Verdict Prediction Agent. The Orchestrator interacts with\na knowledge source and integrates their responses to produce\na final Veracity Result.\n2) We implement a robust evidence retrieval strategy\nthat emphasizes source credibility and leverages full-\ndocument content rather than relying solely on search\nengine snippets.\n3) We conducted extensive experiments on three bench-\nmark datasets including FEVEROUS [13], HOVER [14],\nand SciFact [15], demonstrating that our system achieves\nsuperior performance compared to existing baselines by\n12.3% relative improvement.\nII. R ELATED WORKS\nA. Traditional Fact-Checking Approaches\nFact-checking has its roots in traditional journalism, where\nprofessional fact-checkers manually verified claims using rig-\norous investigation and source analysis [2]. Organizations like\nPolitiFact [16], Snopes [17], and FactCheck.org [18] pioneered\nstandardized methodologies, particularly for political discourse\n[19]. However, these manual approaches face serious scalabil-\nity issues in the digital age, where content is produced and\ndistributed at unprecedented speed.\nThe emergence of semi-automated fact-checking systems\nmarked the beginning of computational support for human\nexperts. Vlachos and Riedel [20] conceptualized fact-checking\nas a classification task and laid the groundwork for early\nbenchmarks. Subsequently, natural language processing (NLP)\ntechniques were incorporated to analyze claims and retrieve\nrelevant evidence. In particular, Hassan et al. [21] proposed\nClaimBuster, a pipeline that identifies checkworthy claims and\nassists in verification, demonstrating the viability of human-AI\ncollaboration in this domain.\nB. Automated Fact-Checking Systems\nAutomated fact-checking systems can generally be divided\ninto content-based and context-based approaches [1]. Content-\nbased methods analyze linguistic and semantic features within\ntext to detect misinformation, while context-based methodsfocus on how information spreads, which examine propagation\npatterns, user interactions, and temporal signals.\nAdvancements in neural architectures have substantially\nimproved system performance. Popat et al. [22] proposed a\ndeep learning model that evaluates the credibility of claims by\nanalyzing the position and reliability of evidence. Augenstein\net al. [23] introduced MultiFC, a benchmark dataset that\nemphasizes the importance of verifying claims against multiple\nand diverse sources. Transformer-based architectures further\npushed the boundaries, with models like GEAR [24] lever-\naging graph-based aggregation to synthesize multi-evidence\nreasoning.\nC. LLMs for Fact-Checking\nLLMs have recently become a powerful tool for fact-\nchecking due to their capabilities in language understanding\nand generation [6]. Initial LLM-based methods used zero-shot\nand few-shot prompting to evaluate claims without requiring\nextensive fine-tuning [25]. Lee et al. [26] showed that LLMs\ncould generate natural language explanations, enhancing inter-\npretability.\nHowever, LLMs suffer from several critical limitations. A\nmajor issue is hallucination, which generates plausible but in-\ncorrect information [7]. Studies such as [27] have documented\nhow LLMs confidently produce inaccurate results, undermin-\ning their trustworthiness. Moreover, static training data results\nin temporal obsolescence, hindering the verification of claims\nabout recent or dynamic events.\nTo mitigate these issues, researchers have proposed sev-\neral enhancements. Retrieval-augmented generation (RAG) [9]\ngrounds LLM outputs using external evidence. Chain-of-\nthought prompting [10] improves the transparency of reason-\ning by encouraging step-by-step explanations. Tool-augmented\napproaches such as FacTool [11] equip LLMs with specialized\nmodules, improving domain robustness and factuality.\nD. Multi-Agent Systems for Complex Reasoning\nMulti-agent systems (MAS) have emerged as a promising\narchitecture to enhance LLM reasoning through task decom-\nposition and modular specialization [28]. In fact-checking,\nMAS approaches distribute responsibilities such as claim\nparsing, query generation, evidence retrieval, and verdict\nsynthesis across individual agents, improving scalability and\ninterpretability.\nLi et al. [29] demonstrated that debate-style MAS can\nenhance decision faithfulness and reduce hallucinations. Sim-\nilarly, Zhang et al. [30] introduced retrieval agents that au-\ntonomously browse the web to collect real-time evidence,\nincreasing adaptability to evolving misinformation. Self-\nverification has also been explored; Wang et al. [31] proposed\nSelf-Checker , which empowers LLMs to critique their own\noutputs using structured reasoning and external tools, thus\nreducing reliance on static knowledge.\nDespite these advances, challenges persist. Effective MAS\nrequire robust inter-agent communication, consistency in dis-\ntributed reasoning, and efficient task allocation. Our proposed\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nFig. 2: Overview of the proposed architecture. The system takes input data and decomposes claims using Input Ingestion Agent.\nVerifiable subclaims are passed to Query Generation Agent, which creates questions from different perspectives. Evidence\nSeeking Agent retrieves and verifies evidence from web sources. Finally, Verdict Generation Agent produces a fact-check label\nand explanation based on the gathered evidence.\nsystem builds on these insights, introducing a structured multi-\nagent framework that prioritizes decomposable reasoning,\ncredible source retrieval, and transparent explanation gener-\nation.\nIII. R ESEARCH METHODOLOGY\nThe proposed multi-agent system for automated fact-\nchecking comprises four specialized agents, including Input\nIngestion Agent, Query Generation Agent, Evidence Seeking\nAgent, and Verdict Prediction Agent, each responsible for a\ndistinct stage in the claim verification pipeline. Each agent\nis powered by LLM, leveraging their advanced reasoning\ncapabilities through carefully engineered prompts that guide\ntheir behavior toward specific verification tasks. As illustrated\nin Figure 2, these agents based on LLM operate sequentially\nto transform raw input claims into structured verified outputs\naccompanied by human-interpretable explanations.\nA. Input Ingestion Agent\nThe Input Ingestion Agent serves as the entry point for the\nfact-checking pipeline, processing raw claims, and preparing\nthem for downstream verification. This agent performs two\ncritical functions: decomposing complex claims into atomic\ncomponents using First-Order Logic (FOL) and filtering out\nnon-verifiable statements.\n1) Decomposing into FOL Subclaims: Complex claims\noften contain multiple propositions, implicit assumptions, or\nconditional dependencies that can hinder effective verification.To address this challenge, the Input Ingestion Agent trans-\nforms multi-faceted claims into simplified sub-claims using\ntechniques inspired by First-Order Logic representation [32].\nAccording to Enderton [32], FOL extends propositional\nlogic by introducing predicates that describe properties of ob-\njects or relations between them. A well-formed FOL formula\nconsists of atomic predicates applied to terms (constants or\nvariables), formally expressed as P(t1, t2, ..., t n)where Pis a\npredicate of arity nand(t1, t2, ..., t n)are terms. For example,\nthe claim \u201dSumo wrestler Toyozakura Toshiaki committed\nmatch-fixing, ending his career in 2011 that started in 1989\u201d\nis decomposed into:\n\u2022Occupation(Toyozakura Toshiaki, \"sumo\nwrestler\")\n\u2022Commit(Toyozakura Toshiaki,\n\"match-fixing\")\n\u2022Ending(Toyozakura Toshiaki, \"his\ncareer in 2011\")\n\u2022Starting(Toyozakura Toshiaki, \"his\ncareer in 1989\")\nEach predicate is accompanied by a natural language ver-\nification goal (e.g., \u201dVerify that Toyozakura Toshiaki is or\nwas a sumo wrestler\u201d ). At this stage, for a given input claim\nC, Input Ingestion Agent initially generates a collection of\npredicates, denoted as P= [p1, . . . , p n]. These predicates\ncorrespond to the constituent sub-claims C= [c1, . . . , c n].\nEach predicate pi\u2208 P is an FOL expression designed to\nguide LLMs in formulating a question-and-answer pair that\nencapsulates the meaning of the sub-claim ci. The general\nclaim Ccan be conceptualized as a conjunction of these\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\npredicates, that is, C=p1\u2227p2\u2227 \u00b7\u00b7\u00b7 \u2227 pn. To establish that\nclaim Cis SUPPORTED, every predicate must be evaluated\nas True. Conversely, if even one predicate is found to be False,\nthe claim is categorized as NON SUPPORTED.\nThe decomposition facilitates granular reasoning, its effec-\ntiveness has been proven by recent research in automated fact\nverification by Wang and Shu [33]. Wang and Shu [33] found\nthat FOL-guided decomposition enables more precise evidence\nretrieval and reduces error propagation in multi-hop reasoning\ntasks.\n2) Filtering out Non-verifiable Subclaims: Following de-\ncomposition, the agent classifies each subclaim as verifiable\nor non-verifiable based on established criteria from Micallef\net al. [34] and Konstantinovskiy et al. [35]. Verifiable claims\nare factual statements that can be independently checked for\naccuracy using objective evidence, while non-verifiable claims\nexpress subjective opinions or lack specificity for objective\nverification.\nThe classification process considers several key character-\nistics:\n\u2022Verifiable claims involve checkable facts or statistics,\ncan be verified using trusted sources, and are objective in\nnature.\n\u2022Non-verifiable claims are often subjective, emotional, or\nanecdotal; cannot be objectively proven or disproven; and\ndepend on personal belief or preference.\nAfter classification, non-verifiable claims are filtered out\nfrom further processing. This filtering ensures that only\nclaims that can be validated with factual evidence proceed to\nsubsequent stages, preventing unnecessary computations and\nfocusing the system\u2019s resources on statements that can be\nobjectively evaluated.\nB. Query Generation Agent\nThe Query Generation Agent transforms atomic subclaims\ninto effective search queries designed to retrieve relevant evi-\ndence. This agent leverages the reasoning capabilities of LLMs\nto generate diverse, well-formulated questions that maximize\nthe likelihood of retrieving accurate information from search\nengines. The development of this module draws inspiration\nfrom previous work [33], [36], which explores the generation\nof queries to support evidence-based reasoning. However, the\ncited work does not provide detailed methodologies for craft-\ning specific questions, leaving a gap in practical implemen-\ntation. To address this, Query Generation Agent incorporates\nprinciples from Search Engine Optimization (SEO) to guide\nthe formulation of questions.\nFor each atomic subclaim produced by the Input Ingestion\nAgent, the Query Generation Agent creates kdistinct search\nqueries. These queries are deliberately formulated to approach\nthe verification task from multiple angles, ensuring compre-\nhensive evidence collection while mitigating potential biases in\nsearch results. The agent incorporates principles from Search\nEngine Optimization (SEO) research ( [37], [38]), including:\n\u2022Use of specific keywords and entities from the claim\n\u2022Incorporation of synonyms and alternative phrasings\u2022Balanced specificity to maximize relevance without over-\nconstraining results\n\u2022Question-based formulations that align with natural\nsearch patterns\nThis multi-perspective generation strategy ensures that the\ndownstream Evidence Seeking Agent is equipped with varied\nentry points for retrieving supporting or refuting material from\nthe website.\nC. Evidence Seeking Agent\nThe Evidence Seeking Agent is responsible for retriev-\ning, validating, and extracting relevant information from\nweb sources. This agent incorporates tools for programmatic\nsearch, credibility assessment, and targeted content extraction\nto ensure that the prediction of the following verdict is based\non comprehensive and reliable evidence. To the best of our\nknowledge, our work is the first to propose an Evidence\nSeeking Agent that integrates programmatic web search with\nsystematic verification of the credibility of the media site and\ncomprehensive extraction of evidence from retrieved content.\n1) Internet Search: The initial stage of evidence gathering\ninvolves programmatic access to search engine results to\nidentify sources relevant to the queries generated. For this\npurpose, we use SerperAPI [39], a high-performance and cost-\neffective Google Search API.\nOur implementation configures SerperAPI with parameters\noptimized for fact-checking purposes:\n\u2022Result limitation (num=10 ): We retrieve the top 10\nsearch results for each query to optimize cost efficiency.\n\u2022Regional targeting (gl=us ): Searches are configured\nfor the US region to ensure consistency in the result\npatterns.\n\u2022Temporal boundaries (tbs): To prevent temporal leak-\nage when evaluating against benchmark datasets, we\nimplement dataset-specific end-date restrictions aligned\nwith publication dates:\n\u2013FEVEROUS [13]: October 12, 2021\n\u2013HoVer [14]: November 16, 2020\n\u2013SciFact [15]: October 3, 2020\nThis temporal bounding is particularly important for main-\ntaining methodological integrity in fact-checking evaluation, as\nit prevents the system from accessing information that would\nnot have been available at the time each benchmark dataset\nwas created.\n2) Credibility Check in Media Sites: Distinguishing be-\ntween reliable and unreliable information sources is critical in\nthe fact-checking process. For source credibility assessment,\nwe integrate the Media Bias/Fact Check (MBFC) API [40],\nwhich provides programmatic access to professional human\nratings of publisher credibility. The system filters potential\nevidence sources based on specific thresholds for the quality\nof factual reporting and political bias (Table I).\nFor domains not listed in the MBFC database, we implement\na fallback credibility assessment mechanism that considers the\nfollowing:\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nCategory Before Filtering After Filtering\nFactuality\n\u2022\u201cvery high\u201d\n\u2022\u201chigh\u201d\n\u2022\u201cmostly factual\u201d\n\u2022\u201cmixed\u201d\n\u2022\u201clow\u201d\n\u2022\u201cvery low\u201d\u2022\u201cvery high\u201d\n\u2022\u201chigh\u201d\n\u2022\u201cmostly factual\u201d\nPolitical\nBias\u2022\u201cleast biased\u201d\n\u2022\u201cleft-center\u201d\n\u2022\u201cright-center\u201d\n\u2022\u201cleft\u201d\n\u2022\u201cright\u201d\n\u2022\u201cextremely left\u201d\n\u2022\u201cextremely\nright\u201d\n\u2022\u201cpro-science\u201d\n\u2022\u201cquestionable\u201d\n\u2022\u201csatire\u201d\n\u2022\u201cconspiracy-\npseudoscience\u201d\u2022\u201cleast biased\u201d\n\u2022\u201cleft-center\u201d\n\u2022\u201cright-center\u201d\n\u2022\u201cpro-science\u201d\nTABLE I: Publisher categorization before and after filtering\nin MBFC.\n\u2022Domain suffix analysis : Academic domains (.edu), gov-\nernment domains (.gov), and established organizational\ndomains (.org) receive higher credibility scores [41].\n\u2022Publication history : Domains with established publica-\ntion histories are considered more reliable than recently\ncreated websites [42].\n\u2022Citation patterns : For scientific claims, sources appear-\ning in citation databases such as Google Scholar or\nPubMed are prioritized.\n3) Evidence Retrieval: This content extraction process\nmarks a substantial improvement over previous methodologies\nthat rely solely on search result snippets ( [33], [43], [44]),\nwhich often lack the necessary context for thorough fact\nverification.\nOur method adopts a robust two-stage content extraction\nstrategy:\n1)Comprehensive full-text retrieval: We utilize Sele-\nnium [45] for browser automation in conjunction with\nBeautifulSoup [46] for HTML parsing. This integration\nsupports full JavaScript rendering, accurate element tar-\ngeting via CSS selectors and XPath expressions, and\nseamless navigation across dynamic web pages.\n2)Contextually relevant passage identification: Using\nGoogle\u2019s Gemini 1.5 Flash model [47], which features\nan expansive context window of approximately one\nmillion tokens, the system is capable of processing and\nanalyzing content from multiple articles simultaneously.\nFor each query, the system selects the top-ranked credible\nsource and extracts the full textual content. A custom prompt\nthen guides the LLM to isolate only the most pertinent\npassages, ensuring relevance to the original subclaim.\nThe extracted passages are stored in a structured evidence\nrepository, along with associated metadata, including thesource URL, credibility rating, and extraction timestamp. This\nrepository serves as the primary input for the subsequent\nVerdict Prediction Agent .\nD. Verdict Prediction Agent\nThe Verdict Prediction Agent serves as the final component\nin our multi-agent fact-checking pipeline. It is responsible for\nsynthesizing the evidence collected by the Evidence Seeking\nAgent and determining the veracity of each decomposed\nsubclaim. This agent evaluates the credibility, consistency\nand relevance of the evidence collected to deliver a final\nverdict either supported ornot_supported , along with\na human-interpretable explanation.\nThe decision-making process follows a structured, multi-\nstep methodology:\n1)Evidence analysis: The agent examines all retrieved\nevidence related to each subclaim and assesses the\ndegree of agreement between multiple sources. A higher\nlevel of consistency across diverse and credible sources\nincreases confidence in the resulting verdict.\n2)Structured voting mechanism: A weighted voting\nsystem is applied, where multiple strong pieces of\nsupporting evidence lead to a supported verdict. In\ncontrast, the presence of contradictory or insufficient\nreliable evidence results in a not_supported verdict.\n3)Explanation generation: For each decision, the agent\ngenerates a detailed explanation that references spe-\ncific evidence and describes the reasoning process. This\nensures both transparency and interpretability of the\noutput.\nIV. E XPERIMENTS\nA. Datasets\nTo evaluate our multi-agent fact-checking system, we con-\nducted experiments on three of the most widely recognized and\npopular benchmark datasets in fact verification. We employed\nstratified sampling to select 100 examples from each dataset,\nensuring balanced label distribution while managing compu-\ntational costs. Our system leverages a zero-shot, training-free\napproach, which mitigates this constraint by enabling robust\ngeneralization without the need for extensive fine-tuning or\nlarge-scale training data.\n\u2022HoVER [14]: Designed to challenge fact verification\nsystems with multi-hop reasoning, HoVER requires con-\nnecting multiple pieces of evidence across Wikipedia\narticles to verify claims. We stratified the samples of\nthe validation set into two-, three- and four-hop claims\nto rigorously evaluate our system\u2019s ability to handle\nincreasing complexity of reasoning.\n\u2022FEVEROUS [13]: This benchmark tests complex claim\nverification using both structured (tables) and unstruc-\ntured (text) data from Wikipedia, covering numerical,\nmulti-hop, and mixed-format reasoning challenges. We\nselected validation set claims to assess the versatility\nof our framework across these diverse reasoning types\ncritical to real-world fact checking.\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\n\u2022SciFact-Open [15]: Focused on open-domain scientific\nclaim verification, this dataset demands retrieving and\nevaluating evidence from the specialized literature with-\nout predefined veracity labels for all claims. We curated\nvalidation set claims with complete supporting or con-\ntradictory evidence, testing our system\u2019s precision in a\ndomain requiring deep knowledge.\nB. Baseline Approaches\nWe compare our multi-agent fact-checking framework\nagainst four representative baseline methods, each reflecting\na distinct paradigm in automated claim verification.\nDirect approach employs a simple closed-book fact-\nchecking strategy. An LLM receives the input claim and\ndirectly produces a veracity label ( true ,false , or\npartially true ) along with an explanatory rationale in\na single inference pass. This method relies entirely on the\nmodel\u2019s internal parametric knowledge without any external\nevidence retrieval. Although straightforward to implement, this\nbaseline is limited by potential knowledge staleness, hallucina-\ntion risks, and the lack of transparency in its reasoning process.\nChain-of-Thought (CoT) method, introduced by Wei et al.\n[48], enhances the model reasoning through structured prompt-\ning that encourages sequential step-by-step inference. For fact\nverification tasks, CoT is applied as a two-stage pipeline: (1)\ndecomposition of the claim into a set of verification sub-\nquestions, and (2) iterative reasoning over each sub-question\nto reach a final verdict. Although this improves interpretability\nvia intermediate reasoning steps, the approach is vulnerable to\ncascading errors and depends critically on the quality of the\ninitial decomposition.\nSelf-Ask with Search Engine (SA+SE) , based on the\nSelf-Ask framework [36], introduces a formalized decompo-\nsition strategy augmented by real-time knowledge retrieval.\nWhen the LLM generates a follow-up sub-question during\nreasoning, the system intercepts it and queries an external\nsearch engine. The retrieved content is then provided as an\nintermediate answer to inform the subsequent reasoning. This\nhybrid strategy combines parametric reasoning with external\nfactual grounding, though it is limited by the model\u2019s ability\nto formulate effective search queries and its lack of built-in\nmechanisms for evaluating source credibility.\nFOLK framework [33] integrates symbolic reasoning with\nexternal evidence grounding. It operates in three key stages:\n(1) decomposition of claims into FOL predicates, representing\natomic factual assertions; (2) external knowledge grounding\nvia search queries associated with each predicate; and (3)\nprediction of predicate-level veracity based on the retrieved\nknowledge.\nAlthough FOLK offers a high degree of interpretability and\nreduces hallucination risk by grounding each sub-claim in\nexternal data, it is hindered by several shortcomings. These\ninclude redundant predicate generation, reliance on shallow\nevidence from search snippets rather than full-document con-\ntent, and the absence of robust source credibility assessment,\nall of which can undermine performance in verifying complex\nor controversial claims.C. Experimental Settings\nWe evaluated our multi-agent system using open source\nmodels (Llama-3.2-1B1and Qwen-2.5-3B2) deployed locally\nvia Ollama3and commercial models (GPT-4o-mini4) accessed\nthrough API services. LangGraph5served as the orchestration\nframework, coordinating the workflow between agents and\nmanaging state transitions throughout the verification pipeline.\nFor evidence retrieval, we integrate Gemini-1.5-flash6to pro-\ncess the complete content of the document. We systematically\nvaried the number of generated questions per subclaim (1-\n5) to optimize the balance between verification thoroughness\nand computational efficiency. All experiments were carried\nout on stratified samples from HoVER, FEVEROUS, and\nSciFact-Open datasets with Macro F1 score as metrics in all\nconfigurations.\nD. Main Results\nTable II presents a comprehensive performance comparison\nbetween our proposed multi-agent system and four established\nbaseline approaches: Direct, CoT, SA+SE, and FOLK. All sys-\ntems were implemented using GPT-4o-mini as the foundation\nmodel, with our multi-agent system configured to generate 3\nqueries per subclaim. Our proposed MAS achieves superior\nperformance in six of seven evaluation tasks, as shown in\nTable II, demonstrating its robustness against diverse reasoning\nchallenges in claim verification. Two primary observations\nhighlight its effectiveness.\nFirst, MAS excels in complex multi-hop reasoning, partic-\nularly on the HoVER dataset. For 3-hop and 4-hop claims,\nMAS achieves F1-scores of 0.617 and 0.507, surpassing the\nbest baseline (FOLK) by 23.15% and 8.80%, respectively. This\nadvancement stems from MAS\u2019s structured pipeline which\ncomprises four specialized agents including Input Ingestion,\nQuery Generation, Evidence Retrieval, and Verdict Prediction.\nBy decomposing complex claims into verifiable subclaims and\ngenerating targeted queries, MAS improves evidence retrieval\nand synthesis accuracy. Unlike baselines such as Direct and\nCoT which falter in deeper reasoning tasks, MAS\u2019s FOL-\nguided approach ensures robust handling of intricate depen-\ndencies, improving reasoning fidelity.\nSecond, MAS demonstrates exceptional performance in di-\nverse evidence contexts, notably in FEVEROUS\u2019s Text+Table\nand SciFact-Open tasks, with F1-scores of 0.681 and 0.770, re-\nspectively. These results reflect a 4.93% relative improvement\nover FOLK in Text+Table and a 4.48% relative improvement\nin SciFact-Open, driven by MAS\u2019s ability to prioritize credible\nsources in real time. Unlike less structured approaches like\nSA+SE and Direct, MAS integrates FOL to ensure consistent\nverdict synthesis across heterogeneous evidence types, mini-\nmizing errors in numerical and multi-modal claims.\n1https://ollama.com/library/llama3.2:1b\n2https://ollama.com/library/qwen2.5:3b\n3https://ollama.com/\n4https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n5https://www.langchain.com/langgraph\n6https://ai.google.dev/gemini-api/docs/models\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nMethod HoVER FEVEROUS SciFact-Open\n2-Hop 3-Hop 4-Hop Numerical Multi-hop Text+Table\nDirect 0.521 0.449 0.419 0.465 0.502 0.591 0.497\nCoT 0.54 0.466 0.44 0.496 0.609 0.618 0.634\nSA+SE 0.542 0.489 0.46 0.553 0.612 0.542 0.609\nFOLK 0.595 0.501 0.466 0.487 0.630 0.649 0.737\nMAS (Ours) 0.600 (+0.005) 0.617 (+0.116) 0.507 (+0.041) 0.548 (+0.061) 0.601 0.681 (+0.031) 0.770 (+0.033)\nTABLE II: Macro F1-score of baseline methods and our Multi-Agent System (MAS) across the HoVER, FEVEROUS, and\nSciFact-Open datasets using GPT-4o-mini. The number of questions generated by the Query Generation Agent is 3. Bold\nindicates the best result in each column. MAS outperforms existing baselines in 6 out of 7 evaluation settings.\nFig. 3: Macro F1-score of varying the number of generated questions per claim from 1 to 5 for Query Generation Agent. The\nMacro F1-score generally increases as the number of generated questions per claim grows, peaking at 3 or 4 questions across\nmost settings.\nE. Impact of Number of Generated Questions\nTo assess the influence of question generation granularity,\nTable 3 presents results obtained by varying the number of sub-\nquestions generated per claim from 1 to 5. The experiments\nwere carried out using the GPT-4o-mini model, which showed\nthe best overall performance in the previous evaluation.\nThe results show that increasing the number of generated\nquestions enhances performance, particularly when scaling\nfrom 1 to 3 questions, with notable gains in the F1 score\nin HoVER 2-hop (0.472 to 0.600) and SciFact-Open (0.462\nto 0.770). This suggests that a finer decomposition of claims\nimproves the accuracy of evidence retrieval and verification.\nHowever, beyond three questions, the gains plateau or slightly\ndecline, as seen in HoVER 2-hop (dropping from 0.600 at\n3 questions to 0.538 at 5), likely due to redundant or noisy\nqueries. For more complex tasks such as HoVER 4-hop and\nSciFact-Open, performance continues to improve up to 4\nto 5 questions, indicating that deeper claims benefit from\ngreater granularity. In general, generating 3\u20134 questions per\nclaim provides the best balance between reasoning depth and\nevidence precision in multi-hop verification.\nF . Comparison with Open-source Models\nThe results in Table III highlight the varying performance\nof the baseline methods across different models and tasks.\nFor Llama-3.2-1B, CoT consistently outperforms all other\nmethods in most tasks, demonstrating its ability to leverage\nintermediate reasoning effectively even with limited model\ncapacity. However, more sophisticated methods such as FOLK\nand MAS struggle in this small architecture, producing scores\nclose to randomness in many settings. This suggests that,\nfor smaller models, adding complexity through multi-hop or\nmulti-agent strategies may overwhelm their ability to performrobust reasoning. This phenomenon is consistent with findings\nin the literature, where smaller models exhibit degraded perfor-\nmance on complex reasoning tasks, such as question answering\nand natural language inference, due to limited computational\ncapacity [49].\nWith Qwen-2.5-3B, the results show greater variability\nand improvement for methods employing more sophisticated\nstrategies. MAS performs strongly on 4-Hop and Numerical\ntasks, while SA+SE and FOLK outperform CoT in certain\ncases, reflecting the benefits of enhanced capacity for multi-\nhop and numerical reasoning. However, performance gains\nare not uniform across all tasks and methods, indicating that\nthe utility of sophisticated strategies depends not only on the\ncomplexity of the task but also on the scale of the underlying\nmodel [49]. Overall, these findings underscore the growing\npotential of multi-hop and multi-agent methods to outperform\nsimpler baselines as model capacity increases.\nV. A NALYSIS\nA. The Impact of Filtering out Non-verifiable Statements\nMost subclaims generated across datasets are verifiable,\nindicating effective decomposition, although the rate of un-\nverifiable components varies with claim complexity. Figure 4\nshows that the number of subclaims grows with the complexity\nof the claim, reflecting the need for finer decomposition to\nhandle multi-hop reasoning. The rate of unverifiable subclaims\nvaries across datasets, with HoVer 3-Hop and 4-Hop having\nthe highest proportions (up to 17.19%) and FEVEROUS and\nSciFact-Open having much lower rates (under 3.13%). This\nsuggests that certain datasets naturally present more subjective\nor hard-to-verify components. Overall, the low proportion\nof unverifiable subclaims, even for complex cases, indicates\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nMethod HoVER FEVEROUS SciFact-Open\n2-Hop 3-Hop 4-Hop Numerical Multi-hop Text+Table\nModel: Llama-3.2-1B\nDirect 0.392 0.493 0.448 0.446 0.443 0.502 0.577\nCoT 0.519 0.499 0.510 0.463 0.513 0.540 0.559\nSA+SE 0.315 0.369 0.329 0.333 0.333 0.333 0.367\nFOLK 0.315 0.346 0.329 0.333 0.333 0.355 0.342\nMAS (Ours) 0.315 0.369 0.329 0.329 0.333 0.333 0.365\nModel: Qwen-2.5-3B\nDirect 0.387 0.350 0.338 0.355 0.423 0.416 0.625\nCoT 0.387 0.310 0.338 0.355 0.405 0.436 0.538\nSA+SE 0.490 0.320 0.372 0.329 0.423 0.448 0.664\nFOLK 0.448 0.369 0.402 0.500 0.366 0.345 0.508\nMAS (Ours) 0.446 0.378 (+0.009) 0.493 (+0.091) 0.533 (+0.033) 0.363 0.502 (+0.054) 0.422\nTABLE III: Macro F1-score of baseline methods and MAS across HoVER, FEVEROUS, and SciFact-Open datasets using\nLlama-3.2-1B and Qwen-2.5-3B. The table highlights the varying performance of methods as model capacity increases, with\nCoT outperforming on small models and sophisticated methods like MAS and SA+SE yielding strong results on Qwen-2.5-3B.\nThe number of questions generated by the Query Generation Agent is 3.\nthat the decomposition process effectively breaks down most\nclaims into components that can be objectively evaluated.\nFig. 4: Number of subclaims and verifiable subclaims across\ndifferent datasets. The number of subclaims increases with\nclaim complexity, while the proportion of unverifiable sub-\nclaims remains low across most datasets.\nB. The Impact of Source Credibility Assessment\nCredibility filtering significantly reduces the number of\nusable links, especially for multi-hop datasets, reflecting a\nhigher prevalence of unreliable sources in more complex\ntasks. Figure 5 reveals that credibility filtering significantly\nreduces usable links across datasets, exposing the prevalence\nof low-credibility sources in open-domain retrieval. For HoVer,\nonly 9.6% of 2-hop and 15.8% of 3-hop links pass filtering,\nemphasizing the need for robust credibility checks to prevent\nunreliable evidence propagation. Conversely, FEVEROUS and\nSciFact-Open retain over 86% of links, indicating more trust-\nworthy retrieval and stronger alignment with credible sources.\nThese results highlight the critical role of credibility filtering\nin multi-hop fact verification and its varying impact across\ndatasets and tasks.\nFig. 5: Comparison of total retrieved links and credible links\nafter credibility filtering across datasets. The results highlight\na significant reduction in usable links for multi-hop datasets,\nreflecting a higher prevalence of unreliable sources.\nC. Assessing the Quality of Explanations\nMAS outperforms other methods by producing the most\ncoherent and comprehensive explanations, topping 5 of 7\ncategories across HoVER, FEVEROUS, and SciFact-Open\ndatasets. To evaluate the quality of explanations generated by\nFOLK, GPT-4 was chosen as the LLM-as-judge for ranking (1\nfor best, 4 for least preferred) due to its proven alignment with\nhuman judgment, outperforming other models in scoring tasks\nand surpassing human-to-human agreement consistency [50].\nWe instructed GPT-4 to compare and rank explanations pro-\nduced by four different methods: CoT, Self-Ask, FOLK and\nMAS. The evaluation focused on the following three criteria\nby methodology established by Atanasova et al. [51]:\n\u2022Coverage : The extent to which the explanation includes\nall salient and relevant information necessary to verify\nthe claim. Annotators were given access to gold-standard\nannotated evidence and asked to judge whether the ex-\nplanation adequately captured the critical points present\nin this evidence.\n\u2022Soundness : The logical consistency of the explanation,\nensuring that it does not contain contradictions with the\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nMethod HoVER FEVEROUS SciFact-Open\n2-Hop 3-Hop 4-Hop Numerical Multi-hop Text+Table\nCoT 3.41 3.15 3.28 3.01 2.85 2.95 2.87\nSA+SE 2.64 2.75 2.82 2.49 2.65 2.85 2.73\nFOLK 2.22 2.20 2.30 2.34 1.95 2.20 2.15\nMAS (Ours) 1.73 (-0.49) 1.90 (-0.30) 1.60 (-0.70) 2.16 (-0.18) 2.55 2.00 (-0.20) 2.25\nTABLE IV: Mean Average Ranks (MARs) of explanations, averaged across Coverage, Soundness, and Readability criteria, for\nHoVER, FEVEROUS, and SciFact-Open datasets. The lower MAR indicates a higher ranking and represents a better quality\nof an explanation. The best result per column is highlighted in bold . MAS consistently achieves the lowest MARs in most\ncategories, excelling in complex multi-hop tasks, while FOLK leads in specialized datasets, and CoT ranks lowest across all\ntasks.\nclaim or the provided gold evidence.\n\u2022Readability : The clarity and coherence of the expla-\nnation, measuring how easily a reader can understand\nthe content. This criterion emphasizes the explanation\u2019s\nlinguistic quality and comprehensibility.\nThe explanation quality results in Table IV show that MAS\nproduces the most coherent and comprehensive explanations\nacross most tasks. MAS tops 5 of 7 categories, particularly\nexcelling in complex 2-Hop (1.73), 3-Hop (1.90), and 4-Hop\n(1.60) scenarios. FOLK performs well in specialized cases,\nsecuring the best ranks in FEVEROUS Multi-hop (1.95) and\nSciFact-Open (2.15), while CoT consistently ranks lowest\n(from 2.85 to 3.41). SA+SE shows moderate performance but\nfalls short of MAS and FOLK. These results align with Ta-\nble II, confirming MAS\u2019s ability to produce more interpretable\nexplanations alongside higher accuracy.\nVI. C ONCLUSION\nThis paper introduces a novel multi-agent system for auto-\nmated fact-checking, designed to enhance accuracy, efficiency,\nand explainability in combating online misinformation. The\nsystem features four specialized agents: Input Ingestion, Query\nGeneration, Evidence Retrieval, and Verdict Prediction. This\narchitecture enables structured reasoning and modular infor-\nmation processing, significantly improving the precision of\nfact-checking. A key contribution is the robust evidence re-\ntrieval strategy, which prioritizes source credibility and utilizes\nthe full content of the document. Experimental evaluations on\nbenchmark datasets like FEVEROUS, HOVER, and SciFact\ndemonstrate the system\u2019s superior performance, achieving a\n12.3% relative improvement in Macro F1-score over baseline\nmethods.\nIn essence, this multi-agent system effectively addresses the\nlimitations of existing automated fact-checking approaches by\nproviding a more accurate, efficient, and transparent verifi-\ncation methodology. Its structured pipeline and emphasis on\ncredible source retrieval offer a scalable solution for real-\nworld fact-checking, aligning closely with human verification\npractices while providing interpretable outcomes.\nVII. L IMITATIONS AND FUTURE WORKS\nOur multi-agent fact verification system currently faces\nthree primary limitations. First, its reliance on the US-centric\nconfiguration of SerperAPI introduces geographic and culturalbias, limiting the effectiveness of claim verification in non-US.\ncontexts or culturally nuanced scenarios.Second, the evaluation\nframework emphasizes quantitative metrics, but does not in-\ncorporate human evaluation to assess the qualitative aspects of\nexplanations. Third, the overall architecture has its credibility\nfiltering based on MBFC, which is constrained by limited\ncoverage of international or domain-specific sources.\nTo address these challenges, future research should ex-\nplore: (1) enhancing geographic and cultural representative-\nness through regional API routing, language-localized search,\nand multilingual query generation [52]; (2) incorporating hu-\nman evaluations to assess explanation clarity and argument\nstrength [33]; (3) designing more comprehensive credibility\nassessment frameworks that integrate trust signals, domain\nexpertise, and dynamic source scoring [53]; (4) extending the\nsystem to handle multimodal claims involving images, videos,\nor audio [54]. These directions aim to build a more transparent\nand globally robust fact-checking system.\nREFERENCES\n[1] A. Bondielli and F. Marcelloni, \u201cA survey on fake news and rumour\ndetection techniques,\u201d Information Sciences , vol. 497, pp. 38\u201355, 2020.\n[2] L. Graves, \u201cUnderstanding the promise and limits of automated fact-\nchecking,\u201d Factsheet , 2018.\n[3] D. M. J. Lazer, M. A. Baum, Y . Benkler, A. J. Berinsky, K. M. Greenhill,\nF. Menczer, M. J. Metzger, B. Nyhan, G. Pennycook, D. Rothschild,\nM. Schudson, S. A. Sloman, C. R. Sunstein, E. A. Thorson, D. J. Watts,\nand J. L. Zittrain, \u201cThe science of fake news,\u201d Science , vol. 359, no.\n6380, pp. 1094\u20131096, 2018.\n[4] M. Cinelli, W. Quattrociocchi, A. Galeazzi, C. M. Valensise, E. Brugnoli,\nA. L. Schmidt, P. Zola, F. Zollo, and A. Scala, \u201cThe covid-19 social\nmedia infodemic,\u201d Scientific Reports , vol. 10, no. 1, p. 16598, 2020.\n[5] K. Shu, A. Sliva, S. Wang, J. Tang, and H. Liu, \u201cFake news detection on\nsocial media: A data mining perspective,\u201d ACM SIGKDD Explorations\nNewsletter , vol. 19, no. 1, pp. 22\u201336, 2017.\n[6] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\nJ. Zhang, Z. Dong et al. , \u201cA survey of large language models,\u201d arXiv\npreprint arXiv:2303.18223 , 2023.\n[7] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . Bang,\nA. Madotto, and P. Fung, \u201cSurvey of hallucination in natural language\ngeneration,\u201d ACM Computing Surveys , vol. 55, no. 12, pp. 1\u201338, 2023.\n[8] S. Feng, W. Jiang, C. Graber, K. Vafa, N. Mathur, Y . Zhou, B. I. P.\nRubinstein, and J. Leskovec, \u201cKnowledge staleness in large language\nmodels,\u201d arXiv preprint arXiv:2310.19215 , 2023.\n[9] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K \u00a8uttler, M. Lewis, W.-t. Yih, T. Rockt \u00a8aschel et al. , \u201cRetrieval-\naugmented generation for knowledge-intensive nlp tasks,\u201d in Advances in\nNeural Information Processing Systems , vol. 33, 2020, pp. 9459\u20139474.\n[10] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi,\nQ. Le, and D. Zhou, \u201cChain of thought prompting elicits reasoning in\nlarge language models,\u201d arXiv preprint arXiv:2201.11903 , 2022.\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\n[11] B. Chern, S. Peng, A. Verma, X. Yin, A. Ranade, N. Jindal, R. Joshi,\nA. Saxena, and R. Sarikaya, \u201cFactool: Factuality detection in generative\nai,\u201darXiv preprint arXiv:2307.13528 , 2023.\n[12] K. Popat, S. Mukherjee, J. Str \u00a8otgen, and G. Weikum, \u201cWhere the truth\nlies: Explaining the credibility of emerging claims on the web and social\nmedia,\u201d Proceedings of the 26th International Conference on World Wide\nWeb Companion , pp. 1003\u20131012, 2017.\n[13] R. Aly, Z. Guo, M. S. Schlichtkrull, J. Thorne, A. Vlachos,\nC. Christodoulopoulos, O. Cocarascu, and A. Mittal, \u201cFEVEROUS:\nFact extraction and VERification over unstructured and structured\ninformation,\u201d in Thirty-fifth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track (Round 1) , 2021.\n[Online]. Available: https://openreview.net/forum?id=h-flVCIlstW\n[14] Y . Jiang, S. Bordia, Z. Zhong, C. Dognin, M. Singh, and M. Bansal,\n\u201cHOVER: A dataset for many-hop fact extraction and claim verification,\u201d\ninFindings of the Association for Computational Linguistics: EMNLP\n2020 , 2020, pp. 3441\u20133460.\n[15] D. Wadden, S. Lin, K. Lo, L. L. Wang, M. van Zuylen, A. Cohan,\nand H. Hajishirzi, \u201cFact or fiction: Verifying scientific claims,\u201d in\nProceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , 2020, pp. 7534\u20137550.\n[16] PolitiFact, \u201cPolitifact,\u201d https://www.politifact.com/, accessed: 2025-03-\n20.\n[17] Snopes, \u201cSnopes,\u201d https://www.snopes.com/, accessed: 2025-03-20.\n[18] FactCheck, \u201cFactcheck,\u201d https://www.factcheck.org/, accessed: 2025-03-\n20.\n[19] M. A. Amazeen, \u201cRevisiting the epistemology of fact-checking,\u201d Critical\nReview , vol. 27, no. 1, pp. 1\u201322, 2015.\n[20] A. Vlachos and S. Riedel, \u201cFact checking: Task definition and dataset\nconstruction,\u201d in Proceedings of the ACL 2014 Workshop on Language\nTechnologies and Computational Social Science , 2014, pp. 18\u201322.\n[21] N. Hassan, G. Zhang, F. Arslan, J. Caraballo, D. Jimenez, S. Gawsane,\nS. Hasan, M. Joseph, A. Kulkarni, A. K. Nayak et al. , \u201cToward\nautomated fact-checking: Detecting check-worthy factual claims by\nclaimbuster,\u201d in Proceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , 2017, pp. 1803\u2013\n1812.\n[22] K. Popat, S. Mukherjee, A. Yates, and G. Weikum, \u201cDeclare: Debunking\nfake news and false claims using evidence-aware deep learning,\u201d in\nProceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing , 2018, pp. 22\u201332.\n[23] I. Augenstein, C. Lioma, D. Wang, L. C. Lima, C. Hansen, C. Hansen,\nand J. G. Pedersen, \u201cMultiFC: A real-world multi-domain dataset for\nevidence-based fact checking of claims,\u201d in Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , 2019, pp. 4685\u20134697.\n[24] J. Zhou, X. Han, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun, \u201cGEAR:\nGraph-based evidence aggregating and reasoning for fact verification,\u201d\ninProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics , 2019, pp. 892\u2013901.\n[25] C. Stiff and F. Johansson, \u201cDetecting covid-19 misinformation with\nveracity assessment of social media posts via deep learning,\u201d Journal of\nMedical Internet Research , vol. 23, no. 9, p. e30315, 2021.\n[26] N. Lee, B. Z. Li, S. Wang, W.-t. Yih, H. Ma, and M. Khabsa, \u201cTowards\nfew-shot fact-checking via perplexity,\u201d in Proceedings of the 2021\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies , 2021, pp.\n1971\u20131981.\n[27] Y . Du, S. Ding, Z. Zhao, Y . Lin, R. Nallapati, B. Xiang, B. Zhou,\nD. Roth, L. Zettlemoyer, X. Liang et al. , \u201cQuantifying and analyzing hal-\nlucinations in large language models,\u201d arXiv preprint arXiv:2310.00905 ,\n2023.\n[28] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, X. Hong, Y . Luo, W. Liang,\nL. Bing, L. Si et al. , \u201cThe rise and potential of large language model\nbased agents: A survey,\u201d arXiv preprint arXiv:2309.07864 , 2023.\n[29] Y . Li and K. Shu, \u201cCan large language models provide faithful explana-\ntions for fake news detection?\u201d arXiv preprint arXiv:2305.15005 , 2023.\n[30] X. Zhang, H. Jiang, W. Yin, X. Ren, and J. Han, \u201cWeb agents: Evaluating\nthe reliability of web search engines for ai agents,\u201d arXiv preprint\narXiv:2312.09254 , 2023.\n[31] M. Wang, Z. Yin, M. Guo, X. Jiang, X. Ren, and J. Han, \u201cSelf-checker:\nPlug-and-play modules for fact-checking with large language models,\u201d\narXiv preprint arXiv:2305.14623 , 2023.\n[32] H. B. Enderton, A mathematical introduction to logic . Elsevier, 2001.\n[33] H. Wang and K. Shu, \u201cExplainable claim verification via knowledge-\ngrounded reasoning with large language models,\u201d in Findingsof the Association for Computational Linguistics: EMNLP 2023 ,\nH. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association\nfor Computational Linguistics, Dec. 2023, pp. 6288\u20136304. [Online].\nAvailable: https://aclanthology.org/2023.findings-emnlp.416/\n[34] D. Micallef, K. Kakaes, A. Haghighi, S. Lightseed, L. Gu, Q. Liao,\nI. Liskovich, Y . W. Tay, and E. Kamar, \u201cClaim matching beyond\nenglish to scale global fact-checking,\u201d in Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1:\nLong Papers) . Association for Computational Linguistics, 2022, pp.\n5249\u20135264.\n[35] L. Konstantinovskiy, O. Price, M. Babakar, and A. Zubiaga, \u201cClaim de-\ntection in biomedical twitter posts,\u201d in Proceedings of the 20th Workshop\non Biomedical Language Processing . Association for Computational\nLinguistics, 2021, pp. 131\u2013142.\n[36] O. Press, M. Zhang, S. Min, L. Schmidt, N. Smith, and M. Lewis,\n\u201cMeasuring and narrowing the compositionality gap in language\nmodels,\u201d in Findings of the Association for Computational Linguistics:\nEMNLP 2023 , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore:\nAssociation for Computational Linguistics, Dec. 2023, pp. 5687\u20135711.\n[Online]. Available: https://aclanthology.org/2023.findings-emnlp.378/\n[37] M. R. Baye, B. De los Santos, and M. R. Wildenbeest, \u201cSearch engine\noptimization: what drives organic traffic to retail sites?\u201d Journal of\nEconomics & Management Strategy , vol. 25, no. 1, pp. 6\u201331, 2016.\n[38] S. Krrabaj, F. Baxhaku, and D. Sadrijaj, \u201cInvestigating search engine\noptimization techniques for effective ranking: A case study of an\neducational site,\u201d in 2017 6th Mediterranean Conference on Embedded\nComputing (MECO) , 2017, pp. 1\u20134.\n[39] Serper, \u201cSerper: Real-time google search api,\u201d https://serper.dev, 2024,\naccessed: 2025-04-05.\n[40] \u201cMedia bias/fact check api,\u201d https://mediabiasfactcheck.com/, accessed:\n2025-04-18.\n[41] M. J. Metzger and A. J. Flanagin, \u201cCredibility and trust of information\nin online environments: The use of cognitive heuristics,\u201d Journal\nof Pragmatics , vol. 59, pp. 210\u2013220, 2013, biases and constraints\nin communication: Argumentation, persuasion and manipulation.\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\nS0378216613001768\n[42] X. L. Dong, E. Gabrilovich, K. Murphy, V . Dang, W. Horn, C. Lugaresi,\nS. Sun, and W. Zhang, \u201cKnowledge-based trust: Estimating the trustwor-\nthiness of web sources,\u201d in Proceedings of the VLDB Endowment , vol. 8,\nno. 9. VLDB Endowment, 2015, pp. 938\u2013949.\n[43] X. Zhou, A. Sharma, A. X. Zhang, and T. Althoff, \u201cCorrecting\nmisinformation on social media with a large language model,\u201d\nArXiv , vol. abs/2403.11169, 2024. [Online]. Available: https://api.\nsemanticscholar.org/CorpusID:268513555\n[44] X. Zhang and W. Gao, \u201cTowards LLM-based fact verification on\nnews claims with a hierarchical step-by-step prompting method,\u201d in\nProceedings of the 13th International Joint Conference on Natural\nLanguage Processing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Linguistics (Volume\n1: Long Papers) , J. C. Park, Y . Arase, B. Hu, W. Lu, D. Wijaya,\nA. Purwarianti, and A. A. Krisnadhi, Eds. Nusa Dua, Bali: Association\nfor Computational Linguistics, Nov. 2023, pp. 996\u20131011. [Online].\nAvailable: https://aclanthology.org/2023.ijcnlp-main.64/\n[45] Selenium Contributors, \u201cSelenium: Web browser automation,\u201d https://\nwww.selenium.dev/, 2023, accessed: 2025-04-24.\n[46] L. Richardson, \u201cBeautiful soup documentation,\u201d 2007.\n[47] G. Team, P. Georgiev, V . I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer,\nD. Vincent, Z. Pan, S. Wang et al. , \u201cGemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context,\u201d arXiv preprint\narXiv:2403.05530 , 2024.\n[48] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi,\nQ. V . Le, and D. Zhou, \u201cChain-of-thought prompting elicits reasoning\nin large language models,\u201d in Proceedings of the 36th International\nConference on Neural Information Processing Systems , ser. NIPS \u201922.\nRed Hook, NY , USA: Curran Associates Inc., 2022.\n[49] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, \u201cLanguage models are few-shot learners,\u201d in Pro-\nceedings of the 34th International Conference on Neural Information\nProcessing Systems , ser. NIPS \u201920. Red Hook, NY , USA: Curran\nAssociates Inc., 2020.\n[50] L. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang, Z. Lin,\nZ. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica, \u201cJudging\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nllm-as-a-judge with mt-bench and chatbot arena,\u201d in Proceedings of\nthe 37th International Conference on Neural Information Processing\nSystems , ser. NIPS \u201923. Red Hook, NY , USA: Curran Associates Inc.,\n2023.\n[51] P. Atanasova, J. G. Simonsen, C. Lioma, and I. Augenstein,\n\u201cGenerating fact checking explanations,\u201d in Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics ,\nD. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, Eds. Online:\nAssociation for Computational Linguistics, Jul. 2020, pp. 7352\u20137364.\n[Online]. Available: https://aclanthology.org/2020.acl-main.656/\n[52] Google, \u201cManaging multi-regional and multilingual sites,\u201d 2024,\naccessed 15 June 2025. [Online]. Available: https://developers.google.\ncom/search/docs/specialty/international/managing-multi-regional-sites\n[53] B. Hilligoss and S. Y . Rieh, \u201cDeveloping a unifying framework\nof credibility assessment: Construct, heuristics, and interaction in\ncontext,\u201d Information Processing & Management , vol. 44, no. 4, pp.\n1467\u20131484, 2008. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S0306457307002038\n[54] A. Giachanou, G. Zhang, and P. Rosso, \u201cMultimodal fake news\ndetection with textual, visual and semantic information,\u201d in Text,\nSpeech, and Dialogue: 23rd International Conference, TSD 2020,\nBrno, Czech Republic, September 8\u201311, 2020, Proceedings . Berlin,\nHeidelberg: Springer-Verlag, 2020, p. 30\u201338. [Online]. Available:\nhttps://doi.org/10.1007/978-3-030-58323-1 3\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nPROMPT FOR MAS\nClaim Decomposition Prompt\nThe claim decomposition prompt instructs the language model to break down complex claims into atomic predicates using FOL representation. This prompt\nis used by the Input Ingestion Agent to transform natural language claims into verifiable components.\nYou are given a problem description and a claim. The task is to define all the predicates in\nthe claim and return them in JSON format, as shown in the example below.\nBelow is the example Claim: Howard University Hospital and Providence Hospital are both located\nin Washington, D.C.\n{ \"response\": \"Predicates:\nLocation(Howard_University_Hospital, Washington_D.C.) ::: Verify Howard University Hospital is\nlocated in Washington, D.C.\nLocation(Providence_Hospital, Washington_D.C.) ::: Verify Providence Hospital is located in\nWashington, D.C.\nBelow is the Claim: In 1959, former Chilean boxer Alfredo Cornejo Cuevas (born June 6, 1933)\nwon the gold medal in the welterweight division at the Pan American Games (held in Chicago,\nUnited States, from August 27 to September 7) in Chicago, United States, and the world\namateur welterweight title in Mexico City.\n{ \"response\": \"Predicates:\nBorn(Alfredo_Cornejo_Cuevas, June 6 1933) ::: Verify that Alfredo Cornejo Cuevas was born June\n6, 1933.\nWon(Alfredo_Cornejo_Cuevas, the gold medal in the welterweight division at the Pan American\nGames in 1959) ::: Verify that Alfredo Cornejo Cuevas won the gold medal in the\nwelterweight division at the Pan American Games in 1959.\nHeld(The Pan American Games in 1959, Chicago United States) ::: Verify that the Pan American\nGames in 1959 were held in Chicago, United States.\nWon(Alfredo_Cornejo_Cuevas, the world amateur welterweight title in Mexico City) ::: Verify\nthat Alfredo Cornejo Cuevas won the world amateur welterweight title in Mexico City.\nListing 1: Claim Decomposition Prompt. This prompt breaks complex claims into atomic predicates using FOL representation,\nenabling systematic verification by isolating individual factual assertions. The structured format facilitates comprehensive fact-\nchecking through clear predicate identification.\nSubclaim Classification Prompt\nThe subclaim classification prompt guides the language model in distinguishing between verifiable and non-verifiable claims. This prompt is utilized by\nthe Input Ingestion Agent to filter out claims that cannot be objectively verified.\n,\nYou are an expert in claim verification. Your task is to determine whether a given claim is\nverifiable or non-verifiable.\nA verifiable claim is a factual statement that can be checked against objective evidence from\nreliable sources. It makes specific assertions about the world that can be proven true or\nfalse through investigation.\nA non-verifiable claim is one that cannot be objectively verified because it:\n- Expresses a subjective opinion, preference, or personal experience\n- Makes vague or ambiguous statements without specific details\n- Refers to future events that haven\u2019t occurred yet\n- Makes normative or ethical judgments about what \"should\" be\n- Contains hypothetical scenarios or counterfactuals\nExamples:\nVerifiable: \"The average global temperature increased by 0.8$\u02c6\\circ$C between 1880 and 2012.\"\nNon-verifiable: \"Climate change is the most important issue facing humanity today.\"\nVerifiable: \"The film \u2019Parasite\u2019 won the Academy Award for Best Picture in 2020.\"\nNon-verifiable: \"Parasite deserved to win the Academy Award for Best Picture.\"\nPlease analyze the following claim and classify it as either VERIFIABLE or NON-VERIFIABLE.\nProvide a brief explanation for your classification.\nClaim: {claim}\nClassification:\nListing 2: Subclaim Classification Prompt. This prompt distinguishes between verifiable and non-verifiable claims using explicit\ncriteria, filtering out subjective opinions, vague statements, and future predictions. This step ensures fact-checking resources\nare directed only toward claims that can be objectively verified.\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\nQuery Generation Prompt\nThe query generation prompt directs the Query Generation Agent in transforming atomic subclaims into effective search queries. This prompt is designed\nto maximize the retrieval of relevant evidence from search engines.\n,\nFor each input subclaim, generate k Google search question(s) that could be used to find\nevidence to verify the subclaim.\nThe questions should be diverse, exploring different aspects or perspectives related to the\nsubclaim, while remaining clear and concise. Follow these guidelines:\n1. Use Specific Keywords: Include precise terms related to entities and relationships in the\nclaim.\n2. Incorporate Synonyms and Related Terms: Use alternative phrasings to overcome vocabulary\nmismatches.\n3. Vary Specificity: Generate both specific queries targeting exact details and broader queries\nthat may capture contextual information.\n4. Consider Different Angles: Approach the claim from multiple perspectives to ensure\ncomprehensive evidence gathering.\n5. Maintain Simplicity: Keep questions straightforward and directly relevant to the claim.\nReturn the output in JSON format like this:\n[{\n\"claim\": \"Location(Howard Hospital, Washington D.C.) ::: Verify Howard University Hospital\nis located in Washington, D.C.\",\n\"questions\": [\"Where is Howard Hospital located?\"]\n}]\n}\nListing 3: Query Generation Prompt. This prompt transforms subclaims into diverse search queries using specific keywords,\nsynonyms, and varying levels of specificity. This approach ensures comprehensive evidence gathering by exploring multiple\nangles while maintaining query relevance to the verification task.\nContent Retrieval Prompt\nThe content retrieval prompt is used by the Evidence Seeking Agent to extract relevant information from web content. This prompt helps to filter lengthy\nweb pages to identify only passages that directly address the verification query. ,\nYou are a helpful assistant who extracts information from text.\nGiven the following query and text content, extract only the sentences or phrases that directly\nrelate to the query. Do not include any information that is not relevant.\nIf the content contains no relevant information, return None.\nQuery: {query}\nContent:\n{content}\nRelevant Information:\nListing 4: Content Retrieval Prompt. This prompt extracts only information directly relevant to verification queries from\npotentially lengthy web content. This focused approach improves efficiency by filtering out extraneous information and ensuring\nthat only pertinent evidence is considered in the verification process.\nVerdict Prediction Prompt\nThe verdict prediction prompt guides the Veracity Prediction Agent in synthesizing evidence and determining whether a subclaim is supported by the\nretrieved information. This prompt represents the final stage of the fact-checking pipeline.\n,\nYou are an AI assistant responsible for determining whether a subclaim is supported by\nretrieved evidence.\n## Provided Information:\nThis is a claim to do fact-checking:\n\\\\n {claim}\nHere is the given subclaims, its subquestions, and retrieved evidence for each subquestion:\n\\\\n {cell}\n## Decision-Making Process:\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\n1. Analyze the Retrieved Evidence\n- Review all provided evidence relevant to the subclaim.\n- Assess the credibility, consistency, and reliability of each piece of evidence.\n2. Apply a Voting System for Classification\n- If multiple sources strongly support the subclaim, classify it as \"supported\".\n- If multiple sources contradict the subclaim, classify it as \"not_supported\".\n- If the evidence is mixed, insufficient, or inconclusive, classify it as \"not_supported\".\n3. Provide a Justification\n- Clearly explain why the subclaim is classified as \"supported\" or \"not_supported\".\n- Reference key pieces of evidence that influenced your decision.\n- If the evidence is inconclusive, explain the limitations or uncertainties.\n- Remember to adjust not to include \" for later parse\n## Response Format:\nYour response must be a structured JSON object:\n\u2018\u2018\u2018json\n{{\n\"label\": \"supported\" or \"not_supported\",\n\"explanation\": \"A concise, evidence-based summary supporting your decision.\"\n}}\nListing 5: Verdict Prediction Prompt. This prompt synthesizes the evidence collected to determine the support for the subclaim\nthrough systematic analysis of credibility, consistency, and reliability. The structured decision-making process ensures justified\nverdicts with clear explanations based on the weight of evidence.\nPROMPT FOR EXPLANATION EVALUATION\nThe prompt is used by GPT-4 for the explanation evaluation task.\nYou are an expert evaluator for automated fact-check explanations. Your task is to:\n- Review the original claim, its label, and the explanations produced by 4 methods. Each method\nmay produce a different label; consider this when evaluating Soundness.\n- Evaluate each explanation according to 3 criteria:\n1. Coverage: To what extent the explanation includes all the salient and relevant information\nnecessary to verify the claim.\n2. Soundness: The logical consistency of the explanation; whether it supports or contradicts\nits own label and the original claim.\n3. Readability: The clarity and coherence of the explanation; how easily a human can follow\nand understand it.\n- Provide a **ranking (1 for best, 4 for worst) **for each criterion.\nHere is the input:\n{\n\"original_claim\": \"\",\n\"explanations\": {\n\"CoT\": { \"label\": \"<label>\", \"explanation\": \"<explanation>\" },\n\"Self-Ask\": { \"label\": \"<label>\", \"explanation\": \"<explanation>\" },\n\"FOLK\": { \"label\": \"<label>\", \"explanation\": \"<explanation>\" },\n\"MAS\": { \"label\": \"<label>\", \"explanation\": \"<explanation>\" }\n}\n}\nThe output should be in the format\n{\n\"ranking\": {\n\"Coverage\": { \"1\": \"<method>\", \"2\": \"<method>\", \"3\": \"<method>\", \"4\": \"<method>\" },\n\"Soundness\": { \"1\": \"<method>\", \"2\": \"<method>\", \"3\": \"<method>\", \"4\": \"<method>\" },\n\"Readability\": { \"1\": \"<method>\", \"2\": \"<method>\", \"3\": \"<method>\", \"4\": \"<method>\" }\n}\n}\nListing 6: Explanation Evaluation Prompt. This prompt assesses fact-checking explanations across multiple methods using\ncoverage, soundness, and readability criteria. This comparative ranking framework provides quantitative measures of explanation\nquality while acknowledging potential differences in verdict determination.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval", "author": ["T Trinh", "M Nguyen", "TS Hy"], "pub_year": "2025", "venue": "arXiv preprint arXiv:2506.17878", "abstract": "The rapid spread of misinformation in the digital era poses significant challenges to public  discourse, necessitating robust and scalable fact-checking solutions. Traditional human-led"}, "filled": false, "gsrank": 303, "pub_url": "https://arxiv.org/abs/2506.17878", "author_id": ["", "", "JiKBo6UAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:TrwaMP1n4d4J:scholar.google.com/&output=cite&scirp=302&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D300%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=TrwaMP1n4d4J&ei=OrWsaI6ND8DZieoPqdqh8QU&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:TrwaMP1n4d4J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2506.17878"}}, {"title": "Annotating and analyzing biased sentences in news articles using crowdsourcing", "year": "2020", "pdf_data": "Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020) , pages 1478\u20131484\nMarseille, 11\u201316 May 2020\nc\rEuropean Language Resources Association (ELRA), licensed under CC-BY-NC\n1478Annotating and Analyzing Biased Sentences in News Articles using\nCrowdsourcing\nSora Lim1, Adam Jatowt1, Michael F \u00a8arber2, Masatoshi Yoshikawa1\n1Kyoto Univiersity,2Karlsruhe Institute of Technology\ninfosora@db.soc.i.kyoto-u.ac.jp, adam@dl.kuis.kyoto-u.ac.jp, michael.faerber@kit.edu, yoshikawa@i.kyoto-u.ac.jp\nAbstract\nThe spread of biased news and its consumption by the readers has become a considerable issue. Researchers from multiple domains\nincluding social science and media studies have made efforts to mitigate this media bias issue. Speci\ufb01cally, various techniques ranging\nfrom natural language processing to machine learning have been used to help determine news bias automatically. However, due to\nthe lack of publicly available datasets in this \ufb01eld, especially ones containing labels concerning bias on a \ufb01ne-grained level (e.g., on\nsentence level), it is still challenging to develop methods for effectively identifying bias embedded in new articles. In this paper, we\npropose a novel news bias dataset which facilitates the development and evaluation of approaches for detecting subtle bias in news\narticles and for understanding the characteristics of biased sentences. Our dataset consists of 966 sentences from 46 English-language\nnews articles covering 4 different events and contains labels concerning bias on the sentence level. For scalability reasons, the\nlabels were obtained based on crowd-sourcing. Our dataset can be used for analyzing news bias, as well as for developing and evalu-\nating methods for news bias detection. It can also serve as resource for related researches including ones focusing on fake news detection.\nKeywords: News Bias, Dataset, Media Bias, Crowd-sourcing\n1. Introduction\nNews articles are not always written in a neutral manner,\nbut may deviate from the norm by using dedicated words,\na speci\ufb01c writing style, or a preferred author\u2019s viewpoint\n(Hackett, 1984; Morstatter et al., 2018). Such characteris-\ntics of media is referred to as media bias and in the con-\ntext of news articles as news bias. News bias has been a\nchallenge for a long time in the world of media. Truthful-\nness, fairness, accuracy, and balanced viewpoints have been\nemphasized in the context of news reporting to avoid news\nbias, because news can have a large in\ufb02uence on the read-\ners, creating viewpoints and attitudes of people towards so-\ncial issues, and eventually changing political views and the\nsociety (Niven, 2002). Recognizing news bias is therefore\nan important goal in the world of media and it can support\n(or alert) not only readers consuming news but also help\nauthors to write texts in neutral style. Nowadays, news ar-\nticles are mainly published online and are read from var-\nious news channels. To monitor and prevent news bias\nin a timely and ef\ufb01cient manner, \ufb01rst computational ap-\nproaches (Park et al., 2009; Hutto et al., 2015; Hamborg\net al., 2018) have been developed which aim to identify\nnews bias automatically. These methods are based on tech-\nniques from natural language processing (NLP) and ma-\nchine learning. However, detecting news bias automatically\nis still a major challenge. This can be traced back to several\nfactors.\nFirst of all, news bias is often subtle. Because fairness,\nfactuality, and veracity are considered as crucial for news\nreporting, the bias often appears with a slight difference of\nmeaning between words and subtle word choice. For in-\nstance, it can make a difference when speaking of \u201cclimate\nchange\u201d or of \u201cglobal warming\u201d (Schuldt et al., 2011), or\nwhen speaking of \u201cillegal immigrants\u201d vs. \u201cundocumented\nimmigrants\u201d (Lim et al., 2018). As these examples illus-\ntrate, identifying such differences, and, thus, media biasoverall, typically requires not only applying sentiment anal-\nysis on the news articles, but also obtaining deep under-\nstanding of reported news events and their context.\nSecondly, there is considerable lack of datasets which con-\ntain proper labels for news bias. Authors of existing ap-\nproaches to news bias detection often create their own\n(manageable) datasets (Morstatter et al., 2018). They\nmostly use RSS feeds or crawl news websites for the dataset\nconstruction. Naturally, creation of these datasets takes a\nlot of efforts. Moreover, news bias is understood in dif-\nferent ways and labels for bias are provided with different\ngranularity. Particularly, there are very few datasets avail-\nable that focus on the \ufb01ne-grained differences on the sen-\ntence level. Note that bias in one or just a few sentences\nin an article might already cause biased opinions in readers\nand therefore focusing on sentences when evaluating bias is\nnecessary. However, prior datasets mostly indicate the bias\nstatus on the document level or on a news source (outlet)\nlevel (see Section 2). In the latter case, all articles from the\nsame source receive the same bias label due to the inheri-\ntance from their source.\nIn this research, we create a news bias dataset1which can\nbe applied for detecting subtle differences in news arti-\ncles, and, thus, for analyzing and creating approaches for\ndetecting news bias on a \ufb01ne-grained level. In particular,\nthe dataset contains news bias annotations on the sentence\nlevel. Furthermore, unlike other resources, our dataset con-\ntains articles about different news topics. In particular, we\nselected 4 topics covering issues on the English news re-\nported between September 2017 and May 2018. Note that\nthe covered news topics are not only associated with pol-\nitics, but also with other issues, such as \u201cRepublican law-\nmaker commits suicide amid sexual molestation allegations\n1The dataset can be downloaded for research\nuse at https://github.com/skymoonlight/\nbiased-sents-annotation .\n1479(named as Johnson)\u201d. All these aspects make our dataset\nunique and promising for future research.\nTo understand how bias appears in the collected news, we\nalso perform an initial analysis of our dataset. Among other\nthings, we derive statistics concerning the agreement be-\ntween crowd workers, and compare different user groups\nw.r.t. the perceived bias.\nThe rest of this paper is structured as follows: In Section 2,\nwe outline existing datasets in the area of news bias detec-\ntion and related approaches. Section 3 presents the way\nin which we constructed the proposed dataset. We also\nshow the format of the dataset to facilitate an easy reuse\nand adoption. After describing our \ufb01ndings on the analysis\nof perceived bias in Section 4, we conclude in Section 5.\n2. Related Work\nNews bias de\ufb01nitions. Several prior works have focused\non media bias in general and news bias in particular. Gen-\nerally, according to D\u2019Allessio and Allen (D\u2019Alessio and\nAllen, 2000), media bias can be divided into three differ-\nent types: (1) gatekeeping , (2) coverage and (3) statement\nbias.Gatekeeping bias is a selection of stories out of the\npotential stories; coverage bias expresses how much space\nspeci\ufb01c positions receive in media; statement bias , in con-\ntrast, denotes how an author\u2019s own opinion is woven into\na text. Similarly, Alsem et al. (Alsem et al., 2008) divide\nnews bias into ideology andspin.Ideology re\ufb02ects news\noutlets\u2019 desire to affect readers\u2019 opinions in a particular di-\nrection. Spin re\ufb02ects the outlet\u2019s attempt to simply create\na memorable story. Given these distinctions, we consider\nthe bias type tackled in this paper as statement bias w.r.t.\n(D\u2019Alessio and Allen, 2000) and as spin bias according to\n(Alsem et al., 2008).\nHyperpartisan detection datasets. Although news arti-\ncles are a popular resource for performing research in com-\nputational linguistics and natural language processing, the\nnumber of datasets dealing with news bias detection is\nvery limited. Noteworthy is, \ufb01rst of all, our preliminary\ndataset (Lim et al., 2018). For this dataset, only a sin-\ngle news event was considered and bias labels were pro-\nvided on the word level. In contrast, the currently proposed\ndataset covers several events and considers bias on the sen-\ntence level. We can further mention the effort to promote\nthe development of novel approaches to media bias detec-\ntion within the frame of the SemEval 2019 Task 4 \u201cHy-\nperpartisan News Detection\u201d (Kiesel et al., 2019). For this\nchallenge, the organizers published a large dataset consist-\ning of 754k news articles. However, bias has been de\ufb01ned\nas hyperpartisan, i.e., w.r.t. a political stance. News arti-\ncles are then labeled as right ,left, and main stream . Fur-\nthermore, apart from the relatively small sub-part which\nwas annotated manually, most of the articles were sim-\nply labeled according to their news sources. Consequently,\nthe source-level bias annotation has just been automatically\nused for assigning the document-level bias.\nHorne et al. (Horne et al., 2018) present a large dataset\nof 136k news articles from 92 news sources for studying\nthe complex media landscape. Both fake sources, satire\nsources, and hyperpartisan political blogs are considered.Similar to the other datasets, labels are provided on the doc-\nument level.\nThe dataset of Cremisini et al. (Cremisini et al., 2019) is\na recently published dataset containing news articles con-\ncerning the Ukrainian Crisis of 2014\u20132015 from 43 coun-\ntries. The bias of each article was classi\ufb01ed as pro-Russion,\npro-Western, or neutral. Although this dataset can be con-\nsidered as the closest one to our proposal, there are still sig-\nni\ufb01cant differences. For instance, the authors labeled data\non the article level, while our dataset has labels on the level\nof sentences. Moreover, our dataset covers several news\ntopics and can thus be used to compare biases across differ-\nent topics and domains.\nStance classi\ufb01cation datasets. The dataset of Ferreira and\nVlachos (Ferreira and Vlachos, 2016) is an example of a\ndataset in the area of stance classi\ufb01cation . Stance classi-\n\ufb01cation describes the task of determining the stance of the\nauthor of a text document: whether the author illuminates\nnot only one party, but also the opposition. Consequently,\nstance classi\ufb01cation is related to bias detection. However,\nagain, while we consider here subtle differences in the writ-\ning, stance classi\ufb01cation operates on the document level.\nFake news detection datasets. Finally, various datasets\nhave been published for fake news detection. Among the\nmost recent and largest datasets in this regard is the one\nfrom Wang et al. (Wang, 2017). Note that this dataset cov-\ners 13,000 manually labeled short statements, but purely\nin the domain of politics. Also the fake news detection\ndataset of Perez-Rosas (P \u00b4erez-Rosas et al., 2018) has been\npublished recently. The authors cover seven news domains.\nNote, however, that fake news detection conceptually dif-\nfers from bias detection, so that such datasets cannot be\nused for bias detection research.\n3. Dataset\n3.1. Constructing News Collection\nNews bias is always relative (i.e., in relation to explicit or\nimplicit reference) and depends on the context of the news\nevent. One way for handling the relative characteristic, is\nto compare the content of different news articles which are\nreporting the same news event, and then, contrast the words\nused in the articles through the different news outlets (Lim\net al., 2018). We also follow this strategy for creating our\nnew dataset. We choose four different news events and\ncollect the news articles reporting those events. The news\nevents used by us are entitled as follows (using the titles of\nreference articles):\n1. \u201cTrump Clashes With Sports World Over Player\nProtests\u201d2,\n2. \u201cFacebook critics want regulation, investigation after\ndata misuse\u201d3,\n3. \u201cTillerson says U.S. ready to talk to North Korea;\nJapan wants pressure\u201d4, and\n2https://www.voanews.com/usa/\ntrump-clashes-sports-world-over-player-protests\n3https://reut.rs/2plNZaZ\n4https://reut.rs/2BfEzFL\n1480Event News Source Title of Target Article Title of Reference Article\nNFLABC News Trump: \u2018Standing with locked arms is good,\nkneeling is not acceptable\u2019(VOA) Trump Clashes With Sports\nWorld Over Player Protests\nDaily Beast Trump\u2019s Attack On Black Athletes May\nBring a League to Its Feet\nFacebookDaily Mail Trump-linked Cambridge Analytica tapped\n50M Facebook pro\ufb01les(Reuters) Facebook critics want\nregulation, investigation after data\nmisuse New York\nTimesData Firm Tied to Trump Campaign Talked\nBusiness With Russians\nNorth KoreaNewsweek U.S. Will Talk To North Korea \u2018Until The\nFirst Bomb Drops,\u2019 Rex Tillerson Says(Reuters) Tillerson says U.S. ready\nto talk to North Korea; Japan wants\npressure ABC News Tillerson tries to quell anxieties at State Dept.\namid questions about his future\nJohnsonWashington\nPostDan Johnson suicide: Lawmaker accused of\nmolesting teen killed himself. His widow calls\nit a\u2018high-tech lynching .\u2019(Reuters) Republican lawmaker\ncommits suicide amid sexual\nmolestation allegations\nNBC News Dan Johnson, Kentucky lawmaker who killed\nhimself, claimed he raised woman from the\ndead\nTable 1: Sample news articles in our dataset. Bold texts show diverse aspects on the news events.\n4. \u201cRepublican lawmaker commits suicide amid sexual\nmolestation allegations\u201d5.\nThese events are in connection with allegation of sexual\nmolestation, con\ufb02ict between racism and patriotism, a data\n\ufb01rm intervened election, and international relationships, re-\nspectively. In the following, we refer to these events shortly\nas (1) NFL, (2) F ACEBOOK , (3) N ORTH KOREA , and (4)\nJOHNSON . Table 1 shows brief examples of each news\nevent.\nFor collecting news articles on the same event from differ-\nent news outlets, we choose Google News which provides\nclustered news articles on the main issues of the current\ntime. We collect the articles linked from the Google news\u2019\npages and extract the title and text content (after manual\ninspection in order to exclude image-only or unrelated ar-\nticles). The resulting dataset consists of 371 articles for\nNFK, 103 articles for the event F ACEBOOK , 39 articles for\nNORTH KOREA , and 44 news articles for the event J OHN -\nSON. The titles of the articles belonging to each news event\nare very diverse, indicating that the news articles emphasize\ndifferent aspects and therefore having potentially different\ndegrees of bias. For example, for the event J OHNSON ,\none news article emphasizes the suicide event as \u201chigh-tech\nlynching\u201d, while another one degrades the event by refer-\nring \u201cclaimed he raised woman from the dead\u201d. Also the\nword choice (e.g., \u201cattack on black athletes\u201d vs. \u201cclashes\nwith sports words over players protests\u201d) for the event NFL\ndiffers signi\ufb01cantly. As our goal is to annotate the sen-\ntences, we split all the news articles into sentences by using\nthe popular module from NLTK (Bird et al., 2009).\n3.2. Annotation of News Bias\nTo overcome scalability issue concerning annotations,\ncrowdsourcing has been recently widely used (Zubiaga et\nal., 2015). We also rely on this approach in our research to\n5https://reut.rs/2AEnW78obtain labels for the news article sentences w.r.t. their de-\ngree of bias. Crowdsourcing has also the additional bene\ufb01t\nof allowing for understanding bias from the point of view of\nordinary users who are likely to be typical readers of news\narticles.\nWe use Figure Eight6as an underlying crowdsourcing plat-\nform, since this platform has already been widely used\nin many other researches, particularly for tasks which are\nrather hard for crowdworkers.\nWe note that it is rather dif\ufb01cult to provide bias-related la-\nbels such as binary judgements on each sentence of news ar-\nticles, as the bias may depend in various ways on the news\nevent and its context. Thus, to effectively design the bias\nlabeling task, we pick one news article as a reference ar-\nticle for each analyzed news event. The reference article\nhas been selected from well-known news agencies which\nsupply news items to the news outlets like Reuters or AP.\nTypically, such news agencies are known to be neutral and\ncontaining the least bias. Having a reference article serves\ntwo functions: Firstly, reading \ufb01rst the reference news arti-\ncle about the described news event lets annotators become\n\ufb01rst familiar with the overall news event. If no such refer-\nence article were provided, crowd-workers might miss im-\nportant context information to properly judge the bias of\nsentences later on. Secondly, assuming that the reference\narticle is relatively bias-free, it might be easier for crowd-\nworkers to recognize biased sentences in the target articles\n(e.g., due to noticing different word choices, e.g., \u201cattack\non black athletes\u201d vs. \u201cclashes over player protests\u201d).\nAs the preliminary stage to construct a large news bias\ndataset, we run the task with a subset of articles which are\nequally sampled according to their news outlet\u2019s political\nstance instead of labeling whole news articles in the dataset.\nFor the sampling step, we make use of Media Bias/Fact\n6https://www.figure-eight.com\n1481\nFigure 1: Screenshot of one part of the user interface for our crowd-sourcing task.\nCheck7website which provides the information of political\nposition of over 2900 media sources according to the ideol-\nogy spectrum. Speci\ufb01cally, for each news event, we picked\nfor each political position randomly four news articles.\nIn the annotation tasks, crowd-workers were asked to rate\nthe degree of bias for each news sentence in each target\narticle based on the four-scale category: neutral and not\nbiased, slightly biased, biased, and very biased.\nAfter the completion of the annotation for sentences in each\ntarget news article, the crowd-workers are also asked about\nthe bias degree of the entire target article compared to the\nreference article, and whether they knew about the reported\nnews event before. Our labeling interface is shown in Fig-\nure 1.\nIn total 28 users contributed to our labeling task after ex-\ncluding untrustworthy annotators. We \ufb01ltered out those\npeople who submitted all the same ratings only or irrelevant\nratings and had unexpectedly quick answering time through\ntheir jobs by using a given function of the crowd-sourcing\nplatform and manual examination. Overall, we collected\n4,515 annotations for 966 sentences (5 annotations per each\nsentence) from 46 news articles through 215 unit labeling\ntasks. An overview of our dataset is given in Table 3.\n4. Analysis of Perceived Bias\nAs we let \ufb01ve different crowd-workers rate the bias de-\ngree of 21 sentences (the article\u2019s title and the \ufb01rst 20 arti-\ncle\u2019s sentences) in a job unit, 996 sentences are tagged with\nmultiple bias category, resulting in 29.97% neutral and not\nbiased, 34.37% slightly biased but acceptable, 28.17% bi-\nased, and 7.49% very biased \u2013 as shown in Figure 2. When\nwe converted bias degrees to binary judgments (such as the\n\ufb01rst two considered as unbiased and the latter two as bi-\nased), 35.55% sentences were assigned to biased sentence\ncategory. Table 2 shows examples of the users\u2019 ratings.\nAfter we collect the bias annotations, we examined the\ninter-rater reliability among the \ufb01ve crowd-workers\u2019 an-\nswers. We calculated Fleiss\u2019 kappa score (Fleiss, 1971),\nwhich is widely used measure to check the extent of the\n7https://mediabiasfactcheck.com\nFigure 2: Bias label distribution on document-level\nanswer agreement among any number of raters giving cat-\negorical ratings to a \ufb01xed number of items (in our case,\n\ufb01ve raters giving 4-category ratings to 21 items). The mean\nscores calculated over all the target articles in each news\nevents are -0.062, -0.078, -0.014 and -0.049 for the labels\nconcerning the news events J OHNSON , FACEBOOK , NFL,\nand N ORTH KOREA , respectively. As shown in Figure 3,\nthe agreement tendency is low. We suspect this low agree-\nment be the result of different sensitivity to the bias be-\ntween users, as also reported during the creation of a sim-\nilar dataset in (Lim et al., 2018). We next narrowed down\nthe diversity caused by sensitiveness gap by aggregating the\nfour-scaled bias category to a binary label. In this way, we\nobtained Fleiss\u2019 kappa mean scores (-0.014, -0.114, 0.0004,\nand -0.0824) (see also Figure 4).\n4.1. User Attributes\nDuring the bias annotation, we also asked the raters whether\nthey already had any knowledge about the reported news\nevents. Based on the answer to this question, we cat-\negorized the crowd-workers into the user group \u201cpeople\nwho knew\u201d and \u201cpeople who didn\u2019t know\u201d the target news\nevent. Here our hypothesis was that when people have\nsome knowledge about the news event, they may already\n1482Sentence Label\nIn the post, written on Wednesday, Johnson paid tribute to his family, saying he had suffered post-traumatic\nstress disorder for 16 years - \u201c a sickness that will take my life \u201d.biased\nWe\u2019ve been promised by Google, Facebook, and other social sites that our personal information is protected\nand that when some of our information is provided to third parties, our identity will never be made known .biased\nSecretary of State Rex Tillerson is ready to talk about talking to North Korea. neutral\nThis Wednesday, Kentucky state representative Dan Johnson was found dead by a bridge in Mount Washington\nwith a \u201csingle gunshot wound\u201d to the head, according to Bullitt County Coroner Dave Billings.neutral\nTable 2: Example sentences from our dataset with the labels obtained via the performed crowdsourcing task. Bold texts are\nconsidered as a clue of the label judgements.\nName Description Content\nevent news event of the articles fJonson, Facebook, NFL, NorthKorea g\ndate event published date of the news articles f2017-12-15, 2018-03-18, 2017-09-24, 2017-12-13 g\nidarticle individual id of the news article from 1 to 46\nlocsentence position of the sentence in the news articles title, from 0 to 19\nsource news sources of the articles fWashington Post, New York Times, ... g\nsource biaspolitical stance of the news source\nof the articlefleft, left-center, least, right-center, right g\nref the reference article Reuters, V oice of America (NFL)\nurl urls of the news article\narticle bias labeled bias degree of the target article fneutral, slightly biased but acceptable, biased, very biased g\nsentence bias labeled bias degree of each sentences fneutral, slightly biased but acceptable, biased, very biased g\npreknow rater\u2019s prior knowledge of the news event fyes, no g\nTable 3: Information stored in our dataset\nJohnson Facebook NFL North Korea\nnews event0.20\n0.15\n0.10\n0.05\n0.000.050.100.15Fleiss' kappa\nFigure 3: Inter-rater reliability on the Crowdsourcing re-\nsult: four-scale case\nhave formed their own opinion based on the previously ac-\nquired information from the news media (which was un-\ncontrolled for us as task providers). Consequently, the hy-\npothesis was that this user group might differ more from the\n\u201cpeople who didn\u2019t know\u201d group w.r.t. bias. Figure 5 shows\nthe distribution of the user groups according to the prior\nknowledge about news events. To determine these user-\ngroup differences, we analyzed the inter-rater agreement\nscores according to the two user groups. The results show\nthat the average agreement scores differ between these two\nuser groups (see Figure 6). If we remove the special cases in\nwhich only one person among the 5 annotators has already\nprior knowledge or no prior knowledge, and then compare\nJohnson Facebook NFL North Korea\nnews event0.25\n0.20\n0.15\n0.10\n0.05\n0.000.050.100.15Fleiss' kappaFigure 4: Inter-rater reliability on the Crowdsourcing re-\nsult: binary case\nthe agreement scores among the two user groups, the \u201cpeo-\nple who knew\u201d group still has a higher agreement score.\n(Note that the event Johnson leads the \u201cpeople who knew\u201d\ngroup to empty after removing the above special cases. For\nthe sake of comparison, the overall scores \ufb01ll in for the\nblank in Figure 6.) However, as Figure 5 shows, overall\nthere was only a considerably small ratio of people who\nknew the news event.\n4.2. News Sources\nEven though our focus is on the sentence-level bias in news\narticles, we should not overlook the political tendency of\nthe news outlet which issues the news article. We can con-\n1483\nFigure 5: Crowd-workers\u2019 prior knowledge about the news\nevents\nFigure 6: Inter-rater reliability according to the prior\nknowledge about the news events\nsider that if people have established their political prefer-\nence and support one speci\ufb01c party, they are less likely to\nrecognize the bias in the news articles of the news sources\nwhich re\ufb02ect their political views. However it seems to\nbe inappropriate to ask directly people about their politi-\ncal preference. Therefore, we indirectly examine the po-\nlitical stance of the target news article\u2019s sources and its re-\nlationship with the inter-rater agreement. In this step, we\nobtained the political tendency of the news outlets by us-\ning the Media Bias/Fact Check website again and then we\ncategorized the news articles according to the political po-\nsitions of their sources. Political positions are categorized\ninto left, left-center, least, right-center, or right. For intu-\nitive understanding, we rearranged these categories to have\n(left, least, and right) as a simple version in which it regards\nleft and left-center with left, and right and right-center with\nright. Figure 7 indicates that according to the news source\nof the news article, news reader from the different political\npreferences may differently perceive the bias. From this,\nif we restrict the annotation task to build on the news arti-\ncles from the same political stance, so that we minimize the\ninterference by inter-rater different preferences, we could\nfurther improve the quality of the achieved labels.\n5. Conclusions\nDetecting news bias is a challenging task for computer sci-\nence as well as linguistics and media research areas due to\nthe subtle nature and heterogeneous, diverse kinds of bi-\nases. In this paper, we presented a corpus of news articles\nwhere sentences of the news articles have been labeled in\na crowd-sourcing task concerning their degree of bias. To\nFigure 7: Inter-rater reliability according to the political\nstance of news sources\nannotate subtle bias by using ordinary people on crowd-\nsourcing who can be considered as general news readership,\nwe provided a standard of comparison of the news content\nas the reference article. We then analyzed these annotations\nin regards to perceived bias. Based on our analysis, we con-\nclude that the prior knowledge about news event can be a\nmajor factor for bias annotation. In addition, we anticipate\nthat limiting the annotation tasks with news articles hav-\ning only a single political stance can improve the quality\nof bias annotation by minimizing the interference by inter-\nrater different preferences.\nFor future work, for fostering robust and practical bias de-\ntection mechanisms the greater topic variety needs to be\nconsidered for further extension such as not only for poli-\ntics but also economy, global health topics and so on. Also,\nthere is still room for further improvement for bias labels.\nIn this research, any random workers put bias labels for sen-\ntences in a document as a job unit. In this regard, employ-\ning \ufb01xed raters through the whole job could be one way to\nincrease label reliability as well as to decrease interference\nof personal political preference.\n6. Acknowledgments\nThis research was supported in part by MEXT grants\n(#17H01828; #18K19841; #18H03243).\n7. References\nAlsem, K. J., Brakman, S., Hoogduin, L., and Kuper, G.\n(2008). The impact of newspapers on consumer con-\n1484\ufb01dence: does spin bias exist? Applied Economics ,\n40(5):531\u2013539.\nBird, S., Loper, E., and Klein, E. (2009). Natural language\nprocessing with python o\u2019reilly media inc.\nCremisini, A., Aguilar, D., and Finlayson, M. A. (2019).\nA Challenging Dataset for Bias Detection: The Case of\nthe Crisis in the Ukraine. In Proceedings of the 12 Inter-\nnational Conference on Social, Cultural, and Behavioral\nModeling , SBP-BRiMS\u201919, pages 173\u2013183.\nD\u2019Alessio, D. and Allen, M. (2000). Media bias in presi-\ndential elections: A meta-analysis. Journal of communi-\ncation , 50(4):133\u2013156.\nFerreira, W. and Vlachos, A. (2016). Emergent: a novel\ndata-set for stance classi\ufb01cation. In Proceedings of the\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies , NAACL-HLT\u201916, pages 1163\u2013\n1168.\nFleiss, J. L. (1971). Measuring nominal scale agreement\namong many raters. Psychological bulletin , 76(5):378.\nHackett, R. A. (1984). Decline of a paradigm? bias and ob-\njectivity in news media studies. Critical Studies in Media\nCommunication , 1(3):229\u2013259.\nHamborg, F., Donnay, K., and Gipp, B. (2018). Automated\nidenti\ufb01cation of media bias in news articles: an inter-\ndisciplinary literature review. International Journal on\nDigital Libraries , pages 1\u201325.\nHorne, B. D., Khedr, S., and Adali, S. (2018). Sampling\nthe News Producers: A Large News and Feature Data\nSet for the Study of the Complex Media Landscape. In\nProceedings of the Twelfth International Conference on\nWeb and Social Media , ICWSM\u201918, pages 518\u2013527.\nHutto, C., Folds, D., and Appling, D. (2015). Computa-\ntionally detecting and quantifying the degree of bias in\nsentence-level text of news stories. In Proceedings of\nSecond International Conference on Human and Social\nAnalytics .\nKiesel, J., Mestre, M., Shukla, R., Vincent, E., Adineh,\nP., Corney, D., Stein, B., and Potthast, M. (2019).\nSemEval-2019 Task 4: Hyperpartisan News Detection.\nInProceedings of The 13th International Workshop on\nSemantic Evaluation (SemEval 2019) . Association for\nComputational Linguistics.\nLim, S., Jatowt, A., and Yoshikawa, M. (2018). Under-\nstanding characteristics of biased sentences in news ar-\nticles. In Proceedings of the CIKM 2018 Workshops co-\nlocated with 27th ACM International Conference on In-\nformation and Knowledge Management (CIKM 2018),\nTorino, Italy, October 22, 2018 .\nMorstatter, F., Wu, L., Yavanoglu, U., Corman, S. R., and\nLiu, H. (2018). Identifying framing bias in online news.\nACM Transactions on Social Computing , 1(2):5.\nNiven, D. (2002). Tilt?: The search for media bias . Green-\nwood Publishing Group.\nPark, S., Kang, S., Chung, S., and Song, J. (2009). News-\nCube: delivering multiple aspects of news to mitigate\nmedia bias. In Proc. of SIGCHI on Human Factors in\nComputing Systems , pages 443\u2013452.\nP\u00b4erez-Rosas, V ., Kleinberg, B., Lefevre, A., and Mihalcea,R. (2018). Automatic detection of fake news. In Pro-\nceedings of the 27th International Conference on Com-\nputational Linguistics , COLING\u201918, pages 3391\u20133401.\nSchuldt, J. P., Konrath, S. H., and Schwarz, N. (2011).\n\u201cglobal warming\u201d or \u201cclimate change\u201d? whether the\nplanet is warming depends on question wording. Public\nOpinion Quarterly , 75(1):115\u2013124.\nWang, W. Y . (2017). \u201cLiar, Liar Pants on Fire\u201d: A New\nBenchmark Dataset for Fake News Detection. In Pro-\nceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics , ACL\u201917, pages 422\u2013426.\nZubiaga, A., Liakata, M., Procter, R., Bontcheva, K., and\nTolmie, P. (2015). Crowdsourcing the annotation of\nrumourous conversations in social media. In Proc. of\nWWW 2015 , pages 347\u2013353.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Annotating and analyzing biased sentences in news articles using crowdsourcing", "author": ["S Lim", "A Jatowt", "M F\u00e4rber"], "pub_year": "2020", "venue": "Proceedings of the Twelfth \u2026", "abstract": "The spread of biased news and its consumption by the readers has become a considerable  issue. Researchers from multiple domains including social science and media studies have"}, "filled": false, "gsrank": 304, "pub_url": "https://aclanthology.org/2020.lrec-1.184/", "author_id": ["", "l2vn9GoAAAAJ", "Jb7JUOsAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:pfsn4iSDxpcJ:scholar.google.com/&output=cite&scirp=303&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D300%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=pfsn4iSDxpcJ&ei=OrWsaI6ND8DZieoPqdqh8QU&json=", "num_citations": 70, "citedby_url": "/scholar?cites=10936572939552160677&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:pfsn4iSDxpcJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aclanthology.org/2020.lrec-1.184.pdf"}}, {"title": "Metaliteracy in the Developmental Classroom", "year": "2024", "pdf_data": " \n \nVolume 1, Issue 1               Fall 2024  \nMetaliteracy in the Developmental Classroom \n \nHeather Michelle McGrew  \nUniversity of Wisconsin- Superior  \n \nAbstract  \n \nThis paper investigates the origins of metaliteracy with a focus on media bias, misinformation, and disinformation and their pedagogical implications for information literacy (IL) in the writing classroom. Grounding the conversation in cognitive dissonance theory and confirmation bias \ntheory, the paper offers an overview of multiple web- based IL tool s\u2014including media bias \ncharts and scales, fact -checking sites, and self -directed tools such as the CRAAP test and Jack \nCaulfield\u2019s SIFT method (aka The Four Moves) \u2014and suggests pedagogical practices \nspecifically for developmental writing classrooms. Concepts and practices that fall under the \numbrella of metaliteracy including digital literacy, cyberliteracy, visual literacy, and transliteracy  \nare discussed as ski lls that are increasingly important for college students as they interact with \ninformation  dynamically in the landscape of today\u2019s complex digital age . \n Keywords:  metaliteracy, information literacy (IL), media bias, misinformation, disinformation, \ndevelopmental writing  \n Recommended Citation  \n McGrew, H. M. (2024). Metaliteracy in the developmental classroom. Journal of the National \nOrganization for Student Success , 1(1), 55- 69. https://doi.org/10.61617/jnoss.14 \n \n  \n\nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  56 \n Introduction \n \nToday\u2019s college students must wade through an unprecedented amount of information \nwhich they are expected to assess daily \u2014some of which legitimately qualifies as \n\u201cmisinformation\u201d and/or \u201cdisinformation.\u201d The techniques required to identify a source\u2019s credi bility, veracity, and lack of bias have become increasingly complex. The need for such \nskills is further heightened for students who arrive at college underprepared and place into developmental reading and/or writing courses. Therefore, higher -education pr actitioners, \nparticularly those who teach developmental courses, must be prepared to dedicate adequate time and resources to teach students effective and robust strategies to make evidence -based decisions \nabout a source\u2019s relevance and usefulness to them as researchers as well as to thoughtfully produce their own content in online communities . Effective metaliteracy instruction requires \npractitioners to familiarize themselves with useful resources, recognize their own biases, and regularly update their approaches to respond to the dynamic nature of how we share, consume, and respond to information in today\u2019s complex world.  \nThe purpose of this paper is to briefly discuss the histories, theories, and current \nresources associated with information literacy (IL) and metaliteracy along with implications for pedagogical practice in higher education, specifically in the developmental classroom. We will look at the background of IL, including the evolution of the term itself and two useful theories that undergird IL i nstruction , as well as the general educational shift to focus more on the \npractice of metaliteracy, which \u201c is a unified construct that supports the acquisition, production, \nand sharing of knowledge in collaborative online communities \u201d (Mackay & Jacobson, 2 011, p. \n62). We will also consider the current state of media bias , how the prevalence of \n\u201cmisinformation,\u201d and \u201cdisinformation\u201d have impacted the metaliteracy practices,  and \ncommon/best classroom practices for IL  and metaliteracy  instruction. Then we will  consider the \nmethodologies, benefits and drawbacks of various resources for classroom use including media bias charts, fact -checking tools , and self -directed evaluation tools . Finally, we will consider \nimplications for the developmental classroom and draw  some conclusions that can inform \npractitioners as we move forward in this rapidly changing field. \n \nBackground: History of Information Literacy and Metaliteracy  \n \nOriginally coined in Australia in 1964, the term \u201cinformation literacy\u201d was first used in \nthe United States in 1974 by Paul G. Zurkowski, the then- president of the Software and \nInformation Industry Association. The term was adopted three years later by the American Librarian Association (ALA) as a key instructional component , and in 2009, President  Obama \nestablished October as National Information Literacy Awareness month. Now half a decade old, the term has survived multiple iterations and definitions as the ways in which information is created, shared, and consumed have changed profoundly.  \nIL ski lls have long been a fundamental component of the higher education curriculum. \nNearly 30 years ago, Shapiro and Hughes (1996) explained the importance of IL in the context of a liberal arts education:  \n[I]nformation  literacy should in fact be conceived more broadly as a new liberal art that \nextends from knowing how to use computers and access information to critical reflection on the nature of information itself, its technical infrastructure, and its social, cultural and \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  57 \n even philosophical context and impact -  as essential to the mental framework of the \neducated information- age citizen as the trivium of basic liberal arts (grammar, logic and \nrhetoric) was to the educated person in medieval society. (p. 2)  \nOver the years, as the search for information has shifted primarily from books and physical \nartifacts to electronic resources, the critical thinking practices required for information literacy have necessarily shifted as well.  \nIn response to this notable shift, Mackey and Jacobs on (2011) suggested a revised term to \nreframe  information literacy: metaliteracy. T o distinguish between traditional IL and \nmetaliteracy, t hey a sserted , \u201cWhile information literacy prepares individuals to access, evaluate, \nand analyze information, metaliteracy prepares individuals to actively produce and share content through social media and online communities \u201d (Mackey & Jacob son, 2011, p. 76). This revised \ndefinition takes into account the dynamic nature of how information in the c urrent age is actively \nand collaboratively produced and distributed. The term also refers to a learner\u2019s ability to regularly reflect on their own thinking processes and adapt to a host of new and emerging technologies such as social media platforms and ot her open spaces where information is shared \nand consumed (Jacobson & Mackey, 2013). Ultimately, then, metaliteracy is an expansion of information literacy that takes into account four domains of learning\u2014cognitive, metacognitive, behavioral, and affective (Jacobson et al., 2021) \u2014and considers new and emerging tools used to \nproduce and share information in a shifting online environment. Metaliteracy is an umbrella term that encompasses concepts and practices such as digital literacy, cyberliteracy, visual li teracy, \nand transliteracy \u2014all skills that are increasingly important for college students as they interact \nwith information in our current digital age. \n \nIL and metaliteracy skills are complicated further by this current era in which the terms \n\u201cdisinformation\u201d and \u201cmisinformation\u201d are widely\u2014and arguably rather liberally \u2014used ; as a \nresult,  today\u2019s college students must understand how to assess sources for reliability, credibility, \nand accuracy as they consume, create, and respond to the copious amount of information that is available to them daily. Indeed, as Pachtman (2012) argues, today\u2019s  students need to \u201cidentify \nimportant questions, locate information, critically evaluate that information, and then communicate it to others\u201d (p. 39). Compounding these existing challenges is the reality that students tend to believe they have more sophist icated IL strategies than objective assessments of \ntheir abilities show (Gross & Latham, 2012; Latham & Gross, 2013). T herefore, educators must \nprioritize the teaching of metaliteracy skills and be prepared to regularly update their pedagogical methods to respond to the ongoing and dynamic nature of information dissemination \nand consumption.\n \nMetaliteracy skills are particularly important for students who arrive at college \nunderprepared and place into developmental courses as first -year students. Cantrell et  al. (2013) \nfound in their study of 100 first -year college students attending a midsized regional public \nUniversity in the southeastern United States that those who were placed into developmental reading courses possessed lower levels of reading self -efficacy compared to their mainstreamed \npeers. Since effective IL and metaliteracy skills hinge on a student\u2019s abilities to read, comprehend, and assess text in various formats, any deficiencies in this area can be profoundly detrimental. Indeed, the process of  developing IL and metaliteracy skills is sophisticated and \nmultifaceted. Diehm and Lupton (2014) describe a hierarchical process that begins with learning to access and process information; from there, the individual uses the information gathered to creat e a product. Final steps in the progression involve the use of new information to build a \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  58 \n personal knowledge base; to inform one\u2019s disciplinary knowledge; and to grow personally as \nwell as to contribute to society. Diehm and Lupton (2014) use the term information literacy exclusively, but it is clear that the later stages of the process they describe require metaliteracy skills, as the individual  moves from being a consumer of information to an active producer of content within collaborative communities. S uch a process requires high -level cognitive skills that \ntake time to develop, which is especially challenging for developmental students who are still honing their basic reading and writing skills as first -order tasks. Finally, the rapid pace of change \nin regards to metaliteracy also complicates the process for students who are already underprepared\u2014in other words, already working to catch up with their mainstream peers\u2019 reading, writing, and/or math skills.  \nClearly, today\u2019s developmental educators have an  increasingly important responsibility to \nadvance students\u2019 metaliteracy skills in a complex age of media bias, \u201cmisinformation,\u201d and \u201cdisinformation\u201d; this requires knowledge of available tools \u2014including their methodologies, \nbenefits, and limitations \u2014along with a willingness to commit adequate time and energy into \nrecursive active learning practices and activities in the developmental classroom.  \n \nTheories that Inform Metaliteracy  \n \nMultiple theories can lay the groundwork for metaliteracy instruction in the  classroom \nincluding cognitive dissonance theory and confirmation bias theory. Cognitive dissonance theory, developed in the 1950s by American psychologist Leon Festinger (1962), \u201ccenters around the idea that if a person knows various things that are not psychologically consistent with one another, he [or she/they] will, in a variety of ways, try to make them more consistent\u201d (p. 93). To relieve the tension created by the dissonance, individuals may avoid or rebuff the conflicting information or even convi nce themselves somehow that there is no actual conflict \n(Duignan, 2022). Humans naturally seek stability, so they may even engage in such practices knowingly. \nOne of the  dissonance -reducing behaviors people employ is confirmation bias, which is \nrooted in c ognitive dissonance. As aforementioned, confirmation bias is the tendency to seek out \nand favor information that aligns with one\u2019s existing beliefs or worldview, especially when an issue is of primary importance and carries emotional relevance for an individual. Due to the sheer volume of information that is available to us today, we need an approach that allows us to process the information quickly, and interpreting information from our existing viewpoint helps us to do so in a way that is self -preserving (Casad, n.d.). The practice of confirmation bias can be \ndetrimental, however, as it can cause individuals to ignore or discount potentially valuable information that could enhance their comprehensive understanding of an issue. Therefore, learning about cognitive dissonance and confirmation bias theories, as well as ways to combat the often -knee -jerk reactions to information that conflicts with people\u2019s own biases, can be an \neffective classroom approach to metaliteracy  as students practice the process of ref lecting on \ntheir own learning.  \n \nMedia Bias, Misinformation, and Disinformation  \n \nTheories such as those discussed above help us to understand and grapple with media \nbias, which is a particularly stark reality in this age of increasing political polarization . \nIllustratively, citing mounting challenges presented by a proliferation of misinformation, \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  59 \n Dictionary.com announced in 2018 that it had chosen \u201cmisinformation\u201d as the word of the year. \nThe online dictionary defines the term as follows: \u201cfalse information  that is spread, regardless of \nwhether there is intent to mislead\u201d (Dictionary.com, 202 3). On the other hand, Dictionary.com \n(2023) defines \u201cdisinformation\u201d as \u201cdeliberately misleading or biased information; manipulated narrative or facts; propaganda.\u201d Intent, therefore, is the key difference between the two terms. Writing for UNESCO (United Nations Education Science and Culture Organization), Abuhmaid (2021) argues that it has become increasingly difficult to filter content with the amount of information w e encounter and cites an infodemic with the rapid spread of inaccurate information, \nparticularly on social media. Abuhmaid (2021) emphasizes the importance of schools teaching critical thinking skills to students through extensive media education so they c an be empowered \nto distinguish between what is true and false, fact versus opinion.  \nEcho chambers and confirmation bias can plague even the most perceptive readers. \nIndividuals do not consume information in a vacuum, and all readers possess a worldview that influences how they receive and analyze new information. Ling (2020) describes confirmation bias as a propensity to center our attention on information that reinforces our social or political  perspectives. Ling (2020) also discusses how the disjointed,  superficial browsing of news on our \nsmartphones can exacerbate the problem. These practices can result in further polarization and hyper partisanship, which can erode our ability to engage thoughtfully and respectfully in the public square. Since young adults frequently access their news via online news sources (Antunovic et al., 2018), IL and metaliteracy skills are increasingly important for today\u2019s \nscholars.  \n \nMetaliteracy in the Classroom  \n \nJust as the definition of information literacy has changed with time, so have the practices \nfor teaching and practicing IL and metaliteracy in the classroom. Kevin McGrew, Director of the Library at the College of Saint Scholastica in Duluth, Minnesota, ha s been working in the library \nscience field for 3 5 years. McGrew recognizes a significant shift in how information is accessed \nand vetted. Early in McGrew\u2019s career, librarians would vet information and ensure that only high- quality, reputable materials tha t supported the institution\u2019s curriculum would be found in an \nacademic library. Now, McGrew observes, the responsibility to find quality, authoritative sources has shifted to the shoulders of the end user (K. W. McGrew, personal communication, June 16, 2022)\n. This process of accessing and analyzing sources for credibility, authority, and \nrelevance is complex; therefore, instructors must be willing to commit adequate time and resources to the task.  \nOne foundational tool that instructors can use is the Framework for Information Literacy \nfor Higher Education, which was adopted by the Association of College and Research Libraries (ACRL) Board in January of 2016. The document puts forth six frames (including Research as Inquiry, Scholarship as Conversation, and Searching as Strategic Exploration); suggestions for faculty to implement the Framework and administrators to support it; background information on how the Framework was developed; and suggested sources for further reading. In addition, the document i ncludes knowledge practices and dispositions that learners should possess as they are \ndeveloping their IL skills. Emphasized in the appendix on faculty implementation of the  \nFramework is the importance of integrating the IL program systemically throughout students\u2019 academic programs. The ACRL encourages faculty to provide contextualized, targeted IL sessions that meet students\u2019 particular needs for specific assignments or tasks related to their \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  60 \n coursework. McGrew agrees, citing the importance of meeting stu dents in the moment and \nproviding them with practices that can help them at their point of need (K. W. McGrew, personal \ncommunication, June 16, 2022) . \nThe Framework has garnered its share of criticism, including concerns about its use of \njargon and, theref ore, lack of accessibility to some audiences; its emphasis on theory, which \nmakes it difficult to assess measurable outcomes; and a misalignment between the Framework  \nand the tenets of critical information literacy, which seeks to challenge existing power structures \n(Beilin, 2015). Concerns have also been voiced over the phasing out of the ACRL Information Literacy Competency Standards for Higher Education, which were adopted in January of 2000. If practitioners share these concerns, they certainly can init iate discussions about these critiques \nshould they want to include that as part of their metaliteracy curriculum, especially in higher -\nlevel classes where  the metaliteracy curriculum has been scaffolded.  \nA related and ongoing conversation centers on whose  job it is to teach  IL and/or  \nmetaliteracy to students. Many models have been employed over the years with a combination of academic librarians and faculty members taking the lead. McGrew argues that academic librarians are the best trained and equipped to teach IL  but acknowledges that most institutions \ndo not have the staffing required to do this on the scale needed. McGrew  et al. (2015) conducted \na study to assess the \u201cability of classroom faculty to support and amplify the instruction given by library faculty.\u201d Results showed that both students and faculty predicted a higher level of \nconfidence and skill in their ability to use the library for their research needs than their performance on the assessment demonstrated. Among the implications of their study was the suggestion that training sessions for faculty by library staff may be beneficial in enhancing classroom metaliteracy instruction.  \n \nClassroom Tools for Metaliteracy  \n \nAlthough training sessions such as those encouraged by McGrew are a key componen t of \neffective classroom metaliteracy instruction, it is also important for the practitioners to proactively analyze a variety of helpful tools that are readily available for classroom use. The Internet offers abundant resources for teaching IL and  metaliteracy skills including many charts \nand scales that rate media bias using multiple techniques. The tools vary in methodology, rigor, and usefulness, so it is best practice for practitioners to familiarize themselves with as many as possible to determine whi ch tools work best for which tasks. Although we cannot discuss every \ntool available here, we will investigate several of the most popular tools available for classroom use. Table 1 below offers a general overview of several of the most common tools, including their methodology/approach, pros/benefits, and cons/limitations. Following the table is a more thorough discussion of each of those individual tools.  \n       \n  \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  61 \n Table 1  \nComparison of IL Tools  \n \nType of Source  Examples  Methodology/Approach  Pros Cons/Limitations  \nMedia bias \ncharts and rating \nscales  Media Bias Fact \nCheck (MBFC)  \n \nAd Fontes Media \nChart  \n Media Bias \nAllSides  Teams of researchers, \nwriters, and/or contributors rate bias (primarily \npolitical) and/or reliability of sources  \n Most include rating scales \n(e.g., 1 -10) to represent \nlevels of bias (e.g., least to \nmost)  \n \nMost ensure a variety of raters from across the \npolitical continuum (left \nleaning, moderate, and right leaning)  \n   Creators of these \ntools e mploy \nmethodologies that have progres sed to \nfurther alleviate \ninternal bias  \n \nCharts and scales are visual and \nrelatively easy to use and navigate  \n \nThey c over a \nwide variety of sources (e.g., the \nstatic Ad Fontes \nMedia Chart \nincludes ratings \nfor over 150 \nsources , and the \nteam has rated over 68,000 \nindividual \narticles ) Methodologies are \ndeveloped by the creators and \ngenerally not tested scientific \napproaches  \n Some do not \nadequately allow for nuance/can be \nreductive  \n \nThey d o not require \ncritical thinking from the user\n \nFact-checking \ntools/sites  PolitiFact  \n Snopes  \n FactCheck.\n \norg A panel of writers, \nresearchers, and/or editors check the veracity of \nspecific reports  \n \nThe t eam focuses on \nexposing questionable or deceptive claims  \n \nMost of these sites reach out \nto original /primary  source s \nto request  evidence and/or \nback -up data to support \nclaims(s)  \n \nMany of these sites refer to \nexperts and/or nonpartisan \nsources  \n \nSome, but not all, use rating \nscales or systems  \n These are u seful \nfor checking \nparticular claims \nfor research \npurposes  \n \nThese tools often \nhelp to dispel \nlegitimate \ndisinformation  \n \nThey h elp to \ndiscourage  \nindividuals (especially \npoliticians) from \nmaking false \nclaims  These sites c heck \nonly particular \nclaims (very \nlimited)  \n \nThey c an be \ninherently biased  \nNote.  Methodology practices as well as pros and cons may vary among the examples listed in the second column.   \n \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  62 \n  \nType of Source  Examples  Methodology/Approach  Pros Cons/Limitations  \nSelf-directed \ntools CRAAP test  \n \nJack Caulfield\u2019s SIFT method (aka \nThe Four Moves)  \n \nCheck, Please: \nStarter Course  \n \nP.R.O.V.E.N.  \n  These tools rely on a self -\ndriven process that requires users to evaluate sources  \n \nMost consider factors such \nas currency, accuracy, \nauthority, and purpose  These tools d o \nnot do the work \nfor the user  \n \nThey r equire \ncritical thinking \nskills  These tools c an be \nreductive for \nexperienced users  \n \nThey c an, on the \nother hand, be overly complex for \ninexperienced users \n(e.g., some users are \nnot ready to \nrecognize conflicts \nof interest, the complex \ncharacteristics  that \ndetermine a source\u2019s \nauthority, etc.)  \nNote.  Methodology practices as well as pros and cons may vary among the examples listed in the second column.   \n \nIt is worth noting here that although the tools listed above in Table 1 appear to aid \nstudents only with the development of more traditional IL skills, they form the foundation for metaliteracy, as they can and often do extend into the creation and product ion of original content \nin dynamic collaborative environments. Also, as McCoy (2022) asserts, there is a profound connection between IL and metaliteracy, as \u201c[i]nformation literacy requires an understanding of \nhow you are thinking about and evaluating the information that is being found and consumed; this is a metacognitive act that can be explicitly taught and practiced in the information literacy classroom \u201d (p. 45) . Indeed, instructors can extend classroom IL lessons into the metaliteracy \ndomain by requiring metacognitive reflection (e.g., consider what kinds of resources they are preferring over others and why; what types of sources are finding the way to the top of  their \nfeeds and why; and what types of sources they are more likely to believe and share and why). Instructors may also require the production of original content within an online community in response to IL learning.  \n \nMedia Bias Charts and Rating Scales  \n \nMedia Bias Fact Check (MBFC). Arguably one of the most comprehensive tools \navailable (the database includes over 7,400 politicians, journalists, and media sources) , Media \nBias Fact Check (MBFC) was created in 2015 by Dave Van Zandt, the source\u2019s primary editor. This tool offers a transparent and comprehensive description of its methodology and \nacknowledg es that there is no possible way to ensure 100% objectivity. To calculate bias, the \nteam of nine researchers, writers, and contributors assesses sources b y considering political bias, \nuse of factual information, and links to other credible sources. Using a 10- point scale, MBFC \nrates sources from least biased (0 -2) to left/right center bias (2 -5), left -right biased (5 -8), and \nextremely biased (8 -10). In its evaluations, the team contemplates, among other things, the \nsource\u2019s use of loaded words; well -sourced evidence; and story selection (i.e., reporting news \nfrom both sides or only one) along with political affiliations, including any organizations or causes which the owners donate to or support. More specifically, the  team  consider s many \ndifferent types of bias (bias by omission, by labeling, by spin, by story or source selection) as \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  63 \n well as how many fact checks a source has failed. Other considerations incl ude connotation and \ndenotation as well as the use of what the site refers to as \u201cpurr words\u201d (\u201cwords that are used to \ndescribe something that is favored or loved\u201d) and \u201csnarl words\u201d (\u201c words used when describing \nsomething that a person is against or hates\u201d)  (MBFC, 2023) . Consideration of these kinds of \nelements generally expands the concept of news bias for most students.  \nMBFC also adheres to the International Fact -Checking Network Fact -checkers' Code of \nPrinciples, which were developed by the Poynter Instit ute to encourage excellence in the practice \nof fact -checking. In addition, MBFC commits to nonpartisanship; transparency of funding, \norganization, and methodology; and a regular practice of authentic and intentional corrections. Other benefits of this tool  are that it includes definitions of important terminology (e.g., \n\u201cpseudoscience,\u201d \u201csatire,\u201d and \u201cquestionable sources\u201d), a list of least -biased sources, a list of \nsources that have been re -evaluated/updated, and a detailed report for each source that incl udes \nits history and funding sources. Finally, MBFC received a perfect rating (100/100) in credibility \nfrom Newsguard, one of its competitors.  \nAlthough MBFC is relatively well  respected, no tool alone is perfect. The site contains a \ndisclaimer on its own limitations, including the fact that the MBFC team developed its own \nmethodology that is not a tested scientific approach. Still, the team commits to correcting any \nfactual err ors it may make and working toward the goal of achieving a \u201cleast -biased\u201d rating \nusing the very criteria it established.  \nAd Fontes Media Bias Chart. Another well -known online bias -checking tool is the Ad \nFontes Media Bias Chart, which employs two axes: rel iability and bias. Articles are individually \nreviewed and rated by at least three analysts (out of 60 total) with varying political leanings: one \nright leaning, one left leaning, and one centrist/moderate. Individuals assign a rating and then \ncompare; if t here is disagreement on a rating, scores may be fine- tuned after discussion. The \nfinal rating for the article represents an average of the three analysts\u2019 scores. If necessary, more \nthan three analysts may be called upon to rate a specific article. The mai n tenets of the bias score \nare an article\u2019s level of political advocacy, both selection and omission of topics, and language use. \nThe methodology described above was created by Ad Fontes Media\u2019s founder, Vanessa \nOtero, who originally analyzed resources alo ne. Since the advent of the Media Bias Chart in \n2016, Otero\u2019s methodology has progressed in response to suggestions from other experts. The revised methodology is an attempt to alleviate bias and commit to a more data -driven approach.  \nThe Ad Fontes Media Bias Chart is wide ranging  with the availability of two charts: one \nstatic and one interactive. The static chart includes ratings for over 150 news sources, while the \ninteractive chart has the capacity to search from thousands of divers e sources (web, print, \npodcast, and TV). Finally, like the MBFC contributors, Otero acknowledges that complete objectivity is impossible to achieve but is transparent about the team\u2019s attempt to lessen bias.  \nNot surprisingly, the Ad Fontes Media Bias Chart  has attracted a fair share of criticism. \nWriting for ACRLog , Benjes -Small and Elwood (2021), for instance, argue d that the Chart \nelevates the political midpoint as if it were entirely unbiased, t ook issue with Otero\u2019s lack of \ninformation literacy training  or expertise, question ed the actual value of a bipartisan analysis, \nchallenge d the Chart\u2019s framing of what constitutes \u201cright- \u201d vs. \u201cleft -\u201d leaning, argue d that the \nChart does not allow for necessary nuance, and asserted  the tool simply reinforces confirm ation \nbias. Otero responded at length to the original post by Benjes -Small and Elwood (2021), \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  64 \n acknowledging that some nuance is sacrificed with the use of a graphic and discouraging users \nfrom relying too much on such charts. Still, Otero argues, it can be  a useful tool, especially for \nthose who may not have access to an academic library or a course in metaliteracy.  \nMedia Bias AllSides. The Media Bias AllSides chart rates only political bias and assigns \nsources to one of five categories: \u201cLeft,\u201d \u201cLean Left ,\u201d \u201cCenter,\u201d \u201cLean Right\u201d and \u201cRight.\u201d \nMedia Bias AllSides considers various types of bias such as \u201cslant, spin, sensationalism, and story choice.\u201d Unlike MBFC and the Ad Fontes Media Bias Chart, Media Bias AllSides recruits unpaid public readers to rate articles in the aforementioned categories. These readers (six to nine per review) self-report their own political leanings through a bias rating test and see only the text \nof the articles they are reviewing, not the outlets that published them. Like the methodology used for the Ad Fontes Media Bias Chart, AllSides employs staffers who identify as left leaning, right leaning, and center.  \nTo offer an example of its rating system, Media Bias AllSides shared the results of its \nMay 2022 blind bias survey. The  team  recruited over 1,300 individuals of various political \nleanings to blindly rate the bias of online content, which had been stripped of any identifying characteristics, derived from  the following sources: The Daily  Wire, The Epoch Times, Forbes, \nThe Hill, and Politico . AllSides calculated the weighted average for each source and determined \nthat The Daily Wire fell on the right, The Epoch Times leaned right, The Hill and Politico fell in the center, and Forbes leaned  left. \nMedia Bias AllSides (2024)  has a mission statement of sorts to \u201c[f]ree people from filter \nbubbles [a term much like confirmation bias] so they  can better understand the world \u2014and each \nother.\u201d  The site offer s an editorial philosophy; biographies of  its founders and team members; \nYouTube videos that explain the tool\u2019s methodology; a news link on its menu that includes \ncontent from the left, center, and right; and AllSides Talks, which bring together people from \ndifferent sides of the political spectr um for respectful dialogue. Finally, Media Bias AllSides \nrecognizes its limitations and acknowledges that no methodology is perfect; therefore, the team welcomes  community feedback on agreement or disagreement with current ratings and public \nparticipation in blind bias surveys.  \n \nFact-Checking Tools  \n \nFact-checking sites and tools are also widely available online and can be used in \nconjunction with the rating charts and scales. Fact -checking sites are designed specifically to \ncheck the veracity of specific reports, so their use as a research tool is a b it more limited, but \nthey can be particularly helpful for current topics and events. Common fact -checking sites \ninclude PolitiFact, which applies Truth -O-Meter ratings to determine a claim\u2019s accuracy; Snopes, \nwhich researches and reports on questionable cl aims; and FactCheck.org, which focuses \nspecifically on exposing deceptive claims and information in the realm of U.S. politics.  \nFact-checking sites may also be susceptible to bias, of course, so extra steps may be taken \nby the user to determine a fact -checking site\u2019s partiality. For instance, MBFC rates PolitiFact and \nSnopes.com as having a left -center bias and FactCheck.org as being \u201cleast biased\u201d with a very \nhigh level of factual reporting. Cross -checking sources in this fashion can help to ensure rigoro us \nIL strategies that add a layer of accountability for the researcher.  \n  \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  65 \n Self-Directed Evaluation Tools  \n \nAlthough media bias charts and fact -checking sites are clearly helpful to students who are \nlearning IL and metaliteracy skills, these tools do the wor k for the researcher by assigning ratings \nto sources primarily through the use of an editorial team that uses a specific methodology. It is \nimportant that students move beyond that, however, to also utilize resources that actively engage them in the proces s of analyzing such important elements as a source\u2019s credibility  and veracity. \nThese strategies also cross over from basic IL skills (accessing and evaluating information) to the metacognitive practices that undergird metaliteracy.  \nCRAAP Test . A common tool used to this end in many composition classrooms is the \nCRAAP test (CRAAP test administrators, 202 3). CRAAP is an acronym that stands for currency, \nrelevance, authority, accuracy, and purpose. Many iterations of the CRAAP test are available online, som e of which pose questions for the researcher under each category (for instance, how \nrelevant is this source to your topic or claim?) and rating scales that help students determine if the source is acceptable or appropriate for their classroom research.  \nThe CRAAP test has received a fair amount of criticism for being reductive. Some argue \nthat students oversimplify the tool, as there is simply too much nuance for them to understand under each category including potential conflicts of interest, sources of funding, and levels/types of authority. Still, many instructors find it a useful tool, especially for 100-  and 200- level \nundergraduate courses.  \nSIFT.  Mike Caulfield, a research scientist for the UW Research Center for an Informed \nPublic, has been an outspoke n critic of the CRAAP test and developed the SIFT (aka The Four \nMoves) model as an alternative (Caulfield, 2019). SIFT is an acronym for Stop, Investigate, Find better coverage, and Trace the original content. Caulfield also offers a three -hour minicourse \ncalled Check, Please: Starter Course (n.d.). The material is free and editable, and Caulfield\u2019s goal is to create a curricular community around metaliteracy practices.  \nP.R.O.V.E.N . Caulfield\u2019s \u201cFour Moves,\u201d in conjunction with the ACRL Framework, has \nalso been used as the foundation for  a source evaluation tool titled P.R.O.V.E.N . (purpose, \nrelevance, objectivity, verifiability, expertise, and newness), available at an Open Educational \nResource titled CORA , an acronym for Community of Online Research Assignments  (Carey, \n2017). The process is designed to get students to consider carefully how sources might meet their unique needs. Embedded in the P.R.O.V.E.N. model is consideration of whether or not the source has been fact checked by sites such as PolitiFact or Snopes. In addition, the model asks researchers to check their own emotions and biases, recognizing how these may influence their analyses of sources.  \n \nApplications for the Developmental Classroom  \n \nAs already mentioned, students who place into developmental classes often face more \nbarriers to academic success than their college -ready peers. It follows, then, that helping this \npopulation develop critical thinking and metaliteracy skills is especially important. A study done \nby Zimmerer et al. (2018) analyzed two groups of students \u2014those who were working through an \ninnovative, contextualized reading curriculum and those who were learning from a traditional reading curriculum with the default course text book\u2014for reading and information literacy skills, \npersistence, course completion, and subsequent registration in the gateway course. Students who \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  66 \n learned via the contextualized reading curriculum (described as an approach that connected \nreading skills with  disciplinary content, required students to develop their reading strategies by \naccessing various types of resources, and gave students task -specific, project -based \nopportunities) performed better than those who learned via the traditional curriculum on information literacy skills, while students in both groups saw gains in their reading strategies. \nAdditionally, course completion, persistence, and successive course registration were similar for the population that used the contextualized curriculum, which consisted of students who scored one or more levels below college ready, and those who used the traditional curriculum, which consisted of only students who had scored a single level below college ready. Zimmerer et al. (2018) concluded that the use of int entional and recursive information literacy practices in a \ndevelopmental reading course can help our most at -risk students to achieve academic success. \n \nStudies such as Zimmerer et al.\u2019s (2018) can help practitioners implement successful \nmetaliteracy lesso ns and activities to allow students in developmental courses to catch up to their \npeers with the use of effective research practices. Designing in -class lectures and activities \naround specific assignments or tasks, using a variety of resources for different tasks, and allowing students plenty of time for practice and discussion during class time can help this population develop confidence in this area.  \n \nSuggested Classroom Lessons  \nA typical metaliteracy unit that uses the resources shared herein may begin with students \nchoosing topics for a research paper. The instructor can start with a basic introduction to Boolean logic and offer in- class opportunities for students to experiment with various search terms/strings \nto identify potential sources using Google, Google Scholar, and library databases. Embedded in this step should be explicit instruction and practice activities to help students understand the \ncharacteristics of scholarly v s. popular sources. Next, the instructor can introduce various graphs \nand charts (e.g., MBFC, the Ad Fontes Media Bias Chart, and/or Media Bias AllSides) so \nstudents can check their non- scholarly sources for bias. At this point, students should consider \nthe potential effectiveness of the sources they have accessed with their audience in mind, asking \nthemselves, will my audience consider this a reputable, credible source based on what I have learned so far?   \nNext, with a basic understanding of some biases that may exist in some of the sources \nthey have accessed and the criteria most often used to determine a source\u2019s reliability, students can move on to the self -directed tools, such as the CRAAP test or Caulfield\u2019s SIFT model. At \nthis point, students should be well poised to determine which sources they should incorporate in their papers and which they should reject based on the processes described above. They also \nshould possess some fundamental skills for subsequent IL tasks as they continue to seek out, \nassess, and use sources throughout their academic careers.  \nFor instructors who are willing and able to move further into metaliteracy (rather than \nstrictly IL) practices, many options exist beyond the basic strategies listed above. One of the most natural and obvious approaches that arises organically from any of the IL practices listed above is asking students to reflect on their news -gathering practices and consider how their \npersonal biases may influence how they access, consume, and share information. Indeed, Stanton et al. (2021) describe metacognition as awareness of and control over one\u2019s thinking and learning processes, so an intentional consideration of one\u2019s owns preferences and prejudices can help \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  67 \n raise students\u2019 self -awareness around their informat ion gathering and sharing practices. Stanton \net al. (2021) also encourage the practice of social metacognition, wherein students share ideas \nwith classmates and both give and receive feedback on these ideas. Starting with individual reflection and then moving into small group discussions and finally asking groups to share some thoughts with the entire class could help to reinforce that everyone has mindsets, worldviews, and preconceptions that shape how they consume and produce information. \nFinally, if an i nstructor wishes to move students beyond metacognition into information \nproduction \u2014the part of metaliteracy that involves the sharing of information in collaborative \nonline communities \u2014there are many resources available to guide them into preparing \nmeaningful experiences in their classrooms. For instance, assignments, articles, prompts, and other materials are available at Metaliteracy.org (n.d.), a blog dedicated to providing open metaliteracy -based resources intended for educators who are dedicated to met aliteracy practices \nin their classrooms. The team at Metaliteracy.org includes Thomas P. Mackey and Trudi Jacobsen who, as mentioned earlier, coined the term and have collaboratively written four books and many articles on the topic of metaliteracy. Other valuable metaliteracy resources can be found online at various library and education sites as well as in scholarly articles available on Google Scholar and through library databases.  \n Conclusion  \nMetaliteracy skills are f oundational  to a modern liberal art s education. Since these skills \nare interw oven  throughout the curriculum and used in most \u2014if not all \u2014disciplines, practitioners \nmust be ready to provide targeted and task- based instruction to help students effectively search \nfor, access, and evaluate a variety of types of sources for credibility, relevance, and usefulness to their research processes; reflect on their learning processes and their own biases; and practice \nsharing information responsibly across a wide spectrum of available online platforms. As argued here, students who place into developm ental courses are especially at risk and need dedicated \nmetaliteracy instruction so they do not fall behind their college -ready peers. Finally, the need for \nrobust metaliteracy instruction is likely to intensify as we find innovative ways to create, share,  \nrespond to, and use information as a society; therefore, institutions would benefit from serious conversations about how, when, and where such instruction will show up in the curriculum.  \n \n \n             \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  68 \n References  \n \nAbuhmaid, H. (2021). Growing up in the age of fake news. The UNESCO Courier: Many Voices, \nOne World. https://en.unesco.org/courier/2021- 2/growing- age-fake-news  \nAd Fontes Media. (2023). Interactive media bias chart . https://adfontesmedia.com/interactive -\nmedia -bias-chart/   \nAntunovic, D., Parsons, P., & Cooke, T. R. (2018). \u2018Checking\u2019  and googling: Stages of news \nconsumption among young adults. Journalism , 19(5), 632- 648. \nhttps://doi.org/10.1177/1464884916663625 \nBeilin , I. G. (2015). Beyond the threshold: Conformity, resistance, and the ACRL information \nliteracy framework for higher education.  https://doi.org/10.7916/D8RR1XDC  \nBenjes- Small, C., & Elwood, N. (2021, Feb. 23). Complex or clickbait?: The problematic Media \nBias Chart. ACRLog . https://acrlog.org/2021/02/23/complex- or-clickbait -the-\nproblematic -media -bias-chart/comment -page -1/ \nCantrell, S. C., Correll , P., Clouse, J., Creech, K., Bridges, S., & Owens, D. (2013). Patterns of \nself-efficacy among college students in developmental reading. Journal of College \nReading and Learning, 44 (1), 8- 34. https://doi.org/10.1080/10790195.2013.10850370 \nCarey, E. (2017, September 15). P.R.O.V.E.N. source evaluation process. Community of Online \nResearch Assignments. https://www.projectcora.org/assignment/proven- source -\nevaluation- process  \nCasad, B. J. (n.d.). Confirmation bias. In Britannica. \nhttps://www.britannica.com/scien ce/confirmation -bias \nCaulfield, M. (2019, June 19). Sift (The four moves). Hapgood. \nhttps://hapgood.us/2019/06/19/sift -the-four-moves/  \nCheck, please! Starter course. (n.d.) https://www.notion.so/checkpleasecc/Check- Please- Starter-\nCourse -ae34d043575e42828dc 2964437ea4eed  \nCRAAP test administrators. (2021). CRAAP Test . https://craaptest.net/  \nDictionary.com, LLC. (2023). Disinformation. In Dictionary.com. Retrieved June 7, 2022, from  \nhttps://www.dictionary.com/browse/ disinformation  \nDictionary.com, LLC. (2023). Misinformation. In Dictionary.com. Retrieved June 7, 2022, from \nhttps://www.dictionary.com/browse/misinformation  \nDiehm, R. A., & Lupton, M. (2014). Learning information literacy. Information Research, 19(1), \n1-15. https://informationr.net/ir/19 -1/paper607.html  \nDuignan, B. (2022, June 23). Cognitive dissonance. In Britannica. \nhttps://www.britannica.com/science/cognitive -dissonance  \nFestinger, L. (1962). Cognitive dissonance. Scientific American , 207(4), 93- 106. \nhttps: //www.jstor.org/stable/24936719  \nFramework for information literacy for higher education. (2016). Association of College & \nResearch Libraries. https://www.ala.org/acrl/standards/ilframework  \nMETALITERACY IN THE DEVELOPMENTAL CLASSROOM  \nJournal of the National Organization for Student Success , 1(1)  69 \n Gross, M., & Latham, D. (2012). What's skill got to do with it?: Information literacy skills and \nself\u2010views of ability among first\u2010year college students.  Journal of the American Society \nfor Information Science and Technology , 63(3), 574-583. \nhttps://doi.org/10.1002/asi.21681  \nJacobson, T. E., & Mackey, T. P. (2013). Proposing a metaliteracy model to redefine information \nliteracy.  Communications in Information L iteracy, 7(2), 84-91. \nhttps://doi.org/10.15760/comminfolit.2013.7.2.138 \nJacobson, T. E., Mackey, T. P., & O'Brien, K. L. (2021). Visualizing the convergence of \nmetaliteracy and the information literacy framework. University Libraries Faculty \nScholarship. 149. https://scholarsarchive.library.albany.edu/ulib_fac_scholar/149 \nLatham, D., & Gross, M. (2013). Instructional preferences of first-year college students with \nbelow-proficient information literacy skills: A focus group study. College & Research Libraries, 74 (5), 430-449. https://doi.org/10.5860/crl-343  \nLing, R. (2020). Confirmation bias in the era of mobile news consumption: The social and \npsychological dimensions. Digital Journalism , 8(5), 596-604. \nhttps://doi.org/10.1080/21670811.2020.1766987 \nMackey, T. P., & Jacobson, T. E. (2011). Reframing information literacy as a metaliteracy. \nCollege & R esearch Libraries , 72(1), 62 -78. https://doi.org/10.5860/crl-76r1  \nMBFC. (2023). Media bias/Fact check. https://mediabiasfactcheck.com/  \nMcCoy, E. J. (2022). Teaching and assessment of metacognition in the information literacy \nclassroom.  Communications in Information Literacy , 16(1), 5. 42-52. \nhttps://doi.org/10.15760/comminfolit.2022.16.1.5 \nMcGrew, K., Bogue, E., & Else, I. (2015, June). Unschooled and unaware: Authentic assessment \nof faculty and student information literacy  [Poster presentation] . American Library \nAssociation (ALA) Annual Conference, San Francisco, California . \nMedia Bias \u2014AllSides. (2024). AllSides . https://www.allsides.com/media- bias. \nMetaliteracy Learning Collaborative. (n.d.). Metaliteracy.org. https://metaliteracy.org/  \nPachtman, A. B. (2012). Developing c ritical thinking for the internet. Research & Teaching in \nDevelopmental Education, 29(1), 39\u201347. https://www.jstor.org/stable/42802400 \nShapiro, J. J., & Hughes, S. K. (1996). Information literacy as a liberal art? Educom R eview , 31, \n31-35. https://wikis.evergreen.edu/selfstudy/images/6/67/Educom_review.pdf \nStanton, J. D., Sebesta, A. J., & Dunlosky, J. (2021). Fostering metacognition to support student \nlearning and performance. CBE \u2014Life Sciences Education, 20 (2), 1-7. \nhttps://doi.org/10.1187/cbe.20-12-0289 \nZimmerer, M., Skidmore, S. T., Chuppa-Cornell, K., Sindel-Arrington, T., & Beilman, J. (2018). \nContextualizing developmental reading through information literacy. Journal of Developmental Education, 41(3),  2-8. https://www.jstor.org/stable/44987487 \n \n \nThis work is licensed under  Creative Commons BY 4.0  \n", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Metaliteracy in the Developmental Classroom", "author": ["HM McGrew"], "pub_year": "2024", "venue": "Journal of the National Organization for Student \u2026", "abstract": "This paper investigates the origins of metaliteracy with a focus on media bias, misinformation,  and disinformation and their pedagogical implications for information literacy (IL) in the"}, "filled": false, "gsrank": 305, "pub_url": "https://www.jnoss.org/article/id/14/", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:uLJlR_ZkSLQJ:scholar.google.com/&output=cite&scirp=304&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D300%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=uLJlR_ZkSLQJ&ei=OrWsaI6ND8DZieoPqdqh8QU&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:uLJlR_ZkSLQJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.jnoss.org/article/id/14/download/pdf/"}}]