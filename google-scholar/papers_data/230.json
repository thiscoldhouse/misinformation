[{"title": "TROPIC\u2013Trustworthiness Rating of Online Publishers Through Online Interactions Calculation", "year": "2025", "pdf_data": "TROPIC \u2013 Trustworthiness Rating of Online\nPublishers through online Interactions\nCalculation\nManuel Pratelli1,2[0000\u22120002\u22129978\u2212791X], Fabio Saracco3,1,4[0000\u22120003\u22120812\u22125927],\nand Marinella Petrocchi2,1[0000\u22120003\u22120591\u2212877X]\n1IMT School for Advanced Studies Lucca, Lucca, Italy\n2CNR-IIT, Pisa, Italy\n3\u2018Enrico Fermi\u2019 Research Center (CREF), Rome, Italy\n4CNR-IAC, Sesto Fiorentino, Italy\nAbstract. Existing methods for assessing the trustworthiness of news\npublishers face high costs and scalability issues. The tool presented in\nthis paper supports the efforts of specialized organizations by providing\na solution that, starting from an online discussion, provides (i) trust-\nworthiness ratings for previously unclassified news publishers and (ii)\nan interactive platform to guide annotation efforts and improve the ro-\nbustness of the ratings. The system implements a novel framework for\nassessing the trustworthiness of online news publishers based on user\ninteractions on social media platforms.\nKeywords: OnlinePublisherTrustworthiness \u00b7SocialNetworkAnalysis\n1 Introduction\nThe decline of traditional journalism, coupled with the increasing challenges\nof maintaining high standards of editorial quality, has raised concerns about\nthe trustworthiness of online news media. Specialized organizations [9,8,7,6,1]\nprovide valuable ratings that assess the credibility of digital news publishers.\nAlthough these ratings provide beneficial insights, they are based on the evalu-\nation of resource-intensive and time-consuming criteria, such as the presence of\nbiased and propagandist content in the news articles [10,11]. As a result, many\nonline publishers remain unlabeled, creating a gap in coverage.\nProposal: Based on the work in [13], this demo paper presents TROPIC\u2013\nTrustworthiness Rating of Online Publishers through online Interactions Cal-\nculation (available at https://tropic.iit.cnr.it ), an interactive tool to im-\nprove current methods for assessing the trustworthiness of an online publisher\nand to avoid potential problems of limited coverage. TROPIC outputs include:\n\u2013 The automatic classification of the level of trustworthiness of an\nonline news publisher: TROPIC provides an assessment of the trustwor-\nthiness of unclassified online news publishers by analyzing social media inter-\nactions between news producers and consumers. The evaluation starts witharXiv:2501.13561v2  [cs.SI]  13 Feb 2025\n2 M. Pratelli, F. Saracco and M. Petrocchi\nthe analysis of an online social discussion and leverages (i) a base-knowledge\nconsisting of a subset of annotated news publishers, (ii) the concept of News\nEngagement Communities , NECs for brevity, which are communities of news\narticles that received the most attention from discussion participants, intro-\nduced in [12,13] and (iii) the propensity of social users to share low-quality\ninformation. This approach provides (i) good accuracy-cost trade-off and (ii)\ngood coverage of unclassified anduntrustworthy news publishers [13].\n\u2013 An interactive guide for base-knowledge extension: The robustness\nof the classificationcanbe continuously improvedbygradually extending the\ncurrentbase-knowledge . The intuition is that an extended knowledge allows\nfor more accurate social user profiling, and thus more accurate predictions\nabout the trustworthiness of unannotated news publishers. To guide the\nexpansion of the base-knowledge , TROPIC provides the end user with a\nset of functionalities that suggest the \"best\" next publisher to annotate.\nThis guides where to focus the extension of the base knowledge, taking best\nadvantage of the prediction system.\nContext: Manual methods of assessing the trustworthiness of online news-\npapers are costly, both in terms of the time required to perform the assessment\nand the cost of finding experienced reviewers. These barriers limit the reach of\nassessable newspapers. The low coverage also stems from the constant emergence\nof new, lesser-known online newspapers. TROPIC aims to streamline the rating\nprocess and expand the reach of rated newspapers.\nAudience: Designed for journalists, media professionals, and researchers\nevaluating the quality of online information, TROPIC provides estimates of the\ntrustworthiness of online publishers. The tool guides the user in the selection of\nonline publishers to manually annotate, so that the subsequent automatic eval-\nuation of news publishers not yet evaluated is effective in terms of prediction\naccuracy and coverage of the largest number of news publishers evaluated.\n2 System Overview\nExternal Data: Figure 1 shows the tool\u2019s architecture. TROPIC takes as input\nan online discussion, in the format edge-list, i.e., a list of URL - user ID pairs.\nThe URL points to an online news story and is present in social posts, the\nuser ID is the username of the social user who posted or shared that URL\non the social platform. The tool is adaptable to data from any social media\nplatform5. A second (optional) input for TROPIC is the initial base-knowledge,\nwhichisalistofonlinepublisherswithassociatedtrustworthinessscores(integers\nfrom 0 to 100). Since TROPIC is designed to support organizations specializing\nin evaluating news publishers\u2019 trustworthiness, the initial knowledge base can\nconsist of publishers already assessed by these organizations6.\n5URL preparation, such as resolving short URLs, is left to the TROPIC user.\n6For example, NewsGuard [9] uses the 0-100 scale, the same as TROPIC. In contrast,\nother organizations, such as Media Bias / Fact Check [8], use descriptive labels and\nmust be converted to numerical values to be used in TROPIC.\nTROPIC \u2013 Trustworthiness Rating of Online Publishers 3\nFig. 1.System Overview\nBackend: As we can observe in Figure 1, the TROPIC backend consists of\nindependent software components that work together to provide (i) a predicted\ntrustworthiness score (with a corresponding confidence level) for each publisher\nnot in the base-knowledge, and (ii) metrics to guide users on which publishers to\nannotate in case they want to manually expand the base-knowledge. The edge\nlist is used both to build NECs communities (which we recall are communities\nof news URLs that have been very successful among online users in terms of\nengagement [12,13]) and to calculate some quantity related to the online discus-\nsion we are studying (IR-related statistics). Once we have selected the publishers\nwhose URLs have been shared in the online discussion, we move on to selecting\na special group of users, called voters, and profiling them. After this profiling\nphase, these voters are asked to classify the publishers they interact with.\nFig. 2.Publisher Trustworthiness Classification\n4 M. Pratelli, F. Saracco and M. Petrocchi\nIn our current implementation:\n1. We select as voters those users who have shared at least one engaging news\n(i.e., a news whose URL belongs to one of the NECs).\n2. For profiling voters - i.e., estimating each voter\u2019s propensity to share low-\nquality content - we follow the approach in [13], which consists of a two-step\nmethod to downscale the problem. First, instead of considering the complete\nset of URLs in a voter\u2019s timeline, we only select engaging URLs, specifi-\ncally those belonging to NECs. Second, instead of evaluating each individual\nonline news article, we evaluate the trustworthiness of the URL by refer-\nencing the publisher-level score, if available, from the base knowledge (e.g.,\nsource-level scores of 95, 90, and 85, as shown in Figure 2, bottom). These\npublisher-level scores are aggregated using a weighted average, resulting in\na single final score for each voter (middle part in Figure 2).\nFinally, to compute the trustworthiness score for an unclassified publisher,\nwe select all voters who share content from that publisher, profile them, and\naggregate their scores into a single (average) number that expresses the trust-\nworthiness score of the considered publisher (Figure 2, top).\nFig. 3.The User Interface\nUser Interface: Figure 3 shows the User Interface (UI). The toolbar al-\nlows to upload the edge list (via the \"File Upload\" button) and, optionally, the\nbase-knowledge , and to initiate the computation via the \"Process data\" button.\nCalculation results are displayed in three plots and a table. The table lists each\nTROPIC \u2013 Trustworthiness Rating of Online Publishers 5\npublisher\u2019s Name, IR-related statistics, an editable Annotated Score field (this\nallowstheusertomanuallyannotateotherdomains,ifdesired,toextendthebase\nknowledge), a Trustworthiness column (showing both binary labels - trustworthy\nT/ untrustworthy N) and prediction scores, a Confidence column, and a State\ncolumn (indicating if the rating comes from human annotation or from TROPIC\n- A stands for annotated, P stands for Predicted). Users can sort the table by\nIR-related metrics, such as the number of voters or the number of news URLs\nshared within NECs. This way, manual annotations can be prioritized based on\na metric of interest. Above the table, plots provide information on the number of\nmanually annotated publishers and the distribution of their scores (left plot); the\nnumber of publishers annotated by human experts (dark color at bottom) and\nthose whose score are calculated by TROPIC (light color at top) (middle plot);\nand the confidence level of the scores calculated by TROPIC (right plot). This\ninformation guides the user to minimize the number of publishers that remain\nunclassified and to improve the confidence level of the predictions (by adding\nnew manual annotations if necessary). The \"Export Actual Knowledge\" button\nallows to export the calculation results in CSV format.\nImplementation and Demonstration: The user interface is built with\nAngular [2], and the backend uses FastAPI [5]. NECs extraction employs the\nbicm Python library [3]. Both UI and backend are containerized separately with\nDocker [4]. For practical testing, we provide the tool in a DEMO version. In this\nconfiguration, the waiting times for calculating the NECs, which could be several\nminutesforaverylargeedgelist,arepre-calculated.Toutilizethissetup,theend\nuser is supplied with a DEMO edge list, which must be selected before pressing\n\"Process Data\". For completeness, a DEMO base-knowledge is also provided,\nfeaturing randomly generated trust scores. If end users want to upload their\nown edge list, the number of edges that can be uploaded in the demo version is\nlimited to 50,000 entries.\n3 Conclusions\nInthisdemopaper,wepresentedaninteractivewebinterfacedesignedtostream-\nline the annotation process for assessing the trustworthiness score of an online\nnewspublisher.TROPICpredictstrustworthinessscoresforpublishersthathave\nnot yet been manually labeled, improving the coverage and reliability character-\nization of news domains traffic exchanged in an online discussion.\nAcknowledgments. This work is partially supported by SERICS (PE00000014) un-\nder the NRRP MUR program funded by the EU - #NGEU. FS was partially supported\nby the project \u201cCODE \u2013 Coupling Opinion Dynamics with Epidemics\u201d, funded under\nNRRP MUR Mission 4 \u201cEducation and Research\u201d - Component C2 - Investment 1.1 -\nNext Generation EU \u201cFund for National Research Program and Projects of Significant\nNational Interest\u201d PRIN 2022 NRRP, grant code P2022AKRZ9.\n6 M. Pratelli, F. Saracco and M. Petrocchi\nReferences\n1. Ad fontes media. https://adfontesmedia.com/ , accessed: October 23, 2024\n2. Angular. https://angular.dev/ , accessed: October 23, 2024\n3. Bicm. https://github.com/mat701/BiCM , accessed: October 23, 2024\n4. Docker. https://www.docker.com/ , accessed: October 23, 2024\n5. Fastapi. https://fastapi.tiangolo.com , accessed: October 23, 2024\n6. The global disinformation index. https://www.disinformationindex.org/ , ac-\ncessed: October 23, 2024\n7. Iffy index. https://iffy.news/index/ , accessed: October 23, 2024\n8. Mediabias fact check. https://mediabiasfactcheck.com/ , accessed: October 23,\n2024\n9. Newsguard. https://www.newsguardtech.com , accessed: October 23, 2024\n10. Bazmi, P., Asadpour, M., Shakery, A.: Multi-view co-attention network for fake\nnews detection by modeling topic-specific user and news source credibility. Infor-\nmation Processing & Management 60(1), 103146 (2023)\n11. Kim, A., Moravec, P., Dennis, A.: Combating fake news on social media with\nsource ratings: The effects of user and expert reputation ratings. Journal of Man-\nagement Information Systems 36(3), 931\u2013968 (2019). https://doi.org/10.1080/\n07421222.2019.1628921\n12. Pratelli, M., Saracco, F., Petrocchi, M.: Entropy-based detection of Twitter echo\nchambers. PNAS Nexus 3(5), pgae177 (04 2024). https://doi.org/10.1093/\npnasnexus/pgae177 ,https://doi.org/10.1093/pnasnexus/pgae177\n13. Pratelli, M., Saracco, F., Petrocchi, M.: Unveiling news publishers trustworthi-\nness through social interactions. In: Proceedings of the 16th ACM Web Sci-\nence Conference. p. 139\u2013148. WEBSCI \u201924, Association for Computing Machin-\nery, New York, NY, USA (2024). https://doi.org/10.1145/3614419.3644015 ,\nhttps://doi.org/10.1145/3614419.3644015", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "TROPIC\u2013Trustworthiness Rating of Online Publishers Through Online Interactions Calculation", "author": ["M Pratelli", "F Saracco", "M Petrocchi"], "pub_year": "2025", "venue": "European Conference on Information \u2026", "abstract": "Existing methods for assessing the trustworthiness of news publishers face high costs and  scalability issues. The tool presented in this paper supports the efforts of specialized"}, "filled": false, "gsrank": 364, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-88717-8_30", "author_id": ["4DENie4AAAAJ", "ZKtoqzUAAAAJ", "3-vjXDsAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:v2-6oJE4LegJ:scholar.google.com/&output=cite&scirp=363&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D360%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=v2-6oJE4LegJ&ei=RrWsaO_lEeHUieoP9LKZ6AI&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:v2-6oJE4LegJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2501.13561"}}, {"title": "Analysis of Strategy and Spread of Russia-sponsored Content in the US in 2017", "year": "2018", "pdf_data": "Analysis of Strategy and Spread of Russia-sponsored Content in the US in 2017\nAlexander Spangher1\u0003, Gireeja Ranade2,3, Besmira Nushi3, Adam Fourney3, and Eric Horvitz3\n1Carnegie Mellon University\n2University of California, Berkeley\n3Microsoft Research\nAbstract\nThe Russia-based Internet Research Agency (IRA) carried out a\nbroad information campaign in the U.S. before and after the 2016\npresidential election. The organization created an expansive set\nof internet properties: web domains, Facebook pages, and Twitter\nbots, which received traffic via purchased Facebook ads, tweets,\nand search engines indexing their domains. We investigate the\nscope of IRA activities in 2017, joining data from Facebook\nand Twitter with logs from the Internet Explorer 11 and Edge\nbrowsers and the Bing.com search engine. The studies demon-\nstrate both the ease with which malicious actors can harness so-\ncial media and search engines for propaganda campaigns, and the\nability to track and understand such activities by fusing content\nand activity resources from multiple internet services. We show\nhow cross-platform analyses can provide an unprecedented lens\non attempts to manipulate opinions and elections in democracies.\nThe Internet Research Agency (IRA) has been identified as\na Russia-based company focused on media and information\npropagation (Intelligence Community Assessment 2017). The\norganization was found to have spent at least 5.8 million\nrubles, or sixty-eight thousand USD, from June 8, 2015\nto July 1, 2017, on disseminating information to the U.S.\npublic via Facebook advertising. They fielded thousands\nof Facebook advertisements and sent millions of Tweets,\npromoting hundreds of web domains and Facebook groups\nthat spanned the political spectrum (Schiff 2018a; 2018b;\nDarren L. Linvill 2018). According to an indictment made\nagainst IRA by the U.S. Special Counsel\u2019s Office on February\n16, 2018, the purpose of IRA \u2019s expenditures of effort and capital\nwas to \u201csow discord in the U.S. political system, including the\n2016 U.S. presidential election\u201d (Robert S. Mueller 2018).\nIn the process of investigating Russian election interference,\nthe U.S. House Intelligence Committee released IRA-linked\nFacebook advertisements and Twitter accounts into the public\ndomain (Schiff 2018a; 2018b). As stated in the release:\nRussia exploited real vulnerabilities that exist across\nonline platforms and we must identify, expose, and defend\nourselves against similar covert influence operations in\nthe future.\nWe have pursued an understanding of how IRA\u2019s campaign\ntargeted these \u201cvulnerabilities, \u201d in the hopes of contributing to\nan awareness of how malicious actors can exploit large-scale\ncommercial internet services. We merge Facebook and Twitter\ndatasets with logs from Microsoft\u2019s web browsers and Bing\nsearch engine, thus fusing data from three major internet\n\u0003Research performed during an internship at Microsoft Research.\nFigure 1: IRA campaign structure. Illustration of the structure\nof IRA sponsored content on the web. Content spread via\na combination of paid promotions (Facebook ads), unpaid\npromotions (tweets and Facebook posts), and search referrals\n(organic search and recommendations). These pathways pointed\nto a combination of pages, some of which were created by the\nIRA (e.g. blackmattersus.com ). IRA properties often\ncontained engagement levers designed to maintain contact with\nusers and push them towards events pages and donation pages.\ncompanies to provide a broader lens on the overall scope and\ninfluence of the IRA generated content (Fig. 1).\nWe find that IRA-generated content spanned the political\nspectrum and covered a combination of apolitical, informative,\nlocal news and inflammatory articles on politically sensitive\nsubjects. During the time frame of our studies, we found that\nthe IRA invested more resources on Facebook in promoting\nleft-leaning content than on right-leaning content. Right-leaning\nTwitter handles received more traffic than left-leaning handles\non Twitter. We describe two case studies that show that the\napolitical content and local news related IRA-properties reached\nusers through search, and played a role in bringing traffic to\nIRA domains. We examine the IRA Facebook ad keyword\ntargeting strategy and we investigate correlations between the\nregions where the ads received clicks, and the demographics\nof those regions.\nOur specific findings are as follows:\n1.IRA Content: The IRA produced and amplified a diverse\narray of left-leaning, right-leaning and apolitical content. We\nshow two examples of Facebook advertisements in Fig. 2.arXiv:1810.10033v1  [cs.SI]  23 Oct 2018\nFigure 2: Two example Facebook ads run by the IRA in 2017,\nout of 1,400 ads run post-election. The top ad is for the group\n\u201cPatriotUs\u201d, while the bottom is for the group \u201cPan-African\nroots\u201d.\nThe advertisements and tweets ranged from emotionally\ncharged to neutral (Fig. 3). IRA content drew traffic\nmainly from social media and search domains. While some\nIRA-established properties amplified politically sensitive\ntopics (e.g. the Facebook group StopAllInvaders) others\nshared apolitical content1. One web property encouraged\nusers to attend events, subscribe to mailing lists, and donate\n(see Tables 1, 4, Sections III, IV).\n2.IRA Strategies : Through a study of traffic to the IRA\nproperties we are able to make educated guesses about\nstrategies that the IRA might have employed. For instance,\nwe find that while there were similar amounts of left and\nright-leaning tweets on Twitter, the right-leaning tweets\nreceived more traffic in our datatset (Fig. 3). On the other\nhand, the IRA produced more left-leaning content on\nFacebook. Furthermore, they spent more money promoting\nleft-leaning ads than right-leaning ads on Facebook.\nWe use a basic model to estimate the effect of the paid\npromotions on traffic spikes. Under certain assumptions\nand for our basic model we find the paid promotions likely\nincreased traffic spikes to the left-leaning properties (Fig. 8).\nWe identify a strategy of employing apolitical content to gar-\nner credibility and engagement; such content received traffic\nvia Bing on Microsoft promotional channels. One IRA article\nabout Black Female Computer Scientists2appeared among\n1For example: \u201dBlack Female Computer Scientists. How Many\nDo Y ou Know?\u201d published at https://blackmattersus.com/31081-black-\nfemale-computer-scientists-how-many-do-you-know/\n2https://blackmattersus.com/31081-black-female-computer-\nscientists-how-many-do-you-know/the search results returned for a query that was promoted\nacross Microsoft device lock-screens (Fig. 7 and Section V).\nFinally, we present a case study on a tactic employed on\nTwitter: rapid tweeting about evolving local news stories\n(Fig. 5). We believe a goal may have been to capture clicks\nbefore local news outlets were able to cover the story.\n3.IRA Outcome/Effects: Despite the large volume of content\ngenerated by the IRA, on average only roughly 1in40;000\ninternet users in our dataset clicked on an IRA-property\n(Tweet, Facebook Group, URL) on a given day. Furthermore,\nwe found only low volumes of traffic to meetup pages,\ndonation pages and contact forms on the IRA domains (Table\n4). Likewise, we found low correlation between geographic\nareas of heavy traffic to IRA domains and U.S. protests in\n2017. We found no significant changes in traffic to news\nwebsites and to extreme news websites or to political dona-\ntion websites (within a single browsing session, after a user\nwas exposed to IRA content). This warrants further research\nthat could take into account what we could not observe, such\nas traffic before and during the election, broader traffic from\nmultiple browsers and search engines, traffic from mobile\napplications, long-term user behavioral change, real-world\nactions and posts made after exposure (see Section VI).\nThe remainder of the work is structured as follows: We begin\nby describing our datasets and methodology (Section II). We\nthen describe the high-level structure of the IRA web properties\nand promotions (Section III), and follow by discussing the\nthemes and messages that were ultimately promoted and clicked\non (Section IV). We discuss the impact of paid promotions on\nFacebook as well as the role played by search through some\ncase studies (Section V). Finally we discuss the implications\nand limitations of this research, and contextualize it with prior\nresearch in this space.\nII. Data and Methodology\nWe have harnessed data from three large internet companies\nto provide a perspective on the strategy of the IRA, including\nviews on the spread and access of IRA-generated content\nfrom January 2017 to August 2017. In addition, this work also\nleverages additional data sources including census information.\nWe describe each data source in turn.\nPrimary datasets\nFacebook ads: On May 10, 2018, the House of Representatives\nPermanent Select Committee on Intelligence released (as\nPDF documents) 3393 Facebook advertisements reported\nby Facebook as paid for by IRA-linked entities3(Schiff\n2018a). We performed optical character recognition (OCR) and\ntemplate-based information extraction, and this process yielded\n3061 ads with actionable data.\nIRA-linked Tweets: Alongside the Facebook ads data, the\nHouse Intelligence Committee also released a dataset on June\n8th consisting of 3841 Twitter account handles believed to\nbe associated with the IRA (Schiff 2018b). Researchers at\nClemson University scraped over 2.9 million tweets from\nthese handles and performed language and topic classification\n(Darren L. Linvill 2018).\n3PDF documents for the individual ads included: text associated\nwith the ad, the ad\u2019s start and end dates, targeting information, and\nthe URL of the web property that the ad was promoting.\nWe leverage their work and focus our analysis on English-\nlanguage tweets occurring from January 1st to August 1st,\n2017 from the following categories they assign: \u201cLeftTrolls\u201d,\n\u201cRightTrolls\u201d, \u201cLocal\u201d and \u201cNews\u201d (See (Darren L. Linvill\n2018) for more details). This resulted in a total of 471;000\ntweets and 320handles.\nAdditionally, many of the tweets link to external domains.\nWe augment the data by resolving Twitter\u2019s link-shorteners\nusing historic instrumentation logs from Microsoft web\nbrowsers, described next.\nBrowsing data: We consider 212 days (January 1 to August\n1, 2017) of instrumentation data collected by Microsoft Edge\nand Internet Explorer 11 desktop web browsers.4Data includes\nanonymized time-stamped records of page visits. Records are\nassembled into sessions of browsing activity: a \u201csession\u201d is\ndefined as a sequence of page visits such that consecutive visits\nare less than 30 minutes apart.\nWe also used this data to identify upload dates for photos and\nvideos in Facebook groups. Facebook assigns photos and videos\nunique ids visible in the url (i.e. facebook.com/<IRAg\nroup>/photos/<id> andfacebook.com/<IRAgro\nup>/videos/<id> ). We group the content by these ids and\nrecover the earliest dates for each ID that we observe clicks.\nThis gives us a sense of when the item was posted.\nWeb search data: Finally, we also analyze the logs of Bing,\na major web search engine. This data reveals which URLs\nwere presented as search results, even if such pages were not\nultimately clicked by users \u2013 something that the browsing\ninstrumentation data cannot provide. This dataset considers the\nsame 212 days of data as the browser instrumentation logs.\nJoins of primary datasets\nWe join the primary datasets to recover the scope of the IRA\ncampaign. We associate IRA Facebook advertisements with\nbrowsing data by identifying clicks from facebook.com\nto a URL promoted by an IRA ad5. An equivalent approach\nwas followed for joining browsing data with Twitter account\nhandles and tweets6.\nSearch engine logs were joined to the social media data by\nmatching the URLs of search results to the URLs of pages\nactively promoted by Facebook ads and Tweets sent from\nIRA-linked twitter accounts. In search logs, we detect both\nwhen links are presented (impressions) and clicked.\nCrowdsourced labeling of primary data\nWe used crowdworkers to label content7to ascertain infor-\nmation about the IRA content\u2019s political leaning, emotional\nintensity, and topic. We ran this study for Facebook ads, tweets,\nand search results related to IRA-owned web domains.\n4Browser data is collected anonymously with user permission.\n5After normalizing and reversing URL-transformations that Face-\nbook applies to URLs (e.g. splicing /pg/ or/?/ from URLs). Such\nevents are consistent with users clicking on ads, but are not sufficient to\nconclude that a particular ad was clicked because (a) a user may have\narrived at a page from elsewhere in Facebook (e.g. from the newsfeed,\nor a notification) and (b) multiple ads promote a common URL.\n6We searched for clicks with twitter.com and\n<accounthandle> as substrings, thus capturing clicks on\ntweets andaccount handles.\n7We use Amazon Mechanical Turk. Workers were paid on average\n$12 per hour, above the national minimum wage at the time of\npublication.For Facebook ads, we rated all 1032 ads that ran after\nJanuary 1, 2017. For Twitter we rated two subsets: a random\nsubset of 500 IRA-linked tweets, and the top 500 most clicked\ntweets. For Search, we rated the 100 URLs with the most\nimpressions (these 100 URLs captured 63% of search-result\nimpressions to URLs in our datasets).\nThe task showed workers the text content of the promotion:\nthe original Facebook ad, tweet text, or the snippet text shown\nin search results. Based on this information, the workers\u2019 task\nwas to label the content with the most relevant option in each of\nthe following categories: 1. Political leaning :fextreme-left, left,\ncenter, right, extreme-right, apolitical, local news g. 2.Emotional\nintensity :fneutral, low, medium, high, very high g. 3.Discussed\ntopic: A predefined topic list.8\nEach item was judged by five different crowd workers. We\ndid quality control by excluding answers from workers with\nhigh disagreement rates (Inel et al .2014). We performed a\nsoft-assignment to label each item. For example, if a certain\nURL received 2 votes for extreme-right, 1 for left and 2\nfor extreme-left, then we counted 2=5;1=5and 2=5clicks\nin each category respectively. To calculate the number of\nclicks on a certain label/topic, we used the soft assignment to\nproportionally divide the clicks across the assigned labels.\nSecondary datasets\nIn addition to the primary datasets, we employ numerous exter-\nnal secondary datasets. We use Mediabias News and Media Cat-\negories (Zandt 2018) and SimilarWeb Domain Categories (Offer\n2018), which provide URL-level categorizations, to characterize\nthe content of URLs in our datasets. We use state-level popular\nvote counts for the 2016 presidential election (David Leip 2016),\nstate-level voter registration data (U.S. Census Bureau 2016),\nstate-level census demographic data (U.S. Census Bureau 2010),\nand state-level protest counts over time (Jeremy Pressman\n2018) to characterize browsing data by geography. GDELT\nGlobal-Events data, which captures media publications over\ntime, is used to characterize external news events (GDELT\n2018). Finally, we use OpenSecrets.org (Center for Responsive\nPolitics 2017) and Ballotpedia (BallotPedia 2017) to identify\nmajor political and donation sites over the period of interest.\nIII. Structure of the IRA Promotions\nBefore describing the traffic outcomes of the IRA campaign in\nSection IV , we present an overview of the scope and structure\nof the campaign to illustrate the breadth of the IRA \u2019s activity\non different platforms.\nFacebook Ads\nWe found that the Facebook advertisements paid for by\nthe IRA promoted a smaller set of 350 distinct URLs.\nThese URLs were distributed across 207 web properties.\nThese properties include Facebook groups and profiles;\nFacebook or meetup.com event pages; news websites\n(including CNN.com ); petitions ( whitehouse.gov ,\nchange.org ); and four domains identified as controlled\n8To extract the topic list we grouped Facebook ad keywords\nbased on their co-occurrence across advertisements. These were then\nmanually labeled (e.g. Black Lives Matter, V eterans, see Figure 4e\nfor a partial list.) See Section IV for more discussion on targeting tags.\nPromoted Content # Properties # Ads\nFacebook groups 104 2674\nEvents & meetups 82 223\nIRA domains 4 128\nNews organizations 4 4\nOther 16 35\n# Total 207 3061\nTable 1: Facebook Ad Categories. Web properties promoted\nby the IRA Facebook campaign includes Facebook groups,\nevents and domains suspected to be under the editorial control\nof the IRA (Schiff 2018a).\nby the IRA ( blackmattersus.com ,dudeers.com ,\nblack4black.com ,donotshoot.us )9(See Table 1).\nThe breadth of different content-types is notable (petitions,\nevents, articles, Facebook groups), and raises questions about\nthe intent of the IRA campaign. In the next section covering\ntraffic patterns, we limit our analysis to traffic. However,\nwe note that the reach of the IRA campaign did go beyond\nwhat can be measured by just traffic. For instance, one of\nthe IRA \u2019s petitions10received 65,000 supporters, well within\nthe range of typical mid-to-high performing change.org\ncampaigns (according to top popular petitions listed at\nhttps://www.change.org/petitions ).\nTwitter\nThe 3;841Twitter handles released by (Schiff 2018b) and\n2.9 million tweets compiled by (Darren L. Linvill 2018)\nwere filtered to 471;000English-language tweets from 320\n\u201cLeftTrolls\u201d, \u201cRightTrolls\u201d, \u201cLocal\u201d and \u201cNews\u201d accounts. Our\nanalysis of links in these tweets revealed links to over 5;500\ndomains, tweets, and other properties. Over 95% of tweets are\nretweets, or reposting of other users\u2019 tweets, and many of the\ntweets point to well-known websites.\nWe found overlap in naming between the Facebook and\nTwitter campaigns: for instance, the Twitter campaign included\na Twitter handle \u201cblackmattersussoldier\u201d and the Facebook cam-\npaign included a \u201cBlackMattersUS\u201d Facebook Group. However,\nwe do not see the Facebook and Twitter campaigns sharing links\nor cross-promotions with each other. We observed only four\nURLs that were promoted both by Facebook ads and Tweets,\nall of them blackmattersus.com URLs with low-traffic.\nWe could only identify a small number of domains from the\nTwitter campaign as IRA-controlled (as compared to the Face-\nbook campaign), and many of the links promoted by IRA tweets\nincluded major domains such as nytimes.com . As a result\nwe focus on an analysis of the tweets themselves (for the subset\noutlined in Section II), and not the domains they promoted.\nWeb Search\nAccording to the February 2018 indictment by the Special\nCounsel\u2019s office, the IRA was \u201c organized into departments,\n9See https://intelligence.senate.gov/sites/default/files/documents/exhibits-\n080118.pdf, https://euronews.com/2018/05/10/sean-hannity-black-\nlives-matter-among-targets-russian-influence-campaign-n872926,\nhttps://money.cnn.com/2017/10/12/media/dont-shoot-us-russia-\npokemon-go/.\n10https://www.change.org/p/barack-obama-u-s-house-of-\nrepresentatives-u-s-senate-list-the-ku-klux-klan-as-an-official-\nterrorist-organizationincluding: a graphics department; a data analysis depart-\nment; a search-engine optimization (SEO) department; an\ninformation-technology (IT)... \u201d (Robert S. Mueller 2018). We\nfind that IRA-promoted web properties were indexed by search\nengines, and surfaced to users in response to organic web search\nqueries. Of the suspected IRA-owned domains we find one\ndomain in particular, blackmattersus.com , received\nsignificant traffic via Bing (Fig. 6 and Fig. 7). We did not\nfind evidence of paid advertising on Bing, but more research\nis needed for a comprehensive evaluation.\nTargeting Decisions\nThe IRA used 890 different targeting keyword across the\nFacebook ads. The most frequent used keywords are all\npolitically charged or demographically salient, with the top\nfive being \u201cafrican american civil rights movement\u201d, \u201cafrican\namerican history\u201d, \u201cmalcolm x\u201d and \u201cmartin luther king jr\u201d.\nThese five keywords are tagged to 32.3% of ads, which link\nto pages capturing 28% of traffic.\nWe identified pairs of keywords used together with\ncorrelation c>:6, (p<:01), then grouped these pairs together\nto identify clusters. We identified 19 such clusters using this\nmethod. When used together the keywords seem to indicate\nan attempt at demographic targeting. (We will analyze the\ndemographic reach of the IRA campaign in Section V).\nOn Twitter, the most frequently used hashtags (of 44,700 total\nunique hashtags) are a combination of political and apolitical\ntags, with the top five being: \u201c#MAGA11\u201d, \u201c#NowPlaying\u201d,\n\u201c#tcot12\u201d, \u201c#top\u201d and \u201c#PJNET13\u201d. The top ten are tagged to\n2.1% of tweets which themselves account for 1.3% of traffic:\nhashtags on Twitter were more diverse and did not repeat as\nfrequently as the Facebook ads keywords.\nIV . IRA Content and Traffic\nHaving described the breadth and size of the IRA \u2019s campaign\nin Section III, we now summarize the content of the campaign\nand provide an overview of the traffic to it.\nTraffic Overview\nAs noted in the introduction, on average roughly 1in40;000\ninternet users was exposed to IRA ads on any given day in our\ndataset.\nPerhaps because the platforms\u2019 audiences are different\n(Hughs 2012), or because there was little cross-promotion\nbetween the Twitter and Facebook IRA campaigns, we find\nvery small overlap between users exposed to both campaigns.\nCross-traffic between IRA Facebook properties and IRA tweets\nis also small, amounting to about 0.02% of the total users in\nour datasets clicking on content from both campaigns in the\nsame day.\nTable 2 shows the most trafficked Facebook groups and\nTwitter handles in each campaign, which account for more than\n3=4th of total observed traffic in each case. The top-clicked Face-\nbook groups seem to show a mix of topics and political leanings.\nFigures 3a and 3b summarize the common referral pathways\nto IRA properties. Traffic to Facebook-advertised URLs\nand groups came mainly from Facebook and Search. The\n11Make America Great Again\n12Top conservatives on Twitter\n13Patriot Journalist Network\n102\n101\nOtherNews and MediaOther Social NetworkTwitterEmailOther Search EngineGoogleBingFacebook(a) Referral pathways for clicks\non URLs included in the IRA\nFacebook ads.\n102\n101\nOtherTV and VideoBingNews and MediaGoogleEmailFacebookRedditTwitter(b) Referral pathways for\nclicks on tweets linked to IRA\naccounts.\nFigure 3: Top traffic channels to IRA properties. Top sources\nthat brought users to IRA properties (Facebook-advertised\nURLs, Facebook groups and Tweets) from January 1, 2017 to\nAugust 1, 2017.\nTop-Clicked Facebook Groups Share of IRA Traffic\nblacktivists 0.208\ngodblessthesouth 0.199\nblackmattersus.mvmnt 0.169\nbrownunitedfront 0.116\npatriototus 0.077\nOther 0.231\nTop-Clicked Twitter Handles Share of IRA Traffic\ntengop 0.462\npamela moore13 0.245\ncrystal1johnson 0.076\nsouthlonestar 0.048\njenn abrams 0.045\nOther 0.123\nTable 2: Top-performing IRA properties. The top five\nFacebook groups (top) and Twitter handles (bottom) with most\ntraffic. These Facebook groups collectively account for 77%\nof traffic to IRA-promoted Facebook properties, while these\nTwitter handles account for 88% of traffic.\nTwitter campaign drew traffic mainly from Twitter, Reddit and\nFacebook, with Search playing a much smaller role.\nTraffic by Content-Type\nFigure 4 summarizes results from our crowdsourcing study for\nboth Facebook and Twitter. The left column summarizes the\nIRA generated content. The right column describes the traffic\n(volume of user cliks) to this content. The figure was created\nusing the proportional assignment of clicks as described in the\nmethodology section. The X-axis in all the histograms shows\nthe distribution of content (ads or tweets) soft-assigned to each\ncategory on the Y -axis.\nWe see from Fig. 4a that there were more left-leaning\nand apolitical ads than right-leaning ads on Facebook. The\nTwitter content appears to be more balanced, with most tweets\nbeing apolitical and roughly equal numbers of left-leaning and\nright-leaning tweets. When we compare this to the traffic that\nwas received in each category, we see that right-leaning and\napolitical content got relatively more traffic in our dataset than\nleft-leaning content on both Facebook and Twitter.\nIn addition to covering both sides of the political spectrum, the\nIRA content spanned a large topic space: Fig. 4e gives the distri-\nbution of content (ads, snippets, tweets) across categories, and\nFig. 4f gives the distribution of traffic14. Especially prominent in\n14Topic labels were chosen by manual assignment based on clusters\n0.0 0.2 0.4\nDistribution of LabelsExtreme Left-leaningLeft-leaningCenter-leaningRight-leaningExtreme Right-leaningApoliticalLocal News and Issues\nFacebook\nTwitter(a) Distribution of political lean-\ning of IRA Facebook posts and\ntweets. We see more left-leaning\nand apolitical links than right\nleaning links on Facebook. Twit-\nter has roughly equal amounts of\nright and left content.\n0.0 0.2 0.4\nDistribution of LabelsExtreme Left-leaningLeft-leaningCenter-leaningRight-leaningExtreme Right-leaningApoliticalLocal News and Issues\nFacebook\nTwitter(b) Distribution of traffic to\ndifferent categories of IRA\nFacebook posts and tweets, by\npolitical-leaning. Even though\nthere was more left-leaning\ncontent on Facebook than right-\nleaning, the right-leaning content\ntook a larger share of the traffic.\n0.0 0.2 0.4\nDistribution of LabelsNeutralLowMediumHighVery HighFacebook\nTwitter\n(c) Distribution of content across\nemotional intensity tags. A large\nfraction of the IRA promoted\nFacebook and Twitter content was\nneural and low emotional intensity.\nEspecially pronounced is the\nTwitter channel, where nearly\n43% of content is rated \u201dNeutral\u201d.\n0.0 0.2 0.4\nDistribution of LabelsNeutralLowMediumHighVery HighFacebook\nTwitter(d) Distribution of traffic across\nthe emotional valence of content.\nRelative to the proportions of\noverall content produced (c),\ncontent with medium and high\nemotional valence received more\ntraffic on Twitter.\n0.00 0.25 0.50\nDistribution of LabelsOtherNewsPoliticsChristianityLGBTQWar/ConflictVeteransBlack EntertainmentMuslim AmericansPatriotic AmericansGun RightsAfrican American HistoryMexican American IssuesBlack Lives Matter\nFacebook\nTwitter\n(e) Distribution of IRA content\nacross topics (Facebook and\nTwitter).\n0.00 0.25 0.50\nDistribution of LabelsOtherNewsPoliticsChristianityLGBTQWar/ConflictVeteransBlack EntertainmentMuslim AmericansPatriotic AmericansGun RightsAfrican American HistoryMexican American IssuesBlack Lives Matter\nFacebook\nTwitter(f) Distribution of traffic to IRA\ncontent across topics (Facebook\nand Twitter).\nFigure 4: Labels Summary: Content and Traffic. This figure\nshows the distribution of the IRA content across political leaning,\nemotional intensity and specific politically charged topics using\nlabels tagged in a crowdsourcing experiment. The assignment\nof content to categories was done using the soft-assignment of\ncrowdsourcing labels as described in the methodology section.\nthe Facebook campaign were topics targeting African American\nand Mexican American interests. The Twitter campaign seems\nto have focused on general news, politics and other topics.\namong targeting tags in the Facebook ads. The assignment of ad/tweet\nto topic was done by crowdsourcing.\nV . IRA Strategy\nHaving summarized the IRA content and traffic to it in Section\nIII, we now share inferences about IRA \u2019s strategies. We start\nwith three case studies.\n\u000fCase study 1: Rapid-response tweeting to local news devel-\nopments helped the IRA receive search engine impressions.\n\u000fCase study 2: Apolitical IRA-generated stories appeared\namong search results returned for search queries on\nBing.com, including some Microsoft-promoted queries (e.g.,\n\u201cwomen scientists\u201d). Overall, apolitical articles from organic\nand Microsoft-promoted queries received more traffic via\nthe Bing.com search engine than other articles.\n\u000fCase study 3: Paid promotions were not the only traffic\ndriving factor. In fact, we observe that for some of the groups,\nthere exist other important unpaid actions (e.g. photo and\nvideo shares) that can predict high traffic.\nWe conclude this section by showing how the IRA reached\nvarious demographically and politically distinct geographic\nregions in the country and extensively used Facebook\nmicrotargeting tags.\nCase study: Search and twitter.com/TEN_GOP\nThe IRA tweeted rapidly and in huge volumes across their\naccounts, tweeting in some cases thousands of times a day, and\nmultiple times within seconds. Figure 4 shows that they prin-\ncipally tweeted about \u201cNews\u201d, \u201cPolitics\u201d, and \u201cEntertainment\u201d.\nThe high volume of rapid tweets allowed them to be unwittingly\nincorporated into legitimate news articles at major Western\noutlets (For example, an article published by msn.com15ref-\nerences a tweet by @SouthLoneStar, a confirmed IRA account).\nIn this example, we show that these tweets performed espe-\ncially well at drawing unwitting traffic from users discovering\na new news story. The IRA \u2019s TEN GOP16Twitter handle,\nwhich drew 46% of traffic in the IRA \u2019s Twitter campaign, is\na good case study of how an IRA Twitter account can capture\nsignificant traffic by being indexed by a search engine during\nthe very early stages of a breaking news cycle.\nA news event about Dr. Henry Bello occurred on June 30,\n2017. This turned out to be a major news story in New Y ork City,\nreceiving thousands of news articles of coverage (GDELT 2018).\nHowever, the bulk of these articles were published and indexed\nby the search engine on July 1, 2017. Fifteen of the stories\npublished on July 1st were published by the Associated Press,\nhowever the remainder came from local domains, with the top\ndomains being: nydailynews.com (12 stories), denik.cz (12\nstories), heraldsun.com.au (11 stories) and news-sentinel.com\n(10 stories). Because the IRA tweeted about the event around\nnoon, they got indexed on June 30, beating many of the\narticles published and receiving a surge of impressions (i.e. be\ndisplayed as a search result) for the story (see Figure Figure 5).\nCase study: blackmattersus.com\nWe showed earlier that a large volume of apolitical\ncontent was produced by the IRA. We examine the\nblackmattersus.com domain as a case study of how this\n15https://www.msn.com/en-gb/news/uknews/london-attack-\nwoman-wearing-hijab-was-distressed-horrified-photographer-says/ar-\nBByFoiY\n16TEN GOP refers to Tennessee GOP . Many IRA accounts\nmimicked political accounts (Robert S. Mueller 2018).\nJan\n2017Feb Mar Apr May Jun Jul Aug0.000.250.500.751.00Impressions (% of max)TEN_GOP: \n \"NEW: \nDr. Henry Bello had  \n 3 previous arrests,  \n one for sexual abuse.\" All accounts\nTEN_GOP(a) Spike in search impressions (normalized as percent of max) that\ndisplayed the TENGOP tweet about Henry Bello on June 30, 2017.\n01\nJul\n201704 30 02 030.00.20.40.6TEN_GOP ImpressionsTime of\nEvent\n0200400600\nNews Articles Published\n(b) Distribution over impressions\nreceived by the TENGOP handle,\ncompared with the number of\nnews articles published on the\nsubject on each day.\n0.0 0.1 0.2\nDistribution over search termsothertennesseetwitterdrhenrybello(c) Top search terms leading to\nimpressions received in subfigure\n(a).\nFigure 5: Case study: Tweet impressions for the story of\nHenry Bello. Subfigure (a) shows impressions on IRA tweets.\nThe surge in impressions is likely linked to the early availability\nof news information that was unavailable elsewhere; most news\noutlets only published stories on this subject the following day\n(Subfigure (b)). Subfigure (c) shows the most common search\nterms that generated the impressions. Most of the stories were\npublished by local news outlets.\ncontent content played a role in a larger strategic aim: search\nengine traffic followed by an on-site engagement funnel.\nSearch Traffic blackmattersus.com contained a mix\nof content: content that was apolitical and emotionally neutral,\nas well as content that was political and emotionally intense (Ta-\nble 3 and Figure 6). We found that content rated as \u201cApolitical\u201d\ndrew more traffic on Bing while \u201cLeft-leaning\u201d drew more on\nFacebook. Additionally traffic to blackmattersus.com\nfrom Facebook was drawn more to pages that were \u201cHigh\u201d or\n\u201cMedium\u201d emotion-level, while traffic from Search centered\non pages that were more likely to be rated \u201cNeutral\u201d or \u201cLow\u201d\nemotion.17We next explore the mechanisms behind this content\nperforming well through search.\nBeyond generating ranked lists of results in response to search\nqueries input by end users, major search engines also power a\nhost of company properties like news verticals, homepages and\nnew-tab pages, which can link out to preformulated queries like,\nfor example, \u201cwomen scientists\u201d. Fig. 7 shows that, on several\noccasions, blackmattersus.com appeared in the results\n17These findings provide support to the speculation voiced by jour-\nnalists at (Mak 2018), that the IRA may have been generating apolitical\ncontent to serve a purpose: to gain Americans\u2019 trust in a network of\nseemingly local news handles, which could later be \u201coperationalize[d]\n[to] significantly influence the narrative on a breaking news story\u201d.\nTop Clicked URLs via facebook.com\n/nj-police-officer-arrested-and-charged-f...\n/her-provocative-ballet-dance-hurts-white...\n/tamron-hall-has-been-replaced-by-megyn-k...\nTop Clicked URLs via Search\n/black-history-month-black-inventor...\n/katherine-johnson-a-black-nasa-pioneer...\n/black-female-computer-scientists...\nTable 3: Case study: Blackmattersus.com, examples. Top\nthree most trafficked pages on blackmattersus.com\nfrom either Facebook (top) or Search (bottom).\nShare Button Contact Page Meetups Donate\n# links 3940 1 896 1\nClick rate 2.64% .28% .11% .056%\nTable 4: Blackmattersus.com Structure: \u201cShare Button\u201d are\nlinks on each article that encourage sharing on social media, the\n\u201cContact Page\u201d allows users to sign up for email correspondence,\n\u201cMeetups\u201d is a listing of local meetups and \u201cDonate\u201d is a page so-\nliciting donations that leads to a paypal.com page. (We saw\nno evidence of clicks to paypal.com in the same session.)\nreturned for such preformulated queries, which were promoted\nacross several Microsoft properties, including bing.com/n\news news verticals, new tab pages, and device lock screens.\nBy publishing neutral, entertaining content the IRA was able\nto draw more users to its pages.\n0.0 0.2\nDistribution of LabelsNeutralLowMediumHighVery HighFacebook\nSearch\n(a) Emotion tags given\nby crowdsourced workers\ntoblackmattersus\n.com URLs, weighed\nby traffic from either\nFacebook or search.\n0.0 0.5\nDistribution of LabelsExtreme Left-LeaningLeft-LeaningCenter-LeaningRight-LeaningExtreme Right-LeaningApoliticalLocal News And Issues\nFacebook\nSearch(b) Emotion tags given by crowd-\nsourced workers to blackmatte\nrsus.com URLs, weighed by traf-\nfic from either Facebook or search.\nFigure 6: Case study: Blackmattersus.com, summary.\nBlackmattersus.com was the most highly trafficked\nIRA-owned domain in our dataset. The emotional intensity\nof pages users reached from Facebook.com is higher than that\nfor pages reached via search, as can be seen by looking at the\ntop-trafficked pages on the domain, as well all trafficked pages\nas labeled by crowd workers.\nEngagement Funnel While gaining a wide reach\nthrough various promotional channels on the internet,\nblackmatters.com was structured to potentially engage\nusers beyond reading articles.\nblackmattersus.com is a particularly expansive site\nwith 4;608pages: articles, pages linking to Facebook eventpages, a donations page, and an input field soliciting users to\njoin the site\u2019s mailing list. Shortly upon arrival, a pop-up asks\nusers if they would like to receive notifications.\nWe performed a site-wide scrape to categorize pages\nhosted on the domain. Table 4 gives our overview of this\ndomain. On the first row, we show the raw number of links we\ncounted devoted to each of four action types: \u201cShare Button\u201d,\n\u201cMeetups\u201d, \u201cContact Page\u201d, and \u201cDonate\u201d. Interestingly, the\n\u201cMeetups\u201d are pages onsite that link out to meetup.com and\nFacebook events, many of which appear to be legitimate events,\nand are still active on Facebook. The \u201cDonate\u201d button links to\napaypal.com page18.\nEach of these link-types is accessible from any article. The\nnumber of users we see engaging with this funnel is very\nlow. The second row of Table 4 shows the conversion rate, or\nthe percentage of users who clicked on one of the links after\nclicking on another blackmattersus.com page. These numbers\nare only one 1/10th typical conversion rates observed across\ne-commerce sites industry-wide (Saleh 2017).\nCase study: Did promotions matter?\nThe IRA \u2019s ad-promotion strategy on Facebook involved\nroughly 531 ads and nearly 1million rubles (or $15;000:00) on\nleft-leaning content, compared with 184 ads and 620thousand\nrubles (or $9;112:00) on right-leaning content. The IRA spent\nmore on left-leaning content during our observation period.\nClearly, paid advertisements were part of the IRA strategy\nto generate traffic to their groups and properties. However, paid\npromotions are only part of the story. There are other important\nknown and unknown factors not to be ignored. Among the\nknown ones: historical traffic, photo and video shares are the\nmost correlated ones with traffic spikes.\nFor example, Fig. 8(a)-(b) illustrates the traffic to different\nFacebook groups over time along with (1) photo and video\nuploads and (2) paid promotions. For the StopAllInvaders Face-\nbook group, it seems some traffic spikes co-occur with events\nlike photo and video uploads, but do not co-occur with paid\npromotions. For another example group, BrownUnitedFront,\nspikes in traffic seem more aligned with paid promotions than\nwith media uploads. To isolate the influence of paid promotions\nfrom other unpaid factors, we trained a model with the goal\nof predicting the changes in spikes of high-traffic Facebook\ngroups in the absence of paid promotions.\nWe used our model to examine six Facebook groups chosen\nfor having high traffic as well as a large number of promotions.\nFor these groups, we infer that without paid promotion, the\nleft-leaning groups would have received fewer traffic spikes,\nwhile traffic spikes for right-leaning groups would have been\nlargely unchanged. However, an important take away here is\nthat paid promotions were not the only factor that lead to traffic\nspikes, which is something to keep in mind for future work.\nA more comprehensive discussion of this is included in\nAppendix 2.\nGeographic patterns in traffic\n87% of the traffic to Facebook-promoted properties was U.S.\nbased, but we observed clicks to IRA \u2019s properties from 176\ncountries.\n18Thepaypal.com page is operated by a merchant with the\nemail address xtimwaltersx@gmail.com , which was one of the emails\nmentioned in the Special Counsel\u2019s Indictment (Robert S. Mueller\n2018).\nFigure 7: Instances of Facebook-Promoted Domains Appearing in Search Results : The domains most associated with the\nFacebook-ad campaign often appeared as results to search queries, with surges of search query impressions occurring from time\nto time. Some of these spikes can be associated with the promotion of various topics. For instance, the largest spike corresponds\nto the page blackmattersus.com/31081-black-female-computer-scientists-how-many-do-you-know\nappearing among the results returned for the query, \u201cwomen scientists\u201d \u2013 a query which was briefly promoted on the lock screen\nof some Microsoft devices.\nstopallinvaders : brownunitedfront :\nJan Mar May Jul\n20170.000.020.04Fraction of Users050100\n# Photos & Videos\n(a) Spikes in unique users per\nday (black) sometimes co-occur\nwith photo and video uploads\nthroughout the period (green).\nJan Mar May Jul\n20170.000.020.04Fraction of Users050100\n# Photos & Videos(b) Spikes over time on this\nproperty, in contrast, do not show\nany clear relation to photo and\nvideo posts.\nJan Mar May Jul\n20170.000.020.04Fraction of Users01020\n# Ads Running\n(c) Spikes in unique users per day\n(black) show no alignment with\npromotions (maroon).\nJan Mar May Jul\n20170.000.020.04Fraction of Users01020\n# Ads Running(d) Spikes in unique users here\nshow alignment with promotions.\nFigure 8: Drivers of Spikes. Many promoted web-properties\n(shown above) seem to have click-spikes associated with factors\nlike paid actions (Facebook-ads) or unpaid actions (photo or\nvideo uploads). The Y -axis in each of the graphs represents the\nfraction of total unique users to the group over the time period\nthat visited on a given day. We quantify the effect of a paid\nFacebook promotion on the day of promotion by predicting\nspikes the property would have received had it not been\npromoted (see Appendix 2). This figure shows that in addition\nto paid promotions, unpaid actions (such as photo/video\nuploads) also impacted the traffic to the Facebook properties.\nWe consider a broad array of geographic features to describe\nthe areas contributing traffic including race-based, rural/urban,\neducation-based, employment-based and other demographicfeatures from the 2010 census (U.S. Census Bureau 2010).\nWe look at voting patterns from the 2016 presidential election\n(David Leip 2016). We also examine voting as a percentage of\nregistered voters, a metric that we use to proxy voter enthusiasm\n(U.S. Census Bureau 2016).\nFig. 9 shows state-level correlations between census features\nand traffic to IRA properties promoted via Facebook. The IRA\nproperties are clustered by targeting keywords and topics as per\nthe results of the crowdsourcing study. We only show a subset\nof features, and only display correlations with p<:1.\nTraffic to IRA content in certain categories is positively cor-\nrelated with demographic features. For example, traffic to IRA\ncontent labeled \u201cBlack Lives Matter\u201d is significantly correlated\nwith the percentage of census respondents self-identifying as\nAfrican American in a state ( c=:88,p < : 001). On the other\nhand, this content is negatively correlated with the percentage\nof census respondents self-identifying as White in a state.\nMoreover, there is a significant negative correlation ( c=\n\u0000:29;p<: 001) between the percentage of of Registered V oters\nwho V oted (in 2016) in a state with traffic across most ad groups.\nThus, there was more traffic to ads in states with lower voter\nturnout. This may support the claim that the IRA operation was\nseeking to target areas of voter discontent (Shane and Mazzetti\n2018), but is subject to many confounders. Furthermore, it is\nimportant to note that our work is looking at data collected in\n2017, two months after the 2016 U.S. Presidential election, and\nthus cannot infer anything about the pre-election timeframe.\nVI. Effects on exposed users\nHaving described case studies of the IRA strategy, we now\nclose with a view of the effects on users. Although our main\nconclusion here is that few effects were observed, more research\nis needed due to the following limitations. First, this analysis\ncannot observe the long-term behavior of internet users online\ndue to extensive anonymization techniques applied in the data.\nSecond, browser and query logs give us only a limited view\ninto users\u2019 online actions (e.g. we cannot see posts, tweets and\nemails), and most importantly we cannot observe real-world\nblacktivists\n0.00010.00020.00030.00040.0005(a) Traffic to Blacktivists Face-\nbook group normalized by aver-\nage daily active users by state.\nsecured.borders\n0.000020.000040.000060.000080.000100.000120.00014(b) Traffic to Secured Borders\nFacebook group normalized av-\nerage daily active users by state.\nBlack Entertainment\nBlack Lives Matter\nAfrican American History\nGun Rights\nMuslim Americans\nChristianity\nPatriotic Americans\nPolitics\nVeterans\nMexican American Issues\nWar/Conflict\nLGBTQ% African American Alone\nPopulation Density (per sq. mile)\n% Registered Voters who Voted\n% Urban\n% Total: $75,000 or more\n% Bachelor's degree or higher\n% Some other race Alone\n% In Armed Forces\n% White Alone\n% 2016 Vote Trump\n0.6\n0.3\n0.00.30.6\n(c) Correlation coefficients across demographic census features with\ncategories of the Facebook advertisements ( p<0:1shown)\nFigure 9: Correlation with state-level census features.\nThe Facebook-ad campaign was targeted primarily through\ninterest-tags. We grouped them into clusters by asking crowd\nworkers to label ad content. The top row shows distributions\nover traffic-normalized click counts by state for two sample\ngroups. The bottom figure shows correlations between census\nfeatures and normalized clicks by state ( p<0:1shown).\nbehaviors. Third, a complete analysis on these effects calls for\ndata from potentially multiple browsers and search engines to\nbetter cover the online user base, as well as data from a broader\nrange of mobile devices and applications. For completeness, we\nsummarize all attempts we made to better understand impact\non exposed users and hope that these attempts will seed further\ninvestigation on the topic.\nOur study explored online news consumption, online political\ndonations, and real-world protests. To categorize online news\nconsumption, we used data from MediabiasFactcheck.com to\nlabel domains known for publishing various news-types (Zandt\n2018). Using these labels, we examined the browsing behaviors\nof users exposed to an IRA ads before versus after exposure to\nthe ad, while controlling for session length. We did not see signif-\nicant changes in the total volume of news consumed, or in the ex-\ntremism of news sites visited before and after exposure to the ad.\nTo understand online political donations, we catalogued the\nwebsites registered for candidates, politicians and major recip-\nients of political donations, using data from OpenSecrets.org\n(Center for Responsive Politics 2017) and BallotPedia.org\n(BallotPedia 2017). Again, we failed to see any significant\nchanges in pages visits before and after exposure to an IRA ad.\nIn addition, we also explored the notion that the IRA was at-\ntempting to draw more people into protests. For example, Robert\nS. Mueller\u2019s indictment points to anecdotes of Americans being\ndriven to attend real-world protests as a result of misinformation\n(Robert S. Mueller 2018). Using Crowd Counting Consortium\u2019s\nnumbers tracking protests in 2017, we examined whether therewas a correlation between where protests occurred in the U.S.\nand the locations of ad-clicks by users (Jeremy Pressman 2018).\nAfter normalizing for state-level populations and state-level\nbrowsing behavior, we found no significant correlation.\nVII. Related Work\nSocial media and politics. The dataset generated by this paper\nwas further used in followup work by Boyd, Spangher et al.\n(Boyd et al .2018). This work examined language differences in\nthe IRA tweets and those posted by a general U.S. population\nduring the same time. Additionally, Boyd et al., report that\nthe majority of Facebook ads were posted during 9am and\n6pm, Moscow Standard Time. As such, (Boyd et al .2018)\nprovides further analysis for the point-of-origin of IRA tweets\nand advertisements.\nThe impact of social media on the political ambit has attracted\nsignificant attention more broadly in both political science and\nsocial media analysis studies. Social networks have facilitated\nfaster and targeted user communication, which has created new\nand amplified effects due to high information diffusion (Bakshy\net al.2012). From a political perspective, such effects can\noften be positive and increase political participation awareness\n(Gil de Z \u00b4u\u02dcniga, Jung, and V alenzuela 2012). However, as any\npowerful means of communication, social platforms can also be\nexploited by external entities to produce negative effects in the\nsociety such as drastic political division (Sunstein 2018) as well\nas manipulation and misinformation (Marwick and Lewis 2017;\nFourney et al .2017). This paper specifically focuses on\nextracting data-analytic insights that can reveal information\nregarding the extreme and potentially divisive characteristics of\nthe IRA campaign. Being aware of the nature and the scale of\nthese characteristics is crucial for increasing general awareness\nand developing robust protective mechanisms in the future.\nDivision and polarization. Political division has been\ninvestigated using the notion of \u201cinformation bubbles\u201d\n(Resnick et al .2013) and \u201cecho chambers\u201d (Bessi 2016;\nGarimella et al .2018). Authors study the phenomenon of\npolitical and intellectual isolation that can occur when users\nare only exposed to information that confirms their own beliefs.\nWhile some of these effects can exist due to organic user\nbehavior, recent research has shown that they have been lever-\naged to increase polarization and division (Stewart, Arif, and\nStarbird 2018; Conover et al .2011; Garimella and Weber 2017;\nBessi 2016). In this context, various tools have been proposed to\nfoster diverse information consumption either via visualization\n(Gillani et al .2018) or exploration strategies that go beyond\nthe personal network bubble (Resnick et al. 2013).\nManipulation and misinformation. Prior research\u2014including\nour own\u2014has found social media can be used to to spread\nand amplify propaganda at a global scale (Menczer 2016;\nFourney et al .2017; Silverman 2016). Combined with the fact\nthat humans are inherently bad at detecting deception (Mihal-\ncea, P \u00b4erez-Rosas, and Burzo 2013; Abouelenien et al .2014;\nP\u00b4erez-Rosas et al .2015), decades of algorithmic advances in\ntargeted advertisement have laid the groundwork for large-scale,\ngeneral-purpose mass manipulation.\nManipulation and misinformation strategies transform in-\nformation in a form that best propagates ideas or agendas of\nspecific groups of interest. A comprehensive summary of these\ntechniques (Marwick and Lewis 2017) shows several examples\nwhere social media has shown to be vulnerable to such attacks\nespecially from extremist political groups and Internet trolling\nentities. In the most harmful form, manipulation can be encoun-\ntered as jointly combined with misinformation, where informa-\ntion is intentionally twisted or even fabricated. Social media has\nbeen a lucrative target of misinformation (Allcott and Gentzkow\n2017; Fourney et al .2017; Faris et al .2017) in the recent years.\nSocial bots on Twitter have played a role in influencing the\nspread of information in a network (V arol et al .2017a; 2017b;\nBessi and Ferrara 2016). V arious approaches have been pro-\nposed to isolate and limit the harmful effects of misinformation\nby harnessing linguistic features, semantic analysis of the con-\ntent, and network topology properties (Shu et al .2017; Budak,\nAgrawal, and El Abbadi 2011; Conroy, Rubin, and Chen 2015).\nThe 2016 US election. The events of the IRA ad campaigns\nin the US presidential election in 2016 are not unique. Similar\ndevelopments have been noticed in the last five years in the\ncase of the Brexit referendum in the UK in 2016 (Howard and\nKollanyi 2016), German Federal election in 2017 (Morstatter\net al.2018), and elections in Pakistan in 2013 (Y ounus et al .\n2014). This work was motivated by the need to better under-\nstand the IRA campaign in 2016 and was enabled by the release\nof the recent datasets from Facebook and Twitter on this matter\nduring the respective testimonies at the United States House\nof Representatives (Schiff 2018a; 2018b). Guided by the same\nmotivation, there is recent and ongoing work tackling important\nquestions related to the structure of the retweet network (Stewart,\nArif, and Starbird 2018) and the characteristics of IRA-promoted\nTwitter handles (Darren L. Linvill 2018). Multiple journalistic ef-\nforts (Shane 2017; Mak 2018) published illustrative examples of\nthe promoted content and informed the broader audience. This\npaper aims at providing a data-oriented analysis, joining multi-\nple sources of information funneling from the promoted ads and\ncontent, to post-election web traffic, to observable user effects.\nVIII. Discussion and Conclusion\nWhile our studies present a set of insights on the strategy,\nstructure, and scope of IRA-related web activities, our analyses\nhave limitations. We highlight these as caveats and as guides\nfor future work in pursuit of confirmation of the results and\ninferences shared in this paper. First, much of our analysis relies\non browsing instrumentation data on desktop computers and ex-\ncludes mobile devices. The released Facebook ads data set and\nthe web search data set we use, on the other hand, do include\nmobile traffic. Secondly, because the browsing instrumentation\ncollects only URLs, our browser logs cannot directly observe\ninteractions that occur within a page (e.g., enabled by AJAX or\nJavaScript). We inspected many of the web properties and, while\nwe found that most activities of interest result in changes to the\nURL, some (e.g., donations brokered by Facebook) could not\nbe detected. Also, as with all studies of this nature, we cannot\nobserve actions taken in the physical world. For example, we\ncannot be sure if a person attended a protest that they read about.\nLikewise, we restrict our analysis to short-term trends because\nour browsing instrumentation data set does not maintain long-\nterm histories. Our data is also collected in 2017, after the 2016\nelection, which was one of the targets of the IRA campaign.\nNevertheless, we have demonstrated multiple ways by which\nmalicious agents can manipulate the digital landscape showcas-\ning the large attack surface susceptible to malevolent interven-\ntions. We highlight how Facebook\u2019s fine-grained interest-based\ntargeting keywords can yield emergent demographic-based\ninformation flow. We show how search platforms, and theiragnostic content-promotion strategies can also be leveraged \u2013\nespecially when agents are able to get their late-breaking content\nindexed before more reputable sources. We hope this work will\nfurther stimulate the development of new approaches to mon-\nitoring, understanding, and uncovering propaganda campaigns\nas well as technology and policy-based solutions that help\navoid political manipulation. Our findings suggest that cross-\norganization collaboration will be valuable in this endeavor.\nCircumstances around the 2016 U.S. presidential elections\nand rising concerns about the influence of propaganda\ncampaigns led to the public availability of valuable datasets\nfor understanding IRA activities. Weaving together the public\ndatasets with proprietary data on search and browsing activity\nprovides a previously unavailable lens on the workings of the\nIRA campaign. We consider this work a preface to numerous\nopportunities ahead and to the many directions that remain to be\nexplored. We see a dual moving forward, with the Web jointly\nholding great promise for strengthening liberal democracies\nwhile also serving as a platform that can be harnessed by\nthose who seek to manipulate and disrupt. As the digital world\ncontinues evolve, and risks to democracy continue to emerge,\nopenness and cooperation among major stakeholders will be\nessential to understand and counter malevolent threats.\nReferences\nAbouelenien, M.; P \u00b4erez-Rosas, V .; Mihalcea, R.; and Burzo, M. 2014.\nDeception detection using a multimodal approach. In Proceedings of\nthe 16th International Conference on Multimodal Interaction , 58\u201365.\nACM.\nAllcott, H., and Gentzkow, M. 2017. Social Media and Fake News\nin the 2016 Election. Journal of Economic Perspectives .\nBakshy, E.; Rosenn, I.; Marlow, C.; and Adamic, L. 2012. The Role\nof Social Networks in Information Diffusion. In WWW .\nBallotPedia. 2017. \u201dBallotPedia: The Encyclopedia of American\nPolitics\u201d. https://ballotpedia.org/ .\nBessi, A., and Ferrara, E. 2016. Social bots distort the 2016 U.S.\nPresidential election online discussion. First Monday 21(11).\nBessi, A. 2016. Personality Traits and Echo Chambers on Facebook.\nComputers in Human Behavior .\nBloomberg. 2018. Company Overview of BuzzFeed, Inc.\nhttps://www.bloomberg.com/research/stocks/pr\nivate/snapshot.asp?privcapId=46607363 .\nBoyd, R. L.; Spangher, A.; Fourney, A.; Nushi, B.; Ranade, G.;\nPennebaker, J. W.; and Horvitz, E. 2018. Characterizing the\nInternet Research Agency\u2019s Social Media Operations During\nthe 2016 U.S. Presidential Election using Linguistic Analyses.\nhttps://psyarxiv.com/ajh2q .\nBudak, C.; Agrawal, D.; and El Abbadi, A. 2011. Limiting the Spread\nof Misinformation in Social Networks. In WWW .\nCenter for Responsive Politics. 2017. Opensecrets.org.\nhttps://www.opensecrets.org/ .\nConover, M.; Ratkiewicz, J.; Francisco, M. R.; Gon c \u00b8alves, B.; Menczer,\nF.; and Flammini, A. 2011. Political Polarization on Twitter. ICWSM .\nConroy, N. J.; Rubin, V . L.; and Chen, Y . 2015. Automatic deception\ndetection: Methods for finding fake news. In ASISST Annual Meeting .\nAmerican Society for Information Science.\nDarren L. Linvill, P . L. W. 2018. \u201dThe Troll Factories: The\nInternet Research Agency and State-Sponsored Agenda Building\u201d.\nhttp://pwarren.people.clemson.edu/Linvill_War\nren_TrollFactory.pdf .\nDavid Leip. 2016. \u201d2016 Presidential General Election Data - Popular\nV ote by State.\u201d. https://uselectionatlas.org/ .\nFaris, R.; Roberts, H.; Etling, B.; Bourassa, N.; Zuckerman, E.; and\nBenkler, Y . 2017. Partisanship, Propaganda, and Disinformation:\nOnline Media and the 2016 U.S. Presidential Election. (ID 3019414).\nFourney, A.; Racz, M. Z.; Ranade, G.; Mobius, M.; and Horvitz, E.\n2017. Geographic and Temporal Trends in Fake News Consumption\nDuring the 2016 US Presidential Election. In CIKM . ACM.\nGarimella, V . R. K., and Weber, I. 2017. A Long-Term Analysis of\nPolarization on Twitter. In ICWSM .\nGarimella, K.; Morales, G. D. F.; Gionis, A.; and Mathioudakis, M.\n2018. Political discourse on social media: Echo chambers, gatekeepers,\nand the price of bipartisanship. In WWW .\nGDELT. 2018. Global events database. https:\n//www.gdeltproject.org .\nGil de Z \u00b4u\u02dcniga, H.; Jung, N.; and V alenzuela, S. 2012. Social media use\nfor news and individuals\u2019 social capital, civic engagement and political\nparticipation. Journal of Computer-Mediated Communication .\nGillani, N.; Y uan, A.; Saveski, M.; V osoughi, S.; and Roy, D. 2018. Me,\nmy echo chamber, and I: introspection on social media polarization.\nInWWW .\nHoward, P . N., and Kollanyi, B. 2016. Bots,# StrongerIn, and#\nBrexit: Computational Propaganda during the UK-EU Referendum.\nhttps://papers.ssrn.com/abstract=2798311 .\nHughs, D. 2012. A Tale of Two Sites: Twitter vs. Facebook and the\nPersonality Predictors of Social Media Usage.\nInel, O.; Khamkham, K.; Cristea, T.; Dumitrache, A.; Rutjes, A.;\nvan der Ploeg, J.; Romaszko, L.; Aroyo, L.; and Sips, R.-J. 2014.\nCrowdtruth: Machine-Human Computation Framework for Harnessing\nDisagreement in Gathering Annotated Data. In ISWC , 486\u2013504.\nSpringer.\nIntelligence Community Assessment. 2017. Background to \u201cAssessing\nRussian Activities and Intentions in Recent US Elections\u201d: The\nAnalytic Process and Cyber Incident Attribution. Technical Report\nICA 2017-01D, Office of the Director of National Intelligence.\nIrwin, N., and Mui, Y . Q. 2013. Washington Post Sale: Details of\nBezos Deal. The Washington Post .\nJeremy Pressman, E. C. 2018. https://sites.google.com\n/view/crowdcountingconsortium .\nMak, T. 2018. Russian Influence Campaign Sought To Exploit\nAmericans\u2019 Trust In Local News. National Public Radio .\nMarwick, A., and Lewis, R. 2017. Media Manipulation and\nDisinformation Online. New York: Data & Society Research Institute .\nMenczer, F. 2016. The Spread of Misinformation in Social Media.\nInWWW .\nMeyer, R. 2016. How Many Stories Do Newspapers Publish Per Day.\nThe Atlantic .\nMihalcea, R.; P \u00b4erez-Rosas, V .; and Burzo, M. 2013. Automatic\ndetection of deceit in verbal communication. In Proceedings of the\n15th ACM on International conference on multimodal interaction ,\n131\u2013134. ACM.\nMorstatter, F.; Shao, Y .; Galstyan, A.; and Karunasekera, S. 2018.\nFrom alt-right to alt-rechts: Twitter analysis of the 2017 german federal\nelection. In WWW .\nOffer, O. 2018. https://www.similarweb.com/ .\nPompeo, J. 2014. Taking Stock of Newsroom Head Counts. Politico .\nP\u00b4erez-Rosas, V .; Abouelenien, M.; Mihalcea, R.; and Burzo, M. 2015.\nDeception detection using real-life trial data. In Proceedings of the\n2015 ACM on International Conference on Multimodal Interaction ,\n59\u201366. ACM.\nResnick, P .; Garrett, R. K.; Kriplean, T.; Munson, S. A.; and Stroud,\nN. J. 2013. Bursting Y our (Filter) Bubble: Strategies for Promoting\nDiverse Exposure. CSCW .\nRobert S. Mueller, I. 2018. Internet research agency indictment.\nhttps://www.justice.gov/file/1035477/download .Saleh, K. 2017. The Average Website Conversion Rate by Industry.\nhttps://www.invespcro.com/blog/the-average-w\nebsite-conversion-rate-by-industry/ .\nScheines, R. 1997. An Introduction to Causal Inference. In Causality\nin Crisis? University of Notre Dame , 185\u2013200. Press.\nSchiff, A. 2018a. Statement on release of facebook advertisements.\nhttps://democrats-intelligence.house.gov/news\n/documentsingle.aspx?DocumentID=379 .\nSchiff, A. 2018b. Statement on release of twitter ads, accounts and\ndata. https://democrats-intelligence.house.gov/\nnews/documentsingle.aspx?DocumentID=396 .\nS.E.C. 2012. The Washington Post Company Form 10K.\nhttps://www.sec.gov/Archives/edgar/data/\n104889/000010488913000009/d10k.htm .\nS.E.C. 2017. The New Y ork Times Company 2017 Annual Report.\nhttps://s1.q4cdn.com/156149269/files/doc_fina\nncials/annual/2017/Final-2017-Annual-Report.\npdf.\nShane, S., and Mazzetti, M. 2018. Inside a 3-Year Russian Campaign\nto Influence U.S. V oters. The New York Times .\nShane, S. 2017. These Are the Ads Russia Bought on Facebook in\n2016. The New York Times .\nShu, K.; Sliva, A.; Wang, S.; Tang, J.; and Liu, H. 2017. Fake news\ndetection on social media: A data mining perspective. KDD .\nSilverman, C. 2016. This Analysis Shows How Viral Fake Election\nNews Stories Outperformed Real News On Facebook. Buzzfeed .\nSpangher, A. 2015. Building the Next New Y ork Times Recommen-\ndation Engine. The New York Times .\nStewart, L. G.; Arif, A.; and Starbird, K. 2018. Examining trolls\nand polarization with a retweet network. In WSDM Workshop on\nMisinformation and Misbehavior Mining on the Web.\nSunstein, C. R. 2018. # Republic: Divided Democracy in the Age of\nSocial Media . Princeton University Press.\nU.S. Census Bureau. 2010. American community survey. Prepared\nby Social Explorer.\nU.S. Census Bureau. 2016. V oting and registration.\nhttps://www.census.gov/topics/public-secto\nr/voting/data/tables.html.\nV arol, O.; Ferrara, E.; Davis, C. A.; Menczer, F.; and Flammini, A.\n2017a. Online Human-Bot Interactions: Detection, Estimation, and\nCharacterization. arXiv:1703.03107 [cs] . arXiv: 1703.03107.\nV arol, O.; Ferrara, E.; Menczer, F.; and Flammini, A. 2017b. Early\ndetection of promoted campaigns on social media. EPJ Data Science\n6(1):13.\nY ounus, A.; Qureshi, M. A.; Saeed, M.; Touheed, N.; O\u2019Riordan, C.;\nand Pasi, G. 2014. Election Trolling: Analyzing Sentiment in Tweets\nDuring Pakistan Elections 2013. In WWW .\nZandt, D. V . 2018. https://mediabiasfactcheck.com/ .\nAppendices\nAppendix 1: Campaign details\nCampaign Structure and Content\nIn this appendix, we describe the campaign in more detail and\ndiscuss how our observations match with statistics released by\nCongress ((Schiff 2018a), (Schiff 2018b)).\nIn the post accompanying the release of Facebook and Twitter\ndata by the U.S. House of Representatives Permanent Select\nCommittee on Intelligence19, Representative Adam Schiff notes:\n\u201cThe Russians ... weav[ed] together fake accounts, pages,\nand communities to push politicized content and videos,\nand to mobilize real Americans to sign online petitions\nand join rallies and protests.\u201d\nWe presented evidence of examples of the IRA promoting\nprotests, meetups, and numerous engagement techniques\nwith their ads, as discussed above. However, we did not find\nevidence of extensive user engagement. On the other hand,\nthe content that we analyze in our paper contains some gaps\nand discrepancies with the content Schiff describes, which we\nexplore and justify here.\nFacebook Rep. Schiff reports identifying the following content\nfor the IRA \u2019s Facebook campaign (Schiff 2018a):\n\u000f3;519Facebook ads.\n\u000f470IRA-created Facebook pages.\n\u000fMore than 80;000pieces of organic content posted to\nFacebook accounts.\nWe observe 3;519released PDF documents, each describing\na Facebook ad, but as noted in Section II, we were only able\nto extract and process usable content from 3;061of these, i.e.\n84% of the posted ads as a result of blank ad-description fields,\nmissing URL fields, blank documents, and OCR errors.\nAs noted in Section III, we only find 104IRA-linked\nFacebook groups being promoted by the ads, in contrast to\n(Schiff 2018a)\u2019s stated 470groups. Formal communication and\nfact-checking with researchers at Facebook have confirmed that\nonly 107of the groups identified were advertised. The missing\n3groups in our data is likely due to OCR errors introduced\nwhen parsing released Facebook-ad PDFs.\nWe seek to verify the volume of content reported by Rep.\nSchiff by deduplicating unique content URLs. We find a total\nof14;860photo and video posts to IRA-linked accounts, shown\nin Figure 10. We were not able to successfully track other post-\ntypes due to the nature of our instrumentation logs, and the\nmanner by which other content appears on Facebook. It is likely\nthat out of the 80;000pieces of content reported by (Schiff\n2018a), the over 65;000pieces that we did not observe included\nboth: organic posts without photos and videos posted to the 104\nIRA-controlled groups included in our study, as well as content\nposted to the 366 IRA-controlled Facebook groups not included\nin our analysis (as detailed above). Discussions with both Face-\nbook representatives and Congressman Adam Schiff\u2019s office for\nthe purposes of fact-checking have confirmed that this interpreta-\ntion is reasonable, and there are ongoing efforts to further clarify.\nTwitter Schiff reports finding the following content for the\nIRA \u2019s Facebook campaign:\n\u000fMore than 36;000Russian-linked bot accounts.\n19https://democrats-intelligence.house.gov/social-media-content/\n0 2000\nTotal photos uploadedsecured.borderswilliamsandkalvinsavemeasapbrownunitedfrontlgbtungodblessthesouthsavethe2ablackmattersus.mvmntstopallinvaderspatriototusblacktivists(a) Total photos posted by account.\nA photo upload is determined by\nthe minimum timestamp of a click\non it\u2019s unique URL key.\n0 200\nTotal videos uploadedsecured.borderswilliamsandkalvinsavemeasapbrownunitedfrontlgbtungodblessthesouthsavethe2ablackmattersus.mvmntstopallinvaderspatriototusblacktivists(b) Total videos posted by account.\nA unique video upload is deter-\nmined the same way a photo upload\nis.\nFigure 10: Facebook photos and video uploads: Metrics\ncapturing the volume of organic (unpaid) posts involving photos\nand videos uploaded to IRA Facebook groups.\n\u000f3;841Twitter accounts affiliated with the IRA.\n\u000fMore than 130;000tweets by accounts linked to the IRA.\nWe had more difficulty verifying Twitter data because of\nambiguity between the definitions of \u201cRussian-linked bot\naccounts\u201d and \u201caccounts affiliated or linked to the IRA \u201d.\nFurther the number 130;000was smaller than what was\nobserved by other researchers (Darren L. Linvill 2018). We\nattempted to fact-check with Representative Schiff\u2019s office and\nrepresentatives from Twitter to resolve these queries but were\nunable to come to a resolution. In this paper, we therefore rely\non data provided by (Darren L. Linvill 2018), which aggregates\nhistorical tweets produced over time by the aforementioned\nIRA affiliated or linked accounts.\nAfter excluding non-English tweets and restricting to the\ncategories as described in the methodology section, we were\nleft with 1;746;000tweets from 917accounts believed to\nbe IRA-affiliated. Further restricting to our time-period of\nobservation, we are left with 471;000from 360accounts.\nCampaign Size and Discussion\nIn this discussion section, we seek to give a comparative\nanalysis of the size of the IRA \u2019s operation in terms of staff,\nbudget and content output. In his indictment of the IRA (Robert\nS. Mueller 2018), Special Counsel Robert Mueller states:\nThe ORGANIZA TION [the IRA] employed hundreds of\nindividuals for its online operations, ranging from creators\nof fictitious personas to technical and administrative\nsupport. The ORGANIZA TION\u2019s annual budget totaled\nthe equivalent of millions of U.S. dollars.\nAs a rough comparison, the New York Times newsroom,\naccording to public information, employs about 1,300\njournalists, writers, copy-editors, social media producers and\nothers (Pompeo 2014). According to the Times\u2019 10-K report,\nits operating cost in 2017 was 1.48 billion USD (S.E.C. 2017).\nTheTimes uploads between 200-300 articles, blogs, and\ninteractives a day (Spangher 2015). From public browsing, it\nappears the Times tweets from 5-10 accounts, about 50-100\ntimes a day. They post on Facebook from 5-10 accounts,\nposting 50-100 times a day as well.\nTheWashington Post likewise has a newsroom of more than\n700 staff. According to public statements, they produce an\naverage of 500 pieces of content a day (videos, photos, articles\nand interactive pieces). The last available public data for the\nPostshows operating costs in the range of 1.8 billion USD, in\n201220(S.E.C. 2012). According to a brief survey of their social\nmedia accounts, they too produce between 50-100 posts on\ntheir Facebook accounts and 50-100 tweets from their Twitter\naccounts a day.\nBuzzfeed , according to public reporting, has a newsroom of\naround 460 staff. They produced roughly 200 articles a day in\n2016 (Meyer 2016). A brief survey on social media reveals that\nBuzzFeed posts to Facebook between 100-200 times a day and\ntweets roughly the same amount.21\nBased on material revealed from congressional testimony,\nthe IRA generated over 2:9million tweets from over 2;800\nactive accounts22. During our observation period, they tweeted\nat least, on average, 2;000times a day. They fielded 3;519\nFacebook ads, roughly 5ads a day during our period of interest.\nAccording to Facebook photo/video URL scraping, discussed\nearlier, they additionally produced at least 14;285photos and\n575videos over the period of interest, or roughly 70photos and\n3videos a day (Figure 12).\nThis does not include content on their owned domains.\nThe domains we have examined show abundant material\nbeing produced. We find blackmattersus.com to be\nparticularly expansive. By performing a site-wide scrape, we\nfind over 3;900articles published, and roughly 5per day during\nour observation period. They logged more than 3new meetup\nlinks a day during our observation period.\nAn apples-to-apples comparison based on raw volume of\noutput is doubtlessly flawed, since we can not compare the\nproduction time each piece of content requires at the Times , the\nPostorBuzzFeed with the time required for content produced\nby the IRA. Much of the IRA \u2019s content is likely derivative or\nsimply copied: many articles on blackmattersus.com\nappear duplicative, and 95% of the tweets recorded from\naccounts of interest are retweets. (Less than 1% of tweets\nfrom major news organizations are retweets). However, even\nwith very conservative estimates, assuming their staff spent\nfractions of the time per piece of content as BuzzFeed , we add to\nMueller\u2019s estimate (Robert S. Mueller 2018) to project that just\ntheir content-focused staff still numbered in the low hundreds.\nTheir budget in the \u201cmillions of USD\u201d (Robert S. Mueller\n2018) means they spent far capital per piece of content than\ntheTimes or the Post, but we emphasize that nevertheless, this\nwas an effort that approached the scope of a large newsroom.\nAn important question is whether the IRA campaign was\nworthwhile. As presented in Table 5 and Figure 11, the cam-\npaign and the agency received a significant volume of coverage\nfrom Western mainstream outlets at hundreds of articles per\noutlet and thousands of articles a day. These facts speak to the\nscale of such campaigns and the role that journalists and scien-\ntists \u2013 present authors included \u2013 should play in these ongoing\nattempts to manipulate the online information landscape.\nAppendix 2: Effect of paid promotions\nAs noted in the paper, the IRA spent more money on left-leaning\ncontent than on right-leaning content on Facebook during our\n20ThePost was bought in 2013 by Jeffrey Preston Bezos, and is\nnow held by a privately owned company, Nash Holdings LLC. (Irwin\nand Mui 2013)\n21BuzzFeed has always been a privately held, and does not disclose\ncosts (Bloomberg 2018).\n22Of the 3841 Twitter handles in the dataset, 1034 handles posted\nno content during the period of observation (Darren L. Linvill 2018).Media Outlet Mentioning IRA Count of articles\nyahoo.com 830\niheart.com 550\nnbcnews.com 306\nmsn.com 256\nwashingtonpost.com 210\nreuters.com 210\nenterprise-security-today.com 168\ndailymail.co.uk 153\nTable 5: IRA mentions in Western media, top outlets. Top\noutlets publishing articles mentioning \u201cRussian Troll\u201d, \u201cIRA \u201d,\n\u201cinternet-research-agency\u201d, \u201cblacktivist\u201d, \u201cblack-matters-us\u201d in\nURLs (calculated using (GDELT 2018)).\nJan Jul Jan Jul\n2017          2018010002000# articles / day\nFigure 11: IRA mentions in Western media, over time.\nNumber of articles in western media mentioning \u201cRussian\nTroll\u201d, \u201cIRA \u201d, \u201cinternet-research-agency\u201d, \u201cblacktivist\u201d,\n\u201cblack-matters-us\u201d in URLs(calculated using (GDELT 2018)).\nFeature type (X) Feature name\nhistorical click volume (7-day window excluding\nthe last 2 days)\nunpaid action #posted photos\n#posted videos\ncontextual #clicks on external similar urls\n#search queries on similar topics\ncontent topics present in the url (binary)\nTable 6: Feature types used in counterfactual modeling.\nobservation period. Furthermore, traffic spikes to IRA Facebook\ngroups were also influenced by events such as photo and video\nuploads in addition to paid promotions. To understand if paid\npromotions mattered more for some groups than for other\ngroups, we asked: Would these groups still have had high traffic\nspikes had they not been advertised on Facebook?\nWe are limited in our ability to answer this question given\nthat we cannot conduct a controlled study of traffic with and\nwithout paid promotions. Hence we decided to build a simple\nmodel to predict traffic to a group in the absence of a paid\npromotion, and use this as a comparison baseline.\nWe trained a model with the goal of predicting how each\nFacebook group would have performed had it not been pro-\nmoted using a paid ad . For this purpose we trained the model\nto predict the probability of a spike (a spike only using ( group\nurl,date ) tuples for which the url was not promoted on the\ngiven date and we omitted the promotion treatment from the\ninput feature set. We then evaluate the predicted performance\non days that articles didreceive promotions and compared our\npredictions.\nWe note that our causal analysis here is restricted. The\nquestion we ask is \u201cWould these groups still have had high\nJan Mar May Jul\n20170255075# Ads / day(a) Number of Facebook ads\nposted over time by IRA accounts.\nJan Mar May Jul\n2017200040006000# tweets / day(b) Number of tweets tweeted\nover time by IRA handles.\nJan Mar May Jul\n20170200400# Photos / day(c) Photos posted on Facebook\nover time to IRA accounts.\nJan Mar May Jul\n20170102030# Videos / day(d) Videos posted on Facebook\nover time to IRA accounts.\nFigure 12: Metrics capturing the volume of unpaid photos and videos uploaded to IRA Facebook groups.\nFacebook Group (a) AUC no prom. (b) AUC prom.\nblacktivists 0.965 0.698\npatriototus 0.876 0.703\nbrownunitedfront 0.961 0.792\nwilliamsandkalvin 0.808 0.771\nsecured.borders 0.792 0.760\nstopallinvaders 0.712 0.724\nTable 7: Area under ROC curve (AUC) of prediction models\ntested on time-window holdout data on days (a) without\npromotions or (b) with promotions, ordered by the difference\nbetween the two columns.\ntraffic spikes had they not been advertised on Facebook?\u201d\nThis is different from the question: \u201cWhat is the impact of a\npromotion?\u201d. In fact, the question we ask is closer to: \u201cWhen\ndid the promotions not matter for traffic spikes?\u201d\nData and Methodology. For this case study, we consider a\nspecific set of Facebook groups that received both large amounts\nof traffic and many promotions during the train and test periods.\nThese constraints yield six groups23. We limit our study to these\ngroups since it is difficult to make any statements on the effect\nof promotions for groups that did not receive many promotions.\nWe formatted our dataset as (group url ;date )pairs.\nWe trained separate `2-constrained Logistic Regression models\nfor each group on all (group url ;date )pairs with nopro-\nmotions to predict the likelihood of a traffic spike to that group\non the given date. Here, a \u201cspike\u201d is defined as click-counts\nlarger than the 50th percentile of all daily click-counts for the\nFacebook group being modeled. We train only on dates without\npromotions, to predict the counterfactual traffic outcome on\ndates withpromotions. Our training data came from 1/1 to 5/1\nand our test data was that collected from 5/2 to 8/1.\nTable 6 summarizes all features used for these models.\nUnpaid action features count the number of photos and videos\nposted by each group on the given date. Content features\nare binary indicators marking topic-relevance based on our\ncrowdsourcing task. Contextual features track traffic to non-IRA\nnews websites properties and volumes of search queries that\nwere topically relevant to any of the topics of any of the groups\nbeing studied. These features capture the possible influence of\nexternal news factors.\nAdditionally, instead of using traffic immediately preceding\neach date to calculate historical features, a buffer of 2 days is\n23These 6 groups together account for 64% of traffic and 36% of\nads ran (15% of ad spend) over the observation period.\nFigure 13: Effect of Promotions. The average log lift of click\npercentile for each group on days with a paid promotion,\ncompared the probability we predict a spike would have\noccurred without promotion. Positive values indicate higher\nimportance of promotions with respect to traffic spikes.\nused. We purposely exclude a feature indicating whether the\nproperty had been promoted during the given day. And we took\nsteps to ensure that no other model features would leak this in-\nformation (e.g. the click volume feature only counts clicks up to\ntwo days before the spike date). We experimented with adding\na feature counting past-promotions, but found little change.\nWe interrogated the independence relationships in our ob-\nserved data to test whether our assumptions hold. We test for\nconditional independence between treatment and all other vari-\nables, given outcome (using corr(xi;xjjy)=0 for all values of\ny2f0;1g, and for all xi,xjpairs). We observe independence\nbetween all treatment and outcome variables ( correlation >\n:1;p<: 1). (We apply significance testing with Bonferroni cor-\nrections to critique our faithfulness assumption.)24\nThese independence observations give us confidence that\nour assumptions are being met.\nThe modeling decisions we made improve the adherence of\nour data to the causal structure we infer.\n1.Treatment assignment: We do not find strong evidence for\na dependence relationship between treatment assignment and\noutcome, mediated through confounding factors. Based on\na Causal Markov assumption, this implies that we can safely\nmodel using these features as independent (Scheines 1997).\n2.Historical treatments: We also did not find significant\nrelationships between historical promotions and current\n24Note: we continued to see correlations between individual features\ninX, but according to the Causal Markov assumption, this does not im-\npact our causal claims regarding the promotion\u2019s effect on the outcome.\npromotions, strengthening our belief that independence\nacross different dates and urls is maintained.\nAdditionally, we took steps to limit overfitting. As mentioned\nabove, we model each Facebook Group separately. We tuned\neach model\u2019s `2hyperparameter using holdout validation data\nduring training. We also iterated to include external news\nand traffic features. We choose to measure the Area Under\nthe Receiver Operating Characteristic Curve (AUC) as a\nperformance measure in order to handle class imbalance and\nalso avoid picking a probability decision threshold for when\na prediction should be leaning towards a spike.\nFindings. The most important features across our models, in\nterms of average coefficient values across all models, were\nhistorical clicks ,# posted photos ,# posted videos . This suggests\na role for unpaid actions in the IRA strategy.\nTable 7 shows the AUCs for each group calculated for days\nwith more than one promotion running ( promo. column) or\nwith no running promotion ( no prom. column) in the test period.\nAgain, since the models were trained only on no prom. days in\nthe training period, the no prom. column is a validation test to\nempirically check how well the model generalizes to previously\nunseen data with no promotions, while the prom. column tests\nour counterfactual extrapolation.\nFor groups with a prom. AUC closer to a no-prom. AUC\n(e.g. \u201cStopAllInvaders\u201d), promotional signal is not necessary\nfor the model to predict a spike: i.e., a variable capturing\npromotion-information would not have added signal and other\nfactors, encoded in the model features, had a higher impact. For\ncases where the AUC score is divergent (e.g. \u201cBlacktivists\u201d),\nmodel-features were not sufficient, meaning that other factors\nunknown to this model, including promotions, played a\nsignificant role. Again, the reason why we look at the difference\nbetween these two columns and not only at the absolute value\nof each, is to decouple the ability of the model to generalize\nto unseen no promotion data from the ability of the model to\nextrapolate traffic spike predictions on dates with promotions.\nFigure 8 shows another view of these results. In Figure 8,\nwe consider only the days withpromotions, and compare the\nobserved traffic patterns (showing prom. traffic) with our model\npredictions (modeling no prom. , or the counterfactual). We\ncalculate the logarithmic ratio between the observed traffic (in\npercentile) and the predicted probability of a spike on that date,\nas assigned by the model.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Analysis of Strategy and Spread of Russia-sponsored Content in the US in 2017", "author": ["A Spangher", "G Ranade", "B Nushi", "A Fourney"], "pub_year": "2018", "venue": "arXiv preprint arXiv \u2026", "abstract": "The Russia-based Internet Research Agency (IRA) carried out a broad information campaign  in the US before and after the 2016 presidential election. The organization created an"}, "filled": false, "gsrank": 365, "pub_url": "https://arxiv.org/abs/1810.10033", "author_id": ["pZEaPR8AAAAJ", "XBjhKdoAAAAJ", "QWTkjB8AAAAJ", "NBoXr9oAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:lo40nNCm5qEJ:scholar.google.com/&output=cite&scirp=364&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D360%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=lo40nNCm5qEJ&ei=RrWsaO_lEeHUieoP9LKZ6AI&json=", "num_citations": 37, "citedby_url": "/scholar?cites=11666195299653947030&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:lo40nNCm5qEJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/1810.10033"}}, {"title": "SciLander: Mapping the scientific news landscape", "year": "2023", "pdf_data": "SciLander: Mapping the Scientific News Landscape\nMaur\u00edcio Gruppi1, Panayiotis Smeros2, Sibel Adal\u01311, Carlos Castillo3, Karl Aberer2\n1Rensselaer Polytechnic Institute, USA\n2\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Switzerland\n3Universitat Pompeu Fabra, Spain\ngouvem@rpi.edu, panayiotis.smeros@epfl.ch, adalis@cs.rpi.edu, chato@acm.org, karl.aberer@epfl.ch\nAbstract\nThe COVID-19 pandemic has fueled the spread of misinfor-\nmation on social media and the Web as a whole. The phe-\nnomenon dubbed \u2018infodemic\u2019 has taken the challenges of in-\nformation veracity and trust to new heights by massively in-\ntroducing seemingly scientific and technical elements into\nmisleading content. Despite the existing body of work on\nmodeling and predicting misinformation, the coverage of\nvery complex scientific topics with inherent uncertainty and\nan evolving set of findings, such as COVID-19, provides\nmany new challenges that are not easily solved by exist-\ning tools. To address these issues, we introduce SciLander,\na method for learning representations of news sources report-\ning on science-based topics. SciLander extracts four hetero-\ngeneous indicators for the news sources; two generic indi-\ncators that capture (1) the copying of news stories between\nsources, and (2) the use of the same terms to mean different\nthings (i.e., the semantic shift of terms), and two scientific in-\ndicators that capture (1) the usage of jargon and (2) the stance\ntowards specific citations. We use these indicators as signals\nof source agreement, sampling pairs of positive (similar) and\nnegative (dissimilar) samples, and combine them in a unified\nframework to train unsupervised news source embeddings\nwith a triplet margin loss objective. We evaluate our method\non a novel COVID-19 dataset containing nearly 1M news ar-\nticles from 500 sources spanning a period of 18 months since\nthe beginning of the pandemic in 2020. Our results show that\nthe features learned by our model outperform state-of-the-art\nbaseline methods on the task of news veracity classification.\nFurthermore, a clustering analysis suggests that the learned\nrepresentations encode information about the reliability, po-\nlitical leaning, and partisanship bias of these sources.\nIntroduction\nThe COVID-19 pandemic has resulted in a significant in-\ncrease in information production and consumption at the\nsame time. With this came a large increase in unreliable in-\nformation, dubbed \u2018infodemic\u2019 (Buchanan 2020). This in-\ncrease was also coupled with the growing scrutiny of me-\ndia sources and purposeful amplification of any errors they\nmade. As the readers sought correct, timely, and trustworthy\ninformation, many news and media sources worked hard to\ndiscredit others and create confusion (Van Bavel et al. 2020).\nCopyright \u00a9 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Overview of SciLander, including agreement indi-\ncator extraction (\u00a7 & \u00a7), triplet sampling and unsupervised\nsource embeddings training (\u00a7), and evaluation on the down-\nstream tasks of classification and clustering (\u00a7).\nGovernments and public health agencies have the respon-\nsibility to respond to the crisis and protect the public from\nmisinformation by utilizing the power of social and news\nmedia (Castillo 2016). Yet, the same social and news media\nwork as a catalyst for the infodemic, allowing disinforma-\ntion to be dispersed on a large scale, regardless of the signif-\nicant effort to hinder its spread (McKay and Tenove 2021).\nDespite the existing body of work on modeling and pre-\ndicting misinformation, coverage of a complex scientific\ntopic with inherent uncertainty and evolving set of findings,\nsuch as COVID-19, provides many new challenges that are\nnot easily solved by existing tools (Zarocostas 2020). On\nthe article level, the evaluation of news stories may be chal-\nlenging as they may contain information that cannot be eas-\nily verified. Furthermore, many sources may not have the\nnecessary staffing for the proper communication of science-\nrelated topics, they may be known to have published incor-\nrect information, this information may also have changed\nover time, or the source may have later corrected it.\nOften, language-based methods fail in such a task because\ndifferent sources may use the same terms to mean different\nthings. Furthermore, many sources may use scientific ref-\nerences to back up their claims; however, the validity of\nthese references is not easily verifiable. Being able to map\nout the consequential and systematic patterns of behavior of\nsuch sources in terms of both content andreferences would\nProceedings of the Seventeenth International AAAI Conference on Web and Social Media (ICWSM 2023)\n269\nbe particularly useful in such scenarios (Chung, Nam, and\nStefanone 2012). It would allow sources to be compared to\nother known sources in terms of their coverage, and develop\nexplanations to the aspects in which they are similar to or\ndifferent from each other.\nTo address these challenges, we introduce a novel method\ncalled SciLander. SciLander builds on a set of novel fea-\ntures, based on the deep processing of news articles pub-\nlished by a set of sources, producing a vector representation\nof these news sources. To build this, we incorporate mea-\nsures of similarity and difference between the sources based\non their citation behavior, the republishing of articles from\neach other, and their general language usage. In particular,\nwe use the coverage of COVID-19 to show that this em-\nbedding has many desirable features that can help multiple\ndownstream tasks.\nOur Contribution. The technical contributions we intro-\nduce are the following:\n\u2022 We propose four news agreement indicators for sources:\ni) the shared content or republished articles, ii) the seman-\ntic shift of terms in the common vocabulary, iii) the usage\nof scientific jargon, and iv) the citation stance of the news\nsources (\u00a7 & \u00a7);\n\u2022 We combine these indicators in a unified framework for\ntraining unsupervised news source embeddings (\u00a7);\n\u2022 We evaluate our method using a dataset of news publi-\ncations related to COVID-19. Sources in this dataset are\nlabeled with respect to reliability and political leaning;\n\u2022 We compare our method to strong baselines on the prob-\nlem of veracity classification of news sources and show a\nsignificant gain in performance when combining the indi-\ncators proposed in this paper;\n\u2022 We test the applicability of our method in an online learn-\ning experiment, showing that it can be used to learn fea-\ntures from sources even if little data is available or if new\ncoming sources are presented in the landscape;\n\u2022 We show that the learned features encode information\nabout the sources\u2019 reliability level, partisanship bias, and\npolitical leaning through a clustering analysis experiment.\nRelated Work\nWe distinguish three levels of granularity for misinformation\nin news and social media: claims, articles, and sources. This\nis a broad research area where results are scattered through\nmultiple disciplines and venues; below we present studies\nrelevant to each of the three aforementioned levels.\nClaims & Articles Veracity. Many of the computational\nmethods for veracity assessment of news articles employ\nmachine learning techniques in supervised binary or multi-\nlabel classification settings (Baly et al. 2019; Reis et al.\n2019; Zhang et al. 2018; Yang et al. 2019; Vishwakarma,\nVarshney, and Yadav 2019). In this setting, a news article is\ngiven as the input to a model and it must predict whether the\narticle contains false information. Other studies aim at de-\ntecting the veracity of information at a more granular level\nby working with claims and rumors (Zubiaga et al. 2018;\nShaar et al. 2020; Hansen et al. 2019; Jiang et al. 2020;Smeros, Castillo, and Aberer 2021). This approach consists\nin detecting fragments of text, e.g., sentences or paragraphs,\nworthy of fact-checking. Thus, a single document or news\narticle may contain several claims, some of which may be\ninaccurate or deceiving.\nSource Veracity. Source-based approaches are holistic ap-\nproaches that evaluate the quality of a news source as a\nwhole, without focusing on individual claims or articles ex-\ntracted from it. Baly et al. (2018, 2019); Li and Goldwasser\n(2019) highlight the importance of features beyond text to\nevaluate the veracity of news sources, such as the presence\nin social media and the existence of a Wikipedia page about\na source. Furthermore, Shu, Wang, and Liu (2019) explore\nthe interactions between users, authors, and sources, while\nGruppi, Horne, and Adal\u0131 (2021) observe content sharing\ntrends among news publishers. Finally, Bourgeois, Rappaz,\nand Aberer (2018); Rappaz, Bourgeois, and Aberer (2019)\nstudy the selection bias in the topic coverage of news sources\nby exploring the co-references of these sources to the same\nnews events, while Ribeiro et al. (2018) infer the biases of\nnews sources by utilizing their advertiser insights into the\ndemographics of their social media audience.\nBoth claim- and article-level veracity assessments require\ndata labeling at a very large scale (e.g., individual claims or\narticles labeled as reliable orunreliable) and heavily rely on\ntext-specific features these short pieces of text provide. Our\napproach is, to the best of our knowledge, the first approach\nthat aggregates information about the writing style and cita-\ntion behavior of news sources to learn unsupervised source\nrepresentations, that is aware of the science-related content\npublished by them.\nCorpus\nOur study targets the reliability of sources when reporting\nnews related to science. Thus, we use a corpus of news ar-\nticles targeted on the emerging scientific topic of COVID-\n19, and a corpus of scientific references, also targeted on\nCOVID-19. We summarize the basic statistics of both cor-\npora in Table 1.\nNELA-GT-2020. The collection of news articles contains\na total of 1.78 million articles published by 519 sources\n(Gruppi, Horne, and Adali 2021). Each article in the dataset\ncontains a title, full text, name of the publishing source, and\npublication timestamp. We use a subset containing only arti-\ncles related to COVID-19, resulting in 991,116 news articles\nfrom 493sources, published over 18 months, between Jan-\nuary 1st 2020 and July 1st 2021. We obtain this subset by\napplying keyword-based filtering using the COVID-19 ter-\nminology from Shugars et al. (Shugars et al. 2021), selecting\narticles that contain at least one COVID-related keyword in\nthe title or body text.\nMedia Bias/Fact Check Labels. We retrieve labels for\nsources in the corpus from the news assessment agency Me-\ndia Bias/Fact Check1. We obtain the political leaning of\nnews sources, represented by direction (left or right) and\nmagnitude (mild, moderate, extreme). These are encoded\n1https://mediabiasfactcheck.com\n270\nNELA-GT-2020\nTotal Articles ~1.8M\nCOVID-19 Articles ~1M\nTotal Sources 493\nLabeled Sources 316\nReliable Sources 122\nUnreliable Sources 194\nPartisan Sources 162\nScientific References\nCOVID-19 Papers (CORD-19) ~300K\nScientific Domains (SciLens) ~1K\nReferences in NELA-GT-2020 ~200K\nTable 1: Summary of the used corpora. We see that more\nthan half of the articles in NELA-GT-2020 are related to the\ntopic of COVID-19. The labels for reliable, unreliable and\npartisan sources are obtained from Media Bias/Fact Check.\nas integer numbers in [\u22123,3], negative values indicate left-\nbias, positive values indicate right bias, and 0represent cen-\nter sources. Furthermore, we obtain a conspiracy-theory la-\nbel, a binary indicator denoting whether a source publishes\nconspiracy theories and/or pseudoscience content. These are\noften highly unreliable sources and may or may not exhibit\npolitical leaning. Finally, we obtain factual reporting, an in-\nteger score from 0 to 5 assigned to each source, where 0\nindicates the least credible score and 5 is the most credible\nscore. A source that constantly publishes misleading con-\ntent, fails to fact-check its publications, and does not dis-\nclose an editorial board tends to be associated with a lower\nfactual reporting score.\nBased on the factual reporting score, we divide news\nsources into two reliability classes, namely the Reliable\nNews Sources and the Uneliable News Sources. The rules\ndefining each class are described as follows:\n\u2022Reliable News Sources: sources whose factual reporting\nscore is greater than 2.\n\u2022Unreliable News Sources: sources flagged as conspiracy-\ntheory news producers or sources whose factual report-\ningscore is less than or equal to 2.\nScientific References. We enhance the news collection de-\nscribed above, by extracting the external scientific refer-\nences of news articles, i.e., the outgoing hyperlinks from\nthe main body of the news articles. We also extract the con-\ntext of each reference, i.e., the passage of the news article\nthat surrounds this reference. We consider two repositories\nof references provided by CORD-19 andSciLens.\nOne of the most prominent collection of papers related\ntoCOVID-19, consisting of peer-reviewed papers as well\nas preprints and other historical coronavirus research, is\nCORD-19 (Wang et al. 2020). We use the 2021-06-14 re-\nlease of CORD-19, which contains a total of 310,833 papers.\nThe second source of scientific references comes from\nSciLens (Smeros, Castillo, and Aberer 2019). SciLens pro-\nvides a list of the top-1000 university domains (as indi-\ncated by CWUR.org), enhanced with a manually curated list\nof open-access publishers and grey literature databases. In-\ndeed, these scientific references are more prevalent in news\nWashington PostWashington PostCharlotte ObserverCharlotte ObserverUS NewsUS NewsCBS NewsCBS NewsRaw StoryRaw Story\nVeterans TodayVeterans TodayGlobal ResearchGlobal Research\nThe Greanville PostThe Greanville PostRussophileRussophile\nBreitbartBreitbartThe Epoch TimesThe Epoch TimesWhat Really HappenedWhat Really HappenedFigure 2: Example of a subgraph of the Content Sharing Net-\nwork where nodes, representing sources, are connected by\ndirected edges denoting the direction of the copied content\nbetween sources. Node color indicates the reliability class of\nthe source (green for Reliable, purple for Unreliable), and\nedge width indicates the amount of content copied.\nthan the CORD-19 papers, because their writing style and\nterminology used is typically more oriented towards a non-\nexpert audience.\nContent Indicators\nIn this section, we introduce two content-based indicators\nthat we use to align news sources. Particularly, we introduce\nan indicator regarding the shared content and an indicator\nregarding the semantic shift of terms between sources.\nCopy Indicator\nContent Sharing Network (CSN) is a model of content repli-\ncation by sources in the news landscape. The sharing of\nnews articles has been shown to be a common factor be-\ntween news sources that adopt similar narratives around cer-\ntain topics, which also correlates with the credibility of these\nsources (Horne, N\u00f8rregaard, and Adal\u0131 2019). Figure 2 illus-\ntrates how sources are related in a CSN, where articles are\ncopied from source to source.\nThe CSN is modeled as a directed graph where nodes rep-\nresent news sources and edges indicate sources that copy\narticles verbatim from one another. Edges weights are pro-\nportional to the amount of content copied between the con-\nnected sources. The adjacency matrix Cof such network\nrepresents the affinity between the news sources. We obtain\nthis matrix using the method proposed by Horne et al. (2019)\nwhich consists of computing document vector representa-\ntions for news articles using a TF-IDF bag-of-words repre-\nsentation. Articles are considered verbatim copies of each\nother if the cosine similarity between their vectors is greater\nthan a threshold of 0.85, and the direction of the copying is\ndetermined by the publication date of the article. The sim-\nilarity threshold is defined following the recommendations\nfrom Horne, N\u00f8rregaard, and Adal\u0131 (2019).\nThe final adjacency matrix is obtained by aggregating all\ncopied articles at the source level. Thus, a directed edge from\nnode itojexists if source jcopies articles from source i.\nThe complement of the degree of relatedness distance be-\ntween sources iandj, is given as a function of the weight of\nthe edge (i, j)and is defined as:\ndcpy(i, j) = 1\u2212|Ai\u2229Aj|\n|Aj|\n271\nSource Usage\nModern Alternative\nMama{...} these specific herbs have strong\nantiviral actions, including against\nother strains of coronavirus.\nHealthy Holistic\nLiving{...} Garlic is known to have potent an-\ntibacterial, antiviral, antifungal and an-\ntiprotozoal abilities.\nThe Guardian {...} overwhelming emergency depart-\nments and causing governments to over-\nspend on antiviral medications.\nThe Washington\nPost{...} although the antiviral drug remde-\nsivir has been shown to help some pa-\ntients {...}\nTable 2: Semantic shift of the term \u201cantiviral\u201d. We observe\na contextual shift of the word. In the top two cases, the term\nis used to describe alternative medicine with herbs, while\nin the bottom two cases, the term is used with its ordinary\n(scientific) connotation.\nwhere AiandAjare articles published by sources iandj;\nthus, their intersection should contain articles from source\nicopied by source j. The value of dcpyincreases as fewer\narticles are copied from itojand decreases as more articles\ninjare copied from i.\nShift Indicator\nWe analyze how specific technical terms are used differently\nbetween news sources. Different uses of a certain term in\ntwo pieces of text can occur if that same term is used in\na different context in each of the texts. Semantic shift is the\nprocess through which the usage of a given word drifts when\ncompared across different sources. Specifically, we consider\nthe lexical semantic shift, which posits the semantics of a\nword to be defined by its contextual relationships to other\nlexicons (Cruse et al. 1986). We argue that significant con-\ntextual shifts of topic-related words may serve as a signal\nof source disagreement, i.e., two sources using a certain tar-\nget word in significantly different contexts may indicate that\nthey use such words with different intents. An illustrative\nexample is shown in Table 2. Note, in both examples, the\nword antiviral is still used to indicate \u201csomething that is ef-\nfective against viruses\u201d; however, the contexts give different\nconnotations to what the antiviral product is.\nSemantic shift has been used extensively in computa-\ntional linguistics studies of language evolution (Hamilton,\nLeskovec, and Jurafsky 2016) and, more recently, in stud-\nies quantifying the linguistic differences across domains\n(Yin, Sachidananda, and Prabhakar 2018; Schlechtweg et al.\n2019). In our method, we use semantic shift as an indi-\ncator of agreement among sources as it helps to uncover\nunique narratives created by unreliable sources, especially\nthose based on conspiracy theories, deviating significantly\nfrom the narratives from reliable media.\nThe semantic shift between two sources iandjis mea-\nsured by the deviation in the usage of words they have in\ncommon. Specifically, we define semantic shift as the aggre-gated distance between word embeddings for terms in the\ncommon vocabulary of sources iandj. However, because\nthe word embeddings are trained independently from each\nother, they cannot be directly compared. For example, sup-\npose that vaandvbdenote word vectors for the word virus\nlearned from the sources The Washington Post andGlobal\nResearch, respectively. The cosine distance dcos(va, vb)is\nnot meaningful unless we first create a mapping between\nthe embedding spaces of each source. This mapping can be\nachieved by applying an orthogonal transformation to one\nof the embedding spaces to minimize the sum of the pair-\nwise Euclidean distances between word vectors of the com-\nmon vocabulary. Being orthogonal means that this transfor-\nmation preserves the inner product of the embeddings in\nthe transformed space; for that reason, this mapping is also\ncalled embedding alignment (Hamilton, Leskovec, and Ju-\nrafsky 2016; Joulin et al. 2018).\nFinding the best alignment of two embedding spaces\nis not a trivial task. Learning a transformation from all\nthe words in the common vocabulary is often undesired,\nas the objective of the mapping is to minimize the dis-\ntance between every pair of word vectors, hence minimiz-\ning the distance between words that are potentially semanti-\ncally distinct (Yin, Sachidananda, and Prabhakar 2018). To\nlearn alignments between word embeddings, we employ the\nstate-of-the-art self-supervised semantic shift (S4) method\n(Gruppi, Chen, and Adali 2021), which is designed to select\nthe best words for generating a mapping between two em-\nbeddings. This procedure is applied to embeddings trained\nusing Word2Vec (Mikolov et al. 2013).\nOnce we train and align the embeddings, we compute the\nsemantic distance between sources iandjas the average\ncosine distance between the top 10% most frequent words\niniandj(stop words excluded). Thus, the distance between\nsources iandjis defined as:\ndsem(i, j) =P\nv\u2208Vi\u2229Vjcos(emb i(v), emb j(v))\n|Vi\u2229Vj|\nwhere ViandVjare the vocabularies of sources iandj,\nembi(v)andembj(v)compute the embeddings representa-\ntion of word v, and coscomputes the cosine distance be-\ntween the embeddings. Additionally, ViandVjmay be re-\nplaced with subsets of the common vocabulary to avoid us-\ning every word in the analysis (e.g., filter for the most fre-\nquent words).\nReference Indicators\nIn this section, we introduce the reference indicators that we\nused to align news sources. Particularly, we introduce two\ndedicated scientific indicators, namely, the usage of scien-\ntific jargon and the citation stance. These indicators are ref-\nerence indicators, i.e., they define a distance among sources\ngiven a common (scientific) reference.\nReference Context Extraction\nTo compute the reference indicators, we need the textual\ncontext of the references, i.e., the paragraph in which these\nreferences are cited. To extract this context, we: i) locate the\n272\nSource Reference Context\nTheNewYorker In June, just three months into a historic health cri-\nsis, a survey by the Center for Disease Control and\nPrevention found that forty per cent of Americans\nwere already struggling with at least one mental-\nhealth issue.\nRedState It is no wonder that many Americans have lost\ntheir faith throughout 2020. Too many leaders\nhave been inconsistent in their actions minus their\ncontinued breaches of the public trust.\nReference Title\nCDC Mental Health, Substance Use, and Suicidal\nIdeation During the COVID-19 Pandemic \u2014\nUnited States, June 24\u201330, 2020.\nTable 3: Usage of scientific jargon when citing a report by\nCDC (Czeisler et al. 2020). We highlight that the citation\ncontext of TheNewYorker is semantically closer to the report\nthan the citation context of RedState.\nreferences by parsing the raw HTML page of each news ar-\nticle of our data collection, and ii) traverse the structural tree\nof the page to discover the most fine-grained text passage\nthat contains the reference. Currently, we do not support\nend-notes within articles, i.e., anchors at the bottom of ar-\nticles where all the scientific references are listed, because it\nis a journalistic practice rarely appearing in our corpus.\nJargon Indicator\nThis indicator quantifies the scientific nature of the context\nin which a reference is used. To estimate this indicator, we\nneed a lexicon of terms (jargon _terms in the following)\nthat are considered jargon in the scientific domain of our\ncorpus. Since, as we explain in \u00a7, our corpus contains news\narticles related to COVID-19, we use the vocabulary of CDC\nA-Z Index2, manually enhanced with common COVID-19\nterminology. After applying standard cleaning (e.g., punctu-\nation removal), we compute the following distance:\ndjar(i, j) =|ctxr(i)\u2229ctxr(j)\u2229jargon _terms|\nwhere ctxr(i)andctxr(j)are the terms in the citation con-\ntexts of sources iandjfor each common reference r.\nWe note that we do not aggregate for all common refer-\nences between sources iandj; hence, we do not limit to a\nsingle distance between these sources. In this way, we en-\ncode the co-citation volume between sources iandj, which\nis useful for our triplet sampling strategy (details in ). Af-\nter computing djar(i, j), we apply Min-Max Normalization\nin the interval [0,1]to comply with the previously-defined\ndistances. As we observe in Table 3, even such a simplistic\nmetric is able to capture cases in which news sources com-\npletely distort the scientific message of the cited reference.\nStance Indicator\nThis indicator quantifies the sentiment charge of the con-\ntext in which a reference is cited. To measure this sentiment\n2https://www.cdc.gov/azSource Reference Context\nFiveThirtyEight {...} based on current CDC guidelines {...} ex-\nperts said that undercounting (deaths) was still\nmore likely than overcounting.\nThe Truth\nAbout CancerPerhaps worst, the CDC has continued to lie\nabout the death count by artificially inflating\nit. CDC guidelines for determining COVID-19\ndeaths include: Anyone who tests positive, even\nif they died from other causes. Anyone who had\nCOVID-19 symptoms, even if they aren\u2019t tested.\nReference Title\nCDC Guidance for Certifying Deaths Due to Coron-\navirus Disease 2019 (COVID-19).\nTable 4: Stance of news sources when citing (using the\nunderlined hypertext) a webinar by CDC (Anderson et al.\n2020). We highlight that The Truth About Cancer uses more\nemotionally loaded words than FiveThirtyEight.\ncharge, we use the Multi-Genre Natural Language Inference\nmodel BART for zero-shot classification (Lewis et al. 2020).\nThis model3computes the probability that we infer a cer-\ntainhypothesis given a premise. Thus, the model needs no\nexplicit training on the downstream task of stance classifi-\ncation since the desired classes are provided implicitly in\nthehypothesis. After experimenting with various templates\nforpremise andhypothesis, we report the ones that yield the\nmost reliable results:\npremise =reference context\nhypothesis =\u201cThe stance of this example is negative\u201d\nThe output of this model is a value in the interval [0,1], de-\nnoting the probability a given premise implies our hypoth-\nesis. We note that, by using this premise and hypothesis,\nwe treat neutral andpositive stances similarly, i.e., as non-\nnegative stances, because we want to highlight extremely\nnegative stances (Table 4). Using this model we compute\nthe following distance:\ndref(i, j) =|stance(ctx r(i))\u2212stance(ctx r(j))|\nwhere stance(.) computes the stance of the citation contexts\nof sources iandjfor each common reference r.\nSimilarly as above, after computing dref(i, j), we apply\nMin-Max Normalization in the interval [0,1]. As we observe\nin Table 4, this indicator distinguishes between the sentiment\nof sources towards a common reference.\nUnsupervised Source Embeddings\nThe previous section described the heterogeneous indicators\nthat we extract from each news source. In this section, we de-\nscribe how we combine these indicators in a unified frame-\nwork to learn unsupervised representations of news sources.\nThe triplet loss function aims at coupling different parts of\nthe input spaces (here, our indicators) into a single repre-\nsentation (Weinberger and Saul 2009). The triplets sampling\n3https://huggingface.co/facebook/bart-large-mnli\n273\nand embeddings training methods employed in this frame-\nwork are well-established methods (Hoffer and Ailon 2015)\nused mainly in learning-to-rank recommendation systems\n(Chen et al. 2016; Wang et al. 2021).\nTriplets Sampling\nOur goal is, using the distances defined by the indicators,\nto discover pairs of similar sources and pairs of dissimilar\nsources. By joining these two sets of pairs, we create triplets\nof the form (anchor, positive, negative), where anchor is the\ncommon element of the pairs, positive is the element simi-\nlar to the anchor, and negative is the element dissimilar to\ntheanchor. For simplicity, in the following, we will refer to\nthese triplets as (a, p, n).\nWe note that these triplets may not occur from the same\nindicator, i.e., the positive pair may occur from an indicator\nthat is more appropriate for capturing the affinity between\nsources, and the negative pair may occur from an indica-\ntor that is more appropriate for capturing the disparity be-\ntween sources. In our experimental evaluation (\u00a7), we evalu-\nate each indicator in its ability to produce good positive and\nnegative pairs as well as full triplets.\nPositive Pair Sampling. We use the distances computed\nfor each indicator to generate pairs of similar sources. For\nall indicators we introduce in \u00a7 & \u00a7, short distance denotes\nsimilarity. Given an indicator f(copy, shift, jargon, or\nreference), we generate a positive pair of similar sources\ni, jwith a probability inversely proportional to the distance\nbetween iandj:\nppf(i, j) =d\u22121\nf(i, j)\nP\nkd\u22121\nf(i, k)\u2200j\u0338=i\nWe draw lpositives samples from this distribution for each\nindicator and each source in the dataset, producing a total of\nlpositive source pairs (a, p).\nNegative Pair Sampling. For negative sampling, we em-\nploy two strategies. For some indicators (e.g., the stance in-\ndicator), a large distance between sources denotes opposing\nsentiment, thus disagreement (e.g., the sources in Table 4).\nHence, we use the inverse distribution we used for generat-\ning positive pairs to generate negative pairs:\nnpf(i, j) = 1\u2212ppf(i, j)\u2200j\u0338=i\nSimilarly as above, we draw lnegative samples from this\ndistribution for each indicator and each source in the dataset,\nproducing a total of lnegative source pairs (a, n).\nNonetheless, there are indicators (e.g., the copy indicator)\nfor which a large distance between sources does not neces-\nsarily denote disagreement; it only denotes the absence of\nagreement. In these cases, we draw the negative pairs uni-\nformly from the set of sources.\nFinally, we employ a cleaning heuristic to increase the ac-\ncuracy of our triplets (detailed experiment in \u00a7). Specifically,\nwe make sure that we do not select a negative pair (a, n)\nwhich we have already selected as positive pair (a, p):\n(a, p)\u2227(a, n)\u21d2p\u0338=nEmbeddings Training\nOnce we extract all the triplets, we use them for training a\ndense representation model for news sources with the Triplet\nMargin Loss (Balntas et al. 2016). The learning objective of\nTriplet Margin Loss is to minimize the distance between an\nanchor and a positive sample while maximizing the distance\nbetween the anchor and the negative sample.\nThe procedure we employ is the following. First, we\ninitialize the embeddings for all the sources into a low-\ndimensional, dense vector space by randomly setting the\nweights in the embedding layer following a normal distri-\nbution N(0,1). Then, given the input triplets (a, p, n ), we\ntrain these embeddings by minimizing the loss function L:\nL(a, p, n) = max{d (a, p)\u2212d(a, n) + M,0}\nwhere dis the distance function, and Mis the margin pa-\nrameter that controls the gap between positive and negative\ndistances. The larger Mis, the larger is the gap between\nd(a, p) andd(a, n). We train the embeddings over several\nepochs until convergence and then use them as the represen-\ntation of the news sources.\nThe parameters of this method are the margin M, the dis-\ntance function d, and the size of the output vectors s. We\nrelease the optimal training parameters as well as the trained\nsources embeddings in our code release (\u00a7).\nExperiments\nOur experimental evaluation is three-fold; first, we evaluate\nthe indicators individually, then we evaluate the source em-\nbeddings on the downstream task of source reliability classi-\nfication, and finally, we perform an unsupervised clustering\nwhere we analyze the patterns in the news sources captured\nby the learned features. In the following experiments, the\nlabels from Media Bias/Fact Check are used, and word em-\nbeddings for the semantic shift are trained using Word2Vec\nwith dimension 100, context window of 10, and minimum\nword count of 20. The parameters for SciLander are margin\nM= 1, vector size s= 50, and distance dused in the loss\nfunction is the cosine distance.\nIndicator Coverage\nIn our first experiment, we measure the overlap of the intro-\nduced indicators in terms of source and triplet coverage. We\nalso measure the accuracy of the triplets computed by these\nindicators.\nWe define the source coverage (sc ) and the triplet cover-\nage (tc) between two indicators i, jas follows:\nsc(i, j ) =|src(i)\u2229src(j)|\n|src(i)|, tc (i, j) =|trpl (i)\u2229trpl(j)|\n|trpl (i)|\nwhere src(.)andtrpl(.)compute the distinct set of sources\nand triplets covered by a given indicator. We note that scand\ntcare non-symmetric; consequently, the heatmaps in Fig-\nure 3 are also non-symmetric.\nTo measure the accuracy of the computed triplets, we use\nthe metric Area Under the Receiver Operating Characteris-\ntics(AUROC), which measures the True Positive Rate over\ntheFalse Positive Rate. We also break down the AUROC\nof the triplets into i) the AUROC pof the positive part of\n274\nthe triplets (a, p), ii) the AUROC nof the negative part of\nthe triplets (a, n), and iii) the AUROC fof the full triplets\n(a, p, n ). Specifically, for each individual AUROC, we con-\nsider the following as true positives:\nAUROC p:{(a, p) s.t. label (a) =label (p)}\nAUROC n:{(a, n) s.t. label (a)\u0338=label (n)}\nAUROC f:{(a, p, n) s.t. label (a) =label (p)\n\u2227label (a)\u0338=label (n)}\nAs we observe in Figure 3, although the sources covered\nby some indicators heavily overlap, the contributed triplets\nare quite unique. Indicatively, the stance indicator covers\n27.5% of the sources, totally overlapping with the copy indi-\ncator. However, the contributed triplets of the stance indica-\ntor are different from the contributed triplets of all the other\nindicators and also more accurate. Indeed, we see that there\nis a trade-off between the source coverage of the indicators\nand the AUROC. Hence, the more specific the indicator is\n(e.g., the stance indicator), the better AUROC it has.\nFinally, we observe that the overall AUROC for positive\nand negative pairs (AUROC pand AUROC n, respectively)\nare above the 50% baseline of a random positive (or nega-\ntive) pair selection is truly positive (or negative).\nIt should be noted that the AUROC for complete triplets\n(AUROC f) is lower than 50%. This happens because the\nchoice of the final triplets involves two independent deci-\nsions: the choice of the positive sample, and the choice of the\nnegative sample. As noted above, each choice has a chance\nof success of 50% if chosen at random. Thus, for a triplet to\nbe correctly selected, the random baseline is that a correct\npositive pair is chosen anda correct negative pair is chosen,\nwhich results in a 0.5\u00d70.5 = 0 .25, or 25% baseline chance.\nAs we see in the following experiments, the model for train-\ning source embedding is robust to noisy triplets as it yields\nhighly accurate results in all the downstream tasks we use it.\nOffline Source Classification\nIn this experiment, we evaluate the computed embeddings\non a downstream classification task. We assume that, for all\nsources in our corpus, we have (offline) access to a signifi-\ncant fraction of their history of published articles.\nBaselines. For this task, we implement baselines using\nStylistic Text Features, Contextualized Embeddings, and Co-\ncitation Embeddings, as well as combinations of the above.\nStylistic Text Features. We utilize stylistic text features\nfrom Horne et al. (Horne and Adali 2017) aggregated at\nthe source level as representations. These features include,\namong others, the number of: part of speech tags, punctua-\ntion symbols, and capitalized words, which are the features\nthat are typically used in news classifiers.\nContextualized Embeddings. We compute BERT (Devlin\net al. 2019) embeddings for a total of 32tokens from the title\nand the opening paragraph of the article, and average them\nfor each source. Similarly, we compute SciBERT (Beltagy,\nLo, and Cohan 2019) instead of BERT embeddings, which\nhave been shown to lead to better performance in tasks in-\nvolving scientific text. The configuration parameters of both\ncopy shift jargon stancecopyshift jargonstance100.00% 72.46% 39.13% 27.54%\n80.91% 100.00% 41.42% 28.16%\n98.54% 93.43% 100.00% 64.23%\n100.00% 91.58% 92.63% 100.00%\ncopy shift jargon stancecopyshift jargonstance100.00% 0.16% 0.13% 0.26%\n0.11% 100.00% 0.02% 0.03%\n0.79% 0.17% 100.00% 7.03%\n1.38% 0.25% 6.23% 100.00%Indicator AUROC pAUROC nAUROC f#sources\ncopy 72.7% 51.0% 36. 3% 257\nshift 61.9% 60.8% 41. 8% 308\nstance 89.7% 73.3% 68. 3% 87\njargon 81.9% 51.0% 42. 9% 126\noverall 77.0% 69.7% 57. 5% 316\nFigure 3: Overlap of indicators in terms of source coverage\n(top left) and triplet coverage (top right); AUROC of the pos-\nitive part, negative part, and full triplets (bottom). Although\nthe sources covered by most indicators heavily overlap, their\ntriplets are quite unique. Also, there is a trade-off between\nthe source coverage of the indicators and their AUROC.\nBERT and SciBERT are those suggested in a widely used\nrelease of this model (Wolf et al. 2019).\nCo-Citation Embeddings. We compute a co-citation graph\nof sources based on their scientific references. We weight\nthis graph either uniformly for each common reference,\nor by emphasizing the uniquely used references, using\ntheir TF-IDF score. In the overall graph, we run node2vec\n(Grover and Leskovec 2016) to extract source embeddings.\nJoint Embeddings. TheContextualized Embeddings and\ntheCo-Citation Embeddings capture two different modal-\nities of news sources; their content and citation behavior.\nThus, we create a joint representation by concatenating the\ntwo embeddings. Since the dimensionality of the joint em-\nbeddings is high, we apply Principal Component Analysis to\nreduce it and compare it with other baseline representations.\nEvaluation. We test the usefulness of the learned repre-\nsentations in the problem of source veracity classification.\nWe use the embeddings computed by i) SciLander trained\non all indicators, ii) SciLander trained only on content indi-\ncators (shift or copy), and iii) the aforementioned baseline\nmodels, to train a Nearest Neighbors classifier in a 10-fold\ncross-validation setting. Figure 4 shows the F1 score of each\nmodel for increasing values of k.\nRelying uniquely on textual features limits classifiers to a\nrestricted set of signals. Our framework combines stylistic,\nsemantic, and behavioral indicators to produce a represen-\ntation that improves the separation of reliable and unreli-\nable sources. Thus, compared to traditional baselines such\nas stylistic features or features extracted by BERT, our em-\nbeddings show significant performance improvement. Our\nmethod obtains the best F1 score (87%) for k=37.\n275\nFigure 4: F1 scores using k-nearest neighbors classifiers\nover the source embeddings representations computed by\nSciLander and the various baselines described in \u00a7. SciLan-\nder obtains the best F1 score (87%) for k=37.\nOnline Source Classification\nIn this experiment, we assume that we have two types of\nsources: i) offline (known) sources, for which we have ac-\ncess to a significant fraction of their publication history, and\nii) online (newcomer) sources, for which we have access to\na limited fraction of their publication history. As assessing\narticles from newcomer sources might be a time-consuming\ntask, we inspect the lowest fraction of articles that is needed\nto accurately classify these sources.\nThe procedure that we employ is the following: i) we\ntrain embeddings for the offline sources (as we explain in\n\u00a7); ii) we freeze these embeddings for the offline sources;\niii) we train embeddings for online sources, in the already\nshaped by the offline sources embeddings space.\nWe conduct the experiment on a 10-fold cross-validation\nsetting. In Figure 5, we report the learning curve (F1 score)\nfor increasing fractions of articles from newcomer sources in\nthe same classification task described in \u00a7. We note that the\ntemporal axis is not in chronological order but sampled ran-\ndomly from the entire corpus (e.g., we sample articles rep-\nresenting a 3-month publishing activity of an online source\nfrom the entire publishing activity of that source). In that\nway, each temporal interval is independent of external events\n(e.g., the development of the vaccines), which affects the ac-\ntivity of most sources. As we observe in Figure 5, SciLander\nis able to reliably (F1>85%) classify sources, using only\nthree months of their publishing activity.\nSource Clustering Analysis\nWe conduct an unsupervised clustering experiment to inves-\ntigate potential trends revealed by the features learned by\nSciLander. Using the same embeddings from the previous\nexperiments (50 dimensions, M= 1), we apply DBSCAN\nclustering to the source vectors with the cosine distance as\ndistance metric, minimum distance parameter \u03f5= 0.1 and\nminimum cluster size n= 1. The resulting clusters are\nshown in Figure 6; each of the 7 clusters is shown in dif-\nferent color shades and labeled from AtoG.\nWe characterize the clusters quantitatively with respect to\nthe density of unreliable sources, political leaning, and the\nlevel of partisanship bias aggregated across the news sources\nFigure 5: Learning curve (F1 score) for increasing fractions\nof articles from newcomer sources. SciLander is able to reli-\nably (F1>85%) classify sources using only 3months of their\npublishing activity.\nwithin them. For each cluster, we compute the proportion of\nunreliable sources to the total number of sources in the clus-\nter. Figure 7a shows the density of unreliable sources within\neach cluster. This result suggests that the source embeddings\ncarry information about source credibility when grouping\nthem, even though credibility labels or related features were\nunknown to the model during training.\nClusters CandEcontain no unreliable sources and hold\nmostly mainstream news sources such as The Washington\nPost, V ox, National Public Radio (NPR), and the Chicago\nTribune. The clusters containing the largest proportions of\nunreliable sources are the clusters A,B, and G, and most\nsources in these clusters are websites that propagate con-\nspiracy theories and promote pseudoscience. Details on the\ndiscovered clusters are shown in Table 5.\nThese results show that the SciLander embeddings are\nable to group sources based on similar reliability. Multi-\nple clusters of relatively high purity with respect to reliabil-\nity are created, some reliable (75%-100% reliable sources),\nsome unreliable (0%-30% reliable sources).\nWe compute the overall political leaning of a cluster by\naveraging the political leaning scores of the sources within\nthat cluster. Partisanship bias is obtained by the absolute\nvalue of leaning, scaled to a value in [0,1], with 0indicating\nthat there is no partisanship bias in the cluster, and 1in-\ndicating the maximum partisanship bias, where all sources\nin the cluster exhibit a strong political leaning. The parti-\nsanship bias describes the agreement between the political\nleanings of sources within the cluster, and the magnitude of\nsuch leanings. The distribution of political leanings and par-\ntisanship bias are shown in Figures 7b and 7c. There is a\nnoticeable disparity between the partisanship bias found in\nthe two biggest unreliable clusters AandB. Sources in clus-\nterAexhibit a strong bias, which is nearly absent in cluster\nB. We explore the particularities of these clusters next.\nDifferent Types of Conspiracy Theories\nWe observe two clusters with high density of unreliable\nsources (clusters AandB). Both clusters include many un-\nreliable news sources, and there exist qualitative differences\nbetween them, which we describe in this section.\nTo uncover qualitative differences between sources in\nclusters AandB, we measure the shift in context between\n276\n1\n 0 1\nPC12\n1\n012PC2Cluster\nA\nB\nC\nD\nE\nF\nGFigure 6: Kernel Density Estimation of the clusters\nCl. (U) (P) Core Sources\nA.70 .25 NewsWars, Vets. Today, The D.C. Clothesline\nB.84 .03 Mercola, Healthy Hol. Living, Vaccine React.\nC.00 .11 The Washington Post, V ox, NPR\nD.25 .00 The American Cons., Roll Call\nE.00 .20 Chicago Tribune\nF.12 .03 Wash. Monthly, FiveThirtyEight, Atlantic\nG.80 .00 Ice Age Now\nTable 5: (U)nreliability score (proportion of unreliable\nsources), average (P)artisanship bias score, and core sources\n(nearest neighbors to the centroid) of the identified clusters.\nthese clusters and the mainstream cluster C. Specifically, we\ncomputed the semantic shift across clusters of sources by\ntraining Word2Vec models EA,EB, and ECusing articles\nfrom the core sources of each cluster and using the same\nhyper-parameters as in the previous experiments. Then, we\nextract the words with the highest cosine distance between\npairs (EC, EA)and(EC, EB)to find the terms that most\ncontribute to the deviation in the news from sources in Cto\neach of the unreliable clusters AandB.\nLetSAandSBbe the lists of the 100words most shifted\ntoC, from AandB, respectively. We find that there is only\none word in common between the SAandSB: \u201cnatural\u201d.\nTo characterize the words in both lists, we identify words\nthat refer to people, entities and places, political issues, and\nhealth and nutrition. Examples of these words are given be-\nlow and listed on Table 6.\nThe largest group of words shifted in cluster A are re-\nlated to individuals, entities, places (25%), and political top-\nics (12%). Almost all individuals found are political figures\n(with a few exceptions). There are only 1.5% of terms re-\nlated to health and nutrition. Many of these news outlets are\nconspiracy theory websites such as NewsWars, Veterans To-\nday, and InfoWars. According to a Media Bias/Fact Check\nanalysis4, these sites often publish hate-speech-filled content\nin addition to misleading or false information.\nIn contrast, the largest group of shifted words was de-\ntected in cluster B (21.5%), with only 2% people and 1%\nrelated to political topics. According to MBFC journalists5,\nthese sources promote alternative health notions, sell ques-\ntionable products and supplements, and promote antivacci-\nnation positions with pseudoscience-based arguments.\n4https://mediabiasfactcheck.com/veterans-today\n5https://mediabiasfactcheck.com/mercolaPeople and Places Political Terms Health\nKamala Harris BLM (Black Lives Matter) Coronavirus\nBernie Sanders Patriot Food\nNancy Pelosi V oting Vaccines\nMike Pence Abortion Doctors\nAlex Jones Partisan Mask\nTable 6: List of words from clusters AandBthat are most\nshifted from the mainstream cluster C. People and Places,\nand Political Terms appear as the most shifted words in clus-\nterA, suggesting that its sources push politically-oriented\nmisinformation, while sources in cluster Bfocus more on\nalternative health solutions.\nBased on this, we conclude that while cluster A is a clus-\nter of mostly politically-unreliable news sources covering\nCOVID-19 stories mixed with other political topics, cluster\nB is much more focused on covering alternative medicine-\nbased misinformation with slight political leaning, presum-\nably to appeal to individuals with different political opin-\nions. On these sites, health-based information is often mixed\nwith promotion and affiliate links to sites selling alternative\nmedicine products and supplements. Our method is able to\nproperly distinguish these different types of COVID-19 mis-\ninformation, without explicitly training on related features.\nDiscussion\nSciLander is a method for embedding news sources. The\nresults of the experiments (\u00a7) show that the representations\nlearned from SciLander outperform other state-of-the-art\nfeature models. Despite the final representation being a set\nof autoencoded features (i.e., embeddings learned from a\nneural network), it is directly explained by the product of\na combination of the aforementioned indicators.\nThe applications of these learned features are not re-\nstricted to classification tasks. They can be used in any sce-\nnario where similarities between news sources are needed,\nsuch as in clustering analysis (details in \u00a7) and recommen-\ndation systems.\nSciLander, like most other AI/ML methods, is heavily\ndata-driven. It uses signals found in the text of news arti-\ncles to infer the relationships between sources. The above\ncan cause SciLander to make biased decisions, especially if\nthe input data is biased towards/against societal groups, such\nas underrepresented minorities and other vulnerable groups.\nWe argue that SciLander, when deciding what content to\nrecommend or promote, can provide assistance in human\ndecision-making but not replace human judgment.\nThe indicators used by SciLander complement each\nother. The experiments shown in \u00a7 demonstrate that the\nembeddings performed better in classification tasks when\nall four indicators are combined (copy, shift, jargon, and\nstance). The latter suggests that the indicators worked in a\ncomplementary manner, where a mistake made by one in-\ndicator is corrected by the other indicators. Furthermore, as\nseen in Figure 3, some indicators were better suited to de-\ntect negative pairs. For example, the indicator copy had a\n277\n1\n 0 1\nPC12\n1\n012PC2\n0.00.20.40.60.81.0\n(a) Density of Unreliable Sources\n1\n 0 1\nPC12\n1\n012PC2\nLeftUnknownRight\n (b) Density of Political Leaning\n1\n 0 1\nPC12\n1\n012PC2\nWeakStrong\n (c) Density of Partisanship Bias\nFigure 7: Density analysis of the clusters computed by SciLander. Components PC1 and PC2, obtained from Principal Compo-\nnent Analysis (PCA) on the source embeddings, are the components with the highest explained variance ratio.\nAUROC score of 72.7% for positive samples and 51% for\nnegative samples. This result can be explained by the fact\nthat while the presence of copy behavior between sources\nxandyis an indicator of similarity between xandy, its\nabsence does not necessarily imply that xandyare very\ndistinct. In short, source xnot copying from ydoes not im-\nply that xis distant from y. Conversely, the stance indicator\nhad a higher negative pair score (73.3%), suggesting that this\nindicator would perform better in finding negative samples.\nSciLander has the potential to be extended to general do-\nmains. We based SciLander on features that work as indica-\ntors of source similarity or dissimilarity, motivated by pre-\nvious research on language and misinformation (Chambers\nand Schilling 2018; Horne, N\u00f8rregaard, and Adal\u0131 2019;\nSmeros, Castillo, and Aberer 2019). The shift andcopy in-\ndicators are agnostic to the news domain since they only re-\nquire the presence of text. However, the stance andjargon\nindicators are closely tied to scientific news.\nTo extend the application of SciLander to other, including\nnon-scientific, domains, a topic of choice must be specified\nprior to the application of the method. The chosen topic must\ninclude a set of entities referred to by news sources, such as\npolitical figures in the political news domain. In this case,\nthe stance towards scientific references would be replaced\nby the stance towards such political figures, and the scien-\ntific jargon would be replaced by political jargon. Topics can\nbe manually defined via a set of keywords and entities, or au-\ntomatically defined, such as by applying topic modeling to\nextract the relevant keywords from the news documents.\nLimitations. Our methodology was only applied and eval-\nuated on an English dataset; extending to other languages\nwould only require translation/adaptation of the domain-\nspecific lexicon used to compute the jargon indicator or sim-\nply skipping this indicator and training using the other three.\nAll the other indicators as well as the introduced embed-\nding model, are based either on language-agnostic or already\nmultilingual models.\nFurthermore, our methodology supports only explicit ci-\ntations, i.e., direct outgoing links to scientific papers, and not\nimplicit mentions of science-related entities (e.g., universi-\nties) because the latter design choice introduces ambiguity\nand noisy source triplets.\nFinally, we implicitly filter the scientific references re-lated to COVID-19 as we filter the news corpus citing\nthese references. Explicit filtering would require download-\ning and parsing the references from different formats, e.g.,\npdf, which is a demanding task not in the scope of this work.\nConclusions\nWe have introduced SciLander, a method for learning a rep-\nresentation of news sources reporting science-related con-\ntent. Our method uses a combination of signals to estimate\nthe similarity between news sources. We have shown that\nthese signals complement each other, capturing relationships\nbetween distinct sets of sources from a dataset of news arti-\ncles related to COVID-19. Furthermore, the features learned\nby our model demonstrated superior performance to base-\nlines for the task of source credibility detection, both in\nan offline and an online setting, requiring as little as three\nmonths of publication activity to accurately classify news\nsources. Lastly, we have shown that the learned source rep-\nresentations encode information of credibility and political\nleaning, forming clusters of sources that show similar reli-\nability and political bias. In particular, we discovered two\nlarge clusters of unreliable sources to which different types\nof conspiracy news sources flock. One of them concentrates\non alternative health misinformation, and the other promotes\nhyper-partisan political conspiracies.\nReproducibility. All the data, code, and models used for this\npaper are publicly available for research purposes in the fol-\nlowing repository: https://github.com/mgruppi/SciLander.\nEthics Statement\nOur work aims at finding representations that capture the\nsimilarities and differences between news sources in their\ncoverage of the COVID-19 pandemic. Our proposed method\nbases this representation on the language usage, content\ncopy/sharing behavior, and their stance towards scientific\nreferences. We show that the representations learned from\nthese signals are useful for several downstream tasks, in-\ncluding understanding the reliability of a source. This is ac-\ncomplished by using proxies to trust scientific references,\nlanguage, and content. One must be careful when applying\nthis method to untested dimensions, such as the presence of\nlanguage usage by minority groups. These groups may be\nunderrepresented in the training data, which may cause the\n278\nmodel to make biased predictions about them. We propose\nthat this method aids the decision-making process as a com-\nplement to human judgment rather than a replacement.\nAcknowledgements\nThis work was supported by the Rensselaer-IBM AI Re-\nsearch Collaboration (http://airc.rpi.edu), part of the IBM AI\nHorizons Network (http://ibm.biz/AIHorizons).\nReferences\nAnderson, R. N.; Warner, M.; Flagg, L. A.; and Ahmad, F.\n2020. Guidance for Certifying Deaths Due to Coronavirus\nDisease 2019 (COVID-19). https://stacks.cdc.gov/view/cdc/\n87029. Accessed: 2021-03-15.\nBalntas, V .; Riba, E.; Ponsa, D.; and Mikolajczyk, K. 2016.\nLearning local feature descriptors with triplets and shallow\nconvolutional neural networks. In BMVC 2016, York, UK,\nSeptember 19-22, 2016. BMV A Press.\nBaly, R.; Karadzhov, G.; Alexandrov, D.; Glass, J. R.; and\nNakov, P. 2018. Predicting Factuality of Reporting and Bias\nof News Media Sources. In EMNLP , Brussels, Belgium, Oc-\ntober 31 - November 4, 2018, 3528\u20133539. ACL.\nBaly, R.; Karadzhov, G.; Saleh, A.; Glass, J. R.; and Nakov,\nP. 2019. Multi-Task Ordinal Regression for Jointly Predict-\ning the Trustworthiness and the Leading Political Ideology\nof News Media. In NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, 2109\u20132116. ACL.\nBeltagy, I.; Lo, K.; and Cohan, A. 2019. SciBERT: A Pre-\ntrained Language Model for Scientific Text. In EMNLP-\nIJCNLP 2019, Hong Kong, China, November 3-7, 2019 ,\n3613\u20133618. ACL.\nBourgeois, D.; Rappaz, J.; and Aberer, K. 2018. Selection\nBias in News Coverage: Learning it, Fighting it. In WWW\n2018, Lyon , France, April 23-27, 2018, 535\u2013543. ACM.\nBuchanan, M. 2020. Managing the infodemic. Nature Pub-\nlishing Group, 16(9): 894\u2013894.\nCastillo, C. 2016. Big crisis data: social media in disasters\nand time-critical situations. Cambridge University Press.\nChambers, J. K.; and Schilling, N. 2018. The handbook of\nlanguage variation and change. John Wiley & Sons.\nChen, X.; Qin, Z.; Zhang, Y .; and Xu, T. 2016. Learning\nto Rank Features for Recommendation over Multiple Cate-\ngories. In Perego, R.; Sebastiani, F.; Aslam, J. A.; Ruthven,\nI.; and Zobel, J., eds., SIGIR 2016, Pisa, Italy, July 17-21,\n2016, 305\u2013314. ACM.\nChung, C. J.; Nam, Y .; and Stefanone, M. A. 2012. Explor-\ning Online News Credibility: The Relative Influence of Tra-\nditional and Technological Factors. Journal of Computer-\nMediated Communication, 17(2): 171\u2013186.\nCruse, D. A.; Cruse, D. A.; Cruse, D. A.; and Cruse, D. A.\n1986. Lexical semantics. Cambridge university press.\nCzeisler, M. \u00c9.; Lane, R. I.; Petrosky, E.; Wiley, J. F.; Chris-\ntensen, A.; Njai, R.; Weaver, M. D.; Robbins, R.; Facer-\nChilds, E. R.; Barger, L. K.; et al. 2020. Mental health,\nsubstance use, and suicidal ideation during the COVID-19pandemic\u2014United States, June 24\u201330, 2020. Morbidity and\nMortality Weekly Report, 69(32): 1049.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL-HLT 2019, Minneapo-\nlis, MN, USA, June 2-7, 2019, Volume 1, 4171\u20134186. ACL.\nGrover, A.; and Leskovec, J. 2016. node2vec: Scalable Fea-\nture Learning for Networks. In SIGKDD, San Francisco,\nCA, USA, August 13-17, 2016, 855\u2013864. ACM.\nGruppi, M.; Chen, P.; and Adali, S. 2021. Fake it Till\nYou Make it: Self-Supervised Semantic Shifts for Monolin-\ngual Word Embedding Tasks. In AAAI 2021, Virtual Event,\nFebruary 2-9, 2021, 12893\u201312901. AAAI Press.\nGruppi, M.; Horne, B. D.; and Adali, S. 2021. NELA-\nGT-2020: A Large Multi-Labelled News Dataset for The\nStudy of Misinformation in News Articles. CoRR,\nabs/2102.04567.\nGruppi, M.; Horne, B. D.; and Adal\u0131, S. 2021. Tell Me\nWho Your Friends Are: Using Content Sharing Behavior for\nNews Source Veracity Detection. CoRR, abs/2101.10973.\nHamilton, W. L.; Leskovec, J.; and Jurafsky, D. 2016. Cul-\ntural Shift or Linguistic Drift? Comparing Two Computa-\ntional Measures of Semantic Change. In EMNLP 2016,\nAustin, Texas, USA, November 1-4, 2016, 2116\u20132121. ACL.\nHansen, C.; Hansen, C.; Alstrup, S.; Simonsen, J. G.; and\nLioma, C. 2019. Neural Check-Worthiness Ranking with\nWeak Supervision: Finding Sentences for Fact-Checking. In\nWWW \u201919, San Francisco, CA, USA, May 13-17, 2019, 994\u2013\n1000. ACM.\nHoffer, E.; and Ailon, N. 2015. Deep Metric Learning Using\nTriplet Network. In Feragen, A.; Pelillo, M.; and Loog, M.,\neds., SIMBAD 2015, Copenhagen, Denmark, October 12-14,\n2015, Proceedings, volume 9370 of LNCS, 84\u201392. Springer.\nHorne, B. D.; and Adali, S. 2017. This Just In: Fake News\nPacks a Lot in Title, Uses Simpler, Repetitive Content in\nText Body, More Similar to Satire than Real News. CoRR,\nabs/1703.09398.\nHorne, B. D.; N\u00f8rregaard, J.; and Adal\u0131, S. 2019. Different\nSpirals of Sameness: A Study of Content Sharing in Main-\nstream and Alternative Media. In ICWSM 2019, Munich,\nGermany, June 11-14, 2019, 257\u2013266. AAAI Press.\nJiang, S.; Baumgartner, S.; Ittycheriah, A.; and Yu, C. 2020.\nFactoring Fact-Checks: Structured Information Extraction\nfrom Fact-Checking Articles. In WWW \u201920, Taipei, Taiwan,\nApril 20-24, 2020, 1592\u20131603. ACM / IW3C2.\nJoulin, A.; Bojanowski, P.; Mikolov, T.; J\u00e9gou, H.; and\nGrave, E. 2018. Loss in translation: Learning bilingual\nword mapping with a retrieval criterion. arXiv preprint\narXiv:1804.07745.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2020. BART: Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and Compre-\nhension. In ACL 2020, Online, July 5-10, 2020, 7871\u20137880.\nACL.\n279\nLi, C.; and Goldwasser, D. 2019. Encoding Social Infor-\nmation with Graph Convolutional Networks forPolitical Per-\nspective Detection in News Media. In ACL 2019, Florence,\nItaly, July 28- August 2, 2019, 2594\u20132604. ACL.\nMcKay, S.; and Tenove, C. 2021. Disinformation as a Threat\nto Deliberative Democracy. Political Research Quarterly,\n74(3): 703\u2013717.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef-\nficient Estimation of Word Representations in Vector Space.\nInICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013.\nRappaz, J.; Bourgeois, D.; and Aberer, K. 2019. A Dy-\nnamic Embedding Model of the Media Landscape. In WWW\n2019, San Francisco, CA, USA, May 13-17, 2019, 1544\u2013\n1554. ACM.\nReis, J. C. S.; Correia, A.; Murai, F.; Veloso, A.; Ben-\nevenuto, F.; and Cambria, E. 2019. Supervised Learning for\nFake News Detection. IEEE Intell. Syst., 34(2): 76\u201381.\nRibeiro, F. N.; Lima, L. H. C.; Benevenuto, F.; Chakraborty,\nA.; Kulshrestha, J.; Babaei, M.; and Gummadi, K. P. 2018.\nMedia Bias Monitor: Quantifying Biases of Social Media\nNews Outlets at Large-Scale. In ICWSM 2018, Stanford,\nCalifornia, USA, June 25-28, 2018, 290\u2013299. AAAI Press.\nSchlechtweg, D.; H\u00e4tty, A.; Tredici, M. D.; and im Walde,\nS. S. 2019. A Wind of Change: Detecting and Evaluating\nLexical Semantic Change across Times and Domains. In\nACL 2019, Florence, Italy, July 28- August 2, 2019, 732\u2013\n746. ACL.\nShaar, S.; Babulkov, N.; Martino, G. D. S.; and Nakov, P.\n2020. That is a Known Lie: Detecting Previously Fact-\nChecked Claims. In ACL 2020, Online, July 5-10, 2020,\n3607\u20133618. ACL.\nShu, K.; Wang, S.; and Liu, H. 2019. Beyond News Con-\ntents: The Role of Social Context for Fake News Detection.\nInWSDM 2019, Melbourne, VIC, Australia, February 11-\n15, 2019, 312\u2013320. ACM.\nShugars, S.; Gitomer, A.; McCabe, S.; Gallagher, R. J.;\nJoseph, K.; Grinberg, N.; Doroshenko, L.; Welles, B. F.; and\nLazer, D. 2021. Pandemics, Protests, and Publics: Demo-\ngraphic Activity and Engagement on Twitter in 2020. Jour-\nnal of Quantitative Description: Digital Media, 1.\nSmeros, P.; Castillo, C.; and Aberer, K. 2019. SciLens: Eval-\nuating the Quality of Scientific News Articles Using So-\ncial Media and Scientific Literature Indicators. In WWW\n2019, San Francisco, CA, USA, May 13-17, 2019, 1747\u2013\n1758. ACM.\nSmeros, P.; Castillo, C.; and Aberer, K. 2021. SciClops:\nDetecting and Contextualizing Scientific Claims for Assist-\ning Manual Fact-Checking. In CIKM \u201921, Queensland, Aus-\ntralia, November 1 - 5, 2021, 1692\u20131702. ACM.\nVan Bavel, J. J.; Baicker, K.; Boggio, P. S.; Capraro, V .; Ci-\nchocka, A.; Cikara, M.; Crockett, M. J.; Crum, A. J.; Dou-\nglas, K. M.; Druckman, J. N.; et al. 2020. Using social\nand behavioural science to support COVID-19 pandemic re-\nsponse. Nature human behaviour, 4(5): 460\u2013471.\nVishwakarma, D. K.; Varshney, D.; and Yadav, A. 2019. De-\ntection and veracity analysis of fake news via scrapping andauthenticating the web search. Cogn. Syst. Res., 58: 217\u2013\n229.\nWang, L. L.; Lo, K.; Chandrasekhar, Y .; Reas, R.; Yang, J.;\nEide, D.; Funk, K.; Kinney, R.; Liu, Z.; Merrill, W.; Mooney,\nP.; Murdick, D. A.; Rishi, D.; Sheehan, J.; Shen, Z.; Stil-\nson, B.; Wade, A. D.; Wang, K.; Wilhelm, C.; Xie, B.; Ray-\nmond, D.; Weld, D. S.; Etzioni, O.; and Kohlmeier, S. 2020.\nCORD-19: The Covid-19 Open Research Dataset. CoRR,\nabs/2004.10706.\nWang, R.; Shivanna, R.; Cheng, D. Z.; Jain, S.; Lin, D.;\nHong, L.; and Chi, E. H. 2021. DCN V2: Improved Deep &\nCross Network and Practical Lessons for Web-scale Learn-\ning to Rank Systems. In Leskovec, J.; Grobelnik, M.; Na-\njork, M.; Tang, J.; and Zia, L., eds., WWW \u201921, Ljubljana,\nSlovenia, April 19-23, 2021, 1785\u20131797. ACM / IW3C2.\nWeinberger, K. Q.; and Saul, L. K. 2009. Distance met-\nric learning for large margin nearest neighbor classification.\nJournal of machine learning research, 10(2).\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; and\nBrew, J. 2019. HuggingFace\u2019s Transformers: State-of-the-\nart Natural Language Processing. CoRR, abs/1910.03771.\nYang, S.; Shu, K.; Wang, S.; Gu, R.; Wu, F.; and Liu, H.\n2019. Unsupervised Fake News Detection on Social Media:\nA Generative Approach. In AAAI 2019, Honolulu, Hawaii,\nUSA, January 27 - February 1, 2019, 5644\u20135651. AAAI\nPress.\nYin, Z.; Sachidananda, V .; and Prabhakar, B. 2018. The\nGlobal Anchor Method for Quantifying Linguistic Shifts\nand Domain Adaptation. In NeurIPS 2018, December 3-8,\n2018, Montr\u00e9al, Canada, 9434\u20139445.\nZarocostas, J. 2020. How to fight an infodemic. The Lancet,\n395(10225): 676.\nZhang, A. X.; Ranganathan, A.; Metz, S. E.; Appling, S.;\nSehat, C. M.; Gilmore, N.; Adams, N. B.; Vincent, E.; Lee,\nJ.; Robbins, M.; Bice, E.; Hawke, S.; Karger, D. R.; and\nMina, A. X. 2018. A Structured Response to Misinfor-\nmation: Defining and Annotating Credibility Indicators in\nNews Articles. In WWW 2018, Lyon , France, April 23-27,\n2018, 603\u2013612. ACM.\nZubiaga, A.; Kochkina, E.; Liakata, M.; Procter, R.;\nLukasik, M.; Bontcheva, K.; Cohn, T.; and Augenstein, I.\n2018. Discourse-aware rumour stance classification in so-\ncial media using sequential classifiers. Inf. Process. Man-\nage., 54(2): 273\u2013290.\n280", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "SciLander: Mapping the scientific news landscape", "author": ["M Gruppi", "P Smeros", "S Adal\u0131", "C Castillo"], "pub_year": "2023", "venue": "Proceedings of the \u2026", "abstract": "The COVID-19 pandemic has fueled the spread of misinformation on social media and the  Web as a whole. The phenomenon dubbedinfodemic'has taken the challenges of information"}, "filled": false, "gsrank": 366, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/22144", "author_id": ["MHHYhXwAAAAJ", "S96SbW0AAAAJ", "Jk3gxBEAAAAJ", "D4NJsXEIh1cJ"], "url_scholarbib": "/scholar?hl=en&q=info:25HU2GPmeNkJ:scholar.google.com/&output=cite&scirp=365&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D360%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=25HU2GPmeNkJ&ei=RrWsaO_lEeHUieoP9LKZ6AI&json=", "num_citations": 7, "citedby_url": "/scholar?cites=15670528219949601243&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:25HU2GPmeNkJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/22144/21923"}}, {"title": "Building a Corpus for Hyperpartisan News Detection", "year": "NA", "pdf_data": "Bauhaus-Universit\u00e4t Weimar\nFaculty of Media\nDegree Programme Computer Science and Media\nBuilding a Corpus for\nHyperpartisan News Detection\nMaster\u2019s Thesis\nPayam  Adineh\n\u221e\ue02eReferee:  Prof. Dr. Benno Stein\nSubmission date: September 12, 2018\nDeclaration\nUnless otherwise indicated in the text or references, this thesis is\nentirely the product of my own scholarly work.\nWeimar, September 12, 2018\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nPayam Adineh\nAbstract\nAs the polarization of societies increases, news publishers that cater to spe-\nci\ufb01c extreme views become attractive to more people. Such hyperpartisan\nnews publishers, however, tend to draw a simple picture of their views being\nthe only truth, thus furthering the divide in the society. As a small step to-\nwards countering this trend, this thesis aims to provide the tools to distinguish\nhyperpartisan news from impartial ones. These tools will enable others to in-\nvestigate, monitor, and act upon the spread and e\ufb00ects of hyperpartisan news.\nThe main contributions of this work are the construction of a corpus of\n1.5 million political news articles from 358 di\ufb00erent news publishers and a de-\ntailed analysis of this corpus with regards to the use of hyperlinks within and\nbetween the publishers, the reuse of articles, and an automatic classi\ufb01cation\nof the articles by publisher bias.\nContents\n1 Introduction 1\n2 Related Work 4\n2.1 Fact Checking and Misinformation . . . . . . . . . . . . . . . . 4\n2.2 Ecosystem of Partisan Websites . . . . . . . . . . . . . . . . . . 4\n2.3 Hyperpartisan and Fake News Detection . . . . . . . . . . . . . 5\n3 Article Collection 6\n3.1 News Producers Discovery . . . . . . . . . . . . . . . . . . . . . 6\n3.1.1 BuzzFeed Partisan News Sites List . . . . . . . . . . . . 6\n3.1.2 Media Bias/Fact Check (MBFC) . . . . . . . . . . . . . 7\n3.1.3 Buzzfeed And MBFC Overlap . . . . . . . . . . . . . . . 8\n3.1.4 NewsIR16 Corpus . . . . . . . . . . . . . . . . . . . . . . 9\n3.2 Article URL Collection . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2.1 Facebook Pages . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2.2 Sitemap . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2.3 Link Statistics . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 Corpus Construction 19\n4.1 Archiving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.1.1 Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.1.2 Checking The Producers . . . . . . . . . . . . . . . . . . 21\n4.1.3 Archive Quality Assurance . . . . . . . . . . . . . . . . . 23\n4.2 Distributed Storage . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.2.1 Hadoop Distributed File System . . . . . . . . . . . . . . 29\n4.2.2 MapFile . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.2.3 MapFile Creator . . . . . . . . . . . . . . . . . . . . . . 29\n4.3 Distributed Archiving . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.3.1 Task Control Automation . . . . . . . . . . . . . . . . . 30\n4.4 Main Content Extraction . . . . . . . . . . . . . . . . . . . . . . 32\n4.4.1 Writing the Wrappers . . . . . . . . . . . . . . . . . . . 32\ni\nCONTENTS\n4.5 Corpus Formatting . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.5.1 JSON Line . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.5.2 XML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n5 Analysis 37\n5.1 Corpus Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.2 Corpus Anchor Element Analysis . . . . . . . . . . . . . . . . . 39\n5.3 Duplicate Article Detection . . . . . . . . . . . . . . . . . . . . 39\n5.3.1 Text Reuse Analysis Pipeline . . . . . . . . . . . . . . . 41\n5.4 Classi\ufb01cation Experiment . . . . . . . . . . . . . . . . . . . . . 42\n5.4.1 Logistic Regression . . . . . . . . . . . . . . . . . . . . . 42\n5.4.2 Naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . 46\n5.4.3 Random Forest Classi\ufb01er . . . . . . . . . . . . . . . . . . 47\n5.4.4 Experiment on SemEval 2019 Corpus . . . . . . . . . . . 49\n6 Conclusion 50\nBibliography 52\nii\nThe conscious and intelligent manipulation of the\norganized habits and opinions of the masses is an\nimportant element in democratic society. Those who\nmanipulate this unseen mechanism of society constitute\nan invisible government which is the true ruling power\nof our country[2005].\n\u2013 Edward Bernays\nAcknowledgements\nMy sincere thanks go to Johannes Kiesel and Dr. Martin Potthast, who guided\nme through the whole work.\nI also want to thank my family for the continuous love and encouragement,\nand my colleagues for their valuable support Sarah Alburakeh, Masoud Allah-\nyari, Milad Alshomary, Alexander Bondarenko, Negin Yaghoobisharif.\nThanks to everyone who made a mark in my life!\niv\nChapter 1\nIntroduction\nPartisanship, in the world of journalism and news producers, refers to the\nideologically biased creation and distribution of news by authors and organi-\nzations. To be more precise, partisan journalists present their perspective on\npolitical incidents in a way that it seems they are living in a parallel universe,\nin which every step and action that their favorable party takes is absolutely\nand unmistakably right. Hyperpartisanship has similar meaning, and is used\nespecially when news producers extremely manipulate the coverage of the re-\nality with a tendency to the right or left wing political parties\nThese days, partisanship, in addition to the widely spread fake news, has\nresulted in a huge threat to the political awareness of people, by publishing\nnot only partially manipulated versions of reality but also totally made-up\nand untruthful stories, as well. According to Mitchel Stephens, a journalism\nprofessor at New York University, this is not new: \u201cJournalism in the United\nStates was born partisan and remained, for much of its history, loud, boister-\nous and combative\u201d1. With the advent of the Internet and social media, these\nhyperpartisan news producers have become more powerful in the matter of\ndistribution and consequently more in\ufb02uential on public opinion. Partisanship\nand fake news have become highly prevalent, widely known and controversial,\nspeci\ufb01cally after the 2016 United States election. There has been much re-\nsearch on the e\ufb00ects of such news on the aforementioned election, and how it\nled the society to a massive political division. However, this partisan separa-\ntion is of great importance for the societies; thinking thoroughly about it, one\ncan realize a continuous spectrum of di\ufb00erent thoughts is an ultimate demand\nto enjoy a decent democracy and without that people can only hear one voice\nof totalitarianism. Consequently, partisanship has its merits for society; Nev-\n1https://www.politico.com/magazine/story/2017/06/26/\ngoodbye-nonpartisan-journalism-and-good-riddance-215305\n1\nCHAPTER 1. INTRODUCTION\nertheless, people ultimately need to be aware of this paradox: democracy does\nnot work properly without partisanship, but in the situation that partisanship\nbecomes a top priority and more important than anything else, it can not\nbe considered as an advantage anymore. As political scientist Lilliana Mason\n[2016] convincingly argues, \u201cThe more partisan we become, the more emotion-\nally we react to normal political events.\u201d And when emotions are heightened,\neverything becomes a threat to status. Politics becomes more about anger.\nAnd, here\u2019s the warning from Mason: \u201cThe angrier the electorate, the less\ncapable we are of \ufb01nding common ground on policies, or even of treating our\nopponents like human beings.\u201d2\nMoreover, biased journalism a\ufb00ects people\u2019s life from both the political\nand nonpolitical side; They not only manipulate people\u2019s minds, but also af-\nfect their identity. In 2009, Sean Westwood, conducted research over National\nElection Study, a survey that collects Americans\u2019 political opinions and behav-\nior[]. \u201cI didn\u2019t expect the political con\ufb02ict to spill over from political aspects\nof our lives to nonpolitical aspects of our lives, and I saw that happening in\nmy social group, Partisanship, for a long period of time, wasn\u2019t viewed as part\nof who we are,\u201d he also mentioned. \u201cIt wasn\u2019t core to our identity. It was just\nan ancillary trait. But in the modern era, we view party identity as something\nakin to gender, ethnicity or race - the core traits that we use to describe our-\nselves to others.\u201d. Considering this vast impact of extreme partisanship on the\nsociety, it is necessary to \ufb01nd a solution to both decline the consequences and\nincrease the awareness.\nIn this thesis, we focus on creating a platform to recognize and distinguish\nextremely biased news and their orientation from unbiased ones. However,\nto follow that direction one needs to have a proper dataset of news articles,\nwhich has not existed thus far; therefore, we create the required dataset. To\ndo so, we chose more than 600 news producers whose orientation was labeled\nin 5 di\ufb00erent categories, from the left to the right bias, by BuzzFeed3and\nMedia Bias Fact Check4. The next step was to extract news articles links from\nthose news producers, and afterwards all the links were crawled to create a\nlarge-scale corpus of approximately 1,5 million articles for the further study\nand analysis. It is worth mentioning that in the \ufb01eld of Computer Science,\nthere have been several studies in this regard. For comparison, in Potthast\net al. 2017 from Webis group of Bauhaus-Universit\u00e4t Weimar, such research\n2https://www.vox.com/the-big-idea/2017/9/5/16227700/\nhyperpartisanship-identity-american-democracy-problems-solutions-doom-loop\n3https://www.buzzfeed.com/craigsilverman/inside-the-partisan-fight-for-your-news-feed\n4https://mediabiasfactcheck.com/\n2\nCHAPTER 1. INTRODUCTION\nwas done on a corpus of 1,627 articles from 9 di\ufb00erent publishers.\nThis thesis aims to achieve two major objectives. Firstly, creating a corpus\nof news articles from the variety of news producers that produce political news\nwith their speci\ufb01c point of view in the United States. Secondly, analyzing the\ncorpus with di\ufb00erent learning methods in order to train a model which is able\nto detect biased news articles with a high accuracy.\n1. The process of collecting articles and constructing the corpus:\n\u000fFind political partisan news producer.\n\u000fFind political news articles using the producers\u2019 Facebook feed and\nsitemap\n\u000fCrawl and archive all the news articles while continuously monitor-\ning the archive quality.\n\u000fExtract the main information from the archived articles, such as\ntitle, author, time, content.\n2. Analysis the corpus:\n\u000fAnalyze the main information to recognize duplicate and near du-\nplicate documents.\n\u000fTrain classi\ufb01ers with di\ufb00erent machine learning models to distin-\nguish extreme bias and also articles orientation.\n3\nChapter 2\nRelated Work\nInthischapter, wedescribeanoverviewofseveralstudiesthatwefoundrelated\ntoourtopic. Theserelatedworkswillbeintroducedindi\ufb00erentsections. First,\nwe present researches about fact-checking and misinformation detection, then\nstudies about hyperpartisan and fake news detection.\n2.1 Fact Checking and Misinformation\nIn the contemporary world of journalism, a rapid growth of misinformation is\nconsidered as a huge concern, which requires attempts to enhance the way that\nwe detect and respond to misinformation. A research by Zhang et al.[2018]\nwas done regarding this subject. They used 40 of the most shared articles on\nsocial media for this research. Moreover, they employed 6 skilled annotators,\n3 for content and 3 for context annotation. Then, they o\ufb00ered a set of initial\nindicators for article credibility in two major categories content and context\nsignals. Content signals can be found from the text of an article, and a context\nsignalcanbefoundfromexternalsourcesormetadataofanarticle. Inthenext\nphase, toanalyzetheannotationdata, theyfocusedontwomeasures: \u201c(1)How\nmuch annotators agreed with one another when identifying indicators, and (2)\nhow much the annotators\u2019 assessments of overall article credibility agreed with\ndomain experts\u2019 assessments.\u201d Finally, they o\ufb00ered a template for creating a\nstandardized set of indicators for evaluating content credibility.\n2.2 Ecosystem of Partisan Websites\nThe 2016 US Presidential election led to a confusion about the factors that\na\ufb00ected Trump\u2019s victory. There are several aspects such as socio-economic,\ncultural, political and technological to discuss. One of the most important\n4\nCHAPTER 2. RELATED WORK\nanswers to that confusion is the creation of a fabricated news sites ecosystem.\nBuzzFeed News\u2019s Craig Silverman did a study1about this discussion, which\ninspected 667 hyperpartisan websites and their corresponding Facebook pages\nin the last three months before US presidential election. They realized 20\ntop-performing false stories about the election from hyperpartisan sites gen-\nerated 8,711,000 shares, reactions, and comments on Facebook. On the other\nhand, 19 major news websites generated 7,367,000 in the same period of time.\nIn addition, they mentioned that the top election content from major out-\nlets had been outperformed by fake election news on Facebook. This study\nalso indicates that hundreds of these websites are begin run by now-famous\nMacedonian teens2, and in one example a Facebook page which is run by a\nMacedonian teenager frequently outperforms some of the larger conservative\npages operated by Americans. In addition, Bhatt et al.[2018] in another sim-\nilar research reached to the conclusion that lots of these news websites that\nwerebeingestablishedduringtheelectioncampaigntimewerethenabandoned\nafter the election. They also indicate that this ecosystem directs users tra\ufb03c\nby creating a link from one site to another and liking each other\u2019s pages and\nposts.\n2.3 Hyperpartisan and Fake News Detection\nIn this regard, Potthast et al. [2017] is an good example, which employed the\nBuzzFeed-Webis fake news corpus with 1,627 articles from 9 di\ufb00erent news\nproducers. These articles have been fact-checked by BuzzFeed journalists.\nThey also are categorized into three categories namely, main-stream, left-wing\nand right-wing. In this approach, they used common features in a combination\nwith ones related to the news domain. The part-of-speech and n-grams are\nthe common features, and the ratios of quoted words and external links, the\nnumberofparagraphsandtheiraveragelengtharethedomainspeci\ufb01cfeatures.\nTheyutilizeseveralbaselinemodelsincluding,atop-basedbag-of-wordsmodel,\na model using only the domain-speci\ufb01c features and \ufb01nally naive baselines.\nThey concluded that with style analysis hyperpartisan news from unbiased\nnews ( F1=0.78), and satire from both ( F1=0.81) are distinguishable. It is also\nmentioned that left-wing and right-wing news enjoys more stylistic similarity\ncompared with ones from the mainstream.\n1https://www.buzzfeednews.com/article/craigsilverman/\nviral-fake-election-news-outperformed-real-news-on-facebook\n2https://www.buzzfeednews.com/article/craigsilverman/\nhow-macedonia-became-a-global-hub-for-pro-trump-misinfo\n5\nChapter 3\nArticle Collection\nThe \ufb01rst step to creating a corpus of partisan news articles is to \ufb01nd a straight-\nforwardmethodtocollectthearticles. Therearelotsofobstaclesinthisregard,\nfor instance, how do we \ufb01nd the news producers? how do we discover URLs of\nnews articles? how do we make sure to only collect the political articles? In\nthis chapter, we describe our solutions to overcome these di\ufb03culties.\n3.1 News Producers Discovery\nFinding general information about political news producers was our \ufb01rst prior-\nity. We obliged to depend on some reliable sources of information to maintain\na list of news producers in addition to their political view preferences. Media\nBias Fact Check and BuzzFeed are two sources that were used for this purpose.\n3.1.1 BuzzFeed Partisan News Sites List\nBuzzFeed is a cross-platform media network, which assumes that they have\nboth infrastructures of a tech company and innovation of being a culturally\nobsessed corporation. BuzzFeed concentrates on shaping a media platform for\ntoday\u2019s world, and the future. For this thesis, an article from BuzzFeed is used\nwhich is described in Section 2.2. As a part of the study they provide a list1\nof political news producers, plus extra information including the bias category\nand the Facebook page of each producer-if they have any-. This list contains\n677 political news websites with information as exempli\ufb01ed in Table 3.1.\nIn this list, bias categories are shortened into the two major groups of\nconservativeandliberal. Moreover, weneedto\ufb01ndthearticlesonthewebsites,\n1https://github.com/BuzzFeedNews/2017-08-partisan-sites-and-facebook-pages/\nblob/master/data/all-partisan-sites.csv\n6\nCHAPTER 3. ARTICLE COLLECTION\nSite Political Category Facebook Id\n100percentfedup.com right 311190048935167\n21stcenturywire.com left 182032255155419\n24dailynew.com right 515629708825640\naattp.org left 108038612554992\nTable 3.1: Excerpt list of BuzzFeed news producers\nbut every website looks di\ufb00erent. To \ufb01nd the articles in these producers,\nchecking the Facebook pages and going through the sitemaps are two di\ufb00erent\napproaches that were applied. First, all of the items in the list are checked for\nFacebookIdentry. Followingthat, allthepageswhichhaveanactiveFacebook\npage were labeled and also all the other items that don\u2019t have a Facebook page\nare examined for the availability of sitemap. As Table 3.2 shows that out of\n490 producers that are supposed to possess a Facebook page, 351 items own an\nactive and available Facebook page and the other Facebookpages are removed\nor deactivated. In addition, 107 news producers websites have sitemaps. The\nother 219 entries are not used for the next stage of the thesis owing to either\nunavailability of their website or duplication of producers in the BuzzFeed\nlist. Table 3.2 displays that 48 items on the list are duplicate entries and 171\nwebsites are not available anymore.\nSites LeftRightTotal\nTotal Number 178499677\nDuplicate 93948\nUnavailable 31140171\nFacebook 120231351\nSitemap 1889107\nTable 3.2: Availability of the Facebook pages and sitemap of the BuzzFeed list\n3.1.2 Media Bias/Fact Check (MBFC)\nMBFC is a fact-checking website which labels websites according to their po-\nlitical view in di\ufb00erent categories from right to left wing bias. As mentioned\non the website their objective is to inspire action and a rejection of overtly\nbiased media and moreover to return to an era of straightforward news re-\nporting. However, their index includes a huge list of websites from all over\n7\nCHAPTER 3. ARTICLE COLLECTION\nthe world that focused on a variety of topics like politics, economy, and sport\nto name but a few. MBFC categorized websites into \ufb01ve major categories,\nnamely extreme left, left-center, least biased, right-center, and extreme right.\nSince we had enough websites for extreme left and right from the BuzzFeed\nlist, we only focused on both right-center and left-center, in addition to the\nleast bias websites. However, due to the lack of left-wing articles, we decided\nto \ufb01nd new left sources from MBFC. In this website for each category, there is\na large list of websites, and for each of them, they provide a detail page which\nhas brief information about the producer. Therefore, we needed to go through\nthe detail page to check if it \ufb01ts our criteria.\nCriteria:\n1. It must cover the United States news.\n2. It must have a political section.\n3. The website must own a proper sitemap.\nWe found that 109 websites out of 1310 ful\ufb01ll our criteria and are thus\nincluded in our corpus. Table 3.3 illustrates the statistics of websites that\nwere checked during this phase of the thesis.\nLeftLeftcenter LeastRightcenter Total\nAll Subjects 290 447 362 211 1310\nPolitical 12 49 28 20 109\nTable 3.3: Media Bias/Fact Check indexed sites statistics\n3.1.3 Buzzfeed And MBFC Overlap\nDuring the process of \ufb01nding news producers from MBFC, we also check for\nthe producers that both Buzzfeed and MBFC share in common, however, these\ntwo have a disagreement over labeling some of the producers. Considering this\nambiguity, we decided to collect the articles but not to include them in the\ncorpus. One can \ufb01nd the list of these news producers which have di\ufb00erent\nlabels in BuzzFeed and MBFC in the Table 3.4.\n8\nCHAPTER 3. ARTICLE COLLECTION\nNews Producer MBFC BuzzFeed\nconsortiumnews.com least left\namgreatness.com rightcenter right\nrare.us rightcenter right\ntheamericanconservative.com rightcenter right\nwashingtonexaminer.com rightcenter right\nliberalmountain.com leftcenter left\nbillmoyers.com leftcenter left\nmintpressnews.com leftcenter left\nsecondnexus.com leftcenter left\nTable 3.4: MFBC and BuzzFeed disagreement over bias label\n3.1.4 NewsIR16 Corpus\nThe Signal Media One-Million News Articles Dataset2was released by Signal\nMedia to assist research on news retrieval. The corpus is in JSON format and\neach article has the following \ufb01elds:\n\u2022id: a unique identi\ufb01er for the article\n\u2022title: the title of the article\n\u2022content: the textual content of the article (may occasionally contain\nHTML and JavaScript content)\n\u2022source: the name of the article source (e.g. Reuters)\n\u2022published: the publication date of the article\n\u2022media-type: either \"News\" or \"Blog\"\nFor several apparent reasons, we decided not to use this corpus. Firstly,\nit only covers news from September 2015 that make this corpus too speci\ufb01c.\nSecondly, it contains news articles from a huge variety of publishers around the\nworld, and Thirdly, it contains not only political news, which does not serve\nour task to collect the articles relevant to US politics.\n2http://research.signalmedia.co/newsir16/signal-dataset.html\n9\nCHAPTER 3. ARTICLE COLLECTION\n3.2 Article URL Collection\nThenextstageafterdiscoveringthenewsproducersisto\ufb01ndURLsofthenews\narticles. One option to \ufb01nd the URLs would be to use a crawler which starts\nfrom the homepage of the news websites and extracts all the internal links\nthat it \ufb01nds, then the crawler adds them to a queue and continues crawling all\nthe pages and add new links to the queue. However, using a crawler is time-\nconsuming and has its own technical di\ufb03culties. Therefore, we decided to go\nin another direction and to use a shortcut, we used the Facebook page and\nsitemap of the news producers. We assume that news producers which own a\nFacebook page, also share their news articles in that medium to in\ufb02uence more\npeople due to the engagement of social media in modern human life. Moreover,\nmost well-design and well-organized websites follow a special standard of using\nsitemap which contains all the links that one can \ufb01nd in a website.\n3.2.1 Facebook Pages\nFacebook is a social networking website which was created in 2004 and since\nthen has gained tremendous growth and popularity over time. Due to the\npopularity of this platform, most of the news producers tried to absorb a new\naudience as well as keep the current ones by using this medium. As a result,\na news producer not only publishes the news on their website, but also they\nshare a link to their new articles on Facebook as well.\nFacebook Graph API\nBy using the so-called Graph API3one can access user-related data program-\nmatically. To collect data from Facebook pages we focused on three entities\nnamely, post, comments, and reactions. Basically, we went through the list of\nFacebook pages that we \ufb01nd in the BuzzFeed list, then we retrieve all the posts\nthattheypublished. Thenextstepwastocollectreactionsandcommentsfrom\nthe users on each post, which we thought would be useful for other types of\nresearch and studies. The information that we collected for each entity can be\nfound in the table 3.5.\nWe collected the data from the Graph API and stored post, comments,\nand reactions in a relational database. However, since the process of retrieving\ncomments and reactions was so time-consuming, we decided to deactivate that\nfeature for this thesis.\n3https://developers.facebook.com/docs/graph-api/\n10\nCHAPTER 3. ARTICLE COLLECTION\nPost\nPost ID\nContent\nLink\nCreation Time\nPage IDComment\nComment ID\nContent\nCreation Time\nUser ID\nUsername\nPost IDReaction\nReaction ID\nUsername\nType\nPost ID\nTable 3.5: Facebook entities accessible through the Graph API\n3.2.2 Sitemap\nA sitemap is a systematic and hierarchical view of the website in an XML\nformat. The sitemap protocol was introduced by Google4to standardize and\nuniformexistingapproaches. Thisprotocolisquitehelpfulandpracticalspecif-\nically when it comes to those links that needed a user interaction on a form to\ngenerate the links. Considering all the bene\ufb01ts of sitemap protocol, we used\nthis method to \ufb01nd articles\u2019 links for all the 109 news producers that we found\nin the Media Bias/Fact Check. We also used a sitemap for 107 news producers\nfrom the BuzzFeed list to compensate for a prevalence of right-wing articles.\nSitemap Link Extractor\nIn order to develop a tool to extract the links from a sitemap, one needs to be\naware of the standard structure of this protocol.\nIn principle, there are two main types of XML sitemaps which are\nURL sitemap and Index sitemap.\n\u000fThe URL sitemap contains the URLs of the website. These \ufb01les usually\nare in XML format and in some cases for archiving purposes, publishers\nstored them in a compressed format of xml.gz.\n\u000fThe Index sitemap contains a list of all the URL sitemap of that website.\nBy going through the URL sitemap structure, three more categories\nof URL sitemap can be found.\n\u000fsitemap for web pages which has all the information regarding the links\nin a webpage.\n4https://www.google.com/sitemaps/protocol.html\n11\nCHAPTER 3. ARTICLE COLLECTION\n\u000fImage sitemap which contains images detail and their URL in a website.\n\u000fVideo sitemap that includes information about the videos and their cor-\nresponding URL.\nBy considering all the aforementioned information in mind, the tools that we\nneeded to use for extracting the URLs should support all of these details.\nMoreover, for all of the news producers that we have, we need to collect a lot\nof extra information as follows:\n1. URL.\n2. Is it index sitemap?\n3. Is it compressed?\n4. Does the sitemap show the category of the news articles such that one\ncan directly use only articles from the political section?\nThe \ufb01rst three items in the lists are obligatory information that is needed\nto have access to a sitemap. In addition, the last parameter which is not\nmandatory gives us an ability to \ufb01lter URLs, due to the factor that we only\nwant political articles and some website also have other subjects to report.\nFor \ufb01ltering the URLs there are several methods. First, we can use a speci\ufb01c\nkeyword as a \ufb01lter in a sitemap index, and it will only consider sitemap URLs\nthat contain that keyword which can be seen in \ufb01gure 3.1. Second, a special\nkeyword can be used in order to restrict the URLs in a URL sitemap and\nFigure 3.2 is an example. Finally, the last parameter that can be used in order\nto limit the URLs is using a pair of tag and value which for instance can be\nseen in \ufb01gure 3.3.\nOur implementation of extracting URLs with features that were described\ncan be simply used by running the following command.\n$ python extract_urls.py --url \"http://site.com/sitemap.xml\"\n--domain \"site.com\" --siteid \"siteid\" [--not_index] [--gzip]\n[--sitemap_filter \"keyword\"] [--url_filter \"keyword\"]\n[--tag_value_filter \"tag*value\"]\nCommand Parameters:\n\u000f\u2013url: URL of the sitemap\n\u000f\u2013domain: Host of the website\n12\nCHAPTER 3. ARTICLE COLLECTION\nFigure 3.1: A sample for \ufb01nding a keyword to \ufb01lter URLs in a index sitemap\nFigure 3.2: A sample for \ufb01nding a keyword to \ufb01lter URLs in a URL sitemap\n\u000f\u2013siteid: Identi\ufb01cation number for the website\n\u000f\u2013not_index: This parameter is optional and is needed when URL pa-\nrameters is not referring to the index sitemap\n\u000f\u2013gzip: This parameter should be used only in the situation that sitemap\nURL is in compressed format\n\u000f\u2013sitemap_\ufb01lter: This parameter can be used, if a user wants to apply\n\ufb01lter on the list of URL sitemap in an index sitemap\n\u000f\u2013url_\ufb01lter: This parameter is useful for \ufb01ltering URLs in a URL sitemap\n\u000f\u2013tag_value_\ufb01lter: To apply \ufb01lter on the metadata in a URL sitemap\nusing a key and value, users can use this parameter\n3.2.3 Link Statistics\nInthissection, wefocusonabasicanalysisofdatathatwecollectusingourtwo\nmajor techniques to \ufb01nd articles\u2019 links from news producers. Considering the\n13\nCHAPTER 3. ARTICLE COLLECTION\nFigure 3.3: A sample for choosing a tag and value to \ufb01lter data in a URL sitemap\nfact that for BuzzFeed producers we apply both techniques and for MBFC we\nonly used the sitemap approach, these statistics will be introduced separately\nin two subsections.\nBuzzFeed producers\nIn the process of link extraction from the news producers that we found on\nBuzzFeed websites, 458 news producers were involved. We managed to extract\nlinks from the Facebook pages for 351 news producers. Furthermore, for the\nother 107 producers, we used sitemaps to discover the links.\nFacebook Pages\nBy using Facebook Graph API, we were able to \ufb01nd 1,239,955 posts for\nboth liberal and conservative news producers. During this process, we found\nout that not all the links belong to those news producers, since in lots of cases\nthey have shared contents from other sources which we called them third-party\nsources in this thesis. This incident happened especially when the content\nsuits their political perspective, and as you can see in table 3.6, approximately\ntwo-thirds of the posts are not produced originally by the actual publishers.\nHowever, this proportion is slightly di\ufb00erent for left and right wing parties. the\nactual publisher produced 37 percent of the content shared on Facebook pages\nof left-wing pages; moreover, 67 percent of news shared by right-wing websites\nare shared from third-party news producers. Finally, we only considered the\noriginal articles from news producers, which amounts to 423,764 news articles;\nthe share of the left party is 169,091 articles, and 254,673 news articles are the\noriginal ones for conservative producers. In \ufb01gure 3.4, the proportion of the\nAll news articles to Original news articles can be found, in which the blue color\nrepresents all the posts including third-party sources and orange color is used\nfor original article; It also shows all the producers that we have investigated\n14\nCHAPTER 3. ARTICLE COLLECTION\nwith Facebook graph API approach.\nAbsolute Average per site\nNumber of articles Left Right Total Left Right Total\nThird-Party 288,552 527,639 816,191 2,327 2,162 2,217\nOriginal 169,091 254,673 423,764 1,409 1,102 1,207\nTotal 457,643 782,312 1,239,955 3,690 3,206 3,369\nTable 3.6: Article statistics from the BuzzFeed list using Facebook Graph API\nFigure 3.4: Number of all news article to the original ones per site\nWealsowentabitfurtherinthisanalysis, aswewantedtobewell-informed\nabout the following facts: Which third-party sources were referenced more by\nour producers? How often did our producers reference one another? For\ninstance, \ufb01gure 3.5a indicates how often news producers have shared content\nfrom the other producers in our list, and \ufb01gure 3.5b shows the list of top 10\nnews producers that produced the most viral news articles. As one can see,\nwesternjournalism.com with a huge margin is at the top of the list and it seems\nthattheyproducedmorecontentthatisworthsharingbytheothers. Theother\nsigni\ufb01cant detail about this list is that 90 percent of it consists of conservative\nwebsites and the only liberal producer is newscorpse.com. From this statistics,\nwe can conclude that conservative websites have a more organized network to\nshare one another materials.\n15\nCHAPTER 3. ARTICLE COLLECTION\n(a)BuzzFeed producers shared by other\nBuzzFeed producers\n(b)Top 10 BuzzFeed list news producers\nreferenced by the other BuzzFeed produc-\ners\nFigure 3.5: Statistics of publishers being referenced by other news producers in the\nBuzzFeed list\nFigure 3.6 displays the top 10 third-party sources that their content was\nshared by the producers on our list. Moreover, as is obvious from the list, there\nis no partisan news producer on the list that we missed to include in this study.\nIn addition, it also shows, links to the Facebook pages and Youtube5videos,\nand also services to shorten URLs are very common to use in the Facebook\u2019s\nposts.\nFigure 3.6: Top 10 third-party sources that produced content which is shared by\nthe BuzzFeed news producers list\nSitemaps\nTo continue the links extraction process, we applied the sitemap approach\nto discover links to news articles for 107 BuzzFeed news producers; we only\n5http://youtube.com\n16\nCHAPTER 3. ARTICLE COLLECTION\ntriedtoapplythisapproachonthenewsproducersthatdidn\u2019thaveaFacebook\npage or we could not \ufb01nd any article for them using Facebook Graph API.\nTable 3.7 shows, 972,866 articles were found during this process. However,\nsome extra \ufb01ltering over the obtained data was required, since there were\na great number of links, including tags, authors homepages, and categories,\nwhich do not point to a news article. Finally, we found 579,208 links, 70,279\nlinks from the left-wing party and the other 508,929 from the right-wing party.\nOn average for each site, we discovered 5,413 articles. Moreover, if we go\ninto more detail, the average number of liberal articles per site is 3,904 and\nconservative producers have 5,718 articles on average.\nAbsolute Average per site\nNumber of articles Left Right Total Left Right Total\nOriginal 70,279 508,929 579,208 3,904 5,718 5,413\nTotal 101,273 871,593 972,866 5,626 9,793 9,092\nTable 3.7: Article statistics of the BuzzFeed list using sitemap approach\nMedia Bias/Fact Check producers\nIn addition to the articles obtained through the BuzzFeed list, we obtained\n3,191,915 articles from 109 news producers of Media Bias/Fact Check. As\nTable 3.8 illustrates Left-center biased producers have 1,392,187 articles and\non average each producer possesses 28,411 news articles. The share of least\nbias and right-center producers was 750,872 and 420,218 news articles. Least\nbias websites have on average 26,816 pieces of news, and right-center ones have\n29,283 articles. Finally, out of 12 news producers that we found in the MBFC\nowing to the reason that we didn\u2019t have enough articles for the left wing party,\nwe discovered 628,638 links in general which become 52,386 article on average\nfor each producer.\nFinal Statistics\nAfter article collection the corpus contains approximately 4.2 million URLs.\nTable 3.9 shows the number of URLs that are attempted to be archived in\nChapter 4.\n17\nCHAPTER 3. ARTICLE COLLECTION\nAbsolute\nNumber of articles left Left-center Least Right-center Total\nTotal 628,638 1,392,187 750,872 420,218 3,191,915\nAverage per site\nNumber of articles left Left-center Least Right-center Total\nAverage 52,386 28,411 26,816 29,283 26,425\nTable 3.8: Article statistics found from MBFC news producers\nAbsolute\nNumber of articles left Leftcenter Least Rightcenter Right Total\nBuzzFeed Facebook 169,091 - - - 254,673 423,764\nBuzzFeed Sitemap 70,279 - - - 508,929 579,208\nMBFC Sitemap 628,638 1,392,187 750,872 420,218 - 3,191,915\nTotal 868,008 1,392,187 750,872 420,218 763,602 4,194,887\nTable 3.9: Corpus statistics after article collection\n18\nChapter 4\nCorpus Construction\nIn this chapter, the process of creating the corpus is described in detail. It\nincludes several steps to deal with di\ufb00erent obstacles that we encountered, and\nalso our solution to solve each problem. After the article collection phase, we\nhad 4,194,887 news articles to crawl for the further use in this study. Accord-\ningly, weneedareliablemethodtobothcrawlandstorethearticles. Moreover,\nwe need to \ufb01nd the main content in the articles and store all the important\n\ufb01elds of the articles in the formats that we want to construct the corpus.\n4.1 Archiving\nWe want to store all of the web pages that we found during article collection\nin our storage owing to the reason that a lot of web pages are disappearing\nfrom the Internet. Accordingly, we want to archive them in a way that we can\naccess the web page as we have access to them when they are still available\nand online. For this purpose, the most signi\ufb01cant concern for us is the quality\nof the archived page and having access to every part of a web page from the\ntext to the media that producers used in their articles.\n4.1.1 Tool\nThere are several ways to crawl a web page from the Internet and which one\nto choose absolutely depends on the kind of content that one wants to archive.\nHere we brie\ufb02y describe the four major approaches1to archive a web page,\nand moreover the reason that we have chosen one to archive our web pages.\n1https://www.labnol.org/internet/archive-web-pages/20192/\n19\nCHAPTER 4. CORPUS CONSTRUCTION\nArchiving Methods\nText Content\nTherearemanyapplicationandbrowserextensionslikeEvernote2andOneNote3\nwhich help users to save the text of a webpage and make it available to access\nthe content through the application.\nPackaged\nPackaged archiving is a simple way to have access to the content of a web\npage without a demand for an additional extension. Storing web pages in a\nPDF format is one example and PDF writers exist in the features of modern\nbrowsers.\nLocal Copy\nAll the modern browsers have a feature to store a web page entirely in the\nlocal machine. Also tools like wget is a good example to save a URL, however,\nit stores nothing more than the HTML content.\nWeb ARChive\nInternet archives like http://Archive.org/web/ and http://Archive.is/\narewebsitesthatstoreanarchivedversionofaURLincludingalltheJavascript\n\ufb01les, and all the other assets in a Warc format. It is possible to download a\nWarc \ufb01le from Archive.is, but the problem is that it doesn\u2019t have all the pages\nthat users are looking for.\nChoosing the Method\nConsidering the brief description that is mentioned above, one can see the\nlatest method provides us with the highest quality of archiving a page, due\nto the fact that we can store an entire page and access content in the way\nthat it originally existed on the Internet. In addition, we already have a well-\nimplemented archiving tool in the Webis group[2018].\nWebis Web Archiver\nUsing simple methods like just crawling the HTML content of a page is not\nenough to have exact results as we normally see in the actual page. This\nhappens for several reasons, for instance, web pages have so many external\nresources like CSS \ufb01les or JavaScript \ufb01les and also multimedia \ufb01les which do\nnot exist in the HTML page. In addition, in some cases, the web page needs\n2https://evernote.com/\n3https://www.onenote.com/\n20\nCHAPTER 4. CORPUS CONSTRUCTION\nto load the content by calling client-side scripts. As a result, we needed to\n\ufb01nd a way to solve all of these issues. Therefore, we employed Webis Web\nArchiver[2018] as a required tool which simulates modern browsers\u2019 behavior\nto create a standard WARC web archive. The WARC \ufb01les which are produced\nby Webis Web Archiver are compatible with the other third-party tools. This\ntool has also introduced another vital feature to render and reproduce a web\npage using its WARC \ufb01le. Moreover, archiving and reproduction can be done\nas follows.\n$./archive.sh --url \"url\" --scriptsdirectory \"directory\" --script\n\"script\" --output \"output\"\n$./reproduce.sh --url \"url\" --archive \"directory\" --scriptsdirectory\n\"directory\" --script \"script\" --output \"output\"\nParameter De\ufb01nition:\n\u2022\u2013url:The URL of the web page that the user wants to archive or repro-\nduce.\n\u2022\u2013scriptsdirectory: The path to the directory where the user stores user\nsimulation scripts\n\u2022\u2013script: The name of the script class.\n\u2022\u2013output: Path to the directory where the user wants to store the result.\n\u2022\u2013archive: Path to the WARC \ufb01le that the user want to reproduce.\n4.1.2 Checking The Producers\nIn our list, we have a huge number of news producers and news article to crawl\nand archive. To begin with, we wanted to make sure our archiving software\nworks properly, also we wanted the output to be satisfying and in the way\nthat we expected. Therefore, from each news producer, we randomly chose 3\nsamples to crawl. Then, after \ufb01nishing the crawling process, we investigate\nthe output. Table 4.1 displays di\ufb00erent types of issues that we faced during\nthis process.\nProblem De\ufb01nition:\n\u2022Modal Windows: In 140 of websites, we have extra windows open on\nthe top of the news articles. For instance, modal windows for newsletter\napplication, petition, and advertisement have occurred the most often.\nFigure 4.1 shows four examples of modal windows.\n21\nCHAPTER 4. CORPUS CONSTRUCTION\nProblem Number Of Sites\nModal Windows 140\nDown 68\nShort URLs 7\nShare Others 5\nRead More 3\nHomepage 3\nCaptcha 2\nTable 4.1: Archiving Errors\n\u2022Down:After crawling these samples from each news producer, we real-\nized 68 of them are not available anymore. For some instances, the server\nwas out of order and down and also for some others even the domain was\nfor sale.\n\u2022Short URLs: There are 7 websites in the list in which all the URLs\nthat we collected are in short format.\n\u2022Share Others: 5 websites in the list only share news from other pro-\nducers.\n\u2022Read More: In 3 websites when a user sends a request for a news\narticle, the article does not load completely and the user still needs to\nclick on read more button to access the entire article. Figure 4.2 displays\none example of this kind.\n\u2022Homepage: All the links that we collected from 3 websites are pointed\nto the homepage of the website.\n\u2022Captcha: Only in two samples, we encountered a request for a captcha.\nSolution:\nOut of 220 problematic news producers, we found a solution for two groups\nof problems namely, modal windows, and read more buttons. For all of these\nadditional windows, there is a close button that should be pressed to remove\nthe windows from the screen. In addition, for read more button it is absolutely\nclear that users have to click on the button. Consequently, we decided to\nsimulate user interaction during the archiving process. To do so, we load a\npage, the next step is to wait for the scrolling action to be \ufb01nished, then\nwe simulate the user click on necessary buttons. For this purpose, we run\n22\nCHAPTER 4. CORPUS CONSTRUCTION\njavascript codes. All of these 143 producers have their speci\ufb01c layout and they\nare totally di\ufb00erent from one another. Therefore, in this stage, we rechecked\nall of these websites and write down the necessary javascript codes to do the\nrequired action. Finally, in the code according to the requested host, we can\nsimply decide which ones to execute. In the following code, you can see two\nexamples for two di\ufb00erent websites http://americasfreedomfighters.com/\nandhttp://angrypatriotmovement.com/ , and their required javascript code\nto close the modal windows.\ndomain = uri.getHost();\nswitch (domain) {\ncase \"americasfreedomfighters.com\":\ncode = \"document.querySelector( '#modaal-close ').click();\";\nbreak;\ncase \"angrypatriotmovement.com\":\ncode = \"document.querySelector( '#revexitcloseme ').click();\";\nbreak;\n}\nFigure 4.1: Modal Windows Examples\n4.1.3 Archive Quality Assurance\nArchive quality check is one of the important steps of creating a high-quality\ncorpus. Due to the reason that we had a huge number of web pages to crawl,\n23\nCHAPTER 4. CORPUS CONSTRUCTION\nFigure 4.2: Read More Example\nwe extremely felt the need of a feature to make sure that the archiving process\nhad worked absolutely \ufb01ne. To do this, we employ a feature of archiving\ntool to perform this action. This method[2018], utilizes machine learning in\norder to predict the quality by comparison features that are extracted from the\narchive and reproduction screenshots. To label the data for the classi\ufb01cation\nstage, human annotators evaluate 6,386 of the 10,000 pages in the Webis Web\nArchive 17. The annotators score the similarity of the archive screenshots and\nreproduction in the rage of 1 to 5; Score of 1 means part of the page are just\nmoved up or down a bit and 5 means that main content is missing and the page\nis not usable. In addition, Deep convolution neural networks are the machine\nlearning method that has been applied to train the classi\ufb01er. To extract the\nfeatures, the \ufb01rst step is to crop the images to 4098 pixels, then they indicate\ntheimageisconvertedtograyscale. Thenextstepistoscaledowntheimageto\n384x128 pixels. Then, for this size, to match the receptive \ufb01eld of the neurons\nin the constitutional layers of the network, icons with the usual width of 32\npixels are scaled down to a width of 3 pixels. Finally, using the aforementioned\nmodel and required code, we are able to predict the similarity between a pair\nof the archive and reproduction screenshots.\nArchive Quality Check Of Producer Samples\nTo make sure that we can archive articles from all the news producers in a\nproper manner, we decided to randomly choose 100 samples from each news\nproducer. Then, we archived and reproduced all of them. We performed this\noperation to realize whether we are able to successfully crawl all the producers\n24\nCHAPTER 4. CORPUS CONSTRUCTION\nand make sure that the data we have collected is reliable. As you can see in\ntable4.2, out of 475 news producers we have 450 producers with at least one\nsuccessfulcrawl. Inaddition, 38,409newsarticlesoutof44,806aresuccessfully\narchived. Moreover, we used the collected data and evaluate them manually\nand automatically. The other important \ufb01nding is that 25 websites do not\nhave any successful crawl, out of which 14 of them faced problems during the\narchiving process, and the other 11 websites are unavailable and their server\nis down.\nSites / Article Number\nTotal Sites 475\nSites With At Least One Successful Crawl 450\nTotal Number Of Articles 44,806\nNumber Of Successful Archived Article 38,309\nTable 4.2: Archived samples statistics\nAutomatic\nFor this purpose, we collected all the screenshots in two folders. For each\narticle, wehaveoneimageinthearchivefolderandanotheroneinreproduction\nfolder with the exact same name. Then we need to run the following command\nto predict the result.\n$ ./evaluate.sh --imagesa archive --imagesb reproduction --output prediction\nParameters De\ufb01nition:\n\u2022\u2013imagesa: Path of the folder that contains archived images.\n\u2022\u2013imagesb: Path of the folder that contains reproduction images.\n\u2022\u2013output: Path of a \ufb01le to store the output.\nFigure 4.3 displays, almost 96 percent of the samples are categorized with\na score of 1 or 2, which means they are mostly similar with some minor di\ufb00er-\nences. A score of 3 means small changes, for example, the comment section of\nreproduction image is missing and 783 articles are in this category. Category 4\nand 5 mean striking di\ufb00erence and unusable page which contains 1,651 articles.\n25\nCHAPTER 4. CORPUS CONSTRUCTION\nFigure 4.3: Automatic archive quality check scores\nIn this phase, we also measure the mean archive quality check the score for\neach news producer. Figure4.4 demonstrates the result of this study. As one\ncan see, only 2 news producers have a mean score of 4 or 5 and 115 number\nof producers produce identical reproduction image. In addition 333 producers\nalso produce reliable archive and reproduction output.\nFigure 4.4: Median archive quality score per site\nIn \ufb01gures 4.5a and 4.5b two samples of failed and successful archived and\nreproduced samples are shown.\nManual\nTo double check the result, we decided to randomly choose 10 instances from\nthe samples that we have already archived and reproduced. We combined\n26\nCHAPTER 4. CORPUS CONSTRUCTION\n(a)Score 5\n (b)Score 1\nFigure 4.5: Automatic archive quality samples\neach pair of images, side by side in one image. Then we manually checked\nthe images. During this process we realized, there are several cases where the\ncrawled pages are unusable even though the images are the same and archive\nquality check put them in the \ufb01rst category.\nProblematic Sites Statistics\nIn table 4.3 you can observe, statistics about the number of sites and di\ufb00erent\nerror that we found when we inspect archive quality check result. For all the\nissues that are mentioned in the table we only \ufb01gure out a way to solve modal\nwindows problem and for the rest of errors, we just ignored the news producers\nin further phases of the thesis.\nProblem De\ufb01nition:\n\u2022ModalWindows: In33ofthewebsites, westillhadthemodalwindows\nproblem, which their Javascript code was \ufb01xed to solve the issue.\n\u2022Park Domain: For 25 news producers, the result page shows that the\ndomain is for sale. Therefore, the archive quality check works quite well\nbut the crawled page was not what we asked for.\n27\nCHAPTER 4. CORPUS CONSTRUCTION\nProblem Number Of Sites\nModal Windows 33\nPark Domain 25\nReproduction Problem 8\nGateway Error 3\nArchiving Problem 2\nCaptcha 1\nSitemap Issue 1\nTable 4.3: Problematic news producers found by archive quality assurance\n\u2022ReproductionProblem: For8newsproducers,wewereabletoarchive\nthe page, however, we faced a problem during the reproduction process,\nwhich means for these instances archiving does not work properly.\n\u2022Gateway Error: In output \ufb01les of 3 producers, we found out it gives\nus gateway error during the archiving process. Also, in this case, archive\nquality categorized them in the \ufb01rst category but the crawled page was\nproblematic.\n\u2022Captcha: Onlyinonesample,weencounteredwitharequestforcaptcha.\nWe have the same images in the archive and reproduction process but\nwe cannot use the page.\n\u2022Sitemap Issue: Finally, one of the news producers has a corrupted\nsitemap \ufb01le; all the links in the sitemap pointed to the homepage of the\nwebsite!\n4.2 Distributed Storage\nBy considering the fact that the process of archiving all web pages is so time-\nconsuming, it was essential for us to store the data in a reliable storage, while\nwe did not want to repeat the process owing to unexpected hardware failures.\nTherefore, we decided to make use of Webis Betaweb cluster4.\n4https://www.uni-weimar.de/en/media/chairs/computer-science-department/\nwebis/facilities/\n28\nCHAPTER 4. CORPUS CONSTRUCTION\n4.2.1 Hadoop Distributed File System\nTheHadoopDistributedFileSystem(HDFS)[2009]5isadistributed\ufb01lesystem\nwhich is designed to store very large data sets reliably. HDFS is highly fault-\ntolerant and is designed to be deployed on low-cost hardware.\n4.2.2 MapFile\nSince we have produced a huge number of \ufb01les in the archiving process, we did\nnot want to copy all the \ufb01les directly into HDFS. Therefore, we utilize a format\nto store \ufb01les which is more convenient for HDFS for both storage and future\ncomputation. MapFile6is the format that we used. MapFile is directory which\ncontains two SequenceFile7data \ufb01le and index \ufb01le. SequenceFile is a persistent\ndata structure for storing pairs of binary key and values, which are append-\nonly and keys are not removable and editable. Data \ufb01le in a map\ufb01le format\nhave all keys and values records. In addition, index \ufb01le holds information\nabout each key and starting byte position of the records which adds an extra\nfeature to the data \ufb01le. This index \ufb01le works as a lookup \ufb01le, and due to the\nsmall size, it can simply \ufb01t in the memory.\n4.2.3 MapFile Creator\nTo create map\ufb01les, we implement Java program which gets a list of the \ufb01les as\nan input and converts all the \ufb01les to a single map\ufb01le. In this implementation,\nwe put the path of the main directory of each article into the input \ufb01le. In ad-\ndition, for each entry, we add 5 \ufb01les into the map\ufb01le namely, archive.html,\narchive.png, archive.warc.gz, reproduction.html, and reproduction.png. To\ngenerate the keys, we used a combination of article id, URL, and \ufb01le name,\nwhich are separated by space. Our implementation of map\ufb01le creator can be\nused as follows:\n$ java -jar ConvertToMapFile.jar InputFile OutPutDirectory\nParameter De\ufb01nition:\n\u2022Input\ufb01le: List of the root directories of archived articles.\n\u2022OutPutDirectory: A path to store the map\ufb01le.\n5https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\n6https://wiki.apache.org/hadoop/io\n7https://wiki.apache.org/hadoop/SequenceFile\n29\nCHAPTER 4. CORPUS CONSTRUCTION\n4.3 Distributed Archiving\nAfter observing the result of archive quality check, we ignored several prob-\nlematic news producers; 3,651,229 news articles were left for us to crawl. By\nusing a single machine, we could approximately crawl 600 pages per day; so,\nwith a simple calculation, it would take us about 6,000 days to crawl all pages.\nTo overcome this issue and speed up the crawling, we employed 80 machines\nfrom Betaweb cluster. In each machine, we had a list of approximately 45,000\narticles to crawl.\n4.3.1 Task Control Automation\nDue to the reason that manual control of all the 80 machine was so time-\nconsuming and also required a lot of e\ufb00ort, we create a crawling baseline to\nautomatically perform the archiving, quality check, and also map\ufb01le creation.\nArchiving and Reproduction\nIn this part, on each machine, we have a list of URLs to archive. Moreover, we\nwanted to have three parallel threads on each machine to crawl three di\ufb00erent\npages simultaneously. To do so, we employ a task manager from Webis Com-\nmands. This command uses a text \ufb01le as a list of task parameter as an input,\nwhich has to be named tasks.txt. Also, it requires an executable \ufb01le next to\nthe tasks.txt \ufb01le, and this \ufb01le has to be named task. Then, you should run the\nfollowing commands as many times as you want to have simultaneous threads.\n$ webis util task work\nDuring the crawling process, we realized there are some cases in which these\nthreads become suspended. To solve this issue, we needed to reset the task\nmanager. Therefore, we used the algorithm 1 to \ufb01x this problem. In this\nalgorithm, wecheckthe numberof successful andfailed tasks every 15minutes.\nThen we check if numbers did not change from the last time that we had\n30\nCHAPTER 4. CORPUS CONSTRUCTION\nchecked, we reset the task manager.\nwhilesuccessfull tasks + failed tasks != total number of tasks do\nGet number of current successful tasks;\nGet number of current failed tasks;\nifcurrent failed tasks = old failed tasks\nAND current successful task = old successful task > then\nRestart the task server;\nelse\nold successful tasks = current successful tasks;\nold failed tasks = current failed tasks;\nWait for 15 minutes;\nend\nend\nAlgorithm 1: Algorithm to reset suspended threads\nArchive Quality\nArchive Quality is employed to check if the archiving process works reliably.\nTo do this, we use algorithm 2 which works on a daily basis. It \ufb01nds the news\narchived instances, then it copies all the archive and reproduction images in\ntwo separate directories. The next step is to run the archive quality checker\nover these images.\nwhileArchiving task is still running do\nCopy archive and reproduction images in two di\ufb00erent folders;\nRun archive quality check;\nWait for 24 hours;\nend\nAlgorithm 2: Daily Archive Quality Check\nMapFiles Creation\nCreating map\ufb01les and also copy them to HDFS, was another task that should\nbehandledautomatically. Toperformthis, we\ufb01ndallthesuccessfullyarchived\ncrawls which are also con\ufb01rmed by archive quality checker. Then, we create\na list of this instances, and every 1,000 items of the list will convert to a\nmap\ufb01le. In the end, this algorithm copies all the generated map\ufb01les to HDFS.\n31\nCHAPTER 4. CORPUS CONSTRUCTION\nwhileArchiving task is still running do\nRecognize news successfully \ufb01nished tasks;\nCreate lists of maximum 1000 entries;\nConvert last step lists to map\ufb01les;\nCopy the map\ufb01les to HDFS;\nend\nAlgorithm 3: Daily map\ufb01le creation\n4.4 Main Content Extraction\nAfter crawling all the articles, the main task is to \ufb01nd a way to extract the\nmain part of the news articles; we considered the following entities as the most\nimportant part of an article.\nArticle Fields\n\u2022ID:Article unique ID.\n\u2022Headline: Title of the article.\n\u2022Content: Main content of article.\n\u2022Author: The person who wrote the article.\n\u2022Date:The date of publication.\n\u2022URL:URL of the article.\n\u2022Orientation: Article biased label.\n\u2022Publisher: The news producer that published the article.\nSeveral items of article \ufb01elds\u2019 list are the information we already had at that\nstage such as ID, URL, Orientation, and Publisher. Moreover, the rest of the\n\ufb01elds are information that we need to extract from the crawled news articles.\n4.4.1 Writing the Wrappers\nIn this stage, we checked all the news producers that at least had one successful\ncrawl and the crawl page also passed the archive quality check; the number\namounts to 383 news producers. Each of these producers in the list has their\nspeci\ufb01c web layout, and as a result, it was impossible to develop a general\nwrapper which is able to extract the required \ufb01elds from this variety of layouts.\nConsequently, we were left with no choice other than going through the layout\n32\nCHAPTER 4. CORPUS CONSTRUCTION\nof each of these websites and writing their own special query selector to \ufb01nd\nrequired information for each \ufb01eld. the following code is an example of writing\na wrapper for http://leftvoice.org/ and its required query selectors to\nextract the information from the HTML article.\nwrappers.addWrapper(new UriSpecificWrapper(\n// This wrapper will only be used for leftvoice.org\n// optional second parameter for path matching\nnew UriPredicate(\"leftvoice.org\"),\n// This wrapper\nnew SelectorBasedWrapper()\n.withTag(Wrapper.TAG_TITLE, \"article div.header-articulo h1\")\n.withTag(Wrapper.TAG_AUTHOR, \"article div.autor-articulo a\")\n.withTag(Wrapper.TAG_DATE, \"article div.row:nth-of-type(2)\ndiv.col-md-12 span\")\n.withTag(Wrapper.TAG_CONTENT, new String[] {\n\"article div.articulo p\",\n})));\nImplementation\nWe stored all the crawled news articles into map\ufb01les and as mentioned be-\nfore we copy the map\ufb01les on HDFS. Accordingly, we found MapReduce[2008]8\nas the most convenient programming model to implement our application.\nMapReduce is a programming paradigm which is designed to process a large\namount of data on the cluster in parallel and distributed algorithm. Our im-\nplementation of content extraction gets a list of map\ufb01les as input, then it uses\nall map\ufb01le \ufb01les as an input of the mapper application. The mapper distributes\nthe records of keys and values to executors. Each executer checks the key and\nif this record contains the \"page.html\" \ufb01le, it starts to perform the content ex-\ntraction. To do so, it \ufb01nds the domain of the article, and it uses HTMLReader\nlibrary to \ufb01nd the needed HTML elements by using domain and HTML con-\ntent of the article. For all the elements including title and author we only keep\nthe plain text without any extra and unnecessary HTML tags. Moreover, for\ncontent and date, we obey the following standard.\nContent\nStoring content needed more thought to realize which HTML tags are helpful\nand necessary to keep for the further analysis. Finally, we decided to keep\nparagraph, quote, and anchor elements from HTML code and remove all the\n8https://en.wikipedia.org/wiki/MapReduce\n33\nCHAPTER 4. CORPUS CONSTRUCTION\nother tags. In addition, for anchor elements, we considered extra criteria and\nwe added an extra property to anchor as internal or external.\n\u2022Internal: When the anchor link pointed to the news producer that\npublished the article, we consider that as internal and then we removed\nthe \"href\" property from the anchor element.\n\u2022External: We consider an anchor element external when its link pointed\nto other websites. In this case, we also kept the \"href\" property for that\nelement.\nDate\nWe have a great variety of formats of date, while each of these news producers\nhas their own speci\ufb01c way to display the publication date. Therefore, we\nrequired a method in order to recognize the date from text regardless of the\nformat. Natty9is the library that we employed to extract the date from a text.\nFinally, we stored the data in ISO 860110which 2000-02-20 is an example.\n4.5 Corpus Formatting\nWe produce the corpus in two di\ufb00erent formats JSON11lines and XML12. We\nmostly used the JSON line format for analysis which is described in more\ndetail in section 5. Furthermore, XML format was also produced to present\nthe corpus to participants of our task in SemEval 201913.\nField Description:\n\u2022id:Article id which is a number with 7 digits.\n\u2022published-at: A date in ISO 8601 format in which a news producer\npublished the article.\n\u2022title:Headline of the article.\n\u2022content: The \ufb01eld content is used to store the main content of the\narticle.\n9http://natty.joestelmach.com/\n10https://en.wikipedia.org/wiki/ISO_8601\n11https://en.wikipedia.org/wiki/JSON\n12https://en.wikipedia.org/wiki/XML\n13https://pan.webis.de/semeval19/semeval19-web/\n34\nCHAPTER 4. CORPUS CONSTRUCTION\n\u2022bias:This\ufb01eldindicatestheorientationofanarticlewhichcanbe\"left\",\n\"left-center\", \"least\", \"right-center\", or \"right\".\n\u2022hyperpartisan: Hyperpartisan is a boolean value. It is \"true\" when\nbias value is \"left\" or \"right\", otherwise the value of this \ufb01eld is \"false\".\n\u2022url:In this \ufb01eld the URL of the article is stored.\n\u2022author: Name of the author of the article can be found in this \ufb01eld.\n4.5.1 JSON Line\nHere, one can see the structure of an article and the way we stored in it in\nJSON format. In the end, the corpus contains several text \ufb01les, and in each\nof these \ufb01les, each line represents an article as a JSON object.\n{\n\"id\":\"0000001\",\n\"published-at\":\"YYYY-MM-DD\",\n\"title\":\"Headline\",\n\"content\":\"Main Content\",\n\"hyperpartisan\":\"true\",\n\"bias\":\"right\",\n\"url\":\"URL\",\n\"author\":\"Author\"\n}\n4.5.2 XML\nWeusedtheXMLformattoproducethehyperpartisancorpusforourSemEval\n2019 task(Kiesel et al.[2018]). For each article in a corpus, we produce a\nseparate XML \ufb01le as you can see a sample below. In addition, for each article,\nwe add an article tag in ground truth \ufb01le which contains the properties id,\nhyperpartisan, bias, url, and labeled-by. The \ufb01eld labeled-by is only used for\nSemEval task and is \ufb01lled by value publisher for the all the articles that we\nhave collected in this thesis.\nArticle Instance:\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<articles>\n<article id=\"0000001\" published-at=\"YYYY-MM-DD\" title=\"HeadLine\">\nContent\n35\nCHAPTER 4. CORPUS CONSTRUCTION\n</article>\n</articles>\nGround Truth:\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<articles>\n<article id= '0000001 'hyperpartisan= 'true 'bias= 'right '\nurl= 'URL'labeled-by= 'publisher '/>\n<article id= '0000002 'hyperpartisan= 'true 'bias= 'least '\nurl= 'URL'labeled-by= 'publisher '/>\n</articles>\n36\nChapter 5\nAnalysis\nIn this chapter, we focused on an analysis of the corpus. We begin with basic\nstatistics about the articles, then we inspect the links that we found in the\ncorpus and we extract some statistics about the anchor elements inside the\ncorpus. The next step was to check the articles for text reuse. Finally, we\ntried several machine learning methods to predict news articles orientation\nand also biased from the unbiased news.\n5.1 Corpus Statistics\nIn this stage of the thesis, we extract basic statistics about the number of the\narticles in general, in addition to the number of articles per each bias category\nin the corpus. Moreover, we also calculate article length and by that we mean\nhow many words each article contains. Considering the length of the articles,\nwe were wondering how many articles we might potentially lose if we consider\na speci\ufb01c number for a minimum number of words per articles. Accordingly,\nwe choose 4 di\ufb00erent thresholds for this minimum length which are 2, 50, 100,\nand 150 words. According to the table 5.1 and 5.2, in total we have 2,282,423\nnews articles in the corpus. We then decided to stay with 50 words length as\nthe minimum threshold for the article length since 50 words is a reasonable\nlength for minimum length and also we do not lose so many articles. As a\nresult, in the \ufb01nal corpus, we have in total 1,493,601 articles with the average\nlength of 694 words, in which the share of the left category is 207,354, left-\ncenter 227,709, least bias 243,606, right-center 476,801, and 339,997 right wing\narticles.\nFigure 5.1 displays a list of top 20 producers that have the most and the\nleast number of articles in our corpus.\n37\nCHAPTER 5. ANALYSIS\nArticles leftLeftcenter Least Rightcenter Right Total\nTotal 311,511 837,422 551,673 335,552 442,461 2,478,620\nMin. 2 words 262,345 228,467 459,324 272,239 362,164 1,583,173\nMin. 50 words 253,311 214,598 423,675 262,078 334,413 1,488,075\nMin. 100 words 243,863 203,245 396,570 238,297 313,334 1,395,309\nMin. 150 words 232,717 189,475 365,429 212,733 290,031 1,289,785\nTable 5.1: Corpus statistics about number of articles with di\ufb00erent length\nAverage Length leftLeftcenter LeastRightcenter RightTotal\nMin. 2 words 867 549 593 750 570691\nMin. 50 words 893 580 637 775 612691\nMin. 100 words 924 608 676 844 648732\nMin. 150 words 965 643 723 930 690782\nTable 5.2: Corpus statistics about average number of words in the articles\n(a)Producers with the most articles\n (b)Producer with the least articles\nFigure 5.1: List of top 20 producers with the most and least articles\n38\nCHAPTER 5. ANALYSIS\n5.2 Corpus Anchor Element Analysis\nIn this section, we inspect the hyperpartisan corpus for anchor elements. As\nit was mentioned earlier in Corpus Construction Chapter 4, we categorized\nlinks into two categories, which are namely internal, and external. Internal\nlinks point to other articles from the same news producer and External links\npoint to the URLs outside of the news producer. Table 5.3 demonstrates\nstatistics about the number of links in the corpus in general, as well as the\ntotal number of links for each bias category. In total, we have 6,627,813 links\nand approximately 79 percent of all the links are external links. The other\ninteresting fact is that on average, left category article have 7.45 links per\narticle, the left-center ones have 5.72, least bias articles have 4.57, right-center\nhave 3.04, and right bias articles have 4.21 links per article.\nArticle left Leftcenter Least Rightcenter Right Total\nTotal 1,545,640 1,230,138 967,071 1,453,404 1,431,560 6,627,813\nInternal 368,951 341,425 289,446 141,971 280,257 1,422,050\nExternal 1,176,689 888,713 677,625 1,311,433 1,151,303 5,205,763\nAverage 7.45 5.72 4.57 3.04 4.21 4.43\nTable 5.3: Corpus anchor elements statistics\nWe also calculate the average number of the internal and external links per\nproducer. Figure 5.2 shows the regarding statistics in the stacked bar chart in\nwhich the blue color represents the number of external links and the orange\ncolor shows the number of internal links per news producer.\nFigure 5.3 displays the top 20 news producers in our corpus which are\nreferenced in articles from other news producers. At the top of the list, we\nhave the New York Times1and Washington Post2being referenced 135,591\nand 97,001 times, both of which are left-wing news producers. Moreover, 7\nnews producers in this list are categorized as left bias, 6 left-center, 4 right,\nand 3 least bias category.\n5.3 Duplicate Article Detection\nThe next obstacle of this thesis was to go through the corpus to examine\nthe articles for similarity metrics. We wanted to be cognizant of how often\n1http://nytimes.com\n2http://washingtonpost.com\n39\nCHAPTER 5. ANALYSIS\nFigure 5.2: Average number of internal and external links per producer\nFigure 5.3: Top 20 news producers referenced by other producers\n40\nCHAPTER 5. ANALYSIS\nnews producers reused the content from one another. Therefore, we focused\non \ufb01nding a way to recognize duplicate and near duplicate articles in the\ncorpus. To overcome this problem, we employed a part of a pipeline that was\nintroduced in Alshomary\u2019s thesis [2018] for text reuse analysis.\n5.3.1 Text Reuse Analysis Pipeline\nTheactualgoalofthispipelineisto\ufb01ndallpossiblepairsoftextreusefromtwo\ncollections of documents. This task has three subtasks including text prepro-\ncessing, candidate \ufb01ltering, and text alignment. Text preprocessing subtask\nalso has its steps, content extraction, text cleaning, and feature extraction.\nHowever, from the \ufb01rst subtask, we only need to use the feature extraction\nstep. The second subtask computes the similarity measure and removes the\npairs with low similarity. Computing the similarity measure is the only step\nthat we used from the second subtask; consequently, we totally ignored the\nthird one. Therefore, we used our hyperpartisan corpus as an input, then we\nemployed a representation method based on TF-IDF weighting scheme to rep-\nresenteachdocumentinthedatasetinasetoffeaturevectors. Then, weutilize\ncandidate similarity measure subtask, which uses cosine similarty3method to\nperform a pairwise similarity comparison on all the possible pair of articles.\nFigure 5.4 displays the statistics about the number of documents in the corpus\nthat we can keep with di\ufb00erent cosine similarity threshold from 0.0 to 1.0.\nFigure 5.4: Number of article / cosine similarity score\nAfterextractingpairwisesimilarityscores,wewantedtodecideaboutwhich\n3https://en.wikipedia.org/wiki/Cosine_similarity\n41\nCHAPTER 5. ANALYSIS\nsimilarity score is the most reasonable one to remove duplicate and near du-\nplicate articles. To do so, we extract several pairs of articles with di\ufb00erent\nsimilarity scores namely, 0.5, 0.75, 0.95, and 0.95. Next, we inspected each\npair manually for text reuse. We then decided to stay with the score of 0.90.\nFurthermore, to remove one of the pairs in the corpus and keep the other\none, we utilize a very simple method. The \ufb01rst priority is to keep the article\nwhich was published most recently, and in the cases that articles do not have\na publishing date, we delete one randomly.\n5.4 Classi\ufb01cation Experiment\nIn this section, we concentrate on the other important objective of this thesis,\nwhich is developing a pipeline to detect both hyperpartisan news articles and\ntheir orientation as well. Accordingly, we developed a two di\ufb00erent pipeline\nwhich gets our corpus as an input and one of them creates a model for hyper-\npartisan detection and the other one predicts articles orientation. The hyper-\npartisan detector is a binary classi\ufb01er which predicts if an article is extremely\nbiased or not, and the orientation detector enjoys a multi-class classi\ufb01er which\npredicts the article bias category such as left, left-center, least, right-center,\nand right. For hyperpartisan detector, we balanced the number of articles, and\nwe used 250K articles from each left and right wings, in addition to other bias\ncategories we also include 167K articles each.\n5.4.1 Logistic Regression\nLogistic regression4is one the most used methods for binary classi\ufb01cation\nwhich gives a discrete binary between 0 and 1 as an outcome. It measures\nthe relationship between dependent variables(labels) and independent vari-\nables(features) and it estimates the probabilities using its regression function.\nIn our experience, we follow these steps.\n\u2022Tokenizing the articles to the words.\n\u2022Removing stop words.\n\u2022We use the bag-of-words5model to represent the articles in the vector\nspace.\n\u2022We randomly split the data to train and test data, which share of the\ntrain data is 70 percent.\n4https://en.wikipedia.org/wiki/Logistic_regression\n5https://en.wikipedia.org/wiki/Bag-of-words_model\n42\nCHAPTER 5. ANALYSIS\n\u2022Finally, we trained the classi\ufb01er using the logistic regression algorithm.\nIn this approach we produced two di\ufb00erent classi\ufb01ers, the \ufb01rst one is trained to\npredict the orientation of the algorithm, and the second one is used to detect\npartisan articles from least biased ones. Table 5.4 displays the performance\nof this algorithm over our corpus, and we were able to predict hyperpartisan\narticle with a recall of 0.84, and also we predict the bias category of articles\nwith a recall of 0.71.\nf1Precision Recall Accuracy\nOrientation 0.7077 0.7394 0.7126 0.7126\nHyperpartisan 0.8388 0.8385 0.8390 0.8398\nTable 5.4: Logistic Regression outcome\n(a)Bias category detection\n (b)Hyperpartisan detection\nFigure 5.5: Confusion matrix of Logistic Regression algorithm\nMethod Two\nThis method also works similar to the previous one, However, in this experi-\nence, weusedtf-idf6toconvertarticlestothevectorspace. Table5.5illustrates\nthat using TF-IDF is not as successful as bag-of-words model. We achieved\na recall of 0.82 for Hyperpartisan prediction and 0.68 to recognize the bias\ncategory of articles.\n6https://en.wikipedia.org/wiki/Tf-idf\n43\nCHAPTER 5. ANALYSIS\nf1Precision Recall Accuracy\nOrientation 0.6732 0.7110 0.6807 0.6807\nHyperpartisan 0.8239 0.8236 0.8231 0.8236\nTable 5.5: Logistic Regression - TF-IDF\n(a)Bias category detection\n (b)Hyperpartisan detection\nFigure 5.6: Confusion matrix of Logistic Regression algorithm\n44\nCHAPTER 5. ANALYSIS\n5-fold Cross Validation\nCross validation7is a method to increase the stability of the machine learning\nmethod. It tries to both \ufb01nd the correct pattern and remove the noises from\nthe training data. K-fold cross-validation randomly splits the training data\ninto K di\ufb00erent subsets, then it trains the classi\ufb01er K time, in each iteration,\nit considers one subset as the test data. Finally, the e\ufb00ectiveness of the model\nwill be increased by averaging the error estimation of all K-folds. We also used\nthis method in combination with our experience logistic regression algorithm\nand using the bag-of-words model, which increased the recall of hyperpartisan\ndetection to 0.85 and 0.75 for the bias categorization. More detail can be seen\nin table 5.6.\nf1Precision Recall Accuracy\nOrientation 0.7495 0.7600 0.7522 0.7522\nHyperpartisan 0.8507 0.8498 0.8492 0.8508\nTable 5.6: Logistic Regression with 5-fold cross validation\n(a)Bias category detection\n (b)Hyperpartisan detection\nFigure 5.7: Confusion matrix of Logistic Regression with 5-fold cross validation\n7https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n45\nCHAPTER 5. ANALYSIS\n5.4.2 Naive Bayes\nNaive Bayes8is a probabilistic classi\ufb01er based on Bayes theorem which is\nvery popular for the text classi\ufb01cation. The fundamental rule of Naive Bayes\nis the independence of the features, and it presumes that the existence of a\nspeci\ufb01c feature in a class is unrelated to the presence of any other feature.\nThe steps that we took to prepare the data is similar to what we had for\nLogistic Regression method 5.4.1. First, we tokenize the articles to words,\nthen we removed the stop words. The next step was to use the bag-of-words\nmodel to represent the articles in the vector space. Finally, we split the data\ninto training and test set, and we trained the classi\ufb01er using the Naive Bayes\nalgorithm. Table 5.7 shows that we reached recall of 0.78 for hyperpartisan\ndetection and 0.59 for orientation categorization.\nf1Precision Recall Accuracy\nOrientation 0.6023 0.6304 0.5954 0.5954\nHyperpartisan 0.7761 0.7784 0.7768 0.7776\nTable 5.7: Naive Bayes algorithm outcome\n(a)Bias category detection\n (b)Hyperpartisan detection\nFigure 5.8: Confusion matrix of Naive Bayes algorithm\n8https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n46\nCHAPTER 5. ANALYSIS\n5.4.3 Random Forest Classi\ufb01er\nRandom Forrest9is a supervised machine learning model, and it produces the\nmodel by merging several decision trees to increase the accuracy and stability\nof the model. We also utilize this algorithm in our study. Here also we followed\nthe steps that we had in the Logistic Regression section 5.4.1. The outcome\nwas not as impressive as our other experiments. The recall value that we\nachieved for hyperpartisan detection is 0.73 and 0.38 for the bias category\ndetection which can be seen in more detail in table 5.8.\nf1Precision Recall Accuracy\nOrientation 0.3005 0.5681 0.3887 0.3887\nHyperpartisan 0.7306 0.7384 0.7328 0.7322\nTable 5.8: Random Forest algorithm outcome\n(a)Bias category detection\n (b)Hyperpartisan detection\nFigure 5.9: Confusion matrix of Random Forest algorithm\n5-fold Cross Validation\nIn this experiment, we train the classi\ufb01er using random forest and also with\n5-fold validation method to make it more accurate. Table 5.9 demonstrates\nthat the recall for hyperpartisan and bias category detection slightly increased\nto 0.75 and 0.41.\n9https://en.wikipedia.org/wiki/Random_forest\n47\nCHAPTER 5. ANALYSIS\nf1Precision Recall Accuracy\nOrientation 0.3300 0.6070 0.4114 0.4114\nHyperpartisan 0.7484 0.7559 0.7501 0.7501\nTable 5.9: Random Forest with 5-fold cross validation\n(a)Bias category detection\n (b)Hyperpartisan detection\nFigure 5.10: Confusion matrix of Random Forest with 5-fold cross validation\n48\nCHAPTER 5. ANALYSIS\n5.4.4 Experiment on SemEval 2019 Corpus\nWe produced this corpus for the participants in our SemEval 2019 Task. It\ncontains 1 million articles, 800K articles for training and 200K articles for the\ntest set. The training set has 200K left, 400K least, and 200K right articles,\nin addition, the test set has 50K left, 100K least, and 50K right articles.\nMoreover, we divided the data in the way that the training and test set share\nno news producers in common. Finally, the task is to predict the extreme bias\nof the news articles. We applied the most successful method that had in our\nexperiment on this corpus, and consequently, we archive the recall of 0.64. The\nmain reason that has this decrement in the accuracy is that in the previous\nexperiment the classi\ufb01er was trained over data from all the news producers.\nHowever, to increase the di\ufb03culty, in this corpus test set and training set have\nno news producer in common.\nFigure 5.11: Confusion matrix of Logistic Regression with 5-fold cross validation\non SemEval 2019 corpus\n49\nChapter 6\nConclusion\nIn this study, we concentrate on a way to deal with ideological creation and\ncontribution of political news. To do so, we felt a demand for a decent news\narticles dataset, and in order to overcome this issue, we create a hyperpartisan\ncorpus as the \ufb01rst contribution of this thesis. To begin with, we found a list\nof news producers with their corresponding ideological agenda from two trust-\nworthy sources namely, BuzzFeed, and Media Bias/Fact Check. In the next\nstage, we inspected Facebook pages and sitemaps of the websites of those news\nproducers, which led us to extract all the links of their news articles. Then,\nwe utilize the Betaweb cluster, in the combination of Webis Web Archiver\ntool to crawl all the links. In this phase, we develop an automatic tool which\ndoes the crawling process in combination with a quality check to guarantee\nthe quality of the task. In the next step, we developed a wrapper to extract\nthe main content from HTML pages, which is completely compatible with 358\nnews producers web layouts. Finally, we construct the corpus of 1.5 million\narticles in two di\ufb00erent formats JSON lines and XML. The XML format was\nbuilt exclusively for the participants of our SemEval 2019 task.\nAs the second contribution of this thesis, we analyze the corpus to answer\nseveral questions. First, we investigate hyperlinks in the articles, and the way\nnews articles pointed to one another. Second, we perform a pairwise similarity\ncheck between all the possible pairs of articles in the corpus. This stage helped\nus to recognize duplicate and near duplicate articles in the corpus. Finally,\nwe developed two pipelines, which utilize several machine learning methods\nto predict the article\u2019s orientation, and also hyperpartisanship. According to\nour best result that we achieved in our experiments, using Logistic Regression\nwith 5-fold Cross Validation and our pipeline, we were able to distinguish\norientation with (( F1=0.74)) and hyperpartisanship of news articles with a\nremarkable score of (( F1=0.85)).\n50\nCHAPTER 6. CONCLUSION\nFuture Work\nBuilding on our contributions in this thesis, there are still possibilities to en-\nhance the quality of the corpus. One example could be further analysis of the\ndata and topic classi\ufb01cation in order to eliminate the non-political articles. In\naddition, there is always a chance to add new articles in the corpus. We can\nalso take advantage of the statistics we collected to detect duplicate and near-\nduplicate documents, and we can go further in this regard to detect possible\ntext reuse in the corpus. Considering our experiments on the data, we are\nlooking forward to using another pipeline from Johannes Kiesel[2018], which\nowing to time limitation we were not able to utilize it. However, we adopt the\nimplementation to be compatible with our corpus and it is quite interesting to\n\ufb01gure out how it can perform with a big corpus, that we have constructed.\n51\nBibliography\nMilad Alshomary. A Pipeline for Scalable Text Reuse Analysis. Master\u2019s\nthesis, Bauhaus-Universit\u00e4t Weimar, Fakult\u00e4t Medien, Computer Science\nand Media, July 2018. 5.3\nE.L. Bernays and M.C. Miller. Propaganda . Ig Publishing, 2005. ISBN\n9780970312594. URL https://books.google.de/books?id=3De8nd_B_\nC8C. (document)\nShweta Bhatt, Sagar Joglekar, Shehar Bano, and Nishanth Sastry. Illuminat-\ning an ecosystem of partisan websites. In Companion Proceedings of the\nThe Web Conference 2018 , WWW \u201918, pages 545\u2013554, Republic and Can-\nton of Geneva, Switzerland, 2018. International World Wide Web Confer-\nences Steering Committee. ISBN 978-1-4503-5640-4. doi: 10.1145/3184558.\n3188725. URL https://doi.org/10.1145/3184558.3188725 . 2.2\nJe\ufb00rey Dean and Sanjay Ghemawat. Mapreduce: Simpli\ufb01ed data processing\non large clusters. Commun. ACM , 51(1):107\u2013113, January 2008. ISSN 0001-\n0782. doi: 10.1145/1327452.1327492. URL http://doi.acm.org/10.1145/\n1327452.1327492 . 4.4.1\nMilad Alshomary Benno Stein Matthias Hagen Martin Potthast Jo-\nhannesKiesel,FlorianKneist. Reproduciblewebcorpora: Interactivearchiv-\ning with automatic quality assessment. Journal of Data and Information\nQuality, 2018. 4.1.1, 4.1.1, 4.1.3, 6\nJohannes Kiesel, Martin Potthast, Maria Mestre, Rishabh Shukla, Benno\nStein, David Corney, Emmanuel Vincent, and Payam Adineh. SemEval\n2019 Task 4 - Hyperpartisan News Detection SemEval 2019 Task 4 - Hy-\nperpartisan News Detection, July 2018. URL https://doi.org/10.5281/\nzenodo.1400316 . 4.5.2\nLilliana Mason. A cross-cutting calmhow social sorting drives a\ufb00ective polar-\nization.Public Opinion Quarterly , 80(S1):351\u2013377, 2016. doi: 10.1093/poq/\nnfw001. URL http://dx.doi.org/10.1093/poq/nfw001 . 1\n52\nBIBLIOGRAPHY\nMartin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendor\ufb00, and\nBennoStein. Astylometricinquiryintohyperpartisanandfakenews. CoRR,\nabs/1702.05638, 2017. URL http://arxiv.org/abs/1702.05638 . 1, 2.3\nIyengar Shanto and Westwood Sean J. Fear and loathing across party lines:\nNew evidence on group polarization. American Journal of Political Science ,\n59(3):690\u2013707. doi: 10.1111/ajps.12152. URL https://onlinelibrary.\nwiley.com/doi/abs/10.1111/ajps.12152 . 1\nTom White. Hadoop: The De\ufb01nitive Guide . O\u2019Reilly Media, Inc., 1st edition,\n2009. ISBN 0596521979, 9780596521974. 4.2.1\nAmy X. Zhang, Aditya Ranganathan, Sarah Emlen Metz, Scott Appling, Con-\nnie Moon Sehat, Norman Gilmore, Nick B. Adams, Emmanuel Vincent,\nJennifer Lee, Martin Robbins, Ed Bice, Sandro Hawke, David Karger, and\nAn Xiao Mina. A structured response to misinformation: De\ufb01ning and\nannotating credibility indicators in news articles. In Companion Proceed-\nings of the The Web Conference 2018 , WWW \u201918, pages 603\u2013612, Republic\nand Canton of Geneva, Switzerland, 2018. International World Wide Web\nConferences Steering Committee. ISBN 978-1-4503-5640-4. doi: 10.1145/\n3184558.3188731. URL https://doi.org/10.1145/3184558.3188731 . 2.1\n53", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Building a Corpus for Hyperpartisan News Detection", "author": ["P Adineh"], "venue": "NA", "pub_year": "NA", "abstract": "As the polarization of societies increases, news publishers that cater to specific extreme views  become attractive to more people. Such hyperpartisan news publishers, however, tend to"}, "filled": false, "gsrank": 367, "pub_url": "https://downloads.webis.de/theses/papers/adineh_2018.pdf", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:sdnzQ-8xIWIJ:scholar.google.com/&output=cite&scirp=366&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D360%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=sdnzQ-8xIWIJ&ei=RrWsaO_lEeHUieoP9LKZ6AI&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:sdnzQ-8xIWIJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://downloads.webis.de/theses/papers/adineh_2018.pdf"}}, {"title": "The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news", "year": "2019", "pdf_data": " Open Information Science 2019; 3:  115\u2013136\nThomas J. Froehlich*\nThe role of pseudo-cognitive authorities and \nself-deception in the dissemination of fake news\nhttps:/ /doi.org/10.1515/opis-2019-0009 \nReceived October 22, 2018; accepted June 3, 2019\nAbstract: This paper draws together insights from a variety of fields (including philosophy, psychology, \ninformation studies, sociology, politics, and media studies) to synthesize insight into why fake news is \ncreated, disseminated, sustained and authorized so as to understand how and why it is successful and how it might be challenged. The premier case for analysis will be Trump, his supporters, his party and his media. Central to this issue is the role of cognitive authorities, a notion first articulated and developed by Patrick Wilson (1983). Honest cognitive authorities have credibility and expertise and are regarded as trustworthy. Their knowledge, based on direct and verifiable knowledge, is sought, communicated and accepted, when an information seeker comes to them about a matter of which an information seeker \nhas come to believe that they have expertise, credibility and knowledge. Pseudo- or false cognitive \nauthorities appear to have the same qualities of credibility, expertise and trustworthiness, but on critical examination they fail in these qualities and strive to impose a partisan agenda irrespective of truth, evidence, logic or facts. Unfortunately, these conditions do not deter believers from accepting them. These authorities are of various types, such news programs or organizations, religious leaders, or social media sites, that create, propagate, authorize and legitimatize fake news stories, that partisan adherents are willing to accept and perpetuate through a form of collective self-deception and who will at the same time denigrate sources and cognitive authorities of genuine and verified information or knowledge. Starting with the InfoWars, we proceed to discuss the nature of the forms of false information on the internet, and the role of deception, particularly self-deception, social self-deception, and collective self-\ndeception in the acceptance real fake news, which is authorized and legitimatized by pseudo-cognitive \nauthorities. In the process we contrast genuine cognitive authorities with dishonest ones, and show how the psychological factors, motivations, and collective self-deception feed each other into a reinforcing collective self-deception so strong it may be equivalent to a cult. This dialogical process (pseudo-cognitive authorities deceiving and self-deceiving themselves and their listeners, who in turn \u201cvalidate\u201d those authorities through word-of-mouth and seeking and associating with like-minded groups) is reinforced by repetition, the Dunning-Kruger effect, agnotology, and other factors. At the conclusion the roles of information professionals will be examined concerning the difficulties confronting fake news and fake news adherents and developing paths for successful strategies in coping with them. \nKeywords: self-deception, social self-deception, collective self-deception, cognitive authority, pseudo-\ncognitive authority, fake newsCommunication\n*Corresponding author, Thomas J. Froehlich, Kent State University, Kent, OH UNITED STATES, tfroehli@kent.edu\n Open Access. \u00a9 2019 Thomas J. Froehlich, published by De Gruyter.  This work is licensed under the Creative Commons Attri-\nbution 4.0 Public License.\n116 \u2003  \u2003T.J. Froehlich\nPrologue: Infowars\nWorld War III has started. While Alex Jones is wrong about many things, his \u201cnews\u201d program\u2019s title, \nInfoWars, correctly indicates a global problem. The first major salvo of WW III was InfoWar I, the 2016 election of Trump. His continued propensity for lies \u2013 an average of 22 per day over the course of his presidency (Kessler, Rizzo, & Kelly, 2019) \u2013 his administration, his political party, and uncritical supporters continues the war, and it is not trivial. It is for the soul of America and represents a threat to people all over \nthe world. I call it a World War because it reflects the use of the internet throughout the world to inflame \ndiscord in most democracies to perpetuate the power of autocrats and fascists. Its insult to freedom lies not only in what is spread on the internet but also what it suppresses. In countries around the world there is a battle to continue to anchor political decision making in science, reason, evidence, fact, democratic values, and humanism. It is a war of disinformation, misinformation, lies, and obfuscation against the evidence and truth, and for power and greed trolling simplistic solutions to complex problems. While this paper will focus on the American InfoWar, has spread throughout the world, with Donald Trump\u2019s 2016 election part of the impetus (Booth & Birnbaum, 2017; Fischer, Heide, & Hoppe, 2018; Karp, 2018; Barel, 2019; PTI, 2018). This paper will consider how disinformation is created, disseminated and legitimized through a key case study, Trump, his administration, party and supporters. \nThe two sides of the American InfoWar have come to align with the political parties, although not all \nRepublicans align with one side and Democrats are not without taint. Fox News, Sinclair Broadcasting, Russian trolls, conspiracy theorists, social media run amok with \u201calternative facts\u201d and a president, an administration, and the Republican Party have committed to the destruction of democratic norms and the resurgence of racism, sexism, fascism, rampant corruption, and climate-change denial, among others. It is a systemic corruption from the top down (Waldman, 2018). CNN, MSNBC, The New York Times, National \nPublic Radio, social media, and the Democrats have aligned to stop them. There are at least two kinds of third parties, (1) the click-bait entrepreneurs whose allegiance is to making money and generally not to either side, though this allegiance leads them to be more likely to promoting right-wing ideology and (2) \nthe disengaged, which by default tend to support Trump and his devotees, whether the Republican Party \nor his advocates, because lack of engagement, whatever its motives such as indifference or rage against the system tend to bolster the status quo. Attempts to treat the two sides in the InfoWar as equivalent can encourage these third parties to boost fake news. \nThe Problem of False Equivalences\nThe sides in the InfoWar are not balanced, for the one side not only spreads disinformation but actively challenges, abuses and attacks those who are committed to truth, evidence, facts, and logic. One side declares its opposition to have no valid grounds for its opinion; the other side seeks to provide evidence, logic and truth in political decision making. Pundits like Sean Hannity declare that all opinions are equal but the right\u2019s opinion invalidates all others. Their opponents try to establish honest justification for their \nopinions. Thus these are not simply competing opinions. Indeed, false stories supported by the right are \noften launched on Web sites that make no pretense of following the norms or processes of journalistic ethics that create reliable information (Benkler, Faris, & Roberts, 2018). Outlets that claim to adhere to the norms of journalism, such as Fox News, then echo these stories without critical analysis. They leave critical analysis and traditional journalistic practices to the so-called left-wing media, and then condemn it as slanted. The difference between the treatment in the media of \u201cPizzagate,\u201d a conspiracy theory involving a pedophilia run by the Clintons and a claim that Donald Trump raped a 13-year-old girl in 1994 illustrates the difference. The left passed the latter story around the internet, and a civil lawsuit in the case was filed against Trump in 2016, but the plaintiff eventually dropped it. Without stronger support, the story vanished. The former story had not a shred of evidence, and it even relied on the existence of \na basement in a building that has none. But Fox News dubbed the crime ring it described the \u201cLolita \nExpress,\u201d and many right-wing outlets aired it many times until a believer fired a gun in the Washington, \n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003117\nD.C. pizza shop supposedly hosting the crime ring in its nonexistent basement. As Yochai Benkler, Robert \nFaris, and Hal Roberts (2018) explain, \nNot one right-wing outlet came out to criticize and expose this blatant lie for what it was. In the grip of the propaganda \nfeedback loop, the right-wing media ecosystem had no mechanism for self-correction, and instead [in the matter of \u201cPizzagate\u201d] exhibited dynamics of self-reinforcement, confirmation, and repetition so that readers, viewers and listeners encountered multiple versions of the same story, over months, to the point that both recall and credibility were enhanced (p. 97).\nUnfortunately, misinformation and disinformation are transmitted more quickly than information \n(Vosoughi, Roy & Aral, 2018). It seems the bigger the lie, the more rabidly and rapidly Trump\u2019s followers will embrace it. Corrections of disinformation do not eliminate it from rational thought and perception. Technology has brought misinformation and disinformation to a qualitatively superior level: it is now possible to make convincing videos showing people saying things they never said, using public footage \nsuch as is readily available, for example, making President Obama say that he used his intelligence agencies \nto conspire against the election of Trump (Warzel, 2018; Lyu, 2018). \nCognitive Authority and Second-Hand Knowledge\nHow do fake news stories take hold and how are they perpetuated? One of the major forces is that of pseudo-cognitive authorities and how they enable the creation and dissemination of real fake news (e.g., propaganda), primarily through collective self-deception, and how it discredits bona fide news. To start off, we must understand what cognitive authorities are, the different forms they take, and the role such authorities play in our interpretation of the world.\nA key issue is the difference between primary knowledge and secondary knowledge. Primary knowledge \nis information we can prove or observe. Secondary knowledge is information we learn from other people, trusting in their perception, logic, reasoning, or experience. This might include friends or acquaintances, medical professionals, teachers, academic experts, and the media, depending on who we consider to be cognitive authorities. \nPatrick Wilson introduced the idea of cognitive authority in his book Second-Hand Knowledge: An Inquiry \ninto Cognitive Authority (1983). He points out that primary knowledge is rather limited, and thus most people rely heavily on secondary knowledge. Cognitive authority is different from organizational or administrative authority or authority that comes from power hierarchies. For example, in seeking information about the salary one can earn at a particular organization, one consults organizational authorities such as the hiring \nmanager. In seeking information about whether the salary offered is fair, one consults cognitive authorities, \nsuch as colleagues on the job or in professional associations.\nIt seems probable that Trump\u2019s followers see Trump himself and Fox News and other ultra-right \nfigures and associations as cognitive authorities. Are these cognitive authorities genuine? Do they have the properties and characteristics that we associate with real cognitive authorities? Or are they something that we might call pseudo-cognitive authorities or false cognitive authorities? If so, how do they differ from true cognitive authorities? And how do they operate?\nCharacteristics of Genuine Cognitive Authorities\nWilson (1983) notes several properties of cognitive authority: (1) Cognitive authority is related to credibility. A person who has cognitive authority on a particular subject is regarded as a credible source for that topic. A bain marie is a baking tool. Thus my friend who works as a chef is a credible source to tell me how to \nuse it. Wilson writes that credibility consists primarily of \u201ccompetence and trustworthiness.\u201d (p. 13). (2) Cognitive authority exists on a spectrum. A person may know a lot or a little about a subject. For example, \nmy chef-friend has more or less expertise on the bain marie depending on whether she has worked in a \nprofessional kitchen that used one, and depending on whether she learned to use one in culinary school. \n118 \u2003  \u2003T.J. Froehlich\nWilson notes that some cognitive authorities have so much knowledge that they become arbiters of settled \nopinion on a subject. (p. 18). The New York Times and Washington Post once played that role. Unfortunately, a steady campaign of false allegations has chipped away at many people\u2019s faith in these authorities. (3) Cognitive authority exists in relation to a sphere of interest. These spheres can be well-defined or ill-defined: an expert on the orchestral recordings of Beethoven versus a general expert in music. (4) Cognitive \nauthority involves at least two people. One can have cognitive authority without being an expert or at least \na recognized expert. A person who owns a bain marie and has used it has some cognitive authority with respect to it, but likely not as much as my chef friend. A person may become a cognitive authority for a specific person or set of persons for a specific topic or set of topics. For example, we may have friends we ask for their movie reviews because we trust their judgment, but our friends are not professional movie critics. (5) There are brands of expertise not related to knowledge, expertise that may not justify the qualification of cognitive authority. Wilson asserts:\nNot only are there brands of expertise, no longer regarded as corresponding to any real knowledge, there are numerous \ninstances of competing brands all claiming authority in the same sphere, and the question arises which brand, if any of them, is the right brand to warrant cognitive authority. (p. 22).\nThus Wilson seemed to anticipate Fox News more than 10 years before its introduction, as the expertise of \nits pundits do not correspond to real knowledge.\nSoo Young Rieh (2010), who has taken up and expanded upon Wilson\u2019s themes, writes:\nRather than having one clear definition, credibility has been defined along with dozens of other related concepts such \nas believability, trustworthiness, fairness, accuracy, trustfulness, factuality, completeness, precision, freedom from bias, objectivity, depth, and informativeness. Most credibility researchers agree that credibility assessment results from simultaneously evaluating multiple dimensions. Among these, two key dimensions are identified: trustworthiness and expertise. Trustworthiness is a core dimension in credibility assessment that captures the perceived goodness and morality of the source. The perception that a source is fair, unbiased, and truthful contributes to the trustworthiness of information. Trustworthiness is, however, not a synonym for credibility because people also must recognize expertise in order to deem information credible. Expertise reflects perceived knowledge, skill, and experience of the source. Expertise is likewise an important factor given its close relationship to people\u2019s perceptions of a source\u2019s ability to provide information that is both accurate and valid. (pp. 1337-1338).\nTraditional media, such as the New York Times, consist of credible and trustworthy reporters and managers, \nwho exhibit expertise about the topics they report and journalistic ethics. Fox viewers believe this to be true of the reporters, commentators, and managers of Fox News as well. They believe that the news reported is believable, trustworthy, fair (compared to liberal bias), accurate, factual, complete, precise, free from (liberal) bias, objective, in-depth, and informative. They believe that conservative viewpoints are moral \nones. Many treat Fox News and sites like InfoWars as their primary cognitive authority on news events. \nIn order to understand why, the next section will look at the kinds of false information that exist on the internet, and the phenomena of self-deception, social self-deception, and collective self-deception. \nVarieties of False Information\nFox News, Alex Jones, Donald Trump, and many others provide many kinds of false information, misinformation, disinformation, omission, ignorance, paltering, and false statements that arise from self-deception. Misinformation includes Fox News\u2019 assertions about the failure of the Mueller investigation. Fox News\u2019 assertions that Rudy Giuliani succeed in defending Trump are disinformation. Omission occurs when Fox News discusses the Mueller investigation without with referencing the conviction of Paul Manafort \nor Michael Cohen\u2019s guilty pleas. A Fox News commentator who said in April 2018 that Japan was once a \ncommunist country most likely spoke out of ignorance (The Daily Beast, 2018). When Trump asserted that there had been zero admission of guilt in a 1973 federal lawsuit that charged his family\u2019s firm with housing discrimination, he was telling the literal truth, but he did so in order to falsely suggest that there was no \n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003119\nlegal recognition that Trump Corporation had committed housing discrimination, in spite of the fact that \nthe conclusion of the suit included stipulations to desegregate Trump properties (McGregor, 2016). This was an instance of paltering, a common strategy of politicians. When self-deception prompts the sharing of false information, it may be unmotivated or motivated. The former occurs when a liberal chooses to read the New York Times as a reliable source of information. The latter occurs when people who have lived in an \nenvironment where Confederate symbols are normalized believe it is not a symbol of racism. Self-deception may explain Trump\u2019s supporters embrace of his lies and the lies of those who support him. The two types align with the distinction between explicit bias and implicit bias. All of us have implicit biases but for most of us, such biases may not interfere with our work. However, this distinction does not capture the range or nature of our biases. In both motivated and unmotivated self-deception, we deceive ourselves about our motives, but one is more or less conscious of them. Sartre\u2019s notion of self-deception as bad faith, which says that we live out contradictions in our beliefs to evade our responsibilities, was the source of my original understanding (Froehlich, 2017) of the success of self-instilled disinformation in personal life. Psychology and political scholarship have fleshed out this notion, as the next section will describe. Self-deception in information seeking is instrumental to the (pseudo-) cognitive authority of fake news authorities, and it \nmay also play a role in impressions of honest cognitive authorities. Fake news is a form of disinformation. \nA national poll delineated the meaning of fake news for most Americans: \u201cJust 25% say the term \u2018fake news\u2019 applies only to stories where the facts are wrong. Most Americans (65%), on the other hand, say that \u2018fake news\u2019 also applies to how news outlets make editorial decisions about what they choose to report\u201d (National\u2026., 2018). This paper takes the majority position.\nSelf-Deception as a Strategic Advantage\nContrary to past research (Froehlich, 2017) self-deception is not a form of false information. It is a method of embracing false information. This appears to be a widespread approach of Trump supporters in the Age \nof Disinformation, which dovetails with a form of a cognitive authority, or at least a false form of it. People on the left are also prone to self-deception, for example, when they willfully ignore evidence that some kinds of gun control legislation will not prevent the tragedy to which they implicitly respond. However, \nself-deception has become so widespread among Trump\u2019s supporters as to become a strategic advantage. \nWilliam von Hippel and Robert Trivers attribute three advantages to spreading false information with \nself-deception over knowingly spreading false information: (1) it \u201cfacilitate[s] interpersonal deception \nby allowing people to avoid the cues to conscious deception that might reveal deceptive intent\u201d; (2) it \u201celiminates the cognitive load that is typically associated with deceiving\u201d; and (3) \u201cit can minimize the retribution if the deception is discovered\u201d (2011, p. 1). It is questionable, however, whether Donald Trump suffers any cognitive load that is associated with deception. First, he has given no indication of possessing a conscience that might cause him to feel guilty about it. Second, he makes no real attempt to be consistent in his lies or to cover them up. He seems to believe that his deceptions will not lead to retribution. Fake news authorities likewise do not seem to have any concern about their deception being discovered. Von Hippel \nand Robert Trivers note that self-deception may support social status. For example, self-deception can help \nus convince others that we are better than we really are. Self-deception that involves self-enhancement or self-aggrandizement is likely an important factor in understanding the devotion to Trump by his supporters. \nVarieties of Self-Deception\nVon Hippel and Robert Trivers describe five varieties of self-deception: They are (1) biased information search; (2) biased interpretation; (3) misremembering; (4) rationalization; and (5) convincing oneself that a lie is true. Biased information searches can be biased because the actor does not search long enough to obtain multiple points of view, because he or she has searched in a way that only exposes information on one side of an issue, or because he or she ignores information that does not suit the preconceived narrative. \nPaying attention only to fake news authorities or friends who support their views is selective searching for \n120 \u2003  \u2003T.J. Froehlich\ninformation. Ignoring the Muller report\u2019s findings on obstruction and focusing on his findings on conspiring \nwith Russia in election tampering amounts to selective attention. \nBiased interpretation consists of maintaining one\u2019s point of view in the face of reliable information \nto the contrary. Von Hippel and Robert Trivers cite the case of two groups of people with strong, differing attitudes toward capital punishment. They were each presented with evidence on both sides of the issue \nwith respect to whether capital punishment deters crime, but neither group changed their opinion at all \n(p. 9). Trump supporters may employ this approach to the question of the success of the talks with North Korea about denuclearization; the renegotiated free trade agreement (NAFTA) with Mexico and Canada; the success of his tariffs on foreign made products, such as steel and aluminum; and the benefits of the new tax law. They\u2014and their ideological opponents\u2014tend to focus on the evidence that backs their original opinion.\nMisremembering occurs when people forget or misremember information inconsistent with their \ncurrent preferences (p. 9). The obvious example are all the Trump supporters who voted for and appreciated the presidency of Barack Obama have been converted into despisers of his programs and legislation that benefitted them: e.g., Obamacare. \nRationalization, according to von Hippel and Trivers, consists of \u201cavoid[ing] telling oneself the whole \ntruth by reconstructing or rationalizing the motives behind the original behavior to make it socially more acceptable\u201d (p. 9). One can imagine a Trump supporter complaining about how Obama\u2019s record unemployment level was no match to Trump\u2019s (although in fact, many of Obama\u2019s strategies led to the latter\u2019s lower unemployment rate). \nConvincing oneself that a lie is true appears to be rampant in the current political environment. Perhaps \nthe most outrageous example was when Trump proclaimed at a rally on July 24, 2018: \u201cJust remember: what you\u2019re seeing and what you\u2019re reading is not what\u2019s happening\u201d (Holmes, 2018). The irony is that Trump, not the news media, lies constantly. Rudy Giuliani\u2019s assertion that \u201ctruth isn\u2019t truth\u201d (Morin & Cohen, 2018) \nin a television interview, in explaining why Trump should not testify before special counsel Robert Mueller \nbecause of the strong risk he would commit perjury also exemplifies convincing oneself that a lie is true. As Lamba and Notyananda (2014) note, \u201cSelf-deceived individuals are better at deceiving others.\u201d Trump is an enormously self-deceived individual, as his claims during the election that he and he alone could solve monumental national and international problems suggest. In his defense of Trump, Rudy Giuliani has emerged as self-deceiving as well. \nSocial Self-Deception\nWhile useful, von Hippel and Trivers\u2019s observations conceive of individuals in an atomistic approach to psychology. We are individual human beings, but our interpretation of ourselves and our world is thoroughly social. Self-deception is not only a learned behavior but a socialized and socializing one as well. As Roy Dings (2017) writes in a paper on \u201cSocial strategies in self-deception\u201d that self-deception can be \na process that is distributed across the social context of a self-deceiver\u2026. Other people may be the means to our self-\ndeceptive ends. That is, we may mislead other people, withhold information or straightforwardly deceive them, and all of these actions may be part of our self-deceptive endeavors. Many researchers would agree that what other people do, say, don\u2019t do or don\u2019t say is information that a self-deceiver can treat in a motivationally biased way. What has hitherto been neglected however, is the fact that we are able to influence what other people do, say, don\u2019t do and don\u2019t say. By determining what others do, say, don\u2019t do or don\u2019t say, we set up the possibility to deceive ourselves. (p. 16). \nDings defines self-deception as \u201c(i) a process that originates in (ii) a motivation or intention\u2026, which leads \nto (iii) a self-deceived end state (which can be the formation of a novel belief or the maintenance of an existing belief or other attitude)\u201d (p. 17). In social self-deception, other people are a means to the self-deceptive process. Dings clarifies that failure to act and non-linguistic statements such as body language and facial expressions can aid in this process, as well as explicit verbal or written statements (Dings, 2017 , \np. 17). While self-deception need not depend on other people, it often does. Dings points out four ways \n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003121\nthat people can support their own self-deception: seek like-minded people for support, avoid people who \ndisagree, try to convince others of their position, and withhold information that contradicts them. These final two are particularly common with fake news authorities.\nWith respect to choosing to associate with like-minded people and/or avoiding associations with \nideological opponents, Dings argues that this strategy can be either intentional or unintentional. In a \ncountry where Democratic and Republican increasingly live separately, and a social media system that \ntends to expose people to points of view with which they agree, it can actually be difficult to associate with people with which one disagrees. But joining a group based on shared political perspectives is a different act from entering a neighborhood bar and finding everyone agrees with your political views.\nDings notes that persuading others to share one\u2019s self-deception \nseem[s] to be especially relevant [to] deceiving ourselves about ourselves, given that how we think about ourselves is often \nbased of how others think of us. Persuasive social self-deception involves strategies in which we manipulate how others think of us, thereby indirectly manipulating how we think of ourselves. (p. 18).\nThe easy access to the internet and its many social media platforms make such strategies so easy to \nundertake: to find people who share the same disinformation, misinformation, conspiracy theories, etc., linking from reinforcing site to another and avoiding sites that provide evidence that conflicts with one\u2019s a priori biases. \nDings provides two frameworks, intentionalism and deflationism, that can guide our understanding \nof both self-deception and social self-deception. In the former, we engage in self-deception and social self-deception through intentions that control our behavior. Thus it is a type of motivated self-deception. In social self-deception, we may convince ourselves while convincing others. A person arguing that the Trump tax cuts will benefit the average American, may convince him or herself of this point of view. In deflationism, the notion of intention is not a requirement for self-deception or social self-deception. Dings asserts:\n[T]he most common kind of self-deception consists of a collection of psychological phenomena (such as confirmation bias \nand availability heuristic) that are influenced by desires or motivations\u2026. On this approach, having a bias in our attention or evidence-gathering due to a certain motivation, which then leads to us to believe a certain proposition p, would suffice. (p. 20).\nDeflationary self-deceivers convince themselves unconsciously to believe their peer group and its authorities, \nwho in turn support the peer-group and its authorities as the latter support a group self-deception.\nCollective Self-Deception\nDings acknowledges the notion of \u201ccollective self-deception,\u201d which DeWeese-Boyd (2016) developed. In the case of Trump\u2019s presidency, a group of individuals share levels of resentment about the status quo and share \u201cthe same belief for similar reasons and by similar means.\u201d Trump supporters share their beliefs in the success of his presidency with one another by watching the same media outlets (e.g., Fox News). DeWeese-Boyd explains:\nWhat distinguishes collective self-deception from solitary self-deception just is its social context, namely, that it occurs \nwithin a group that shares both the attitudes bringing about the false belief and the false belief itself. Compared to its solitary counterpart, self-deception within a collective is both easier to foster and more difficult to escape, being abetted by the self-deceptive efforts of others within the group. (DeWeese-Boyd, 2016, Section 7 .1)\nGroup self-deception is a dialogical process where the group absorbs and perpetuates false beliefs, \nreinforced by its elements and the collective results.\nDeWeese-Boyd writes that collective self-deception may be summative or non-summative. The former \nrefers to \u201cthe holding of a false belief in the face of evidence to the contrary by a group of people as a result of shared desires, emotions, or intentions (depending upon the account of self-deception) favoring \n122 \u2003  \u2003T.J. Froehlich\nthat belief\u201d (DeWeese-Boyd, 2016). This is reminiscent of Stephen Colbert\u2019s concept of \u201ctruthiness,\u201d \nwhich refers to believing something to be true, despite clear evidence to the contrary (\u201cTruthiness,\u201d 2017). Non-summative collective self-deception takes its approach from the collective itself. As DeWeese-Boyd writes, members of a group \u201cjointly commit\u201d to the notion and collective self-deception occurs, although individuals within the group may dissent, at least privately. For example, the editorial board of a news \noutlet, might commit to a set of values (e.g., Fox News\u2019s decision to support Trump in all his endeavors) \nwithout all of the individual members of the board concurring. As we have seen, non-summative collective self-deception (e.g., the editorial view of Fox News) can facilitate the summative collective self-deception of other groups (e.g., Trump supporters deceiving themselves through Fox News as they deceive their peers).\nWhite evangelical Christians\u2019 support of Donald Trump constitutes a form of collective self-deception. \nThe Public Religion Research Institute in March, 2018, found that 75% of white evangelical Christians (81% of men and 71% of women) had a positive opinion of Donald Trump and 22 percent held an unfavorable view (Burton, 2018). Yet in 2011, only 30 percent of white evangelicals said that \u201can elected official who commits an immoral act in their personal life can still behave ethically and fulfill their duties in their public and professional life\u201d (Kurtzleben, 2016). The dramatic change is probably due to positive situating self-\ndeception. That is, it seems likely that white evangelical Christians have found themselves among people and \nleaders who have reversed their position on the significance of immoral acts in private lives. For example, James Dobson, a prominent Christian conservative, wholeheartedly supports and supported Trump in spite of overwhelming evidence that Trump has been unfaithful to each of his wives (Dobson 2016). Likewise Pat Robertson, Jack van Impe, Jerry Falwell, Jr., Kenneth Copeland, and John Hagee have wholeheartedly supported the president. It seems clear that this is self-deception motivated by the expectation that Trump would appoint conservative federal judges and lead to the overturning of Roe vs. Wade, among other things. Trump is the savior who has removed the yoke of oppression from Christians (such oppression seems to lie in requiring Christians to provide goods and services equally to gay people, allowing transpeople to use their \nrestroom of choice, and in being wished a happy holiday). They have begun to use the language of freedom \nof religion to describe these accommodations to others\u2019 freedom. Some describe him in messianic terms, predicting he will usher in the end-times where the true believers will be saved and the rest of humankind will be thrown into hell. \nA related collective self-deception of the Christian right is of dominionism, centered on the erasure of \nthe separation of church and state. Many Christian fundamentalist denominations advocate for Christians control of all political and cultural institutions, seeking to make the United States a Christian government as it was interpreted to be by certain Christian sects early in US history (Brockman, 2016). The supposed attacks on Christians\u2019 beliefs, support this narrative. Recent polling shows that 57% of white evangelical Protestants feel that they are persecuted for their beliefs, which is more than 30% of white mainline \nProtestants, 26% of white Catholics, 40% of non-white Protestants, 23% of those unaffiliated, and 66% of \nall Americans (Green, 2017). Proponents of freedom of religion seem to self-deceptively misremember the Constitution and engage in selective evidence gathering about practices and beliefs at the beginning of the Republic.\nEighty-one percent of white evangelicals voted for a president whose personal life was more-than-\nallegedly everything they preached against in church \u2014 adultery, multiple marriages, divorces, a bizarre work ethic (\u201cdeals\u201d and bankruptcies), broken promises, lies, racism, and sexual assault. In exchange for their principles, Trump gives them anti-abortion federal judges and the \u201cfreedom of religion\u201d among other things (Mehta, 2017). Looking back to the New Testament, one cannot imagine Christ saying to his disciples, \nthat the end justifies the means, and therefore the disciples could make shady deals with High Priests or \nPhilistines to avoid imprisonment, or that Christ approved of Judas\u2019s betrayal of him because it would lead to his believers\u2019 salvation.\nMany of the evangelicals rationalize their behavior, either by positive situating social self-deception\u2014\nspending time with people who agree with them\u2014or seeking positive persuasive social self-deception, such as finding \u201ccognitive authorities\u201d like James Dobson to support their pro-Trump views. Apart from dominionism and the view that Trump is saving evangelicals from persecution, Trump\u2019s evangelical supporters have based their enthusiasm on likening the president to Persia\u2019s King Cyrus II (the Great). \n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003123\nThough a pagan, Cyrus was seen as an instrument appointed by God, as recounted in the Book of Isaiah, \nbecause he freed the Jews from captivity in Babylonia and returned them to Jerusalem so that they could rebuild the Temple and to restore Jerusalem in the sixth century B.C.E. He is celebrated for having allowed the people of the lands he conquered to retain their traditions. Like Trump, he was a powerful historical figure who is not a God or savior; indeed, Cyrus was not even a Jew. Like Cyrus, they assert, Trump is God\u2019s \ninstrument and his actions as a strong leader fighting on the side of the righteous serve God\u2019s master plan, \nwhich will involve building the Third Temple on the Temple Mount. A thousand theaters in the United States screened The Trump Prophecy before the mid-term elections, which told the story of a former firefighter, \nMark Taylor, who claimed that God told him in 2011 that Trump would be elected president. As the film reaches a crescendo, Taylor (represented by an actor) picks up a Bible and turns to the 45\nth chapter of Isaiah, \nwhich tells of God\u2019s anointment of King Cryus. The film was produced by professors and students at Liberty University, where Jerry Falwell, Jr. serves as chancellor (Stewart, 2018). For evangelicals as well as right-wing Jews, the greatest evidence of Donald Trump\u2019s connection to Cyrus is his defiance of conventional wisdom in Middle East politics, in particular his support of right-wing policies in Israel through moving \nthe US embassy to Jerusalem as well as other actions. Trump has been quick to embrace and foster the \ncomparison (Sommer, 2018). A New York Times op-ed reported that Ralph Dollinger, who leads a weekly Bible study frequently attended by the Vice President frequelty refers to Trump as \u201dKing Trump.\u201d As the op-ed writer, who frequently reports on evangleicals, explained: \nThe great thing about kings like Cyrus, as far as today\u2019s Christian nationalists are concerned, is that they don\u2019t have to \nfollow rules. They are the law. This makes them ideal leaders in paranoid times\u2026. Another important thing to understand about Cyrus is that he is not a queen. In the Christian nationalist world, legitimate political power is largely male power\u2026. This isn\u2019t the religious right we thought we knew. The Christian nationalist movement today is authoritarian, paranoid and patriarchal at its core. They aren\u2019t fighting a culture war. They\u2019re making a direct attack on democracy itself. (Stewart, 2016)\nIn fact, careful reading of the Bible would clearly distinguish Cyrus from Trump. For example, Cyrus did not \nfoster divisiveness, hatred, false accounts, or challenges to civil authorities or institutions to maintain his authority. Reviewing all the hard facts and evidence that have to be ignored, the amount of rationalization and motivated self-deception is astounding, particularly not only in excusing Trump\u2019s behavior before he became a politician but also his daily activity on the campaign trail and in the presidency. \nPerhaps a more astounding case of motivated social self-deception, either situating or persuasive, was \nthe attempted election of Roy Moore in Alabama to the US Senate. Despite the testimony of four women who accused Moore of predatory behavior and sexual assault against them when they were adolescents, his statements which did not tend to be exculpatory, and his two-time removal from the bench for infringements against the Constitution, his supporters claimed that any evidence of wrongdoing on his part was fake news or manufactured evidence, and that he would have represented God\u2019s will for the country in the Senate. They engaged in the selective avoidance of evidence and the selected presentation of evidence. Motivated self-deception sacrifices facts, reason, and evidence in the support of a priori beliefs (or illusions) of group think, or collective social self-deception, which at this point seem to be the posture of many right religious zealots. \nThe Cyrus comparison, dominionism, and similar religious views are illustrations of collective self-\ndeception, fostered by \u201ccognitive authorities,\u201d such as specific religious leaders, peer leaders, media spokesmen, and media organizations. The next section will look at why such people may be motivated to deceive themselves and others\u2014and why some people are motivated to believe them. \nMotivations for Deception, Self-Deception, Social Self-Deception, \nand Collective Self-Deception\nWhether conscious or unconscious, motivation must exist for all of the methods and varieties of deception \nand self-deception. At the conscious level, many Trump supporters are driven by hatred, lust for power, and/or greed. Certainly, the wealthy 1% seek to keep and expand their wealth. The same can likely be said \n124 \u2003  \u2003T.J. Froehlich\nof Republicans who hold political power. But the average Trump supporter may not have wealth or political \npower. Thomas Pettigrew\u2019s (2017) paper, \u201cSocial Psychological Perspectives on Trump Supporters,\u201d shines a light on this group. Without dismissing the political factors that may be at work, he identifies an array of factors reflecting five major social psychological phenomena that account for the bulk of Trump supporters\u2019 devotion: authoritarianism, social dominance orientation (SDO), prejudice, low intergroup contact, and \nrelative deprivation. \nPettigrew documents that many Trump supporters are attracted to authoritarian characters. \nAuthoritarianism is characterized by such traits as \u201cdeference to authority, aggression toward outgroups \n[meaning any group with which the individual does not identify], a rigidly hierarchical view of the world, and resistance to new experience\u201d (Pettigrew, 2017 , p. 108). Authoritarians see the world as dangerous and fear guides their response to it. While there is a debate among social psychologists about whether authoritarianism is a personality construct or a political ideology, Pettigrew argues that \u201cthere is no necessary conflict between these two perspectives\u201d and that authoritarianism usually starts as a personality orientation which then leads to an engagement with right-wing political ideology. From an authoritarian view, motivation lies in fear, and the rhetoric of Trump provides fuel for the fire, which \nleads them to consider him to be a sort of authority of matters of American security, such as securing \nthe borders against outgroups through such things as a border wall between the US and Mexico.\nPettigrew defines SDO is as \u201can individual\u2019s preference for the societal hierarchy of groups and \ndomination over lower-status groups\u201d (p. 108). People who want to maintain the current social hierarchy have an SDO. They believe members of other groups are inferior to members of their own. People with a strong SDO are \u201ctypically dominant, driven, tough-minded, disagreeable, and relatively uncaring seekers of power\u201d (p. 108). Trump\u2019s assertions that he alone can solve the nation\u2019s problems and that those who oppose him are \u201closers\u201d is a good example. Losers now include all newspapers and media who are critical of him, while Fox News, Republicans, and conservatives are winners. I would argue that the motivation \nbehind this factor is a need for self-justification or self-righteousness, that people want to feel that the \ncrowd that they have supported and which supports them is better than other people in the outgroups \u2013 that conservatives are better than liberals. We justify and self-deceive ourselves about the justification through our in-group who reinforce our and the group\u2019s self-justification.\nThe notion of rampant political corruption is related to this sense of superiority. Trump kept \npromising that he was \u201cgoing to drain the swamp.\u201d Yet he has installed many Washington insiders and his administration has been remarkably corrupt. But his supporters do not seem to be concerned. Philosopher Jason Stanley argues that their notion of corruption differs from the conventional understanding. He writes that for fascist politicians, corruption exists in contrast to purity instead of in contrast to lawfulness. \nOfficially, the fascist politician\u2019s denunciations of corruption sound like a denunciation of political corruption. But such \ntalk [i.e., of corruption] is intended to evoke corruption in the sense of the usurpation of the traditional order. (Stanley, 2018) \nReferencing Stanley\u2019s argument, journalism professor Peter Beinart asserted in the Atlantic Monthly: \nWhen Trump instructed Cohen to pay off women with whom he\u2019d had affairs, he may have been violating the law. But he was upholding traditional gender and class hierarchies. Since time immemorial, powerful men have been cheating on their wives and using their power to evade the consequences. \nBeinart contrasts conservative acceptance of Trump\u2019s behavior with their reaction to a case in Iowa in \nwhich a Latino was accused of killing a white woman. As he explains, it \u201csignifie[d] the inversion\u2014the corruption\u2014of that \u2018traditional order.\u2019\u201d \nThroughout American history, few notions have been as sacrosanct as the belief that white women must be protected from \nnonwhite men\u2026. For many Republicans, Trump remains uncorrupt\u2014indeed, anticorrupt\u2014because what they fear most isn\u2019t the corruption of American law; it\u2019s the corruption of America\u2019s traditional identity. (Beinart, 2018).\n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003125\nBeinart\u2019s approach at least partially explains why Trump\u2019s supporters ignore his administration\u2019s \nrampant corruption. It seems to underscore the orientation of the ingroup, while male privilege. Beinart \nearlier notes that this point of view trades in a nostalgia for a golden time when white men ruled and dominated in an attempt restore a time of American greatness (Beinart, 2014). Nostalgia, however misplaced, does seem to motivate many in the conservative sectors. \nNostalgia also plays a role in prejudice, Pettigrew\u2019s third pheomenon. Trump supporters are anti-\noutgroup generally as well as anti-immigrant. In the 2016 election, Trump launched full scale attacks on immigrants, Mexicans and Muslims. His actions in office have reinforced that stance: bans on Muslims entering the country from certain Muslim countries, harsh restrictions on asylum, the separation of children from their parents at the border as a measure to discourage immigration, and claiming that some white nationalists are \u201cvery fine people.\u201d Support for Trump correlates highly with a standard scale of modern racism. \nPettigrew (p. 108) also observes that there is growing evidence that Trump\u2019s white supporters have \nlittle contact with groups other than their own. They have less experience with minorities such as Muslims, Mexicans, or even Black Americans, than other Americans. Low intergroup contact makes it easier to \ndismiss members of other groups as foreign, un-American, or inferior. Ignorance of others allows one to \nself-enforce negative stereotypes, as in Trump\u2019s references to immigrants as \u201canimals\u201d (Davis & Chokshi, 2018).\nPettigrew\u2019s fifth factor, relative deprivation, is particularly supportive of collective social self-deception. \nA myth arose after the election that Trump had won because he appealed to poor and unemployed people. However, Trump supporters were less likely than others to be unemployed, employed part-time, or looking for work. And those voters living in districts with more manufacturing were actually less inclined to vote for Trump. Yet the original narrative rightly identified a sense of deprivation. It just failed to identify that this was a perception of deprivation, not actual deprivation. As Pettigrew observed: \nTrump adherents feel deprived relative to what they expected to possess at this point in their lives and relative to what \nthey erroneously perceive other \u201cless deserving\u201d groups have acquired. Rapidly rising costs of housing and prescription drugs have aggravated their financial concerns. Their savings may not allow the type of ideal retirements they had long envisioned. And hopes for their children advancing beyond their status and going to college are being dashed by rising tuitions. (p. 111). \nThus Trump supporters nurture resentment, which motivates their deception and self-deception. Hours \nof Fox News and social media sites denigrating \u201cwelfare queens,\u201d welfare programs, and the media\u2019s and advertising\u2019s version of what ordinary American home is supposed to be like are fanning the flames. Trump supporters feel impotent to change their lot in life, but feel they can gain potency through elevating their ingroup by supporting someone who promises to defend the existing social hierarchy. They feel that they \nare victims of the forces of politics, corporations, education, and demographic shifts and the president\u2019s \nfocus on those themes makes them feel empowered. Trump\u2019s notion of self-empowerment ironically lies beside his claim that they have little power, but this irony appears to elude them.\nThe notion of resentment is also a key motivator for the self-deception of Trump supporters, and \nit may be one of the political factors Pettigrew acknowledges as working alongside the psychological factors he identifies. Salena Zito and Brad Todd wrote a book entitled The Great Revolt: Inside the Populist Coalition Reshaping American Politics, which analyzes the behavior of those people who voted for Barack Obama in 2008 and 2012 and then for Trump in 2016 by examining 10 counties in 10 different states that switched from Obama to Trump. According to the authors, a key commonality in Obama/Trump voters was not trusting \u201cbig banks, big Wall Street, big corporations, the establishment of both parties and \ntheir lobbyists, and big media corporations.\u201d Their statements suggested they did not care about Trump\u2019s \nbehavior or even his rhetoric; it was Zito (2016) who wrote the famous line, in a separate article that referenced the findings of the book, \u201cthe press takes him literally, but not seriously; his supporters take him seriously, but not literally.\u201d Obama voters who voted for Trump did so to lodge a protest against the status quo in America. The fact that he is part of the one percent and that he lies constantly has no power to disrupt their support.\n126 \u2003  \u2003T.J. Froehlich\nIn sum a multitude of factors contribute to the motivation behind self-deception: fear, hatred, ignorance, \nthe search for power and significance, resentment, nostalgia and greed, to name a few. Socioeconomic \nstatus may affect which factors dominate for a particular Trump supporter. We should note that Trump supporters are on a spectrum, from vaguely committed to fully committed. For many, politics exists at the periphery of their lives, and their engagement is minimal, and they are likely to be informed by casual \nhearsay. Yet many Trump supporters are fully engaged, and this likely reflects strong motivations for self-\ndeception. There appears to be an emotional touchstone that drives Trump supporters.\nMedia and Cognitive Authority \nMedia of various sorts can take the role of a cognitive authority. It seems reasonable to assume that loyalty to a particular media source is a measure of one\u2019s belief in them as trustworthy and reliable. Loyalty is a matter not only of belief but behavior. The Pew Research Center did a study of the loyalty of consumers to media and they found the following (unless otherwise specified, findings do not differ by party affiliation; Mitchell et al., 2016):\n \u2013 About half, 51% of Americans say that they are loyal to their news sources, while 48% say they are not particularly loyal. \n \u2013 Nonetheless, 76% of Americans say they usually turn to the same sources for news. This would seem to contradict the belief of 24% who say they are not particularly loyal, as their behavior is quite loyal.\n \u2013 The 46% who both describe themselves as loyal and go to the same sources repeatedly, are disproportionately old and female. This \u201cvery loyal\u201d group is also more likely to follow news than other groups; 67% say they do. They are also more likely to think organizations do a good job informing people and to trust the information they provide.\n \u2013 TV is the preferred news platform among loyal news consumers, as 54% favor it as a news source, while 15% favor news websites/apps, 13% favor radio, 12% favor print newspapers, and 5% favor social \nmedia.\n \u2013 Despite digital advances, most still share news by word of mouth (85%).\n \u2013 Young adults ages 18-29 are less enthusiastic about news than seniors 65+, but they are more likely to \nget their news online than seniors.\n \u2013 Democrats are more trusting of information from the national news media than Republicans are, but moderate Democrats are about as likely to see bias as moderate, liberal Republicans.\n \u2013 Liberal Democrats and conservative Republicans are more likely to get one-sided news from family and friends online, but conservative Republicans are more likely to think that it is OK.\nAs Trump\u2019s presidency has continued, it seems likely that the bias issues have gotten stronger. The results of the Pew study show the diversity of media in play, the variety among news consumers, and their differing levels of loyalty to diverse media. The younger adults are more flexible in their choice of media and more skeptical in the use of traditional media. Yet, all rely on cognitive authorities for news, across the various pipelines in which they convey their message. Most news consumers share news, both fake and real, by word of mouth, meaning they provide attitude toward the content as well as the \u201cfacts\u201d themselves. The question becomes how we contrast a reliable cognitive authority with a dishonest cognitive authority. \nAuthentic Cognitive Authority Versus Pseudo-Cognitive Authority\nCognitive authorities include religious leaders, television news channels, newspapers, organizations, friends and ingroup associates, etc. It would be useful to discuss two specific instances of cognitive authorities, that are exemplary of the current (dis)information marketplace: MSNBC and Fox News. These outlets exhibit comparable levels of bias: on a scale of extreme left, left, left center, least biased, right center, right, extreme right, Media Bias/Fact Check rates MSNBC as \u201cleft\u201d and Fox News as \u201cright\u201d ) so \n(MSNBC, n.d.; Fox News, n.d.).\n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003127\nWith respect to news channels such as MSNBC, Rieh (2010) writes that \u201ctrustworthiness is a core \ndimension in credibility assessment that captures the perceived goodness and morality of the source\u201d; \nand \u201ctrustworthiness is, however, not a synonym for credibility because people also must recognize expertise in order to deem information credible.\u201d Trustworthiness implies that reporting is based on evidence or facts. If there is a question, it can be traced back to sources of evidence or facts, as they are \nknown at the time of reporting. Factual reporting means that the disclosure of truth may be progressive \nor even regressive. The first details of an event may be sketchy, if not incorrect, and what matters is that the reporting is consonant with the latest details of an event, and that it is faithful to the evidence. MSNBC primarily relies on NBC reporters for their news, and while their factual rating is mixed, that is due to MSNBC\u2019s use of political pundits. Reliable cognitive authorities only change the facts they report if they actually change. When they discover errors in their reporting, they make corrections (MSNBC, n.d.). While experts are used, they appear to make appropriate assessments and judgments based on their experience and knowledge. However, many liberals may fall into self-deceptive and collective self-deceptive practices, if they believe MSNBC assessments, unless they independently verify the basis of such assessments. \nFox News, like MSNBC, claims to be trustworthy and have expertise (Fox News). They tout a lineup \nof daily reporters and experts who claim to be reliable and credible. They have instilled these beliefs in their viewers. But Fox News has primarily restricted what fact-checking they do to the Wall Street Journal. Their pro-Trump stories continuously report factually incorrect data. For example, Trump has declared that the Mueller Report completely exonerated him, and all of Fox News and its pundits echo this. But in fact, the Mueller report explicitly stated that he could not conclude that Trump was in fact exonerated. When reporting that a \u201cwitchhunt\u201d had tarnished Trump\u2019s otherwise unblemished reputation, Fox News and its pundits rarely reference the number of indictments and guilty pleas that resulted from the Mueller investigation. Using a poll by Monmouth University, Fox News reported that 48% of the American public \ntrusted CNN more than Trump while 35% trusted Trump more than CNN, and 45% trusted MSNBC more \nthan Trump and 32% trusted Trump more than MSNBC. Fox reported that it had the lowest trust rating: 30% trusted it over Trump and 20% trusted Trump more than Fox (National\u2026., 2018). It seems ironic that they reported their low trustworthy status. \nWhile the overall trust is not great, this is not true for selected audiences. According to a Pew \nResearch Center survey, \u201cFox News was the main source [of news] for 40% of Trump voters\u201d during the 2016 election (Mitchell, Gottfried & Barthel, 2017). Another Pew survey summarizes, \u201cWhen it comes to choosing a media source for political news, conservatives orient strongly around Fox News. Nearly half of consistent conservatives (47%) name it as their main source for government and political news\u201d (Mitchell, Matsa, Gottfried & Kiley, 2014). \u00a0\nAccording to Eric Wemple, the influence of Fox News cannot be underestimated: \nThere\u2019s simply no outlet that dominates any other part of the political spectrum in the way Fox News dominates the \nright. With that dominance, Fox News has done great damage. It\u2019s not as if Fox News\u2019s influence extends to only however many millions may be viewing in prime time. There\u2019s what experts call a \u201cmedia ecosystem\u201d out there, where people take nonsense uttered on Fox News, then share it on Twitter, on Facebook, with their neighbor. Nonsense has a high pass-around rate. (Wemple, 2019)\nSome of the fake stories that Fox News has propagated include: fear mongering about caravans overrunning \nthe southern U.S. border; a claim that rather than Russia, Seth Rich had provided emails to WikiLeaks, and then been murdered for it in Washington DC in 2016 (Wemple, 2019); the \u201cdeep state\u201d conspiracy theory, which asserts that President Trump was a victim of a plot by the members of the national security establishment, now leading to an investigation by Attorney General William Barr to investigate such \n\u201cspying\u201d activity; and Hillary Clinton\u2019s responsibility for the Benghazi attacks. As noted at the beginning \nof the paper, Fox News lacks journalistic integrity in that publicizes right-wing conspiracy theories without factual basis. While they may retract such stories eventually, such retractions do not prevent them from having an effect.\n128 \u2003  \u2003T.J. Froehlich\nFox News fans believe the nonsense that it creates, propagates, transmits, and retransmits, despite \nthe fact that, Politifact, for a time period that is not specified, estimates that of the statements \u201cmade \non air by Fox,\u00a0 Fox News and Fox Business personalities and their pundit guests\u201d: 10% are true, 12% mostly true, 19% half-true, 21% mostly false, 29% false and 9% pants-on-fire false. (Fox\u2019s File, 2018). Thus a majority of statements, 59%, are less than half-true. It should be noted that Politifact\u2019s ratings are \ncompletely transparent; they document the statements that are uttered on the air and by whom and why \nthey apply a particular score. A real cognitive authority would present stories that are consistent, cohesive, and coherent over time, with few inconsistencies or reversals (but not none). This description does not apply to Fox News (Zorn, 2018). Inconsistences abound in the network\u2019s news reporting: the diverse, inconsistent views of the president are repeated on the news without acknowledging such changes, and the conservative vision of not so many years ago seems to have disappeared as Republican leaders and administrators demonstrate a lack of moral character, a failure to implement fiscal responsibility, and, contradicting the libertarian wing of the conservative movement, increasing government intrusion in the form of the carceral state, interference with women\u2019s reproductive rights, and immigration restriction. Instead of promoting second-hand \u201cknowledge,\u201d Fox News promotes second-hand opinion, opinion that \ncould rarely, if ever, be converted into fact. It promulgates a cognitive state in which neither opinion, \nright opinion, or knowledge, but demonstrably \u201cfalse knowledge,\u201d is paraded as fact. In short, it is primarily propaganda. The general aim of such news producers is deception\u2014at best alternative opinions but at worst to substitute lies for knowledge. They promote deception, which facilitates their and their audience\u2019s self-deception and collective self-deception. Deception comes in many flavors. \nDon Fallis\u2019s \u201cThe Varieties of Disinformation\u201d (2014) details the goals of deception. He cites Chisholm \nand Feehan\u2019s \u201cThe Intent to Deceive\u201d (1977 , pp. 143-159) to articulate four of these. The first two, which are achieved by positive deception (causing a false belief) are (1) creating a new false belief and (2) maintaining an existing false belief. The second two use negative deception. They are (3) causing the loss \nof a true belief; and (4) preventing the acquisition of a true belief. To illustrate based on recent events, \nin their attempt to discredit FBI agent Bryce Ohr, Republicans said he played a central role in the Obama Justice Department\u2019s investigation into Trump\u2019s 2016 campaign and had key ties to the infamous Steele dossier. Thus they created a new false belief. William Barr claimed that the Mueller report exonerated Trump, maintaining an existing false belief of Trump and his supporters. When Trump lies, he may cause the loss of a true belief. The claim that the White House staff runs interference with Trump, to prevent him from doing something reckless politically represents prevention of the acquisition of a true belief (Fallis, 2014, p. 140). While Fallis\u2019s classification scheme is enlightening for the aims of deception, as outlined above such deception is linked to social self-deception and collective self-deception: Fox News pundits convince themselves of their own lies while engaging/inflaming the collective self-deception of \nits viewers.\nFox News pundits promote a pseudo-orthodoxy: rather than \u201cright opinion\u201d (consistent with \ncultural, scientific, or ethical norms), it is conformity with the opinion of \u201cour group\u201d \u2013 biases against \nthe human dignity of some groups, the discrediting of the past and ongoing contributions of immigrants, etc. All of this is reinforced though social media sites on the internet that do a great job of linking (and therefore repeating) disinformation, misinformation, etc. that sometime leads Fox News, as we have seen earlier, to use these sources for their fake news stories. These false cognitive authorities claim to \nbe authorities for Fox News, itself a false cognitive authority, in a perverse inversion of Patrick Wilson\u2019s notion that librarians are authorities about authorities (i.e., they may not know the answer to a particular \nreference question, but they know which authoritative sources to use to answer that question). These \nfalse authorities reinforce each other. A fundamentalist minister endorses Fox News that refers to sites that reinforce and accelerate the same sound bites of disinformation. From president to fundamentalist preacher to news organization, to news channel, to politician, to political PAC to social media, all claim to be authorities about authorities. They echo each other, convincing and reinforcing each other of the same misinformation of their authoritativeness. But they are false authorities promoting not the truth but typically propaganda. This leads to what I have labelled enhancers and accelerators of fake news.\n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003129\nEnhancers and Accelerators\nA study entitled \u201cPrior exposure increases perceived accuracy of fake news,\u201d reported that repeating \ninformation, true or not, increases its believability and this applies to newspaper headlines, statements, or speeches (Pennycook, Cannon & Rand). There is also the Dunning-Kruger effect that suggests that people are uncritical about their own abilities and uncritical of their lack of critical thinking. To put it simply, people of poor intelligence lack the intelligence to recognize it (Dunning\u2013Kruger effect, 2017). Once acquired, false \ninformation is hard to dispel. David Rapp\u2019s research on memory and learning reveals that our brains retain \ninformation without retaining its source and therefore we do not recall a key fact about its validity. He also finds that it is difficult to remember that information we had previously believed is false (Waters and Hargadon, 2017). This suggests the lingering effect of Fox News propagation of false conspiracy theories. Finally, Robert N. Proctor coined a word for the study of culturally induced ignorance or doubt, agnotology, identifying a specialized technique for spreading misinformation that makes information seekers more doubtful of views or information that they already hold (\u201cAgnotology,\u201d 2016). By way of example Proctor described the tobacco industry\u2019s use of advertising to generate doubt that smoking causes cancer or other illnesses. Climate change deniers, proponents of fracking, pesticide manufacturers, and opponents of allegedly \u201cfake news\u201d use a similar approach. The echoing of Trump\u2019s attacks on the justice department, \nthe FBI, the Democratic party and other intelligence agencies on Fox News and alt-right social media plays the same role. \nGenuine cognitive authority involves some intellectual assessment about credibility and trustworthiness \nof the authority. When one thinks of a cognitive authority, one tends to think of a rational process, gathering evidence about whether someone or institution is a real cognitive authority. In the case of Trumpism, it seems less a matter of cognition, but rather one of emotion. Rather than a cognitive authority, Trump and Fox News are emotive authorities presenting themselves as genuine authorities. In fact, the emotive part (based in power, greed, resentment, fear, prejudice, and other motivations mentioned above) enables Trump supporters to dismiss at the ultimate limit the meaning of the content of their message (e.g., Trump \ncould kill someone on Madison Avenue and get away with it). That is, what is extraordinary is that the \nauthority in the case of Trump overrides the content of the message: none of his supporters care if he contradicts himself, makes promises that he does not keep, or lies. Whatever the content of the current message, they cheer. Fox News and Trump possess a pseudo-cognitive authority that utters falsehoods and relies on collective self-deception, reinforced by tribal motivations, such as prejudice, fear, resentment, greed, power and anger, particularly about the status-quo, especially about the imagined status quo. \nSummary\nThis paper attempts to synthesize various theories and data from diverse fields to understand how Trump supporters generate, accept, disseminate and legitimatize fake news, and how they fail to exercise any critical thinking ability about the president, his administration, the GOP , or his religious and media supporters. It began by assessing the varieties of false information on the web, and how people can deceive themselves, first individually, then socially and collectively. As an example of collective self-\ndeception, I have referred to white evangelicals and some of their objectives: the defeat of the separation \nof church and state, the establishment of Christian governance over the United States, and the filling of the courts with conservative judges to achieve the overturn of Roe v. Wade, as well as other goals. Many fundamental religious leaders and their congregations have set the agenda, claiming God as the authority that has appointed Trump as his instrument and the price of not believing correctly is pretty onerous (i.e., hell). These cognitive authorities support each other: Religious leaders are supported by fundamentalist religious programming and their spiritual leaders who in turn support Fox News and similar media and social media. Beneath these cognitive authorities lie motivations of various sorts that seem to drive and anchor their strategies. These motivations are not all of one stripe: they include fear, anger, hatred, resentment, nostalgia, power, prejudice, feeling impotent but gaining potency through \n130 \u2003  \u2003T.J. Froehlich\none\u2019s in-group, greed, among others. These motivations are aggravated and enflamed by cognitive \nauthorities, including Trump. Depending on their place in the spectrum of supporters, members of Trump\u2019s base may vary in their trigger motivations.  GOP senators or representatives trying to retain their \nseats have different incentives from fundamentalist leaders trying to retain their flocks, whose incentives differ from average supporters who feel resentment about their lot in life and nostalgia for a time when \nwhite male privilege was stronger. The psychological studies detail in social psychological terms the \nsource of their motivations: prejudice, preference for one\u2019s ingroup, fear and rejection of outgroups, etc. From this context we build a characterization of good cognitive authorities contrasted with poor cognitive authorities, comparing the practices of MSNBC and Fox News, who are considered equally biased in opposite directions yet exhibit significantly different practices. The fake news stories, motivations and pseudo-cognitive authorities are enhanced and accelerated by such factors as repetition from diverse media echoing the same stories, culturally induced ignorance, the Dunning-Kruger effect, etc. All of these threads culminate in a spiraling, dialectical, self-reinforcing collective social self-deception. False cognitive authority succeeds because of self-deception, especially collective self-deception. Its authority is that of the crowds and leader, but false because it fails to be supported by evidence, facts, or reason. \nIt is an authority based in emotion. It creates a bubble for its adherents that is difficult to breach, but not \nonly that, they mock those who berate those who challenge their view of reality. What is disturbing is that even without Trump this InfoWar will likely continue throughout the world, abetted by foreign meddling, no matter the country.\nThe Role of Information Professionals\nHow can information professionals respond to the Age of Disinformation? When Patrick Wilson (1983) wrote about cognitive authority, he suggested that librarians and information specialists were authorities about authorities (p. 179 ff.), that is, that they know which authorities to consult to answer a query. When the very notion of genuine authority comes under attack, this becomes complicated. It seems that librarians and information specialists should become authorities about false cognitive authorities as well. Consider \nthe first two precepts of the Library Bill of Rights:\nI.  Books and other library resources should be provided for the interest, information, and enlightenment \nof all people of the community the library serves. Materials should not be excluded because of the origin, background, or views of those contributing to their creation.\nII.  Libraries should provide materials and information presenting all points of view on current and \nhistorical issues. Materials should not be proscribed or removed because of partisan or doctrinal disapproval. (Library Bill of Rights).\nThere is a tension between the objective of providing enlightenment (i.e., genuine understanding based on evidence and reasoning) and providing materials representing all points of view, including apparently \nthose containing disinformation or misinformation. Having said that, volumes of disinformation could be \ninstructive about for example, hate literature, if a patron or information seeker has the critical thinking to evaluate it appropriately. \nIn relation to this tension, it might be useful to look at library policies, reflecting on a 1989 debate \nbetween John Swan and Noel Peattie detailed in book entitled The freedom to lie: a debate about democracy (Swan & Peattie, 1989). John Swan argued that role of the librarian is to promote access, and this includes access to all sorts of materials, including that which is intolerable. Thus he argued libraries should have copies of McCalden\u2019s The Holocaust Did Not Happen, a holocaust revisionist tract. Swan\u2019s main points include:\n \u2013 Toleration is meaningless without tolerance for what some may regard as detestable.\n \u2013 The librarian\u2019s truth is freedom, freedom of access.\n \u2013 Librarians are caught in a dilemma: \u201cWe are committed both to the search for truth and the freedom of expressions of untruth.\u201d\n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003131\n \u2013 Given these assumptions, Swan argues that a librarian\u2019s \u201cchief professional commitment must be to \naccess rather than truth\u201d (p. 16). This would include lies and misrepresentations. Denying access to them does not eliminate them \u2014 they still exist in the hearts and minds of those who believe them, and suppressing such ideas will not eliminate them. Librarians work against intellectual freedom when they deny patrons access to materials because \u201cthe ideas in the materials are too dangerous to tolerate.\u201d\nNoel Peattie (1989) takes a more conservative view in response, saying that a library is under no obligation to collect such works, because truth does matter, as an important factor in making collection development decisions. He writes:\n \u2013 Either we do know or do not know some matters of fact, and if we do, then we have no obligation to support lies, or to omit the notion that it is a lie from our consideration in whether to purchase a source or not.\n \u2013 Either we know or do not know some matters of fact. The Holocaust did happen. Climate change is happening.\n \u2013 Of the variety of truths in the library, the librarian can only hope, not know, that a reader has enough \neducation, patience, and discernment, to engage in the sorting process and come out with the right \nanswer.\n \u2013 McCalden\u2019s views are lies, falsehoods deliberately uttered to deceive and hurt people, by a person who really knows the truth but deliberately denies or distorts it. \nTheir debate retains its relevance today. The nature of some of the lies and deceptions have changed, if not considerably enlarged. The American Library Association tends to take Swan\u2019s position, but, given the Age of Disinformation, Peattie\u2019s position may be more realistic. When reflecting on a library\u2019s policy for collection development, it is useful to reconsider a particular library\u2019s position on this issue, and whether \nit still fits the needs of contemporary patrons.\nThe Anti-Enlightenment\nIn many ways the Age of Disinformation is the Age of the Anti-Enlightenment, in which knowledge \ngained systematically and through careful observation of the environment is rejected and replaced by an arrogant anti-science, anti-humanitarian propaganda whose misinformation or disinformation is transmitted through cable broadcasting and social media. The Enlightenment (roughly starting in the 18\nth \ncentury Europe) encompassed a variety of ideas centered on reason as the primary source of authority and rightfulness, not church, royalty, or political or inherited rank. It advanced ideals of individual liberty, constitutional government, separation of church and state and religious tolerance. Many of these notions were and are institutionalized in the United States Constitution and in the structure of the US government. \nIn the current environment, individual liberty is now claimed to support partisan politics (only my politics \nare true), to erase separation of church and state (America was established as a Christian nation), and \nto attack reason and evidence, so as to support intolerance of those whose views are different from my partisan view (intolerance of tolerance). My partisan framework has \u201cthe truth,\u201d particularly in political or religious matters. For Sr. Joan Chittister, a Benedictine nun, the results of the Enlightenment have increasingly favored radical individualism and denigrated the common good (Landers, 2018). Its fruition lies in many examples of contemporary culture. It has become fashionable for radicalized individuals to fail to see the role of public education or to want to contribute to it (no levies, no taxes even when the underfunding seems obvious) or to refuse to take insurance for the common good or to support the general welfare. The notion of a public good has been currently challenged by those who to see that, for example, \neducation for all has benefits for all or that vaccination is a public good, not seen as a private matter, given \nthat the unvaccinated pose risks of medical outbreaks, with serious consequences to public health (for example, the resurgence of measles). Another public good that is often denigrated is the library that has an important role in education.\n132 \u2003  \u2003T.J. Froehlich\nInformation Literacy Programs\nTraining library patrons in information literacy may enable them to distinguish reliable sites from unreliable \nones to evaluate the quality of materials in the library. Most libraries and library associations, such as IFLA, provide basic information literacy guidelines (e.g., How to Spot Fake News, 2017). Librarians are likely to find more success in these programs with political supporters at the margin, with citizens with low political engagement, or with those who have not been indoctrinated. However, they can teach something \nabout the nature of the InfoWars and its importance, to arm their patrons and themselves with tools to \ncope with fake news seekers or substantiators and perhaps to deal more effectively with the warriors of arrogance and ignorance. However, most strong Trump supporters may be immune to information literacy programs, especially because they are so thoroughly consumed with the president. The problem is that Trump supporters are so enveloped with a collective self-deception, continuously reinforced by word of mouth by their ingroup, by the media they embrace, by the conservative religious leaders, media pundits, and political heroes who are their cognitive authorities. Even if they undergo deprogramming, the environment is so laden with the need to continue to belong to their in-group, that they revert easily to their pre-deprogrammed position, something close to an addiction (to anger, self-righteousness, hatred, power or whatever their core emotional touchpoint). Shaming them or bating them will not work, as most \nsociologists agree. Janja Lalich, who has studied cults extensively, suggests that\nmembers of \u201ctotalistic\u201d cults\u2014those that consider their ideology the one true path\u2014share four key characteristics. They 1) \nespouse an all-encompassing belief system; 2) exhibit excessive devotion to the leader; 3) avoid criticism of the group and its leader; and 4) feel disdain for non-members (Jacobs, 2018). \nIn her view, Trump followers so heavily share these traits makes them practically a cult. Lalich notes \nthat Trump\u2019s campaign rallies\u2014he is the first president in history to begin campaigning so soon after his election\u2014are \u201cboth a recruitment technique and a way to keep his followers happy.\u201d \nIs there any chance to reach these people? Lalich suggests using dialog to ask questions and reinforce \ndoubts, using testimonials of previous cult members, if available. The Socratic method may be useful. Thus \ninformation professionals might act (1) as a benumber, such as an electric eel or gadfly (metaphors used \nto describe Socrates), helping the patron to learn something by stinging them into an awareness of their ignorance, the Socratic approach to \u201cwoking\u201d or (2) as a midwife, setting a course for the individual to learn something for themselves by asking them challenging questions. If a Trump supporter came to the library seeking to authenticate Trump\u2019s lie that only a few persons in Puerto Rico were injured as a result of Hurricane Maria an information specialist might lead the patron to sites on both sides of the issue. If the client begins to see than an overwhelming number of sites show that Trump lied, then perhaps the first stage has been achieved. This could lead to elementary lessons in information literacy, i.e., teaching the patron to recognize how to recognize good from bad web sites. Rick Alan Ross of the Cult Education Institute suggests that if conversing with a Trump supporter, pick an emotionally charged issue, such as \nreproductive health rights, and explain that Trump supports defunding Planned Parenthood and holds \noutmoded opinions about women (Matthews, 2018). This might act as a benumbing moment, challenging the interlocuter into an awareness that her idol holds a position contrary to her beliefs. Ross suggests some other techniques. To sway a\u00a0Trump supporter, one can start identifying persons that she respects, looking for people who have spoken in opposition to Trump. In this way, one can play the role of a midwife, by suggesting other high profile figures or sources, that the interlocuter respects. Ross indicates that \u201cthe key to introducing more critical thinking is pointing out ambiguity and nuance, rather than challenging core beliefs directly\u201d (Matthews, 2018). For a Trump supporter, this might mean showing them evidence that a border wall is an ineffective method for maintaining border security and that separating children from their \nparents does not deter asylum seekers. \nInformation professionals, to the extent possible, can strive to be fair in providing balanced and \ntruthful information to clients, and individually and professionally to help restore fairness in reporting in \nmedia. Because of the influence of such media as Fox News, it would be helpful if the fairness doctrine of \n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003133\nthe Federal Communications Commission (FCC) were restored to challenge some of the propaganda pushed \nby such media. This doctrine was introduced in 1949 and required broadcast license holders to present both sides of issues of public importance in a manner that was honest, equitable, and balanced. It was eliminated in 1987 on the basis that it \u201crestricts the journalistic freedom of broadcasters ... [and] actually inhibits the presentation of controversial issues of public importance to the detriment of the public and \nthe degradation of the editorial prerogative of broadcast journalists.\u201d (\u201cFCC Fairness Doctrine,\u201d 2018). In \n1987 in an FCC Video, NBCUniversal, made the claim, \u201cToday we reaffirm our faith in the American people. Our faith in their ability to distinguish between fact and fiction without any help from government\u201d (\u201cFCC Fairness Doctrine,\u201d 2018, footnote 18 of Wikipedia entry). It seems obvious that many American people lack this ability. Furthermore, the current government\u2019s lack of transparency and its publishing misinformation or disinformation or withholding information that would enhance the public good (for example, the real effects of climate change) aggravates the matter, by diminishing trust in government information.\nAs information professionals, we could encourage ourselves, personally and professionally, and others \nto take the Pro-Truth Pledge (https://www.protruthpledge.org/) where we promise only to share verified truth as completely as possible, to honor truth (to acknowledge and defend it) and to encourage truth (to \nask for lies to be retracted, educate ourselves and others, and acknowledge genuine experts). It would help \naddress and beat back the verbal pollution that exists in public space. \nGiven the ethical view that the information specialist should work with the client\u2019s view, it is difficult \nto see how the specialists could proceed when confronted with a fake news seeker or validator, except to politely profess ignorance for themselves or try Socratic methods. It is more likely that librarians can try to prevent willing, new, or regular patrons from falling prey to fake news, through lessons in information literacy, a commitment to competence, and lively engagement. This presumes that places like libraries and information centers will persist and continue to receive funding. Contrary to the belief that libraries can be or are replaced by the internet, they are even more now an essential bastion for democracy, as argued \nby such authors as Eric Klinenberg, in an article for the New York Times, \u201cTo restore civil society, start \nwith the library.\u201d He states, \u201cLibraries stand for and exemplify something that needs defending: the public institutions that \u2014 even in an age of atomization, polarization and inequality \u2014 serve as the bedrock of civil society.\u201d He was particularly worried about the library as a public good, whose value was attacked by an economist in an ill-conceived article in Forbes magazine who argued that \u201clibraries no longer served a purpose and did not deserve public support\u201d and that they should be replaced by an Amazon-like operation as \u201ca free-market option.\u201d The reaction was so strongly negative that Forbes magazine retracted the article (Klinenberg, 2018). \nHopefully, this research article has richly pointed out the problems of pseudo-cognitive authorities who \nseduce their believers into smug ignorance and the risks of having no authorities on the authorities, with \nno way to distinguish between sources, the honest and the dishonest ones. Libraries can create literate and \ncritically thinking citizens, a serious antidote to fake news and their purveyors in the InfoWars in the Age of Disinformation. \nBibliography\nAgnotology (2016). In: Wikipedia. Retrieved September 9, 2016, from https:/ /en.wikipedia.org/wiki/Agnotology\nBarel, O. (2019, April 9). Why are Israeli elections extremely sensitive to fake news? Council on Foreign Relations. Retrieved \nApril 10, 2019, from https:/ /www.cfr.org/blog/why-are-israeli-elections-extremely-sensitive-fake-news \nBenkler, Y., Faris, R. & Roberts, H. (2018). Network propaganda: manipulation, disinformation, and radicalization in american \npolitics. Oxford, UK: Oxford University Press.\nBeinart, P . (2014, November 13). The Republican obsession with \u2018restoring\u2019 America. Atlantic Monthly. Retrieved April 12, 2019, \nfrom https:/ /www.theatlantic.com/politics/archive/2014/11/the-republican-obsession-with-restoring-america/382689/ \nBeinart, P . (2018, August 22). Why Trump supporters believe he is not corrupt. Atlantic Monthly. Retrieved September 9, 2018, \nfrom https:/ /www.theatlantic.com/ideas/archive/2018/08/what-trumps-supporters-think-of-corruption/568147/?utm_source=facebook&utm_campaign=the-atlantic-fb-test-312-4-&utm_content=edit-promo&utm_medium=social \nBooth, W., & Birnbaum, M. (2017, November 14). British and Spanish leaders say Russian trolls meddled in their elections. \nWashington Post. Retrieved April 10, 2019 from https:/ /www.washingtonpost.com/world/europe/britain-and-spanish-\n134 \u2003  \u2003T.J. Froehlich\nleaders-say-russian-trolls-meddled-in-their-elections/2017/11/14/51ffb64a-c950-11e7-b506-8a10ed11ecf5_story.\nhtml?utm_term=.be1a1ae5908e \nBrockman, D. R. (2016, June 2). The radical theology that could make religious freedom a thing of the past. Texas Observer. \nRetrieved September 14, 2018, from https:/ /www.texasobserver.org/dominion-theology/ \nBurton, T. I. (2018, April 20). Poll: White evangelical support for Trump is at an all-time high: Republican partisanship and a \nutilitarian evangelical strategy might help explain why. Vox . Retrieved September 9, 2018, from https:/ /www.vox.com/\nidentities/2018/4/20/17261726/poll-prri-white-evangelical-support-for-trump-is-at-an-all-time-high \nDavis, J. H. & Chokshi, N. (2018, May 17). Trump defends \u2018animals\u2019 remark, saying it referred to MS-13 gang members. New \nYork Times. Retrieved September 13, 2018, from https:/ /www.nytimes.com/2018/05/17/us/trump-animals-ms-13-gangs.html\nDeWeese-Boyd, I. (2016). Self-deception. In E. N. Zalta (Ed.). The Stanford encyclopedia of philosophy (Spring 2012 ed.). \nRetrieved April 24, 2018, from https:/ /plato.stanford.edu/entries/self-deception/\nDings, R. (2017, December). Social strategies in self-deception. New Ideas in Psychology, (47), 16-23. Elsevier Science.Dobson, J. (2016, September 23). Why I am voting for Donald Trump. Christianity Today. Retrieved September 4, 2018, from \nhttps:/ /www.christianitytoday.com/ct/2016/october/james-dobson-why-i-am-voting-for-donald-trump.html \nDunning\u2013Kruger effect. (2017). In: Wikipedia. Retrieved November 24, 2017, from https:/ /en.wikipedia.org/wiki/\nDunning%E2%80%93Kruger_effect\nFallis, D. (2014). \u201cThe varieties of disinformation\u201d In: L. Floridi and P . Illari (eds.), The Philosophy of Information Quality. \nSynthese Library 358, DOI 10.1007/978-3-319-07121-3_8. Switzerland: Springer International Publishing.\nFCC Fairness Doctrine. (2018). In: Wikipedia. Retrieved September 15, 2018, from https:/ /en.wikipedia.org/wiki/FCC_fairness_\ndoctrine \nFischer, E., Heide, D., & Hoppe, T. (2018, December 05). Battle of the bots: Germany muscles up to take on Russian trolls. \nHandelsblatt. Retrieved April 10, 2019, from https:/ /www.handelsblatt.com/today/politics/battle-of-the-bots-germany-muscles-up-to-take-on-russian-trolls/23716162.html?ticket=ST-75324-RDD7nykW36s5RDnoQXjV-ap2 \nFox\u2019s File. (2018). Politifact. Retrieved September 7, 2018, from https:/ /www.politifact.com/punditfact/tv/fox/ Fox News. (n.d.). Media Bias/Fact Check. Retrieved April 14, 2019, from https:/ /mediabiasfactcheck.com/fox-news/Froehlich, T. (2017). A not-so-brief account of current information ethics : the ethics of ignorance, missing information, \nmisinformation, disinformation and other forms of deception or incompetence. BiD: textos universitaris de bibliote -\nconomia i documentaci\u00f3, n\u00fam. 39 (desembre). Retrieved September 4, 2018, from http:/ /bid.ub.edu/en/39/froehlich.htm doi: http:/ /dx.doi.org/10.1344/BiD2017.39.8\nGreen, E. (2017, March 10). White evangelicals believe they face more discrimination than Muslims, The Atlantic. Retrieved \nFebruary 3, 2019 from: https:/ /www.theatlantic.com/politics/archive/2017/03/perceptions-discrimination-muslims-christians/519135/?utm_source=atltw. Data is from a poll of the Public Religion Research Institute cited in this source.\nHolmes, J. (2018, July 25). Trump\u2019s war on the truth has officially gone full Orwell, Esquire. Retrieved September 3, 2018, from \nhttps:/ /www.esquire.com/news-politics/a22547037/donald-trump-orwellian-truth-reality-eyes-ears/\nHow to Spot Fake News. (2019). International Federation of Library Associations publications, updated August 13, 2017. \nRetrieved April 18, 2019 from https:/ /www.ifla.org/publications/node/11174 \n Jacobs, T. (2018, June 21). A cult expert finds familiar patterns of behavior in Trump\u2019s GOP . Pacific Standard. Retrieved April 13, \n2019, from https:/ /psmag.com/news/a-sociologist-explains-the-similarities-between-cults-and-trumps-gop   \nKarp, P . (2018, November 20). Russian twitter trolls stoking anti-Islamic sentiment in Australia, experts warn. The Guardian. \nRetrieved April 10, 2019, from https:/ /www.theguardian.com/australia-news/2018/nov/20/russian-twitter-trolls-stoking-anti-islamic-sentiment-in-australia-experts-warn\nKessler, G., Rizzo, S., & Kelly, M. (2019, April 01). President Trump has made 9,451 false or misleading claims over 801 days. \nRetrieved April 10, 2019, from https:/ /www.washingtonpost.com/politics/2019/04/01/president-trump-has-made-false-or-misleading-claims-over-days/?utm_term=.96d7585bb3fe \nKlinenberg, E. (2018, September 8). To restore civil society, start with the library. New York Times. Retrieved September 15, \n2018, from https:/ /www.nytimes.com/2018/09/08/opinion/sunday/civil-society-library.html\nKurtzleben, D. (2016, October 23). POLL: White evangelicals have warmed to politicians who commit \u2018immoral\u2019 acts. NPR. \nRetrieved September 5, 2014, from https:/ /www.npr.org/2016/10/23/498890836/poll-white-evangelicals-have-warmed-to-politicians-who-commit-immoral-acts\nLamba, S. & Notyananda, V. (2014, August). Self-deceived individuals are better at deceiving others. Plos One, vol 9, 8.Landers, J. (2018, July 31). Joan Chittister illustrates how mastering humility could save America. The Chautauquan Daily. \nRetrieved January 6, 2019 from: http:/ /chqdaily.com/2018/07/joan-chittister-illustrates-how-mastering-humility-could-save-america/\nLibrary Bill of Rights. (2019). American Library Association. Retrieved April 10, 2019 from http:/ /www.ala.org/advocacy/\nintfreedom/librarybill \nLyu, S. (2018, August). \u00a0 \u201cDetecting \u2018deepfake\u2019 videos in the blink in the eye,\u201d\u00a0The Conversation. \u00a0Retrieved September 9, \n2018, from https:/ /theconversation.com/amp/detecting-deepfake-videos-in-the-blink-of-an-eye-101072?__twitter_impression=true \n The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news  \u2003135\nMatthews, D. (2018, July 09). We asked a cult deprogramming expert how to talk your friends out of voting for Donald Trump. \nSplinter. Retrieved April 14, 2019, from https:/ /splinternews.com/we-asked-a-cult-deprogramming-expert-how-to-talk-\nyour-f-1793857315 \nMehta, H. (2017, September 14). .Shocker: White evangelicals no longer care about a president\u2019s private morality. Friendly \nAtheist. Retrieved January 15, 2018, from http:/ /www.patheos.com/blogs/friendlyatheist/2017/09/14/shocker-white-evangelicals-no-longer-care-about-a-presidents-private-morality/#hCVezDDYSWpcGZB8.99\nMcGregor, J. (2016, December 29). When telling the truth is actually dishonest. Washington Post. Retrieved April 11, 2019, \nfrom https:/ /www.washingtonpost.com/news/on-leadership/wp/2016/12/29/when-telling-the-truth-is-actually-dishonest/?utm_term=.5b7a76aa69c1 \nMitchell, A., Barthel, M., Shearer, E., & Gottfried, J. (2016, July 7). News sources and audience loyalty. Pew Research Center \nJournalism and Media. Retrieved from https:/ /www.journalism.org/2016/07/07/loyalty-and-source-attention/ \nMitchell, A., Gottfried, J., & Barthel, M. (2017, January 18). Trump, Clinton Voters Divided in Their Main Source for \nElection News. Pew Research Center Journalism and Media. Retrieved April 14, 2019, from https:/ /www.journalism.org/2017/01/18/trump-clinton-voters-divided-in-their-main-source-for-election-news/ \nMitchell, A., Matsa, K. E., Gottfried, J., & Kiley, J. (2014, September 21). Political Polarization & Media Habits. Pew Research \nCenter Journalism and Media. Retrieved April 14, 2019, from https:/ /www.journalism.org/2014/10/21/political-polarization-media-habits/ \nMorin, R. & Cohen, D. (2018, August 19). Giuliani: \u2018Truth isn\u2019t truth.\u2019 Politico. Retrieved September 4, 2018, from https:/ /www.\npolitico.com/story/2018/08/19/giuliani-truth-todd-trump-788161\nMSNBC. (n.d.). Media Bias/Fact Check. Retrieved April 14, 2019, from https:/ /mediabiasfactcheck.com/msnbc/ National: \u2018fake news\u2019 threat to media; editorial decisions, outside actors at fault. Monmouth University Poll. (2018, April 2). \nRetrieved April 14, 2019, from https:/ /www.monmouth.edu/polling-institute/documents/monmouthpoll_us_040218.pdf/\nPennycook, G., Cannon, T. D., & Rand, D. G. (2018). Prior exposure increases perceived accuracy of fake news. Journal \nof Experimental Psychology: General, 147(12), 1865-1880. doi: http:/ /dx.doi.org.proxy.library.kent.edu/10.1037/xge0000465 \nPettigrew, T. F. (2017). \u201cSocial psychological perspectives on Trump supporters,\u201d Journal of Social and Political Psychology (5). \nRetrieved August 18, 2017, from https:/ /jspp.psychopen.eu/index.php/jspp/article/view/750/html \nPTI. (2018, August 03). Russia now targeting elections in India, Brazil: Oxford expert tells US lawmakers. Times of India. \nRetrieved April 10, 2019, from https:/ /timesofindia.indiatimes.com/india/russia-now-targeting-elections-in-india-brazil-oxford-expert-tells-us-lawmakers/articleshow/65249727.cms \nRieh, S. Y. (2010). Credibility and cognitive authority of information. In: M. Bates & M. N. Maack (Eds.) Encyclopedia of Library \nand Information Sciences, 3rd Ed. (pp. 1337\u20131344), New York: Taylor and Francis Group, LLC. Retrieved: August 18, 2017, from http:/ /hdl.handle.net/2027.42/106416 \nRieh, S. Y. (n.d.). Cognitive authority. Retrieved from August 18, 2017, from http:/ /rieh.people.si.umich.edu/papers/rieh_\nIBTheory.pdf\nSommer, A. K. (2018, January). Christians and Jews now compare Trump to Persian King Cyrus \u2013 Will he build the third temple? \nHaaretz. Retrieved January 25, 2018, from https:/ /www.haaretz.com/israel-news/trump-s-compared-to-persian-king-cyrus-will-he-build-the-third-temple-1.5628538\nStanley, J. (2018). How Facism Works. New York: Random House.Stewart, K. (2018, December 31). Why Trump reigns as King Cyrus. New York Times. Retrieved January 1, 2019 from: \nhttps:/ /www.nytimes.com/2018/12/31/opinion/trump-evangelicals-cyrus-king.html?em_pos=large&emc=edit_ty_20190101&nl=opinion-today&nlid=967626edit_ty_20190101&ref=img&te=1.\nSwan, J. & Peattie, N. (1989). The Freedom to Lie: A Debate About Democracy. Jefferson, NC: McFarland & Co., Inc. Publishers.The Daily Beast. (2018, August 16). \u2018Fox & Friends\u2019: Defeat of \u2018Communist Japan\u2019 proves U.S. is a great nation. Retrieved April \n10, 2019, from https:/ /www.thedailybeast.com/fox-and-friends-defeat-of-communist-japan-proves-us-is-a-great-nation \nTruthiness. (2017). In: Wikipedia.. Retrieved November 24, 2017, from http:/ /en.wikipedia.org/wiki/TruthinessVon Hippel, W., & Trivers, R. (2011). The evolution and psychology of self-deception. Behavioral and Brain Sciences, 34, 1-16. \ndoi:10.1017/S0140525X10001354.\nVosoughi, S., Roy, D., & Aral, S. (2018, March 9). \u201cThe spread of true and false news online,\u201d Science, 359 (6380), 1146-1151. \nRetrieved September 5, 2018, from http:/ /science.sciencemag.org/content/359/6380/1146\nWaldman, P . (2018, December 14). Is there anything Trump touches that isn\u2019t corrupt? Washington Post. Retrieved April \n11, 2019, from https:/ /www.washingtonpost.com/opinions/2018/12/14/is-there-anything-trump-touches-that-isnt-corrupt/?utm_term=.f75502432957 \nWarzel, C. (2018, February 11).\u00a0 \u201cHe predicted the 2016 fake news crisis. Now he\u2019s worried about an information apocalypse. \nBuzzFeed News. Retrieved September 9, 2018, from https:/ /www.buzzfeed.com/charliewarzel/the-terrifying-future-of-fake-news?utm_term=.jwvN21Z4O#.qrkw5JrxY\u00a0\nWaters, A.; Hargadon, S. (2017, Spring). \u201cMind the misinformation,\u201d Northwestern Campus Life. Retrieved August 18, 2017, \nfrom http:/ /www.northwestern.edu/magazine/spring2017/campuslife/mind-the-misinformation-david-rapp-explains-appeal-of-fake-news.html\n136 \u2003  \u2003T.J. Froehlich\nWemple, E. (2019, April 11). Yes, Fox News matters. A lot. Washington Post. Retrieved April 14, 2019, from https:/ /www.\nwashingtonpost.com/opinions/2019/04/11/yes-fox-news-matters-lot/?noredirect&utm_term=.8ad57d66b52f \nWilson, P . (1983). Second-Hand Knowledge: an Inquiry into Cognitive Authority, Westport, CT: Greenwood Press. \nZito, S. (2016, September 23). Taking Trump seriously, not literally. The Atlantic. Retrieved September 7, 2018, from https:/ /\nwww.theatlantic.com/politics/archive/2016/09/trump-makes-his-case-in-pittsburgh/501335/ \nZito, S. & Todd, B. (2018). The great revolt: inside the populist coalition reshaping American politics. New York, NY: Crown \nForum. \nZorn, E. (2108, March 20). Commentary: The foolish inconsistency of the Fox News propaganda machine. Chicago Tribune. \nRetrieved September 14, 2018, from http:/ /www.chicagotribune.com/news/opinion/zorn/ct-perspec-zorn-fox-trump-obama-korea-20180320-story.html ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "The role of pseudo-cognitive authorities and self-deception in the dissemination of fake news", "author": ["TJ Froehlich"], "pub_year": "2019", "venue": "Open Information Science", "abstract": "This paper draws together insights from a variety of fields (including philosophy, psychology,  information studies, sociology, politics, and media studies) to synthesize insight into why"}, "filled": false, "gsrank": 368, "pub_url": "https://www.degruyterbrill.com/document/doi/10.1515/opis-2019-0009/html", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:6EIf-mSfU8wJ:scholar.google.com/&output=cite&scirp=367&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D360%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=6EIf-mSfU8wJ&ei=RrWsaO_lEeHUieoP9LKZ6AI&json=", "num_citations": 45, "citedby_url": "/scholar?cites=14723286862846182120&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:6EIf-mSfU8wJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.degruyterbrill.com/document/doi/10.1515/opis-2019-0009/pdf"}}, {"title": "Time series analysis of key societal events as reflected in complex social media data streams", "year": "2024", "pdf_data": "Time Series Analysis of Key Societal Events as Reflected in Complex Social Media\nData Streams\nAndy Skumanich1, Han Kyul Kim2\n1Innov8ai Inc.\n413 Hershner Dr.\nLos Gatos, California, 95032-4002 USA\n2University of Southern California\nASkuman@Innov8ai.com, HanKyulK@USC.edu\nAbstract\nSocial media platforms hold valuable insights, yet extract-\ning essential information can be challenging. Traditional top-\ndown approaches often struggle to capture critical signals\nin rapidly changing events. As global events evolve swiftly,\nsocial media narratives, including instances of disinforma-\ntion, become significant sources of insights. To address the\nneed for an inductive strategy, we explore a niche social\nmedia platform GAB and an established messaging service\nTelegram, to develop methodologies applicable on a broader\nscale. This study investigates narrative evolution on these\nplatforms using quantitative corpus-based discourse analysis\ntechniques. Our approach is a novel mode to study multiple\nsocial media domains to distil key information which may\nbe obscured otherwise, allowing for useful and actionable in-\nsights. The paper details the technical and methodological\naspects of gathering and preprocessing GAB and Telegram\ndata for a keyness (Log Ratio) metric analysis, identifying\ncrucial nouns and verbs for deeper exploration. Empirically,\nthis approach is applied to a case study of a well defined\nevent that had global impact: the 2023 Wagner mutiny. The\nmain findings are: (1) the time line can be deconstructed to\nprovide useful data features allowing for improved interpre-\ntation; (2) a methodology is applied which provides a basis\nfor generalization. The key contribution is an approach, that\nin some cases, provides the ability to capture the dynamic\nnarrative shifts over time with elevated confidence. The ap-\nproach can augment near-real-time assessment of key social\nmovements, allowing for informed governance choices. This\nresearch is important because it lays out a useful methodol-\nogy for time series relevant info-culling, which can enable\nproactive modes for positive social engagement.\nIntroduction\nSocial networks have had a profound impact on the way\nwe communicate, share information, and interact with each\nother. They have become a central part of modern society,\nenabling people to connect with each other regardless of\ntheir location and participate in online communities (Hromic\nand Hayes 2019). However, the rise of social networks has\nalso led to several challenges, including the spread of dis-\ninformation (Barbaro and Skumanich 2023), cyberbullying\n(Giumetti and Kowalski 2022), and the propagation of hate\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.speech and extremist ideologies (Govers et al. 2023), along\nwith in general what we call mal-info (Zhen et al. 2023).\nMal-info strategies encompass the use of social media to\nspread rumors, mobilize protests, and disseminate false nar-\nratives. For instance, in the broader geopolitical context of\nthe Wagner scenario, a Telegram post by an alleged Wagner\noperative claimed France was involved in the mass removal\nof children for sinister purposes. These instances of disinfor-\nmation could be detected and addressed.1.\nThe time series analysis of social media has garnered at-\ntention given the challenges of extracting meaningful signal-\nto-noise. Several recent papers have highlighted these chal-\nlenges.2As well as others where they investigate the prob-\nlem of mining timelines of entities in social media.3\nAmid the challenges of information overload on social\nmedia, including platforms like GAB or Telegram, there is\na growing interest in developing inductive, machine-guided\nmethods to mine narratives from vast amounts of posts.\nEarly examples of computational narrative analysis can be\ntraced back to work on artificial intelligence and story struc-\ntures. Recent research has further explored the underlying\ndynamics of narratives by representing them as evolving\nnetworks of relations between key actors identified through\nNamed Entity Recognition (NER)(Loper and Bird 2002).\nCo-occurrence networks (Edwards et al. 2018) and\nempirically-informed approaches have shed light on the\nstructural connections that allow online conspiracy theo-\nries and narratives to be discerned from seemingly unrelated\nconcepts and information.\nThis paper aims to assist the study of online narratives,\nemphasizing the significance of bottom-up methods to iden-\ntify idiosyncratic and evolving concepts that form these\nnarratives. Bridging cultural-theoretical and computational-\nlinguistic approaches, previous literature has used word em-\nbeddings to reveal how platforms like 4chan foster robust\nvernacular innovations (Peeters et al. 2021). Addressing the\nneed for inductive approaches to narrative evolution on so-\ncial networks, especially on new ones such as GAB, and\nmessaging platforms, such as Telegram, this paper employs\n1https://apnews.com/article/niger-coup-jihadis-west-africa-\n9032a0e1161551ffcfde4b785f6cf74a\n2Wang, et. al. https://doi.org/10.1016/j.neucom.2021.04.020\n3Hills, et. al. http://dx.doi.org/10.18653/v1/2023.eacl-main.274arXiv:2403.07090v1  [cs.IR]  11 Mar 2024\nquantitative methods from corpus-based discourse analysis.\nThe technical and methodological aspects include data col-\nlection and preprocessing of messages from the two plat-\nforms to conduct a keyness (Log Ratio) analysis, identi-\nfying significant nouns and verbs for further investigation\n(Willaert 2022). The empirical part of the paper presents\na case study using GAB and Telegram datasets4spanning\nthe Wagner militia mutiny attempt. The study tests the hy-\npothesis that during this event, GAB posts that previously\nspread disinformation narratives on subjects like the War in\nUkraine or US domestic issues, would embrace misdirect-\ning narratives about the mutiny, and be likely to reflect the\nRussian discourses instead of the Ukrainian one. The pa-\nper concludes with a broader reflection on the potential and\nfuture prospects of this overview approach to narratives in\nconjunction with established interpretative practices.\nData Collection\nThe social network GAB\nSeveral new social networks have developed recently in re-\nsponse to reactions about perceived censorship and moder-\nation on already established social media platforms. Some\nusers feel that their freedom of speech is being limited on\nplatforms such as Twitter and Facebook and that their con-\ntent is being targeted or removed. This is a broader discus-\nsion beyond the scope of this paper as it is part of the de-\ntermination of what constitutes free speech . As shown by\n(Stocking et al. 2022), more and more Americans are us-\ning these platforms for news ( 6%in 2022). Using tools like\nSimilarweb5, we can extrapolate that this number must have\ndoubled in 2023.\nThese new niche platforms often have little to no modera-\ntion and highly lenient content policies. Although this allows\nfor a wider range of opinions and viewpoints to be shared,\nsome of which can indeed be mal-info. However, they can\nbe legitimately criticized for allowing hate speech, harass-\nment (Abarna et al. 2022), and disinformation to spread\nunchecked. In this myriad of new social networks, the GAB\n(or Gab) platform is one of particular interest as a newly fre-\nquently used channel in the unfettered dissemination mode.\nGAB6is a social networking platform launched in 2016\nand bills itself as an unfettered speech (so-called free speech )\nalternative to mainstream social media sites. It was cre-\nated in response to the perceived censorship of conservative\nviews on traditional social media sites. GAB allows users to\npost messages called gabs , share photos, and interact with\nother users. It has been observed as being a platform for hate\nspeech and far-right extremism.\nTelegram\nTelegram is a cloud-based instant messaging app and so-\ncial media platform. It was developed by Pavel Durov and\nhis brother Nikolai Durov and was first launched in 2013.\n4Both datasets will be made available on open-source plat-\nforms.\n5https://www.similarweb.com/\n6https://gab.comTelegram allows users to send text messages, voice mes-\nsages, multimedia files, and make voice and video calls. One\nof its distinguishing features is its focus on security and\nprivacy, offering end-to-end encryption for messages and\na self-destruct timer for messages. Telegram also supports\ngroup chats, channels, and the ability to create public or pri-\nvate communities for sharing content and interacting with\nothers. It is available on various platforms including smart-\nphones, tablets, and desktop computers. Telegram has now\nmore than 700 million global monthly active users and it is\nincreasingly seen as a source of information (AlAwadhi and\nDashti 2021).\nData Collection\nThe focus of this paper is on the analysis of narrative time se-\nries evolution in posts on GAB and Telegram for extracting\nmeaningful insights. Examined in particular is the Wagner\nmutiny in Russia that happened on the 24th of June 2023.\nThese events are of particular note because of both the spe-\ncific events as well as the broader geo-political implications.\nImportantly from a time series data analysis perspective, the\nevent represents a very clearly defined occurrence similar\nto a mathematical delta function which is a singular pulse\nin time. Given this delta function aspect, the responses can\nbe better observed in the time series developments. It is es-\nsential to note the exploratory nature of this work and that\nalthough it is based on this single event, it allows for greater\nclarity in the analysis, and provides a starting point for more\nexpansive studies. Our follow-on studies expand on this ini-\ntial investigation.\nTo retrieve the necessary time series data, as no API is\navailable for GAB, a tailor-made scraper based on Python\u2019s\nSelenium library (Salunke 2014) was used to automate and\nscale up this process. We then retrieved all posts containing\n#Wagner, #Russia and #Ukraine from the 22nd to the 26th\nof June 2023. Table 1 shows the number of posts by hashtag.\nIn total, we retrieved 2061 unique posts. We can see that only\na minority of posts contain the #Wagner, with users concen-\ntrating more on the terms #Russia and #Ukraine during this\nperiod.\nTable 1: Number of posts by hashtag from the 22nd to the\n26th of June 2023.\nHashtag Nb. of posts\n#Wagner 340\n#Russia 923\n#Ukraine 798\nTotal 2061\nFigure 1 shows the number of posts over time. It is inter-\nesting to note that the #Wagner appeared mainly at the start\nof the mutiny on 24 June and almost ceased to exist at the\nend of the day on 26. Just before the end of that day on the\n26th, people posted mainly about the agreement between the\nhead of the Wagner Militia and the Russian President. It is\nalso interesting to note that the number of posts containing\nFigure 1: Distribution of posts by hashtags over time (UTC\ntime) for GAB.\nthe hashtags #Russia and #Ukraine during this period is\nsimilarly the same except for the time when Wagner militia\ntroops were advancing towards Moscow, around 3 PM UTC\non the 24 of June. We suggest a connection between the tim-\ning of the peaks and the occurrence of public statements by\nthe key players. This connection indicates an EU and US set\nof time zones in terms of responses (note that GAB is US\nbased). The rapid onset and drop-off are notable, showing\nnotable time series behavior in the surge and ebb in GAB\nactivity, the reasons behind which could be further investi-\ngated by subsequent multi-domain research including social\nscience. The extraction of the time series patterns can now\nallow for cross-domain analysis. The societal evaluation is\ndependent on clean data to allow for interpretation, which\nthis case study exemplifies. This is also a case of data quality\ninstead of data quantity providing the analysis value. Even\nwith the low post count a good singnal-to-noise is extracted.\nWe then applied a classical preprocessing step in which\nwe removed punctuation, number and stopwords (Barbaro\n2022). After preprocessing, it was found that the retrieved\nmessages all contained text. Automated language detection,\nusing the langdetect library (Shuyo 2010), revealed that the\ncorpus was multilingual, with English being the most promi-\nnent language. Of the message texts, 86% (1774) were clas-\nsified as written in English. In what follows, we restrict our-\nselves to analysing posts in English.\nAfterwards, we take a look at the URL that users share\nas a part of their posts. Of the 1774 posts in English, 1356\nposts contained an URL. Within the top 50 domains that\nwere shared, we find several domains that are known Russia\nstate-sponsored media (www.RT.com), or tagged as ques-\ntionable (www.rumble.com) and prone to conspiracies and\npseudoscience (www.zerohedge.com) by Media Bias / Fact\nCheck (MBFC)7. MBFC is an independently operated web-\nsite that classifies domains into certain factual and political\nlean categories, based on specific criteria that are described\nfurther in their per-domain synopsis.\nTo compare and contrast our GAB results, we retrieved\ndata from Telegram representing the main channels of the\nbelligerents in the war in Ukraine, namely Ukraine and Rus-\nsia, from the 22nd to the 26th of June 2023. To do so, we\n7https://mediabiasfactcheck.com/\nFigure 2: Distribution of aggregated posts by countries over\ntime (UTC time) for Telegram.\nused the API provided by Telegram and used the Python\npackage Telethon8. We selected two channels for each coun-\ntry in the Russian language.\nFor Ukraine, we choose Ukraine Now9and Union10, re-\nspectively with more than 1725000 and850000 subscribers.\nUkraine Now is a source of information entirely based on\nTelegram while Union is a more traditional online media.\nFor Russia, we choose Ria Novosti11and Union12, respec-\ntively with more than 2950000 and200000 subscribers. RIA\nNovosti is a Russian state-owned domestic news agency and\nKommersant is a nationally distributed daily newspaper pub-\nlished in Russia mostly devoted to politics and business with\nclose ties to the Kremlin.\nTable 2 shows the number of posts by source and country.\nIn total, we retrieved 1397 posts for Ukraine and 867posts\nfor Russia. It seems that Russian sources were less inclined\nto talk about the mutiny at first and were perhaps waiting\nfor official directives from the Kremlin. A time delay on\nthe Russia feed is apparent in Figure 2. Indeed, there is a\nlarge gap between the peak of news published by Ukrainian\nsources and those published by Russian sources, even with\nthe same provoking events. Still, there is a similarity be-\ntween the peaks of posts present on Gab and on Telegram.\nTable 2: Number of posts by hashtag from the 22nd to the\n26th of June 2023.\nUkraine Russia\nSource Ukraine\nNowUnion Ria\nNovostiKomme-\nrsant\nNb posts 824 543 591 276\nTotal 1367 867\n8https://pypi.org/project/Telethon/.\n9https://t.me/u now\n10https://t.me/uniannet\n11https://t.me/rian ru\n12https://t.me/kommersant\nFigure 3: Schematic overview of the approach. Data are\ngrouped by timestamps. For data at each timestamp (the tar-\nget corpus), keyness scores (Log Ratio) for nouns and verbs\nare calculated in relation to data of the reference corpus (pre-\nvious timestamps).\nMethodology\nTo effectively identify signals of narrative evolution in the\ncollected data, this paper employs the method of keyness\nanalysis. Drawing inspiration from the fields of corpus lin-\nguistics and corpus-based discourse analysis, this approach\nfocuses on identifying \u2019key\u2019 items, such as words or phrases,\nwithin a target corpus about a reference corpus. By compar-\ning the frequencies of items in both corpora, keyness analy-\nsis allows for an exploratory examination of texts, providing\ninsights into their underlying themes and subjects.\nOne of the primary reasons for choosing keyness analysis\nin this study is its suitability for identifying emerging nar-\nrative signals within texts. The keyness metric adopted for\nthis paper is the Log Ratio, which is defined as the binary\nlog of the ratio of relative frequencies (Hardie 2014). Unlike\nmeasures of statistical significance, the Log Ratio provides\na measure of the actual observed difference in frequencies\nbetween two corpora for a key item. This feature enables\nthe sorting of items based on the size of the frequency dif-\nference, facilitating the identification of the top N most key\nitems (emerging from the prior period). As such, a keyness\nanalysis can support an exploratory approach to texts that in-\ndicates their aboutness , i.e. what matters to the posting com-\nmunity(Gabrielatos 2018).\nTo calculate the Log Ratio for an item in the target corpus\n(C1) and the reference corpus (C2), the binary logarithm of\nthe ratio of the normalized frequencies of the term in both\ncorpora is taken. For readability purposes, these frequencies\nare multiplied by a factor of 1,000 ,000. The Log Ratio met-\nric allows for a quantitative assessment of the significance\nof key items in terms of their deviation from the norm.\nIn the context of narrative detection on GAB and Tele-\ngram, the overall approach involves detecting key items\nfrom a reference corpus comprising texts grouped by day\nas illustrated in Figure 3. These key items are then exam-\nined about the remaining data, enabling further interpreta-\ntion. This process allows for the identification of the itemswith the highest keyness scores at each timestamp, provid-\ning valuable insights into the evolving narratives within the\ndata.\nSpecifically, the technical pipeline for this analysis con-\nsists of the following sequential steps:\n1. We filter the data to retain only English posts for GAB\nand Russian posts for Telegram and group posts by times-\ntamps (per day);\n2. In addition to classic preprocessing, we clean posts by\nremoving hyperlinks and emojis ;\n3. We perform part of speech tagging and retain only nouns\nand verbs using the library NLTK (Loper and Bird 2002);\n4. We calculate the frequencies for these items per times-\ntamp;\n5. We calculate the Log Ratio of the target corpus compared\nto the reference corpus as in Figure 3;\n6. Finally, we rank words by keyness score.\nIn a conceptual sense, this approach generates keyness\nscores for items relative to the preceding data, providing\na current perspective on distinct narrative signals for each\nday\u2019s dataset in relation to the entire preceding period. The\nkeyness scores for the final timestamp hold particular sig-\nnificance as they unveil key items in relation to all preced-\ning data, shedding light on what is deemed significant at the\nlatest observation moment. It is important to note that this\nkeyness analysis does not currently incorporate semantics,\nexcept for considering nouns and verbs as key indicators of\nnarratives.\nTo exemplify this methodology and contribute empiri-\ncally to the examination of narrative dynamics on GAB and\nTelegram, the following section focuses on a case study that\nexplores the narratives surrounding the mutiny organized by\nthe Wagner Militia.\nCase Studies and Findings\nRecent events, such as the coronavirus pandemic and the war\nin Ukraine, have sparked interest among researchers, civil\nsociety actors, and journalists in understanding the evolving\nnature of (disinformation) narratives. A comparative analy-\nsis of international fact-checks has highlighted notable simi-\nlarities in terms of style and content between disinformation\nsurrounding these events (Willaert and Sessa 2022). Exam-\nples include references to Nazism, such as labeling the coro-\nnapass as a Nazi health passport or advocating for the de-\nnazification of Ukraine. In addition, various recurring con-\nspiracy theories have emerged in these sub-groups, the sup-\nposed Covid vaccine effects (micro-chip injections), witch\nhunts against the former US president, or the supposed cost\nof aid to Ukraine. Specifically, a relevant question is - do\nthe same communities that previously spread false narratives\nabout US politic or the coronavirus also engage in disinfor-\nmation regarding the war in Ukraine? - and, how did they\nexpress the events surrounding the Wagner militia mutiny?\nAccording to a recent study conducted by the Institute\nof Strategic Dialogue (J. Smirnova and Arcostanzo 2022),\nit has been confirmed that this co-aligned narrative phe-\nnomenon does occur. The study examined 229 German-\nlanguage Telegram channels associated with far-right and\nconspiracy theory communities, covering the period from\nNovember 1, 2021, to February 27, 2022. The analysis re-\nvealed that terms related to Russia, Ukraine, the breakaway\nregions in Eastern Ukraine, and the Russia-Ukraine crisis\nfrom a predefined list of 80 keywords became more coin-\ncident and prevalent in the discourse of these communities.\nThe study further examined the actual narratives, particu-\nlarly those promoting a pro-Russian perspective, through a\ndetailed analysis of articles from the most frequently shared\ndomains within the dataset. The GAB dataset we study in\nthis article differs in the source but concerns the same pop-\nulation. A population close to conspiracy and far-right cir-\ncles as well as to Russian state argumentation, as we saw\nearlier with the links shared in the posts. By employing our\ninductive approach, we hypothesize that we can uncover in-\ntricate traces of the actual narratives that emerge. This, in\nturn, provides us with insights into the underlying dynamics\nthat drive this narrative evolution at a deeper level.\nThe telegram channels we selected are directly involved\nin the information war as they present facts from divergent\nsides of the ground war. It will assist us to understand how\na particular group evolves during the Wagner mutiny and if\nwe can find any similarities or an echo in Gab\u2019s posts.\nGab case study\nTo conduct our analysis of narratives, we focus on the four\nlast days of our database, from 23 June to 26 June. Note that\nthe corpus of 22 June serves as the reference corpus for 23\nJune. This time frame allows for covering key elements of\nthe Wagner militia mutiny, from Prigozhin\u2019s allegations of\nan attack on a Wagner militia base by the regular Russian\narmy13, on 23 June, to the subsequent outcome of his exile\nin Belarus and his first speech on 26 June14after the mutiny\nwas aborted.\nA first observation on the basis of our keyness analysis,\nis that we can indeed see emerging traces of narratives con-\ncerning the attempted mutiny but also more recurrent topics\nsuch as the war in Ukraine, US aid policy towards Ukraine\nand Covid Vaccine. Table 3 shows the top 10 nouns and\nverbs (by keyness score) retrieved for the last four days of\nEnglish posts texts in the dataset. It\u2019s worth noting how\nquickly the narratives evolve in response to current events,\nreturning to more traditional hard-right topics such as Amer-\nican politics and the Covid vaccine.\nIn a second, more detailed observation, the examination\nof terms with the highest keyness provides insights into\nthe community\u2019s perspective, indicating a narrative closely\naligned with Russian discourse over time.\nOn June 23, the emerging narratives, compared to those\non June 22, primarily focused on battles in Ukraine, utilizing\n13https://www.bellingcat.com/news/2023/06/23/site-of-alleged-\nwagner-camp-attack-recently-visited-by-war-blogger/\n14https://www.euronews.com/2023/06/26/wagner-leader-\nprigozhin-breaks-his-silence-issuing-first-audio-statement-since-\nmutinyTable 3: Top 10 nouns and verbs with highest keyness scores\nfor GAB\u2019s message texts in English for the last four days of\nthe dataset (23 June to 26 June).\n2023-06-23 2023-06-24 2023-06-25 2023-06-26\nBattlefield Combat Enters Weakness\nTax Pmc Stand Empire\nStrike Soldier Incident Class\nUkrops Speech Contact Overview\nEnemy Agency Intervention Politician\nFire Path Expense Perspective\nTerritory Psyop Superiority Russiahoax\nPutin Prigozhin Exile Counteroffensive\nPatriot Wagner Journal Pfizer\nterms like battlefield ,strike orterritory . Additionally, posts\nreferred to the Ukrainian army as ukrops orennemy , reflect-\ning the Russian argumentation. Moreover, posts portrayed\nPrigozhin as a patriot compared to putin .\nMoving to June 24, the terms with the highest keyness\nhighlighted the mutiny of the wagner militia and Putin\u2019s\nspeech .\nOn June 25, the key terms underscored Prigozhin\u2019s exile\nand Putin\u2019s unwavering stance despite the incident .\nLastly, on June 26, users discussed the weakness of the\nRussian state, as expressed by various politician s from the\nRepublican Party. Additionally, notable terms that reap-\npeared were pfizer and the term russiahoax , suggesting their\nperception of a smear campaign against the former US pres-\nident, due to the decrease in news from Russia.\nIt is also interesting to note that throughout this period,\nwe find the markers of the supposed cost of American aid to\nUkraine with the word taxas well as the various expense s. In\naddition, posts refer to the supposed failure of the Ukrainian\ncounteroffensive in line with the Russian state\u2019s arguments,\nas stated by Putin, who claimed that the Ukrainian counter-\noffensive had failed and that his army had suffered heavy\nlosses15.\nTelegram case study\nWe proceeded in the same manner as with GAB to detect\nnarratives in Telegram posts and detect the difference be-\ntween Ukrainian and Russian-oriented news. Similarly, with\nthe narratives found in the Gab analysis, a first observation\nthat can be made on the basis of our keyness analysis, is that\nwe can indeed see emerging traces of narratives concerning\nthe attempted mutiny and, as expected, that theses sources\nfocused much more on the war in Ukraine and its conse-\nquences. Tables 4 and 5 show respectively the top 10 nouns\nand verbs translated into English with highest keyness scores\nfor Ukrainian and Russian Telegram\u2019s posts in Russian for\nthe last four days of the dataset (23 June to 26 June).\nIn a second, more detailed observation, the examination\nof terms with the highest keyness provides inside into the\n15https://www.bbc.com/news/world-europe-65899424\nTable 4: Top 10 nouns and verbs translated into English\nwith highest keyness scores for Ukrainian Telegram\u2019s posts\nin Russian for the last four days of the dataset (23 June to 26\nJune).\n2023-06-23 2023-06-24 2023-06-25 2023-06-26\nMoscow Loukachenko Teenager Regional\ncouncil\nPrigozhin Wagnerian NSDC Construction\nWagner V oronej Government Business\nChannel Dnipro Medvedchuk Performance\nPMC Kalibr Church Warriors\nSoldier Kharkiv Lugansk Assets\nFighter Map Resignation Wish\nGroup VKS Lithuania Error\nEmployee Minsk Shore Company\nMO Guarantee Dictator Ukroboron-\nprom\nTable 5: Top 10 nouns and verbs translated into English with\nhighest keyness scores for Russian Telegram\u2019s posts in Rus-\nsian for the last four days of the dataset (23 June to 26 June).\n2023-06-23 2023-06-24 2023-06-25 2023-06-26\nSanction Rostov Price Prison\nEU Prigozhin Recovery Broadcast\nGroup Don Hymn Ministry of\nFinance\nFact Headquarter Apartment Airplane\nRegime Highway Mediation Sukhoi\nAttempt Support Dawn Intelligence\nAdviser Building Epicentre Analyst\nAlarm Moscow\nRegionSecretary\nof stateTrump\nConnection Situation Damage Tax\nFSB Road Militia Silovikdifference of narratives spread on Ukraine and Russian Tele-\ngram.\nAs shown in Table 4, as early as 23 June 2023, the\nUkrainian Channels started to write news about the mutiny\nattempt as highlighted by words Prigozhin and PMC .\nThen, the next day, they continued to talk about the ad-\nvance towards Moscow of Wagner\u2019s troops, noting the town\nofVoronej and the losses suffered by the Russian Air\nForce ( VKS), but above all about the aftermath of this at-\ntempted mutiny, talking about the agreements reached by\nLoukachenko . Afterwards, on 25 June 2023, news focuses on\nthe consequences of the attempted mutiny on Putin ( Dicta-\ntor), such as the supposed resignation s of the Chief of Staff\nand the Russian Defence Minister, and Russia\u2019s desire, be-\ncause of the war, to establish Medvedchuk at the head of\nUkraine. On 26 June 2023, news focused on Ukrainian com-\npanies Ukroboronprom and President Zelensky\u2019s speech in\nthe Donbas region near the front, wishing the best for his\nwarriors .\nFor Russian channels, as shown in Table 5, we can ob-\nserve a certain latency in the evocation of events linked to\nthe mutiny. Indeed, on 23 June 2023, they focused on the\nsanction s imposed by the EUand the various strikes ( alarm )\nagainst the Kyiv regime . On the contrary, on 24 June 2023,\nall words are connected with the attempted mutiny, updating\non the situation with places, as Rostov -on-Don andMoscow ,\nand the path, Highway andRoad , taken by Prigozhin \u2019s men\nto reach Moscow . For the next two days, the Russian chan-\nnels focus on restoring order through the values of the state\n(Hymn ) and the Broadcast of Putin\u2019s address to the Russian\nnation. The news also mentions the mediation led by Belarus\nto stop the mutiny and the damage caused to the airplane s of\nthe Russian air force. Finally, it is interesting to note the re-\nturn of a popular subject in the Russian state media, namely\nthe case of the former US president, Trump .\nDiscussion\nOverall we establish that there is a significant value in an-\nalyzing the time series of niche social media and an estab-\nlished messaging service for key narratives.\nIn line with our hypotheses, the analysis conducted on\nGAB reveals the presence of narratives related to the Wag-\nner mutiny attempt within communities previously focused\non the war in Ukraine and US domestic issues such as Covid\nvaccination and politics. Also, our inductive approach high-\nlights two key dynamics shaping this transition.\nPrincipally, with our analysis, we observe the flow\nand ebb of new narratives alongside persistent underlying\nthemes common to this channel. These flow and ebb aspects\nappear to correspond to key global events. In addition, our\ncase study demonstrates the recurrence of certain narratives\nover time for GAB where peaks in communication occur\nwhich include similar narrative elements. This type of anal-\nysis is inherently complex given the challenges of extract-\ning information from big data sources and dynamical multi-\nparty inputs.\nGiven the unique nature of GAB, these observations pro-\nvide deeper insights into the characteristics of disinforma-\ntion narratives. It becomes evident that sustaining disinfor-\nmation requires a foundation of familiar and recurring el-\nements, while also allowing for adaptation to major global\nevents. On GAB, this ongoing process of recurrence and\nadaptation is facilitated by the platform\u2019s permissive envi-\nronment, under the banner of freedom of expression.\nIn a comparative context, our choice of Telegram chan-\nnels has illuminated discernible narratives. Across different\ncountries, particularly in our selection of news channels for\nUkraine and Russia, fresh narratives have emerged in cor-\nrelation with recent events. However, an important observa-\ntion pertains to the time lag in reporting the Wagner Mutiny\namong the Russian Channels compared to the Ukraine chan-\nnels. Additionally, it\u2019s noteworthy that the news posted on\nGab appears to be synchronized with the rhythm of Russian\ndevelopments and propaganda. As an example, on 23 June\n2023, both Gab and Russian Telegram channels seem to fo-\ncus only on strike s (alarm ) which have been carried out in\nUkraine and do not mention the Wagner events. Instead, only\non 24 June 2023, these two started to include the mutiny and\non 25 June 2023, to position the discourse to emphasize the\ndurability of the Russian state despite the incident . Finally,\non 26 June 2023, both are writing about the US politic in\nconnection with Trump and the Russianhoax .\nConclusions and Future Work\nWe provide data in support of our mode of time series anal-\nysis of social network flows which allows us to extract use-\nful narrative signatures which correlate to events and which\ndevelop with time (flow and ebb). These signatures can be\ncompared across the various channels for a better under-\nstanding of the dynamics of narrative detection and devel-\nopment. We demonstrate the ability to detect evolving nar-\nratives on social media with useful characteristics. This pa-\nper offers several contributions. Firstly, it presents a techni-\ncal pipeline utilizing keyness analysis (Log Ratio) to iden-\ntify traces of narrative innovations and continuity. Secondly,\nit applies this methodology to the case study of narrative\nevolution on GAB, a relatively unexplored social network\nand a known messaging service, Telegram. Thirdly, given\nthe growth and proliferation of these niche channels like\nGAB, it\u2019s necessary to study them to at least baseline the\nfeatures. We believe this is the first semi-quantitative and\nqualitative analysis of GAB. Numerous prior studies have\nfocused primarily on Twitter (Govers et al. 2023) and to\nsome extent YouTube, Telegram (Willaert 2022) and Red-\ndit. The same approach we present can be done for other\nniche channels such as GETTR16, Bitchute17and Rumble18.\nWe submit that it is crucial for the broader research commu-\nnity to now incorporate these other channels in the analy-\nsis. This paper can act as an introduction to these channels\nas very few articles have focused on these new social net-\nworks (Peucker and Fisher 2023). Finally, and importantly,\nwe suggest that studying the more homogeneous niche so-\ncial networks can provide elements which can help extract\ninformation from the substantially bigger channels. Our ini-\n16https://gettr.com/\n17https://www.bitchute.com/\n18https://rumble.com/tial work evaluating Twitter posts during this period indeed\nfound a detectable Wagner narrative presence but at a sub-\nstantially reduced overall percentage. The use of niche chan-\nnels is a potential mode to extract more signal from the\nnoise in regard to developing methods for social media anal-\nysis. As an outward-looking point, these fringe social media\nchannels can also provide a basis for the behavioral science\nstudy of social trends. The key insights provide a basis for a\ndeeper understanding. It should be emphasized that this ex-\nploratory study is limited in scope and our follow-on anal-\nysis will look at two key expansion aspects: longer term,\nnon-delta-function events; and also, larger-scale social me-\ndia time series data. We anticipate that given the methodol-\nogy, the general principles will transfer, it is likely that the\nsignal-to-noise may drop, but that will be investigated.\nThe application of keyness analysis on GAB and Tele-\ngram data demonstrates its effectiveness in inductively de-\ntecting emerging or persistent narratives, providing valuable\ninsights for further investigation. We acknowledge the limi-\ntations of the exploratory scope of this paper and suggest the\nresults as a starting point for further research.\nTo enhance the analysis, future research could incorpo-\nrate wider semantic networks by employing statistically-\ninformed co-occurrence analyses. Additionally, introduc-\ning more granularity through graph-like representations of\nnarratives inferred from argument structures could offer\npromising avenues for contextualizing key items.\nOn a conceptual level, this analysis prompts broader ques-\ntions of meaning and interpretation. The keyness analysis it-\nself does not capture the semantic nuances of the examined\nposts. Therefore, assigning meaning to key items requires\nhuman interpretation, such as considering combinations of\nkey items, referring to the corpus for keyword exploration,\nand contextualizing within cultural and media-theoretical\nframeworks.\nThe main contributions of this paper are to provide a type\nof methodology for deciphering time series events in social\nmedia. These initial results do not allow for broad gener-\nalizations, but are indicative. The potential limitations can\nalso be noted and potentially addressed. For example, the\nkeyness analysis, while quantitatively robust, might miss\nsome of the (human perceived) deeper semantic meanings\nand nuances that develop in online narratives. These limita-\ntions highlight the extreme challenges to narrative time se-\nries analysis. However, a simplified case study such as pre-\nsented, allows for a mode of attack while determining the\nextent of the applicability.\nThis raises the need for the development of critical frame-\nworks that integrate inductive methods. Proposals such as\ndata hermeneutics have been introduced (Romele, Severo,\nand Furia 2020), providing opportunities for future research\nto explore actionable implementations in this domain. More-\nover, it will be interesting to analyse images and videos con-\ntained in posts to help the contextualization (Li et al. 2023).\nReferences\nAbarna, S.; Sheeba, J.; Jayasrilakshmi, S.; and Devaneyan,\nS. P. 2022. Identification of cyber harassment and intention\nof target users on social media platforms. Engineering ap-\nplications of artificial intelligence , 115: 105283.\nAlAwadhi, S.; and Dashti, M. 2021. The Use of the Tele-\ngram Application as an Information-Sharing Tool. Journal\nof Information & Knowledge Management , 20(02).\nBarbaro, F. 2022. Analyse exploratoire et classification de\ntextes . Theses, Paris 1 - Panth \u00b4eon-Sorbonne.\nBarbaro, F.; and Skumanich, A. 2023. Addressing Socially\nDestructive Disinformation on the Web with Advanced AI\nTools: Russia as a Case Study. In Companion Proceedings\nof the ACM Web Conference 2023 , 204\u2013207. New York, NY ,\nUSA: Association for Computing Machinery.\nEdwards, M.; Mitchell, L.; Tuke, J.; and Roughan, M. 2018.\nThe one comparing narrative social network extraction tech-\nniques. arXiv:1811.01467.\nGabrielatos, C. 2018. Keyness Analysis: nature, metrics and\ntechniques , 225\u2013258. United Kingdom: Routledge.\nGiumetti, G. W.; and Kowalski, R. M. 2022. Cyberbullying\nvia social media and well-being. Current Opinion in Psy-\nchology , 45: 101314.\nGovers, J.; Feldman, P.; Dant, A.; and Patros, P. 2023. Down\nthe Rabbit Hole: Detecting Online Extremism, Radicalisa-\ntion, and Politicised Hate Speech. ACM Comput. Surv.\nHardie, A. 2014. Log ratio: An informal introduction. ESRC\nCentre for Corpus Approaches to Social Science (CASS) .\nHromic, H.; and Hayes, C. 2019. Characterising and evalu-\nating dynamic online communities from live microblogging\nuser interactions. Social Network Analysis and Mining , 9.\nJ. Smirnova, P. M.; and Arcostanzo, F. 2022. Support\nfrom the conspiracy corner: German language disinforma-\ntion about the Russian invasion of Ukraine on Telegram.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023. BLIP-2: Boot-\nstrapping Language-Image Pre-training with Frozen Image\nEncoders and Large Language Models. arXiv:2301.12597.\nLoper, E.; and Bird, S. 2002. NLTK: The Natural Language\nToolkit.\nPeeters, S.; Tuters, M.; Willaert, T.; and Zeeuw, D. 2021. On\nthe Vernacular Language Games of an Antagonistic Online\nSubculture. Frontiers in Big Data , 4.\nPeucker, M.; and Fisher, T. J. 2023. Mainstream media\nuse for far-right mobilisation on the alt-tech online platform\nGab. Media, Culture & Society , 45(2): 354\u2013372.\nRomele, A.; Severo, M.; and Furia, P. 2020. Digital\nHermeneutics: From Interpreting with Machines to Interpre-\ntational Machines. AI Soc. , 35(1): 73\u201386.\nSalunke, S. S. 2014. Selenium Webdriver in Python: Learn\nwith Examples . North Charleston, SC, USA: CreateSpace\nIndependent Publishing Platform, 1st edition.\nShuyo, N. 2010. Language Detection Library for Java.\nStocking, G.; Mitchell, A.; Matsa, K. E.; Widjaya, R.; Ju-\nrkowitz, M.; Ghosh, S.; Smith, A.; Naseer, S.; and Aubin,\nC. S. 2022. The Role of Alternative Social Media in the\nNews and Information Environment. Pew Research Center.\nWillaert, T. 2022. Detecting Traces of Narrative Evolution\non Telegram. Inductive Methods from Corpus-Based Dis-\ncourse Analysis. Proceedings of IJCAI-ECAI , 23\u201329.Willaert, T.; and Sessa, M. 2022. From Infodemic to Infor-\nmation War (EDMO BELUX Investigative report): A con-\ntextualization of current narrative trends and evolutions\nin Dutch-language disinformation communities . EDMO\nBELUX: BE-LUX Research Hub on Digital Media and Dis-\ninformation.\nZhen, L.; Yan, B.; Tang, J. L.; Nan, Y .; and Yang, A. 2023.\nSocial network dynamics, bots, and community-based on-\nline misinformation spread: Lessons from anti-refugee and\nCOVID-19 misinformation cases. The Information Society ,\n39(1): 17\u201334.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Time series analysis of key societal events as reflected in complex social media data streams", "author": ["A Skumanich", "HK Kim"], "pub_year": "2024", "venue": "arXiv preprint arXiv:2403.07090", "abstract": "Social media platforms hold valuable insights, yet extracting essential information can be  challenging. Traditional top-down approaches often struggle to capture critical signals in"}, "filled": false, "gsrank": 369, "pub_url": "https://arxiv.org/abs/2403.07090", "author_id": ["jrocusUAAAAJ", "wVZWuQ8AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:UEWCkihcU6kJ:scholar.google.com/&output=cite&scirp=368&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D360%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=UEWCkihcU6kJ&ei=RrWsaO_lEeHUieoP9LKZ6AI&json=", "num_citations": 6, "citedby_url": "/scholar?cites=12201197144803263824&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:UEWCkihcU6kJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2403.07090"}}, {"title": "We can detect your bias: Predicting the political ideology of news articles", "year": "2020", "pdf_data": "We Can Detect Your Bias:\nPredicting the Political Ideology of News Articles\nRamy Baly1, Giovanni Da San Martino2, James Glass1, Preslav Nakov2\n1MIT Computer Science and Arti\ufb01cial Intelligence Laboratory\n3Qatar Computing Research Institute, HBKU\nfbaly,glassg@mit.edu\nfgmartino,pnakov g@hbku.edu.qa\nAbstract\nWe explore the task of predicting the lead-\ning political ideology or bias of news articles.\nFirst, we collect and release a large dataset of\n34,737 articles that were manually annotated\nfor political ideology \u2013left, center, or right\u2013,\nwhich is well-balanced across both topics and\nmedia. We further use a challenging exper-\nimental setup where the test examples come\nfrom media that were not seen during train-\ning, which prevents the model from learning\nto detect the source of the target news arti-\ncle instead of predicting its political ideology.\nFrom a modeling perspective, we propose an\nadversarial media adaptation, as well as a spe-\ncially adapted triplet loss. We further add\nbackground information about the source, and\nwe show that it is quite helpful for improv-\ning article-level prediction. Our experimental\nresults show very sizable improvements over\nusing state-of-the-art pre-trained Transformers\nin this challenging setup.\n1 Introduction\nIn any piece of news, there is a chance that the\nviewpoint of its authors and of the media organiza-\ntion they work for, would be re\ufb02ected in the way\nthe story is being told. The emergence of the Web\nand of social media has lead to the proliferation of\ninformation sources, whose leading political ide-\nology or bias may not be explicit. Yet, systematic\nexposure to such bias may foster intolerance as\nwell as ideological segregation, and ultimately it\ncould affect voting behavior, depending on the de-\ngree and the direction of the media bias, and on the\nvoters\u2019 reliance on such media (DellaVigna and Ka-\nplan, 2007; Iyengar and Hahn, 2009; Saez-Trumper\net al., 2013; Graber and Dunaway, 2017). Thus,\nmaking the general public aware, e.g., by track-\ning and exposing bias in the news is important for\na healthy public debate given the important role\nmedia play in a democratic society.Media bias can come in many different forms,\ne.g., by omission, by over-reporting on a topic, by\ncherry-picking the facts, or by using propaganda\ntechniques such as appealing to emotions, preju-\ndices, fears, etc. (Da San Martino et al., 2019,\n2020a,b) Bias can occur with respect to a spe-\nci\ufb01c topic, e.g., COVID-19, immigration, climate\nchange, gun control, etc. (Darwish et al., 2020;\nStefanov et al., 2020) It could also be more system-\natic, as part of a political ideology, which in the\nWestern political system is typically de\ufb01ned as left\nvs. center vs. right political leaning.\nPredicting the bias of individual news articles\ncan be useful in a number of scenarios. For news\nmedia, it could be an important element of internal\nquality assurance as well as of internal or external\nmonitoring for regulatory compliance. For news\naggregator applications, such as Google News, it\ncould enable balanced search, similarly to what\nis found on AllSides.1For journalists, it could\nenable news exploration from a left/center/right\nangle. It could also be an important building block\nin a system that detects bias at the level of entire\nnews media (Baly et al., 2018, 2019, 2020), such\nas the need to offer explainability, i.e., if a website\nis classi\ufb01ed as left-leaning, the system should be\nable to pinpoint speci\ufb01c articles that support this\ndecision.\nIn this paper, we focus on predicting the bias\nof news articles as left-, center-, or right-leaning.\nPrevious work has focused on doing so at the level\nof news media (Baly et al., 2020) or social me-\ndia users (Darwish et al., 2020), but rarely at the\narticle level (Kulkarni et al., 2018). The scarce\narticle-level research has typically used distant su-\npervision, assuming that all articles from a given\nmedium should share its overall bias, which is not\nalways the case. Here, we revisit this assumption.\n1http://allsides.com/arXiv:2010.05338v1  [cs.CL]  11 Oct 2020\nOur contributions can be summarized as follows:\n\u2022We create a new dataset for predicting the po-\nlitical ideology of news articles. The dataset\nis annotated at the article level and covers\na wide variety of topics, providing balanced\nleft/center/right perspectives for each topic.\n\u2022We develop a framework that discourages the\nlearning algorithm from modeling the source\ninstead of focusing on detecting bias in the\narticle. We validate this framework in an ex-\nperimental setup where the test articles come\nfrom media that were not seen at training time.\nWe show that adversarial media adaptation is\nquite helpful in that respect, and we further\npropose to use a triplet loss, which shows siz-\nable improvements over state-of-the-art pre-\ntrained Transformers.\n\u2022We further incorporate media-level representa-\ntion to provide background information about\nthe source, and we show that this information\nis quite helpful for improving the article-level\nprediction even further.\nThe rest of this paper is organized as follows:\nWe discuss related work in Section 2. Then, we\nintroduce our dataset in Section 3, we describe\nour models for predicting the political ideology of\na news article in Section 4, and we present our\nexperiments and we discuss the results in Section 5.\nFinally, we conclude with possible directions for\nfuture work in Section 6.\n2 Related Work\nMost existing datasets for predicting the political\nideology at the news article level were created\nby crawling the RSS feeds of news websites with\nknown political bias (Kulkarni et al., 2018), and\nthen projecting the bias label from a website to all\narticles crawled from it, which is a form of distant\nsupervision. The crawling could be also done us-\ning text search APIs rather than RSS feeds (Horne\net al., 2019; Gruppi et al., 2020).\nThe media-level annotation of political leaning\nis typically obtained from specialized online plat-\nforms, such as News Guard,2AllSides,3and Media\nBias/Fact Check,4where highly quali\ufb01ed journal-\nists use carefully designed guidelines to make the\njudgments.\n2http://www.newsguardtech.com\n3http://allsides.com/\n4http://mediabiasfactcheck.comAs manual annotation at the article level is very\ntime-consuming, requires domain expertise, and\nit could be also subjective, such annotations are\nrarely available at the article level. As a result,\nautomating systems for political bias detection have\nopted for using distant supervision as an easy way\nto obtain large datasets, which are needed to train\ncontemporary deep learning models.\nDistant supervision is a popular technique for\nannotating datasets for related text classi\ufb01cation\ntasks, such as detecting hyper-partisanship (Horne\net al., 2018; Potthast et al., 2018) and propa-\nganda/satire/hoaxes (Rashkin et al., 2017). For\nexample, Kiesel et al. (2019) created a large cor-\npus for detecting hyper-partisanship (i.e., articles\nwith extreme left/right bias) consisting of 754,000\narticles, annotated via distant supervision, and ad-\nditional 1,273 manually annotated articles, part of\nwhich was used as a test set for the SemEval-2019\ntask 4 on Hyper-partisan News Detection. The win-\nning system was an ensemble of character-level\nCNNs (Jiang et al., 2019). Interestingly, all top-\nperforming systems in the task achieved their best\nresults when training on the manually annotated\narticles only and ignoring the articles that were la-\nbeled using distant supervision, which illustrates\nthe dangers of relying on distant supervision.\nBarr \u00b4on-Cedeno et al. (2019) extensively dis-\ncussed the limitations of distant supervision in a\ntext classi\ufb01cation task about article-level propa-\nganda detection, in a setup that is similar to what\nwe deal with in this paper: the learning systems\nmay learn to model the source of the article instead\nof solving the task they are actually trained for.\nIndeed, they have shown that the error rate may\ndrastically increase if such systems are tested on\narticles from sources that were never seen during\ntraining, and that this effect is positively correlated\nwith the representation power of the learning model.\nThey analyzed a number of representations and ma-\nchine learning models, showing which ones tend\nto over\ufb01t more, but, unlike our work here, they fell\nshort of recommending a practical solution.\nBudak et al. (2016) measured the bias at the\narticle level using crowd-sourcing. This is risky\nas public awareness of media bias is limited (Ele-\njalde et al., 2018). Moreover, the annotation setup\ndoes not scale. Finally, their dataset is not freely\navailable, and their approach of randomly crawling\narticles does not ensure that topics and events are\ncovered from different political perspectives.\nLin et al. (2006) built a dataset annotated with\nthe ideology of 594 articles related to the Israeli-\nPalestinian con\ufb02ict published on bitterlemons.\norg. The articles were written by two editors and\n200 guests, which minimizes the risk of modeling\nthe author style. However, the dataset is too small\nto train modern deep learning approaches.\nKulkarni et al. (2018) built a dataset using distant\nsupervision and labels from AllSides. Distant su-\npervision is \ufb01ne for the purpose of training, but they\nalso used it for testing, which can be problematic.\nMoreover, their training and test sets contain arti-\ncles from the same media, and thus models could\neasily learn to predict the article\u2019s source rather\nthan its bias. In their models, they used both the\ntext and the URL contents of the articles.\nOverall, political bias has been studied at the\nlevel of news outlet (Dinkov et al., 2019; Baly et al.,\n2018, 2020; Zhang et al., 2019), user (Darwish\net al., 2020), article (Potthast et al., 2018; Saleh\net al., 2019), and sentence (Sim et al., 2013; Saez-\nTrumper et al., 2013). In particular, Baly et al.\n(2018) developed a system to predict the political\nbias and the factuality of news media. In a follow-\nup work, Baly et al. (2019) showed that bias and\nfactuality of reporting should be predicted jointly.\nA \ufb01ner-grained analysis is performed in (Horne\net al., 2018), where a model was trained on 10K\nsentences from a dataset of reviews (Pang and Lee,\n2004), and used to discriminate objective versus\nnon-objective sentences in news articles. Lin et al.\n(2006) presented a sentence-level classi\ufb01er, where\nthe labels were projected from the document level.\n3 Dataset\nIn this section, we describe the dataset that we cre-\nated and that we used in our experiments. While\nmost of the platforms that analyze the political\nleaning of news media provide in-depth analysis of\nparticular aspects of the media, AllSides stands out\nas it provides annotations of political ideology for\nindividual articles, which ensures high-quality data\nfor both training and testing, which is in contrast\nwith distant supervision approaches used in most\nprevious research, as we have seen above. In All-\nSides, these annotations are made as a result of a\nrigorous process that involves blind bias surveys,\neditorial reviews, third-party analysis, independent\nreviews, and community feedback.5\n5http://www.allsides.com/media-bias/\nmedia-bias-rating-methodsFurthermore, AllSides uses the annotated arti-\ncles to enable its Balanced Search , which shows\nnews coverage on a given topic from media with\ndifferent political bias. In other words, for each\ntrending event or topic (e.g., impeachment orcoro-\nnavirus pandemic ), the platform pushes news ar-\nticles from all sides of the political spectrum, as\nshown in Figure 1. We took advantage of this and\ndownloaded all articles along with their political\nideology annotations ( left,center , orright ), their\nassigned topic(s), the media in which they were\npublished, their author(s), and their publication\ndate. Thus, our dataset contains articles that were\nmanually selected and annotated, and that are rep-\nresentative of the real political scenery. Note that\nthecenter class covers articles that are biased to-\nwards a centrist political ideology, and not articles\nthat lack political bias (e.g., sports andtechnology ),\nwhich commonly exist in news corpora that were\nbuilt by scraping RSS feeds.\nWe collected a total of 34,737 articles published\nby 73 news media and covering 109 topics.6In this\ndataset, a total of 1,080 individual articles (3.11%)\nhave a political ideology label that is different from\ntheir source\u2019s. This suggests that, while the distant\nsupervision assumption generally holds, we would\nstill \ufb01nd many articles that defy it. Table 1 shows\nsome statistics about the dataset.\nPolitical Ideology Count Percentage\nLeft 12,003 34.6%\nCenter 9,743 28.1%\nRight 12,991 37.3%\nTable 1: Statistics about our dataset.\nFigure 2 illustrates the distribution of the differ-\nent political bias labels within each of the most\nfrequent topics. We can see that our dataset is able\nto represent topics or events from different political\nperspectives. This is yet another advantage, as it\nenables a more challenging task for machine learn-\ning models to detect the linguistic and the semantic\nnuances of different political ideologies in news\narticles, as opposed to cases where certain topics\nmight be coincidentally collocated with certain la-\nbels, in which case the models would be actually\nlearning to detect the topics instead of predicting\nthe political ideology of the target news article.\n6In some cases, an article could be assigned to multiple\ntopics, e.g., it could go simultaneously into coronavirus ,public\nhealth , and healthcare .\nFigure 1: AllSides: balanced search on the topic of reopening after the coronavirus lockdown .\nFigure 2: Political ideology for the most frequent top-\nics:elections ,immigration ,coronavirus , and politics .\nIt is worth noting that since most article labels\nare aligned with their source labels, it is likely that\nmachine learning classi\ufb01ers would end up model-\ning the source instead of the political ideology of\nthe individual articles. For example, a model would\nbe learning the writing style of each medium, and\nthen it would associate it with a particular ideology.\nTherefore, we pre-processed the articles in a way\nthat eliminates explicit markers such as the name of\nthe authors, or the name of the medium that usually\nappears as a preamble to the article\u2019s content, or in\nthe content itself. Furthermore, in order to ensure\nthat we are actually modeling the political ideol-\nogy as it is expressed in the language of the news,\nwe created evaluation splits in two different ways:\n(i) randomly, which is what is typically done (for\ncomparison only), and ( ii) based on media, where\nall articles by the same medium appear in either\nthe training, the validation, or the testing dataset.The latter form of splitting would help us indi-\ncate what a trained classi\ufb01er has actually learned.\nFor instance, if it modeled the source, then it would\nnot be able to perform well on the test set, since all\nits articles would belong to sources that were never\nseen during training. In order to ensure fair one-to-\none comparisons between experiments, we created\nthese two different sets of splits, while making sure\nthat they share the same test set, as follows:\n\u2022Media-based Split: We sampled 1,200 arti-\ncles from 12 news media (100 per medium)\nand used them as the testset, and we excluded\nthe remaining 5,470 articles from these media.\nThen, we used the articles from the remaining\n61 media to create the training and the vali-\ndation sets, where all articles from the same\nmedium would appear in the same set: train-\ning, development, or testing. This ensures that\nthe model is \ufb01ne-tuned and tested on articles\nwhose sources were not seen during training.\n\u2022Random Split: Here, the testset is the same\nas in the media-based split. The 5,470 articles\nthat we excluded from the 12 media are now\nadded to the articles from the 61 remaining\nmedia. Then, we split this collection of arti-\ncles (using strati\ufb01ed random sampling) into\ntraining andvalidation sets. This ensures that\nthe model is \ufb01ne-tuned and evaluated only on\narticles whose sources were observed during\ntraining.\nTable 2 shows statistics about both splits, includ-\ning the size of each set and the number of media\nand topics they cover. We release the dataset, along\nwith the evaluation splits, and the code,7which can\nbe used to extend the dataset as more news articles\nare added to AllSides.\n7http://github.com/ramybaly/\nArticle-Bias-Prediction\nTrain Valid. Test\nMedia-basedCount 22,969 5,098 1,200\nMedia 46 15 12\nTopics 108 105 93\nRandomCount 26,828 6,709 1,200\nMedia 73 73 12\nTopics 108 107 93\nTable 2: Statistics about our dataset and its two splits:\nmedia-based andrandom .\n4 Methodology\n4.1 Classi\ufb01ers\nThe task of predicting the political ideology of\nnews articles is typically formulated as a classi-\n\ufb01cation problem, where the textual content of the\narticles is encoded into a vector representation that\nis used to train a classi\ufb01er to predict one of C\nclasses (in our case, C= 3:left,center , and right ).\nIn our experiments, we use two deep learning archi-\ntectures: ( i)Long Short-Term Memory networks\n(LSTMs), which are Recurrent Neural Networks\n(RNNs), which use gating mechanisms to selec-\ntively pass information across time and to model\nlong-term dependencies (Hochreiter and Schmid-\nhuber, 1997), and ( ii)Bidirectional Encoder Rep-\nresentations from Transformers (BERT), with a\ncomplex architecture yielding high-quality contex-\ntualized embeddings, which have been successful\nin several Natural Language Processing tasks (De-\nvlin et al., 2019).\n4.2 Removing Media Bias\nUltimately, our goal is to develop a model that can\npredict the political ideology of a news article. Our\ndataset, along with some others, has a special prop-\nerty that might stand in the way of achieving this\ngoal. Most articles published by a given source\nhave the same ideological leaning. This might con-\nfuse the model and cause it to erroneously associate\nthe output classes with features that characterize en-\ntire media outlets (such as detecting speci\ufb01c writing\npatterns, or stylistic markers in text). Consequently,\nthe model would fail when applied to articles that\nwere published in media that were unseen during\ntraining. The experiments in Section 5 con\ufb01rm this.\nThus, we apply two techniques to de-bias the mod-\nels, i.e., to prevent them from learning the style of\na speci\ufb01c news medium rather than predicting the\npolitical ideology of the target news article.4.2.1 Adversarial Adaptation (AA)\nThis model was originally proposed by Ganin et al.\n(2016) for unsupervised domain adaptation in im-\nage classi\ufb01cation. Their objective was to adapt a\nmodel trained on labelled images from a source\ndomain to a novel target domain, where the images\nhave no labels for the task at hand. This is done\nby adding an adversarial domain classi\ufb01er with\na gradient reversal layer to predict the examples\u2019\ndomains. The label predictor\u2019s is minimized for\nthe labelled examples (from the source domain),\nand the adversarial domain classi\ufb01er\u2019s loss is max-\nimized for all examples in the dataset. As a result,\nthe encoder can extract representation that is ( i) dis-\ncriminative for the main task and also ( ii) invariant\nacross domains (due to the gradient reversal layer).\nThe overall loss is minimized as follows:\nX\ni=1:N\ndi=0Li\ny(\u0012f;\u0012y)\u0000\u0015X\ni=1:NLi\nd(\u0012f;\u0012d); (1)\nwhereNis the number of training examples,\nLi\ny(\u0001;\u0001)is the label predictor\u2019s loss, the condi-\ntiondi= 0 means that only examples from the\nsource domain are used to calculate the label pre-\ndictor\u2019s loss,Li\nd(\u0001;\u0001)is the domain classi\ufb01er\u2019s loss,\n\u0015controls the trade-off between both losses, and\nf\u0012f;\u0012y;\u0012dgare the parameters of the encoder, the\nlabel predictor, and the domain classi\ufb01er, respec-\ntively. Further details about the formulation of this\nmethod is available in (Ganin et al., 2016).\nWe adapt this architecture as follows. Instead of\nadomain classi\ufb01er , we implement a media clas-\nsi\ufb01er , which, given an article, tries to predict the\nmedium it comes from. As a result, the encoder\nshould extract representation that is discriminative\nfor the main task of predicting political ideology,\nwhile being invariant for the different media. This\napproach was originally proposed as an unsuper-\nvised domain adaptation, since labelled examples\nwere available for one domain only, whereas in our\ncase, all articles from different media were labelled\nfor their political ideology. Therefore, we jointly\nminimize the losses of both the label predictor and\nthemedia classi\ufb01er over the entire dataset. The\nnew objective function to minimize is as follows:\nX\ni=1:NLi\ny(\u0012f;\u0012y)\u0000\u0015X\ni=1:NLi\nm(\u0012f;\u0012m);(2)\nwhereLi\nm(\u0001;\u0001)is the loss of the media classi\ufb01er ,\nand\u0012mis its set of parameters.\n4.2.2 Triplet Loss Pre-training (TLP)\nIn this approach, we pre-train the encoder using\na triplet loss (Schroff et al., 2015). The model is\ntrained on a set of triplets, each composed of an\nanchor, a positive, and a negative example. The\nobjective in Eq. 3 ensures that the positive example\nis always closer to the anchor than the negative\nexample is, where a,pandnare the encodings\nof the anchor, of the positive, and of the negative\nexamples, respectively, and D(\u0001;\u0001)is the Euclidean\ndistance:\nL= max (D(a;p)\u0000D(a;n) +\u000f;0):(3)\nFigure 3 shows an example of such a triplet. The\npositive example shares the same ideology as the\nanchor\u2019s, but they are published by different media.\nThe negative example has a different ideology than\nthe anchor\u2019s, but they are published by the same\nmedium. In this way, the encoder will be cluster-\ning examples with similar ideologies close to each\nother, regardless of their source. Once the encoder\nhas been pre-trained, its parameters, along with\nthe softmax classi\ufb01er\u2019s, are \ufb01ne-tuned on the main\ntask by minimizing the cross-entropy loss when\npredicting the political ideology of articles.\nFigure 3: An example triplet used for de-biasing.\n4.3 Media-level Representation\nFinally, we explore the bene\ufb01ts of incorporating\ninformation describing the target medium, which\ncan serve as a complementary representation for\nthe article. While this seems to be counter-intuitive\nto what we have been proposing in Subsection 4.2,\nwe believe that medium-level representation can be\nvaluable when combined with an accurate represen-\ntation of the article. Intuitively, having an accurate\nunderstanding of the natural language in the article,\ntogether with a glimpse into the medium it is pub-\nlished in, should provide a more complete picture\nof its underlying political ideology.Baly et al. (2020) proposed a comprehensive set\nof representation to characterize news media from\ndifferent angles: how a medium portrays itself, who\nis its audience, and what is written about it. Their\nresults indicate that exploring the Twitter bios of a\nmedium\u2019s followers offers a good insight into its\npolitical leaning. To a lesser extent, the content\nof aWikipedia page describing a medium can also\nhelp unravel its political leaning. Therefore, we\nconcatenated these representations to the encoded\narticles, at the output of the encoder and right be-\nfore the SOFTMAX layer, so that both the article\nencoder and the classi\ufb01cation layer that is based on\nthe article and the external media representations\nare trained jointly and end-to-end.\nSimilarly to (Baly et al., 2020), we retrieved\nthe pro\ufb01les of up to a 1,000 Twitter followers for\neach medium, we encoded their bios using the\nSentence-BERT model (Reimers and Gurevych,\n2019), and we then averaged these encodings to\nobtain a single representation for that medium. As\nfor the Wikipedia representation, we automatically\nretrieved the content of the page describing each\nmedium, whenever applicable. Then, we used\nthe pre-trained base BERT model to encode this\ncontent by averaging the word representations ex-\ntracted from BERT\u2019s second-to-last layer, which is\ncommon practice, since the last layer may be biased\ntowards the pre-training objectives of BERT.\n5 Experiments and Results\nWe evaluated both the LSTM and the BERT mod-\nels, assessing the impact of ( i) de-biasing and\n(ii) incorporating media-level representation.\n5.1 Experimental Setup\nWe \ufb01ne-tuned the hyper-parameters of both models\non the validation set using a guided grid search\ntrial while \ufb01xing the seeds of the random weights\ninitialization. For LSTM, we varied the length of\nthe input (128\u20131,024 tokens), the number of layers\n(1\u20133), the size of the LSTM cell (200\u2013400), the\ndropout rate (0\u20130.8), the learning rate ( 1e\u00003to\n1e\u00005), the gradient clipping value (0\u20135), and the\nbatch size (8\u2013256). The best results were obtained\nwith a 512-token input, a 2-layer LSTM of size\n256, a dropout rate of 0.7, a learning rate of 1e\u00003,\ngradient clipping at 0.5, and a batch size of 32.\nThis model has around 1.1M trainable parameters,\nand was trained with 300-dimensional GloVe input\nword embeddings (Pennington et al., 2014).\nFor BERT, we varied the length of the input, the\nlearning rate, and the gradient clipping value. The\nbest results were obtained using a 512-token input,\na learning rate of 2e\u00005, and gradient clipping at 1.\nThis model has 110M trainable parameters.\nWe trained our models on 4 Titan X Pascal GPUs,\nand the runtime for each epoch was 25 seconds for\nthe LSTM-based models and 22 minutes for the\nBERT-based models. For each experiment, the\nmodel was trained only once with \ufb01xed seeds used\nto initialize the models\u2019 weights.\nFor the Adversarial Adaptation (AA), we have\nan additional hyper-parameter \u0015(see Equation 2),\nwhich we varied from 0 to 1, where 0 means no\nadaptation at all. The best results were obtained\nwith\u0015= 0:7, which means that we need to pay\nsigni\ufb01cant attention to the adversarial classi\ufb01er\u2019s\nloss in order to mitigate the media bias.\nFor the Triplet Loss Pre-training (PLT), we sam-\npled 35,017 triplets from the training set, such that\nthe examples in each triplet discuss the same topic\nin order to ensure that the change in topic has mini-\nmal impact on the distance between the examples.\nTo evaluate our models, we use accuracy and\nmacro-F1score (F1averaged across all classes),\nwhich we also used as an early stopping criterion,\nsince the classes were slightly imbalanced. More-\nover, given the ordinal nature of the labels, we\nreport the Mean Absolute Error (MAE), shown in\nEquation (4), whereNis the number of instances,\nandyiand^yiare the number of correct and of\npredicted labels, respectively.\nMAE =1\nNNX\ni=1jyi\u0000^yij (4)\n5.2 Results\nBaseline Results The results in Table 3 show the\nperformance for LSTM and for BERT at predicting\nthe political ideology of news articles for both the\nmedia-based and the random splits. We observe\nsizable differences in performance between the two\nsplits. In particular, both models perform much\nbetter when they are trained and evaluated on the\nrandom split, whereas they both fail on the media-\nbased split, where they are tested on articles from\nmedia that were not seen during training. This\nobservation con\ufb01rms our initial concerns that the\nmodels would tend to learn general characteristics\nabout news media, and then would face dif\ufb01culties\nwith articles coming from new unseen media.Model Split Macro F1Acc. MAE\nMajority 19.61 41.67 0.92\nLSTMMedia-based 31.51 32.30 0.97\nRandom 65.50 66.17 0.52\nBERTMedia-based 35.53 36.75 0.90\nRandom 80.19 79.83 0.33\nTable 3: Baseline experiments (without de-biasing or\nmedia-level representation) for the two splits.\nRemoving the Source Bias In order to further\ncon\ufb01rm the bias towards modeling the media, we\nran a side experiment of \ufb01ne-tuning BERT on the\ntask of predicting the medium given the article\u2019s\ncontent, which is a 73-way classi\ufb01cation problem.\nWe used strati\ufb01ed random sampling to create the\nevaluation splits and to make sure each set contains\nall labels (media). The results in Table 4 con\ufb01rm\nthat BERT is much stronger than the majority class\nbaseline, despite the high number of classes, which\nmeans that predicting the medium in which a target\nnews article was published is a fairly easy task.\nModel Macro F1 Acc.\nMajority 0.25 10.21\nBERT 59.72 80.12\nTable 4: Predicting the medium in which a target news\narticle was published.\nIn order to remove the bias towards modeling the\nmedium, we evaluated the impact of the adversarial\nadaptation (AA) and the Triplet Loss Pre-training\n(TLP) with the media-based split. The results in\nTable 5 show sizeable improvements when either\nof these approaches is used, compared to the base-\nline (no de-biasing). In particular, TLP yields an\nimprovement of 14.12 points absolute in terms of\naccuracy, and 12.73 points in terms of macro- F1.\nModel De-bias Macro F1Acc. MAE\nLSTMNone 31.51 32.30 0.97\nAA 40.33 40.57 0.69\nTLP 45.44 46.42 0.62\nBERTNone 35.53 36.75 0.90\nAA 43.87 46.22 0.59\nTLP 48.26 51.41 0.51\nTable 5: Impact of de-biasing (adversarial adaptation\nand triplet loss) on article-level bias detection.\nLSTM BERT\n# Representation Macro F1Acc. MAE Macro F1Acc. MAE\n1Article (baseline) 31.51 32.30 0.97 35.53 36.75 0.90\n2Article with TLP 45.44 46.42 0.62 48.26 51.41 0.51\n3Wikipedia 41.39 41.86 0.92 41.39 41.86 0.92\n4Wikipedia +Article 40.49 40.79 0.92 42.33 41.90 0.90\n5Wikipedia +Article with TLP 48.25 46.47 0.69 51.16 49.75 0.32\n6Twitter bios 60.30 62.69 0.42 60.30 62.69 0.42\n7Twitter bios +Article 60.30 62.69 0.42 60.42 63.12 0.40\n8Twitter bios +Article with TLP 62.02 70.03 0.32 64.29 72.00 0.29\nTable 6: Impact of adding media-level representations to the article-level representations (with and without de-\nbiasing). Note that the results in rows 3 and 6 are the same for both LSTM and BERT because no articles were\ninvolved, and the media-level representations were directly used to train the classi\ufb01er.\nImpact of Media-Level Representation Fi-\nnally, we evaluated the impact of incorporating the\nmedia-level representation (Twitter followers\u2019 bios\nand Wikipedia content) in addition to teh article-\nlevel representation. Table 6 illustrates these re-\nsults in an incremental way. First, we evaluated\nthe performance of the media-level representation\nalone at predicting the political ideology of news\narticles (see rows 3 and 6). We should note that\nthese results are identical for the LSTM and the\nBERT columns since no article was encoded in\nthese experiments, and the media representation\nwas used directly to train the logistic regression\nclassi\ufb01er. Then, adding the article representation\nfrom either model, without any de-biasing, had\nno or little impact on the performance (see rows\n4 vs. 3, and 7 vs. 6). This is not surprising, since we\nhave shown that, without de-biasing, both models\nlearn more about the source than about the bias in\nthe language used by the article. Therefore, the\nill-encoded articles do not provide more informa-\ntion than what the medium representation already\ngives, which is why no or too little improvement\nwas observed.\nWhen we use the triplet loss to mitigate the\nsource bias, the resulting article representation is\nmore accurate and meaningful, and the medium rep-\nresentation does offer complementary information,\nand eventually contributes to sizeable performance\ngains (see rows 5 and 8 vs. 2). The Twitter bios rep-\nresentation appears to be much more important than\nthe representation from Wikipedia, which shows\nthe importance of inspecting the media followers\u2019\nbackground and their point of views, which is also\none of the observations in (Baly et al., 2020).Overall, comparing the best results to the base-\nline (rows 8 vs. 1), we can see that ( i) using the\ntriplet loss to remove the source bias, and ( ii) in-\ncorporating media-level representation from Twit-\nter followers yields 30.51 and 28.76 absolute im-\nprovement in terms of macro F1on the challenging\nmedia-based split.\n6 Conclusion and Future Work\nWe have explored the task of predicting the leading\npolitical ideology of news articles. In particular, we\ncreated a new large dataset for this task, which fea-\ntures article-level annotations and is well-balanced\nacross topics and media. We further proposed an\nadversarial media adaptation approach, as well as a\nspecial triplet loss in order to prevent modeling the\nsource instead of the political bias in the news arti-\ncle, which is a common pitfall for approaches deal-\ning with data that exhibit high correlation between\nthe source of a news article and its class, as is the\ncase with our task here. Finally, our experimental\nresults have shown very sizable improvements over\nusing state-of-the-art pre-trained Transformers.\nIn future work, we plan to explore topic-level\nbias prediction as well as going beyond left-center-\nright bias. We further want to develop models that\nwould be able to detect speci\ufb01c fragments in an\narticle where the bias occurs, thus enabling explain-\nability. Last but not least, we plan to experiment\nwith other languages, and to explore to what extent\na model for one language is transferable to another\none given that the left-center-right division is not\nuniversal and does not align perfectly across coun-\ntries and cultures, even when staying within the\nWestern political world.\nAcknowledgments\nThis research is part of the Tanbih project8, which\naims to limit the effect of \u201cfake news,\u201d propaganda\nand media bias by making users aware of what\nthey are reading. The project is developed in col-\nlaboration between the Qatar Computing Research\nInstitute, HBKU and the MIT Computer Science\nand Arti\ufb01cial Intelligence Laboratory.\nReferences\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\nEMNLP \u201918, pages 3528\u20133539, Brussels, Belgium.\nRamy Baly, Georgi Karadzhov, Jisun An, Haewoon\nKwak, Yoan Dinkov, Ahmed Ali, James Glass, and\nPreslav Nakov. 2020. What was written vs. who\nread it: News media pro\ufb01ling using text analysis and\nsocial media context. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics , ACL \u201920, pages 3364\u20133374.\nRamy Baly, Georgi Karadzhov, Abdelrhman Saleh,\nJames Glass, and Preslav Nakov. 2019. Multi-task\nordinal regression for jointly predicting the trustwor-\nthiness and the leading political ideology of news\nmedia. In Proceedings of the 17th Annual Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies , NAACL-HLT \u201919, pages 2109\u2013\n2116, Minneapolis, MN, USA.\nAlberto Barr \u00b4on-Cedeno, Israa Jaradat, Giovanni\nDa San Martino, and Preslav Nakov. 2019. Proppy:\nOrganizing the news based on their propagandistic\ncontent. Information Processing & Management ,\n56(5):1849\u20131864.\nCeren Budak, Sharad Goel, and Justin M Rao. 2016.\nFair and balanced? Quantifying media bias through\ncrowdsourced content analysis. Public Opinion\nQuarterly , 80(S1):250\u2013271.\nGiovanni Da San Martino, Alberto Barr \u00b4on-Cede \u02dcno,\nHenning Wachsmuth, Rostislav Petrov, and Preslav\nNakov. 2020a. SemEval-2020 task 11: Detection\nof propaganda techniques in news articles. In Pro-\nceedings of the International Workshop on Semantic\nEvaluation , SemEval \u201920, Barcelona, Spain.\nGiovanni Da San Martino, Stefano Cresci, Alberto\nBarr\u00b4on-Cede \u02dcno, Seunghak Yu, Roberto Di Pietro,\nand Preslav Nakov. 2020b. A survey on compu-\ntational propaganda detection. In Proceedings of\n8http://tanbih.qcri.org/the 29th International Joint Conference on Arti\ufb01-\ncial Intelligence and the 17th Paci\ufb01c Rim Interna-\ntional Conference on Arti\ufb01cial Intelligence , IJCAI-\nPRICAI \u201920, pages 4826\u20134832, Yokohama, Japan.\nGiovanni Da San Martino, Seunghak Yu, Alberto\nBarron-Cedeno, Rostislav Petrov, and Preslav\nNakov. 2019. Fine-grained analysis of propaganda\nin news articles. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing , EMNLP \u201919, pages 5636\u20135646, Hong\nKong, China.\nKareem Darwish, Michael Aupetit, Peter Stefanov, and\nPreslav Nakov. 2020. Unsupervised user stance de-\ntection on Twitter. In Proceedings of the Interna-\ntional AAAI Conference on Web and Social Media ,\nICWSM \u201920, pages 141\u2013152, Atlanta, GA, USA.\nStefano DellaVigna and Ethan Kaplan. 2007. The Fox\nNews effect: Media bias and voting. The Quarterly\nJournal of Economics , 122(3):1187\u20131234.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies , NAACL-HLT \u201919, pages 4171\u20134186, Min-\nneapolis, MN, USA.\nYoan Dinkov, Ahmed Ali, Ivan Koychev, and Preslav\nNakov. 2019. Predicting the leading political ide-\nology of YouTube channels using acoustic, textual,\nand metadata information. In Proceedings of the\n20th Annual Conference of the International Speech\nCommunication Association , INTERSPEECH \u201919,\npages 501\u2013505, Graz, Austria.\nErick Elejalde, Leo Ferres, and Eelco Herder. 2018. On\nthe nature of real and perceived bias in the main-\nstream media. PloS one , 13(3):e0193765.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, Franc \u00b8ois Lavi-\nolette, Mario Marchand, and Victor Lempitsky.\n2016. Domain-adversarial training of neural net-\nworks. The Journal of Machine Learning Research ,\n17(1):2096\u20132030.\nDoris A Graber and Johanna Dunaway. 2017. Mass\nmedia and American politics . SAGE Publications.\nMaur \u00b4\u0131cio Gruppi, Benjamin D. Horne, and Sibel Adal\u0131.\n2020. NELA-GT-2019: A large multi-labelled news\ndataset for the study of misinformation in news arti-\ncles. arXiv preprint arXiv:2003.08444 .\nSepp Hochreiter and J \u00a8urgen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation ,\n9(8):1735\u20131780.\nBenjamin D. Horne, William Dron, Sara Khedr, and\nSibel Adali. 2018. Assessing the news landscape:\nA multi-module toolkit for evaluating the credibility\nof news. In Proceedings of the The Web Conference ,\nWWW \u201918, pages 235\u2013238, Lyon, France.\nBenjamin D Horne, Jeppe N\u00f8rregaard, and Sibel Adal\u0131.\n2019. Different spirals of sameness: A study of con-\ntent sharing in mainstream and alternative media. In\nProceedings of the International AAAI Conference\non Web and Social Media , ICWSM \u201919, pages 257\u2013\n266, Munich, Germany.\nShanto Iyengar and Kyu S Hahn. 2009. Red media,\nblue media: Evidence of ideological selectivity in\nmedia use. Journal of communication , 59(1):19\u201339.\nYe Jiang, Johann Petrak, Xingyi Song, Kalina\nBontcheva, and Diana Maynard. 2019. Team Bertha\nvon Suttner at SemEval-2019 Task 4: Hyperpartisan\nnews detection using ELMo sentence representation\nconvolutional network. In Proceedings of the 13th\nInternational Workshop on Semantic Evaluation , Se-\nmEval \u201919, pages 840\u2013844, Minneapolis, MN, USA.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 Task 4: Hyperpartisan news detection. In Pro-\nceedings of the 13th International Workshop on Se-\nmantic Evaluation , SemEval \u201919, pages 829\u2013839,\nMinneapolis, Minnesota, USA.\nVivek Kulkarni, Junting Ye, Steven Skiena, and\nWilliam Yang Wang. 2018. Multi-view models for\npolitical ideology detection of news articles. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing , EMNLP \u201918, pages\n3518\u20133527, Brussels, Belgium.\nWei-Hao Lin, Theresa Wilson, Janyce Wiebe, and\nAlexander Hauptmann. 2006. Which side are you\non? Identifying perspectives at the document and\nsentence levels. In Proceedings of the Tenth Confer-\nence on Computational Natural Language Learning ,\nCoNLL \u201906, pages 109\u2013116.\nBo Pang and Lillian Lee. 2004. A sentimental edu-\ncation: Sentiment analysis using subjectivity sum-\nmarization based on minimum cuts. In Proceed-\nings of the 42nd Annual Meeting of the Association\nfor Computational Linguistics , ACL \u201904, pages 271\u2013\n278, Barcelona, Spain.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. GloVe: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP \u201914, pages 1532\u20131543, Doha, Qatar.\nMartin Potthast, Johannes Kiesel, Kevin Reinartz,\nJanek Bevendorff, and Benno Stein. 2018. A stylo-\nmetric inquiry into hyperpartisan and fake news. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics , ACL \u201918,\npages 231\u2013240, Melbourne, Australia.Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nV olkova, Yejin Choi, and Paul G Allen. 2017. Truth\nof varying shades: Analyzing language in fake news\nand political fact-checking. In Proceedings of the\n2017 Conference on Empirical Methods in Natu-\nral Language Processing , EMNLP \u201917, pages 2931\u2013\n2937, Copenhagen, Denmark.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing , EMNLP-IJCNLP \u201919,\npages 3973\u20133983, Hong Kong, China.\nDiego Saez-Trumper, Carlos Castillo, and Mounia Lal-\nmas. 2013. Social media news communities: Gate-\nkeeping, coverage, and statement bias. In Proceed-\nings of the 22nd ACM International Conference on\nInformation & Knowledge Management , CIKM \u201913,\npage 1679\u20131684, San Francisco, CA, USA.\nAbdelrhman Saleh, Ramy Baly, Alberto Barr \u00b4on-\nCede \u02dcno, Giovanni Da San Martino, Mitra Mo-\nhtarami, Preslav Nakov, and James Glass. 2019.\nTeam QCRI-MIT at SemEval-2019 Task 4: Propa-\nganda analysis meets hyperpartisan news detection.\nInProceedings of the 13th International Workshop\non Semantic Evaluation , SemEval \u201919, pages 1041\u2013\n1046, Minneapolis, MN, USA.\nFlorian Schroff, Dmitry Kalenichenko, and James\nPhilbin. 2015. FaceNet: A uni\ufb01ed embedding for\nface recognition and clustering. In Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition , CVPR \u201915, pages 815\u2013823,\nBoston, MA, USA.\nYanchuan Sim, Brice D. L. Acree, Justin H. Gross, and\nNoah A. Smith. 2013. Measuring ideological pro-\nportions in political speeches. In Proceedings of the\n2013 Conference on Empirical Methods in Natural\nLanguage Processing , EMNLP \u201913, pages 91\u2013101,\nSeattle, Washington, USA.\nPeter Stefanov, Kareem Darwish, Atanas Atanasov,\nand Preslav Nakov. 2020. Predicting the topical\nstance and political leaning of media using tweets.\nInProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , ACL \u201920,\npages 527\u2013537.\nYifan Zhang, Giovanni Da San Martino, Alberto\nBarr\u00b4on-Cede \u02dcno, Salvatore Romeo, Jisun An, Hae-\nwoon Kwak, Todor Staykovski, Israa Jaradat, Georgi\nKaradzhov, Ramy Baly, Kareem Darwish, James\nGlass, and Preslav Nakov. 2019. Tanbih: Get to\nknow what you are reading. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing ,\nEMNLP-IJCNLP \u201919, pages 223\u2013228, Hong Kong,\nChina.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "We can detect your bias: Predicting the political ideology of news articles", "author": ["R Baly", "GDS Martino", "J Glass", "P Nakov"], "pub_year": "2020", "venue": "arXiv preprint arXiv:2010.05338", "abstract": "We explore the task of predicting the leading political ideology or bias of news articles. First,  we collect and release a large dataset of 34,737 articles that were manually annotated for"}, "filled": false, "gsrank": 370, "pub_url": "https://arxiv.org/abs/2010.05338", "author_id": ["zJuI3D8AAAAJ", "URABLy0AAAAJ", "pfGI-KcAAAAJ", "DfXsKZ4AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:49oi5S-3UeEJ:scholar.google.com/&output=cite&scirp=369&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D360%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=49oi5S-3UeEJ&ei=RrWsaO_lEeHUieoP9LKZ6AI&json=", "num_citations": 207, "citedby_url": "/scholar?cites=16235959547982961379&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:49oi5S-3UeEJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2010.05338"}}, {"title": "BaitWatcher: A lightweight web interface for the detection of incongruent news headlines", "year": "2020", "pdf_data": "BaitWatcher: A lightweight web interface for the\ndetection of incongruent news headlines?\nKunwoo Park1, Taegyun Kim2;3, Seunghyun Yoon4, Meeyoung Cha3;2, and\nKyomin Jung4\n1Qatar Computing Research Institute, Doha, Qatar\n2Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea\n3Institute for Basic Science, Daejeon, Republic of Korea\n4Seoul National University, Seoul, Republic of Korea\nmcha@ibs.re.kr\nAbstract. In digital environments where substantial amounts of infor-\nmation are shared online, news headlines play essential roles in the selec-\ntion and di\u000busion of news articles. Some news articles attract audience\nattention by showing exaggerated or misleading headlines. This study\naddresses the headline incongruity problem, in which a news headline\nmakes claims that are either unrelated or opposite to the contents of the\ncorresponding article. We present BaitWatcher , which is a lightweight\nweb interface that guides readers in estimating the likelihood of incon-\ngruence in news articles before clicking on the headlines. BaitWatcher\nutilizes a hierarchical recurrent encoder that e\u000eciently learns complex\ntextual representations of a news headline and its associated body text.\nFor training the model, we construct a million scale dataset of news arti-\ncles, which we also release for broader research use. Based on the results\nof a focus group interview, we discuss the importance of developing an\ninterpretable AI agent for the design of a better interface for mitigating\nthe e\u000bects of online misinformation.\nKeywords: Online news \u0001Deep learning \u0001Misinformation \u0001Headline\nincongruity \u0001Browser extension\n1 Introduction\nThe dissemination of misleading or false information in news media has become\na critical social problem [27]. Because information propagation online lacks ver-\ni\fcation processes, news contents that are rapidly disseminated online can put\nveiled threats to society. In digital environments that are under information over-\nload, people are less likely to read or click on the entire contents; instead, they\nread only news headlines [18]. A substantial amount of news sharing is headline-\nbased, where people circulate news headlines without necessarily having checked\n?This research article is published as a book chapter of Fake News, Disinformation,\nand Misinformation in Social Media-Emerging Research Challenges and Opportuni-\nties. Springer, 2020.arXiv:2003.11459v1  [cs.CL]  23 Mar 2020\nFig. 1. An example of news article with the incongruent headline.\nthe full news story. News headlines are known to play an essential role in making\n\frst impressions on readers [33], and these \frst impressions have been shown to\npersist even after the full news content has been read [14]. Therefore, if a news\nheadline does not correctly represent the full news story, it could mislead read-\ners into the promotion of overrated or false information, which becomes hard to\nrevoke.\nThis paper tackles the problem of headline incongruence [11], where a news\nheadline makes claims that are unrelated to or distinct from the story in the\ncorresponding body text. Figure 1 shows such an example, where the catchy news\nheadline promises to describe the bene\fts of yoga, yet the body text is mainly an\nadvertisement for a new yoga program. While this mismatch can be recognized\nwhen people read both the headline and the body text, it is almost impossible\nto detect it before clicking on the headline in online platforms. Incongruent\nnews headlines make not only incorrect impressions on readers [14] but also\nbecome problematic when they are shared on social media, where most users\nshare content without reading the full text [18]. Therefore, the development of\nan automated approach that detects incongruent headlines in news articles is\ncrucial. Identifying incongruent headlines in advance will more e\u000bectively assist\nreaders in selecting which news stories to consume and, thus, will decrease the\nlikelihood of encountering unwanted information.\nPrevious research has tried to detect misleading headlines by either analyz-\ning linguistic features of news headlines [7,10] or analyzing textual similarities\nbetween news headlines and body texts [16,42]. However, the lack of a large-scale\npublic dataset hinders the development of sophisticated deep learning approaches\nthat will be better suited for such challenging detection tasks, which typically\nrequire a million-scale dataset across various domains [29,21]. This study aims\nat \flling this gap by proposing an automated approach for generating a million-\nscale dataset for headline incongruity, developing deep learning approaches that\nare motivated by hierarchical structures of news articles, and evaluating the\nmodel in the wild by developing a lightweight web interface that estimates the\nlikelihood of an incongruent news headline.\nOur contributions are summarized as follows:\n1. We develop a million-scale dataset for the incongruent headline problem,\nwhich covers almost all of the news articles that were published in a nation\nover two years. The corpus is composed of pairs of news headlines and body\ntexts along with the annotated incongruity labels. The automatic approach\nfor annotation can be applied to any news articles in any language and, there-\nfore, will facilitate future research on the detection of headline incongruity\nin a broader research community.\n2. We propose deep hierarchical models that encode full news articles from the\nword-level to the paragraph-level. Experimental results demonstrate that\nour models outperform baseline approaches that were proposed for similar\nproblems. We extensively evaluate our models with real data. Manual veri-\n\fcation successfully demonstrates the e\u000ecacy of our dataset in training for\nincongruent headline detection.\n3. To facilitate news reading in the wild, we present BaitWatcher | a lightweight\nweb interface that presents the prediction results that are obtained based on\ndeep learning models before readers click on news headlines. Along with\nthis study, implementation details and codes will be shared. BaitWatcher\nis platform-agnostic; hence, it can be applied to any online news service.\nThe results of a user study of focus group interviews not only support the\ne\u000bectiveness of the BaitWatcher web interface but also reveal a need for the\ndevelopment of interpretable models.\n2 Related work\n2.1 Learning-based approaches for detecting misleading headlines\nInterest has been growing in the automatic detection of misleading headlines.\nPrevious studies constructed datasets that were annotated by journalists or\ncrowd-sourced workers and proposed machine learning approaches. For example,\na recent study suggested a co-training method for the detection of ambiguous\nand misleading headlines from pairs that consist of a title and a body text [43].\nWe review the literature for each type of misinformation and its relation to the\nheadline incongruence problem.\nA series of studies have focused on the detection of clickbait headlines , which\nare a type of web content that attracts an audience and induces them to click on a\nlink to a web page [10]. There is no single and concise de\fnition in the literature;\nclickbait is regarded as an umbrella term that describes many techniques for\nattracting attention and invoking curiosity to entice the reader to click on a\nheadline [26]. One study [8] released a manually labeled dataset and developed\nan SVM model for the prediction of clickbait based on linguistic patterns of\nnews headlines. Using this dataset, other researchers suggested a neural network\napproach that predicts the clickbait likelihood [35]. A national-level clickbait\nchallenge was held, where the objective was to identify social media posts that\nentice readers to click on a link [2]. The signi\fcant di\u000berence between clickbait\nand headline incongruence is that clickbait is characterized solely by the headline,\nwhereas an incongruent headline de\fned by the relationship between the headline\nand the body text. These de\fnitions are not mutually exclusive: a clickbait\nheadline can also be incongruent with its main article. Clickbait headlines may\nbe acceptable if they represent corresponding body texts accurately; however,\nthe consequences can be more severe if catchy headlines mislead people with\nincorrect information.\nThe detection of headline incongruence is also related to the stance detection\ntask, which aims at identifying the stance of speci\fed claims against a reference\ntext. The Emergent project [16] provides a dataset of 300 rumored claims and\n2,595 associated news articles, each of which is labeled by journalists to indicate\nwhether the stance of the article is for,against , or observing the claim. The\nFake News Challenge 2017 was held to promote the development of methods\nfor estimating the stance of a news article [3]. This dataset provides 50,000\npairs of headlines and body texts that were generated from 1,683 original news\narticles. Each data entry is annotated with one of the following four stances:\nagree ,disagree ,discuss , and unrelated . While many teams attempted to employ\ndeep learning models (e.g., [12,34]), the winning model was a simple ensemble\napproach that combines predictions from XGBoost [9] that are based on hand-\ndesigned features and a deep convolution dual encoder that independently learns\nword representations from headlines and body texts using convolutional neural\nnetworks.\nStance detection is technically similar to the headline incongruence problem\nin that they consider the textual relationship between a headline (claim) and\nthe corresponding body text (reference text). It may be possible to transform\nthree or four stances into binary classes, such as related andunrelated . But most\navailable datasets cannot be directly utilized for the training of deep learning\nmodels for the headline incongruence problem because a related headline can\nalso be incongruent . For example, the article in Figure 1 would be labeled as\nrelated because both the headline and the body text cover the topic of yoga;\nhowever, the headline and the body text are incongruent.\nThis current study tackles the headline incongruence problem [11], which is\na signi\fcant kind of misinformation that originates from a discrepancy in a news\nheadline and the corresponding body text. No million-scale dataset has been\nopenly available for this problem.\n2.2 Designing web interfaces for news readers\nA line of research was conducted in which a news service was constructed as\na separate system. A decade ago, a pioneering study provided a news service\non the web [31]. The researchers designed NewsCube, which was a news service\nthat aimed at mitigating the e\u000bect of media bias. The service provided readers\nwith multiple classi\fed viewpoints on a news event of interest, which facilitated\nthe formulation of more balanced views. Most recently, a study implemented a\nweb system that highlights objective sentences in a user text to mitigate the\nbiased reporting that facilitates polarization [28]. Another study developed a\nvisualization tool that enables Twitter users to explore the politically-active\nparts of their social networks [20] and conducted a randomized trial to evaluate\nthe impact of recommending accounts of the opposite political ideology. The\nconstruction of a separate news service enables researchers to investigate the\ne\u000bects of a machine while controlling for other factors. However, it mostly serves\nas a proof of concept; hence, the impact on end-users is limited.\nMany stand-alone systems su\u000ber from gaining tra\u000ec. Therefore, other studies\nhave developed a lightweight browser widget that operates with available news\nservices on the web, which enables more users to be reached in practice. One\nstudy presented a browser widget that encourages the reading of diverse political\nviewpoints based on the selective exposure theory [30]. According to a \feld\ndeployment study, showing feedback led to more balanced exposure. A browser\nextension was also implemented to help people determine whether a headline is\nclickbait or a general headline [8]; however, the e\u000bects of this mechanism were\nnot evaluated.\nMotivated by these studies that employ browser widgets, this study imple-\nments a lightweight web interface that helps readers determine whether a spec-\ni\fed headline is incongruent before clicking on the headline. We also conduct a\nuser study via questionnaires and in-depth interviews to estimate the impact of\nthe web interface.\n3 Data and Methodology\nThis section presents the approach to building a million-scale dataset for the\nheadline incongruence problem and the methodology for detecting such mislead-\ning headlines via neural networks. The objective is to determine whether a news\narticle contains an incongruent headline, given a pair that consists of a headline\nand a body text. For detection models, we call the output probability of being\nan incongruent headline the incongruence score .\n3.1 Dataset generation\nOne natural method for constructing a labeled dataset is for researchers and\ncrowdsourced workers to manually annotate data. However, the training of so-\nphisticated classi\fcation algorithms requires a large dataset, which is not feasible\nto obtain via manual annotation due to high cost and reliability issues. Alterna-\ntively, this work presents a systematic approach for the automatic generation of\nmillion-scale datasets that are composed of incongruent and correct headlines.\nFig. 2. An illustration on the generating process of incongruent headlines.\nFirst, we crawled a nearly complete set of news articles that were published\nin South Korea from January 2016 to October of 2017. From over 4 million news\narticles, we conducted a series of cleansing steps, such as removal of noncritical\ninformation (e.g., reporter name and nontextual information such as photos and\nvideos). Next, we transformed word tokens to integers, which is released with\nvocabulary to help researchers utilize the dataset without being hindered by a\nlanguage barrier.\nTo label the incongruity of headlines for millions of news articles, we im-\nplanted unrelated or topically inconsistent content into the body text of original\nnews articles rather than crafting new headlines. Figure 2 illustrates the gen-\neration process of incongruent headlines. This process can produce a pair that\nconsists of a headline and a body text such that the headline tells stories that\ndi\u000ber from the body text content. Hence, the automation process for creating\nincongruent-labeled data involves the following steps: (1) sampling a target ar-\nticle from the corpora, (2) sampling part-of-content from another article of the\ncorpora, and (3) inserting this part-of-content into the target article. We con-\ntrolled the topics of the sampled paragraphs to be similar to each target article\nby employing the meta-information on news articles (e.g., news category).\nWe created the congruent-labeled data by selecting them from suitable cor-\npora. No headline in this set overlaps with the incongruent-labeled data. Nonethe-\nless, this process may yield false-negative instances if a real article that has an\nincongruent headline is chosen inappropriately as a target. We conducted addi-\ntional steps to reduce the number of false negatives via rule-based processing,\nsuch as the inspection of advertising phrases with an n-gram dictionary. We\nalso hired human annotators to read 1,000 randomly sampled articles from the\ncreated dataset and to check whether those articles are labeled correctly. These\ne\u000borts minimize the number of errors that can arise from the automatic genera-\ntion process. The \fnal corpus is composed of a training set of 1.7 M news articles\nthat are balanced against the incongruity label. For evaluation, we maintained\nMean Std. Error\nNumber of tokens in headline 13.71 0.003\nNumber of tokens in body text 513.97 0.208\nNumber of paragraphs in body text 8.17 0.004\nNumber of tokens in paragraph 61.7 0.018\nTable 1. Dataset statistics.\nseparate development and test sets of 0.1 M instances each. The statistics of the\ndatasets are presented in Table 1.\nThis approach is language-agnostic; hence, it can be applied to any news\ncorpora of any language. The generated dataset is publicly available on the\nGitHub page.5\n3.2 Baseline approaches\nWe introduce four baseline approaches that have been applied to the headline\nincongruence problem. Feature-based ensemble algorithms have been widely uti-\nlized for their simplicity and e\u000bectiveness. Among various methods, the XGBoost\nalgorithm has shown superior performance across various prediction tasks [9]. For\nexample, in a recent challenge on determining the stances of news articles [3],\nthe winning team applied this algorithm based on multiple features to measure\nsimilarities between the headline and body text [40]. As a baseline, we imple-\nmented the XGBoost (XGB) classi\fer by utilizing the set of features that are\ndescribed in the winning model, such as cosine similarities between the headline\nand body text. In addition to this model, we trained support vector machine\n(SVM) classi\fers based on the same set of features.\nRecurrent Dual Encoder (RDE): A recurrent dual encoder that consists of\ndual Recurrent Neural Networks (RNN) has been utilized to calculate a similarity\nbetween two text inputs [29]. We apply this model to the headline incongruence\nproblem via dual RNNs based on gated recurrent unit (GRU) that encode the\nheadline and body, respectively. When a RNN encodes word sequences, each\nword is passed through a word-embedding layer that converts a word index to a\ncorresponding 300-dimensional vector. After the encoding step, the probability of\nbeing incongruent headline is calculated by using the \fnal hidden states of RNNs\nfor headline and body text. The incongruence score in the training objective is\nas follows:\np(label) =\u001b((hH\nth)|M hB\ntb+b);\nL=\u0000logNY\nn=1p(labelnjhH\nn;th;hB\nn;tb);(1)\nwherehH\nthandhB\ntbare last hidden state of each headline and body text RNN\nwith the dimensionality h2Rd. TheM2Rd\u0002dand biasbare learned model\n5http://github.com/david-yoon/detecting-incongruity/\nparameters. Nis the total number of samples used in training and \u001bis the\nsigmoid function.\nConvolution Dual Encoder (CDE): Following the Convolutional Neural Net-\nwork (CNN) architecture for text understanding [25], we apply Convolutional\nDual Encoder to the headline incongruence problem. Taking the word sequence\nof headline and body text as input to the convolutional layer, we obtained a\nvector representation v=fviji= 1;\u0001\u0001\u0001;kgfor each part of the article through\nthe max-over-time pooling after computing convolution with k\flters as follows:\nvi=g(fi(W)); (2)\nwheregis max-over-time pooling function, fiis the CNN function with i-th\nconvolutional \flter, and W2Rt\u0002dis a matrix of the word sequence. We use\ndual CNNs to encode a pair of headline and body text into vector representations.\nAfter encoding each part of a news article, the probability that a given article\nhas the incongruent headline is calculated in a similar way to the equation (1).\n3.3 Proposed methods\nWhile the available approaches perform reasonably for short text data, deal-\ning with a long sequence of words in news articles will result in degraded per-\nformance [32]. For example, RNN that is utilized in RDE performs poorly in\nremembering information from the distant past. While CDE learns local depen-\ndencies between words, the typical short length of its convolutional \flter prevents\nthe model from capturing any relationships between words in distinct positions.\nThe inability to handle long sequences is a critical drawback of the standard\ndeep approaches to the headline incongruence problem because a news article\ncan be very long. As presented in Table 1, the average word count per article is\n513.97 in our dataset.\nTherefore, we \fll this gap by proposing neural architectures that e\u000eciently\nlearn hierarchical structures of long text sequences. We also present a data aug-\nmentation method that e\u000eciently reduces the length of the target content while\nincreasing the size of the training set.\nHierarchical Recurrent Dual Encoder (HRDE): Inspired by previous ap-\nproaches that models text using a hierachical architecture [46,44,45], this model\ndivides the text into a list of paragraphs and encodes the entire text input from\nthe word level to the paragraph level using a two-level hierarchy of RNN archi-\ntectures.\nFor each paragraph, the word-level RNN encodes the word sequences wp=\nfwp;1:tgtohp=fhp;1:tg. Next, the hidden states of the word-level RNN are fed\ninto the next-level RNN that models a sequence of paragraphs while preserving\nthe order. The hierarchical architecture can learn textual patterns of news ar-\nticles with fewer sequential steps for RNNs compared to the steps required for\nRDE. While RDE involves an average of 513.97 steps to learn news articles in\nour dataset, AHDE only accounts for 61.7 and 8.17 steps on average for word-\nFig. 3. Diagram of AHDE. Every paragraph is encoded into a hidden state, and\nthe sequence of the hidden states corresponding to each paragraph is further encoded\ninto the hidden state corresponding to the entire body text. The model can learn\nimportance of paragraphs in a body text for detecting headline incongruity from an\nattention mechanism.\nand paragraph-level of RNN, respectively (see Table 1). The hidden states of\nhierarchical RNNs are as follows:\nhp;t=f\u0012(hp;t\u00001;wp;t);\nup=g\u0012(up\u00001;hp);(3)\nwhereupis the hidden state of the paragraph-level RNN at the p-th paragraph\nsequence, and hpis the word-level RNN's last hidden state of each paragraph\nhp2fh1:p;tg. We use the same training objective as the RDE model such that the\nincongruence score, the probability of having incongruent headlines, is calculated\nas follows:\np(label) =\u001b((uH\nph)|M uB\npb+b) (4)\nAttentive Hierarchical Dual Encoder (AHDE): In addition to the hierar-\nchical architecture of HRDE, attention mechanism is employed to the paragraph-\nlevel RNN to enable the model to learn the importance of each paragraph in a\nbody text for detecting incongruity embedded in the corresponding headline. Ad-\nditionally, we utilize bi-directional RNNs for the paragraph-level RNN to learn\nsequential information in both directions from the \frst paragraph and the last\nparagraph.\nFigure 3 illustrates a diagram of AHDE. Each upof a body text is aggregated\naccording to its correspondence with the headline as follows:\nsp= v|tanh(WB\nuuB\np+WH\nuuH);\nai= exp(si)=P\npexp(sp);\nuB=P\niaiuB\ni;(5)\nFig. 4. Diagram of the IP method. A body text is divided into multiple paragraphs,\neach of which is compared to the corresponding headline to calculate the incongruence\nscore of each paragraph. The maximum value of all scores is the incongruence score for\nthe pair of the headline and the body text.\nwhereuB\npindicates the p-th hidden state of the paragraph-level RNN that learns\nthe representation of a body text. The uHindicates the last hidden state of the\nparagraph-level RNN with the corresponding headline. Similar to HRDE, the\nincongruence score is calculated as follows:\np(label) =\u001b((uH)|M uB+b) (6)\nHierarchical Recurrent Encoder (HRE): The HRDE and AHDE model\nuses two hierarchical RNNs for encoding text from the word level to the para-\ngraph level. Compared to non-hierarchical alternatives such as RDE and CDE,\nthose models require higher computation resources in training and inference.\nTherefore, we investigate a moderate approach that models hierarchical struc-\ntures of news articles with a simpler neural architecture. A body text is divided\ninto paragraphs, each of which is represented by averaging word-embedding\nvectors of words within the paragraph. In other words, HRE calculates hpin\nequation (3) by the average of the word vectors in the paragraph p,hp=P\niembedding (wi); wi\u001ap-th paragraph. Then, a paragraph-level RNN is ap-\nplied to the paragraph-encoded sequence input, hp, for retrieving the \fnal en-\ncoding vector of the entire body text. The incongruence score is calculated by\np(label) =\u001b((hH)|M hp+b) (7)\nwherehHindicates the average embedding vector of the words in the headline.\nIndependent Paragraph (IP) method In addition to the neural architec-\nture, we propose a data augmentation method that divides a body text into\nmultiple paragraphs and learns the relationship between each paragraph and\nthe corresponding headline independently. For that purpose, we transformed ev-\nery pair that consists of a headline and a body text in the original dataset into\nmultiple pairs of the headline and each paragraph. This conversion process not\nonly reduces the length of text that a model should process but also increase the\ntotal number of training instances. For example, the average number of words in\na body text shrinks from 513.97 to 61.7 (see Table 1), and the number of training\ninstances increases from 1.7 M to 14.2 M. We expect that this di\u000berence makes\nthe proposed deep learning models e\u000eciently learn the pattern of the semantic\nmismatch between a headline and its body text.\nFigure 4 illustrates the diagram of the IP method, which computes incon-\ngruence score of each paragraph from its relationship with the corresponding\nheadline. The \fnal incongruence score for the pair of the headline and the body\ntext is determined as the maximum of the incongruence score for the headline\nand each paragraph as follows:\np(label) =max(s1:p); (8)\nwherespis the incongruence score calculated from the p-th paragraph of the\nbody text and the headline. The selection of the maximum score can better\nidentify news articles that contain a paragraph that is highly unrelated to the\nnews headline. We also tested other aggregation methods such as average and\nminimum, but max function led to the best performance.\nWith the use of the IP method, hierarchical approaches consider sentence\nin a paragraph the lower unit in two-level hierarchy of neural architectures. In\nparticular, the incongruence score of each detection model is calculated in the\nfollowing ways:\n{ XGB/SVM with IP: For each paragraph, XGB/SVM measures the incon-\ngruence score by extracting features from its headline and the paragraph.\n{ RDE/CDE with IP: Both models encode word sequences in each para-\ngraph of a body text and compare them with the corresponding headline.\n{ HRDE/AHDE with IP: To obtain the incongruence score for each para-\ngraph, the \frst-level RNN encodes word sequences for each sentence in the\nparagraph, and the second-level RNN takes as input the hidden states of the\nsentences that are retrieved from the \frst-level RNN.\n{ HRE with IP: HRE calculates the mean of word vectors for each sentence.\nThen, a RNN encodes a sequence of sentences by taking the averaged word\nvectors as input.\n4 Evaluation Experiments\n4.1 Automatic evaluation\nTable 2 presents the performances of all approaches on the test set. We report\nthe accuracy and the AUROC (area under the receiver operating characteristic\ncurve) value, which is a balanced metric for the label distribution. Here, we make\nthree main observations.\nFirst, among the baseline models, RDE realized the best performance with\nan accuracy of 0.845 and an AUROC of 0.939. The decent performance of RDE\nsuggests that recurrent neural networks are well suited to the learning of se-\nquential text representations of news articles, in contrast to the feature-based\nModelWithout IP With IP\nAccuracy AUROC Accuracy AUROC\nSVM 0.640 0.703 0.677 0.809\nXGB 0.677 0.766 0.729 0.846\nCDE 0.812 0.9 0.870 0.959\nRDE 0.845 0.939 0.863 0.955\nHRDE 0.885 0.944 0.881 0.962\nAHDE 0.904 0.959 0.895 0.977\nHRE 0.85 0.927 0.873 0.952\nTable 2. Model performance with and without the Independent Paragraph (IP)\nmethod. Top-2 scores are marked as bold. The top 4 rows indicate the baseline perfor-\nmance and the bottom 3 rows shows the performance of the proposed models.\napproaches and the convolutional encoders, which learn the local dependencies\nof word tokens.\nSecond, the performance margin increased signi\fcantly when hierarchical\nstructures were applied to RDE. In HRDE, the accuracy and AUROC increased\nby 0.04 and 0.05, respectively. Considering the hierarchical structure of news ar-\nticles in the design of neural architectures may facilitate the learning of textual\ninformation of news articles more e\u000eciently, such that headline incongruity can\nbe more accurately identi\fed. In contrast, in HRE, merely inputting the mean\nword representation for each paragraph into a single layer recurrent network did\nnot yield a signi\fcant improvement. Compared to RDE, the accuracy increased\nwith a margin of 0.005; however, the AUROC decreased by 0.012. Third, we\nfound the attention ability of AHDE further enhanced the performance up to an\naccuracy of 0.904 and an AUROC of 0.959, namely, knowledge of relevant para-\ngraphs in the detection of incongruent headlines facilitated the e\u000ecient exami-\nnation of the relationship between the headline and each paragraph by AHDE.\nLast, the prediction performance increased signi\fcantly when the IP method\nwas applied. RDE and CDE bene\ftted most from the application of the IP\nmethod; they even showed performances that were comparable to those of the\nhierarchical models. Although those simple models do not have a suitable struc-\nture for handling lengthy news data (on average, the body texts and the para-\ngraphs contain 518.97 and 61.7 words, respectively, according to Table 1), the\nIP method helped them examine the relationship between the headline and each\nparagraph more e\u000eciently.\n4.2 Manual evaluation\nTo test the e\u000ecacies of the dataset and the proposed models for the detection\nof incongruent headlines in the wild, we evaluated the pretrained models on\nmore recently published news articles. We gathered 232,261 news articles that\nwere published from January to April of 2018. Via evaluation of the model\non this recently assembled dataset, we can measure the generalizability of our\napproaches to dataset generation and headline incongruity detection in practice.\n0.60.70.80.91.0\n25 50 75 100 125 150 175 200 225 250\nTop N by the incongruence scorePrecisionAHDE with IP\nAHDEFig. 5. Precision values for detecting news articles with incongruent headlines in the\nnewly gathered dataset. The x-axis shows the top-N articles by incongruence scores,\nand the y-axis presents its corresponding precision.\nFirst, we manually inspected random samples of news articles to determine\nwhether they have incongruent headlines; however, we could not retrieve su\u000e-\nciently many instances with incongruent headlines for evaluation. The lack of\nmisleading articles is possibly due to the sparsity of such headlines in practice,\ndespite their critical importance. Therefore, instead of manually labeling ran-\ndomly sampled news articles, a majority of which may correspond to general\nheadlines, we manually evaluated the top- Narticles in terms of the incongru-\nence scores that are assigned by our model. Since models assign incongruence\nscores (output probabilities) based on their con\fdence for classi\fcation, we be-\nlieve such evaluation successfully estimates the degree of precision of a prediction\nmodel. This type of assessment is widely used in tasks in which it is impossible to\ncount all possible real cases in a dataset such as question answering system [17].\nFigure 5 presents the precision scores for the AHDE models that are trained\nwith and without the IP method. The x-axis corresponds to the top-N articles\nin terms of the incongruence scores that are assigned by the models out of the\nnewly gathered news articles over four months. The y-axis corresponds to the\nprecision values of the top-N articles. Here, we make three observations.\nFirst, the AHDE model with the IP augmentation consistently shows higher\nprecision than the AHDE model without the IP method. This \fnding sup-\nports the superior performance of the IP method across evaluations. Second,\nthe AHDE model with the IP method realized a precision of 1.0 for the top 25\narticles. Even though the model was trained on a separate dataset, it success-\nfully \fltered out real cases in which the headline conveys di\u000berent information\nthan the associated body text. Third, when we evaluate the top 250 articles,\nthe precision of the AHDE model with the IP method reduced to 0.82. This\nprecision value is su\u000eciently high for the detection of incongruent headlines in\nreal news platforms.\n5 BaitWatcher: A lightweight web interface for the\ndetection of headline incongruity\nThis section introduces a new web interface that aims at reducing the adverse\ne\u000bects of incongruent news headlines on the news reading experience. Incongru-\nent headlines can mislead readers with an unexpected body text because they\nare one of the critical cues that are used in the selection of news articles in online\nenvironments. Before clicking on a headline and reading the body text, news-\nreaders are not able to determine in advance the content of the news story. We\nhypothesize that news readers will be empowered if they are given a choice and\nadditional information about the headline incongruence score. As a proof of con-\ncept, we designed and implemented a web interface, namely, BaitWatcher , that\nquickly reports the incongruence score. We conducted a focus group interview\nto investigate the e\u000bects of the web interface.\n5.1 Design and implementation\nThe main feature of BaitWatcher is that it reveals the likelihood of a speci\fed\nnews headline being incongruent to its full body text based on the presented\ndeep learning model. This information is made visible to users before they click\non the headline to read the entire story. BaitWatcher is platform-agnostic and\ncan be implemented on top of any news platforms. We expect that revealing\nthe hidden information through a simple interface will empower news readers\nby helping them determine by themselves whether to read news articles with\npotentially incorrect headlines or not. As shown in Figure 6, if a user hovers\na mouse pointer over a news headline of interest, the BaitWatcher interface\nimmediately displays the prediction result (the sigmoid output) of a pretrained\ndeep neural network via a tooltip view. This additional information facilitates\nreaders in the selection of which news articles to read in detail. Once a user\ndecides to read a news article, the full news story will be made available to\nthem as usual, along with a user feedback section that appears at the bottom of\nthe page. This feedback section was implemented in the form of a button that\nsignals whether the news story was consistent with the context that was provided\nby the news headline. This process enables the system to gather manual labels\non incongruent news articles in the wild, which will be used to train the deep\nlearning model periodically to increase the accuracy and robustness of detection.\nTo reduce the computational burden of running a deep model, BaitWatcher\nwas implemented as a browser extension that is based on a client-server archi-\ntecture. After installing the extension on a browser (e.g., Chrome), online users\ncan choose to read news articles on any news platforms and obtain information\nabout the incongruence scores of news headlines before reading the full corre-\nsponding body text. As shown in the left side of Figure 6, if a reader hovers the\nmouse cursor over a news headline, BaitWatcher sends an HTTP request from\nthe client to the API server with the hyperlink on the article. The server parses\nthe news content via the Python Newspaper3k library6, which uses advanced al-\n6https://newspaper.readthedocs.io/\nFig. 6. The user interface of BaitWatcher.\ngorithms with web scrapping to extract and parse online newspaper articles. The\nparsed content is fed into the pretrained deep learning model to return the in-\ncongruence score. The AHDE model with IP was selected as the model since this\nalgorithm realized the best performance in the evaluation experiment. Because\nthe Python Newspaper3k library automatically detects headline and body text,\nBaitWatcher can be run on any news website. The code and implementation\ndetails are publicly available on the GitHub page.7\n5.2 Focus group interview\nAfter implementing BaitWatcher, we evaluated its performance in a realistic set-\nting. We conducted a small-scale focus group interview to gain insight into how\nthe provision of additional information about headline incongruence can improve\nthe news reading experience. A total of fourteen participants of ages 20 to 29\nwere recruited from the second author's institute, all of whom identi\fed them-\nselves as moderate to avid news consumers. All participants said they actively\nread news articles at least once a week. After hearing a brief introduction to\nBaitWatcher's functionality, each focus group participant was given 30 minutes\nto read news articles through the BaitWatcher interface. While BaitWatcher can\nbe deployed on any news website as discussed earlier, we asked the focus group\nparticipants to visit a common news portal for \fnding news [4] to minimize the\ne\u000bects of distinct media outlets on the perception of headline incongruence. Af-\nter the 30-minute news reading experience in the lab, we conducted an open\ninterview with each participant. Institutional Review Board had approved this\nfocus group survey and the news assistant experimental design at the second\nauthors institute (Approval code: #KH2018-62).\nThe open interview included the following questions, which capture the news\nreading habits of users and quantify the e\u000bectiveness of BaitWatcher:\nQ1. How often do you read news online in a week?\n7https://github.com/bywords/BaitWatcher\nParticipant Gender Age Q1 (Freq) Q2 (Interests) Q3 (Full story) Q4 (BaitWatcher)\nP1 Male 24 7 days Politics Less likely No\nP2 Male 23 7 days Politics Less likely No\nP3 Male 21 3-4 days Politics Less likely Yes\nP4 Female 22 2 days Entertainment Less likely Yes\nP5 Female 20 7 days Politics More likely Yes\nP6 Female 22 7 days Social issues More likely Yes\nP7 Male 22 5 days Social issues Less likely Yes\nP8 Male 20 4 days Sports Less likely Yes\nP9 Male 26 3-4 days Life & Culture Less likely Yes\nP10 Male 26 7 days Economics More likely Yes\nP11 Female 24 7 days Entertainment Less likely No\nP12 Female 24 2 days IT & Science More likely No\nP13 Female 24 7 days Sports Less likely Yes\nP14 Male 28 7 days Politics Less likely Yes\nTable 3. Participants' information and questionnaire results\nQ2. Which category of news are you mostly interested in?\nQ3. When you are reading news online, how likely are you to read the full\nstory?\nQ4. Does showing incongruence scores a\u000bect the choice of news articles to\nread?\nTable 3 displays the necessary information about the participants and the\nquestionnaire results. Here, we make observations on their reading behaviors\nand the e\u000bects of BaitWatcher in preventing readers from clicking on incongru-\nent headlines. First, as previous studies noted [15,18], a signi\fcant degree of\nparticipants (78.5%) reported that they are more likely to consume headlines\nwithout reading the full news stories. This skimming behavior may enable them\nto browse a more extensive set of news stories every day; however, it makes them\nvulnerable to misleading headlines such as clickbait and incongruent headlines.\nThis result supports the necessity of showing the incongruence score before the\nuser clicks on a headline.\nIn response to the question on the e\u000bects of BaitWatcher (Q4), ten out of four-\nteen participants (71.4%) reported that the use of this interface a\u000bected their\nchoices of news articles to read, whereas four participants (28.6%) responded\nthat they were not in\ruenced by or did not bene\ft from this web interface.\nParticularly, P12, whose interest is in reading `IT & Science' news, responded\n`No' to this question because the participant did not encounter any news stories\nfor which the incongruence score was alarmingly high within the set of news\nstories that were browsed. Therefore, the user could not experience the bene-\n\fts of BaitWatcher. The frequency of incongruent headlines is typically low in\npractice and can vary across topics. Nonetheless, according to a more signi\fcant\nproportion of the participants, having this additional information seems useful\nand empowering.\nThose who answered `Yes' to Q4 reported that BaitWatcher was \\interest-\ning\" and \\e\u000bective\" in that they avoided clicking on news headlines with high\nincongruence scores, as we had hypothesized. Three participants (P4, P8, and\nP14) mentioned that they were attracted to such incongruent headlines because\nthey wanted to inspect the articles that BaitWatcher reported to be incongruent\nto their headlines.\n(P4) \\ ... At \frst, I became curious about why certain headlines were\nlabeled as incongruent by BaitWatcher, so I clicked on them and checked\nhow the articles looked ... \"\nThe unexpected browsing behaviors support the \fndings of previous studies\non the adverse e\u000bects of labeling on the prevention of fake news [19,13]. From the\nopinions of two participants (P8 and P9), we identify new potential to mitigate\nthe unnecessary attention that high incongruence scores receive. A possible strat-\negy is to pursue the interpretability of the deep learning models and to present\nthe results as grounds for the high score and how the algorithm works. When\nan algorithm looks like a black box, users will naturally question its prediction\nresults. Another strategy is to present ample examples of news articles with high\nincongruence scores in advance of the experiment to facilitate understanding of\nthe participants regarding the general performance of BaitWatcher.\n(P8) \\ ... When BaitWatcher displayed a high score, it made me wonder,\n\\why does this headline have such a high score?\" This led me to click\nthe headline and guess the reasoning that the AI used in making the\ndecision... \"\n(P9) \\ ... I did not click incongruent headlines because BaitWatcher warned\nme not to do so. Nonetheless, whenever it (BaitWatcher) showed high\nscores, I was curious why the AI made such a decision. It may be ef-\nfective for people to see the internal reasoning process of this AI model\n...\"\nOverall, our focus group study demonstrated that the provision of the incon-\ngruence score in today's news reading is empowering to users. Web interfaces\nsuch as BaitWatcher will not only prevent newsreaders from clicking on news\nheadlines that are likely incongruent to their full linked stories but also gradu-\nally build people's trust over time in the model's predictions. Whether one is an\navid news reader or not, spending time on incongruent stories is an unpleasant\nexperience in most cases. A headline might be deliberately misleading due to\nsarcasm, in which case readers could still click on the news article and enjoy\nreading it even if BaitWatcher's reported score is high. The deep models that\nare proposed in this work do not yet provide high interpretability, and detection\nmodels that are also interpretable could be developed in future studies.\n6 Discussion\nThe role of headlines in the news reading experience has been studied in journal-\nism and communication research. News headlines should provide a concise and\naccurate summary of the main story, thereby enabling readers to decide whether\nto read the news story [38]. Online social media and the web have become con-\nvenient platforms for news consumption. According to Digital News Reports by\nReuters Institute [1], a third fourth of the survey participants replied that they\nconsume news through online media. In contrast to news consumption via tra-\nditional outlets such as newspapers, the main content is not shown to readers in\nonline media; only headlines and visual snippets are exposed. Hence, newsread-\ners are more likely to consume only the news headlines and not the full news\nstories|a behavior that some refer to as a shopper of headlines [15]. In such en-\nvironments, if a news headline does not accurately represent the story, it could\nmislead readers into disseminating false information [6,41], which could lead to\npressing social problems. Even though the proportion of incongruent headlines is\nnot large against the numerous news articles that are published each day, an inac-\ncurate impression can percolate through a user's online networks and eventually\nlead to severe social problems such as polarization, as a previous study similarly\ndiscovered in the context of fake news consumption on Twitter [22]. This study\nidenti\fed the dangers and problems that are associated with headline-led news\nreading, and its contributions are three-fold.\nFirst, we release a dataset of 1.7 million news articles that are constructed\non the entire articles published in a nation over two years. Due to the sparsity\nof incongruent headlines in the wild, it requires a considerable amount of time\nand e\u000bort to develop a sizable and balanced dataset via manual annotation.\nTherefore, previous studies introduced small datasets that are not suitable for\ntraining sophisticated models. To address the issue of scalability in the construc-\ntion of a dataset, we automatically generate incongruent headlines by implanting\nparagraphs of other articles into the body text. This generation process can be\napplied to any set of news articles in any language, which will facilitate future\nstudies on the application of data-driven approaches to incongruent headlines.\nSecond, this study proposes an attention-based hierarchical neural network\nfor the headline incongruence problem. While recurrent neural networks are ef-\n\fcient in modeling sequential information such as text, a body text hinders the\npropagation of error signals via backpropagation. Thus, inspired by the hier-\narchical structure of a news article that is composed of paragraphs, we design\na hierarchical recurrent network that models word sequences of each paragraph\ninto a hidden state and combines the sequence of the paragraphs through another\nlevel of the recurrent neural network. This newly proposed model outperformed\nbaseline approaches with an AUROC of 0.977 on the detection task.\nThird, we implement a lightweight web interface that facilitates the selec-\ntion by readers of relevant articles to read in a typical scenario of online news\nconsumption in which only headlines are shown. The results of a focus group\ninterview demonstrate the e\u000bectiveness of the interface in preventing users from\nselecting those articles and suggest a future direction for the improvement of\ndeep learning models. Similar to the \fndings of a recent work [23], the partici-\npants require a high level of interpretability on model predictions, which is not\nembedded in the proposed models. Following the recent e\u000borts on deep learn-\ning [36], the development of an interpretable model will help build a high level of\ntrust in machine-based decisions on incongruent headlines, which will be crucial\nfor the utilization of such interfaces in practice.\n6.1 Hierarchical encoders for stance detection\nTo further evaluate the generalizability of the deep approaches that are proposed\nin this paper, we conducted an additional experiment on the FNC-1 dataset [3],\nwith the objective of stance detection . This problem is similar to the headline\nincongruence problem in that one must compare the textual relationships be-\ntween news headlines and the corresponding full content but di\u000berent in that\nits target label consists of four separate cases (unrelated, agree, disagree, and\ndiscuss). To obtain a similar setting to that of our task, we transformed these\nfour labels into binary labels: \\unrelated\" and \\others\".\nWe compared our hierarchical deep learning approaches (AHDE, HRDE, and\nHRE) with feature-based methods and standard deep learning models. We also\nconsidered ensemble models that combine the predictions of XGB and each deep\nlearning model, because an ensemble of XGB and CDE was the winning model\nof the FNC-1 challenge [40]. XGB outperformed the other single models with\nan accuracy of 0.9279. Among deep learning models, the AHDE model realized\nthe highest accuracy of 0.8444. The superior performance of XGB over deep\napproaches might result from insu\u000ecient variations among the training instances\nin the FNC-1 dataset. Even though the training set contains approximately\n50 K examples, many news articles that correspond to the independent label\nwere generated from 1,683 original news articles by swapping headlines with one\nanother; thus, 29.7 cases had identical body text.\nThese reasons might have led the challenge winners to use ensemble mod-\nels that combine the predictions of feature-based approaches and deep neural\nnetworks. The XGB+CDE ensemble model realized the accuracy of 0.9304 and\noutperformed all the single models. When we combined the predictions of AHDE\nwith XGB, the ensemble model produced the best accuracy of 0.9433. Incorpo-\nrating the results in Table 2, this \fnding suggests that the proposed hierarchical\nneural networks e\u000bectively learn textual relationships between two texts in con-\ntrast to standard approaches. We \frmly believe that the highest accuracy of\nXGB among the single models is due to the limitation of the FNC-1 dataset,\nas discussed earlier; hence, the ensemble approach may not be necessary if the\ndataset is su\u000eciently large for neural network training. According to additional\nexperiments on the dataset that was proposed in this paper, the AHDE model\noutperformed all combinations of other approaches for the ensemble.\n6.2 Varying perceptions on headline incongruence\nSo far, we have treated the incongruent score as an inherent value that is \fxed\nfor each news article. We conducted additional surveys using the Amazon Me-\nchanical Turk (MTurk) service to determine whether the general public would\nagree with the predictions by our models regarding which news articles contain\nincongruent headlines. We also evaluated whether people's perceptions of incon-\ngruence scores vary according to their partisanship, and we hypothesize based\non a previous \fnding that people's knowledge of the veracity of news varies by\npolitical stance [5].\nFirst, we manually gathered news articles from two media outlets. To retrieve\nas many incongruent news headlines as possible, we selected two media outlets\nthat are considered not trustworthy by common journalistic standards (referring\nto mediabiasfactcheck.com): one was chosen from the conservative media ( Media\nA) and another from the liberal media (we call Media B ). We do not reveal these\nmedia names, as the choices of media outlets are less of a concern in our study.\nGiven the de\fnition of an incongruent headline and the incongruent articles\nthat are selected by the model from each media outlet, we asked 100 Amazon\nMechanical Turk workers to answer the following question \\Do you think the\nheadline of the above article is incongruent with its body text?\"\n0255075\nConservative Liberal Middle-of-the-roadPercenta ge\n0255075\nConservative Liberal Middle-of-the-road(a) Media A (Conservative) (b) Media B (Liberal)\nCongruent\nIncongruent\nFig. 7. MTurk results indicating political stances of survey participants (the x-axis)\nand their responses to articles of high incongruence score (the y-axis).\nAccording to Figure 7, MTurk workers tend to \fnd that articles with high\nincongruence scores contain misleading headlines. One exciting trend is the de-\npendence of the perceived incongruence score on individual belief. While non-\nliberal participants considered news samples from Media B to have a similar\nlevel of incongruence to samples from Media A , liberal participants found Me-\ndia B to be less incongruent. This \fnding suggests that while our approach is\napplicable in general scenarios, the perceived incongruence level may be judged\ndi\u000berently among news topics (such as politics). News service providers and re-\nsearchers should be cautious when employing human coders and crowdsourcing\nworkforces to obtain fair labels on misinformation and fake news.\n6.3 Future directions\nA natural extension of this study is the development and improvement of pre-\ndiction models for detecting news articles with incongruent headlines by incor-\nporating NLP techniques with deep learning approaches. For example, one could\napply named entity recognition as a preprocessing step to represent word tokens\nin an embedding space better. It would also be possible to consider syntactic\nfeatures in modeling text by developing tree-shaped deep neural architectures\nthat are similar to LSTM-tree [39].\nAnother future direction is to devise a learning-based approach for gener-\nating headlines. To construct a million-scale dataset for training incongruity\ndetection models, we modi\fed the body text while keeping the original news\nheadline unchanged. While the process has shown to generate a training corpus\ne\u000bectively, researchers could develop an AI agent that rewrites a headline that is\nincongruent with its body text. While the research on text generation has lagged\nbehind the remarkable achievements in image domains due to the di\u000eculty of\nhandling discrete outputs, future research could be extended from recent studies\non controlled generation [24] or cross-alignment style transfer [37].\nBeyond the online news domain, this work could lead to new measurements of\nthe incongruence of title and content across other types of online content. The ti-\ntle plays a crucial role in enticing users to click and consume digital content such\nas blog articles, online videos, and even scienti\fc papers. Similar to the incon-\ngruent headline problem, the automatic identi\fcation of such incongruent titles\nof various content will improve people's online experiences. Future researchers\ncould share multiple types of datasets and could develop AI approaches that\nmeasure the inconsistency between title and content.\n7 Conclusions\nHere, we study the detection of incongruent headlines that make claims that\nare unsupported by the corresponding body texts. We release a million-scale\ndata corpus that is suitable for the detection of the misleading headline. We\nalso propose deep neural networks that e\u000eciently learn the textual relationship\nbetween headline and body text via a recurrent hierarchical architecture. To\nfurther facilitate news reading in practice, we present BaitWatcher, which is a\nlightweight web interface that presents to readers the prediction results that are\nbased on deep learning models before the readers click on news headlines. The\ncode and implementation details are released for broader use, and we hope this\nstudy contributes to the construction of a more trustworthy online environment\nfor reading news.\nAcknowledgements\nThis research was supported by Basic Science Research Program through the\nNational Research Foundation of Korea (NRF) funded by the Ministry of Science\nand ICT (No. NRF-2017R1E1A1A01076400).\nReferences\n1. Reuters Institute Digital Report. http://www.digitalnewsreport.org/survey/\n2016/ (2016), [Online; accessed 15-April-2019]\n2. Clickbait Challenge. http://www.clickbait-challenge.org (2017), [Online; ac-\ncessed 15-April-2019]\n3. Fake News Challenge. http://www.fakenewschallenge.org/ (2017), [Online; ac-\ncessed 15-April-2019]\n4. Naver News. http://news.naver.com (2019), [Online; accessed 15-April-2019]\n5. Allcott, H., Gentzkow, M.: Social media and fake news in the 2016 election. Journal\nof Economic Perspectives 31(2), 211{36 (2017)\n6. Allport, F.H., Lepkin, M.: Building war morale with news-headlines. Public Opin-\nion Quarterly 7(2), 211{221 (1943)\n7. Blom, J.N., Hansen, K.R.: Click bait: Forward-reference as lure in online news\nheadlines. Journal of Pragmatics 76, 87{100 (2015)\n8. Chakraborty, A., Paranjape, B., Kakarla, S., Ganguly, N.: Stop clickbait: Detecting\nand preventing clickbaits in online news media. In: Proceedings of the ASONAM\n(2016)\n9. Chen, T., Guestrin, C.: Xgboost: A scalable tree boosting system. In: Proceedings\nof the KDD (2016)\n10. Chen, Y., Conroy, N.J., Rubin, V.L.: Misleading online content: Recognizing click-\nbait as false news. In: Proceedings of the ACM Workshop on Multimodal Deception\nDetection (2015)\n11. Chesney, S., Liakata, M., Poesio, M., Purver, M.: Incongruent Headlines: Yet An-\nother Way to Mislead Your Readers. In: Proceedings of the EMNLP Workshop.\npp. 56{61 (2017)\n12. Chopra, S., Jain, S., Sholar, J.M.: Towards Automatic Identi\fcation of Fake News:\nHeadline-Article Stance Detection with LSTM Attention Models (2017)\n13. Clayton, K., Blair, S., Busam, J.A., Forstner, S., Glance, J., Green, G., Kawata,\nA., Kovvuri, A., Martin, J., Morgan, E.: Real Solutions for Fake News? Measuring\nthe E\u000bectiveness of General Warnings and Fact-Check Tags in Reducing Belief in\nFalse Stories on Social Media. Political Behavior pp. 1{23 (2019)\n14. Ecker, U.K., Lewandowsky, S., Chang, E.P., Pillai, R.: The e\u000bects of subtle mis-\ninformation in news headlines. Journal of experimental psychology: applied 20(4),\n323 (2014)\n15. English, E.: A study of the readability of four newspaper headline types. Journalism\nBulletin 21(3), 217{229 (1944)\n16. Ferreira, W., Vlachos, A.: Emergent: a novel data-set for stance classi\fcation. In:\nProceedings of the NAACL-HLT (2016)\n17. Ferrucci, D.A.: Introduction to this is watson. IBM Journal of Research and De-\nvelopment 56(3.4), 1{1 (2012)\n18. Gabielkov, M., Ramachandran, A., Chaintreau, A., Legout, A.: Social clicks: What\nand who gets read on Twitter? ACM SIGMETRICS Performance Evaluation Re-\nview44(1), 179{192 (2016)\n19. Gao, M., Xiao, Z., Karahalios, K., Fu, W.T.: To Label or Not to Label: The E\u000bect\nof Stance and Credibility Labels on Readers' Selection and Perception of News\nArticles. Proceedings of the ACM on Human-Computer Interaction 2(CSCW), 55\n(2018)\n20. Gillani, N., Yuan, A., Saveski, M., Vosoughi, S., Roy, D.: Me, my echo chamber,\nand i: introspection on social media polarization. In: Proceedings of the WWW.\npp. 823{831 (2018)\n21. Go, A., Bhayani, R., Huang, L.: Twitter sentiment classi\fcation using distant\nsupervision (2009)\n22. Grinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B., Lazer, D.: Fake news\non Twitter during the 2016 US presidential election. Science 363(6425), 374{378\n(2019)\n23. Horne, B.D., Nevo, D., O'Donovan, J., Cho, J.H., Adali, S.: Rating Reliability\nand Bias in News Articles: Does AI Assistance Help Everyone? arXiv preprint\narXiv:1904.01531 (2019)\n24. Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., Xing, E.P.: Toward controlled\ngeneration of text. In: Proceedings of the ICML. pp. 1587{1596 (2017)\n25. Kim, Y.: Convolutional neural networks for sentence classi\fcation. In: Proceedings\nof the EMNLP. pp. 1746{1751 (2014)\n26. Kuiken, J., Schuth, A., Spitters, M., Marx, M.: E\u000bective headlines of newspaper\narticles in a digital environment. Digital Journalism 5(10), 1300{1314 (2017)\n27. Kwon, S., Cha, M., Jung, K., Chen, W., Wang, Y.: Prominent features of rumor\npropagation in online social media. In: Proceedings of the ICDM (2013)\n28. Lovering, C., Lu, A., Nguyen, C., Nguyen, H., Hurley, D., Agu, E.: Fact or Fiction.\nProceedings of the ACM on Human-Computer Interaction 2(CSCW), 111 (2018)\n29. Lowe, R., Pow, N., Serban, I.V., Pineau, J.: The ubuntu dialogue corpus: A large\ndataset for research in unstructured multi-turn dialogue systems. In: Proceedings\nof the SIGDIAL (2015)\n30. Munson, S.A., Lee, S.Y., Resnick, P.: Encouraging reading of diverse political view-\npoints with a browser widget. In: Proceedings of the ICWSM (2013)\n31. Park, S., Kang, S., Chung, S., Song, J.: Newscube: delivering multiple aspects of\nnews to mitigate media bias. In: Proceedings of the CHI. pp. 443{452. ACM (2009)\n32. Pascanu, R., Mikolov, T., Bengio, Y.: On the di\u000eculty of training recurrent neural\nnetworks. In: Proceedings of the ICML (2013)\n33. Reis, J., Benevenuto, F., de Melo, P.V., Prates, R., Kwak, H., An, J.: Breaking\nthe news: First impressions matter on online news. In: Proceedings of the ICWSM\n(2015)\n34. Riedel, B., Augenstein, I., Spithourakis, G., Riedel, S.: A simple but tough-to-beat\nbaseline for the fake news challenge stance detection task. corr abs/1707.03264\n(2017)\n35. Rony, M.M.U., Hassan, N., Yousuf, M.: Diving deep into clickbaits: Who use them\nto what extents in which topics with what e\u000bects? In: Proceedings of the ASONAM.\npp. 232{239. ACM (2017)\n36. Samek, W., Wiegand, T., M\u007f uller, K.R.: Explainable arti\fcial intelligence: Un-\nderstanding, visualizing and interpreting deep learning models. arXiv preprint\narXiv:1708.08296 (2017)\n37. Shen, T., Lei, T., Barzilay, R., Jaakkola, T.: Style transfer from non-parallel text\nby cross-alignment. In: Proceedings of the NIPS. pp. 6830{6841 (2017)\n38. Smith, E.J., Fowler Jr, G.L.: How comprehensible are newspaper headlines? (1982)\n39. Tai, K.S., Socher, R., Manning, C.D.: Improved Semantic Representations From\nTree-Structured Long Short-Term Memory Networks. In: Proceedings of the ACL.\nvol. 1, pp. 1556{1566 (2015)\n40. Talos, C.: Fake News Challenge - Team SOLAT IN THE SWEN.\nhttps://github.com/Cisco-Talos/fnc-1 (2017), [Online; accessed 15-April-2019]\n41. Tannenbaum, P.H.: The e\u000bect of headlines on the interpretation of news stories.\nJournalism Bulletin 30(2), 189{197 (1953)\n42. Wang, Z., Hamza, W., Florian, R.: Bilateral multi-perspective matching for natural\nlanguage sentences. In: Proceedings of the ICJAI. pp. 4144{4150. AAAI Press\n(2017)\n43. Wei, W., Wan, X.: Learning to identify ambiguous and misleading news headlines.\nIn: Proceedings of the IJCAI. pp. 4172{4178. AAAI Press (2017)\n44. Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., Hovy, E.: Hierarchical attention\nnetworks for document classi\fcation. In: Proceedings of the NAACL. pp. 1480{\n1489 (2016)\n45. Yoon, S., Shin, J., Jung, K.: Learning to rank question-answer pairs using hi-\nerarchical recurrent encoder with latent topic clustering. In: Proceedings of the\nNAACL-HLT. vol. 1, pp. 1575{1584 (2018)\n46. Zhou, P., Shi, W., Tian, J., Qi, Z., Li, B., Hao, H., Xu, B.: Attention-based bidirec-\ntional long short-term memory networks for relation classi\fcation. In: Proceedings\nof the ACL. pp. 207{212 (2016)", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "BaitWatcher: A lightweight web interface for the detection of incongruent news headlines", "author": ["K Park", "T Kim", "S Yoon", "M Cha", "K Jung"], "pub_year": "2020", "venue": "\u2026 , and Fake News in Social Media \u2026", "abstract": "In digital environments where substantial amounts of information are shared online, news  headlines play essential roles in the selection and diffusion of news articles. Some news"}, "filled": false, "gsrank": 371, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-42699-6_12", "author_id": ["xiZ1ImoAAAAJ", "xcfi6UAAAAAJ", "UpymOMwAAAAJ", "iFlnVCoAAAAJ", "u3uMl4MAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:h5K3cY2kxyAJ:scholar.google.com/&output=cite&scirp=370&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D370%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=h5K3cY2kxyAJ&ei=SLWsaJjrIPnSieoPxKLpgQ0&json=", "num_citations": 19, "citedby_url": "/scholar?cites=2362037456984314503&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:h5K3cY2kxyAJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2003.11459"}}, {"title": "A bert-based explainable system for covid-19 misinformation identification", "year": "2023", "pdf_data": "A BERT-based Explainable System for COVID-19 Misinformation Identification\nLwin Moe, Arghya Kundu, and Uyen Trang Nguyen\nDepartment of Electrical Engineering & Computer Science\nYork University, Toronto, Canada\nlwinmoe@yorku.ca, arghyak@yorku.ca, utn@eecs.yorku.ca\nAbstract\nMisinformation related to COVID-19 can have serious con-\nsequences, such as undermining public health efforts to com-\nbat the pandemic. To address this problem, many COVID-19\nmisinformation detection models have been proposed. Most\nof them focused on classification accuracy, but none provides\njustifications or explanations for their classification output.\nIn this paper, we present an explainable COVID-19 misin-\nformation detection system that classifies whether a claim (a\nsentence) related to COVID-19 is true, false, or partially true,\nusing an existing BERT-based classification model. The sys-\ntem then provides rationales behind the predictions using the\nLIME XAI tool, which allow machine learning practitioners\nto understand in depth how the model works to debug, fine\ntune, and optimize the model. Furthermore, the system pro-\nvides explanations for the prediction using relevant sentences\nextracted from news articles (which serve as the ground truth\nin the classification process). The sources of the news arti-\ncles are listed along with a ranking of the credibility of the\npublisher of each article (e.g., high or medium). These pieces\nof information explain to end users how a classification deci-\nsion is reached, what data sources were used to arrive at the\ndecision and whether the data sources are trustworthy. Such\ninformation and explanations will instill trust in the end users\nof the system. To the best of our knowledge, our system is\nthe first explainable fact checking system designed to combat\nCOVID-19 misinformation. We present examples of explain-\nability output provided by our system to demonstrate the ef-\nfectiveness of the above explainability features. The proposed\nexplainability framework can be readily applied to misinfor-\nmation identification models in other domains, e.g., politics,\nfinance, sports, and entertainment.\nIntroduction\nFalse information (fake news) is a persistent problem on so-\ncial media and the Internet. We can broadly divide fake news\ninto three categories based on intent: 1) misinformation, 2)\ndisinformation and 3) malinformation (Verrall 2022). Mis-\ninformation and disinformation are both false information,\nbut the former is spread unknowingly and the latter, inten-\ntionally to cause harm. Malinformation is in fact true, but\nspread to cause harm on a person or organization. For the\nsake of simplicity, we will refer to all categories of fake news\nCopyright \u00a9 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.as misinformation in this paper. Furthermore, we consider\nCOVID-19 misinformation in various contexts, for example,\nmedical, social, cultural, political and public health context.\nMisinformation related to COVID-19 can have serious\nconsequences, such as undermining public health efforts\nto combat the pandemic. To address this problem, many\nCOVID-19 misinformation detection models have been pro-\nposed (Sanaullah et al. 2022). Most of them focus on clas-\nsification aaccuracy, but none provides reasons or explana-\ntions for their classification decisions.\nJustifications and explanations are crucial in a misinfor-\nmation detection system. End users will want to know how a\nclassification decision was reached, what data sources were\nused to arrive at the decision, and whether the data sources\nare trustworthy. Such information and explanations will in-\nstill trust in the end users of the system. Furthermore they\nallow machine learning practitioners to understand in depth\nhow their model works to debug, fine tune, optimize and ex-\ntend the model.\nIn this paper, we present a COVID-19 misinformation de-\ntection system that\n\u2022 classifies whether a claim (a sentence) related to COVID-\n19 is true, false, or partially true, using a BERT-based\nclassification model proposed by Ou (2021);\n\u2022 provide explanations for the classification using relevant\nsentences extracted from news articles. The sources of\nthe news articles are listed along with a ranking of the\ncredibility of the publisher of each article (e.g., high or\nmedium);\n\u2022 provide rationales behind the predictions using the LIME\nXAI tool (Ribeiro, Singh, and Guestrin 2016).\nTo the best of our knowledge, our system is the first ex-\nplainable fact checking system designed to combat COVID-\n19 misinformation.\nThe proposed fact checking system consists of the follow-\ning three components:\n\u2022 Component 1: Given a claim (sentence), a BERT-based\nmodel (Ou 2021) classifies the claim as true, partly true\nor false using a set of news articles whose contents are\nrelated to the claim (see Table 1 for a sample entry from\nthe dataset). The set of related articles, collected from\nreputable sources, serves as the ground truth to assess\nthe validity of the claim. In other words, given a claim\nID 3739\nClaim COVID-19 vaccine trial killed 7 children in Senegal.\nClaimant Social Media\nRelated Articles https://factcheck.afp.com/senegalese-children-did-not-die-coronavirus-vaccine-which-does-\nnot-yet-exist\nhttps://factcheck.afp.com/busting-coronavirus-myths\nhttps://web.archive.org/web/20200410170614/https://www.weblyf.com/2020/04/covid-19-\nvaccines-killed-seven-children-in-senegal-africa/\n...\nLabel False\nDate 2020-04-09\nTable 1: Sample Data from the COVMIS Dataset (Ou 2021).\ncand related articles, the goal is to predict its label\ny\u2208 {true,false,partially true }using a BERT-based clas-\nsification function f:C \u2192 Y , where Cis the space of all\npossible claims and Yis the space of possible labels.\n\u2022 Component 2: Given a claim, this module extracts sen-\ntences from the related articles that are the most relevant\n(similar) to the claim using TF-IDF, transformer-based\nembeddings and cosine similarities. These extracted sen-\ntences are also used by the BERT-based model to classify\nthe claim. This module also includes a news source cred-\nibility checker that provides a credibility ranking (high or\nmedium) to a news outlet. For each claim that is classi-\nfied, the extracted sentences are displayed along with the\nURLs of the related articles, and the credibility ranking\nof the publishers of the articles. The sentences displayed\nexplains to the end user the rationale behind the classi-\nfication output. The user can follow the links to read the\nrelated articles in their entirety for more information. The\ncredibility ranking allows the user to make informed de-\ncisions based on the trustworthiness of the sources.\n\u2022 Component 3: We incorporated the LIME XAI frame-\nwork into the BERT-based classifier to provide explana-\ntions for the predictions. The LIME framework provides\nlocal interpretations by generating a set of weighted fea-\ntures that contribute the most to a prediction. The out-\nput from LIME allows machine learning researchers to\nunderstand the reasons behind the predictions. This in-\nformation will assist them in debugging, fine tuning, and\noptimizing the classifier.\nThe contributions of this paper are Components 2 and 3,\nand the integration of the BERT-based classifier (Ou 2021)\nto form a complete explainable fact checking system. The\ncontributions of this study can be applied to other domains\nbeyond COVID-19 misinformation detection (e.g., election\nmisinformation), where explainability and transparency are\ncrucial to building and maintaining trust in automated deci-\nsion making systems.\nThe remainder of the paper is organized as follows. In\nsection \u2018Related Work\u2019, we discuss existing work on mis-\ninformation identification, including those with XAI meth-\nods, and COVID-19 datasets and models for misinformation\nidentification. Section \u2018Methodology\u2019 provides a summary\nof the BERT-based classifier (Ou 2021), and detailed de-scriptions of Components 2 and 3. In section \u2018Results and\nDiscussion\u2019, we present examples of explainability output\nprovided by our system. We summarize the paper and out-\nline future work in section \u2018Conclusion and Future Work\u2019.\nRelated Work\nMisinformation detection has been an active area of research\nin recent years due to the growing problem of false informa-\ntion spreading through social media and online platforms.\nIn this section, we review the related work on detecting and\nexplaining misinformation.\nMisinformation detection methods are generally divided\ninto three categories: 1) content-based, 2) feedback-based,\nand 3) intervention-based (Sharma et al. 2019). Content-\nbased methods use text and linguistic features as the pri-\nmary input for detection models (Fuller, Biros, and Wilson\n2009; Ott et al. 2011; Feng, Banerjee, and Choi 2012). Other\ntypes of input such as videos and images are also used as\nvisual-based features (Gupta et al. 2013; Jin et al. 2017).\nIn feedback-based methods, information such as propaga-\ntion patterns, temporal patterns, response texts and response\nusers are used (Ma, Gao, and Wong 2017). In intervention-\nbased methods, network monitoring, crowd-sourcing and\nuser behaviour modeling are used (Amoruso et al. 2017;\nKim et al. 2018).\nTransformer-based models were also used to encode con-\ntent for a classification model to identify misinformation\n(Ou 2021). BERT (Devlin et al. 2019), which stands for\nBidirectional Encoder Representations from Transformers,\nis one of the transformer-based language models. There\nare many pre-trained BERT models using unlabeled texts.\nFor example Med-BERT is specifically trained on electronic\nhealth records (Rasmy et al. 2020). As a result, it may not\nbe effective for detecting misinformation appearing in the\nnews or social media in social, cultural or political context.\nPre-trained BERT models can be fine-tuned with an addi-\ntional output layer for many downstream NLP tasks includ-\ning misinformation detection.\nCurrently most pre-trained BERT models limit the token\nlength to a maximum of 512 tokens. There are models such\nas LongFormer (Beltagy, Peters, and Cohan 2020) that al-\nlows token lengths of more than 512, but they require a large\namount of GPU memory.\nDue to this limitation, text summarization is used to trim\nthe input to more relevant content to feed into the BERT-\nbased classifier. Text summarization is used as a feature in\nthe misinformation detection task (Esmaeilzadeh, Peh, and\nXu 2019; Ou 2021). In addition, Talarico and Viviani (2022)\ndiscussed extractive summarization in which important sen-\ntences are extracted, and their credibility are also assessed at\nthe same time to generate a credible summary.\nMany studies have incorporated XAI into the classifiers.\nFujita et al. (2022) proposed a machine learning model,\nwhich uses the output of XAI, to identify misclassified ma-\nlicious activities in cyberspace. XFake (Yang et al. 2019) is\nan explainable fake news detector, which assists a user with\nnews attribute analysis, and linguistic analysis such as noun,\nverb and adjective ratios. Their system assists the user with\nword frequency analysis and part-of-speech ratios.\nDuring the COVID pandemic, various corpora and tech-\nniques for the detection of COVID-19 misinformation have\nbeen proposed. TweetsCOV19 (Dimitrov et al. 2020) is a\ncorpus of tweets related to COVID-19, annotated with enti-\nties, hashtags, user mentions, sentiments, and URLs. CoAID\n(Cui and Lee 2020) is a dataset of COVID-19 healthcare\nmisinformation, with which various machine learning meth-\nods were tested. Wani et al. (2021) use deep learning ap-\nproaches such as CNN, LSTM, and BERT to classify the\ntweets from the COVID-19 Fake News Dataset (Patwa et al.\n2020). COVIDLIES is a collection of tweets related to\nCOVID-19. Existing NLP systems were evaluated, using\nCOVIDLIES dataset, to retrieve the misconception and clas-\nsified whether the tweet agrees or disagrees with the miscon-\nception (Hossain et al. 2020).\nMost of the existing COVID-19 misinformation detection\nsystems (Sanaullah et al. 2022) focus on the classification\ntasks. They do not have an XAI component to explain the\nrationales behind the classification decisions. Our proposed\nsystem provides such explanations, along with the input to\nthe model (i.e., sentences extracted from related articles) and\nthe credibility ranking of the news sources from which the\nrelated articles were collected. We will discuss our method-\nology next in the following section.\nMethodology\nWe use the BERT-based model proposed by Ou (2021) to\nclassify a COVID-related claim as true, false or partially\ntrue. We explain the model decision and output using the\nLIME XAI tool and our own user interface developed to help\nend users understand the rationale behind the classification.\nFigure 1 shows the architecture of our proposed system. Our\nsystem has three important components:\n1. Classifying if the claim is true, false, or partially true us-\ning the BERT-based classifier proposed by Ou (2021).\n2. Explaining the features (summary sentences) for the user\nto make an informed decision about the result from the\nmodel.\n3. Explaining the model with LIME.\nThe first module, the BERT-based classifier is a method\nreported in Ou (2021). We first use TF-IDF based query-\nfocused summarization to extract sentences most similar toa claim we are going to classify. Sentences from the arti-\ncles related to a claim are extracted using TF-IDF similar-\nity scores to the claim. We extract five sentences as a sum-\nmary of the articles. In addition to TF-IDF, we also use\ntransformer-based embedding vectors and cosine similari-\nties to extract most relevant sentences for the summary. We\nthen use a pre-trained BERT model to encode the summary\nwith embedding vectors to train a classifier to classify the\nclaim.\nThe second module explains the extracted summary sen-\ntences by colour-coding them based on their source credi-\nbility scores. The News Source Credibility Checker deter-\nmines the credibility of the related articles, from where the\nsentences are extracted, and provides that information to the\nuser as part of the explanation. This helps the user better un-\nderstand the credibility of the information they are receiving\nand make an informed decision about trusting the model.\nThe third module explains the BERT classifier using\nLIME framework. We will discuss each component of our\nproposed system in details in the following sections.\nWe use the COVMIS dataset (Ou, Nguyen, and Ismail\n2022), which is freely available on GitHub1, for our exper-\niments. It is a dataset of COVID-19 related claims, which\nare labelled as true, partly true, and false. It consists of a\nset of news articles related to the claims. The articles serve\nas the ground truth to assess the validity of the claim. The\nclaims and related articles are collected from different fact-\nchecking websites such as PolitiFact, Snopes, Africa Check,\nPoynter and Google Fact Check Tool. The dataset has a total\nof 14,384 claims, 10,158 of which are false, 2192 partly true\nand 2034 true.\nClassification Model Using BERT Embeddings\nIn our experiments, we use a pre-trained BERT model, bert-\nbase-uncased , from HuggingFace (Devlin et al. 2019). It\nwas trained on the BookCorpus, a dataset of 11,038 unpub-\nlished books and English Wikipedia (Hugging Face 2023).\nFor the training parameters, we used the optimal learning\nrate of 4e\u22125reported in Ou (2021). We trained our models\nfor five epochs.\nThe BERT model allows a maximum token length of 512.\nAs a result, we need to extract the data most relevant to a\nclaim from the set of articles related to the claim. The fol-\nlowing section discusses the methods for extracting the most\nrelevant data from a set of related articles.\nExtractive Summarization\nSentences from the related articles of a claim are extracted\nbased on their similarity to the claim (i.e., similarity scores).\nFor each claim, we combined the articles related to the claim\ninto a single document. For each sentence in the document,\nwe calculated the similarity score of the sentence against the\nclaim using TF-IDF. We then selected five sentences having\nthe highest TF-IDF scores, which forms a summary of the\narticles. We repeated the same summarization process but\nused BERT embeddings and cosine similarity scores, and\ncompared this method with the TF-IDF method.\n1https://github.com/caryou/COVMIS\nFigure 1: In our proposed system, COVID-19 related claims and summaries of articles related to the claims are classified with\na BERT-based classifier. We used LIME and input sentences visualization to explain the model predictions.\nExp # Input Selections P R F1 Accu\nExp. 1 BERT+TF-IDF 0.78 0.81 0.78 0.81\nBERT+Transformer 0.78 0.80 0.78 0.80\nExp. 2 BERT+TF-IDF 0.81 0.82 0.82 0.82\nBERT+Transformer 0.82 0.83 0.82 0.83\nTable 2: Results for the COVID misinformation classifica-\ntion using different input selection methods: 1) BERT + TF-\nIDF input sentence selection, 2) BERT + transformer-based\ninput sentence selection. (P is precision, R is recall, and\nAccu is accuracy.)\nTo compare the TF-IDF and BERT-based sentence extrac-\ntion methods, we ran two experiments by dividing the COV-\nMIS dataset into two halves. The first half of the dataset\ncontains 5753, 719, and 720 claims for training, validation\nand testing, respectively. We then compared the performance\nbetween the TF-IDF and transformer-based input selection\nmethods. We repeated the experiment with the second half\nof the dataset. The results are in Table 2. The performance\nfor the TF-IDF based selection was better in the first exper-\niment, but the transformer-based selection was better in the\nsecond. We conclude that the TF-IDF selection method per-\nforms as well as and requires less computing resources than\nthe BERT-based method.\nExplanation Using Summary Sentences\nWe built a user interface that displays the summary sen-\ntences related to a claim to help an end user obtain informa-\ntion about how the prediction was make and what data was\nused in the classification. (The summary sentences are also\nthe input into the BERT-based classifier.) The goal is to help\nthe user make an informed decision about the output of the\nmodel for transparency, and help them understand the out-\nput. The explanation includes the summary sentences along\nwith the links to the related articles, from which the sen-\ntences were extracted. The sentences are also colour-coded\nbased on the credibility of the related articles, using the\nNews Source Credibility Checker discussed below, so userscan make their own judgments about the credibility of the\nsource materials that the model used to classify the claim.\nNews Source Credibility Checker\nChecking the credibility of a news source is crucial for de-\ntecting and mitigating the spread of fake news (Allcott and\nGentzkow 2017). By identifying reliable sources of informa-\ntion, we improve the accuracy and reliability of our classifi-\ncation system, and help prevent the dissemination of false or\nmisleading information (Pennycook and Rand 2021). Thus,\nwe develop a module that finds the credibility of a news site\nby crawling Media Bias Fact Check (MBFC) website2us-\ning the domain name of the related articles and using the\nmetrics provided by the MBFC website. The module per-\nforms a search query, on the MBFC website, for the news\nsite under consideration, and retrieves the relevant metrics,\nincluding bias rating, factual reporting, press freedom rating,\ntraffic/popularity, and MBFC credibility rating.\nThe MBFC metrics used in this module have been chosen\nfor their relevance and credibility in evaluating the credi-\nbility of a news site. The bias rating and factual reporting\nmetrics assess the site\u2019s political leaning and accuracy of re-\nporting. The press freedom rating evaluates the degree of\nfreedom of the press in the country where the news site op-\nerates. The traffic/popularity metric provides an indication\nof the site\u2019s influence and reach, while the MBFC credibility\nrating is a composite score that summarizes the site\u2019s overall\ncredibility.\nTo account for inconsistencies and noise in the MBFC\nwebsite, the system uses several additional methods. First,\nwe use regular expressions to filter and process the scores\nfrom the MBFC result page and make the values consis-\ntent. Second, we compare the URL from the related articles\nwith the source URL listed on the MBFC webpage to en-\nsure that we are retrieving the correct webpage. Third, if the\nscores are still not determined, it is likely that the search\nquery failed due to issues with the MBFC search module.\nIn such cases, we retrieve the webpage title from the news\n2https://www.mediabiasfactcheck.com\nsite, and use it to search the MBFC website to obtain the cor-\nrect MBFC result. Finally, we also classify credible websites\nthat may not have information on MBFC, such as govern-\nment websites, by checking the Top-Level Domain (TLD)\nsection of the website address against a manually curated\nlist of TLDs that are assured to be backed by credible edu-\ncational, government, or international organizations.\nIt should be noted that all the articles related to a claim\nare supposed to be collected from trustworthy websites, i.e.,\nthose with a ranking of \u2018high\u2019 or \u2018medium\u2019. If an error is\nmade in the data collection process that includes a web-\nsite with a \u2018low\u2019 ranking in the set of related articles, the\ndisplayed ranking will help mitigate the impact of the er-\nror. That is, the end user would discount or discard the low\nranked article based on the provided \u2018low\u2019 ranking. The ar-\nticle will then be removed from the dataset.\nIf a news source is not currently on our list of credibil-\nity ranking, it will be displayed as \u2018Unknown\u2019 (credibility).\nWe will then manually evaluate the source (using informa-\ntion on the Internet, if any, and our own judgment), assign a\ncredibility rank (high, medium or low) and update the list.\nExplanation Using LIME\nWe use LIME to explain the BERT-based classifier. LIME\nbuilds a surrogate model based on the original BERT-based\nmodel to explain the model\u2019s behaviour.\nIn our experiments, we use the model trained with a max-\nimum length of 48 tokens as an input. We tried the maxi-\nmum length of 512, but we were not able to load the LIME\nexplainer into memory because of the limitation in our com-\nputing environment. To solve the problem, we ran a series\nof experiments to compare the performance of the model\nwith different token lengths 48, 64, 128, 256 and 512. As\nshown in Table 3, the model that uses a token length of 48\nexhibits comparable performance to the model that uses a\ntoken length of 512. We settled with a token length of 48 for\nthe purpose of LIME explanation as the model performed as\nwell as the one with 512 token lengths.\nMax-lengths Precision Recall F1 Accuracy\n48 0.82 0.84 0.81 0.84\n64 0.83 0.82 0.83 0.82\n128 0.80 0.82 0.81 0.82\n256 0.82 0.83 0.82 0.83\n512 0.83 0.85 0.84 0.85\nTable 3: Classifying COVID-related claims using different\nmaximum token lengths for BERT. For our dataset, the to-\nken lengths do not impact the performance. Since the model\nperforms as well with the token length of 48 as 512, LIME\nframework will be used with the token length of 48 for effi-\nciency.\nIt should be noted that that although short and long token\nlengths yielded similar performance, the token length had\na significant influence on the model\u2019s training time. By de-\ncreasing the token length, the training time in our comput-\ning environment dropped from 20 minutes to two minutes\nper epoch, with only a slight decrease in performance. Thetoken length of 48 was the only model we were able to use\nwith the LIME explainer because of the memory limitation\nin our computing environment.\nResults and Discussion\nIn this section, we will discuss examples of the explainabil-\nity output from the proposed system, which targets both ma-\nchine learning practitioners and end users. For the former,\nwe discuss the LIME-explainer output of the model. For\nthe latter, we present summary sentences, which are colour-\ncoded based on the credibility ranking of the news sources\nfrom which the summary sentences are collected.\nIn our system to classify the claims from the COVMIS\ndataset, we display the claim, the prediction output, the\nLIME explanation and the input summary sentences along\nwith their source credibility. The LIME explanation high-\nlights the keywords that are important in the classification\noutput based on their surrogate models. The summary sen-\ntences are colour-coded based on their source credibility us-\ning the News Source Credibility Checker explained in the\n\u2018Methodology\u2019 section. We will discuss the explanation out-\nput in details in the following sections.\nExplanation Using LIME\nThe LIME framework provides an explanation of what key-\nwords are important for the prediction model. LIME builds a\nsurrogate model based on the original BERT-based model to\nexplain the model\u2019s behaviour. In Figure 2, we have a LIME-\nbased explanation for the classification of the claim: \u201cThe\n\u2018biological\u2019 lab in Wuhan where the COVID-19 virus was\ncreated was \u2018funded\u2019 by President Barak Hussein Obama in\n2015 to the tune of $3,800,000 American dollars\u201d. We can\nsee the highlighted keywords for the prediction. Keywords\nsuch as \u201cbiological\u201d, \u201cwuhan\u201d, \u201cpresident\u201d, \u201cbarak\u201d, \u201chus-\nsein\u201d plays important roles in the model prediction process.\nThe claim, in this case, is correctly predicted as \u201cFasle\u201d.\nHowever, in Figure 3, the model incorrectly predicts the\nclaim,\u201cA randomized, double-blind study by Henry Ford\nHealth System proved that hydroxychloroquine is effective\nagainst COVID-19\u201d, as Partly True. Looking at the high-\nlighted keywords from the LIME explainer shows that the\nkeywords used by the model are not very relevant to the\nclaim. The summary sentences also refute the claim. The\nusers can make an informed decision whether to trust the\nprediction of the model using the explanation output. The\nvisualization of the input summary sentences also helps ex-\nplain the prediction process as we will discuss next.\nExplanation Using The Summary Sentences\nOur system displays summary sentences for the users to be\nable to judge, on their own, whether the output of the clas-\nsifier is trustworthy and reliable. The summary sentences\nfrom the articles related to the claim are used by the model\nto predict the truthfulness of the claim. In Figure 4, the\nclaim, \u201cU.S. coronavirus response \u2018slowly introducing\u2019 mar-\ntial law\u201d, is correctly labelled by the model as False. We can\nsee, in the figure, that the summary sentences from credi-\nble sources support the model\u2019s prediction. In Figure 5, on\nFigure 2: The claim, \u201cThe \u2018biological\u2019 lab in Wuhan where the COVID-19 virus was created was \u2018funded\u2019 by President Barak\nHussein Obama in 2015 to the tune of $3,800,000 American dollars\u201d, is correctly classified as False (class label 0) by the model.\nLIME framework highlights the keywords with high probabilities used by the model for the prediction.\n(a)\n(b)\nFigure 3: (a) LIME explanation for the model with highlighted keywords, (b) Explanation of the input sentences along with\nsource credibility. In this example, the model incorrectly predicts the claim as Partly True while the ground truth is False.\nHowever, looking at the highlighted keywords from the LIME explainer shows that the keywords that are used by the model are\nnot very relevant to the claim. The summary sentences also refute the claim. The users can make an informed decision about\nthe prediction of the model using the explanation output.\nFigure 4: The claim, \u201cU.S. coronavirus response \u2018slowly introducing\u2019 martial law\u201d, is correctly labelled by the model as False.\nWe can see that the summary sentences from credible sources support the model\u2019s prediction.\n(a)\n(b)\nFigure 5: (a) LIME explanation for the model with highlighted keywords, (b) Explanation of the input sentences along with\nsource credibility. The claim, \u201cSix coronavirus cases confirmed in Wichita, Kansas\u201d, is incorrectly labeled by the model as\nTrue. Some of the sentences that the model uses to predict the veracity of the claim are from the sites with unknown credibility.\nThe web page, indeed, is a fake news about the claim. The credible one in green does not mention coronavirus cases in Kansas.\nInstead, it is related to Texas. This highlights the data problem that the model is using for prediction. The related articles are\nsupposed to be from the reputable websites. The explainable module catch these data issues, and inform the user whether to\ntrust the model\u2019s prediction.\nthe other hand, the claim, \u201cSix coronavirus cases confirmed\nin Wichita, Kansas\u201d, is incorrectly labeled by the model as\nTrue. Some of the sentences that the model uses to predict\nthe veracity of the claim are from a web page with unknown\ncredibility. The web page, indeed, is a fake news about the\nclaim. The credible one in green does not mention coron-\navirus cases in Kansas. Instead, it is related to Texas. This\nhighlights the data problem that the model is using for pre-\ndiction. The related articles are supposed to be from the\nreputable websites. The explainable module catch the data\nproblem, and inform the user whether to trust the model\u2019s\nprediction or not.\nConclusion and Future Work\nWe have developed a system to detect misinformation and\nexplain the classification output. In order to provide more\ntransparency and explainability to end users, we also imple-\nmented the LIME XAI framework to explain the model\u2019s\nprediction process. Our approach provides two perspectives\nfor understanding the model: one for machine learning prac-\ntitioners and one for end users. The LIME framework ex-\nplains the model by highlighting the keywords used in the\nclassification process along with the weight of each word.\nThe summary sentences along with the credibility ranking of\nthe news sources explain to an end user how the prediction\nwas made. The results of our study show that the proposed\nsystem is effective for identifying misinformation related to\nCOVID-19 claims. In our future work, we will improve the\naccuracy of the BERT-based classifier by taking into account\ncontext and semantics of linguistic components in the claims\nand related articles. We will improve the user interface and\ndisplayed explanations, and have them evaluated by human\nparticipants in terms of clarity, usefulness, and user friendli-\nness. We will also enhance the explainability of the system\nusing knowledge graphs.\nEthical Statement\nOur research aims to limit the spread of misinformation,\nwhile acknowledging the possibility of classification errors.\nWe have implemented measures to mitigate these risks and\nprovide users with explanations and context for informed\ndecision-making. Additionally, we have considered the eth-\nical implications of our data collection process, which con-\nsists of publicly available information. We recognize the\nbroader impact and ethical considerations of our work, and\nstrive to ensure the reliability, transparency, and trustworthi-\nness of our model\u2019s predictions.\nReferences\nAllcott, H.; and Gentzkow, M. 2017. Social media and fake\nnews in the 2016 election. Journal of Economic Perspec-\ntives, 31(2): 211\u2013236.\nAmoruso, M.; Anello, D.; Auletta, V .; and Ferraioli, D.\n2017. Contrasting the Spread of Misinformation in Online\nSocial Networks. In Proceedings of the 16th Conference on\nAutonomous Agents and MultiAgent Systems , AAMAS \u201917,\n1323\u20131331. Richland, SC: International Foundation for Au-\ntonomous Agents and Multiagent Systems.Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer:\nThe Long-Document Transformer.\nCui, L.; and Lee, D. 2020. CoAID: COVID-19 Healthcare\nMisinformation Dataset. arXiv:2006.00885.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , 4171\u20134186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nDimitrov, D.; Baran, E.; Fafalios, P.; Yu, R.; Zhu, X.; Zloch,\nM.; and Dietze, S. 2020. TweetsCOV19 - A Knowledge\nBase of Semantically Annotated Tweets about the COVID-\n19 Pandemic. In Proceedings of the 29th ACM Interna-\ntional Conference on Information &amp; Knowledge Man-\nagement , CIKM \u201920, 2991\u20132998. New York, NY , USA: As-\nsociation for Computing Machinery. ISBN 9781450368599.\nEsmaeilzadeh, S.; Peh, G. X.; and Xu, A. 2019. Neural\nAbstractive Text Summarization and Fake News Detection.\nCoRR , abs/1904.00788.\nFeng, S.; Banerjee, R.; and Choi, Y . 2012. Syntactic Sty-\nlometry for Deception Detection. In Proceedings of the 50th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers) , 171\u2013175. Jeju Island, Ko-\nrea: Association for Computational Linguistics.\nFujita, K.; Shibahara, T.; Chiba, D.; Akiyama, M.; and\nUchida, M. 2022. Objection!: Identifying Misclassified Ma-\nlicious Activities with XAI. In ICC 2022 - IEEE Interna-\ntional Conference on Communications , 2065\u20132070. IEEE.\nISBN 1538683474.\nFuller, C. M.; Biros, D. P.; and Wilson, R. L. 2009. Decision\nsupport for determining veracity via linguistic-based cues.\nDecision Support Systems , 46(3): 695\u2013703.\nGupta, A.; Lamba, H.; Kumaraguru, P.; and Joshi, A. 2013.\nFaking Sandy: Characterizing and Identifying Fake Images\non Twitter during Hurricane Sandy. In Proceedings of the\n22nd International Conference on World Wide Web , WWW\n\u201913 Companion, 729\u2013736. New York, NY , USA: Associa-\ntion for Computing Machinery. ISBN 9781450320382.\nHossain, T.; Logan IV , R. L.; Ugarte, A.; Matsubara, Y .;\nYoung, S.; and Singh, S. 2020. COVIDLies: Detecting\nCOVID-19 Misinformation on Social Media. In Proceed-\nings of the 1st Workshop on NLP for COVID-19 (Part 2) at\nEMNLP 2020 . Online: Association for Computational Lin-\nguistics.\nHugging Face. 2023. bert-base-uncased.\nhttps://huggingface.co/bert-base-uncased. Accessed on\nMar 23, 2023.\nJin, Z.; Cao, J.; Zhang, Y .; Zhou, J.; and Tian, Q. 2017.\nNovel Visual and Statistical Image Features for Microblogs\nNews Verification. Trans. Multi. , 19(3): 598\u2013608.\nKim, J.; Tabibian, B.; Oh, A.; Sch \u00a8olkopf, B.; and Gomez-\nRodriguez, M. 2018. Leveraging the Crowd to Detect and\nReduce the Spread of Fake News and Misinformation. In\nProceedings of the Eleventh ACM International Conference\non Web Search and Data Mining , WSDM \u201918, 324\u2013332.\nNew York, NY , USA: Association for Computing Machin-\nery. ISBN 9781450355810.\nMa, J.; Gao, W.; and Wong, K.-F. 2017. Detect Rumors\nin Microblog Posts Using Propagation Structure via Kernel\nLearning. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers) , 708\u2013717. Vancouver, Canada: Association for\nComputational Linguistics.\nOtt, M.; Choi, Y .; Cardie, C.; and Hancock, J. T. 2011. Find-\ning Deceptive Opinion Spam by Any Stretch of the Imagina-\ntion. In Proceedings of the 49th Annual Meeting of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies , 309\u2013319. Portland, Oregon, USA: Associa-\ntion for Computational Linguistics.\nOu, J. Y . 2021. Misinformation Identification Using Natural\nLanguage Processing . Master\u2019s thesis, York University.\nOu, J. Y .; Nguyen, U. T.; and Ismail, T. 2022. COVMIS:\nA Dataset for Research on COVID-19 Misinformation. In\n2022 5th International Conference on Data Science and In-\nformation Technology (DSIT) , 1\u201311.\nPatwa, P.; Sharma, S.; PYKL, S.; Guptha, V .; Kumari,\nG.; Akhtar, M. S.; Ekbal, A.; Das, A.; and Chakraborty,\nT. 2020. Fighting an Infodemic: COVID-19 Fake News\nDataset. CoRR , abs/2011.03327.\nPennycook, G.; and Rand, D. G. 2021. Fighting misinfor-\nmation on social media using crowdsourced judgments of\nnews source quality. Proceedings of the National Academy\nof Sciences , 118(15): e1921201118.\nRasmy, L.; Xiang, Y .; Xie, Z.; Tao, C.; and Zhi, D.\n2020. Med-BERT: pre-trained contextualized embeddings\non large-scale structured electronic health records for dis-\nease prediction. CoRR , abs/2005.12833.\nRibeiro, M.; Singh, S.; and Guestrin, C. 2016. \u201cWhy Should\nI Trust You?\u201d: Explaining the Predictions of Any Classifier.\nInProceedings of the 2016 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguis-\ntics: Demonstrations , 97\u2013101. San Diego, California: Asso-\nciation for Computational Linguistics.\nSanaullah, A. R.; Das, A.; Das, A.; Kabir, M. A.; and Shu,\nK. 2022. Applications of machine learning for COVID-19\nmisinformation: a systematic review. Soc. Netw. Anal. Min. ,\n12(1): 94.\nSharma, K.; Qian, F.; Jiang, H.; Ruchansky, N.; Zhang, M.;\nand Liu, Y . 2019. Combating Fake News: A Survey on Iden-\ntification and Mitigation Techniques. ACM Transactions on\nIntelligent Systems and Technology , 10: 1\u201342.\nTalarico, F. A. E.; and Viviani, M. 2022. Credible Text\nSummarization in Social Media. In IEEE/WIC/ACM In-\nternational Conference on Web Intelligence and Intelli-\ngent Agent Technology , WI-IAT \u201921, 603\u2013610. New York,\nNY , USA: Association for Computing Machinery. ISBN\n9781450391153.\nVerrall, N. 2022. COVID-19 Disinformation, Misinforma-\ntion and Malinformation During the Pandemic Infodemic:A View from the United Kingdom , 81\u2013112. Cham: Springer\nInternational Publishing. ISBN 978-3-030-94825-2.\nWani, A.; Joshi, I.; Khandve, S.; Wagh, V .; and Joshi, R.\n2021. Evaluating Deep Learning Approaches for Covid19\nFake News Detection. In Chakraborty, T.; Shu, K.; Bernard,\nH. R.; Liu, H.; and Akhtar, M. S., eds., Combating Online\nHostile Posts in Regional Languages during Emergency Sit-\nuation , 153\u2013163. Cham: Springer International Publishing.\nISBN 978-3-030-73696-5.\nYang, F.; Pentyala, S. K.; Mohseni, S.; Du, M.; Yuan, H.;\nLinder, R.; Ragan, E. D.; Ji, S.; and Hu, X. B. 2019. XFake:\nExplainable Fake News Detector with Visualizations. In\nThe World Wide Web Conference , WWW \u201919, 3600\u20133604.\nNew York, NY , USA: Association for Computing Machin-\nery. ISBN 9781450366748.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "A bert-based explainable system for covid-19 misinformation identification", "author": ["L Moe", "A Kundu", "UT Nguyen"], "pub_year": "2023", "venue": "Accessed: Oct", "abstract": "Misinformation related to COVID-19 can have serious consequences, such as undermining  public health efforts to combat the pandemic. To address this problem, many COVID-19"}, "filled": false, "gsrank": 372, "pub_url": "https://workshop-proceedings.icwsm.org/pdf/2023_46.pdf", "author_id": ["LIqzRS8AAAAJ", "IukgjnIAAAAJ", "eo1FpKIAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:8itEXvcna4oJ:scholar.google.com/&output=cite&scirp=371&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D370%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=8itEXvcna4oJ&ei=SLWsaJjrIPnSieoPxKLpgQ0&json=", "num_citations": 3, "citedby_url": "/scholar?cites=9974109743134026738&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:8itEXvcna4oJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://workshop-proceedings.icwsm.org/pdf/2023_46.pdf"}}, {"title": "Differential impact from individual versus collective misinformation tagging on the diversity of Twitter (X) information engagement and mobility", "year": "2025", "pdf_data": "Article https://doi.org/10.1038/s41467-025-55868-0\nDifferential impact from individual versus\ncollective misinformation tagging on thed i v e r s i t yo fT w i t t e r( X )i n f o r m a t i o nengagement and mobility\nJunsol Kim1, Zhao Wang2, Haohan Shi3, Hsin-Keng Ling4&\nJames Evans1,2,5\nFears about the destabilizing impact of misinformation online have motivated\nindividuals and platforms to respond. Individuals have increasingly challengedothers \u2019online claims with fact-checks in p ursuit of a healthier information\necosystem and to break down echo chamb ers of self-reinforcing opinion.\nUsing Twitter (now X) data, here we show the consequences of individualmisinformation tagging: tagged poste rs had explored novel political infor-\nmation and expanded topical interest s immediately prior ,b u tb e i n gt a g g e d\ncaused posters to retreat into informa tion bubbles. These unintended con-\nsequences were softened by a collective veri \ufb01cation system for misinforma-\ntion moderation. In Twitter \u2019s new feature, Community Notes, misinformation\ntagging was peer-reviewed by other fact -checkers before revelation to the\nposter. With collective misinformation tagging, posters were less likely toretreat from diverse information engagement. Detailed comparison demon-strated differences in toxicity, sentime nt, readability, and delay in individual\nversus collective misinformation tagging messages. These \ufb01ndings provide\nevidence for differential impacts from i ndividual versus collective moderation\nstrategies on the diversity of informat ion engagement and mobility across the\ninformation ecosystem.\nThe visibility of mis- and disinformation online have attracted sub-\nstantial attention around the world wi th demonstrations of their direct\nin\ufb02uence on major collective action in the world1\u20135. These actions\nrange from buying and selling stocks2and avoidance of vaccines3to\nthe attempted coup and occupation of the U.S. Capitol by rioters4.\nLegitimate fears about the destabilizing in \ufb02uence of false online\ninformation have inspired and put pressure on both individuals andplatforms to respond. Individuals proactively correct others \u2019claims by\ndeploying links to fact-checking websites, such as PolitiFact andSnopes\n6\u201310. With the potential for amplifying misinformation through\ufb01lter bubbles11,12, social media platforms like Twitter and Facebook\nhave come under public and political pressure to implement mis-information moderation strategies\n13\u201315.\nIndividuals have become empowered to challenge others \u2019online\nclaims with misinformation tags (or fact-checks) in pursuit of a healthyinformation ecosystem and to break down ideological echochambers\n6\u20138. These misinformation tags tend to target political\noutgroups6,7,9, exposing tagged posters to opposing ideological per-\nspectives. It is less clear, however, whether their misinformation tag-ging motivates targeted posters to explore diverse political contentsReceived: 2 December 2023\nAccepted: 2 January 2025\nCheck for updates\n1Department of Sociology, University of Chicago, Chicago, IL, USA.2Computational Social Science, University of Chicago, Chicago, IL, USA.3School of\nCommunication, Northwestern University, Evanston, IL, USA.4Department of Sociology, University of Michigan, Ann Arbor, MI, USA.5Santa Fe Institute, Santa\nFe, NM, USA. e-mail: jevans@uchicago.edu\nNature Communications |          (2025) 16:973 11234567890():,;\n1234567890():,;\nafterward. Earlier research on motivated reasoning suggests that\nmisinformation tags contradicting targeted poster \u2019s beliefs could\nback\ufb01re and reinforce preexisting beliefs16,17, which could discourage\npeople from exploring diverse information18.B yc o n t r a s t ,ag r o w i n g\nbody of research argues that misinformation tagging does not back-\n\ufb01re, but reduces engagement with misinformation and expands it with\ndiverse information13,14,19,20. These mixed \ufb01ndings suggest that the\neffects of misinformation tagging could depend on the method ofcorrecting misinformation. Individual misinformation tagging byother users often involves toxic and intolerant messages that dehu-manize targeted posters\n9,21, potentially hindering their willingness to\nexplore diverse information22.\nPlatforms have experimented with institutionalized systems that\nverify the accuracy of content through collective inputs from a widerdistribution of users. Notably, on Twitter \u2019s new platform, Community\nNotes (formerly Birdwatch), misinformation tags undergo a formal\npeer-review process by diverse users before being revealed to theo r i g i n a lp o s t e r sa n db r o a d e rT w i t t e ru s e rc o m m u n i t y\n8,13,14.O t h e r\nplatforms, including YouTube and Facebook, have recently tes-ted or announced plans to implement features similar to CommunityNotes\n23,24. Rather than indiscriminately exposing users to mis-\ninformation tags, Community Notes selectively exposes misinforma-tion tags that receive votes from heterogeneous user groups, ensuringthat they are veri \ufb01ed across a broad spectrum of perspectives\n13to\nactivate the wisdom of crowds25,26. The platform also assesses the\nalignment of users \u2019prior contributions with the crowd \u2019sd e c i s i o n s ,\n\ufb01ltering out voters who frequently oppose and backlash against valid\nfact-checks on misinformation. Although individual tags may be noisyand less effective, aggregating them collectively could lead to high-quality crowd judgments that align with expert fact-checks across arange of topics, from COVID-19 to politics\n14,27\u201329. Furthermore, the\nCommunity Notes platform has speci \ufb01cally instituted norms that deter\ntoxic and intolerant misinformation tagging messages30,p o t e n t i a l l y\nenhancing the ef \ufb01cacy of misinformation moderations and gently\nencouraging posters to leave their echo chambers and explore abroader world of diverse information.\nIn this study, we explore the impacts of individual and collective\nmisinformation tagging on tagged posters \u2019echo chambers. Echo\nchambers refer to \u201cbounded, enclosed media spaces that have the\npotential to both magnify messages delivered within them and insulatethem from rebuttal \u201d\n31,32, which could increase susceptibility to\nmisinformation11,33,34. One indicator of echo chambers is their lack of\ninteraction with politically diverse, cross-cutting sources of informa-tion. Prior research has measured echo chambers by selectiveengagement with like-minded news sources, which insulate peoplefrom opposing perspectives that could empower rebuttal\n35,36.T h i s\nmeasure strongly correlates with other echo chamber indicators, suchas intensive interactions with like-minded users (i.e., homophily)\n37,38.\nLiterature suggests that lack of exposure to and cross-veri \ufb01cation\nthrough opposing perspectives could erode the ability to \ufb01nd, evalu-\nate, and use information effectively11,39,40. It could provide users with\nthe illusion that their views are publicly supported41,42,w e a k e n i n gt h e i r\noverall immunity against misinformation.\nThe other key indicator of echo chambers is their absence of\ncontent diversity resulting from limited engagement with diverse,unfamiliar topics. Emerging literature has documented the rise ofsocio-political endogamy, noting that both left and right increasinglydevelop distinct topical interests, encompassing knowledge bases,cultural tastes, and lifestyles\n43\u201345. For example, left-leaning individuals\nare more likely to engage with basic science books about physics,\nastronomy, and zoology, while right-leaning individuals prefer those\nabout applied and commercial sciences like criminology, medicine,and geophysics\n45. In this way, political polarization spills over into a\nvariety of other topics, leading to multi-dimensional segregationwhere opposing political groups share progressively less commonground and inhabit different realities even in topics apparently unre-\nlated to politics\n43,46. Topical echo chambers, which magnify topics\nprevalent within one political group and insulate them from others,can problematize intergroup communication and interaction.\nDoes exposure to each type of misinformation tagging encou-\nrage or discourage posters from exploring diverse information andbreaking out of echo chambers? To answer this question, we uselarge-scale digital traces from the platform formerly known asTwitter ( Xas of July, 2023) to identify posters exposed to each\napproach of misinformation tagging. First, we identify posters tar-geted by individual misinformation tags. These posters \u2019tweets\nreceived other individuals \u2019voluntary replies, citing fact-checking\narticles from PolitiFact, one of the largest and most studied pro-fessional fact-checking organizations in the United States\n7,10. Sec-\nond, we examine posters targeted by collective misinformation\ntags. These posters \u2019tweets received notes that contain collectively\nveri\ufb01ed fact-checks through Twitter \u2019s Community Notes platform.\nFigure 1a visualizes the mechanism of each type of misinformation\ntagging, which represent the most prevalent misinformation mod-eration strategies on Twitter\n6\u201310,13\u201315. Supplementary Fig. 1 presents\nan example of individual and collective tags that correct topicallyidentical, COVID-19 misinformation.\nUsing 712,948 tweets that cite news sources \u2014including posts,\nretweets, and quotes \u2014posted by 7733 users before and after they were\ntargeted by misinformation tags, we estimate the effects of these tagson the posters \u2019echo chambers. Speci \ufb01cally, we measure echo cham-\nbers using political and content diversity in their posting and sharing\nbehavior (see Fig. 1b). Political diversity measures whether a poster \u2019s\ntweet cites a source with opposing political stance (e.g., a right-leaningposter references left-leaning articles)\n5,47. Content diversity measures\nwhether a tweet discusses novel topics unfamiliar in the poster \u2019s his-\ntorical tweets. We apply a transformer-based sentence embeddingmodel (SentenceBERT) to extract a high-dimensional, semantic vectorrepresentation for each tweet, and aggregate the vectors of eachauthor \u2019s historical tweets to produce an average semantic vector for\neach poster. We then measure the distance between a particular tweetand the poster to assess the degree to which this tweet expands the\nposter \u2019s content diversity. As our data focus on tweets citing news\nsources, we assume that the increase of content diversity indicates theexploration of novel political news topics. For example, consider a userwho regularly consumes and shares news about COVID-19 but beginsto discuss U.S. tax and labor issues as well. This shift indicates anincrease in the user \u2019s content diversity, as detailed in Supplementary\nTable 1. We consider both political and content diversity because theyrepresent different dimensions that could reinforce one another inlimiting exposure to information and exacerbating echo chambers onsocial media\n17,43,48.\nResults\nWe aim to investigate the effects of individual and collective mis-\ninformation tagging on political and content diversity using large-scaleTwitter data. In our observational data, treatments (i.e., exposure tomisinformation tagging), however, are not randomly assigned tomisinformation posters, which pose challenges for identifying thecausal effects of misinformation tagging. To address these concerns,we apply interrupted time series (ITS) and delayed feedback (DF)analysis, which help eliminate non-causal explanations under certainassumptions.\nInterrupted time series (ITS) analysis\nInterrupted Time Series (ITS) analysis investigates whether the trend inpolitical and content diversity shifts after misinformation tagging. ITSassumes that without the intervention of misinformation tagging, thepre-treatment trend (i.e., before misinformation tagging) would per-sist, and the immediate change in trend after misinformation tagging isArticle https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 2\nattributed to effects from tagging. We control for user-level \ufb01xed\neffects to correct for time-invariant user characteristics.\nFigure 2aa n dT a b l e 1report results from our ITS analysis (Political\nDiversity: R2=0 . 1 7 3 ,C o n t e n tD i v e r s i t y : R2= 0.243). Posters manifest an\nincreasing tendency to explore novel political information beforebeing fact-checked by misinformation tags. Speci \ufb01cally, before indi-\nvidual and collective misinformation tagging, posters increase thepolitical diversity ( \u03b2= 0.237, 95% CI = [0.125, 0.349], t(418115) = 4.14,\np< 0.001) and content diversity ( \u03b2= 0.007, 95% CI = [0.004, 0.010],\nt(418115) = 4.79, p< 0.001) of their information engagement over time.\nHaving their posts criticized by individual misinformation tags,\nhowever, causes posters to retreat within an information bubble.Immediately after tagging, posters signi \ufb01cantly decrease the political\ndiversity ( \u03b2=\u22121.009, 95% CI = [ \u22121.447, \u22120.571], t(418115) = \u22124.52,Fig. 2 | Political and content diversity change with the intervention of indivi-\ndual and collective misinformation tagging. a Results from Interrupted Time\nSeries (ITS) analysis. The x-axis denotes the timeline of tweets posted before and\nafter tagging, with negative values representing the number of weeks before\nposting tagged tweets and positive values representing the number of weeks after.\nThe y-axis represents political and content diversity, with dots indicating thediversity score for each corresponding week as estimated by the ITS analysis, anderror bars showing 95% con \ufb01dence intervals. Solid lines connect the dots revealing\ntrends of political and content diversity before and after tagging, with gray dottedlines tracing the counterfactual trend if fact-checks had not occurred. The sample\nsize is 424,969 tweets. There is no control group in this analysis; however, a com-parative interrupted time series analysis with a control group can be found inSupplementary Method 3. bIllustration of political and content diversity dynamics\nbefore and after tagging. Before individual and collective tagging, posters exhibit\nincreased political and content diversity, which increases the likelihood ofencountering a fact-checker. After individual tagging, posters retreat into infor-mation bubbles; after collective tagging, they venture further beyond them.\nFig. 1 | Misinformation Tagging and Outcomes Measurement. a Individual mis-\ninformation tagging in which individuals cite PolitiFact fact-checking articles. Col-\nlective misinformation tagging through the Community Notes platform, which\nselectively exposes veri \ufb01ed misinformation tags that receive diverse votes as\nhelpful. bOperationalization of tweet political and content diversity. Political\ndiversity captures whether a poster cites a source with opposing political stance(binary 0/1), assessed from the aggregate stances of referenced sources. Content\ndiversity captures whether a post discusses topics unfamiliar to the author \u2019s his-\ntorical tweets (continuous), assessed with the distance between the poster \u2019sa v e r -\nage tweet and a particular tweet within a contextual embedding (sentenceBERTpre-trained on Twitter)\n74.Article https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 3\np< 0.001) and content diversity ( \u03b2=\u22120.030, 95% CI = [ \u22120.042,\n\u22120.019], t(418115) = \u22125.10, p< 0.001) of their posts. After tagging, the\nslope becomes nearly \ufb02at, indicating that posters \u2019future posts con-\ntinue to collapse in both political diversity ( \u03b2=0 . 0 8 7 , 9 5 % C I=\n[\u22120.020, 0.194], t(418115) = 1.60, p= 0.110) and content diversity\n(\u03b2=\u22120.003, 95% CI = [ \u22120.006, 0.000], t(418115) = \u22121.95, p= 0.51).\nBy contrast, there is no statistically signi \ufb01cant evidence that col-\nlective misinformation tagging causes individuals to retreat withint h e i rp r i o ri n f o r m a t i o nb u b b l e .T h ed a t ae v e nr e v e a l sas l i g h t ,a l t h o u g hnot signi \ufb01cant, increase in political diversity ( \u03b2=0 . 2 7 0 , 9 5 % C I=\n[\u22120.824, 1.363], t(418115) = 0.48, p= 0.629) and a signi \ufb01cant increase in\ncontent diversity ( \u03b2=0 . 0 4 0 , 9 5 %C I=[ 0 . 0 1 2 , 0 . 0 6 9 ] , t(418115) = 2.74,\np= 0.006) immediately after tagging. Nevertheless, collective mis-\ninformation tagging has only a temporary effect on individual posters.Especially, the slope for content diversity changes signi \ufb01cantly after\ntagging ( \u03b2=\u22120.014 ,9 5 %C I = [ \u22120.024, \u22120.003] ,t(418115) = \u22122.60 ,\np= 0.009), eventually converging to levels experienced before the\ninitial misinformation tags occur. Despite the steepness of the slopefollowing collective tagging, our analysis indicates that contentdiversity does not signi \ufb01cantly drop below the pre-tagged period (see\nSupplementary Method 1).\nWe\ufb01nd that the gap between the effects of individual and col-\nlective misinformation tagging is signi \ufb01cant, particularly regarding the\nimmediate intercept change in political diversity ( \u03b2\nIndividual =\u22121.009,\n\u03b2Collective =0 . 2 7 0 , \u03b2Collective \u2212\u03b2Individual = 1.279, 95% CI = [0.101, 2.457],\nt(418115) = 2.13, p= 0.033) and in content diversity ( \u03b2Individual =\u22120.030,\n\u03b2Collective =0 . 0 4 0 , \u03b2Collective -\u03b2Individual = 0.070, 95% CI = [0.039, 0.102],\nt(418115) = 4.44, p< 0.001).\nAdditional analyses reveal the effects of misinformation tagging\non the proximity between posters and misinformation taggers. Thissuggests that Twitter navigation likely makes posters more visible tofact-checkers as they venture into foreign territory (see Fig. 2b).\nExposure to fact-checks causes them to retreat back into their infor-mation bubbles, distancing them from the foreign stances that fact-\nchecked them (see Supplementary Method 2).\nBecause time-variant confounders (e.g., viral news, platform\nalgorithm changes, or signi \ufb01cant external events) can affect ITS out-\ncomes, we conduct additional analyses to control for these factors.First, we control for major events during the study period throughsensitivity analyses. Second, we apply comparative interrupted time\nseries (CITS) analyses. These additional analyses support our initial\ufb01ndings (see Supplementary Method 3). Additionally, to address\nautocorrelated posting behaviors among social media users, weinclude autoregressive terms in the ITS models, further enhancing therobustness of our \ufb01ndings (see Supplementary Method 4).\nTo better understand what happens when posters retreat to their\ninformation bubbles, we conduct a series of descriptive analyses (seeSupplementary Table 2). When posters reduce their political andcontent diversity, the number of tweets (comprising posts, retweets,and quotes) posted per day signi \ufb01cantly increases, indicating that\nusers are more active within their information bubbles. Speci \ufb01cally,\nthe number of tweets per day is negatively correlated with politicaldiversity ( r=\u22120.107, t(712946) = \u221290.87, 95% CI = [ \u22120.109, \u22120.105],\np< 0.001) and content diversity ( r=\u22120.052, t(712946) = -43.97, 95%\nCI = [ \u22120.054, \u22120.050], p<0 . 0 0 1 ) . S i m i l a r l y , w e \ufb01nd that the type of\nposting is different; the proportion of retweets (i.e., tweets simplysharing other users \u2019tweets) out of the entire tweets per day is nega-\ntively correlated with political diversity ( r=\u22120.046,\nt(712946) = \u221238.88, 95% CI = [ \u22120.048, \u22120.044], p< 0.001) but posi-\ntively correlated with content diversity ( r= 0.012, t(712946) = 10.13,\n95% CI = [0.010, 0.014], p< 0.001). This indicates that users actively\npost tweets rather than passively retweet other users \u2019tweets when\nthey exhibit low political diversity. To demonstrate the signi \ufb01cant\neffects of misinformation tagging on political and content diversity,irrespective of these factors, we have adjusted for the number oftweets posted per day. We have also controlled for the proportion ofretweets per day, which did not meaningfully change our results (seeSupplementary Table 3).\nDelayed feedback (DF) analysis\nWe employ delayed feedback (DF) analysis to further strengthen ourcausal inference\n49. In our DF analysis, we estimate baseline changes\n(i.e., changes in outcomes that occur without tags) to answer the\nquestion: \u201cAre shifts in political and content diversity attributable to\ntagging, or do similar changes occur even without tagging? \u201dPairs of\ntweets containing similar misinformation, targeted by misinformationtagging at different times, are matched to construct a control group,consisting of posters whose problematic tweets have not yet beenTable 1 | Interrupted Time Series (ITS) Model Results for Political and Content Diversity\nOutcome Political diversity (%) Content Diversity (z)\nType of misinformation\ntaggingIndividual Collective Difference\n(Collective - Individual)Individual Collective Difference\n(Collective - Individual)\nSlope before posting\nthe tweet0.237***\n[0.125, 0.349]t=4 . 1 4 , p<0 . 0 0 10.309*\n[0.041, 0.578]t=2 . 2 6 , p=0 . 0 2 40.072\n[\u22120.219, 0.363]\nt=0 . 4 8 0 , p=0 . 6 2 80.007***\n[0.004, 0.010]t=4 . 7 9 , p<0 . 0 0 10.003\n[\u22120.004, 0.010]\nt=. 7 4 , p= 0.461\u22120.005\n[\u22120.012, 0.003]\nt=\u22121.17,p=0 . 2 4 3\nImmediate intercept\nchange after tagging\u22121.009***\n[\u22121.447, \u22120.571]\nt=\u22124.52, p<0 . 0 0 10.270\n[\u22120.824, 1.363]\nt=0 . 4 8 , p= 0.6291.279*\n[0.101, 2.457]t=2 . 1 3 , p=0 . 0 3 3\u22120.030***\n[\u22120.042, \u22120.019]\nt=\u22125.10, p<0 . 0 0 10.040**\n[0.012, 0.069]t=2 . 7 4 , p=0 . 0 0 60.070***\n[0.039, 0.102]t= 4.44, p\n<0 . 0 0 1\nSlope after tagging 0.087\n[\u22120.020, 0.194]\nt=1 . 6 0 , p= 0.110\u22120.049\n[\u22120.334, 0.235]\nt=\u22120.34, p=0 . 7 3 4\u22120.136\n[\u22120.440, 0.167]\nt=\u22120.88, p=0 . 3 7 9\u22120.003\n[\u22120.0006, 0.000]\nt=\u22121.95, p=0 . 0 5 1\u22120.011**\n[\u22120.019, \u22120.004]\nt=\u22122.88, p=0 . 0 0 4\u22120.008*\n[\u22120.016, 0.000]\nt=\u22122.01, p=0 . 0 4 4\nSlope change\n(After - Before)\u22120.150\n[\u22120.306, 0.006]\nt=\u22121.89, p= 0.059\u22120.358\n[\u22120.749, 0.033]\nt=\u22121.80, p=0 . 0 7 2\u22120.208\n[\u22120.629, 0.213]\nt=\u22120.97, p=0 . 3 3 2\u22120.010***\n[\u22120.014, \u22120.006]\nt=\u22124.79, p< 0.001\u22120.014**\n[\u22120.024, \u22120.003]\nt=\u22122.60, p=0 . 0 0 9\u22120.004\n[\u22120.015, 0.007]\nt=\u22120.64, p= 0.520\nR20.173 0.243\nAdjusted R20.159 0.230\nObservations 424,969\ndf 418,115\nNotes: *** p<0 . 0 0 1* * p<0 . 0 1* p< 0.05. We multiply political diversity by 100 to interpret the estimates as absolute percentage point changes. We normalize content diversity to z-s cores (the number\nof standard deviations from the mean). All regressions control for user \ufb01xed effects and the number of tweets per day. The statistical signi \ufb01cance of regression coef \ufb01cients is tested using two-sided t-\ntests. Con \ufb01dence intervals (95%) are provided in brackets, along with the corresponding t-statistics, degrees of freedom, and exact p-values. More details can be found in Methods: Interrupted Time\nSeries (ITS) Analysis.Article https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 4\ntagged due to delayed feedback, and a treatment group of posters who\nhave. For instance, Supplementary Fig. 2 presents an illustrativeexample involving a pair of matched tweets and tags.\nIn Fig. 3a, post-treatment ( t\n1) represents the time window when\ntreatment tweets are tagged but c ontrol tweets are not, and pre-\ntreatment ( t0) represents the time window with equal duration t1when\nboth treatment and control tweets are untagged. Changes in the out-comes between t\n0andt1in the control group re \ufb02ect baseline changes,\nwhich indicate changes without tags. Changes between t0andt1in the\ntreatment group re \ufb02ect treated changes, which indicate changes with\nt a g s .W ec o m p a r et h ed i f f e r e n c ei np r e - p o s tc h a n g eb e t w e e nc o n t r o land treatment groups (i.e., baseline vs. treated changes) to identify theeffects of misinformation tagging on political and content diversity. DFanalysis assumes that, in the absence of treatment, both control andtreatment groups would exhibit parallel trends. We control for user-\nlevel \ufb01xed effects to control for time-invariant, user-speci \ufb01c\ncharacteristics.\nFigure 3ba n dT a b l e 2present results from the DF analysis (Poli-\ntical Diversity: R\n2= 0.274, Content Diversity: R2= 0.358). Our DF ana-\nlysis demonstrates that changes are indeed due to tagging, showingthat treated changes are signi \ufb01cant above and beyond baselinechanges. Consistent with the ITS \ufb01ndings, DF analysis indicates that\nindividual misinformation tags lead to a signi \ufb01cant decrease in poli-\ntical diversity ( \u03b2=\u22125.886, 95% CI = [ \u22129.633, \u22122.138], t(8182) = \u22123.08,\np= 0.002). Nevertheless, individual misinformation tagging does not\nsigni\ufb01cantly affect content diversity ( \u03b2=0 . 0 1 8 ,9 5 %C I=[ 0 . 1 4 5 ,0 . 4 0 3 ] ,\nt(8182) = 4.17, p= 0.652). Although ITS analyses show that content\ndiversity decreases after tagging, DF analyses indicate no statisticallysigni\ufb01cant evidence that content diversity decreases beyond baseline\nchanges observed without tags. Collective misinformation tags, bycontrast, do not produce a signi \ufb01cant decrease in political diversity\n(\u03b2=1 . 2 1 9 ,9 5 %C I=[ \u22124.777, 7.215], t(8182) = 0.40, p=0 . 6 9 0 )a n de v e n\nincrease content diversity following tagging ( \u03b2=0 . 2 7 4 ,9 5 %C I=[ 0 . 1 4 5 ,\n0.403], t(8182) = 4.17, p< 0.001). The gap between the effects of indi-\nvidual and collective tagging is signi \ufb01cant for both political\ndiversity ( \u03b2= 7.105, 95% CI = [0.069, 14.140], t(8182) = 1.98, p= 0.048)\nand content diversity ( \u03b2=0 . 2 5 6 , 9 5 % C I=[ 0 . 1 0 5 , 0 . 4 0 7 ] ,\nt(8182) = 3.32, p= 0.001).\nLinguistic characteristics of misinformation tags\nIndividual and collective misinformation tagging messages manifest\ndifferent linguistic characteristics. As shown in Fig. 4and Supplementary\nFig. 3 | Delayed feedback (DF) analysis. a Pre- and post-treatment periods. Post-\ntreatment ( t1) represents the time window when treated tweets are tagged but\ncontrol tweets are not. Pre-treatment ( t0) represents the time window with equal\nduration t1when both treatment and control tweets remain untagged. bThe effects\nof individual and collective misinformation tagging on political and contentdiversity are estimated by the difference in pre-post changes in outcomes betweenthe treatment and control groups. Dots represent the difference in pre-post\nchanges for each outcome between the treatment and control groups, with errorbars indicating 95% con \ufb01dence intervals. Pairs of tweets containing similar mis-\ninformation, targeted by misinformation tagging at different times, are matched toconstruct the control group. The control group consists of users whose proble-\nmatic tweets had not yet been tagged due to delayed feedback, while the treatment\ngroup consists of users whose problematic tweets had already been tagged (see\u201cMethods \u201d: Delayed Feedback (DF) Analysis for details). The sample size is 8901\ntweets.\nTable 2 | Delayed feedback (DF) model resul ts for political and content diversity\nOutcome Political diversity Content Diversity\nType of misinformation\ntaggingIndividual Collective Difference\n(Collective - Individual)Individual Collective Difference\n(Collective - Individual)\nDifference in Pre-Post Change\n(Treatment - Control)\u22125.886**\n[\u22129.633, \u22122.138]\nt=\u22123.08, p=0 . 0 0 21.219\n[\u22124.777, 7.215]\nt= 0.40, p= 0.6907.105*\n[0.069, 14.140]\nt= 1.98, p=0 . 0 4 80.018\n[\u22120.062, 0.099]\nt= 0.45, p= 0.6520.274***\n[0.145, 0.403]\nt=4 . 1 7 , p<0 . 0 0 10.256**\n[0.105, 0.407]\nt= 3.32, p= 0.001\nR20.274 0.358\nAdjusted R20.211 0.301\nObservations 8901df 8182\nNotes: *** p< 0.001 ** p<0 . 0 1* p< 0.05. Each cell presents the difference in pre-post change (treatment group - control group) in each outcome. We multiply political diversity by 100 to interpret the\nestimates as absolute percentage point changes. We normalize content diversity to z-scores (the number of standard deviations from the mean). All re gressions control for user \ufb01xed effects and the\nnumber of tweets per day. The statistical signi \ufb01cance of regression coef \ufb01cients is tested using two-sided t-tests. Con \ufb01dence intervals (95%) are provided in brackets, along with the corresponding t-\nstatistics, degrees of freedom, and exact p-values. More details can be found in Methods: Delayed Feedback (DF) Analysis.Article https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 5\nTable 4, we \ufb01nd that individual misinforma tion tags exhibit twice the\ntoxic content (Mean Individual =0 . 1 3 9 ,M e a n Collective = 0.076, Mean Collective -\nMean Individual =\u22120.063, t(7496) = 9.86, p< 0.001, Cohen \u2019sd=\u22120.228)\nand convey more negative sentiment compared to collective mis-information tags (Mean\nIndividual =\u22120.082, Mean Collective =\u22120.050,\nMean Collective -Mean Individual = 0.032, t(7731) = 2.14, p= 0.033, Cohen \u2019sd\n= 0.049). Collective tags express slightly higher positive sentiment andproduce messages with more neutral sentiment than individual tags.Furthermore, individual tag messages are much shorter(Mean\nIndividual =179.31, Mean Collective = 288.87, Mean Collective -\nMean Individual =1 0 9 . 5 6 , t(7731) = 26.95, p<0 . 0 0 1 ,C o h e n \u2019sd=0 . 6 1 3 )a n d\nmore readable ( \u03c72(7) = 155.32, p<0 . 0 0 1 , C r a m e r \u2019sV=0 . 1 5 5 ) t h a n c o l -\nlective tags. While 53.53% of individual tags necessitate a college-level\nreading comprehension or higher, 75.77% of collective tags demand\nthis level. Moreover, the delay between posting misinformation and re-ceiving fact-checks is shorter for individual than collectivetagging (Mean\nIndividual =3.037, Mean Collective = 6.322, Mean Collective -\nMean Individual =3 . 2 8 5 , t(7731) = 2.13, p= 0.033, Cohen \u2019sd=0 . 0 4 8 ) .T h e s e\n\ufb01ndings demonstrate that individual tags convey their messages quickly\nthrough messages that are succinct , straightforward, emotive, and\nsometimes toxic. In contrast, collective tags are more slowly commu-nicated through lengthy, complex messages, devoid of emotionalundertone or toxicity.\nBased on linguistic differences between individual and collective\ntags, we question whether gaps in the effects of individual versus\ncollective tags persist, even when the linguistic characteristics of thesetags are similar. First, we control for toxicity by excluding tags with atoxicity level higher than 0.4 and retrain only non-toxic tags. Second,we control for sentiment by removing tags with either positive (> 0.2)or negative (< \u22120.2) sentiments, keeping only neutral tags. Third, we\ncontrol for length by excluding tags longer than 400 characters andretaining short tags. Fourth, we control for readability by excludingtags that require college-level or higher readability and selecting tagsthat are relatively easy to read. Fifth, we control for delay by omittingany tags associated with delays longer than 48 hours (log-transformeddelay > 1.10) and focusing on quick tags.\nWe\ufb01nd that the gap between individual and collective tagging\nremains statistically signi \ufb01cant, except when controlling for length. As\nshown in Supplementary Table 5, the gap in political diversity is notstatistically signi \ufb01cant after controlling for length ( \u03b2= 1.071, 95% CI =\n[\u22120.231, 2.373], t(399236) = 1.61, p= 0.107). Nevertheless, controllingfor length only accounts for 16.26% of the gap between individual and\ncollective tagging in political diversity. This indicates that linguistic\ncharacteristics explain a modest but nontrivial portion of the differ-\nential impacts between individual and collective tagging. Nevertheless,these measured qualities do not account for the vast majority of thedifference.\nControl analyses\nIn this section, we identify systematic differences in misinformationthat receive individual versus collective tagging, as well as differencesin the posters corrected by each type. Even after controlling for thesedifferences in additional interrupted time series (ITS) analyses, indi-vidual and collective tagging have signi \ufb01cantly different effects in the\ndirections identi \ufb01ed by our unconstrained analysis.\nFirst, we observe that individual taggers focus more on political\ntopics, while collective taggers correct a more diverse range of topics(see Supplementary Table 6). As shown in Supplementary Table 7, thenine most frequent topics in our dataset include political topics knownto trigger divisive, polarized reactions in US politics (see \u201cMethods \u201d:\nTopic modeling). These topics account for 84.06% of the correctionsmade through individual tagging but only 59.49% of the correctionsmade through collective tagging.\nTherefore, we control for topics of the corrected misinformation,\n\ufb01nding that the gaps between individual and collective tags are sig-\nni\ufb01cant and even slightly larger when they correct identical topics of\nmisinformation. Speci \ufb01cally, we employ propensity score weighting\n(PSW) method (see Supplementary Method 5). The results demon-strate that even when individual and collective tagging correct topi-cally identical messages, the gap between individual and collectivetagging is signi \ufb01cant, both in the immediate change of political\ndiversity ( \u03b2= 2.380, 95% CI = [0.200, 4.560], t(296544) = 2.14,\np= 0.032) and content diversity ( \u03b2= 0.048, 95% CI = [0.003, 0.092],\nt(296544) = 2.11, p= 0.035). We note that collective tagging is less likely\nto correct political topics than individual tagging but is more effectivein causing original posters to explore diverse content when success-fully deployed on political topics. Refer to Supplementary Table 8 for\ndetails.\nSecond, we \ufb01nd that the proportion of right-leaning users cor-\nrected by individual tagging is 53.17% while right-leaning users cor-rected by collective tagging is 44.14%. We also analyze the distributionof political stance among taggers (i.e., those who write individual tags)Fig. 4 | Linguistic characteristics of fact-checking messages. a univariate kernel\ndensity function for toxicity. bunivariate kernel density function for sentiment.\ncunivariate kernel density function for length (characters). dhistogram for readingease ( e) univariate kernel density function for delay (log-transformed days). The\npurple line (or bar) represents the distribution within individual misinformation\ntags, while the yellow line (or bar) represents the distribution within collective tags.Article https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 6\nand voters (i.e., those who vote on the exposure of collective tags) (see\nSupplementary Method 6 and Supplementary Table 9). We comparethe effects of individual and collective tagging in the common scenariowhere right-leaning posters are corrected by left-leaning ones. Speci-\n\ufb01cally, we focus on cases where right-leaning posters are corrected\neither by individual tags from left-leaning taggers or by collective tagsapproved by a majority of left-leaning voters (i.e., Community notesapproved by voters, where at least 50% of those with identi \ufb01able\npolitical stances are left-leaning). In this analysis, the differencebetween the effects of individual and collective tagging is still sig-ni\ufb01cant, both in the immediate change of political diversity ( \u03b2=1 . 7 8 0 ,\n95% CI = [0.118, 3.441], t(238081) = 2.10, p= 0.036) and content diver-\nsity ( \u03b2= 0.076, 95% CI = [0.033, 0.119], t(238081) = 3.46, p= 0.001).\nRefer to Supplementary Table 10 for details.\nThird, we \ufb01nd that popular users are more likely to receive col-\nlective tags than individual tags, which is consistent with prior litera-\nture (see Supplementary Fig. 3)\n8. To examine the differences between\nindividual and collective tags when focusing on less popular, everydayusers, we exclude those whose number of followers exceeds 2967, theaverage number of followers among users corrected by individual tags.We\ufb01nd the results are consistent overall (see Supplementary Table 11),\nbut suggest that collective tagging of low popularity posters is slightlymore effective, relative to individual tagging, than with high popularityusers. In particular, the difference between individual and collectivetagging is signi \ufb01cant, both in the immediate change of political\ndiversity ( \u03b2= 3.612, 95% CI = [0.824, 6.399], t(235632) = 2.54, p= 0.011)\nand content diversity ( \u03b2= 0.081, 95% CI = [0.000, 0.162],\nt(238081) = 1.97, p= 0.049). This may indicate the inoculation of pop-\nular users to critique, an increased sensitivity among unpopular usersto collective nudges\n50, or both.\nRobustness checks\nWe verify our \ufb01ndings with a battery of robustness checks. First, we\nseek to avert concerns over the presence of bots on Twitter by rea-nalyzing our data excluding identi \ufb01ed bot accounts\n2,5.S e c o n d ,w e\nreanalyze the relationship controlling for potentially insincere infor-mational activities, such as citing sources of low credibility and\nintentionally spreading fake news. Third, we attempt to avoid situa-\ntions in which posters simply criticize distant information withouthonest consideration by \ufb01ltering out posts with negative sentiment.\nFourth, we identify all tweets with i nt h es a m p l et h a tm e n t i o nk e y w o r d s\nrelated to receiving community notes broadly and remove them, asthey could confound our measure of content diversity. To addressconcern regarding the effect of replying directly to individual taggers,which could confound the measure of political diversity, we alsoidentify and remove all tweets that reply directly to individual taggers.Fifth, to strictly identify individual tags (i.e., PolitiFact links) that cor-rect the original posters, we prompt ChatGPT to annotate whether the\nlinks are used to correct the original poster rather than support them.\nThen, we limit the sample to links that correct the original posters.Sixth, considering the low visibility of individual tags in Twitter \u2019s\nmessage-reply interface\n6,8, we restrict the sample to original posters\nwho replied to (and thereby read) the individual tags and remove non-responders. These alterations do not meaningfully impact our repor-ted outcomes (see \u201cMethod \u201d: Robustness Checks).\nDiscussion\nThis study provides empirical evidence regarding the impact of indi-\nvidual and collective misinformation tagging on echo chambers.Before misinformation tagging, posters show an increased curiosity in\ndiverse political and topical content. This challenges the conception\nthat misinformation is generated and corrected when people retreatinto echo chambers\n11,33. On the contrary, posters become fact-checked\nwhen they venture outside those bubbles. Why is exploration followedby misinformation tagging? First, posters could misinterpretunfamiliar and diverse information from a lack of information\nliteracy\n51, increasing the chance of posting the misinformation being\ntagged. Second, news feed algorithms may increase the probabilitythat posters \u2019tweets become visible to people from political outgroups,\nwho are highly motivated to fact-check foreign posters\n6,7,14.O u ra n a -\nlysis shows that posters increase the closeness to misinformationtaggers before fact-checks, which could increase the chance ofappearing in fact-checkers \u2019news feeds.\nIndividual misinformation tagging discourages posters from\nexploring diverse information. Posters tagged by individuals manifestan immediate drop in political diversity, as evidenced by both inter-rupted time series (ITS) and delayed feedback (DF) analyses. Contentdiversity also decreases in ITS analyses, although DF analyses do notreveal a signi \ufb01cant drop. This suggests that while content diversity\ndecreases after tagging, it does not fall below the baseline change\nexpected without tags. These unintended consequences are mitigated\nby collective misinformation tagging. Unlike individual tagging, thereis no statistically signi \ufb01cant evidence that collective tagging dimin-\nishes political and content diversity in both ITS and DF analsyzes;moreover, it results in a short-term rise in content diversity.\nOur analyses show that individual tagging involves short, toxic,\nand emotion-driven messages. Collective tagging, on the other hand,involves longer, less toxic, emotionally neutral, and deliberative mes-sages revealed to posters longer after their offending posts. Theseresults suggest the trade-off between the effectiveness of establishedsystems for promoting openness and mobility across the information\necosystem, but the ef \ufb01ciency of individuals in cleaning it. Low visibility\nof individual misinformation tagging in Twitter \u2019sm e s s a g e - r e p l y\ninterface\n6,8may motivate taggers to use short and potentially toxic\nmessages. Community Notes responded by implementing a morevisible interface for collaborative tagging, which reduces the tendencyto terseness, facilitating long and deliberate discussion. Also, normsand values underlying participation in Community Notes could pre-vent taggers from disseminating succinct yet in \ufb02ammatory messages\nviewed as unhelpful and instead source diverse perspectives\n13.\nWhat mechanisms drive differences in the effects of individual\nand collective misinformation tagging on echo chambers? We \ufb01nd that\nlinguistic characteristics, such as toxicity, sentiments, and length only\npartially explain differential impacts between individual and collectivetagging. This implies that differences in quality other than linguisticcharacteristics also exert a direct in \ufb02uence. Literature on the wisdom\nof crowds suggests that while individual tags are susceptible to biasesand noise, aggregating tags collectively could correct individual bias,increasing the quality of nonexpert fact-checks\n28,52,53. For example,\ncompared to individual tags, collective tags are more closely alignedwith professional fact-checks from experts on a variety of topics,ranging from COVID-19 to politics\n14,27\u201329. Even though we focus on\nindividual tags that cite professional fact-checks (i.e., PolitiFact), it is\npossible that interpretations within individual tags might be less\neffective when not cross-validated like collective tags. For example,individual tags might fail to convey the key points of PolitiFact articlesor clearly articulate the relevance of these articles to the original post.Additionally, when multiple fact-checkers co-validate collective tags,these decisions may be perceived as more legitimate and less sus-ceptible to biases, encouraging the original posters to seek out morediverse and cross-validating information\n28.\nOverall, our \ufb01ndings suggest that misinformation is posted and\nfact-checked when original posters who were accustomed to like-minded sources associated with low credibility (see SupplementaryTable 2) suddenly increase their political and content diversity. In the\nshort term, some might believe that pushing them back into their echo\nchambers with individual tags seems like an effective way to curbmisinformation. Nevertheless, over the long term, this approach couldexpand the cluster of users immersed in misinformation, deprivingthem of opportunities to educate themselves with opposingArticle https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 7\nperspectives. The ethical and normative aspects of our research\nremain open questions, but we suggest that collective taggingencouraging exploration might be better for the long-term health ofthe information ecosystem.\nOur analyses have several notable limitations. First, our method\nfor assessing posters \u2019political stances is indirect, through their\nposting behavior\n5. This approach has been successfully applied to\npredict political party af \ufb01liation and self-described ideology in\nprevious literature53, but using a direct measure of political ideol-\nogy or af \ufb01liation with social media and survey data would\nstrengthen our assessments. Second, our quasi-experimentalmethodologies (ITS and DF) depend on assumptions for causalinference. We employ topic modeling and matching to enhancetweet comparability within treatment and control groups, butacknowledge that unobserved time-variant confounders may in \ufb02u-\nence posters \u2019responses. Third, although we have employed a\npopular bot detection algorithm, recent studies have suggestedthat algorithmic removal of bots is challenging and may introduceadditional bias\n54. Therefore, we report the full results with and\nwithout the algorithmic removal of bots, demonstrating that ourresults are consistent. To thoroughly remove bots, future researchcould match social media data with survey or administrative data(e.g., voter records) to ensure the authenticity of participants\n55.\nFourth, Twitter (X as of July, 2023) closed access to the AcademicResearch API, which had been freely available to eligible researchersuntil May 2023. This could limit other researchers \u2019ability to\nreproduce our \ufb01ndings with recent data after May 2023\n56. Collective\ntagging systems are increasingly being deployed across socialmedia platforms, such as Twitter \u2019s Community Notes and similar\nfeatures currently being tested on platforms like Facebook andYouTube\n23,24. Future research should examine whether our \ufb01ndings\nare reproducible across different platforms, time periods, and cul-tural contexts. Fifth, we employ the topic modeling and propensityscore weighting (PSW) method to control for semantic differencesbetween tweets tagged by individual and collective tagging (refer toSupplementary Method 5). Nevertheless, PSW might fail to addressthe confounding effects of unobserved semantic differences\nbeyond topics. Despite these limitations, our study uncovers a sig-\nni\ufb01cant and substantial relationship between fact-checks and\nreduced information diversity. We also demonstrate the power ofdesigned institutions, like collective fact-checking on Twitter, tomoderate the negative, narrowing effects of fact-checking oninformation exploration.\nMethods\nData\nOur study complied with the terms of all data sources used in the study(including but not limited to Twitter/X). Using the Twitter API v2.0 with\nacademic research access, we collected Twitter data to explore the\neffects of individual and collective misinformation tagging. First, weidenti \ufb01ed 9,372 users targeted by individual misinformation tagging\nfrom 2021/10/1 to 2022/3/25. We selected users whose tweets receivedfact-checking replies that contain URLs to fact-checking articles from\u201cpolitifact.com. \u201dSecond, we identi \ufb01ed 1,465 users targeted by collec-\ntive tagging from 2022/12/19 to 2023/3/31, when Community Noteswere made public to Twitter users globally\n57.I nC o m m u n i t yN o t e s ,\nusers can \ufb02ag any tweets as misinformation with notes, and other\nmembers vote for the helpfulness of the notes. (Users also have theoption to \ufb02ag tweets they believe are free from misinformation;\nhowever, these instances have been excluded from our analysis.)\nCollectively veri \ufb01ed notes that received the above-threshold help-\nfulness votes from a diverse set of users are then made public to theoriginal user (who posted the misinformation) and the broad Twitteraudience\n13. In our work, we only considered notes with above-\nthreshold helpfulness votes. Note that the platform also assesses thealignment of users \u2019prior contributions with the crowd \u2019sd e c i s i o n s ,\n\ufb01ltering out voters who frequently oppose and backlash against valid\nfact-checks on misinformation (see Supplementary Method 7).\nDue to the rate limit of Twitter API, we only collected data from\nregular Twitter users, excluding organizations \u2019and celebrities \u2019\naccounts with 50,000 or more followers. Additionally, to focus onindividual users, rather than organizational accounts (e.g., CNN, FoxNews, etc), we removed 1,659 users identi \ufb01ed as organization accounts\nby the M3Inference library\n58,59. We further removed 1445 users who\nwere fact-checked more than once within the period of data collectionto avoid the potential for them to become desensitized for repeatedfact-checks. After \ufb01ltering the data, our \ufb01nal dataset included 7733\nusers, where 6760 users were targeted by individual misinformationtagging and 973 users were targeted by collective misinformationtagging. We found that individual tagging is more frequent than col-\nlective tagging in our dataset due to the cross-validation process\nrequired to expose collective tags. This leads to an imbalance in groupsize between users corrected by individual and collective tags.Nevertheless, our statistical models (interrupted time series anddelayed feedback models) do not assume equal group size for com-parison between the effects of individual and collective tagging. Also,we found that 16.33% of tweets that received individual tags and 15.60%of tweets that received collective tags were removed by Twitter or bythe original poster. The probability of removal is similar betweenindividual and collective tags (Difference = 0.73%, z= 0.629, p=0 . 5 2 9 ) .\nFinally, we collected users \u2019historical tweets \u2014including posts,\nretweets, and quotes \u2014which span two months before posting tagged\ntweets and two months after exposure to misinformation tagging,resulting in 1,409,845 tweets in total. Posts typically indicate activeengagement with diverse political sources and topics, allowing users toexpress their opinions. In contrast, retweets and quotes \u2014which involve\nsharing others \u2019tweets \u2014suggest more passive engagement, not\nnecessarily re \ufb02ecting personal views. We utilize these three types of\nbehaviors for a more comprehensive measurement of users \u2019informa-\ntion engagement\n60,61. We assume that individual misinformation tag-\ngings are exposed to users when they are posted, and collectivemisinformation taggings are exposed to users when they are made\npublic following the above-threshold helpfulness votes. For our sta-\ntistical analyses, we included 712,948 tweets with observed politicaland content diversity scores. This research study received a determi-nation from the University of Chicago Social & Behavioral SciencesInstitutional Review Board that the study is not considered humansubjects research and does not require review (Institutional ReviewBoard Protocol IRB24-0051).\nPolitical diversity\nPolitical diversity measures whether a user posted a tweet that refer-enced sources having an opposite political stance. Speci \ufb01cally, we\ndetermine the political stance of the referenced source by extracting\nthe domain (e.g., cnn.com ) of the source and check it from MediaBias/\nFactCheck database (MBFC; https://mediabiasfactcheck.com/ )\n5,47.\nMBFC provides a continuous score for 4874 websites to indicate eachsource \u2019s political stance, ranging from -1 (extreme left) to 1 (extreme\nright). Our additional analysis shows that political stance scores fromMBFC show signi \ufb01cant inter-rater reliability with another database of\nthe political stance of news media, AllSides.com (see SupplementaryMethod 8).\nWe then calculate a user \u2019s political stance by averaging the poli-\ntical stance scores of sources referenced in their historical tweetswhich span two months before posting tagged tweets and two months\nafter misinformation tagging (see Supplementary Fig. 4). Users who\npredominantly cite left-leaning media are considered left, and thosewho cite right-leaning media are considered right. Speci \ufb01cally, users\nwith negative average political stance scores are categorized as left,while those with positive scores are categorized as right. Finally, weArticle https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 8\nassign a binary value to represent a user \u2019s political diversity: 1 (diverse)\nif a user cited a source that has an opposite political stance from theuser \u2019s own political stance, 0 (not-diverse) if a user cited a source with\nthe same political stance.\nThe mean political diversity score is 0.166, and the standard\ndeviation is 0.372 ( N= 712,948). Political diversity is negatively corre-\nlated with the number of tweets posted per day ( r(712946) = \u22120.107,\n95% CI = [ \u22120.109, \u22120.105], p< 0.001) and the proportion of retweets\n(r(712946) = \u22120.046, 95% CI = [ \u22120.048, \u22120.044], p< 0.001). This indi-\ncates that users are more active within information bubbles, activelyposting tweets rather than passively retweeting other users \u2019tweets\nwithin these bubbles (see Supplementary Table 2).\nContent diversity\nContent diversity measures whether a user posted a tweet with a topic\nthat is rarely discussed in the user \u2019s historical tweets. We apply the\nTwitter4SSE model, a transformer-based sentence embedding model(SentenceBERT) that was initialized from BERTweet (a RoBERTa modeltrained on 850 million tweets from 2012/1 to 2019/8 and 5 milliontweets related to COVID-19 pandemic), to encode the meaning of atweet into a 768-dimensional vector\n62,63. The model was further opti-\nmized based on recent data (75 million tweets from 2020/11 to 2020/12) using Multiple Negatives Ranking Loss (MNRL) to identify semanticsimilarity based on the principle that tweets quoting or replying to thesame original tweet are likely discussing related ideas\n62. If a pair of\ntweets quoted or replied to the same tweet, the semantic similarity\nbetween them is assumed to be high.\nTo apply the Twitter4SSE model, we \ufb01rst conduct the identical\ndata preprocessing steps to clean the tweets, which includes: eliminateURLs and mentions and transform the text to lowercase to reduce thepresence of generic texts\n62. Next, we represent each tweet with a 768-\ndimensional semantic embedding (Supplementary Fig. 5 shows thevisualization). Finally, we measure the cosine distance between theuser embedding and tweet embedding (see Fig. 1b) to represent the\ncontent diversity of the current tweet. The user embedding is theaverage embedding of the user \u2019s historical tweets (see Fig. 1b). Esti-\nmating the distance in the embedding space has been frequently used\nto quantify the diversity of user activities in the online platform\n48,64.\nThe distance ranges from 0 to 0.835, with 0 representing homo-geneous content and 0.835 representing extremely diverse content.The mean content diversity score is 0.357, and the standard deviationis 0.109 ( N= 712,948). We \ufb01nd that political and content diversity are\nslightly correlated ( r(712946) = 0.020, 95% CI = [0.018, 0.022],\np< 0.001), assessing conceptually distinct aspects of diversity.\nTable 1shows an example of how content diversity scores are\nassigned. In this example, the user primarily shows interests in COVID-19 related misinformation. However, as the user explores diversetopics \u2014tax, LGBTQ + , international issues, and labor \u2014the content\ndiversity score increases.\nContent diversity is negatively correlated with the number of\ntweets posted per day ( r(712946) = \u22120.052, 95% CI = [ \u22120.054, \u22120.050],\np< .001) but positively correlated with the proportion of retweets\n(r(712946) = 0.012, 95% CI = [0.010, 0.014], p< 0.001). In other words,\nusers tend to retweet others \u2019tweets rather than posting their own\ntweets when increasing content diversity (see Supplementary Table 2).\nInterrupted time series (ITS) analysis\nWe apply Interrupted Time Series (ITS) analysis to examine how indi-vidual and collective misinformation tagging affect the trend of poli-tical and content diversity in posting behavior. We \ufb01tt h eI T Sm o d e lt o\nthe time series around fact-checking events, spanning \ufb01ve weeks\n(35 days) before posting the fact-checked tweet and \ufb01ve weeks after\nfact-checking. To compare the differential impacts of individual andcollective misinformation tagging, we formulate the following multi-group ITS model. We control for user \ufb01xed effects to eliminate theuser-related unobserved time-invariant heterogeneity that could pos-\nsibly affect the outcomes. Additionally, the number of tweets postedper day is negatively correlated with political diversity(r(712946) = \u22120.107, 95% CI = [ \u22120.109, \u22120.105], p< 0.001) and content\ndiversity ( r(712946) = \u22120.052, 95% CI = [ \u22120.054, \u22120.050], p<0 . 0 0 1 ) ,\nindicating that users are more active within information bubbles.Therefore, we control for the number of tweets posted per day toensure that our analysis focuses on variations in diversity rather thanengagement volume.\nFor each tweet, let Ybe the outcome variable (i.e., political or\ncontent diversity of a speci \ufb01c tweet), Wis the weeks before posting the\ntweet with misinformation (negative values) or after misinformationtagging (positive values). Note that we measure Wby dividing the days\nby 7. For example, if a particular tweet is posted 3 days before postingthe tweet, Wis\u22123/7.Tis an indicator of the treatment status where 0\nrepresents a tweet posted before misinformation tagging and 1\nrepresents after tagging. Cis an indicator of the type of misinformation\ntagging where 0 represents individual tagging and 1 represents col-lective tagging. Ncorresponds to the number of tweets per day (con-\ntrol variable), \u03b1corresponds to the user \ufb01xed effect, and and \u03f5is the\nerror term. Then the ITS model is de \ufb01ned:\nY=\u03b2\n0+\u03b21W+\u03b22T+\u03b23WT+\u03b24WC+\u03b25TC+\u03b26WTC +\u03b27N+\u03b1+\u03f5\u00f01\u00de\nHere, \u03b20is the intercept, \u03b21is the slope before individual mis-\ninformation tagging. \u03b22is the change in the outcome immediately after\nthe individual misinformation tagging. \u03b23is the slope change before\nand after individual misinformation tagging. \u03b21+\u03b24is the slope before\ncollective misinformation tagging. \u03b22+\u03b25is the change in the outcome\nimmediately after the collective misinformation tagging. \u03b23+\u03b26is the\nslope change before and after collective misinformation tagging. Thus,\n\u03b24,\u03b25,\u03b26are the terms that estimate the differences between the\neffects of individual and collective misinformation tagging. Supple-mentary Table 12 shows how these estimates correspond to each cell inTable 1for each outcome.\nBefore estimating the model, political diversity (binary variable)\nhas been multiplied by 100 so that the coef \ufb01cients are interpretable as\nabsolute percentage point changes. Content diversity has been nor-malized to z-scores (i.e., the number of standard deviations from the\nmean). When estimating the statistical signi \ufb01cance of the estimates, all\np-values are two-sided. The thresholds for statistical signi \ufb01cance is set\natp< 0.05, and marginal signi \ufb01cance is set at p<0 . 1 .\nDelayed feedback (DF) analysis\nIn addition to the interrupted time series (ITS) analysis, we conduct a\ndelayed feedback (DF) analysis to estimate the causal impacts. Webegin by establishing control and treatment groups: each tweet ispaired with another tweet that was subject to misinformation taggingat an earlier time. Speci \ufb01cally, for every tweet in a control group, we\nsearch for a corresponding treatment tweet using the following cri-teria: (1) They must have been fact-checked using the same approach,either individual or collective misinformation tagging. (2) They should\nhave been fact-checked prior to the control tweet. (3) They should\nhave the same topic, considering that distinct topics of misinformationcould lead to different levels of political and content diversity (see\u201cMethod \u201d: Topic Modeling for a detailed explanation of the topic\nmodeling process). (4) They should have been posted no more thanseven days apart from the control tweet. In cases where we havemultiple tweets that meet these criteria, we choose the one with theclosest posting time to the control tweet. This results in 476 pairs oftweets in control and treatment groups.\nFor each pair of tweets, we identify two time windows: pre-\ntreatment ( t\n0) and post-treatment ( t1).t1represents the time window\nwhen the treatment tweets are fact-checked but the control tweets are\nnot. If the duration of t1exceeds a seven-day window, we use the dataArticle https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 9\nwithin the seven-day window after receiving the tags, considering that\nthe timing of the fact-check could affect the outcome. t0represents the\ntime window (with equal duration of t1) when both the treatment and\ncontrol tweets are not fact-checked. Then we design the following\ndifference-in-differences model to assess the impacts of misinforma-\ntion tagging.\nFor each tweet, let Ybe the outcome variable (i.e., political or\ncontent diversity of a speci \ufb01ct w e e t ) . Tis a binary variable indicating\nwhether the treatment tweet, but not control tweet, receives thetreatment (i.e., misinformation tagging). Gis a binary variable indi-\ncating whether the tweet is assigned in the treatment (i.e., 1) or controlgroup (i.e., 0). Cis an indicator of the type of misinformation tagging\nwhere 0 represents individual tagging and 1 represents collectivetagging. Ncorresponds to the number of tweets per day (control\nvariable), \u03b1corresponds to the user \ufb01x e de f f e c t ,a n da n d \u03f5is the error\nterm.\nY=\u03b2\n0+\u03b21TG+\u03b22TGC +\u03b23T+\u03b24TC+\u03b25N+\u03b1+\u03f5 \u00f02\u00de\n\u03b20is the intercept. \u03b21is the difference in pre-post change in the\noutcome between the control and treatment group for individualmisinformation tagging. \u03b2\n1+\u03b22is the difference in pre-post change for\ncollective misinformation tagging. Thus, \u03b22is the term that estimates\nthe difference between the effects of individual and collective mis-information tagging. \u03b2\n3and\u03b24account for the baseline changes in the\noutcomes. Supplementary Table 13 shows how these estimates cor-respond to each cell in Table 2for each outcome.\nLike ITS models, political diversity (binary variable) has been\nmultiplied by 100 so that the coef \ufb01cients are interpretable as absolute\npercentage point changes. Content diversity has been normalized to\nz-scores (i.e., the number of standard deviations from the mean). When\nestimating the statistical signi \ufb01cance of these coef \ufb01cients, all p-values\nare two-sided. The thresholds for statistical signi \ufb01cance is set at\np< .05, and marginal signi \ufb01cance is set at p<0 . 1 .\nSupplementary Fig. 6 illustrates day-to-day changes in the out-\ncome variables (political and content diversity) for the control andtreatment groups over the pre-intervention period, controlling fortime-invariant differences across users (i.e., user \ufb01xed effects). Sup-\nplementary Fig. 6 suggests that control and treatment groups followsimilar trends in the absence of the tagging intervention. To statisti-cally test this, we \ufb01tt a linear model: Diversity =\u03b2\n0+\u03b21Day +\n\u03b22IsTreatmentGroup + \u03b23Day \u22c5IsTreatmentGroup +\u03b5,w h e r e Dayis the\nnumber of days in the matched pre-intervention period, and IsTreat-\nmentGroup is 1 for the treatment group and 0 for the control group. If\nthe parallel trends assumption holds, we would \ufb01nd a non-signi \ufb01cant\nslope difference ( \u03b23). Our results show no signi \ufb01cant slope difference\nfor both political diversity ( \u03b23=\u22120.493, 95% CI = [ \u22121.644, 0.725],\nt(3) = \u22120.864, p= .408) and content diversity ( \u03b23= 0.011, 95% CI =\n[\u22120.036, 0.058], t(3) = 0.511, p= 0.620) (see Supplementary Table 14).\nSupplementary Fig. 7 illustrates average baseline changes of\npolitical and content diversity obtained from the control group in theDF analysis. According to the baseline changes, we \ufb01nd that political\ndiversity signi \ufb01cantly increases ( \u03b2= 1.956, 95% CI = [0.068, 3.843],\nt(8182) = 2.03, p= 0.042), but content diversity does not signi \ufb01cantly\ndecrease ( \u03b2=\u22120.039, 95% CI = [ \u22120.079, 0.002], t(8182) = \u22121.88,\np= 0.060) if the problematic tweet is not tagged. In other words, we\n\ufb01nd that political diversity consistently increases over time without\ntagging in the control group. On the other hand, we \ufb01nd that content\ndiversity does not signi \ufb01cantly change. Our results show that the\neffects of individual and collective tagging are above and beyond thesebaseline changes (see Results: Delayed Feedback (DF) Analysis).\nTopic modeling\nWe apply BERTopic to extract latent topics from tweets that receivedmisinformation tags\n65.S p e c i \ufb01cally, we \ufb01rst represent each tweet with a768-dimensional semantic embedding using Twitter4SSE. Then, we\nmap the embeddings to a 5-dimensional space via UMAP (UniformManifold Approximation and Projection) to mitigate the curse ofdimensionality\n66,67. Next, we apply HDBSCAN (Hierarchical Density-\nBased Spatial Clustering of Applications with Noise) to identify clusters\nof topics68. Unlike k-means algorithms, HDBSCAN does not require the\nuser to pre-specify the number of clusters, and HDBSCAN is adept atidentifying and handling noise, distinguishing between topics andoutliers, which is crucial for maintaining the integrity of the clusteredtopics.\nTraditional methods such as LDA extract topics based on bag-of-\nwords and often fall short when applied to short texts like tweets\n69.\nBERTopic emerges as particularly advantageous for analyzing datafrom Twitter and it preserves the semantic structure of the text\n63,t h u s\nenhancing its ability for short-text analysis compared to traditional\nmodels.\nWe generate 23 topics for 6660 fact-checked tweets, and 1073\ntweets are not assigned any topic and thus considered as outliers.These outliers are excluded from the process of assigning tweets intocontrol and treatment groups in the delayed feedback (DF) analysis.Most frequent topics with the keywords are shown in the Supple-mentary Table 7. As shown in Supplementary Table 7, the nine mostfrequent topics in our dataset include political topics that are known totrigger divisive, polarized reactions in US politics, such as COVID-19vaccine-related misinformation (Topic 1), election- and politician-related misinformation (Topic 4, 5, 7, 8), policy-related misinformation\n(Topic 3, 6), and environment and disaster-related misinformation\n(Topic 9). These topics account for 84.06% of the corrections madethrough individual tagging and 59.49% of the corrections madethrough collective tagging. Given the time period of collection, themost frequent topic is about COVID-19 pandemic and vaccination.\nLinguistic characteristics of misinformation tagging messages\nFor each misinformation tagging, we analyze the message \u2019s toxicity,\nsentiment, length, reading ease, and delayed response time to provideinsights into the qualitative differences between individual and col-lective misinformation tagging. Supplementary Table 4 shows the\ndescriptive statistics of the following variables.\n\u0081Toxicity: We apply Google Jigsaw Perspective API to measure the\nprobability that a particular message is toxic (range from 0to 1\n10,70).\n\u0081Sentiment: We conduct Vader sentiment analysis to estimate\nsentiment scores of messages (on a [ \u22121, 1] scale14,71). The scale\nspans from -1, denoting negative sentiment, to 1, denoting posi-tive sentiment.\n\u0081L e n g t h :W em e a s u r et h el e n g t ho fm e s s a g e sb a s e do nt h en u m b e r\nof characters\n14.\n\u0081Reading ease score: We evaluate the readability of messages with\nthe Flesch-Kincaid Reading Ease score (on a [1,100] scale, where\nlarge value indicates easier readability14,72). The Flesch \u2013Kincaid\nreading ease score was transformed into an 8-level categoricalvariable: \u201c5th grade \u201dfor scores 100 \u201390, \u201c6th grade \u201dfor 90 \u201380,\n\u201c7th grade \u201dfor 80 \u201370, \u201c8th & 9th grade \u201dfor 70 \u201360, \u201c10th to 12th\ngrade \u201dfor 60 \u201350, \u201cCollege \u201dfor 50 \u201330, \u201cCollege graduate \u201dfor\n30\u201310, and \u201cProfessional \u201dfor 10 \u20130.\n\u0081Delayed response time: We calculate it as the number of days\nbetween original tweets and misinformation tagging.\nRobustness checks\nWe verify our \ufb01ndings with a battery of robustness checks. First, there\nmight be concerns that our conclusions about the effects of mis-\ninformation tagging on human users may be biased by the presence ofbots on Twitter. Many studies have utilized bot detection algorithms toexclude users who are likely to be bots to address this concern\n2,5,b u t\nothers argue these algorithms lead to false negatives (i.e., botsArticle https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 10\nmisclassi \ufb01ed as humans) and positives (i.e., humans misclassi \ufb01ed as\nbots) that could further bias analyses, even when used cautiously54.T o\nmitigate concerns of bot prevalence, we reanalyze our data excludingaccounts identi \ufb01ed as bots using BotometerLite API. Speci \ufb01cally, using\nBotometerLite API, we evaluate the likelihood of users in our dataset\nbeing bot accounts and remove 360 accounts that have a likelihoodhigher than 50%\n73, which do not meaningfully change our results. In\ninterrupted time series (ITS) models, after individual tagging, bothpolitical diversity ( \u03b2=\u22120.882, 95% CI = [ \u22121.326, \u22120.438],\nt(394654) = \u22123.89, p< 0.001) and content diversity ( \u03b2=\u22120.026, 95%\nCI = [ \u22120.037, \u22120.014], t(394654) = \u22124.27, p< 0.001) signi \ufb01cantly\ndecrease. After collective tagging, content diversity signi \ufb01cantly\nincreases ( \u03b2= 0.052, 95% CI = [0.020, 0.084], t(394654) = 3.16,\np= 0.002) (refer to Supplementary Table 15 and 10). In delayed feed-\nback (DF) models, the effects of individual tagging on political diver-\nsity ( \u03b2=\u22125.348, 95% CI = [ \u22129.154, \u22121.542], t(7766) = \u22122.75, p=. 0 0 6 )a n d\ncollective tagging on content diversity are signi \ufb01cant ( \u03b2=0 . 2 9 2 ,9 5 %\nCI = [0.149, 0.435], t(7766) = 4.00, p< 0.001) (refer to Supplementary\nTable 17).\nIn terms of applying BotometerLite API, some features are missing\nin our dataset collected with Twitter API 2.0: (1) default_pro \ufb01le(whe-\nther the user altered the theme or background of their user pro \ufb01le); (2)\npro\ufb01le_use_background_image (whether the user has a background\nimage or not); and (3) favourites_count (number of likes posted by the\nuser, which were only available in Twitter API 1.1). To address this issue,we conduct missing data imputation with the IterativeImputer in\nSklearn. We train an imputation model with 90,000 tweets randomly\nselected in August 2021 from the Twitter Stream Grab ( https://archive.\norg/details/twitterstream ). We then evaluate the model with a held-out\nsample of 10,000 tweets. The model performance for predicting themissing features is as follows: default_pro \ufb01le at 0.95 F-1score, pro \ufb01-\nle_use_background_image at 0.90 F-1\nscore, favourites_count was 0.10\nR2value. Finally, we apply this imputation model to recover the missing\nfeatures in our dataset.\nSecond, we control for potentially insincere informational\nactivities, such as citing sources of low credibility and intentionallyspreading fake news. Some might question whether the increase in\npolitical and content diversity is associated with these insincere\nactivities. Put simply, users might be engaging with diverse infor-mation that includes misleading claims and conspiracy theories. Foreach tweet posted by each poster, we measure the credibility of thereferenced source. Speci \ufb01cally, we use the binary credibility scores\n(1 = low credibility; 0=medium or high credibility) from the Media-Bias/FactCheck database. Our analysis indicates a strong negativecorrelation between the engagement of low-credibility sources andmeasures of political diversity ( r=\u22120.227, 95% CI = [ \u22120.229,\n\u22120.225], p< 0.001) and content diversity ( r=\u22120.030, 95% CI = [ \u2212\n0.032, \u22120.028], p< 0.001). This implies that the increase of diversity\nin information engagement re \ufb02ects engagement with a healthier\ninformation ecosystem, rather than the reverse. Furthermore, wereassess our data while controlling for credibility of sources, and\ufb01nd that our results remain unaffected. In ITS models, after indivi-\ndual tagging, both political diversity ( \u03b2=\u22120.972, 95% CI = [ \u22121.395,\n\u22120.549], t(418114) = \u22124.50, p< 0.001) and content diversity\n(\u03b2=\u22120.030, 95% CI = [ \u22120.042, \u22120.018], t(418114) = \u22125.08, p< 0.001)\nsigni\ufb01cantly decrease. After collective tagging, content diversity\nsigni\ufb01cantly increases ( \u03b2= .041, 95% CI = [0.012, 0.070],\nt(418114) = 2.76, p= 0.006) (refer to Supplementary Table 15 and 10).\nIn DF models, the effects of individual tagging on political diversity(\u03b2= -4.958, 95% CI = [ \u22128.591, \u22121.324], t(8181) = \u22122.67, p= 0.007) and\ncollective tagging on content diversity are signi \ufb01cant ( \u03b2= 0.279, 95%\nCI = [0.150, 0.407], t(8181) = 4.25, p< 0.001) (refer to Supplementary\nTable 17). This implies that posters \u2019\nexploration prior to being tag-\nged was likely well-intentioned and would have been ef \ufb01cacious had\nthey not been prompted to retreat.Third, we attempt to avoid situations in which posters simply\ncriticize distant information without honest consideration by \ufb01ltering\nout tweets with negative sentiment. For each tweet posted by eachposter, we conduct Vader sentiment analysis to estimate sentiment\nscores (on a [ \u22121, 1] scale\n14,71). Then we exclude tweets that have a sen-\ntimental score lower than 0, which do not meaningfully change ourresults, except for making the immediate change of content diversityafter collective misinformation tagging not signi \ufb01cant in ITS models\n(\u03b2=0 . 0 0 9 ,9 5 %C I=[ \u22120.031, 0.048], t(262015) = 0.43, p= 0.666). In ITS\nmodels, after individual tagging, both political diversity ( \u03b2=\u22121.470,\n95% CI = [-2.032, -.909], t(262015) = -5.13, p< .001) and content diver-\nsity ( \u03b2=\u22120.027, 95% CI = [ \u22120.042, \u22120.011], t(262015) = \u22123.35, p= 0.001)\nsigni\ufb01cantly decrease (refer to Supplementary Table 15 and 10). In DF\nmodels, the effects of individual tagging on political diversity(\u03b2=\u22128.075, 95% CI = [ \u221212.757, \u22123.394], t(4973) = \u22123.38, p=0 . 0 0 1 )a n d\ncollective tagging on content diversity are signi \ufb01cant ( \u03b2=0 . 2 9 4 ,9 5 %\nCI = [0.114, 0.474], t(4973) = 3.20, p= 0.001) (refer to Supplementary\nTable 17).\nFourth, we address concerns regarding the possibility of mis-\ncoding mentions of \u201ccommunity note. \u201dSpeci \ufb01cally, we identify all\ntweets that mention keywords about receiving community notesbroadly (i.e., community note, birdwatch, fact-check, factcheck, poli-tifact) within the sample and remove those tweets. To address theconcern regarding the effect of replying back to individual taggers, weidentify all tweets that reply directly to the individual taggers andremove them. We \ufb01nd that the effects on political and content diver-\nsity do not meaningfully change in both ITS and DF analyses. In ITS\nmodels, after individual tagging, both political diversity ( \u03b2=\u22121.048,\n95% CI = [ \u22121.488, \u22120.608], t(416183) = \u22124.67, p< 0.001) and content\ndiversity ( \u03b2=\u22120.030, 95% CI = [ \u2212\n0.041, \u22120.018], t(416183) = \u22125.00,\np< 0.001) signi \ufb01cantly decrease. After collective tagging, content\ndiversity signi \ufb01cantly increases ( \u03b2= 0.041, 95% CI = [0.012, .069],\nt(416183) = 2.75, p= 0.006) (refer to Supplementary Table 18). In DF\nmodels, the effects of individual tagging on political diversity(\u03b2=\u22125.329, 95% CI = [ \u22129.139, \u22121.519], t(8088) = \u22122.74, p= .006) and\ncollective tagging on content diversity are signi \ufb01cant ( \u03b2=0 . 2 7 6 ,9 5 %\nCI = [0.148, 0.405] t(8088) = 4.22, p< 0.001) (refer to Supplementary\nTable 19).\nFifth, to strictly identify individual tags (i.e., PolitiFact links) that\ncorrect the original posters, we submit original posts, replies con-taining PolitiFact links, and the cited PolitiFact fact-checking articles toChatGPT (gpt-4o-2024-05-13). We prompt the model to annotatewhether the PolitiFact link was used to correct the original posterrather than support them. Consequently, we identify 5592 PolitiFactl i n k so u to f6 7 6 0l i n k s( 8 2 . 7 2 % )a sc o r r e c t i v e( s e eS u p p l e m e n t a r yMethod 9). Subsequently, we limit the sample to the 5592 links iden-ti\ufb01ed by ChatGPT from the individual tagging data, which does not\nmeaningfully alter the results. In ITS models, after individual tagging,\nboth political diversity ( \u03b2=\u22121.086, 95% CI = [ \u22121.558, \u22120.614],\nt(363273) = \u22124.51, p< 0.001) and content diversity ( \u03b2=\u22120.039, 95%\nCI = [ \u22120.051, \u22120.026], t(363273) = \u22126.11, p< 0.001) signi \ufb01cantly\ndecrease. After collective tagging, content diversity signi \ufb01cantly\nincreases ( \u03b2= 0.041, 95% CI = [0.012, 0.069], t(416183) = 2.75, p=. 0 0 6 )\n(refer to Supplementary Table 20). In DF models, the effects of indi-vidual tagging on political diversity ( \u03b2=\u22127.135, 95% CI = [ \u221211.068,\n\u2212\n3.203], t(7508) = \u22123.56, p< 0.001) and collective tagging on content\ndiversity are signi \ufb01cant ( \u03b2=0 . 2 7 2 ,9 5 %C I=[ 0 . 1 4 3 ,. 4 0 1 ] t(7508) = 4.13,\np< 0.001) (refer to Supplementary Table 21).\nSixth, we restrict the sample to original posters who have replied\nto (and thereby read) the individual tags (i.e., fact-checking replies)\nand remove non-responders. Speci \ufb01cally, out of 6760 original posters\nwho received individual tags, we remove 4288 posters who did notreply to the tags, resulting in 2472 posters. After that, we comparethese 2472 posters with 973 posters who received collective tags. Even\nafter removing the non-responders, we \ufb01nd that results regardingArticle https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 11\ntagging \u2019se f f e c t sr e m a i nc o n s i s t e n t .S p e c i \ufb01cally, as with the complete\nsample, we identically \ufb01nd that individual tagging causes immediate\ndecrease in political and content diversity. After individual tagging,both political diversity ( \u03b2=\u22121.481, 95% CI = [ \u22122.337, \u22120.626]\nt(164724) = \u22123.39, p= 0.001) and content diversity ( \u03b2=\u22120.026, 95%\nCI = [ \u22120.048, \u22120.004], t(164724) = \u22122.30, p=0 . 0 2 1 ) s i g n i \ufb01cantly\ndecrease. After collective tagging, content diversity signi \ufb01cantly\nincreases ( \u03b2= .040, 95% CI = [.010, .071], t(164724) = 2.59, p= 0.010)\n(refer to Supplementary Table 22)\nReporting summary\nFurther information on research design is available in the NaturePortfolio Reporting Summary linked to this article.\nData availability\nThe Twitter data collected and analyzed in this study have beendeposited in the Open Science Foundation (OSF) database at https://\ndoi.org/10.17605/OSF.IO/TXGSR . The data required to replicate our\nanalyses are available in the repository. However, in accordance withTwitter \u2019s privacy policy, we cannot disclose individual-level user\ninformation or the contents of tweets. Instead, processed and anon-ymized data are available in the repository.\nCode availability\nThe code required to replicate our results is available at https://doi.\norg/10.17605/OSF.IO/TXGSR .\nReferences\n1. Lazer, D. M. J. et al. The science of fake news. Science 359,\n1094 \u20131096 (2018).\n2. Vosoughi, S., Roy, D. & Aral, S. The spread of true and false news\nonline. Science 359,1 1 4 6 \u20131151 (2018).\n3. Loomba, S., de Figueiredo, A., Piatek, S. J., de Graaf, K. & Larson, H.\nJ. Measuring the impact of COVID-19 vaccine misinformation onvaccination intent in the UK and USA. Nat. Hum. Behav. 5,\n337 \u2013348 (2021).\n4. Green, J., Hobbs, W., McCabe, S. & Lazer, D. Online engagement\nwith 2020 election misinformation and turnout in the 2021 Georgia\nrunoff election. P r o c .N a t lA c a d .S c i .U S A 119, e2115900119 (2022).\n5. Bovet, A. & Makse, H. A. In \ufb02u e n c eo ff a k en e w si nT w i t t e rd u r i n gt h e\n2016 US presidential election. Nat. Commun. 10,7( 2 0 1 9 ) .\n6 . H a n n a k ,A . ,M a r g o l i n ,D . ,K e e g a n ,B .&W e b e r ,I .G e tb a c k !Y o ud o n \u2019t\nknow me like that: The social mediation of fact checking interven-tions in Twitter conversations. Proc. Int. AAAI Conf. Web Soc. Media\n8,1 8 7 \u2013196 (2014).\n7. Shin, J. & Thorson, K. Partisan selective sharing: the biased diffusion\nof fact-checking messages on social media. J. Commun. 67,\n233 \u2013255 (2017).\n8. Pilarski, M., Solovev, K. & Pr\u00f6ll ochs, N. Community notes vs. snop-\ning: how the crowd selects fact-checking targets on social media.Proc. Int. AAAI Conf. Web Soc. Media 18,1 2 6 2 \u20131275 (2023).\n9 . M i c a l l e f ,N . ,H e ,B . ,K u m a r ,S . ,A h a m a d ,M .&M e m o n ,N .T h er o l eo f\nthe crowd in countering misinformation: a case study of the COVID-19 infodemic. in 2020 IEEE International Conference on Big Data\n.https://doi.org/10.1109/bigdata50022.2020.9377956 (IEEE, 2020).\n10. Mosleh, M. & Rand, D. G. Measuring exposure to misinformation\nfrom political elites on Twitter. Nat. Commun. 13, 7144 (2022).\n11. Rhodes, S. C. Filter bubbles, echo chambers, and fake news: how\nsocial media conditions individuals to be less critical of politicalmisinformation. Polit. Commun. 39,1\u201322 (2022).\n12. Bhadani, S. et al. Political audience diversity and news reliability in\nalgorithmic ranking. Nat. Hum. Behav. 6, 495 \u2013505 (2022).\n13. Wojcik, S. et al. Birdwatch: Crowd Wisdom and Bridging Algorithms\ncan Inform Understanding and Reduce the Spread of Misinforma-tion. Preprint at https://arxiv.org/abs/2210.15723 (2022).1 4 . A l l e n ,J . ,M a r t e l ,C .&R a n d ,D .G . Birds Of A Feather Don \u2019t Fact-check\nEach Other: Partisanship And The Evaluation Of News In Twitter \u2019s\nBirdwatch Crowdsourced Fact-checking Program .i nProceedings of\nthe 2022 CHI Conference on Human Factors in Computing Systems\n1\u201319 (Association for Computing Machinery, 2022).\n15. Clayton, K. et al. Real solutions for fake news? Measuring the\neffectiveness of general warnings and fact-check tags in reducingbelief in false stories on social media. Polit. Behav. 42,\n1073 \u20131095 (2020).\n16. Nyhan, B. & Rei \ufb02er, J. When corrections fail: the persistence of\npolitical misperceptions. Polit. Behav. 32,3 0 3 \u2013330 (2010).\n17. Bail, C. A. et al. Exposure to opposing views on social media can\nincrease political polarization. Proc. Natl Acad. Sci. USA 115,\n9216 \u20139221 (2018).\n18. Mosleh, M., Martel, C., Eckles, D. & Rand, D. Perverse Downstream\nConsequences of Debunking: Being Corrected by Another User for\nPosting False Political News Increases Subsequent Sharing of LowQuality, Partisan, and Toxic Content in a Twitter Field Experiment. in\nProceedings of the 2021 CHI Conference on Human Factors inComputing Systems 1\u201313 (Association for Computing Machin-\nery, 2021).\n19. Swire-Thompson, B., DeGutis, J. & Lazer, D. Searching for the\nback\ufb01re effect: measurement and design considerations. J. Appl.\nRes. Mem. Cogn. 9,2 8 6 \u2013299 (2020).\n2 0 . W o o d ,T .&P o r t e r ,E .T h ee l u s i v eb a c k \ufb01re effect: mass attitudes \u2019\nsteadfast factual adherence. Polit. Behav. 41,1 3 5 \u2013163 (2019).\n21. Jiang, S. & Wilson, C. Linguistic signals under misinformation and\nfact-checking: evidence from user comments on social media.Proc. ACM Hum. -Comput. Interact. 2,1\u201323 (2018).\n22. Masullo, G. M. & Kim, J. Exploring \u2018angry \u2019and \u2018like \u2019reactions on\nuncivil facebook comments that correct misinformation in thenews. Digital Journalism 9, 1103 \u20131122 (2021).\n23. YouTube. Testing new ways to offer viewers more context and\ninformation on videos. YouTube Of \ufb01cial Blog https://blog.\nyoutube/news-and-events/new-ways-to-offer-viewers-more-context/ (2024).\n24. Kaplan, J. More Speech and Fewer Mistakes. Meta Newsroom\nhttps://about.fb.com/news/202 5/01/meta-more-speech-fewer-\nmistakes/ (2025).\n25. Page, S. E. The Difference: How the Power of Diversity Creates Better\nGroups, Firms, Schools, and Societies - New Edition .( P r i n c e t o n\nUniversity Press, 2008).\n26. Page, S. E. The Diversity Bonus: How G reat Teams Pay Off in the\nKnowledge Economy . (Princeton University Press, 2019).\n27. Saeed, M., Traub, N., Nicolas, M., Demartini, G. & Papotti, P.\nCrowdsourced Fact-Checking at Twitter: How Does the CrowdCompare With Experts? in Proceedings of the 31s t ACM International\nConference on Information & Knowledge Management 1736 \u20131746\n(Association for Computing Machinery, 2022).\n28. Allen, J., Arechar, A. A., Pennycook, G. & Rand, D. G. Scaling up fact-\nchecking using the wisdom of crowds. Sci. Adv. 7, eabf4393 (2021).\n2 9 . M a r t e l ,C . ,A l l e n ,J . ,P e n n y c o o k ,G .&R a n d ,D .G .C r o w d sc a n\neffectively identify misinformation at scale. Perspect. Psychol. Sci .\n19,4 7 7 \u2013488 (2023).\n30. Twitter. Values. Community Notes Guide https://communitynotes.\ntwitter.com/guide/en/contributing/values (2023).\n31. Jamieson, K. H. & Cappella, J. N. Echo Chamber: Rush Limbaugh and\nthe Conservative Media Establishment .( O x f o r dU n i v e r s i t y\nPress, 2008).\n32. Muller, D. Democracy Under Strain .i nJournalism and the Future of\nDemocracy 9-25 (Palgrave Macmillan, Cham, 2022).\n33. T\u00f6rnberg, P. Echo chambers and viral misinformation: modeling\nfake news as complex contagion. PLoS One 13,e 0 2 0 3 9 5 8( 2 0 1 8 ) .\n34. Del Vicario, M. et al. The spreading of misinformation online. Proc.\nNatl Acad. Sci. USA 113,5 5 4 \u2013559 (2016).Article https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 12\n35. Flamino, J. et al. Political polarization of news media and in \ufb02uencers\non Twitter in the 2016 and 2020 US presidential elections. Nat. Hum.\nBehav. 7,9 0 4 \u2013916 (2023).\n3 6 . S a m o r y ,M . ,A b n o u s i ,V .K .&M i t r a, T. Characterizing the social\nmedia news sphere through user co-sharing practices. Proc. Int.\nAAAI Conf. Web Soc. Media 14,6 0 2 \u2013613 (2020).\n37. Bessi, A. et al. Homophily and polarization in the age of mis-\ninformation. Eur. Phys. J. Spec. Top. 225,2 0 4 7 \u20132059 (2016).\n3 8 . C i n e l l i ,M . ,D eF r a n c i s c iM o r a l e s ,G . ,G a l e a z z i ,A . ,Q u a t t r o c i o c c h i ,W .\n& Starnini, M. The echo chamber effect on social media. Proc. Natl\nA c a d .S c i .U S A 118, e2023301118 (2021).\n39. Bode, L. Political news in the news feed: learning politics from social\nmedia. Mass Commun. Soc. 19,2 4 \u201348 (2016).\n4 0 . C o n o v e r ,M .D . ,G o n c a l v e s ,B . ,R a t k i e w i c z ,J . ,F l a m m i n i ,A .&\nMenczer, F. Predicting the Political Alignment of Twitter Users. in\n2011 IEEE Third International Conference on Privacy, Security, Risk\nand Trust and 2011 IEEE Third International Conference on SocialComputing 192 \u2013199 (IEEE, 2011).\n41. Luzsa, R. & Mayr, S. False consensus in the echo chamber: Exposure\nto favorably biased social media news feeds leads to increasedperception of public support for own opinions. Cyberpsychology\n15, 3 (2021).\n4 2 . C o o k e ,N .A . Fake News and Alternative Facts: Information Literacy in\na Post-Truth Era . (American Library Association, 2018).\n43. DellaPosta, D., Shi, Y. & Macy, M. Why Do Liberals Drink Lattes? Am.\nJ. Sociol. 120,1 4 7 3 \u20131511 (2015).\n44. Mutz, D. C. & Rao, J. S. The Real Reason Liberals Drink Lattes. PS\nPolit. Sci. Polit. 51,7 6 2 \u2013767 (2018).\n45. Shi, F., Shi, Y., Dokshin, F. A., Evans, J. A. & Macy, M. W. Millions of\nonline book co-purchases reveal partisan differences in the con-sumption of science. Nat. Hum. Behav. 1,1\u20139( 2 0 1 7 ) .\n46. Milbauer, J., Mathew, A. & Evans, J. A. Aligning multidimensional\nworldviews and discovering id eological differences. in Proceedings\nof the 2021 Conference on Empirical Methods in Natural LanguageProcessing 4832 \u20134845 (Association for Computational Linguis-\ntics, 2021).\n4 7 . G a l l o t t i ,R . ,V a l l e ,F . ,C a s t a l d o ,N . ,S a c c o ,P .&D eD o m e n i c o ,M .\nAssessing the risks of \u2018infodemics \u2019in response to COVID-19 epi-\ndemics. Nat. Hum. Behav. 4,1 2 8 5 \u20131293 (2020).\n48. Anderson, A., Maystre, L., Anderson, I., Mehrotra, R. & Lalmas, M.\nAlgorithmic Effects on the Diversi ty of Consumption on Spotify. in\nProceedings of the 2020 World Wide Web Conference 2155 \u20132165\n(Association for Computing Machinery, 2020).\n49. Srinivasan, K. B., Danescu-Niculescu-Mizil, C., Lee, L. & Tan, C.\nContent Removal as a Moderation Strategy: Compliance and OtherOutcomes in the ChangeMyView Community. Proc. ACM Hum.\n-Computer Interact. 3,1\u201321 (2019).\n50. Thaler, R. H. & Sunstein, C. R. Nudge: The Final Edition .\n(Penguin, 2021).\n51. Jones-Jang, S. M., Mortensen, T. & Liu, J. Does Media Literacy Help\nIdenti \ufb01cation of Fake News? Information Literacy Helps, but Other\nLiteracies Don \u2019t.A m .B e h a v .S c i . 65,3 7 1 \u2013388 (2021).\n52. Becker, J., Porter, E. & Centola, D. The wisdom of partisan crowds.\nP r o c .N a t lA c a d .S c i .U S A 116,1 0 7 1 7 \u201310722 (2019).\n53. Shi, F., Teplitskiy, M., Duede, E. & Evans, J. A. The wisdom of\npolarized crowds. Nat. Hum. Behav. 3,3 2 9 \u2013336 (2019).\n54. Rauch \ufb02eisch, A. & Kaiser, J. The False positive problem of automatic\nbot detection in social science research. PLoS One 15,\ne0241045 (2020).\n55. Grinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B. & Lazer,\nD. Fake news on Twitter during the 2016 U.S. presidential election.\nScience 363,3 7 4 \u2013378 (2019).\n56. Ledford, H. Researchers scramble as Twitter plans to end free data\naccess. Nature 614,6 0 2 \u2013603 (2023).57. Community Notes. Beginning today, Community Notes are visible\naround the world. Twitter https://twitter.com/CommunityNotes/\nstatus/1601753552476438528 (2022).\n58. Wang, Z. et al. Demographic Inference and Representative Popula-\ntion Estimates from Multilingual Social Media Data. in Proceedings of\nthe 2019 World Wide Web Conference 2056 \u20132067 (Association for\nComputing Machinery, 2019).\n5 9 . B a g r o w ,J .P . ,L i u ,X .&M i t c h e l l ,L .I n f o r m a t i o n \ufb02ow reveals prediction\nlimits in online social activity. Nat. Hum. Behav. 3,1 2 2 \u2013128 (2019).\n60. Rao, A., Morstatter, F. & Lerman, K. Retweets Amplify the Echo\nChamber Effect. in Proceedings of the 2023 IEEE/ACM International\nConference on Advances in Social Networks Analysis and Mining30\u201337 (Association for Compu ting Machinery, 2024).\n61. Barber\u00e1, P., Jost, J. T., Nagler, J., Tucker, J. A. & Bonneau, R.\nTweeting From Left to Right: Is Online Political Communication\nMore Than an Echo Chamber? Psychol. Sci. 26\n,1 5 3 1 \u20131542 (2015).\n62. Di Giovanni, M. & Brambilla, M. Exploiting Twitter as Source of Large\nCorpora of Weakly Similar Pairs for Semantic Sentence Embeddings .\ninProceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing 9902 \u20139910 (Association for Compu-\ntational Linguistics, 2021).\n63. Nguyen, D. Q., Vu, T. & Nguyen, A. T . BERTweet: A pre-trained lan-\nguage model for English Tweets .i nProceedings of the 2020 Con-\nference on Empirical Methods in Natural Language Processing:System Demonstrations 9\u201314 (Association for Computational Lin-\nguistics, 2020).\n64. Waller, I. & Anderson, A. Generalists and Specialists: Using Com-\nmunity Embeddings to Quantify Activity Diversity in Online Plat-forms. inProceedings of the 2019 World Wide Web Conference\n1954 \u20131964 (Association for Computing Machinery, 2019).\n65. Grootendorst, M. BERTopic: Neural topic modeling with a class-\nbased TF-IDF procedure. Preprint at https://arxiv.org/abs/2203.\n05794 (2022).\n66. McInnes, L., Healy, J. & Melville, J. UMAP: uniform manifold\napproximation and projection for dimension reduction. J. Open\nSource Softw. 3,8 6 1( 2 0 1 8 ) .\n6 7 . H o u l e ,M .E . ,K r i e g e l ,H . - P . ,K r \u00f6 g e r ,P . ,S c h u b e r t ,E .&Z i m e k ,A . Can\nShared-Neighbor Distances Defeat the Curse of Dimensionality ?i n\nProceedings of the 22nd Interna tional Conference on Scienti \ufb01ca n d\nStatistical Database Management 482 \u2013500 (Springer, 2010).\n68. Ester, M., Kriegel, H.-P., Sander, J. & Xu, X. A Density-based Algo-\nrithm For Discovering Clusters In Large Spatial Databases WithNoise .i nProceedings of the Second International Conference on\nKnowledge Discovery and Data Mining 226 \u2013231 (AAAI, 1996).\n69. Tierney, G., Bail, C. & Volfovsky, A. Author clustering and topic\nestimation for short texts. Preprint at http://arxiv.org/abs/2106.\n09533 (2021).\n70. Lees, A. et al. A New Generation of Perspective API: Ef \ufb01cient Mul-\ntilingual Character-level Transformers. in Proceedings of the 28th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining3197 \u20133207 (Association for Computing Machinery, 2022).\n71. Hutto, C. & Gilbert, E. VADER: a parsimonious rule-based model for\nsentiment analysis of social media text. Proc. Int. AAAI Conf. Web\nSoc. Media 8,2 1 6 \u2013225 (2014).\n72. Kincaid, J. P., Fishburne, R. P., Jr, Rogers, R. L. & Chissom, B. S.\nDerivation Of New Readability Formulas (Automated ReadabilityIndex, Fog Count And Flesch Reading Ease Formula) For NavyEnlisted Personnel . (US Naval Air Station, 1975).\n73. Yang, K.-C., Ferrara, E. & Menc zer, F. Botometer 101: social bot\npracticum for computational social scientists. J. Comput Soc. Sc. 5,\n1511 \u20131528 (2022).\n74. Reimers, N. & Gurevych, I. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks .i n\nProceedings of the 2019 Con-\nference on Empirical Methods in Natural Language Processing andthe 9th International Joint Conference on Natural LanguageArticle https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 13\nProcessing 3982 \u20133992 (Association for Computational Linguis-\ntics, 2019).\nAcknowledgements\nWe are grateful for the Discovery Partners Institute of Illinois for a granton the topic of Misinformation Tagging to Kevin Leicht, PI. We thankDonghyun Kang, Yoosik Youm, and Byungkyu Lee for their helpfulfeedback on our manuscript. We also thank the members and alumni ofthe Knowledge Lab (University of Chicago) and Yonsei Social Networks &Neuroscience Lab for their comments.\nAuthor contributions\nJ.K., Z.W., and J.E. collaboratively conceived and designed the study,drafted, revised, and edited the manus cript. J.K. and H.S. performed the\nanalysis. J.K., H.S., and H.L. gathered and cleaned the data. J.K. and Z.W.\nproduced the visualizations.\nCompeting interests\nJ.E. has a commercial af \ufb01liation with Google, but Google had no role in\nthe design and analysis of this study. The authors declare no othercompeting interests.\nAdditional information\nSupplementary information The online version contains\nsupplementary material available athttps://doi.org/10.103 8/s41467-025-55868-0 .\nCorrespondence and requests for materials should be addressed to\nJames Evans.Peer review information Nature Communications thanks Sarah Shugar,\nJevin West and the other, anonymous, reviewer(s) for their contributionto the peer review of this work. A peer review \ufb01le is available.\nReprints and permissions information is available at\nhttp://www.nature.com/reprints\nPublisher \u2019s note Springer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional af \ufb01liations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,adaptation, distribution and reproduction in any medium or format, aslong as you give appropriate credit to the original author(s) and thesource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article \u2019s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is notincluded in the article \u2019s Creative Commons licence and your intended\nuse is not permitted by statutory re gulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyrightholder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/ .\n\u00a9 The Author(s) 2025Article https://doi.org/10.1038/s41467-025-55868-0\nNature Communications |          (2025) 16:973 14", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Differential impact from individual versus collective misinformation tagging on the diversity of Twitter (X) information engagement and mobility", "author": ["J Kim", "Z Wang", "H Shi", "HK Ling", "J Evans"], "pub_year": "2025", "venue": "Nature Communications", "abstract": "Fears about the destabilizing impact of misinformation online have motivated individuals and  platforms to respond. Individuals have increasingly challenged others\u2019 online claims with"}, "filled": false, "gsrank": 375, "pub_url": "https://www.nature.com/articles/s41467-025-55868-0", "author_id": ["O__9HOwAAAAJ", "iqF0jskAAAAJ", "-MZ5LSEAAAAJ", "", "kV4N4zoAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:ICp0jMwNT4MJ:scholar.google.com/&output=cite&scirp=374&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D370%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=ICp0jMwNT4MJ&ei=SLWsaJjrIPnSieoPxKLpgQ0&json=", "num_citations": 4, "citedby_url": "/scholar?cites=9461796514309614112&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:ICp0jMwNT4MJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.nature.com/articles/s41467-025-55868-0.pdf"}}]