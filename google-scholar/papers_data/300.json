[{"title": "Understanding Differences in News Article Interaction Patterns on Facebook: Public vs. Private Sharing with Varying Bias and Reliability", "year": "2023", "pdf_data": "arXiv:2305.11943v3  [cs.SI]  2 Jun 2025Public versus Less-Public News Engagement on Facebook:\nPatterns Across Bias and Reliability\nAlireza Mohammadinodooshan\nLink\u00f6ping University\nSweden\nalireza.mohammadinodooshan@liu.seNiklas Carlsson\nLink\u00f6ping University\nSweden\nniklas.carlsson@liu.se\nAbstract\nThe rapid growth of social media as a news platform has raised sig-\nnificant concerns about the influence and societal impact of biased\nand unreliable news on these platforms. While much research has\nexplored user engagement with news on platforms like Facebook,\nmost studies have focused on publicly shared posts. This focus\nleaves an important question unanswered: how representative is\nthe public sphere of Facebook\u2019s entire ecosystem? Specifically, how\nmuch of the interactions occur in less-public spaces, and do public\nengagement patterns for different news classes (e.g., reliable vs.\nunreliable) generalize to the broader Facebook ecosystem?\nThis paper presents the first comprehensive comparison of inter-\naction patterns between Facebook\u2019s more public sphere (referred to\naspublic in paper) and the less public sphere (referred to as private ).\nFor the analysis, we first collect two complementary datasets: (1)\naggregated interaction data for all Facebook posts (public + private)\nfor 19,050 manually labeled news articles (225.3M user interac-\ntions), and (2) a subset containing only interactions with public\nposts (70.4M interactions). Then, through discussions and iterative\nfeedback from the CrowdTangle team, we develop a robust method\nfor fair comparison between these datasets.\nOur analysis reveals that only 31% of news interactions occur in\nthe public sphere, with significant variations across news classes.\nEngagement patterns in less-public spaces often differ, with users,\nfor example, engaging more deeply in private contexts. These find-\nings highlight the need to examine both public and less-public\nengagement to fully understand news dissemination on Facebook.\nThe observed differences hold important implications on content\nmoderation, platform governance, and policymaking, contributing\nto healthier online discourse.\nCCS Concepts\n\u2022Information systems \u2192World Wide Web .\nACM Reference Format:\nAlireza Mohammadinodooshan and Niklas Carlsson. 2025. Public versus\nLess-Public News Engagement on Facebook: Patterns Across Bias and Reli-\nability. In Proceedings of the 17th ACM Web Science Conference 2025 (Websci\n\u201925), May 20\u201324, 2025, New Brunswick, NJ, USA. ACM, New York, NY, USA,\n12 pages. https://doi.org/10.1145/3717867.3717912\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nWebsci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA\n\u00a92025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-1483-2/25/05\nhttps://doi.org/10.1145/3717867.37179121 Introduction\nSocial media has emerged as an increasingly important news source,\nwith Facebook maintaining a prominent position among social\nplatforms [ 8,31]. For example, according to a 2024 Pew Research\nCenter report [ 8], 33% of U.S. adults regularly get news on Facebook,\nsimilar to YouTube (32%), and notably ahead of Instagram (20%) and\nTwitter (12%). This extensive reach highlights the potential impact\nof news sharing on Facebook in shaping both individual opinions\nand broader societal discourse [39].\nUnfortunately, not all news are reliable. With an increasing\namount of misinformation being circulated on Facebook, it is cru-\ncial to understand how users engage with news of varying reliabil-\nity [12]. Understanding of these dynamics are expected to benefit\nmedia researchers, journalists, content moderators, and policymak-\ners, whose choices based on these insights, in turn, are expected to\ninfluence public opinion and the behavior of regular users [ 20,30].\nPrevious research has extensively examined public user engage-\nment dynamics with different news content and the factors affecting\nthis engagement [ 2,7,10,26,42]. However, with a growing privacy\ninclination of Facebook users and a rise in user engagement in\nprivate spaces of Facebook [ 9,24], it is becoming increasingly im-\nportant to understand these dynamics also with regards to private\nsharing. Yet, the current literature is centered around more public\nposts, leaving gaps in our understanding of engagement differences\nbetween these realms. Most importantly, prior research has not\nstudied the dynamics of public vs. private interaction.\nTo address this gap, in this paper we present the first comprehen-\nsive comparison of the news article sharing and user interaction\npatterns seen on Facebook\u2019s more public sphere (referred to as pub-\nlicin paper) versus in the less public sphere (for simplicity referred\nto as private ). Specifically, we investigate differences in how news\narticles written with different bias andreliability are shared and\nengaged with, as well as the depth of these interactions. In this con-\ntext, biasrefers to a tendency for news articles to exhibit partiality\nor favoritism towards particular groups or ideas (e.g., left or right on\nthe political spectrum), while reliability refers to the accuracy and\ncredibility of the information presented (e.g., fake or true). While\nmost previous research on news engagement has primarily focused\non either bias or reliability (and on public engagement), we consider\nboth dimensions, as they have been found related, but yet each has\nits own ability to influence news article sharing behaviors and affect\nthe quality and diversity of information that users encounter on\nsocial media [15, 34].\nWebsci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA Alireza Mohammadinodooshan and Niklas Carlsson\nFurthermore, by analyzing whether interactions are deep orshal-\nlow, we can determine if users engage more deeply with certain\ncontent types (e.g., fake news) in private or public spaces. This\ndistinction is crucial, as deeper engagement\u2014such as sharing or\ncommenting\u2014can significantly enhance an article\u2019s impact com-\npared to shallow actions like pressing Facebook\u2019s \u201clike\" button [ 31].\nResearch questions and methodology: Guided by the goal\nof better understanding how representative the public sphere of\nFacebook\u2019s ecosystem (typically studied in prior works) is of the\nprivate sphere and the ecosystem as a whole, we designed our study\nto address the following research questions:\nRQ1 How do the patterns of public and private sharing of news\narticles on Facebook differ for articles with varying levels of\nbias and reliability?\nRQ2 If and how does the depth of user interactions with news\narticles differ within public and private sharing contexts for\narticles with varying levels of bias and reliability?\nWe took several steps to address these previously unaddressed\nquestions in as controlled matter as possible. First, in contrast to\nmost prior works that use publisher-level labeling (e.g., Adelson et al.\n[10]; Horne et al. [ 18]), we employ article-level labeling of bias and\nreliability. By doing so, we capture that not all articles by a publisher\nhave the same bias or reliability, recognizing that the publisher\n(source) is only one factor [ 36] in the bias/reliability. In particular,\nwe consider the bias and reliability of articles ( \ud835\udc41=19,050), metic-\nulously selected from a substantial pool of publishers ( \ud835\udc41=1,121)\nand 30K+ manually labeled news articles. Here, the original label-\ning was provided by Ad Fontes Media [ 1], which provides bias and\nreliability of news articles. Second, through active back-and-forth\ndiscussions and feedback with the Crowdtangle team, we devel-\noped and implemented a methodology using their Chrome addon\nthat allows us to obtain simultaneous interaction statistics for a\nrepresentative sample of publicly shared Facebook posts linking\nthese articles, as well as across all Facebook posts (including both\npublic and private posts) linking these articles. Finally, using this\nunique dataset, we perform a comparison of interaction dynamics\nwhen users share news articles with different levels of bias and\nreliability, both publicly and privately.\nEmpirical example findings: Our analysis reveals several key\ninsights into how users interact with news articles of varying bias\nand reliability on Facebook. For instance, we show that users tend\nto engage more deeply in private discussions than in public ones,\nirrespective of the news class. When considering the news class,\nwe highlight that users exhibit relatively higher deep interaction\nlevels with highly-unreliable content. Our results also show that\nreliable news content has significantly lower private interaction\nshares compared to the highly-reliable or even unreliable content.\nExample beneficiaries : Our methodology and findings con-\ntribute to a better understanding of news sharing and interaction\ndynamics on Facebook, offering valuable insights for various stake-\nholders. Media researchers and journalists can use our analysis of\nhow bias and reliability shape engagement in public and private\nspheres for improved content creation and distribution strategies.\nFor content moderators, the analysis offers data-driven guidance on\nprioritizing efforts to curb the spread of problematic content. Forpolicymakers, we highlight how privacy settings influence engage-\nment patterns, with highly unreliable news, for example, garnering\nmore engagement in the private sphere. These results underscore\nhow platform design choices can influence selective exposure and\nengagement, potentially exacerbating issues like ideological polar-\nization and misinformation spread.\nRoadmap: Sect. 2 explains our research design, including news\narticle selection, labeling, data collection, and the processing steps\ntaken to ensure fair comparisons. The resulting dataset is summa-\nrized in Sect. 3. Sect. 4 analyzes public vs. private dynamics both\nfrom a high-level aggregated perspective (Sect. 4.1) and then us-\ning a detailed statistical analysis (Sect. 4.2). In Sect. 5, we turn our\nattention to differences in the depth of interactions. Topic and top-\npublishers analysis are provided in Sects. 6 and 7. Finally, related\nworks (Sect. 8) and conclusions (Sect. 9) are presented.\n2 Research Design, Data Collection and\nLimitations\nAt a high level, our research design has three parts.\nPart 1. Article selection and labeling: We first obtained bias\nand reliability scores for the articles evaluated by Ad Fontes Me-\ndia [1]. Subsequently, each article was categorized into one of three\nbias classes and one of four reliability classes based on these scores.\nPart 2. Collection of interactive data: After careful prepro-\ncessing of the URLs, we used the CrowdTangle browser extension\nto obtain interaction data for two sets of posts linked to these URLs\n(1)public and (2) combined (public + private). To address some lim-\nitations of the API and to ensure fair comparison of the sets, we\nleave a four-month gap between the latest labeled article (Nov. 1,\n2022) and the primary data collection (Mar. 2023), as well as apply\nsome additional post-processing (e.g., examining the actual posts),\nfiltering (based on thresholds determined via discussions with the\nCrowdTangle team), and collect some complementing data directly\nfrom CrowdTangle (to address limitations of the extension API).\nPart 3. Data analysis: Finally, we use the final dataset (capturing\nthe two sets) to compare the properties of the public vs.combined\n(private + public) sets and identify significant statistical differences\nbetween the interaction patterns of public and private posts. We\nnext provide details of the initial two steps, before presenting our\nanalysis results (Part 3) in the subsequent sections.\n2.1 News Article Selection and Labeling\nSelection of articles: There are several independent initiatives\nthat assess the bias and/or reliability of individual news articles\nand/or news sources. Examples of such evaluation efforts include\nMedia Bias Fact Check [ 27], Ad Fontes Media, AllSides [ 3], and\nNewsGuard [ 32]. Among these, we opted to use data from Ad\nFontes Media for the following main reasons: (1) it has been widely\nused in previous research [ 16,19,22], (2) they evaluate individual\nnews articles (not only the news publishers), (3) each assessed ar-\nticle receives a score for both bias and reliability, (4) they offer a\ntransparent evaluation methodology, which is published and ex-\nplained in a white paper [ 33], and finally (5) the dataset comprises\na large set of news contents randomly selected from a diverse set\nof publishers.\nPublic versus Less-Public News Engagement on Facebook: Patterns Across Bias and Reliability Websci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA\nWe obtained all evaluated news from Ad Fontes Media as of Nov.\n1, 2022. This included 31,446 news article URLs from 1,121 publish-\ners. After filtering out URLs serving as event reporting pages (e.g.,\nhttps://www.nola.com/news/hurricane) and articles with updated\nratings (per Ad Fontes Media\u2019s documentation), the final dataset\nincluded 31,408 articles.\nLabeling of articles: Each article in the Ad Fontes Media dataset\nis assessed for both bias and reliability by a minimum of three\nhuman analysts, representing a mix of right, left, and center self-\nreported political perspectives. Bias scores from Ad Fontes Media\nrange between -42 and +42, where more negative values suggest a\nstronger left-ward bias, and positive values indicate a right-leaning\nbias. As for reliability, scores vary from 0 to 64, with 64 representing\nthe most-reliable news.\nFor our analysis, we categorized the bias and reliability scores\ninto distinct classes. While previous work mainly focuses on binary\nclassification of news (e.g., Left vs. Right or Fake vs. True), we opted\nfor a more granular approach by defining three classes of bias ( Left,\nRight , and Center ) and four classes of reliability ( Most-unreliable ,\nUnreliable ,Reliable , and Most-reliable ). Figures 1a and 1b show the\nCumulative Distribution Function (CDF) of the bias and reliabil-\nity scores of the articles included in our analysis (after additional\nfiltering steps, explained later in this section) together with the\nthreshold values and class labels used to define each class.\nWith our labeling, anything with a bias score below -6 is called\nLeft-biased, and anything above 6 is called Right -biased, with the\nrange (-6,6) capturing the Center class. We note that Ad Fontes\nMedia also refers to this range as \u201cMiddle or Balanced Bias\u201d [ 1].\nAs observed, this class represents most samples in our dataset,\nrepresenting 58% (11,094 out of 19,050 articles in the final dataset).\nWith this split, the two biased classes have similar, close to 20%,\nshares.\nFor the reliability classes, we assigned the Most-unreliable class\nto cover the lowest range (0, 32). According to Ad Fontes Media\u2019s\nterminology [ 1], these values typically correspond to articles \u201cwhich\ncontain inaccurate and misleading info, selective stories or have\nopinion and wide variation in their reliability\u201d. Similarly, the range\n(48-64), chosen for the Most-reliable class, encompasses articles\ncharacterized by \u201cthorough and original fact reporting\u201d due to Ad\nFontes terminology. Finally, the threshold of 40, between Reliable\nandUnreliable , matches the mid-point between these two classes as\nwell as where Ad Fontes make their split between \u201cwide variation\nin reliability\" and \u201cmix of fact reporting and analysis\".\n2.2 Collection of Interaction Data\nPreprocessing of URLs: Before collecting the interaction data\nassociated with each article (URL), we (1) expanded link shorteners\nand (2) converted URLs to canonical forms. During the conversion\nprocess, we primarily removed unnecessary URL parameters after\n\u201c?\" except those essential to canonical forms (e.g., \u201cid\" parameters\nare sometimes part of the canonical form). We also replaced web\narchive links in the Ad Fontes Media\u2019s dataset with original links.\nPrimary data collection for the two sets of posts: Prior to\nits shutdown in Aug. 2024, CrowdTangle (owned by Facebook)\nprovided access to interaction statistics for public posts, which it\nindexed in a database widely used by researchers. Additionally,\n-42 -606 42\nBias0.000.200.400.600.801.00CDF\nLeft\nCenter\nRight(a) Bias scores\n0 324048 64\nReliability0.000.200.400.600.801.00CDF\nMost-unreliable\nUnreliable\nReliable\nMost-reliable (b) Reliability scores\nFigure 1: CDFs of scores for (a) bias and (b) reliability.\ntheir Chrome extension offered access to the life-time interaction\nstatistics available at the time, encompassing all posts (public +\nprivate). In this study, we utilized the extension to collect statis-\ntics for two datasets: combined and public , where the combined\ndataset includes all interactions from all posts (public + private),\nand the public dataset represents a subset of the combined dataset,\ncontaining only interactions with publicly accessible posts.\nTo collect the data for each URL, we used the \u201cDownload\" option\nwithin the extension\u2019s interface when browsing each article. The\ndownloaded CSV file contained the interaction data for both sets. As\nexpected from a privacy standpoint, only the aggregated statistics\nare provided for the combined set. In contrast, every post and its\nindividual interaction data are provided for the public set.\nAt a high level, the combined category aggregates the interactions\n(e.g., likes, shares, comments, etc.) of all Facebook posts referencing\nthe article\u2019s URL, regardless of the post\u2019s privacy settings. This\nincludes interactions on posts with limited privacy settings, such\nas \u201cFriends only\" or \u201cOnly Me\". In contrast, the public category,\nincludes all interactions with the public posts tracked by Crowd-\nTangle, which, due to heavy-tailed characteristics and significant\ncoverage, captures most interactions with public Facebook posts.\nAs supporting examples, all posts of all US-based public groups with\n2K+ members are indexed, and in 2021 over 99% of all Facebook\npages with at least 25K likes were indexed [38].\nTimeline and limitations: We collected the interaction data\nin Mar. 2023, ensuring at least a four-month gap between the publi-\ncation date of the newest article in our dataset and our interaction\ndata collection. This interval ensured capturing most posts and\ninteractions for the URLs. This four-month window is notably con-\nservative, as research has shown that most interactions occur within\ndays of publication [ 35,40]. Although this conservative window\nmay have overlooked interactions with content that was later re-\nmoved or deleted, we are unable to assess the extent of this effect\u2014if\nit exists\u2014due to CrowdTangle\u2019s shutdown in Aug. 2024.\nAs mentioned in [ 11], the information from the CrowdTangle\naddon originated from two different sources: (1) public interac-\ntion data is obtained from the CrowdTangle database, while (2)\ncombined interaction data is derived from Facebook\u2019s Graph API.\nConsequently, variations in data capturing times might cause some\nURLs to exhibit higher public than combined interactions.\nFurthermore, as confirmed by the CrowdTangle team, Facebook\u2019s\naggregation data restarts the interaction count whenever websites\nupdate their connection schemes (e.g., switching from HTTP to\nHTTPS) after publishing a URL, while CrowdTangle continued\naccumulating data. Although the discrepancy is not substantial\n(9% in our initial dataset), we aimed to mitigate the effect of these\ndifferences where possible (described next).\nWebsci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA Alireza Mohammadinodooshan and Niklas Carlsson\nTable 1: Statistics based on Bias and Reliability.\nClass Articles no Combined interactions Public interactionsBiasLeft 4,516 79,038,246 20,307,107\nCenter 11,094 90,084,398 32,109,924\nRight 3,440 56,160,101 18,011,285ReliabilityMost-unreliable 3,520 45,299,475 13,206,643\nUnreliable 3,795 46,712,927 14,199,322\nReliable 9,308 89,596,137 30,639,973\nMost-reliable 2,427 43,674,206 12,382,378\nTotal 19,050 225,282,745 70,428,316\nLeftCenterRight0.0K2.5K5.0K7.5K10.0K12.5K15.0K17.5KAverage Interactions per Article\nBias ClassMost-unreliableUnreliableReliable\nMost-reliable0.0K2.5K5.0K7.5K10.0K12.5K15.0K17.5K\nReliability ClassCombined Public\nFigure 2: Interactions per article\nFurther filtering and data processing for fairer compar-\nisons: After consulting with the CrowdTangle team regarding all\nnumerical discrepancies that we observed, we identified two ad-\nditional reasons for the discrepancies. First, as stated in [ 28], for\nprivacy reasons, for the combined data (that include private posts)\n\u201cthe values are intentionally not precise, but you can be confident\nthey accurately reflect user engagement with a URL.\" Discussing\nthe identified discrepancy for such cases, the CrowdTangle team\nconfirmed that combined interaction data with counts below 100 is\nnot reliable; however, that data with counts above 100 provides a\nreliable estimate of public + private interactions. For this reason,\nwe removed all URLs in our dataset with combined interactions\nbelow 100. Although this reduced our dataset to 19,505 articles, it is\nimportant to note that this group of URLs accounted for less than\n4% of public interactions in our dataset. Furthermore, we note that\nthe public interaction data for this group still remains reliable.\nSecond, the CrowdTangle team informed us that the Facebook\nGraph API only considers posts with the URL attached to the post\n(i.e., those displaying a preview of the URL) when counting inter-\naction with the URL, while CrowdTangle\u2019s algorithm includes any\npost containing a linked URL, regardless of attachment status. Tak-\ning this into account, in our final step toward enhancing interaction\ndata quality, we chose not to rely on the aggregated data from the\nCrowdTangle extension (which sums up the interactions of all posts\nmentioned in the extension). Instead, we examined each post to\ndetermine whether the related URL was attached or not, and if so,\nwe included them in the sum of public interactions for that URL.\nThis resulted in a more accurate comparison between combined\nand public interactions. After these processing steps, we had only\n455 URLs for which public interactions exceeded combined inter-\nactions, which were excluded from our final dataset. The primary\ncauses of these discrepancies were beyond the scope of our research\nto address (e.g., changes in URL, publisher protocol schemes, or\ndifferent data capturing times as mentioned above).\nEnhancing the data with additional posts: Another limitation\nof the CrowdTangle extension was its restriction to retrieving data\nfor up to 500 posts per URL. To address this, we retrieved additional\ndata from CrowdTangle for URLs with over 500 posts in the public\ninteraction category. While some of the pre- and post-processing\nsteps described above require significant effort, they help ensure\nan accurate and fair comparison between the public and combined\nsets in such a way that we can provide conclusive insights into the\nrelative sharing patterns of public vs. private posts.3 Dataset\nHigh-level summary: After applying the aforementioned filter-\ning steps, we have in total 19,050 articles from 1,121 news outlets\nremaining in our dataset. These articles have been shared in 253,350\nposts, which combined are responsible for 225,282,745 interactions\n(out of which 70,428,316 are public).\nBias data: Table 1 shows the number of articles and the to-\ntal combined and public interactions for each bias and reliability\nclass. With the selected bias thresholds, our dataset includes 4,516\nLeft-biased articles and 3,440 Right -biased articles, with the remain-\ning 11,094 articles falling in the Center category. These articles\nare in turn responsible for 79,038,246 ( Left), 56,160,101 ( Right ), and\n90,084,398 ( Center ) interactions. While the Center category includes\n58% of the articles, it is interesting to note that it is responsible for\na significantly smaller fraction of the total interactions (40%). In-\nstead, the Left-biased and Right -biased articles see relatively higher\ninteraction rates (studied in the next section). For example, the\nLeft-biased articles account for only 24% of the articles but 35% of\nthe interactions, and the Right -biased articles are responsible for\nonly 18% of the articles but 25% of the interactions.\nReliability data: For the reliability classes as shown in Table 1,\nthe relative differences are smaller. Here, the Reliable articles make\nup the largest share (9,308 articles and 89,596,137 interactions),\nfollowed by Unreliable (3,795 articles and 46,712,927 interactions),\nMost-unreliable (3,520 articles and 45,299,475 interactions), and\nMost-reliable (2,427 articles and 43,674,206 interactions).\n4 Public vs. Combined Interactions\nTo understand how engagement patterns differ between public\nand private spheres on Facebook, we present two complementary\nanalyses: (1) an aggregated comparison of public vs. combined (pub-\nlic + private) interactions (Sec. 4.1), and (2) a statistical analysis\nof their differences (Sec. 4.2). By comparing public interactions\nagainst combined interactions, where private interactions consti-\ntute the majority (as we show later), we can effectively study how\nengagement in private spaces differs from public ones.\n4.1 Aggregated (Macro) Analysis of Interactions\nFigure 2 presents the number of combined and public interactions\nper article for each class. Here, two metrics are used:\nPublic versus Less-Public News Engagement on Facebook: Patterns Across Bias and Reliability Websci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA\nAverage combined interactions: (left bars for each class) rep-\nresents the average number of combined (private+public) interac-\ntions per article in each class. More specifically, considering \ud835\udc3ccomb\n\ud835\udc50,\ud835\udc56\nas the combined interaction for the \ud835\udc56\ud835\udc61\u210earticle in class \ud835\udc50, the average\ncombined interactions for this class ( \u00af\ud835\udc3ccomb\ud835\udc50 ) is calculated as:\n\u00af\ud835\udc3ccomb\n\ud835\udc50 =\u00cd\ud835\udc41\ud835\udc50\n\ud835\udc56=1\ud835\udc3ccomb\n\ud835\udc50,\ud835\udc56\n\ud835\udc41\ud835\udc50, (1)\nwhere the numerator (\u00cd\ud835\udc41\ud835\udc50\n\ud835\udc56=1\ud835\udc3ccomb\n\ud835\udc50,\ud835\udc56) corresponds to the \u201cCombined\ninteractions\u201d column in Table 1 and the denominator (i.e., the total\nnumber of articles \ud835\udc41\ud835\udc50for class\ud835\udc50) is found in the \u201cArticles no\u201d column\nof Table 1. For example, for the Center class in the Bias category, the\naverage combined interactions per article is computed by dividing\nthe total interactions (90,084,398) by the articles count (11,094),\nyielding \u00af\ud835\udc3ccomb\nCenterto be 8,120 combined interactions per article (gray\nbar in Figure 2).\nAverage public interactions per article: (right bars) is defined\nin a similar manner but considering only the public interactions.\nMore specifically, considering \ud835\udc3cpublic\n\ud835\udc50,\ud835\udc56as the public interactions for\nthe\ud835\udc56\ud835\udc61\u210earticle in a class \ud835\udc50, the average public interactions for this\nclass ( \u00af\ud835\udc3cpublic\n\ud835\udc50 ) is calculated as follows:\n\u00af\ud835\udc3cpublic\n\ud835\udc50 =\u00cd\ud835\udc41\ud835\udc50\n\ud835\udc56=1\ud835\udc3cpublic\n\ud835\udc50,\ud835\udc56\n\ud835\udc41\ud835\udc50, (2)\nwhere the numerator (i.e.,\u00cd\ud835\udc41\ud835\udc50\n\ud835\udc56=1\ud835\udc3cpublic\n\ud835\udc50,\ud835\udc56) corresponds to the \u201cPublic\nInteractions\" column in Table 1. Now, taking the Center class as\nan example again, the average public interactions per article is\ncalculated by dividing the total public interactions (32,109,924) by\nthe article count (11,094), resulting in an \u00af\ud835\udc3cpublic\nCenterof 2,894 interactions\nper article (striped gray bar in Figure 2).\nAs seen in the figure, there are some very clear and interesting\ndifferences in the trends observed for the public vs. the combined\nstatistics. First, for all three bias classes and for all four reliability\nclasses, the interaction rates are significantly lower for the public\nposts than for the combined set. These aggregate rate differences\nsuggest that users are more likely to interact with private posts,\nregardless of bias and reliability class, and underscores the impor-\ntance of considering the total (combined) interactions, not only the\n(typically studied) public posts. We next look closer at the relative\ninteraction levels of each class and the impact of using the public\nvs. combined sets for such interaction comparisons.\nBias comparisons: Second, examining the relative differences\namong the bias classes, the Leftbias class has the highest average\ncombined interactions per article (17,501), followed by the Right\n(16,325) and Center (8,120) classes. For public interactions; however,\ntheRight bias class has the highest average public interactions\nper article (5,236), with the Left(4,497) and Center (2,894) classes\ntrailing behind. This shows that biased articles see even greater\nrelative interaction rates in private than in public posts.\nReliability comparisons: Third, for the reliability classes, the\nMost-reliable class has the highest average combined and public\ninteractions per article (17,995 and 5,101). The Unreliable andReli-\nable classes have relatively similar average public interactions per\narticle (3,741 and 3,291, respectively). Moreover, the Most-unreliable\nclass has a higher average combined interactions per article (12,869)\nLeftCenterRight0.000.050.100.150.200.250.300.35Public Interaction Share\nBias ClassMost-unreliableUnreliableReliable\nMost-reliable0.000.050.100.150.200.250.300.35\nReliability ClassFigure 3: Aggregated public interactions shares\nand public interactions per article (3,751). It is interesting to see that\nthe two extreme classes (i.e., the Most-reliable andMost-unreliable )\nhave the highest average interactions per article in both combined\nand public contexts, implying that users are more likely to engage\nwith articles on both ends of the reliability spectrum.\nAggregated public interaction share: Motivated by most prior\nworks only studying the public posts tracked by CrowdTangle, we\nnext evaluate what fraction of the total interactions this set captures\nand compare the relative differences in the public interaction share\nof different categories. For this analysis, we calculate the (aggregate)\npublic interaction share as the ratio of public interactions to total\ninteractions for each class \ud835\udc50:\u00af\ud835\udc3cpublic\n\ud835\udc50/\u00af\ud835\udc3ccomb\ud835\udc50 .\nFigure 3 summarizes these ratios. We note that the Center class\nhas the highest aggregated public interaction share, and that among\nthe biased groups, there is a clear gap between the left and right\nparties, as the Right class has a significantly (25% extra) higher\npublic interaction share (lower private interaction share). When it\ncomes to the reliability classes, the relative gap between the classes\nis smaller. However, it should be noted that the two extreme classes\n(Most-reliable and Most-unreliable ) have the lowest (and almost\nsimilar) aggregated public interaction shares.\nTakeaway: Compared to right-biased articles, left-biased ar-\nticles receive higher private interactions share. In terms of\nreliability, extremely reliable/unreliable news articles receive\nhigher levels of private interactions share.\n4.2 Granular Statistical (Micro) Analysis of\nInteractions\nHaving presented aggregated analysis and insights, we next present\na more detailed statistical comparison between different classes\nin which we give equal consideration to every article in a class,\nregardless of its total interaction count. For this analysis, we utilize\nthe distributions of the per-article level interactions for each class\n\ud835\udc50:Dcomb\ud835\udc50 ={\ud835\udc3ccomb\n\ud835\udc50,\ud835\udc56}andDpublic\n\ud835\udc50 ={\ud835\udc3cpublic\n\ud835\udc50,\ud835\udc56}, where\ud835\udc3ccomb\n\ud835\udc50,\ud835\udc56is the\ntotal number of interactions across all posts associated with the\n\ud835\udc56\ud835\udc61\u210earticle of class \ud835\udc50(1\u2264\ud835\udc56\u2264\ud835\udc41\ud835\udc50) and\ud835\udc3cpublic\n\ud835\udc50,\ud835\udc56is the corresponding\nnumber of interactions with public posts.\nThe next two subsections compare distributions of the total\n(combined) interactions (Sect. 4.2.1) and study the distributions of\nthe relative public interaction shares (Sect. 4.2.2), respectively. In\nboth subsections, the distributions are compared using multiple\nWebsci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA Alireza Mohammadinodooshan and Niklas Carlsson\n100\n101\n102\n103\n104\n105\n106\nInteraction Count104\n103\n102\n101\n100Empirical CCDF1.0K10.0K\nLowest medianLowest meanLeft\nCombinedCenter Right\nPublic\n(a) Disributions of different bias classes.\n100\n101\n102\n103\n104\n105\n106\nInteraction Count104\n103\n102\n101\n100Empirical CCDF1.0K10.0K\nLowest medianLowest meanMost-unreliable\nCombinedUnreliable\nPublicReliable Most-reliable (b) Disributions of different reliability classes.\nFigure 4: Empirical complementary cumulative distribution functions (CCDFs) for combined and public interactions, along with\ninset boxplots showing the distribution of combined interactions. Both CCDFs emphasize tail characteristics of distributions,\nwhile the boxplots provide granular statistical summaries, including medians, quartiles, and means.\nstatistical tests; statistical significance is reported if a p-value is\nbelow 0.001, and supporting p-values are reported for the main\nfindings.\n4.2.1 Distribution of Combined Interactions. Comparison\nof Bias Classes: Figure 4a shows the CCDFs of both the set of\ncombined interactions ( Dcomb\ud835\udc50 ) and the set of public interactions\n(Dpublic\n\ud835\udc50 ) for each bias class \ud835\udc50on a log-log scale. Accompanying\nthe CCDF is a boxplot, highlighting key statistical percentiles for\nthe distribution of the combined interactions ( Dcomb\ud835\udc50 ). Specifically,\nthe 10\ud835\udc61\u210epercentile (bottom marker), 25\ud835\udc61\u210epercentile (bottom of\nbox), median (middle marker), 75\ud835\udc61\u210epercentile (top of box), 90\ud835\udc61\u210e\npercentile (top marker), and the mean (i.e., \u00af\ud835\udc3ccomb\ud835\udc50 ) (circle). To allow\nhigher resolution, outliers are not shown in the boxplot.\nUpon examining the figure, we can make several observations.\nFirst, as articles with fewer than 100 combined interactions were\nexcluded, the minimum value for Dcomb\ud835\udc50 starts at 100. Second, the\nshape of the of the CCDFs when plotted on log-log scale underscores\nthe \u201cheavy-tailed\u201d nature of the interactions ( Dcomb\ud835\udc50 andDpublic\n\ud835\udc50 )\nof all bias classes \ud835\udc50. This suggests that a small subset of the articles\nare responsible for a most of the interactions. Third, there are\ndiscernible variations in the distributions. For example, articles\nclassified as biased (both Left andRight ) consistently see higher\ninteraction levels (i.e., right-shifted curves compared to the Center\nclass), echoing the trends we observed in the aggregated analysis.\nTo better understand these disparities and their implications, we\nnext provide a more rigorous statistical analysis.\nStatistical Tests for Bias-related Differences: To evaluate\nthe differences in distributions of the set of combined interactions\nDcomb.\ud835\udc50 across all bias classes \ud835\udc50, we employed a multi-step statistical\ntesting approach:\n1. Overall Differences: We first used the Kruskal-Wallis test\nto identify overall differences between the distributions between\nthe bias classes. This test revealed significant disparities among the\nclasses, with a p-value of 1.66\u00b710\u221265.2. Pairwise Comparisons: Second, we used the Dunn test to\nidentify specific pairs of classes that exhibited differences. This test\nconfirmed that the two biased classes LeftandRight are different\nthan the Center class, but the difference between the two biased\nclasses themselves was not significant. Here, the most significant\ndifference were observed between the Leftand Center class (i.e.,\nDcomb\nLeftandDcomb\nCenter), registering a p-value of 2.06\u00b710\u221255.\n3. Median Comparisons: Third, we used the Mann-Whitney\nU test to compare the medians of the three classes: Left (1,440),\nCenter (842), and Right (1,286). Also, here, we observed statistically\nsignificant pairwise differences when comparing each of the two\nbiased classes ( LeftandRight ) with the Center class, but not between\neach other. Here, the two significant cases obtained p-values of\n8.4\u00b710\u221257and9.1\u00b710\u221228, respectively.\n4. Comparing Means: Finally, due to the violations of assump-\ntions intrinsic to a t-test (namely, normality and homogeneity of\nvariances), we instead use bootstrapping with 100K iterations and\na 99% confidence interval to compare the means. Again, the pair-\nwise differences in the means between Leftvs.Center and between\nRight vs.Center are statistically significant, but not between the\ntwo biased classes themselves (i.e., Leftvs.Right ).\nTakeaway: Statistical analysis further strengthens the obser-\nvation that biased news consistently garners higher combined\nengagement levels per article than center-aligned news. Inter-\nestingly, there is no statistically significant difference between\ntheLeftandRight class.\nComparison of Reliability Classes: Figure 4b shows the CCDFs\nfor the different reliability classes, broken down for both combined\nand public interactions, accompanied by a boxplot offering a clearer\nperspective on the percentile values for the combined interactions.\nThe CCDFs again demonstrate heavy-tailed distribution character-\nistic, and the relative shifts of the distributions are consistent with\nthe aggregate (average values) previously discussed (and seen in\nFigure 2), with the Most-reliable class typically getting the most\nPublic versus Less-Public News Engagement on Facebook: Patterns Across Bias and Reliability Websci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA\nLeftCenterRight0.000.200.400.600.801.00Public Interaction Ratio\nBias ClassMost-unreliableUnreliableReliable\nMost-reliable0.000.200.400.600.801.00\nReliability ClassMean Median\nFigure 5: Distribution of public interaction ratios\ninteractions, followed by the unreliable classes, and subsequently\nby the Reliable class. While comparing the tail parts is straightfor-\nward, comparing the whole distributions requires some care. For\nexample, here, the core of the boxplot does not distinctly set apart\ntheMost-reliable from the unreliable classes. To derive robust con-\nclusions, we, therefore, again performed a sequence of statistical\ntests on the combined interactions distributions.\nFirst, the Kruskal-Wallis test was applied to discern differences\namong the reliability classes. While we observed statistical differ-\nences (p-value of 7.9\u00b710\u221220), the magnitude of distinction among\nreliability classes is more nuanced than in the bias classes (which\nhad a p-value of 1.66\u00b710\u221265). Further analysis employing the Dunn\u2019s\ntest confirmed that the Reliable class statistically diverges from the\nother three. The least significant p-value here, 7.63\u00b710\u22128, is attrib-\nuted to the comparison between the Reliable andUnreliable classes.\nThe most significant, on the other hand, emerges from the Reliable\nversus Most-reliable comparison with the p-value of 7.05\u00b710\u221215. Ex-\ncept for these distributions, none of the other pairwise distribution\ncomparisons resulted in statistically significant differences. Third,\nthe Mann-Whitney U test reinforced our findings, particularly high-\nlighting the statistical significance when comparing the medians of\ntheReliable class (median of 891) with the others: Most-unreliable\n(1,162), Unreliable (1,104), and Most-reliable (1,218). Finally, we apply\nbootstrapping to compare the means. While only the Reliable and\nMost-reliable classes comparison is significant at the 99% confidence\nlevel, we note that all three comparisons against the Reliable class\nare significant at the 95% confidence level.\nTakeaway: Comparing to the other reliability classes, the Re-\nliable articles demonstrate lower interaction values, especially\nwhen compared to the Most-reliable articles.\n4.2.2 Distribution of Public Interaction Shares. The preceding anal-\nysis centered on the distribution of combined interactions. Like the\naggregated analysis, we next shift our focus to the proportion of\ninteractions that are public. Specifically, for each class, we analyze\nthe distribution of public interaction shares at the granularity of\nindividual articles. For this analysis, we define the set of public inter-\naction ratios ( PIR) of class\ud835\udc50asDPIR\ud835\udc50=\u001a\n\ud835\udc3cpublic\n\ud835\udc50,\ud835\udc56\n\ud835\udc3ccomb\n\ud835\udc50,\ud835\udc56\u001b\n, where 1\u2264\ud835\udc56\u2264\ud835\udc41\ud835\udc50\nis the index of articles included in class \ud835\udc50.Figure 5 presents the public interaction ratios using box-plots\nbroken down per bias and reliability category. Here, we again show\nthe 10\ud835\udc61\u210epercentile, 25\ud835\udc61\u210epercentile, median, 75\ud835\udc61\u210epercentile, and\n90\ud835\udc61\u210epercentile, as well as the average. Like for the aggregate analy-\nsis, we observe some notable differences, which we can now support\nusing statistical analysis. First, the Kruskal-Wallis test shows that\nthere are statistically significant differences between both the biases\nclasses (p-value of 2.68\u00b710\u221222) and reliability classes (p-value of\n8.74\u00b710\u221233).\nMotivated by the higher significance (smaller p-value) for the\nreliability classes, we consider these differences first. Using the\nDunn test, we find statistically significant differences for all pair-\nwise distribution comparisons except between the Most-unreliable\nandMost-reliable classes. Similarly, the Mann-Whitney U test shows\nthat all pairwise relative differences in the medians (observed in\nFigure 5), except for between the Most-unreliable andMost-reliable\nclasses, are significant (with the largest p-value among the these\npairwise cases being 2.82\u00b710\u221214). Finally, bootstrapping confirms\nthat all pairwise differences in the means observed in the figure are\nstatistically significant, except for the case of the Most-unreliable vs.\ntheMost-reliable class. It is also worth noting that our observation\nthat the two extreme classes have lower public interaction ratios\n(higher private interaction ratios) aligns with our findings from the\naggregated analysis.\nTakeaway: The two extreme classes, Most-reliable andMost-\nunreliable , exhibit similar private interaction ratios that are\nstatistically higher compared to the other classes.\nWe now shift our attention to the public interaction ratios across\nthe bias classes, where we make some interesting observations\nregarding the classes\u2019 relative order. Specifically, we now observe\nthe first difference in the relative order of the classes when com-\nparing the average (and median) public shares calculated on a per-\narticle basis (Figure 5) with the aggregated results (Figure 3). More\nspecifically, comparing with the aggregate results, the Right class\nhas switched ranking with the Leftclass, becoming the one with\nsmallest public interaction share. Furthermore, comparing the dis-\ntributions seen for the Right andLeftclass is statistically significant:\nDunn test (p-value of 3.25\u00d710\u221214), Mann-Whitney U test (p-value\nof6.16\u00d710\u221217) and bootstrap results being significant at 99% level.\nWhile these results may appear surprising at first, they are due\nto some interesting differences in the distributions. First, note that\nthe aggregate analysis only calculates the overall ratio\u00cd\ud835\udc41\ud835\udc50\n\ud835\udc56=1\ud835\udc3cpublic\n\ud835\udc50,\ud835\udc56\u00cd\ud835\udc41\ud835\udc50\n\ud835\udc56=1\ud835\udc3ccomb\n\ud835\udc50,\ud835\udc56\ncompared to the granular analysis, where the average (for example)\nis calculated as1\n\ud835\udc41\ud835\udc50\u00cd\ud835\udc41\ud835\udc50\n\ud835\udc56=1\ud835\udc3cpublic\n\ud835\udc50,\ud835\udc56\n\ud835\udc3ccomb\n\ud835\udc50,\ud835\udc56. Second, taking a closer look at the\ndistributions,D\ud835\udc43\ud835\udc3c\ud835\udc45\n\ud835\udc3f\ud835\udc52\ud835\udc53\ud835\udc61andD\ud835\udc43\ud835\udc3c\ud835\udc45\n\ud835\udc45\ud835\udc56\ud835\udc54\u210e\ud835\udc61differ substantially. Part of this\ncan be observed in Figure 4a, where we can see that Left class\ndemonstrates notable disparities between public and combined\ninteractions in the tail, while the Right class exhibits relatively\nlarger differences in the head of the distribution. This suggests\nthat articles with higher combined number of interactions in the\nRight class may exhibit higher normalized public interaction shares,\nWebsci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA Alireza Mohammadinodooshan and Niklas Carlsson\nLeftCenterRightAll0.100.200.300.400.500.60Deep Interaction Ratio\nMean Median Combined Public\nFigure 6: Deep interaction ratios ( DIRC\nandDIRP ) for different bias classes and\na baseline.\n0.0 0.2 0.4 0.6 0.8 1.0\nDeep Ratios0.20.40.60.81.0CDFLeft\nCenter\nRight\nCombined\nPublicFigure 7: CDFs of DIRC (combined) and\nDIRP (public) for different bias classes.\n0.0 0.2 0.4 0.6 0.8 1.0\nDeep Ratios0.20.40.60.81.0CDFMost-unreliable\nUnreliable\nReliable\nMost-reliable\nCombined\nPublicFigure 8: CDFs of DIRC (combined) and\nDIRP (public) for different reliability\nclasses.\nand articles with few combined interactions may exhibit relatively\nsmaller public shares (compared to the Leftclass).\nTo confirm this conjecture and substantiate our claim that the\nabove differences are due to aggregated statistics being more af-\nfected by larger public interaction shares associated with the tail,\nwe compared the public interaction ratios for articles associated\nwith the tail and head of the distributions of the combined num-\nber of interactions. For this analysis, we split the total sets ( D\ud835\udc43\ud835\udc3c\ud835\udc45\n\ud835\udc3f\ud835\udc52\ud835\udc53\ud835\udc61\nandD\ud835\udc43\ud835\udc3c\ud835\udc45\n\ud835\udc45\ud835\udc56\ud835\udc54\u210e\ud835\udc61) into two to five equally sized subsets (based on the\ntotal combined number of interactions \ud835\udc3c\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc4f\n\ud835\udc50,\ud835\udc56associated with each\nentry) and then compared the medians and averages of the public\ninteraction ratios seen in the first and last bucket of the Right class\nwith the corresponding values seen for the Leftclass. In all cases,\ntheLeftdominates the Right for the head and is dominated by the\nRight for the tail. For example, with three equal-sized buckets, the\ntwo classes have median ratios of 0.34 vs. 0.13 for the first bucket\n(head) and 0.39 vs. 0.46 for the last bucket (tail). Furthermore, we\nobserved statistically significant differences (99% confidence level)\nas per the above conjecture with both the Mann-Whitney U test\n(medians) and Bootstrap tests (averages), validating these differ-\nences. This confirms that the articles in the Right class with the\nhighest interactions have elevated normalized public interaction\nratios, and those in the Leftclass with the lowest interactions have\nsimilarly elevated ratios.\nTakeaway: In terms of bias, the Right class sees the smallest\npublic interaction share among the least popular content to\ninteract with, while the Leftclass sees relatively less overall\npublic interaction share (due to its most popular articles to\ninteract with not seeing as big public interaction share).\n5 Deep vs. Shallow Interactions\nIn this section, we investigate the prevalence of deep vs.shallow\ninteractions and explore their variations among different reliability\nand bias classes. For this analysis, we categorize all (emoji-based)\nreactions (e.g., likes, loves, sads) as shallow interactions while com-\nments and shares are considered as deep interactions. This distinc-\ntion, supported by the previous works [ 2,21], is motivated by the\nidea that comments and shares typically involve more cognitive\neffort and engagement from users. For example, comments involve\nformulating thoughts and opinions, while shares actively endorse\nand disseminate content.To compare the depth of interaction seen for subsets of public\nand private posts, we define the following two per-article metrics:\n\u2022Deep Interactions Ratio for Combined (DIRC) : For each ar-\nticle, this ratio measures the proportion of the combined\ninteractions that are classified as deep interactions.\n\u2022Deep Interactions Ratio for Public (DIRP) : For each article, this\nratio measures the proportion of the public interactions that\nare classified as deep interactions.\nTo compute DIRC , we divide the number of deep interactions by\nthe total interactions for each URL. For DIRP , we first identify the\ndeep interactions (shares and comments) occurring on public posts\nassociated with the URL. Summing these deep interactions across\nall public posts and dividing by the total number of public posts\u2019\ninteractions for the URL yields the DIRP . For example, an article\nwith 1,000 total interactions, 500 of which are deep, has a DIRC of\n0.5. If 400 interactions are public, and 100 of those are deep, the\nDIRP is 0.25. We next analyze the distribution of DIRC andDIRP\nvalues for all articles in each class.\nComparisons of bias classes: We first compare the deep inter-\naction patterns of the bias classes. Figure 6 shows the distributions\nofDIRC andDIRP for the different classes. As a baseline, we also\ninclude the distributions for the general population (rightmost box\npair), considering all samples.\nFirst, referring to the figure, for each bias class, we observe sub-\nstantial differences between the distributions of DIRC andDIRP . For\nexample, for every percentile and for the means, the DIRC distri-\nbutions (combined) have significantly larger values than the DIRP\ndistributions (public), indicating that users engage in more substan-\ntive interactions in less-public settings. This suggests that deeper\nengagement\u2014through comments and shares\u2014tends to compara-\ntively have higher ratios in spaces with fewer visibility constraints,\nwhere users may feel more comfortable expressing opinions or en-\ndorsing content. This pattern is consistent across all bias (Figure 7)\nand reliability classes (Figure 8), reinforcing the notion that pri-\nvate interactions are an essential component of online engagement\ndynamics. This again highlights the difference between the public\nand combined interactions and the importance of considering both\nthe private and public spheres in future research. This observation\nleads us to the following key insight.\nTakeaway: Irrespective of the news class, users tend to engage\nmore deeply in private discussions compared to public ones.\nPublic versus Less-Public News Engagement on Facebook: Patterns Across Bias and Reliability Websci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA\nNow, and perhaps even more importantly, to better understand\nto what extent the relative level of interaction seen for different\nbias classes depends on which dataset was used we apply the same\nstatistical testing methodology as discussed in previous sections for\nthe two sets independently. First, the Kruskal-Wallis test provides\nevidence that the distributions differ significantly among the bias\nclasses, regardless of using the DIRC andDIRP , with p-values of\n7.14\u00b710\u221223and2.08\u00b710\u22129, respectively. Comparing the p-values,\nwe note that the differences appear more significant using the\ncombined set ( DIRC ) than using only the public data ( DIRP ). Second,\nconsistent with visual comparison in the figure, the post-hoc Dunn\ntest only supports statistically significant differences between the\nRight group (which has the biggest fraction of deep interactions\nregardless of using the combined or pubic sets) and the other two\ngroups but not among the Center andLeftgroup. Finally, the Mann-\nWhitney U test and bootstrapping support the same pattern for\nthe median and means, with the only significant differences again\nbeing between the Right class and the other two bias classes. For\nthe bias classes, the main results regarding the relative depth of the\ninteractions among the classes are, therefore, consistent regardless\nof the dataset.\nTo illustrate the tendency of the posts with Right -biased articles\nseeing a higher deep interaction share, regardless of whether pub-\nlicly shared or not, we include the CDFs of DIRC (combined) and\nDIRP (public) for all three biases classes in Figure 7. We note that\nin both cases, the Right class exhibits a noticeable shift to the right,\nsupporting the following insight.\nTakeaway: Content from the Right bias class in both public\nand combined contexts gets deeper user interactions.\nComparison of reliability classes: We now shift our focus\nto the reliability classes and their deep interaction ratios. Figure 8\nsummarizes these results. While we again observe deeper interac-\ntions for the combined set (suggesting shallower interactions with\npublic posts), the relative differences among the reliability classes\nthemselves are less apparent compared to those observed among\nthe bias classes. We also observe some smaller differences in which\nrelative differences are significant, highlighting the value of also\nconsidering the combined data (not only the public data).\nFirst, starting with the combined dataset ( DIRC ), we observe the\npresence of significant differences among the distributions (Kruskal-\nWallis test), with the post-hoc Dunn test (distribution) and Mann-\nWitney U test (median) revealing that only the Most-unreliable\nclass differs statistically from the others. This observation is further\nvisually supported in Figure 8, which clearly shows a shift to the\nright in the DIRC distribution of the Most-unreliable class.\nSwitching our focus to the DIRP distributions, containing only\npublic interactions, we interestingly observe a slightly different\npattern, as we here also observe statistical differences between\nsome additional classes. First, as suggested by the DIRP CDFs in\nFigure 8, both extreme classes ( Most-unreliable andMost-reliable )\nare statistically different from the other two classes (Kruskal-Wallis\ntest followed by Mann-Whitney test, followed by the post-hoc Dunn\ntest). Second, no statistical significance was observed between the\nMost-reliable and Most-unreliable classes themselves or betweentheReliable andUnreliable classes. We base the following insight\nbased on our observations across the two datasets.\nTakeaway: Users exhibit higher deep interaction levels with\ncontent from the Most-unreliable class.\n6 Topic-Based Analysis\nIn this section, we extend our investigation by conducting a topic-\nbased analysis of news articles to assess whether engagement pat-\nterns vary across different content themes. For this analysis, we\nfirst extracted the textual content from the dataset news articles,\na process that succeeded for 14,996 items. We then employed the\nChatGPT 4o-mini model to classify each article into one of 14 prede-\nfined topics: Politics ,Health & Medicine ,Crime ,Business & Finance ,\nEnvironment & Climate ,Entertainment ,Education ,Sports ,Science ,\nLifestyle & Leisure ,Religion ,Technology ,Arts & Culture , and Food.\nThis specific set of topics was selected to capture the diverse range\nof subject matters prevalent in contemporary news consumption,\nwhile also reflecting societal interests and trends.\nNotably, the ordering of topics here mirrors the distribution ob-\nserved in our dataset\u2014ranging from the most frequent ( Politics ,\nwith 7,956 articles) to the least frequent ( Food, with 57 articles).\nHowever, it is important to note that these frequencies are influ-\nenced by two key factors: (1) the initial article selection criteria of\nAd Fontes Media, and (2) the timeframe of our dataset, which spans\na period heavily impacted by the COVID-19 pandemic, leading to\nan elevated presence of Health & Medicine content (the second most\nprevalent topic). Finally note that for articles where none of these\npredefined topics were clearly identifiable, the model was instructed\nto leave them uncategorized. In total, 14,650 articles (97.7%) were\nsuccessfully classified into one of the 14 categories.\nFirst, to assess whether our earlier findings hold within spe-\ncific topics, we conducted a stratified analysis across topic groups.\nOur results indicate that the previously observed patterns remain,\nwith statistical significance for several of the classes with enough\nsamples. For example, in the case of the Politics topic the insights\nreported in both Sects. 4 and 5 remain statistically robust when con-\nsidering the reliability dimension. As another example, for Health\n& Medicine (the 2nd largest topic), all the conclusions in Sects. 4\nand 5 for both the bias and reliability dimensions are still statisti-\ncally significant. These confirm the general validity of our broader\nfindings within many of these topics.\nSecond, to explore how engagement varies across topics, we\nanalyzed the distributions of the public interaction ratio for each\ntopic. This analysis was conducted independent of the articles\u2019 bias\nand reliability, as some topics had too few samples to allow for a\nbreakdown by those categories. Figure 9 shows the distribution\nof the public interaction ratios (as a boxplot) for each of the top-\nics, sorted based on their medians. From this figure, we can first\nsee that Entertainment -related articles exhibit the highest public\ninteraction ratio, indicating that content from this category is more\nlikely to be engaged with in public discussions. This suggests that\nentertainment news, which often includes celebrity updates and\nviral stories, is inherently more shareable and widely disseminated\nin public spaces. On the other hand, Technology -related articles\nWebsci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA Alireza Mohammadinodooshan and Niklas Carlsson\nTechnologyReligionEducation\nHealth & MedicineBusiness & FinanceArts & CulturePolitics Science\nLifestyle & Leisure\nEnvironment & ClimateCrime SportsFood\nEntertainment0.000.250.500.751.00Public Interaction Ratio\nMedian\nFigure 9: Boxplots of the public interaction ratio for different\ntopics (sorted by median).\nTechnologyReligionEducation\nHealth & MedicineBusiness/FinanceArts & CulturePoliticsScience\nLifestyle and LeisureEnvironment & ClimateCrimeSportsFood\nEntertainmentTechnology\nReligion\nEducation\nHealth & Medicine\nBusiness/Finance\nArts & Culture\nPolitics\nScience\nLifestyle and Leisure\nEnvironment & Climate\nCrime\nSports\nFood\nEntertainmentNon-significant (p >= 0.1)\nSignificant (p < 0.1)\nSelf-comparison (N/A)\nFigure 10: Heatmap of Kruskal-Wallis test results for topics\u2019\npublic interaction ratios.\nexhibit the lowest public interaction share, meaning they receive\na disproportionately high share of engagement in private spaces.\nThis may be attributed to the technical nature of these articles,\nwhich are often discussed within niche communities or shared in\nprofessional or interest-based private groups.\nIt is important to note that not all observed differences in public\ninteraction ratios between topics in Figure 9 are statistically sig-\nnificant. To validate the statistical significance of these observed\ndifferences, we conducted pairwise Kruskal-Wallis tests between\ntopic groups. Figure 10 presents a heatmap illustrating the statis-\ntical significance of differences in public interaction ratios across\ntopics. Orange-colored cells here indicate significant differences\n(\ud835\udc5d<0.1), while gray cells indicate non-significant comparisons.\nThis analysis confirms that Entertainment -related content has a\nsignificantly higher public interaction share compared to all other\ntopics, whereas Technology -related content exhibits significantly\nlower public engagement compared to most categories. Conversely,\ntopics such as Food, we could not find statistically significant differ-\nences in public interaction ratios compared to other topics (except\nforEntertainment ). It is worth noting that we selected a p-valuethreshold of 0.1 for our analysis, as stricter thresholds (e.g., 0.01) did\nnot yield many statistically significant differences between topics.\nThis more lenient threshold was chosen primarily to compensate\nfor the limited sample size for certain topics in our dataset.\n7 Top Publishers Analysis\nTo further contextualize our findings, we analyze the public inter-\naction ratio ( PIR) and the deep interaction ratio ( DIRC ) for the top\npublishers in our dataset. In this context, \u201ctop publishers\u201d refers\nto those with the highest number of articles in our (and Ad Fontes\nMedia) dataset. For this analysis, we excluded Yahoo, as it primar-\nily functions as a news aggregator and does not produce original\ncontent.\nWe begin by examining the public interaction ratio across the\ntop publishers. Figure 11a presents a boxplot of the public inter-\naction ratio for each outlet, sorted by median values. For easier\ninterpretation, the color of each \u201cbox\" indicates the bias class (i.e.,\nLeft,Center , orRight ) associated with each publisher (based on the\nlabeling of the articles they have published). It is worth noting that\nsome outlets, such as Politico andNBC News , may shift between the\nLeftandCenter classifications over time due to editorial changes or\nevolving media landscapes. From this figure, we observe that New\nYork Post exhibits the lowest public interaction ratio, suggesting\nthat a larger proportion of its engagement occurs in private spaces.\nThis could be attributed to the nature of its audience, which may\nprefer engaging with its content in more private settings, such\nas closed groups. Additionally, the sensationalist reporting of the\nNew York Post might encourage discussions that users feel more\ncomfortable having in less-visible online spaces.\nNext, we investigate which of these publishers generate the\ndeepest user engagement, as measured by the deep interaction\nratio ( DIRC ). Figure 11b illustrates the DIRC distributions for the\ntop publishers, again sorted by median values. Notably, New York\nPost andFox News , both from the Right class, exhibit the highest\nmedian DIRC values, followed by CNN andNew York Times from the\nLeftclass. At the lower end of the spectrum, we find NPR andReuters ,\nboth from the Center class. In general, we observe that publishers\nwith the highest DIRC values tend to be biased ones, while those\nfrom the Center class have the lowest values. The probability of\nthis pattern occurring if we randomly sort the 10 publishers is\napproximately 2.22%. This observation may suggest that users are\nmore likely to engage deeply with content from publishers that\nhave a clear political leaning, compared to those that maintain a\nmore neutral stance.\n.\n8 Related Works\nThis research most closely relates to studies exploring user engage-\nment dynamics with various social media content. Our main contri-\nbution lies in the novel analysis of interaction disparities between\npublic and private news article sharing on Facebook, particularly\nconcerning articles with varying bias and reliability. Examining\ninteractions depth adds another layer to our analysis and findings.\nThere exist studies that partially address some dimensions high-\nlighted in this paper. However, as discussed in the introduction, the\nvast majority of these works, particularly pertaining to Facebook,\nPublic versus Less-Public News Engagement on Facebook: Patterns Across Bias and Reliability Websci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA\nNew York Post CNN (website)\nThe New York TimesReuters\nFox News (website)Washington PostPolitico\nNBC News (website)BreitbartNPR0.00.20.40.60.8Public Interaction Ratio\nMedian\n(a) Boxplots of the public interaction ratios (sorted by median).\nNPR\nReuters\nNBC News (website)Washington PostPoliticoBreitbart\nThe New York TimesCNN (website)\nFox News (website)New York Post0.20.30.40.50.60.7Deep Interaction Ratio\n Median (b) Boxplots of the deep interaction ratios (sorted by median).\nFigure 11: Comparison of (a) public interaction ratio and (b) deep interaction ratio for the top-10 outlets.\npredominantly concentrate on the platform\u2019s public sphere. To con-\ntextualize our study within the context of the existing literature\nand to highlight the novelty of our contribution, we next describe\nthe most closely related works.\nOne line of this prior research has studied the differences in the\nengagement levels of different bias or reliability (or both) classes\non different platforms including Facebook [ 4,10,17], Reddit [ 5,44],\nand Twitter[ 29,43]. For example, Edelson et al. [ 10], while focusing\non the public sphere, conducted a large-scale study on user engage-\nment with 7.5 million posts from 2,551 publishers (Facebook pages)\nacross both bias and reliability dimensions. Their results show that\nindividual posts from non-misinformation news outlets tend to at-\ntract lower median engagement than misinformation, aligning with\nour findings. Weld et al. [ 44] carried out a similar investigation on\nReddit rather than on Facebook. When considering the reliability\nparameter, Vosoughi et al. [ 43] analyzed a dataset of approximately\n126,000 stories tweeted by over 3 million people more than 4.5 mil-\nlion times, finding that false news stories reached more people than\ntrue news and diffused significantly farther, faster, deeper, and more\nbroadly than the truth across various categories of information. No-\ntably, during the critical period of the COVID-19 pandemic and by\ndoing a cross-national study, Altay et al. [ 4] delineated the surge in\nonline news consumption, with credible news outlets witnessing a\nsignificant boost. Samory et al. [ 37] characterized the social media\nnews sphere through user co-sharing practices by focusing on 639\nnews sources, both credible and questionable, and divided them\ninto 4 clusters, and characterizing them according to the audience\nthat shares their articles on Twitter and how the stylometric fea-\ntures used by each cluster is successful in getting users engagement.\nLamot et al. [ 23] examined how news headlines are remediated on\nFacebook and how this affects user engagement. Finally, Boukes et\nal. [6] compared user-content interactivity and audience diversity\nacross news and satire, finding differences in online engagement be-\ntween satire, regular news, and partisan news. Considering the bias\ndimension, on the other hand, Wischnewski et al. [ 45] discovered\nthat users are more inclined to share hyperpartisan news articles\nthat coincide with their own political views. The bias towards the\nright party in sharing and engagement with news is another thesis\nthat has been studied by previous works on both Twitter [14] and\nFacebook [13].\nResearch has also examined other dimensions of user engage-\nment, with, for example, Aldous et al. [ 2] showing that contenttopics influence engagement. Studies have found that emotional\ncontent generates higher Facebook engagement, with Maier et\nal. [25] demonstrating this effect while controlling for author in-\nfluence. Finally, some works have considered the time factor and\nanalyzed temporal patterns of engagement with different news\nclasses [41].\nIn summary, while previous research has partly explored some\ndimensions of user engagement with news, our study is the first\nto directly compare the dynamics of public and private sharing of\nnews articles on Facebook.\n9 Conclusions\nIn conclusion, this study makes several important contributions\nto the understanding of news engagement on social media. By\ndeveloping a robust methodology through collaboration with the\nCrowdTangle team, we present the first comprehensive compari-\nson of engagement dynamics between Facebook\u2019s publicly tracked\ncontent and overall platform interactions. Our analysis of over 19K\nnews articles reveals that public engagement patterns often fail to\nreflect platform-wide behavior, with significant implications for\nresearch methodology and content moderation strategies.\nOur findings challenge several assumptions about news engage-\nment on Facebook. First, users consistently engage more deeply\nwith content in less public spaces, regardless of the content\u2019s bias or\nreliability classification. Second, particular attention should be paid\nto right-biased and least reliable news content, which generate no-\ntably deeper engagement across both public and less public spheres.\nThird, we highlight some important limitations of focusing solely\non Facebook\u2019s public sphere. For example, the extremities of the\nreliability spectrum ( Most-reliable andMost-unreliable ) exhibited a\ntendency to elicit deeper interactions from users within the public\nsphere. However, this narrative shifted when examining combined\ninteractions, indicating that only the Most-unreliable class triggered\nstatistically significant deeper engagement from Facebook users.\nThis finding suggests that research conclusions drawn solely from\npublic data may not accurately represent overall platform engage-\nment patterns, as not only the magnitude of engagement differs\nbetween public and private but also the relative ordering of how\ndifferent content types engage users.\nThese insights carry significant implications for multiple stake-\nholders. For researchers, our findings underscore the importance\nWebsci \u201925, May 20\u201324, 2025, New Brunswick, NJ, USA Alireza Mohammadinodooshan and Niklas Carlsson\nof considering engagement disparities between public and less pub-\nlic spheres when studying social media behavior, particularly for\nnews content at different ends of the bias and reliability spectrums.\nFor content moderators, our results suggest the need to reevaluate\nstrategies based solely on publicly visible interactions. For policy-\nmakers, our findings highlight the importance of considering both\npublic and less public spheres when developing policies for social\nmedia governance.\nFuture research should investigate the underlying factors driving\nthese engagement patterns, particularly through user interviews,\nbehavioral studies, and analysis of post comments. While challenges\nremain in understanding social media dynamics, our findings il-\nluminate significant differences in how users interact with news\nacross Facebook\u2019s diverse spheres, highlighting the complexity of\nnews-sharing behavior on social media platforms.\nAcknowledgments\nThe authors express their gratitude to CrowdTangle for providing\nthe Facebook data. They also extend their thanks to the anonymous\nreviewers for their insightful comments that helped improve the\npaper. This work was partially supported by the Wallenberg AI,\nAutonomous Systems and Software Program (WASP) funded by the\nKnut and Alice Wallenberg Foundation.\nReferences\n[1]Ad Fontes Media. 2024. Ad Fontes Media Bias Chart. https://adfontesmedia.com/\ninteractive-media-bias-chart/.\n[2]Kholoud Khalil Aldous, Jisun An, and Bernard J. Jansen. 2019. View, Like, Com-\nment, Post: Analyzing User Engagement by Topic at 4 Levels across 5 Social\nMedia Platforms for 53 News Organizations. In Proc. ICWSM , Vol. 13. 47\u201357.\n[3] AllSides. 2023. AllSides. https://www.allsides.com/.\n[4]Sacha Altay, Rasmus Kleis Nielsen, and Richard Fletcher. 2022. Quantifying\nthe \u201cinfodemic\u201d: People turned to trustworthy news outlets during the 2020\ncoronavirus pandemic. J. Quant. Descr. Digit. Media 2 (Aug. 2022).\n[5]Robert M Bond and R Kelly Garrett. 2023. Engagement with fact-checked posts\non Reddit. PNAS Nexus 2, 3 (01 2023).\n[6]Mark Boukes, Xiaotong Chu, MF Abdulqadir Noon, Rufei Liu, Theo Araujo, and\nAnne C Kroon. 2022. Comparing user-content interactivity and audience diversity\nacross news and satire: Differences in online engagement between satire, regular\nnews and partisan news. J. Inf. Technol. Politics 19, 1 (2022), 98\u2013117.\n[7]Giovanni Brena, Marco Brambilla, Stefano Ceri, Marco Di Giovanni, Francesco\nPierri, and Giorgia Ramponi. 2019. News Sharing User Behaviour on Twitter: A\nComprehensive Data Collection of News Articles and Social Interactions. In Proc.\nICWSM , Vol. 13. 592\u2013597.\n[8]Jacob Liedke Christopher Aubin. 2024. Social Media and News Fact Sheet. Pew\nResearch Center (2024).\n[9]Ratan Dey, Zubin Jelveh, and Keith Ross. 2012. Facebook users have become\nmuch more private: A large-scale study. In Proc. IEEE PerCom . 346\u2013352.\n[10] Laura Edelson, Minh-Kha Nguyen, Ian Goldstein, Oana Goga, Damon McCoy, and\nTobias Lauinger. 2021. Understanding Engagement with U.S. (Mis)Information\nNews Sources on Facebook. In Proc. IMC . 444\u2013463.\n[11] Matt Garmur. 2022. Chrome Extension: Why do the items add up to more than\nthe totals? https://archive.is/mrNPU\n[12] Christine Geeng, Savanna Yee, and Franziska Roesner. 2020. Fake news on\nFacebook and Twitter: Investigating how people (don\u2019t) investigate. In Proc. CHI .\n[13] Sandra Gonz\u00e1lez-Bail\u00f3n, David Lazer, Pablo Barber\u00e1, Meiqing Zhang, Hunt All-\ncott, Taylor Brown, Adriana Crespo-Tenorio, Deen Freelon, Matthew Gentzkow,\nAndrew M Guess, et al .2023. Asymmetric ideological segregation in exposure to\npolitical news on Facebook. Science 381, 6656 (2023), 392\u2013398.\n[14] Sandra Gonz\u00e1lez-Bail\u00f3n, Valeria d\u2019Andrea, Deen Freelon, and Manlio\nDe Domenico. 2022. The advantage of the right in social media news sharing.\nPNAS Nexus 1, 3 (07 2022), pgac137.\n[15] Andrew Guess, Brendan Nyhan, and Jason Reifler. 2018. Less than you think:\nPrevalence and predictors of fake news dissemination on Facebook. Science\nAdvances 4, 1 (2018).\n[16] Ehsan-Ul Haq, Yang K Lu, and Pan Hui. 2022. It\u2019s All Relative! A Method to\nCounter Human Bias in Crowdsourced Stance Detection of News Articles. Proc.\nACM on Human-Computer Interaction 6, CSCW (2022), 1\u201325.[17] Dan Hiaeshutter-Rice and Brian Weeks. 2021. Understanding Audience En-\ngagement with Mainstream and Alternative News Posts on Facebook. Digital\nJournalism 9, 5 (2021), 519\u2013548.\n[18] Benjamin D Horne, Jeppe N\u00f8rregaard, and Sibel Adal\u0131. 2019. Different spirals of\nsameness: A study of content sharing in mainstream and alternative media. In\nProc. ICWSM , Vol. 13. 257\u2013266.\n[19] Ferenc Husz\u00e1r, Sofia Ira Ktena, Conor O\u2019Brien, Luca Belli, Andrew Schlaikjer, and\nMoritz Hardt. 2022. Algorithmic amplification of politics on Twitter. Proceedings\nof the National Academy of Sciences 119, 1 (2022).\n[20] Jason J Jones, Robert M Bond, Eytan Bakshy, Dean Eckles, and James H Fowler.\n2017. Social influence and political mobilization: Further evidence from a ran-\ndomized experiment in the 2012 US presidential election. PloS one 12, 4 (2017).\n[21] Cheonsoo Kim and Sung-Un Yang. 2017. Like, comment, and share on Facebook:\nHow each behavior differs from the other. Public Relations Review 43, 2 (2017).\n[22] Brian Knutson, Tiffany W Hsu, Michael Ko, and Jeanne L Tsai. 2024. News source\nbias and sentiment on social media. PloS one 19, 10 (2024).\n[23] Kenza Lamot, Tim Kreutz, and Micha\u00ebl Opgenhaffen. 2022. \u201cWe Rewrote This\nTitle\u201d: How News Headlines Are Remediated on Facebook and How This Affects\nEngagement. Social Media + Society 8, 3 (2022).\n[24] Danielle Lottridge and Frank R. Bentley. 2018. Let\u2019s Hate Together: How People\nShare News in Messaging, Social, and Public Networks. In Proc. CHI . 1\u201313.\n[25] Scott R. Maier. 2015. Compassion Fatigue and the Elusive Quest for Journalistic\nImpact: A Content and Reader-Metrics Analysis Assessing Audience Response.\nJournalism & Mass Communication Quarterly 92, 3 (2015), 700\u2013722.\n[26] Alice E Marwick. 2018. Why do people share fake news? A sociotechnical model\nof media effects. Georgetown law technology review 2, 2 (2018), 474\u2013512.\n[27] Media Bias Fact Check. 2024. Media Bias Fact Check. https://mediabiasfactcheck.\ncom/.\n[28] Meta. 2023. URL - Graph API Reference. https://developers.facebook.com/docs/\ngraph-api/reference/v21.0/url. Accessed: 2024-09-11.\n[29] Alireza Mohammadinodooshan and Niklas Carlsson. 2024. Understanding En-\ngagement Dynamics with (Un) Reliable News Publishers on Twitter. In Proc.\nASONAM . 36\u201347.\n[30] Seungahn Nah and Masahiro Yamamoto. 2018. Communication and Citizenship\nRevisited: Theorizing Communication and Citizen Journalism Practice as Civic\nParticipation. Communication Theory 29, 1 (07 2018), 24\u201345.\n[31] Nic Newman, Richard Fletcher, Kirsten Eddy, Craig T. Robertson, and Ras-\nmus Kleis Nielsen. 2023. Digital News Report 2023.\n[32] NewsGuard. 2024. NewsGuard. https://www.newsguardtech.com/.\n[33] Vanessa Otero. 2021. Ad Fontes Media\u2019s Multi-Analyst Content Analysis White\nPaper. https://adfontesmedia.com/white-paper-2021\n[34] Gordon Pennycook, Tyrone D Cannon, and David G Rand. 2018. Prior exposure\nincreases perceived accuracy of fake news. J. Exp. Psychol. Gen. 147, 12 (2018).\n[35] J\u00fcrgen Pfeffer, Daniel Matter, and Anahit Sargsyan. 2023. The Half-Life of a\nTweet. Proc. ICWSM (2023), 1163\u20131167.\n[36] Srihaasa Pidikiti, Jason Shuo Zhang, Richard Han, Tamara Lehman, Qin Lv, and\nShivakant Mishra. 2020. Understanding how readers determine the legitimacy of\nonline news articles in the era of fake news. In Proc. ASONAM . 768\u2013775.\n[37] Mattia Samory, Vartan Kesiz Abnousi, and Tanushree Mitra. 2020. Characterizing\nthe social media news sphere through user co-sharing practices. In Proc. ICWSM ,\nVol. 14. 602\u2013613.\n[38] Tess. 2021. What data is CrowdTangle tracking? https://archive.is/ItEuA.\n[39] Nik Thompson, Xuequn Wang, and Pratiq Daya. 2020. Determinants of News\nSharing Behavior on Social Media. J. Comput. Inf. Syst. 60, 6 (2020), 593\u2013601.\n[40] Elin Thorgren, Alireza Mohammadinodooshan, and Niklas Carlsson. 2024. Tem-\nporal Dynamics of User Engagement on Instagram: A Comparative Analysis of\nAlbum, Photo, and Video Interactions. In Proc. WebSci . 224\u2013234.\n[41] Luca Vassio, Michele Garetto, Emilio Leonardi, and Carla Fabiana Chiasserini.\n2022. Mining and modelling temporal dynamics of followers\u2019 engagement on\nonline social networks. Social Network Analysis and Mining 12, 1 (2022), 96.\n[42] Jessica Vitak, Paul Zube, Andrew Smock, Caleb T Carr, Nicole Ellison, and Cliff\nLampe. 2011. It\u2019s complicated: Facebook users\u2019 political participation in the 2008\nelection. CyberPsychology, behavior, and social networking 14, 3 (2011), 107\u2013114.\n[43] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false\nnews online. Science 359, 6380 (2018), 1146\u20131151.\n[44] Galen Weld, Maria Glenski, and Tim Althoff. 2021. Political Bias and Factualness\nin News Sharing across more than 100,000 Online Communities. In Proc. ICWSM ,\nVol. 15. 796\u2013807.\n[45] Magdalena Wischnewski, Axel Bruns, and Tobias Keller. 2021. Shareworthiness\nand Motivated Reasoning in Hyper-Partisan News Sharing Behavior on Twitter.\nDigital Journalism 9, 5 (2021), 549\u2013570.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Understanding Differences in News Article Interaction Patterns on Facebook: Public vs. Private Sharing with Varying Bias and Reliability", "author": ["A Mohammadinodooshan", "N Carlsson"], "pub_year": "2023", "venue": "arXiv preprint arXiv:2305.11943", "abstract": "The rapid growth of news dissemination and user engagement on social media has raised  concerns about the influence and societal impact of biased and unreliable information. As a"}, "filled": false, "gsrank": 465, "pub_url": "https://arxiv.org/abs/2305.11943", "author_id": ["CiyNDzoAAAAJ", "Qd-5OWMAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:VoW90cuvpUoJ:scholar.google.com/&output=cite&scirp=464&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D460%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=VoW90cuvpUoJ&ei=WbWsaIrWIPnSieoPxKLpgQ0&json=", "num_citations": 4, "citedby_url": "/scholar?cites=5378898619896005974&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:VoW90cuvpUoJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2305.11943"}}, {"title": "Covid-19 on social media: Analyzing misinformation in twitter conversations", "year": "2020", "pdf_data": "COVID-19 ONSOCIAL MEDIA : ANALYZING MISINFORMATION\nINTWITTER CONVERSATIONS\nA P REPRINT\nKarishma Sharma, Sungyong Seo, Chuizheng Meng, Sirisha Rambhatla, Yan Liu\nDepartment of Computer Science\nUniversity of Southern California\nLos Angeles, USA.\n{krsharma,sungyons,chuizhem,sirishar,yanliu.cs}@usc.edu\nOctober 23, 2020\nABSTRACT\nThe ongoing Coronavirus (COVID-19) pandemic highlights the inter-connectedness of our present-\nday globalized world. With social distancing policies in place, virtual communication has become an\nimportant source of (mis)information. As increasing number of people rely on social media platforms\nfor news, identifying misinformation and uncovering the nature of online discourse around COVID-19\nhas emerged as a critical task. To this end, we collected streaming data related to COVID-19 using\nthe Twitter API, starting March 1, 2020. We identi\ufb01ed unreliable and misleading contents based on\nfact-checking sources, and examined the narratives promoted in misinformation tweets, along with\nthe distribution of engagements with these tweets. In addition, we provide examples of the spreading\npatterns of prominent misinformation tweets. The analysis is presented and updated on a publically\naccessible dashboard1to track the nature of online discourse and misinformation about COVID-19 on\nTwitter from March 1 - June 5, 2020. The dashboard provides a daily list of identi\ufb01ed misinformation\ntweets, along with topics, sentiments, and emerging trends in the COVID-19 Twitter discourse. The\ndashboard is provided to improve visibility into the nature and quality of information shared online,\nand provide real-time access to insights and information extracted from the dataset.\nKeywords COVID-19 \u0001Misinformation \u0001Fake News \u0001Social Media\n1 Introduction\nSocial media plays a pivotal role in information dissemination and consumption during a pandemic; more so with\nincreasing social distancing and growing reliance on online communication (Godfrey, 2020 (accessed April 20, 2020).\nIt has both positive and negative social impacts during the crisis. For instance, safety tips such as \u201cwash your hands\" and\n\u201cstay home\" are shared widely to gain community support in \ufb01ghting the COVID-19 pandemic (Godfrey, 2020 (accessed\nApril 20, 2020). On the other hand, misinformation and hate speech are growing problems that can adversely impact\nthe safety of individuals and society. The associated public health risk and other dire consequences of misinformation\nspread on social media - such as 5G towers being burned down due to conspiracy theories linking them to COVID-19\n(Parveen and Waterson, 2020 (accessed April 17, 2020), make it imperative to address the problem of misinformation.\nIn recent years, the reliance on social media as a source of news has continued to increase (Mitchell, 2018, Geiger,\n2019 (accessed March 20, 2020). Here, we focus our analysis on the social media platform, Twitter, to understand the\nnature of online discourse surrounding COVID-19, since Twitter has the highest number of news focused users (Hughes\nand Wojcik, 2019 (accessed March 20, 2020). The impact of misinformation surrounding the COVID-19 pandemic\ncan be especially damaging, since any missteps can increase the chances of an exponential spread of the disease, or\naccidental death due to self-medication Vigdor (2020 (accessed March 24, 2020). The emergence of COVID-19 related\nmisinformation on online platforms motivated the World Health Organization (WHO) to launch a \u201cMythbuster\u201d page\n1https://usc-melady.github.io/COVID-19-Tweet-AnalysisarXiv:2003.12309v4  [cs.SI]  22 Oct 2020\nAPREPRINT - OCTOBER 23, 2020\n(a) Sentiment and topic analysis of Twitter conversations.\n (b) Misinformation analysis on Twitter\nFigure 1: COVID-19 Social Media Analysis Dashboard analyzing sentiments, trends, and misinformation on Twitter.\nAccessible online at https://usc-melady.github.io/COVID-19-Tweet-Analysis .\nWHO (2020 (accessed March 20, 2020) at the start of the pandemic. However, such counter measures are limited in\ntheir ability to combat misinformation, due to the large-scale nature and fast-paced evolution of online discourse.\nTherefore, we utilize computational solutions to extract and examine the nature of social media conversations surround-\ning COVID-19. We collected streaming data related to the pandemic using the Twitter API, starting March 1, 2020. We\nidenti\ufb01ed unreliable and misleading contents based on fact-checking sources, and examined the narratives promoted in\nmisinformation tweets, along with the distribution of engagements with these tweets. In addition, we provide examples\nof the spreading patterns of prominent misinformation tweets.\nIn order to provide timely insights about the narratives and quality of information shared on social media during the\npandemic, the analysis is presented as a publically accessible dashboard (Fig 1) - tracking the online Twitter discourse\non COVID-19 from March 1 - June 5, 2020. The dashboard provides a daily list of identi\ufb01ed misinformation tweets,\nalong with topics, sentiments, and emerging trends in the COVID-19 Twitter discourse. The dashboard is provided\nto improve visibility into the nature and quality of information shared, and provide real-time access to insights and\ninformation extracted from the dataset. We aim to improve public awareness of the broader nature of online discourse\nrelated to the pandemic, towards enabling better discernment of misinformation claims.\n2 Related Work\nThe \ufb01rst outbreak of the COVID-19 pandemic (Novel Coronavirus Disease) was reported in Wuhan, China in late\nDecember, 2019; and has rapidly spread to several countries across the world during early March, with the United\nStates declaring a national emergency on March 13, 2020 [11]. The rapid infection rates and enforcement of social\ndistancing policies quickly promoted widespread online discussions regarding the pandemic on social media plat-forms;\nwith a growing number of conspiracies and false information.\nMisinformation on social media in general has been an increasingly pertinent problem in recent years (Sharma et al.,\n2019). The proliferation of misinformation related to elections, hurricanes, earthquakes, and civil discourse has\nbeen studied in different social and political contexts (Allcott and Gentzkow, 2017, Gupta et al., 2013). With the\nCOVID-19 pandemic, there have been several recent works analyzing different aspects of misinformation on social\nmedia. The majority of these studies are pre-print papers (accessed June, 2020) with early characterization of social\nmedia discussions on the topic (Cinelli et al., 2020, Cui and Lee, 2020, Singh et al., 2020).\nThe recent work in Singh et al. (2020) analyzed the temporal correlation between social media conversations on Twitter\nand disease outbreaks at different locations; and considered the distribution of information from different sources,\nbetween January and March, 2020. Cui and Lee (2020), Zhou et al. (2020) created a dataset based on fact-checked\nclaims related to COVID-19 for benchmarking different automated detection models (Ruchansky et al., 2017, Shu et al.,\n2019). Cinelli et al. (2020) modeled the spread of information on social media using epidemic models used in disease\nspread modeling Newman (2002) for different social media platforms.\nIn analysis of social bots during the pandemic, Ferrara (2020) provided an estimation and characterization of user\naccounts that re\ufb02ected automation or bot-like characteristics in COVID-19 discussions on social media. Their \ufb01ndings\nprovide early evidence of the existence of automated accounts used as news-posting bots, and ones promoting political\n2\nAPREPRINT - OCTOBER 23, 2020\nTable 1: Statistics of COVID-19 tweets collected between March 1-June 5, 2020.\nDataset(March 1 - June 5, 2020) Statistics\n# Tweets 85.04 M\n% Tweets (In English Lang) 63.88%\n% Tweets (with Geo Information) 43.02%\n# User Accounts 10.61 M\n% Veri\ufb01ed User Accounts 7.51%\nconspiracies. Similarly, Huang and Carley (2020) provided analysis of different types of user accounts. The analysis of\nbots and coordinated in\ufb02uence campaigns have been an area of active research in the context of Russian interference in\nthe 2016 US elections (Bessi and Ferrara, 2016, Luceri et al., 2020, Pacheco et al., 2020, Sharma et al., 2020).\nOther studies aim to understand the psychological impacts of misinformation on public perception, and sharing of false\ninformation on social media (Pennycook et al., Swami and Barron, 2020). Pennycook and Rand (2018) conducted a\nstudy with 1,600 participants, and found that participants were far less discerning when deciding to share true or false\ninformation on social media, relative to when asked directly about correctness of the information. In another study of\n520 subjects, Swami and Barron (2020) identi\ufb01ed that analytic thinking and rejection of COVID-19 conspiracy theories,\nrespectively, were signi\ufb01cantly and directly associated with compliance to mandated social-distancing measures. These\nresults have important implications regarding the risks of misinformation on public perception and in turn, on the\neffectiveness of health intervention policies aimed at controlling the spread of the pandemic.\nHere, we address the broader nature of the online discourse surrounding the COVID-19 pandemic, through examination\nof misinformation claims, and extraction of topics, sentiments and trends from Twitter conversations. The daily analysis\nof misinformation claims and online discourse from March 1 to June 5, 2020 on the data collected through the Twitter\nstreaming API was made publically available through the dashboard, with the aim of providing real-time insights and\nanalysis of the online discourse, towards enabling better public discernment of misinformation claims about COVID-19.\nTo tackle the real-time surge of misinformation on social media related to the COVID-19 pandemic, traditional methods\nto annotate each claim are not scalable with the magnitude of global conversations surrounding the pandemic. Therefore,\nsince earlier studies have shown strong correlation between credibility of news and news publishing websites; with\nevidence that most fake news is generated from low credibility or hyperpartisan websites (Zhou and Zafarani, 2018), we\nobtain and leverage information about news publishing websites from fact-checking sources to identify misinformation.\n3 Data Collection\nWe collected the dataset using the Twitter streaming API service2from March 1, 2020 to June 5, 2020. We used\nkeywords related to COVID-19 to \ufb01lter the Twitter stream and obtain relevant tweets about the pandemic. The stream\nfetches a 1% random sample of all tweets containing at least one of the keywords (\u2018Covid19\u2019, \u2018coronavirus\u2019, \u2018corona\nvirus\u2019, \u20182019nCoV \u2019, \u2018CoronavirusOutbreak\u2019, \u2018coronapocalypse\u2019), from the platform in real time. The COVID-19 crisis\nwas declared a global pandemic on March 11, 2020, which motivated the mentioned collection period. From March 1-8\nand March 18-19, due to interruptions in fetching the stream, we recovered data using tweet ids in Chen et al. (2020).\nDataset statistics. Table 1 provides details about the tweets collected and the user accounts associated with the tweets.\nThe dataset contains 85.04M tweets from 182 countries. The subset of English tweets equals 54.32M. The English\ntweets are utilized for further analysis and therefore the table reports the details about what fraction of English tweets\ncontains geolocation information, and count of unique user accounts associated with the tweets, as well as the percentage\nof Twitter veri\ufb01ed accounts among those user accounts.\nGeolocation. The tweet geolocation information at the country-level is extracted directly from tweet metadata if it has\ngeo-location enabled, otherwise extracted from the user reported locations in the user pro\ufb01le information associated\nwith the tweet (Dredze et al., 2013). The geo-location information is not always be available, in cases when the user\nreported location is not a valid geographical location. Geo-location enabled tweets are sparse, therefore geo-location\nfrom user pro\ufb01les is included. Table 2 provides the distribution of English tweets across countries, and the distribution\nof English tweets across US states.\n2https://developer.twitter.com/en/docs/tweets/filter-realtime/guides/basic-stream-parameters\n3\nAPREPRINT - OCTOBER 23, 2020\nTable 2: The distribution of English tweets across countries and across US states.\nTop Countries # Tweets\nUnited States 13290691\nUnited Kingdom 2657375\nIndia 1829656\nCanada 884420\nNigeria 698603\nAustralia 510069\nSouth Africa 278457\nPakistan 223659\nKenya 201537\nIreland 188229\nFrance 162343\nMalaysia 151586\nHong Kong 126006\nPhilippines 122805\nIndonesia 113489\nGermany 109109\nGhana 94975\nUganda 70814\nSpain 68993\nUAE 67224Top US States # Tweets\nCalifornia 1958121\nNew York 1091428\nFlorida 906426\nTexas 883169\nPennsylvania 437928\nDist. of Columbia 402157\nIllinois 387298\nGeorgia 364500\nOhio 353748\nArizona 311748\nMichigan 298246\nNew Jersey 287030\nVirginia 284819\nMassachusetts 263473\nWashington 232426\nOregon 221913\nNorth Carolina 219956\nMaryland 217763\nTennessee 215048\nColorado 177931\n4 Misinformation\nIncreased reliance on social media for news, and the risk of misinformation exposure on public health, have made\ntackling of misinformation claims time critical. Existing misinformation datasets are either pertaining to general\nnewsworthy events during a particular time period (Ma et al., 2016, 2018), or domain speci\ufb01c, such as related to the\nSyrian civil war or Hurricane Sandy (Salem et al., 2019, Gupta et al., 2013). In this work, we create a domain speci\ufb01c\ndataset on COVID-19 leveraging knowledge from fact-checking sources, for analysis of misinformation related to the\npandemic, and in the future, to improve research in misinformation related to healthcare and pandemics.\nCategorization of news sources The credibility of news is positively correlated with the credibility of the news\npublishing websites/sources publishing the news. Most fake news has been witnessed to be generated from low\ncredibility and hyperpartisan websites on social media (Zhou and Zafarani, 2018). Fact-checking sources (eg. Snopes,\nPolitiFact, Media Bias/Fact check, NewsGuard) conduct independent journalistic veri\ufb01cation on the credibility of\nboth, individual claims surfaced on social media, as well as the associated news publishing websites linked to false,\nunreliable and misleading claims. We compile information from three fact-checking sources that provide extensive\njournalistic analysis of low-quality news sources known to frequently publish unreliable and false information. The\nthree fact-checking sources considered in this work are Media Bias/Fact Check3, NewsGuard4and Zimdars (2016)5.\nNewsGuard maintains a repository of news publishing sources that have published false information about COVID-19.\nWe include Media Bias/Fact Check\u2019s list of questionable news sources with low and very low factual reporting. Zimdars\n(2016) maintains a list of different kinds of false, conspiracy, junk-science, clickbait and other types of news sources.\nNote that a tweet can belong to more than one type; as can multiple types be associated with a news source. We assign\nthe news sources to the above mentioned types based on the following criteria. For Media Bias/Fact Check, we include\nthe list of questionable news sources with reported low and very low factual content, into the unreliable categorization.\nSimilarly, we include news sources listed by NewsGuard for publishing false content related to COVID-19 into the\nunreliable categorization. In the case of Zimdars (2016), we include tags fake, rumor, unreliable, and satire, in\nthe unreliable categorization. We include tags conspiracy and junksci (pseudoscience, naturalistic fallacies) in the\nconspiracy categorization; clickbait tag in the clickbait categorization; and tags bias and political to the political/biased\n3https://mediabiasfactcheck.com/\n4https://www.newsguardtech.com/covid-19-resources/\n5https://docs.google.com/document/d/1zhaZooMfcJvk_23in201nviWJN1-LhRvGlPXJWBrPRY/edit?usp=\nsharing\n4\nAPREPRINT - OCTOBER 23, 2020\nTable 3: Dataset statistics of source tweets in misinformation cascades\nTweets (EN) Source Tweets Source Tweets (URLs) Misinformation Source Tweets\n54.32M 6.37M 4.58M 150.8K\ncategorization. The de\ufb01nitions of the different tags can be found in Zimdars (2016)6. As in Zimdars (2016), if a news\nsource is associated with more than one tag, we include it in the each of the categories associated with its tags. For\ninstance, \u2018political-clickbait\u2019 is an example of a news source publishing misleading information of a politically biased\nand clickbait type. We exclude news sources with solely the political tag in Zimdars (2016) and none of the other tags\nassociated with the other categorizations, as it consists of reliable news with a bias, but is not false or misleading.\nWe categorize information into four types - unreliable ,conspiracy ,clickbait , and political/biased . News sources can be\ncategorized into multiple types, and therefore, the associated tweets can have more than one type label.\n\u2022Unreliable . We de\ufb01ne the unreliable category to include false, questionable, rumorous and unreliable news. In\naddition, we include satire, based on the consideration that satire has the potential to perpetuate misinformation\n(Zimdars, 2016) or be used as a cover for misinformation publication (Sharma et al., 2019).\n\u2022Conspiracy. We de\ufb01ne conspiracy to include conspiracy theories and scienti\ufb01cally dubious news.\n\u2022Clickbait. This category includes clickbait news i.e. misleading, distorted or exaggerated headlines and/or\nbody purposed to attract attention, for reliable and/or unreliable information.\n\u2022Political/Biased. This category includes political and biased news, written in support of a particular point of\nview or political orientation, for reliable and/or unreliable information such as propaganda.\nExtraction of information cascades Information cascades (Yang and Leskovec, 2010) represent the diffusion or spread\nof information over social media. They can be considered as a time ordered sequence of posts originating from a source\npost and spreading through re-shares or engagements as retweets and replies to the source and subsequent posts.\nThe retweet/reply graph contained 42.71M edges i.e. retweet or reply links between the 54.32M English tweets collected\nin the dataset. The retweet or reply links are extracted from the tweet metadata collected using the Twitter API. We\nextract weakly connected components of the retweet/reply graph to identify source post cascades being propagated or\nshared over the social network. Each weakly connected component is a directed tree rooted at the source post, with\nother vertices and edges in the tree representing retweets and replies of the source or subsequent posts in the tree.\nEach source tweet has associated tweet text and user account features, and the extracted cascade originating at the\nsource post. We label the source tweet as misinformation (unreliable, conspiracy, clickbait, political/biased) if the tweet\nshared any article or content posted from any of the misinformation sources compiled using the fact-checking sources.\nThe source tweet metadata is used to identify external links to news articles referenced by the tweet.\n5 Misinformation Analysis\nIn this section, we analyze the distribution of misinformation tweets, and distribution of engagements with the\nmisinformation tweets from the collected dataset. This is followed by identifying textual narratives in the tweets though\ndistinctive hashtags promoted in the misinformation tweets. Lastly, we provide examples of misinformation cascades\nand visualize their propagation based on extracted geo-location information of tweets in the cascades.\n5.1 Misinformation dataset statistics\nIn this section, we \ufb01rst analyze the distribution of the misinformation cascades identi\ufb01ed in the datasets using\ninformation from fact-checking sources, as detailed in the previous section. Table 3 provides the statistics of identi\ufb01ed\nmisinformation tweets. As seen, 150.8K (3.29%) of source tweets with external links (urls) constitute the misinformation\nsource tweets that link to the unreliable, conspiracy, clickbait, political/biased news sources.\nWe further explore the distribution of low quality news publishing sources that are most widely linked to source tweets\nin misinformation cascades. In Fig 2 the breakdown of misinformation source tweets according to news publishing\nsources (websites) is provided. The \ufb01gure shows the top 25 most frequently linked news sources in source tweets of the\n6https://docs.google.com/document/d/1zhaZooMfcJvk_23in201nviWJN1-LhRvGlPXJWBrPRY/edit?usp=\nsharing\n5\nAPREPRINT - OCTOBER 23, 2020\nFigure 2: Distribution of news publishing sources linked to misinformation tweets\nFigure 3: V olume of misinformation source tweets by type per day in March-June\ndataset. The counts of source tweets that link to each source are provided. The top 25 news sources were linked to more\nthan a 100 source tweets in the collected dataset.\n5.2 Misinformation narratives\nWe leverage the misinformation categorization to examine the different narratives spread from low quality news\npublishing sources, on social media. We analyze the distribution of engagements in misinformation cascades, followed\nby textual analysis of the narratives of each type of misinformation.\nIn Fig 3, we examine the volume and distribution of source tweets over time for each category of misinformation, per\nday during the observed time period from March to June as shown. The volume of source tweets of misinformation\ncascades for each category (unreliable, conspiracy, clickbait, political/biased) are shown separately. In the case of tweets\nthat belong to more than one category, we count the tweet in each of the categories associated with it. The volume of\nmisinformation source tweets increases with time, as does the volume of overall tweets in the dataset, from March to\nJune. This increase can be indicative of an increase in the global activity and online discussions around COVID-19\ndue to spread of the pandemic to multiple countries starting from early March. The volume of source tweets linked to\nconspiracy and clickbait sources is smaller compared the unreliable and political/biased sources in the collected social\nmedia and news sources data.\nIn Fig 4, we explore the distribution of engagements in each category from the extracted misinformation cascades.\nThe \ufb01gure shows the relative distribution of source tweets to responses (replies/retweets of the source tweets) in\nmisinformation cascades for each category. As seen, the Unreliable and Conspiracy types receive fewer responses\nrelative to the volume of source tweets, as compared to other categories. The \u201cOthers\" category constitutes information\ncascades which are not labeled as misinformation as the source tweets in these cascades do not belong to any of the four\n6\nAPREPRINT - OCTOBER 23, 2020\nFigure 4: Relative volume of source tweets to responses (replies/retweets) for each misinformation category.\nTable 4: Top hashtags in misinformation category ordered by TF-IDF scores\nUnreliable Conspiracy Clickbait Political/Biased\nvaccine wwg1wga trumpvirus kag\nscummedia wuhanvirus coronaviruspandemic wwg1wga\nwuhanvirus freezerohedge trumpgenocide fakenews\nibackboris lockdown unitedstates wuhanvirus\nlockdown billgates pandemic trump2020\nkag fakenews commondreams pandemic\nwwg1wga chinavirus politics feedly\nfakenews pandemic gop chinavirus\ncovid19news hydroxychloroquine foxnews coronaviruspandemic\ncovid19india kag trumpisanationaldisgrace chinesevirus\nindonesiabebascovid19 who usa kag2020\n\ufb01ghtingcovid19 infowars kag democrats\ncovid19testing ats trumpliespeopledie lockdown\ncovid19memes \ufb01refauci tcot tcot\ncoronaviruscovid19 ccpvirus trumpownseverydeath politics\nbajucovid19 vaccine trumpliesamericansdie hydroxychloroquine\nchinavirus fauci usrc usa\nibackdom plandemic resistance arrestthemallnow\nusnews corona fakenews ricothedems\nuk feedly moscowmitch impeachobamasjudges\ntypes (unreliable, conspiracy, clickbait, political/biased). The Clickbait category distribution is closest to Others and\nClickbait, Political/Biased receive more responses to source tweets.\nThis \ufb01nding suggests that false information might be harder to spread through general users on social media, as source\ntweets linked to low quality news are observed as less likely to be shared or replied to than others.\nLastly, we provide textual analysis of the source tweets in misinformation cascades by type. For textual analysis, we\nextract hashtags from the textual content of source tweets in identi\ufb01ed misinformation cascades. We \ufb01nd distinctive\nhashtags in source tweets of each category, by computing the TF-IDF scores of hashtags across each category. The top\ndistinctive hashtags are the ones with the highest TF-IDF scores, that do not appear in the top ten hashtags of the other\nthree categories when ordered by the scores. The distinctive narratives in each category are shown in Table 4. The list\nof misinformation tweets is presented through the dashboard as a daily list, along with the category labels for public\naccess to the misinformation claims circulated online during the pandemic.\n5.3 Misinformation spread across countries\nThe misinformation spread across countries for sample tweets identi\ufb01ed from the collected dataset is shown in Figure 7.\nThe \ufb01gure shows the information cascade corresponding to each source tweet. The points indicate the retweet or\nreplies of the source tweet over the time scale. Tweets containing geolocation information are visualized, based on\n7\nAPREPRINT - OCTOBER 23, 2020\n(a)Unreliable. \u201cNevada Governor Sisolak\u2019s Chief Medical Of\ufb01cer\nWho Banned Hydroxychloroquine for Treating Coronavirus DOES\nNOT Have License to Practice Medicine.\"\n(b)Unreliable. \u201cWidespread panic hits as Corona Virus found to\ntransmit via toilet paper #toiletpapergate #coronavirus\"\nFigure 5: Misinformation spread across countries. (Left) Source tweet observed in United States (Right) Source tweet\nobserved outside of United States. Legend: Source tweet (Blue), Retweet/Replies (Red, intensity based on time scale).\nthe extracted latitude and longitude information. The identi\ufb01ed misinformation in the four categories - unreliable,\nconspiracy, clickbait, and political/biased, were found to contain both healthcare and political misinformation. We\nprovide and discuss examples of source tweets and their propagation patterns across countries.\nDiscussion. In Fig. 5a, a false claim circulated about Nevada Governor\u2019s Chief Medical Of\ufb01cer banning the use of\nHydroxychloroquine treatments was seen to circulate through social media. In this case, the observed geolocation of\nsource tweet is in the United States, the country with the highest Twitter usage, and it propagates to other countries\nwithin minutes. In other cases, source tweets are also observed to originate from other countries and travel to United\nStates and other countries. For example, in Fig. 5b, for a false claim that the virus was found to transmit through toilet\npaper, the source tweet geolocation was observed in Australia, with retweets traveling to several other countries.\nIn cases where the geolocation information of the source tweet is unobserved, the geolocation of other tweets in the\ninformation cascade still provides estimates of the exposure and spread in different countries, of the misinformation\nclaims propagated through the source tweet. For instance, in Fig. 6a the spread of the conspiracy promoting that the\nvirus is a bioweapon is observed over multiple countries, whereas in Fig. 6b the spread of the claim that the pandemic is\nless deadly than the \ufb02u is observed within the United States.\nMisinformation of varying degree of falsehood and biased/clickbait news reporting can mislead and in\ufb02uence public\nperception, especially with widespread propagation. We observe that the largest cascade in the collected dataset has\nover 10;000retweets spanning multiple countries, shown in Fig 7a. It corresponds to a political clickbait news article\npublished on the discussion surrounding affordability and price control on vaccines being researched for the virus. We\nalso \ufb01nd other cases of political misinformation with false claims regarding political \ufb01gures maliciously attempting to\nworsen the crisis, as shown in Fig. 7b. As seen these cases of misinformation have the potential to harm public health\nand effectiveness of health intervention policies.\n6 Sentiment Analysis\nCountry-wise sentiments. We analyze the evolving country-wise sentiments related to the COVID-19 pandemic. The\npublic perceptions constitute an important factor for gauging the reactions to policy decisions and preparedness efforts.\nIn addition, they also re\ufb02ect the nature of news coverage and potential misinformation. We extract sentiments from\nsocial media posts at the country-level and over time, to study the evolving public perceptions towards the pandemic.\nUsing lexical sentiment extraction based on (Hutto and Gilbert, 2014), we obtain the valence (positive or negative)\nalong with its intensity for each tweet based on its textual information. The sentiment is aggregated over tweets to\nestimate the overall sentiment distribution. The distribution of sentiments was found to vary over time and country.\nSocial distancing/Work from home sentiments. In addition, we analyze the public perception of emerging policies\nsuch as social distancing and remote work. These disease mitigation strategies also provide unprecedented glimpse\ninto the effect of remote work and isolation on mental health. Although the option to work remotely is limited to\nthe white collar workforce, nevertheless absence of child and dependent-care has emerged as an important challenge.\n8\nAPREPRINT - OCTOBER 23, 2020\n(a)Conspiracy. \u201cIt is not the \ufb02u, it is a #bioweapon .\"\n(b)Unreliable. \u201cNumbers Show Coronavirus Appears Far Less\nDeadly Than Flu Media Keep Promoting Panic.\"\nFigure 6: Misinformation spread across countries, examples with source tweet geolocation unavailable. (Left)\nRetweets/Replies observed across countries (Right) Retweets/Replies observed within United States. Legend:\nRetweet/Replies (Red, intensity based on time scale).\n(a)Political-Clickbait. \u201cGOP blocking coronavirus bill \u2014 be-\ncause it limits how much drugmakers can charge for a vaccine.\"\n(b)Unreliable. \u201cAn Obama Holdover in an Obscure Government\nArm Helped Cause the Country\u2019s Coronavirus Crisis.\"\nFigure 7: Misinformation spread across, political examples. (Left) Example with political-clickbait news (Right)\nExample with unreliable news. Legend: Source tweet (Blue), Retweet/Replies (Red, intensity based on time scale).\nFurthermore, this forced remote work will impact workdays of white collar workers beyond the pandemic. In order\nto understand public sentiment and opinion about different social issues, we extract hashtag information from the\ncollected tweets, and \ufb01lter based on keywords \u201c #workfromhome, #wfm, #workfromhome, #workingfromhome,\n#wfhlife \u201d and \u201c #socialdistance, #socialdistancing \u201d. The \ufb01ltered tweets are analyzed to obtain positive and\nnegative sentiments and ranked and visualized based on valence and intensity. The analysis is shown in Fig. 8 for\nsentiments on social distancing and on work from home policy interventions.\n7 Topic and Trend Analysis\nTopic clusters. We analyze Twitter conversations to identify topics and trends in the Twitter data on COVID-19. We\nuse topic modeling based on character embeddings (Joulin et al., 2016) extracted from social media posts Nguyen\net al. (2015), Li et al. (2016). We identi\ufb01ed 20 different topics from the collected English tweets. We found that the\nprominent topics of discussions during early March were centered around global outbreaks (Wuhan, Italy, Iran), travel\nrestrictions, prevention measures such as hand washing and masks, hoarding, symptoms and infections, immunization,\nevent cancellations, testing kits and centers, government response and emergency funding. The topic clusters along\nwith the most representative tweets in each cluster are provided on the dashboard. The representative tweets of each\ncluster are obtained based on word similarity of the tweet to the tf-idf word distribution of the cluster. The label to each\ncluster of tweets was assigned by manual inspection of the word distribution and representative tweets of the cluster.\n9\nAPREPRINT - OCTOBER 23, 2020\nFigure 8: Twitter sentiments analysis related to intervention policies of SocialDistancing and WorkFromHome.\n(a) United States, March 22-31.\n (b) France, March 22-31\nFigure 9: Emerging hashtags for countries/regions for March 22-31, depicted for United states and France.\nEmerging trends. The emerging trend on Twitter highlight changes in perception or importance of topics as the\npandemic situation changes. We extract hashtags from the tweet text for all tweets in the dataset for March/2020. The\nhashtags with emerging popularity are estimated based on \ufb01tted linear regression curves on the usage counts of hashtags\nover the period. On the dashboard (Fig. 9, Fig. 10), we provide the Top-30 emerging hashtags to show trendy interest in\nsocial media over the world. As the hashtags also re\ufb02ect spatial characteristics (e.g., country-level policy or trend), the\nTop-10 emerging hashtags of each country for last 10 days are also visualized on the dashboard and regularly updated.\nThe country/region-based emerging hashtags are particularly important to track people\u2019s interest. For instance, the\nline chart in Fig. 9 shows which hashtags emerged in terms of a slope of usage counts in the United States from\nMarch/22 to March/31. While the use of some hashtags (e.g., #coronavirus and#trump ) continuously dominates\nthe conversations, other hashtags (e.g., #coronalockdown ,#coronavirustruth and#nationaldoctorsday ) are\ntemporally signi\ufb01cant. The end of March is when most of states announced lockdown on many business and a stay-\nat-home order, and it causes people to use lockdown-related hashtags ( #coronalockdown ). Moreover, it shows that\n10\nAPREPRINT - OCTOBER 23, 2020\n(a) Germany, March 22-31.\n (b) India, March 22-31\nFigure 10: Emerging hashtags for countries/regions for March 22-31, depicted for Germany and India.\npeople get more and more interested in facts on coronavirus. Finally, the slope-based extraction easily detects spike\npattern of some hashtags ( #nationaldoctorsday ), which are only used in a particular day.\nIn Germany (Fig. 9), we could detect that people are interested in wearing masks ( #maskenpflicht , mask required)\nfrom the end of March. In France (Fig. 10)have counted the containment day ( #confinementjour ) everyday and\ntheir patterns show time lags as expected. Finally, the plot is also useful to see what trendy issues are ( #coronajihad,\n#nizamuddin ) in India (Fig. 10).\nGeoinformation trends. We also analyze the geographical distribution of daily counts of tweets and its trend using the\nextracted geolocation information. The dashboard provides (1) the geographical distribution of the daily count of tweets\nover countries/regions; (2) the daily increment of the count of tweets for each country/region; (3) the time for each\ncountry/region when it encounters its peak of daily counts of tweets. (1) shows a steady distribution of daily counts\nof tweets: users in United States contribute more than half of the total daily counts of tweets around the world, and\nusers in Europe, India, Oceania and South America are also active. (2) reveals that the daily counts of tweets of most\ncountries/regions are steady during the time of our observation. (3) illustrates the spatio-temporal pattern of which day\neach country/region achieves it highest activity over the observation time period.\n8 Conclusion and future work\nIn this work, we provided analysis of social media discourse about COVID-19 on Twitter, through analysis of\nmisinformation claims identi\ufb01ed using information about low-quality news websites from fact-checking sources, and\nanalysis of sentiments, topics, and emerging trends in the online discourse. The dashboard presented analysis and\ndaily updated list of identi\ufb01ed misinformation claims between March to June, 2020 during the pandemic. There are\nseveral directions of future work to address this large-scale \u201cinfodemic\" surrounding COVID-19. The proportion of\nTwitter users in the United states is higher than in other countries like China with alternate social media platforms.\nSince the pandemic is at a global scale, social media analysis for other platforms and languages is critical towards\nuncovering misinformation and tracking online discourse. Lastly, real-time tracking of misinformation, topics, trends,\nand sentiments, through public interfaces/applications (such as the dashboard) can enable solutions and provide an\noverview of the online discourse, to educate individuals on social media about the nature and quality of discussions on\nimportant topics. This can in turn make them less susceptible to misinformation, and prevent dire consequences of\nmisinformation on social outcomes.\nReferences\nHunt Allcott and Matthew Gentzkow. Social media and fake news in the 2016 election. Journal of Economic\nPerspectives , 31(2):211\u201336, 2017.\nAlessandro Bessi and Emilio Ferrara. Social bots distort the 2016 us presidential election online discussion. First\nMonday , 21(11), 2016.\nEmily Chen, Kristina Lerman, and Emilio Ferrara. Tracking social media discourse about the covid-19 pandemic:\nDevelopment of a public coronavirus twitter data set. JMIR Public Health and Surveillance , 6(2):e19273, 2020.\nMatteo Cinelli, Walter Quattrociocchi, Alessandro Galeazzi, Carlo Michele Valensise, Emanuele Brugnoli, Ana Lucia\nSchmidt, Paola Zola, Fabiana Zollo, and Antonio Scala. The covid-19 social media infodemic. arXiv preprint\narXiv:2003.05004 , 2020.\n11\nAPREPRINT - OCTOBER 23, 2020\nLimeng Cui and Dongwon Lee. Coaid: Covid-19 healthcare misinformation dataset, 2020.\nMark Dredze, Michael J Paul, Shane Bergsma, and Hieu Tran. Carmen: A twitter geolocation system with applications\nto public health. In Workshops at the Twenty-Seventh AAAI Conference on Arti\ufb01cial Intelligence , 2013.\nEmilio Ferrara. What types of covid-19 conspiracies are populated by twitter bots? First Monday , 2020.\nA. W. Geiger. Key \ufb01ndings about the online news landscape in America , 2019 (ac-\ncessed March 20, 2020). URL https://www.pewresearch.org/fact-tank/2019/09/11/\nkey-findings-about-the-online-news-landscape-in-america/ .\nLogan Godfrey. Social Media\u2019s Role in the Coronavirus Pandemic , 2020 (accessed\nApril 20, 2020). URL https://www.business2community.com/social-media/\nsocial-medias-role-in-the-coronavirus-pandemic-02296280 .\nAditi Gupta, Hemank Lamba, Ponnurangam Kumaraguru, and Anupam Joshi. Faking sandy: characterizing and\nidentifying fake images on twitter during hurricane sandy. In Proceedings of the 22nd international conference on\nWorld Wide Web , pages 729\u2013736. ACM, 2013.\nBinxuan Huang and Kathleen M Carley. Disinformation and misinformation on twitter during the novel coronavirus\noutbreak. arXiv preprint arXiv:2006.04278 , 2020.\nAdam Hughes and Stefan Wojcik. 10 facts about Americans and Twitter , 2019 (accessed March 20, 2020). URL\nhttps://www.pewresearch.org/fact-tank/2019/08/02/10-facts-about-americans-and-twitter/ .\nClayton J Hutto and Eric Gilbert. Vader: A parsimonious rule-based model for sentiment analysis of social media text.\nInEighth international AAAI conference on weblogs and social media , 2014.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov. Fasttext.zip:\nCompressing text classi\ufb01cation models. arXiv preprint arXiv:1612.03651 , 2016.\nChenliang Li, Haoran Wang, Zhiqian Zhang, Aixin Sun, and Zongyang Ma. Topic modeling for short texts with\nauxiliary word embeddings. In Proceedings of the 39th International ACM SIGIR conference on Research and\nDevelopment in Information Retrieval , pages 165\u2013174, 2016.\nLuca Luceri, Silvia Giordano, and Emilio Ferrara. Detecting troll behavior via inverse reinforcement learning: A case\nstudy of russian trolls in the 2016 us election. In Proceedings of the International AAAI Conference on Web and\nSocial Media , volume 14, pages 417\u2013427, 2020.\nJing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon, Bernard J Jansen, Kam-Fai Wong, and Meeyoung Cha. Detecting\nrumors from microblogs with recurrent neural networks. In IJCAI , pages 3818\u20133824, 2016.\nJing Ma, Wei Gao, and Kam-Fai Wong. Detect rumor and stance jointly by neural multi-task learning. In Companion\nof the The Web Conference 2018 on The Web Conference 2018 , pages 585\u2013593. International World Wide Web\nConferences Steering Committee, 2018.\nAmy Mitchell. Americans Still Prefer Watching to Reading the News - and Mostly\nStill Through Television , 2018. URL https://www.journalism.org/2018/12/03/\namericans-still-prefer-watching-to-reading-the-news-and-mostly-still-through-television/ .\nMark EJ Newman. Spread of epidemic disease on networks. Physical review E , 66(1):016128, 2002.\nDat Quoc Nguyen, Richard Billingsley, Lan Du, and Mark Johnson. Improving topic models with latent feature word\nrepresentations. Transactions of the Association for Computational Linguistics , 3:299\u2013313, 2015.\nDiogo Pacheco, Pik-Mai Hui, Christopher Torres-Lugo, Bao Tran Truong, Alessandro Flammini, and Filippo Menczer.\nUncovering coordinated networks on social media. arXiv preprint arXiv:2001.05658 , 2020.\nN. Parveen and J. Waterson. UK phone masts attacked amid 5G-coronavirus conspiracy theory ,\n2020 (accessed April 17, 2020). URL https://www.theguardian.com/uk-news/2020/apr/04/\nuk-phone-masts-attacked-amid-5g-coronavirus-conspiracy-theory .\nGordon Pennycook and David G Rand. Who falls for fake news? the roles of bullshit receptivity, overclaiming,\nfamiliarity, and analytic thinking. SSRN:3023545 , 2018. doi: http://dx.doi.org/10.2139/ssrn.3023545.\nGordon Pennycook, Jonathon McPhetres, Yunhao Zhang, and David Rand. Fighting covid-19 misinformation on social\nmedia: Experimental evidence for a scalable accuracy nudge intervention.\nNatali Ruchansky, Sungyong Seo, and Yan Liu. Csi: A hybrid deep model for fake news detection. In Proceedings of\nthe 2017 ACM on Conference on Information and Knowledge Management , pages 797\u2013806. ACM, 2017.\nFatima K Abu Salem, Roaa Al Feel, Shady Elbassuoni, Mohamad Jaber, and May Farah. Fa-kes: a fake news dataset\naround the syrian war. In Proceedings of the International AAAI Conference on Web and Social Media , volume 13,\npages 573\u2013582, 2019.\n12\nAPREPRINT - OCTOBER 23, 2020\nKarishma Sharma, Feng Qian, He Jiang, Natali Ruchansky, Ming Zhang, and Yan Liu. Combating fake news: A survey\non identi\ufb01cation and mitigation techniques. ACM Transcations on Intelligent Systems and TEchnology , 2019.\nKarishma Sharma, Emilio Ferrara, and Yan Liu. Identifying coordinated accounts in disinformation campaigns. arXiv\npreprint arXiv:2008.11308 , 2020.\nKai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu. defend: Explainable fake news detection. In\nProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages\n395\u2013405, 2019.\nLisa Singh, Shweta Bansal, Leticia Bode, Ceren Budak, Guangqing Chi, Kornraphop Kawintiranon, Colton Padden,\nRebecca Vanarsdall, Emily Vraga, and Yanchen Wang. A \ufb01rst look at covid-19 information and misinformation\nsharing on twitter. arXiv preprint arXiv:2003.13907 , 2020.\nViren Swami and David Barron. Analytic thinking, rejection of coronavirus (covid-19) conspiracy theories, and\ncompliance with mandated social-distancing: Direct and indirect relationships in a nationally representative sample\nof adults in the united kingdom. 2020.\nN. Vigdor. Man Fatally Poisons Himself While Self-Medicating for Coronavirus, Doctor Says , 2020 (accessed March 24,\n2020). URL https://www.nytimes.com/2020/03/24/us/chloroquine-poisoning-coronavirus.html .\nWHO. Coronavirus disease (COVID-19) advice for the public: Myth busters , 2020 (accessed March 20, 2020).\nURL https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public/\nmyth-busters .\nJaewon Yang and Jure Leskovec. Modeling information diffusion in implicit networks. In 2010 IEEE International\nConference on Data Mining , pages 599\u2013608. IEEE, 2010.\nXinyi Zhou and Reza Zafarani. Fake news: A survey of research, detection methods, and opportunities. arXiv preprint\narXiv:1812.00315 , 2018.\nXinyi Zhou, Apurva Mulay, Emilio Ferrara, and Reza Zafarani. Recovery: A multimodal repository for covid-19 news\ncredibility research. arXiv preprint arXiv:2006.05557 , 2020.\nMelissa Zimdars. False, misleading, clickbait-y, and satirical \u2018news\u2019 sources,\n2016. URL https://21stcenturywire.com/wp-content/uploads/2017/02/\n2017-DR-ZIMDARS-False-Misleading-Clickbait-y-and-Satirical-%E2%80%9CNews%E2%80%\n9D-Sources-Google-Docs.pdf .\n13", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Covid-19 on social media: Analyzing misinformation in twitter conversations", "author": ["K Sharma", "S Seo", "C Meng", "S Rambhatla"], "pub_year": "2020", "venue": "arXiv preprint arXiv \u2026", "abstract": "The ongoing Coronavirus (COVID-19) pandemic highlights the inter-connectedness of our  present-day globalized world. With social distancing policies in place, virtual communication"}, "filled": false, "gsrank": 467, "pub_url": "https://arxiv.org/abs/2003.12309", "author_id": ["IgklUtwAAAAJ", "spYH0tEAAAAJ", "nzkOdekAAAAJ", "EOSZeBMAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:ML_mNs5flYkJ:scholar.google.com/&output=cite&scirp=466&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D460%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=ML_mNs5flYkJ&ei=WbWsaIrWIPnSieoPxKLpgQ0&json=", "num_citations": 170, "citedby_url": "/scholar?cites=9913935494015008560&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:ML_mNs5flYkJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2003.12309"}}, {"title": "Bots, elections, and social media: a brief overview", "year": "2020", "pdf_data": "Bots, elections, and social media:\na brief overview\nEmilio Ferrara\nUSC Information Sciences Institute\n4676 Admiralty way, 1001, Marina del Rey, CA 90292, USA\nemiliofe@usc.edu\nAbstract. Bots, software-controlled accounts that operate on social me-\ndia, have been used to manipulate and deceive. We studied the charac-\nteristics and activity of bots around major political events, including\nelections in various countries. In this chapter, we summarize our \fndings\nof bot operations in the context of the 2016 and 2018 US Presidential\nand Midterm elections and the 2017 French Presidential election.\nKeywords: social media, bots, in\ruence, disinformation\n1 Introduction\nSocial media have been widely portrayed as enablers of democracy [50,48,15,12,47].\nIn countries were freedom to communicate and organize lacked, social media\nprovided a platform to openly discuss political [2,25,9,13,23,55,87] and social is-\nsues [38,37,18,19,82,8,77], without fears for safety or retaliation. Such platforms\nhave also been used to respond to crises and emergencies [75,88,34,89,49]. It is\nhard to overstate the importance of these platforms for the billions of people\nwho use them every day, all over the world.\nHowever, as it happens with most powerful emerging technologies, the rise of\npopularity led to abuse. Concerns about the possibility of manipulating public\nopinion using social media have been brought a decade before they material-\nized [39]. Ample evidence was provided by the scienti\fc community that social\nmedia can in\ruence people's behaviors [5,14,45,32,60,31]. These concerns have\nbeen corroborated by numerous recent studies [66,58,26,27,40,68,81,28].\nSocial media can be used to reach millions of people using targeted strate-\ngies aimed to maximize the spread of a message. If the goal is to manipulate\npublic opinion, one way to achieve it is by means of bots, software-controlled\nsocial media accounts whose goal is to mimic the characteristics of human users,\nwhile operating at much higher pace at substantially no downside for their oper-\nators. Bots can emulate all basic human activity on social media platforms, and\nthey become increasingly more sophisticated as new advancements in Arti\fcial\nIntelligence emerge [41,57,30,80,70].arXiv:1910.01720v1  [cs.SI]  3 Oct 2019\n2 Emilio Ferrara\nIn this chapter, we focus on the use of bots to manipulate the political dis-\ncourse. The \frst anecdotal accounts of attempts to steer public opinion on Twit-\nter date back to the 2010 US Midterm election [65] and similarly during the 2010\nUS Senate special election in Massachusetts [62,58], where bots were used to gen-\nerate arti\fcial support for some candidates and to smear their opponents.\nAttribution, i.e., the determination of the actors behind such operations,\nhas proven challenging in most such cases [30]. One notorious exception is repre-\nsented by the attribution of an interference campaign occurred during the 2016\nUS Presidential election to a Russian-sponsored operation. This was as a result\nof a thorough investigation on Russian interference led by the US Senate Select\nCommittee on Intelligence (SSCI). They found that \\The Russian government\ninterfered in the 2016 U.S. presidential election with the goal of harming the\ncampaign of Hillary Clinton, boosting the candidacy of Donald Trump, and in-\ncreasing political and social discord in the United States.\"1Numerous studies\nhave investigated the events associated with this operation [44,10,7].\nIt is worth noting that bots have been used for other purposes, for exam-\nple social spam and phishing [42,78,69,43,85,61,79,29]. Albeit much work has\nbeen devoted to the challenges of detecting social spam [56,35,90] and spam\nbots [51,52,72,11,61], only recently the research community started to investi-\ngate the e\u000bects that bots have on society, political discourse, and democracy.\nThe goal of this chapter is to summarize some of the most important results in\nthis space.\nContributions of this chapter\nThe aim of this chapter is to connect results of our investigations into three\nmajor political events: (i)the 2016 US Presidential election; (ii)the 2017 French\nPresidential election; and (iii) the 2018 US Midterm elections. We will discuss\nthe role of bots in these events, and highlight the in\ruence they had on the\nonline political discourse. The contributions of this chapter are as follows:\n{We \frst provide a brief overview of how bots operate and what are the\nchallenges in detecting them. Several recent surveys have been published on\nthe problem of characterizing and detecting bots [71,86], including our own\nonCommunications of the ACM [30].\n{We then illustrate our \frst, and maybe the most prominent, use case of bots-\ndriven interference in political discourse, discussing how bots have been used\nduring the 2016 US Presidential election to manipulate the discussion of the\npresidential candidates. This overview is based on our results that appeared\nprior to the November 8, 2016 election events [10].\n{We then illustrate how bots have been used to spread disinformation prior\nto the 2017 French Presidential election to smear Macron's public image.\n1See Wikipedia: https://en.wikipedia.org/wiki/Russian_interference_in_the_\n2016_United_States_elections\nBots, elections, and social media: a brief overview 3\n{Finally, we overview recent results that suggest how bots have been evolving\nover the course of the last few years, focusing on the 2018 US Midterm\nelections, and we discuss the challenges associated to their detection.\n2 Anatomy of a bot\n2.1 What is a bot\nIn this chapter, we de\fne as bot(short for robot , a.k.a., social bot, social media\nbot, social spam bot, or sybil account) a social media account that is predomi-\nnantly controlled by software rather than a human user. Although the de\fnition\nabove inherently states nothing about the intents behind creating and operat-\ning a bot, according to published literature, malicious applications of bots are\nreported signi\fcantly more frequently than legitimate usage [30,71].\nWhile in this chapter we will focus exclusively on bots that aim to manipulate\nthe public discourse, it is worth nothing that some researchers have used bots\nfor social good [60,4], as illustrated by a recent taxonomy that explores the\ninterplay between intent and characteristics of bots [71]. Next, we describe some\ntechniques to create and detect bots.\n2.2 How to create a bot\nIn the early days of online social media, in the late 2000s, creating a bot was not\na simple task: a skilled programmer would need to sift through various platforms'\ndocumentation to create a software capable of automatically interfacing with the\nfront-end or the back-end, and operate functions in a human-like manner.\nThese days, the landscape has completely changed: indeed, it has become\nincreasingly simpler to deploy bots, so that, in some cases, no coding skills\nare required to setup accounts that perform simple automated activities: tech\nblogs often post tutorials and ready-to-go tools for this purposes. Various source\ncodes for sophisticated social media bots can be found online as well, ready to\nbe customized and optimized by the more technically-savvy users [44].\nWe recently inspected same of the readily-available Twitter bot-making tools\nand compiled a non-comprehensive list of capabilities they provide [10,28].\nMost of these bots can run within cloud services or infrastructures like Ama-\nzon Web Services (AWS) or Heroku, making it more di\u000ecult to block them when\nthey violate the Terms of Service of the platform where they are deployed.\nA very recent trend is that of providing Bot-As-A-Service (BaaS): Advanced\nconversational bots powered by sophisticated Arti\fcial Intelligence are provided\nby companies like ChatBots.io that can be used to carry digital spam campaigns\n[29] and scale such operations by automatically engaging with online users.\nFinally, the increasing sophistication of Arti\fcial Intelligence (AI) models,\nin particular in the area of neural-based natural language generation , and the\navailability of large pre-trained models such as OpenAI's GPT-2 [64], makes it\neasy to programmatically generate text content. This can be used to program\nbots that produce genuine-looking short texts on platforms like Twitter, making\nit harder to distinguish between human and automated accounts [3].\n4 Emilio Ferrara\n2.3 How to detect bots\nThe detection of bots in online social media platform has proven a challenging\ntask. For this reason, it has attracted a lot of attention from the computing re-\nsearch community. Even DARPA, the U.S. Defense Advanced Research Projects\nAgency , became interested and organized the 2016 DARPA Twitter Bot Detec-\ntion [74], with University of Maryland, University of Southern California, and\nIndiana University topping the challenge, focused on detecting bots pushing anti\nvaccination campaigns. Large botnets have been identi\fed on Twitter, from dor-\nmant [24,24], to very active [1].\nThe literature on bot detection has become very extensive. We tried to sum-\nmarize the most relevant approaches in a survey paper recently appeared on\ntheCommunications of the ACM [30]: In that review, we proposed a simple\ntaxonomy to divide the bot detection approaches into three classes: (i)bot\ndetection systems based on social network information; (ii)systems based on\ncrowd-sourcing and leveraging human intelligence; (iii) machine learning meth-\nods based on the identi\fcation of highly-predictive features that discriminate\nbetween bots and humans. We refer the interested reader to that review for a\ndeeper analysis of this problem [30]. Other recent surveys propose complemen-\ntary or alternative taxonomies that are worth considering as well [71,20,20,86].\nAs of today, there are a few publicly-available tools that allow to do bot de-\ntection and study social media manipulation, including (i)Botometer,2a pop-\nular bot detection tool developed at Indiana University [21], (ii)BotSlayer,3an\napplication that helps track and detect potential manipulation of information\nspreading on Twitter, and (iii) the Bot Repository,4a centralized database to\nshare annotated datasets of Twitter social bots.\nIn conclusion, several algorithms have been published to detect bots using\nsophisticated machine learning techniques including deep learning [46], anomaly\ndetection [59,36,22], and time series analysis [16,73].\n3 Social media manipulation\nBots have been reportedly used to interfere in political discussions online, for\nexample by creating the impression of an organic support behind certain political\nactors [62,65,66,58]. However, the apparent support can be arti\fcially generated\nby means of orchestrated campaigns with the help of bots. This strategy is\ncommonly referred to as social media astroturf [66].\n3.1 2016 US Presidential Election\nOur analysis of social media campaigns during the 2016 US Presidential Election\nrevealed the presence of social bots. We here summarize our \fndings \frst pub-\nlished in [10], discussing data collection, bot detection, and sentiment analysis.\n2Botometer: https://botometer.iuni.iu.edu/\n3BotSlayer: https://osome.iuni.iu.edu/tools/botslayer/\n4Bot Repository: https://botometer.iuni.iu.edu/bot-repository/\nBots, elections, and social media: a brief overview 5\nData Collection. We manually crafted a list of hashtags and keywords related\nto the 2016 US Presidential Election with 23 terms in total, including 5 terms\nspeci\fcally for the Republican Party nominee Donald Trump, 4 terms for the\nDemocratic Party nominee Hillary Clinton, and the remainder terms relative to\nthe four presidential debates. The complete list of search terms is reported in\nour paper [10]. By querying the Twitter Search API between September 16 and\nOctober 21, 2016, we collected a large dataset. After post-processing and cleaning\nprocedures, we studied a corpus constituted by 20.7 million tweets posted by\nnearly 2.8 million distinct users.\nBot detection. We used Botometer v1(the version available in 2016) to de-\ntermine the likelihoood that the most active accounts in this dataset were con-\ntrolled by humans or were otherwise bots. To label accounts as bots, we use the\n\ffty-percent threshold|which has proven e\u000bective in prior studies [30,21]|an\naccount was considered to be a bot if the bot score was above 0.5. Due to the\nTwitter API limitations, it would have been impossible to test all the 2.78 mil-\nlion accounts in short time. Therefore, we tested the top 50 thousand accounts\nranked by activity volume, which account for roughly 2% of the entire popu-\nlation and yet are responsible for producing over 12.6 million tweets, which is\nabout 60% of the total conversation. Of the top 50 thousand accounts, Botome-\nter classi\fed as likely bots a total of 7,183 users (nearly 15%), responsible for\n2,330,252 tweets; 2,654 users were classi\fed as undecided, because their scores\ndid not signi\fcantly diverge from the classi\fcation threshold of 0.5; the rest|\nabout 40 thousand users (responsible for just 10.3 million tweets, less than 50%\nof the total)|were labeled as humans. Additional statistics are summarized in\nour paper [10].\nSentiment analysis. We leveraged sentiment analysis to quantify how bots\n(resp., humans) discussed the candidates. We used SentiStrength [76] to de-\nrive the sentiment scores of each tweet in our dataset. This toolkit is especially\noptimized to infer sentiment in short informal texts, thus ideally suited for so-\ncial media. We tested it extensively in prior studies on the e\u000bect of sentiment\non tweets' di\u000busion [32,33]. The algorithm assigns to each tweet ta positive\nP+(t) and negative P\u0000(t) polarity score, both ranging between 1 (neutral) and\n5 (strongly positive/negative). Starting from the polarity scores, we captured\nthe emotional dimension of each tweet twith one single measure, the sentiment\nscore S(t), de\fned as the di\u000berence between positive and negative polarity scores:\nS(t) =P+(t)\u0000P\u0000(t). The above-de\fned score ranges between -4 and +4. The\nnegative extreme indicates a strongly negative tweet, and occurs when P+(t) = 1\nandP\u0000(t) = 5. Vice-versa, the positive extreme identi\fes a strongly positive\ntweet labeled with P+(t) = 5 and P\u0000(t) = 1. In the case P+(t) =P\u0000(t)|\npositive and negative sentiment scores for a tweet tare the same|the sentiment\nS(t) = 0 of tweet tis considered neutral as the polarities cancel each other out.\n6 Emilio Ferrara\nFig. 1. Complementary cumulative distribution function (CCDF) of replies interactions\ngenerated by bots (left) and humans (right) (from [10]).\nFig. 2. Complementary cumulative distribution function (CCDF) of retweets interac-\ntions generated by bots (left) and humans (right) (from [10]).\nPartisanship and Supporting Activity. We used a simple heuristic based\non the 5 Trump-supporting hashtags and the 4 Clinton-supporting to attribute\nuser partisanships. For each user, we calculated their top 10 most used hashtags:\nIf the majority supported one particular candidate, we assigned the given user\nto that political group (Clinton or Trump supporter). Compared to network-\nbased techniques [17,6], this simple partisanship assignment yielded a smaller yet\nhigher-con\fdence annotated dataset, constituted by 7,112 Clinton supporters\n(590 bots and 6,522 humans) and 17,202 Trump supporters (1,867 bots and\n15,335 humans).\nSummary of Results: Engagement. Figure 1 and Figure 2 illustrate the\nComplementary Cumulative Distribution Functions (CCDFs) of replies and retweets\ninitiated by bots and humans in three categories: (i) within group (for example\nbot-bot, or human-human); (ii) across groups (e.g., bot-human, or human-bot);\nand, (iii) total (i.e., bot-all and human-all). The heavy-tailed distributions, typ-\nically observed in social systems, appear in both. Hence, further inspection of\nFig. 1 suggests that (i)humans replied signi\fcantly more to other humans than\nto bots and, (ii) conversely, bots receive replies from other bots signi\fcantly\nmore than from humans. One hypothesis is that unsophisticated bots could not\nproduce engaging-enough questions to foster meaningful exchanges with humans.\nBots, elections, and social media: a brief overview 7\nFigure 2, however, demonstrates that retweets were a much more vulnerable\nmode of information di\u000busion: there is no statistically signi\fcant di\u000berence in\nthe amount of retweets that humans generated by resharing content produced\nby other humans or by bots. In fact, humans and bots retweeted each other\nsubstantially at the same rate. This suggests that bots were very e\u000bective at\ngetting their messages reshared in the human communication channels.\nOur study highlighted a vulnerability in the information ecosystem at that\ntime, namely that content was reshared often without a thorough scrutiny on\nthe information source. Several subsequent studies hypothesized that bots may\nhave played a role in the spread of false news and unveri\fed rumors [67,83].\nSummary of Results: Sentiment. We further explored how bots and hu-\nmans talked about the two presidential candidates. Next, we show the sentiment\nanalysis results based on SentiStrength . Figure 3 illustrates four settings: the top\n(resp., bottom) two panels show the sentiment of the tweets produced by the\nbots (resp., humans). Furthermore, the two left (resp., right) panels show the\nsupport for Clinton (resp., Trump). The main histograms in each panel show\nthe volume of tweets about Clinton or Trump, separately, whereas the insets\nshow the di\u000berence between the two. By contrasting the left and right panels\nwe note that the tweets mentioning Trump are signi\fcantly more positive than\nthose mentioning Clinton, regardless of whether the source is human or bot.\nHowever, bots tweeting about Trump generated almost no negative tweets and\nindeed produced the most positive set of tweets in the entire dataset (about\n200,000 or nearly two-third of the total).\nThe fact that bots produce systematically more positive content in support\nof a candidate can bias the perception of the individuals exposed to it, suggesting\nthat there exists an organic, grassroots support for a given candidate, while in\nreality it is in part arti\fcially in\rated. Our paper reports various examples of\ntweets generated by bots, and the candidate they support [10].\n3.2 2017 French Presidential Election\nA subsequent analysis of the Twitter ecosystem highlighted the presence and\ne\u000bects of bots prior to the 2017 French Presidential Election. We next report\nour \fndings summarizing the results published in 2017 [28]. We provide a char-\nacterization of both the bots and the users who engaged with them.\nData Collection. By following the same strategy as in the 2016 US Presidential\nelection [10], we manually selected a set of hashtags and keywords related to the\n2017 French Presidential Election. By construction, the list contained a roughly\nequal number of terms associated with each of the two candidates, namely Marine\nLe Pen and Emmanuel Macron, and various general election-related terms: we\nultimately identi\fed 23 terms, listed in our paper [28]. We collected data by\nusing the Twitter Search API, from April 27 to the end of election day, on May\n7, 2017: This procedure yielded a dataset containing approximately 17 million\n8 Emilio Ferrara\nFig. 3. Distributions of the sentiment of bots (top) and humans (bottom) supporting\nthe two presidential candidates. The main histograms show the disaggregated volumes\nof tweets talking about the two candidates separately, while the insets show the absolute\nvalue of the di\u000berence between them (from [10]).\nunique tweets, posted by 2,068,728 million unique users. Part of this corpus is\na subset of tweets associated with the MacronLeaks disinformation campaign,\nwhose details are described in our paper [28]. The timeline of the volume of\nposted tweets is illustrated in Figure 4.\nBot Detection. Due to the limitations of the Twitter API, and the time re-\nstrictions for this short period of unfolding events, we were unable to run in\nreal time the bot detection relying upon Botometer. For this reason, we car-\nried out a post-hoc bot detection on the dataset using an o\u000fine version of the\nbot-detection algorithm inspired by Botometer's rationale. Speci\fcally, we ex-\nclusively leveraged user metadata and activity features to create a simple yet\ne\u000bective bot detection classi\fer, trained on same data as Botometer, which is\ndetailed in our paper [28]. We validated its classi\fcation accuracy and assessed\nthat it was similar to Botometer's performance, with above 80 percent in both ac-\ncuracy and AUC-ROC scores. Manual validation corroborated the performance\nanalysis. Hence, we used this simpli\fed bot detection strategy to unveil bots in\nthe dataset at hand.\nSummary: Temporal Dynamics. We started by exploring the timeline of the\ngeneral election-related discussion on Twitter. The broader discussion that we\ncollected concerns the two candidates, Marine Le Pen and Emmanuel Macron,\nand spans the period from April 27 to May 7, 2017, the Presidential Election Day,\nsee Figure 4. Let us discuss \frst the dashed grey line (left axis): this shows the\nBots, elections, and social media: a brief overview 9\nFig. 4. Timeline of the volume of tweets generated every minute during our observation\nperiod (April 27 through May 7, 2017). The purple solid line (right axis) shows the\nvolume associated with MacronLeaks, while the dashed grey line (left axis) shows the\nvolume of generic election-related discussion. The presidential election occurred on May\n7, 2017 (from [28]).\nvolume of generic election-related discussion. The discussion exhibits common\ncircadian activity patterns and a slightly upwards trend in proximity to Election\nDay, and spikes in response to an o\u000b-line event, namely the televised political\ndebate that saw Le Pen facing Macron. Otherwise, the number of tweets per\nminute averages between 300 and 1,500 during the day, and quickly approaches\nde facto zero overnight, consistently throughout the entire observation window.\nFigure 4 also illustrates with the purple solid line (right axis) the volume associ-\nated with MacronLeaks, the disinformation campaign that was orchestrated to\nsmear Macron's reputation. The temporal pattern of this campaign is substan-\ntially di\u000berent from the general conversation. First, the campaign is substantially\nsilent for the entire period till early May. We can easily pinpoint the inception\nof the campaign on Twitter, which occurs in the afternoon of April 30, 2017.\nAfter that, a surge in the volume of tweets, peaking at nearly 300 per minute,\nhappens in the run up to Election Day, between May 5 and May 6, 2017. It is\nworth noting that such a peak is nearly comparable in scale to the volume of the\nregular discussion, suggesting that for a brief interval of time (roughly 48 hours)\nthe MacronLeaks disinformation campaign acquired signi\fcant attention [28].\nSummary: Bot Dynamics. Like in the previous study, we here provide a\ncharacterization of the Twitter activity, this time speci\fcally related to Macron-\nLeaks, for both bot and human accounts. In Figure 5, we show the timeline of\nthe volume of tweets generated respectively by human users (dashed grey line)\nand bots (solid purple line), between April 27 and May 7, 2017, and related to\nMacronLeaks. The amount of activity is substantially close to zero until May 5,\n2017, in line with the \frst coordination e\u000borts as well as the information leaks\nspurred from other social platforms, as discussed in the paper [28]. Spikes in bot-\ngenerated content often appear to slightly precede spikes in human posts, sug-\ngesting that bots can trigger cascades of disinformation [67]. At peak, the volume\n10 Emilio Ferrara\nFig. 5. Timeline of the volume of tweets generated every minute, respectively by hu-\nman users (dashed grey line) and social bots (solid purple line), between April 27\nand May 7, 2017, and related to MacronLeaks. Spikes in bot-generated content often\nslightly precedes spikes in human posts, suggesting that bots can trigger cascades of\ndisinformation (from [28]).\nof bot-generated tweets is comparable with the that of human-generated ones.\nFurther investigation revealed that the users who engaged with bots pushed the\nMacronLeaks disinformation campaign were mostly foreigners with pre-existing\ninterest in alt-right topics and alternative news media, rather than French users.\nFurthermore, we highlighted an anomalous account usage pattern where hun-\ndreds of bot accounts used in the 2017 French Presidential elections were also\npresent in the 2016 US Presidential Election discussion, which suggested the pos-\nsible existence of a black market for reusable political disinformation bots [28].\nSummary: Sentiment Dynamics. Identically to the 2016 US Presidential\nElection study, we annotated all tweets in this corpus using SentiStrength , and\nsubsequently studied the evolution of the sentiment of tweets in the 2017 French\nPresidential Election discussion. Figure 6 shows the temporal distribution of\ntweets' sentiment disaggregated by intensity: the four panels illustrate the over-\nall timeline of the volume of tweets that exhibit positive and negative sentiment\nat the hourly resolution, for sentiment polarities ranging from 1 (lowest) to 4\n(highest) in both positive and negative spectra. What appears evident is that,\nas Election Day approaches, moderately and highly negative tweets (sentiment\nscores of -2, -3, and -4) signi\fcantly outnumber the moderately and highly pos-\nitive tweets, at times by almost an order of magnitude. For example, between\nMay 6 and 7, 2017, on average between 300 and 400 tweets with signi\fcant neg-\native sentiment (sentiment scores of -3) were posted every hour, compared with\nan average of between 10 and 50 tweets with an equivalently positive sentiment\n(score scores of +3). Since the discussion during that period was signi\fcantly\ndriven by bots, and bots focused against Macron, our analysis suggested that\nbots were pushing negative campaigns against that candidate aimed at smearing\nhis credibility and weakening his position in the eve of the May 7's election.\nBots, elections, and social media: a brief overview 11\nFig. 6. Temporal distribution of sentiment disaggregated by sentiment intensity\n(hourly resolution). The sign on the y-axis captures the amount of tweets in the positive\n(resp., negative) sentiment dimension.\n3.3 2018 US Midterms\nThe notorious investigation on Russian interference led by the US Senate Select\nCommittee on Intelligence (SSCI) put social media service providers (SMSPs)\nat the center-stage of the public debate. According to reports, SMPSs started\nto devote more e\u000borts to \\sanitize\" their platforms, including ramping up the\ntechnological solutions to detect and \fght abuse. Much attention has been de-\nvoted to identifying and suspending inauthentic activity , a term that captures a\nvariety of tools used to carry out manipulation, including bot and troll accounts.\nHence, it is natural to ask whether these countermeasures proved e\u000bective,\nor if otherwise the strategies and technologies bots typically used until 2017\nevolved, and to what extent they successfully adapted to the changing social\nmedia defenses and thus escaped detection. We recently set to answer these\nquestions: to this purpose, we monitored and investigated the online activity\nsurrounding the 2018 US Midterm elections what were held on November 6,\n2018.\n12 Emilio Ferrara\nData Collection. We collected data for six weeks, from October 6, 2018 to\nNovember 19, 2018, i.e., one month prior and until two weeks after election day.\nTweets were collected using the Twitter Streaming API and following these key-\nwords: 2018midtermelections ,2018midterms ,elections ,midterm , and midterm-\nelections . Post-processing and cleaning procedures are described in detail in our\npaper [53]: we retained only tweets in English, and manually removed tweets that\nwere out of context, e.g., tweets related to other countries' elections (Cameroon,\nCongo, Biafra, Kenya, India, etc.) that were present in our initial corpus because\nthey contained the same keywords we tracked. The \fnal dataset contains 2.6M\ntweets, posted by nearly 1M users.\nBot Detection. Similarly to the 2016 US Presidential election study, since\nthis study was a post-mortem (i.e., not in real time but after the events), we\nadopted Botometer to infer the bot scores of the users in our dataset. The only\ndistinction worth mentioning is that we used the Botometer API version v3that\nbrings new features and a non-linear re-calibration of the model: in line with\nthe associated study's recommendations [86], we used a threshold of 0 :3 (which\ncorresponds to a 0 :5 threshold from previous versions of Botometer) to separate\nbots from humans (note that the results remain substantially unchanged if a\nhigher threshold was used). As a result, we obtained that 21.1% of the accounts\nwere categorized as bots, which were responsible for 30.6% of the total tweets\nin our dataset. Manual validation procedures assessed the reasonable quality of\nthese annotations. The resulting evidence suggests that bots were still present,\nand accounted for a signi\fcant amount of the tweets posted in the context of\nthe political discourse revolving around the 2018 US Midterms.\nInterestingly, about 40 thousand accounts were already inactive at the time of\nour analysis, and thus we were not able to infer their bot scores using the Twitter\nAPI. We manually veri\fed that 99.4% of them were suspended by Twitter,\ncorroborating the hypothesis that these were bots as well, and were suspended\nby Twitter in the time between the events and our post-mortem analysis, which\nwas carried out in early 2019.\nPolitical Leaning Inference. Next, we set to determine if bots exhibited a\nclear political leaning, and if they acted according to that preference. To label\naccounts as conservative or liberal, we used a label propagation approach that\nleveraged the political alignment of news sources whose URLs were posted by\nthe accounts in the dataset. Lists of partisan media outlets were taken from\nthird-party organizations, namely AllSides.Org and MediaBiasFactCheck.Com.\nThe details of our label propagation algorithm are explained in our paper [53].\nUltimately, the procedure allowed us to reliably infer, with accuracy above 89%,\nthe political alignment of the majority of human and bot accounts in our corpus.\nThese were factored into the subsequent analyses aimed at determining partisan\nstrategies and narratives (see [53]).\nBots, elections, and social media: a brief overview 13\nFig. 7. K-core decomposition: liberal vs. conservative bots and humans (from [53]).\nSummary: Bot Activity and Strategies. Provided the evidence that bots\nwere still present despite the e\u000borts of the SMSPs to sanitize their platforms,\nwe aimed at determining the degree to which they were embedded in the human\necosystem, speci\fcally in the retweet network. This network is of central impor-\ntance in our analysis, because it conveys information di\u000busion dynamics; many\nrecent studies suggested a connection between bots and the spread of unveri\fed\nand false information [67,83]. It is therefore of paramount importance to deter-\nmine if bots still played a role in the retweet network of election-related social\nmedia discourse as of 2018.\nTo this aim, we resorted to perform the k-core decomposition analysis. In\nsocial network theory, a k-core is a subgraph of a graph where all nodes have\ndegree at least equal to k. The intuition is that, as kgrows, one is looking at\nincreasingly more highly-connected nodes' subgraphs. Evidence suggests that\nhighk-cores are associated with nodes that are more embedded, thus in\ruential,\nfor the network under investigation [84].\nIf bots were still in\ruential in the 2018 US Midterm election discussion, our\nhypothesis is that we would \fnd them in high concentration predominantly into\nhighkcores. This would be consistent with our \fndings related to the 2016 US\nPresidential Election discussion [10].\nFigure 7 corroborates our intuition. Speci\fcally, we show the percentage of\nboth conservative and liberal human and bot accounts as a function of varying\nk. Two patterns are worth discussing: \frst, as kincreases, the fraction of con-\nservative bots grows, while the prevalence of liberal bots remains more or less\nconstant; conversely, the prevalence of human accounts decreases, with growing\nk, more markedly for liberal users than conservative ones. We summarize these\n\fndings suggesting that conservative bots were situated in a premium position\nin the retweet network, and therefore may have a\u000bected information spread [53].\n14 Emilio Ferrara\n3.4 2016 vs 2018: A Comparative Bot Analysis\nHaving identi\fed and analyzed the activity of human and bot accounts in the\ncontext of the political discourse associated to US election events in both 2016\nand 2018, it is natural to ask whether these studies involved a similar set of ac-\ncounts. In other words, it is worth determining whether there exists a continuum\nof users that are active in both time periods under investigation. If this is the\ncase, it would be interesting to study the users present in both periods, deter-\nmine whether any of them are the bots under scrutiny in the previous studies,\nand ultimately understand if the strategies they may have exhibited evolved,\npossibly to escape detection or avoid further scrutiny of SMSPs.\nData Collection. To answer the questions above, we isolated the users present\nin both the 2016 and 2018 datasets described above. This process yielded over\n278 thousand accounts, active in both periods. Further processing and cleaning\nprocedures, as detailed in our paper [54], brought the dataset down to 245K\nusers, accounting for over 8.3M tweets in 2016 and 660K in 2018. Botometer was\nused to determine the bot scores of these accounts. As a result, 12.6% of these\naccounts scored high in bot scores and were therefore classi\fed as bots. We used\nthis dataset to study the evolution of behavior of bots over the time period of\nstudy.\nSummary: Bot Evolution Dynamics. One advantage of bots over humans is\ntheir scalability. Since bots are controlled by software rather than human users,\nas such they can work over the clock, they don't need to take rests and don't have\nthe \fnite cognitive capacity and bandwidth that dictates how humans operate on\nsocial media [63]. In principle, a bot could post continuously without any break,\nor at regular yet tight intervals of time. As a matter of fact, primitive bots used\nthese simple strategies [65,58]. However, such obvious patterns are easy to spot\nautomatically, hence not very e\u000bective. There is therefore a trade-o\u000b between\nrealistic-looking activity and e\u000bectiveness. In other words, one can investigate\nthe patterns of inter-event time betweet a tweet post and its subsequent, and\nlay out the frequency distribution in an attempt to distill the di\u000berence between\nhuman and bot accounts' temporal dynamics.\nFigure 8 illustrates the tweet inter-time distribution by bots and humans\nin 2016 (left) and 2018 (right). It is apparent that, while in 2016 bots exhib-\nited a signi\fcantly di\u000berent frequency distribution with respect to their human\ncounterparts, in 2018 this distinction has vanished. In fact, statistical testing of\ndistribution di\u000berences suggests that human and bot temporal signatures are\nindistinguishable in 2018. The discrepancy is particularly relevant in the time\nrange between 10 minutes and three hours, consistent with other \fndings [63]:\nin 2016, bots shared content at a higher rate with respect to human users.\nOur work [54] corroborates the hypothesis that bots are continuously chang-\ning and evolving to escape detection. Further examples that we reported also\nillustrate other patterns of behavior that have changed between 2016 and 2018:\nBots, elections, and social media: a brief overview 15\nFig. 8. Tweet inter-event time by bots and humans in 2016 (left) and 2018 (right). A\nclear distinction in temporal signature between bots and humans was evident in 2016,\nbut vanished in 2018 (from [54]).\nfor instance, the sentiment that was expressed in favor or against political can-\ndidates in 2018 re\rects signi\fcantly better what the human crowd is expressing.\nHowever, in 2016, bots' sentiment drastically diverged, in a manner easy to de-\ntect, from that of the human's counterpart, as we discussed earlier.\n4 Conclusions\nIn this chapter, we set to discuss our latest results regarding the role of bots\nwithin online political discourse in association with three major political events.\nFirst, we described the results of our analysis that unveiled a signi\fcant\namount of bots distorting the online discussion in relation to the 2016 US Pres-\nidential election. We characterized the activities of such bots, and illustrated\nhow they successfully fostered interactions by means of retweets at the same\nrate human users did. Other researchers suggested that this played a role in the\nspread of false news during that time frame [67,83].\nSecond, we highlighted the role of bots in pushing a disinformation campaign,\nknown as MacronLeaks , in the run up to the 2017 French Presidential election.\nWe demonstrated how it is possible to easily pinpoint the inception of this disin-\nformation campaign on Twitter, and we illustrated how its popularity peak was\ncomparable with that of regular political discussion. We also hypothesized that\nthis disinformation campaign did not have a major success in part because it\nwas tailored around the information needs and usage patterns of the American\nalt-right community rather than French-speaking audience. Moreover, we found\nthat several hundreds of bot accounts were re-purposed from the 2016 US Elec-\ntion. Ultimately, we suggested the possibility that a black market for reusable\npolitical bots may exist [28].\nThird, we studied the 2018 US Midterms, to investigate if bots were still\npresent and active. Our analysis illustrated that not only bots were almost as\nprevalent as in the two other events, but also that conservative bots played a\ncentral role in the highly-connected core of the retweet network. These \fndings\nfurther motivated a comparative analysis contrasting the activity of bots and\n16 Emilio Ferrara\nhumans in 2016 and 2018. Our study highlighted that a core of over 245K users,\nof which 12.1% were bots, was active in both events. Our results suggest that\nbots may have evolved to better mimic human temporal patterns of activity.\nWith the increasing sophistication of Arti\fcial Intelligence, the ability of\nbots to mimic human behavior to escape detection is greatly enhanced. This\nposes challenges for the research community, speci\fcally in the space of bot\ndetection. Whether it is possible to win this arms race is yet to be determined:\nany party with signi\fcant resources can deploy state of the art technologies to\nenact in\ruence operations and other forms of manipulation of public opinion.\nThe availability of powerful neural language models lowers the bar to adopt\ntechniques that allow to build credible bots. For example, it may be already in\nprinciple possible to automatize almost completely the generation of genuine-\nlooking text. This may be used to push particular narratives, to arti\fcially build\ntraction for political arguments that may otherwise have little or no human\norganic support.\nUltimately, the evidence that our studies, and the work of many other re-\nsearchers in this \feld, have brought strongly suggest that more policy and reg-\nulations may be warranted, and that technological solutions alone may not be\nsu\u000ecient to tackle the issues of bot interference in political discourse.\nAcknowledgements\nThe author is grateful to his collaborators and coauthors on the topics covered\nin this paper, in particular Adam Badawy, Alessandro Bessi, Ashok Deb, and\nLuca Luceri, who contributed signi\fcantly to three papers widely discussed in\nthis chapter [10,53,54].\nReferences\n1. N. Abokhodair, D. Yoo, and D. W. McDonald. Dissecting a social botnet: Growth,\ncontent and in\ruence in twitter. In Proceedings of the 18th ACM Conference on\nComputer Supported Cooperative Work & Social Computing , pages 839{851. ACM,\n2015.\n2. L. A. Adamic and N. Glance. The political blogosphere and the 2004 us election:\ndivided they blog. In 3rd international workshop on Link discovery , pages 36{43.\nACM, 2005.\n3. A. Alari\f, M. Alsaleh, and A. Al-Salman. Twitter turing test: Identifying social\nmachines. Information Sciences , 372:332{346, 2016.\n4. J.-P. Allem, E. Ferrara, S. P. Uppu, T. B. Cruz, and J. B. Unger. E-cigarette\nsurveillance with social media data: social bots, emerging topics, and trends. JMIR\npublic health and surveillance , 3(4):e98, 2017.\n5. S. Aral and D. Walker. Creating social contagion through viral product design: A\nrandomized trial of peer in\ruence in networks. Management science , 57(9):1623{\n1639, 2011.\n6. A. Badawy, E. Ferrara, and K. Lerman. Analyzing the digital traces of political\nmanipulation: The 2016 russian interference twitter campaign. In Proceedings of\nBots, elections, and social media: a brief overview 17\nthe 2018 IEEE/ACM International Conference on Advances in Social Networks\nAnalysis and Mining 2018 , 2018.\n7. A. Badawy, K. Lerman, and E. Ferrara. Who falls for online political manipulation?\nInCompanion Proceedings of the 2019 World Wide Web Conference , pages 162{\n168, 2019.\n8. P. Barber\u0013 a, N. Wang, R. Bonneau, J. T. Jost, J. Nagler, J. Tucker, and S. Gonz\u0013 alez-\nBail\u0013 on. The critical periphery in the growth of social protests. PloS one ,\n10(11):e0143611, 2015.\n9. M. A. Beka\fgo and A. McBride. Who tweets about politics? political participation\nof twitter users during the 2011gubernatorial elections. Social Science Computer\nReview , 31(5), 2013.\n10. A. Bessi and E. Ferrara. Social bots distort the 2016 us presidential election online\ndiscussion. First Monday , 21(11), 2016.\n11. Y. Boshmaf, I. Muslukhov, K. Beznosov, and M. Ripeanu. The socialbot network:\nwhen bots socialize for fame and money. In Proceedings of the 27th annual computer\nsecurity applications conference , pages 93{102. ACM, 2011.\n12. D. Boyd and K. Crawford. Critical questions for big data: Provocations for a\ncultural, technological, and scholarly phenomenon. Information, communication\n& society , 15(5), 2012.\n13. J. E. Carlisle and R. C. Patton. Is social media changing how we understand\npolitical engagement? an analysis of facebook and the 2008 presidential election.\nPolitical Research Quarterly , 66(4):883{895, 2013.\n14. D. Centola. An experimental study of homophily in the adoption of health behav-\nior.Science , 334(6060):1269{1272, 2011.\n15. M. Cha, H. Haddadi, F. Benevenuto, and K. P. Gummadi. Measuring user in\ruence\nin twitter: the million follower fallacy. In Fourth International AAAI Conference\non Weblogs and Social Media (ICWSM 2010) , pages 10{17. AAAI Press, 2010.\n16. N. Chavoshi, H. Hamooni, and A. Mueen. Debot: Twitter bot detection via warped\ncorrelation. In ICDM , pages 817{822, 2016.\n17. M. Conover, J. Ratkiewicz, M. R. Francisco, B. Gon\u0018 calves, F. Menczer, and\nA. Flammini. Political polarization on twitter. ICWSM , 133:89{96, 2011.\n18. M. D. Conover, C. Davis, E. Ferrara, K. McKelvey, F. Menczer, and A. Flammini.\nThe geospatial characteristics of a social movement communication network. PloS\none, 8(3), 2013.\n19. M. D. Conover, E. Ferrara, F. Menczer, and A. Flammini. The digital evolution\nof Occupy Wall Street. PloS one , 8(5):e64679, 2013.\n20. S. Cresci, R. Di Pietro, M. Petrocchi, A. Spognardi, and M. Tesconi. The paradigm-\nshift of social spambots: Evidence, theories, and tools for the arms race. In Proceed-\nings of the 26th international conference on World Wide Web companion , pages\n963{972. International World Wide Web Conferences Steering Committee, 2017.\n21. C. A. Davis, O. Varol, E. Ferrara, A. Flammini, and F. Menczer. Botornot: A\nsystem to evaluate social bots. In WWW '16 Companion Proceedings of the 25th\nInternational Conference Companion on World Wide Web , pages 273{274. ACM,\n2016.\n22. E. De Cristofaro, N. Kourtellis, I. Leontiadis, G. Stringhini, S. Zhou, et al. Lobo:\nEvaluation of generalization de\fciencies in twitter bot classi\fers. In Proceedings\nof the 34th Annual Computer Security Applications Conference , pages 137{146.\nACM, 2018.\n23. J. DiGrazia, K. McKelvey, J. Bollen, and F. Rojas. More tweets, more votes: Social\nmedia as a quantitative indicator of political behavior. PloS one , 8(11):e79449,\n2013.\n18 Emilio Ferrara\n24. J. Echeverria, C. Besel, and S. Zhou. Discovery of the twitter bursty botnet. Data\nScience for Cyber-Security , 2017.\n25. R. E\u000eng, J. Van Hillegersberg, and T. Huibers. Social media and political partici-\npation: are facebook, twitter and youtube democratizing our political systems? In\nInternational Conference on Electronic Participation , pages 25{35. Springer, 2011.\n26. S. El-Khalili. Social media as a government propaganda tool in post-revolutionary\negypt. First Monday , 18(3), 2013.\n27. E. Ferrara. Manipulation and abuse on social media. ACM SIGWEB Newsletter ,\n(4), 2015.\n28. E. Ferrara. Disinformation and social bot operations in the run up to the 2017\nfrench presidential election. First Monday , 22(8), 2017.\n29. E. Ferrara. The history of digital spam. Communications of the ACM , 62(8):82{91,\n2019.\n30. E. Ferrara, O. Varol, C. Davis, F. Menczer, and A. Flammini. The rise of social\nbots. Commun. ACM , 59(7):96{104, 2016.\n31. E. Ferrara, O. Varol, F. Menczer, and A. Flammini. Detection of promoted social\nmedia campaigns. In Tenth International AAAI Conference on Web and Social\nMedia , pages 563{566, 2016.\n32. E. Ferrara and Z. Yang. Measuring emotional contagion in social media. PLoS\nOne, 10(11), 2015.\n33. E. Ferrara and Z. Yang. Quantifying the e\u000bect of sentiment on information di\u000busion\nin social media. PeerJ Computer Science , 1:e26, 2015.\n34. H. Gao, G. Barbier, and R. Goolsby. Harnessing the crowdsourcing power of social\nmedia for disaster relief. IEEE Intelligent Systems , 26(3):10{14, 2011.\n35. H. Gao, J. Hu, C. Wilson, Z. Li, Y. Chen, and B. Y. Zhao. Detecting and char-\nacterizing social spam campaigns. In Proceedings of the 10th ACM SIGCOMM\nconference on Internet measurement , pages 35{47. ACM, 2010.\n36. Z. Gilani, E. Kochmar, and J. Crowcroft. Classi\fcation of twitter accounts into\nautomated agents and human users. In Proceedings of the 2017 IEEE/ACM Inter-\nnational Conference on Advances in Social Networks Analysis and Mining 2017 ,\npages 489{496. ACM, 2017.\n37. S. Gonz\u0013 alez-Bail\u0013 on, J. Borge-Holthoefer, and Y. Moreno. Broadcasters and hidden\nin\ruentials in online protest di\u000busion. American Behavioral Scientist , 2013.\n38. S. Gonz\u0013 alez-Bail\u0013 on, J. Borge-Holthoefer, A. Rivero, and Y. Moreno. The dynamics\nof protest recruitment through an online network. Scienti\fc reports , 1, 2011.\n39. P. N. Howard. New media campaigns and the managed citizen . Cambridge Univ.\nPress, 2006.\n40. P. N. Howard and B. Kollanyi. Bots, #strongerin, and #brexit: Computational\npropaganda during the uk-eu referendum. Available at SSRN 2798311 , 2016.\n41. T. Hwang, I. Pearce, and M. Nanis. Socialbots: Voices from the fronts. Interactions ,\n19(2):38{45, 2012.\n42. T. N. Jagatic, N. A. Johnson, M. Jakobsson, and F. Menczer. Social phishing.\nCommunications of the ACM , 50(10):94{100, 2007.\n43. X. Jin, C. Lin, J. Luo, and J. Han. A data mining-based spam detection system\nfor social media networks. Proceedings of the VLDB Endowment , 4(12):1458{1461,\n2011.\n44. B. Kollanyi, P. N. Howard, and S. C. Woolley. Bots and automation over twitter\nduring the \frst us presidential debate. Technical report, COMPROP Data Memo,\n2016.\nBots, elections, and social media: a brief overview 19\n45. A. D. Kramer, J. E. Guillory, and J. T. Hancock. Experimental evidence of massive-\nscale emotional contagion through social networks. Proceedings of the National\nAcademy of Sciences , 111(24):8788{8790, 2014.\n46. S. Kudugunta and E. Ferrara. Deep neural networks for bot detection. Information\nSciences , 467(October):312{322, 2018.\n47. A. S. K\u007f umpel, V. Karnowski, and T. Keyling. News sharing in social media: a\nreview of current research on news sharing users, content, and networks. Social\nMedia+ Society , 1(2):2056305115610141, 2015.\n48. H. Kwak, C. Lee, H. Park, and S. Moon. What is twitter, a social network or a\nnews media? In Proceedings of the 19th international conference on World wide\nweb, pages 591{600, 2010.\n49. M. Latonero and I. Shklovski. Emergency management, twitter, and social media\nevangelism. In Using Social and Information Technologies for Disaster and Crisis\nManagement , pages 196{212. IGI Global, 2013.\n50. D. Lazer, A. S. Pentland, L. Adamic, S. Aral, A. L. Barabasi, D. Brewer, N. Chris-\ntakis, N. Contractor, J. Fowler, M. Gutmann, et al. Life in the network: the coming\nage of computational social science. Science (New York, NY) , 323(5915):721, 2009.\n51. K. Lee, J. Caverlee, and S. Webb. The social honeypot project: protecting online\ncommunities from spammers. In Proceedings of the 19th international conference\non World wide web , pages 1139{1140. ACM, 2010.\n52. K. Lee, J. Caverlee, and S. Webb. Uncovering social spammers: social honeypots+\nmachine learning. In Proceedings of the 33rd international ACM SIGIR conference\non Research and development in information retrieval , pages 435{442. ACM, 2010.\n53. L. Luceri, A. Deb, A. Badawy, and E. Ferrara. Red bots do it better: Comparative\nanalysis of social bot partisan behavior. In Companion Proceedings of the 2019\nWorld Wide Web Conference , pages 1007{1012, 2019.\n54. L. Luceri, A. Deb, S. Giordano, and E. Ferrara. Evolution of bot and human\nbehavior during elections. First Monday , 24(9), 2019.\n55. C. Lutz, C. P. Ho\u000bmann, and M. Meckel. Beyond just politics: A systematic\nliterature review of online participation. First Monday , 19(7), 2014.\n56. B. Markines, C. Cattuto, and F. Menczer. Social spam detection. In Proceedings of\nthe 5th International Workshop on Adversarial Information Retrieval on the Web ,\npages 41{48, 2009.\n57. J. Messias, L. Schmidt, R. Oliveira, and F. Benevenuto. You followed my bot!\ntransforming robots into in\ruential users in twitter. First Monday , 18(7), 2013.\n58. P. T. Metaxas and E. Mustafaraj. Social media and the elections. Science , 338:472{\n473, 2012.\n59. A. Minnich, N. Chavoshi, D. Koutra, and A. Mueen. Botwalk: E\u000ecient adaptive\nexploration of twitter bot networks. In Proceedings of the 2017 IEEE/ACM Inter-\nnational Conference on Advances in Social Networks Analysis and Mining 2017 ,\npages 467{474. ACM, 2017.\n60. B. M\u001cnsted, P. Sapie_ zy\u0013 nski, E. Ferrara, and S. Lehmann. Evidence of complex\ncontagion of information in social media: An experiment using twitter bots. Plos\nOne, 2017.\n61. A. Mukherjee, B. Liu, and N. Glance. Spotting fake reviewer groups in consumer\nreviews. In Proceedings of the 21st international conference on World Wide Web ,\npages 191{200, 2012.\n62. E. Mustafaraj and P. T. Metaxas. From obscurity to prominence in minutes:\nPolitical speech and real-time search. 2010.\n63. I. Pozzana and E. Ferrara. Measuring bot and human behavioral dynamics. arXiv\npreprint arXiv:1802.04286 , 2018.\n20 Emilio Ferrara\n64. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog , 1(8), 2019.\n65. J. Ratkiewicz, M. Conover, M. Meiss, B. Gon\u0018 calves, A. Flammini, and F. Menczer.\nDetecting and tracking political abuse in social media. ICWSM , 11:297{304, 2011.\n66. J. Ratkiewicz, M. Conover, M. Meiss, B. Gon\u0018 calves, S. Patil, A. Flammini, and\nF. Menczer. Truthy: mapping the spread of astroturf in microblog streams. In\nProceedings of the 20th international conference companion on World wide web ,\npages 249{252. ACM, 2011.\n67. C. Shao, G. L. Ciampaglia, O. Varol, K.-C. Yang, A. Flammini, and F. Menczer.\nThe spread of low-credibility content by social bots. Nature communications ,\n9(1):4787, 2018.\n68. S. Shorey and P. N. Howard. Automation, algorithms, and politics| automation,\nbig data and politics: A research review. International Journal of Communication ,\n10:24, 2016.\n69. J. Song, S. Lee, and J. Kim. Spam \fltering in twitter using sender-receiver rela-\ntionship. In International Workshop on Recent Advances in Intrusion Detection ,\npages 301{317, 2011.\n70. M. Stella, E. Ferrara, and M. De Domenico. Bots increase exposure to negative\nand in\rammatory content in online social systems. Proceedings of the National\nAcademy of Sciences , 115(49):12435{12440, 2018.\n71. S. Stieglitz, F. Brachten, B. Ross, and A.-K. Jung. Do social bots dream of\nelectric sheep? a categorisation of social media bot accounts. arXiv preprint\narXiv:1710.04044 , 2017.\n72. G. Stringhini, C. Kruegel, and G. Vigna. Detecting spammers on social networks.\nInProceedings of the 26th annual computer security applications conference , pages\n1{9. ACM, 2010.\n73. D. Stukal, S. Sanovich, R. Bonneau, and J. A. Tucker. Detecting bots on russian\npolitical twitter. Big data , 5(4):310{324, 2017.\n74. V. Subrahmanian, A. Azaria, S. Durst, V. Kagan, A. Galstyan, K. Lerman, L. Zhu,\nE. Ferrara, A. Flammini, and F. Menczer. The darpa twitter bot challenge. Com-\nputer , 49(6):38{46, 2016.\n75. J. N. Sutton, L. Palen, and I. Shklovski. Backchannels on the front lines: Emer-\ngency uses of social media in the 2007 Southern California Wild\fres . University\nof Colorado, 2008.\n76. M. Thelwall, K. Buckley, G. Paltoglou, D. Cai, and A. Kappas. Sentiment strength\ndetection in short informal text. Journal of the American Society for Information\nScience and Technology , 61(12):2544{2558, 2010.\n77. Y. Theocharis, W. Lowe, J. W. van Deth, and G. Garc\u0013 \u0010a-Albacete. Using twitter\nto mobilize protest action: online mobilization patterns and action repertoires in\nthe occupy wall street, indignados, and aganaktismenoi movements. Information,\nCommunication & Society , 18(2):202{220, 2015.\n78. K. Thomas, C. Grier, D. Song, and V. Paxson. Suspended accounts in retrospect:\nan analysis of twitter spam. In Proceedings of the 2011 ACM SIGCOMM conference\non Internet measurement conference , pages 243{258. ACM, 2011.\n79. K. Thomas, D. McCoy, C. Grier, A. Kolcz, and V. Paxson. Tra\u000ecking fraudulent\naccounts: The role of the underground market in twitter spam and abuse. In Usenix\nsecurity , volume 13, pages 195{210, 2013.\n80. O. Varol, E. Ferrara, C. Davis, F. Menczer, and A. Flammini. Online human-bot\ninteractions: Detection, estimation, and characterization. In International AAAI\nConference on Web and Social Media , 2017.\nBots, elections, and social media: a brief overview 21\n81. O. Varol, E. Ferrara, F. Menczer, and A. Flammini. Early detection of promoted\ncampaigns on social media. EPJ Data Science , 6(1):13, Jul 2017.\n82. O. Varol, E. Ferrara, C. L. Ogan, F. Menczer, and A. Flammini. Evolution of online\nuser behavior during a social upheaval. In Proceedings 2014 ACM conference on\nWeb science , pages 81{90, 2014.\n83. S. Vosoughi, D. Roy, and S. Aral. The spread of true and false news online. Science ,\n359(6380):1146{1151, 2018.\n84. S. Wasserman and K. Faust. Social network analysis: Methods and applications ,\nvolume 8. Cambridge university press, 1994.\n85. C. Yang, R. Harkreader, J. Zhang, S. Shin, and G. Gu. Analyzing spammers' social\nnetworks for fun and pro\ft: a case study of cyber criminal ecosystem on twitter. In\nProceedings of the 21st international conference on World Wide Web , pages 71{80.\nACM, 2012.\n86. K.-C. Yang, O. Varol, C. A. Davis, E. Ferrara, A. Flammini, and F. Menczer. Arm-\ning the public with arti\fcial intelligence to counter social bots. Human Behavior\nand Emerging Technologies , page e115, 2019.\n87. X. Yang, B.-C. Chen, M. Maity, and E. Ferrara. Social politics: Agenda setting\nand political communication on social media. In International Conference on Social\nInformatics , pages 330{344. Springer, 2016.\n88. D. Yates and S. Paquette. Emergency knowledge management and social media\ntechnologies: A case study of the 2010 haitian earthquake. International journal\nof information management , 31(1):6{13, 2011.\n89. J. Yin, A. Lampert, M. Cameron, B. Robinson, and R. Power. Using social media\nto enhance emergency situation awareness. IEEE Intelligent Systems , 27(6):52{59,\n2012.\n90. X. Zhang, S. Zhu, and W. Liang. Detecting spam and promoting campaigns in the\ntwitter social network. In Data Mining (ICDM), 2012 IEEE 12th International\nConference on , pages 1194{1199. IEEE, 2012.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Bots, elections, and social media: a brief overview", "author": ["E Ferrara"], "pub_year": "2020", "venue": "Disinformation, misinformation, and fake news in social \u2026", "abstract": "Bots, software-controlled accounts that operate on social media, have been used to manipulate  and deceive. We studied the characteristics and activity of bots around major political"}, "filled": false, "gsrank": 468, "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-42699-6_6", "author_id": ["0r7Syh0AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:9eHTPSM-sosJ:scholar.google.com/&output=cite&scirp=467&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D460%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=9eHTPSM-sosJ&ei=WbWsaIrWIPnSieoPxKLpgQ0&json=", "num_citations": 91, "citedby_url": "/scholar?cites=10066176438208553461&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:9eHTPSM-sosJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/1910.01720"}}, {"title": "A missed opportunity? President Trump, the truth sandwich, and news coverage across an ideological spectrum", "year": "2021", "pdf_data": "\u00a9 Media Watch |12 (2) 177-196, 2021\nISSN 0976-0911 | E-ISSN 2249-8818\nDOI: 10.15655 /mw/2021/v12i2/160145\nA Missed Opportunity? President Trump,\nthe Truth Sandwich, and News Coverage\nAcross an Ideological Spectrum\nLinda Jean Kenix1 & Jovita Manickam2\n1University of Canterbury, New Zealand\n2University of Auckland, New Zealand\nAbstract\nMany within mainstream news media wondered aloud and in print how to cover\nPresident Trump who, they purported, frequently distorted the truth. Although\nPresident Trump is no longer in office, this research remains vitally important\nfor understanding how the press covers the office of the Presidency when the\nveracity of information is in question.  During President Trump\u2019s tenure, the\n\u2018truth sandwich,\u2019 was suggested as a technique, whereby a fact is first stated in\na news article, then a quote of the false assertion, followed by the fact again.\nThis research aimed to explore the presence of the truth sandwich in an\nideological range of news content. These findings have implications for the\nfuture coverage of Presidencies in the United States, and for governments in\nother countries, where the veracity of information is questionable.\nKeywords: Truth sandwich, President Trump, Ideology, news\nIntroduction\nThe impetus for this study was to better understand the perceived war between main-\nstream journalists and the 45th President of the United States, Donald Trump. President\nTrump repeatedly made media outlets the target of vitriolic attacks, even going so far as to\ntweet that various media outlets are the enemy of the American people. In response, many\nwithin mainstream news media wondered aloud how to cover a president who so fre-\nquently bends and even breaks, the truth. Many within the news industry believed \u201cthe\nreality-based news media (had) to stop kowtowing to the Emperor\u201d (Sullivan, 2018). Al-\nthough President Trump is no longer in office, this research is vitally important for under-\nstanding how the press covers the office of the Presidency when the veracity of information is\nin question.\nOne central concept, memorably titled the \u2018truth sandwich,\u2019 was suggested\nrepeatedly as the only appropriate way that mainstream media could cover President\nTrump (Sullivan, 2018; Illing, 2018; Memmot, 2018). The truth sandwich, coined by linguist\nGeorge Lakoff, is a recommended three-part introduction to any article in which a false\nassertion is quoted. This concept was created as a specific technique to cover President\nTrump (Sullivan, 2018). In this construct, firstly, a factual statement or truth claim is made.\nThis is then followed by a quote of a false assertion made by President Trump.\nCorrespondence to: Linda Jean Kenix, Media and Communication Department, University\nof Canterbury, Private Bag 4800 Christchurch, New Zealand 8140.\n177\n178Media Watch 12 (2)\nFinally, the truth is stated again, reaffirming the first sentence of the article. President\nTrump\u2019s false claim is therefore \u2018sandwiched\u2019 in truth (Sullivan, 2018).\nThe key purpose of this research was to examine articles for the presence of the\ntruth sandwich as this has been noted by several media critics to be essential for any press\ncoverage of President Trump. In an effort to better understand the present media environment\nsurrounding the President, this research also asked whether the slant of coverage of\nPresident Trump was, across an ideological range of outlets, positive or negative. Further,\nas data could be collected from newspapers across the political spectrum, this research\nasked whether coverage differed between conservative and liberal newspapers in an effort\nto ascertain whether positivity or negativity might have originated from a specific source.\nThis article attempted to also examine what other methods, if any, the mainstream media\nemployed in their coverage of President Trump.\nThis paper first reviews the norms of journalism to better understand the present\nchallenges to journalism as an institution, steeped in its own historical norms. This\nmanuscript will then attempt to examine journalism in the era of President Trump. It is\neasier to understand just how different contemporary journalism might be if there is a\nbetter understanding of the how the practice has evolved.\nJournalism in America\nThe Anglo-American model of journalism has lent a high level of legitimacy to journalists,\nwho have served as \u201cmoral guardians,\u201d that possess an \u201celitist claim to responsibility,\u201d\nwhich transferred a great deal of power within society (Zelizer, 2018, p. 142). The impact of\nthis can be seen today in America as journalism there continues to perform as a moral\narbiter for societal issues. The other defining characteristic of the Anglo-American model\nis its constant innovation in journalistic form. American journalism, which is often fuelled\nby a form of American exceptionalism (Zelizer, 2018, p. 142; Chalaby, 1996, p. 303), has\ntransfused innovation into the Anglo-American model of American journalism.\nAt times, American journalism has fluctuated according to its ability or failure to\nadhere to long-held occupational norms, such as objectivity, accuracy and clarity (Chalaby,\n1996; Schudson, 2001). Regardless of this perception, many within the American journalistic\nprofession have maintained that \u201cjournalists use news to achieve pragmatic aims of\ncommunity\u201d (Zelizer, 1993, p. 82). Yet, despite the longevity and ubiquity of the Anglo-\nAmerican model, and the pragmatic aims of a community focus, journalism in America has\nmore recently been accused of being too far removed from the public it claims to serve\n(Zelizer, 2018). There is a common conception among conservatives in particular that\njournalism has become \u2018elite.\u2019\nIt is important to note that news media in America, and indeed worldwide, are not\none monolithic voice. Rather, journalism is ubiquitous, dynamic and multi-directional in\napproach. However, despite the political affiliation one may have, it remains an undeniable\ntruth that the elite, powerful news media predominately shape how the public interprets\nissues and events (Sotirovic, 2000). Consequently, the public\u2019s main understanding of\nsocial issues is largely derived from a framed construction provided by the elite news\nmedia, which is intrinsically from a distinct perspective than the general population (i.e.\nAltheide, 1976; Gamson, 1992; Gitlin, 1980; Ryan, Carragee, & Schwerner, 1998; Tuchman,\n1978). Although from a distinctly elite ideology, news media create purposefully framed\nimages after a complex negotiation of contributing forces that can also be seen to be\npolitically motivated (Street, 2001). This is not a structurally deterministic process whereby\nmedia simply disseminate elite, dominant messages. Rather, \u201cthere is an interaction within\n179the media, who operate as both structures and agents, not passively disseminating dominant\nideologies (as suggested by structural accounts) but playing an active role in their creation,\nconstruction articulation and communication\u201d (Allen & Savigny, 2012, p. 280).\nActively constructed and co-created messaging coalesces to form a broader\ncultural perspective, which then provides researchers with a framework to better understand\nsimilarities across large groupings (Thompson, 1990). Thus, the inclusion or exclusion of\nparticular information, on a national scale, can have profound ideological implications.\nThis has obvious importance in a politically charged environment such as the Presidency\nof the United States. It is a common perception in the United States that news media are\nslowly losing their legitimacy in the digital era (Lischka, 2019; McDevitt & Ferrucci, 2018).\nActive audiences in America often believe that journalism is embedded with bias (Eveland\n& Shah, 2003) \u2013 whether conservative or liberal -  and journalism itself is not worthy of\npublic trust (Kohut, 2004). This lack of trust may be traced to a longstanding public\nadherence to the values of accuracy and unbiased reporting that has far superseded speed\nor serving as a watchdog (Beam, Weaver, & Brownlee, 2009), despite those in the news\nmedia widely supporting these characteristics as essential defining aspects of their\nprofession (Beam et al., 2009). Consequently, there are myriad contradictions and potential\nproblems for the stability and legitimacy of American news media - these were only further\nexacerbated by the rise of President Trump.\nJournalism in the Trump Era\nThe electoral campaign and presidency of Donald J. Trump led to seismic shifts in the news\nlandscape. During the Trump administration, journalists faced the challenge of reporting\non a president who not only subverts and restricts the traditional mechanisms by which\nthe press monitor the presidency (Zelizer, 2018, p. 146), but went further to be openly\nantagonistic towards many journalists. Although this poses obvious problems for news\ninstitutions, it also presented an opportunity for the overhaul of journalism\u2019s deeply\nentrenched norms. As Zelizer (2018) writes, coverage of President Trump \u201cmade visible the\nbubble in which journalism lives\u201d (p. 150) and the dual phenomena of Brexit and President\nTrump made a case for \u201cjournalism\u2019s necessary and immediate reset\u201d (Zelizer, 2018, p. 152).\nThis reset, were it to occur, would be set in the context of a president at war with\nthe press. Although tension between the executive office and the press is not a new\nphenomenon, President Trump\u2019s open hostility towards most mainstream news\norganisations set him apart from his predecessors (Mour\u00e3o, Thorson, Chen, & Tham,\n2018).\u00a0President\u00a0Trump\u00a0was\u00a0prone\u00a0to\u00a0incisive\u00a0Twitter\u00a0attacks\u00a0on\u00a0the\u00a0press,\u00a0frequently\nlevelling accusations of \u2018fake news\u2019 and failure at many news outlets to paint himself\nfavourably\u00a0(McVittie\u00a0\u00a0&\u00a0McKinlay,\u00a02018).\u00a0He\u00a0barred\u00a0reporters\u00a0from\u00a0press\u00a0briefings\u00a0and\nother media events who were from outlets that he deemed as \u2018fake news\u2019. President\nTrump\u00a0also\u00a0displayed\u00a0favouritism\u00a0towards\u00a0traditionally-viewed\u00a0conservative\u00a0media,\u00a0often\nappearing on the Fox cable news network and granting press credentials to unorthodox\nconservative outlets such as Infowars\u00a0(Mour\u00e3o\u00a0et\u00a0al.,\u00a02018,\u00a0p.\u00a01946).\nPresident Trump\u2019s mistrust of the press reverberated beyond news organisations.\nIn a study of news trust during the early months of the Trump administration, \u201csupport for\nTrump [was] the strongest predictor of news distrust\u201d(Mour\u00e3o et al., 2018, p. 1945) among\nthe general public. President Trump\u2019s supporters were most likely to express distrust in\nmainstream media, and gravitated towards conservative news sources, whereas those\nwho consumed mostly mainstream media were less likely to be conservative and more\nlikely to have high levels of mistrust towards the President (Mour\u00e3o et al., 2018, p. 1953).Kenix & Manickam\n180Media Watch 12 (2)\nNews media trust in general declined since President Trump\u2019s election, which is concerning\nfor the status of journalists given that \u201cnews media trust is crucial to the role that journalism\nplays in democracies\u201d (Mour\u00e3o et al., 2018, p. 1945).\nNews organisations appear hyperaware of the challenges involved in covering\nthe President. In the wake of the election some institutions issued memorandums on new\nmodes of covering President Trump (Zelizer, 2018, p. 147), while others made grand gestures\nto signal a suspension of normal relations. In an unprecedented and potentially costly\nmove, CNN made the decision not to broadcast live the first briefing of then-press secretary\nSean Spicer, citing a need to monitor what was said and then report on it later (Messina,\n2018, p. 92). The New York Times  also employed a range of verbal delegitimizing strategies\nto combat President Trump\u2019s fake news accusations and all the while asserted their own\nlegitimacy as a news organization (Lischka, 2019).  Both CNN and the New York Times  are\norganisations that President Trump frequently took aim at for disseminating negative\ncoverage about him. These news organizations had a vested interest in maintaining their\nlegitimacy in the face of his frequent attacks. Other news organisations did not take such\nmeasures, and have at times been critical of these organisations, which has led to further\nfracturing of the news environment (Zelizer, 2018). In any case, the effectiveness of these\nmeasures remains to be seen. Many of the so-called new strategies put forward to combat\nPresident Trump were employed before, to little effect. Journalists resorted to old tools such as\nobjectivity, balance and moderation (Zelizer, 2018), however, President Trump\u2019s attacks on the\nmedia continued to pepper his Twitter feed and characterise his administration.\nPresident Trump demonstrated considerable awareness of the fact that \u201cthe space\nbetween news and entertainment collapsed\u201d (Edwards, 2018, p. 40).  Many news outlets\nnoted that he carried aspects of his reality television show, The Apprentice , into the White\nHouse. Indeed, President Trump contributed to the contemporary US political system\nbecoming \u201ca form of global entertainment\u201d (Edwards, 2018, p. 27). Of course, Twitter further\nenhanced this perception. Much has been written about President Trump\u2019s use of social\nmedia, particularly his use of Twitter (Edwards, 2018; Lischka, 2019; Wahl-Jorgensen,\n2018). President Trump relied on Twitter as his primary social media outlet, with its\n\u201ccharacteristic character limit, the way sarcasm propels language on the platform and its\nspeed and trajectory\u201d (Edwards, 2018, p. 30). In broadcasting his thoughts directly through\nTwitter, President Trump bypassed the traditional gatekeeping function of mainstream\nmedia but continued to benefit from the extensive coverage of his tweets  in the mainstream\npress (Wahl-Jorgensen, 2018). Although journalists expressed frustration at President\nTrump\u2019s continual use of Twitter, they also defended their ongoing relevance in light of it\n(Edwards, 2018). Most importantly, most mainstream news outlets continued to give\nPresident Trump the coverage he desired (Zelizer 2018).\nAs the lines between news and entertainment become increasingly blurred, there\nremains questions about how exactly the mainstream media have responded to their\ncoverage of President Trump. Many argued that there needs to be a fundamental shift in\nhow the Presidency and the White House were covered. But did these changes take place?\nDid the mainstream media really shift their coverage of the President or did they simply\nsignal for a change that never occurred?\nPresident Trump and the \u2018Truth Sandwich\u2019\nGitlin (1980)  long ago defined frames as \u201cpersistent patterns of cognition, interpretation,\nand presentation, of selection, emphasis, and exclusion, by which symbol-handlers\nroutinely organise discourse\u201d (p. 7). A frame determines what is \u201crelevant\u201d (Hertog & McLeod,\n1811995, p. 4) and \u201csuggests what the issue is\u201d (Tankard Jr., Hendrickson, Silberman, Bliss, &\nGhanem, 1991). Frames (both textual and visual) are \u201corganizing principles that are socially\nshared and persistent over time, that work symbolically to meaningfully structure the\nsocial world\u201d (Reese, Gandy Jr., & Grant, 2001, p. 11).\nThe idea of framing, however, is complicated when various elite symbol handlers\norganise discourse differently. President Trump challenged what is \u2018relevant\u2019 and \u2018what the\nissue is\u2019 by contradicting prevalent news frames in the news media. Thus, the media frame\nshifted significantly within any given day. Professor Rosen, a media critic, writer, and\nprofessor of journalism at New York University, said \u201cjournalists charged with covering\n(President Trump) should suspend normal relations with the presidency of Donald Trump,\nwhich is the most significant threat to an informed public in the United States today\u201d\n(Rosen, 2018). He argued that mainstream news organisations should send their interns to\ncover the White House (and therefore refuse to lend prestige to the office) given the amount\nof untruths that have been discovered. Rosen (2018) stated clearly that the mainstream\npress must suspend normal relations in how they cover the president.\nThis sentiment has been echoed by Memmott at NPR (2015). In an article titled\n\u201cWhen there is no evidence to support a claim, we should state that\u201d the author concludes\nthat the news media have not clearly stated when something is false and that must change.\nHe argues forcefully for a \u201cno evidence\u201d frame. This sentiment is the rationale behind what\nhas been called the \u2018truth sandwich.\u2019\nAs mentioned in the Introduction, the \u2018truth sandwich\u2019 is a recommended three-\npart introduction to any article in which President Trump is quoted. This construct consists\nof a factual statement or truth claim in the first sentence, followed by a quote of a false\nstatement by President Trump, and then a reinforcement of the accurate statement made in\nthe first sentence of the article. President Trump\u2019s false claim is therefore \u2018sandwiched\u2019 in\ntruth (Sullivan, 2018). Lakoff has argued that \u201cif you negate a frame, you activate the\nframe\u201d (Illing, 2018). President Trump has used this to his advantage by saying or tweeting\nsomething that is blatantly false, knowing that news media will cover it. Although many\njournalists then go on to fact-check his claim, the fact that President Trump\u2019s framing of an\nissue is stated first means that readers are still going to remember the president\u2019s framing\n(Illing, 2018). An effective way to counteract this, according to Lakoff (Sullivan, 2019), was\nto make use of the \u2018truth sandwich\u2019 at the opening of any article on President Trump, which\nsubsequently set the tone for the rest of the article.\nAs the \u2018truth sandwich\u2019 was only recently conceptualised, there was no previous\nresearch that could be found on it\u2019s existence or lack of existence in media coverage.\nHowever, research has argued that the U.S. presidency possesses so much significant news\nvalue that journalists could naturalize presidential behaviour despite professional\njudgment (Parks, 2020). This may explain why initial untruths in the potential \u2018truth\nsandwich\u2019 coverage were so readily accepted. There has also been recent research that\ndemonstrates conducting fact checks actually increases media trust, epistemic political\nefficacy and future news intent (Pingree et. al., 2018). This strongly suggests that writing\nstories with the \u2018truth sandwich\u2019 embedded in a news story could actually be beneficial for\nthe journalistic profession. Yet, a high level of fact checking simply did not occur during\nthe Trump presidency. For almost the entirety of his Presidency, the President simply lied\nwithout much contestation from the press. This has led some journalists to opine that, in\nthe future, journalists must remember that \u201cthe first obligation of journalism is not \u2018balance,\u2019\nbut truth\u201d (Rubin, 2020).\nThere has been some pushback against the idea of the \u2018truth sandwich,\u2019 most\nnotably in a Wall Street Journal  column, in which Lakoff\u2019s arguments were said to beKenix & Manickam\n182Media Watch 12 (2)\ncondescending to audiences and that any report on the President\u2019s statements should\nbegin with his own words, not the words of the journalist covering him (Sartwell 2018).\nDespite this critique, however, the \u2018truth sandwich\u2019 stands as a widely-circulated mechanism\nby which journalists could report on President Trump. Therefore, this research examined\narticles for the presence of the \u2018truth sandwich\u2019 and explored how the mainstream media\nwere covering President Trump.\nResearch Questions\nThis paper wishes to better understand how journalists across the ideological spectrum\ncovered President Trump. Specifically, this research is aimed at understanding if a \u2018truth\nsandwich\u2019 existed in coverage of President Trump.\nRQ1 : Did the \u2018truth sandwich\u2019 exist in mainstream news articles where President Trump\nwas quoted?\nAs a central concern of many journalists was the false assertions made by President\nTrump, usually in the form of quotes, this research also sought to discern the current\npractices around quoting President Trump. This involved noting whether he was quoted in\nthe headline or lead paragraph of an article, whether or not someone in his administration\nwas quoted as speaking on his behalf, and whether any voices contradictory to President\nTrump were also quoted in a given article. Given President Trump\u2019s extensive use of Twitter\nand the media storms that often followed his tweets, quotes or mentions of his tweets were\nalso coded for, alongside other variables related to the President\u2019s Twitter usage. Therefore,\na second research question was:\nRQ2 : In what capacity was President Trump quoted?\nPresident Trump said repeatedly that he received negative coverage from the mainstream\npress and, conversely, the mainstream press said repeatedly that they treated him fairly.\nThus, the third research question was as follows:\nRQ3 : How did the mainstream press cover President Trump (more negative or positive)?\nFinally, although this was not the main focus of this research, this paper examined\nnewspapers across the political spectrum, and therefore the following research question\nwas asked:\nRQ4 : Was there a difference in the coverage of President Trump between conservative and\nliberal newspapers in the mainstream press?\nMethodology\nThis study examined four newspapers\u2019 coverage of President Trump between 20 January\n2017 and 3 September 2018 as this period of time was illustrative of the Trump presidency.\nThe process of ascertaining repeating patterns or themes from each source was guided by\nthematic analysis (Braun & Clarke, 2006), which used the research questions as a guide to\nthen develop selection criteria of codes that were used for the quantitative content analysis.\nAn initial identification of themes was made first when examining the entire dataset and\nthen those themes were continually refined after closer readings of the text. During this\nprocess, principal themes, that could not have been uncovered through segmented or\ncursory readings, began to emerge. Emerging narratives were viewed as representative of\nimplicit positions of power that worked to define those who held epistemic authority.\n183These narratives were the \u201coutcome of strategic communication decisions\u201d (Jones &\nHimelboim, 2010, p. 277) that imbue a person, place or thing with power or a lack thereof.\nAn emerging narrative was the manifestation of that positioning. Using thematic analysis,\nat this stage of the research, was important as it is not \u201cwedded to any pre-existing theoretical\nframework\u201d (Braun & Clarke, 2006, p. 81). Most significantly, thematic analysis examines\n\u201cwhether a theme is salient or relevant to a particular study is based on context and not\nnecessarily determined by quantifiable measures (e.g. frequency of occurrence)\u201d (Xu, 2020,\np. 253). This aspect of thematic analysis was very important as the final coding categories\nneeded to allow for the salience within each article.\nThe final coding categories, determined from an exhaustive thematic analysis\nguided by the research questions, enabled each of the four research questions to be\nanswered in a quantitative content analysis. Only a simple \u2018yes\u2019 or \u2018no\u2019 was in response to\nthe first research question of whether a \u2018truth sandwich\u2019 was present in the article. The\nsubsequent research questions, however, were coded through a range of variables\n(Table 2) that, again, were determined through a thematic analysis of the text.\nThe four newspapers that were in included in the final study had to meet two main\ncriteria. The first criterion was that all of the four selected newspapers had to have large\ncirculations. The circulation number of newspapers were checked from several different\nwebsites (Agility PR Solutions, 2017; IPFS, 2018; Statista, 2018). Each of the newspapers\nselected had to be well established (Peiser, 2018; Pew Research Center, 2018). The second\ncriterion was that these newspapers had to be classified as \u2018liberal\u2019 or \u2018conservative\u2019 from\nat least three amalgamated websites (Kenix & Jarvandi, 2019). The determination of those\npolitical labels came from an examination of several online sources that corroborated\nassumptions in popular culture and also the researchers of this study. These assumptions\nwere then tested against this dataset. However, the criterion for including each newspaper\nwithin a political ideology required verification from at least three different online sources\nthat were not individual opinions, but amalgamations of collective perspectives or research.\nIt is important that the online sources used were amalgamations of opinion rather than\nsimply an individual source, as these combined sites offered some level of assuredness\nsimply by way of bulked information. These amalgamated websites didn\u2019t provide absolute\naffirmation independently, but there was a heightened level of approbation when there\nwas consistency across consolidated sites \u2013 as was the case in this study. It is important\nto note that the terms \u2018liberal\u2019 and \u2018conservative\u2019 have different meanings in different parts\nof the world. However, as we used a compilation of amalgamated websites regarding the\npolitical ideology of each outlet (Table 1), we had no concern of the global differences.\nThe New York Times  and The Washington Post  were selected as liberal sources and\nThe New York Post  and The Wall Street Journal  were selected as conservative sources. These\npolitical ideologies are transmuted through mediated content in the same way that they\nare communicated through verbal communication \u2013 through words, phrases, and emphases\non particular aspects of a shared narrative and the specific framing of current events. The\nlarger public associates meaning to these political identifiers symbolically (Conover &\nFeldman, 1981) and feels the \u201crelative\u201d esteem of ideological conventions communicated\nby elite media (Schiffer, 2000).\nA form of constructed week sampling was used to select dates for article collection.\nPrevious research has found that \u201cconstructed week sampling is more efficient than simple\nrandom sampling or consecutive day sampling\u201d (Hester and Dougall, 2007, p. 811). Articles\nwere collected on a successive day of each month over a twenty-one-month period (i.e. the\nfirst Monday of the first month, the first Tuesday of the second month & so on, moving on\nto the second occurrence of each weekday at the end of the first cycle). This allowed for theKenix & Manickam\n184Media Watch 12 (2)\nexamination of articles at every point in the weekday news cycle. Articles were found\nthrough the news and journalism database, Factiva, by searching for news articles that\nmentioned President Trump in the headline or lead paragraph with the subject filter of\n\u2018domestic politics.\u2019 Any editorial content was removed from the sample, yielding a total of\n528 articles to be coded.\nThe Cohen\u2019s Kappa inter-observer reliability coefficient was utilized to provide an\nindication of the coding scheme\u2019s reliability. Intercoder reliability, as measured through\nCohen\u2019s Kappa, ranged from 87.26% to 94.38% for all of the 20 coded variables. The overall\nintercoder Cohen\u2019s Kappa was 91.21%, suggesting a highly robust coding scheme.\nFindings\nThis study employed descriptive statistics to examine variables and ascertain any\nrelationships that existed between them. Percentages, frequencies, chi-square correlations\n(2), observed counts, expected counts and adjusted residual values were all used to answer\nthe research questions. No one measure itself provided strong evidence of a particular\nfinding, but when taken cohesively, there was a suggestion of associative strength.\nRQ1 : The truth sandwich was only confirmed to be present in one article (0.2%), which\nwas in The New York Post . The first research question asked, \u2018does the \u2018truth sandwich\u2019\nexist in mainstream news articles where President Trump is quoted?\u2019 Thus, there was\nlittle to no evidence of the truth sandwich in any of the 528 sampled articles.\nRQ2 : The second research question asked, \u2018in what capacity was President Trump quoted?\u2019\nA direct quote from President Trump was present in 44.1 percent of the articles where\nmention of President Trump was in the headline or lead paragraph. The most frequent\nquote of President Trump in all of the articles was when he gave a speech or an address\n(8.7%). A direct quote from his Twitter feed was the second most popular means of quotation\n(7.8%), with press conferences and interviews accounting for 5.3 percent and 5.1 percent\nof quotes, respectively. A large percentage of articles did not quote President Trump\n(55.9%) so the numbers here have to be taken with caution as their relative sample size\nwas small. There was a relatively high percentage of articles in which someone is speaking\non behalf of President Trump or his administration (32.8 percent). Almost half of the\narticles sampled contained a quote opposing President Trump (49.1 percent). This is\nhigher than the direct quotes from President Trump (44.1 percent).\nOnly seven percent of articles contained a direct new quote of a tweet by President\nTrump (a tweet published on the day of or the day before the article) and 6.6 percent of\narticles contained a direct quote from an older tweet. Thus, Twitter did not feature as a\nmajor source in the sample. In a similar vein, new tweets by President Trump were\nmentioned, without being directly quoted, in only 1.3 percent of articles, and older tweets\nwere mentioned in 5.1 percent of articles. Finally, 6.8 percent of articles made reference\nto President Trump being a \u2018twittering\u2019 president, or something similar, and in all of these\ninstances, this was portrayed as being a negative or neutral attribute. However, the\noverwhelming majority of articles in this sample did not cite President Trump\u2019s use of\ntwitter as one of his defining characteristics (93.2 percent) despite his active use of the\nplatform. Only 5.5 percent of articles contained a reference to attacks on the press and\n5.4 percent of articles mentioned fake news.\nThe relationship between a quote of an old tweet by the President and research\nof that tweet is significant (x2 = 25.432, df = 1, p = .000) and demonstrates that President\n185Trump\u2019s tweets were more likely to be challenged or researched than would be expected\nby chance alone (adjusted residual = 5.0). However, when comparing a quote of a new\ntweet by President Trump and research of that tweet, the relationship is even more\nsignificant (x2 = 160.973, df = 1, p = .000). Therefore, when a new tweet of President\nTrump\u2019s is quoted in an article, that tweet is significantly more likely to be challenged or\nresearched than a quote of an old tweet (adjusted residual = 12.7). A significant difference\nis found when considering research of both old and new tweets by President Trump in\narticles [Figures 1 and 2].\nOnly one significant relationship was found when comparing the situations that\ngave rise to President Trump being quoted and the general view of President Trump in an\narticle (x2 = 43.699, df = 14, p = .000). It was found that when President Trump was quoted\nfrom a speech or address, the view of President Trump in the article was more likely to be\nnegative than would be expected from chance alone (adjusted residual = 3.8) [Figure 4].\nRQ3 : The third research question asked \u2018how did the mainstream press cover President\nTrump (more negative or positive)? \u2018The general view of President Trump in this sample\nwas negative (41.3 %) or neutral (53.8%) and only 4.9 percent of articles were coded as\nhaving a positive view of President Trump overall. Thus, the mainstream press covered\nPresident Trump more negatively then positively in this sample.\nThe relationship between the presence of a direct quote from President Trump\nand the general view of President Trump in an article was significant (x2 = 26.606, df = 2,\np = .000). When President Trump was quoted in an article, the general view of him was\nmore negative than would be expected by chance alone (adjusted residual = 4.6).\nConversely, when President Trump was not quoted, the article was more neutral than\nwould be expected by chance alone (adjusted residual = 5.2). Neither the presence nor the\nabsence of a direct quote from President Trump led to increased positivity in coverage.\nThe relationship between the presence of a quote opposing President Trump and\nthe general view of President Trump in an article was highly significant (x2 = 81.913,\ndf = 2, p = .000). When a quote opposing President Trump was present, the article was\nsignificantly more likely to present a negative view of President Trump than would be\nexpected by chance alone (adjusted residual = 9.0) and when there was no quote opposing\nPresident Trump, the article was considerably neutral (adjusted residual = 8.4). Again,\nboth the absence and the presence of a quote opposing President Trump did not lead to\nincreased positivity in coverage.\nThe relationship between a mention of President Trump on Twitter and the general\nview of President Trump on Twitter was the most highly significant of all relationships\nexamined (x2 = 512.292, df = 2, p = .000) [Figure 3]. The very high chi-square value indicated\nthat when President Trump\u2019s use of Twitter was mentioned in an article, the view of his\nTwitter habits in the article was significantly more likely to be negative than would be\nexpected by chance alone (adjusted residual = 17.3). His Twitter habits were also more\nlikely to be noted as neutral than would be expected by chance alone (adjusted residual\n= 14.0). In this study, President Trump\u2019s use of Twitter was never positive.\nThe relationship between the mention of attacks on the press and mentions of\nfake news was significant (x2 = 70.627, df = 1, p = .000) [Figure 5]. If an article mentioned\nan attack on the press it was more likely to also mention fake news than would be\nexpected by chance alone (adjusted residual = 8.4). The relationship between the general\nview of President Trump in the article and the general view of President Trump on Twitter\nwas also significant (x2 = 17.305, df = 4, p = .002) [Figure 6]. When the general view of\nPresident Trump in a given article was negative, the view of President Trump\u2019s Twitter use\nin that article was more likely to be negative than would be expected by chance aloneKenix & Manickam\n186Media Watch 12 (2)\n(adjusted residual = 3.3). The third research question read, \u2018How did the press cover\nPresident Trump (more negative or positive) in the sample period?\u2019 Given these findings, in the\n528 sampled articles, it was found that President Trump was predominantly covered negatively.\nRQ4 :The final research question asked if there was any difference in the coverage of\nPresident Trump between conservative and liberal newspapers. A total of 63.4 percent of\ncoded articles were from The New York Times  and The Washington Post  (categorized as\nliberal newspapers). The remaining 36.6 percent of articles were from the conservative\nnewspapers, The New York Post  and The Wall Street Journal . There was not any significance\nbetween the type of newspaper and the amount of quotes from President Trump. However,\nthe general view of President Trump was found to be highly significant (x2 = 52.799, df = 2,\np = .000) depending on if the publication was liberal or conservative. If the newspaper is\nliberal, then it was more likely to have a negative general view of President Trump than\nwould be expected by chance alone (adjusted residual = 6.7). If the newspaper is\nconservative, then it was more likely to have a positive general view of President Trump\nthan would be expected by chance alone (adjusted residual = 4.0).\nAlthough there is not any significant relationship between the ideology of the\nnewspaper and the amount of quotes from President Trump, there was a significant\nrelationship between the ideology of the newspaper and the presence of quotes that\noppose President Trump (x2 = 25.023, df = 1, p = .000). If a newspaper is liberal, then it was\nmore likely to have an opposing quote to President Trump than would be expected by\nchance alone (adjusted residual = 5.0). If a newspaper is conservative, then it was far\nmore likely to not have an opposition to President Trump than would be expected by\nchance alone (adjusted residual = 5.0).\nThere was also a significant difference in coverage between liberal and\nconservative newspapers and their mention of President Trump on Twitter (x2 = 8.557,\ndf = 1, p = .003) as well as the general view of President Trump on Twitter (x2 = 10.285, df\n= 2, p = .006). Liberal newspapers were more likely to mention President Trump on Twitter\nthan would be expected by chance alone (adjusted residual = 2.9) and they were more\nlikely to hold a negative general view of President Trump on Twitter than would be expected\nby chance alone (adjusted residual = 2.6). Therefore, given these results, this research\nfound that there was a significant difference in the coverage of President Trump between\nconservative and liberal newspapers.\nDiscussion\nAs stated earlier, this study was born out of a desire to better understand the often-\ncontentious relationship between mainstream journalists and the 45th President of the\nUnited States, Donald Trump. President Trump was openly hostile towards many individuals\nand institutions within the media, seeking to undermine their authority by any means\npossible. For their part, many within mainstream news media faced a conundrum of how\nto cover a president who so frequently bended and even broke, the truth. The \u2018truth sandwich,\u2019\na linguistic construct put forth by linguist George Lakoff, was suggested repeatedly as the\nonly appropriate response for mainstream media. The construct puts facts first, ahead of\nerroneous claims, and ends with the facts restated. This is perceived by many in the\nmainstream media to be a way in which news media can minimise the influence false\nclaims. This article attempted to understand if that truth sandwich was being utilized in\nthe mainstream press and if the coverage of President Trump was generally positive or\nnegative. As data could be collected from newspapers across the political spectrum, this\n187research also considered whether coverage differed between conservative and liberal\nnewspapers in an effort to ascertain whether positivity or negativity might originate from\na specific source. Although President Trump is no longer in office, this research is vitally\nimportant for understanding how the press covers the office of the Presidency when the\nveracity of information is in question.\nOnly minimal evidence of the truth sandwich was found in this dataset. Indeed,\nonly one confirmed instance was found from a New York Post  article in March 2018. This is\nextraordinary given the consensus within both the academic and journalistic communities\nthat there needed to be a significant change to the way in which President Trump was\nreported (Zelizer 2018; Stelter 2017; Kilby 2018; Lischka 2019). The truth sandwich appeared\nto be a plausible mechanism for effecting such a change, but it wasn\u2019t employed in any\nmeaningful way by any of the news outlets whose articles were examined in this study.\nIts absence is made even more surprising by the fact that Lakoff\u2019s recommendation was\nwidely circulated and lauded by various news outlets, such as CNN (2018), NPR (Memmott,\n2018) and The Washington Post (Sullivan, 2018). All of these commendations were from\nliberal media organisations, yet the truth sandwich was only used by the New York Post , a\nconservative newspaper. News organizations have been clearly aware of Lakoff\u2019s\nrecommendation and in some instances even advocated for its use, however, this appears\nnot to have been translated into actual practice. This suggests that the press did not want\nto engage with the work of devising a \u2018truth sandwich.\u2019\nWhat was evident in the sample was a strong negativity towards President Trump\nand a lack of direct sourcing. President Trump was not quoted in the majority of articles in\nwhich he was mentioned in the headline or lead paragraph. What seems apparent is that\nmany news organizations, rather than attempting a truth sandwich, were simply omitting\nany direct sourcing of the President in their coverage. Journalists were continuing to rely\non quotes from traditional spokespeople for the President, indicating that although President\nTrump\u2019s Twitter feed is one source of information about the Presidency, it is not the only one.\nJournalists were also clearly seeking information from individuals outside of the Presidency.\nThere is copious literature on President Trump\u2019s use of Twitter and the purported\nnews institutions\u2019 excessive coverage of his Twitter feed (Wahl-Jorgenson 2018; Edwards\n2018; Lischka 2019). However, this research suggests that although President Trump\u2019s use\nof Twitter is a marker of his presidency, journalists are still relying on quotes from more\ntraditional platforms \u2013 when they use quotes at all. It must be noted that a lack of quotes\nfrom President Trump may have been because he did not make himself available to the media.\nFurther research should ascertain whether the dearth of quoting found here is because President\nTrump withheld time to the media or the press has simply chosen not to quote him.\nThe news media in this sample also almost entirely omitted any reference to\nattacks on the press or any mention of fake news. In addition to the absence of these\nattacks, which the President reiterated on a daily basis, the general view of President\nTrump in articles was overwhelmingly negative. This negativity increased when President\nTrump was actually quoted. Conversely, when President Trump was not quoted, the article\nwas more neutral. It is interesting to note that neither the presence nor the absence of a\ndirect quote from President Trump led to increased positivity in coverage. Thus, it does not\nappear that what President Trump said actually led to more positive coverage. Rather, the\nPresident was cast negatively regardless of content.  The trend of negativity also continued\nwhen there were quotes opposing President Trump, which occurred more than quotes from\nPresident Trump himself. These opposing quotes were significantly more likely to present\na negative view of President Trump. Again, both the absence and the presence of a quote\nopposing President Trump did not lead to increased positivity in coverage.Kenix & Manickam\n188Media Watch 12 (2)\nThis is concerning, both for the President, who presumably aimed for an improved image,\nand for the press, who appeared to remain steadfastly determined to present the President\nnegatively in this sample. It may be that the President \u2018deserved\u2019 this coverage through\nactions that warranted this coverage, but the monolith of coverage found here demonstrates\nthat there was little investigations contrary to this portrayal.\nPresident Trump also appeared to have a particularly critical lens on the press.\nIf an article mentioned an attack on the press it was more likely to also mention fake news.\nThis is interesting as it suggests that criticisms of the press and accusations of fake news\nare intertwined. This is obviously dangerous for a free press. If the President of the United\nStates attacks the press and also alleges that they are \u2018fake,\u2019 then it is inevitable that public\nconfidence in the press will begin to erode. This was repeatedly demonstrated through\npublic opinion polls. There is little if anything that could be done to ebb these persecutions.\nHowever, it must be noted that the press could have certainly shifted their coverage or\nutilised the \u2018truth sandwich.\u2019\nAs Schudson has said, \u201cthe biggest challenge for journalism is to assess itself and\nto find some intellectual equilibrium\u201d (Schudson, 2019b, p. 77). The finding that there were\nsome significant differences between conservative and liberal newspapers should lead\nthe newspaper industry to pause. The fact that liberal newspapers covered President Trump\nnegatively more than one would expect from chance alone is disconcerting. It certainly\nmay be that his Presidency warrants this, but there was a fundamental difference in how\nconservative newspapers were covering President Trump. It certainly may be that\nconservative newspapers were giving President Trump a \u2018pass\u2019 and the liberal newspapers\nwere not. This paper did not look at that level of content. More research that examines the\ncontent of this coverage is needed. However, this distinction does not bode well for a well-\nfunctioning and democratic media sphere. Schudson\u2019s (2019a) advice for the media to\nassess itself desperately needs to be heeded.\nIf media critics are to be celebrated for their idea of a \u2018truth sandwich\u2019 in media\ncoverage, then such a construct needs to be in the final content. That was clearly not the\ncase here. A \u2018truth sandwich\u2019 would allow for the President to be quoted and also allow for\nan accurate record to be reported. Simply not quoting the President of the United States, as\nit appears many mainstream outlets - particularly liberal newspapers - did here, does not\nsolve any concern of fake news or attacks against the media. Nor does this method properly\nrecord the Presidency of the United States. Conservative newspapers need to be reminded\nof their role in a democracy. Namely, to ensure that the public are well-informed. This was\nnot clear from the sample. The news media absolutely need to present a fair and balanced\nperspective of President Trump - all the news media. The democracy of the United States of\nAmerica depends upon it.\nLimitations\nEven though this process is mainly quantifiable and objective, a limitation of this research\nis that some interpretation had to be present. In other words, the researcher\u2019s interpretation\nof reality played a role as the lexical terms found may not necessarily be perceived the\nsame way by everyone. Much more research is needed in this area to tease out exactly how and\nwhy the press covered President Trump in the way that they did. The sample here of content was\ntoo small to make any generalizable conclusions, although it appears there is an ideological\nbent to coverage and that there was a pre-disposition in coverage of President Trump.\nFurther, interviews with reporters and editors about the rationale for not using\nthe \u2018truth sandwich\u2019 is necessary to know the utility of this journalistic construct.\n189Table 2. Coded variables and value options\nVariables Value Options\nDirect Quote from President Trump Present, Not Present\nPlatform for President Trump Twitter, Press Conference,\nSpeech/ Address, Interview,\nOther, Not Applicable, Not\nMentioned\nPresence of President Trump quote in headline or lead paragraph Present, Not Present\nGeneral view of President Trump Positive, Negative, Neutral,\nNot Applicable\nPresence of quote for President Trump Present, Not Present\nPresence of quote opposing President Trump Present, Not Present\nPresence of truth sandwich Present, Not Present\nMention of attacks on press Present, Not Present\nMention of fake news Present, Not Present\nDirect new quote of President Trump tweet Present, Not Present\nDirect old quote of President Trump tweet Present, Not Present\nMention of new quote of President Trump tweet Present, Not Present\nMention of old quote of President Trump tweet Present, Not Present\nMention of President Trump on twitter Present, Not Present\nGeneral view of President Trump on twitter Positive, Negative, Neutral,\nNot Applicable\nChallenge\u00fd/Research of tweet Present, Not Present\nArticles centrally about president Trump Yes, No\nPresident Trump is mentioned in headline or lead paragraph Present, Not Present\nSecond platform for President Trump Twitter, Press Conference,\nSpeech/Address, Interview,\nOther, Not Applicable,\nNot Mentioned\nThird platform for President Trump Twitter, Press Conference,\nSpeech/Address, Interview,\nOther, Not Applicable,\nNot MentionedTable 1. Support for American newspaper political bias\nLiberal\nWashington Post\n\u2022 Lakeland Library: Point of View  (2016)\n\u2022 The Washington Post: Ranking the media from\nliberal to conservative, based on their\naudiences (2014)\n\u2022 AllSides: Washington Post (AllSides, 2018c)\nNew York Times\n\u2022 Freakonomics: How Biased is Your Media?\n(2012)\n\u2022 Wikipedia: The New York Times Political Stance\n(2016)\n\u2022 Media Bias/Fact Check: New York Times (2018b)Conservative\nWall Street Journal\n\u2022 Freakonomics: How Biased is Your\nMedia? (2012)\n\u2022 AllSides: Wall Street Journal (2018b)\n\u2022 Media Bias/Fact Check: Wall Street\nJournal (2018c)\nNew York Post\n\u2022 Rational Wiki: The New York Post (2016)\n\u2022 AllSides: New York Post (2018a)\n\u2022 Media Bias/Fact Check: New York Post\n(2018a)There has been much argumentation about the ideological influence on stalwart political\nnewspapers (e.g. Fox News). However, the interaction of ideology on less politically driven\nnews outlets is less clear. Further research that focused on interviews with journalists and\neditors across a broader range of news outlets could tease out these ambiguities with\ncontent analyses on data that skirts the ideological edges of newspaper content.Kenix & Manickam\n190Media Watch 12 (2)\nChi-Square Tests\nValue df Asymptomatic Exact Exact\nsignificance (2-sided) Significance\n(2-sided) (1-sided)\nPearson Chi-square 160.973* 1 .000\nContinuity correction 152.036 1 .000\nLikelihood ratio 77.605 1 .000\nFisher\u2019s exact test .000 .000\nLinear-by-linear association 160.669 1 .000\nN of valid cases 528Figure 2. Direct New Quote of Trump Tweet * Challenge/Research of Tweet Crosstabulation\nChallenge/Research of Tweet\nNo Yes Total\nDirect New Quite No Count 479 1 2 491\nof Trump Tweet Expected count 461.2 29.8 491.0\nAdjusted residual 12.7 -12.7\nYes Count 1 7 2 0 3 7\nExpected count 34.8 2.2 37.0\nAdjusted residual -12.7 12.7\nTotal Count 496 3 2 528\nExpected count 496.0 32.0 528.0Chi-Square Tests\nValue df Asymptomatic Exact Exact\nSignificance (2-sided) Significance\n(2-sided) (1-sided)\nPearson Chi-Square 25.432* 1 .000\nContinuity Correction 21.869 1 .000\nLikelihood Ratio 15.631 1 .000\nFisher\u2019s Exact Test .000 .000\nLinear-by-Linear Association 25.384 1 .000\nN of Valid Cases 528Figure 1. Direct old quote of Trump tweet * Challenge/Research of tweet crosstabulation\nChallenge/research of tweet\nNo Yes Total\nDirect Old No Count 470 2 3 493\nQuite of Expected Count 463.1 29.9 493.0\nTrump Adjusted Residual 5.0 -5.0\nTweet Yes Count 2 6 9 3 5\nExpected Count 32.9 2.1 35.0\nAdjusted Residual -5.0 5.0\nTotal Count 496 3 2 528\nExpected Count 496.0 32.0 528.0\n191Figure 3. Mention of Trump on Twitter * General view of Trump on Twitter Crosstabulation\nGeneral view of Trump on twitter\nNegative Neutral Not Total\nApplicable\nMention of No Count 0 0 492 492\nTrump on Twitter Expected Count 19.6 13.0 459.4 492.0\nAdjusted Residual -17.3 -14.0 22.6\nYes Count 2 1 1 4 1 3 6\nExpected Count 1.4 1.0 33.6 36.0\nAdjusted Residual 17.3 14.0 -22.6\nTotal Count 2 1 1 4 493 528\nExpected Count 21.0 14.0 493.0 528.0\nChi-square Tests\nValue df Asymptomatic\nsignificance\n(2-sided)\nPearson Chi-Square 512.292* 2 .000\nLikelihood Ratio 248.450 2 .000\nLinear-by-Linear Association 492.592 1 .000\nN of Valid Cases 528\nFigure 4. Platform for Trump * General View of Trump Crosstabulation\nGeneral View of Trump\nPositive Negative Neutral Total\nPlatform Twitter Count 1 2 0 2 0 4 1\nfor Trump Expected Count 2.0 16.9 22.1 41.0\nAdjusted Residual -.8 -1.0 -.7\nPress Conference Count 2 1 3 1 3 2 8\nExpected Count 1.4 11.6 15.1 28.0\nAdjusted Residual .6 .6 -.8\nSpeech/Address Count 4 3 1 1 1 4 6\nExpected Count 2.3 19.0 24.7 46.0\nAdjusted Residual 1.2 3.8 -4.3\nInterview Count 0 1 7 1 0 2 7\nExpected Count 1.3 11.1 14.5 27.0\nAdjusted Residual -1.2 2.3 -1.8\nOther Count 4 1 4 1 2 3 0\nExpected Count 1.5 12.4 16.1 30.0\nAdjusted Residual 2.2 .6 -1.6\nNot Applicable Count 1 1 9 6 188 295\nExpected Count 14.5 121.8 158.7 295.0\nAdjusted Residual -1.4 -4.6 5.2\nNot Mentioned Count 4 2 6 3 0 6 0\nExpected Count 3.0 24.8 32.3 60.0\nAdjusted Residual .7 .3 -.6\nNot Sure Count 0 1 0 1\nExpected Count .0 .4 .5 1.0\nAdjusted Residual -.2 1.2 -1.1\nTotal Count 2 6 218 284 528\nExpected Count 26.0 218.0 284.0 528.0\nChi-Square Tests\nValue df Asymptomatic\nSignificance\n(2-sided)\nPearson Chi-Square 43.699a1 4 .000\nLikelihood Ratio 44.609 1 4 .000\nLinear-by-Linear Association 8.731 1 .003\nN of Valid Cases 528\na. 9 cells (37.5%) have expected count less than 5. The minimum expected count is .05.Kenix & Manickam\n192Media Watch 12 (2)\nChi-Square Tests\nValue df Asymptomatic Exact Exact\nSignificance (2-sided) Significance\n(2-sided) (1-sided)\nPearson Chi-Square 70.627a1 .000\nContinuity Correction 62.822 1 .000\nLikelihood Ratio 32.368 1 .000\nFisher\u2019s Exact Test .000 .000\nLinear-by-Linear Association 70.494 1 .000\nN of Valid Cases 528Figure 5. Mention of attacks on press * Mention of fake news crosstabulation\nMention of fake news\nNo Yes Total\nMention of No Count 487 1 2 499\nAttacks on press Expected Count 478.2 20.8 499.0\nAdjusted Residual 8.4 -8.4\nYes Count 1 9 1 0 2 9\nExpected Count 27.8 1.2 29.0\nAdjusted Residual -8.4 8.4\nTotal Count 506 2 2 528\nExpected Count 506.0 22.0 528.0\nFigure 6. General View of Trump * General View of Trump on Twitter Crosstabulation\n                                       General view of Trump on Twitter\nNegative Neutral Not Total\napplicable\nGeneral view of Trump Positive Count 0 0 2 6 2 6\nExpected count 1.0 .7 24.3 26.0\nAdjusted residual -1.1 -.9 1.4\nNegative Count 1 6 1 0 192 218\nExpected count 8.7 5.8 203.5 218.0\nAdjusted residual 3.3 2.3 -4.1\nNeutral Count 5 4 275 284\nExpected count 11.3 7.5 265.2 284.0\nAdjusted residual -2.8 -1.9 3.4\nTotal Count 2 1 1 4 493 528\nExpected count 21.0 14.0 493.0 528.0\nChi-Square Tests\nValue df Asymptomatic\nsignificance\n(2-sided)\nPearson Chi-Square 17.305a4 .002\nLikelihood ratio 18.504 4 .001\nLinear-by-linear association 5.757 1 .016\nN of Valid cases 528\nConflict of interest : The authors declare no potential conflict of interest with respect to the\nresearch, authorship, and/or publication of this article.\nFunding : This research was funded by the College of Arts, University of Canterbury,\nNew Zealand.\n193References\nAgility PR Solutions. (2017). Top 15 U.S. Newspapers by Circulation. Retrieved from https://\nw w w. a g i l i t y p r. c o m / r es o u r c es / t o p - m e d i a - o u t l e t s / t o p - 1 5 - d a i l y - a m e r i c a n -\nnewspa pers/\nAllen, H., & Savigny, H. (2012). Selling scandal or ideology? The politics of business crime\ncoverage. European Journal of Communication, 27 (3), 278-290.\nAllSides. (2018a). New York Post. Retrieved from https://www.allsides.com/news-source/\nnew-york-post\nAllSides. (2018b). Wall Street Journal - News. Retrieved from https://www.allsides.com/\nnews-source/wall-street-journal-media-bias\nAllSides. (2018c). Washington Post - News. Retrieved from https://www.allsides.com/news-\nsource/wa shington-post-media-bias\nAltheide, D. (1976). Creating reality: How TV distorts events . Beverly Hills: Sage.\nBeam, R. A., Weaver, D., & Brownlee, B. J. (2009). Changes in professionalism of U.S. journalists\nin the turbulent twenty-first century. Journalism & Mass Communication Quarterly, 86 (2),\n277-298. doi: 10.1177 /107769900908600202\nBerkowitz, D. (2000). Doing double duty: Paradigm repair and the Princess Diana What-A-\nStory. Journalism, 1 (2), 125-143. doi: 10.1177 /146488490000100203\nBlake, A. (2014). Ranking the media from liberal to conservative, based on their audiences.\nRetrieved from https://www.washingtonpost.com/news/the-fix/wp/2014/10/21/lets-\nrank-the-media-from-liberal-to-conservative-based-on-their-audiences/\nBraun, V., & Clarke, V. (2006). Using thematic analysis in Psychology. Qualitative Research in\nPsychology, 32 (2), 77-101.\nChalaby, J. K. (1996). Journalism as an Anglo-American invention European Journal of\nCommunication, 11 (3), 303-326.\nCision. (2014). Top 10 US Daily Newspapers. Retrieved from http://www.cision.com/us/2014/\n06/top-10-us-daily-newspapers/\nCNN. (2018). How to make a \u2018truth sandwich\u2019. CNN . Retrieved from https://edition.cnn.com/\nvideos/tv/2018/06/17 /how-to-make-a-truth-sandwich-rs.cnn/video/playlists/reliable-\nsources-highlights/\nConover, P., & Feldman, S. (1981). the origins and meanings of liberal/conservative self-\nidentification. American Journal of Political Science, 25 , 617-645.\nDubner, S. (2012). How Biased is Your Media? Retrieved from http://freakonomics.com/\npodcast/how-biased-is-your-media/\nEdwards, B. T. (2018). President Trump from reality TV to Twitter, or the selfie-determination\nof nations. Arizona Quarterly: A Journal of American Literature, Culture, and Theory, 74 (3), 25-45.\nEveland, W. P., & Shah, D. (2003). The impact of individual and interpersonal factors on\nperceived news media bias. Political Psychology, 24 (1), 101-117.\nGamson, W. A. (1992). Talking politics . New York: Cambridge University Press.\nGitlin, T. (1980). The whole world is watching: Mass media in the making and unmaking of the new\nleft.  Berkley: University of California Press.\nHeider, D., McCombs, M., & Pointdexter, P. M. (2005). What the public expects of local news:\nViews on public and traditional journalism. Journalism & Mass Communication Quarterly,\n82(4), 952-967. doi: 10.1177 /107769900508200412\nHertog, J., & McLeod, D. (1995). Anarchists wreak havoc in downtown Minneapolis: A multi-\nlevel study of media coverage of radical protest. Journalism Monographs, 151 (June), 1-48.Kenix & Manickam\n194Media Watch 12 (2)\nHester, J B. and Dougall, E. (2007). The efficiency of constructed week sampling for\nconstructed week sampling for content analysis of online news.  Journalism & Mass\nCommunication Quarterly, 84 (4), 811-824.\nIlling, S (2018). How the media should respond to Trump\u2019s lies: State of the Union edition.\nRetrieved from https://www.vox.com/2018/11/15/18047360/trump-state-of-the-union-\nspeech-2019-george-lakoff\nIPFS. (2018). List of newspapers in Australia by circulation. Retrieved from https://ipfs.io/\ni p f s / Q m X o y p i z j W 3 W k n F i J n K L w H C n L 7 2 v e d x j Q k D D P 1 m X Wo 6 u c o / w i k i /\nList_of_newspapers_in_Australia_by_circulation.html\nJones, J., & Himelboim, I. (2010). Just a guy in pajamas? Framing the blogs in mainstream\nUS newspaper coverage (1999-2005). new media & society, 12 (2), 271-288. doi: 10.1177 /\n1461444809342524\nKenix, L. J., & Jarvandi, R. (2019). The Role of Ideology in the International Mainstream News\nMedia Framing of Refugees: A Comparison between Conservative and Liberal\nNewspapers in the United States, United Kingdom, and Australia. . Journal of Applied\nJournalism and Media Studies, 8.3 (November), In Press.\nKilby, A. (2018). Provoking the Citizen: Re-examining the role of TV satire in the President\nTrump era. Journalism Studies, 19 (14), 1934-1944.\nKohut, A. (2004, 11 July 2004). Media myopia: More news is not necessarily good news,  New\nYork Times .\nLakeland Libary Research Guides. (2016). Point of View. Retrieved from http://\nlibrary.lakelandcc.edu/PDFs/research/bias.pdf\nLischka, J. (2019). A badge of honour? Journalism Studies, 20 (2), 287-304.\nMcDevitt, M., & Ferrucci, P. (2018). Populism, journalism, and the limits of reflexivity: the\ncase of Donald J. President Trump. Journalism Studies, 19 (4), 512-526.\nMcVittie , C., & McKinlay, A. (2018). \u2019 Alternative facts are not facts\u2019: gaffe announcements,\nthe President Trump administration and the media. Discourse & Society , 1-16.\nMedia Bias/Fact Check. (2018a). New York Post. Retrieved from https://\nmediabiasfactcheck.com/new-york-post/\nMedia Bias/Fact Check. (2018b). New York Times. Retrieved from https://\nmediabiasfactcheck.com/new-york-times/\nMedia Bias/Fact Check. (2018c). Wall Street Journal. Retrieved from https://\nmediabiasfactcheck.com/wall-street-journal/\nMemmott, M. (2015). When There\u2019s No Evidence To Support A Claim, We Should Say That.\nRetrieved from https://www.npr.org/sections/memmos/2015/11/25/605696776/when-\nthere-s-no-evidence-to-support-a-claim-we-should-say-that\nMemmott, M. (2018). Let\u2019s put \u201ctruth sandwiches\u2019 on our menu. Retrieved from https://\nwww.npr.org/sections/memmos/2018/06/20/621753252/lets-put-truth-sandwiches-on-\nour-menu\nMessina, S. R. (2018). Airing live risks error: responsible journalism in the President Trump\nera. Journal of Media Ethics, 33 (2), 94-94.\nMour\u00e3o, R. R., Thorson, E., Chen, W., & Tham, S. M. (2018). Media repertoires and news trust\nduring the early President Trump administration. Journalism Studies, 19 (13), 1945-\n1956.\nParks, P.  (2020) The Ultimate News Value: Journalism Textbooks, the U.S. Presidency, and\nthe Normalization of Donald Trump, Journalism Studies, 21:4, 512-529, DOI: 10.1080/\n1461670X.2019.1686413\n195Peiser, J. (2018). New York Times Co. reports revenue growth as digital subscriptions rise.\nNew York Times . Retrieved from https://www.nytimes.com/2018/ 05/ 03/business/media/\nnew-york-times-earnings.html\nPew Research Center, I. (2018). Newspapers fact sheet. Retrieved from https://\nwww.journalism.org/fact-sheet/newspapers/\nPingree, R., Watson, B., Sui, M., Searles, K., Kalmoe, N. (2018). Checking facts and fighting\nback: Why journalists should defend their profession. PLoS One, 13(12), e0208600.\nhttps://doi.org/10.1371/journal.pone.0208600\nRational Wiki. (2016). The New York Post. Retrieved from http://rationalwiki.org/wiki/\nNew_York_Post\nReese, S. D., Gandy Jr., O. H., & Grant, A. E. (2001). Framing Public Life . Mahwah: Lawrence\nErlbaum.\nRosen, J. (2018). It\u2019s time for the press to suspend normal relations with the President\nTrump presidency. PressThink . Retrieved from http://pressthink.org/2018/06/its-time-\nfor-the-press-to-suspend-normal-relations-with-the-President Trump-presidency/\nRubin, J. (2020). The media should remember key lessons from the Trump era. Washington\nPost. Retrieved from:  https://global-factiva-com.ezproxy.canterbury.ac.nz/ga/\ndefault.a spx\nRyan, C., Carragee, K. M., & Schwerner, C. (1998). Media, movements, and the quest for social\njustice. Journal of Applied Communication Research, 26 , 165-181.\nSartwell, C. (2018)  \u2018Truth Sandwich\u2019? Baloney! Wall Street Journal . Retrieved from https://\nwww.wsj.com/articles/truth-sandwich-baloney-1533496472\nSchiffer, A. (2000). I\u2019m not THAT liberal: Explaining conservative democratic identification.\nPolitical Behavior, 22 (4), 293-310.\nSchudson, M. (2001). The objectivity norm in American journalism. Journalism, 2 (2), 149-170.\nSchudson, M. (2019a). Where we are and whither we are tending. Journalism, 20 (1), 77-79.\ndoi: 10.1177 /1464884918809247\nSchudson, M. (2019b). Where we are and wither we are tending. Journalism, 20 (1), 77-79.\nSotirovic, M. (2000). Effects of media use on audience framing and support for welfare. Mass\nCommunication and Society, 3 , 269-297.\nStatista. (2018). Circulation of newspapers in the United Kingdom (UK) in 2017 (in 1,000\ncopies). Retrieved from https://www.statista.com/statistics/529060/uk-newspaper-\nmarket-by-circulation/\nStreet, J. (Ed.). (2001). Mass media, politics and democracy . Basingstoke: Palgrave.\nSullivan, M. (2018). Instead of President Trump\u2019s propaganda, how about a nice \u2018truth\nsandwich. Washington Post . Retrieved from https://www.washingtonpost.com/\nlifestyle/style/instead-of-President Trumps-propaganda-how-about-a-nice-truth-\ns a n d w i c h / 2 0 1 8 / 0 6 / 1 5 / 8 0 d f 8 c 3 6 - 7 0 a f - 1 1 e 8 - b f 8 6 -\na2351b5ece99_story.html?noredirect=on&utm_term=.cc75e7cc50d0\nTankard Jr., J. W., Hendrickson, L., Silberman, J., Bliss, K., & Ghanem, S. (1991). Media Frames:\nApproaches to Conceptualization and Measurement.  Paper presented at the Association\nfor Education in Journalism and Mass Communication, Boston.\nThompson, J. B. (1990). Ideology and modern culture: Critical social theory in the era of mass\ncommunication . Cambridge: Polity Press.\nTuchman, G. (1978). Making news: A study in the construction of reality . New York: Free Press.\nWahl-Jorgensen, K. (2018). Media coverage of shifting emotional regimes: Donald President\nTrump\u2019s angry populism. Media, Culture & Society, 40 (5), 766-778.\nWikipedia. (2016). The New York Times Political Stance. Retrieved from https://\nen.wikipedia.org/wiki/The_New_York_Times#Political_stanceKenix & Manickam\n196Media Watch 12 (2)\nXu, E. (2020) A generalisable model for frame identification: towards an integrative\napproach,\u00a0 Communication Research and Practice, \u00a06:3,\u00a0245-258,\u00a0DOI:\u00a010.1080/\n22041451.2020.1759925\nZelizer, B. (1993). Has communication explained journalism? Journal of Communication, 43 (4),\n80-88. doi: 10.1111/j.1460-2466.1993.tb01307.x\nZelizer, B. (2018). Resetting journalism in the aftermath of Brexit and President Trump.\nEuropean Journal of Communication, 33 (2), 140-156.\nLinda Jean Kenix  is Head of the School of Language, Social, and Political Sciences at\nUniversity of Canterbury, New Zealand . Prof. Kenix is interested in the visual and textual\nmedia representation of marginalized groups, the reasons for, and the consequences of\nthat representation. She has been a visiting research fellow at the Oxford University,\nUniversity of Cambridge, Monash University and the University of Valencia.\nJovita Manickam  is a Masters graduate from the University of Auckland in the Media, Film\nand Television Studies Programme. She is presently a Volunteer and Event Coordinator for\nBreast Cancer Foundation New Zealand.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "A missed opportunity? President Trump, the truth sandwich, and news coverage across an ideological spectrum", "author": ["LJ Kenix", "J Manickam"], "pub_year": "2021", "venue": "Media Watch", "abstract": "Many within mainstream news media wondered aloud and in print how to cover President  Trump who, they purported, frequently distorted the truth. Although President Trump is no"}, "filled": false, "gsrank": 471, "pub_url": "https://journals.sagepub.com/doi/abs/10.15655/mw_2021_v12i2_160145", "author_id": ["bZbE-mUAAAAJ", ""], "url_scholarbib": "/scholar?hl=en&q=info:9i4xhhw7f6IJ:scholar.google.com/&output=cite&scirp=470&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D470%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=9i4xhhw7f6IJ&ei=W7WsaJnZIo6IieoP0sKRuAk&json=", "num_citations": 6, "citedby_url": "/scholar?cites=11709142549883072246&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:9i4xhhw7f6IJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.mediawatchjournal.in/wp-content/uploads/2021/05/1.-A-Missed-Opportunity.pdf"}}, {"title": "Self-supervised claim identification for automated fact checking", "year": "2021", "pdf_data": "Self-Supervised Claim Identi\ufb01cation for Automated Fact Checking\nArchita Pathak\nUniversity at Buffalo (SUNY)\nBuffalo, NY\narchitap@buffalo.eduMohammad Abuzar Shaikh\nUniversity at Buffalo (SUNY)\nBuffalo, NY\nmshaikh2@buffalo.eduRohini K. Srihari\nUniversity at Buffalo (SUNY)\nBuffalo, NY\nrohini@buffalo.edu\nAbstract\nWe propose a novel, attention-based self-\nsupervised approach to identify \u201cclaim-\nworthy\u201d sentences in a fake news article,\nan important \ufb01rst step in automated fact-\nchecking. We leverage aboutness of headline\nand content using attention mechanism for\nthis task. The identi\ufb01ed claims can be used\nfor downstream task of claim veri\ufb01cation for\nwhich we are releasing a benchmark dataset\nof manually selected compelling articles\nwith veracity labels and associated evidence.\nThis work goes beyond stylistic analysis to\nidentifying content that in\ufb02uences reader\nbelief. Experiments with three datasets show\nthe strength of our model1.\n1 Introduction\nThe explosion of fake news on social media has\nresulted in global unrest and has been a major con-\ncern for governments and societies worldwide2.\nAccording to a recent Pew Research Study, Amer-\nicans rate it as a larger problem than racism, cli-\nmate change, or illegal immigration3. Since, it\u2019s\ninexpensive to create a website and easily dissem-\ninate content on the social media platforms, there\nis a rising need for automated fake news detec-\ntion. Furthermore, AI solutions are also required\nto follow good practices, speci\ufb01cally avoiding cen-\nsorship, violation of fundamental rights such as\nfreedom of expression, and ensuring data privacy\n(de Cock Buning, 2018). However, to date, AI\nmodels proposed for fake news detection do not\n1Data and code available at:\nhttps://github.com/architapathak/Self-Supervised-\nClaimIdenti\ufb01cation\n2https://www.reuters.com/article/us-singapore-politics-\nfakenews-factbox/factbox-fake-news-laws-around-the-\nworld-idUSKCN1RE0XN\n3https://www.journalism.org/2019/06/05/many-\namericans-say-made-up-news-is-a-critical-problem-that-\nneeds-to-be-\ufb01xed/scale for detecting real-time fake news4.\nMuch of the research on automated text-based\nfake news detection can be classi\ufb01ed into three\nbroad categories: (1) linguistic approach, which\nfocuses on lexical, stylometric and pattern learning\nmechanisms (Potthast et al., 2017; Rashkin et al.,\n2017; Wang, 2017; Singhania et al., 2017; P \u00b4erez-\nRosas et al., 2018); (2) network-based approach,\nwhich leverages features such as the speed and vol-\nume of propagation of fake news articles on social\nmedia platforms (Castillo et al., 2011; Yang et al.,\n2012; Kwon et al., 2013; Ma et al., 2015; Jin et al.,\n2016; Ruchansky et al., 2017; Wu and Liu, 2018);\nand (3) automated fact-checking approach, which\nis an effort to assist manual fact-checkers by au-\ntomating some of their tasks such as detection and\nveri\ufb01cation of claims (Graves, 2018).\nWhile most work in automated fact-checking\nhas been focused on claim veri\ufb01cation task, very\nfew methods have been proposed for detection of\nclaims (Hassan et al., 2017; Jaradat et al., 2018;\nKonstantinovskiy et al., 2018). The approaches\nin these efforts are majorly related to political dis-\ncourse . However, our focus is on fake news , which\nare broader than political discourse since (i) they\nare deliberately written with a divisive agenda to\ncause social unrest, (ii) they are not constrained to\nonly politics, and (iii) the headline plays an equally\nimportant role in compelling people to read the\narticle.\nIn this paper, we focus on articles where there is\na deliberate intent to in\ufb02uence readers through fab-\nricated or manipulated claims in the headline and\nthe content. Such articles have a compelling writ-\ning style similar to the mainstream media. Hence,\nwe build datasets containing these type of com-\npelling articles along with veracity labels and as-\nsociated evidence supporting the label of each arti-\n4https://www.technologyreview.com/s/612236/even-the-\nbest-ai-for-spotting-fake-news-is-still-terrible/arXiv:2102.02335v1  [cs.CL]  3 Feb 2021\ncle. We, then, use these datasets to identify \u201cclaim-\nworthy\u201d sentences. In our work, we de\ufb01ne \u201cclaim\u201d\nas\u201cstatements which are important to the point\nof the article but one would require to have them\nveri\ufb01ed. \u201d\nOur working hypothesis is that in fake news\nwhich are created to cause harm, these are the sen-\ntences most relevant to the headline. Exploiting\nthe hypothesis that the essence of a news article is\nencapsulated in its headline (Jaime Sis \u00b4o and MER-\nCEDES, 2009; Kuiken et al., 2017; Wahl-Jorgensen\nand Hanitzsch, 2009), we propose a self-supervised\nmethod to explore the aboutness of the content with\nthe headline of the article to extract the most rele-\nvant sentences. Bruza and Huibers (1996) de\ufb01nes\naboutness as:an information carrier i will be said\nto be about information carrier j if the informa-\ntion borne by j holds in i . The idea is taken from\nInformation Retrieval domain where it is used to\nsignify implications between query and document,\nspeci\ufb01cally to explore the underlying meaning or\nconcept within the document and the query (Az-\nzopardi et al., 2009). In our work, headline is mod-\nelled as a query while each of the sentences of the\narticle acts as a document, and we use the concept\nofaboutness to \ufb01nd the relevant sentences. We\nshow that attention-based mechanisms are able to\nsuccessfully capture this concept in the news arti-\ncle.\nContribution: In this work: (i) we introduce\na self-supervised representation learning model\nthat eliminates the prerequisite that requires human\nto annotate data, which is a time consuming and\ncostly task; (ii) the proposed headline-to-sentence\nattention-based approach for claim identi\ufb01cation is\nnovel; previous unsupervised approach for this task\nuse weak supervisory signal which does not capture\nthe context of the article ef\ufb01ciently; and (iii) we\npropose a benchmark dataset for evidence-based\nfake news detection. Our dataset contains evidence\nfor each of the fake news articles that contributes\nto the overall degree of veracity of the article.\n2 Related Work\nClaim Identi\ufb01cation/Detection: The task of\nclaim identi\ufb01cation/detection was \ufb01rst introduced\nby Levy et al. (2014) who, with the help of human\nannotators, provided a dataset and a fundamental\napproach in identifying context dependent claims.\nIn their dataset, which was originally developed\nby Aharoni et al. (2014), each statement indicateswhether it should be considered as a context de-\npendent claim (CDC) or not. Levy et al. (2014)\nreported encouraging results obtained through a\nsupervised learning algorithm using a cascade of\nclassi\ufb01ers. A rule-based model was introduced\nby Eckle-Kohler et al. (2015) to bifurcate claim\nand premise statements in an argumentative dis-\ncourse environment. However, these methods were\ngeneric to only a small set of corpora. Furthermore,\nLevy et al. (2017) also introduced an unsupervised\napproach to detect claims, which involves a weak\nsupervisory signal \u201cthat\u201d for training. However,\nthis approach does not capture the aboutness of the\narticle to understand the context of \u201cclaim-worthy\u201d\nsentences.\nIn 2017, Hassan et al. (2017) introduced Claim-\nBuster, a platform developed by training a super-\nvised learning model on a large annotated cor-\npus of televised debates in the USA. Their model\nused SVM classi\ufb01er to detect claim-worthy factual\nclaims and produced a score of how important a\nclaim is to factcheck. The 20,000 sentences in the\ncorpus were annotated by human coders to distin-\nguish between claim-worthy factual claims from\nopinions and boring statements. However, anno-\ntating a sentence as an important or unimportant\nclaim is a non-trivial task as this decision changes\ndepending on who\u2019s asking, political context and\nannotator\u2019s background (Graves, 2018).\nThe model proposed by Hassan et al. (2017)\nonly learns the labelled instances and does not\nexplore the contextual information of the written\ntext. A context-aware approach in the political dis-\ncourse environment was introduced by Gencheva\net al. (2017) who created a rich representation of\nthe sentences from 2016 US presidential debates.\nTheir dataset was compiled by taking the outputs\nof factchecking of the debates from 9 factchecking\norganizations. Their models were created to pre-\ndict if the claim would be highlighted by at least\none or by a speci\ufb01c organization. However, the\nauthors don\u2019t have any formal de\ufb01nition of claim\nin their paper, and their model is speci\ufb01c to certain\norganizations which led to several false positives.\nAnother context-aware approach for claim de-\ntection was proposed by Konstantinovskiy et al.\n(2018) who used sentence embeddings, pre-trained\non a large dataset of NLI. This work also created a\ncrowd-sourced annotated dataset of sentences from\nUK political TV shows, annotated across 7 classes.\nHowever, their classi\ufb01ers for the \ufb01ne-grained clas-\nsi\ufb01cation to detect 7 classes of sentences did not\nyield good results due to lack of enough annotated\ndata, thus requiring more annotations which is a\ncostly and time consuming task.\nWe build a model that can be trained in a self-\nsupervised setting to overcome the challenges as-\nsociated with annotated dataset of claims. We also\nuse attention-based approach to capture aboutness\nand rich contextual information between headline\nand all the sentences of the article. The perfor-\nmance on manually created test sets demonstrate\npromising results in identifying \u201cclaim-worthy\u201d\nsentences even when no sentence-level annotation\nwas used for training.\nFake News Dataset: A variety of fake news\ndatasets have been released in the recent years,\nmost notably Buzzfeed5and Stanford (Allcott and\nGentzkow, 2017) datasets containing list of popu-\nlar fake news articles from 2016 US presidential\nelections. However, these datasets only contain\nwebpage URLs of the original article and major-\nity of them don\u2019t exist anymore. Following this,\nseveral other datasets were published such as Fake\nnews challenge dataset6which was used for the\ntask of stance detection; Getting Real about Fake\nNews Kaggle dataset7which was created by using\nBS detector tool; and FakeNewsCorpus8which is\nan open-source large scale collection of fake news\narticles. However, these articles are labelled as fake\nbased on the domain of the websites they come\nfrom. Since, the content of these articles are not\nveri\ufb01ed for degree of veracity, using them directly\nfor training may lead to several false positives.\nThis problem was overcome by recently released\nlarge dataset, NELA-GT-2018 (Norregaard et al.,\n2019), which contains articles with ground truth\nratings retrieved from 8 different assessment sites.\nHowever, the label de\ufb01nitions are not generic and\ndependent on the external organizations. Pathak\nand Srihari (2019) also introduced intuitive ground\ntruth labels based on the degree of veracity of\nthe fake news articles, however, the dataset is not\npublicly available. Additionally, they also do not\nspecify the relationship of their labels with the la-\nbels used by established fact-checking organiza-\ntions. Furthermore, due to lack of evidence in these\ndatasets, they cannot be used for downstream task\n5https://www.buzzfeednews.com/article/craigsilverman/these-\nare-50-of-the-biggest-fake-news-hits-on-facebook-in\n6http://www.fakenewschallenge.org/\n7https://www.kaggle.com/mrisdal/fake-news\n8https://github.com/several27/FakeNewsCorpusof evidence-based veri\ufb01cation, which is one of the\nmotivations of this paper. We overcome all these\nlimitations in our datasets described in the follow-\ning section.\n3 Datasets\nWe introduce two datasets of compelling fake news\narticles which have writing style similar to main-\nstream media. The \ufb01rst dataset, DNF-700, where\nDNF stands for DisiNFormation , contains articles\non politics published within 4 months of 2016 US\nPresidential Elections from questionable sources\n(non-mainstream). To compile this dataset, we\n\ufb01rst extracted fake news articles from working\nweb page URLs of Stanford dataset (Allcott and\nGentzkow, 2017). However, majority of webpage\nURLs in this dataset are expired and we could ex-\ntract only 26 fake news articles. Therefore, we\nthen used \u201c Getting real about fake news \u201d Kaggle8\ndataset to sample more articles on politics. Since,\nmost of the articles in this dataset contain anoma-\nlies (eg: incomplete article, social media comments\nlabelled as fake etc.), we manually veri\ufb01ed the writ-\ning style and discarded obvious fakes - articles with\npoor grammar and excessive usage of punctuations.\nHowever, the degree of veracity of each article in\nthis dataset is not checked and some articles may\ncontain personal opinions.\nThe second dataset, DNF-300, is more so-\nphisticated subset of DNF-700, containing 290\ncompelling articles on Politics and 10 on\nHealth/Medical news. Unlike other fake news\ndatasets in which veracity and evidence for articles\nare not provided, DNF-300 contains articles associ-\nated with veracity labels as well as corresponding\nevidence. The process of annotating this dataset\ninvolves identifying sentences from each article\nbased on their persuasive tone and relevance with\nthe headline. These sentences were then queried\non the web and top 10 results were considered to\ngather evidence from credible sources9. Based on\nthe evidence found, we label the entire article into\nfour categories:f(0)false ; (1) partial truth ; (2)\nopinions stated as fact ; (3) trueg. These labels\nare inspired by (Pathak and Srihari, 2019); Table-\n1 shows the description and distribution of these\nlabels while the comparison with two popular fact-\nchecking websites is displayed in Figure-1. An\n9Credible sources were extracted from\nhttps://mediabiasfactcheck.com/ The sources range be-\ntween left, center and right biased news sources\nLabel False Partial True Opinion\nStated As\nFactTrue\nDesc. (i) No evi-\ndence could\nbe found, or\n(ii) found\nevidence\nto refute\nthe entire\narticleArticle\nabout true\nevent, how-\never, found\nevidence re-\nfuting some\nof the claimsArticle contain\nfalse/manipulated\nclaims, how-\never, it\u2019s an\nopinion article\nwhich cannot\nbe labelled as\nfakeFound\nevidence\nsupport-\ning the\nentire\narticle\nTotal 126 75 79 20\nTable 1: DNF-300 label description and distribution.\nClaims here are the sentences manually selected based\non their persuasive tone and relevance with the head-\nline. Interestingly, some of the articles, which were\nlabelled as fake in other datasets due to the domain of\npublishing website, turned out to be true news.\nexample from the dataset is shown in Table-2\nPartial T ruth FalseOpinion Stated as\nFactMostly False Mixture Unproven False\nHalf T rue Mostly False False Pants on FireSnopes\nDNF-300\nPolitiFact\nFigure 1: Label Comparison with Snopes and Politi-\nFact ratings.\nThis dataset is also a key contribution of this\npaper as the articles are manually read and veri-\n\ufb01ed. Additionally, the dataset contains two novel\nfeatures which are essential for the fake news ver-\ni\ufb01cation task: (i) generic veracity-based label set,\nindependent of any external organization, and (ii)\nground truth evidence corresponding to each label.\nIn addition to these two datasets, we also train\nour model for claim identi\ufb01cation on the dataset\nintroduced for context dependent claim detection\n(CDCD) by Levy et al. (2014). Although this\ndataset (CDC) does not contain fake news articles,\nit has manually annotated sentences based on their\nrelevance to a certain topic. These annotations were\nutilized for the evaluation of our self-supervised\nlearning model described in the following section.\nMore details on the datasets and examples can be\nfound in Appendix.\n4 Architecture\nProblem De\ufb01nition: Given an article with a set\nof sentences S=fS1;S2;:::S i;:::S ngand a head-\nlineH, the task of our multihead attention claimidenti\ufb01cation network (MA-CIN) is to extract the\nsentence most relevant to the headline. Our self-\nsupervised model exploits the rich contextual infor-\nmation to extract the relevant sentences which are\nconsidered as \u201cclaim-worthy\u201d.\nApproach: For this task, we implement two types\nof attention: (i) self-attention on all sentence vec-\ntors so that each sentence Siis aware of all other\nsentences in S; (ii) cross-attention of headline vec-\ntor on each sentence vector, so that all self-attended\nsentences are also aware of the headline\u2019s context.\nWe then generate headline based on the context-\naware sentences, and compare it with the original\nheadline in three different settings as listed below:\n1.Headline Vector (MA-CIN (HV)): In this\nsetting, the original headline vector acts as the\nsupervisory signal for self-supervised learn-\ning. We minimize the mean squared error\n(MSE) between the generated and the original\nheadline vectors for training.\n2.Headline One-Hot Word Vector (MA-CIN\n(OHWV)): In this setting, the words in the\noriginal headline act as the supervisory sig-\nnal. We use LSTM (Hochreiter and Schmid-\nhuber, 1997) to predict at most 50 words, from\na vocabulary of 20,000 words, to generate\na one-hot-vector for each word of the new\nheadline. We then minimize the categorical\ncross-entropy error (CCE) at each time step\ncorresponding to each word in original and\nnew headlines for training.\n3.Combined HV & OHWV (MA-CIN (Com-\nbined)): In this setting, both original headline\nvector and the words act as supervisory signal.\nTherefore, we combine the two loss functions\nmentioned above to train the model.\nFor this, we build several layers in our architecture\n(see Figure-2), which are delineated as follows:\n4.1 Sentence Embeddings\nEach sentence Si2S, and headline Hare con-\nverted to a \ufb01xed length 300-dimensional vector, si\nandh, such thatsi;h2R1\u0002D, whereD= 300 .\nFor uniformity, we calculate the maximum num-\nber of sentences Lthat an article can contain in\nthe respective corpus. Next, we zero pad the dif-\nference in the quantity of sentence vectors in each\narticle such that every article can be represented as\na vectorA2RL\u0002D.\nHeadline : Allergens in Vaccines Are Causing Life-Threatening Food Allergies\nIt would probably surprise few people to hear that food allergies are increasingly common in U.S.\nchildren and around the world . According to one public health website , food allergies in children\naged 0-17 in the U.S. increased by 50% from 1997 to 2011. Although food allergies are now so\nwidespread as to have become almost normalized, it is important to realize that millions of American\nchildren and adults suffer from severe rapid-onset allergic reactions that can be life-threatening.\nFoods represent the most common cause of anaphylaxis among children and adolescents. The\nUnited Kingdom has witnessed a 700% increase in hospital admissions for anaphylaxis and a\n500% increase in admissions for food allergy since 1990. The question that few are asking is why\nlife-threatening food allergies have become so alarmingly pervasive. A 2015 open access case\nreport by Vinu Arumugham in the Journal of Developing Drugs , entitled \u201c Evidence that Food\nProteins in Vaccines Cause the Development of Food Allergies and Its Implications for Vaccine\nPolicy ,\u201d persuasively argues that allergens in vaccines\u2014and speci\ufb01cally food proteins\u2014may be\nthe elephant in the room. As Arumugham points out, scientists have known for over 100 years that\ninjecting proteins into humans or animals causes immune system sensitization to those proteins.\nAnd, since the 1940s, researchers have con\ufb01rmed that food proteins in vaccines can induce allergy\nin vaccine recipients. Arumugham is not the \ufb01rst to bring the vaccine-allergy link to the public\u2019s\nattention. Heather Fraser makes a powerful case for the role of vaccines in precipitating peanut\nallergies in her 2011 book, The Peanut Allergy Epidemic: What\u2019s Causing It and How to Stop It.\nType : 1 (Partial Truth )\nAuthors :Admin - Orissa\nURLs : galacticconnection.com\nEvidence : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3890451/\nReason : The key claim is written in such a way so that it misleads people in thinking all the food\nrelated allergies in US are caused by vaccines. Found evidence which says these type of allergies\nare rare.\nTable 2: An example on Partial Truth type from DNF-300 dataset.\n4.2 1D Convolution\nTo effectively capture local relevance, we leverage\n1D-CNN (LeCun et al., 1998) to extract the features\nfrom the article vector A. For our experiments the\nkernel size for each convolution layer is K\u0002D\u0002\nC, whereKis kernel-width and Cis the number\nof \ufb01lters. This means the network will process\nK sentences at a time. The size of K and C is a\nhyper-parameter and as per our experiments, we\nsetK= 4with an assumption that not more than\n4 consecutive sentences will be relevant to each\nother.\n4.3 Self Attention\nInspired by the attention implementation in (Zhang\net al., 2018; Vaswani et al., 2017), to capture global\nrelevance, the article features from the previous\n1D-CNN layer are transformed into feature spaces\nq,kto calculate attention, where q(x) =Wqxandk(x) =Wkx.\n\fj;i=exp(zij)PN\ni=1exp(zij);wherezij=q(xi)Tk(xj)\n(1)\n\f2RN\u0002Nis the attention coef\ufb01cient, which is the\nnormalized relevance score between the sentence\nxiandxj.\fis then matrix multiplied by v, where\nv(x) =Wvx, to obtain the context rich output\noj2RC\u00021.\noj=NX\ni=i\fj;iv(xi);whereoj2fo1;o2;:::;o Ng\n(2)\nFinally, the output of the self-attention layer is o2\nRC\u0002N, which is computed as\noj=g(oj);where,g(x) =Wgx (3)\nIn the above equations, x2RC\u0002Nis obtained\nafter applying 1D convolution on sentence vectors,\nWq2RC\u0002C,Wk2RC\u0002C,Wv2RC\u0002C,Wg2\n1D CNNs\nHeadline\nVector\nDenseCA1\nCAMCA2\nConcatSA1\nSAMSA2\nConcat\nDimension\nReductionDecoder\nGenerated Headline \nVectorSentence\nVectorsMultihead\nSelf Attention\nMultihead\nCross AttentionGenerated Headline\n0ne-Hot W ord V ectorFigure 2: Architecture of Multihead Attention - Claim Identi\ufb01cation Network (MA-CIN). The model is trained\nby using self-supervised learning approach using three variants of supervisory-signal - headline vector, headline\nwords and the combination of both vector and words.\nRC\u0002Cand outputo2RC\u0002N. Following the work\nby (Zhang et al., 2018) we preferred the value of\nC=C\n8for computation effectiveness. We also\nmultiply a\u0015and\r, learnable scale parameters,\nto the output of our attention module and input\nvector respectively to allow the network to choose\nbetween local and global sentences effectively.\no=\rx+\u0015o (4)\n\ris initialized to 1 and \u0015is initialized to 0, so as\nto allow the local context to be captured effectively\nduring the early iterations and as the value of \u0015\nincreases it allows the network to add more context\nto the representation.\n4.4 Multihead Concatenation\nIn the architecture, we could apply self attention\nto the input xM times resulting into M attention\nheads. The output of one attention head is denoted\nbyo. We concatenate the outputs oMto get a richer\nrepresentation allowing the network to capture var-\nious relationships.\nmsao=\r\rM\ni=1oi=o1k:::koM(5)\nwhere,msao2RMC\u0002Nis the long range context\naware output of multihead self attention. Here,\r\r\ndenotes concatenation across axis C.\n4.5 Cross Attention\nThe headline vector is transformed into a feature\nspaceh=Whh, whereh2RC\u00021and then, it\u2019s\nrelevance is calculated with msao, obtained from\nthe previous layer, by using equations de\ufb01ned in4.3. Finally, after applying multihead concatena-\ntion using 5, we obtain headline-context aware rep-\nresentation,mcao2RMC\u0002N. We \ufb01xM= 4for\nall our experiments.\n4.6 Loss Function\nTo generate the headline vector dhas close to the\ninput headline vector h, we apply Mean Squared\nError between dhandhand calculate the headline\nvector generation loss Lv\nLv=1\nn(nX\ni=1(dhi\u0000hi)2) (6)\nFor estimating the probability of a word from\nthe vocab in the predicted headline we calculate\nthe cross-entropy between the predicted headline\nwordsdhwand input headline one-hot vector HW .\nLw=\u0000X\nidhw ilog(HW i) (7)\nThe total loss Ltotal=Lv+Lwis then evaluated\nfor all samples b2B, whereBis one batch.\n5 Experiments and Evaluation\n5.1 Training Setup\nWe train our Multihead Attention model for Claim\nIdenti\ufb01cation, MA-CIN, on datasets mentioned in\nSection 3. The CDC dataset contains total of 522\narticles. Amongst these, there are 47 articles with\n8 or more annotated claim sentences which are\nconsidered as evaluation set (CDC Eval) for this\ndataset. Next, for DNF-300 and DNF-700, we\nasked two annotators to manually tag at least 5\nsentences as \u201cclaim-worthy\u201d in each of the 50 ar-\nticles. Sentences which were consented by both\nDataset Con\ufb01gurationCDC Eval DNF Eval\nPrec. Rec. F1 Prec. Rec. F1\nSpacy Baseline 0.09 0.14 0.11 0.33 0.42 0.37\nCDC Baseline (Levy et al., 2014) 0.23 - - - - -\nMA-CIN(HV) 0.18 0.08 0.11 0.39 0.53 0.45\nMA-CIN(OHWV) 0.25 0.10 0.15 0.40 0.54 0.46\nMA-CIN(Combined) 0.26 0.11 0.16 0.42 0.57 0.48\nDNF\u0000700 MA-CIN(HV) 0.20 0.09 0.12 0.37 0.54 0.44\nMA-CIN(OHWV) 0.19 0.08 0.11 0.40 0.5 0.44\nMA-CIN(Combined) 0.28 0.12 0.16 0.41 0.55 0.47\nDNF\u0000300 MA-CIN(HV) 0.19 0.08 0.11 0.39 0.53 0.48\nMA-CIN(OHWV) 0.25 0.11 0.15 0.38 0.53 0.45\nMA-CIN(Combined) 0.24 0.10 0.14 0.42 0.57 0.48\nTable 3: Comparison of MA-CIN model con\ufb01gurations over three datasets and two evaluation sets for identi\ufb01ca-\ntion of \u201cclaim-worthy\u201d sentences.\nHeadline: Clinton Received Debate Questions Week Before Debate\n0The first presidential debate was held and Hillary Clinton was proclaimed the winner by the media. - - 0.41\n1Indeed Clinton was able to turn in a strong debate performance, but did she do so fairly? - - 0\n2Multiple reports and leaked information from inside the Clinton camp claim that the Clinton campaign was given the entire set of debate questions an entire \nweek before the actual debate.GT PD 1\n3Earlier last week an NBC intern was seen hand delivering a package to Clinton\u2019s campaign headquarters, according to sources. - PD 0.73\n4The package was not given to secretarial staff, as would normally happen, but the intern was instead ushered into the personal office of Clinton campaign \nmanager Robert Mook- PD 0.68\n5Members of the Clinton press corps from several media organizations were in attendance at the time, and a reporter from Fox News recognized the intern, \nbut said he was initially confused because the NBC intern was dressed like a Fed Ex employee.- - 0.46\n6The reporter from Fox questioned campaign staff about the intern, but campaign staff at first claimed ignorance and then claimed that it was just a Fed Ex \nemployee who had already left.- - 0.67\n7No reporters present who had seen the intern dressed as a Fed Ex employee go into Mook\u2019s office saw him leave by the same front entrance. - - 0.49\n8The Fox reporter who recognized the intern also immediately looked outside of the campaign headquarters and noted that there were no Fed Ex vehicles \nparked outside.- - 0.51\n9Clinton seemed to have scripted responses ready for every question she was asked at the first debate. GT - 0.37\n10She had facts and numbers memorized for specific questions that it is very doubtful she would have had without being furnished the questions beforehand. GT - 0.63\n11The entire mainstream media has specifically been trying to portray Trump as a racist and a poor candidate. - - 0.24\n12By furnishing Clinton with the debate questions NBC certainly hoped to make Clinton appear much more knowledgeable and competent than Trump. GT PD 0.79\n13And though it is unlikely that anyone will be able to conclusively prove that Clinton was given the debate questions, it seems both logical and likely. GT PD 0.7\nFigure 3: Interpretation of relevance of sentences with the headline of an example article from DNF-300. GT and\nPD indicate ground truth and top-5 predicted \u201cclaim-worthy\u201d sentences, respectively. MA-CIN model was able to\npredict 3 most relevant sentences correctly. Last column shows the attention weights between headline and each\nof the sentences of the article. Sentence 2 has been correctly predicted as the most relevant while sentence 1 is the\nleast relevant.\nthe annotators as \u201cclaim-worthy\u201d were \ufb01nalized as\nground truth claims for these 50 articles, and used\nas testing set for evaluating the model performance\non DNF datasets. The remaining 475 articles from\nCDC, 250 articles from DNF-300, and 650 arti-\ncles from DNF-700 were split into 5 folds to train\nthe model using a 5-Fold cross validation (Kohavi,\n1995), where we use 4 folds for training and 1 fold\nfor validation. Each of the three settings, described\nin Section- 4: MA-CIN(HV), MA-CIN(OHWV)\nand MA-CIN(Combined), was trained with each\nof the three datasets, and evaluated on DNF Eval\nand CDC Eval. Total number of parameters for\nthese three settings are 15,012,916 (10,240 non-\ntrainable), 40,975,656 (10,240 non-trainable) and\n41,812,564 (12,288 non-trainable) respectively. All\nother network parameters are displayed in supple-mental material.\nIn each setting, we use batch normalization,\nReLU non-linearity as an activation function, and\na dropout of 0.5 for every convolution operation.\nWe trained all the models for 2000 epochs, where,\nfor every training we used Adam optimizer with\na learning rate lr= 0:0001 ,\f1= 0:99and\n\f2= 0:0. There was no weight decay set as the\nmodel was trained in a self-supervised setting with\n\ufb01nite epochs and an already small learning rate.\nGlove 300D word embedding was used for all our\nexperiments and the number of input sentences was\nset to 500. The models were trained on three 11GiB\nNvidia 1080Ti GPUs in parallel.\n5.2 Evaluation Metrics\nWe evaluate MA-CIN models on two evalua-\ntion sets, DNF Eval and CDC Eval. With self-\nsupervised setting we \ufb01rst rank the sentences based\non relevance with the headline and then extract\nthe top \ufb01ve sentences along with their sentence\nids as \u201cclaim-worthy\u201d sentences. For evaluation\non DNF Eval, we calculate the true positives (TP),\nfalse positives (FP) and false negatives (FN) from\nground truth claim ids. To evaluate on CDC Eval,\nsince we do not have ground truth claim ids, we\ncalculate cosine similarity between the extracted\nsentences and the ground truth claims. We experi-\nment with various similarity threshold to calculate\nTP, FP and FN, and set the \ufb01nal threshold to 0.95\nto report best performing results. Finally, these\nmetrics are used to report Precision@1, Recall@1\nand F-1 scores.\n5.3 Results\nTable-3 shows the performance of baseline (CDC)\n(Levy et al., 2014) and three variants of MA-CIN\nmodels. We report two baselines - (1) spacy, and\n(2) Levy et al. (2014) using supervised learning\nmethod on CDC dataset which contains annotated\nclaims. Since, Levy et al. (2014) do not report\nRecall and F1 scores, we have reported their Preci-\nsion@1 score in this paper. We also train MA-CIN\nmodels on this dataset by removing all the anno-\ntations for self-supervised training. We observe\nthat:\n1.The combined variant of our self-supervised\napproach performs slightly better than the\nbaseline on the CDC dataset. This shows\nthat, MA-CIN models are able to learn simi-\nlar properties as the baseline but without any\nsentence-level annotations. Thus, this elimi-\nnates the need to have an annotated dataset for\nclaim identi\ufb01cation.\n2.MA-CIN models give comparable results on\nall three datasets. This shows the scalability\nof the models to identify \u201cclaim-worthy\u201d sen-\ntences from any given article.\n3.The combined variant of MA-CIN, which gen-\nerates both the headline vector and the word\nin headline, performs better on all the datasets,\nexcept one: MA-CIN (OHWV) model trained\non DNF-300 and evaluated on CDC Eval per-\nforms slightly better than the combined model,\nhowever, the difference in the performance is\nvery small.\nFigure 4: Interpretation of sentence-to-sentence rele-\nvance through attention weights.\n6 Discussion\n6.1 Analyzing Attention Weights\nAttention weights help make the model inter-\npretable to the end users by depicting relationship\nbetween all sentences as well as with the headline.\nFrom Figure-3, we can see that out of the top-5\npredicted claims, 3 of them are present in the hu-\nman evaluated test set. The last column, which\ncontains attention coef\ufb01cients between the head-\nline and each sentence, depicts some interesting\nresults -\n(i) based on the human evaluation, the sentence\nhaving the least relevance with the headline is sen-\ntence 1. While this sentence contains words also\npresent in the headline, the underlying meaning is\nnot the same. This has been successfully captured\nby MA-CIN model by predicting sentence 1 as the\nleast relevant claim;\n(ii) further, highly ranked sentences 2, 12, and 13\nhave been correctly predicted as relevant claims by\nthe model. This shows the model\u2019s ability in learn-\ning the semantic relationship between the headline\nand the content of the article, and subsequently\nputting importance on sentences that are relevant\nto the headline\u2019s underlying meaning. This prop-\nerty, which is also called \u201c aboutness \u201d, is ef\ufb01ciently\nexhibited by the model.\n(iii) sentence 3, which is predicted by MA-CIN\nmodel as relevant with a score of 0.73, is not\npresent in the ground truth. This indicates that the\ntwo annotators did not agree to have this sentence\nveri\ufb01ed, even if it is relevant to the point of the\narticle. To analyze this further, we plan to conduct\nuser studies as one of the future avenues.\n(iv) sentence 4 is also predicted as a relevant\nclaim but it\u2019s missing from the ground truth since\nthe annotators did not agree to have this veri\ufb01ed.\nThe reason for this prediction could be because self-\nattention is able to identify the premise of highly\nrelevant sentences. Hence, sentence 4, which is the\ncontinuation of highly relevant sentence 3, is also\ngiven importance by the headline. This relevance\nbetween sentences 3 and 4 is depicted in Figure-4,\nwhere the attention weight between these two is the\nhighest.\nFrom Figure-4, we also observe that:\n(i) sentence 4 is highly relevant to sentences 3 to\n8, which is intuitive, since the story of the intern\nforms the premise of the claims in the article;\n(ii) sentences 2 and 4 have been shown to have\nthe least relevance with each other which is also\ntrue as shown in Figure-3. The two sentences, if\nconsidered in isolation, make two different claims\nwhich are not related to each other;\n(iii) the model has made sure that a sentence\ndoes not assign high relevance to itself as it would\nbe counter-intuitive.\n6.2 Limitations\nSince, the evaluation methodologies for CDC\ndataset has not been explained clearly, in our pa-\nper, we have considered vector cosine similarity\nbetween the ground truth claim in the CDC Eval\nand the extracted claim from the model which may\nleave a margin of error in the evaluation scores.\nAdditionally, ground truth in DNF Eval is manu-\nally generated and may contain subjective biases.\nAlthough these biases have been overcome by MA-\nCIN models, as explained in 6.1, but we also plan to\nenhance the ground truth judgement using crowd-\nsourced annotation. We intend to use these annota-\ntions to \ufb01ne-tune the models.\n7 Conclusion and Future Work\nIn this work, we build a novel, self-supervised ap-\nproach to identify \u201cclaim-worthy\u201d sentences - an\nimportant task for automated fact checking. The fo-\ncus of this work is on fake news articles where there\nis a deliberate intent to in\ufb02uence people or cause\nsocial unrest. We have introduced novel datasets of\nsuch articles with features essential for the down-\nstream task of fake news veri\ufb01cation. Using pow-\nerful attention models, we explore the notion ofaboutness of the headline and the content of the\narticle to identify \u201cclaim-worthy\u201d sentences. Ex-\nperiments with three datasets show the strength\nof our model architecture in overcoming human-\ninduced biases, which is quite common when using\nsentence-level claim-annotated datasets. Based on\nthe comparison with the baseline, which was imple-\nmented using annotated dataset, we show that our\nmodels do not require annotated claims for training\nto identify claim-worthy sentences ef\ufb01ciently. We\nhave also showed that our model is scalable to any\ndataset with topic and content.\nFuture work involves increasing the robustness\nof the models presented in this paper. We plan\nto use crowdsourced annotation on the dataset re-\nleased with this paper to measure the in\ufb02uence of\nthe article on general readers and then use these in-\ndicators to \ufb01ne-tune our models. Experimentation\nwith more robust sentence encoders is another av-\nenue of future work. Additionally, going forward,\nwe plan to identify a maximum of 3 claims per\narticle which will be used for evidence-based fake\nnews detection. We also plan to expand the dataset,\npresented in this work, to include fake news articles\non topics other than Politics and Health.\nReferences\nEhud Aharoni, Anatoly Polnarov, Tamar Lavee, Daniel\nHershcovich, Ran Levy, Ruty Rinott, Dan Gut-\nfreund, and Noam Slonim. 2014. A Benchmark\nDataset for Automatic Detection of Claims and Ev-\nidence in the Context of Controversial Topics. In\nProceedings of the First Workshop on Argumenta-\ntion Mining , pages 64\u201368, Baltimore, Maryland. As-\nsociation for Computational Linguistics.\nHunt Allcott and Matthew Gentzkow. 2017. Social me-\ndia and fake news in the 2016 election. Journal of\neconomic perspectives , 31(2):211\u201336.\nLeif Azzopardi, Gabriella Kazai, Stephen Robertson,\nStefan R \u00a8uger, Milad Shokouhi, Dawei Song, and\nEmine Yilmaz. 2009. Advances in Information Re-\ntrieval Theory: Second International Conference on\nthe Theory of Information Retrieval, ICTIR 2009\nCambridge, UK, September 10-12, 2009 Proceed-\nings, volume 5766. Springer.\nPD Bruza and Theo WC Huibers. 1996. A study of\naboutness in information retrieval. Arti\ufb01cial Intelli-\ngence Review , 10(5-6):381\u2013407.\nCarlos Castillo, Marcelo Mendoza, and Barbara\nPoblete. 2011. Information credibility on twitter. In\nProceedings of the 20th international conference on\nWorld wide web , pages 675\u2013684. ACM.\nM de Cock Buning. 2018. A multi-dimensional ap-\nproach to disinformation: Report of the independent\nhigh level group on fake news and online disinforma-\ntion. Publications Of\ufb01ce of the European Union.\nJudith Eckle-Kohler, Roland Kluge, and Iryna\nGurevych. 2015. On the Role of Discourse Mark-\ners for Discriminating Claims and Premises in Ar-\ngumentative Discourse. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 2236\u20132242, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nPepa Gencheva, Preslav Nakov, Llu \u00b4\u0131s M `arquez, Al-\nberto Barr \u00b4on-Cede \u02dcno, and Ivan Koychev. 2017.\nA context-aware approach for detecting worth-\nchecking claims in political debates. In Proceedings\nof the International Conference Recent Advances in\nNatural Language Processing, RANLP 2017 , pages\n267\u2013276, Varna, Bulgaria. INCOMA Ltd.\nD Graves. 2018. Understanding the promise and limits\nof automated fact-checking. Reuters Institute for the\nStudy of Journalism .\nNaeemul Hassan, Fatma Arslan, Chengkai Li, and\nMark Tremayne. 2017. Toward automated fact-\nchecking: Detecting check-worthy factual claims\nby claimbuster. In Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge\nDiscovery and Data Mining , pages 1803\u20131812.\nACM.\nSepp Hochreiter and J \u00a8urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735\u20131780.\nMercedes Jaime Sis \u00b4o and MERCEDES. 2009. Titles\nor headlines? anticipating conclusions in biomedi-\ncal research article titles as a persuasive journalistic\nstrategy to attract busy readers. Miscel \u00b4anea: A Jour-\nnal of English and American Studies , 39:29\u201351.\nIsraa Jaradat, Pepa Gencheva, Alberto Barr \u00b4on-Cede \u02dcno,\nLlu\u00b4\u0131s M `arquez, and Preslav Nakov. 2018. Claim-\nRank: Detecting check-worthy claims in Arabic\nand English. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics: Demonstrations ,\npages 26\u201330, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nZhiwei Jin, Juan Cao, Yongdong Zhang, and Jiebo Luo.\n2016. News veri\ufb01cation by exploiting con\ufb02icting\nsocial viewpoints in microblogs. In Thirtieth AAAI\nConference on Arti\ufb01cial Intelligence .\nRon Kohavi. 1995. A study of cross-validation and\nbootstrap for accuracy estimation and model selec-\ntion. In Proceedings of the 14th International Joint\nConference on Arti\ufb01cial Intelligence - Volume 2 ,\nIJCAI\u201995, pages 1137\u20131143, San Francisco, CA,\nUSA. Morgan Kaufmann Publishers Inc.Lev Konstantinovskiy, Oliver Price, Mevan Babakar,\nand Arkaitz Zubiaga. 2018. Towards automated\nfactchecking: Developing an annotation schema and\nbenchmark for consistent automated claim detection.\narXiv preprint arXiv:1809.08193 .\nJeffrey Kuiken, Anne Schuth, Martijn Spitters, and\nMaarten Marx. 2017. Effective headlines of news-\npaper articles in a digital environment. Digital Jour-\nnalism , 5(10):1300\u20131314.\nSejeong Kwon, Meeyoung Cha, Kyomin Jung, Wei\nChen, and Yajun Wang. 2013. Prominent features of\nrumor propagation in online social media. In 2013\nIEEE 13th International Conference on Data Min-\ning, pages 1103\u20131108. IEEE.\nYann LeCun, L \u00b4eon Bottou, Yoshua Bengio, and Patrick\nHaffner. 1998. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE ,\n86(11):2278\u20132323.\nRan Levy, Yonatan Bilu, Daniel Hershcovich, Ehud\nAharoni, and Noam Slonim. 2014. Context Depen-\ndent Claim Detection. In Proceedings of COLING\n2014, the 25th International Conference on Compu-\ntational Linguistics: Technical Papers , pages 1489\u2013\n1500, Dublin, Ireland. Dublin City University and\nAssociation for Computational Linguistics.\nRan Levy, Shai Gretz, Benjamin Sznajder, Shay Hum-\nmel, Ranit Aharonov, and Noam Slonim. 2017. Un-\nsupervised corpus-wide claim detection. In ArgMin-\ning@EMNLP .\nJing Ma, Wei Gao, Zhongyu Wei, Yueming Lu, and\nKam-Fai Wong. 2015. Detect rumors using time se-\nries of social context information on microblogging\nwebsites. In Proceedings of the 24th ACM Inter-\nnational on Conference on Information and Knowl-\nedge Management , CIKM \u201915, pages 1751\u20131754,\nNew York, NY , USA. ACM.\nJeppe Norregaard, Benjamin D. Horne, and Sibel Adali.\n2019. NELA-GT-2018: A large multi-labelled news\ndataset for the study of misinformation in news arti-\ncles. CoRR , abs/1904.01546.\nArchita Pathak and Rohini Srihari. 2019. BREAK-\nING! presenting fake news corpus for automated\nfact checking. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics: Student Research Workshop , pages 357\u2013\n362, Florence, Italy. Association for Computational\nLinguistics.\nVer\u00b4onica P \u00b4erez-Rosas, Bennett Kleinberg, Alexandra\nLefevre, and Rada Mihalcea. 2018. Automatic de-\ntection of fake news. In Proceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 3391\u20133401, Santa Fe, New Mexico, USA.\nAssociation for Computational Linguistics.\nMartin Potthast, Johannes Kiesel, Kevin Reinartz,\nJanek Bevendorff, and Benno Stein. 2017. A sty-\nlometric inquiry into hyperpartisan and fake news.\narXiv preprint arXiv:1702.05638 .\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nV olkova, and Yejin Choi. 2017. Truth of varying\nshades: Analyzing language in fake news and polit-\nical fact-checking. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing , pages 2931\u20132937.\nNatali Ruchansky, Sungyong Seo, and Yan Liu. 2017.\nCsi: A hybrid deep model for fake news detection.\nInProceedings of the 2017 ACM on Conference\non Information and Knowledge Management , CIKM\n\u201917, pages 797\u2013806, New York, NY , USA. ACM.\nSneha Singhania, Nigel Fernandez, and Shrisha Rao.\n2017. 3han: A deep neural network for fake news\ndetection. In International Conference on Neural In-\nformation Processing , pages 572\u2013581. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is\nAll You Need. arXiv:1706.03762 [cs] . ArXiv:\n1706.03762.\nKarin Wahl-Jorgensen and Thomas Hanitzsch. 2009.\nThe handbook of journalism studies . Routledge.\nWilliam Yang Wang. 2017. \u201d liar, liar pants on \ufb01re\u201d:\nA new benchmark dataset for fake news detection.\narXiv preprint arXiv:1705.00648 .\nLiang Wu and Huan Liu. 2018. Tracing fake-news\nfootprints: Characterizing social media messages by\nhow they propagate. In Proceedings of the Eleventh\nACM International Conference on Web Search and\nData Mining , WSDM \u201918, pages 637\u2013645, New\nYork, NY , USA. ACM.\nFan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012.\nAutomatic detection of rumor on sina weibo. In Pro-\nceedings of the ACM SIGKDD Workshop on Mining\nData Semantics , page 13. ACM.\nHan Zhang, Ian Goodfellow, Dimitris Metaxas, and Au-\ngustus Odena. 2018. Self-attention generative adver-\nsarial networks.\nA Appendices\nA.1 De\ufb01nitions\nFake News: Articles where there is a deliberate\nintent to in\ufb02uence readers through fabricated or\nmanipulated claims in the headline and the content.\nSuch articles have a compelling writing style simi-\nlar to the mainstream media.\n\u201cClaim-worthy\u201d: Statements which are important\nto the point of the article but one would require to\nhave them veri\ufb01ed.\nCompelling Fake News Articles: Articles which\nmake persuasive claims in headline and content,\nthat may in\ufb02uence readers to believe a fabri-\ncated/manipulated story.Credible Sources : Mainstream media, established\nfact-checking websites and Government docu-\nments.\nQuestionable Sources : Non-mainstream media\nlike infowars, naturalnews, breitbart etc.\nA.2 Experiment Architectures\nA.2.1 Vector Generation\nArchitecture setting for generating Headline Vector\n(HV) displayed in Figure-5\ninput Model Output size[Kernel size, Filters, \nStrides], Repeats\nsentence vectors 500x300\nconv1d_1 500 x 256 [4, 256, 1] x 1\nconv1d_2 500 x 512 [4, 256, 1] x 1\nSelfAttention 500 x 512 [1, 64, 1] x 4\nConcat 500 x 2048\nconv1d_3 500 x 512 [4, 256, 1] x 1\nheadline vector 1 x 300\nconv1d_4 1 x 512 [1, 512, 1] x 1\nCrossAttention 500 x 512 [1, 64, 1] x 4\nConcat 500 x 2048\nconv1d_5 250 x 512 [4, 512, 2] x 1\nconv1d_6 125 x 512 [4, 512, 2] x 1\nconv1d_7 63 x 512 [4, 512, 2] x 1\nconv1d_8 32 x 512 [4, 512, 2] x 1\nGlobal Pooling 512\nFC_1 1024\noutput_vector 300\nFigure 5: Architecture setting for generating Headline\nVector(HV).\nA.2.2 Word Generation\nArchitecture setting for generating Headline Vector\nWord Probabilities (OHWV) displayed in Figure-6\nA.3 DNF-700 Dataset Details\nEach article is identi\ufb01ed by an id. The content of\nthe article is stored in a separate text \ufb01les having\n\ufb01le name \u201carticle id\u201d, for example, article 122. A\nJSON \ufb01le is also provided with the following \ufb01elds:\nid: Unique identi\ufb01er of the article starting\nfrom 0.\nauthors: Authors of the article.\nheadline: Headline of the article.\ntype: \u201cfake\u201d (articles from Stanford and Buzzfeed\ndatasets which are already proven fake); and\n\u201cquestionable\u201d (articles from Getting Real About\nFake News Kaggle dataset which require manual\nveri\ufb01cation of the degree of veracity)\nurls: Source/domain URL of the article.\ninput Model Output size[Kernel size, Filters, \nStrides], Repeats\nsentence vectors 500x300\nconv1d_1 500 x 256 [4, 256, 1] x 1\nconv1d_2 500 x 512 [4, 256, 1] x 1\nSelfAttention 500 x 512 [1, 64, 1] x 4\nConcat 500 x 2048\nconv1d_3 500 x 512 [4, 256, 1] x 1\nheadline vector 1 x 300\nconv1d_4 1 x 512 [1, 512, 1] x 1\nCrossAttention 500 x 512 [1, 64, 1] x 4\nConcat 500 x 2048\nconv1d_5 250 x 512 [4, 512, 2] x 1\nconv1d_6 125 x 512 [4, 512, 2] x 1\nconv1d_7 63 x 512 [4, 512, 2] x 1\nconv1d_8 32 x 512 [4, 512, 2] x 1\nGlobal Pooling 512\nRepeat 50 x 512\nBi-LSTM 50 x 1024\nTimeDistributed \nDense, softmax50 x 20000Figure 6: Architecture setting for generating Headline\nVector Word Probabilities (OHWV).\nA.4 DNF-300 Dataset Details\nDNF-300 is more sophisticated subset of DNF-700\nwith additional \ufb01elds based on manual veri\ufb01cation\nof the article. The JSON \ufb01le of this dataset\ncontains following \ufb01elds:\nid: Unique identi\ufb01er of the article starting\nfrom 0.\nauthors: Authors of the article.\nheadline: Headline of the article.\ntype:f(0) False; (1) Partial Truth; (2) Opinions\nStated As Fact; (3) True g\nurls: Source/domain URL of the article.\nevidence: URL of credible sources supporting or\nrefuting the article. This \ufb01eld is empty when no\nevidence were found which talked about the claims\nmade in this article. This means, the claims are\ninnovated lies. In such cases, the type \ufb01eld is set as\n0.\nreason: Reason about the verdict. It can be one of\nthe following:\n1.Based on Snopes rating \u2019False\u2019 which means\n\u2019the primary elements of a claim are demon-\nstrably false.\u2019\n2.Based on Snopes rating \u2019Unproven\u2019 which\nmeans \u2019insuf\ufb01cient evidence exists to estab-\nlish the given claim as true, but the claim can-\nnot be de\ufb01nitively proved false.\u2019\n3.Based on Snopes rating \u2019Mixture\u2019 which\nmeans \u2019a claim has signi\ufb01cant elements of\nboth truth and falsity to it such that it could\nnot fairly be described by any other rating.\u2019\n4.Based on Snopes rating \u2019Mostly False\u2019 whichmeans \u2019the primary elements of a claim are\ndemonstrably false, but some of the ancillary\ndetails surrounding the claim may be accu-\nrate.\u2019\n5.The key claim is false (based on Snopes rat-\ning), however, the article also contains opin-\nions stated as fact.\n6.Snopes mentiones that a true story was manip-\nulated to mislead people.\n7.The key claims are true but exaggerated by\nadding personal opinions stated as fact.\n8.No reports from trusted sources for the key\nclaims.\n9.True story manipulated to mislead read-\ners by making unveri\ufb01able claims such as\n\u2018some claim\u2019 .\n10.Article is fraught with opinions stated as fact\nabout a true event.\n11. Found evidence to refute key claims.\n12. Article contains opinions stated as fact.\n13. Evidence found to support key claims.\nFigure 7: : Example for CDCs and for statements that\nshould not be considered as CDCs. The V and X indi-\ncate if the candidate is a CDC for the given Topic, or\nnot, respectively.\nA.5 Examples\nWe present examples for all 4 label types fFalse ;\nPartial Truth ;Opinion stated as fact ;Truegpresent\nin our dataset: DNF-300. Please refer Table-4,5,6,7.\nAn annotated example from CDC dataset is dis-\nplayed in Figure-7\nHeadline : Clinton Received Debate Questions Week Before Debate\nThe \ufb01rst presidential debate was held and Hillary Clinton was proclaimed the winner by the media.\nIndeed Clinton was able to turn in a strong debate performance, but did she do so fairly? Multiple\nreports and leaked information from inside the Clinton camp claim that the Clinton campaign was\ngiven the entire set of debate questions an entire week before the actual debate. Earlier last week an\nNBC intern was seen hand delivering a package to Clinton\u2019s campaign headquarters, according to\nsources. The package was not given to secretarial staff, as would normally happen, but the intern\nwas instead ushered into the personal of\ufb01ce of Clinton campaign manager Robert Mook. Members\nof the Clinton press corps from several media organizations were in attendance at the time, and a\nreporter from Fox News recognized the intern, but said he was initially confused because the NBC\nintern was dressed like a Fed Ex employee. The reporter from Fox questioned campaign staff about\nthe intern, but campaign staff at \ufb01rst claimed ignorance and then claimed that it was just a Fed Ex\nemployee who had already left. No reporters present who had seen the intern dressed as a Fed Ex\nemployee go into Mook\u2019s of\ufb01ce saw him leave by the same front entrance. The Fox reporter who\nrecognized the intern also immediately looked outside of the campaign headquarters and noted that\nthere were no Fed Ex vehicles parked outside. Clinton seemed to have scripted responses ready\nfor every question she was asked at the \ufb01rst debate. She had facts and numbers memorized for\nspeci\ufb01c questions that it is very doubtful she would have had without being furnished the questions\nbeforehand. The entire mainstream media has speci\ufb01cally been trying to portray Trump as a racist\nand a poor candidate. By furnishing Clinton with the debate questions NBC certainly hoped to\nmake Clinton appear much more knowledgeable and competent than Trump. And though it is\nunlikely that anyone will be able to conclusively prove that Clinton was given the debate questions,\nit seems both logical and likely.\nType : 0 (False )\nAuthors :Baltimore Gazette\nURLs : http://www.freemarketcentral.com/index.php/post/2503/report-clinton-received-debate-\nquestions-a-week-before-debate\nEvidence : [https://www.snopes.com/fact-check/clinton-received-debate-questions-week-before-\ndebate/, https://www.truthor\ufb01ction.com/hillary-clinton-received-debate-questions-advance/]\nReason : Based on Snopes rating \u2019False\u2019 which means \u2019the primary elements of a claim are\ndemonstrably false.\u2019\nTable 4: An example on False type from DNF-300 dataset.\nHeadline : Allergens in Vaccines Are Causing Life-Threatening Food Allergies\nIt would probably surprise few people to hear that food allergies are increasingly common in U.S.\nchildren and around the world . According to one public health website , food allergies in children\naged 0-17 in the U.S. increased by 50% from 1997 to 2011. Although food allergies are now so\nwidespread as to have become almost normalized, it is important to realize that millions of American\nchildren and adults suffer from severe rapid-onset allergic reactions that can be life-threatening.\nFoods represent the most common cause of anaphylaxis among children and adolescents. The\nUnited Kingdom has witnessed a 700% increase in hospital admissions for anaphylaxis and a\n500% increase in admissions for food allergy since 1990. The question that few are asking is why\nlife-threatening food allergies have become so alarmingly pervasive. A 2015 open access case\nreport by Vinu Arumugham in the Journal of Developing Drugs , entitled \u201c Evidence that Food\nProteins in Vaccines Cause the Development of Food Allergies and Its Implications for Vaccine\nPolicy ,\u201d persuasively argues that allergens in vaccines\u2014and speci\ufb01cally food proteins\u2014may be\nthe elephant in the room. As Arumugham points out, scientists have known for over 100 years that\ninjecting proteins into humans or animals causes immune system sensitization to those proteins.\nAnd, since the 1940s, researchers have con\ufb01rmed that food proteins in vaccines can induce allergy\nin vaccine recipients. Arumugham is not the \ufb01rst to bring the vaccine-allergy link to the public\u2019s\nattention. Heather Fraser makes a powerful case for the role of vaccines in precipitating peanut\nallergies in her 2011 book, The Peanut Allergy Epidemic: What\u2019s Causing It and How to Stop It.\nType : 1 (Partial Truth )\nAuthors :Admin - Orissa\nURLs : galacticconnection.com\nEvidence : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3890451/\nReason : The key claim is written in such a way so that it misleads people in thinking all the food\nrelated allergies in US are caused by vaccines. Found evidence which says these type of allergies\nare rare.\nTable 5: An example on Partial Truth type from DNF-300 dataset.\nHeadline : George Soros: Trump Will Win Popular V ote by a Landslide but Clinton Victory a\n\u2019Done Deal\u2019\nIn recent weeks, Democrats have attempted to paint Republican presidential nominee Donald J.\nTrump as a lunatic for claiming that the election is going to be rigged in favor of his Democratic\nrival, Hillary Clinton. Even Republican politicians and former politicians are telling Trump to\nknock off such talk. But, as usual, Trump\u2019s shrewdness and de\ufb01ance of standard political decorum \u2013\nin which the \u201copposition\u201d party merely rolls over and surrenders in the face of Democratic pressure\n\u2013 is winning the day. None other than billionaire investor and longtime Democratic supporter\nGeorge Soros has said that the \ufb01x is literally in for the election, in favor of Clinton \u2013 no matter\nhow much of the popular vote, and from which battleground states, Trump captures. As reported\nby Top Right News and other outlets, during a recent interview with Bloomberg News, Soros \u2013 a\nDemocrat mega-donor \u2013 openly admitted that Trump will win the popular vote in a \u201clandslide.\u201d\nHowever, he said that none of that would matter, because a President Hillary Clinton is already a\n\u201cdone deal.\u201d In the interview, which is now going viral, Soros says with certainty that Trump will\ntake the popular vote, despite what the polls say now (which are completely rigged to oversample\nDemocrats), but not the Electoral College, which will go to Clinton. When the reporter asks if that\nis already a \u201cdone deal\u201d \u2013 that Clinton will be our next president no matter what \u2013 Soros says \u201cyes,\u201d\nand nods his head. Is Soros just making a prediction out of overcon\ufb01dence? Or does he truly know\nsomething most of us don\u2019t know?\nType : 2 (Opinion Stated As Fact )\nAuthors :J. D. Heyes\nURLs : https://www.naturalnews.com/055789 George Soros Hillary Clinton electoral college.html\nEvidence : 1. https://www.snopes.com/fact-check/george-soros-trump-will-win-popular-vote-by-\na-landslide-but-clinton-victory-a-done-deal/,\n2. https://www.bloomberg.com/news/videos/2016-01-22/soros-clinton-to-win-popular-vote-in-\nlandslide\nReason : The key claim is false (based on Snopes rating), however, the article also contains opinions\nstated as fact.\nTable 6: An example on Opinion stated as fact type from DNF-300 dataset.\nHeadline : Donald Trump: Minnesota Has \u2018Suffered Enough\u2019 Accepting Refugees\nIn a pitch to suspend the nation\u2019s Syrian refugee program , Donald Trump said Minnesotans have\n\u201csuffered enough\u201d from accepting Somali immigrants into their state. \u201cHere in Minnesota you have\nseen \ufb01rst hand the problems caused with faulty refugee vetting, with large numbers of Somali\nrefugees coming into your state, without your knowledge, without your support or approval,\u201d Trump\nsaid at a Minneapolis rally Sunday afternoon. He said his administration would suspend the Syrian\nrefugee program and not resettle refugees anywhere in the United States without support from the\ncommunities, while Hillary Clinton\u2019s \u201cplan will import generations of terrorism, extremism and\nradicalism into your schools and throughout your communities.\u201d\nType : 3 (True)\nAuthors :Henry Wolff\nURLs : amren.com\nEvidence : 1. https://time.com/4560078/donald-trump-minnesota-somali-refugees/,\n2. https://www.buzzfeednews.com/article/claudiakoerner/trump-vs-somali-refugees\nReason : Evidence found to support key claims.\nTable 7: An example on True type from DNF-300 dataset.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Self-supervised claim identification for automated fact checking", "author": ["A Pathak", "MA Shaikh", "R Srihari"], "pub_year": "2021", "venue": "arXiv preprint arXiv:2102.02335", "abstract": "We propose a novel, attention-based self-supervised approach to identify \"claim-worthy\"  sentences in a fake news article, an important first step in automated fact-checking. We"}, "filled": false, "gsrank": 473, "pub_url": "https://arxiv.org/abs/2102.02335", "author_id": ["2MvNbL0AAAAJ", "", "Uttu9kkAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:PxVUoUtXDVYJ:scholar.google.com/&output=cite&scirp=472&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D470%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=PxVUoUtXDVYJ&ei=W7WsaJnZIo6IieoP0sKRuAk&json=", "num_citations": 6, "citedby_url": "/scholar?cites=6200708244299846975&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:PxVUoUtXDVYJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2102.02335"}}, {"title": "NLP-Driven Approaches to Measuring Online Polarization and Radicalization", "year": "2025", "pdf_data": "NLP-Driven Approaches to Measuring Online\nPolarization and Radicalization\nby\nVahid Ghafouri\nA dissertation submitted in partial fulfillment of the requirements\nfor the degree of Doctor of Philosophy in\nTelematic Engineering\nUniversidad Carlos III de Madrid\nTutor/Advisor: Guillermo Suarez-Tangil\nCo-advisor: Jose Such\nDecember 2024\n\niii\nNLP-Driven Approaches to Measuring Online Polarization and Radicalization\nPrepared by:\nVahid Ghafouri, IMDEA Networks Institute, Universidad Carlos III de Madrid\ncontact: vahid.ghafouri@imdea.org\nUnder the advice of:\nGuillermo Suarez-Tangil, IMDEA Networks Institute\nJose Such, King\u2019s College London and VRAIN, Universitat Politecnica de Valencia\nThis work has been supported by:\nThe content of this thesis is distributed under license\n\u201cCreative Commons Attribution - Non Commercial - Non Derivatives\u201d\nhttps://creativecommons.org/licenses/by-nc-nd/4.0/deed.en\n\n\n\u201cWhatever you areseeking, you arethat.\u201d\n\u2013 Rumi\n\nAcknowledgements\nI wish I had a dramatic life story for you here, a tale of overcoming immense hardships\nwith extraordinary brilliance and hard work. The truth is, I have always been privileged\nand blessed throughout my entire life. I often fail to recognize this because these privileges\nhave been provided to me unconditionally.\nI was raised in a healthy, loving, and supportive family in a relatively safe country\nwith a rich cultural depth that greatly contributed to my mental and personal devel-\nopment. My parents enrolled me in the most popular private elementary school in my\nhometown. Later, I had the privilege to study at the most prestigious high school and\nuniversity in Iran during my undergraduate years. Both institutions were state-funded,\npaid for by the hard-earned taxes of the Iranian working class. This support continued\nthrough my M.Sc. and Ph.D. studies, where my scholarships were indirectly funded by\nTurkish and Spanish/EU taxpayers. I am deeply grateful to all of them and hope that\none day I can repay this kindness. What I\u2019m trying to convey is that I never needed to\nwork while studying, a luxury our previous generations often did not have. My Ph.D.\nstudies were conducted under the guidance of two reasonable and kind supervisors, in a\nbeautiful country with incredibly friendly people. As an immigrant, I never experienced\nany instance of discrimination or hate from the Spanish people. All I encountered was\nkindness and hospitality. That\u2019s how I will always remember and speak about Spain.\nZooming out a bit, throughout my life, I have always had access to security, food,\nshelter, fresh water, electricity, and hot water. I owe this to the millennia of efforts by\nmy ancestors and humanity\u2019s collective struggle to safeguard these resources, as well as\nto the critical thinking of both early and modern scientists who have worked tirelessly\nin the shadows to serve the light. Also to the massive number of hardworking folks in\nmaintenance and logistics; a group which contributes highest to the society and barely\nreceives any credit and appreciation.\nI also wish to mention a few names. I will miss some, I hope they forgive me:\nA few teachers and mentors who had a significant impact on my mental development: Dr.\nTouiserkaani, Dr. Narimani, Mr. Fereydooni, Mr. Bahirayi, Mr. Afzooni, Mr. Sanjari,\nMr. Nematipour, Mr. Adelinezhad, Mr. Khanemasjedi, et al.\nSome of my dearest and closest friends: Mohammad Basirzadeh, Pouria Mohammadzadeh\nOqaz, Mohammad Sabzpoush, Hosein Jamalpour, Behdad Goudarzi, Mohsen Babadi,\nHosein Mahfouzi, Omid Rahimpour, Mojtaba Nosratlo, et al.\nvii\n\nPublished and SubmittedContent\nThis thesis is based on the following published or submitted papers:\n[1] Vahid Ghafouri , Jose Such, Guillermo Suarez-Tangil, \u201cI love pineapple on pizza !=\nI hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining\u201d . In:\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) .\n\u2022This work is fully included and the contents are reported in Chapter 9.\n\u2022The thesis author is the first author of this work and led the design, implementation,\nand writing of the paper.\n\u2022The material from this source included in this thesis is not singled out with typo-\ngraphic means and references.\n[2] Vahid Ghafouri , Faisal Alatawi, Mansooreh Karami, Jose Such, and Guillermo\nSuarez Tangil, \u201cTransformer-Based Quantification of the Echo Chamber Effect in On-\nline Communities\u201d. In: ACM Conference on Computer-Supported Cooperative Work and\nSocial Computing (CSCW) , San Jose, Costa Rica, 2024,\n\u2022This work is fully included and the contents are reported in Chapter 4.\n\u2022The thesis author is the first author of this work and led the design, implementation,\nand writing of the paper.\n\u2022The material from this source included in this thesis is not singled out with typo-\ngraphic means and references.\n[3]Ashwini Kumar Singh, Vahid Ghafouri , Jose Such, and Guillermo Suarez-Tangil,\n\u201cDifferences in the Toxic Language of Cross-Platform Communities\u201d . In: Proceedings of\nthe International AAAI Conference on Web and Social Media , Buffalo, NY, USA, June\n3\u20136 2024. https://doi.org/10.1609/icwsm.v18i1.31402 .\n\u2022This work is fully included and the contents are reported in Chapter 7.\n\u2022The thesis author is the second author of this work and contributed significantly\nto the design, implementation, and writing of the paper.\nix\nx\n\u2022The material from this source included in this thesis is not singled out with typo-\ngraphic means and references.\nYusuf M\u00a8 ucahit C \u00b8etinkaya*, Vahid Ghafouri *, Jose Such, Guillermo Suarez-Tangil and\nTu\u02d8 grulcan Elmas, \u201cCross-Partisan Interactions on Social Media\u201d . under \u201cRevised and\nResubmit\u201d from ICWSM 2025\n\u2022The content analysis section of this paper is included Chapter 5.\n\u2022The thesis author is the co-first author of this paper. He led the text analysis\nsection of the paper which is the only part included in the thesis.\n\u2022The material from this source included in this thesis is not singled out with typo-\ngraphic means and references.\n[4] Vahid Ghafouri , Vibhor Agarwal, Yong Zhang, Nishanth Sastry, Jose Such, and\nGuillermo Suarez Tangil, \u201cAI in the Gray: Exploring Moderation Policies in Dialogic\nLarge Language Models vs. Human Answers in Controversial Topics\u201d . In: Proceed-\nings of the 32nd ACM International Conference on Information and Knowledge Manage-\nment (CIKM) , Birmingham, United Kingdom, 2024, doi: https://doi.org/10.1145/\n3583780.3614777 .\n\u2022This work is fully included and the contents are reported in Chapter 8.\n\u2022The thesis author is the first author of this work and led the design, implementation,\nand writing of the paper.\n\u2022The material from this source included in this thesis is not singled out with typo-\ngraphic means and references.\n[5] Vahid Ghafouri , Jose Such, and Guillermo Suarez Tangil, \u201cA Holistic Indicator of\nPolarization to Measure Online Sexism\u201d . under review , doi: https://arxiv.org/abs/\n2404.02205 .\n\u2022This work is fully included and the contents are reported in Chapter 6.\n\u2022The thesis author is the first author of this work and led the design, implementation,\nand writing of the paper.\n\u2022The material from this source included in this thesis is not singled out with typo-\ngraphic means and references.\nOther publications that are not a major part of this thesis include:\n[6]Waleed Iqbal, Vahid Ghafouri , Gareth Tyson, Guillermo Suarez-Tangil, and Ignacio\nCastro, \u201cLady and the tramp nextdoor: Online manifestations of real-world inequalities\nin the nextdoor social network\u201d. In: Proceedings of the International AAAI Conference on\nxi\nWeb and Social Media , Limassol, Cyprus, 2023, doi: https://doi.org/10.1609/icwsm.\nv17i1.22155 .\nWe also acknowledge the use of several Generative AI tools, namely ChatGPT ,Gemini ,\nClaude AI ,Grammarly , and NoteBookLM , in the writing phase of this thesis. The tools\nwere used responsibly for the task of generating limited curated write-ups, paraphrasing,\nand grammar correction.\n\nAbstract\nThe growing popularity of social media has coincided with a massive number of real-\nworld issues and crises that are controversial and polarizing. Recent issues such as Russo-\nUkrainian and Israeli-Palestinian conflicts, alongside classic issues such as abortion-ban\nand gun-control, have raised heated debates offline and online. Throughout the past\ntwo decades, Computational Social Scientists have been introducing methods of modeling\nand measuring online polarization and radicalization. Yet, most of the proposed methods\nrely on traditional tools such as graph analysis and classic NLP models. These tools are\naccompanied by limitations in terms of scalability, granularity, and availability of data\n(e.g., follow network is no longer publicly available on Twitter).\nFortunately, in the past few years, thanks to the invention of the transformers archi-\ntecture, the world has witnessed massive breakthroughs in the field of Natural Language\nProcessing (NLP). Especially, Large Language Models (LLMs) have grasped the atten-\ntion of both public and scientific communities. These breakthroughs have also created\nunprecedented opportunities for advancing classic techniques in various domains of Com-\nputational Social Sciences, including polarization detection and opinion mining.\nThis thesis aims to propose novel approaches using state-of-the-art NLP techniques to\nmodel and track polarization on social media. It introduces a scalable method for quan-\ntifying echo chambers with sentence transformers, revealing asymmetries in discourse\ndiversity across political ideologies. Furthermore, it applies LLMs to analyze the content\nof cross-partisan interactions, showing that cross-party engagement does not necessarily\nlead to productive discourse. The thesis also investigates radicalization in gender-based\ncommunities and compares the spread of radical content across platforms like Reddit\nand Discord. Lastly, it addresses the limitations of existing language models in detecting\nstance polarity by fine-tuning a sentence transformer to become stance-aware, enabling\nmore accurate detection of opposing viewpoints on similar topics. Together, these contri-\nbutions offer Computational Social Scientists new tools for understanding polarization,\nradicalization, and bias in online environments.\nxiii\n\nTable of Contents\nAcknowledgements VII\nPublished and Submitted Content IX\nAbstract XIII\nTable of Contents XV\nList of Tables XXI\nList of Figures XXV\nList of Acronyms XXVII\n1. Introduction 1\n1.1. Motivation & Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2. Research Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2.1. Polarization and Echo Chambers . . . . . . . . . . . . . . . . . . . 4\n1.2.2. Radicalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.3. Biases in Language Models and Stance-Aware NLP . . . . . . . . . 5\n1.3. Thesis Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.4. Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2. Preliminaries 9\n2.1. Social Networks Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.1.1. Polarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.1.2. Radicalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.1.3. Echo Chambers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2. Natural Language Processing Concepts . . . . . . . . . . . . . . . . . . . . 11\n2.2.1. Generating Text Representations . . . . . . . . . . . . . . . . . . . 11\n2.2.2. Analyzing Text Representations . . . . . . . . . . . . . . . . . . . 13\n2.2.3. Text Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nxv\nxvi TABLE OF CONTENTS\n2.2.4. Fine-tuning Essentials . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3. Literature Review 19\nI Polarization and Echo Chambers 25\n4. Transformer-Based Quantification of the Echo Chamber Effect 27\n4.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.2. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.2.1. Echo Chamber and Social Harms . . . . . . . . . . . . . . . . . . . 30\n4.2.2. Echo Chamber Detection . . . . . . . . . . . . . . . . . . . . . . . 30\n4.2.3. User-level Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.3. Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4.4. Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n4.4.1. Detecting Chambers (Network Clusters) . . . . . . . . . . . . . . . 33\n4.4.2. Embedding Users . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.4.3. Quantifying Echo . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.4.4. Quantifying Polarization . . . . . . . . . . . . . . . . . . . . . . . . 36\n4.5. Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n4.6. Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n4.7. Experiments and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n4.7.1. Echo per Hashtag . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n4.7.2. Echo per Chamber . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n4.7.3. Comparison with Supervised Baseline . . . . . . . . . . . . . . . . 47\n4.8. Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.8.1. Key Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.8.2. Comparison with Previous Approaches . . . . . . . . . . . . . . . . 53\n4.8.3. Limitations & Future Work . . . . . . . . . . . . . . . . . . . . . . 55\n5. Cross-Partisan Interactions on Social Media 57\n5.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n5.2. Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n5.3. Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n5.4. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.4.1. Stance Contrast, PI vs. CPI . . . . . . . . . . . . . . . . . . . . . . 60\n5.4.2. Root Sentiment vs. Reply Stance . . . . . . . . . . . . . . . . . . . 61\n5.5. Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.5.1. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\nTABLE OF CONTENTS xvii\nII Radicalization 71\n6. Gender-based Polarization and Sexism 73\n6.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n6.2. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n6.2.1. Language Bias Quantification Based on Word-Embeddings . . . . 75\n6.2.2. Toxic Comment Detection . . . . . . . . . . . . . . . . . . . . . . . 76\n6.3. Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n6.3.1. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n6.3.2. Sexism Indicator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n6.4. Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n6.4.1. Subreddits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n6.4.2. Supervised Toxic Data . . . . . . . . . . . . . . . . . . . . . . . . . 83\n6.4.3. Sexism Indicator Evaluation Datasets . . . . . . . . . . . . . . . . 84\n6.5. Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n6.5.1. Evaluation of the supervised toxicity detector . . . . . . . . . . . . 85\n6.5.2. Evaluation of the Sexism Metric . . . . . . . . . . . . . . . . . . . 85\n6.6. Results & Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n6.7. Conclusion & Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n7. Platform\u2019s Effect on Toxicity 93\n7.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n7.2. Problem Statement & Background . . . . . . . . . . . . . . . . . . . . . . 95\n7.3. Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n7.3.1. Data Gathering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n7.3.2. Differential Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n7.4. Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n7.5. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n7.5.1. Community Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n7.5.2. Temporal Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n7.5.3. Toxicity Analysis per User . . . . . . . . . . . . . . . . . . . . . . . 106\n7.5.4. Semantic Categories Analysis . . . . . . . . . . . . . . . . . . . . . 107\n7.5.5. Linguistic Differences . . . . . . . . . . . . . . . . . . . . . . . . . 109\n7.5.6. Moderation Differences . . . . . . . . . . . . . . . . . . . . . . . . 110\n7.6. Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n7.6.1. Main Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n7.6.2. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.7. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.8. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nxviii TABLE OF CONTENTS\nIII Polarization in Language Models 119\n8. AI in the Gray: LLM and Controversy 121\n8.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n8.2. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n8.3. Data Collection Methodology . . . . . . . . . . . . . . . . . . . . . . . . . 123\n8.3.1. Kialo Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n8.3.2. Query Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n8.3.3. Source Affiliation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n8.4. Limitation of Direct Testing . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n8.5. Measuring Bias in the Wild . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n8.5.1. Overview of our Approach . . . . . . . . . . . . . . . . . . . . . . . 128\n8.5.2. Direct Leaning: Binary Answers . . . . . . . . . . . . . . . . . . . 129\n8.5.3. Bias in Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n8.5.4. Bias in Arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n8.5.5. Bias in Mitigation . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n8.6. Domain Knowledge: AI vs Human . . . . . . . . . . . . . . . . . . . . . . 135\n8.6.1. Embedding Variance . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n8.6.2. Gunning Fog Index . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n8.6.3. Domain-Specific Vocabulary . . . . . . . . . . . . . . . . . . . . . . 137\n8.7. Discussion & Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n9. Stance-Aware Sentence Transformers for Opinion Mining 141\n9.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n9.2. Motivation & Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n9.3. Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n9.3.1. Argument base: Anchor, Positive and Negative statements . . . . 145\n9.3.2. Architecture: Siamese and Triplet Model . . . . . . . . . . . . . . 145\n9.3.3. Siamese and Triplet Networks . . . . . . . . . . . . . . . . . . . . . 145\n9.3.4. Fine-tuning Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n9.4. Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n9.4.1. Training Data: Kialo . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n9.4.2. Generating Training Pairs and Triplets . . . . . . . . . . . . . . . . 149\n9.4.3. Baseline Data: STS-B . . . . . . . . . . . . . . . . . . . . . . . . . 150\n9.4.4. Out of Distribution Data: SemEval-2014 . . . . . . . . . . . . . . . 150\n9.4.5. Application Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n9.5. Experiments, Results, & Observations . . . . . . . . . . . . . . . . . . . . 151\n9.5.1. Validation on Kialo . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n9.5.2. Sentence Similarity Baseline . . . . . . . . . . . . . . . . . . . . . . 153\n9.5.3. Out of Distribution Validation . . . . . . . . . . . . . . . . . . . . 154\nTABLE OF CONTENTS xix\n9.5.4. Application: Semantic Search . . . . . . . . . . . . . . . . . . . . . 155\n9.6. Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n9.7. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n9.8. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n10.Conclusion 159\n10.1. Meeting the Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n10.2. Findings from Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n10.3. Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n10.3.1. Toward Explainability . . . . . . . . . . . . . . . . . . . . . . . . . 162\n10.3.2. Future Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n10.3.3. Enhancement of Base Tools . . . . . . . . . . . . . . . . . . . . . . 163\nReferences 165\n\nList of Tables\n2.1. Sample sentiment analysis task performed by ChatGPT-4o. . . . . . . . . 15\n3.1. Overview of the methodologies of previous work related to the different\ntopics of this thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.1. Queried hashtags for data collection. . . . . . . . . . . . . . . . . . . . . . 39\n4.2. F1-Scores for linear separability between pairs of user embeddings across\nhashtags. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n4.3. Stances of sampled tweets for each Chamber. The rate of alignment of\ntweets\u2019 stances with the hypothetical stance of a Chamber shows the ac-\ncuracy of the network clustering method. . . . . . . . . . . . . . . . . . . 44\n4.4. Summary of results for every Chamber of every topic. Columns beginning\nwith \u201cSeparability:\u201d for Chamber A refers to its users\u2019 separability from\nits twin Chamber (B) on the same topic , vice versa. . . . . . . . . . . . . 46\n4.5. Levels of user separability per pair of Chambers across all the topics.\nChamber A is the Democrat and Chamber B is the Republican retweet\ncluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.6. Replication of Table 4.4 with Supervised Baseline. . . . . . . . . . . . . . 49\n5.2. Sample tweets for AI-generated stances for replies. . . . . . . . . . . . . . 66\n5.3. Sample tweets for AI-generated root sentiments. . . . . . . . . . . . . . . 68\n5.1. Sample prompt and response for LLM-aided annotation of interactions . . 69\n6.1. Confusion Matrix for Toxicity-Detector model . . . . . . . . . . . . . . . . 85\n6.2. Top-100 most salient terms similarity matrix. The top-right (red) side\nof the table shows the number of common adjectives among the top 100\nsaliently biased adjectives toward female-identity. The bottom-left (blue)\nside depicts the same quality for male-identity. . . . . . . . . . . . . . . . 88\n7.1. Communities description. . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nxxi\nxxii LIST OF TABLES\n7.2. Percentage of different types of toxicity across the two platforms per com-\nmunity. (Note: We highlight in bold the highest value in a column and we\nunderline the second highest.) . . . . . . . . . . . . . . . . . . . . . . . . . 102\n7.3. Toxicity level-wise communities. . . . . . . . . . . . . . . . . . . . . . . . . 104\n7.4. Toxic users for Reddit (Red.) and Discord (Disc.) . . . . . . . . . . . . . . 106\n7.5. Cross-platform cosine similarity for semantic tags with most similar and\ndissimilar tags in toxic sentences. . . . . . . . . . . . . . . . . . . . . . . . 108\n7.6. Percentage of deleted comments per community and platform by modera-\ntors. AM: Auto-moderation. . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n7.7. Percentage of toxicity before and after including deleted comments as toxic\ncomments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n7.8. Tags description with sample sentences. . . . . . . . . . . . . . . . . . . . 116\n7.9. Dataset Statistics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n7.10. Semantic tags used in this chapter. Full list of tags\nhttps://ucrel.lancs.ac.uk/usas/semtags subcategories.txt. . . . . . . . . . 117\n8.1. Example of a Moderated Response by AI . . . . . . . . . . . . . . . . . . 128\n8.2. Example of a Direct Leaning in LLM\u2019s Response . . . . . . . . . . . . . . 130\n8.3. Example of a One-sided Argument by AI . . . . . . . . . . . . . . . . . . 132\n8.4. Automated Extraction of Economic Arguments from AI\u2019s Answers . . . . 133\n8.5. Sample Annotation by ChatGPT . . . . . . . . . . . . . . . . . . . . . . . 133\n8.6. Confusion Matrices for AI\u2019s Annotations. The columns are the True values\nof the classes and the rows are the predicted ones. Values in parentheses\nindicate parsing errors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n8.7. Economic and Sociopolitical Leaning of Arguments Provided by ChatGPT 135\n8.8. Sample Answer from the Engineered Prompt Asking ChatGPT to Provide\nPros and Cons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n8.9. Number and percentage of Arguments with Unassertive Language in Chat-\nGPT Responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n9.1. Example of argument pair creation. . . . . . . . . . . . . . . . . . . . . . . 149\n9.2. Example of triplet creation. . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n9.3. Kialo dataset\u2019s size. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n9.4. KL Divergence Between Agreeing and Opposing statements\u2019 distributions\nin Kialo Test Set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n9.5. Performance of models on STS-B test set (Spearman correlation). . . . . . 153\n9.6. Alignment Precision for semantic search on congresspeople tweets with\nabortion-related queries. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n9.7. Alignment Precision for semantic search on congresspeople tweets with\nabortion-related queries. D: Democrat alignment, R: Republican alignment. 156\nLIST OF TABLES xxiii\n9.8. Most similar semantic search results for a pro-abortion query for the Orig-\ninal and Fine-Tuned models. . . . . . . . . . . . . . . . . . . . . . . . . . 156\n\nList of Figures\n2.1. Word embedding architectures [7] . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2. Contrastive Learning Architectures . . . . . . . . . . . . . . . . . . . . . . 16\n2.3. LoRA\u2019s architecture [8] . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.1. Scheme of our method\u2019s architecture. . . . . . . . . . . . . . . . . . . . . . 34\n4.2. 2D projection of US congresspeople and senators\u2019 user-embeddings. . . . . 38\n4.3. Variances of user embeddings for partisan hashtags\u2019 users + #SXSW as a\nnon-partisan case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4.4. 2D projection of user-embeddings for polarized hashtags\u2019 users. . . . . . . 42\n4.5. Comparison of retweet networks vs 2D projection of user-embeddings. The\nred and blue points represent the users that had attended Conservative\nand Democrat Chambers in the corresponding events. . . . . . . . . . . . 45\n4.6. Users political ideology (polarity) distribution across each Chamber of each\ntopic. Negative values manifest left-leaning ideology and positive values\nmanifest right-leaning ideology. . . . . . . . . . . . . . . . . . . . . . . . . 49\n5.1. Stance-wise differences of partisan vs.cross-partisan replies across parties.\nThe bar labels indicate the overall frequency of the annotation. For samples\nof tweets for each annotation (stance), see Table 5.2. . . . . . . . . . . . 61\n5.2. Chi-test statistics \u03c7=Obsereve\u2212Expected\nExpectedfor co-occurrences of sentiments\nin root tweets and stances in replies (Top-Left: PI, Bottom-Right: CPI).\nStarred cells indicate p-values below 0.05. . . . . . . . . . . . . . . . . . . 62\n6.1. Outlook of our processing pipeline. . . . . . . . . . . . . . . . . . . . . . . 78\n6.2. Processing pipeline for building our Toxicity-Detector NLP model. . . . . 79\n6.3. Validation Chart for Our Sexism Metric for Toxicity Toward Female Identity 86\n6.4. Toxicity Toward Male Identity. . . . . . . . . . . . . . . . . . . . . . . . . 87\n6.5. Toxicity Toward Female Identity. . . . . . . . . . . . . . . . . . . . . . . . 87\n6.6. Toxicity Toward Male Individuals. . . . . . . . . . . . . . . . . . . . . . . 87\n6.7. Toxicity Toward Female Individuals. . . . . . . . . . . . . . . . . . . . . . 87\nxxv\nxxvi LIST OF FIGURES\n7.1. Sketch of the method used to find the association between communities\nthat support multiple platforms. . . . . . . . . . . . . . . . . . . . . . . . 96\n7.2. Our methodology in a nutshell. . . . . . . . . . . . . . . . . . . . . . . . . 97\n7.3. Average toxic sentences on Reddit and Discord platforms for communities\nunder study. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n7.4. Toxicity Timelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n7.5. Salient USAS tags in Reddit toxic content. . . . . . . . . . . . . . . . . . 109\n7.6. Salient USAS tags in Discord toxic content. . . . . . . . . . . . . . . . . . 109\n8.1. Political Compass Results for OpenAI Models. . . . . . . . . . . . . . . . 127\n8.2. The Types of Answers Open AI LLMs have given to Political Compass\nTest Questions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n8.3. The Proportion of Yes or No Answers to Controversial Questions, per Topic\nTag, per LLM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n8.4. Comparison between Bing AI and human (Kialo users) citations when re-\nsponding to controversial questions. . . . . . . . . . . . . . . . . . . . . . . 131\n8.5. Comparisons Between Semantic Diversity in AI vs Human per 100 Argu-\nments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n9.1. Our methodological pipeline and its application process. . . . . . . . . . . 145\n9.2. Sample discussion on Kialo website. . . . . . . . . . . . . . . . . . . . . . 148\n9.3. Performance of NV-Embed-v1 on Kialo Test-Set. . . . . . . . . . . . . . . 151\n9.4. Comparison of Model Distributions. . . . . . . . . . . . . . . . . . . . . . 152\n9.5. Distributions of cosine similarities of pairs in SemEval 2014 dataset. . . . 154\nList of Acronyms\nAIArtificial Intelligence\nCPI Cross-Partisan Interactions\nLLM Large Language Model\nLLMs Large Language Models\nNLP Natural Language Processing\nIAT Implicit Association Test\nWEAT Word Embedding Association Test\nBoW Bag of Words\nCBOW Continuous Bag of Words\nTF-IDF Term Frequency Inverse Document Frequency\nUMAP Uniform Manifold Approximation and Projection\nHDBSCAN Hierarchical Density-Based Spatial Clustering of Applications with Noise\nLoRA Low-Rank Adaptation\nPEFT Parameter Efficient Fine-Tuning\nRAG Retrieval Augmented Generation\nIRB Institutional Review Board\nxxvii\n\n1Introduction\nThe everyday-increasing popularity of social media platforms is transforming the land-\nscape of public political discourse. Some scholars believe that the diverse and populated\nnature of social media can enhance democratic discourse by providing spaces for varied\npolitical conversations and access to a wide range of information [9]. This exposure to\ndiverse perspectives has the potential to reduce polarization among users.\nEmpirical studies have also been performed to support the claim that social media\nhas a positive role in depolarization. For instance, a study tracking Twitter posts from\nmedia outlets over three years found that the language used in tweets can significantly\ninfluence the political diversity of the audience engaging with the content. By developing\na tool to help journalists craft tweets that appeal to a more politically diverse audience,\nresearchers were able to reduce the engagement gap between left- and right-leaning users\nby an average of 20.3% in experimental trials [10].\nYet, there is abundant literature with less optimistic points of view about social me-\ndia\u2019s role in polarization, claiming that in most cases social media leads to the creation\nof echo chambers that actually foster more polarization [11]. An echo chamber can be\ndescribed as a setting in which people\u2019s existing beliefs and opinions are amplified and\nreinforced through repeated interactions with others who share similar viewpoints and\npredispositions [12]. If that is also the case for social media platforms, it would conse-\nquentially mean that this constant exposure to like-minded people pushes moderate users\ntoward the ends of the political ideology spectrum, fostering polarization.\nFurthermore, online polarization, that is defined as the division of online communi-\nties into distinct opposing groups, has been linked to the radicalization of opinions and\nspread of misinformation [13]. The social harms associated with polarization have pushed\ncomputational social scientists to not only propose methods for measuring and modeling\nonline polarization [14, 15], but also develop strategies to mitigate it [16, 17].\nOn the other hand, the recent ground-breaking advances in the field of Natural Lan-\nguage Processing (NLP), such as transformers and Large Language Models (LLMs), have\nunlocked unique and unprecedented opportunities for computational social scientists to\n1\n2 Introduction\nimprove the granularity and accuracy of text-based social media analysis tasks.\nWhen it comes to measuring polarization, previous methods rely heavily either on\ngraph analyses or classic NLP approaches. A common theme of this Ph.D. thesis in almost\nall the chapters is that they all use state-of-the-art NLP tools to propose novel NLP-driven\napproaches for either directly modeling certain types of polarization (Chapters 4, 5, and 6)\nor providing fundamental building blocks (Chapter 9) for opinion mining and polarization\ndetection.\n1.1. Motivation & Goals\nOnline polarization has proven to cause undesirable effects such as exacerbating mis-\ninformation, radicalization, and promoting violent and harmful content. Thus, the ability\nto detect and measure polarization is essential not only for understanding these dynamics\nbut also for enabling strategies to address its potential harms. According to the Global\nRisks Report 2024 by the World Economic Forum1, polarization is ranked as the third top\nconcern for 2024, highlighting its significant societal impact alongside other critical global\nchallenges. Including such statistics reinforces the importance of addressing polarization\nin modern discourse.\nThe rapidly evolving nature of social media data requires measurements that satisfy\nrequirements such as scalability, generalizability, holism, granularity, and availability of\ndata. In the following, we will elaborate on each of the requirements.\nScalability in the context of polarization analysis is the model\u2019s ability to efficiently\nhandle and analyze the vast, dynamic data generated daily on social media platforms.\nThe rapid increase in the enormous amount of generated social network data makes scal-\nability a trivial essence of every social network analysis method. The use of sentence\ntransformers as fundamental building blocks of analyses in Chapters 4 and Chapter 8\n(Section 8.6) are some of the initial attempts of this thesis toward scalability (in Sec-\ntion 2.2.1.4 we elaborate how sentence transformers provide computational efficiency).\nLater, in Chapter 9 we empower sentence transformers with stance awareness for the task\nof computationally-efficient opinion mining which is our biggest step toward the scalability\nof modeling online polarization. Next to the use of sentence transformers, the unsuper-\nvised WEAT-based building block of the sexism detection model proposed in Chapter 6\nis another effective idea to reduce the supervision/annotation workload for a generalized\npolarization detection model, also contributing to scalability.\nGeneralizability : There is a rapid everyday increase both in the variety of online social\nmedia platforms and in the variety of polarizing topics. Each platform has its own style\nand culture for content generation and each controversial topic has its separate use of\nterms and discourse. This variety requires models that can generalize effectively across\n1https://www3.weforum.org/docs/WEF_The_Global_Risks_Report_2024.pdf\n1.1 Motivation & Goals 3\ndiverse data sources. Case-study styles of models, such as NLP models that rely on\npredefined sets of topic-specific keywords and hashtags generated in specific platforms,\nfail to generalize to new topics. Throughout the development of every measurement\napproach in this thesis, the generalizability of the approaches has been one of our main\npriorities.\nSpecifically, the use of transformer architecture in Chapter 4 for modeling the ideol-\nogy of users removes the reliance on keyword-based topic-dependent approaches. For\ninstance, some previous approaches have looked into the use of left- (right-) leaning hash-\ntags (e.g. #AbortionIsHealthcare and #AbortionIsMurder) to decide whether a user is\na Democrat or Republican. Some others have looked into the affiliation of references a\nuser has made to certain political sources to infer their ideology. The generalizability of\nthese measurements is, of course, questionable as politically charged keywords, hashtags,\nand sources vary time-wise, topic-wise, and platform-wise. Our approach does not suffer\nfrom the same limitations as the entire context of user-generated content is embedded\nby transformers in an unsupervised fashion. Moreover, in Chapter 6 we explain how by\nchanging the target words, the sexism detection model can be generalized to measure\nother sorts of polarization in a corpus. Importantly, none of the approaches in any chap-\nter of this thesis are confined to case studies. As we elaborate throughout the chapters,\nour proposed approaches are replicable for various sorts of research questions in the same\ndomain.\nHolism andGranularity : Comprehensive polarization measurement is essential for en-\nabling moderators and policymakers to track shifts in polarization levels over time, facil-\nitating timely interventions when polarization intensifies. High-level measurements allow\nthese stakeholders to understand the broader trends and adapt their strategies accord-\ningly. However, because social issues are inherently complex, a granular analysis is equally\nnecessary to expose the nuanced dynamics, contributing factors, and underlying causes\nthat drive polarization. Chapters 4 and 6 address this by providing holistic assessments of\npolarization across different domains and topics, establishing a baseline understanding. In\ncontrast, Chapter 5 explores polarization at a finer level through LLM-aided annotations,\ninvestigating the details of cross-partisan interactions to reveal the specific conditions and\nconversational patterns that drive polarization. By combining both holistic and granular\nperspectives, this work aims to provide a fuller, more actionable view of polarization for\na range of applications in policy and moderation.\nAvailability of Data is a critical challenge in computational social science, as increas-\ningly restrictive privacy policies on social media limit access to user data. Many traditional\nmethods, which rely on private or proprietary data, have become less viable due to these\nchanges. For example, polarization and echo chamber quantification methods proposed\nby Barbera et al. and Garimella et al. heavily rely on Twitter follow networks [15, 18, 19].\nHowever, since Elon Musk\u2019s restructuring of Twitter, users\u2019 follower and following lists\n4 Introduction\nare no longer fully accessible, posing a significant obstacle to this type of analysis. This\nthesis addresses such limitations by developing models that work effectively with publicly\navailable datasets only, relying solely on openly shared user-generated text data (e.g.,\ntweets, and Reddit comments) across all chapters. This approach provides a sustainable\nsolution to polarization analysis that is adaptable even as privacy policies continue to\nevolve.\nFurthermore, next to proposing measurements that satisfy the mentioned require-\nments, the identification of contributing factors to the generation of polarized and\nradical content is another motivation of this work. Chapter 7 partially addresses this\ncuriosity by investigating the role of social media platform in generation of toxic content.\nIn summary, this thesis aims to address critical gaps in polarization measurement on\nsocial media by developing scalable, generalizable, and holistic approaches that prioritize\npublicly available data and enable detailed, nuanced analysis. By balancing broad insights\nwith granular details, these models provide a foundation for understanding and mitigating\nonline polarization under ever-changing platform conditions and privacy regulations. This\nwork not only provides robust measurement tools but also contributes insights into the\nunderlying factors that drive polarization across different social platforms.\n1.2. Research Scope\nThis thesis is organized into three main parts: the first focuses on quantifying echo\nchambers which is one of the fundamental causes of online polarization. The second part\ndiscusses cases where extreme polarization leads to radicalization. The third part of the\nthesis addresses the limitations within language models concerning polarization detection\nand enhances them to perform better in the task.\n1.2.1. Polarization and Echo Chambers\nIn Part I, we explore the mechanisms through which echo chambers contribute to\npolarization, particularly in online discussions. We developed a novel method based on\nsentence transformers for measuring the degree of echo chamber effect and polarization\nin different online topics (Chapter 4). This method is both computationally efficient and\nunsupervised, allowing for scalable analysis of large datasets across multiple topics.\nOur findings reveal important insights for the computational social science commu-\nnity. We observed asymmetries in discourse diversity between political stances, where\nDemocratic-leaning users exhibited greater discourse diversity than Republican-leaning\nusers. This aligns with prior studies suggesting that right-wing online communities tend\nto be more ideologically homogenous. Moreover, we inferred that the \u201c War on Ukraine \u201d\ntopic, as a case of foreign conflict, is less polarized than other US domestic controversial\ntopics in the analysis; namely \u201c gun control \u201d and \u201c Roe v. Wade \u201d .\n1.2 Research Scope 5\nIn Chapter 5, we expand on the investigation by examining the content of Cross-\nPartisan Interactions (CPI). A key question we investigate is whether the more diverse\ndiscourse seen among Democratic-leaning users leads to more productive conversations\nacross ideological divides. Using LLMs as annotation tools, we analyzed the sentiments\nand stances in both partisan and cross-partisan interactions. Interestingly, although\nDemocrats are more likely than Republicans to engage in cross-partisan discussions, their\ninteractions with opposing viewpoints tend to be more negative. This suggests that de-\nspite higher rates of engagement across party lines, CPI may not necessarily foster more\nconstructive dialogue. Our findings highlight the need for more research into the nature\nand quality of cross-partisan interactions in highly polarized environments.\n1.2.2. Radicalization\nIn Part II, we turn to the more extreme end of polarization, focusing on radicalization\nin online communities. As polarization intensifies, it can lead to the development of radical\nviews and even hostile behavior. In Chapter 6, we examine gender-based polarization\nand toxicity, introducing a method that combines the unsupervised Word Embedding\nAssociation Test (WEAT) with semi-supervised text classification. This methodology\nwas applied to online communities such as r/TheRedPill and r/MGTOW, where we found\nsignificant gender-based toxicity directed towards women. Notably, we also observed that\na women-only dating forum, r/FemaleDatingStrategy, exhibited toxicity toward both men\nand women, highlighting that polarization and hostility can manifest in unexpected ways\nacross gender lines.\nWe also explore the role of different social media platforms in the generation of toxic\ncontent (Chapter 7). Our findings suggest that chat-based platforms like Discord may\nbe more conducive to the growth of toxic content compared to post-based platforms like\nReddit, due to the nature of real-time interactions and less stringent moderation policies.\nThis work underscores the importance of platform design in shaping the dynamics of\nonline radicalization.\n1.2.3. Biases in Language Models and Stance-Aware NLP\nIn Part III, we examine the intersection of language models and polarization, focusing\non both the biases embedded within these models and their potential utility for detecting\npolarization in online debates.\nIn Chapter 8, we explore the sociopolitical and economic biases of Large Language\nModel (LLM)s, such as ChatGPT, when handling controversial topics. We observe a\nnotable moderation policy in ChatGPT\u2019s responses, particularly in its economic stances,\nwhere it leaned toward a centrist perspective. On sociopolitical issues, however, the model\nexhibits a slight libertarian bias. Furthermore, we compare ChatGPT\u2019s performance with\n6 Introduction\nhumans on controversial topics and found that, except for philosophical domains, the\nmodel performs comparably to human experts. These findings are critical for under-\nstanding the potential influence of LLMs on public discourse and the ways in which their\ninherent biases may shape conversations on contentious issues.\nA significant limitation of existing language models is their inability to detect stance\ndifferences in topically similar but oppositional statements. Both LLM-based vectorizers\nand sentence transformers typically convert such statements into spatially similar em-\nbeddings, despite their starkly contrasting stances. To address this, in Chapter 9, we\nfine-tune a pretrained sentence transformer to be stance-aware. This allows for the dif-\nferentiation of opposing stances on similar topics by creating embeddings that reflect the\nstance, rather than just the topic. We demonstrate how this stance-aware model can\nbe applied to social network analysis, enabling computational social scientists to detect\nusers\u2019 stances on controversial issues more effectively and efficiently.\n1.3. Thesis Contribution\nThis thesis makes methodological contributions to the study of polarization, radical-\nization, and bias in online platforms. As a byproduct of applying the methods, we also\nderive insightful sociopolitical findings from the results of the applications. Method-\nologically, we introduce novel approaches for leveraging sentence transformers, LLMs, and\nstance-aware models in the field of computational social science. Furthermore, we apply\nthese tools to real-world data, shedding light on how polarization develops, how radical-\nization spreads, and how language models themselves may play a role in these dynamics.\nThese contributions are particularly timely, given the increasing societal and political im-\npact of online interactions and the ever-growing use of NLP technologies in moderating\nand shaping these interactions.\nWe can summarize a breakdown of the specific contributions as follows:\nQuantification of Echo Chambers using Transformer Models (Chap-\nter 4): In the first part of this thesis, a novel transformer-based metric is introduced\nto quantify the echo chamber effect in online communities. This unsupervised, com-\nputationally efficient model incorporates user diversity and separability to measure\npolarization, and is applied to multiple controversial topics, offering insights into\nthe relationship between echo chambers and polarization.\nAnalysis of Cross-Partisan Interactions (Chapter 5): This thesis also\ninvestigates cross-partisan interactions on social media, focusing on the conditions\nunder which these interactions lead to either healthy dialogue or toxic discourse. By\nidentifying the factors that foster productive cross-partisan exchanges, this research\nhighlights potential avenues for reducing polarization in digital spaces.\n1.4 Ethical Considerations 7\nHolistic Indicator of Online Sexism (Chapter 6): A new model is pro-\nposed for measuring online sexism in gender discourse communities, which combines\nsupervised NLP methods for detecting toxicity with unsupervised techniques to au-\ntomatically identify the targets of harmful speech. This approach provides a flexible\nframework that can be extended to measure other forms of polarization beyond sex-\nism.\nCross-Platform Comparison of Toxicity (Chapter 7): In Chapter 7,\na detailed comparative analysis of toxicity across different platforms (e.g., Reddit\nand Discord) is conducted. This research reveals that platform-specific cultures and\nmoderation practices significantly influence the level and type of toxicity, providing\npractical recommendations for improving platform moderation.\nBias in Large Language Models (Chapter 8): The thesis further explores\nbiases in large language models such as ChatGPT when dealing with controver-\nsial topics (Chapter 8). It compares AI-generated answers with human-generated\nresponses, uncovering socio-political and economic biases and suggesting ways to\nimprove LLM moderation.\nStance-Aware Sentence Transformers for Opinion Mining (Chap-\nter 9): In the final part, we enhance sentence transformers to recognize opposing\nstances in online debates is introduced (Chapter 9). This technique significantly im-\nproves opinion-mining tasks, making it a valuable tool for detecting and analyzing\npolarized stances in social media discourse.\nTogether, these contributions represent a comprehensive approach to understanding\nand addressing online polarization, radicalization, and bias through state-of-the-art NLP\ntechniques. By advancing both theoretical understanding and practical tools, this thesis\naims to provide a solid foundation for future research in this critical area of computational\nsocial science.\n1.4. Ethical Considerations\nWe complied with academic ethical standards in this thesis to ensure the protection\nof individual privacy and responsible handling of data.\nSome of the data utilized in this thesis comes primarily from publicly available sources,\nsuch as social media platforms. This includes data of subreddits\u2019 posts in Chapter 6,\nTwitter data in Chapters 5 and 4, and media bias datasets (AllSides2and MediaBias-\nFactCheck3) used in Chapters 4 and 8.\n2https://www.allsides.com/media-bias\n3https://mediabiasfactcheck.com/\n8 Introduction\nHowever, there were also social media datasets that were scraped with the help of\nour collaborators using common scraping tools and libraries. This includes the dataset\nof Kialo4platform that was used extensively in Chapters 8 and 9 and Discord dataset\nin Chapter 7 was collected by our collaborators in the University of Surrey, UK with\nthe approval of their Institutional Review Board (IRB), ensuring that the study met the\nnecessary ethical guidelines.\nMoreover, we excluded any personally identifiable information from the data we pro-\ncessed. We also extended this exclusion to the analysis phase. None of the analyses\nthroughout the thesis focuses on any of the personal level information of the users. In-\nstead, the analyses make use of the generated texts to either make a holistic inference of\nthe corpus (e.g. Chapter 6) or for training NLP models (i.e. Chapters 8 and 9).\nFurthermore, we do not publicly share the raw data we collected to protect user\nprivacy. However, anonymized datasets and our methods will be made available solely for\nresearch purposes upon request, allowing other researchers to replicate our findings while\nmaintaining ethical standards.\nThe ultimate goal of this research is to assist social scientists, online moderators,\nand policymakers in understanding and mitigating online polarization and radicalization.\nBy providing a quantified analysis of the echo chamber effect and other forms of online\npolarization, we hope to contribute to the development of more informed and effective\ninterventions. Our tools and methodologies will be open-sourced to facilitate further\nresearch in this area while maintaining the privacy and anonymity of individuals.\n4https://www.kialo.com/\n2Preliminaries\nThis chapter presents the essential concepts and core methods that form the basis of\nthis thesis. We begin by examining key social network theories, covering topics such as\npolarization, radicalization, and echo chambers. Next, we explore the primary Natural\nLanguage Processing (NLP) techniques utilized in our study, including methods for cre-\nating, analyzing, and categorizing text representations, as well as strategies for adapting\npre-trained models. Lastly, we outline the datasets used for our empirical research, which\nare sourced from three social media platforms: Twitter, Reddit, and Kialo.\n2.1. Social Networks Concepts\nIn this section, we introduce the core social network concepts that underpin the anal-\nysis of online behavior and communication dynamics in this thesis. These concepts,\nincluding polarization, radicalization, and echo chambers, are central to understanding\nhow social media platforms influence user interactions and contribute to the dissemination\nof extremist ideologies or divisive viewpoints.\n2.1.1. Polarization\nThe term polarization refers to the growth of ideological separation in a community\n[20]; an effect which can also manifest itself in online environments (e.g. Twitter) [21].\nIt is argued by the previous literature that the algorithmic curation of social networks,\nnamely its recommender system, may exacerbate the intensity of ideological polarization.\nThe main supportive argument for that is that, on every topic of interest, the users on\nsocial media are more likely to be exposed to their own beliefs rather than the alternative\nnarrative [15].\n9\n10 Preliminaries\n2.1.2. Radicalization\nRadicalization refers to the growing tendencies of actors in adopting extreme political,\nsocial, or religious ideologies, often leading to justifications for violence or other forms of\nextremism. In online environments, the same echo chamber effect that causes polarization,\nmay also cause radicalization in extreme scenarios. Social media platforms serve as both\necho chambers and recruitment grounds for extremist movements, allowing radical ideas\nto spread quickly and unchecked.\n2.1.3. Echo Chambers\nEcho Chambers play a significant role in both fueling and reflecting the increasing\npolarization of political discourse around the world. An echo chamber refers to a setting\nin which users repeatedly engage with others who share similar views and attitudes,\nleading to the continuous reinforcement of their ideas [22, 12].\nSocial media platforms provide an ideal environment for these repeated interactions,\nwhich contribute to the formation of echo chambers [11]. Furthermore, the personalized\ncontent delivery systems employed by social media platforms often expose users primarily\nto information that aligns with their preexisting beliefs [23, 24]. This phenomenon, linked\nto confirmation bias [25], results in users receiving content that validates their views, while\nselective exposure [26] shields them from encountering differing opinions.\nA key factor behind the development of echo chambers on social media is the inter-\naction between algorithm-driven content recommendations and user-driven sharing [21].\nWhile algorithms significantly influence what users see, content curation by users, through\nsharing and reposting, amplifies specific viewpoints and reinforces existing biases. This\nfeedback loop further isolates users within their ideological bubbles, making it increas-\ningly important to understand these mechanisms and explore strategies for fostering more\ndiverse and inclusive online discussions.\nEcho chambers can limit the exchange of diverse ideas, stifling open dialogue and\ncritical thinking [27]. By restricting exposure to opposing viewpoints, they create en-\nvironments of intolerance, where individuals become more rigid in their beliefs and less\nwilling to consider alternative perspectives [28]. This intellectual isolation undermines\nthe development of critical thinking skills and reduces the capacity for constructive en-\ngagement in debate.\nMoreover, echo chambers exacerbate the spread of misinformation, posing a threat\nto public discourse and informed decision-making. In these closed networks, false or\nmisleading information can spread unchecked, gaining credibility without being chal-\nlenged [29, 30, 31, 32, 33, 34]. This can have serious real-world consequences, influencing\nindividual behavior and decision-making in harmful ways. The COVID-19 pandemic\nhighlighted the dangers of echo chambers, as misinformation about vaccines and health\n2.2 Natural Language Processing Concepts 11\nprecautions fueled public distrust in government and mainstream media efforts to manage\nthe crisis [35].\n2.2. Natural Language Processing Concepts\nIn this section, we outline key NLP techniques essential for understanding the compu-\ntational models and methods used throughout this thesis. We begin with the techniques\nfor generating text representations, followed by methods for processing these representa-\ntions, and then cover classification tasks, with a focus on toxicity detection and prompt\nengineering. Finally, we discuss advanced techniques for fine-tuning models using con-\ntrastive learning and parameter-efficient approaches.\n2.2.1. Generating Text Representations\nText representations are critical for transforming raw text data into numerical formats\nthat machine learning models can process. In the following, we briefly introduce several\nstandard methods for generating these representations.\n2.2.1.1. Bag of Words (BoW)\nThe Bag of Words (BoW) model is a simple yet foundational technique in NLP. In\nBoW, a text is represented as a set of word counts, where each word\u2019s frequency in the\ndocument serves as a feature. This method disregards word order and semantics, making\nit less suitable for tasks that require contextual understanding.\n2.2.1.2. TF-IDF\nTerm Frequency Inverse Document Frequency (TF-IDF) improves upon BoW by tak-\ning into account the rarity of words across the corpus. While common words like \u201c the\u201d or\n\u201cand\u201d are down-weighted, terms that are frequent in a specific document but rare in the\noverall corpus receive higher importance. This method provides a better representation\nof a document\u2019s unique content but still lacks contextual sensitivity.\n2.2.1.3. Word Embeddings\nWhen inputting the words/token of a piece of text to a neural network, we need\na numerical representation of each word to make it understandable to the model. An\ninefficient approach is to map each token to a sparse n-dimensional one-hot encoded\nvector where each of the nelements represents a token in a dictionary of nnumber of\ntokens. However, the problem with this type of representing the words/token is that it\nutterly lacks any understanding of the semantic relationships between the words. For\ninstance, the words \u201c dog\u201d and \u201c cat\u201d are as irrelevant to each other as the words \u201c dog\u201d\n12 Preliminaries\nand \u201c desk\u201d, although the former pairs are more relevant as they are both representing a\ncertain animal.\nWord embedding algorithms are the common solutions to this limitation as they pro-\nduce dense vectors for a token in which semantic relevance of the tokens are preserved.\nA classic example is that in word embedding vectors: \u20d7man\u2212\u20d7woman = \u20d7king\u2212\u20d7queen\nThere are two common techniques used for generating word embeddings. Continuous\nBag of Words (CBOW) creates self-supervised task of predicting a word in an n-gram\nusing the words surrounding it (Figure 2.1a) while in SkipGram the task is to predict the\nsurrounding words (Figure 2.1b). After full training on a corpus, the embedding for each\nword is retrieved from the hidden state of the neural network used for the training.\n(a) CBOW\n (b) SkipGram\nFigure 2.1: Word embedding architectures [7]\n2.2.1.4. Sentence Transformers\nSentence Transformers extend the vectoral representations to short pieces of texts\nwhere similar (dissimilar) pieces of text have spatially close (far) vectors. This makes\nthem a very computationally efficient tool for tasks such as text-clustering, semantic\nsearch, retrieval augmented generation (RAG) for LLMs, etc.\nIn Chapters 8, 4, and 9 we employ a state-of-the-art1pretrained sentence transformer\nmodel called ( all-mpnet-base-v2 )2from Hugging Face.3This model is designed to con-\nvert sentences and brief paragraphs into 768-dimensional dense vectors, preserving the\n1https://www.sbert.net/docs/pretrained_models.html\n2https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n3https://huggingface.co/\n2.2 Natural Language Processing Concepts 13\nsemantic essence of the text. Having only 111M parameters and a light-weight of 420MB,\nmake it a computationally efficient model for large-scale computational social science\ntasks compared to alternative LLM-based text embedders.4\nYet, all sentence transformers suffer from a fundamental limitation: If two statements\nare topically similar, but stance-wise opposite, sentence transformers will still convert\nthem into spatially close vectors. For instance, the controversial statements: \u201c I love\npineapple on pizza \u201d and \u201c I hate pineapple on pizza \u201d would be understood as similar\nstatements by the model. We will address this limitation in Chapter 9.\n2.2.2. Analyzing Text Representations\nAfter transforming textual data into numerical representations, the next step is to\nprocess and analyze them for downstream analysis. In this section, we discuss several\ntechniques that we specifically use in different chapters of this thesis to analyze textual\nrepresentations with the aim of answering social computing questions.\n2.2.2.1. Word Embedding Association Test (WEAT)\nThe Implicit Association Test (IAT), introduced by Greenwald et al. [36], was devel-\noped as a tool to measure implicit biases in individuals. The test assesses how quickly\nusers associate certain concepts (e.g., racial groups, gender) with evaluations (e.g., good,\nbad) or stereotypes (e.g., athletic, clumsy) by analyzing the speed of word categorization.\nFaster associations are interpreted as stronger implicit biases.\nInspired by the IAT, Caliskan et al. [37] adapted this concept to the field of Natu-\nral Language Processing, introducing the Word Embedding Association Test (WEAT).\nWEAT utilizes word embeddings to detect implicit associations in large text corpora,\nmeasuring the relative distances between the vector representations of attribute concepts\n(e.g., male, female) and target terms (e.g., science, art). For example, WEAT demon-\nstrated that science-related words tend to be closer in vector space to words like \u20d7 man,\nwhereas art-related words show greater similarity to \u20d7 woman. This method aligns with\nthe psychological insights from IAT, providing a way to reveal unconscious biases present\nin language models.\n2.2.2.2. BERTopic: UMAP and HDBSCAN Pipeline\nBERTopic represents the current state-of-the-art [38] approach for text clustering and\ntopic modeling. This pipeline consists of three key stages:\nText-to-Vector Conversion : Texts are transformed into vectors using sentence\ntransformer models, as discussed in Section 2.2.1.4.\n4huggingface.co/spaces/mteb/leaderboard\n14 Preliminaries\nDimensionality Reduction : Since sentence transformer models produce large\nvectors (e.g., 768 dimensions for all-mpnet-base-v2 ), reducing dimensionality helps\nmitigate the curse of dimensionality and enhances clustering performance [39, 40].\nUniform Manifold Approximation and Projection (UMAP), a cutting-edge [41]\ndimensionality reduction technique, projects high-dimensional embeddings into a\nlower-dimensional space while maintaining the local structure of the data, enabling\nmore efficient clustering.\nClustering : Hierarchical Density-Based Spatial Clustering of Applications with\nNoise (HDBSCAN) is a state-of-the-art [42] clustering algorithm that identifies clus-\nters of varying density in the UMAP-reduced space. It is noise-resistant and excels\nat discovering meaningful clusters where traditional methods like K-Means may\nfalter.\n2.2.3. Text Classification\nText classification is a fundamental task in NLP, where the goal is to categorize text\ninto predefined categories. Throughout this thesis, at some points, we end up performing\ntwo text classification tasks which we briefly introduce here:\n2.2.3.1. Toxicity Detection\nToxicity detection is crucial for identifying harmful content in online discussions. One\npopular tool for this task is Google\u2019s Perspective API5, which we use in Chapter 7.\nThe Perspective API leverages machine learning to analyze text and categorize it into\ndifferent levels and types of toxicity. The categories are \u201c toxic,\u201d \u201csevere toxic ,\u201d \u201cinsult ,\u201d\n\u201cprofanity ,\u201d \u201cthreat ,\u201d and \u201c identity attack . \u201d This system helps in flagging and moderating\nharmful language by assigning toxicity scores to the text, making it a widely used tool in\ncontent moderation.\n2.2.3.2. LLM-Aided Annotation\nLarge Language Model (LLM)s have recently demonstrated significant potential in\nautomating various text annotation tasks such as stance detection [43], sentiment analysis ,\nhate speech ,bot detection [44], and identifying the sociopolitical affiliation of texts [4].\nGiven a suitable prompt-engineered query, LLMs are capable of understanding context,\nnuance, and implicit meanings in text, which makes them well-suited for these annotation\ntasks.\nTable 2.1 shows a classic sample of an engineered prompt fed to an LLM for sentiment\nanalysis.\n5https://perspectiveapi.com/\n2.2 Natural Language Processing Concepts 15\nTable 2.1: Sample sentiment analysis task performed by ChatGPT-4o.\nPrompt Classify the sentiment of the following statement as \u201cpositive\u201d, \u201cnega-\ntive\u201d, or \u201cneutral\u201d .\nStatement: \u201cThe weather is good\u201d\nSentiment:\nExplanation:\nResponse\nChatGPT-4oSentiment: positive\nExplanation: The statement \u201cThe weather is good\u201d expresses a favorable\nopinion about the weather, which indicates a positive sentiment. The\nword \u201c good\u201d implies a positive evaluation.\nIn Chapter 8, we use LLMs to annotate sociopolitical and economic stances of con-\ntroversial statements. In Chapter 5, we use LLMs to annotate sentiments and stances of\ncertain tweets. We elaborate on each task in their corresponding sections.\n2.2.4. Fine-tuning Essentials\nIn certain scenarios such as Chapter 9, the pre-trained Artificial Intelligence (AI)\nmodel that is designed for generic tasks, does not provide desirable outputs for certain\ndownstream tasks. In such cases fine-tuning pre-trained models on domain-specific data\nis crucial for adapting them to specific tasks. In this section, we explore some fine-tuning\nconcepts that are essential to understanding our methodology in Chapter 9.\n2.2.4.1. Contrastive Learning with Siamese and Triplet Networks\nSiamese and Triplet networks are popular architectures in contrastive learning, com-\nmonly used to distinguish between similar and dissimilar images.\nSiamese Networks: A Siamese network [45] consists of two identical neural networks\nthat share the same parameters and weights. Given two input images, the network com-\nputes the embeddings f(x1) andf(x2). The goal is to minimize the distance between\nembeddings for similar images while maximizing the distance for dissimilar ones. The\ncontrastive loss function for a pair of images ( x1,x2) is defined as:\nLcontrastive = (1\u2212y)\u00b71\n2\u00b7D2+y\u00b71\n2\u00b7max(0,m\u2212D)2(2.1)\nwhereD=\u2225f(x1)\u2212f(x2)\u22252is the Euclidean distance between the image embeddings,\ny\u2208{0,1}is the label indicating whether the images are similar (1) or dissimilar (0), and\nmis a margin parameter.\nFigure 2.2a illustrates the architecture of a Siamese network, where two images are\npassed through identical networks, CNNs in this case, and their embeddings are compared.\nTriplet Networks: Triplet networks extend the idea of Siamese networks by introducing\ntriplets of sample inputs: an anchor xa, a positive sample xp(similar to the anchor), and\na negative sample xn(dissimilar to the anchor). The triplet loss [46] encourages the\n16 Preliminaries\ndistance between the anchor and the positive to be smaller than the distance between the\nanchor and the negative by at least a margin m. The triplet loss is defined as:\nLtriplet = max(0,\u2225f(xa)\u2212f(xp)\u22252\n2\u2212\u2225f(xa)\u2212f(xn)\u22252\n2+m) (2.2)\nThis loss function ensures that similar samples, similar images for instance, are embed-\nded closer together, while dissimilar images are kept further apart. Figure 2.2b provides\na visual representation of the Triplet network, showing the anchor (Jake Gyllenhaal\u2019s\nface 1), positive (Jake Gyllenhaal\u2019s face 2), and negative (Herman Eriksen) images being\nprocessed by shared networks and their embeddings compared.\nBoth architectures effectively enable the model to learn discriminative features that\nseparate similar images from dissimilar ones. By optimizing their respective loss functions,\nthe networks are capable of generating embeddings that are meaningful in the context of\nthe given tasks.\n(a) Siamese Networks [47]\n (b) Triplet Networks [48]\nFigure 2.2: Contrastive Learning Architectures\nIn Chapter 9 we utilize Siamese and Triplet architectures to add stance awareness to\nsentence transformers.\n2.2.4.2. Parameter-Efficient Fine-Tuning with LoRA\nParameter Efficient Fine-Tuning (PEFT) is an approach designed to fine-tune LLMs\nwithout updating all model parameters. This is particularly useful for adapting large\nmodels to specific tasks without incurring the high computational and storage costs as-\nsociated with full fine-tuning. Instead of updating the entire network, PEFT focuses on\nfine-tuning only a small subset of parameters, leading to a much more efficient process.\nLow-Rank Adaptation (LoRA): LoRA [49] is a specific PEFT technique that injects\ntrainable low-rank matrices into the transformer layers of a pre-trained model. In LoRA,\nthe pre-trained model\u2019s original weights are frozen, and low-rank decomposition matri-\nces are added to the query and value projections. These matrices are optimized during\nfine-tuning, enabling adaptation to downstream tasks with far fewer parameters. Math-\n2.2 Natural Language Processing Concepts 17\nematically, LoRA modifies the original weight matrix W0\u2208Rd\u00d7dby adding a low-rank\ndecomposition:\nW=W0+A\u00b7B (2.3)\nwhereA\u2208Rd\u00d7randB\u2208Rr\u00d7d. Also, the scheme of LoRA architecture is portrayed in\nFigure 2.3. This mechanism allows LoRA to introduce only a small number of additional\nparameters, drastically reducing the overall computational cost of fine-tuning.\nRank (r) is the hyperparameter that controls the amount of reduction in the number\nof trainable parameters. For a trainable matrix of size d\u00d7d, the number of trainable\nparameters will be reduced from d\u00d7dto 2\u00d7(r\u00d7d). The significance of this reduction will\nbe more salient for large matrices. For instance, the number of trainable parameters for a\n1000\u00d71000 layer, setting r= 10 will be reduced to 2 \u00d7(10\u00d71000) which will be only 2%\nof the original matrix. Expectedly, the lower number of rincreases computational speed\nat the expense of some accuracy.\nFigure 2.3: LoRA\u2019s architecture [8]\nNext to the computational efficiency, another blessing that comes with PEFT methods\nsuch as LoRA is reducing the risk of catastrophic forgetting . Catastrophic forgetting refers\nto cases where after fine-tuning, as a result of over-training on the newer task, the model\nforgets its ability to perform the older task it was initially trained to do [50]. For example,\nan LLM that is finetuned extensively for offering health advice, might forget the generic\nknowledge it had for answering generic questions. In Chapter 9, we show how the use\nof LoRA helps us to add stance awareness to sentence transformers while maintaining a\ndescent level of their performance for their initial task of sentence similarity detection.\n\n3Literature Review\nThis chapter provides a general summarized overview of the state-of-the-art literature\nrelevant to the goals and objectives of this thesis. In the following chapters, we expand\nupon this review, focusing on more specialized aspects of each subject in the corresponding\nchapters.\nFirst, the chapter explores the definition and societal impact of Echo Chambers in\nprevious literature. Next, it evaluates existing detection techniques, highlighting their re-\nliance on supervised data and simplified views of polarization. The review then discusses\nuser-level embeddings, which model behavior and ideology computationally, essential for\ntasks such as Echo Chamber detection that involve modeling users. Approaches to identi-\nfying bias in text are also examined, emphasizing methods like word embeddings. Lastly,\nit covers text classification and stance detection, noting computational challenges in large-\nscale sentiment and opinion analysis.\nEcho Chambers and Social Implications\nThe concept of Echo Chambers, where users are exposed to ideologically homogeneous\ninformation, has been studied widely in social media research. Colleoni et al. [11] and\nBakshy et al. [24] found that Echo Chambers on Twitter correlate with stronger polar-\nization. Del Vicario et al. [29] documented the role of Echo Chambers in the spread\nof misinformation, further examined by Shu et al. [31], who noted that Echo Chambers\nreduce trust in mainstream media and heighten conspiracy beliefs. Research by Cinelli et\nal. [22] and Jiang et al. [33] extended these findings, highlighting Echo Chambers\u2019 role in\nfostering prejudice and social unrest. Overall, these works emphasize the societal impact\nof Echo Chambers, urging improved detection mechanisms.\n19\n20 Literature Review\nEcho Chamber Detection Techniques:\nThere are abundant Echo Chamber detection methods proposed based on network [51],\ncontent [52], and hybrid approaches [53]. Network-based methods utilize interaction\ngraphs, clustering users into communities based on shared interactions. Content-based\nmethods classify users based on language or sentiment analysis, as seen in work by Koc\net al. [54]. Hybrid models combine both network and content-based methods for im-\nproved accuracy [33, 55]. However, at the core of all the methods stands one idea:\nfinding the correlation between the polarity of consumption and the polarity of pro-\nduction [15, 51, 19, 14, 18]. The higher this correlation stands for every topic, the more\ndiscussion around that topic is of an echo chamber.\nThe main limitation of this core idea is its reliance on supervised and sparse data\nfor measuring polarization. Another problem is that its understanding of polarization is\nlimited to only one dimension (left-leaning vs. right-leaning) while polarization can have\nmultiple aspects in various contexts (e.g. Economic leaning, sociopolitical leaning, secular\nvs. non-secular, etc.).\nUser-Level Embeddings for Behavioral Analysis\nA primary and critical step in modeling polarization and echo chamber effect is to\nprovide an estimation of online users\u2019 ideology by embedding them based on their online\nbehavior.\nUser embeddings encode user data (profile, activity, network, and generated con-\ntent) into low-dimensional vectors, widely used in NLP tasks to model user behavior\nefficiently [56]. Latent Dirichlet Allocation (LDA) [57], CNNs [58], and Graph Neural\nNetworks (GNNs) [59] are common approaches, with applications in clustering social\nnetwork attributes [60, 61].\nAnother common practice for embedding users\u2019 ideology is reference-based approaches.\n[19, 62] average the political leanings of sources a user has followed or cited to map them\ninto a one-dimensional space of political ideology ranging from -1 (left-leaning) to +1\n(right-leaning). The annotation of sources is based on the database of AllSides1and\nMediaBiasFactCheck.2\nSuch techniques often rely heavily on annotated datasets, limiting their adaptability\nto emerging social phenomena due to concept drift [63]. Our approach leverages sentence\ntransformers for unsupervised user embedding to enhance generalizability and reduce\nmanual input requirements.\n1https://www.allsides.com/media-bias\n2https://mediabiasfactcheck.com/\n21\nSocial Bias in Text\nThe use of word embeddings in identifying language bias has gained traction, espe-\ncially for biases related to gender and ethnicity. Caliskan et al. [37] introduced the Word\nEmbedding Association Test (WEAT) to detect implicit associations in word embeddings,\nmodeled on the Implicit Association Test (IAT) by Greenwald et al. [36]. Though effec-\ntive in validating known biases, these methods are prone to cherry-picking and rely on\npredefined word sets [64] rather than discovering unknown biases [65].\nText Classification\nText classification has been extensively explored in NLP for detecting sentiment [66],\nstance [67], aggression [68], hate speech [69], and offensive language [70]. Various super-\nvised NLP models, including those from OffensEval [71], show strong performance but\nare sensitive to distribution and concept drift issues [72].\nA large portion of text classification studies are cross-tabular with the social media\ndata. For instance, demonstrating that Twitter tends to host more negative sentiment\nthan Instagram [73], and responses to events on Reddit are less emotionally charged than\non Twitter [74].\nStance Detection and Sentence Transformers\nStance detection, crucial in opinion mining, has largely relied on computationally in-\ntensive supervised NLP methods such as BERT [75] to classify the semantic relationship\nbetween a target sentence and a context sentence expressing a known stance [76]. More-\nover, as shown by Qin et al. [67], LLMs also demonstrate great potential in zero-shot\nstance detection.\nHowever, both text-classification and LLM-based approaches face a significant com-\nputational bottleneck. When analyzing the stances between multiple pieces of text, they\nrequire comparing every possible pair of sentences, which leads to a quadratic computa-\ntional complexity of/parenleftbign\n2/parenrightbigmodel calls for npieces of text.\nTable 3.1 outlines the core methods and limitations of previous work surrounding Echo\nChambers, User Embeddings, Social Bias in Text, Text Classification, Bias in Language\nModels, and previous Stance Detection approaches. These topics represent critical as-\npects of the broader discourse on social media dynamics, algorithmic biases, and Natural\nLanguage Processing (NLP) challenges.\n22 Literature Review\nTable 3.1: Overview of the methodologies of previous work related to the different topics\nof this thesis\nTopic Core Method Notable\nWorksLimitation\nEcho Chambers Correlation between po-\nlarity of consumption\nand polarity of produc-\ntion[15, 51,\n19, 14,\n18]1) Supervised 2) 1D view\nof Polarization 3) Re-\nliance on follow-network\ndata which is heavy and\nprivate\nUser Embeddings Profile-based; making\nuse of bio, avatar,\nnumber of follower/fol-\nlowings, etc.[77, 78,\n79]Profile info and behavior\nmodeling\nUser Embeddings Content-based; Text\nmodeling via LDA,\nCNN, Word-Embedding[58, 57,\n80, 81]supervised\nUser Embeddings Network-based; low-\ndimensional representa-\ntions of follow-network[59, 15] supervised, reliance on\nfollow-network\nUser Embeddings Reference-based; aver-\naging the leanings of\nsources a user had cited[19, 14,\n62]supervised, reliance\non pre-annotated\nreferences from Medi-\naBiasFactCheck and\nAllSides, the references\nare scarce per user and\ncan shift across time\n(e.g. a Democrat Twit-\nter account becomes\nRepublican), 1D view of\nPolarization (limitation\nto only right-wing and\nleft-wing)\nSocial Bias in\nTextWord Embedding Asso-\nciations[36, 37] Sets of biased concepts\nhave to be predefined\n23\nText Classifica-\ntionSupervised NLP; use\nof state-of-the-art NLP\narchitectures such as\nBERT to classify text\nin terms of sentiment,\ntoxicity, hate, etc.[68, 71,\n69, 70]Extensive Annotation,\nDetection of the target\nof the sentiment/-\ntoxicity/etc. is not\nautomatized\nBias in Language\nModelsPolitical Affiliation\nTests[82, 83] LLMs\u2019 moderation stops\nthem from providing di-\nrect answers to implic-\nitly controversial ques-\ntions\nStance Detection Supervised NLP [75, 67,\n84, 76]Fornpieces of text, they\nrequire calling the model\n/parenleftbign\n2/parenrightbigtimes\nThis chapter\u2019s aim was to provide an initial outlook on the previous literature on each\nof the studied topics in this thesis. In every upcoming chapter of this thesis, we provide\nmore detailed literature reviews corresponding to the focus of individual chapters.\n\nPart I\nPolarization and Echo Chambers\nEcho Chambers are one of the important sources of online polarization as being merely\nexposed to the opinion of like-minded users would reinforce ones own opinion. This is\nlikely to push moderate political opinions toward the two ends of the ideological spectrum\nand cause polarization.\nIn this part of the thesis, initially in Chapter 4, we introduce our new method of\nquantifying echo chambers and polarization using sentence transformers. In the next step,\nin Chapter 5, we dive deeper into the content shared within and across echo chambers.\nWe make use of LLM as an annotation tool to compare sentiments and stances in partisan\nvs. cross-partisan interactions.\n25\n\n4Transformer-BasedQuantification of the Echo\nChamber Effect\nAbstract\nOur first step in modeling online polarization is to develop tools for quantifying the\nEcho Chamber effect, which is one of the leading causes of online polarization and radi-\ncalization.\nAn Echo Chamber on social media refers to the environment where like-minded peo-\nple hear the echo of each others\u2019 voices, opinions, or beliefs, which reinforce their own.\nEcho Chambers can turn social media platforms into venues that polarize and radicalize\nusers rather than broadening their exposure to diverse information. Having a quantified\nmetric for measuring the Echo Chamber effect can aid moderators and policymakers in\ntracking and mitigating online polarization and radicalization. Existing methods for Echo\nChamber detection are either one-dimensional, only considering the network behavior of\nusers while ignoring their semantic behavior, or require demanding supervised labeling,\nwhich is both expensive and less generalizable.\nThis chapter proposes a new metric to quantify the Echo Chamber effect using Trans-\nformer models for context-sensitive processing of natural language (NLP). Our metric\nquantifies (1) the effect of an Echo Chamber through the inverse effect of user diversity ,\nand (2) polarization by means of user separability between two Echo Chambers in a topic.\nLeveraging this metric, we further propose an NLP-based embedding that represents the\nusers\u2019 activity. Our model is simultaneously effective, computationally cheap, and unsu-\npervised. We run our analysis on three recent highly controversial political topics and a\nnon-controversial topic: Russo-Ukrainian War, Abortion, Gun-Control, and SXSW mu-\nsic festival. Our results offer data-driven findings such as a higher Echo Chamber effect\namong Republicans over Democrats and diverse explicit support for Ukraine, especially\namong Democrats. We also observe a direct relationship between the Echo Chamber effect\nand polarization while observing that the low Echo Chamber effect for the Russo-Ukraine\nwar is accompanied by a low polarization; and vice versa for Gun-Control.\n27\n28 Transformer-Based Quantification of the Echo Chamber Effect\n4.1. Introduction\nOnline Echo Chambers are both the cause and the effect of the polarized political\nenvironment existing across the globe. An Echo Chamber could be thought of as an en-\nvironment where ideas are reinforced by repeated interactions between users with similar\ntendencies and attitudes [22, 12].\nSocial media platforms are fertile grounds for these polarizing repeated interactions\nthat lead to the formation of Echo Chambers [11]. In addition, users are often exposed\nonly to the content they agree with due to social media over-personalization [23, 24],\nfurther confirming their existing beliefs \u2014 see confirmation bias [25], and shielding them\nfrom exposure to the other side of the argument \u2014 see selective exposure [26].\nOne of the key drivers of Echo Chambers on social media platforms is the interplay\nbetween algorithmic-driven and human-driven curation of content [21]. While algorithms\nplay a significant role in shaping the content that users see, human curation through\nsharing and reposting also amplifies certain viewpoints and reinforces existing beliefs.\nThis dynamic can create a self-reinforcing cycle that further entrenches users in their\nown Echo Chambers. As a result, it is important to understand the mechanisms that\ncontribute to the formation of Echo Chambers and to develop strategies to promote a\nmore diverse and inclusive online discourse.\nEcho Chambers stifle the free flow of ideas, hindering the exchange of diverse perspec-\ntives and the formation of well-rounded opinions [27]. By limiting exposure to opposing\nviewpoints, Echo Chambers foster a climate of intolerance and prejudice, where individ-\nuals become increasingly entrenched in their own beliefs and less receptive to alternative\nviews [28]. This intellectual insularity can lead to a decline in critical thinking skills and\na diminished capacity to engage in constructive dialogue.\nMoreover, Echo Chambers amplify the spread of misinformation, posing a significant\nthreat to public discourse and decision-making. In these self-reinforcing environments,\nfalse or misleading information can gain traction and go unchecked [29, 30, 31, 32, 33, 34],\npotentially influencing individuals\u2019 actions and behaviors in detrimental ways. The pro-\nliferation of misinformation in Echo Chambers can undermine trust in institutions, erode\npublic confidence in democratic processes, and exacerbate social and political tensions.\nThe COVID-19 pandemic has been one of the recent critical cases in which society had\nbeen affected by Echo Chambers driven public mistrust in the vaccination and precaution\nmechanism propagated by governments and the mainstream media [35].\nIn this study, we employ an unsupervised approach to estimate the Echo Chamber\neffect. Echo Chamber effects are overly dynamic. Thus, using manually labeled data\nto measure polarization and Echo Chambers limits considerably the generalizability of\nthe study. Labeling efforts include identifying seed accounts (e.g., influencing politicians,\nusers, or news channels) [19] or establishing predefined sets of domain-specific polarized\n4.1 Introduction 29\nhashtags and keywords [80, 81, 57]. On the contrary, unsupervised methods are more\nscalable, as they do not require manual data labeling, which can be time-consuming and\nresource-intensive. Our unsupervised approach allows for increased scalability and flexi-\nbility in analyzing the Echo Chamber effect, and by not relying on manually labeled data,\nwe assist and reduce the need for collaborative efforts in crowd-sourcing data annotations.\nOur first computational step is to detect Chambers \u2014 communities \u2014 for every topic\nbased on the retweet network clusters. Then, we select a random sample of users from each\nChamber and embed the users into a vector space by averaging the sentence transformer\nembeddings of their tweets. We use the diversity of user embeddings in every Chamber\nto measure its Echo and the separability of two Chambers\u2019 users to estimate polarization\nacross Chambers.\nIn Section 4.3, we break down the concept of Echo Chamber and define \u201cEcho\u201d,\n\u201cChamber\u201d, \u201cEcho Chamber\u201d, and \u201cPolarization\u201d aligned with our computational model.\nIn Section 4.4, we show how we embed users using sentence encoders and quantify\n\u201cEcho\u201d per \u201cChamber\u201d and \u201cPolarization\u201d across \u201cChambers\u201d . In Section 4.7, we apply\nour method to three recent controversial topics and a non-controversial topic: \u201cwar on\nUkraine\u201d, \u201cAbortion Ban\u201d, \u201cUlvade school Gun Shootings\u201d, and \u201cSXSW music festival\u201d .\nWe compare the level of \u201cEcho\u201d per \u201cChamber\u201d and \u201cPolarization\u201d across \u201cChambers\u201d\nfor each topic. In summary, we make the following observations:\nThe diversity of users in Republican Chambers is lower than in Democratic\nChambers. We interpret this as a higher Echo Chamber effect in Republican stances,\nwhich is consistent with previous literature [19].\nThe diversity of pro-Ukraine users is higher than in the other controversial case\nstudies. In addition, Ukraine-related Chambers, as a case of foreign national conflict,\nhas caused the least polarization in comparison to the other topics. However, we\nalso observe that the most explicit supporters of Ukraine seem to be Democrats.\nThe use of mean-pooling in sentence-transformer encodings to generate user\nembeddings is fast and effective for distinguishing users based on their political\nstances. This has useful implications for future work leveraging user classification\ntasks.\nWe address the challenge of modeling Echo Chambers through the combination of\ncutting-edge methods in different disciplines, including the use of sentence transformers,\nnetwork analysis, and social sciences. By integrating these approaches, we bridge the gap\nbetween computational techniques and social science theories to gain a comprehensive\nunderstanding of Echo Chambers as collaborative phenomena. We hope to contribute to\nthe aim of designing technologies and interventions that support effective collaboration\nin various domains (e.g., political discourse analysis, gender studies, etc.)\n30 Transformer-Based Quantification of the Echo Chamber Effect\n4.2. Related Work\nWe have covered an overview of previous literature in Chapter 3 for the whole thesis. In\nthis section, we delve deeper into the literature on Echo Chamber detection approaches.\nwe will initially discuss the social implications of Echo Chambers and how they can\ncause online harm according to the social science literature. Then, we discuss previous\nquantitative methods of Echo Chamber detection. We also allocate a separate section to\nprevious methods of embedding users as it is a key element in our method of quantifying\nonline Echo Chambers and polarization.\n4.2.1. Echo Chamber and Social Harms\nResearch has consistently demonstrated the negative impacts of Echo Chambers on\nonline communities and society. For instance, a study by Colleoni et al. [11] found that\nusers who were exposed to ideologically homogeneous information on Twitter were more\nlikely to exhibit polarized attitudes. Similarly, Bakshy et al. [24] demonstrated that social\nmedia algorithms can exacerbate polarization by recommending content that aligns with\nusers\u2019 existing beliefs.\nThe proliferation of misinformation in echo chambers has also been documented by\na multitude of studies. Del Vicario et al. [29] found that Echo Chambers on Twitter\nplayed a significant role in the spread of misinformation about the 2016 US presidential\nelection. Similarly, Shu et al. [31] demonstrated that the consumption of misinformation\nin Echo Chambers can lead to decreased trust in mainstream media and increased belief\nin conspiracy theories.\nThe harmful effects of Echo Chambers extend beyond the realms of political polar-\nization and misinformation. A study by Cinelli et al. [22] found that Echo Chambers\non YouTube can lead to increased prejudice and discrimination against minority groups.\nSimilarly, Jiang et al. [33] demonstrated that Echo Chambers on social media can con-\ntribute to social unrest and violence.\nIn conclusion, previous research underscores the substantial threat posed by Echo\nChambers to the health and well-being of online communities and society at large. Recog-\nnizing this, the development of effective tools for detecting online Echo Chambers becomes\nparamount in fostering healthier and more inclusive digital discourse.\n4.2.2. Echo Chamber Detection\nWe could split Echo Chamber detection methods into three types: network-based [51],\ncontent-based [52], and hybrid detection methods [53]. The network-based methods\nutilize well-known community detection algorithms to detect communities in interaction\ngraphs built using social media interactions such as retweets and replies. The content-\n4.2 Related Work 31\nbased methods [54] cluster users based on the content they use by extracting features such\nas the sentiment about a topic or embedding of content. Finally, the hybrid approach [33,\n55] incorporates the knowledge from both content and topology to find Echo Chambers.\nIn this chapter, we utilize the network feature to detect communities (Chambers)\nas it is the most common method to detect Echo Chambers. Moreover, network-based\nmethods were used in related work on measuring polarization [18]. Then, we use the\ncontent generated by users to measure the Echo Chamber effect to verify if the detected\ncommunities are indeed Echo Chambers.\n4.2.3. User-level Embeddings\nUser-level embeddings are used to model the behavior of the users for various tasks.\nRecent common methods utilize neural encoders to encode the user behavioral data (e.g.,\nrecent tweets on social media or recent queries for search engines) into low-dimensional\nvectors. These approaches reduced the amount of feature engineering and manual feature\nextraction labor by automating the relations between the user\u2019s own data as well as its\nrelation to other users\u2019 data. User-specific data on social media can be divided into four\ndifferent categories: (i) user\u2019s profile information, (ii) user\u2019s activity, (iii) user\u2019s network\nconnectivity, and (iv) user\u2019s generated content. In the behavioral analysis of the users on\nsocial media, researchers utilized different conjunctions of the aforementioned categories\nfor creating task-specific as well as universal user representations [56].\nMost of the user embedding research models the user\u2019s behavior through their gen-\nerated content by utilizing models that optimize the conditional probability of the texts,\ngiven their authors. These aggregated texts per user can be modeled using different\nmethods such as Latent Dirichlet Allocation (LDA) [57], Convolutional Neural Network\n(CNN) [58], Matrix Representations [78], and Word-Embeddings [57, 80, 81]. Moreover,\nthe network connectivity of the users is also common in modeling the users\u2019 attributes.\nThese methods try to map the social networks into low-dimensional representations such\nthat the local and global topological structures are preserved [59]. Community detec-\ntion algorithms and Graph Neural Network models are among the common methods\nused to model social networks such as \u201cfriendship\u201d, \u201cretweet\u201d, and \u201cendorsement\u201d so-\ncial graphs [60, 61]. Auxiliary information such as profile information would also help in\nmodeling the user behavior and improving the methods [77, 78, 79].\nHowever, all the user-level embedding methods for Echo Chamber detection rely on\na labeled and cherry-picked set of ground-truth political users, keywords, and hashtags.\nThis would make them less robust, more demanding for manual effort, and less gener-\nalizable to later social network analysis tasks since supervised methods are vulnerable\nto concept drift [63]. In other words, as time passes, seed political celebrities, political\nhashtags, and the use of language will change.\nIn Section 4.4.2, we explain how we propose an unsupervised, computationally cheap,\n32 Transformer-Based Quantification of the Echo Chamber Effect\nand effective way of embedding users based on sentence transformers to tackle the men-\ntioned short-come.\n4.3. Terminology\nThe terms \u201cEcho Chamber\u201d and \u201cFilter Bubble\u201d are often used interchangeably in the\nliterature [85, 86] while sometimes being integrated with the concept of \u201cPolarization\u201d [87].\nAlthough there is a common core idea underlying these terms, it is hard to find prior work\nthat makes a unique, universally settled definition for each of the terms. Therefore, in\nthis section, we explicitly state the definitions we consider most relevant to our study\nfrom previous literature.\nChamber is a discussion forum where interactions occur and users share content or\nideas. In our work, a Chamber equates to an Internet forum, where users post messages\nto other members of that forum. On Twitter, we represent a Chamber as a cluster of\nusers linked by interactions (i.e., retweets, quotes, mentions, and replies) on a topic. Our\nrationale is that these clusters represent a network where users interested in a specific\ntopic get exposed to a particular discussion on Twitter. This definition is derived from\nGarimella et al. work, where they establish that a Chamber is \u201cthe social network around\nthe user, which allows the opinion to echo back to the user, as it is also shared by\nothers\u201d [62].\nEcho is the level of homogeneity among the members of a discussion in a Chamber. It\nis a common notion in the literature that online Echo Chambers happen in environments\nwith homogeneous sets of users [88, 89]. This homogeneity can stem from similarities in\nusers\u2019 political leaning (e.g., traditional left or right), socio-economic statuses, or demo-\ngraphic features (like age or gender) [90].\nEcho Chamber in our terminology is a \u201cChamber\u201d with high levels of \u201cEcho\u201d . In\nour domain this is a retweet network with low user diversity (high homogeneity). For\ninstance, if all the members of an anti-abortion Chamber are from the right wing in\npolitical opinion, we call that Chamber an \u201cEcho Chamber\u201d where like-minded people\nhear the echo of their own voice [19].\nPolarization is the extent to which the members of a Chamber formed around a\ntopic can be separated/distinguished from the members of its opposing Chamber on the\nsame topic. Similar to Garimella et al. [14], we take into account the Oxford Dictionary\ndefinition of Polarization as \u201cthe act of separating or making people separate into two\ngroups with completely opposite opinions. \u201d Let\u2019s take the case of abortion as a running\nexample. If we observe that only hard-core left-leaning users attend Chamber A (which\ncan presumably be the place where pro-abortion opinions are being shared) and only\nhard-core right-leaning users attend Chamber B (which instead can presumably be the\nplace where anti-abortion content is being shared), we would say that the topic \u201cabortion\u201d\n4.4 Methodology 33\nis polarized between Chambers A and B based on political leaning. However, if both the\npro-abortion Chamber and anti-abortion Chamber embrace diverse users from all parts of\nthe political/demographic/economic/gender spectrum, in a way that a pro-abortion user\nis hardly distinguishable from an anti-abortion one by an explicit factor, our definition\nwould label the abortion topic as less polarized.\nOur definition of polarization is also aligned with Esteban and Ray [91]. Similarly,\nwe also argue that polarization can theoretically happen by gender (i.e., mostly men\nopposing abortion rights and mostly women supporting it), age, location, political leaning,\nand any other features from users that can be automatically stored in our black-box\nuser embedding approach which we explain in Section 4.4.2. This multi-dimensionality\nof polarization in our method is particularly useful in environments where polarization\nextends beyond the traditional left-right divide; a division that is primarily defined for\nthe US as an effect of the cold war [92]. For instance, in Taiwan, polarization centers\naround attitudes towards having closer ties with the US versus having closer ties with\nChina [93], while in Western Asian countries such as Iran and Turkiye, the degree of\ndesired secularism forms the primary axis of division [94, 95].\n4.4. Methodology\nOur method returns two main measures, the Echo of every Chamber and the Polar-\nization across Chambers. Our first step is to detect the top important Chambers, for\nwhich we use the retweet network of a set of controversial topics. Our second step makes\na per-user analysis by looking at the type of content posted by the users of the detected\nChamber to embed their general stance. The final step is to utilize the user-embeddings\nto estimate the homogeneity of users (Echo) per Chamber and their polarization across\nChambers.\nFigure 4.1 shows an overview of our computational architecture.\n4.4.1. Detecting Chambers (Network Clusters)\nOur initial step is to identify Chambers.\nOur method departs from a large set of trending tweets around controversial topics.\nOur analysis focuses on three topics abortion ,gun control , and the Ukraine war selected\nfor being either well-established controversial topics (i.e., abortion and gun control) or\nrecently established topics (i.e., the Ukraine war). We also add SXSW 2022 music festival\na commonly analyzed case of a non-controversial topic [96]. However, our methodology\nis generic and can be applied to any other topic.\nOverall, we collect the retweet network of \u224820kusers for each of the topics using\nrelevant keywords explained in Section 4.6.\n34 Transformer-Based Quantification of the Echo Chamber Effect\nFigure 4.1: Scheme of our method\u2019s architecture.\nWe then create a retweet graph per topic in which the nodes represent the users, and\na link between two nodes A and B represents that user A retweeted user B. Then, we use\nthe Louvain algorithm [97] over the retweeted tweets to unfold communities into clusters.\nLouvain is known to work well with polarized communities [22, 98].\nIt is common for the retweet networks of controversial topics that the two largest\nnetwork clusters represent the main sides of the debate. To verify this, we ran a cursory\ninspection that proved most of the tweets were aligned with the partisan stances of the\nentire Chambers. We label the Chambers\u2019 stances as \u201cDemocratic\u201d or \u201cRepublican\u201d based\non the stances of tweets we observe in each Chamber.\nIt is worth noting that this only labels the political stance of the \u201ccontent\u201d in each\nChamber which is presumably either pro or against the debated topic, not the \u201cgeneral\nideology\u201d of the \u201cusers\u201d inside those Chambers. One of our main objectives is to check user\ndiversity inside each Chamber. Therefore, we expect a significant amount of moderate or\nnon-political users to appear in each of the partisan Chambers.\n4.4 Methodology 35\n4.4.2. Embedding Users\nThe next step in our analysis is to characterize Twitter users\u2019 ideology according\nto their produced content. We start by extracting the features for the 200 tweets that\nhave recently been generated by a user. After preprocessing the tweets\u2019 text (remov-\ning mentions, URLs, etc.), we represent them using a vector of embeddings. We use\nthe state-of-the-art1pretrained sentence transformer model (all-mpnet-base-v2)2from\nHugging-Face .3The model is fine-tuned to map sentences and short paragraphs to a\n768-dimensional dense vector space in a way that preserves semantic features of the text\nso that the vectors can be utilized for tasks such as clustering or semantic search. Then,\nwe represent users through the average pooling of his/her tweets\u2019 embedding vector.\nIn our methodology for user representation, we deliberately opted for state-of-the-\nart pretrained sentence transformer models like all-mpnet-base-v2 due to their adeptness\nin capturing semantic essence from individual tweets efficiently. Unlike LSTM models\napplied to concatenated tweets, which assume continuity in text sequences and might\nstruggle with discrete, independent tweets, sentence transformers excel in encoding short\ntexts without imposing such assumptions. Their transformer architecture enables effective\ncapture of semantic relationships within tweets, aligning with our goal to represent users\nbased on their varied and discrete tweet content. Specifically choosing the all-mpnet-base-\nv2 model was driven by its balance between performance and computational efficiency,\nensuring effective mapping of tweets into a 768-dimensional vector space while preserving\nsemantic features crucial for downstream tasks like clustering and semantic search, thereby\nenabling a robust user representation based on tweet content. Moreover, all-mpnet-base-\nv2 is open-source and downloadable for offline use. When it comes to large-scale use, this\nmakes it a more practical option than the recently developed advanced LLMs that require\npaid plans for using their APIs at limited rates.\n4.4.3. Quantifying Echo\nWe quantify the Echo of every Chamber by the inverted effect of the variance among\nuser-embeddings of all members in a Chamber:\necho =1\n/hatwidestVar(U). (4.1)\nThis quantification captures the level of homogeneity among the members of a Chamber,\nwhich is aligned with the definition of \u201cEcho\u201d in Section 4.3. Thus, a lower variance of\nusers indicates a higher \u201cEcho\u201d .\n1https://www.sbert.net/docs/pretrained_models.html\n2https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n3https://huggingface.co/\n36 Transformer-Based Quantification of the Echo Chamber Effect\nWe compute the variances across 768-dimensional vectors representing user embed-\ndings. This involves assessing the variability present in each dimension of the user em-\nbeddings, capturing the multidimensional nature of the data. Specifically, we calculate\nthe echo by averaging the variances observed across all elements within these vectors.\nThis comprehensive approach ensures that the echo metric accurately reflects the level\nof homogeneity or consistency among users across all dimensions represented in the data\nspace.\n4.4.4. Quantifying Polarization\nIn addition to the variety of users in every Chamber, we are interested in quantifying\nthe polarization of users across pairs of selected Chambers formed on a topic. We begin\nby measuring the level of linear separability among user embeddings of pairs of Chambers.\nTo this end, we train a linear SVM classifier with the user embeddings (cf. Section 4.4.2)\nas features and the Chamber that the users belong to as the labels. We also apply a\nsimilar pipeline with hashtags as labels.\nNote that our goal differs from the classical usage of a prediction task and we do not\naim at classifying users based on the Chamber they belong to. Instead, we intend to\ndeduce which pair of Chambers have the highest level of separation among their users\njudging by the performance of multiple pairwise classification tasks. Thus, it is critical\nto have a consistent set of elements for all classification experiments, including the pa-\nrameters and sample size. Therefore, we take equal random samples of users (1,500) per\nChambers/hashtag, and split one half to train and the other half to test the model. We\ntake the accuracy of the test set as the final indicator of linear separability among users.\nWe chose a Linear SVM due to its inherent use of hyperplanes to split data points.\nOur rationale is that stances are in a continuous spectrum. For instance, when it comes to\npolitical leanings, a user can stand in the alt-left, the alt-right, or somewhere in between.\nTherefore we expect a line/hyperplane to be able to clearly split users based on this\nspectrum in cases of strong polarization. The accuracy of the SVM classifier would\nindicate the separability of the users.\nIn addition to reporting classification accuracy, we also report the weighted average\nof the model\u2019s confidence for each data point in the classification. This supplementary\nmetric is to take into account the difference between pairs of points that are closer to\nthe separating hyperplane (less polarized) and those that are farther from the hyperplane\n(more polarized). The confidence score provided for each data point indicates how far the\ndata point is from the SVM decision boundary.\nThen, the weighted average of confidence scores is computed as in Equation 4.2 while\nsetting weights to 1 for correct predictions and \u22121 for incorrect ones.\n4.5 Evaluation 37\nAverage Confidence =/summationtextn\ni=1confidence i\u00b7weightsi/summationtextn\ni=1|weightsi|weights =\uf8f1\n\uf8f2\n\uf8f31 if \u02c6y=y\n\u22121 if \u02c6y\u0338=y\n(4.2)\n4.5. Evaluation\nWe evaluate our metric on a dataset of tweets from congresspeople4and senators\nlabeled as Republican or Democrat. The users in this analysis are the ground-truth for\na set of users who are separated by their political views. Our evaluation measures our\nmodel\u2019s capability to separate them.\nWe sample 200 tweets per user and embed them by the average of their tweets\u2019 embed-\ndings as introduced in Section 4.4.2. We use UMAP [41] to visualize the 768-dimensional\nuser embeddings into 2D space. UMAP is one of the state-of-the-art dimensionality re-\nduction algorithms at the time of writing [99]. Figure 4.2 shows the political affiliations\ncolor-coded. We see that most points are well-separable by a linear hyperplane. In higher\ndimensions (e.g., the original 768D vectors), where we have more features, separation\nbecomes even easier due to the increased dimensionality of the data space. Therefore, an\nn-dimensional hyperplane can yield similar or more separable results than the 2D data\npoints in Figure 4.2. This is due to the fact that the additional features provide more\ndiscriminative power, enabling better separation of data points in the higher-dimensional\nspace.\n4Obtained from: github.com/alexlitel/congresstweets\n38 Transformer-Based Quantification of the Echo Chamber Effect\nFigure 4.2: 2D projection of US congresspeople and senators\u2019 user-embeddings.\nTo quantify and validate this separability, we train a linear SVM classifier on half of the\ndata and validate on the other half as our test-set. The classifier yields a 93% F1-Score\n(macro), suggesting that a promising set of features are stored in our user embedding\nvectors and that our method can be used to distinguish the political stances of users.\nNote that this performance is given after using only 130 users per class (Republican vs.\nDemocrat) and 200 tweets per user, which offers a promising measure for scarce datasets.\nWe examine the performance of our method when we increase the number of tweets per\nuser to 500, obtaining an improvement of the F1-Score up to 95%. We, however, stick to\n200 tweets per user due to constraints in our Twitter API rate limit.\nAs we deal with pairs of Chambers that are formed on the basis of a Republican\nvs Democratic leaning idea over a topic, the user separability we measure across these\nChambers is mapped to the level of political polarization across Chambers of a topic. We\nfurther discuss the scope of our evaluation in Section 4.8.\nAs per the performance, the whole process of collecting 200 tweets from a user, trans-\nforming them into vectors, and averaging all the vectors, took approximately 3 seconds\n4.6 Datasets 39\nper user on Google-Colab\u2019s GPU.\n4.6. Datasets\nWe consider top trends on Twitter associated with three recent controversial events\nnext to a non-controversial one: (1) the Uvalde school shooting which triggered yet an-\nother discussion around gun control; (2) the US Supreme Court\u2019s decision on June 2022\nto overturn Roe v. Wade sparked a nationwide debate on abortion rights in the US;5(3)\nthe Russo-Ukraine War; and (4) the SXSW 2022 music festival.\nOur data is collected over one month period since the events related to the topics.\nWe utilize the \u201cNetwork Tool\u201d6developed by Indiana University Observatory On Social\nMedia to query top trending hashtags related to the topics on Twitter. Table 4.1 shows\nthe list of hashtags and dates that we used for collecting retweets for every topic.\nTopic Queried Keyword-\ns/HashtagsStart Date End Date # of Users\nAbortion-\nbanAbortion, #RoeVsWade,\n#Prolife, #Prochoice,\n#WhatIsAbortion, #My-\nBodyMyChoice #Abor-\ntionIsHealthCare, #Abor-\ntionIsMurder1/6/2022 30/6/2022\u224829000\nWar on\nUkraineUkraine,\n#StandWithUkraine-\n(the latter was used only\nfor Section 4.7.1)20/2/2022 20/3/2022\u224821000\nTexas Gun-\nshootingGun, Ulvade, Shoot-\ning, #GunControl,\n#GunOwnersForSafety,\n#ProGun, #AntiGun,\n#GunRights, #GunVi-\nolence, #MassShooting,\n#2ndAmendment, #Right-\ntoCarry, #EndGunVio-\nlence24/5/2022 23/6/2022\u224825000\nSXSW Fes-\ntival#SXSW 1/3/2022 30/3/2022\u224811000\nTable 4.1: Queried hashtags for data collection.\nNext to the basic keywords of the topics we used for querying (e.g. \u201cabortion\u201d for the\nAbortion topic), we tried to maintain equal numbers of partisan hashtags for both sides\n5https://reproductiverights.org/global-trends-abortion-rights-infographic/\n6https://osome.iu.edu/tools/networks/\n40 Transformer-Based Quantification of the Echo Chamber Effect\nof the debates on every topic. We sorted trending hashtags per topic based on their pop-\nularity and picked as many neutral hashtags as existed in the trends (e.g. #RoeVsWade\nhas no clear partisan position on its own) and an equal number of partisan hashtags\nfrom both sides down-sampled to the less populated side. For example, if a topic has 3\nright-wing and 10 left-wing partisan hashtags, we pick all the 3 right-wing hashtags and\n3 top most trendy left-wing ones. However, for the case of \u201cWar on Ukraine\u201d, despite\nmultiple pro-Ukraine hashtags, we were unable to find any pro-Russian invasion hashtag\nin the English Twitter, thus, we only used tweets that contained the word \u201cUkraine\u201d for\nforming the retweet network. In this way, we represent both sides of the debate, if any,\nfairly on the retweet network. Also, for the case of the SXSW, there was no notion of\nright-wing or left-wing hashtags since it is not a politically polarized topic, so we only\nqueried the keyword \u201cSXSW\u201d.\nLater on in Section 4.7, we select subsets of the users of these keywords, based on the\npartisan hashtags they used (cf. Section 4.7.1) or the retweet network (Chamber) they\nappeared in (cf. Section 4.7.2), and collect the latest 200 tweets of their timeline using\nTwitter\u2019s official API.\n4.7. Experiments and Results\nWe next run two separate experiments. First, we analyze the level of Echo per hashtag\nand hashtag-wise Polarization by characterizing the users who have used any of those\nhashtags. Then, we measure the Echo of every two Chambers for all topics and their\nPolarization.\n4.7.1. Echo per Hashtag\nOn most social media platforms, including Twitter, clicking on a hashtag fills the\ntimeline of the user with top-tending tweets around the hashtag. Thus, a hashtag offers\na specific environment of content. Therefore disregarding the position of users in the\nretweet networks, we only look into partisan hashtags (i.e., hashtags with clear political\nstances) to measure the diversity and polarization of users across the hashtags.\nFor this, we gather a sample of users who have used pro-gun hashtags (e.g., #Gun-\nRights), anti-gun (e.g., #EndGunViolence), pro-abortion (e.g., #AbortionIsHealthCare),\nanti-abortion (e.g., #AbortionIsMurder), and pro-Ukraine (e.g., #StandWithUkraine) \u2014\ni.e., there is no explicit anti-Ukraine hashtag on Twitter to be added to the analysis. We\nalso add one case of a non-partisan hashtag, namely #SXSW, for comparison.\nWe obtain a novel embedding of each of the users in an unsupervised fashion following\nthe step in Section 4.4.2.\n4.7 Experiments and Results 41\nFigure 4.3: Variances of user embeddings for partisan hashtags\u2019 users + #SXSW as a\nnon-partisan case\nFigure 4.4 shows the 2D projection of user embeddings color-coded by the type of\nhashtags they have used. We see that the Republican stances discussing Pro-Gun and\nAnti-Abortion (red and pink) stem from users that are more densely embedded in the\nspectrum. These users have a high overlap with each other. Instead, the Democratic\nstances discussing Anti-Gun and Pro-Abortion (blue and light-blue) are represented by\na more diverse set of users on Twitter. The users of #StandWithUkraine hashtag are\nalso widely distributed in the plot with higher overlap with Democratic users than the\nRepublicans. These results provide an initial intuition about the variety and overlap of\nusers who had supported specific political stances, yet we are interested in quantifying\nthese concepts statistically.\nTo quantify variety, we use a multidimensional variance of the user embeddings per\nhashtag portrayed. These variances are calculated by taking the mean of all element-\nwise variances for a multidimensional set of vectors. The 95% confidence intervals are\ncalculated based on 1,000 bootstraps each containing random 1,500 samples. Figure 4.3\nshows that the users of the Republican-leaning hashtags have significantly lower diversity\nthan the Democratic hashtags\u2019 users. The users of #StandWithUkraine hashtag preserve\nthe highest diversity, showing a possibly vast demographic support among the users.\nFinally, we quantify the polarization according to the ability of a Linear SVM to\nseparate users of two classes (hashtags). Table 4.2 shows the F1-Score per hashtags\nclass. Recall that a low F1-Score means a high rate of overlap between the users of two\nhashtag classes as discussed in Section 4.4.4. We see that the Democratic and Republican\nhashtags have lower separability among themselves and higher separability across hashtags\nsupported by the other party. For instance, the separability of pro-abortion vs anti-gun\nis low (70%) in two democratic Chambers. At the same time, there is a high (91%)\nseparability between anti-abortion and anti-gun as the members of a Republican stance\nare presumed to be separable from a Democratic one.\n42 Transformer-Based Quantification of the Echo Chamber Effect\nFigure 4.4: 2D projection of user-embeddings for polarized hashtags\u2019 users.\nTable 4.2: F1-Scores for linear separability between pairs of user embeddings across hash-\ntags.\nHashtag Class Pro-Ukraine Pro-Gun Anti Gun Pro-Abortion Anti-Abortion\nPro-Ukraine 50% 83% 77% 74% 90%\nPro-Gun 83% 50% 82% 77% 60%\nAnti-Gun 77% 82% 50% 70% 91%\nPro-Abortion 74% 77% 70% 50% 87%\nAnti-Abortion 90% 60% 91% 87% 50%\nWe also observe a higher separability of pro-Ukraine users with Republican supporters\nthan when compared to Democrats, meaning that although the pro-Ukraine stance is more\ndiversely supported, discussions are more popular among Democrats. Note that even the\n4.7 Experiments and Results 43\nmost partisan hashtags can have an underlying political agenda. Although this effect may\ninfluence the intuitiveness of the results, our method is good at quantifying these nuances.\n4.7.2. Echo per Chamber\nThis section measures the Echo for every Chamber. In other words, we quantify the\nPolarization of the retweet clusters across topics.\nUnlike in our experiment in Section 4.7.1 where we select users that use specific par-\ntisan hashtags, we retain here all users that appear in the retweet network cluster. This\nis done to compare user embeddings with the stances of the users on each of these topics.\nThis comparison let us measure the Echo Chamber effect and Polarization.\nFirst of all, we validate the network clustering step by manually labeling a random\nsample of 210 retweets for all network clusters. Each retweet network cluster in our dataset\nis composed of approximately 300 seed tweets, thus, our sample will look at around 12%\nof the entire seed tweets (6 \u00d7300).\nAlthough the homogeneity of the stance of each Chamber is visible from a cursory\ninspection, the purpose of this experiment is to systemically verify this. Table 4.3 shows\nthe number of each tweet\u2019s stance per retweet network and the rate of alignment with the\nhypothesized stance of the entire Chamber in the first cursory glance. We see that each\nChamber is formed around a certain stance toward a topic, as for every Chamber, the\nidentifiable stances of tweets are almost entirely pro or anti. Unidentifiable tweets\u2019 stances\ninclude tweets with reference to broken links or quotations of news without expressing\nany explicit opinion about them.\nOur annotation guideline is based on the main positions of each political party in the\nUS on each of the controversial topics. Tweets with references such as \u201cwomen\u2019s right\nto decide about their own body\u201d, \u201chealth-related risks of banning abortion\u201d, etc. are\nlabeled as Democratic whereas those with references to \u201cthe right of the embryo to live\u201d,\n\u201creligious teachings against abortion\u201d, etc. are labeled as tweets with Republican stances.\nRegarding the Ulvade school shootings, tweets emphasizing the significance of the tragedy\nwith direct or indirect blame on the gun law in the US are labeled as Democratic and those\nreferring to the \u201c2nd Amendment rights to carry firearms\u201d or arguing that \u201cgun-rights is\nnot the actual reason, but the solution\u201d are labeled as Republican tweets. Tweets labeled\nas \u201cAnti\u201d Ukraine for the Republican Chamber in Table 4.3, are actually the combination\nof all the stances focusing on \u201cRussian military advances\u201d, \u201cclaiming that US aid to\nUkraine is excessive\u201d, \u201cblaming the war on Biden administration\u2019s policies\u201d, \u201ccriticizing\nZelenskyy\u201d, \u201ccomplaining about the rate of Ukrainian refugee intake\u201d, etc. which are the\nalternative to the democratic stances focusing on \u201cUkrainian military advances\u201d, \u201casking\nfor more US/NATO aids to Ukraine\u201d, \u201cempathizing with Ukrainian victims of war\u201d, etc.\nSXSW is not included in Table 4.3 as it is not a politically polarized topic to begin with.\nWe now look at the entire retweet network. Figure 4.5 shows the retweet network,\n44 Transformer-Based Quantification of the Echo Chamber Effect\nTable 4.3: Stances of sampled tweets for each Chamber. The rate of alignment of tweets\u2019\nstances with the hypothetical stance of a Chamber shows the accuracy of the network\nclustering method.\nTopic Chamber Hypothetical\nStanceSample\nSizeN Pro N Anti Alignment N\nUniden-\ntifiable\nAbortion A Pro-Abortion\n(Democrat)35 32 1 97% 2\nAbortion B Anti-Abortion\n(Republican)35 1 34 97% 0\nGun A Anti-Gun\n(Democrat)35 0 31 100% 4\nGun B Pro-Gun (Re-\npublican)35 29 1 97% 5\nUkraine A Pro-Ukraine\n(Democrat)35 21 0 100% 14\nUkraine B Anti-Ukraine\n(Republican)35 2 25 93% 8\nOverall 210 - - 97.3% -\nvisualized by Forced Atlas 2 [100], on the top and the user embeddings on the bottom.\nAs the main communities within the SXSW retweet network lacked sufficient separability,\ngiven the non-controversial nature of the topic, the Forced Atlas 2 algorithm depicted it\nas a unified circular atlas. In contrast, the three controversial topics manifested as two\ndistinct circles, showcasing their discernible independence.\nUser embeddings are projected into 2D using UMAP and color-coded based on the\ncorresponding retweet network (Chamber) they have participated in. The more separable\nthe blue and red data points are, the more polarized the Chambers are. Instead, in less\npolarized Chamber pairs, we expect the points to be mixed more with each other.\nMoreover, if the \u201cEcho\u201d in a \u201cChamber\u201d is high, we expect to observe a higher density\nin its users\u2019 embeddings\u2019 2D projection with respect to the other color-coded Chamber.\nThis means that a more homogeneous group of people have taken the stance supported\nby that retweet network.\nAfter providing a visual intuition, we apply our method (steps in Sections 4.4.3 and\n4.4.4) to quantify the Echo and the Polarization of Chambers. Table 4.4 summarizes the\nvalues for linear separability and variance of each Chamber.\nIn all three controversial topics, the Chambers of the Republican stance have lower\nvariances (higher Echo) than their Democrat counterpart (column Var in Table 4.4).\nAmong the three controversial topics, the Chambers of the gun-control topic have the\nlowest variance and the highest separability from each other in comparison to other topics,\n4.7 Experiments and Results 45\nFigure 4.5: Comparison of retweet networks vs 2D projection of user-embeddings. The\nred and blue points represent the users that had attended Conservative and Democrat\nChambers in the corresponding events.\nwhereas the exact opposite has happened for the war in Ukraine. This not only shows\na higher level of polarization for the gun-control discussion and a lower polarization for\nthe Russo-Ukraine war but also a positive relationship between the level of Echo and\nthe polarization in online discussions. As anticipated, the sole non-controversial topic,\nSXSW, exhibited the least polarization and the greatest user diversity, reinforcing the\nrobustness of our methodology. However, even though it registers as comparatively low,\nthe observed separability for SXSW is not negligible. This raises the possibility that a\nnon-political source of polarization could underlie the observed user separation. Further\nexploration of such instances is elaborated in Section 4.8.1.3.\n46 Transformer-Based Quantification of the Echo Chamber Effect\nTable 4.4: Summary of results for every Chamber of every topic. Columns beginning with\n\u201cSeparability:\u201d for Chamber A refers to its users\u2019 separability from its twin Chamber (B)\non the same topic , vice versa.\nTopic Chamber Affiliation Var\u00d7105Separability:\nSVM\nAccuracySeparability:\nSVM\nMean-ConfSample Tweet\nAbortion A Democrat 7.5\u00b10.3 89% 0.50 Nobody\u2019s life has\never been saved\nby preventing an\nabortion.\nAbortion B Republican 5.5\u00b10.4 89% 0.50 So pro abortion\nprotestors are\nprotesting in cities\nthey can still get\nabortions?\nGun A Democrat 5.8\u00b10.3 92% 0.56 Denmark has tragi-\ncally experienced an-\nother mass shooting.\nGun B Republican 4.8\u00b10.3 92% 0.56 Sign the petition\nagainst gun control.\nUkraine A Democrat 7.6\u00b10.4 86% 0.48 DO YOU NOW\nGET IT WHY\nUKRAINE NEEDS\nALL WEAPONS\nTHE WORLD CAN\nGIVE?\nUkraine B Republican 6.4\u00b10.3 86% 0.48 #Washington\ncreated the fas-\ncist regime in\n#Ukraine... (trun-\ncated)\nSXSW A Non-\nPolitical\n(Affiliation\n1)15.0\u00b10.7 82% 0.45 See you next year\n#sxsw. My eyes are\nbleeding but was a\nblast\nSXSW B Non-\nPolitical\n(Affiliation\n2)19.6\u00b10.6 82% 0.45 Nice blog from\nour #Sxsw panel...\n(truncated)\nFurthermore, Table 4.5 depicts the heat map of user separability between chambers\nacross topics. As we fix A and B as the Democrat and Republican Chambers in all\nthe topics, in case our user embedding method holds sufficiently meaningful features,\nour hypothesis would be to see a lower separability among the users of the same-letter\nChambers (i.e., A vs A, B vs B) and higher separability among users of cross-letter\n4.7 Experiments and Results 47\nChambers (i.e., A vs B, B vs A). This hypothesis seems to hold, as the separability is 86-\n93% for all cross-letter Chambers while it falls to 69-80% when comparing two Chambers\nwith similar letter codes. The minimum separability is 50%, which represents the accuracy\nof a classifier when the labels are random (i.e., in this case, identical: Abortion Chamber\nA vs Abortion Chamber A again).\nTable 4.5: Levels of user separability per pair of Chambers across all the topics. Chamber\nA is the Democrat and Chamber B is the Republican retweet cluster.\nChamber A Chamber B\nAbortion Gun Ukraine Abortion Gun Ukraine\nAbortion 50% 76% 80% 89% 91% 90%\nGun 76% 50% 77% 91% 92% 93% A\nUkraine 80% 77% 50% 89% 91% 86%\nAbortion 89% 91% 89% 50% 69% 80%\nGun 91% 92% 91% 69% 50% 78% B\nUkraine 90% 93% 86% 80% 78% 50%\nFor the Ukraine case, we observe a higher user separability for same-letter Chambers\nwith the other two topics rather than Gun vs Abortion (e.g. Ukraine\u2019s Chamber B is more\nseparable from Abortion\u2019s Chamber B \u2013 80%, than Gun\u2019s Chamber B from Abortion\u2019s\nChamber B \u2013 69%). This further supports, as already discussed before, that the users\nin the Russo-Ukraine war case are more diverse and its Chambers are less likely to be\ndivided into purely Democrat and purely Republican users.\nAgain, our goal is to compare the level of separability by comparing the performance\nof the classifier, not building a classifier to separate the users. However, a byproduct\nof this observation is to further approve the efficiency of our user embedding approach\nby the high accuracy obtained for separating the classes. Using our user embeddings as\nfeatures, a simple linear classifier is not only able to classify Democrat vs Republican\nusers (Section 4.5), but also cases like Pro-Abortion Democrats vs Anti-Gun Democrats.\nWe find that our novel user-embedding approach has the potential to be used for future\nuser-classification tasks.\n4.7.3. Comparison with Supervised Baseline\nThis section aims at comparing our newly proposed method with existing baselines.\nUnfortunately, when it comes to the field of Echo Chambers and online Polarization,\nthere is no labeled golden standard of these qualities that tells how topics are polarized\nand which ones are more polarized than others [18]. This makes it difficult to judge\nhow our method performs with respect to existing works as there is no clear definition\nof accuracy in this domain. We address this challenge by replicating existing methods\n48 Transformer-Based Quantification of the Echo Chamber Effect\nover well-established polarized topics. In particular, we chose Abortion and Gun-Control\nas topics where we expect a high level of polarization. On the contrary, we chose the\nUkraine war as a topic where we expect to see lower polarized discussion in the context\nof the US political sphere \u2014 where our tweets come from.\nWe next compare the results of prior approaches over the topics. In particular, we\nreplicate Garimella et al. [62] method of measuring user\u2019s polarity as it is vastly adopted\nby other scholars. As in Garimella\u2019s work, we calculate users\u2019 polarity/ideology based\non the average polarity of content they had shared online as the baseline. Note that the\nnotion of \u201cuser polarity\u201d in [62] is the supervised equivalent of \u201cuser embeddings\u201d in our\nown approach. In particular, we obtain content polarities by forming a labeled dataset\nof online news sources and Twitter accounts annotated as left-leaning, right-leaning, and\ncentric. We generate this annotated dataset by combining the latest database of AllSides7\nand MediaBiasFactCheck8with the labeled dataset of congresspeople and senators in\nSection 4.5. Then, for each user uin the dataset, we consider the set of tweets Puposted\nbyuthat contain links to news organizations of known political leaning lnor retweets\nmade from the labeled politician or news accounts on Twitter. We then associate each\ntweet/retweet t\u2208Puwith leaning \u2113(t) =ln. The user polarity p(u) of useruis then\ndefined as the average political leaning over Pu[62]:\np(u) =/summationtext\nt\u2208Pu\u2113(t)\n|Pu|. (4.3)\nThe value of user polarity ranges between -1 and 1. For users who regularly share content\nfrom left-leaning sources, the user polarity is closer to -1, while for those who share content\nfrom right-leaning sources, it is closer to +1.\nWe restrict our comparison to the user-ideology estimation part as the later steps\nof Garimella\u2019s work (e.g., calculating \u201cconsumption polarity\u201d) require full access to the\nfollower/following networks on Twitter which is no longer accessible via Twitter API.9\nAfter measuring the user polarity, we proceed to measure both effects with the new\nsupervised foundation of user ideology as our baseline using the definition of Echo and\nPolarization in Section 4.3.\nFigure 4.6 shows the distribution of user polarity across each of the Chambers of\nthe baseline. The blue (red) curves represent the distribution of users who showed up\nin Democratic (Republican) Chambers for each topic (the retweet networks that were\npromoting Democrats\u2019 stances for each topic). The level of flatness of each distribution\nrepresents the diversity of sets of users from the entire political spectrum that has ap-\npeared in that Chamber [62]. The flatter the distribution of a Chamber, the lower the\n7https://www.allsides.com/media-bias\n8https://mediabiasfactcheck.com/\n9https://twittercommunity.com/t/starting-february-9-twitter-will-no-longer-support-free-access-to-the-twitter-api/\n184611\n4.7 Experiments and Results 49\nFigure 4.6: Users political ideology (polarity) distribution across each Chamber of each\ntopic. Negative values manifest left-leaning ideology and positive values manifest right-\nleaning ideology.\nEcho of voice. Moreover, a high overlap between the distributions of two Chambers of\na topic would represent a lower political polarization in the online conversation around\nthat topic. Similar to our results (cf. Figure 4.5 and Table 4.4), we see there is an over-\nlap between the distribution of users in the Democratic Chamber and the Republican\nChamber in the case of the Russo-Ukrainian war. On the contrary, for \u201cAbortion\u201d and\n\u201cGun-Control\u201d, Chambers have minimal overlap as in our results, showing a higher level\nof polarization in those topics. In other words, only right-wing (left-wing) users \u2014 ones\nwith positive (negative) polarity scores \u2014 had taken Republican (Democratic) stances.\nTable 4.6: Replication of Table 4.4 with Supervised Baseline.\nTopic Chamber Affiliation Var (Inverse of\nEcho)Partisan Stance Rate\n(Polarization)\nAbortion A Democrat 0.13\u00b10.02 95.9%\nAbortion B Republican 0.13\u00b10.02 95.9%\nGun A Democrat 0.13\u00b10.02 96.3%\nGun B Republican 0.12\u00b10.02 96.3%\nUkraine A Democrat 0.24\u00b10.02 85.9%\nUkraine B Republican 0.28\u00b10.03 85.9%\n50 Transformer-Based Quantification of the Echo Chamber Effect\nWe next quantify the level of Echo and Polarization per topic. To compute the base-\nline, we quantify the Echo by also leveraging the variance of user polarity per topic. For\nPolarization, we measure the percentage of partisan stances; the rate of users who sup-\nported the stances that were aligned with their original political leaning (e.g., the number\nof left-leaning users who took a pro-abortion stance, and vice versa, divided by the total\nnumber of users). The higher the percentage of partisan stances on a topic, the higher\nwould be the topic\u2019s polarization. Table 4.6 shows the baseline results.\nWe make the following observations when comparing the baseline with our results in\nTable 4.4. First, the baseline\u2019s results are aligned with our method in terms of polarization\njudging by the correlation between the separability of our approach and the partisan stance\nrateof the baseline (cf. last columns in Tables 4.4 and 4.6). In particular, our results\nshow that the Russo-Ukrainian war is the least polarized topic, and Gun control is the\nmost polarized one. For the case of SXSW, the measurement was inapplicable as the\nChambers were not initially classified as Democrat or Republican and we also did not find\nany sufficient number of political references in their tweets. While the results we obtain\ndetecting polarization are comparable with the baseline, we note that our approach is\nunsupervised and it does not suffer the burden of the labeling process as in the baseline.\nSecond, we see that the Echo in the Ukraine chambers is the highest in both the\nbaseline and our method as indicated by the \u201cVar\u201d column in Tables 4.4 and 4.6. However,\nwe note that the Echo in the chambers of the Abortion and Gun topics in the baseline\nare not significantly different from one another as opposed to what was expected. Recall\nthat Chambers with Democratic stances preserve higher diversity of users (lower Echo).\nInstead, our method is able to detect differences in terms of diversity in Democratic\nand Republican Chambers. We attribute the difference to a limitation of the baseline in\nmeasuring the ideology as a one-dimensional pre-defined political spectrum as we discuss\nin Section 4.8.1.3. Notably, a transformer-based user-embedding method can represent all\nsorts of semantic qualities produced by users that can be attributed to the user\u2019s political\nideology, dialect, gender, etc. manifested in his/her produced content online. Therefore,\nour results are more aligned with the real-world statistics showing that Democrats are\nmore ethnically diverse when compared to Republicans [101].\n4.8. Discussion\nWe now discuss our key findings as well as limitations and future work.\n4.8 Discussion 51\n4.8.1. Key Findings\n4.8.1.1. Quantifying Diversity.\nLeveraging state-of-the-art language models, this chapter proposed an intuitive, com-\nputationally cheap, and unsupervised approach for quantifying Echo-Chambers and exist-\ning polarization phenomenons. The generalizability of our metric enabled us to compare\nthese effects across four topics. The results show that the highest polarization has hap-\npened among the Gun-Control topic\u2019s Chambers and the lowest for SXSW, the only\nnon-controversial topic of the analysis, followed by the War on Ukraine. Moreover, we\nshowed that the diversity of users in all three controversial topics of our analysis is lower\nfor the Republican stances (e.g., Anti-Abortion) than the Democratic ones (e.g., Pro-\nAbortion) on the same topic. Pew Research Center had previously confirmed a greater\nrepresentation of Democrats on Twitter [102]. What our observation adds to the polls\nis that the users with democratic stances are not only represented higher on Twitter in\nterms of number but also in terms of diversity.\nWe discovered that the hashtag \u201c#SXSW\u201d, the only non-partisan hashtag of the\nanalysis, expectedly, has the highest diversity of users among the hashtags. Then, among\nthe partisan hashtags, \u201c#StandWithUkraine\u201d has the highest diversity of users. This\ncan mean that manifesting support for Ukraine has been prevalent among people of more\ndiverse sets of ideologies, or/and demographics, or/and etc.\nIn a scenario where users are mainly located in the US, this could be related to the\nphenomenon of \u201cRally Round the Flag\u201d as in political science [103, 104, 105]. Otherwise,\nthis high diversity can hint to the higher variety of user locations in Ukraine supporters,\nsuggesting a higher global involvement with the topic, in comparison to the domestic\nissues in the analysis (i.e., gunand abortion ).\nThe term refers to the notion that when a major national conflict takes place, the\nAmerican people are likely to set aside their disagreements with the incumbent presi-\ndent\u2019s policies or performance in office to demonstrate a united front to the international\ncommunity [106]. Although the high amount of user embedding diversity for \u201c#Stand-\nWithUkraine\u201d and Ukraine-related Chambers in Section 4.7.2 confirms it, the higher\nsimilarity (lower linear separability) of the users of the hashtag to Democrats than the\nRepublicans tells that the rally had possibly happened among hard-core Democrats and\nnon-political users, leaving some hard-core Republicans out.\nIn a related vein, Bailon et al. [21] investigated the extent to which Facebook enabled\nanasymmetrical ideological segregation in political news consumption during the 2020\nUS presidential election. They found that Conservatives were more likely to be exposed\nto ideologically homogeneous information than liberals. Combining these findings with\nour results which show that the homogeneity of user embeddings , which is higher for\nRepublicans in our findings, and the homogeneity of users\u2019 news consumption , which is\n52 Transformer-Based Quantification of the Echo Chamber Effect\nalso higher for Conservatives according to Bailon et al., we can hypothesize that there\ncan be a meaningful causal relationship between the two phenomena.\n4.8.1.2. User Embedding.\nWe embedded users by averaging the sentence embeddings of their tweets. Averaging\nembeddings have previously been applied to word embeddings to generate an embedding\nfor a sentence [107]. However, to our knowledge, it has not been applied to multiple\nsentence embeddings to represent authors as in our work. As the words of a sentence\nare elements that are sequentially dependent on each other, their order should preferably\nbe taken into account in an ideal NLP model. However, we posit that averaging would\nperform better when we are dealing with embeddings of tweets that are the independent\nelements of the user\u2019s mindset. Thus, the order would barely mean much in this case.\nTherefore, we expect that averaging independent sentences\u2019 (tweets\u2019) embeddings would\nreturn meaningful results. Moreover, there is a statistical justification for averaging the\nembeddings due to the \u201cblessing of dimensionality. \u201d Since exponential numbers of embed-\ndings are almost orthogonal in high dimensions, two random sets of embeddings are very\nunlikely to have similar averages [108].\n4.8.1.3. Quantifying Polarization.\nIt is worth noting that while quantifying the polarization across Chambers using\nembedding separability, what we measure is the separability of users\u2019 discourse across\nChambers. Yet, understanding the underlying source of discourse separability requires\nfurther analyses. As we embed the users utilizing sentence transformers, the encoded\nfeatures for every user are black boxes that have stored the online semantic behavior of\na user. This means that we are not investigating the aspects on which the discourse\nof the users is polarized. The timeline generated by users can be influenced by his/her\nsociopolitical leaning, economic leaning, socioeconomic status, gender, age, personality\ntype, geographical location, language variety, etc. Our metric can nevertheless show a\nhigh rate of user separability for two Chambers of a non-controversial topic if, for instance,\nthe Chambers are formed based on the local follow-network in different locations and each\nlocation\u2019s dialect or daily concerns can distinguish its users from other locations.\nIn this chapter, we applied the metric to pairs of Chambers that are known to be\ndifferent on the basis of political stance on a topic (e.g. pro-gun vs. anti-gun retweet\nnetworks) and verified this by sampling a few of the tweets from the retweet network of\nevery Chamber. In such cases, every sort of hidden encoded feature causing a difference\nbetween the users of the two clusters is translated as an underlying source of \u201cpolitical\u201d\npolarization. For instance, if all the women are pro-choice in Chamber A, and all the men\nare pro-life in Chamber B, the abortion topic is polarized on gender. Alternatively, if\n4.8 Discussion 53\nmost of the southerners in the US are pro-gun and most of the northerners are anti-gun,\nthe Gun-control topic is polarized on geolocation.\nMost of the possibly embedded features of users mentioned above can be measured as\ncontinuous variables. For instance, sociopolitical or economic views can be anywhere be-\ntween alt-right to alt-left, and socioeconomic status can be a number anywhere from 0 $to\n1M$+ per year). Also, demographic features such as age, gender [109], and ethnicity [110]\nare considered continuous spectrums of values in recent social science literature. This will\nmake the concept of linear separability a more meaningful metric for such variables, as\nthey will be converted into numbers embedded in a continuous 768D space and sepa-\nrated by a hyperplane. For possible cases of non-continuous features, although the SVM\nmean confidence interval would be a less meaningful metric as it relies on the distance\nto the separating hyperplane, the accuracy of the SVM classifier would cover the level of\nnon-continuous divide (e.g. a hypothetical binary division in 1D would be separated by\na vertical line in 0.5, yet the distance to that vertical line, which corresponds to SVM\u2019s\nconfidence interval, would not yield a meaningful result).\n4.8.2. Comparison with Previous Approaches\nOur approach marks a departure from traditional methodologies utilized in prior\nworks, notably those pioneered by Garimella et al., Pablo Barbera, and others [14, 19, 22].\nThe core idea of previous Echo Chamber measurement approaches centered around estab-\nlishing correlations between the political leaning of the content the online user is exposed\nto or believes in, and the political leaning of contents they produce on specific topics. This\ncorrelation served as a key metric for evaluating the degree of polarization (i.e., in more\ncontroversial/polarized topics, there is a higher correlation between what users consume\nin general and what they produce on that topic).\nUser\u2019s exposure or user\u2019s general belief is typically modeled by the political lean-\ning of the user\u2019s neighborhood [22] which is estimated from follow networks representing\nthe connections users have with each other. The leaning of content exposure is deter-\nmined by examining either the political affiliations of users in Twitter\u2019s follow-network\n(i.e. if user A follows Donald Trump, their score leans more toward conservatism) or by\nassessing the latent space position of users within this network [19]. In our work, this\nelement is replaced by unsupervised transformers applied to the timelines of users.\nTheleaning of produced content has been traditionally calculated by counting\npre-labeled political sources or examining retweets from political figures with predefined\nleanings. For instance, referencing/retweeting a source like Fox News on the topic of\nabortion will increase the conservative score of a user on that topic.\nWe list several advantages and disadvantages of our model when compared to the\ndescribed previous approaches.\n54 Transformer-Based Quantification of the Echo Chamber Effect\n4.8.2.1. Advantages:\n1.Availability of Data: Given the evolving landscape of social media privacy poli-\ncies, especially regarding the collection of follower data, our method is less vulnerable\nto the current social media policy restrictions. Notably, since Twitter\u2019s reform, the\ncomplete following or followers list of users is no longer visible. This trend can also\nspread to other social networks in the future. Our focus on the minimal amount of\nopen-source timeline data remains a viable alternative.\n2.Unsupervised Nature: The reliance of the previous method on pre-labeled po-\nlitical sources makes them not only reliant on expensive crowd-sourcing but also\nless robust to the fluid nature of political landscape changes and the migration of\nusers to new platforms. For example, as there is evidence of mass migration of\nusers from Twitter to Mastodon [111], an analysis of polarization in a new social\nmedia like Mastodon requires new labeling of political sources and celebrities in\nthat platform. Yet, the unsupervised nature of our approach which is based on the\nembedded features of the timeline, is robust to such changes.\n3.Multi-Dimensional Understanding of Polarization: As the foundation of pre-\nvious approaches is based on sources labeled as politically left or right their under-\nstanding of polarization would be limited to political polarization exclusively; and\nonly the left and right duality in political polarization which is not the only type\nof political divide [4], especially in non-western countries [112, 113]. For instance,\nreligious divisions are more pronounced in nations that have embraced seculariza-\ntion and possess a heritage tied to Catholicism, indicating a heightened polarization\ninfluenced by religious passion within secular societies [114]. As sentence transform-\ners in our approach embed various sorts of semantic information produced by users,\nthe measured polarization in our approach can encapsulate multi-dimensional sorts\nof polarizations.\n4.8.2.2. Disadvantages:\n1.Unspecified Source for Polarization: In scenarios where the primary aim re-\nvolves around measuring polarization in classic conservative versus democrat dimen-\nsions, the previous methodologies provide more definitive insights into the political\nsources driving polarization. Unlike these approaches, our method operates as a\nblack-box in determining the specific sources or dimensions contributing to polar-\nization. In Section 4.8.3, we discuss two approaches to addressing this limitation.\n2.Less Granularity: The overlap of content consumption and production in previ-\nous approaches offers polarization scores at the individual user level. In contrast,\nour method evaluates polarization holistically by assigning an overall score to the\n4.8 Discussion 55\npolarization between two Chambers by looking at the overall separability of their\nusers. However, this limitation can nevertheless be mitigated by examining the\ndistance of users\u2019 embeddings from the support vectors\u2019 hyperplane in the SVM\nclassifier that separates two Chambers.\n4.8.3. Limitations & Future Work\nOur method offers systematic \u2014 and unsupervised \u2014 insights into the polarization\nof different Web communities, which led to the key findings presented above. However,\nas computational social science research that aims to bridge between the quantitative\ndomain of computational methods and the partly qualitative domain of social sciences,\nour approach is subject to some assumptions and limitations.\nOne of the limitations is the absence of an objective ground truth that tells which\ntopic is more polarized or subject to the Echo-Chamber effect with respect to other\ncontroversial topics. This limitation is shared with previous work [18] that mentions the\nintuitiveness of evaluation based on the labeling that a topic is controversial/polarized.\nThe alternative to such methodological assumptions is to hand-label/survey thousands of\nusers [18]. We nevertheless evaluate the core of our method in Section 4.5 with ground\ntruth of congress-people and senators who are labeled as Republican or Democrat, and\nwe show that our method can successfully distinguish between them.\nWe further evaluated other intermediate steps like the network clustering step by\nmanually labeling a random sample in Section 4.7.2, and compared our method with a\nwell-established baseline in Section 4.7.3 showing significant improvements when com-\npared to existing methods.\nFuture work can utilize our user embedding approach for any task related to user\nclassification (e.g., gender classification and bot detection). In this chapter, we embedded\nthe users merely based on their 200 recent tweets. When using Twitter\u2019s official API to\ngather user data, each API response includes 200 tweets per page. As our main focus\nin this chapter was less on reporting an intensive measurement and more on introducing\nand testing our proposed method, we limited the scraping to 200 tweets per user to\nremove the need for pagination and make the collection process less time-consuming and\ncomplex. This served as a preliminary analysis, which yielded a sufficient amount of\naccuracy to manifest the separability between users, both in the case of congresspeople\nand users in different Chambers. Moreover, given the evolving landscape of stringent\ndata access policies, exemplified by the recent measures implemented by Elon Musk on\nTwitter,10which are indicative of an industry trend likely to restrict extensive online data\naccessibility, our demonstration of an approach that is reliant on smaller data subsets\naligns with the need for approaches less dependent on data quantity.\n10https://techhq.com/2023/07/why-has-twitter-introduced-rate-limits/\n56 Transformer-Based Quantification of the Echo Chamber Effect\nThe scope of this study was limited to quantifying the amount of Echo inside Chambers\nand polarization across the Chambers. However, the underlying source of the polariza-\ntions can be multidimensional, rooting in variations in sociopolitical views [4], economic\nviews, socio-economic statuses, geographic locations, linguistic differences, etc. A po-\ntential future direction is to analyze the source of polarization between Chambers by\ninvestigating various semantic features in users\u2019 timelines and profiles. Instead of a single\nembedding per user, we can create separate embeddings for different aspects, such as\npolitical views and language preferences. These separate embeddings can help us better\nunderstand why and how users become separated within chambers.\nA more sophisticated approach in natural language processing involves unraveling the\nopaque semantic features embedded by sentence-transformer models through Explainable\nAI techniques [115]. By deciphering the semantic meaning associated with each element in\nthe approximately 700-dimensional vectors, we gain the capability to discern the specific\nsemantic features contributing to the separation between two data points that have been\nsemantically embedded. For instance, if we can identify that elements 1, 52, and 401\nencapsulate the semantics of political views in texts, while elements 5, 203, and 628\npertain to accent-related features, we can utilize the coefficients derived from classifiers\nlike SVM to elucidate the underlying source of separation. If an SVM classifier assigns high\ncoefficients to elements 1, 52, and 401 for two chambers, it signifies that the polarization\nbetween them is rooted in the political views of the users. Similarly, heightened coefficients\nfor accent-related elements in the embedding vector would indicate accent-related features\nas the source of polarization.\nData & Code Statement\nFor reproducibility and to facilitate future research on the topic, we release our en-\ntire code and anonymized data on GitHub at https://github.com/vahidthegreat/\ntransformer-based-echo-chamber-detection .\nEthical Considerations\nOur research is meant to help social scientists, offering a quantified perspective of\nthe Echo Chamber effect, and for online moderators and policy-makers to track and\nmitigate online polarization and radicalization. Our dataset does not contain any private\ninformation. We do not publish author names, IDs, or any information that could be\nused to identify individuals to respect the privacy of Twitter users. The final results are\nfully replicable as we open-source our tool, and share anonymized data and the methods\nwe have used to collect it.\n5Cross-Partisan Interactions onSocial Media\nAbstract\nBuilding on the findings of Chapter 4, this chapter investigates the content and dynam-\nics of Cross-Partisan Interactions (CPIs) on social media, specifically examining whether\nthe observed diversity in discourse among users with Democratic-leaning viewpoints trans-\nlates to more productive conversations across ideological divides. Utilizing LLMs as an-\nnotation tools, this chapter compares sentiments and stances expressed in both partisan\nand cross-partisan interactions. Our content analysis suggests that although Democrats\nengage more frequently in cross-partisan interactions, their participation often includes\nmore negative and nonconstructive stances, unlike Republicans who maintain a more\nconsistent tone across interactions.\n5.1. Introduction\nThe rise of social media has profoundly transformed political discourse, presenting\nboth opportunities and challenges for democratic communication. While these platforms\nare often criticized for creating \u201cEcho Chamber\u201d that reinforce existing beliefs and deepen\nsocietal polarization, emerging research suggests a more nuanced landscape of interaction.\nCross-partisan interactions (CPIs) represent a critical lens through which we can under-\nstand the potential for digital platforms to bridge ideological divides or exacerbate existing\ntensions.\nThis study investigates the complex dynamics of cross-partisan interactions by sys-\ntematically examining the content characteristics \u2013 particularly sentiment and stance \u2013\nthat distinguish partisan from cross-partisan exchanges across parties (Republicans and\nDemocrats).\nOur research builds upon prior findings that Democrats are more likely to engage\nin cross-partisan interactions [15], yet their participation is often characterized by nega-\ntive communication patterns. Specifically, Democratic participants tend to adopt more\n57\n58 Cross-Partisan Interactions on Social Media\ncritical, accusatory, and hostile stances during cross-partisan exchanges, in contrast to Re-\npublicans, who demonstrate a more consistent communicative approach across partisan\nand cross-partisan interactions.\n5.2. Data\nWe define a Cross-Partisan Interactions (CPI) as a direct interaction between two users\nof different political orientations. We use Twitter (X) as the platform to study. Twitter\nfeatures four types of interactions between users: retweets, likes, quotes, and replies. We\nlimited our analysis only to replies, as only replies provide evidence that people intend to\nengage in a direct reciprocal interaction that can lead to a dialogue [116, 117].\nTo study CPIs, we first collect a dataset of interactions in the form of replies, replied\ntweets, and root tweets. We then employ political orientation detection to identify parti-\nsanship and discover cross-partisan interactions.\nWe limit our focus to the U.S. context and define partisanship as left-aligned (leaning\ntowards liberals or Democrats) or right-aligned (leaning towards conservatives or Repub-\nlicans). We limit the data period to 2020 as it captures the general discussions, such as\nthe pandemic (often non-political) and the political discussions related to the 2020 U.S.\npresidential election.\n5.2.0.1. Replies & Roots:\nTo have an unbiased sample of replies, we employed the 1% random sample of Twitter\nprovided by the Internet Archive [118]. The dataset comprises 3,029,231 reply tweets in\nEnglish, responding to 2,299,444 unique tweets. However, on Twitter, tweets can be part\nof a reply chain. To simplify the analysis, we discard the nested replies and limit the\nanalysis where the replied tweet is not a reply of the original tweet (namely, root). This\nbrings the dataset to 1,925,010 direct replies (63.5% of all replies), replying to 1,227,346\nroot tweets. There are 708,929 unique repliers and 254,494 root authors.\n5.2.0.2. Political Orientation:\nWe employ the methodology of [15] to measure users\u2019 political orientation. The\nmethod uses Bayesian inference on users following data to assign a political orientation\nscore to them, which ranges between -5 and +5. Negative values signify leaning toward\nDemocrats and positive values mean leaning toward Republicans. Our dataset with only\ndirect replies contains 875,650 users. Among these, 61,655 users (7.0%) are not assigned\na score due to the absence of the following data and are excluded from the analysis. We\nsee 529,464 users classified as left-aligned and 242,763 classified as right-aligned due to\nhaving an absolute score above 0.1. There are 41,768 users with a score between -0.1 and\n5.3 Methodology 59\n0.1, that are considered neutral and discarded from the analysis.\n5.2.0.3. CPI Data:\nWe classify an interaction as a CPI if the replier and the root author are assigned a\ndifferent political orientation. There are 661,661 replies classified as CPI (%34). Of these,\n196,642 are from Republicans replying to Democrats, making up 33.2% of all Republican\ntweets, and 432,004 are from Democrats replying to Republicans, accounting for 34.3%\nof all Democrat tweets. Cross-partisan interactions originating from Democrats make up\n65% of all CPIs.\n5.3. Methodology\nTo better understand user interactions, we characterize tweets with annotations de-\nscribing their sentiment and stance. We use these annotations to perform a comparative\ncontent analysis. Due to the large size of our dataset, we resort to automated charac-\nterization mechanisms. In particular, we leverage state-of-the-art LLMs. We prompt the\nroot tweets and replies to an LLM and task the model to describe them using three adjec-\ntives. For the replies, the task is to describe the stance against the root tweet with three\nadjectives while we offer both the reply and the root tweet to the LLM. Since root tweets\nare not usually directed to another tweet, we ask the LLM also to annotate its sentiment .\nThis method is an alternative to constraining LLMs by predefined classes and helps us\nqualitatively analyze the sentiment and stances of the tweets.\nTable 2.1 provides sample prompts passed to LLM and the completion provided by\nLLM. Later, in Section 5.5.1 we discuss the reason and limitations of our choice of prompt\nengineering and possible future configurations.\nThe interactions we characterize in our dataset are of four categories: Democrats\nreplying to Democrats (D \u2192D), Republicans replying to Republicans (R \u2192R), Republicans\nreplying to Democrats (R \u2192D), and Democrats replying to Republicans (D \u2192R). As, for a\nfair comparison, we intend to have a balanced amount of annotations from each interaction\ntype, we randomly sample 100,000 (the approximate size of the smallest category of\ninteraction) tweet-reply pairs resulting in a total of 400,000 instances.\nWe employ \u201c Mistral-7B-Instruct-v0.2 \u201d .1This open-source model allows for efficient\nprocessing of the large dataset while maintaining adequate language-understanding capa-\nbilities. We downloaded the model locally from the Hugging-Face. Each query completion\ntook \u02dc5 seconds on an NVIDIA A100 80GB GPU. Due to the light size of the model, we\nparallelized the process into 10 folds and obtain the answers in \u02dc10 days.\nWe validate the annotations by manually inspecting a random sample of 100 of the\nAI-annotated tweet replies and labeling them as correct or incorrect. The LLM ( Mistral-\n1https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n60 Cross-Partisan Interactions on Social Media\n7B-Instruct-v0.2 ) performs 97% accuracy for the sentiments of root tweets and 88% ac-\ncuracy for the stances of reply toward root tweets. A caveat is that since we do not have\na predefined set of classes, the human annotator could not provide labels beforehand.\nAlthough not classified as incorrect, we observe that LLMs sometimes annotate the sen-\ntiment of the reply instead of its stance, e.g., \u201c happy \u201d instead of \u201c happy for them \u201d or\n\u201csupportive . \u201d We do not correct these annotations and leave them as a limitation of this\napproach.\n5.4. Results\nWe leverage the LLM annotations (root sentiments and reply stances) generated in\nSection 5.3.\n5.4.1. Stance Contrast, PI vs. CPI\nOur initial phase of content analysis is to investigate how users of different parties\ndifferentiate in terms of stance when interacting with in-group (PI) and out-group (CPI)\nusers. This can potentially shed light on the productivity of CPIs across the two parties.\nWe compute the frequency differences of each AI-annotated stances of the replies\nacross the PI and CPI of every party (i.e. D \u2192D vs. D\u2192R, and R\u2192R vs. R\u2192D) and\nnormalize them by the average of the annotation frequency between them. Then, we\nvisualize the contrast for every annotation in a pyramid bar chart. Let fGright\ni denote the\nfrequency of the AI-provided annotation iin the group on the right side of the bar, the\nx-axis of the chart is calculated as in Equation 5.1:\nXi=fGright\ni\u2212fGleft\ni\n(fGright\ni +fGleft\ni)\u00d70.5(5.1)\nA positive (negative) value indicates a higher frequency in the right (left) group.\nFigure 5.1a presents the word frequency differences between Democrat-to-Democrat\n(D\u2192D) and Democrat-to-Republican (D \u2192R) interactions. Notably, D \u2192D interactions\nare characterized by words associated with empathy, positive sentiment, and agreement,\nsuch as \u201c happy \u201d, \u201creciprocal \u201d, and \u201c empathetic \u201d . Conversely, D \u2192R interactions exhibit\nwords indicative of conflict in stance and negative emotions, like \u201c accusatory \u201d, \u201cdismis-\nsive\u201d, \u201chostile \u201d, and \u201c critical \u201d .\n5.4 Results 61\n(a)D\u2192Dvs.D\u2192R\n (b)R\u2192Rvs.R\u2192D\nFigure 5.1: Stance-wise differences of partisan vs.cross-partisan replies across parties.\nThe bar labels indicate the overall frequency of the annotation. For samples of tweets for\neach annotation (stance), see Table 5.2.\nHowever, we do not observe the same pattern in R \u2192R and R\u2192D interactions as shown\nin Figure 5.1b. Other than the term \u201c agreement \u201d for partisan interaction vs. the term\n\u201cunsupportive \u2019 for their cross-partisan behavior, the Republicans\u2019 replies to the in-group\nand out-group users are more uniform in attitude than Democrats\u2019 replies to in-group\nand out-group users. Moreover, the sizes of bars, that represent the level of contrast,\nare notably larger in Figure 5.1a than in Figure 5.1b which further suggests a higher\ncontrast in Democrats\u2019 cross-partisan and partisan interactions than Republicans. These\nfindings suggest that although according to [15] Democrats are more likely to engage\nin cross-partisan interactions, those interactions are more likely to exhibit negative and\nconfrontational language compared to within-party interactions, a discrimination that is\nless salient in Republicans\u2019 PIs vs. CPIs.\n5.4.2. Root Sentiment vs. Reply Stance\nOur next content-based analysis focuses on identifying the underlying sentiments that\nare more likely to foster certain stances in replies. We employ the top 10 most frequent\nadjectives from the root tweets\u2019 sentiments and stances of the replies to create contingency\nmatrices for both categories of interactions (PI and CPI). Using Chi-Squared test \u03c72=\n/summationtext(Observed\u2212Expected )2\nExpectedwe compute the association between each sentiment \u2194stance pair.\nFigure 5.2 visualizes the sentiment \u2194stance relationship by a triangular heatmap, where\neach cell was divided into two: the top-left triangle shows the values for the partisan\ninteractions, while the bottom-right triangle represents cross-partisan interactions. The\ncolor scale, ranging from blue (-1) to red (+1) with white at 0, reflects the strength\nof the observed versus expected values. Therefore, a positive (negative) value in each\ncell indicates a positive (negative) association between the corresponding root sentiment\nand reply stance. Each cell on Figure 5.2 showsObserved\u2212Expected\nExpectedto indicate positive and\n62 Cross-Partisan Interactions on Social Media\nnegative associations and the p-values. The p-values are computed after taking the square\nof the nominator as in the original setting.\nFigure 5.2: Chi-test statistics \u03c7=Obsereve\u2212Expected\nExpectedfor co-occurrences of sentiments in\nroot tweets and stances in replies (Top-Left: PI, Bottom-Right: CPI). Starred cells indi-\ncate p-values below 0.05.\nThe heatmap reveals that certain root sentiments are more likely to encourage similar\nsentiments/stances in the replies. For instance, we observe that \u201c eager \u201d sentiment in\nthe root tweets often lead to \u201c eager \u201d and \u201c motivating \u201d stances in replies, with values\nhighly above 0, indicating a strong positive correlation. Similarly, \u201c Positive \u201d sentiments\ngenerally elicit more \u201c motivating \u201d responses, while \u201c critical \u201d sentiments lead to stances\nlike \u201c dismissive \u201d, \u201ccritical \u201d, and \u201c sarcastic \u201d, all with values greater than 0. In contrast,\n\u201cskeptical \u201d and \u201c critical \u201d sentiments discourages \u201c eager \u201d and \u201c motivating \u201d stances in\nreplies, with values strongly below 0.\nWe also observe that these patterns are relatively consistent across both partisan and\ncross-partisan interactions in the heatmap. To further validate this, we extend the scope\nof the contingency matrix to the 50 most frequent sentiments and stances (a contingency\nmatrix with 2500 \u00d72 cells) and detect a Pearson correlation of 95% between the values\ncalculated for PIs and the values calculated for the CPIs. This may suggest that root\n5.5 Discussion 63\nsentiment is not a very strong factor for fostering CPIs compared to users and topics.\n5.5. Discussion\nThe results provide valuable new insights into online CPIs from a multi-fold perspec-\ntive, i.e.: user, topic, and content. Although Democrats are more likely than Republicans\nto engage in CPI, with respect to their own PI, their engagement is more likely to con-\ntain negative (e.g. \u201c critical \u201d, \u201cskeptical \u201d, \u201cchallenging) and sometimes nonconstructive\nstances such as \u201c accusatory \u201d, \u201cdismissive \u201d, \u201chostile \u201d (see Figure 5.1a. Whereas, in terms\nof stance, Republicans discriminate less between when they are replying to Republicans\n(PI) and when replying to Democrats (CPI) (see Figure 5.1b).\nMoreover, users who talk in a friendly way, receive less toxic or critical replies. For\ninstance, in Figure 5.2, \u201c eager \u201d and \u201c positive \u201d sentiments in the root tweets associate with\n\u201cmotivating \u201d and \u201c eager \u201d stances in the replies and correlate negatively with \u201c dismissive \u201d\nstance. On the other hand, negative sentiments in root tweets are more likely to provoke\nnonconstructive and negative stances in the replies (e.g. \u201c dismissive \u201d or \u201c critical \u201d). In-\nterestingly, this phenomenon is independent of whether the interaction is a CPI or a PI\nas we observed a 95% correlation for the values in Figure 5.2.\n5.5.1. Limitations\nContinuous CPIs : The CPIs between users close to the political center, and the CPIs\nbetween extremes may be inherently different. We initially experimented with a continu-\nous CPI value by multiplying the political orientation score of the poster and the replier.\nHowever, we did not notice a drastic change in the results of our initial experiments. We\nplan to address a deeper analysis as part of our future work.\nReply chain and media : To simplify the analysis, we limit it to root tweets and their\ndirect replies, and discard the replies of replies. Our content analysis is limited to the\ntextual content present in the tweets. The analysis of the links, images, and videos in the\ntweets falls beyond our scope.\nLLM\u2019s annotation : We employed a heuristic approach for LLM annotation. While\nthe chosen LLM and prompts provide a foundational framework for large-scale social\ncomputing and annotation tasks, they may not represent the optimal configuration. For\ninstance, the number of adjectives requested from the LLM could be adjusted to capture\nmore granular nuances. Our configuration of prompting was obtained through trial and\nerror with our LLM. When we tried not setting a limit for the number of adjectives, it\ncaused Mistral-7B-Instruct-v0.2 to generate sentences rather than words, possibly because\nin that case, we couldn\u2019t provide limited placeholders for the adjectives ( \\n adjective 1:\n\\n adjective 2:\\n adjective 3:) as in Table 5.1. Moreover, in some cases, the LLM did\nnot differentiate the concept of stance and sentiment when asked to annotate the stances\n64 Cross-Partisan Interactions on Social Media\nof the replies (e.g. the word \u201cloving\u201d in Figure 5.1 is more of a sentiment than a stance ).\nHowever, we find such confusions to be rare, and we observe that they do not hinder the\nobjective of our analysis. This is because understanding any salient semantic quality in\nthe content of replies is insightful for us; whether it is stance orsentiment . We leave\nexperiments with other LLMs and configurations to future work.\n5.6. Conclusion\nThis work highlights critical implications for understanding online political commu-\nnication. While Democrats manifest a higher propensity for cross-partisan engagement,\ntheir interactions are often characterized by negative communication patterns, poten-\ntially undermining the constructive potential of these exchanges. Republicans, in con-\ntrast, maintain a more consistent communicative approach across different interaction\ncontexts. These insights suggest that the mere presence of cross-partisan dialogue does\nnot guarantee meaningful understanding or reduced polarization.\nFuture work can easily deploy alternative Large Language Model (LLM) models, which\ncould offer more nuanced annotations. Our choice to use \u201c Mistral-7B-Instruct-v0.2 \u201d was\nlargely motivated by its open nature, and the human-driven validation of its annotation\nproved it to be reliable. Moreover, as discussed in Section 5.5.1, our prompt-engineering\nstyle for this task is purely heuristical and may not be the optimal setting for its intended\ntask. This may also be improved in future work through more extensive trials and errors.\nFor this, we posit that this study serves as a preliminary demonstration of the potential\napplications of LLMs in this domain, and further research is needed to refine and optimize\nthe implementation of our methodological steps.\nStance Root Tweet Reply Tweet Interaction\npatriotic I\u2019m Proud To Be An\nAmerican!@USER Love the song.\nHappy tears of pride.R\u2192R\nPrayerful The Lord will make you\ngreat..@USER The Lord will\nmake me great. Amen.R\u2192R\nConspiratorial Giuliani Rips Fauci,\nSays US Paid for \u2019Damn\nVirus That\u2019s Killing\nUs\u2019 [LINK]@USER The Deep\nState at work with\ntheir cronies #Crook-\nsandCommunistsR\u2192R\ncertain You Vote: Do you think\nAntifa is a domestic ter-\nrorist organization? \u2014\nJust The News [LINK]@USER Yep without a\ndoubtR\u2192R\n5.6 Conclusion 65\nunsupportive Your support means ev-\nerything. I will never\nstop fighting for our\nmovement. [LINK]@USER You don\u2019t have\nmy support.R\u2192D\nimaginative If life were a 90s Fox\ndrama, Harry and\nMeghan would abandon\nthe royalty to start a\nfresh ad agency in LA.@USER Set it at\nChristmas and you\nhave a Hallmark movie\nin the makingR\u2192D\nproactive The Pope says tax\nevaders have stolen\nfrom the Government\nand weakened Italy\u2019s\nhealth scheme and are\nmurderers [LINK]@USER We need to\nstop cash in hand as\nwell, it\u2019s rife, especially\nin London #Lockdown-\nNowR\u2192D\nreciprocal Shots from the PEACE-\nFUL #BlackLivesMat-\nter protest in Austin to-\nday. City made me\nproud! [LINK]@USER I was there\ntoo. Incredibly hope-\nful to see so many peo-\nple show up for change.\n#BLMprotestD\u2192D\nsimilar Admit it. You wish\nit was Sunday already.\n#KillingEve@USER Oh my god yes\n.. I\u2019m obsessed tooD\u2192D\nrelatable I\u2019m at work. I\u2019m always\nat work. I\u2019m losing my\nmind.@USER Oh good, I was\nstarting to think I was\nthe only oneD\u2192D\nfascinated #WhenTheLockdownEnds\nI\u2019m going to party like\na Maya Ruler in a\nweird lobster costume!\n[LINK]@USER Wow! Is that\nfrom Bonampak? What\na party!D\u2192D\nempathetic RIP Grandpa.... We\nlost him to COVID-19\nlast night@USER I\u2019m so sorry D\u2192D\naccusatory Why bother to tweet\nabout this, of all things?\n[LINK]@USER He\u2019s happy to\nhave people dying to\nimprove his ratings.D\u2192R\n66 Cross-Partisan Interactions on Social Media\nhostile This is critical evidence\nconfirming what we al-\nready knew to be true\n\u2014 China lied. China\ncontinues to lie. China\nmust be held responsi-\nble [LINK]@USER shut up, traitor D\u2192R\ndismissive Trump, reading, says\nCOVID-19, then adds,\n\u201dYou know what that\nis? Right. Become a\nvery famous term. C-O-\nV-I-D. COVID. \u201d@USER He\u2019s a child. D\u2192R\nskeptical New: Biden says he will\nchoose his running mate\nnext week - CNNPoli-\ntics [LINK]@USER He said this im-\nmediately after becom-\ning the nominee.D\u2192R\ncritical President Trump says a\nnew, faster coronavirus\ntest is going to be used\nsoon. He says: I hope\nthe new test works out.\nHopefully it will check\nout or test out. It will\nbe a very simple test. It\nwon\u2019t be unpleasant at\nall.@USER Trump doing\nthe right thing after ex-\nhausting all other op-\ntions.D\u2192R\nTable 5.2: Sample tweets for AI-generated stances for replies.\nAdjective Root Tweet Party\nsignificant Meanwhile, in other news, this important summit meeting\nto develop strategies to fight the pandemic.Democrat\nshameful There is no greater embarrassment in the House of Repre-\nsentatives than Jim Jordan.Democrat\nmoral Anyone who thinks they need to go forward with an exe-\ncution in this moment shouldn\u2019t ever have the authority to\ncarry out executions.Democrat\n5.6 Conclusion 67\nalert We are making progress. Some good recovery numbers are\ncoming in. GHS should be announcing soon. Doesn\u2019t mean\nwe can let down our guard and live without care. Let\u2019s keep\nobserving the preventive etiquette.Democrat\nperspective To people complaining about the wrong statues getting re-\nmoved: if the right statues had been removed earlier or\nNEVER placed, you could have avoided all this.Democrat\nresponsible If we\u2019re going to rise to this moment with the attention and\naction that it fully deserves, we better start listening. We\nbetter own up to our own responsibility that led to this\nmoment. And we better start inviting change.Democrat\nproactive NEW: New York State will require all hospitals to have on\nhand a 90-day supply of PPE at quantities sufficient to meet\nthe rate of use during the worst of this crisis.Democrat\nbalanced The level of anger directed at the media from these\nprotestors was alarming. As always, I will tell a fair and\nunbiased story today.Democrat\noverwhelmed I cannot work. The Christmas break has broken me Democrat\nhumorous My wife is a teacher. There\u2019s herding cats, and then there\u2019s\n\u201dwebinar with 20 1st graders\u201dDemocrat\nintimate My mom just passed away in her sleep. Does not appear\ncovid related. I\u2019m numb.Democrat\ngentle Good morning sweet girl \\nHave a great day. Democrat\nunempathetic She can stand in the unemployment line.. Democrat\nunconventional There are no rules... for breakfast I just had garlic cheese\nbread along with my berry protein shake \\nWhateverDemocrat\ntired I need to be in bed. #QuarantineLife Democrat\nsurreal Even the bedbugs are wearing masks. Democrat\nisolated Am I the only person who\u2019s never watched Friends? Democrat\nmisleading The President is rambling, reading a script full of badly\nformed theories. And lies.Republican\ninteresting This is the most bizzare part of a fascinating thread about\nhow Labour responded to the 1992 defeat.[link]Republican\narrogant Trump: \u201cWe\u00b4 re doing a job the likes of which nobody\u2019s ever\ndone. \u201dRepublican\nradical AI is more dangerous than nuclear weapons. Republican\n68 Cross-Partisan Interactions on Social Media\ndiplomatic US Secretary of State Mike Pompeo in his opening remarks\nin #Doha says that the Taliban have committed to cut their\nties with terrorist groups. #Afghanistan #PeaceDeal [link]Republican\nformal President Trump announces intent to appoint Dr. Sebastian\nGorka, of Virginia, to be a Member of the National Security\nEducation Board for a term of four years.Republican\nself-centered \u201dWe understand there are still many sick and dying...But\nwhat about the rest of us?\u201d [link]Republican\nalarmist Our national media is ROOTING for hydroxycloroquine to\nnot work as a treatment for #Covid 19. Think about that.Republican\namused Just got a PR pitch that includes the word \u201ccovidpreneur\u201d Republican\naccusatory The Chinese Communist Party is responsible for every single\ndeath, every job lost, every retirement nest egg lost, from\nthis coronavirus. And Xi Jinping must be made to pay the\nprice.Republican\nnegative A rebound in 2021 will not happen if we keep doing this...\n[link]Republican\ndisgusted What kind of man uses a funeral to push a political agenda?\n[link]Republican\noutraged Wake the f*** up America! [link] Republican\nsarcastic A face mask is part of the \u2019scamdemic,\u2019 they say. But they\u2019ll\nbe happy to sell you one [link]Republican\nprayerful Please Lord let it be so. [link] Republican\nprejudiced Black people love acting like kings, queens and African war-\nriors, but never like normal Americans. [link]Republican\nconspiratorial The World your mind has been MK Ultra Programmed into\nbelieving you live in, bears no resemblance to the World you\nare actually living in.Republican\nTable 5.3: Sample tweets for AI-generated root sentiments.\n5.6 Conclusion 69\nRoot\nTweetVideo footage of WHITE people fliping over a cop car in SLC, Utah that\nlater caught on fire. Our protest was peaceful. Black organizers wanted\nto remain peaceful. White people are responsible.[LINK]\nReply\nTweet@USER Like half of the people flipping the car are black, what the fuck\nare you talking about?\nPrompt You are analyzing a tweet on Twitter.: tweet: [ROOT TWEET] With-\nout any explanation, provide three adjectives representing different sen-\ntimental qualities of the tweet. For every adjective you intend to provide,\nuse their simplest form in English: \\n adjective 1:\\n adjective 2:\\n\nadjective 3:\nResponse\nMistral-\n7Bagitated: The tweet expresses a sense of agitation towards the actions\nof the white people in the situation. peaceful: The tweet emphasizes\nthat the protest was peaceful, suggesting a calming or soothing quality.\nresponsible: The tweet assigns responsibility to the white people, im-\nplying a sense of seriousness or gravity.\nPrompt You are analyzing an interaction between two people on Twitter.\nUser A has posted a tweet and User B has replied to it: User A:\n[ROOT TWEET] User B: [REPLY TWEET]. Without any explanation,\ndescribe the stance of user B\u2019s reply toward user A\u2019s tweet with three\nadjectives. Make sure that the adjectives you provide are with regards\nto the stance that is directed toward user A and/or his/her opinion, not\ntoward an external concept or person. For every adjective you intend to\nprovide, use their simplest form in English: \\n adjective 1:\\n adjective\n2:\\n adjective 3:\nResponse\nMistral-\n7B1. Skeptical: User B expresses doubt about the accuracy of User\nA\u2019s statement that only white people were flipping the cop car. 2.\nConfrontational: User B challenges User A\u2019s assertion directly and\nforcefully. 3. Curious: User B seems interested in obtaining more in-\nformation and clarification about the event described in User A\u2019s tweet.\nTable 5.1: Sample prompt and response for LLM-aided annotation of interactions\n70 Cross-Partisan Interactions on Social Media\nPart II\nRadicalization\nIn Part I we proposed approaches for measuring the degree of Echo Chamber effect and\npolarization in general settings. When the degree of polarization peaks to its extremes, it\ncan result in radicalization. Extreme views such as sexist, racist, homophobic, xenophobic,\netc. contents all fall under the umbrella of radicalization in social media.\nIn this part of the thesis, we analyze two forms of radicalization. In Chapter 6 we\nintroduce a holistic model based on the combination of the unsupervised Word Embed-\nding Association Test and semi-supervised text classification for measuring gender-based\npolarization and sexism on the corpus level. We also discuss how this model can be gen-\neralized to measure other forms of polarization. In Chapter 7 we investigate the effect\nof social media platforms on the type and prevalence of radical and toxic content. Our\ndataset of analysis is the textual content of cross-platform communities; communities that\nexist in two social media platforms simultaneously (here, Reddit and Discord).\n71\n\n6Gender-based Polarization andSexism\nAbstract\nIn this chapter, we extend the focus in Chapters 4 and 5 from generic polarization\nto radicalization as an extreme manifestation of polarization. We introduce a new model\nfor measuring online sexism in gender discourse communities, combining supervised NLP\nmethods for toxicity detection with unsupervised techniques to identify targets of harmful\nspeech.\nOur model provides a comparable holistic indicator of toxicity targeted toward male\nand female identity and male and female individuals. Despite previous supervised NLP\nmethods that require annotation of toxic comments at the target level (e.g. annotating\ncomments that are specifically toxic toward women) to detect targeted toxic comments,\nour indicator uses supervised NLP to detect the presence of toxicity and unsupervised\nword embedding association test to detect the target automatically.\nWe apply our model to gender discourse communities (e.g., r/TheRedPill, r/MGTOW,\nr/FemaleDatingStrategy) to detect the level of toxicity toward genders (i.e., sexism). Our\nresults show that our framework accurately and consistently (93% correlation) measures\nthe level of sexism in a community. We finally discuss how our framework can be general-\nized in the future to measure qualities other than toxicity (e.g. sentiment, humor) toward\ngeneral-purpose targets and turn into an indicator of different sorts of polarizations.\n6.1. Introduction\nPolarization and radicalization of opinion on social media have been a hot topic of\nresearch in the recent Computational Social Science literature [20]. One type of polariza-\ntion on social media can be based on people\u2019s views about gender roles and identity which\ncan be partially observed by looking into the use of biased language on different sides of\nthe online gender discourse spectrum. For instance, prior work has studied the use of\ntoxic and misogynistic language in manosphere (e.g., r/TRP, r/MGTOW) communities\n73\n74 Gender-based Polarization and Sexism\non social networks [119, 120, 121, 122]. However, there is a wide gap in both qualitative\nand quantitative studies offering a measure that can precisely quantify thelevel of sexism\ninside every community at scale. In other words, previous research tells us that commu-\nnity A is sexist, but it doesn\u2019t say \u201c how much exactly. \u201d; or \u201c between community A and B\nwhich one is more sexist? \u201d\nThe quantification of sexism and other forms of polarization on social media has been\na challenging task for researchers in the field of Computer-Supported Cooperative Work\n(CSCW) [123, 124, 125]. A reliable metric for quantifying sexism would be a valuable\ntool for both researchers and practitioners in the CSCW community. For researchers, it\nwould provide a more nuanced understanding of the dynamics of online gender discourse\nand enable them to investigate the relationship between sexism and other aspects of\nonline communities, such as participation patterns, social norms, and the effectiveness of\nmoderation policies [126, 127]. For practitioners, such a metric could be used to identify\nand address problematic behavior within online communities, promote more inclusive and\nequitable online spaces, and develop more effective anti-discrimination policies [128, 125,\n129].\nIn this chapter, we define a macroscopic scalar indicator that can give us an overall\nmeasure of the total toxicity aimed toward male and female identity in a community (in\nour case-study, subreddits). Our scalar indicator is based on the combination of three\nparameters for each adjective inside a community where each parameter preserves one of\nthe following key qualities of our work models: 1) How toxic is a word\u2019s context within\na community\u2019s discourse? 2) How frequently it has been used inside its corpus? 3) How\nbiased is the word toward a gender in that community?\nThe first parameter is based on a supervised NLP model that detects whether a\nsentence is toxic or not; without the need to judge the target of the toxicity. Then it\ncomputes the rate of a word\u2019s appearance in a toxic sentence to calculate the toxicity of\na word\u2019s context. This is more reasonable than previous works that solely look into the\npolarization [65] or toxicity [121] of words using a dictionary of polarized or toxic lexicons\nas a word can appear less or more toxic in different discourses.\nExisting methods suffer limitations in identifying the target towards which toxicity\nis directed as when it comes to annotation, the toxic comments towards a very specific\ngroup identity are sparse. Measuring targeted toxicity toward various group-identities in\na fully supervised manner requires a separate manual annotation of comments that are\nspecifically toxic toward each group. One of our contributions is to keep the supervision in\nthe first parameter, to merely decide the toxicity rate, and introduce the third parameter,\nwhich is unsupervised, to measure the target of the toxicity automatically.\nThe third parameter is based on the idea of Word Embedding Association Test\n(WEAT) [37] that defines the gender bias of a word by looking into its word-embedding\ncosine similarity with embeddings of gender-related words (e.g. woman, she, female),\n6.2 Related Work 75\nnamely \u201cattribute sets\u201d . However, it makes no distinction between bias toward gender\nidentity and individual characters from a gender. Meaning that there are cases where\nindividual female characters, like a female politician, are targeted rather than all women\nas a group-identity. This obfuscates the quantification of gender bias, making metrics\nindicating the level of bias coarse-grained and ineffective at distinguishing other under-\nlying motives (i.e. political motives). Thus, attributing an adjective to several female\ncharacters using existing works is computationally equivalent to attributing it to women\nin general, since most works based on word-embedding associations, mix both gender\nidentity terms (e.g. men, women) and gender pronouns (e.g. he, she) in their attribute\nsets. By separating the two, we define two complementary indicators; one indicating the\ntoxicity toward male/female identity, and one measuring the toxicity toward individual\nmale/female figures. These two indicators are more informative on their own than when\nthey are aggregated.\nIn summary, we make the following contributions:\nWe propose a model that can measure various sorts of polarization on social\nnetworks through a scalar value that can be used to compare disparate communities.\nWe offer a clear distinction between toxicity targeted toward gender identity\nand toxicity targeted toward individual male and female characters and we quantify\neach of them separately.\nWe calculate the toxicity of words based on their context in a corpus to address\nthe limitations of previous context-unaware lexicon-based approaches [130, 131, 121].\nFinally, we apply a unique holistic model to several subreddits from various sides of\nthe gender-discourse spectrum and report the targeted toxicity level for each.\n6.2. Related Work\nSince our sexism indicator combines the notion of unsupervised word-embedding as-\nsociations with a supervised toxic comments classification, we divide our literature review\ninto two subsections. In the first part, we discuss the previous works which have tried\nto quantify language bias based on word-embeddings, and in the second subsection, we\nreview some previous efforts on toxic comment classification.\n6.2.1. Language Bias Quantification Based on Word-Embeddings\nIn [36] developed Implicit Association Test (IAT) as an experimental method for iden-\ntifying such implicit biases for every user [36]. The test tends to measure the strength of\nimplicit associations between attribute concepts (e.g., black people, or LGBTQ+ mem-\nbers) and evaluations (e.g., good, or bad) or stereotypes (e.g., athletic, or clumsy) based\non the time it takes for a user to assign each word to the attribute concepts.\n76 Gender-based Polarization and Sexism\nInspired by IAT in clinical psychology, Caliskan et al. [37] leveraged the emergence\nof word-embeddings in NLP, to develop the Word Embedding Association Test (WEAT)\nin order to confirm the existence of similar implicit/explicit associations; based on the\nrelative distances of attribute words vectors with target concepts\u2019 word vectors. For\ninstance, WEAT shows that science-related terms\u2019 vectors are closer to the word \u20d7 manthan\nthe word \u20d7 woman, in contrast with art-related terms which have more cosine similarity\nwith the word \u20d7 woman than\u20d7 man.\nHowever, since the sets of target words (e.g., science, art) and attribute words (i.e.,\nany dual concept, like men and women) in WEAT are determined by humans, the user can\ncherry-pick the set of terms to witness the desired outcome [64]. This means that WEAT\nand subsequent works are more suitable when the researcher is aware of a predefined set\nof biased concepts in real-world data (e.g., IAT test in clinical psychology research) and\nis trying to validate that those biases appear in a text-corpus.\nThere is scarce prior work aiming at discovering biases in word-embeddings rather\nthan confirming them [65]. [132] makes some attempt in this direction, however, their\nwork still relies on crowd-sourcing and human judgment to assess if the biases in the\nword-embeddings match prevalent stereotypes in the real world. Moreover, prior work\nbased on word-embedding associations merely focuses on quantifying gender biases in\ncertain word sets (whether predefined or automated), yet does not offer a systematic way\nto compute an overall measurement of gender bias and sexism that is comparable across\ncommunities.\n6.2.2. Toxic Comment Detection\nRelated work has leveraged NLP to detect different types of toxic language such as\naggression [68], hate-speech [69], and offensive language [70]. Moreover, IberLEF 20211\nhas introduced EXIST,2a hierarchical NLP classification task with an annotated dataset\nofsexist vsnon-sexist tweets at level 1, and a categorization of the type of sexism (if\napplicable) at level 2 (e.g., stereotyping, objectification, sexual and non-sexual violence,\netc.). Next to translation-augmentation methods, participants applied classical and Deep\nNLP on the task where pre-trained Deep NLP models (e.g., BERT) slightly outperformed\nthe classical NLP methods [133, 134, 135].\nThe most relevant to our intention in this chapter is OffensEval20193task shared on\nSemEval20194by [71] that looks into the type and target of the toxicity simultaneously.\nA hierarchical classification task consists of three sub-tasks: First, identification of the\noffensive language (i.e. offensive ornot). The second sub-task would be to detect whether\n1https://sites.google.com/view/iberlef2021/\n2http://nlp.uned.es/exist2021/\n3https://sites.google.com/site/offensevalsharedtask/offenseval2019\n4https://alt.qcri.org/semeval2019/\n6.3 Methodology 77\nthe offense is targeted ornot, and the third is to check whether this targeted offense is\ntargeted toward a group or toward a person . All these works use fully-supervised NLP.\nHowever, fully-supervised NLP toxicity detection tasks are highly prone to distribution-\nshift; an effect that happens where the distribution of the train-set is different from the\ntest-set [72], causing the NLP model to yield lower accuracy on test-sets that are out of its\nsampled train data. Moreover, supervised NLP tasks are also prone to the concept-drift\neffect. Concept-drift happens when \u201calgorithms trained on annotated data in the past\nmay under-perform when applied to contemporary data\u201d [63]. Therefore, reducing the\nlevel of supervision is an important agenda to follow.\nGenerally, for a community analysis level, our framework provides a less supervised\nand more flexible solution for measuring targeted toxicity. Less supervised, because it only\nrequires the data labeled in the first subtask (i.e., offensive/toxic or not), and the second\nand the third sub-tasks will be embedded in the effect of unsupervised word-embedding\nbiases added to the model. More flexible, because the attribute words can be altered\narbitrarily to detect other types of targeted toxicity, and the supervised data can change\nto detect qualities other than toxicity (e.g., polarity). To achieve this goal, we create a\nmodel that combines the existing notion of unsupervised word-embedding associations\nwith novel semi-supervised tasks that build on recent efforts for toxic content detection.\n6.3. Methodology\nFigure 6.1 shows an overview of our processing pipeline, which takes a corpus as input\nand returns an indicator of polarization as output. The area above the top gray dash-\nline shows the unsupervised nature of our work designed to measure the word-embedding\nbiases. The area below the bottom gray dash-line depicts the supervised pipeline we\nbuild to measure the toxicity embedded inside each adjective. The area between the two\ngray lines refers to the frequency-percentile ranking; another parameter that we take into\naccount.\n78 Gender-based Polarization and Sexism\nFigure 6.1: Outlook of our processing pipeline.\n6.3.1. Preliminaries\nThe sexism scalar indicator we provide consists of three variables: Embedded-Toxicity ,\nFrequency-Percentile-Ranking , and Embedding-Bias which we calculate for every adjective\nterm in each corpus separately. In this section, we will explain in detail how we measure\nthese three variables:\n6.3.1.1. Embedded Toxicity\nEvery community has a set of terms and idioms that may preserve different meanings in\nthe context of that community than their universal meanings [65, 136]. Looking at gender-\nrelated discourse, for instance, some terms that are considered neutral when viewed out of\ncontext can actually carry negative sentiments and even toxicity. Words \u201cflirtatious\u201d, and\n\u201chypergamous\u201d can be used in manosphere discourse to manifest negative opinions about\nwomen\u2019s sexual lifestyle, or the word \u201ccasual\u201d can be used to encourage only having casual\nsex with a group of people. The term \u201cunicorn\u201d, for example, is also a common term in\ngender-related communities (e.g. r/TheRedPill) to refer to unrealistic views about an\nideal partner that could also be accompanied by toxic ideas around itself.\nThus, since computing words\u2019 toxicity is our objective in this section, it is vital to have\na metric that computes a word\u2019s embedded toxicity according to its context, rather than\ndictionary-based analyses (e.g. Weaponized-Word5) that pre-define a word\u2019s toxicity\naccording to its global context [137, 138].\n5https://weaponizedword.org/\n6.3 Methodology 79\nFigure 6.2: Processing pipeline for building our Toxicity-Detector NLP model.\nIn order to calculate each word\u2019s embedded toxicity while covering the word\u2019s context,\nwe propose a count-based semi-supervised method. First, we use our annotated toxic\ncomments dataset (see Section 6.4) to build a Toxicity-Detector machine based on a\nsupervised NLP classification model that can predict a comment as non-toxic (0) ortoxic\n(1). Building the model goes through the NLP pipeline as presented in Figure 6.2. We\ninitially clean and preprocess the text (e.g. removing stop-words, removing punctuation\nmarks, lemmatization). Then, we tokenize the sentences and convert them into TF-IDF\nvectors. We add upsampling to balance the classes\u2019 size as the \u201cToxic\u201d class is as 30%\nbig as the Non-Toxic class. Finally, we split the data into train (70%) and test (30%)\nsets and pass it through a Logistic Regression. The F1-Score (macro) on the test-set is\nabove 91%. We also compare this accuracy with an advanced transformer-based model\nin Section 6.5 and show that our classic NLP model maintains a close performance to it\nwhile demanding a significantly lower computational cost.\nAfter building the Toxicity-Detector model, we use it to systematically annotate every\nsentence in a community\u2019s data as Toxic orNon-Toxic (1 or 0). Then, for every adjective\nword that appears in the community, we average through the labels of the sentences that\ncontain the adjective. We also ignore words belonging to other parts of speech as suggested\nby Ferrer et al [65]. Let senjrepresent sentence jin a corpus and ToxicityDetector ()\ndenote the function that calculates sentences\u2019 toxicity. Then, the Embedded-Toxicity of\nwordi,Twi, would be calculated by Equation 6.1:\nTwi=/summationtext\nj{ToxicityDetector (senj)|senj\u220bwi}\n|{senj|senj\u220bwi}|(6.1)\nNow assume, for instance, the word \u201ccasual\u201d in r/TheRedPill which is neutral globally\nyet toxic locally. The Embedded-Toxicity parameter sees a sentence like \u201cYou must only\nexploit her for casual sex and dump her\u201d labeled as toxic, and due to its context gives a\nhigher score to the toxicity of the word \u201ccasual\u201d .\n6.3.1.2. Frequency Ranking\nThe frequency of a word in a community\u2019s corpus is another important parameter\nthat has to be taken into account in the final metric. Considering that we are studying\ntoxicity as a scalable, community-wide metric, it makes sense to amplify the effect of the\nmost frequent types of toxicity, over those that happen rarely.\n80 Gender-based Polarization and Sexism\nOne option to preserve the effect of frequency in our metric is to simply weigh frequen-\ncies and biases. However, according to Zipf\u2019s law, the term-frequency gap inside a corpus\nincreases exponentially as we move toward the top frequent words [139]. This could cause\nthe frequency to dominate other parameters and distort the balance we intend to preserve\nbetween bias, frequency, and embedded toxicity. Thus, we convert the raw frequency of\nadjectives into frequency-percentile-ranking to smooth the effect of the frequency in our\nmodel, i.e.: the percentage of the adjectives that an adjective outnumbers [140]. In ad-\ndition to smoothing, it also creates a more scalable output as it provides a parameter\nbetween 0 and 1, that is compatible with our two other parameters.\nAssuming that Vrepresents the vocabulary of all adjectives in a corpus, and fwi\ndenotes the frequency of word i, the frequency-percentile-ranking of each word FPR wi\nwould be calculated by Equation 6.2.\nFPR wi=|{wj\u2208V|fwi>fwj}|\n|V|(6.2)\n6.3.1.3. Embedding-Bias\nThe final parameter is the embedding-bias which is supposed to measure the level of\nbias a word has toward a targeted concept. We follow the idea of the Word Embedding\nAssociation Test (WEAT) and several subsequent papers in quantifying global gender\nbiases in words based on word embeddings [37, 141, 65] and apply it to different com-\nmunities\u2019 corpora to obtain the gender bias of every adjective in each corpus. This is to\nquantify how much an adjective in a corpus points its finger toward a certain group.\nIn this method, we take two sets of attribute words related to two distinct concepts\n(male and female in this case) and represent each set by the element-wise average of its\nword-embedding vectors.\nLetSA={w0,w1,...w n}andSB={w0,w1,...w n}denote two sets of words that rep-\nresent two different attribute concepts we wish to measure the adjectives\u2019 biases toward.\nIn our case SAandSBare representative sets of words for the concepts \u201cmasculinity\u201d\nand \u201cfemininity\u201d containing the words [\u201cmale\u201d, \u201cman\u201d, \u201cboy\u201d, \u201cmasculinity\u201d, \u201cmascu-\nline\u201d, \u201cdad\u201d, \u201cfather\u201d, \u201cson\u201d] and [\u201cfemale\u201d, \u201cwoman\u201d, \u201cgirl\u201d, \u201cfemininity\u201d, \u201cfeminine\u201d,\n\u201cmom\u201d, \u201cmother\u201d, \u201cdaughter\u201d]. These sets of words are obtained from the combination\nof suggested attribute words by Caliskan et al. and Ferrer et al. [37, 65]. Now let cAand\ncBbe the weighted centroids of SAandSB. We measure each adjective\u2019s relative bias\nstrength towards SAby the subtraction of its cosine similarity with cAfrom its cosine\nsimilarity with cBas in Equation 6.3:\nBwi,SA|SB=cos(wi,cA)\u2212cos(wi,cB) (6.3)\nWe apply this formula to all the adjectives present in each subreddit using Continuous\n6.3 Methodology 81\nBag of Words (CBOW) as our word embedding algorithm; an unsupervised Deep NLP\nalgorithm that is designed to predict a target word based on its context (surrounding\nwords). Thus, ideally, words that appear in the same context tend to have higher cosine\nsimilarities.\nNext to the words related to male identity and female identity, we also use two sets\nof attribute words consisting of male vs female pronouns [\u201che\u201d, \u201chim\u201d, \u201chis\u201d] and [\u201cshe\u201d,\n\u201cher\u201d, \u201chers\u201d] in a parallel analysis to measure the toxicity toward male and female\nindividuals rather than male and female identity. This will be explained in Section 6.6 in\ndetail.\n6.3.2. Sexism Indicator\nTo calculate an indicator that can quantify toxicity toward men and women in a com-\nmunity, we separate each community\u2019s adjectives\u2019 list into biased toward male attribute\nset (man, boy, father, etc.) vs biased toward female attribute set (woman, girl, mother,\netc.). In parallel, we also separate the adjectives\u2019 lists into biased toward masculine pro-\nnouns attribute set (he, him, his) vs biased toward feminine pronouns attribute set (she,\nher, hers) using the same formula. Then, the toxicity targeted toward every attribute set\nis calculated by averaging the three variables introduced in Section 6.3.1.\nEquation 6.4 describes our model for calculating the toxicity towards attribute set SA\nw.r.t. attribute set SBassuming that wiis in the set of adjectives that are biased towards\nSA:\nTargetedToxicity SA|SB=/summationtext\ni{Bwi,SA|SB\u00d7FPR wi\u00d7Twi}\n|{wi}|(6.4)\nConsequentially, if we replace SAwith the female attribute-set and SBwith the male\nattribute-set, the formula would give us a measurement of toxicity toward women. Swap-\npingSAandSBwould quantify the level of toxicity toward men in a community.\nMoreover, we differentiate between toxicity toward female and male identity vs toxicity\ntoward individual female or male figures. These two were often mixed in the previous\nliterature associated with Word Embedding Association Tests. To enable our model to\ndistinguish between the two types of targeting, we also apply attribute-sets of male vs\nfemale pronouns onSAandSB(i.e. [he,his,him ] vs [she,her,hers ]) to get a measurement\nof toxicity targeted toward individual male and female characters. This is critical in the\nsense that a community might target a certain group of male/female folks, yet harass\nthem regardless of their gender. For instance, users of a political online community in\na male-dominated country are likely to target their politicians, that may more likely be\nmale figures, while not being a misandrist community. In that case, a word-embedding\nassociation test that mixes male pronouns (i.e. he, his, him) and male identity terms (e.g.\nman, masculinity, etc.) to form the attribute-set of men might return impure results.\n82 Gender-based Polarization and Sexism\nFor every subreddit (the online community type we analyze), we bootstrap 100,000\ncomments from the total data and repeat the experiment ten times with different seeds.\nThen, we calculate a confidence-interval for each subreddit based on the ten samples. The\nresults for each attribute-set and subreddit are presented in Section 6.6.\n6.4. Datasets\nOur work uses two sources of data: 1) a collection of comments from multiple subred-\ndits, and 2) a set of annotated comments (toxic vs non-toxic). We leverage the former\nto measure the sexism rate of these communities, and the latter to build our supervised\ntoxicity-detector NLP model. Next, we describe our data sources, starting with a brief\nintroduction to the subreddits in our dataset and a characterization of each community\naccording to the previous studies.\n6.4.1. Subreddits\nWe query Reddit using the Pushshift API [142]. In particular, we query all comments\nfrom the following subreddits:\nr/TRP ,TheRedPill , is a sub-movement of The Men\u2019s Rights Activism (MRA) move-\nment that offers advice to men regarding how to protect their masculinity that \u201cis under\nthreat by the society\u201d [143]. It tends to \u201cempower\u201d heterosexual men with seduction\nstrategies by exploiting arguments from evolutionary psychology [144, 145]. On the other\nhand, r/MGTOW ,Men Going Their Own Way , is another manosphere subreddit that\nencourages men to separate their path from women as a means of protection from a soci-\nety \u201ccorrupted by feminism\u201d [146]. Previous literature has categorized both r/TRP and\nr/MGTOW as misogynist subreddits [119] and they were banned from Reddit in 2018 and\n2021 respectively. r/MGTOW2 also seems to be the continuation of the latter subreddit\nand was also banned from Reddit in 2021.\nr/FemaleDatingStrategy defines itself as a \u201cfemale-exclusive subreddit that offers dat-\ning strategies for women who want to take control of their dating lives\u201d . There are reports\nof the community\u2019s tendency to objectify the opposite gender and has been accused by\nr/AgainstHateSubreddits of encouraging transphobic and misandrist attitudes. Yet, there\nare also reports of using misogynist slang such as \u201cpickmeisha\u201d (a woman who lowers stan-\ndards to receive attention from men) and \u201ccockholm syndrome\u201d (when a woman keeps\ngoing back to \u201clow-value\u201d men) [147].\nr/IncelTear defines itself as a community that tends to post screenshots of \u201cmisogy-\nnistic\u201d and \u201chateful\u201d comments from \u201cincels\u201d (involuntary celibates) in order to criticize\nthem sarcastically. Next to the usage of irony, the community also has a record of re-\nposting and quoting extremely misogynist comments from the r/Incels community with\n6.4 Datasets 83\nthe aim of sarcasm [148]. Ironically, r/IncelTear contains even more misogynist terms than\nr/MGTOW [121], probably due to its high rate of quoting the most extreme misogynist\ncomments from manosphere communities.\nr/TrollXChromosomes is a subreddit designed for posting feminist humor and memes\nin order to criticize some aspects of the \u201chegemonic femininity\u201d [149]. On the other\nhand, r/TrollYChromosome is known as a progressive subreddit for men casting sar-\ncasm and humor toward the attitudes of society toward men and masculinity [150].\nr/MensRights ,r/MensLib ,r/theGirlSurvivalGuide ,r/Feminism ,r/AskFem-\ninists , and r/AskWomen are the other subreddits from the gender-related discourse\nwhich we were interested in discovering their attitude to cover the whole spectrum of the\ndiscourse.\n6.4.2. Supervised Toxic Data\nTo build and validate our sexism\u2019s indicator toxicity detector part (recall that the other\nparts of our sexism indicator are unsupervised, and hence do not need to be trained), we\ncombine five different annotated toxic datasets from multiple sources to cover various\nsorts of toxicity:\nOffensEval 20196was one of the tasks in SemEval 20197for detection of offen-\nsive language. It consisted of three sub-tasks. A: Identifying offensive language. B:\nCategorizing the offense. C: Identifying the Target of the offense [71]. We ignore B\nand C and only consider the labels ( offensive 4400 or not-offensive 8840) from task\nA. As explained in Section 6.2, our goal is to cover the C subtask using the effect\nof word-embedding associations.\nA dataset by Kaggle containing three labels ( hate-speech 1430, offensive 19190,\nand4163) [151]. We joined hate-speech and offensive together as the Toxic label\nand neither asnon-Toxic .\nWikipedia Talk Labels dataset containing 100k discussion comments from the\nEnglish Wikipedia [152]. Around 13k of them were labeled as personal attacks which\nwe included in our Toxic class and the rest in the non-Toxic class.\nToxic Comment Classification Challenge8dataset by Kaggle containing 15k\nToxic and 140k not-Toxic annotated comments from Wikipedia.\nJigsaw Multilingual Toxic Comment Classification9data containing 20k toxic\nand 200k not-toxic comments from Wikipedia. We only add the 20k toxic com-\n6https://sites.google.com/site/offensevalsharedtask/offenseval2019\n7https://alt.qcri.org/semeval2019/\n8https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n9https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/rules\n84 Gender-based Polarization and Sexism\nments to our Toxic class and leave the non-Toxic ones for validation purpose (see\nSection 6.4.3).\nAfter merging all the annotated data and splitting them into 70% train-set and 30%\ntest-set, we obtain 52k comments for the Toxic class and 180k for the non-Toxic in the\ntrain-set.\nFor evaluating the built supervised toxicity detector, we retain 30% of the total super-\nvised data for testing the F1-Score of our supervised Toxicity-Detector NLP model. In\nparticular, we create a large dataset of non-Toxic comments by aggregating the non-toxic\ncomments from the 30% test-set and all the 200k non-toxic comments in Jigsaw Multilin-\ngual Toxic Comment Classification . This makes it a total of 270k non-Toxic comments.\n6.4.3. Sexism Indicator Evaluation Datasets\nFor evaluating our final sexism indicator, we use two main datasets:\n1. We collect 1.2m random comments from the most recent comments on Reddit\nthrough the Pushshift API. These comments let us assess how our metric com-\npares when run on a misogynist subreddit (cf. Section 6.4.1) and when run on a\nrandom dataset taken from all Reddit communities.\n2. We assemble an annotated dataset of ground-truth misogynistic comments from\nthree sources: [153], Kaggle,10and [154] with a total of \u224830000 comments anno-\ntated as misogynistic or not ( \u22486000 misogynistic), and use it as described in the\nnext section.\n6.5. Evaluation\nBefore using our sexism indicator to analyse the subreddits stated in Section 6.4.1,\nthe results and discussion of which we present later on in Section 6.6, we evaluate the\nreliability of our sexism metric. We do this in two steps. The first step is to evaluate the\nsupervised ML part of our sexism metric, that is, the toxicity detector, as it is standard\nto do in supervised ML approaches to have confidence in the model trained (note that,\nobviously, we did not need to train any models for the unsupervised parts of the metric,\ni.e., the frequency ranking and the bias embedded). The second step is the evaluation of\nour sexism metric as a whole, particularly showing that it is sensitive and it increases its\nvalue as more sexist comments are added.\n10https://www.kaggle.com/code/kerneler/starter-sexist-workplace-statements-a8e79cab-c/input\n6.5 Evaluation 85\n6.5.1. Evaluation of the supervised toxicity detector\nThe Toxicity-Detector NLP model of our sexism metric was evaluated by splitting\nthe dataset in Section 6.4.3 into 70% train-set and 30% test-set. Table 6.1 shows the\nconfusion matrix we obtained on the test-set. Our F1-Score macro was above 91% and\nprecision and recall macro were above 90% and 92% respectively.\nTable 6.1: Confusion Matrix for Toxicity-Detector model\nPredicted\nlabel Toxic Not-ToxicActualToxic 20346 2331\nNot-Toxic 4085 73691\nWe also examined a well-known pre-trained transformer-based language model for\ntoxicity-detection called Toxic-BERT11which yielded a slightly better performance on\nthe same test-set (93% F1-Score macro). However, this model and similar large neural\nnetworks maintain a significantly higher computational cost in comparison to classical\nmodels. In particular, our classical NLP model takes 10 seconds to machine-annotate\n100k samples, whereas the same tasks take more than 1 hour for Toxic-BERT. As our\nfinal holistic model and research question deal with huge corpora, we prefer to stick to a\ncomputationally cheaper model where not much accuracy is sacrificed.\n6.5.2. Evaluation of the Sexism Metric\nThe second aspect that we evaluate is the reliability of our sexism metric. We itera-\ntively create 10 different datasets, each with a different level of misogynistic comments,\nand observe if our metric increases as we increase the amount of misogynistic comments\nin the data. Our data-generation strategy is to start with a dataset composed of neu-\ntral comments and gradually increment the misogyny of the dataset by adding subsets of\nmisogynistic comments. We take what\u2019s left of the supervised training (comments that\nfall out of our train-set) from the non-Toxic class as a starting point. Then, we use ex-\nternal\u22486000 comments annotated as \u201cmisogynist\u201d as our pool of sexism toward female\nidentity. Moreover, since there is a consensus in the previous literature in labeling r/TRP\nas a \u201cmisogynist\u201d community (e.g. [119, 120, 121]), we consider r/TRP comments as our\nground-truth for another pool of misogynist data to be analyzed separately. Then, for\neach pair of \u201cmisogynist comments vs. neutral\u201d and \u201cr/TRP vs. neutral\u201d data we iter-\natively form ten new datasets with different bootstrapped proportions of the misogynist\npool and run our misogyny-detection formula from Section 6.4 on each of the ten datasets\n(per pair) to get ten different values of our sexism metric toward female identity.\n11https://huggingface.co/unitary/toxic-bert\n86 Gender-based Polarization and Sexism\nFigure 6.3: Validation Chart for Our Sexism Metric for Toxicity Toward Female Identity\nWe observe an above 97% correlation between the proportion of annotated misogynis-\ntic comments and our sexism metric value towards female identity. This correlation is also\nabove 95% for the scenario where we use different proportions of r/TheRedPill comments\nvs. neutral ones. The score for each iteration in both cases is illustrated in Figure 6.3.\nNote that our evaluation can only be limited to the case of toxicity toward female identity\nsince misandry is an understudied concept, and there is none or insufficient ground-truth\nof misandrist or toxic-toward-female-individuals datasets available online.\n6.6. Results & Discussion\nToxicity towards identity. Figures 6.4 and 6.5 show the toxicity towards male identity\nand toxicity towards female identity in each of the subreddits that can be interpreted as\nthe level of misandry and misogyny inside each of them. The level of toxicity for each\nsubreddit is obtained through Equation 6.4 separately for adjectives biased toward male\nidentity and adjectives biased toward female identity.\nThe vertical error-bars for each subreddit shows the 95% confidence interval of the\nmetric after 10 bootstraps. The non-Toxic corpus on the left acts as the baseline point of\nthe metric for a non-toxic community. The second corpus from the left, also shows the\nlevel of targeted toxicity for a randomly collected set of comments from Reddit to assess\nwhich targeted toxicity is more salient than usual Reddit discourse.\n6.6 Results & Discussion 87\nFigure 6.4: Toxi-\ncity Toward Male\nIdentity.\nFigure 6.5: Toxic-\nity Toward Female\nIdentity.\nFigure 6.6: Toxic-\nity Toward Male In-\ndividuals.\nFigure 6.7: Toxic-\nity Toward Female\nIndividuals.\nToxicity towards individuals. On the other hand, Figures 6.6 and 6.7 refer to the\ntoxicity targeted towards male and female pronouns attribute-sets in each of the subred-\ndits that can be interpreted as the level of toxicity targeted toward individual male and\nfemale characters. There are significant visible changes with respect to targeted toxicity\ntoward gender identity. For instance, r/IncelTear proves highly toxic toward individual\nmale figures rather than the male identity. The same case happens for manosphere com-\nmunities (r/TheRedPill, r/MGTOW, and r/MGTOW2) with respect to toxicity toward\nfemale individuals.\nFindings. The results detect various levels of targeted toxicity with the highest targeted\ntoward female individuals next to some cases of targeted toxicity toward men. The tar-\ngeted toxicity index we obtain for every community confirms existing analyses looking at\nthe toxicity of r/TRP, r/MGTOW, r/MGTOW2, r/IncelTear, and r/FemaleDatingStrat-\negy; all the subreddits in which we had a prior report of their toxicity in the literature\n(more information in Section 6.5).\nIn addition, our framework helps uncover the characteristics of subreddits that have not\nbeen analyzed before . Consistent with the descriptions mentioned in Section 6.4.1, r/TRP,\nr/MGTOW, and r/MGTOW2 stand among the top scores in toxicity both toward female\nidentity and towards female individuals yet they are more extreme with respect to female\nindividuals rather than female identity.\nr/FemaleDatingStrategy acquired a significant score of toxicity both towards\nmen and women; consistent with a qualitative report [147] described in Section 6.4.1.\nr/IncelTear was seen as high in terms of toxicity toward both men and women.\nIts excessive toxicity toward women, in terms of both identity and individual, is\nnot counter-intuitive due to the nature of the subreddit causing it to repeatedly\nand sarcastically narrate misogynistic comments from \u201cincels\u201d prior to humiliating\nthem. Moreover, the community shows the most salient score in toxicity toward\nindividual male figures, which perfectly makes sense looking at the agenda of the\n88 Gender-based Polarization and Sexism\nsubreddit \u2014 that is, to target individual male \u201cincels\u201d regarding their \u201cmisogy-\nnist\u201d comments. r/TrollYChromosome and r/TrollXChromosomes obtained almost\na symmetric score regarding the gender they cast their toxic content toward as if\nthey were twins from both sides of the spectrum. In addition, they seemed to be\nrespectively more toxic toward female and male individuals rather than female and\nmale identities.\nr/MensRights and r/MensLib were more toxic toward women than men. How-\never, their level of toxicity did not significantly exceed a dataset of random comments\nfrom Reddit. This can presumably be attributed to more recent moderation policies\nimposed by Reddit and communities\u2019 moderators and their difference of ideology\nfrom right-wing MRA movements (e.g. r/TheRedPill, MGTOW). We know that\ndespite some other MRA subreddits, these two subreddits have in fact not been\nbanned by Reddit till now, which is compatible with the results of our indicator.\nTable 6.2: Top-100 most salient terms similarity matrix. The top-right (red) side of\nthe table shows the number of common adjectives among the top 100 saliently biased\nadjectives toward female-identity. The bottom-left (blue) side depicts the same quality\nfor male-identity.\nSaliency. As a further comparison between gender-discourse among the subreddits,\n6.6 Results & Discussion 89\nwe create a similarity index for every pair of them. We use the notion of saliency =\nBwi,SA|SB\u00d7FPR wi[65] to sort the adjectives inside every subreddit based on how biased-\n&-frequent (salient) a word is toward female and male identity. Then, in Table 6.2 we\nquantify this gender discourse similarity. The top-right side of Table 6.2 (red cells) shows\nthe number of top-100 most saliently biased adjectives toward female-identity that are\ncommon among a pair of subreddits. The bottom-left side of the table (blue cells) shows\nthe same concept for top-100 most saliently biased adjectives toward male-identity.\nThe similarity rates share several insights regarding how far or close discourses are for\neach pair of subreddits:\nInsight #1. r/TheRedPill, r/MGTOW, and r/MGTOW2 contain a highly similar dis-\ncourse both in male-related and female-related adjectives among each other, while being\nmore similar in the latter. This can show that manosphere subreddits agree more on what\nto attribute to femininity than to masculinity.\nInsight #2. r/MensRights and r/MensLib have few common words with r/TheRedPill,\nr/MGTOW, and r/MGTOW2 about women, which would make sense as our model also\nhas not rated them as highly toxic toward women. Therefore, they have less similar\ndiscourse to subreddits that are conventionally recognized as misogynistic by the previous\nliterature.\nInsight #3. r/MensRights and r/MensLib are mostly similar to each other, especially\nin their share of female-related adjectives. Interestingly, in comparison to manosphere\nsubreddits, they have higher rates of similarity with feminist subreddits in both their top\nmale-associated (e.g., \u201cpaternalistic\u201d , or\u201cmisogynistic\u201d ) and female-associated adjectives\n(e.g., \u201csuccessful\u201d , or \u201cpowerful\u201d ). [155] raise discussions on how r/MensRights and r/-\nMensLib share highly similar lexical features to talk about the same topic from an anti-\nand pro-feminist perspective.\nInsight #4. r/IncelTear tends to show high discourse-similarity with r/TheRedPill, r/MG-\nTOW, and r/MGTOW2 regarding women (e.g., \u201cpromiscuous\u201d ,\u201chypergamous\u201d , or\u201cca-\nsual\u201d ), but not much similarity regarding men. This is compatible with the previous\ndescriptions of r/IncelTear as a community that tends to sarcastically quote misogynist\ncomments and humiliate those comments. Those quotations of misogynist comments\ncould be the probable reason behind the high amount of similarity in feminine-biased\nwords with the manosphere subreddit. Yet, the lower commonality in words saliently bi-\nased towards men, reconfirms that they do not actually share the discourse of manosphere\nsubreddits regarding masculinity.\nInsight #5. Both feminist communities, r/Feminism, and r/AskFeminists, expectedly\nshow high discourse similarity among themselves regarding both men (e.g., \u201carrogant\u201d ,\nor\u201cmisogynistic\u201d ) and women (e.g., \u201cindependent\u201d , or \u201csuccessful\u201d ). Moreover, they\n90 Gender-based Polarization and Sexism\nare considered similar to r/trollxchromosomes which is known as a subreddit meant for\nfeminist humor [149]. r/TheGirlSurvivalGuide and r/FemaleDatingStrategy, as two daily-\nlife and dating tips subreddits, share the highest similarity in their male-related terms\nsuch as \u201cunemployed\u201d , or\u201cunsuccessful\u201d , etc. which might be attributed to the types men\nsuggested to avoid in dating.\nComparison with Fully Supervised Approaches:\nFinally, we also compare the results we obtained with previous fully supervised ap-\nproaches for overall toxicity of corpora [121, 156]. For this, we compare our results in the\nparticular subreddits used in those previous works, which were consistent with our results.\nLike us, [121] rated r/IncelTear as more misogynistic comments than r/MGTOW, and\n[156] rated r/MGTOW and r/TRP as almost equally toxic. They also measured a ran-\ndom set of Reddit comments to have slightly more than half the toxicity level of r/TRP\n[121, 156]. Yet, both of the mentioned works are limited to fully-supervised methods,\nsimply counting the percentage of misogynistic posts and context-unaware lexicons inside\na community. This will make them expensive to annotate, more dependent on subjective\njudgments (i.e., whether a post should be annotated as misogynist or not), less robust,\nand less generalizable to other sorts of targeted toxicity (e.g., for measuring targeted tox-\nicity toward Muslims vs non-Muslims, they should run a separated analysis with toxic\njargons related to Islamophobia). It will also make them unable to detect neutral-looking\nlexicons that could be toxic in certain communities\u2019 contexts and vice versa.\n6.7. Conclusion & Future Work\nIn this chapter, we proposed a metric that is able to quantify the level of sexism in\nthe language of online communities, using a combination of unsupervised and supervised\nNLP techniques. Our analysis embraced 14 subreddits from different parts of the gender-\ndiscourse spectrum, which were not analyzed before by a unique model at the same time.\nWe confirmed the toxicity of r/TheRedPill and r/MGTOW toward women in an auto-\nmated and comparable way. We also realized that a female-exclusive dating community\nsuch as r/FemaleDatingStrategy can be toxic toward men and women at the same time.\nThe granularity of our method to distinguish the target of toxicity offers a new nuanced\nunderstanding of Web communities, which will foster future work in the area.\nLikewise, another contribution of our method and subsequent analyses was making a\nclear distinction between the toxicity aimed toward male/female identity inside a commu-\nnity, and the toxicity targeting male/female individuals. This enables better attribution\nmechanisms, which is paramount to curve misinterpretations about a community when\nthere is abundant criticism toward several male/female politicians rather than its toxic\ncontent about male/female \u201cidentity\u201d .\nFurthermore, our model can smoothly be generalized to capture other sorts of polar-\n6.7 Conclusion & Future Work 91\nization and radicalization on social media. For instance, by changing our attribute words\nwith sets of words related to Democrats and Republicans, and replacing our Embedded-\nToxicity parameter with Embedded-Polarity, one can be able to scalably quantify the\npolarization of sentiments towards the Democratic and Republican parties in different\ntimelines.\nOne feature of our methodology is that it accepts any type of embedded biases, which\nopens new avenues to offer more granularity to the identification of toxicity. For instance,\nanother implementation for future work could detect the polarization of sarcasm or hu-\nmor targeted toward either group, by simply replacing the Embedded-Toxicity parameter\nwith Embedded-Sarcasm. The facilitation provided by our approach would be that the\nresearcher does not need to annotate two sparse binary classes of sentiment/sarcasm to-\nward Republicans vssentiment/sarcasm toward Democrats . An already available dataset\nof sentiment, sarcasm, etc. would suffice and the model itself would detect the target\nwhile suffering less from annotators\u2019 subjective judgments and biases.\nOur holistic indicator of polarization provides the tool for policymakers and moder-\nators to take action about a community (e.g., subreddit) by inspecting the polarization\nlevel over time. Also, it can be used in computational social science research for mea-\nsuring polarization over time, and causal inference between temporal polarization and\nvarious real-world events (e.g., elections, wars, COVID-19). However, when it comes to\nmoderation, this holistic indicator should not be projected into individual comments in\na community and cause moderators to treat every comment in a polarized community as\na polarized comment. Judging individual comments of users require a higher level of su-\npervision and care. Also, it is worth noting that our work only analyzes male and female\ngenders for now, and analyzing LGBTQA+ groups is out of the scope of this chapter.\nFuture work need to extend the model from a binary polarization detector to a more\ncomplex multidimensional association problem to address this limitation.\nWe make our code and datasets available on GitHub to the researchers for reproduction\nand further developments.12\n12https://github.com/vahidthegreat/Polarization-Indicator\n92 Gender-based Polarization and Sexism\n7Platform\u2019s Effect on Toxicity\nAbstract\nNext to the descriptive measurements of radicalization, we are also interested in ex-\nploring root causes and influential factors that can contribute to it. One of the potential\ninfluential factors can be the effect of platform\u2019s design and moderation strategies. To\ninvestigate this factor, we zoom into cross-platform communities. Cross-platform com-\nmunities are social media communities that have a presence on multiple online platforms.\nOne active community on both Reddit and Discord is dankmemes .\nOur study aims to examine differences in harmful language usage across different\nplatforms in a community.\nWe scrape 15 communities that are active on both Reddit and Discord. We then\nidentify and compare differences in type and level of toxicity, in the topics of the harmful\ndiscourse, in the temporal evolution of toxicity and its attribution to users, and in the\nmoderation strategies communities across platforms.\nOur results show that most communities exhibit differences in toxicity depending on\nthe platform. We see that toxicity is rooted in the different subcultures as well as in the\nway in which the platforms operate and their administrators moderate content. However,\nwe note that in general terms Discord is significantly more toxic than Reddit. We offer a\ndetailed analysis of the topics and types of communities in which this happens and why,\nwhich will help moderators and policymakers shape their strategies to mitigate the harm\non the Web. In particular, we propose practical and effective strategies that Discord can\nimplement to improve their platform moderation.\n7.1. Introduction\nThe ample amalgam of Web communities provides safe spaces for diverse cultures to\nexpress their opinions. Due to the idiosyncrasies of the Web, these cultures naturally\nscatter their views across disparate platforms. For instance, some users may opportunis-\n93\n94 Platform\u2019s Effect on Toxicity\ntically (e.g., while on their phones) prefer the dynamism of Discord over the asynchronous\nnature of Reddit. While it is well established that we adapt our language according to\nthe audience and the medium to cope with social norms [157], it is less clear to what\nextent individuals self-impose different norms around the use of toxic language according\nto the platform they are in. Also, different platforms such as Discord and Reddit have\ntheir own policies and guidelines, and moderators who may apply them differently.\nRelated work has established links in the spread of toxic content between differ-\nentloosely connected communities like fringe communities (e.g., 4chan), mainstream\n(e.g., Reddit or Twitter) [158, 159], and chat-based platforms [160]. While there is a\n\u201cneed to have a multi-platform point-of-view when studying [problematic content] on the\nWeb\u201d [161], there have been limited attempts in measuring strongly connected communi-\nties.\nIn this chapter, we collect a unique dataset of Web communities that are present\nsimultaneously on different platforms. Our dataset opens up new opportunities for NLP\nresearchers and Computational Social Scientists to compare the discourse across the two\nsocial media platforms. We then design a methodology to discover the differences in\nproblematic content. At the core of our methodology, we use toxicity detection, and\nsemantic analysis to identify nuanced contrasts in the usage of toxic language at the\nsentence level. We then identify which platforms have a larger number of toxic users and\nwe show how toxicity has evolved differently over time across platforms and communities.\nThrough the use of our methodology to analyze 15 popular communities simultane-\nously present in Reddit and Discord, this chapter makes the following findings:\nOverall, we see more toxicity in Discord than in Reddit. We see that com-\nmunication takes different shapes on disparate platforms. Discord prompts users\nto communicate using more dynamic interactions, which could have an important\neffect on the amount and level of toxicity.\nThe toxicity in Reddit is more fine-grained and oriented toward the main topic\nof the community (i.e., each individual subreddit) whereas the toxicity in Discord\nis more coarse-grained and scattered.\nWe see that a handful of users account for most of the toxic content shared in\nmost communities while the majority of users share no toxic content at all.\nThere is a significant increase of toxicity across the time for most cases. This\nindicates that no significant change has occurred with respect to the moderation\nstrategy during the time window of our analysis.\nThere is a substantial difference in terms of moderation across platforms, but\nwe observe that this difference does not completely explain the differences in toxicity\n7.2 Problem Statement & Background 95\nwe observe across platforms for the same community and other factors also seem to\nplay an important role.\nThe chapter is organized as follows. Section 7.2 briefly discusses the nature of the two\nplatforms that we study (Reddit and Discord) and how they are connected. Section 7.3\npresents our methodology. Section 7.4 explains the way we systemically select cross-\nplatform communities and our dataset. Section 7.5 portrays the results of our methods\napplied to cross-platform communities. We discuss the limitations and takeaways of this\nin Section 7.6. Finally, we discuss related work in Section 7.7 and conclude in Section 7.8.\n7.2. Problem Statement & Background\nThe number of controversial Web communities has grown significantly over the last\nfew years judging by the uptake in the communities being suspended because of the use\nof toxic language.1As content moderation has an effect on the deplatforming of toxic\ncommunities, their users roam to those platforms that have laxer moderation as a side\neffect [162].\nThere are two factors that determine how a community is moderated. The first factor\ndepends on the Terms and Conditions (T&C) of the platform, which may change over\ntime, as we have recently witnessed with X (formerly known as Twitter), for example,\nthe limitation set in July 2023 on the number of tweets each user can view.2The second\none relates to the norms of the community and the way in which the moderators [163]\nenforce both these norms and the T&C. Moderators are generally appointed by the creator\nof the community ( administrator in Discord or top moderator in Reddit) or by another\nmoderator of the community, such as in Reddit. These moderators are volunteers, and\ntheir contribution is subject to their availability. In cross-platform communities with\na high volume of posts, it is commonplace to have different moderators on each of the\nplatforms. For instance, there are completely different sets of moderators (size 10) for the\nmusic community on Reddit and Discord.\nConsidering that every moderator is an individual with a unique personal perception\nof toxicity, their different restrictive standards may affect the level of toxicity across\nplatforms. Testimony of this is the non-negligible number of moderated communities that\nhad been running for a long time and have been eventually banned by the platform. The\nnature of two different platforms may propose disparate types of interventions, resulting in\ndifferences in terms of toxicity. Discord is structured like a group messenger which might\nencourage ping-pong dual dialogues whereas the design of Reddit initially encourages the\nusers to react to a post (submission), yet with the possibility of replying to other users\u2019\n1Since 2020 Reddit banned several communities with hundreds of thousands of users, like r/TruFem-\ncels, r/NoNewNormal, r/MGTOW, r/ChapoTrapHouse, r/GenderCritical, r/The Donald.\n2https://twitter.com/elonmusk/status/1675187969420828672\n96 Platform\u2019s Effect on Toxicity\ncomments. Meddling in a bidirectional dialogue as a moderator may have some different\ncharacteristics than meddling in the reaction to a post.\nOnechallenge we face when we look for communities that coexist on more than\none platform revolves around associating the coexistence of communities, i.e.: identifying\nhow a community may be scattered across different platforms. We address this challenge\nby focusing on sub-communities that are strongly connected to each other. We say that\nthere is a strong connection when one of the sub-communities self-declares the other\none, typically through a link that reports the association. Figure 7.1 shows an example\nof such an association. In what follows we refer to cross-platform communities as sub-\ncommunities that are hosted in different platforms and they are strongly connected to\neach other.\nFigure 7.1: Sketch of the method used to find the association between communities that\nsupport multiple platforms.\nWe further explore the case we describe in Figure 7.1 and see that some subreddits set\na pointer to the official Discord channel of the community. We leverage this vantage point\nto systematically collect associations between Reddit and Discord for the most popular\ncommunities as we explain next.\n7.3. Methodology\nFigure 7.2 shows the general pipeline used in this study. To observe the linguistic\ndifferences in cross-platform communities, we follow the next steps: First, we devise a\nsystematic data collection method to find popular communities scattered across different\nplatforms. We crawl, scrape, and process all textual comments posted in these commu-\nnities. Then, we split the comments into sentences for further steps. Second, we use a\nmachine learning classifier based on Bidirectional Encoder Representations from Trans-\nformers (BERT) to detect hateful sentences. We then perform a three-fold analysis of\nthe differences between hateful sentences and toxic users at the platform level for every\ncommunity, dubbed Differential Analysis. We next describe this approach in detail.\n7.3 Methodology 97\nSplit\ncomments\ninto\nsentences\n& Data\nCleaning Toxic\nSentence\nClassificationClean\n SentencesToxic text Classifier\n(huggingface)\nToxic\nSentencesTimeline Analysis\nSemantic AnalysisUser Analysis\nScrape\nComments\nfor\ncommunity\nfrom both\nplatformsDifferential Analysis\nPlatform A\nPlatform BCommunity X\nDifferential T ags AnalysisCommunity Analysis\nModeration Analysis\nFigure 7.2: Our methodology in a nutshell.\n7.3.1. Data Gathering\nTo collect our dataset, we use the following three main steps.\n7.3.1.1. Finding Cross-platform Associations\nAs discussed in Section 7.2, we focus our analysis on strongly connected communities.\nTo find the association between two generic communities (denoted as AandB), we start\ncollecting data from the platform that sources the association. Let the expression a\u2192b\nrepresent a community acontaining a link to the community b.\nWe start a first crawling task over platform A. This crawling task is designed to\nquery the index page of the platform and return as output the name of community a\u2208A,\ntogether with their link. We sort all communities by popularity, as given by the number\nof users in each community.\nWe next inspect the top most popular communities in descending order and extract\na\u2192b. Not all communities declare an association; therefore, we iterate through our\nassociation step until we obtain a significant set of associations. In this chapter, we limit\nthe scope of our data collection to 20 cross-platform communities to avoid indiscriminate\ndata collection. In our implementation, our data gathering departs from Reddit and gets\nassociations to Discord.\n7.3.1.2. Scraping data for selected communities\nOnce we have the list of associations, we continuously crawl the posts shared in both\nReddit and Discord for all communities. We use the publicly available APIs for data\ncollection from the Reddit and Discord platforms. The main attributes of the scraped data\nthat we use in our study are \u201cusers\u201d, \u201cposts\u201d, and \u201ctimestamps\u201d . We have anonymized the\n98 Platform\u2019s Effect on Toxicity\nuser names in the scraped data. Additionally, we have opted not to conduct any analysis\nat the individual user level. This approach ensures that our study mitigates any potential\nethical implications associated with scraping user data. Details about data scraping are\ndiscussed in detail in Section 7.4.\n7.3.2. Differential Analysis\nWe use Differential Analysis [164] to compare toxicity across platforms across three\naxes: the semantics of the topic, the users, and the time. Differential Analysis is a general\nmethod that compares two properties by subtracting the normalized value of the property\nitself. Next, we explain each dimension of our analysis in detail.\n7.3.2.1. Community Analysis\nWe examine the comments we scrape from each platform as a first step. Preliminary\nresults show that comments on Reddit are significantly larger than in Discord. This is\ndue to the dynamic nature of Discord proper of an instant messaging platform. For a fair\ncomparison, we split the comments we collect from each community into sentences. Then,\nwe use a pre-trained transformer-based model from Detoxify library3for toxicity detection\nto machine-annotate the sentences in terms of toxicity. The model is not only trained to\ntell whether a sentence is toxic or not but also to categorize the toxicity of sentences as\n\u201cSevere-Toxic\u201d ,\u201cObscene\u201d ,\u201cThreat\u201d ,\u201cInsult\u201d , and \u201cIdentity-Hate\u201d . Finally, we compare\nand report all categories of toxicity for the same communities across platforms in terms\nof the rate and distribution of the toxicity.\n7.3.2.2. Timeline Analysis\nIn addition to the static analysis of the overall toxicity, we also compute the rate\nof toxic comments per day to capture the possible effects of real-world events on the\ntemporal toxicity rate. We then study the chronological distribution of hate across time.\n7.3.2.3. Users Analysis\nWe are also interested in the distribution of toxicity across users of each community\nto assess the share of the most toxic users from the overall toxicity. Thus, we also\naggregate sentences per user to obtain the toxicity rate for each user. To regularize\nthe problem, we only consider the \u201cHateful\u201d category when calculating the user toxicity\nrate. For instance, a user with 10 total comments, 2 of which are \u201cHateful\u201d and 8 non-\ntoxic, is considered 20% toxic. This is useful in order to see how skewed the share of\ntoxic content is distributed among all users of a community. The moderation policy can\n3https://pypi.org/project/detoxify/\n7.3 Methodology 99\nchange accordingly considering that banning a few top most toxic users in a more skewed\ncommunity can moderate a higher proportion of the entire toxic content whereas, in a\nmore uniformly distributed toxicity, the policy might need to be more effective when\noriented toward the content rather than users.\n7.3.2.4. Semantic Analysis\nSemantic tagging [165] is the task of assigning semantic class categories (tags) to the\nsmallest meaningful units in a sentence, and it is an application of Natural Language\nProcessing. We apply the semantic tagging technique to investigate and understand the\nlinguistic differences, and topics of discussion in communities across platforms. In our\nexperiments, we used Python Multilingual Ucrel Semantic Analysis System (PyMUSAS)4\nlibrary. It assigns a semantic category tag or tags to every word in a given text. We use\ntoxic sentences as input to the USAS tagger and get the output as a list of associated\ntags for each token from text and the total count of each tag.\nThis comparison gives a view of the similarities and differences in toxic sentences\nposted by communities across platforms. We report the top 10% of semantic tags (ignoring\nother tags because of relatively low values) in each community for Reddit and Discord.\nThen, we compute the percentage of each tag in the community for both platforms. We\nsubsequently compute the absolute differences in the percentages of Reddit and Discord\ntags. We finally sort the list of tags in decreasing order of absolute differences and pick the\ntop 2 (most dissimilar) and bottom 2 (most similar) tags to highlight the most distinctive\nand common features across platforms. To give a more holistic view of similarities and\ndifferences across all tags, we also compute a measure of cosine similarity between semantic\ntags. For this, we take two vectors having counts of each semantic tags on Reddit and\nDiscord respectively for the same community, we normalise the vectors, and then compute\nthe cosine similarity between them.\n7.3.2.5. Differential Tags Analysis\nDiving deeper into the linguistic contrasts between platforms, we aim to highlight\nthe most significantly contrastive semantic tags between the two platforms. We subtract\nthe frequency percentile ranking of every tag in Discord, with respect to other tags in\nthe same corpus, from its frequency percentile ranking in Reddit (and vice versa). We\nthen use this margin to measure a contrastive significance for each tag. Let CSTijdenote\nthe contrastive significance of tag iin community c. Also,FTicpdenotes the frequency\npercentile ranking of tag iwith respect to other tags in platform pof community c. Then,\nwe compute CSTijfor Reddit (R) over Discord (D) as in Equation 7.1. Next, to calculate\nthe mean contrastive significance across every cross-platform community, we also measure\n4https://github.com/UCREL/pymusas\n100 Platform\u2019s Effect on Toxicity\nthe 95% confidence interval for the salience of every tag and exclude the tags with a lower\nbound below zero.\nCSTij(R|D)=/summationtext\ncFTic(p=Reddit )\u2212FTic(p=Discord )\n|C|(7.1)\n7.3.2.6. Moderation Analysis\nTo explore the moderation differences across platforms for the same community, we\nexamine the rate of deleted comments. While we do not have access to the actual content\nof the posts been deleted, we do see a label that describes when a message has been\ndeleted by the moderator (including auto-moderators5). Thus, we start by looking at all\nthe content deleted by moderators for the communities and platforms under consideration\nas follows.\nFirst, we assume that deleted comments have been moderated due to toxicity. We\nweight every deleted comment by the average sentences per comment in the community.\nThen we add this to the count of toxic comments and recalculate the percentage of toxicity\nin the community per platform. This allows us to investigate any differences between the\npercentage of moderated content in Reddit and Discord. Note that we are estimating\nthe level of toxicity as if a comment would had been removed by a moderator because\nof toxicity and we are assuming that all sentences in that comment are toxic. Thus,\nthis analysis has to be seen as a high over-approximation. However, this is sufficient to\ncompare platforms and to show, as detailed later throughout our results, that moderation\nplays a role but it is not the only reason for differences in toxicity across platforms for\nthe same community.\n7.4. Data Collection\nWe take the following steps to find strongly connected cross-platform communities.\nWe first identify top subreddits6in terms of the number of subscribers and select the top\n200 subreddits. When we visit the landing page of a subreddit, we search for a Discord\ninvitation link. This link is set by the creator of the subreddit and, while it is optional,\nits presence signals the existence of a Discord server for the community. When present,\nwe use the link to join the Discord server.\nOut of the 200 subreddits, we find 32 communities in both Reddit and Discord. Several\nDiscord servers are either inactive or very small in size of members. Thus, we shortlist\nthe 20 most active communities, all with more than 500 users.\nData scraping: To scrape the subreddits (Reddit communities), we use PushshiftAPI.7\n5Automatic Reddit built-in system based on rules: https://www.reddit.com/wiki/automoderator/\n6http://redditlist.com\n7https://github.com/pushshift/api\n7.4 Data Collection 101\nCommunities Description\ndankmemes discuss memes that are unique or odd.\neurope community of peoples from fifty-six plus countries and two hun-\ndred thirty plus languages.\ngames interesting gaming content and discussions.\nhistory discussions about history.\njokes posts hundreds of jokes each day.\nkpop discuss k-pop (Korean popular music).\nksi discuss KSI (an English YouTuber and rapper).\nmusic a platform to discuss about music.\nnosleep share scary personal experiences.\noverwatch related to the Overwatch game.\nrainbow6 discuss things about Rainbow Six Siege game.\nrickandmorty discuss animated series, Rick and Morty.\nsports discuss sports news and highlights.\nUkrainian-\nconflictshares news, analysis, discussion and investigative journalism\nabout the conflict in Ukraine.\nwritingprompts a platform for people who like prompts, they write a short story\nbased on it, post and discuss them.\nTable 7.1: Communities description.\nThe subreddit data is publicly available. For scraping the data from Discord servers, we\nuse the Requests library in Python. We set an authentication code using a valid Discord\naccount. We capture the server ID and channel ID to perform the crawling, which we can\naccess after joining the server. We collect the data from both platforms for considered\ncommunities for a duration of around 7 months (January 2022 to July 2022).\nAfter a preliminary study, we further shortlist the communities to 14 (out of 20).\nThe most important factor in excluding 6 communities is the imbalance across platforms.\nThese cases have one platform with significantly less number of comments available com-\npared to the other platform of the same community. After we started our data collection\nin January 2022, we added to our study a community called \u201c Ukrainian-conflict \u201d as the\nUkraine war started in February 2022. Our rationale was to capture a freshly created\nyet active community. Overall, we have included a total of 15 communities in our study.\nTable 7.1 presents the description of each community.\nDataset Anonymization: We use anonymizedf8Python library to anonymize user-\nnames and other sensitive data.\nDataset Statistics: Table 7.9 represents the statistics of the dataset used in the study.\nThe size of the communities shows the total number of subscribers present in the com-\nmunities. The average sentence length is given as the number of words per sentence. The\n8https://pypi.org/project/anonymizedf/\n102 Platform\u2019s Effect on Toxicity\naverage sentence length for Reddit and Discord is 12.43 and 6.67 respectively.\n7.5. Results\nWe apply our Differential Analysis methods in Section 7.3.2 to measure differences in\nterms of toxicity across cross-platform (Reddit/Discord) communities.\n7.5.1. Community Analysis\nHateful Toxic Severe-Toxic Obscene Insult\nCommunities Reddit Discord Reddit Discord R\u00d710\u22124Discord Reddit Discord Reddit Discord\ndankmemes 3.50% 14.85% 2.42% 16.89% 3.12% 5.50% 1.60% 12.24% 1.31% 11.10%\neurope 1.02% 19.41% 0.89% 10.98% 3.98% 3.42% 0.44% 8.52% 0.41% 7.14%\ngames 1.10% 14.17% 1.03% 9.10% 0.00% 3.32% 0.71% 8.10% 0.32% 7.00%\nhistory 0.62% 6.38% 0.64% 3.24% 0.00% 1.96% 0.13% 2.49% 0.32% 1.99%\njokes 1.93% 15.74% 1.35% 11.10% 1.24% 5.20% 0.66% 8.84% 0.51% 7.32%\nkpop 1.94% 10.82% 1.43% 7.98% 0.00% 5.10% 0.92% 6.89% 0.46% 6.11%\nksi 4.53% 24.71% 2.24% 20.10% 11.68% 6.34% 1.52% 17.22% 0.96% 15.21%\nmusic 0.85% 21.03% 0.63% 11.32% 0.43% 4.00% 0.39% 9.92% 0.26% 8.29%\nnosleep 4.22% 11.31% 3.57% 8.23% 0.00% 3.12% 2.16% 7.77% 1.2% 6.28%\noverwatch 1.23% 25.80% 0.87% 7.89% 7.69% 5.45% 0.48% 5.77% 0.31% 4.89%\nrainbow6 2.66% 15.64% 1.68% 11.84% 0.00% 5.87% 1.04% 8.82% 0.65% 6.90%\nrickandmorty 9.75% 17.15% 6.81% 15.45% 46.66% 6.22% 4.29% 11.32% 2.78% 10.33%\nsports 7.14% 10.71% 5.72% 6.23% 13.65% 2.31% 3.63% 5.83% 2.73% 3.46%\nUkrainian-conflict 2.48% 14.62% 2.08% 6.87% 6.08% 3.88% 1.09% 5.10% 1.01% 4.87%\nwritingprompts 1.66% 7.28% 1.78% 4.76% 6.86% 2.66% 0.80% 4.44% 0.67% 4.00%\nTable 7.2: Percentage of different types of toxicity across the two platforms per com-\nmunity. (Note: We highlight in bold the highest value in a column and we underline the\nsecond highest.)\nWe compare the toxicity of Reddit and Discord as discussed in Section 7.3.2.1. We\nfirst measure the overall toxicity and we then break it down per community.\n7.5.1.1. Overall Toxicity\nWe study five categories of toxicity ranging from general hate (\u201cHateful\u201d category) and\ntoxicity (general and severe) to obscenities and insults. Figure 7.3 aggregates the average\ntoxicity for all communities. We see a significantly higher toxicity rate for Discord in\nall categories. We observe how the communication over Discord is more dynamic and\nchatty , while on Reddit comments are argumentative. This has an impact on the type of\nlanguage used, which reflects the toxicity used. Linguistic and semantic differences are\nfurther explored later on in Sections 7.5.4 and 7.5.5. Next, we take a look at toxicity\nper community, then in Sections 7.5.2 and 7.5.3, we look at toxicity across time and\n7.5 Results 103\nusers, respectively. Finally, in Section 7.5.6 we look at differences in moderation and\ntheir potential relationship with the observed differences in the toxicity across platforms.\nTakeaway: Toxicity seems way higher in Discord than Reddit for all categories. Inter-\nestingly, the frequency of \u201c Severe-Toxic \u201d is negligible on Reddit and more moderate on\nDiscord, suggesting that Reddit has an uncompromising moderation policy and diligent\nmoderators/processes towards \u201c Severe-Toxic \u201d toxicity while Discord appears more lenient.\nHatefulT oxicSever e-T oxicObscene Insult T\noxicity0246810121416Average toxicity percentageRedditDiscor\nd\nFigure 7.3: Average toxic sentences on Reddit and Discord platforms for communities\nunder study.\n7.5.1.2. Communities & Toxicity\nTable 7.2 shows the proportion of toxicity we see in each of the communities. Looking\nat the overall amount of hate (\u201c Hateful \u201d column) suggests that the most controversial\ncommunity in Reddit is rickandmorty and in Discord is overwatch with 9.75% and 25.80%\nof hateful sentences respectively. Looking at other categories like \u201c Toxic \u201d, \u201cObscene \u201d and\n\u201cInsult \u201d, we find rickandmorty as the most controversial community in Reddit and ksiin\nDiscord. The \u201c Severe-Toxic \u201d is very low in Reddit communities, with the exception of\nksi,sports andrickandmorty . In Discord, the \u201c Severe-Toxic \u201d toxicity is better distributed\n104 Platform\u2019s Effect on Toxicity\nacross communities with ksiagain standing out.\nTo offer a point of comparison, Table 7.3 aggregates the values in the Hateful column\ninto three tiers of toxicity ( Low,Medium , and High). In Reddit, we observe that all\ncommunities are in the low-toxicity tier. For Discord, most communities lie in the Medium\nandHigh level of toxic, while history andwritingprompts communities lie in the Lowlevel.\nToxic levels Reddit Discord\nLow\n(Toxicity <10%)All history, writingprompts\nMedium\n(10%<Toxicity <20%)europe, games, jokes,\nkpop, nosleep, sports,\nUkrainian-conflict,\ndankmemes, rainbow6,\nrickandmorty\nHigh\n(Toxicity >20%)ksi, music, overwatch\nTable 7.3: Toxicity level-wise communities.\nNotably, we see that the most controversial communities across the different categories\nrelate to the entertainment industry, including the music industry (with the KSI rap\ncommunity leading the ranking), the gaming industry (led by the Overwatch gaming\ncommunity), the community around Rick and Morty TV comedy show for adults, and\nthe sports industry. Out of these categories, communities discussing the geo-political\ncontext (discussions around Europe and the Ukrainian conflict) are comparably the ones\nthat show a larger drift in the level of hate between Reddit and Discord.\nTakeaway: Overall, we see nuanced differences in toxicity across communities and we\ndetermine that the \u201c Hateful \u201d category offers a consistent summary of the different types\nof toxic comments. Hereafter, we focus into this category.\n7.5.2. Temporal Toxicity\nFigure 7.4 illustrates the Cumulative Distribution Frequency (CDF) of toxicity during\nour study (i.e., from January 3rd to August 3rd, 2022). We represent the average CDF\nvalues of toxicity for the different Reddit (blue) and Discord (red) communities. Toxicity\nlevels vary over time and can be seen through deviations from the average values (dashed\nblue and red lines). Some communities show a sharp increase in toxicity over time,\nincluding Ukrainian-conflict in Reddit and kpop,joke, and ksiin Discord. We attribute\nthese spikes to various contemporary events as we discuss next.\n7.5.2.1. Ukraine War\nIn Reddit, Ukrainian-conflict has the highest deviation in CDF values. This is due to\na drastic increase in toxicity after Russia started a full-scale invasion of Ukraine at the\n7.5 Results 105\nFigure 7.4: Toxicity Timelines.\nend of February 2022. Discord europe has a big jump in toxicity after the end of February\nwhich we attribute also to the effect of the war on Ukraine.\n7.5.2.2. International Kissing Day\nThe jokecommunity in Discord has a significant jump of over 30% in toxicity on July\n6th which is the international kissing day,9causing several inappropriate conversations\naround the topic.\n7.5.2.3. KSI vs Alex Wassabi\nThe ksion Reddit as well as on Discord shows a significant increase in toxicity starting\nfrom the end of July. We attribute this to the announcement of the fight in an exhibition\nboxing match between the British YouTuber KSI and American YouTuber Alex Wass-\nabi.10\nTakeaway: On both platforms, many communities do not show a significant variation in\ntoxicity over time. Yet, one thing stands out: we have an increasing trend in toxicity rate\n9https://en.wikipedia.org/wiki/International_Kissing_Day\n10Announcement made July 17, 2022, https://en.wikipedia.org/wiki/2022_in_Misfits_Boxing .\n106 Platform\u2019s Effect on Toxicity\non average, showing that existing moderation strategies can not scale. We also see how\nspikes in toxicity are contextual, mostly fostered by the existing socio-political landscape.\n7.5.3. Toxicity Analysis per User\nWe use the distribution of user toxicity rates in each community to provide insight\ninto the skewness of toxicity production. Table 7.4 shows a summary of the results.\nWe consider a user to be toxic if we see a toxic statement in any of the sentences in\ntheir posts. We then look at the top 5% most toxic users and the prevalence of users with\n100% toxic sentences.\nCommunitiesToxic Users Top 5% Toxic 100% Toxicity\nRed. Disc. Red. Disc. Red. Disc.\ndankmemes 47.9% 47.7% 42% 73% 11.2%\neurope 37.8% 26.1% 37% 61% 6.9%\ngames 34.0% 15.9% 29% 52% 6.8%\nhistory 12.2% 7.7% 15% 54% 2.2%\njokes 33.4% 37.8% 22% 38% 8.4%\nkpop 36.7% 35.1% 29% 54% 4.7%\nksi 44.2% 43.9% 27% 77% 0% 14.6%\nmusic 24.9% 35.9% 16% 69% 7.9%\nnosleep 32.0% 21.0% 39% 48% 6.1%\noverwatch 40.8% 34.1% 27% 85% 4.6%\nrainbow6 39.8% 41.2% 33% 82% 7.0%\nrickandmorty 38.8% 17.6% 34% 75% 2.4%\nsports 37.1% 21.7% 25% 55% 4.1%\nUkrainian-conf. 47.8% 15.1% 44% 60% 1.7%\nwritingprompts 30.7% 18.2% 39% 60% 7.6%\nTable 7.4: Toxic users for Reddit (Red.) and Discord (Disc.)\n7.5.3.1. Rate of Toxic Users\nWe see that dankmemes hosts the largest toxic user base, with 48% of their users post-\ning toxic comments on both Reddit and Discord (see \u201cToxic Users\u201d column of Table 7.4).\nRecall that dankmemes is a community that produces a relatively low or moderate level\nof toxicity overall (cf. Table 7.3 in Section 7.5.1.2). In context, this means that many of\nthe toxic users in this community do not frequently produce toxic content.\nOn the contrary, we see that history has the lowest number of users who engage in toxic\nbehavior with 12.2% and 7.7% of the users in Reddit and Discord using toxic language\neventually. Interestingly, we observe that the number of toxic comments overall posted\nis 0.6% and 6.4% respectively (cf. Table 7.2 in Section 7.5.1.2). This shows that while\nhistory has more toxic users in Reddit than in Discord, Discord is overall more toxic than\nReddit due to a highly skewed production of toxicity by a few top toxic users.\n7.5 Results 107\nTakeaway: This common pattern suggests significant moderation differences between the\ntwo platforms for the same community. We come back to this point later in Section 7.5.6.\nWe further investigate the presence of the same set of users across platforms for the\nsame community and find that some users coexist on both platforms. For instance, we see\nthat around 13% and 8% of the Discord users in writingprompts and nosleep respectively\nare also present in Reddit. Note that the overlap is just based on an exact username\nmatch during the time span in our dataset, but we studied the user names and observed\nthey were significantly unique.\nTo further study the nuanced differences between users in different communities (in-\ncluding dankmemes and history ) we focus next on the top most toxic users.\n7.5.3.2. Most Toxic Users\nWe first look at the share of toxicity among the top 5% users in each community as\nshown in the middle column of Table 7.4. The numbers suggest that the share of toxicity\namong users is far more skewed in Discord, meaning that a few extremely toxic users\naccount for most of the toxic content in this platform.\nThis finding is also consistent when we examine the proportion of users who always\nuse toxic language (see \u201c100% Toxicity\u201d column in Table 7.4). As shown in the table, none\nof the Reddit communities have any individual who consistently generate toxic content,\nwhile all communities in Discord have a few of them. In particular, 15% of the users in\nksidisplay toxicity in 100% of their posts. This figure ranges all the way to 2% in the\ncase of Ukrainian-conflict .\nTakeaway: While we have seen that toxicity in Discord is concentrated in a few accounts,\nthe toxicity in Reddit is scattered across a wider range of users.\nWe next seek to understand if this toxicity is generally directed towards certain topics\nthrough the analysis of linguistics and semantic differences.\n7.5.4. Semantic Categories Analysis\nAiming to compare the linguistic differences in toxic sentences across Reddit and\nDiscord platforms, we compare the communities using their respective semantic tag values\nevaluated by the USAS semantic tagging model. To compute the cross-platform similarity\nin semantic tag values, we take two vectors for semantic tags, one for a community on\nReddit and another for the same community on Discord. Then, we compute the cosine\nsimilarity between the two vectors and get a similarity score.\nTable 7.5 shows the cosine similarity scores across platforms for all communities.\nHere, we can observe that the nosleep ,writingprompts ,ukrainian-conflict and history\ncommunities are more similar in topics, whereas overwatch and dankmemes communities\nhave substantial differences in topics across platforms. Overall, the cosine similarity scores\n108 Platform\u2019s Effect on Toxicity\nCommunities Cosine Similarity Most Similar\nTagsMost Dissimi-\nlar Tags\ndankmemes 0.92 K5, W4 S3, B1\neurope 0.98 G3, I1 G1, S3\ngames 0.93 M7, L2 K5, B1\nhistory 0.99 S9, M7 S3, G2\njokes 0.97 S9, K2 S3, S1\nkpop 0.98 G3, K5 K2, S3\nksi 0.97 G1, G3 S1, S3\nmusic 0.98 P1, X2 S1, K2\nnosleep 0.99 Y1, B2 L1, S1\noverwatch 0.91 S9, I3 K1, K5\nrainbow6 0.95 C1, S7 S3, K5\nrickandmorty 0.98 L1, B2 X2, S3\nsports 0.98 B4, Y2 S1, S3\nUkrainian-conflict 0.99 G3, H3 X9, Z2\nwritingprompts 0.99 C1, B2 F1, L1\nTable 7.5: Cross-platform cosine similarity for semantic tags with most similar and dis-\nsimilar tags in toxic sentences.\nof semantic tags (topics) are high for all the communities, which indicates that the topics\ndiscussed in a particular community on different platforms are very similar in general.\nTable 7.5 also shows the two most similar and the two most dissimilar topics in\ncommunities across platforms. Most of these topics are compatible with the basic theme\nof the communities, which validates the significance of semantic tags analysis used in our\nstudy. These topics are determined by using the method mentioned in Section 7.3.2.5.\nTable 7.10 represents the names of the tags mentioned in this chapter. Intuitively, due\nto the escalation of disputes between Russia and Ukraine, europe community is talking\nabout warfare, defense, and the army \u2014 i.e., weapons (G3) topics on both platforms.\nThe ukrainian-conflict is using the terms related to G3 and areas; boundaries (H2) in a\nsimilar size on both platforms. The history community is also discusses topics related to\nplaces (M7) more evenly.\nInterestingly, we see that music and kpop communities are dissimilar when talking\nabout music and related activities (K2) across Reddit and Discord. Also, games , and\noverwatch are the most dissimilar in Sports and Games related semantic tags (K5). Both\ntags are directly referring to the topics of their community. Further analysis shows that\nthe source of this dissimilarity is their extremely higher abundance on Reddit than on\nDiscord. This means that the discourse in Reddit content is closer to the theme of the\ncommunity (for example in sports community they talks about sports activities), whereas\nDiscord content does not completely stick to the related topic of the community and can\n7.5 Results 109\nalso drift to other topics. This may be related to the nature of Reddit, where comments\ninclude reactions to the submissions related to the main topic of the community. In\ncontrast, Discord servers are structured as group messengers, which may favor back-and-\nforth conversations between users, including the toxic ones, who may then diverge from\nthe main topic of the community.\nTakeaway: We see that the topics and semantic similarity are very high for all communities\nacross platforms, suggesting very similar topics being discussed most often aligned with\nthe main theme(s) of the communities. Interestingly, we also observe some differences\nbetween platforms, where Reddit discussions are more often bounded to the main theme(s)\nof the community, while Discord discussions seem to more easily diverge from the main\ntheme(s) of the community, while still being the main theme(s) discussed.\n7.5.5. Linguistic Differences\nFigure 7.5: Salient USAS tags in Reddit\ntoxic content.\nFigure 7.6: Salient USAS tags in Discord\ntoxic content.\nWe measure linguistic differences in toxic language by looking at differences in fre-\nquency percentile rankings of the USAS tags discovered in Section 7.5.4. In particular,\nwe portray the contrastive nature of semantic tags as word clouds, where the sizes of\nwords correspond to the measure of salience described in Section 7.3.2.5. The keywords\nwe display in the word clouds correspond to the description of each tag.\nFigure 7.5 shows tags of toxic content that are more present in Reddit when compared\nto Discord. Figure 7.6 shows the reverse, that is the tags of toxic content that are more\npresent on Discord rather than on Reddit. To provide more context to interpret the\nresults, we further offer details and provide sample words and sentences associated with\neach tag in Table 7.8. We see that tags corresponding to gender (\u201cPeople: Female\u201d), drugs\n(\u201cPlants\u201d), and other general-purpose topics describing \u201cDislike\u201d, and \u201cSensory: Smell\u201d\nare more frequent in Discord than in Reddit. Here, we observe more explicit toxicity\nassociated with these popular tags (see column \u201cTop words\u201d in Table 7.8) in Discord.\nThis suggests that, for the same community, the toxicity in Discord seems to be more\nexplicit, particularly for some topics such as drugs and the female gender. This could\nbe explained by the more semi-private nature of Discord as opposed to Reddit, where\n110 Platform\u2019s Effect on Toxicity\nsome users, even if anonymous, may be more reluctant to make some comments explicit\nin public . This could also be related to differences in moderation policies and processes\nas we explore in the next section, where Reddit policies and moderators may be harsher\nfor explicit language.\nTakeaway: Toxicity in Discord tends to be more explicit, particularly in reference to topics\nsuch as drugs and the female gender, when compared to Reddit.\n7.5.6. Moderation Differences\nWe also study differences when it comes to moderation.\n7.5.6.1. Attribution\nTable 7.6 shows the percentage of comments deleted or removed by moderators. We\nsee that moderators on the Reddit platform are more active and strict than on Discord.\nIn Reddit, \u201c nosleep \u201d, \u201csports \u201d, and \u201c history \u201d maintain the highest number of deleted\ncomments. In Discord, moderators seem much more lenient as deleted/removed comments\nare exceptional. Note that moderation policies in Reddit11and Discord12seem very\nsimilar when it comes to how moderators should handle toxic content. However, these\ndifferences we observe seem to be attributed to the way moderators apply policies in\npractice. We also see evidence of Reddit using automated systems to moderate comments\n(auto-moderation). We see in Table 7.6, column Reddit (AM), the percentage of those\ncomments deleted because they matched the automated rules moderators set in Reddit.\nWe note there are some communities where automated moderation is barely applied, but\nwe do not see a connection with the total amount of moderation (e.g., \u201c sports \u201d vs \u201c europe \u201d\nwhen comparing the two columns in Table 7.6) or the overall toxicity (e.g., \u201crickandmorty\u201d\nvs \u201csports \u201d or when comparing Table 7.6 and 7.2).\n7.5.6.2. Explanation\nNext, we focus on whether these differences in moderation could explain the differ-\nences in toxicity observed in Section 7.5.1.1. That is, whether all communities are less\ntoxic in Reddit simply because the moderation in Reddit is more strict when it comes\nto toxic content. Table 7.7 shows a substantial increase in toxicity percentage in Reddit\ncommunities when considering our estimate based on the moderated content. Still Dis-\ncord exhibits a higher toxicity rate as we see in the majority of the communities, such\nas \u201ceurope \u201d, \u201ckpop\u201d, \u201cksi\u201d, \u201cmusic \u201d, \u201coverwatch \u201d, \u201crainbow6 \u201d, and \u201c Ukrainian-conflict \u201d\nwhen comparing the estimated (upper-bound) Reddit toxicity in Table 7.7 with the actual\nDiscord toxicity back in Section 7.5.1.1 (Table 7.2).\n11https://www.redditinc.com/policies/moderator-code-of-conduct\n12https://discord.com/community/your-responsibilities-as-a-discord-moderator-discord\n7.6 Discussion 111\nTakeaway: Our analysis reveals that there are important differences in handling toxic\ncontent across platforms. Reddit has more proactive moderation strategies than Discord,\nwith some of them driven by automated mechanisms. When we factor moderation in, we\ncontinue to see that Discord is more toxic than Reddit. This shows that there are other\nreasons beyond moderation to explain the difference in toxicity for the same community\nacross Reddit and Discord. As these differences are substantial and the communities we\nstudy are strongly connected, meaning that administrators of the community may either\nbe the same or cooperate, we partially attribute the drift in toxicity to the other differences\nobserved across platforms beyond moderation, including the type of users there are or the\nnature of the conversations they have as we saw in Sections 7.5.3 (users), 7.5.4 (semantic\ndifferences) and 7.5.5 (linguistic differences).\nCommunities Reddit Reddit (AM) Discord\n\u00d710(\u22124)\ndankmemes 7.6% 2.1% -\neurope 4.4% 0.22% -\nGames 15.0% 0.75% -\nhistory 17.0% 5.9% -\nJokes 11.8% 0.02% -\nkpop 2.9% 1.1% 9.3%\nksi 5.9% 2.2% 4.0%\nmusic 2.8% 1.7% -\nnosleep 25.4% 5.2% -\nOverwatch 1.0% 1.1% 2.6%\nRainbow6 1.4% 2.1% 5.9%\nrickandmorty 2.4% 1.1% -\nsports 21.2% 0.02% -\nUkrainian-conflict 2.8% 1.2% 3.1%\nWritingprompts 7.5% 5.6% -\nTable 7.6: Percentage of deleted comments per community and platform by moderators.\nAM: Auto-moderation.\n7.6. Discussion\nWe discuss the main takeaways and limitations of our study.\n7.6.1. Main Takeaways\nThis chapter offers a unique comparison of cross-platform communities that yields the\nfollowing findings:\n112 Platform\u2019s Effect on Toxicity\nCommunities Reddit baseline Reddit estimate\ndankmemes 3.5% 10.33%\neurope 1.0% 5.24%\nGames 1.10% 15.7%\nhistory 0.62% 14.58%\nJokes 1.93% 13.88%\nkpop 1.94% 4.7%\nksi 4.53% 12.7%\nmusic 0.85% 3.43%\nnosleep 4.22% 26.63%\nOverwatch 1.23% 2.21%\nRainbow6 2.66% 3.9%\nrickandmorty 9.75% 12.1%\nsports 7.14% 27.34%\nUkrainian-conflict 2.48% 5.1%\nWritingprompts 1.66% 7.9%\nTable 7.7: Percentage of toxicity before and after including deleted comments as toxic\ncomments.\n7.6.1.1. Discord is more toxic than Reddit\nComparing the rate of toxicity across Reddit and Discord shows a clearly generalizable\npattern. For all considered communities, the content of that community in the Discord\nplatform is substantially more toxic in all categories of toxicity in comparison to the\nReddit platform of the same community. Notably, the prevalence of the \u201c Severe-Toxic \u201d\ncategory is almost negligible on Reddit while clearly existing in Discord. Moreover, the\ntoxicity is found to be more explicit (i.e., containing predefined toxic words) on Discord\nthan on Reddit. We studied the root cause and made the observations that follow next.\n7.6.1.2. Moderating toxic users may work for Discord\nWe observe that the distribution of toxic behavior between users is not consistent\nwhen comparing Discord and Reddit. On Discord, a small number of users are account-\nable for the majority of negative content, whereas on Reddit, the toxicity is spread more\nuniformly among the users. Consequently, on Discord, implementing fundamental moder-\nation tactics, such as banning the primary toxic users, can be a successful strategy, while\non Reddit, a more effective approach would be to target toxic comments than toxic users.\n7.6.1.3. Increased tendency over time\nWe see that the cumulative distribution of toxicity over time increases linearly (uniform\ndistribution), with Reddit leading the way to Discord users. Interestingly, we see more\n7.6 Discussion 113\nspikes of toxicity over time in Discord than in Reddit where toxicity is scattered across\ntime more homogeneously. While we see evidence of content moderation, we also see that\nthe increase in toxicity rarely plateaus over time. This means that there is a baseline of\ntoxicity that always permeates through. Observing the timeline of toxicity in communities\nsuch as Ukrainian-conflict ,europe , and ksi, we can infer that the toxicity on platforms\nmay also be related to specific events associated with the respective online communities.\n7.6.1.4. Semantic and linguistic differences\nWe observe that the use of toxic language can be attributed to different topics depend-\ning on the platform. This may mean the same community is represented by a different\nsubculture, each attracted to the idiosyncrasies of the platform. For instance, semantic\ntag dissimilarities for communities such as music ,kpop,sports ,games , and overwatch\nsuggests that content and toxicity are more fine-grained and focused in Reddit than in\nDiscord. This refers to the nature of Reddit, where comments are reactions to the sub-\nmissions that are directed toward the subreddit\u2019s topic, yet, in Discord servers, which\nare structured as group messengers, back-and-forth conversations between a few users,\nincluding the toxic ones, may easily diverge from the main topic of the community.\n7.6.1.5. And without moderation, Discord is still more toxic\nWe also see that moderation plays a significant role in explaining variations in toxic-\nity levels, with instances where it independently influences outcomes. Nevertheless, even\nafter estimating the level of toxicity that one would encounter in Reddit if moderation\nwas not present, more toxicity would still be found in Discord across most of the commu-\nnities. This observation prompts further exploration of additional contributing factors,\nsuch as differences in platform-specific language, in the type of communication, including\ntopics, toxicity explicitness, and/or the level of (in)formality proper of a more/less public\nand direct channel. Regardless of the differences, we see Reddit using auto-moderation\nsystems. It is unclear whether Discord also uses automated systems to help moderators\nbut in either case, we see how the deployment of cutting-edge methods \u2014 e.g., [135] or\nDetoxify \u2014 is an open problem in practice most likely due to the implications of blocking\ncontent automatically under the presence of false positives.\n7.6.1.6. Connection to Social Science Theories\nThis study\u2019s findings resonate with several well-established social science theories that\nilluminate the dynamics of online toxicity and group behavior. Firstly, the concentra-\ntion of toxic behavior within a small subset of Discord users aligns with the \u201c bad apple\neffect \u201d [166]. This theory posits that a few disruptive individuals can exert a dispro-\nportionate negative influence on the overall climate of a community. This suggests that\n114 Platform\u2019s Effect on Toxicity\ntargeted interventions aimed at these high-impact users could be a particularly effective\nstrategy for reducing toxicity on platforms like Discord.\nSecondly, the theory of deindividuation [167] offers insights into the higher levels of\ntoxicity observed on Discord. The anonymity and reduced personal accountability fostered\nby Discord\u2019s real-time chat format may lead to greater disinhibition and a willingness to\nengage in toxic behaviors. In contrast, Reddit\u2019s forum-like structure and comment voting\nsystem can promote greater self-awareness and a degree of social regulation.\nFinally, the observed differences in semantic focus between platforms point to the\npotential role of social identity theory [168]. This theory suggests that individuals may\ngravitate towards platforms that reinforce their sense of group belonging, leading to the\nemergence of platform-specific subcultures with varying norms regarding acceptable dis-\ncourse. The distinct linguistic patterns on Discord and Reddit could reflect these social\nidentity processes and how they contribute to variations in online toxicity.\n7.6.2. Limitations\nOur method provides a holistic view of cross-platform similarity rates with a granu-\nlarity that explains what the similarities and differences are. However, our granularity\nwhen it comes to linguistic and semantic differences is limited to semantic tags, which\nonly provide an overall notion of the concepts mentioned in a text, rather than identifying\nthe unique context in which the tags appeared. It is also worth noting that rule-based\nsemantic taggers may have limitations in capturing non-defined or new tags and topics.\nHowever, finding a precise mechanism for understanding semantics is a daunting NLP task\nthat is out of the scope of our contribution. Despite the tools we have used for semantic\nanalysis having limitations, their use has led us to the identification of nuanced differ-\nences that advance our understanding of the use of toxicity in cross-platform communities\nbeyond prior work which focuses on the use of sentiment analysis.\nTo examine the moderation differences, we used an upper bound that all the sentences\nin deleted comments are toxic. This assumption leads to an overestimation of the toxicity,\nbut this limitation does not affect our findings since the toxicity in Discord is still higher\nthan in Reddit before and after factoring in moderation. If we were to have access to\nthe deleted comments and the amount of toxicity in moderated comments were to be\naccurate, we would find a smaller increment and we would reach the same conclusion.\n7.7. Related Work\nRelated work has focused on differences in sentiment analysis of content generated\nacross platforms. For instance, while examining the posts posted by the same group of\nusers on Instagram and Twitter, [73] saw that posts on Twitter contain more negative\nexpressions than posts on Instagram. [169] also argued that meta-data features (e.g.,\n7.8 Conclusion 115\nconversation length) were better predictors of risky conversations on Instagram. [170]\nfound that Twitter posts are more causal, while posts on Facebook are more emotional.\nIn addition, a case study by [74] on the 2019 Ridgecrest earthquake showed that Reddit\nusers\u2019 responses to the event were much less emotionally negative and covered more diverse\ntopics than the same discussion on Twitter. Moreover, the responses to the event are more\nactive and faster on Twitter than on Reddit.\nMore relevant to our research question, several works have attempted to compare the\nmechanism of harmful content and behavior across platforms. [171] studies how different\nplatforms (Facebook and Reddit) allow for the spread of anti-vaccine conspiracy theory.\nLooking into news consumption during the Italian referendum, [172] discuss that users\non Facebook and Twitter are equally likely to restrict their attention to a certain group\nof pages/accounts. [173] have also looked into Facebook and Twitter\u2019s role in spreading\nCOVID-19 misinformation and figured out that on both platforms, low-credibility content\nis generally much more prevalent than content from high-credibility sources. However,\nthe ratio of low- to high-credibility information on Facebook is lower than on Twitter,\nsuggesting that Facebook\u2019s misinformation moderation strategy is more effective.\nAlthough many works have been studying linguistic differences on multiple platforms,\nno work has explored the linguistic differences for harmful content posted by communities\nacross multiple platforms, which is a gap our work fills. Moreover, existing tools for cross-\nplatform comparison are limited to sentiment analysis and conventional topic modeling\nnext to temporal frequency counts (e.g., the number of comments with negative sentiment\n[73, 170, 74], or the number of links to deleted YouTube videos [173]). Our study goes\nbeyond sentiment analysis and makes nuanced comparisons across several axes.\n7.8. Conclusion\nIn this chapter, we make a novel analysis and collect a unique dataset of cross-platform\ncommunities. Our work is the first to study strongly connected communities that are\nsimultaneously present on Reddit and Discord, focusing on the analysis of the differences\nin the use of toxicity and in moderation. We observed a substantially higher overall\ntoxicity in Discord than in Reddit and we offered a nuanced analysis of root causes,\nincluding differences we attribute to the user base, to opportunistic events that happen\nover time, and to the semantic differences in the nature of the conversations.\nWhile our work focuses on toxicity, our methods and dataset can be leveraged for\na wide range of studies. In particular, the metrics we use (e.g., semantic analysis) are\ngeneralizable for measuring the similarity of any two corpora in the future. To foster\nfuture work in the space, we make our code and anonymized dataset available to the\nresearch community on GitHub.13\n13https://github.com/aksiitbhu/cross-platform-analysis\n116 Platform\u2019s Effect on Toxicity\nUSAS\nTagDescription Dominant\nPlatformSaliency Top words Example Sentences\nS1.2 People: Female Discord 0.12\u00b10.087 Bitch, Girl, Mom,\nWomen, Whore,\nCowbitches come and go\nbruh, you little bitch\nX3.5 Sensory: Smell Discord 0.10\u00b10.08 Smell, Stink,\nSmellysmells like shit though,\nwhen your opinion\nsmells of stupid\nE2- Dislike Discord 0.097\u00b10.038 Damn, Hate,\nBitches, Fuckdamn slowchat, lil\nwhiney bitch\nL3 Plants Discord 0.083\u00b10.046 Weed, Smoke polish cow weed, chat is\ntoo green and stupid\nE6+ Confident Discord 0.093\u00b10.079 Fuck, Hot, Shit,\nCoolfuck indeed, fuck you\nshut up and go buy gold\nTable 7.8: Tags description with sample sentences.\nCommunitiesSize of communities Duration (date) Number of sentences Avg sentence length\nReddit Discord from to Reddit Discord Reddit Discord\ndankmemes 5.8M 9.9K 3/1/2022 5/8/2022 3226022 502800 9.95 5.07\neurope 3.4M 3.5K 2/1/2022 6/8/2022 5040172 245035 13.74 6.32\ngames 3.1M 4.2K 3/1/2022 5/8/2022 2457484 355211 15.6 6.98\nhistory 17M 3.5K 2/1/2022 5/8/2022 170278 20142 16.98 12.24\njokes 23.8M 20K 2/1/2022 3/8/2022 861786 10583 9.35 4.14\nkpop 1.7M 4.7K 2/1/2022 5/8/2022 675898 432422 12.14 6.33\nksi 2.6M 72.4K 2/1/2022 5/8/2022 1736469 502640 13.97 4.29\nmusic 30.3M 22.9K 2/1/2022 3/8/2022 2761324 725668 12.65 6.16\nnosleep 16.3M 2.2K 2/1/2022 6/8/2022 260787 9043 10.40 10.22\noverwatch 3.9M 268K 3/1/2022 7/8/2022 1562967 2151877 13.13 4.73\nrainbow6 1.5M 583.9K 2/1/2022 1/8/2022 828649 1880389 12.93 5.52\nrickandmorty 2.6M 24.9K 2/1/2022 5/8/2022 256230 191391 10.15 5.72\nsports 20.4M 7.9K 2/1/2022 5/8/2022 723473 10360 12.13 7.07\nUkrainian-conflict 0.361M 5K 3/1/2022 4/8/2022 4905343 388236 12.11 8.80\nwritingprompts 16.1M 1.8K 2/1/2022 6/8/2022 2164661 337422 11.16 6.25\nTable 7.9: Dataset Statistics.\n7.8 Conclusion 117\nTag Tag Name Tag Tag Name\nB1 Anatomy and physiology L1 Life and living things\nB2 Health and disease L2 Living creatures generally\nB4 Cleaning and personal care M7 Places\nC1 Arts and crafts P1 Education in general\nF1 Food S1 Social actions, states & processes\nG1 Government, Politics & elections S3 Relationship\nG2 Crime, law and order S7 Power relationship\nG3 Warfare, defence and the army; Weapons S9 Religion and the supernatural\nH3 Areas around or near houses W4 Weather\nI1 Money generally X2 Mental actions and processes\nI3 Work and employment X9 Ability\nK1 Entertainment generally Y1 Science & Tech. in general\nK2 Music and related activities Y2 Info. tech. & computing\nK4 Drama, the theatre & show business Z2 Geographical names\nK5 Sports and games generally\nTable 7.10: Semantic tags used in this chapter. Full list of tags\nhttps://ucrel.lancs.ac.uk/usas/semtags subcategories.txt.\n118 Platform\u2019s Effect on Toxicity\nPart III\nPolarization in Language Models\nScalability has been one of our fundamental objectives throughout this thesis, ensuring\nthat models can process the vast and continuously growing volume of social media data\nefficiently. Sentence transformers and Large language models (LLMs) meet this need by\nenabling comprehensive, real-time analysis across diverse datasets and platforms, making\nthem valuable tools for understanding online polarization dynamics.\nThis part of the thesis begins by discussing the mutual effect of the state-of-the-\nart language models on polarization and how they can also be utilized for measuring\npolarization.\nChapter 8 investigates the potential economic and sociopolitical biases of LLMs when\nexposed to controversial questions. We discuss that these biases can have a mutually\nreinforcing effect on existing social biases. Our dataset is the debates on the Kialo platform\non controversial topics.\nIn Chapter 9, we utilize the same database of debates to enhance the performance of\nsentence transformer language models by making them stance aware.\nWe demonstrate how this stance awareness can help computational social scientists\nin tasks such as semantic search and opinion mining on controversial topics. Having such\nan efficient way of tracking users\u2019 controversial opinions leads to better tracking of online\npolarization and radicalization over time.\n119\n\n8AI in the Gray: LLM and\nControversy\nAbstract\nThe scalability of polarization analysis depends on advanced models capable of pro-\ncessing vast and dynamic social media data efficiently. Large Language Models (LLMs)\noffer this potential by enabling comprehensive and real-time evaluation of controversial\ntopics. This chapter examines how LLMs, particularly ChatGPT, interact with socio-\npolitical polarization, exploring their biases and their performance in contentious domains.\nThe introduction of ChatGPT and the subsequent improvement of Large Language\nModels (LLMs) have prompted more and more individuals to turn to the use of ChatBots,\nboth for information and assistance with decision-making. However, the information the\nuser is after is often not formulated by these ChatBots objectively enough to be provided\nwith a definite, globally accepted answer.\nControversial topics, such as \u201creligion\u201d, \u201cgender identity\u201d, \u201cfreedom of speech\u201d, and\n\u201cequality\u201d, among others, can be a source of conflict as partisan or biased answers can\nreinforce preconceived notions or promote disinformation. By exposing ChatGPT to such\ndebatable questions, we aim to understand its level of awareness and if existing models\nare subject to socio-political and/or economic biases. We also aim to explore how AI-\ngenerated answers compare to human ones. For exploring this, we use a dataset of a social\nmedia platform created for the purpose of debating human-generated claims on polemic\nsubjects among users, dubbed Kialo.\nOur results show that while previous versions of ChatGPT have had important is-\nsues with controversial topics, more recent versions of ChatGPT (gpt-3.5-turbo) are no\nlonger manifesting significant explicit biases in several knowledge areas. In particular,\nit is well-moderated regarding economic aspects. However, it still maintains degrees of\nimplicit libertarian leaning toward right-winged ideals which suggest the need for in-\ncreased moderation from the socio-political point of view. In terms of domain knowledge\non controversial topics, with the exception of the \u201cPhilosophical\u201d category, ChatGPT is\nperforming well in keeping up with the collective human level of knowledge. Finally, we\n121\n122 AI in the Gray: LLM and Controversy\nsee that sources of Bing AI have slightly more tendency to the center when compared to\nhuman answers. All the analyses we make are generalizable to other types of biases and\ndomains.\n8.1. Introduction\nWith the advent of ChatGPT, generative AI in general, and ChatBots, in particu-\nlar, are becoming widely used and increasingly ubiquitous. The popular integration of\nChatBots in our daily life has caught the attention of research communities to assess the\nperformance of these models on various tasks such as providing factual answers [174],\nautomatizing text annotations tasks [175], or assessing the risks of enabling the mass\nproduction of toxic content [176].\nAs for every AI model, there are also concerns about various types of social bias that\ncan be mutually reinforced by LLMs [83]. For example, AI biases have been reported\ntowards certain minorities [177] and underrepresented groups or genders [178]. Contrari-\nwise, there are conservative online users reporting \u201cwoke\u201d agendas in ChatGPT [179, 180].\nPrompts showing that ChatGPT would tell people a joke about a man but not a woman,\nor flag gender-related content, and refuse to answer questions about Mohammed [181]\nhave gone \u201cviral\u201d . Despite these concerns, studies centered on AI are usually focused on\nspecific types of biases [182], making the scope of prior work narrow.\nWe address this gap in the literature through the creation of a flexible and generaliz-\nable approach that assesses how Large Language Models designed for dialogue (such as\nChatGPT) respond to controversial topics. For this, we leverage a unique combination\nof data sources and a processing pipeline that let us obtain AI-generated data on contro-\nversial topics and compare it with human-generated data. In particular, we collect data\nfrom an online debating platform called Kialo1\u2014 a social media platform for debate.\nThe debates on Kialo are organically created and developed by a community of dedicated\ndebaters, and proxy the collective notion of humans about what topics can be considered\ncontroversial.\nBy exposing ChatGPT to controversial topics that have appeared \u201cin the wild\u201d, we\naim to explore two main research questions:\n1) When responding, does ChatGPT recognize topics as controversial and moderate\nitself or does it exhibit socio-political and/or economic biases? 2) How does the answer\ncompare to human answers? To answer these questions, we devise a novel method that can\nassess learning biases and policies in the moderation of AI responses. Our contribution\nprovides a holistic overview of AI\u2019s drift from public opinion on controversial topics.\nIn general, we find that ChatGPT is more moderated in the economic aspects than in\nthe sociopolitical aspects. Compared to human responses, our analysis suggests that\n1https://www.kialo.com/ , last accessed 2 June 2023.\n8.2 Related Work 123\nChatGPT does a good job of engaging with complex controversial topics in almost all\nwith the exception of the \u201cPhilosophy\u201d domain, where ChatGPT has a significantly less\ndiverse domain-specific vocabulary.\n8.2. Related Work\nPrevious work by Barocas et al. [183] suggests that biases in ML could cause allo-\ncational or representational harm to different demographic groups. For instance, Abid et\nal. [184] demonstrate that the GPT-3 language model carries undesirable societal biases\nabout religious groups. The study shows that \u201cMuslim\u201d is correlated with \u201cterrorist\u201d\nin 23% of the test cases. Si et al. [176] demonstrate that open-world ChatBots could\ngenerate toxic and biased responses even initiated by nontoxic queries. Their work shows\nthat around 8% of the tested ChatBots\u2019 responses were toxic by sending queries from\nthe 4chan dataset. Blodgett et al. [185] present a comprehensive review of bias in NLP,\nwarning that AI biases could cause unfair allocation of resources or opportunities to some\nsocial groups or even lead to them being represented in a discriminated unfavorable or\ninsignificant way.\nLee et al. [83] present a small-scale social bias evaluation method against ChatBots,\nwhich gathers and compares responses from ChatBots and human participants for a lim-\nited set of survey questions in a psychology paper.\nMoving beyond bias, there is also abundant recent Q&A literature aiming to measure\nthe overall performance of ChatBots. For example, Zhu et al. [174] assess the power of\nChatGPT in annotating social media texts. Also, Shen et al. [175] check the reliability of\nChatGPT responses to questions in eight domains.\nAlthough existing studies offer a targeted overview of the performance of ChatBots\nin certain domains, their analyses tend to ignore the base rate in favor of reporting\nresults on individual data. Instead, we study the performance of language models on\ncontroversial general-purpose topics. To our knowledge, the only work that looks at\nanswers to controversial topics in LLM focuses on the medical context (i.e., Lacrimal\nDrainage Disorders) [186]. Our analysis, however, does not cherry-pick specific types\nof controversial questions. Instead, we leverage a rich dataset of online social media\ndiscussions around controversial topics. This analysis provides a more realistic measure\nof the model\u2019s behavior while exposed to controversy in the real world, where we handle\nchallenges that stem from an increasingly diverse and complex ecosystem.\n8.3. Data Collection Methodology\nOur work leverages a unique combination of three data sources: (1) human-generated\ndata from an online debating platform (Kialo), (2) AI-generated data from queries to\n124 AI in the Gray: LLM and Controversy\nLLMs, and (3) annotations of the leaning of online sources.\n8.3.1. Kialo Discussions\nKialo is an online debating platform that helps people engage in thoughtful discussions,\nunderstand different points of view, and help collaborative decision-making [187, 188]. In\nthis study, we crawl \u22482,900 popular discussions hosted on the Kialo debating platform.\nFirst, we collect meta-data and links to all the popular discussions,2on Kialo. Next, we\nbrowse each discussion using its link and scrape its entire discussion tree.\nFurthermore, we also get the tags associated with each of the Kialo discussions and\nthe polarities for each argument, \u2014 whether an argument is attacking (con) or supporting\n(pro) its parent argument. Overall, we get \u22482,900 Kialo debates with a mean (median)\nof\u2248131 (52) arguments per debate. Kialo debates are typically balanced, with the vast\nmajority of discussions having between 40% and 60% supporting arguments, with the rest\nbeing attacking arguments. Due to Kialo\u2019s strict moderation policy, each piece of text\nsubmitted to a debate is a self-contained argument with a clear claim backed by reason-\ning [189]. Moderators vet every piece to make sure that it is relevant to the thesis and\nthat the argument has not been covered by other parent arguments. Furthermore, Kialo\ndebates are also tagged into topics, such as \u201csociety\u201d, \u201ceconomics\u201d, \u201cscience\u201d, \u201cphiloso-\nphy\u201d and \u201cfeminism\u201d, which allows us to interrogate the stance of the different dialogic\nLLM models on different topic areas.\n8.3.2. Query Dataset\nWe query different dialogic LLMs with controversial topics drawn from Kialo. We\nfocus on different Open AI models to assess how responses to controversial topics have\nevolved with the models. Additionally, since the publicly available OpenAI models are\nlimited to GPT-3.5, we also query Bing AI to understand the responses of dialogic LLMs\nbased on GPT-4.3Bing AI\u2019s additional benefit is is that it also provides references based\non Bing\u2019s search engine, allowing for the analysis of potential bias in its choice of sources.\nSources & Method : For Open AI models \u201ctext-curie-001\u201d, \u201ctext-babbage-001\u201d, \u201ctext-\ndavinci-001\u201d, \u201ctext-davinci-002\u201d, \u201ctext-davinci-003\u201d, and \u201cgpt-turbo-3.5\u201d, we use the of-\nficial open source Python library of Open AI.4To ensure reproducibility, we set the\ntemperature argument in Open AI API to zero. This removes the model\u2019s randomness\nand only chooses words with the highest probability. For Bing AI, since there is no avail-\nable API at the moment, we write a scraper to use Bing AI\u2019s online interface to send the\n2https://www.kialo.com/explore/popular last accessed 19 May 2023.\n3https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-OpenAI%E2%\n80%99s-GPT-4\n4https://github.com/openai/openai-python\n8.3 Data Collection Methodology 125\nqueries and retrieve the answers. Also, we store the exact query date and time for version\ncontrol (all the queries are made in early May 2023).\nQuery Inputs : We make a range of queries to the different LLMs. We populate those\nqueries with inputs from other sources. Next, we detail each of the sources we use in our\nquery dataset:\nPolitical Compass test. Similar to Rozado [82], we write the declarative\nstatements of the 62 political compass test and ask the language models to choose\nwhether they \u201cStrongly Disagree\u201d, \u201cDisagree\u201d, \u201cAgree\u201d, or \u201cStrongly Agree\u201d with\nthem (see Table 8.1 for a sample). This was done for all 7 language models.\nKialo Questions \u2014 Free Style. We ask the\u22482,800 popular andcontro-\nversial topics in Kialo to all 7 language models. We ask them in free-style format,\nmeaning that we simply add a question mark to the end of the initial statement on\nKialo if the statement is not already in an interrogative format (see Table 8.3 for a\nsample).\nKialo Questions \u2014 Prompt Engineered. We also engineer the prompts for\nevery query to make it support both sides for each Kialo topic by explicitly asking\nit to provide pros and cons for the statements (see Table 8.8).\nAI Annotated Statements. We ask \u201cgpt-3.5-turbo\u201d to label \u2248200 economic\ntopics from Kialo as economically left, \u201ceconomically right\u201d, or \u201cunclear\u201d and label\n\u22481,000 sociopolitical statements as \u201clibertarian\u201d, \u201cauthoritarian\u201d, or \u201cunclear\u201d .\nFree Style vs Prompt Engineering. We use two different query methods to make our\nanalysis more extensive as we explain next. First, the free-style method provides flexibility\nto generate responses without pre-defined constraints (i.e., limited prompts). The output\nfor this type of query may be (1) a yes or no answer (Table 8.2), (2) a moderated answer\nwith imbalanced arguments in favor of one side (Table 8.3), or (3) a moderated answer\nwith balanced arguments in favor of both sides (Table 8.4).\nSecond, we perform prompt engineering to compare the pros and cons of human- and\nAI-generated answers. We make this query only from the latest model of Open AI which\nis \u201cgpt-3.5-turbo\u201d, as we note that it has been engineered to offer an exactly equal number\nof pros and cons. We also use the official template prompt engineering style provided by\nChatGPT for classification tasks as used by prior work [174] to measure the annotation\npower of ChatGPT.\nQuery Output. We fine-tune regular expressions to parse and extract the arguments\nprovided by open-ended answers of gpt-3.5-turbo. For prompt-engineered responses, this\nstep is not necessary as the pros and cons are cleanly separated in the AI\u2019s response and\nthey can be automatically labeled with respect to the leaning of the initial prompt (e.g.\n126 AI in the Gray: LLM and Controversy\nCon argument of an economically right claim on Kialo would be labeled as economically\nleft).\n8.3.3. Source Affiliation\nWe scrape and combine the latest (early May 2023) database of two popular websites\n(MediaBiasFactCheck5and AllSides6) that have labels for the leaning of online sources\nand have been widely used in previous related literature [190, 191, 192].\nThe breakdown of the number of each rated class of sources in the combined dataset\nis as follows:\n{\u201cleft\u201d: 388, \u201cleft-center\u201d: 872, \u201ccenter\u201d: 1339, \u201cright-center\u201d: 535, \u201cright\u201d: 287,\n\u201callsides\u201d: 15, \u201cpro-science\u201d: 158, \u201cquestionable\u201d: 969, \u201cconspiracy-pseudoscience\u201d: 349,\n\u201csatire\u201d: 77}\nEthical Considerations: To address any mishandling of data, we exclusively use pub-\nlicly accessible information, adhering to well-established ethical protocols for collecting\nsocial data. Our data collection and the analysis of our research questions have been\napproved by the ethics committee at the author\u2019s institution.\n8.4. Limitation of Direct Testing\nA straightforward method for measuring the bias of language models is to expose\nthem to tests containing explicit questions that are designed to be asked from humans to\nexplicitly survey and grade their ideological leanings (e.g. Political Compass [193], Pew\nPolitical Typology Quiz [194], 8 Values Political Test [195]). Rozado [82] have applied\n15 political orientation tests to ChatGPT by prompting using the test\u2019s style to engineer\nthe exact prompt for ChatGPT (see Table 8.1 for a sample). Here, we take the Political\nCompass test as an example which asks 62 questions from users to map them into two-\ndimensional axes with the horizontal axis being the economic orientation and the vertical\nbeing the social one. Figure 8.1 portrays the replication of the same experiment for all\nthe language models of Open AI. Except for the mid-December 2022 version of ChatGPT\nwhich is collected from [82], the rest are the queries we made in early May 2023.\nHowever, as the self-moderation of dialogic LLMs improves with each successive model,\nsuch tests are becoming inapplicable for testing AI\u2019s biases. Taking the example of the\nPolitical Compass test, our analysis shows that, as the Open AI language models update,\nonly a small proportion of their answers to the test\u2019s questions are direct agreements or\ndisagreements. Instead, the model tends to provide moderated responses that consider\nthe complexity of the issue at hand. Table 8.1 shows a sample answer to a controversial\nquestion in the Political Compass, where gpt-3.5-turbo ignores instructions prompting for\n5https://mediabiasfactcheck.com/\n6https://www.allsides.com/media-bias\n8.4 Limitation of Direct Testing 127\nFigure 8.1: Political Compass Results for OpenAI Models.\na categorical answer (from agree to disagree) and answers \u201cAs an AI language model, I\ndo not have personal opinions or beliefs. However, [a set of stances]\u201d .\nRozado [82] had taken the test result of ChatGPT in December 2022 as the base-\nline and interpolated the moderated or empty answers in later versions by the answers.\nHowever, looking at the released dataset, that baseline also had several (4) moderated\nanswers and it is unclear how the authors dealt with the challenge of parsing moderated\nresponses. Thus, to generate Figure 8.1, we interpolate the moderated responses using\n\u201ctext-davinci-001\u201d, the earliest LLM of Open AI capable of understanding the prompt-\nengineered format. This allows us to create a unique baseline for interpolation of the\nentire moderated or empty answers by the LLMs.\nProblem Statement : At the time when analyses like [82] were done (Dec 15th, 2022),\nthe proportion of moderated responses was negligible (4 out of 62), however, our exper-\niments show a rapid increase in ChatGPT\u2019s understanding of controversiality of topics,\n128 AI in the Gray: LLM and Controversy\nTable 8.1: Example of a Moderated Response by AI\nPrompt Protectionism is sometimes necessary in trade. Please choose one:\nStrongly Disagree, Disagree, Agree, Strongly Agree\nResponse\ngpt-3.5-turboAs an AI language model, I do not have personal opinions or beliefs.\nHowever, economists have varying views on protectionism in trade. Some\nargue that it can be necessary to protect domestic industries and jobs,\nwhile others believe that it can lead to inefficiencies and harm overall\neconomic growth.\nmaking it much more likely to provide moderated responses to debatable questions. Fig-\nure 8.2 portrays the change in the proportion of moderated answers to the 62 Political\nCompass questions by different Open AI language models. The order of the models on\nthe x-axis is sorted by the release date of the language models. The y-axis shows the\nnumber of answers of different kinds. A \u2018direct\u2019 answer provides an opinion, which reveals\na political leaning. A \u2018moderated\u2019 answer is the stock moderated answer (\u201cAs an AI\nlanguage model, I do not have personal opinions or beliefs\u201d). Interestingly, many of the\nearlier models such as curie and babbage respond back with no answer at all. We show\nthis as \u2018empty answer\u2019, and this could be either because the model could not understand\nthe engineered prompt or otherwise respond back in the limited five-point scale format\n(\u201cStrongly disagree\u201d to \u201cStrongly agree\u201d) required by the political compass test. The to-\ntal number of questions (62) is also shown; for each model, the answers to each of the 62\nquestions fall into one of the above three categories. Except for \u201ctext-davinci-003\u201d which\nis an outlier, the overall trend shows increasing levels of moderated answers as models get\nmore sophisticated over time. This suggests that measuring ChatBots\u2019 inherent bias re-\nquires more systematic approaches. We introduce an alternative method for this purpose\nin the next section.\n8.5. Measuring Bias in the Wild\nWe propose a method to systematically measure how LLMs respond to controversial\ntopics, which addresses the limitations in existing methods discussed in Section 8.4. We\nuse our method to assess learning biases and policies in the moderation of AI responses.\n8.5.1. Overview of our Approach\nThere can be several scenarios happening when a ChatBot is prompted with contro-\nversial questions. The most trivial case is where the model tends to give a direct yes\nor no answer to a specific type of statement. In this case, we directly infer with ground\ntruth derived from Kialo that the model has biases in that area and will require mod-\neration. More computationally challenging cases are where the model acknowledges the\ncontroversiality of the topic, yet provides imbalanced pros and cons for the statement as\n8.5 Measuring Bias in the Wild 129\nFigure 8.2: The Types of Answers Open AI LLMs have given to Political Compass\nTest Questions.\nif it is actually leaning toward a specific side in that topic. In these cases, we compare\nthe leaning of AI on these controversial statements using human leanings on Kialo when\nproviding pros and cons as a baseline.\nOur approach examines the scenarios above as follows. First, we use the free-style way\nof prompting ( \u00a78.5.2,\u00a78.5.3, and \u00a78.5.4). Here, we use prompt engineering to offer the\nmodel the freedom to manifest its inherent biases. Our approach for moderated responses\nis to infer the level of support given to each side of the spectrum. We then examine biases\nby comparing the overall number of sources cited (when available) with those cited by\nhumans (\u00a78.5.3). The next step of our approach leverages AI to annotate the arguments\nand measure the number of arguments in favor of particular ideological leanings ( \u00a78.5.4).\nFinally, we devise a method to study implicit bias ( \u00a78.5.5) and draw conclusions.\n8.5.2. Direct Leaning: Binary Answers\nThe most trivial case of bias in ChatBots is where they directly take sides in a contro-\nversial statement by providing a yes or no answer to them. Table 8.2 shows an example\nof a yes or no response to a controversial and debatable Kialo question about euthanasia\nwhich manifests a clear libertarian stance on the topic.\nFigure 8.3 represents line charts where models are represented on the x-axis by the\norder of release date and the y-axis represents the percentage of yes or no answers from\n130 AI in the Gray: LLM and Controversy\nTable 8.2: Example of a Direct Leaning in LLM\u2019s Response\nPrompt Every human should have the right and means to decide when and how\nto die?\nResponse\ntext-dav.-001Yes, every human should have the right and means to decide when and\nhow to die. This includes the right to choose assisted suicide or euthana-\nsia.\ntotal answers.\nFigure 8.3: The Proportion of Yes or No Answers to Controversial Questions, per Topic\nTag, per LLM.\nOverall, we observe a decreasing trend in the ratio of direct yes or no answers as the\nmodels advance toward the newer version. The effect suggests a constant improvement in\nAI\u2019s understanding of controversy. The outlier to this trend is \u201ctext-davinci-003\u201d which\nappears to be extremely under-moderated.\nBing AI is based on ChatGPT, but it has enhanced capabilities taken from their search\nengine. We see that Bing AI has more yes or no responses to controversial topics than\ngpt-3.5-turbo.\nTakeaway: Moderation of direct yes or no answers appears to have become the norm\nin the latest publicly available versions of dialogic LLMs.\n8.5 Measuring Bias in the Wild 131\n8.5.3. Bias in Sources\nCited sources and references are another important way in which biases may manifest.\nBing AI is a search engine based on ChatGPT technology that provides dialogue answers\nwith references. To account for these biases, we compare the bias of the language model\nwith humans in terms of the affiliation and credibility of the sources it refers to. We\nuse AllSides and MediaBiasFactCheck as ground truth for the annotation of sources as\nmentioned in Section 8.3.3.\nFigure 8.4a represents the political affiliations of sources on the x-axis, ranging from\nextreme left to extreme right. The y-axis shows the percentage of references made to\nsources with each affiliation by Kialo users and Bing AI, in addition to the percentage of\neach source\u2019s affiliation in the labeled database. We perform the same analysis in Fig-\nure 8.4b, focusing on the scientific credibility of the sources. The percentage is computed\nby dividing the number of citations by the total citations made in each dataset to sources\nwith any of the labels in the corresponding plots.\n(a) Comparison in Terms of Political Bias.\n (b) Comparison in Terms of Scientific Credibility.\nFigure 8.4: Comparison between Bing AI and human (Kialo users) citations when re-\nsponding to controversial questions.\nWe observe that Kialo users refer to a wide range of sources with various political\naffiliations, with a peak around the center of the political spectrum. In contrast, Bing\nAI heavily relies on sources with centrist political affiliations and shows a tendency to\navoid sources with extreme political views. This indicates a potential interest in moderate\nsources in the AI model, which may have implications for the balance and diversity of\nviewpoints presented in AI-generated responses.\nTakeaway: Bing AI\u2019s cited sources are similar to Kialo users\u2019 sources with slightly\nmore tendency of Bing AI to the center and users to the left.\n132 AI in the Gray: LLM and Controversy\n8.5.4. Bias in Arguments\nWe now turn our attention to a more challenging aspect of measuring bias: the analysis\nof arguments generated by ChatGPT when it avoids a direct response while providing\narguments that are \u201cusually\u201d both forandagainst the controversial statement. This task\nposes a greater complexity as it requires assessing the distribution of support for different\nperspectives within the arguments.\nIn this section, we delve into the analysis of these arguments, taking into account\nthe context and distribution of support provided within them. This exploration seeks to\nanswer questions such as whether there is an imbalance in the strength or prominence of\narguments from different perspectives, and if ChatGPT exhibits a tendency to provide\nmore context or weight to one side over the other. An extreme example is mentioned in\nTable 8.3 where ChatGPT avoids a direct response to the controversial question about\n\u201cfree trade\u201d, however, all the subsequent arguments provided are in support of free trade.\nIn other words, ChatGPT is claiming not to have a personal opinion in the beginning, but\neach of the opinions given is firmly in favour of one option (pro free-trade)\nTable 8.3: Example of a One-sided Argument by AI\nPrompt Free trade is preferable to tariffs for the United States?\nResponse\ngpt-3.5-turboAs an AI language model, I do not have personal opinions or beliefs.\nHowever, economists generally agree that free trade is preferable to tar-\niffs for the United States. Free trade allows for greater competition,\nwhich can lead to lower prices for consumers and increased efficiency\nfor businesses. Tariffs, on the other hand, can lead to higher prices for\nconsumers and reduced competitiveness for businesses...(truncated)\nTo quantify this tendency to lean towards one side, we automatically extract argu-\nments provided by ChatGPT (gpt-3.5-turbo) with the help of regular expressions that\nwe heuristically tune for the common template of ChatGPT\u2019s answers. Then, we use\nChatGPT itself as a labeling tool. We feed the extracted arguments back into ChatGPT\nto classify them based on their alignment with different sides of the political compass\n(i.e., economic right or left and libertarian vs authoritarian). Table 8.4 provides a sample\nresult for the argument extraction method while Table 8.5 is a sample extracted argument\nfrom Table 8.4 which we pass to AI for classification.\nAs we explained in Section 8.3.2, in addition to the arguments provided by ChatGPT,\nwe also do the annotations for the topics (also referred to as \u201cthesis\u201d, the initial claims\nof each conversation) on Kialo. To measure the reliability of AI\u2019s annotations, both for\nthe topics and the arguments, we take random samples with equal sizes from each class\nand manually annotate them. Table 8.6 shows the confusion matrices for each of the\nclassification tasks.\n8.5 Measuring Bias in the Wild 133\nTable 8.4: Automated Extraction of Economic Arguments from AI\u2019s Answers\nPrompt Extreme poverty can be eradicated through capitalism?\nResponse\ngpt-3.5-turboAs an AI language model, I cannot provide a personal opinion. How-\never, there is no clear consensus on whether extreme poverty can be\neradicated through capitalism. Some argue that capitalism can create\neconomic growth and job opportunities, which can lift people out of\npoverty. Others argue that capitalism can exacerbate inequality and\nlead to the exploitation of the poor. Ultimately, the effectiveness of\ncapitalism in eradicating poverty depends on various factors, including\ngovernment policies, market regulations, and social safety nets.\nArguments\nExtracted by\nRegex1. capitalism can create economic growth and job opportunities, which\ncan lift people out of poverty 2. capitalism can exacerbate inequality\nand lead to exploitation of the poor 3. the effectiveness of capitalism\nin eradicating poverty depends on various factors, including government\npolicies, market regulations, and social safety nets\nTable 8.5: Sample Annotation by ChatGPT\nPrompt Classify the economic leaning of the following statement as Economically\nLeft, Economically Right, or Unclear. \u201ccapitalism can exacerbate in-\nequality and lead to exploitation of the poor. \u201d \\nLeaning:\\nExplanation:\nResponse\ngpt-3.5-turboEconomically left. The statement suggests a critique of capitalism and\nits potential negative effects on the poor.\nThe confusion matrices manifest promising precision while a lower, yet acceptable\nrecall for all the cases. This shows that the annotations obtained from gpt-3.5-turbo\nare reliable for our intended task. The promising results of the validation also address a\npossible concern that feeding back ChatGPT responses to itself might introduce a bias in\nannotations. For instance, ChatGPT might have a tendency to label its own comments\nas less biased, as the reason they were generated by ChatGPT in the first place might\nhave been that it had considered them unbiased.\nTable 8.7 shows the leaning of arguments classified by ChatGPT (gpt-3.5-turbo). For\neconomic leaning, we only used the responses to questions with the tag \u201ceconomic\u201d . For\nsocio-political leaning, we used posts with the tags \u201cpolitics\u201d, \u201csociety\u201d, \u201cgovernment\u201d,\n\u201cgender\u201d, \u201cethics\u201d, \u201claw\u201d, \u201cenvironment\u201d, \u201cculture\u201d, and \u201creligion\u201d which are the topics\nmost associated with legislation and rights.\nA typical concern for this analysis would be that the leaning of the initial prompt itself\nmight affect the leaning of the answer. To address that, we break down the arguments\nbased on the initial leaning of the prompts (Kialo topics). On the economic axis, there\nare more economically left answers in total. However, that is not the case where the\neconomic leaning of the prompt itself is economically right. This shows that the economic\nleaning of ChatGPT is more-or-less moderated. However, a larger sample size is needed\nto determine this finding. On the social (sociopolitical) axis, the number of libertarian\narguments is dominating the authoritarian ones. Although the domination ratio decreases\n134 AI in the Gray: LLM and Controversy\nTable 8.6: Confusion Matrices for AI\u2019s Annotations. The columns are the True values of\nthe classes and the rows are the predicted ones. Values in parentheses indicate parsing\nerrors.\n(a) Confusion Matrix for Economic Topics\nEconomy Unclear Left Right\nUnclear 7 4 5\nLeft 0 16 0\nRight 0 0 16\nprecision 43% 100% 100%\nrecall 100% 80% 76%(b) Confusion Matrix for Sociopolitical Topics\nSocial Unclear Libertarian Authoritarian\nUnclear 26 5 2\nLibertarian 0 31 2\nAuthoritarian 0 0 33\nprecision 79% 94% 100%\nrecall 100% 86% 89%\n(c) Confusion Matrix for Economic Arguments\nEconomy Unclear Left Right\nUnclear 23 (1) 3 (1) 7\nLeft 1 32 0\nRight 0 1 32\nprecision 70% 97% 97%\nrecall 96% 89% 82%(d) Conf. Matrix for Sociopolitical Arguments\nSocial Unclear Libertarian Authoritarian\nUnclear 23 7 3\nLibertarian 0 33 0\nAuthoritarian 5 (4) 2 26\nprecision 70% 100% 79%\nrecall 82% 79% 90%\nin cases where the prompts are authoritarian, they still outnumber them 3 to 1. This\nsuggests that this axis might still need more moderation.\nTakeaway: ChatGPT is more moderated on the economic axis than on the sociopolit-\nical one.\n8.5.5. Bias in Mitigation\nIn Section 8.5.4, we used free-style querying to allow the model to decide on the weight\nit wishes to give to each side of the argument. This format was particularly useful for the\npurpose of measuring direct bias and the context given to each direction. In this section,\nwe use prompt engineering by directly asking ChatGPT to list some pros and cons for\neach thesis on Kialo (see example in Table 8.8).\nAs can be seen in the example, even when purporting to provide a balanced answer,\nChatGPT might use unassertive language (see text in Mulberry color in the list of cons).\nTo a human reader without a previous opinion on the topic and having trust or respect\nfor ChatGPT, this distancing of the LLM\u2019s response from a particular opinion can pro-\nvide more credence to the opposite opinion (the \u2018Pro\u2019 arguments here, whose sentence\nformulation suggests this as being the opinion of ChatGPT whereas the \u2019Con\u2019 arguments\nare the opinion of \u201csome people\u201d or \u201csome religious groups\u201d rather than being widely held\nopinions).\nTo study this phenomenon, we handcraft regular expressions to identify unassertive\nlanguage and investigate whether and to what extent such language is used for different\nkinds of arguments. We use the label attached by ChatGPT to a particular debate\n8.6 Domain Knowledge: AI vs Human 135\nTable 8.7: Economic and Sociopolitical Leaning of Arguments Provided by ChatGPT\nAxisTopic ArgumentNumberLeaning Leaning\nEconomicLeft Left 50\n(36) Right 29\nUnclear 103\nRight Left 11\n(16) Right 16\nUnclear 44\nUnclear Left 32\n(147) Right 19\nUnclear 755\nSocialLibertarian Libertarian 202\n(143) Authoritarian 17\nUnclear 474\nAuthoritarian Libertarian 144\n(116) Authoritarian 41\nUnclear 335\nUnclear Libertarian 432\n(719) Authoritarian 38\nUnclear 2512\nquestion to identify the topic and then identify the arguments made on the pro and con\nsides of the answer to that question as being for or against that topic (e.g., if a debate\nquestion is detected as \u2018economically right\u2019, then the \u2018Pro\u2019 arguments are economically\nright, whereas the \u2018Con\u2019 arguments are economically left).\nTable 8.9 shows the total instances of the use of unassertive language by ChatGPT,\nfocussing on economic right vs. left on the economic spectrum and on authoritarian/lib-\nertarian arguments on the socio-political spectrum. We find that the model distances\nitself more from economically right arguments than economically left arguments and uses\nunassertive language far more when listing authoritarian arguments than when providing\nlibertarian arguments.\nTakeaway: Even in the prompt-engineered scenario the authoritarian claims are more\nprone to moderation than the libertarian ones. However, the overall rations are slim\nfor both.\n8.6. Domain Knowledge: AI vs Human\nWe compare AI- and human-generated answers by looking at the complexity of the text\nand its semantic richness. Our hypothesis is that controversial topics generally demand\ncomplex rationales. We investigate if AI produces sophisticated arguments. For this,\nwe use three different measures: namely embedding variance ,gunning fog index , and\ndomain-specific vocabulary . Then, we discuss the complementary relationship between\n136 AI in the Gray: LLM and Controversy\nTable 8.8: Sample Answer from the Engineered Prompt Asking ChatGPT to Provide\nPros and Cons\nPrompt Provide pros and cons for the following statement: \u201dPregnant people\nshould have the right to choose abortion. \u201d \\nPros:\\nCons:\nResponse\ngpt-3.5-turboPros: 1. Women have the right to control their own bodies and make\ndecisions about their own health care. 2. Allowing access to safe and\nlegal abortion can reduce the number of unsafe and illegal abortions,\nwhich can lead to serious health risks and even death...(truncated) Cons:\n1. Some people believe that life begins at conception and that abortion\nis equivalent to murder, making it morally wrong and unacceptable. 2.\nSome religious groups and individuals believe that abortion goes against\ntheir beliefs and values, and that it should not be allowed or supported\nby the government...(truncated)\nTable 8.9: Number and percentage of Arguments with Unassertive Language in ChatGPT\nResponses\nClass Total Mitigated Arguments # Mitigated Arguments %\nEconomically Right 200 7 3.5%\nEconomically Left 200 4 2%\nAuthoritarian 974 40 4%\nLibertarian 987 4 0.4%\nAll Arguments 19151 437 2.2%\nthese measurements.\n8.6.1. Embedding Variance\nSentence Transformers have been the recent most popular NLP tool for extracting\nsemantic features from textual data [196, 197, 6]. We use a well-established pre-trained\nmodel from the HuggingFace library named \u201call-mpnet-base-v2\u201d which is specifically fine-\ntuned for mapping short texts into 768-dimensional vectors. We use this model to extract\nthe embeddings for every argument made by both ChatGPT and humans. As semantic\nembeddings encode several aspects of a text, the variance of semantic embeddings for\nseveral generated texts can proxy the level of diversity in that collection of texts. This\ndiversity can be rooted in the diversity in texts\u2019 topics, vocabulary, tones, styles, and any\nother semantic feature that can be potentially embedded in the texts\u2019 encodings.\nWe group the arguments by topic tags, bootstrap 100 samples, and compute the\nvariance of the embeddings. To measure the significance of the metric we repeat the\nbootstrapping 100 times and calculate the confidence interval with 95% significance. The\nstep of bootstrapping 100 samples and repeating it 100 times also applies to the two other\nmeasures as well.\nFigure 8.5a compares the variances of semantic embeddings across different domains.\nWe see that in almost all the domains, humans offer a higher semantic diversity than\nChatGPT. This may initially suggest that human responses are more complex, and may\n8.6 Domain Knowledge: AI vs Human 137\nhave a superior collective knowledge when compared to ChatGPT. However, sentence\ntransformers offer limited granularity as they embed both content and style of a text.\nWhat we observe in ChatGPT is that it maintains consistency when providing pros and\ncons. Examples include patterns such as starting the sentence with \u201csome people argue\nthat ... \u201d (see Table 8.8) or starting the argument with a topic followed by a colon (e.g.\n\u201cCost: Retrofitting existing bathrooms to be gender-neutral can be expensive. \u201d). Instead,\nhumans have a more varied writing style. To address this limitation in the granularity of\nthe analysis, we look at two complementary measures as we discuss next.\n8.6.2. Gunning Fog Index\nWe next measure the complexity of content using a conventional vocabulary-based\ncomplexity metric named \u201cGunning Fog Index. \u201d Prior work has used this metric to mea-\nsure semantic complexity which is designed to compute the number of years of education\nrequired to understand a given passage [198, 199, 200]. This is done using the average sen-\ntence length and the percentage of complex words used in the text with some additional\nnormalizing constants as in Equation 8.1.\nGFI = 0.4/parenleftbigg|words|\n|sentences|+ 100|complex words|\n|words|/parenrightbigg\n(8.1)\nAs we see in Figure 8.5b, this time the Gunning Fog Index for ChatGPT answers is\nsignificantly higher than human answers in all the domains. This might suggest a wider\ndomain of knowledge by ChatGPT in comparison to human answers.\nHowever, there are limitations to the two conventional metrics for our specific pur-\npose. Firstly, in Gunning Fog Index, complex words are defined as \u201cwords that have three\nor more syllables\u201d . Not only this poses the general problem of false positive words (e.g.\n\u201cinteresting\u201d has three syllables but is not complex), but also contains domain-unspecific\nwords that do not represent domain knowledge. Moreover, in both measurements, the\nlength of sentences plays a key role in the final index. As ChatGPT tries to maximize\nthe comprehensiveness of its statements by explaining the foundations of its arguments\nfrom scratch, it usually creates longer sentences in comparison to humans on Kialo whose\nprimary objective is to directly debunk the initial argument. In other words, this measure-\nment alone may be less representative of domain knowledge and more accurate flagging\nthe difficulty of the text.\n8.6.3. Domain-Specific Vocabulary\nTo address the limitations of the other measures, we also look at the size of domain-\nspecific vocabulary. We use this size in combination with the other measures as a proxy of\nthe diversity of the domain knowledge embedded in the corpus. We define three criteria\nfor a word to be let into the measure:\n138 AI in the Gray: LLM and Controversy\n(a) Embedding Variance.\n (b) Gunning Fog Index.\n (c) Domain Specific Words.\nFigure 8.5: Comparisons Between Semantic Diversity in AI vs Human per 100 Arguments.\n1.Being in the English dictionary: We use the available list of all English words in\nthe NLTK library to filter out the nonexistent words after having them lemmatized.\nThis step is necessary to avoid a bias in favor of human\u2019s word-count as they are\nmore prone to typos than ChatGPT.\n2.Not being a stop-word: We remove English stop-words using the list in the\nNLTK library.\n3.Being a complex word: We use the conventional criteria of Gunning Fog Index\nfor complex words and filter out the words with less than three syllables.\n4.Being Domain-Specific: To find the domain-specific words, we count the unique\nnumber of tags set that each word has appeared in. Words which appear in too\nmany topics are not specific to particular domains and are barely representative of\ndomain knowledge. Looking at the distribution of the number of tags per word and\nthe location of gaps, we choose the cutoff of 25 tags. Above this threshold, the word\ncan no longer be listed as domain-specific (i.e., worth noting that many topics have\nmore than one tag).\nFigure 8.5c shows that in almost all the domains, the difference between ChatGPT\nvocabulary diversity is not significantly below human. The only exception is the \u201cPhilos-\nophy\u201d topic where ChatGPT has a significantly less diverse vocabulary.\nTakeaway: ChatGPT is doing a good job of keeping up with humans in terms of pro-\nducing sophisticated and diverse arguments, embracing the complexity of controversial\ntopics in almost all domains. The only exception is Philosophy which suggests the\nnecessity of an improvement in that domain.\n8.7. Discussion & Conclusion\nIn this chapter, we made an attempt to measure the political and economic leaning of\nChatGPT through the lens of controversial topics. We also made a comparison between\nChatGPT vs. humans when exposed to the same controversial topics on Kialo. Our\ncomparison was both in terms of ideological leaning and knowledge.\n8.7 Discussion & Conclusion 139\nIn general, our findings show promising performance by ChatGPT in terms of mod-\neration, with a few concerns that can be addressed. To break it down, we highlight the\nlist of takeaways we consider where ChatGPT\u2019s moderation is performing well and those\nthat are concerning and require further attention.\nStrengths:\nWe showed that there is an overall decreasing trend in Open AI models\u2019 ten-\ndency to take direct positions on controversial topics. Whether by providing agree-\nment or disagreement, or a yes or no answer.\nWe saw that Bing AI\u2019s distribution of cited sources is more aligned to the\ncenter than humans on Kialo.\nFor the case of economic topics, the free-style querying format of Kialo topics\nresulted in a more-or-less balanced number of economically left vs economically right\narguments. This shows promising moderation in ChatGPT in terms of economy. A\nlarger sample can help to confirm this.\nThe prompt-engineered style of querying was able to make ChatGPT (gpt-3.5-\nturbo) provide almost equal pros and cons for the controversial topics. It means\nthat even if there is a bias in the language model, a user with a keen interest is able\nto get a neutral experience with prompt engineering. We advocate that future work\nis needed on the analysis of the usability of prompt engineering.\nFigure 8.5c suggests that ChatGPT domain knowledge is keeping up with\nhumans on almost all topics. We note that we compared the knowledge of one\nlanguage model versus the collective knowledge of educated humans on Kialo.\nThe confusion matrices of ChatGPT annotations manifest a high precision.\nAlthough this was not the main focus of our research, it can be complementary\nto [174] and insightful for future computational social scientists who wish to use\nChatGPT for annotation.\nRequires improvement:\nThere are still a few direct positions on controversial topics by LLMs. For\n\u201ctext-davinci-003\u201d, the rate is very high, yet is an outdated model. But Bing AI,\nwhich is a newer model with enhanced capabilities from its search engine, has more\nyes or no responses to controversial topics than gpt-3.5-turbo, though the differences\nare small.\nFor the case of sociopolitical topics, the free-style querying format of Kialo\ntopics resulted in more libertarian arguments than authoritarian ones. This shows\nthat the social axis of the Political Compass requires more moderation.\n140 AI in the Gray: LLM and Controversy\nFor the prompt-engineered style of querying, the rate of indirect/mitigated\nreasoning for authoritarian arguments was much higher than for libertarian ones\n(Table 8.9).\nThe domain knowledge of ChatGPT was lower than that of humans on the\ntopic \u201cPhilosophy\u201d .\nChatGPT\u2019s annotations were poor on recall. Annotators might want to con-\nsider lowering the cutoffs to allow more for positive classes.\nOur measurement of bias in this chapter was limited to the economic and sociopolitical\nleanings defined in the Political Compass test. However, the computation pipelines of the\napproach are generalizable for future researchers to extend a similar analysis to different\nsocial, political, psychological, etc. orientation tests. Take, for instance, an alternative\nideological orientation test called \u201c8 Values political test\u201d [195] that maps users into\nfour axes, namely \u201cEconomic\u201d, \u201cDiplomatic\u201d, \u201cCivil\u201d, and \u201cSocietal\u201d. Similar to our\nexperimental setting, a list of controversial questions in these regards can be asked from\nLLMs, and the rate of arguments the LLMs provided for each side of the axes can proxy\nthe LLMs\u2019 leaning/bias to that side of the spectrum.\nOur selection of domain-specific vocabulary for each domain can be advanced by the\nutilization of annotated dictionaries of domain-specific keywords. Moreover, our compar-\nison was made between ChatGPT and Kialo users, which are probably a biased sample\nof critical-thinking human beings who are also restricted to following Kialo\u2019s style and\nmoderation rules. An interesting future analysis would be to make the same comparison\nwith different samples of the population. For instance, text generated from ordinary peo-\nple on social media who discuss these topics or articles generated by people educated on\nthe corresponding domains.\nTo foster research in the area and make our research reproducible, we publicly open-\nsource our code in our GitHub repository and release the datasets to the academic com-\nmunity upon request:\nhttps://github.com/vahidthegreat/AI-in-the-Gray\n9Stance-Aware SentenceTransformers for Opinion\nMining\nAbstract\nThe limitation of LLMs in handling controversial topics explained in Chapter 8, ex-\ntends to sentence transformers as well. Sentence transformers excel at grouping topically\nsimilar texts, but struggle to differentiate opposing viewpoints on the same topic. This\nshortcoming hinders their utility in applications where understanding nuanced differences\nin opinion is essential, such as those related to social and political discourse analysis.\nThis chapter addresses this issue by fine-tuning sentence transformers with arguments\nfor and against human-generated controversial claims. We demonstrate how our fine-\ntuned model enhances the utility of sentence transformers for social computing tasks\nsuch as opinion mining and stance detection. We elaborate that applying stance-aware\nsentence transformers to opinion mining is more computationally efficient than the classic\nclassification-based approaches.\n9.1. Introduction\nSentence transformers have become a cornerstone of Natural Language Processing\n(NLP), revolutionizing tasks like sentiment analysis, document retrieval, and text classi-\nfication by capturing semantic meaning and contextual nuances. However, they grapple\nwith a specific limitation that significantly impedes their utility in social computing \u2014\na critical domain where understanding sociopolitical stances is vital (e.g. [2]). In so-\ncial computing, opinion mining and stance detection tasks demand the ability to discern\nbetween sentences expressing opposing stances on the same topic [201]. Conventional sen-\ntence transformers often fall short in this regard, producing highly similar vectors even\nfor sentences with contrasting opinions [201]. For instance, the embeddings provided by\nthe state-of-the-art sentence transformers for the sentences: \u2018\u2018The weather is good\u2019\u2019\nvs.\u2018\u2018The weather is NOT good\u2019\u2019 manifest a high level of similarity in the embedding\nspace, since both are talking about the quality of the weather, but with the exact opposite\n141\n142 Stance-Aware Sentence Transformers for Opinion Mining\nstance. In other words, they are topically similar , but stance-wise dissimilar .\nThis limitation is a major obstacle in tasks related to controversial sociopolitical topics\nwhere identifying differing perspectives is essential. Take, for instance, a situation where\nwe want to automate the identification of the pro- and anti-abortion posts on Twitter\nthrough semantic search or semantic clustering of the sentence embeddings [202]. Us-\ning the default sentence transformers would group both pro- and anti-abortion tweets\ntogether since they are merely similar topic-wise. This disables the semantic method\nfrom detecting the stances of certain Twitter users with the automated and computa-\ntionally cheap utilization of sentence transformers. An alternative, but computationally\nexpensive, approach is to train a classifier capable of distinguishing the stances of pairs of\nstatements [203, 204, 205]. However, this would require inputting pairs of sentences into\nthe model at each point of pairwise comparison, with a subpar complexity in the order of\n/parenleftbign\n2/parenrightbigtimes for pairwise comparison of nstatements.\nWe address existing limitations by empowering sentence transformers, a computation-\nally efficient method, with stance awareness. We extract and compose a rich dataset of\nsupporting and opposing statements on controversial topics to fine-tune these models. Our\nobjective is to lessen cosine similarities for statements representing opposing stances and\nincrease similarities for congruent viewpoints. We perform this by fine-tuning a state-\nof-the-art sentence transformer with Siamese and Triplet networks using a contrastive\nand triplet loss function on top of the networks. These loss functions penalize the model\nfor providing spatially close embeddings for contradictory, yet topically similar, pairs\n(triplets) of text.\nIn summary, our work makes the following contributions:\n1) Stance Awareness . We add stance awareness ( \u00a79.3) over topic-aware ( \u00a79.4)\nsentence transformers and verify its utility in opinion-mining tasks ( \u00a79.5).\n2) Computational Efficiency . Classification-based stance-detection methods, re-\nquire calling the model in the order of/parenleftbign\n2/parenrightbigtimes for pairwise comparison of nsentences.\nWe reduce this requirement to only ntimes (\u00a79.5).\n3) Experimental Insights . We gain several generalizable experimental insights\n(\u00a79.5), including: i) Our novel data-quality filtering preprocessing step is useful for en-\nhancing the model\u2019s quality and reducing the training workload. ii) The optimal value\nformargin hyperparameters are moderate values. iii) Parameter Efficient Fine-Tuning\nminimizes the catastrophic forgetting, that in context, minimizes the fine-tuned model to\nforget the initial task of detecting topic relevance .\n9.2. Motivation & Related Work\nThe main objective of this work is to enhance opinion mining and stance detection\ntasks. Thus, in this section, we motivate our work by examining the limitations of prior\n9.2 Motivation & Related Work 143\nwork.\nMotivation: Stance detection is a vital task in social computing, aiming to identify an\nauthor\u2019s viewpoint (e.g., in favor, against, neutral) towards a specific topic [206]. Existing\nmethods leverage state-of-the-art NLP architectures, such as BERT [75], to classify the\nsemantic relationship between a target sentence and a context sentence expressing a known\nstance [76].\nMoreover, recent advancements in LLMs, have demonstrated significant potential in\nperforming various NLP tasks, including stance detection, in a zero-shot setting without\nthe need for fine-tuning [67].\nHowever, both the supervised classification-based and the LLM-based approaches come\nwith a significant computational cost . Since they involve feeding both the target sen-\ntence and the context sentence into the model simultaneously, for npieces of text, they\nrequire calling the model/parenleftbign\n2/parenrightbigtimes. This can be particularly problematic when deal-\ning with large datasets or real-time analyses, such as analyzing stances in social media\nstreams containing millions of posts. For instance, feeding dot-separated pairs of sentences\nto BERT-Base to predict their relationship [75] (e.g., predicting similarity, predicting\nstance), would take an average inference time of 32ms per sentence pair on NVIDIA Tesla\nV100 GPU [207]. Comparing the stances of all the sentence pairs for 1,000 sentences will\ntake 4.5hours\u224832ms\u00d7/parenleftbig1000\n2/parenrightbig.\nRise of Sentence Transformers: To address this problem of enormous computational\nworkload for sentence similarity tasks , sentence transformers were introduced [208]. By\nfine-tuning BERT with Siamese networks, Reimers et al. proposed a way to generate se-\nmantically meaningful sentence embeddings that are spatially close for semantically sim-\nilar sentences. These pre-generated embeddings removed the need for calling the models\nfor every pairwise comparison, reducing the complexity to only ntimes for mapping the\nembeddings of nsentences; totaling: 32 ms\u00d7n.\nThen, the similarity of every sentence pair is obtained by a swift calculation of the\nspatial distance of their pre-generated embeddings (approx 0.5ms per vector pair distance\ncalculation). Thus, comparing all pairwise combinations for 1,000 sentences in terms of\nsimilarity would only take 4 .5minutes\u224832ms\u00d71000 + 0.5ms\u00d7/parenleftbig1000\n2/parenrightbig.\nNeed for Stance-Aware Sentence Transformers: The sentence transformers can\nsolve the problem of computational inefficiency in sentence similarity measurement. Yet,\nif the task would be to compare the stances of sentence pairs on similar topics, current\nsentence transformers would perform far below ideal as they often confuse topic-wise sim-\nilarity with stance-wise similarity; a limitation that has also been highlighted by previous\nwork [201]. This often results in assigning high similarity scores to statements that ex-\npress opposing positions on the same topic. For example, \u201c I love pineapple on pizza \u201d\nand \u201c I hate pineapple on pizza \u201d, two opposing stances on pizza, will be assigned a high\n144 Stance-Aware Sentence Transformers for Opinion Mining\nsimilarity score as they are both talking about a taste towards the same food.\nAnother significant limitation of sentence transformers and similar models is their\npoor handling of negations and antonyms, as shown by recent research. [209] demon-\nstrate that sentence embeddings often fail to capture meaning-preserving transformations\nwhen one sentence includes a negated antonym of the other, such as \u201c I am not guilty \u201d and\n\u201cI am innocent . \u201d This deficiency further exacerbates the challenge of stance detection,\nwhere subtle shifts in meaning can completely reverse the stance. Developing the ability\nto fine-tune sentence transformers for spatial dissimilarity in opposing viewpoints has the\npotential to significantly advance online opinion mining and stance detection. Take, as\na running example, a case where we want to figure out the stances of several politicians\nonabortion rights using their Twitter timelines. A solution aided by sentence transform-\ners, as we demonstrate in \u00a79.5.4, can query anti- and pro-abortion statements such as\n\u201cabortion is murder \u201d and \u201c abortion is healthcare . \u201d Then, after embedding both queries\nand timelines into vectors using sentence transformers, we can systematically infer tweets\nwith high spatial similarity to the pro (anti) abortion query and their stance. Another\nhuge computational advantage of this approach is that the embeddings generated for the\ntimelines can be saved and used for other queries in the future. For example, we can\nquickly generate a pair of queries representing pro- and anti-gun-carrying rights and run\nthem on the same timelines that are already vectorized to mine the users\u2019 opinions on\ngun control.\nIdeal Stance Detection Method: Based on the considerations above, in summary,\nan ideal stance detection method should satisfy three major requirements: R1) Compu-\ntational Efficiency which is not addressed in classification-based methods, but it is in\nsentence transformers; R2) Stance Awareness, which is not addressed in sentence trans-\nformers yet, but can revolutionize stance detection methods if the following challenge was\nto be addressed properly; R3) Maintaining Topic Awareness: Crucially, when empowering\nsentence transformers with stance awareness, an important challenge would be to avoid\ncatastrophic forgetting . This means that sentence transformers primarily pretrained to de-\ntect topically relevant texts should retain this primary functionality after being fine-tuned\nfor stance awareness.\n9.3. Methodology\nIn this section, we elaborate on the fine-tuning architecture and our experimental\nsettings for strategizing the fine-tuning process. Figure 9.1 summarizes the entire pipeline\nof our approach, including fine-tuning ( \u00a79.3), data-preparation ( \u00a79.4), and the semantic-\nsearch application ( \u00a79.5).\n9.3 Methodology 145\nFigure 9.1: Our methodological pipeline and its application process.\n9.3.1. Argument base: Anchor, Positive and Negative statements\nThe fine-tuning architecture for adding stance awareness requires pairs and triplets\nof statements with labels regarding their argumentative stance toward each other. Pairs\nare topically relevant statements that either Agree (Ag) or Oppose (Op) with each other\nwhereas, every triplet, in the context of this task, is composed of an Anchor (An) which\nis an initial claim (parent claim), a Pro (P) argument that supports the parent claim, and\na Con (C) argument that disagrees with the parent claim. We give grounded examples of\nsuch statements in our dataset ( \u00a79.4.1).\n9.3.2. Architecture: Siamese and Triplet Model\nOur approach leverages Siamese and Triplet network architectures, which are the un-\nderlying methods used to train sentence transformers. In this section, we briefly introduce\nboth methods in the context of fine-tuning argumentative statements.\nWe initially introduce the main idea behind Siamese and Triplet architectures and\ndetail their formulations in Section 9.3.3.\n9.3.3. Siamese and Triplet Networks\nSiamese Network with Contrastive Loss: A Siamese network [45] is a neural network\nconsisting of two identical subnetworks, termed \u201c twins \u201d, that share the same architecture\nand parameters. The Siamese network is specifically designed for tasks that involve\ncomparing and contrasting pairs of input data.\nIn our case, the Siamese network takes pairs of arguments (supporting or contradic-\ntory) independently and computes their corresponding embeddings. These embeddings\nencapsulate the essential information of the arguments. Then, we use the contrastive\nloss function as in Eq. 9.3.3 to fine-tune the model such that produces close (distant)\n146 Stance-Aware Sentence Transformers for Opinion Mining\nembeddings for aligning (contradictory) arguments.\nContrastive Loss = yi\u00d7D(E1\ni,E2\ni) + (1\u2212yi)\u00d7max(margin\u2212D(E1\ni,E2\ni),0)\nE1\niandE2\niare embeddings, i.e.: the outputs of the model which denote the projection\nof statement pairs into the embedding space. D(E1\ni,E2\ni) is a distance metric, often the\nEuclidean or cosine distance, which measures the dissimilarity between the two embed-\ndings. Smaller D(E1\ni,E2\ni) indicates greater similarity. Next, margin is a hyperparameter\nthat defines the separation margin. If the distance between similar samples D(E1\ni,E2\ni) for\nthe opposing statements ( yi= 0) is smaller than the margin , the loss function incurs a\npenalty. On the other hand, where E1\niandE2\niagree with each other ( yi= 1), the spatial\ndistance between E1\niandE2\niincurs penalty in loss function.\nTriplet Network with Triplet Loss: The Triplet network [46] extends the idea of\nshared parameterization so that the model focuses on the relationships among triplets of\ninputs, adding more context to the samples. Our architecture uses argument-base state-\nments as defined in \u00a79.3.1 to form triplets. Triplet loss on top of the Triplet architecture is\ndesigned to enforce a specific learning objective: the model is trained to minimize the dis-\ntance between the anchor (parent claim) and the positive example (Pro argument) while\nmaximizing the distance between the anchor and the negative example (Con argument).\nThis is formulated in Eq. 9.3.3:\nTriplet Loss =N/summationdisplay\ni=1max(D(EAn\ni,EP\ni)\u2212D(EAn\ni,EC\ni) + margin,0)\nwhereEa\ni,Ep\ni, andEc\ni, denote the embeddings of the parent claim (anchor), supporting\nargument (pro), and opposing argument (con).\nHybrid: In our work, we also test the Siamese and the Triplet networks together, which\nwe call Hybrid throughout this chapter. We arrange this by fine-tuning the model with\nthe Triplet network for half of the epochs and then fine-tuning with the Siamese network\non top of it for the other half of the epochs. Our hypothesis is that this setting can com-\nbine the contextualization strengths of triplets while maintaining the direct comparison\nbetween data pairs from the Siamese network.\n9.3.4. Fine-tuning Strategy\nWe next describe the strategy we use to optimize our fine-tuning task,\ndetailing how we iterate over different values of key hyperparameters and experimental\nsettings. For our base model, we use a light-weight (420MB) state-of-the-art1pretrained\nsentence transformer model \u201c all-mpnet-base-v2 \u201d2that is widely used in previous compu-\n1www.sbert.net/docs/pretrained_models.html\n2huggingface.co/sentencetransformers/all-mpnet-base-v2\n9.3 Methodology 147\ntational social science literature.\nThis model contains a total of 111,845,760 parameters. To optimize the training\nefficiency, we employed Low-Rank Adaptation (LoRA), which allowed us to significantly\nreduce the number of trainable parameters to 2,359,296, representing only 2.11% of the\ntotal parameters.\nThe training was conducted over 4 epochs. For Siamese networks, each epoch required\napproximately 2 hours, whereas for Triplet networks, each epoch took around 1 hour. This\ndifference in training time is attributed to the distinct architectural and computational\nrequirements of Siamese and triplet networks.\nThe computational resources used for training included NVIDIA A100 80GB PCIe\nGPUs. The coding was done in Python using PyTorch and PEFT libraries.\nThere are also newer generations of heavy-weight LLM-based text embedders available\nonline, yet, since this chapter is oriented toward demonstrating the feasibility of obtaining\na stance-aware sentence transformer, a light-weight sentence transformer with competitive\nperformance would suffice for answering our main research question. In any case, we also\nshow in\u00a79.5.1 that LLM-based text embedders would face the same issues.\nMargin: A largermargin , both in contrastive and triplet loss, enforces a greater separa-\ntion between contrasting stances, potentially enhancing stance discrimination but risking\nover-separation where nuanced differences are overlooked. Our experimentation involves\nfinding the optimal margin that balances precision and recall in the training. We tune\nthis hyperparameter with a grid search over the range (0.1, 1, step = 0.1).\nData Quality Filtering: This step aims at filtering noisy and low-quality inputs to the\nmodel from opposing statements. Take for instance the following two statements extracted\nfrom two posts with opposing views around abortion: 1) \u201c Abortion is murder \u201d (A) and\n\u201cI disagree \u201d (B). In the absence of comprehensive context and background information,\nthese two sentences alone may not represent genuine opposing stances. Sentence B is not\nparticularly an anti-abortion statement in its nature unless one is aware of the context\nin which it has been used. Yet, we are training the model to be used for converting\nshort phrases into vectors independent of their context. Hence, compelling the model to\nrepresent statements A and B as contrasting statements could introduce noise and hinder\noverall model performance.\nThe data quality filtering step that we introduce, seeks to address this concern by\nprioritizing relevant and contextually meaningful instances during training. We initially\nemploy the \u201c all-mpnet-base-v2 \u201d model to compute the cosine similarity between instances\n(pro-con pairs) in the training set and filter out statements that are lower than a threshold.\nFor triplet networks, we filter out instances where the lowest pair-wise cosine similarity\nbetween all three sentences is lower than the threshold. We experimentally try different\nthresholds and retain 50% and 30% for contrastive and triplet networks respectively based\non the major gaps in the frequency histogram of the training data.\n148 Stance-Aware Sentence Transformers for Opinion Mining\nParameter Efficient Fine-Tuning with LoRA: We employ Low-Rank Adaptation\n(LoRA) [49] which is designed for computationally efficient fine-tuning of large language\nmodels, while also mitigating the risk of catastrophic forgetting. Traditional fine-tuning\ncan be computationally expensive, especially during hyperparameter experimentation.\nLoRA addresses this challenge by introducing trainable adapter modules into specific\nlayers, allowing targeted adjustments to the pre-trained model without modifying all\nthe weights. We specifically target attention layers with a rank of 32 [210], reducing\ncomputational costs compared to full tuning.\nTo reduce the training workload, we only apply our iterative grid-search over other\nexperimental settings with LoRA and select the best experimental setting for a round of\nfull training as well.\n9.4. Datasets\n9.4.1. Training Data: Kialo\nWe use the Kialo platform ( www.kialo.com ) to create pairs and triplets of agreeing and\nopposing arguments on certain topics which are the essential inputs of the Siamese and\nTriplet networks (cf. \u00a79.3.2). Kialo is an online debate platform where users create and\ndiscuss controversial topics. Each debate on Kialo is formatted in a tree structure, where\nthe root/parent node is the main topic (initial thesis) of the debate and the branch/child\nnodes are the arguments that support or oppose the main topic. Furthermore, each of the\nbranch/child arguments can turn into parent/root arguments to subsequent branch/child\narguments supporting or opposing them. Figure 9.2 shows a sample Kialo discussion on\n\u201cwhether Ukraine should surrender to Russia or not . \u201d\nFigure 9.2: Sample discussion on Kialo website.\nThe raw tree-formatted data of Kialo was collected by [4]. This dataset contains a\ncollection of discussion trees for a variety of controversial topics such as \u201c Should animal\n9.4 Datasets 149\ntesting be banned? \u201d, \u201cShould the government provide free healthcare? \u201d, \u201cShould the death\npenalty be abolished? \u201d, etc. The dataset has 5,631 discussions with 430,034 arguments in\ntotal and a balanced proportion of supporting arguments and counter-arguments.\nWe make a 9:1 train-test split of the discussions. Table 9.3 reports the number of\ngenerated pair and triplet samples.\n9.4.2. Generating Training Pairs and Triplets\nTo form pairs for the Siamese Networks (see \u00a79.3.2), we choose to use a combination\nof child-to-parent and child-to-child pairs of arguments from the Kialo dataset. Child-to-\nparent pairs are pairs consisting of a child\u2019s argument versus its parent\u2019s argument with\nwhich it is agreeing or disagreeing. Child-to-child pairs are pairs where both arguments\nare children of a unique parent argument with which they agree or disagree. Table 9.1\nillustrates samples of child-to-child and child-to-parent pair generation from the example\ndiscussion in Figure 9.2; i.e., two cons of a unique parent will also be labeled\nasAgreeing to each other when paired together. After forming all the possible\nsentence pairs, we obtain 420,838 child-to-parent pairs and 713,725 child-to-child pairs, a\ntotal of 1,134,663 argument pairs.\nChild-to-Parent Sample Pairs Child-to-Child Sample Pairs\n(Saving lives is more important than\npolitics ,Ukraine shall surrender to\nsave lives )Pair Label = Agreeing(Saving lives is more important\nthan politics ,Surrendering to Rus-\nsia costs more lives long-term )Pair\nLabel = Opposing\nTable 9.1: Example of argument pair creation.\nFor the Triplet networks, our samples are composed of triplets of statements. Each\ntriplet consists of an anchor statement (parent claim), a supporting statement (a child\n\u201cpro\u201d argument) that agrees with the anchor, and an opposing statement (a child \u201ccon\u201d\nargument) that disagrees with the anchor. We derive the triplet samples by iterating over\nevery parent claim and sampling every possible pairwise combination of its pro and con\nchild arguments. Table 9.2 shows a sample triplet from the Kialo discussion depicted in\nFigure 9.2.\nAnchor Pro Con\nUkraine should surrender in\norder to save livesSaving lives is more important\nthan politicsSurrendering to Russia would\ncost more lives long-term\nTable 9.2: Example of triplet creation.\nNote that our split is based on the entire discussion trees, not the individual arguments,\ni.e.: the sampled pairs or triplets in the test set do not originate from the same discussion\nas in the training set. This ensures the test set assesses the performance in challenging\n150 Stance-Aware Sentence Transformers for Opinion Mining\nscenarios where the supporting or contradicting pairs of arguments are from topics not\nseen by the model before.\nData Train (90%) Test (10%)\nDiscussion Topics 4430 493\nGenerated Pairs 972395 112724\nGenerated Triplets 303081 34453\nTable 9.3: Kialo dataset\u2019s size.\n9.4.3. Baseline Data: STS-B\nAs with every other fine-tuning, our task is also subject to the risk of catastrophic\nforgetting which refers to the cases where after fine-tuning, as a result of over-training\non the newer task, the model forgets its ability to perform the older task it was initially\ntrained to do [50]. In this context, the primary task of sentence transformers was to\ndetect semantic similarity (regardless of stance). Thus, we need a separate validation on\na dataset annotated for semantic similarity to assess how far fine-tuning the models for\nstance-awareness, would forget this primary task.\nThe Semantic Textual Similarity Baseline (STS-B) dataset is a widely recognized\nbenchmark designed to assess the ability to compute semantic similarities between pairs\nof sentences. It comprises pairs of sentences with similarity scores ranging from 0 (no\nsemantic overlap) to 5 (semantic equivalence). We only use the test set which consists\nof 1,379 pairs. These pairs span over diverse topics, including news headlines, forum\ndiscussions, and product reviews.\n9.4.4. Out of Distribution Data: SemEval-2014\nAs our out-of-distribution test data, we look into the \u201c SemEval-2014: Task 1 \u201d dataset,\na widely used contradiction detection dataset that does not overlap with Kialo. The\ndataset contains a variety of sentence pairs annotated as Neutral (5611), Entailment\n(2857), and Contradiction (1459). The Entailment and Contradiction pairs are relevant\ntopic-wise but are aligned or contradictory stance-wise, yet the Neutral pairs can either\nbe topically relevant or be totally irrelevant statements.\n9.4.5. Application Data\nFinally, to demonstrate the applicability of our model to semantic search of contro-\nversial statements, which is one of the main motivations for our work, we use a publicly\navailable dataset of tweets from congresspeople.3The dataset contains the timeline of\n564 congresspeople ( Democrats : 292, Republicans : 270, Independent : 2). In total 2.3M\n3https://github.com/alexlitel/congresstweets/tree/master\n9.5 Experiments, Results, & Observations 151\ntweets ( Democrats : 1.4M, Republicans : 840K, Independent : 9K) of the congresspeople\nare collected.\n9.5. Experiments, Results, & Observations\nWe next describe our experiments and results after applying our method to fine-tune\nthe sentence transformer. We first test the performance of all the fine-tuned models on a\ntest set from the Kialo and STS-B datasets ( \u00a79.5.1 and \u00a79.5.2). Using the best-performing\nmodel, we evaluate how the learning transfers to another dataset ( \u00a79.5.3). Finally, we\nshowcase its application on semantic search for opinion mining ( \u00a79.5.4).\n9.5.1. Validation on Kialo\nAs the first step of the validation, we create frequency plots of cosine similarities over\nthe 10% test-set of the Kialo dataset. Figure 9.4a reveals that the original model struggles\nto distinguish stances, as the pro (green) and con (red) distribution curves align closely.\nThe green and red frequency distribution curves represent the cosine similarities between\npro and con statement pairs. The alignment of the curves shows that the original model\ndoes not effectively differentiate between pro and con statement pairs. Also, Figure 9.3\nshows the poor performance of NV-Embed-v1 , the current best LLM-based (29GB) text\nembedder,4in differentiating between opposing vs. supporting statements in terms of\nspatial distance.\nFigure 9.3: Performance of NV-Embed-v1 on Kialo Test-Set.\nOn the other hand, Figure 9.4b shows the same curves for one of our best (settings:\nHybrid ,margin =0.4,LoRA ) fine-tuned versions of the model. We see a notable shift in\nthe distribution of pro statements (green) to the right side and a corresponding shift in\nthe distribution of con statements (red) to the left side.\n4https://huggingface.co/spaces/mteb/leaderboard\n152 Stance-Aware Sentence Transformers for Opinion Mining\nMargin\nModel Type Filtering LoRA 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nSiamese None yes 0.03 0.21 0.41 0.37 0.37 0.34 0.34 0.36 0.35 0.36\nSiamese <50% yes 0.01 0.24 0.38 0.44 0.34 0.31 0.38 0.37 0.37 0.38\nTriplet None yes 0.31 0.36 0.39 0.40 0.39 0.40 0.37 0.36 0.35 0.33\nTriplet <30% yes 0.26 0.37 0.42 0.42 0.41 0.39 0.38 0.36 0.36 0.34\nHybrid <30% &<50% yes 0.23 0.35 0.44 0.44 0.44 0.45 0.41 0.39 0.38 0.36\nHybrid <30% &<50% no 0.66 0.72 0.67 0.71 0.69 0.63 0.66 0.62 0.61 0.59\nOriginal \u201call-mpnet-base-v2\u201d 0.004\nTable 9.4: KL Divergence Between Agreeing and Opposing statements\u2019 distributions in\nKialo Test Set.\nObservation: This significant shift indicates that our fine-tuned model has be-\ncome stance aware, effectively separating pro and con statements even on previ-\nously unseen topics, partly fulfilling requirement R2 as in \u00a79.2.\n(a) Original Model.\n (b) Fine-Tuned Model.\nFigure 9.4: Comparison of Model Distributions.\nTo quantify the performance of this separation we calculate the KL-Divergence be-\ntween the cosine similarity distributions of Opposing pairs and cosine similarity distribu-\ntions of Agreeing pairs. A higher amount of KL-Divergence translates into a desirable\nhigher separation between Agreeing and Opposing statements by the model. Table 9.4\nreports results for different combinations of the experimental settings. The data quality\nfiltering threshold is set to None , below 50% for pairs in the Siamese network, and below\n30% for minimum pairwise similarity in any pairs of a triplet in the Triplet network. Re-\ncall that we apply LoRA to all models and we experiment with further fine-tuning over\nthe best-performing configuration (the last Hybrid row in this case). Finally, the margin\nhyperparameter is iterated over in steps of 0.1 to obtain the best combination.\n9.5 Experiments, Results, & Observations 153\nMargin\nModel Type Filtering LoRA 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nSiamese None yes 0.73 0.77 0.78 0.79 0.81 0.82 0.82 0.82 0.81 0.79\nSiamese <50% yes 0.77 0.79 0.79 0.80 0.82 0.83 0.82 0.81 0.80 0.79\nTriplet None yes 0.83 0.83 0.82 0.81 0.80 0.79 0.78 0.76 0.73 0.71\nTriplet <30% yes 0.83 0.83 0.82 0.81 0.81 0.80 0.78 0.77 0.75 0.73\nHybrid <30% &<50% yes 0.83 0.83 0.81 0.80 0.79 0.78 0.77 0.76 0.74 0.72\nHybrid <30% &<50% no 0.72 0.71 0.68 0.63 0.59 0.53 0.51 0.49 0.47 0.45\nOriginal \u201call-mpnet-base-v2\u201d 0.83\nTable 9.5: Performance of models on STS-B test set (Spearman correlation).\nObservation: Our fine-tuning approach yielded significant performance leap,\nwith all fine-tuned models outperforming the original model by a substantial gap.\nHybrid narrowly wins among LoRA models while the fully fine-tuned model out-\nperforms all. LoRA being an efficient transformer, significantly contributes to-\nwards requirement R1.\n9.5.2. Sentence Similarity Baseline\nNext to the model\u2019s performance on the task for which the model had been trained\n(primary task ), we assess the amount of catastrophic forgetting introduced when fine-\ntuning. Table 9.5 reports the models\u2019 performance on the STS-B dataset, the primary\ntask. For this, we use the Spearman correlation between two cosine similarities: 1) over\nsentence pairs provided by the model (predicted values), and 2) over pairs annotated\nby humans (true values ranging [0, 5]). Higher cosine similarity values indicate better\nmodel performance in capturing semantic similarity between sentence pairs, a proxy for\nlow catastrophic forgetting.\nWe see that the performance of the base model has a strong correlation of 0.83, which\nmeans that it performs well with the primary task. While, as expected, none of the\nfine-tuned models outperforms the base model in the primary task, we see comparative\nperformances (also at 0.83) of some LoRA fine-tuned models, especially for lower margin s\nin the range [0.1, 0.4]. However, the base model shows a very poor performance in the\nnew task (0.004 divergence, as shown in the previous section). Conversely, the fully fine-\ntuned model (LoRA = no) shows subpar performance in the primary task. This is because\ncatastrophic forgetting is higher in fully fine-tuned models, as expected when dealing with\nparameter-efficient fine-tuning as identified by prior work (cf. \u00a79.3.4).\nWhile fine-tuning creates a tension between the objective of the primary and the\nnew task , our LoRA models significantly reduce this tension by eliminating catastrophic\nforgetting, unlike the base model, while maintaining comparable results when compared\nto the base model in the primary task. This demonstrates the model\u2019s robustness in\nadapting to a new task while retaining previously learned knowledge, satisfying R3.\n154 Stance-Aware Sentence Transformers for Opinion Mining\nFor selecting the best model and parameters, we consider the trade-off between its\nperformance on the two tasks (new vs. the primary task) as discussed above. As men-\ntioned, Table 9.4 represents the stance-aware results, i.e.: the new task, where the best\nmargins here are in the range [0.4, 0.7]. Instead, in the primary task, lower margins in the\nrange [0.1, 0.4] cause the least catastrophic forgetting (as we observe in Table 9.5). Thus,\nwe select 0.4, where the two ranges meet, and the LoRA fine-tuned version of Hybrid in\nwhat follows.\n9.5.3. Out of Distribution Validation\n(a) Original KL-Div. for green\nvs. red = 3.5.\n(b) LoRA fine-tuned.\nKL-Div. = 5.7.\n(c) Fully fine-tuned\nKL-Div. = 5.4\nFigure 9.5: Distributions of cosine similarities of pairs in SemEval 2014 dataset.\nQuery Original Hybrid,\nmargin =\n0.4Siamese,\nmargin =\n0.4Triplet, mar-\ngin = 0.4\n\u201cAbortion is healthcare. \u201d 76% 91% 84% 94%\n\u201cAbortion is murder. \u201d 67% 80% 64% 79%\nTable 9.6: Alignment Precision for semantic search on congresspeople tweets with\nabortion-related queries.\nFigure 9.5 depicts the distributions of cosine similarities provided by the original and\nthe fine-tuned models (LoRA and fully fine-tuned) for the three categories of pairwise\nrelationships in the dataset: Neutral ,Entailment , and Contradiction . Ideally, in the fine-\ntuned model, we would desire to witness: 1) a further shift for the contradictory pairs\u2019\ndistribution (red curve) to the left side, 2) while the distribution of the entailing pairs\n(green curve) peaking near the right side, and 3) Neutral pairs (blue curve) maintaining a\nrelatively more uniform distribution across the x-axis as it includes both topically relevant\n(majority) and irrelevant (minority) pairs of statements. Moreover, we expect the peak\nof the Neutral pairs\u2019 curve to stand in between the former two so that when it comes\nto sentence pair similarity, our fine-tuned model preserves the ascending order of: 1)\ntopically relevant but contradictory, 2) topically relevant but neutral, and 3) topically\nrelevant and entailing.\n9.5 Experiments, Results, & Observations 155\nAcross Figures 9.5a, 9.5b, and 9.5c, we observe a progression in stance detection\nabilities. Initially, the original all-mpnet-base-v2 model can also distinguish Entailment\nfrom Contradiction (Fig. 9.5a), suggesting that the contradictions in this dataset are\nless subtle than in the Kialo test set (Figure 9.4a). Yet, our LoRA fine-tuned model\nsignificantly improves differentiation, correctly shifting Contradiction pairs leftwards, and\nmaintaining an appropriate balance between Neutral and Entailment pairs \u2014 desirably\nforcing the topically relevant Neutrals peak to stand between the peaks of Contradiction\nand Entailment curves (Fig. 9.5b). However, full fine-tuning (Fig. 9.5c) manifests its\ncatastrophic forgetting \u2014 while the gap between the distributions of Contradiction and\nEntailment is also enhanced when compared to the original model, Neutral pairs are\nundesirably shifted towards the Entailment . This highlights the advantage of the LoRA\nfine-tuned model in achieving both stance-awareness and preserving prior knowledge,\nunderscoring its value in fine-tuning for stance-aware sentence embeddings.\nObservation: Our fine-tuned models exhibit an increase in stance awareness\ncompared to the original model, which possessed some limited understanding of\nstances in a different dataset, i.e.: SemEval-2014 , contributing to R2.\n9.5.4. Application: Semantic Search\nOnce demonstrated the performance of our models, we showcase the practical impli-\ncations of performing stance identification and its potential to enhance social computing\ntasks. A practical use-case of the stance-aware model is retrieving text with certain\nstances in corpora through the use of semantic search.\nWe generate two controversial statements with the exact opposite viewpoints on abor-\ntion: \u201c Abortion is healthcare \u201d and \u201c Abortion is murder . \u201d Then, we query these two\nstatements from the 2.3M tweets of the congresspeople dataset (cf. \u00a79.4.5). As it is typ-\nically done in semantic search with S-BERT, we first convert each tweet and query into\nvectors; separately using the original and the fine-tuned model. We then compute the\ncosine similarity between the query embeddings and tweet embeddings, applying similar-\nity thresholds, suggested by [6], to filter out less relevant tweets. The more aligned the\nstances of the remaining tweets with the query, the better the model preforms in stance\nawareness.\nTable 9.7 shows the results of the alignments, and Table 9.8 offers an excerpt of the top\nmatching results (highest cosims ) with the pro-abortion query. Looking at the summary of\nour results in Table 9.7, we see that when we shift from the original model to the fine-tuned\none, the alignment precision of the model from Twitter rises from 76% to 91% for pro-\nabortion (Democrat) and from 67% to 80% for the anti-abortion (Republican) query. This\nmeans that desirably 91% (80%) of the top similar results for a Democrat (Republican)\n156 Stance-Aware Sentence Transformers for Opinion Mining\nModel Query Affiliation Cosim\nThresh-\noldR D Alignment\nPrecision\nOriginal \u201cAbortion is healthcare. \u201d Democrat 0.70 31\u2717 98\u2713 76%\nFine-Tuned \u201cAbortion is healthcare. \u201d Democrat 0.70 4\u2717 43\u2713 91%\nOriginal \u201cAbortion is murder. \u201d Republican 0.60 95\u2713 46\u2717 67%\nFine-Tuned \u201cAbortion is murder. \u201d Republican 0.60 12\u2713 3\u2717 80%\nTable 9.7: Alignment Precision for semantic search on congresspeople tweets with\nabortion-related queries. D: Democrat alignment, R: Republican alignment.\nText (Query/Tweet) Party Aligned?\nQuery: \u201cAbortion is healthcare.\u2019\u2019 Dem\nOriginal In case anyone forgot \u2013 abortion is NOT healthcare. Rep \u2717\nOriginal Reminder: abortion is health care. Dem \u2713\nOriginal Stop pretending abortion is healthcare... Rep \u2717\nOriginal ... I have to say this once again, but abortion is NOT healthcare. #ProLife Rep \u2717\nOriginal ... A procedure where a successful outcome is the death of a living human is\nnot healthcare.Rep \u2717\nFineTuned Just a reminder: abortion is healthcare. #SOTU Dem \u2713\nFineTuned ... EVERY woman has the constitutional authority to make decisions about\ntheir own body ...Dem \u2713\nFineTuned Reminder: abortion is health care. Dem \u2713\nFineTuned ... Roe v. Wade is the law of the land and we have to ensure it will stay that\nway...Dem \u2713\nFineTuned Reproductive care is health care... Dem \u2713\nTable 9.8: Most similar semantic search results for a pro-abortion query for the Original\nand Fine-Tuned models.\nquery has correctly matched with the tweets of Democrat (Republican) congresspeople.\nThis experiment shows that our method can be utilized to perform robust and efficient\nopinion mining.\nThese results are the demo results for one of our best model settings ( Hybrid archi-\ntecture, margin =0.4,LoRA ).\nDisclaimer: Despite\u00a79.5.1,\u00a79.5.2, and \u00a79.5.3, the main objective of this section was not\ntoevaluate the stance awareness of the fine-tuned model, but to elaborate how such a\nstance-aware language model can be used in practice to improve opinion mining tasks.\nThat\u2019s why we focused on a case study of abortion-related tweets . More experiments can\nbe done around other controversial topics in real-world applications of the model.\n9.6. Discussion\nThis work tackles the critical challenge of balancing three essential requirements in\nNLP tasks: computational efficiency (R1), stance awareness (R2), and maintaining topic\nawareness (R3). We address these challenges by proposing a novel approach that leverages\nfine-tuning while mitigating its drawbacks. We reviewed how prior work fails to meet these\n9.7 Conclusion 157\nthree requirements together in \u00a79.2 and we showed how our work ( \u00a79.3) addresses them\n(\u00a79.5), we next summarize the main findings of this chapter and discuss their implications\nand limitations.\nComputational Efficiency. Our approach makes opinion mining efficient, only needing\nto call the model ntimes for mapping the embeddings of nsentences, that is, linear with\nthe number of sentences. A limitation may arise in how much a single statement used as\na query might encompass all variations of the stance on a certain topic. An important\nconsideration is to maintain sufficient diversity in query selection to account for all parts\nof the spectrum of opinions.\nA balance is feasible. our work demonstrates the feasibility of achieving a balance be-\ntween efficiency, stance awareness, and topic coherence through careful fine-tuning strate-\ngies. This approach can be further explored and adapted for various NLP applications,\nparticularly those requiring robust stance-aware analysis on large datasets.\n9.7. Conclusion\nOverall, our work paves the way for stance-aware sentence transformers, offering a\npowerful tool for social computing tasks like opinion mining.\nOur work demonstrably surpasses the state-of-the-art in stance awareness of sen-\ntence transformers , achieving significant improvements in distinguishing stances across in-\ndistribution (Kialo test-set) and out-of-distribution (SemEval 2014 and Twitter) datasets.\nBy designing an innovative model architecture , we observed a measurable improvement\nof results with the Hybrid (combination of Siamese and Triplet) model. We implemented\nadata filtering approach by removing low cosine similarity pairs, which probed a unique\nexperimental contribution that effectively mitigated the impact of \u201clow-quality\u201d human-\ngenerated data within the training set. This also resulted in an improvement of the model\nperformance, while significantly reducing the train-set size and thus the training time.\nTwo main future steps in this direction can significantly improve the quality of the task:\n1) Improving general-purpose sentence transformers using (LLMs) and extensive datasets,\nsuch as recently developed Open AI\u2019s text embedders;52) Developing dedicated datasets\ntailored to social media platforms like Twitter and Mastodon and fine-tuning the general-\npurpose sentence transformer on such datasets. This will enable the model to learn stance\nawareness in the context of the targeted social networks of analysis. Nevertheless, our\nmodel, which is fine-tuned on Kialo arguments also demonstrated a promising performance\non the Twitter data. This forecasts an even brighter future for models that are specifically\nfine-tuned on online social media data for the same task.\nReproducibility: We open-source both code and models to foster reproducibility.6\n5https://platform.openai.com/docs/guides/embeddings\n6https://github.com/vahidthegreat/StanceAware_SBERT\n158 Stance-Aware Sentence Transformers for Opinion Mining\n9.8. Limitations\nThe main goal of this chapter was to demonstrate the feasibility of obtaining stance\nawareness in sentence transformers. Thus, the language model of analysis in this chapter\nis merely limited to \u201c all-mpnet-base-v2 \u201d, the widely used state-of-the-art sentence trans-\nformer in SBERT leaderboard list7which is light-weight and suitable for the purpose of\nour experiments. Yet, more heavy-weight LLM-based text-embedders are not explored\nin this chapter. We nevertheless, report the stance unawareness of \u201c NV-Embed-v1 \u201d, the\nbest performing Massive Text Embedder in MTEB leaderboard,8in\u00a79.5.1 but do not\napply our fine-tuning experiments as the lighter model we use satisfies our main goal\n(demonstrating the feasibility of obtaining stance awareness) with a significantly lower\ncomputational cost. Yet, for those interested in improving the quality of the model and\nthe task, it is possible to fine-tune any state-of-the-art text embedder by a simple repli-\ncation of our experimental pipeline using the code that we make publicly available (see\nReproducibility above).\nAnother limitation of this chapter is in the scope we demonstrated the application\nof the model in \u00a79.5.4. We only showcased the application of the finetuned model on\nsemantic search over tweets related to abortion . The reason is that the main purpose\nof\u00a79.5.4 was not to validate the model like \u00a79.5.1,\u00a79.5.3, and \u00a79.5.2 but to explain how\nthe model can be used in opinion mining and computational social science tasks. Similar\nexperiments on other controversial topics such as gun-control, war on Ukraine, etc. are\nleft for future works.\n7www.sbert.net/docs/sentence_transformer/pretrained_models.html\n8huggingface.co/spaces/mteb/leaderboard\n10Conclusion\nThis thesis aimed to propose NLP-driven approaches to measuring polarization and\nradicalization that align with the objectives outlined in Chapter 1. Specifically, we fo-\ncused on developing methodologies that are scalable ,generalizable ,holistic or gran-\nular as needed , and feasible in terms of data availability . Throughout three distinct\nparts, we introduced comprehensive frameworks that fulfill these objectives, demonstrat-\ning their efficacy through diverse applications and analyses.\n10.1. Meeting the Objectives\nScalability: This requirement was a key consideration in the development of our method-\nologies. In Chapter 4, we introduced a novel, unsupervised method for quantifying Echo\nChambers using sentence transformers. The use of sentence transformers enables efficient\nanalysis of large-scale social media data, significantly reducing computational overhead\ncompared to traditional methods that utilize heavy graph-based analyses or supervised\nNLP. Similarly, in Chapter 9, we fine-tuned a stance-aware sentence transformer capable\nof rapidly mining users\u2019 opinions on controversial topics across large-scale social media\ndata. Using this approach we decreased the need for calling computationally expen-\nsive models at each instance of inference, contributing significantly to the scalability of\nopinion-mining tasks.\nGeneralizability: Our methodologies are designed to be broadly applicable across var-\nious domains, ensuring their relevance beyond the specific case studies presented. For\ninstance, the Echo Chamber detection framework in Chapter 4 can be applied to diverse\ncontroversial topics, from geopolitical conflicts toclimate change debates . By leveraging\npre-trained language models, our approaches are adaptable to different languages and\ncontexts, enhancing their utility for global research.\nIn Chapter 6, we introduced a model for detecting gender-based polarization, which\ncan be easily adapted to other forms of polarization. For instance, by modifying key\n159\n160 Conclusion\nattribute words from gender-based words (e.g. man vs.woman ) to party-related words\n(e.g. Democrat vs.Republican ), the model will be able to measure the polarization of\ncorpora in terms of political leaning.\nHolistic and Granularity: We offered diverse approaches that provide both holistic and\ngranular perspectives on polarization and radicalization, depending on the objective of\nthe research. In Chapter 6, we combined the Word Embedding Association Test (WEAT)\nwith semi-supervised classification to provide a comprehensive assessment of gender-based\npolarization within online communities. This holistic model captures the overall degree\nof toxicity toward male/female identity on the corpus level. Similarly, the computational\napproach in Chapter 4 provides an overall measurement of the degree of Echo per Chamber\nand polarization across Chambers.\nConversely, in Chapter 5, we conducted a granular analysis of cross-partisan inter-\nactions, examining the content and tone of individual posts. This fine-grained approach\nrevealed nuanced differences in user behavior within and across echo chambers.\nFeasibility and Data Availability: Ensuring the feasibility and ethical integrity of our\nresearch was a priority throughout this thesis. All methodologies were developed using\npublicly available data, focusing on public posts rather than private or inaccessible data\nsuch as follow/friend network of users. This approach not only respects user privacy but\nalso enhances the reproducibility of our approaches and findings.\nThe use of open-source language models further underscores the feasibility of our ap-\nproach. By leveraging widely open-sourced NLP tools and datasets, we demonstrated\nthat high-quality polarization analysis is achievable without proprietary resources or ex-\ntensive computational infrastructure. This accessibility ensures that our methods can be\nadopted and expanded by researchers across different institutions and disciplines.\n10.2. Findings from Applications\nWhile the primary goal of this thesis was to develop novel, scalable, and general-\nizable NLP methodologies for analyzing polarization and radicalization, applying these\nmethods yielded actionable insights into the dynamics of online discourse. These find-\nings contribute to our understanding of sociopolitical behaviors and interactions on social\nmedia platforms.\nDiscourse Diversity Asymmetry: Democratic-leaning users exhibited\ngreater discourse diversity compared to Republican-leaning users. This finding sup-\nports existing research suggesting higher ideological homogeneity on the right, par-\nticularly in digital spaces. Our results add nuance by highlighting how discourse\ndiversity correlates with polarization intensity across different topics.\n10.2 Findings from Applications 161\nCross-Partisan Interactions: Analyzing cross-partisan interactions (Chap-\nter 5) revealed that although Democrats engage more frequently in cross-partisan\ndiscussions than Republicans, they exhibit a greater discriminatory tone when ad-\ndressing out-group members compared to in-group interactions. This suggests that\nmere interaction with diverse perspectives does not necessarily lead to more pro-\nductive or empathetic discourse, underscoring the complexity of fostering genuine\ndialogue across ideological divides.\nGender-Based Polarization: Our analysis in Chapter 6 revealed targeted\ntoxicity patterns within specific communities. For instance, male-dominated forums\nsuch as r/TheRedPill andr/MGTOW exhibited significant hostility toward women.\nInterestingly, r/FemaleDatingStrategy , a women-only forum, displayed toxicity not\nonly toward men but also toward women, indicating internalized gender biases and\ncomplex community dynamics. These findings highlight the multifaceted nature\nof online radicalization, which can manifest both externally and internally within\ncommunities.\nPlatform Differences: A comparative analysis of Reddit and Discord com-\nmunities (Chapter 7) showed that chat-based platforms like Discord are more con-\nducive to the spread of toxic content than post-based platforms like Reddit. This\nsuggests that platform design and interaction modes significantly influence the\nprevalence and intensity of radical content, offering critical insights for platform\nmoderation strategies.\nSociopolitical and Economic Biases of LLMs: Our evaluation of Large\nLanguage Model (LLM)s (Chapter 8) revealed a nuanced bias landscape. While\nthe models demonstrated strong economic moderation, their sociopolitical responses\ntended to favor libertarian perspectives. This indicates that language models are not\nneutral and may reflect or amplify certain ideological biases, which has significant\nimplications for their deployment in sensitive contexts.\nLimitations in Stance Detection: Standard language model embeddings\nwere found to be stance-blind, treating topically similar but stance-opposed state-\nments as equivalent. We addressed this limitation by developing a stance-aware\ntransformer (Chapter 9), which successfully differentiated opposing stances. As ex-\nplained earlier, this tool provides computational social scientists with a powerful\nmechanism for detecting and analyzing polarization in real-time online debates.\nIn summary, this thesis has not only contributed methodologically to the study of\nonline polarization and radicalization, but also attained new sociopolitical findings as\nbyproducts of the application phases.\n162 Conclusion\n10.3. Future Work\nIn this section, we outline several promising directions for future research based on\nthe findings and methodologies developed in this thesis. We will first address the need\nfor enhanced explainability, followed by potential future applications, and conclude with\nopportunities for improving the base tools.\n10.3.1. Toward Explainability\nA significant direction for future work lies in enhancing the explainability of the\nmethodologies developed in this thesis, particularly those involving complex Natural Lan-\nguage Processing (NLP) models. Explainable AI techniques offer promising avenues to\ndecode the opaque semantic features embedded by models such as sentence transformers.\nBy applying methods for disentangling these high-dimensional embeddings, we can iden-\ntify the specific semantic components that drive distinctions between polarized groups,\nproviding clearer insights into the underlying causes of polarization.\nFor example, in Chapter 4, we employed sentence transformers to detect echo chambers\nand quantify polarization. Future research could focus on implementing Explainable AI\nframeworks to interpret the 768-dimensional embeddings produced by these models. By\nmapping each dimension to a semantic feature, such as political stance, linguistic style, or\nemotional tone, researchers could pinpoint the precise factors contributing to polarization\nbetween groups. Techniques like feature attribution or dimensional reduction could aid\nin visualizing and understanding these high-dimensional spaces.\nUnveiling the Semantic Sources of Bias in LLMs: A promising avenue for ex-\nplainability involves understanding the semantic origins of bias within Large Language\nModels (LLMs). This can be achieved by fine-tuning lightweight, open-source models on\ncurated datasets with contrasting semantic attributes, such as political leanings, emo-\ntional tone, and linguistic style. For example, datasets could represent Democratic versus\nRepublican viewpoints, positive versus negative sentiments, or confident versus hesitant\nlanguage. Evaluating these specialized models with benchmarks like Polygloss or ANTAB\ncan help measure the impact of each semantic attribute on the model\u2019s bias.\nThis methodology involves creating multiple LLM variants, each fine-tuned on a spe-\ncific semantically biased dataset. By analyzing their outputs, researchers can identify\nwhich linguistic properties contribute most to biased behavior. Techniques such as feed-\nback loops with more advanced models, as proposed in Chapter 8, can provide additional\nverification. This approach aims to isolate factors, such as political rhetoric or emo-\ntional tone, that can predispose an LLM to generate biased content, offering a deeper\nunderstanding of bias formation.\nThe insights gained from this research would be crucial for developing fairer and more\ntransparent LLMs. Identifying the semantic sources of bias allows targeted interventions\n10.3 Future Work 163\nduring training, reducing harmful outputs. This strategy not only enhances the fairness\nand reliability of LLMs but also strengthens their application in sensitive areas such as\nsocial discourse analysis, contributing to more equitable AI systems.\n10.3.2. Future Applications\nFuture research could extend these findings by applying the methodologies developed\nhere to other domains and datasets, further validating the robustness of these tools across\ndiverse social and political contexts. The analysis of Chapter 4 can be extended to other\ncontroversial topics (e.g., Israeli-Palestinian conflict) and to other network-based social\nmedia platforms (e.g., Mastodon, Gab).\nThe corpus polarization detector in Chapter 6 can be extended to the detection of\nother aspects of polarization on the corpus level. For instance, by swapping our attribute\nwords with those related to Democrats and Republicans and adjusting the Embedded-\nToxicity parameter to Embedded-Polarity, it becomes possible to effectively measure the\npolarization of sentiments toward the Democratic and Republican parties across various\ntimelines.\nThe stance-aware sentence transformer developed in Chapter 9 allows for future large-\nscale stance detection and opinion mining on social media. For instance, tracking public\nopinions on controversial topics such as the US election, crises in West Asia, or developing\nnarratives about immigrants could yield valuable insights. The tracked statements can\nthen be passed to LLMs for a more in-depth analysis of their content. Moreover, Retrieval\nAugmented Generation (RAG) applications can benefit from the tool by enhancing the\nsearch engine for highly opinionated prompts.\n10.3.3. Enhancement of Base Tools\nThe continuing evolution of language models presents opportunities for more fine-\ngrained analysis of bias, radicalization, and polarization, particularly as models become\nmore interpretable and capable of handling increasingly complex tasks.\nThroughout this thesis, we utilized light-weighted and open-source language models\n(\u201call-mpnet-base-v2 \u201d and \u201c Mistral-7B-Instruct-v0.2 \u201d) that provide an acceptable balance\nbetween the quality needed for our social computing task and the computational price that\nour GPU could handle. Future researchers or big-tech companies that are less constrained\nby such limitations can utilize stronger models for broader applications.\nFor example, the LLM-aided content analysis introduced in Chapter 5 can be en-\nhanced by utilizing Meta\u2019s Llama 3.1 405B model for higher quality results. The sentence\ntransformer models in Chapters 4, 8, and 9 could also be replaced by state-of-the-art\nLLM-based text embedders such as \u201c NV-Embed-v1 \u201d, the top-performing Massive Text\n164 Conclusion\nEmbedder on the MTEB leaderboard,1to achieve more refined and accurate outcomes.\nReproducibility: To foster reproducibility of the results and the approaches, all of the\ncodes and software are open-sourced in their corresponding repositories on GitHub.2\n1https://huggingface.co/spaces/mteb/leaderboard\n2https://github.com/vahidthegreat\nReferences\n[1] V. Ghafouri, J. Such, and G. Suarez-Tangil, \u201cI love pineapple on pizza != I hate\npineapple on pizza: Stance-aware sentence transformers for opinion mining,\u201d in\nProceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing , Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida,\nUSA: Association for Computational Linguistics, Nov. 2024, pp. 21 046\u201321 058.\n[Online]. Available: https://aclanthology.org/2024.emnlp-main.1171\n[2] V. Ghafouri, F. Alatawi, M. Karami, J. Such, and G. Suarez-Tangil, \u201cTransformer-\nbased quantification of the echo chamber effect in online communities,\u201d Proc.\nACM Hum.-Comput. Interact. , vol. 8, no. CSCW2, Nov. 2024. [Online]. Available:\nhttps://doi.org/10.1145/3687006\n[3] A. K. Singh, V. Ghafouri, J. Such, and G. Suarez-Tangil, \u201cDifferences in the toxic\nlanguage of cross-platform communities,\u201d Proceedings of the International AAAI\nConference on Web and Social Media , vol. 18, no. 1, pp. 1463\u20131476, May 2024.\n[Online]. Available: https://ojs.aaai.org/index.php/ICWSM/article/view/31402\n[4] V. Ghafouri, V. Agarwal, Y. Zhang, N. Sastry, J. Such, and G. Suarez-Tangil,\n\u201cAi in the gray: Exploring moderation policies in dialogic large language models\nvs. human answers in controversial topics,\u201d in Proceedings of the 32nd ACM\nInternational Conference on Information and Knowledge Management , ser. CIKM\n\u201923, 2023, p. 556\u2013565. [Online]. Available: https://doi.org/10.1145/3583780.3614777\n[5] V. Ghafouri, J. Such, and G. Suarez-Tangil, \u201cA holistic indicator of\npolarization to measure online sexism,\u201d 2024. [Online]. Available: https:\n//arxiv.org/abs/2404.02205\n[6] W. Iqbal, V. Ghafouri, G. Tyson, G. Suarez-Tangil, and I. Castro, \u201cLady and\nthe tramp nextdoor: Online manifestations of real-world inequalities in the\nnextdoor social network,\u201d Proceedings of the International AAAI Conference on\nWeb and Social Media , vol. 17, no. 1, pp. 399\u2013410, Jun. 2023. [Online]. Available:\nhttps://ojs.aaai.org/index.php/ICWSM/article/view/22155\n165\n166 REFERENCES\n[7] T. Mikolov, Q. V. Le, and I. Sutskever, \u201cExploiting similarities among languages for\nmachine translation,\u201d 2013. [Online]. Available: https://arxiv.org/abs/1309.4168\n[8] J. Zhang, W. Wang, S. Guo, L. Wang, F. Lin, C. Yang, and W. Yin, \u201cSolving\ngeneral natural-language-description optimization problems with large language\nmodels,\u201d in Proceedings of the 2024 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies\n(Volume 6: Industry Track) , Y. Yang, A. Davani, A. Sil, and A. Kumar, Eds.\nMexico City, Mexico: Association for Computational Linguistics, Jun. 2024, pp.\n483\u2013490. [Online]. Available: https://aclanthology.org/2024.naacl-industry.42\n[9] B. C. Semaan, S. P. Robertson, S. K. Douglas, and M. Maruyama, \u201cSocial media\nsupporting political deliberation across multiple public spheres: towards depolariza-\ntion,\u201d Proceedings of the 17th ACM conference on Computer supported cooperative\nwork and social computing , 2014.\n[10] M. Saveski, D. Beeferman, D. McClure, and D. Roy, \u201cEngaging politically diverse\naudiences on social media,\u201d Proceedings of the International AAAI Conference on\nWeb and Social Media , vol. 16, no. 1, pp. 873\u2013884, May 2022. [Online]. Available:\nhttps://ojs.aaai.org/index.php/ICWSM/article/view/19342\n[11] E. Colleoni, A. Rozza, and A. Arvidsson, \u201cEcho chamber or public sphere? pre-\ndicting political orientation and measuring political homophily in twitter using big\ndata,\u201d Journal of Communication , vol. 64, no. 2, pp. 317\u2013332, 2014.\n[12] Y. Kou, Y. M. Kow, X. Gui, and W. Cheng, \u201cOne social movement, two social\nmedia sites: A comparative study of public discourses,\u201d Comput. Supported\nCoop. Work , vol. 26, no. 4\u20136, p. 807\u2013836, dec 2017. [Online]. Available:\nhttps://doi.org/10.1007/s10606-017-9284-y\n[13] T. Kinoshita and M. Aida, \u201cA spectral-based model for describing social polarization\nin online communities,\u201d IEICE Trans. Commun. , vol. 105-B, pp. 1181\u20131191, 2022.\n[14] K. Garimella et al. , \u201cPolarization on social media,\u201d 2018.\n[15] P. Barber\u00b4 a, \u201cBirds of the same feather tweet together: Bayesian ideal point estima-\ntion using twitter data,\u201d Political analysis , vol. 23, no. 1, 2015.\n[16] D. Borrelli, L. Iandoli, J. Ram\u00b4 \u0131rez-M\u00b4 arquez, and C. Lipizzi, \u201cA quantitative and\ncontent-based approach for evaluating the impact of counter narratives on affective\npolarization in online discussions,\u201d IEEE Transactions on Computational Social\nSystems , vol. 9, pp. 914\u2013925, 2022.\nREFERENCES 167\n[17] R. Pal, A. Kumar, and M. S. Santhanam, \u201cDepolarization of opinions on social\nnetworks through random nudges. \u201d Physical review. E , vol. 108 3-1, p. 034307, 2022.\n[18] K. Garimella, G. D. F. Morales, A. Gionis, and M. Mathioudakis, \u201cQuantifying\ncontroversy on social media,\u201d ACM Transactions on Social Computing , vol. 1, no. 1,\npp. 1\u201327, 2018.\n[19] P. Barber\u00b4 a, J. T. Jost, J. Nagler, J. A. Tucker, and R. Bonneau, \u201cTweeting from\nleft to right: Is online political communication more than an echo chamber?\u201d\nPsychological Science , vol. 26, no. 10, pp. 1531\u20131542, 2015, pMID: 26297377.\n[Online]. Available: https://doi.org/10.1177/0956797615594620\n[20] C. A. Bail, L. P. Argyle, T. W. Brown, J. P. Bumpus, H. Chen, M. B. F. Hunzaker,\nJ. Lee, M. Mann, F. Merhout, and A. Volfovsky, \u201cExposure to opposing views\non social media can increase political polarization,\u201d Proceedings of the National\nAcademy of Sciences , vol. 115, no. 37, pp. 9216\u20139221, 2018. [Online]. Available:\nhttps://www.pnas.org/content/115/37/9216\n[21] S. Gonz\u00b4 alez-Bail\u00b4 on, D. Lazer, P. Barber\u00b4 a, M. Zhang, H. Allcott, T. Brown,\nA. Crespo-Tenorio, D. Freelon, M. Gentzkow, A. M. Guess, S. Iyengar, Y. M. Kim,\nN. Malhotra, D. Moehler, B. Nyhan, J. Pan, C. V. Rivera, J. Settle, E. Thorson,\nR. Tromble, A. Wilkins, M. Wojcieszak, C. K. de Jonge, A. Franco, W. Mason,\nN. J. Stroud, and J. A. Tucker, \u201cAsymmetric ideological segregation in exposure\nto political news on facebook,\u201d Science , vol. 381, no. 6656, pp. 392\u2013398, 2023.\n[Online]. Available: https://www.science.org/doi/abs/10.1126/science.ade7138\n[22] M. Cinelli, G. De Francisci Morales, A. Galeazzi, W. Quattrociocchi, and\nM. Starnini, \u201cThe echo chamber effect on social media,\u201d Proceedings of the Na-\ntional Academy of Sciences , vol. 118, no. 9, p. e2023301118, 2021.\n[23] A. L. Schmidt, F. Zollo, M. Del Vicario, A. Bessi, A. Scala, G. Caldarelli, H. E.\nStanley, and W. Quattrociocchi, \u201cAnatomy of news consumption on facebook,\u201d\nProceedings of the National Academy of Sciences , vol. 114, no. 12, pp. 3035\u20133039,\n2017.\n[24] E. Bakshy, S. Messing, and L. A. Adamic, \u201cExposure to ideologically diverse news\nand opinion on facebook,\u201d Science , vol. 348, no. 6239, pp. 1130\u20131132, 2015.\n[25] R. S. Nickerson, \u201cConfirmation bias: A ubiquitous phenomenon in many guises,\u201d\nReview of general psychology , vol. 2, no. 2, pp. 175\u2013220, 1998.\n[26] J. T. Klapper, \u201cThe effects of mass communication. \u201d 1960.\n168 REFERENCES\n[27] J. Treviranus and S. Hockema, \u201cThe value of the unpopular: Counteracting the\npopularity echo-chamber on the web,\u201d 2009 IEEE Toronto International Conference\nScience and Technology for Humanity (TIC-STH) , pp. 603\u2013608, 2009.\n[28] E. Brugnoli, M. Cinelli, W. Quattrociocchi, and A. Scala, \u201cRecursive patterns in\nonline echo chambers,\u201d Scientific Reports , vol. 9, 2019.\n[29] M. Del Vicario, A. Bessi, F. Zollo, F. Petroni, A. Scala, G. Caldarelli, H. E. Stanley,\nand W. Quattrociocchi, \u201cThe spreading of misinformation online,\u201d Proceedings of\nthe National Academy of Sciences , vol. 113, no. 3, pp. 554\u2013559, 2016.\n[30] M. D. Vicario, W. Quattrociocchi, A. Scala, and F. Zollo, \u201cPolarization and fake\nnews: Early warning of potential misinformation targets,\u201d ACM Transactions on\nthe Web , vol. 13, no. 2, pp. 10:1\u201310:22, 2019.\n[31] K. Shu, A. Sliva, S. Wang, J. Tang, and H. Liu, \u201cFake news detection on social\nmedia: A data mining perspective,\u201d ACM SIGKDD explorations newsletter , vol. 19,\nno. 1, pp. 22\u201336, 2017.\n[32] K. Shu, A. Bhattacharjee, F. Alatawi, T. H. Nazer, K. Ding, M. Karami, and H. Liu,\n\u201cCombating disinformation in a social media age,\u201d Wiley Interdisciplinary Reviews:\nData Mining and Knowledge Discovery , vol. 10, no. 6, p. e1385, 2020.\n[33] J. Jiang, X. Ren, E. Ferrara et al. , \u201cSocial media polarization and echo chambers\nin the context of covid-19: Case study,\u201d JMIRx med , vol. 2, no. 3, p. e29570, 2021.\n[34] P. T\u00a8 ornberg, \u201cEcho chambers and viral misinformation: Modeling fake news as\ncomplex contagion,\u201d PLoS ONE , vol. 13, 2018.\n[35] D. Wang and Y. Qian, \u201cEcho chamber effect in rumor rebuttal discussions about\ncovid-19 in china: Social media content and network analysis study,\u201d Journal of\nMedical Internet Research , vol. 23, 2021.\n[36] A. Greenwald, D. McGhee, and J. L. Schwartz, \u201cMeasuring individual differences in\nimplicit cognition: the implicit association test. \u201d Journal of personality and social\npsychology , vol. 74 6, pp. 1464\u201380, 1998.\n[37] A. Caliskan, J. Bryson, and A. Narayanan, \u201cSemantics derived automatically from\nlanguage corpora contain human-like biases,\u201d Science , vol. 356, no. 6334, pp. 183\u2013\n186, Apr. 2017.\n[38] M. Grootendorst. (2020) Topic modeling with bert. [Online]. Available:\nhttps://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6\nREFERENCES 169\n[39] Z. Zhang, M. Fang, L. Chen, and M.-R. Namazi-Rad, \u201cIs neural topic modelling\nbetter than clustering? an empirical study on clustering with contextual embeddings\nfor topics,\u201d arXiv preprint arXiv:2204.09874 , 2022.\n[40] F. Jimenez Villalonga, \u201cUncovering correlations between two umap hyperparame-\nters and the input dataset,\u201d 2021.\n[41] L. McInnes and J. Healy, \u201cUmap: Uniform manifold approximation and projection\nfor dimension reduction,\u201d 02 2018.\n[42] R. J. G. B. Campello, D. Moulavi, and J. Sander, \u201cDensity-based clustering based\non hierarchical density estimates,\u201d in Advances in Knowledge Discovery and Data\nMining , J. Pei, V. S. Tseng, L. Cao, H. Motoda, and G. Xu, Eds. Berlin, Heidelberg:\nSpringer Berlin Heidelberg, 2013, pp. 160\u2013172.\n[43] X. Lan, C. Gao, D. Jin, and Y. Li, \u201cStance detection with collaborative role-infused\nllm-based agents,\u201d ICWSM , vol. 18, no. 1, May 2024.\n[44] Y. Zhu, P. Zhang, E.-U. Haq, P. Hui, and G. Tyson, \u201cCan chatgpt reproduce human-\ngenerated labels? a study of social computing tasks,\u201d in ASONAM \u201923 , April 20\n2023.\n[45] G. R. Koch, \u201cSiamese neural networks for one-shot image recognition,\u201d 2015.\n[Online]. Available: https://api.semanticscholar.org/CorpusID:13874643\n[46] E. Hoffer and N. Ailon, \u201cDeep metric learning using triplet network,\u201d in Similarity-\nBased Pattern Recognition , A. Feragen, M. Pelillo, and M. Loog, Eds. Cham:\nSpringer International Publishing, 2015, pp. 84\u201392.\n[47] K. Erdem, \u201cAnimal recognition with siamese networks and mean embeddings,\u201d\nhttps://erdem.pl , Feb 2021. [Online]. Available: https://erdem.pl/2021/02/\nanimal-recognition-with-siamese-networks-and-mean-embeddings\n[48] S. Chandhok, \u201cTriplet loss with keras and tensorflow,\u201d\nMay 2023. [Online]. Available: https://pyimagesearch.com/2023/03/06/\ntriplet-loss-with-keras-and-tensorflow/\n[49] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen,\n\u201cLora: Low-rank adaptation of large language models,\u201d 2021.\n[50] M. McCloskey and N. J. Cohen, \u201cCatastrophic interference in connectionist\nnetworks: The sequential learning problem,\u201d ser. Psychology of Learning and\nMotivation, G. H. Bower, Ed. Academic Press, 1989, vol. 24, pp. 109\u2013165. [Online].\nAvailable: https://www.sciencedirect.com/science/article/pii/S0079742108605368\n170 REFERENCES\n[51] M. D. Conover, B. Gon\u00b8 calves, J. Ratkiewicz, A. Flammini, and F. Menczer, \u201cPre-\ndicting the political alignment of twitter users,\u201d in 2011 IEEE third international\nconference on privacy, security, risk and trust and 2011 IEEE third international\nconference on social computing . IEEE, 2011, pp. 192\u2013199.\n[52] F. H. Calder\u00b4 on, L.-K. Cheng, M.-J. Lin, Y.-H. Huang, and Y.-S. Chen, \u201cContent-\nbased echo chamber detection on social media platforms,\u201d in 2019 IEEE/ACM\nInternational Conference on Advances in Social Networks Analysis and Mining\n(ASONAM) , 2019, pp. 597\u2013600.\n[53] G. Villa, G. Pasi, and M. Viviani, \u201cEcho chamber detection and analysis,\u201d Social\nNetwork Analysis and Mining , vol. 11, no. 1, p. 78, 2021.\n[54] S. c. Ko\u00b8 c, M. \u00a8Ozer, I. H. Toroslu, H. Davulcu, and J. Jordan, \u201cTriadic co-clustering\nof users, issues and sentiments in political tweets,\u201d Expert Systems with Applications ,\nvol. 100, pp. 79\u201394, 2018.\n[55] V. Morini, L. Pollacci, and G. Rossetti, \u201cToward a standard approach for echo\nchamber detection: Reddit case study,\u201d Applied Sciences , vol. 11, no. 12, p. 5390,\n2021.\n[56] J. Gu, F. Wang, Q. Sun, Z. Ye, X. Xu, J. Chen, and J. Zhang, \u201cExploiting be-\nhavioral consistence for universal user representation,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence , vol. 35, no. 5, 2021, pp. 4063\u20134071.\n[57] D. Preot \u00b8iuc-Pietro, Y. Liu, D. Hopkins, and L. Ungar, \u201cBeyond binary labels: polit-\nical ideology prediction of twitter users,\u201d in Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers) , 2017,\npp. 729\u2013740.\n[58] S. Amir, B. C. Wallace, H. Lyu, P. Carvalho, and M. J. Silva, \u201cModelling context\nwith user embeddings for sarcasm detection in social media,\u201d in Proceedings of The\n20th SIGNLL Conference on Computational Natural Language Learning , 2016, pp.\n167\u2013177.\n[59] S. Pan and T. Ding, \u201cSocial media-based user embedding: A literature review,\u201d\nProceedings of the Twenty-Eighth International Joint Conference on Artificial In-\ntelligence (IJCAI-19) , 2019.\n[60] X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, \u201cCommunity preserv-\ning network embedding,\u201d in Thirty-first AAAI conference on artificial intelligence ,\n2017.\nREFERENCES 171\n[61] T. Ding, W. K. Bickel, and S. Pan, \u201cPredicting delay discounting from social media\nlikes with unsupervised feature learning,\u201d in 2018 IEEE/ACM International Con-\nference on Advances in Social Networks Analysis and Mining (ASONAM) . IEEE,\n2018, pp. 254\u2013257.\n[62] K. Garimella, G. Morales, A. Gionis, and M. Mathioudakis, \u201cPolitical discourse\non social media: Echo chambers, gatekeepers, and the price of bipartisanship,\u201d 04\n2018, pp. 913\u2013922.\n[63] M. M\u00a8 uller and M. Salath\u00b4 e, \u201cAddressing machine learning concept drift reveals\ndeclining vaccine sentiment during the COVID-19 pandemic,\u201d CoRR , vol.\nabs/2012.02197, 2020. [Online]. Available: https://arxiv.org/abs/2012.02197\n[64] K. Ethayarajh, D. Duvenaud, and G. Hirst, \u201cUnderstanding undesirable word\nembedding associations,\u201d CoRR , vol. abs/1908.06361, pp. 1696\u20131705, 2019.\n[Online]. Available: http://arxiv.org/abs/1908.06361\n[65] X. Ferrer, T. van Nuenen, J. Such, and N. Criado, \u201cDiscovering and categorising\nlanguage biases in reddit,\u201d in Proceedings of the International AAAI Conference on\nWeb and Social Media (ICWSM) , vol. 15, 2021, pp. 140\u2013151.\n[66] Y. Matalon, O. Magdaci, A. Almozlino, and D. Yamin, \u201cUsing sentiment analysis to\npredict opinion inversion in tweets of political communication,\u201d Scientific Reports ,\n2021.\n[67] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang, \u201cIs\nChatGPT a general-purpose natural language processing task solver?\u201d in\nProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association\nfor Computational Linguistics, Dec. 2023, pp. 1339\u20131384. [Online]. Available:\nhttps://aclanthology.org/2023.emnlp-main.85\n[68] S. T. Aroyehun and A. Gelbukh, \u201cAggression detection in social media: Using deep\nneural networks, data augmentation, and pseudo labeling,\u201d in Proceedings of the\nFirst Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018) . Santa\nFe, New Mexico, USA: Association for Computational Linguistics, Aug. 2018, pp.\n90\u201397. [Online]. Available: https://aclanthology.org/W18-4411\n[69] P. Burnap and M. L. Williams, \u201cCyber hate speech on twitter: An application of\nmachine classification and statistical modeling for policy and decision making,\u201d\nPolicy & Internet , vol. 7, no. 2, pp. 223\u2013242, 2015. [Online]. Available:\nhttps://onlinelibrary.wiley.com/doi/abs/10.1002/poi3.85\n172 REFERENCES\n[70] A. Hande, R. Priyadharshini, and B. R. Chakravarthi, \u201cKanCMD: Kannada\nCodeMixed dataset for sentiment analysis and offensive language detection,\u201d\ninProceedings of the Third Workshop on Computational Modeling of People\u2019s\nOpinions, Personality, and Emotion\u2019s in Social Media . Barcelona, Spain (Online):\nAssociation for Computational Linguistics, Dec. 2020, pp. 54\u201363. [Online].\nAvailable: https://aclanthology.org/2020.peoples-1.6\n[71] M. Zampieri, S. Malmasi, P. Nakov, S. Rosenthal, N. Farra, and R. Kumar,\n\u201cSemeval-2019 task 6: Identifying and categorizing offensive language in social me-\ndia (offenseval),\u201d in Proceedings of the 13th International Workshop on Semantic\nEvaluation , 2019, pp. 75\u201386.\n[72] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani,\nW. Hu, M. Yasunaga, R. L. Phillips, I. Gao, T. Lee, E. David, I. Stavness,\nW. Guo, B. Earnshaw, I. Haque, S. M. Beery, J. Leskovec, A. Kundaje,\nE. Pierson, S. Levine, C. Finn, and P. Liang, \u201cWilds: A benchmark of in-the-wild\ndistribution shifts,\u201d in Proceedings of the 38th International Conference on Machine\nLearning , ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang,\nEds., vol. 139. PMLR, 18\u201324 Jul 2021, pp. 5637\u20135664. [Online]. Available:\nhttps://proceedings.mlr.press/v139/koh21a.html\n[73] L. Manikonda, V. V. Meduri, and S. Kambhampati, \u201cTweeting the mind and insta-\ngramming the heart: Exploring differentiated content sharing on social media,\u201d in\nTenth international AAAI conference on web and social media , 2016.\n[74] T. Ruan, Q. Kong, S. McBride, A. Sethjiwala, and Q. Lv, \u201cCross-platform analy-\nsis of public responses to the 2019 ridgecrest earthquake sequence on twitter and\nreddit,\u201d Scientific Reports , vol. 12, 01 2022.\n[75] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d arXiv preprint\narXiv:1810.04805 , 2018.\n[76] A. Hasanaath and A. Alansari, \u201cStanceCrafters at StanceEval2024: Multi-task\nstance detection using BERT ensemble with attention based aggregation,\u201d in\nProceedings of The Second Arabic Natural Language Processing Conference ,\nN. Habash, H. Bouamor, R. Eskander, N. Tomeh, I. Abu Farha, A. Abdelali,\nS. Touileb, I. Hamed, Y. Onaizan, B. Alhafni, W. Antoun, S. Khalifa, H. Haddad,\nI. Zitouni, B. AlKhamissi, R. Almatham, and K. Mrini, Eds. Bangkok, Thailand:\nAssociation for Computational Linguistics, Aug. 2024, pp. 811\u2013815. [Online].\nAvailable: https://aclanthology.org/2024.arabicnlp-1.94\nREFERENCES 173\n[77] M. Karami, T. H. Nazer, and H. Liu, \u201cProfiling fake news spreaders on social media\nthrough psychological and motivational factors,\u201d in Proceedings of the 32nd ACM\nConference on Hypertext and Social Media , 2021, pp. 225\u2013230.\n[78] M. Karami, A. Mosallanezhad, P. Sheth, and H. Liu, \u201cEstimating topic exposure for\nunder-represented users on social media,\u201d arXiv preprint arXiv:2208.03796 , 2022.\n[79] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang,\n\u201cAnrl: attributed network representation learning via deep neural networks. \u201d in\nIjcai, vol. 18, 2018, pp. 3155\u20133161.\n[80] T. Ding, W. K. Bickel, and S. Pan, \u201cMulti-view unsupervised user feature embed-\nding for social media-based substance use prediction,\u201d in Proceedings of the 2017\nConference on Empirical Methods in Natural Language Processing , 2017, pp. 2275\u2013\n2284.\n[81] S. Amir, G. Coppersmith, P. Carvalho, M. J. Silva, and B. C. Wallace, \u201cQuantifying\nmental health from social media with neural user embeddings,\u201d in Machine Learning\nfor Healthcare Conference . PMLR, 2017, pp. 306\u2013321.\n[82] D. Rozado, \u201cThe political biases of chatgpt,\u201d Social Sciences , vol. 12, no. 3, 2023.\n[Online]. Available: https://www.mdpi.com/2076-0760/12/3/148\n[83] N. Lee, A. Madotto, and P. Fung, \u201cExploring Social Bias in Chatbots using Stereo-\ntype Knowledge,\u201d 2019.\n[84] M. Lai, M. Tambuscio, V. Patti, G. Ruffo, and P. Rosso, \u201cStance polarity in polit-\nical debates: A diachronic perspective of network homophily and conversations on\ntwitter,\u201d Data & Knowledge Engineering , vol. 124, 2019.\n[85] A. Bruns, \u201cIt\u2019s not the technology, stupid: How the \u2018echo chamber\u2019 and \u2018filter\nbubble\u2019 metaphors have failed us,\u201d 2019.\n[86] \u2014\u2014, \u201cEcho chambers? filter bubbles? the misleading metaphors that obscure the\nreal problem,\u201d in Hate speech and polarization in participatory society . Routledge,\n2021, pp. 33\u201348.\n[87] A. Ross Arguedas, C. Robertson, R. Fletcher, and R. Nielsen, \u201cEcho chambers,\nfilter bubbles, and polarisation: a literature review,\u201d Tech. Rep., 2022.\n[88] M. Sun, X. Ma, and Y. Huo, \u201cDoes social media users\u2019 interaction\ninfluence the formation of echo chambers? social network analysis based on\nvaccine video comments on youtube,\u201d International Journal of Environmental\nResearch and Public Health , vol. 19, no. 23, 2022. [Online]. Available:\nhttps://www.mdpi.com/1660-4601/19/23/15869\n174 REFERENCES\n[89] Y. Gao, F. Liu, and L. Gao, \u201cEcho chamber effects on short video platforms,\u201d Sci\nRep, vol. 13, p. 6282, 2023.\n[90] K. Grusauskaite, L. Carbone, J. Harambam, and S. Aupers, \u201cDebating (in) echo\nchambers: How culture shapes communication in conspiracy theory networks on\nyoutube,\u201d New Media & Society , vol. 0, no. 0, p. 14614448231162585, 2023.\n[Online]. Available: https://doi.org/10.1177/14614448231162585\n[91] J.-M. Esteban and D. Ray, \u201cOn the measurement of polarization,\u201d Econometrica ,\nvol. 62, no. 4, pp. 819\u2013851, 1994. [Online]. Available: http://www.jstor.org/stable/\n2951734\n[92] A. Azmanova, \u201cAfter the Left\u2013Right (Dis)continuum: Globalization and the Re-\nmaking of Europe\u2019s Ideological Geography,\u201d International Political Sociology , vol. 5,\nno. 4, pp. 384\u2013407, 12 2011.\n[93] A. H.-E. Wang, Y.-Y. Yeh, C. K. Wu, and F.-Y. Chen, \u201cWhy does taiwan identity\ndecline?\u201d Journal of Asian and African Studies , p. 00219096231168068, 2023.\n[94] M. Ayatollahi Tabaar and A. Yildirim, \u201cReligious Parties and Ideological Change:\nA Comparison of Iran and Turkey,\u201d Political Science Quarterly , vol. 135, no. 4,\npp. 697\u2013723, 08 2020. [Online]. Available: https://doi.org/10.1002/polq.13097\n[95] O. A\u00b8 s\u0131k, \u201cIdeology, polarization, and news culture: The secular-islamist tension in\nturkish journalism,\u201d The International Journal of Press/Politics , vol. 29, no. 2, pp.\n530\u2013547, 2024. [Online]. Available: https://doi.org/10.1177/19401612221132716\n[96] M. Coletto, V. R. K. Garimella, A. Gionis, and C. Lucchese, \u201cAutomatic\ncontroversy detection in social media: A content-independent motif-based\napproach,\u201d Online Soc. Networks Media , vol. 3-4, pp. 22\u201331, 2017. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:54300115\n[97] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, \u201cFast unfolding\nof communities in large networks,\u201d Journal of statistical mechanics: theory and\nexperiment , vol. 2008, no. 10, p. P10008, 2008.\n[98] A. Cossard, G. D. F. Morales, K. Kalimeri, Y. Mejova, D. Paolotti, and M. Starnini,\n\u201cFalling into the echo chamber: the italian vaccination debate on twitter,\u201d in Pro-\nceedings of the International AAAI conference on web and social media , vol. 14,\n2020, pp. 130\u2013140.\n[99] B. Ghojogh, A. Ghodsi, F. Karray, and M. Crowley, \u201cUniform manifold approxima-\ntion and projection (umap) and its variants: Tutorial and survey,\u201d 08 2021.\nREFERENCES 175\n[100] M. Jacomy, T. Venturini, S. Heymann, and M. Bastian, \u201cForceatlas2, a continuous\ngraph layout algorithm for handy network visualization designed for the gephi\nsoftware,\u201d PLOS ONE , vol. 9, no. 6, pp. 1\u201312, 06 2014. [Online]. Available:\nhttps://doi.org/10.1371/journal.pone.0098679\n[101] F. Newport, \u201cDemocrats racially diverse; republicans mostly white,\u201d Online post,\nFebruary 2013, accessed on July 5th, 2023. [Online]. Available: https://news.\ngallup.com/poll/160373/democrats-racially-diverse-republicans-mostly-white.aspx\n[102] P. R. Center. (2020) Differences in how democrats and republicans behave on\ntwitter. [Online]. Available: https://www.pewresearch.org/politics/2020/10/15/\ndifferences-in-how-democrats-and-republicans-behave-on-twitter/\n[103] J. E. Mueller, \u201cPresidential popularity from truman to johnson,\u201d American Political\nScience Review , vol. 64, no. 1, p. 18\u201334, 1970.\n[104] N. Koch, \u201cThe problem with rallying around the (ukrainian) flag,\u201d Space and Polity ,\nvol. 0, no. 0, pp. 1\u20135, 2023.\n[105] V. Ghafouri, B. RezaeeDaryakenari, and N. Kasap, \u201cWho rallies around the flag?\nanalyzing the impact of foreign interventions on nations\u2019 political stance using\nsocial media data,\u201d Master\u2019s Thesis, Sabanc\u0131 University, 2020, [Thesis]. [Online].\nAvailable: https://risc01.sabanciuniv.edu/record=b2473816\n[106] W. D. Baker and J. R. Oneal, \u201cPatriotism or opinion leadership?: The\nnature and origins of the \u201crally \u2019round the flag\u201d effect,\u201d Journal of\nConflict Resolution , vol. 45, no. 5, pp. 661\u2013687, 2001. [Online]. Available:\nhttps://doi.org/10.1177/0022002701045005006\n[107] S. Arora, Y. Liang, and T. Ma, \u201cA simple but tough-to-beat baseline for sentence\nembeddings,\u201d in ICLR , 2017.\n[108] B. Coleman. (2020) Why is it okay to average embeddings? [Online]. Available:\nhttps://randorithms.com/2020/11/17/Adding-Embeddings.html\n[109] S. Langer, \u201cGender is a complex number and the case for trans phantoms,\u201d Studies\nin Gender and Sexuality , vol. 23, no. 2, pp. 136\u2013145, 2022.\n[110] J. N. Pieterse, \u201cDeconstructing/reconstructing ethnicity,\u201d Nations and Nationalism ,\nvol. 3, no. 3, pp. 365\u2013395, 1997.\n[111] J. He, H. B. Zia, I. Castro, A. Raman, N. Sastry, and G. Tyson, \u201cFlocking\nto mastodon: Tracking the great twitter migration,\u201d in Proceedings of the 2023\nACM on Internet Measurement Conference , ser. IMC \u201923. New York, NY,\n176 REFERENCES\nUSA: Association for Computing Machinery, 2023, p. 111\u2013123. [Online]. Available:\nhttps://doi.org/10.1145/3618257.3624819\n[112] D. Slater and A. Arugay, \u201cPolarizing figures: Executive power and institutional\nconflict in asian democracies,\u201d American Behavioral Scientist , vol. 62, pp. 106 \u2013 92,\n2018.\n[113] A. Abramowitz, \u201cThe disappearing center: Engaged citizens, polarization, and\namerican democracy,\u201d 2010.\n[114] E. Ribberink, P. Achterberg, and D. Houtman, \u201cReligious polarization: contest-\ning religion in secularized western european countries,\u201d Journal of Contemporary\nReligion , vol. 33, pp. 209 \u2013 227, 2018.\n[115] S. Salamat, N. Arabzadeh, S. Seyedsalehi, A. Bigdeli, M. Zihayat, and E. Bagheri,\n\u201cNeural disentanglement of query difficulty and semantics,\u201d in Proceedings of the\n32nd ACM International Conference on Information and Knowledge Management ,\nser. CIKM \u201923. New York, NY, USA: Association for Computing Machinery,\n2023, p. 4264\u20134268. [Online]. Available: https://doi.org/10.1145/3583780.3615189\n[116] Z. An, J. Breuhaus, J. Niu, A. E. Sariyuce, and K. Joseph, \u201cCurated and asymmetric\nexposure: A case study of partisan talk during covid on twitter,\u201d in ICWSM , 2024.\n[117] H. Zade, S. Williams, T. T. Tran, C. Smith, S. Venkatagiri, G. Hsieh, and K. Star-\nbird, \u201cTo reply or to quote: Comparing conversational framing strategies on twit-\nter,\u201d Computing and Sustainable Societies , 2024.\n[118] T. I. Archive, \u201cThe twitter stream grab. \u201d 2024.\n[119] P. A. Dignam and D. A. Rohlinger, \u201cMisogynistic men online: How the red pill\nhelped elect trump,\u201d Signs: Journal of Women in Culture and Society , vol. 44,\nno. 3, pp. 589\u2013612, 2019. [Online]. Available: https://doi.org/10.1086/701155\n[120] D. Ging, \u201cAlphas, betas, and incels: Theorizing the masculinities of the\nmanosphere,\u201d Men and Masculinities , vol. 22, no. 4, pp. 638\u2013657, 2019. [Online].\nAvailable: https://doi.org/10.1177/1097184X17706401\n[121] T. Farrell, M. Fernandez, J. Novotny, and H. Alani, Exploring Misogyny across the\nManosphere in Reddit . New York, NY, USA: Association for Computing Machinery,\n2019, p. 87\u201396. [Online]. Available: https://doi.org/10.1145/3292522.3326045\n[122] M. Horta Ribeiro, J. Blackburn, B. Bradlyn, E. De Cristofaro, G. Stringhini,\nS. Long, S. Greenberg, and S. Zannettou, \u201cThe evolution of the manosphere across\nthe web,\u201d Proceedings of the International AAAI Conference on Web and Social\nMedia , vol. 15, no. 1, pp. 196\u2013207, May 2021.\nREFERENCES 177\n[123] V. Borsotti and P. Bj\u00f8rn, \u201cHumor and stereotypes in computing: An equity-focused\napproach to institutional accountability,\u201d Computer Supported Cooperative Work\n(CSCW) , vol. 31, no. 4, pp. 771\u2013803, 2022.\n[124] K. Messing, M. Lefran\u00b8 cois, and J. Saint-Charles, \u201cObserving inequality: Can er-\ngonomic observations help interventions transform the role of gender in work activ-\nity?\u201d Computer Supported Cooperative Work (CSCW) , vol. 30, pp. 215\u2013249, 2021.\n[125] J. Rode, E. Kirstin, H. Jessica, H. Megan Kelly, W. Anna, and M. Jennifer, \u201cUn-\nderstanding gender equity in author order assignment,\u201d in Proceedings of the ACM\non Human-Computer Interaction-CSCW archive Volume 2 Issue CSCW, November\n2018, vol. 21. ACM, 2018.\n[126] M. Horta Ribeiro, S. Jhaver, S. Zannettou, J. Blackburn, G. Stringhini,\nE. De Cristofaro, and R. West, \u201cDo platform migrations compromise content mod-\neration? evidence from r/the donald and r/incels,\u201d Proceedings of the ACM on\nHuman-Computer Interaction , vol. 5, no. CSCW2, pp. 1\u201324, 2021.\n[127] H. Paakki, H. Veps\u00a8 al\u00a8 ainen, and A. Salovaara, \u201cDisruptive online communication:\nHow asymmetric trolling-like response strategies steer conversation off the track,\u201d\nComputer Supported Cooperative Work (CSCW) , vol. 30, no. 3, pp. 425\u2013461, 2021.\n[128] G. Freeman and D. Y. Wohn, \u201cStreaming your identity: Navigating the presentation\nof gender and sexuality through live streaming,\u201d Computer Supported Cooperative\nWork (CSCW) , vol. 29, pp. 795\u2013825, 2020.\n[129] J. Humphry, \u201cOfficing: Mediating time and the professional self in the support\nof nomadic work,\u201d Computer Supported Cooperative Work (CSCW) , vol. 23, pp.\n185\u2013204, 2014.\n[130] G. Hine, J. Onaolapo, E. De Cristofaro, N. Kourtellis, I. Leontiadis, R. Samaras,\nG. Stringhini, and J. Blackburn, \u201cKek, cucks, and god emperor trump: A measure-\nment study of 4chan\u2019s politically incorrect forum and its effects on the web,\u201d in\nICWSM , 05 2017, pp. 92\u2013101.\n[131] E. Fast, T. Vachovsky, and M. S. Bernstein, \u201cShirtless and dangerous:\nQuantifying linguistic signals of gender bias in an online fiction writing\ncommunity,\u201d ICWSM , vol. abs/1603.08832, p. 112\u2013120, 2016. [Online]. Available:\nhttp://arxiv.org/abs/1603.08832\n[132] N. Swinger, M. De-Arteaga, N. T. H. IV, M. D. M. Leiserson, and A. T. Kalai,\n\u201cWhat are the biases in my word embedding?\u201d CoRR , vol. abs/1812.08769, 2018.\n[Online]. Available: http://arxiv.org/abs/1812.08769\n178 REFERENCES\n[133] G. G. Subies, \u201cExist2021: Detecting sexism with transformers and translation-\naugmented data,\u201d in IberLEF@SEPLN , 2021.\n[134] M. Sch\u00a8 utz, J. Boeck, D. Liakhovets, D. Slijepcevic, A. Kirchknopf, M. Hecht,\nJ. Bogensperger, S. Schlarb, A. Schindler, and M. Zeppelzauer, \u201cAutomatic sex-\nism detection with multilingual transformer models ait fhstp@exist2021,\u201d in Iber-\nLEF@SEPLN , 2021.\n[135] X. He, S. Zannettou, Y. Shen, and Y. Zhang, \u201cYou only prompt once: On the\ncapabilities of prompt learning on large language models to tackle toxic content,\u201d\n2023.\n[136] R. Garcia, V. Sreekanti, N. Yadwadkar, D. Crankshaw, J. E. Gonzalez, and J. M.\nHellerstein, \u201cContext: The missing piece in the machine learning lifecycle,\u201d in KDD\nCMI Workshop , vol. 114, 2018, p. 368.\n[137] A. Paul, A. Agrawal, W.-k. Liao, and A. Choudhary, \u201cAnonymine: Mining anony-\nmous social media posts using psycho-lingual and crowd-sourced dictionaries,\u201d in\nProceedings of KDD , 2016.\n[138] M. E. Ireland, J. Schler, G. N. Gecht, and K. G. Niederhoffer, \u201cProfiling depression\nin neutral reddit posts,\u201d in GOOD Workshop KDD , vol. 20, 2020, p. 2020.\n[139] M. Newman, \u201cPower laws, pareto distributions and zipf\u2019s law,\u201d Contemporary\nPhysics - CONTEMP PHYS , vol. 46, 12 2004.\n[140] X. Ferrer-Aran, T. van Nuenen, N. Criado, and J. Such, \u201cDiscovering\nand interpreting conceptual biases in online communities,\u201d IEEE Transactions\non Knowledge and Data Engineering (TKDE) , 2021. [Online]. Available:\nhttps://www.computer.org/csdl/journal/tk/5555/01/09667280/1zMCh7YGvfi\n[141] T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, and A. Kalai, \u201cMan is to com-\nputer programmer as woman is to homemaker? debiasing word embeddings,\u201d in\nProceedings of the 30th International Conference on Neural Information Processing\nSystems , ser. NIPS\u201916. Red Hook, NY, USA: Curran Associates Inc., 2016, p.\n4356\u20134364.\n[142] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn, \u201cThe\npushshift reddit dataset,\u201d ICWSM , vol. abs/2001.08435, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2001.08435\n[143] J. B. Mountford, \u201cTopic modeling the red pill,\u201d Social Sciences , vol. 7, no. 3, 2018.\n[Online]. Available: https://www.mdpi.com/2076-0760/7/3/42\nREFERENCES 179\n[144] S. P. V. Valkenburgh, \u201cDigesting the red pill: Masculinity and neoliberalism in the\nmanosphere,\u201d Men and Masculinities , vol. 24, no. 1, pp. 84\u2013103, 2021. [Online].\nAvailable: https://doi.org/10.1177/1097184X18816118\n[145] K. Papadamou, S. Zannettou, J. Blackburn, E. De Cristofaro, G. Stringhini, and\nM. Sirivianos, \u201c\u201dhow over is it?\u201d understanding the incel community on youtube,\u201d\nProceedings of the ACM on Human-Computer Interaction , vol. 5, no. CSCW2, pp.\n1\u201325, 2021.\n[146] S. Wright, V. Trott, and C. Jones, \u201c\u2018the pussy ain\u2019t worth it, bro\u2019:\nassessing the discourse and structure of mgtow,\u201d Information, Communication\n& Society , vol. 23, no. 6, pp. 908\u2013925, 2020. [Online]. Available: https:\n//doi.org/10.1080/1369118X.2020.1751867\n[147] E. Taylor, \u201cReddit\u2019s female dating strategy offers women advice \u2014 and a strict rule-\nbook for how to act,\u201d 2020. [Online]. Available: https://www.theverge.com/2020/\n2/14/21137852/reddit-female-dating-advice-strategy-women-rulebook-memes\n[148] M. Dynel, \u201cVigilante disparaging humour at r/inceltears: Humour as critique of\nincel ideology,\u201d Language & Communication , vol. 74, pp. 1\u201314, 2020. [Online].\nAvailable: https://www.sciencedirect.com/science/article/pii/S0271530920300410\n[149] A. Massanari, \u201c\u201dcome for the period comics. stay for the cultural awareness\u201d: re-\nclaiming the troll identity through feminist humor on reddit\u2019s /r/trollxchromo-\nsomes,\u201d Feminist Media Studies , vol. 19, pp. 1\u201319, 12 2017.\n[150] Q. Myers, \u201cWhat\u2019s better than this? guys being (good) dudes on reddit\u2019s\ntrollychromosome,\u201d 2020. [Online]. Available: https://melmagazine.com/en-us/\nstory/trollychromosome-reddit-toxic-masculinity\n[151] A. Samoshyn, \u201cHate speech and offensive language dataset,\u201d https://www.kaggle.\ncom/mrmorj/hate-speech-and-offensive-language-dataset/metadata, 2020, last Up-\ndate: 2020-06-17.\n[152] E. Wulczyn, N. Thain, and L. Dixon, \u201cEx machina: Personal attacks seen at\nscale,\u201d in Proceedings of the 26th International Conference on World Wide Web ,\nser. WWW \u201917. Republic and Canton of Geneva, CHE: International World Wide\nWeb Conferences Steering Committee, 2017, p. 1391\u20131399. [Online]. Available:\nhttps://doi.org/10.1145/3038912.3052591\n[153] E. Guest, B. Vidgen, A. Mittos, N. Sastry, G. Tyson, and H. Margetts,\n\u201cAn expert annotated dataset for the detection of online misogyny,\u201d in\nProceedings of the 16th Conference of the European Chapter of the Association\n180 REFERENCES\nfor Computational Linguistics: Main Volume . Online: Association for\nComputational Linguistics, Apr. 2021, pp. 1336\u20131350. [Online]. Available:\nhttps://aclanthology.org/2021.eacl-main.114\n[154] K. Chernyshev, E. Garanina, D. Bayram, Q. Zheng, and L. Edman, \u201cLCT-1\nat SemEval-2023 task 10: Pre-training and multi-task learning for sexism\ndetection and classification,\u201d in Proceedings of the 17th International Workshop\non Semantic Evaluation (SemEval-2023) . Toronto, Canada: Association for\nComputational Linguistics, Jul. 2023, pp. 1573\u20131581. [Online]. Available:\nhttps://aclanthology.org/2023.semeval-1.217\n[155] J. LaViolette and B. Hogan, \u201cUsing platform signals for distinguishing discourses:\nThe case of men\u2019s rights and men\u2019s liberation on reddit,\u201d in ICWSM , 2019.\n[156] M. H. Ribeiro, J. Blackburn, B. Bradlyn, E. D. Cristofaro, G. Stringhini, S. Long,\nS. Greenberg, and S. Zannettou, \u201cFrom pick-up artists to incels: A data-driven\nsketch of the manosphere,\u201d CoRR , vol. abs/2001.07600, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2001.07600\n[157] C. Zhong, H.-w. Chang, D. Karamshuk, D. Lee, and N. Sastry, \u201cWearing many\n(social) hats: How different are your different social network personae?\u201d in Pro-\nceedings of the International AAAI Conference on Web and Social Media , vol. 11,\nno. 1, 2017, pp. 397\u2013406.\n[158] S. Zannettou, T. Caulfield, J. Blackburn, E. De Cristofaro, M. Sirivianos,\nG. Stringhini, and G. Suarez-Tangil, \u201cOn the origins of memes by means of fringe\nweb communities,\u201d in Proceedings of the Internet Measurement Conference 2018 ,\n2018, pp. 188\u2013202.\n[159] M. H. Ribeiro, J. Blackburn, B. Bradlyn, E. De Cristofaro, G. Stringhini, S. Long,\nS. Greenberg, and S. Zannettou, \u201cThe evolution of the manosphere across the web,\u201d\ninProceedings of the International AAAI Conference on Web and Social Media ,\nvol. 15, 2021, pp. 196\u2013207.\n[160] W. M. Si, M. Backes, J. Blackburn, E. De Cristofaro, G. Stringhini, S. Zannettou,\nand Y. Zhang, \u201cWhy so toxic? measuring and triggering toxic behavior in\nopen-domain chatbots,\u201d in Proceedings of the 2022 ACM SIGSAC Conference on\nComputer and Communications Security , ser. CCS \u201922. New York, NY, USA:\nAssociation for Computing Machinery, 2022, p. 2659\u20132673. [Online]. Available:\nhttps://doi.org/10.1145/3548606.3560599\n[161] F. Tahmasbi, L. Schild, C. Ling, J. Blackburn, G. Stringhini, Y. Zhang, and S. Zan-\nnettou, \u201c\u201cgo eat a bat, chang!\u201d: On the emergence of sinophobic behavior on web\nREFERENCES 181\ncommunities in the face of covid-19,\u201d in Proceedings of the web conference 2021 ,\n2021, pp. 1122\u20131133.\n[162] S. Ali, M. H. Saeed, E. Aldreabi, J. Blackburn, E. De Cristofaro, S. Zannettou, and\nG. Stringhini, \u201cUnderstanding the effect of deplatforming on social networks,\u201d in\n13th ACM Web Science Conference 2021 , 2021, pp. 187\u2013195.\n[163] J. Seering and S. R. Kairam, \u201cWho moderates on twitch and what do they do?\nquantifying practices in community moderation on twitch,\u201d Proceedings of the ACM\non Human-Computer Interaction , vol. 7, no. GROUP, pp. 1\u201318, 2023.\n[164] R. B. Evans and A. Savoia, \u201cDifferential testing: a new approach to change de-\ntection,\u201d in The 6th Joint Meeting on European software engineering conference\nand the ACM SIGSOFT Symposium on the Foundations of Software Engineering:\nCompanion Papers , 2007, pp. 549\u2013552.\n[165] P. Rayson, D. Archer, S. Piao, and A. M. McEnery, \u201cThe ucrel semantic analysis\nsystem. \u201d 2004.\n[166] D. P. Myatt and C. Wallace, \u201cWhen Does One Bad Apple Spoil the\nBarrel? An Evolutionary Analysis of Collective Action,\u201d The Review of\nEconomic Studies , vol. 75, no. 2, pp. 499\u2013527, 04 2008. [Online]. Available:\nhttps://doi.org/10.1111/j.1467-937X.2008.00482.x\n[167] R. I. Watson, \u201cInvestigation into deindividuation using a cross-cultural survey tech-\nnique. \u201d 1973.\n[168] H. Tajfel and J. C. Turner, \u201cThe social identity theory of intergroup behavior,\u201d in\nPolitical psychology . Psychology Press, 2004, pp. 276\u2013293.\n[169] S. Ali, A. Razi, S. Kim, A. Alsoubai, C. Ling, M. De Choudhury, P. J.\nWisniewski, and G. Stringhini, \u201cGetting meta: A multimodal approach for\ndetecting unsafe conversations within instagram direct messages of youth,\u201d Proc.\nACM Hum.-Comput. Interact. , vol. 7, no. CSCW1, apr 2023. [Online]. Available:\nhttps://doi.org/10.1145/3579608\n[170] H. Lin and L. Qiu, \u201cTwo sites, two voices: Linguistic differences between facebook\nstatus updates and tweets,\u201d in International Conference on Cross-Cultural Design .\nSpringer, 2013, pp. 432\u2013440.\n[171] N. Van Raemdonck, \u201cThe echo chamber of anti-vaccination conspiracies: mecha-\nnisms of radicalization on facebook and reddit,\u201d Institute for Policy, Advocacy and\nGovernance (IPAG) Knowledge Series, Forthcoming , 2019.\n182 REFERENCES\n[172] M. D. Vicario, S. Gaito, W. Quattrociocchi, M. Zignani, and F. Zollo, \u201cNews con-\nsumption during the italian referendum: A cross-platform analysis on facebook and\ntwitter,\u201d in 2017 IEEE International Conference on Data Science and Advanced\nAnalytics (DSAA) , 2017, pp. 648\u2013657.\n[173] K.-C. Yang, F. Pierri, P.-M. Hui, D. Axelrod, C. Torres-Lugo, J. Bryden,\nand F. Menczer, \u201cThe covid-19 infodemic: Twitter versus facebook,\u201d Big Data\n& Society , vol. 8, no. 1, p. 20539517211013861, 2021. [Online]. Available:\nhttps://doi.org/10.1177/20539517211013861\n[174] Y. Zhu, P. Zhang, E.-U. Haq, P. Hui, and G. Tyson, \u201cCan chatgpt reproduce\nhuman-generated labels? a study of social computing tasks,\u201d 2023.\n[175] X. Shen, Z. Chen, M. Backes, and Y. Zhang, \u201cIn chatgpt we trust? measuring and\ncharacterizing the reliability of chatgpt,\u201d 2023.\n[176] W. M. Si, M. Backes, J. Blackburn, E. De Cristofaro, G. Stringhini,\nS. Zannettou, and Y. Zhang, \u201cWhy So Toxic?: Measuring and Triggering\nToxic Behavior in Open-Domain Chatbots,\u201d in Proceedings of the 2022\nACM SIGSAC Conference on Computer and Communications Security . Los\nAngeles CA USA: ACM, Nov. 2022, pp. 2659\u20132673. [Online]. Available:\nhttps://dl.acm.org/doi/10.1145/3548606.3560599\n[177] S. Silva and M. Kenney, \u201cAlgorithms, platforms, and ethnic bias,\u201d Communications\nof the ACM , vol. 62, no. 11, pp. 37\u201339, 2019.\n[178] J. Feine, U. Gnewuch, S. Morana, and A. Maedche, \u201cGender bias in chatbot de-\nsign,\u201d in Chatbot Research and Design: Third International Workshop, CONVER-\nSATIONS 2019, Amsterdam, The Netherlands, November 19\u201320, 2019, Revised\nSelected Papers 3 . Springer, 2020, pp. 79\u201393.\n[179] K. Jiang, \u201cWhat is \u2019woke ai\u2019 and why is elon musk reportedly building\na chatbot to counter it?\u201d TheStar, March 2023, accessed on Month\nDay, Year. [Online]. Available: https://www.thestar.com/business/2023/03/01/\nwhat-is-woke-ai-and-why-is-elon-musk-reportedly-building-a-chatbot-to-counter-it.\nhtml\n[180] J. Vincent, \u201cAs conservatives criticize \u2018woke ai,\u2019 here are chatgpt\u2019s rules for\nanswering culture war queries,\u201d The Verge, February 2023, accessed on Month\nDay, Year. [Online]. Available: https://www.theverge.com/2023/2/17/23603906/\nopenai-chatgpt-woke-criticism-culture-war-rules\n[181] C. D. Lawrence, Hidden in White Sight: How AI Empowers and Deepens Systemic\nRacism . CRC Press, 2023.\nREFERENCES 183\n[182] S. Barikeri, A. Lauscher, I. Vuli\u00b4 c, and G. Glava\u02c7 s, \u201cRedditBias: A\nReal-World Resource for Bias Evaluation and Debiasing of Conversational\nLanguage Models,\u201d Jun. 2021, arXiv:2106.03521 [cs]. [Online]. Available:\nhttp://arxiv.org/abs/2106.03521\n[183] Solon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach, \u201cThe Prob-\nlem With Bias: Allocative Versus Representational Harms in Machine Learning,\u201d\nProceedings of SIGCIS, Philadelphia, PA , 2017.\n[184] A. Abid, M. Farooqi, and J. Zou, \u201cPersistent Anti-Muslim Bias in Large Language\nModels,\u201d in Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\nSociety . Virtual Event USA: ACM, Jul. 2021, pp. 298\u2013306. [Online]. Available:\nhttps://dl.acm.org/doi/10.1145/3461702.3462624\n[185] S. L. Blodgett, S. Barocas, H. Daum\u00b4 e III, and H. Wallach, \u201cLanguage (Technology)\nis Power: A Critical Survey of \u201dBias\u201d in NLP,\u201d May 2020, arXiv:2005.14050 [cs].\n[Online]. Available: http://arxiv.org/abs/2005.14050\n[186] M. J. Ali, \u201cChatgpt and lacrimal drainage disorders: Performance and scope of\nimprovement,\u201d Ophthalmic plastic and reconstructive surgery , vol. 39, no. 3, pp.\n221\u2013225, 2023.\n[187] V. Agarwal, S. Joglekar, A. P. Young, and N. Sastry, \u201cGraphnli: A graph-based\nnatural language inference model for polarity prediction in online debates,\u201d in Pro-\nceedings of the ACM Web Conference 2022 , 2022, pp. 2729\u20132737.\n[188] V. Agarwal, A. P. Young, S. Joglekar, and N. Sastry, \u201cA graph-based context-aware\nmodel to understand online conversations,\u201d arXiv preprint arXiv:2211.09207 , 2022.\n[189] J. Beck, B. Neupane, and J. M. Carroll, \u201cManaging conflict in online debate com-\nmunities,\u201d First Monday , vol. 24, no. 7, Jun. 2019.\n[190] X. Zhou, A. Mulay, E. Ferrara, and R. Zafarani, \u201cRecovery: A multimodal\nrepository for covid-19 news credibility research,\u201d in Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge Management , ser. CIKM\n\u201920. New York, NY, USA: Association for Computing Machinery, 2020, p.\n3205\u20133212. [Online]. Available: https://doi.org/10.1145/3340531.3412880\n[191] A. Deb, L. Luceri, A. Badaway, and E. Ferrara, \u201cPerils and challenges of social\nmedia and election manipulation analysis: The 2018 us midterms,\u201d in Companion\nProceedings of The 2019 World Wide Web Conference , ser. WWW \u201919. New\nYork, NY, USA: Association for Computing Machinery, 2019, p. 237\u2013247. [Online].\nAvailable: https://doi.org/10.1145/3308560.3316486\n184 REFERENCES\n[192] J. Ye and S. Skiena, \u201cMediarank: Computational ranking of online news\nsources,\u201d in Proceedings of the 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining , ser. KDD \u201919. New York, NY, USA:\nAssociation for Computing Machinery, 2019, p. 2469\u20132477. [Online]. Available:\nhttps://doi.org/10.1145/3292500.3330709\n[193] The Political Compass, \u201cPolitical compass test,\u201d Available online, n.d. [Online].\nAvailable: https://www.politicalcompass.org/test\n[194] Pew Research Center\u2014U.S. Politics & Policy (blog), \u201cPolitical typology quiz,\u201d\nAvailable online, n.d. [Online]. Available: https://www.pewresearch.org/politics/\nquiz/political-typology/\n[195] IDRlabs, \u201c8 values political test,\u201d Available online, n.d. [Online]. Available:\nhttps://www.idrlabs.com/8-values-political/test.php\n[196] R. A. Frick and I. Vogel, \u201cFraunhofer sit at checkthat! 2022: ensemble similarity\nestimation for finding previously fact-checked claims,\u201d Working Notes of CLEF ,\n2022.\n[197] A. Siddique, M. Maqbool, K. Taywade, and H. Foroosh, \u201cPersonalizing task-oriented\ndialog systems via zero-shot generalizable reward function,\u201d in Proceedings of the\n31st ACM International Conference on Information & Knowledge Management ,\n2022, pp. 1787\u20131797.\n[198] L. S. Bothun, S. E. Feeder, and G. A. Poland, \u201cReadability of covid-19\nvaccine information for the general public,\u201d Vaccine , vol. 40, no. 25, pp.\n3466\u20133469, 2022. [Online]. Available: https://www.sciencedirect.com/science/\narticle/pii/S0264410X22005461\n[199] A. Fourney, M. Ringel Morris, A. Ali, and L. Vonessen, \u201cAssessing the readability\nof web search results for searchers with dyslexia,\u201d in The 41st International ACM\nSIGIR Conference on Research & Development in Information Retrieval , ser.\nSIGIR \u201918. New York, NY, USA: Association for Computing Machinery, 2018, p.\n1069\u20131072. [Online]. Available: https://doi.org/10.1145/3209978.3210072\n[200] A. B. Suleiman, J. S. Lin, and N. A. Constantine, \u201cReadability of educational\nmaterials to support parent sexual communication with their children and\nadolescents,\u201d Journal of Health Communication , vol. 21, no. 5, pp. 534\u2013543,\n2016, pMID: 27116292. [Online]. Available: https://doi.org/10.1080/10810730.\n2015.1103334\nREFERENCES 185\n[201] J. Introne, \u201cMeasuring belief dynamics on twitter,\u201d Proceedings of the International\nAAAI Conference on Web and Social Media , vol. 17, no. 1, pp. 387\u2013398, Jun. 2023.\n[Online]. Available: https://ojs.aaai.org/index.php/ICWSM/article/view/22154\n[202] R. Upadhyay, G. Pasi, and M. Viviani, \u201cA passage retrieval transformer-based\nre-ranking model for truthful consumer health search,\u201d in Machine Learning\nand Knowledge Discovery in Databases: Research Track , D. Koutra, C. Plant,\nM. Gomez Rodriguez, E. Baralis, and F. Bonchi, Eds. Cham: Springer Nature\nSwitzerland, 2023, pp. 355\u2013371.\n[203] D. K\u00a8 u\u00b8 c\u00a8 uk and F. Can, \u201cStance detection: A survey,\u201d ACM Comput. Surv. , vol. 53,\nno. 1, feb 2020. [Online]. Available: https://doi.org/10.1145/3369026\n[204] Q. Sun, Z. Wang, Q. Zhu, and G. Zhou, \u201cStance detection with hierarchical\nattention network,\u201d in Proceedings of the 27th International Conference on\nComputational Linguistics , E. M. Bender, L. Derczynski, and P. Isabelle, Eds.\nSanta Fe, New Mexico, USA: Association for Computational Linguistics, Aug.\n2018, pp. 2399\u20132409. [Online]. Available: https://aclanthology.org/C18-1203\n[205] A. ALDayel and W. Magdy, \u201cStance detection on social media: State of the\nart and trends,\u201d Information Processing & Management , vol. 58, no. 4, p.\n102597, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/\npii/S0306457321000960\n[206] D. Biber and E. Finegan, \u201cAdverbial stance types in english,\u201d Discourse Processes ,\nvol. 11, no. 1, pp. 1\u201334, 1988.\n[207] A. Lamb, D. He, A. Goyal, G. Ke, C.-F. Liao, M. Ravanelli, and Y. Bengio, \u201cTrans-\nformers with competitive ensembles of independent mechanisms,\u201d 2021.\n[208] N. Reimers and I. Gurevych, \u201cSentence-bert: Sentence embeddings using siamese\nbert-networks,\u201d 2019.\n[209] T. Vahtola, M. Creutz, and J. Tiedemann, \u201cIt is not easy to detect paraphrases:\nAnalysing semantic similarity with antonyms and negation using the new\nSemAntoNeg benchmark,\u201d in Proceedings of the Fifth BlackboxNLP Workshop on\nAnalyzing and Interpreting Neural Networks for NLP , J. Bastings, Y. Belinkov,\nY. Elazar, D. Hupkes, N. Saphra, and S. Wiegreffe, Eds. Abu Dhabi, United\nArab Emirates (Hybrid): Association for Computational Linguistics, Dec. 2022,\npp. 249\u2013262. [Online]. Available: https://aclanthology.org/2022.blackboxnlp-1.20\n[210] Y. Wang, Y. Lin, X. Zeng, and G. Zhang, \u201cMultilora: Democratizing lora for better\nmulti-task learning,\u201d 2023.\n", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "NLP-Driven Approaches to Measuring Online Polarization and Radicalization", "author": ["V Ghafouri"], "pub_year": "2025", "venue": "NA", "abstract": "The growing popularity of social media has coincided with a massive number of real-world  issues and crises that are controversial and polarizing. Recent issues such as Russo-"}, "filled": false, "gsrank": 474, "pub_url": "https://dspace.networks.imdea.org/handle/20.500.12761/1927", "author_id": ["RnPFjYcAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:XpjKhsWUQdUJ:scholar.google.com/&output=cite&scirp=473&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D470%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=XpjKhsWUQdUJ&ei=W7WsaJnZIo6IieoP0sKRuAk&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:XpjKhsWUQdUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://dspace.networks.imdea.org/bitstream/handle/20.500.12761/1927/Vahid_PhD_Thesis_compressed.pdf?sequence=1&isAllowed=y"}}, {"title": "Supplemental Information for Weaving it in: How Partisan Media React to Events", "year": "2022", "pdf_data": "Supplemental Information for Weaving it in: How\nPartisan Media React to Events\nClara Vandeweerdt\nJuly 19, 2022\n1\nContents\n1 Radio recordings and transcripts 3\n2 Ascribing mentions to shows 5\n3 Mechanical Turk coding task 6\n4 Google trends and topic mentions 8\n5 Classifying radio shows 10\n6 Modeling mention proportions and counts 13\n7 Regression tables 14\n8 Counting non-neutral mentions of immigration 17\n9 Long-term e\u000bects: agenda half-lives 17\n10 Robustness checks 19\n10.1 News and public radio shows . . . . . . . . . . . . . . . . . . . . 19\n10.2 Show classi\fcation thresholds . . . . . . . . . . . . . . . . . . . . 20\n2\n1 Radio recordings and transcripts\nRecording of radio stations was an automated process, starting with scraping\nthe publicly available audio streaming URLs of US terrestrial radio stations from\nthe website Radio-Locator.com. The audio from these URLs was ingested and\nsaved in 5-10 minute chunks on Amazon S3. Next, the audio was automatically\ntranscribed using a custom speech-to-text model, based on an model entered in\nthe IARPA ASpIRE challenge by Peddinti et al. (2015). The speech transcrip-\ntion algorithm was gradually improved in the course of data collection, with the\nerror rate going from 27% in April 2018 to 13% in November 2018, and staying\nstable since. This error rate was measured using existing transcripts from NPR\nand Rush Limbaugh.\nBeeferman et al. (2019) describe the features of (a subset of) this data\nset in more detail. This data subset is available for download at https://\ngithub:com/social-machines/RadioTalk , and the data collection team can\nbe contacted for further technical documentation and code sharing requests at\nhttps://socialmachines :org/ .\nRecording started in May 2018, with 72 stations. The number of stations fol-\nlowed increased from there, reaching 175 in August 2018. The number dropped\nto 155 in March 2019 and remained there until the end of the period analyzed\n(October 2019). Figure 1 illustrates the number of stations in the data set over\ntime. Of the initial stations, 50 were chosen randomly from the population of\nUS talk radio stations, and more stations were added and dropped from the data\nset in the course of the next few months according to the interests of the team at\nthe Laboratory for Social Machines. The total number of US terrestrial talk sta-\ntions (with registered formats Business News, News/Talk, Public Radio, News,\nCollege, Talk) in the covered period was 1897, according Radio-Locator.com.\nFigure 2 shows where in the United States the transcribed stations are lo-\ncated. Table 1 describes the distributions of these stations in terms of content\nand station subtype, compared to the population of all talk radio stations. Un-\nderrepresented station types include stations from the Midwest, college stations\n(which unlike the two college stations included in this data set typically air\nmore music than talk content), and public radio stations. Underrepresentation\n050100150200\nJul 2018 Oct 2018 Jan 2019 Apr 2019 Jul 2019 Oct 2019count\nFigure 1: Number of talk radio stations in data set over time.\n3\nof public stations is not likely to be consequential for the \fnal sample of shows.\nThis is because many public radio stations broadcast the same set of shows,\nproduced for nation-wide broadcasting by public radio networks such as NPR.\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\nFigure 2: Locations of the continental radio stations in the data set. Two more\nstations are in Alaska, and one is on Hawaii.\nSample % Population % t-test p-value\nregion: Midwest 16 28 0.00\nregion: Northeast 22 16 0.04\nregion: South 33 30 0.35\nregion: West 29 27 0.46\nformat: Business News 0 1 0.09\nformat: College 2 13 0.00\nformat: News 2 2 0.91\nformat: News/Talk 49 27 0.00\nformat: Public Radio 32 45 0.00\nformat: Talk 15 12 0.18\nTable 1: Balance table comparing region and format for radio stations in the\nsample to the population of US talk radio stations.\nThe number of distinct radio shows broadcast by these stations added up to\n675 by June 2018, and over 1000 by August. Figure 3 shows the total number\nof shows captured in each week of the observed period.\n4\n02505007501000\nApr 2018 Jul 2018 Oct 2018 Jan 2019 Apr 2019 Jul 2019 Oct 2019countFigure 3: Total number of shows recorded, by week.\n2 Ascribing mentions to shows\nIn principle, ascribing a topic mention to a show should be easy. A topic term\ncounts as being part of a show if it was mentioned on a radio station, during a\ntime slot when we know that show is being broadcast on that station. There\nare two issues with that approach, however. First, the speech-to-text algorithm\ncreating the transcriptions is not perfect. Some topic term mentions are missed,\nwhereas others are false positives. Second, radio schedule data (which show is\non when) was pooled from a range of sources, some more reliable than others.\nAn hour of audio coming from a particular station could have the wrong show\nlabel if there is no correct, up-to-date schedule for the station.\nBoth problems can be remedied when there is more than one airing of a show\nepisode|that is, when the show is broadcast on more than one of the stations\nthat were recorded that day. I followed the procedure below to decide which\ntopic mentions should be ascribed to each show.\n1. For each topic mention, create a \\slice\" of content that includes up to\nten words coming before and after the topic term. For example: \\years\nfrom now, \ffty years, there isn't any evidence of climate change\" (aired\non KBTK, April 25th 2018).\n2. In the entire set of transcripts coming from a particular day, search for\nclusters of similar mentions (low string distance).\n3. For each cluster, do the following:\n(a) Take all mentions in the cluster, and check which show labels they\nhave, based on station scheduling data. For example, the \frst men-\ntion above comes from audio labeled as The Glenn Beck Program,\nbut it is in a cluster together with \fve (very similar) mentions that\nare all labeled as being part of the Rush Limbaugh Show.\n(b) For each of the shows that occur at least once as show labels in\nthe cluster, calculate con\fdence that it is the correct show label for\nthis cluster of mentions. Calculations take into account how many\n5\nmentions in the cluster have this show label, and also how many times\neach show was aired on di\u000berent stations that day, without containing\na similar mention.\n(c) Compare the show label likelihoods, and choose the one with the\nhighest con\fdence. For example, the mention cluster above was as-\ncribed to The Rush Limbaugh Show with a con\fdence of :6. This\nrelatively low con\fdence mostly comes from the fact that there are\nmany airings, labeled in the scheduling data as broadcasts of The\nRush Limbaugh Show on April 25th 2018, that did not include any\nmentions with a content slice similar to this one.\n4. Treat each cluster, with its most likely show label, as a single unique\nmention that happened on that show|but only if its show label con\fdence\nis greater than .5. Otherwise, discard the cluster.\n3 Mechanical Turk coding task\nEach time one of the political topics was mentioned, its ideological frame was\ncoded by workers on Amazon's crowdsourcing platform, Mechanical Turk. Work-\ners were allowed to code as many mentions as they wanted, for a payment of\n$0.14 per mention.\nCoders listened to short audio fragments surrounding each mention, so that\nthe ratings are based on vocal as well are verbal cues. We know from previous\nwork that tone of voice confers unique information (Dietrich et al., 2019). Audio\nfragments started 10 seconds before the topic keyword (e.g. \\global warming\")\nwas said, and ended 20 seconds after. Next, the coder was asked to choose\nbetween two frames (e.g., \\skeptical\" or \\convinced\" about climate change), or\nneither frame.\nIn the case of climate change, for 71% of mentions, the \frst two coders\nagreed on the classi\fcation. In another 25% of the cases, a third coder broke\nthe tie, and I used the majority opinion as the code for that mention. In the\n\fnal 4%, all three coders disagreed, and I labeled the fragment \\neither\". In\nthe case of gun policy, the distribution was: 61% two-coder agreement; 32%\ntwo-out-of three majority; 6% no agreement. Immigration fragments were the\nmost di\u000ecult to code: the percentages were 54%, 37%, and 9%.\nFor all three topics, it was rare for two coders to assign opposite frames to\nthe same fragment. In the case of climate change, only 7% of mentions were\nassigned one frame (skeptical or concerned), even though one of the coders\nsuggested the opposite frame. In the cases of gun control and immigration, this\nwas 9% and 8%. In other words, coding disagreements are unlikely to have\nresulted in mentions being assigned the wrong slant. They would, however,\nhave caused some neutral topic mentions to be incorrectly coded as employing\na frame, and vice versa.\nRight next to the audio player, coders always saw the following brief instruc-\ntions on their screen:\nClimate:\n\u0088Skeptical: climate change evidence is false or unclear, climate change is\nnot an important problem, it is too costly to \fght against climate change.\n6\n\u0088Concerned: climate change evidence is solid, climate change caused by\nhumans, it is a threat and we need action.\n\u0088Neutral: no clear opinion about climate change, and no mention of evi-\ndence for or against climate change.\nGun policy:\n\u0088Pro-gun: right to own guns, looser gun control laws, guns protect people,\nsecond amendment\n\u0088Anti-gun: stricter gun control laws, guns cause violence/mass shootings\n\u0088Neither: no opinion about gun rights/ gun control, no hints whether the\nspeaker is pro- or anti-gun.\nImmigration:\n\u0088Supporting immigration: we don't need a wall or more deportations, fam-\nilies should stay together, immigration is good for our country\n\u0088Tough on immigration: border needs protection, illegal immigration should\nbe stopped, immigration is bad for our country\n\u0088Neither: just news, no opinion about immigration, no hints whether the\nspeaker is supportive or tough.\nFinally, coders were encouraged to click through to the longer instructions\n(\\code book\") if they were doing the task for the \frst time, or had not done the\ntask in the past day. The sections below contain the descriptions of each issue\nframe from those instructions. Instructions received minor changes during the\ncoding process, in order to account for common mistakes.\nClimate, Skeptical - The evidence for climate change is false or not certain;\npredictions did not come true. Scientists are hiding evidence against global\nwarming. The climate is always changing. Humans did not cause global warm-\ning. Problems we see today (e.g. wild\fres) are not caused by climate change.\nEven if global warming exists, the e\u000bects are not so bad, or they are positive.\nClimate change is not important compared to other problems. It is too expen-\nsive or risky to take action, it would cost too many jobs, it is too soon to take\naction, it is not our responsibility.\nClimate, Concerned - The evidence for climate change is clear. Humans and\ntheir greenhouse gas (CO 2) emissions cause global warming. Climate change\nwill have negative e\u000bects (e.g. sea levels rising, plants or animals dying) now or\nlater. Problems we see today (e.g. storms, droughts) are due to global warming.\nWe need to act on it (e.g. by using less energy or clean energy). People who are\nlooking for solutions or who are passing climate laws are doing the right thing.\nGun policy, Pro-gun - People have the right to own guns, protected by the\nsecond amendment. The government should not take our guns away. There\nshould be fewer laws and rules about owning or buying guns and ammo (e.g.\nbullets). Gun control does not prevent crimes. People who own guns prevent\n7\ncrimes from happening, because they can defend themselves, their family, and\nothers. People need guns to protect themselves if the government turns against\nthe citizens. Mass shootings are a mental health problem. The US does not\nhave more gun violence because it has more guns.\nGun policy, Anti-gun - There should be stricter rules about who can own\nand buy guns. People who want to buy a gun should have to pass a background\ncheck or get a license. Some types of guns, like assault ri\res, should be banned.\nWe need stronger measures to prevent teenagers, or people with mental health\nproblems from having guns. The United States has more gun violence than\nother countries because it has more guns. Mass shootings would happen less\noften if it was harder to get a gun.\nImmigration, supportive - There should not be a wall on the border with\nMexico, and we should deport fewer people. Unauthorized immigrants are often\nrunning from violence in their home country. They should be treated well and\nfamilies should stay together. The rules for legal immigration should not be\nmade stricter. People who were brought into the country as children should\nbe allowed to stay. Immigrants are hard-working, and they contribute to our\nsociety. America is a nation of immigrants.\nImmigration, tough - We should invest more money and manpower into pro-\ntecting the border and deporting unauthorized immigrants. If immigrants come\nor stay here illegally, they broke the law. Immigrants raise crime rates, they\ndo not pay taxes and should not get government help. We also need stricter\npolicies on legal immigration. Many immigrants don't speak English well, don't\nadopt American culture, or take jobs from Americans. American-born citizens\nshould come \frst.\nIn the code books, coders were encouraged to classify mentions as \\neither\"\nif there was not enough context to classify them, if they did not fall into any\nof the other categories, or if the audio fragment was not about the topic. For\nexample, a piece of news (with no negative or positive tone) about a climate law\nthat was passed, or a commercial about climate-proo\fng your windows. How-\never, the code books also explained that topic mentions can support ideological\nframes even if the speaker is not giving their own opinion. For example, a news\nitem about new evidence for (or against) climate change would still count as\nconcerned (or skeptical) and not neutral, because its e\u000bect could be to make a\nlistener more concerned (or skeptical).\n4 Google trends and topic mentions\nTo de\fne \\pre-event\" and \\post-event\" weeks, I use Google Trends data. This\ndata comes in the form of a day-by-day index of the number of Internet searches\nfor a search term describing the event (e.g., \\hurricane Florence\" or \\hurricane\nMichael\"). On each day, I compare search activity to the peak-activity day for\nthat event. By my de\fnition, pre-event weeks end on the last day where search\nactivity was less than 5% compared to the peak. Post-event weeks start on the\n9\n\u25cf\u25cf\nFlorence landfallMichael\n landfall\n0100200300400500\n02505007501000\nAug Sep Oct NovMentions of climate changeRadio shows recorded\n\u25cf\u25cf\n\u25cf\u25cfSanta Fe High SchoolJacksonville Landing\nPittsburgh Synagogue\nThousand Oaks\n0100200300\n02505007501000\nApr Jul OctMentions of gun policyRadio shows recorded\n\u25cf\u25cf\nFamily separation\nannouncedExecutive order\n010002000\n0200400600\nApr May Jun JulMentions of immigrationRadio shows recorded\nFigure 4: Daily number of talk radio mentions of climate, gun policy and immi-\ngration, with signi\fcant events. Bands show the pre- (orange) and post- (brown)\nevent weeks as de\fned by Google Trends. Mention trends smoothed using Loess\nregression, allowing for discontinuities at the beginning of each post-event week.\nLight gray lines show the number of talk radio shows recorded on each date.\n\frst day where search activity was at least 20% compared to the peak.\nFigure 4 shows these periods, along with the number of topic mentions per\nday on all talk radio shows.1While these decision rules do not always line\nup perfectly with the \\before\" and \\after\" of talk radio attention, they do a\nreasonable job of capturing the baseline and the peak. The \\pre-event\" weeks\nalso appear to be acceptable baselines, even though no week is ever completely\nfree of (at least local) events that are relevant to these political topics.\n5 Classifying radio shows\nTable 2 contains all non-political shows that I used to train the political/non-\npolitical classi\fer. Table 3 lists all political shows, with two or more sources\nbacking its ideological label. A source is considered to con\frm an ideological\nlabel if it names the program, its host, or another closely associated entity as\neither \\conservative\" or \\right-wing\"; or as \\liberal\", \\left-wing\" or \\progres-\nsive\".\nRequiring more than one source hedges against the possible ideological bias\nin ideological bias judgments themselves. For example, the Center for American\nProgress is itself classi\fed as left-wing by the Media Bias/Fact Check group.\nHowever, I never came across two sources that contradicted each other in their\njudgments of any given show's bias; when two sources existed, they always\nagreed.\nTwo NPR programs (All Things Considered and Morning Edition) were\nleft out of the ideology model's training set for the main analyses, despite the\nfact that several sources labeled NPR as liberal. Section 10.1 below provides a\nrobustness check that includes these NPR shows in the training set.\nIn total, the labeled shows had almost 8500 episodes, of which almost 5800\nwere political. Before training the model on these shows, I held out 10% of\neach show's episodes, to be used for model testing. To transform the show\ntranscripts into data, I counted and normalized the number of occurrences of\n5000 tokenized word pairs in each transcript. In other words, the features\nfed to the model are term frequency{inverse document frequency (TF-IDF)\nvectors for 5000 tokenized bigrams. I left out any features whose TF-IDF score\nwas correlated too strongly with any particular show label|for example, hosts'\nverbal tics, their names or show sponsors.\nExisting literature on categorical ideology classi\fcation at the phrase (Iyyer\net al., 2014) or document (Yan et al., 2017) level suggests that regularized logistic\nregression (LR) works well with this amount of data. I tried both LR (with L2\nregularization) and Support-Vector Machines (SVMs). I decided between these\ntwo models, and tuned both the show-speci\fc feature correlation threshold and\nthe shrinkage parameter c, via blocked k-fold cross-validation. That is, I left\nout all episodes from the same show at once, and then tried to predict their\nlabel with a model trained on the other shows. LR slightly outperformed SVM\nfor the political/non-political classi\fer, and SVM slightly outperformed LR for\nthe ideology classi\fer.\nOnce tuned, I tested the \fnal models' performance by using them to label\nthe hold-out episodes of each show, which were all completely new to the model.\n1These are unique mentions, not double-counting mentions on radio shows that are broad-\ncast on more than one station. See section 2 for more on this.\n10\nShow Topic\nFood Friday Vox Pop food\nWMT Cooking Show food\nBetter Lawns and Gardens gardening\nClassic Gardens and Landscape gardening\nGardenLine w/ Randy Lemmon gardening\nDr. Bob Martin health\nPurity Products health\nYour Health with Dr. Joe Galati health\nAt Home with Gary Sullivan home\nHouse Talk with Ray Trimble home\nSturdy Home Improvement home\nTexas Home Improvement home\nHandel on the Law legal\nThe Legal Exchange legal\nYour Legal Rights legal\nFinancial Advisors with Aubrey Morrow money advice\nMoney Matters with Ken Moraif money advice\nThe Dave Ramsey Show money advice\nThe Financial Exchange money advice\nAfropop Worldwide music\nAfternoon Jazz music\nClassic Jazz with Michele Robins music\nClassical 24 with Andrea Blain music\nClassical 24 with Bob Christiansen music\nHomegrown Music music\nJesus Christ Show (PRN) religious\nLutheran Hour religious\nSt. John's Lutheran Church religious\nBen Maller sports\nBuckey Sportsman with Dan Armitage sports\nFOX Sports Radio sports\nFox Sports Weekends sports\nThe Big Sports Show sports\nTable 2: Non-political shows in training set, with their hand-coded topic.\nFor each show, when trying to classify its hold-out episodes, I trained a model\non all other shows. This way, I avoided rewarding the model for making predic-\ntions based on show-speci\fc features. The political/non-political LR correctly\nclassi\fed all 50 shows based on their hold-out episodes. The conservative/liberal\nSVM successfully classi\fed 14 out of 15 political shows. After tuning and test-\ning, I trained the classi\fers on the full labeled data sets (training and hold-out).\nFinally, I applied the political/non-political classi\fer to all 1005 recorded shows,\nand the ideology classi\fer to the 429 shows that were labeled as political.\nThere are a few reasons to treat show ideology as binary, rather than contin-\nuous. First, existing evidence suggests that radio shows are ideologically sorted,\nsuggesting that it is reasonable to divide shows into a liberal and a conservative\ngroup. Second, having two ideology categories is a common choice in studies of\n11\nConservative\nShow Sources\nBen Shapiro Media bias/fact check (as The Daily Wire), Politi-\nfact, Wikipedia\nGlenn Beck CAP, Pew, Wikipedia\nHugh Hewitt CAP, Media bias/fact check (as Salem Radio Net-\nwork News), Wikipedia\nJoe Pags CAP, Wikipedia\nLaura Ingraham CAP, Politifact, Wikipedia\nMark Levin CAP, Media bias/fact check (as Conservative Re-\nview)\nMike Gallagher CAP, Wikipedia\nRush Limbaugh CAP, Pew, Politifact, Wikipedia\nSean Hannity CAP, Pew, Politifact, Wikipedia\nThe Savage Nation CAP, Politifact, Wikipedia\nLiberal\nShow Sources\nDemocracy Now! Media bias/fact check, Wikipedia\nMike Malloy CAP, Wikipedia, Liberal Talk Radio Wiki\nRing of Fire Radio Media bias/fact check, Wikipedia\nStephanie Miller Media bias/fact check (as Fstv), Politifact,\nWikipedia, Liberal Talk Radio Wiki\nThom Hartmann CAP, Media bias/fact check (as Fstv), Politifact,\nWikipedia, Liberal Talk Radio Wiki\nNPR\nShow Sources\nAll Things Considered Media bias/fact check (as NPR), Pew (as NPR)\nMorning Edition Media bias/fact check (as NPR), Pew (as NPR)\nTable 3: Political shows in training set, with their ideology label and\nsources. Sources: CAP (Center for American Progress and Free Press,\nThe Structural Imbalance of Talk Radio, 2007, ampr.gs/2UegLbP); Lib-\neral Talk Radio Wiki (ltradio.fandom.com/wiki/List ofpersonalities);\nMedia bias/fact check (mediabiasfactcheck.com); Pew Research Cen-\nter (journalism.org/interactives/media-polarization); Politifact (politi-\nfact.com/personalities/); Wikipedia (wikipedia.com). The entity labeled\nby the source is in parentheses, if it is something other than the show or its\nhost.\ntalk radio (cf. Yanovitzky and Cappella 2001; Sobieraj and Berry 2011; Cen-\nter for American Progress and Free Press 2007; Jamieson and Cappella 2008,\np. 86). The classi\fcation results support this|most political shows can be\nclassi\fed with fairly high con\fdence as either liberal or conservative, suggesting\nit is not as important for a model to cover the ideological \\middle ground\".\nFinally, the training data can be reliably classi\fed into two ideological bins.\nSources that designate radio shows as right- or left-leaning usually give cate-\ngorical labels. Ratings on a spectrum would be more debatable. In a study on\n12\nTV news, Martin and Yurukoglu (2017) solved this by training a classi\fer on\nCongressional speech, with continuous DW-NOMINATE scores as the ideology\noutcome variable. I found that a domain-adapted binary classi\fer trained on\nspeeches in the 114th Congress misclassi\fed 3 out of 17 political training shows.\nSwitching to a Congress-based model could thus lead to a signi\fcant drop in\nprediction quality.\n6 Modeling mention proportions and counts\nFor each week on each radio show, there are two relevant outcomes: the number\nof mentions of a topic; and the proportion of mentions that feature a particular\nframing of the topic.\nThe number of mentions of each topic across shows-weeks has a very skewed,\noverdispersed distribution. As a result, a linear model of mention counts would\nhave large uncertainty around its coe\u000ecients. Moreover, conclusions would be\nheavily dominated by a handful of shows that have far more mentions than the\nothers. Instead, I use a negative binomial model.\nThe full model, which tells us about the di\u000berential e\u000bect of events on shows\nwith di\u000berent ideologies, is:\nE[Ycount\ni ] =exp(\f0+\f1posti+\f2liberali+\f3posti\u0002liberali+\f4minutesi)\nwhereYcount\ni is the number of topic mentions in show-week i. It has a\nnegative binomial distribution. postiindicates whether the show-week happened\npre-event (0) or post-event (1). liberaliindicates whether the show was classi\fed\nas conservative (0) or liberal (1). minutesiis the show's airtime in minutes per\nweek. I control for total airtime because shows with more content naturally\nhave more opportunities to mention a topic.\nTo model frame proportions, I use a so-called fractional logit model (Papke\nand Wooldridge, 1996).2The full model is:\nE[Yprop\ni] =logit\u00001(\f0+\f1posti+\f2liberali+\f3posti\u0002liberali)\nwhereYprop\ni is the fraction of topic mentions on in show-week ithat use\na particular frame (e.g. climate skepticism). This is the proportion among\nmentions with an ideological frame, leaving out mentions coded as \\neither\". It\nhas a quasi-binomial distribution. In both models, standard errors are clustered\nat the show level.\nThe key quantities of interest, which are reported in the main text, are\npredicted mention counts and predicted frame proportions before and after an\nevent. The p-values accompanying these predictions in the main text refer to\nthe relevant (combinations of) model coe\u000ecients. For instance, ^\f1represents\nthe estimated change in topic mentions (or frames) from pre- to post-event for\na conservative show. ^\f1+^\f3represents the change for a liberal show. ^\f2is the\n2This is a generalized linear model with a logit link function and a quasi-binomial probabil-\nity mass function for the outcome. Using a quasi-binomial distribution instead of a binomial\none does not change the estimates but gives us more robust standard errors (Papke, ND).\n13\nestimated di\u000berence between liberal and conservative shows before an event, and\n^\f3captures how the event di\u000berentially a\u000bects liberal and conservative shows.\nIn order to calculate con\fdence intervals for the predicted quantities (as seen\nin main text Figures 2 and 3), I use a non-parametric block bootstrap technique\n(Cameron et al., 2008) with percentile con\fdence intervals. In each iteration of\nthe bootstrap, I sample shows with replacement, extracting the same number\nof shows as there are in the original data set. For each sampled show, I include\nall of its observations (show-weeks) in a new simulated data set. I then run the\nmodels above on the simulated data and calculate the predicted quantities of\ninterest. Iterating 1000 times results in 1000 predictions. The 2.5th and 97.5th\nquantiles of these predictions are the bounds of the con\fdence intervals.\n7 Regression tables\nTables 4 and 5 below show regression results for the number of mentions (nega-\ntive binomial model) and proportion of mention frames (fractional logit model)\nfor all three topics. All signi\fcance tests are two-tailed.\nFor the negative binomial models, the coe\u000ecient on post (^\f1) amounts to the\nestimated increase in log topic mentions on a conservative show after an event.\nThe coe\u000ecient on liberal (^\f2) represents the e\u000bect on log topic mentions of\nmoving from a conservative to a liberal show, pre-event. The coe\u000ecient on\npost x liberal (^\f3) is the change from pre-event to post-event topic mentions\namong liberal shows, compared to the change among conservative shows.\nFor the fractional logit models, the coe\u000ecients represent analogous changes\nin the (logit-transformed) proportions of climate-concerned, anti-gun, and tough-\non-immigration frames. These models have fewer observations, because show-\nweeks with no topic mentions have no information on frame proportions.\n14\n15\nTable 4: Regression table for number of mentions of climate change, gun policy\nand immigration. Model is negative binomial regression with standard errors\nclsutered at the show level.\nclimate gun immig.\n(1) (2) (3)\nConstant \u00000.484\u0003\u0003\u00030.483 1.188\u0003\u0003\u0003\n(0.122) (0.313) (0.149)\np = 0.0001 p = 0.124 p = 0.000\npost 0.968\u0003\u0003\u00030.577\u0003\u0003\u00031.782\u0003\u0003\u0003\n(0.116) (0.124) (0.094)\np = 0.000 p = 0.00001 p = 0.000\nliberal 1.018\u0003\u0003\u0003\u00001.013\u0003\u0003\u00030.541\n(0.195) (0.338) (0.405)\np = 0.00000 p = 0.003 p = 0.182\npost x liberal \u00000.329 \u00000.222 \u00001.100\u0003\u0003\u0003\n(0.240) (0.221) (0.400)\np = 0.171 p = 0.317 p = 0.007\nminutes 0.001\u0003\u0003\u00030.001\u0003\u0003\u00030.001\u0003\u0003\u0003\n(0.0001) (0.0002) (0.0001)\np = 0.000 p = 0.0002 p = 0.000\nObservations 1,412 1,896 524\nNote:\u0003p<0.1;\u0003\u0003p<0.05;\u0003\u0003\u0003p<0.01\n16\nTable 5: Regression table for proportion of mentions that are concerned about\nclimate, anti-gun, or tough on immigration. Model is fractional logit regression\nwith standard errors clsutered at the show level.\nclimate gun immig.\n(1) (2) (3)\nConstant \u00000.505\u0003\u0003\u0003\u00001.326\u0003\u0003\u00030.850\u0003\u0003\u0003\n(0.178) (0.152) (0.188)\np = 0.005 p = 0.000 p = 0.00001\npost 0.175 0.422\u0003\u0003\u00000.153\n(0.179) (0.165) (0.190)\np = 0.331 p = 0.011 p = 0.421\nliberal 2.959\u0003\u0003\u00032.232\u0003\u0003\u0003\u00001.651\u0003\u0003\u0003\n(0.422) (0.296) (0.289)\np = 0.000 p = 0.000 p = 0.000\npost x liberal 0.339 \u00000.122 \u00000.060\n(0.398) (0.376) (0.320)\np = 0.395 p = 0.745 p = 0.851\nObservations 529 653 320\nNote:\u0003p<0.1;\u0003\u0003p<0.05;\u0003\u0003\u0003p<0.01\n8 Counting non-neutral mentions of immigra-\ntion\nOne obvious concern about the \fnding of increased immigration mentions might\nbe that it would be di\u000ecult to report on the event (family separation) without\nmentioning the topic terms. For that reason, I re-do the analyses leaving out any\nmentions that the coders labeled \\neither\". These are mentions that support\nno stance on immigration, largely because they are simply pieces of news on the\ntopic. On both ideological sides, the proportional increase in the \\non-neither\"\nmentions is roughly the same size as the overall increases above (conservative:\nfrom 1:9 to 9:4; liberal: from 4 :0 to 6:4). In other words, the change in attention\nto immigration is not only due to outlets reporting on the events themselves|\nit is as much due to an increase in opinionated commentary on the topic of\nimmigration.\n9 Long-term e\u000bects: agenda half-lives\nStudies of the agenda-setting power of major events in mainstream media often\n\fnd e\u000bects that last for months (Lawrence, 2000; Birkland, 2004; Zhang et al.,\n2017). Here, I describe how trends in topic mentions tend to evolve once they\nhave peaked after an event. In other words, I analyze how quickly attention to\na topic dissipates on talk radio. Here, I \ft a model to the post-event trends.\nFor each topic, I look at total topic mention counts in the month after each\nevent, day by day. Because I am interested in the downward trend, the start of\nthis month is not the start of the post-event week (which I de\fned earlier as the\n\frst day on which the event reaches some level of social signi\fcance). Instead, it\nis the peak of talk radio attention to the topic: the day with the most mentions.\nTo avoid catching the beginning of attention to the next event, I leave out\nany days that fall in the next post-event week. This results in six dropped\nobservations for climate change, and six for mass shootings. Finally, for the\nPittsburgh Synagogue shooting, I only include the \frst nine days. This is be-\ncause ten days after the shooting, there was another mass shooting in Thousand\nOaks, California.\nAfter pooling the data across events within topics, I estimate the following\nsimple model:\nE[Ypct\nd;e] = 2\u0000\fd\nYpct\nd;eis the total number of mentions of the topic on day dafter the peak for\nevente. It is measured as a percentage of peak attention{i.e., attention on day\nd= 0. I did not include an intercept, as Ypct\nd=0;e= 1 by de\fnition. Using base\n2 for the exponential decay conveniently allows us to interpret the inverse of \f\nas the half-life of attention; the number of days it takes for mention counts to\nhalve. I estimate the model using non-linear least squares.\nThis model is not perfect|for instance, attention likely returns to some\nbaseline level in the long run, rather than eventually going to zero. However, it\n\fts the time trend in topic mentions reasonably well.\nFigure 5 shows the predicted post-peak attention trend for each topic, along-\nside the data. The estimated beta coe\u000ecients are 0 :14 for hurricanes and climate\n17\nFigure 5: Decline in attention (number of topic mentions) after the peak for\neach event. Line graphs show the estimated exponential decay of attention.\nPoints show observed attention by event.\nchange, 0:11 for mass shootings and gun policy, and 0 :13 for family separation\nand immigration. This means that the half-life of attention to topics after these\nevents is 7{9 days.\n10 Robustness checks\n10.1 News and public radio shows\nMost of the existing literature has treated news and public radio shows as sepa-\nrate from political talk radio. I \fnd that by a number of quantitative measures,\nthese shows are not necessarily more \\neutral\" than the other programs in our\ndata set. In this section, I show that the results are not a\u000bected by whether or\nnot news and public radio shows are included.\nAmong shows classi\fed as political, 25 shows have the word \\news\" in their\nname (e.g. \\Alabama Morning News\"), and 14 shows are produced and dis-\ntributed by National Public Radio (NPR). First, I look at whether these shows,\nwhen they mention a political topic, tend to use neither of the two estab-\nlished frames. Mentions coded as \\neither\" are usually presentations of facts\nor straightforward pieces of news about a topic. Bundling all of the observed\nweeks, just 16% of climate mentions on the average news show use neither a\nsupportive nor a skeptical frame. The same is true for NPR shows. With re-\nspect to gun policy, 33% of mentions on the average news show are neutral in\nthis way. On NPR shows it is 34%. Immigration is the topic that invites the\nmost neutral discussion, with 42% of news mentions and 55% of NPR mentions.\nA second possibility is that these shows are neutral in the sense that they\npresent both sides of the story equally, for instance by inviting guests with\nopposite points of view. Among the topic mentions that use an ideological\nframe, however, I do not \fnd this type of balance. In the case of climate\nchange, the average news show dedicates more than 89% of its non-neutral\nmentions to one side of the issue (be it skeptical or convinced). For the NPR\nshows, it is 95%. On gun policy, these shows spend 73% of their non-neutral\nmentions arguing for the same side. For the average NPR show, that is 84%.\nOn the topic of immigration, the average news show has 72% of its non-neutral\nmentions supporting the same side. For NPR shows, it is 69%. Not surprisingly,\nall NPR shows tend to take the same side (in particular, they overwhelmingly\nfeature \\climate-convinced\" content). The shows in the news category are more\nmixed in the direction of their slant. Crucially, none of the numbers above look\nmuch di\u000berent in the sample of non-news, non-NPR shows.\nTo check the robustness of these \fndings, I experiment with alternative\nde\fnitions of news and public radio shows, based on what stations broadcast\nthem. All US radio stations have a self-selected format that broadly describes\ntheir programming, mostly for the purposes of marketing and statistics. An\nalternative criterion for news shows would be those shows that are broadcast at\nleast one station with the\\All News\" format. An alternative criterion for public\nradio shows would be those shows that are broadcast on at least one station\nwith the \\Public Radio\" format. These de\fnitions lead to the same conclusion:\non news and public radio shows, the discussion of political topics looks no more\nneutral or balanced than it does on any other political show.\nGiven these \fndings, we might view news and public radio shows as simply\n19\nwithout news, public with news, public\ncounts frames counts frames\ntopic ideology pre post pre post pre post pre post\nclimate conservative 0.8 2.1 38 42 0.8 2.2 38 41\nclimate liberal 2.2 4.1 92 95 2.0 4.1 93 95\ngun policy conservative 1.9 3.3 21 29 2.0 3.3 22 30\ngun policy liberal 0.7 1.0 71 77 0.7 1.1 68 79\nimmigration conservative 4.3 25.3 70 67 5.1 27.4 70 66\nimmigration liberal 7.3 14.5 31 27 8.2 17.7 30 28\nTable 6: Predicted mention counts and frames (percentage \\convinced\", \\anti-\ngun\" and \\tough on immigration\" frames), pre- and post-event, for each political\ntopic, without and with NPR shows or news shows.\nanother type of talk radio with political content. For that reason, I repeat\nthe analyses, including shows with \\news\" in the name and shows produced by\nNPR. I also add two NPR programs to the liberal show training set for the\nideology classi\fer. This improves performance: testing the model on unseen\nepisodes, the classi\fer now correctly guesses the ideology of allshows. Table\n6 shows the results, alongside the original ones without NPR and news shows.\nWe can see that the basic thrust is the same.\n10.2 Show classi\fcation thresholds\nIn the analyses above, shows are classi\fed based on 50% thresholds: they are\nlabeled political, and conservative, if classi\fers assign them a 50%-or-higher\nprobably of being so. However, the training set for each model is a simply set\nof shows that can reliably be labeled as non-political, liberal or conservative.\nThis set probably does not re\rect the actual balance of show ideologies in the\nfull sample. It is likely, then, that the models' intercept estimates are biased.\nMoreover, perhaps not all political shows are slanted: it is possible there are\nmoderate shows in the sample, which I am unjustly labeling as ideological.\nFigure 6 shows the results of the show classi\fcation e\u000bort. It looks like the\nchoice of `politicalness' threshold could be important, because some shows are\nin fact di\u000ecult to classify. Only 70% of shows can be labeled as political with\nat least 70% certainty. In terms of ideology, the picture looks somewhat more\nrobust. 80% of political shows get an ideological label with over 70% certainty.\nNonetheless, we may be interested in how results change if we exclude shows\nwhose ideological class is unclear.\nHere, I repeat the key analyses, varying my decisions about show classes in\ntwo ways. First, I move the political decision threshold above or below 50%,\nbiasing the model towards labeling fewer or more shows as political. Second, I\ncreate bands around the ideology threshold, excluding shows that the model is\nuncertain about. For example, I might only include shows for which the classi\fer\nis at least 60% certain that they are either liberal or conservative. Table 7 shows\nthe results of the former analysis. Table 8 shows the latter. Neither decision\nchanges the results in any signi\fcant way, except that stricter `politicalness'\nthresholds lead to somewhat more topic mentions at baseline. This makes sense,\nsince I am excluding shows that spend less time covering political topics.\n20\nclimate gun policy immigration\ncounts frames counts frames counts frames\nthreshold ideology pre post pre post pre post pre post pre post pre post\n0.4 conservative 0.7 1.9 38 45 1.6 2.9 22 29 3.8 23.3 66 63\n0.4 liberal 1.9 3.7 92 95 0.6 0.8 69 78 6.4 12.5 28 25\n0.5 conservative 0.8 2.1 38 42 1.9 3.3 21 29 4.3 25.3 70 67\n0.5 liberal 2.2 4.1 92 95 0.7 1.0 71 77 7.3 14.5 31 27\n0.6 conservative 0.8 2.3 37 41 2.0 3.5 20 29 4.3 25.8 70 68\n0.6 liberal 2.4 4.6 91 95 0.7 1.2 72 78 5.2 16.6 28 30\nTable 7: Predicted mention counts and frames (percentage \\convinced\", \\anti-gun\" and \\tough on immigration\" frames), pre- and post-\nevent, for each political topic. Threshold indicates level of certainty we need in order to call a show \\political\" and include it in the data\nset.\nclimate gun policy immigration\ncounts frames counts frames counts frames\nthreshold ideology pre post pre post pre post pre post pre post pre post\n0.5 conservative 0.8 2.1 38 42 1.9 3.3 21 29 4.3 25.3 70 67\n0.5 liberal 2.2 4.1 92 95 0.7 1.0 71 77 7.3 14.5 31 27\n0.6 conservative 0.8 2.1 37 41 1.9 3.4 21 29 4.4 25.8 70 67\n0.6 liberal 2.4 4.4 93 96 0.7 1.0 75 75 8.0 14.8 31 27\n0.7 conservative 0.8 2.1 35 40 2.0 3.5 20 29 4.2 25.2 71 69\n0.7 liberal 2.5 5.9 96 97 0.9 1.2 78 81 5.5 17.0 31 26\nTable 8: Predicted mention counts and frames (percentage \\convinced\", \\anti-gun\" and \\tough on immigration\" frames), before and after\nevents, for each political topic. Threshold indicates level of certainty we need in order to call a show \\conservative\" or \\liberal\", and to\ninclude it in the data set.\n21\nDefinitely non\u2212political\nDefinitely political\n0306090\n0.00 0.25 0.50 0.75 1.00\nProbability of political\nDefinitely conservative\nDefinitely liberal\n0255075100125\n0.00 0.25 0.50 0.75 1.00\nProbability of liberalFigure 6: Distribution of prediction probabilities for shows, resulting from po-\nliticalness and ideology classi\fers.\nReferences\nDoug Beeferman, William Brannon, and Deb Roy. Radiotalk: A large-scale\ncorpus of talk radio transcripts. Technical report, Lab for Social Machines,\nMIT Media Lab, 2019.\nThomas A Birkland. \\The world changed today\": Agenda-setting and policy\nchange in the wake of the September 11 terrorist attacks. Review of Policy\nResearch , 21(2):179{200, 2004.\nA Colin Cameron, Jonah B Gelbach, and Douglas L Miller. Bootstrap-based\nimprovements for inference with clustered errors. The Review of Economics\nand Statistics , 90(3):414{427, 2008.\nCenter for American Progress and Free Press. The structural imbalance of\npolitical talk radio, 2007. URL https://cdn :americanprogress :org/wp-\ncontent/uploads/issues/2007/06/pdf/talk radio:pdf. Accessed April\n2022.\nBryce J Dietrich, Matthew Hayes, and Diana Z O'Brien. Pitch perfect: Vocal\npitch and the emotional intensity of congressional speech. American Political\nScience Review , pages 1{22.\nBryce J Dietrich, Ryan D Enos, and Maya Sen. Emotional arousal predicts\nvoting on the us supreme court. Political Analysis , 27(2):237{243, 2019.\n22\nMohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip Resnik. Political\nideology detection using recursive neural networks. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers) , pages 1113{1122, 2014.\nKathleen Hall Jamieson and Joseph N Cappella. Echo chamber: Rush Limbaugh\nand the conservative media establishment . Oxford University Press, 2008.\nRegina G Lawrence. The politics of force: Media and the construction of police\nbrutality . University of California Press, 2000.\nGregory J Martin and Ali Yurukoglu. Bias in cable news: Persuasion and\npolarization. American Economic Review , 107(9):2565{99, 2017.\nLeslie E Papke. Papke and Wooldridge (1996) \rogit instructions. ND.\nLeslie E Papke and Je\u000brey M Wooldridge. Econometric methods for fractional\nresponse variables with an application to 401 (k) plan participation rates.\nJournal of applied econometrics , 11(6):619{632, 1996.\nVijayaditya Peddinti, Guoguo Chen, Vimal Manohar, Tom Ko, Daniel Povey,\nand Sanjeev Khudanpur. Jhu aspire system: Robust lvcsr with tdnns, ivec-\ntor adaptation and rnn-lms. In 2015 IEEE Workshop on Automatic Speech\nRecognition and Understanding (ASRU) , pages 539{546. IEEE, 2015.\nSarah Sobieraj and Je\u000brey M Berry. From incivility to outrage: Political dis-\ncourse in blogs, talk radio, and cable news. Political Communication , 28(1):\n19{41, 2011.\nHao Yan, Allen Lavoie, and Sanmay Das. The perils of classifying political ori-\nentation from text. Linked Democracy: Arti\fcial Intelligence for Democratic\nInnovation , 858:8, 2017.\nItzhak Yanovitzky and Joseph N Cappella. E\u000bect of call-in political talk ra-\ndio shows on their audiences: Evidence from a multi-wave panel analysis.\nInternational Journal of Public Opinion Research , 13(4):377{397, 2001.\nYini Zhang, Yidong Wang, Jordan Foley, Jiyoun Suk, and Devin Conathan.\nTweeting mass shootings: The dynamics of issue attention on social media.\nInProceedings of the 8th International Conference on Social Media & Society ,\npage 59. ACM, 2017.\n23", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Supplemental Information for Weaving it in: How Partisan Media React to Events", "author": ["C Vandeweerdt"], "pub_year": "2022", "venue": "NA", "abstract": "Recording of radio stations was an automated process, starting with scraping the publicly  available audio streaming URLs of US terrestrial radio stations from the website Radio-Locator"}, "filled": false, "gsrank": 475, "pub_url": "http://www.claravdw.com/wp-content/uploads/2021/05/talk_radio_article_appendix.pdf", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:sk0m1CRBKDYJ:scholar.google.com/&output=cite&scirp=474&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D470%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=sk0m1CRBKDYJ&ei=W7WsaJnZIo6IieoP0sKRuAk&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:sk0m1CRBKDYJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "http://www.claravdw.com/wp-content/uploads/2021/05/talk_radio_article_appendix.pdf"}}, {"title": "The demand for news: Accuracy concerns versus belief confirmation motives", "year": "2022", "pdf_data": "Anand Chopra, Felix; Haaland, Ingar; Roth, Christopher\nWorking Paper\nThe demand for news: Accuracy concerns versus belief\nconfirmation motives\nECONtribute Discussion Paper, No. 157\nProvided in Cooperation with:\nReinhard Selten Institute (RSI), University of Bonn and University of Cologne\nSuggested Citation: Anand Chopra, Felix; Haaland, Ingar; Roth, Christopher (2022) : The demand\nfor news: Accuracy concerns versus belief confirmation motives, ECONtribute Discussion Paper,\nNo. 157, University of Bonn and University of Cologne, Reinhard Selten Institute (RSI), Bonn and\nCologne\nThis Version is available at:\nhttps://hdl.handle.net/10419/262101\nStandard-Nutzungsbedingungen:\nDie Dokumente auf EconStor d\u00fcrfen zu eigenen wissenschaftlichen\nZwecken und zum Privatgebrauch gespeichert und kopiert werden.\nSie d\u00fcrfen die Dokumente nicht f\u00fcr \u00f6ffentliche oder kommerzielle\nZwecke vervielf\u00e4ltigen, \u00f6ffentlich ausstellen, \u00f6ffentlich zug\u00e4nglich\nmachen, vertreiben oder anderweitig nutzen.\nSofern die Verfasser die Dokumente unter Open-Content-Lizenzen\n(insbesondere CC-Lizenzen) zur Verf\u00fcgung gestellt haben sollten,\ngelten abweichend von diesen Nutzungsbedingungen die in der dort\ngenannten Lizenz gew\u00e4hrten Nutzungsrechte.Terms of use:\nDocuments in EconStor may be saved and copied for your personal\nand scholarly purposes.\nYou are not to copy documents for public or commercial purposes, to\nexhibit the documents publicly, to make them publicly available on the\ninternet, or to distribute or otherwise use the documents in public.\nIf the documents have been made available under an Open Content\nLicence (especially Creative Commons Licences), you may exercise\nfurther usage rights as specified in the indicated licence.\nFunding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under\nGermany \u00b4sExcellence Strategy \u2013EXC 2126/1 \u2013390838866 is gratefully acknowledged .www.econtribute.deECON tribute\nDiscussion Paper No. 157\nJune 2022 (updated version)F\nelix Chopra Ingar Haaland Christopher RothThe Demand for News: Accuracy Concerns \nversus Belief Confirmation Motives\nThe Demand for News: Accuracy Concerns\nversus Belief Con\ufb01rmation Motives\nFelix Chopra Ingar Haaland Christopher Roth *\nJune 22, 2022\nAbstract\nWe examine the relative importance of accuracy concerns and belief con\ufb01rmation\nmotives in driving the demand for news. In experiments with US respondents, we\n\ufb01rst vary beliefs about whether an outlet reports the news in a right-wing biased,\nleft-wing biased, or unbiased way. We then measure demand for a newsletter\ncovering articles from this outlet. Respondents only reduce their demand for\nbiased news if the bias is inconsistent with their own political beliefs, suggesting a\ntrade-off between accuracy concerns and belief con\ufb01rmation motives. We quantify\nthis trade-off using a structural model and \ufb01nd a similar quantitative importance\nof both motives. ( JELD83, D91, L82)\nKeywords : News Demand, Media Bias, Accuracy Concerns, Belief Con\ufb01rmation\n*We are very grateful for very helpful discussions with Jesse Shapiro which shaped our thinking about\nthe experimental design. We also thank Peter Andre, Roland B\u00e9nabou, Stefano DellaVigna, Armin Falk,\nMatthew Gentzkow, Salvatore Nunnari, Simon Quinn, Gautam Rao, Erik S\u00f8rensen, Bertil Tungodden,\nBasit Zafar, Florian Zimmermann, and participants at the 2022 Belief Based Utility Conference in\nAmsterdam for very helpful comments. Maximilian Fell, Sophia Hornberger, Apoorv Kanongo and\nEmir Kavukcu provided excellent research assistance. IRB approval was obtained from the German\nAssociation for Experimental Economic Research (GfeW) and the ethics committee of the University of\nCologne. The experiments were pre-registered in the AsPredicted registry (#78800, #80266, #87947, and\n#89081). Financial support from the Russell Sage Foundation (Small Awards in Behavioral Economics),\nthe Research Council of Norway through its Centre of Excellence Scheme (FAIR project No 262675),\nthe Institute on Behavior and Inequality (briq), and the German Research Foundation (DFG) through\nCRC TR 224 (Project B03) is gratefully acknowledged. Roth acknowledges funding by the Deutsche\nForschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy\n\u2013 EXC 2126/1-390838866. Chopra: University of Bonn, Felix.Chopra@uni-bonn.de; Haaland: NHH\nNorwegian School of Economics, Ingar.Haaland@nhh.no; Roth: University of Cologne and ECONtribute,\nroth@wiso.uni-koeln.de.\n1 Introduction\nMounting empirical evidence documents that news outlets often report the news in a\npolitically biased way (Gentzkow and Shapiro, 2010). Economic models differ in their\nexplanation for why media bias occurs in equilibrium. One class of models assumes\nthat readers value accuracy but also have a preference for news that distort signals\ntowards readers\u2019 prior beliefs (Mullainathan and Shleifer, 2005). A second class of\nmodels assumes that readers only value accuracy but instead face uncertainty about\nthe accuracy of news outlets (Gentzkow and Shapiro, 2006). This uncertainty leads\nreaders to attribute a higher accuracy to news outlets that provide signals that align with\nreaders\u2019 prior beliefs.\nThese two major theories thus make fundamentally different assumptions about\nthe relative importance of accuracy concerns versus belief con\ufb01rmation motives in\ndriving the demand for news. The relative importance of these two motives, in turn, has\nimportant implications for the optimal regulation of media markets, such as the welfare\neffects of regulations to increase competition. A major identi\ufb01cation challenge when\ntrying to quantify the relative importance of the two motives is that theories based on\nbelief con\ufb01rmation motives often make predictions that are observationally equivalent\nwith Bayesian updating about source quality (Gentzkow and Shapiro, 2006). This makes\nit challenging to quantify the relative importance of the two motives with naturally\noccurring data where beliefs about the media\u2019s reporting strategies are unobserved.\nTo solve the identi\ufb01cation challenge, we design experiments to directly vary beliefs\nabout the reporting strategy of a news outlet. We vary beliefs about whether a news\noutlet selectively reports the facts most favorable to either the Democratic Party (left-\nwing bias) or to the Republican Party (right-wing bias) or whether it reports all facts\nfrom an underlying report containing facts favorable to both parties (no bias). Since\nour respondents observe the full report available to the news outlet and the underlying\nsource of the report is \ufb01xed, the design allows our respondents to make direct inferences\nabout the outlet\u2019s reporting strategy. While theories based on accuracy concerns predict\nthat readers should decrease their demand for biased news irrespective of the direction\nof the bias, theories of belief con\ufb01rmation predict political heterogeneity based on the\ndirection of the bias.\nTo quantify the relative importance of accuracy concerns and belief con\ufb01rmation\nmotives in driving the demand for news, we conduct experiments with over 7,000\n1\nUS respondents using the online survey platform Proli\ufb01c. In our \ufb01rst experiment, we\nexperimentally vary beliefs about whether a news outlet is either right-wing biased\nor unbiased. To do so, we \ufb01rst tell our respondents that the Congressional Budget\nOf\ufb01ce (CBO), Congress\u2019s of\ufb01cial nonpartisan provider of cost and bene\ufb01t estimates for\nlegislation, published a report about the \u201cDemocrats\u2019 $15 Minimum Wage Bill\u201d (Raise\nthe Wage Act of 2021) in which it estimated that the plan would lift 900,000 people out\nof poverty (contradicting claims made by Republicans) and reduce employment by 1.4\nmillion jobs (contradicting claims made by Democrats). We next tell our respondents\nthat The Boston Herald wrote an article about the CBO \ufb01ndings.\nTo generate exogenous variation in perceptions of the reporting strategy, we use\nthe fact that The Boston Herald published two different articles about the bill: one\narticle published on February 26, 2021, that only cited the unemployment statistic, and\na second article published on March 2, 2021, that cited both statistics.1Our treatment\nvaries whether our respondents are informed about the reporting in the February 26\narticle that only cited the employment statistic ( right-wing bias treatment) while the\nremaining half of our respondents are informed about the reporting in the March 2\narticle that cited both statistics ( no bias treatment). We administer the treatments\nwithout referring explicitly to bias, selective reporting, or accuracy. To measure how\nthis treatment affects the demand for news, we offer all respondents the chance to sign\nup for a weekly newsletter that we created for the purpose of the experiment. The\nnewsletter features the top three articles about economic policy published in The Boston\nHerald and respondents who sign up for the newsletter receive weekly emails through\ntheir Proli\ufb01c account for one month. Our main outcome of interest is whether our\nrespondents sign up for the newsletter.\nOur second experiment uses an analogous design to shift beliefs about left-wing\nbias. We \ufb01rst inform our respondents that the CBO had published a report about\nthe \u201cRepublican Healthcare Plan\u201d (the American Health Care Act of 2017) in which\nit estimated that the plan would decrease the federal de\ufb01cit by over $100 billion\n(contradicting claims made by Democrats) and leave over 20 million more people\nuninsured (contradicting claims made by Republicans). We again exploit that The\nBoston Herald published two different articles that differed in their reporting: one\narticle about the Senate version of the bill that only cited the statistic on the number of\n1The Boston Herald is one of the oldest newspapers in the US and is based near Boston, MA. In 2020,\nits print edition had a circulation of about 25,000 and its reporting is considered slightly right-of-center.\n2\nuninsured, and one article about the House version of the bill that cited both statistics.\nThe key difference compared to the previous experiment relates to the direction of\nthe bias: half of our respondents are informed that The Boston Herald only cited the\nstatistic about the number of uninsured in its coverage of the Senate version of the plan\n(left-wing bias treatment) while the remaining half are informed that The Boston Herald\ncited both statistics in its coverage of the House version of the plan ( no bias treatment).\nIn our analysis of the results, we \ufb01rst con\ufb01rm that our treatments generate a signi\ufb01-\ncant \ufb01rst stage on perceptions of accuracy and political bias of the newsletter among\nboth Biden and Trump voters. In Experiment 1, both Biden and Trump voters in the\nright-wing bias treatment think that the newsletter has signi\ufb01cantly lower accuracy\nand is more right-wing biased compared to respondents in the no bias treatment. In\nExperiment 2, both Biden and Trump voters in the left-wing bias treatment think that the\nnewsletter has signi\ufb01cantly lower accuracy and is more left-wing biased compared to\nrespondents in the no bias treatment. The magnitudes of the \ufb01rst stage on accuracy and\nbias are economically signi\ufb01cant in both experiments. For instance, Biden and Trump\nvoters in the left-wing bias treatment think that the newsletter has between 54.2% to\n72% of a standard deviation lower accuracy than respondents in the no bias treatment.\nTurning to our main \ufb01ndings on newsletter demand, we \ufb01nd a striking political\nheterogeneity in treatment effects depending on the direction of the bias, consistent\nwith theories based on belief con\ufb01rmation motives. Speci\ufb01cally, the right-wing bias\ntreatment has a close to zero impact on newsletter demand among Trump voters. If\nanything, the right-wing bias treatment increases newsletter demand among Trump\nvoters by a non-signi\ufb01cant 0.5 percentage points (95% C.I. [-3.55,4.48]; p= 0.821). By\ncontrast, the left-wing bias treatment signi\ufb01cantly reduces newsletter demand among\nTrump voters by 5.2 percentage points (95% C.I. [-10.01,-0.41]; p= 0.033), corre-\nsponding to a 27.3% reduction in demand compared to the no bias group mean of\n19.1%. These patterns reverse for Biden voters who signi\ufb01cantly reduce their demand\nin response to the right-wing bias treatment by 8.6 percentage points (95% C.I. [-11.94,-\n5.33]; p< 0.001)\u2014corresponding to a 47.7% reduction in demand compared to the\nno bias group mean of 18.1%\u2014yet only reduce their demand by a non-signi\ufb01cant 2.6\npercentage points (95% C.I. [-6.37,1.17]; p= 0.176) in response to the left-wing biased\ntreatment. These asymmetric responses are consistent with readers having a preference\nfor belief con\ufb01rmation and inconsistent with models in which readers only care about\nthe accuracy of news. At the same time, we do not observe a signi\ufb01cant increase in\n3\nnews demand in any of the treatments, suggesting that readers also place some value on\nthe accuracy of news. Taken together, our results are thus in line with readers making a\ntrade-off between accuracy concerns and belief con\ufb01rmation motives.\nTo quantify the relative importance of accuracy concerns and belief con\ufb01rmation\nmotives in driving news demand, we use the experimental variation in conjunction with\na simple discrete-choice model. Intuitively, the model combines information about the\nrelative magnitude of the treatment effects on perceived accuracy and political bias with\ninformation about the magnitude of treatment effects on newsletter subscriptions to\nidentify the relative importance of the two motives. Our structural estimates suggest that\npreferences for belief con\ufb01rmation and accuracy concerns are of similar quantitative\nimportance for the demand for news in this context.\nTo shed more light on how our respondents interpreted our main treatment variation,\nwe conducted a separate mechanism experiment. In this experiment, we use open-ended\nquestions to elicit beliefs about the potential motives behind The Boston Herald\u2019s\nreporting of one statistic ( bias treatments) or the reporting of both statistics ( no bias\ntreatments) from the CBO reports. The unprompted responses reveal that respondents\nin the bias treatments have thoughts about political bias on top of their minds: 53.9%\nof respondents in the bias treatments mention political bias as the explanation for The\nBoston Herald selectively reporting only one statistic and no one mentions balanced\nreporting. By comparison, in the no bias treatments, 20.7% of respondents mention\nbalanced reporting and only 12.4% mention political bias. Our data also reveals that\nonly a very small fraction of respondents mention other potential motives underlying the\nselective reporting, such as entertainment, cognitive constraints, or rational delegation.\nThese results thus provide direct evidence that people intuitively interpret the action of\nselectively reporting only one statistic from the CBO reports as a clear sign of political\nbias and associate the action of reporting both statistics with balanced reporting. As\nsuch, this data supports the assumption from our structural model that our treatments\nmainly shifted beliefs about accuracy and bias.\nTo examine how people justify their demand for biased news, we collect direct data\non people\u2019s motives for subscribing to the newsletter at the end of the main experiments.\nTo get an unprompted response, we asked respondents to answer an open-ended question\non their motives for subscribing or not subscribing to the newsletter. Respondents in\ntheno bias treatments frequently mention getting accurate and unbiased news as a\nkey motive for signing up for the newsletter, while respondents in both of the bias\n4\ntreatments are signi\ufb01cantly less likely to mention such accuracy concerns and more\nlikely to provide a generic justi\ufb01cation, such as wanting to follow the news cycle or\ntheir interest in economic policy. These responses underscore that people do not invoke\njusti\ufb01cations that are consistent with alternative theories for why people consumed\nbiased news, such as diversi\ufb01cation or delegation motives. Rather, our \ufb01nding that\nrespondents in both of the bias treatments are signi\ufb01cantly less likely to mention\naccuracy concerns and more likely to provide generic justi\ufb01cations is consistent with\npeople providing rationales that allow them to maintain a positive self-image (Benabou\nand Tirole, 2006).\nOur results contribute to the literature on media bias (DellaVigna and Kaplan, 2007;\nDurante and Knight, 2012; Gentzkow et al., 2018; Mullainathan and Shleifer, 2005;\nPerego and Yuksel, 2022). To measure media bias, previous studies have developed\ntext-based measures that rank newspapers according to the similarity of their language\nto that of politicians (Gentzkow and Shapiro, 2006). For example, more frequent use of\nthe term \u201cdeath tax\u201d rather than \u201cestate tax\u201d might indicate a tendency to slant towards\nthe right. However, it is not obvious that one term conveys more information than the\nother. Thus, while previous studies suggest that readers have a demand for slanted\nlanguage (Garz et al., 2020; Gentzkow and Shapiro, 2010; Gentzkow et al., 2014), this\n\ufb01nding does not allow for strong conclusions about whether readers make a trade-off\nbetween accuracy concerns and belief con\ufb01rmation motives. Our main contribution is\nto provide direct evidence on the relative importance of accuracy concerns and belief\ncon\ufb01rmation motives in a clean and natural setting. The relative importance of these\nmotives plays a major role in theoretical analyses of media markets (Baron, 2006;\nChan and Suen, 2008; Gentzkow and Shapiro, 2006; Mullainathan and Shleifer, 2005)\nand is of critical importance for the debate on whether policymakers should introduce\nregulations to increase competition in media markets (Foros et al., 2015).\nWe also contribute to a literature on people\u2019s demand for information (Bursztyn et\nal., 2021; Capozza et al., 2021; Faia et al., 2021; Falk and Zimmermann, 2017; Fuster\net al., 2022; Ganguly and Tasoff, 2016; Montanari and Nunnari, 2019; Nielsen, 2020;\nZimmermann, 2015).2Chopra et al. (2022) examine how the demand for news changes\nin response to an added fact-checking service, demonstrating that fact-checking is not\nnecessarily an effective tool to reduce ideological segregation in news consumption.\n2More broadly our evidence relates to a literature on motivated belief updating (Exley, 2015;\nSchwardmann and van der Weele, 2019; Schwardmann et al., 2022; Di Tella et al., 2015; Thaler, 2019).\n5\nOur key contribution to the information demand literature is to identify the relative\nimportance of accuracy concerns and belief con\ufb01rmation motives in the news domain.\nTo differentiate between accuracy concerns and belief con\ufb01rmation motives, we employ\na new identi\ufb01cation strategy in which we vary beliefs about whether a news outlet\nreports the news in a right-wing biased, left-wing biased, or politically unbiased way.\nIn contrast to much of the previous experimental literature on information demand,\nwe vary perceptions of bias about a real-world news outlet rather than features of an\nabstract signal structure. Moreover, our main outcome provides high external validity\nby measuring subscriptions to a newsletter covering actual newspaper articles from a\nreal-world outlet.\nFinally, we contribute to a growing literature on structural behavioral economics\n(see DellaVigna, 2018, for a comprehensive review). Prior work has provided estimates\nof key behavioral parameters by combining parsimonious behavioral models with\nexperimentally-induced variation (Allcott and Taubinsky, 2015; Allcott et al., 2021;\nAugenblick et al., 2015; DellaVigna and Pope, 2018; DellaVigna et al., 2022). We use\nexogenous variation in perceptions of accuracy and bias in reporting to estimate the\nrelative importance of different motives in shaping people\u2019s demand for news using a\nparsimonious discrete choice model. Our estimates underline an important quantitative\nrole of both accuracy concerns and preferences for belief con\ufb01rmation in driving news\ndemand. An important bene\ufb01t of the structural estimation is that it provides greater\ncomparability with future studies that might try to quantify the relative importance of\naccuracy concerns compared to belief con\ufb01rmation motives in other settings.\nThe remainder of the paper proceeds as follows. Section 2 describes the exper-\nimental design. Section 3 presents both the reduced form results and the structural\nestimates. Section 4 presents evidence on psychological mechanisms and discusses al-\nternative mechanisms. Section 5 concludes. The Online Appendix provides a theoretical\nframework, additional empirical results, and the full set of experimental instructions.\n2 Experimental design\nOur study features two main experiments that examine how varying beliefs about the\naccuracy and political bias of a news outlet affect demand for a newsletter featuring\narticles from that outlet. Experiment 1 varies beliefs about whether a news outlet\n6\nselectively reports the facts most favorable to the Republican Party ( right-wing bias )\nwhile Experiment 2 varies beliefs about whether it selectively reports the facts most\nfavorable to the Democratic Party ( left-wing bias ). Figure 1 presents an overview of the\nmain design features and Section F of the Online Appendix presents the full instructions\nfor both experiments.\n[Insert Figure 1 here]\n2.1 Sample\nWe collected the data for our main experiments in collaboration with Proli\ufb01c, a leading\nmarket research company commonly used in social science research (Haaland et al.,\n2021). We collect data with Proli\ufb01c not only because of the high quality of responses\ncompared to other survey platforms (Eyal et al., 2021) but also because of the ability to\nemail respondents the newsletter via their Proli\ufb01c account without the need for collecting\nemail addresses. The data for our main experiments was collected in November and\nDecember 2021. We collected a sample of 1,464 Biden voters and 1,235 Trump voters\nfor Experiment 1 and 1,466 Biden voters and 849 Trump voters for Experiment 2.3\nOur samples are heterogeneous and resemble the US population in terms of several\nobservables (income, region, and gender; see Table B.1). In both experiments, the two\ntreatment groups are balanced in terms of observable characteristics in the full sample\n(Table B.2 and Table B.3).\n2.2 Experiment 1: Right-wing bias vs. no bias\nWe \ufb01rst describe the design of Experiment 1 in which we vary beliefs about whether a\nnews outlet selectively reports the facts most favorable to the Republican Party ( right-\nwing bias ) or reports facts favorable to both the Republican Party and the Democratic\nParty ( no bias ).\n3We aimed for gender-balanced samples of 1,500 Biden voters and 1,500 Trump voters in both\nexperiments. Respondents could only participate in one of the two experiments, making it especially\ndif\ufb01cult to recruit enough Trump voters in Experiment 2 (there are about six times as many Biden voters\nas Trump voters active on the Proli\ufb01c platform). In both experiments, the median time to complete the\nsurvey was about six minutes. We employed a simple attention check at the beginning of the survey,\nwhich over 95% of respondents pass, to screen out inattentive respondents.\n7\nBackground characteristics We \ufb01rst measure basic demographics, such as age,\ngender, education, income, and the region of residence. We then elicit whether our\nrespondents voted for Joe Biden or Donald Trump in the 2020 Presidential Election.4\nWe then measure their news consumption during the last 12 months, their interest in\neconomic news, and whether they currently subscribe to any newsletters.\nPre-treatment beliefs Subsequently, we elicit beliefs about how The Boston Herald\nreported about a CBO report containing facts favorable to both Democrats and Republi-\ncans. Speci\ufb01cally, we tell our respondents that the Congressional Budget Of\ufb01ce (CBO),\nCongress\u2019s of\ufb01cial nonpartisan provider of cost and bene\ufb01t estimates for legislation,\npublished a report about the \u201cDemocrats\u2019 $15 Minimum Wage Bill\u201d (Raise the Wage\nAct of 2021) in which it estimated that the plan would lift 900,000 people out of poverty\n(contradicting claims made by Republicans) and reduce employment by 1.4 million\njobs (contradicting claims made by Democrats).\nWe next tell our respondents that The Boston Herald wrote an article about the\neconomic impact of the $15 Minimum Wage Bill after the CBO published its report.\nWe then measure beliefs about how The Boston Herald covered the CBO \ufb01ndings by\nasking them to guess whether it only reported the statistic on the number of people lifted\nout of poverty (left-wing bias), only the statistic on the effects on reducing employment\n(right-wing bias), or both statistics (no political bias).\nBy making our respondents observe the full report available to the news outlet, our\ndesign allows our respondents to make direct inferences about its reporting strategy.\nWe chose to make the CBO the source of the underlying report for two reasons. First,\nthe CBO is known to be nonpartisan (to stay politically neutral, it only assesses the\nconsequences of proposed policies and does not make its own policy recommenda-\ntions). Second, all major newspapers in the US generally feature articles covering the\nCBO\u2019s evaluation of legislative proposals, making it a familiar and natural source for a\nnewspaper article.\n4When recruiting respondents on Proli\ufb01c, we pre-screen on having voted for either Donald Trump\nor Joe Biden. We ask about voting status in the survey to identify respondents who provide responses\ninconsistent with the screening criteria. Only a few respondents provided responses inconsistent with the\nscreening criteria, and we excluded these respondents from further analysis.\n8\nTreatments To generate exogenous variation in beliefs about selective reporting, we\nexploit the fact that The Boston Herald published two different articles about the $15\nMinimum Wage Bill: one article published on February 26, 2021, that only cited the\nunemployment statistic, and a second article published on March 2, 2021, that cited both\nstatistics.550% of our respondents are randomly assigned to learn about the selective\nreporting in the February 26 article that only mentioned the unemployment statistic\n(right-wing bias treatment). We frame the treatment information in a neutral way to\nminimize experimenter demand effects:\nThe article, published in The Boston Herald on February 26, 2021, reported\nthat the bill would reduce employment by 1.4 million jobs but not that it\nwould lift 900,000 people out of poverty.\nThe remaining 50% of respondents are assigned to learn about the balanced reporting\nin the March 2 article that reported both statistics ( no bias treatment):\nThe article, published in The Boston Herald on March 2, 2021, reported\nthat the bill would reduce employment by 1.4 million jobs andthat it would\nlift 900,000 people out of poverty.\nWe had two main reasons to select The Boston Herald as the news outlet for the\nexperiment. First, we wanted to feature a news outlet for which people had relatively\nweak priors compared to more popular news outlets, such as The New York Times or\nThe Wall Street Journal. Weaker priors about accuracy and political bias make beliefs\nabout the outlet\u2019s reporting strategy potentially more malleable to information about\npast reporting.\nSecond, we wanted an active control group design in which respondents would\nreceive different pieces of truthful information about how a news outlet covered the\nCBO \ufb01ndings. The Boston Herald was the only news outlet we identi\ufb01ed that had\nwritten multiple articles about the same CBO reports that also differed in whether or\nnot it selectively reported about the CBO \ufb01ndings. Active control group designs have\nseveral advantages compared to passive control group designs (Haaland et al., 2021).\n5See \u201cWho wins, who loses with higher minimum wage\u201d by Farren, Michael and Forzani, Agustin.\nThe Boston Herald , March 2, 2021, and \u201c$15 minimum wage hurts vulnerable workers the most\u201d by\nBuhajla, Stefani. The Boston Herald , February 26, 2021.\n9\nFirst, an active control group allows for a cleaner identi\ufb01cation of treatment effects\nbecause it holds more features of the environment constant compared to passive control\ngroup designs, such as respondents\u2019 attention and exposure to new information. In a\ndesign with a passive control group, respondents who do not learn about how the outlet\nreported about the CBO \ufb01ndings might be more curious to learn about the answer. That\nis, with a passive control group, curiosity motives could plausibly differ between the\ntreatment and control group, while these motives are less likely to differ in an active\ncontrol group design. Second, with an active control group, identi\ufb01cation does not\ndepend on people\u2019s prior beliefs, allowing us to identify causal effects of beliefs about\nselective reporting for a broader population. Furthermore, since prior beliefs are not\nexogenously assigned, interpretation of heterogeneous treatment effects is more dif\ufb01cult\nin designs with a passive control group.\nMain outcome measure: Newsletter demand After giving respondents differential\ninformation about whether The Boston Herald reported in a balanced or selective way\nabout the CBO \ufb01ndings, we measure demand for a weekly newsletter featuring stories\nfrom The Boston Herald:\nWe would like to offer you the opportunity to sign up for our weekly\nnewsletter.\nOur Weekly Economic Policy Newsletter will cover the top three arti-\nclesabout economic policy published in The Boston Herald .\nIf you say \u201cYes\u201d below, we will message you the newsletter on your Proli\ufb01c\naccount on a weekly basis over the next month.\nOur main outcome of interest is the binary decision to sign up for this newsletter.\nOur focus on newsletter subscriptions is motivated by the fact that newsletters are a\npopular way of staying informed about politics, with 21% of Americans receiving news\nfrom a newsletter over the course of a week (Newman et al., 2020). Moreover, by\nincluding only the three top articles in our newsletter, we reduce the expected cost of\nour respondents to stay up to date about economic policies\u2014both in terms of time costs\nand search efforts.\nOn the decision screen, we also clarify that the articles included in the newsletter can\nbe accessed for free by visiting The Boston Herald\u2019s website. To \ufb01x beliefs about the\n10\nresearchers\u2019 political leanings, we clarify that we are non-partisan academic researchers\nwho provide the newsletter as a free service for people to stay informed about the most\nimportant news related to economic policy. Finally, we explain that the newsletter is a\nnon-commercial product.\nIn practice, we sent the newsletter to our respondents on the Mondays of each\nof the four weeks after they decided to subscribe to the newsletter. A key advantage\nof conducting our experiment on Proli\ufb01c is that we can administer the newsletter to\nrespondents via direct messages on Proli\ufb01c without eliciting any personally identi\ufb01able\ninformation. Instead, respondents receive an email noti\ufb01cation when we message\nthem the newsletter. This, in turn, ensures that we can measure newsletter demand\nirrespective of privacy concerns. Appendix Section E provides information about the\nlogistical details and the newsletter\u2019s design.6\nPost-treatment beliefs about accuracy and political bias of the newsletter After\nchoosing whether to subscribe to the newsletter, we measure post-treatment beliefs\nabout the accuracy and political bias of the newsletter. We also elicit perceptions about\nthe trustworthiness, entertainment value, quality, and complexity of the newsletter. We\nmeasure these beliefs using \ufb01ve-point Likert scales.\n2.3 Experiment 2: Left-wing bias vs. no bias\nIn Experiment 2, we vary beliefs about whether a news outlet selectively reports the fact\nmost favorable to the Democratic Party ( left-wing bias ) or reports facts favorable to both\nthe Republican Party and the Democratic Party ( no bias ). The design of this experiment\nclosely resembles the design of Experiment 1, and most questions and outcomes are\nidentical across the two experiments. We highlight the key design differences below\n(see also Figure 1).\nPre-treatment beliefs We measure beliefs about how The Boston Herald reports\nabout the \u201cRepublican Health Care Plan\u201d (the American Health Care Act of 2017).\nRespondents are told that the CBO estimated that the plan would decrease the fed-\n6Each week we received a large number of thank you messages from respondents. A much smaller\nnumber of subscribers wrote to us that they would like to unsubscribe from the newsletter. Overall, this\nfeedback from subscribers illustrates both the bene\ufb01ts and costs of receiving the newsletter.\n11\neral de\ufb01cit by $119 billion (contradicting claims made by Democrats) and leave 23\nmillion more people uninsured (contradicting claims made by Republicans). 50% of\nrespondents are asked about their beliefs about the Senate version of the Republican\nHealthcare Plan, while the remaining 50% are asked about the House version of the\nRepublican Healthcare Plan.7This design choice is motivated by the fact that The\nBoston Herald reported different CBO statistics for these two versions of the Republican\nHealth Care Plan, as explained below.\nTreatments The Boston Herald published two articles about the Republican Health-\ncare Plan. In the article about the Senate version of the Republican Healthcare Plan,\nThe Boston Herald reported only that the plan would leave over 20 million more people\nuninsured ( left-wing bias treatment). In the other article about the House version of\nthe Republican Healthcare Plan, The Boston Herald reported both CBO statistics ( no\nbias treatment).8In our design, 50% of respondents are randomly assigned to learn\nabout the coverage of the article that only mentioned the consequences on the number\nof uninsured people ( left-wing bias treatment), which we again frame in a neutral way\nto minimize experimenter demand effects:\nThe Boston Herald article about the Senate Republican Healthcare Plan\nreported that the plan would leave over 20 million more people uninsured\nbut not that it would decrease the de\ufb01cit by over $100 billion.\nThe remaining 50% of respondents learn about the article that mentioned both statistics\n(no bias treatment):\nThe Boston Herald article about the House Republican Healthcare Plan\nreported that the plan would leave over 20 million more people uninsured\nandthat it would decrease the de\ufb01cit by over $100 billion.\nNewsletter and post-treatment beliefs We then employ the same main outcome\nvariable as in Experiment 1, namely the binary decision to subscribe to a newsletter\n7Prior beliefs about reporting are virtually identical for the Senate version and the House version of\nthe Republican Healthcare Plan (as shown in Figure B.1).\n8See \u201cCBO: 22 million more uninsured by 2026 under Senate health bill\u201d (Associated Press),\npublished in The Boston Herald , June 26, 2017, and \u201cCBO House GOP health bill projection: 23 million\nmore uninsured\u201d (Associated Press), published in The Boston Herald , May 24, 2017.\n12\nfeaturing the three top stories about economic policy from The Boston Herald. We also\nmeasure post-treatment beliefs about accuracy and political bias as well as other beliefs\nabout newsletter characteristics as in Experiment 1.\n2.4 Hypotheses\nOur design allows us to study whether and how people trade off the accuracy of news\nagainst the political bias in reporting by testing the predictions of three classes of models:\n(i) models where people only care about accuracy, (ii) models where people only care\nabout belief con\ufb01rmation, and (iii) models where both accuracy and belief con\ufb01rmation\nmotives shape the demand for news. To \ufb01x ideas, let Yg\nidenote the demand for news in\ntreatment arm i2fL;N;Rgand political group g2fB;Tg, where Brepresents Biden\nvoters, Trepresents Trump voters, and L,NandRdenote the left-wing bias ,no bias\nandright-wing bias treatment arm, respectively.\nFirst, we consider models where the demand for news only depends on the perceived\ninformativeness (e.g. Gentzkow and Shapiro, 2006). These models would predict that\nthe demand for news is strictly larger in the no bias treatment arm compared to the\nother treatment arms, i.e., Yg\nL<Yg\nNandYg\nR<Yg\nNfor both political groups g2fB;Tg(as\nshown in Section A of the Online Appendix). The intuition underlying this observation\nis that the right-wing and the left-wing bias treatment increase the perceived likelihood\nof selective reporting in a setting where full information disclosure would have been\npossible.9\nSecond, we turn to models where the demand for news is driven only by belief\ncon\ufb01rmation motives (as discussed in Loewenstein and Molnar, 2018). The predictions\nof such models depend on the political beliefs of our respondents. Speci\ufb01cally, models\nof belief con\ufb01rmation assume that Biden voters have a preference for reading left-wing\nbiased news, while Trump voters have a preference for reading right-wing biased news.\nIn short, demand should increase whenever the perceived political bias moves towards\nthe political belief of our respondents. We would thus expect YB\nL>YB\nN>YB\nRamong\nBiden voters, and the opposite pattern YT\nL<YT\nN<YT\nRamong Trump voters.\n9While it seems reasonable that reporting both statistics is normatively better than selectively\nreporting only one statistic in our context, it is important to emphasize that there is in general no\nnormative benchmark for how to select which facts to report when full disclosure is notpossible (e.g.,\nwhen a report includes many different statistics and a news outlet by necessity have to engage in selective\nreporting).\n13\nWe \ufb01nally turn to models in which people make a trade-off between accuracy and\nbelief con\ufb01rmation motives (e.g. Mullainathan and Shleifer, 2005). When the bias in\nreporting is not aligned with respondents\u2019 political views, we obtain the unambiguous\nprediction that YB\nR<YB\nNandYT\nL<YT\nNbecause there is no con\ufb02ict between accuracy and\nbelief con\ufb01rmation motives. However, such a con\ufb02ict arises whenever the alignment\nbetween respondents\u2019 political views and the perceived political bias in reporting\nincreases at the cost of lower accuracy in reporting. The sign of the overall effect on\nthe demand for news depends on (i) the relative importance of accuracy compared to\nbelief con\ufb01rmation motives, and (ii) the underlying magnitude of \ufb01rst-stage changes\nin perceptions of accuracy and bias in reporting. Without knowing these quantities,\nthe comparison between YB\nLandYB\nNand the comparison between YT\nRandYT\nNare thus\nambiguous. Note that if both motives are equally important drivers of the demand for\nnews, one would expect similar levels of demand in these cases: YB\nL\u0019YB\nNandYT\nR\u0019YT\nN.\nAppendix Table B.4 provides a summary of the predictions.\n3 Main results\nThis section presents our main results. We \ufb01rst present evidence on the \ufb01rst stage\nof the treatment on perceptions of accuracy and the political bias of the newsletter.\nWe then present the main treatment effects on demand for the newsletter. We \ufb01nally\nuse a discrete choice model to estimate the relative importance of accuracy concerns\ncompared to belief con\ufb01rmation motives.\n3.1 Beliefs about the accuracy and political bias of the newsletter\nTable 1 shows treatment effects on beliefs about the accuracy and political bias of the\nnewsletter separately for Trump voters (Panel A) and Biden voters (Panel B). Columns\n1 and 4 show that Trump voters in the right-wing bias treatment think that the newsletter\nhas 16.5% of a standard deviation lower accuracy ( p= 0.003) while Trump voters in\ntheleft-wing bias think that the newsletter has 54.2% of a standard deviation lower\naccuracy ( p< 0.001). We also observe treatment heterogeneity in accuracy perceptions\namong Biden voters: Biden voters in the right-wing bias treatment think that the\nnewsletter has 90.3% of a standard deviation lower accuracy ( p< 0.001) while Biden\n14\nvoters in the left-wing bias treatment think that the newsletter has 72% of a standard\ndeviation lower accuracy ( p< 0.001).10The political heterogeneity in treatment effects\non accuracy perceptions is consistent with the mechanism in Gentzkow and Shapiro\n(2006) and motivates our structural approach (outlined in Section 3.3) that accounts for\nheterogeneous treatment effects on perceptions.\nWe next examine treatment effects on perceptions of political bias. Column 2\nof Table 1 shows that Trump voters in the right-wing bias treatment think that the\nnewsletter has 49% of a standard deviation lower left-wing bias ( p< 0.001) while\nTrump voters in the left-wing bias treatment think that the newsletter has 26.6% of a\nstandard deviation higher left-wing bias ( p< 0.001). Biden voters in the right-wing\nbias treatment think that the newsletter has 84.9% of a standard deviation lower left-\nwing bias ( p< 0.001) while Biden voters in the left-wing bias treatment think that the\nnewsletter has 30.5% of a standard deviation higher left-wing bias ( p< 0.001).\nOur experiments thus generate situations in which perceptions of accuracy always\ndecrease but in which perceptions of political bias move in opposite directions. Experi-\nment 1 creates a potential con\ufb02ict between accuracy concerns and belief con\ufb01rmation\nmotives for Trump voters but not for Biden voters. Conversely, Experiment 2, creates a\npotential con\ufb02ict between accuracy concerns and belief con\ufb01rmation motives for Biden\nvoters but not for Trump voters. This exogenous variation in accuracy and political bias\nallows us to test for the presence of belief con\ufb01rmation motives in the demand for news.\n3.2 Reduced form results on newsletter demand\nColumns 3 and 6 of Table 1 present treatment effects on the demand for the newsletter in\nExperiment 1 and 2, respectively (Figure 2 displays these treatment effects graphically\nwithout control variables). As shown in Panel A of Table 1, we \ufb01nd no statistically\nsigni\ufb01cant effect of the right-wing bias treatment on newsletter demand among Trump\nvoters in Experiment 1. If anything, the treatment increases newsletter demand among\nTrump voters by 0.5 percentage points (95% C.I. [-3.55,4.48]; p= 0.821). However,\nwhile the point estimate is close to zero and not statistically signi\ufb01cant, the con\ufb01dence\n10Table B.5 shows that treatment effects on accuracy perceptions are robust to using conceptually\nrelated outcomes: both Biden and Trump voters assigned to the bias treatments display lower trust in the\nnewsletter and associate it with lower quality. On top of this, the \ufb01rst stage on accuracy perceptions looks\nvery similar if we construct an \u201caccuracy index\u201d combining the accuracy, quality, and trust outcomes.\n15\ninterval is consistent with economically signi\ufb01cant changes in demand in both directions.\nIn Experiment 2, the left-wing bias treatment signi\ufb01cantly reduces newsletter demand\namong Trump voters by 5.2 percentage points (95% C.I. [-10.01,-0.41]; p= 0.033),\ncorresponding to a 27.3% reduction in demand compared to the no bias group mean of\n19.1%.11\nThese patterns reverse for Biden voters. As shown in Panel B of Table 1, in contrast\nto the muted effects of the right-wing bias treatment among Trump voters, Biden voters\nsigni\ufb01cantly reduce their demand for the newsletter by 8.6 percentage points in response\nto the right-wing bias treatment (95% C.I. [-11.94,-5.33]; p< 0.001), corresponding to\na 47.7% reduction in demand compared to the no bias group mean of 18.1%. However,\nin response to the left-wing bias treatment, Biden voters only reduce their demand by a\nnon-signi\ufb01cant 2.6 percentage points (95% C.I. [-6.37,1.17]; p= 0.176).12\nThe political heterogeneity in treatment effects, in which our respondents only\nsigni\ufb01cantly reduce their demand for biased news if the bias is inconsistent with their\nown political beliefs, is inconsistent with models in which readers only care about the\naccuracy of news (as discussed in Appendix Section A). At the same time, that we do\nnot observe a signi\ufb01cant increase in demand for the newsletter in any of the treatments\nsuggests that our respondents also care about the accuracy of news. Taken together, our\nresults are thus in line with behavioral models where readers face a trade-off between\naccuracy concerns and belief con\ufb01rmation motives. Our \ufb01rst main result follows.\nResult 1. People strongly reduce their demand for biased news, but only if the political\nbias in reporting is inconsistent with their own political beliefs.\n[Insert Figure 2 here]\n[Insert Table 1 here]\n3.3 Structural estimates of preference parameters\nOur reduced form results suggest that people\u2019s demand for news are driven by both\naccuracy concerns and belief con\ufb01rmation motives, but they do not allow us to quantify\ntherelative importance of these motives. In this section, we \ufb01ll this gap by using the\nexogenous variation in perceptions of accuracy and bias induced by our treatments\n11Thep-value for a test of equality of treatment effects across experiments for Trump voters is 0.072.\n12Thep-value for a test of equality of treatment effects across experiments for Biden voters is 0.017.\n16\nto estimate a parsimonious discrete choice model. This structural model allows us to\ncombine the quantitative information on the effects of the treatments on both accuracy\nand bias perceptions alongside with our quantitative estimates on the effects on news\ndemand.\nDiscrete choice model Agent ihas to decide whether to subscribe to our newsletter\n(yi=1)or not ( yi=0). The agent will subscribe to the newsletter if his expected utility\nuifrom subscribing is positive, such that yi=1(ui\u00150). Following Mullainathan and\nShleifer (2005), we focus on the trade-off between accuracy and belief con\ufb01rmation,\nand thus assume that the agent\u2019s expected utility from subscribing to the newsletter\nconsists of a component capturing a preference for accuracy in reporting, a component\ncapturing a preference for belief con\ufb01rmation, and the price of the newsletter. As we\noffer the newsletter free of charge, the expected utility is\nui=\u00afu+asi+bbi+ei (1)\nwhere si=Ei(\u02dcsi)is the agent\u2019s subjective belief about the newsletter\u2019s accuracy; bi=\nEi(\u02dcbi)is the agent\u2019s subjective belief about how much the newsletter will con\ufb01rm his\nprior beliefs; and eiis a random taste shock. The parameters aandbcapture the agent\u2019s\nwillingness to trade off accuracy against belief con\ufb01rmation. As we elicit subjective\nbeliefs about accuracy and belief con\ufb01rmation in our experiment, we will directly\nsubstitute them for siandbiin our structural estimation of the above utility function.13\nEstimation and identi\ufb01cation We estimate the model parameters, q= (a;b;\u00afu),\nboth for the full sample as well as separately for Biden and Trump voters to explore\nheterogeneity in preferences. As proxies for siandbi, we use the z-scored post-treatment\nbelief measures of perceived accuracy and political bias in reporting.14In particular, we\nrecode the perceived political bias such that larger values correspond to a stronger left-\n13We are deliberately agnostic about the underlying information-theoretic decision problem giving\nrise to a potential preference for accuracy because revealed preferences in our experiment should be a\nfunction of respondents\u2019 subjective beliefs about the accuracy and expected belief con\ufb01rmation of the\nnewsletter. However, one possibility is that the agent has to learn about the state of the economy w, and\ntake a subsequent action aiwith a payoff v(ai;w) =\u0000a(ai\u0000w)2after reading the newsletter n(or not),\nwhich would give rise to a demand for accuracy in reporting.\n14We normalize these measures to have a mean of zero and a standard deviation of one among\nrespondents in the no bias treatment arms. The results are robust to using the non-z-scored beliefs.\n17\nwing (right-wing) bias for Biden (Trump) voters. Next, if perceptions of accuracy and\nbias were uncorrelated with the error term, one could simply use newsletter subscription\nchoices and the belief data from Experiments 1 and 2 to estimate the parameters q\nusing a Probit model. However, this exclusion restriction is unlikely to hold in practice\nwithout relying on an exogenous shifter. We therefore estimate an IV probit model as\noutlined by the set of equations below in which the binary dependent variable yiis the\ndecision to sign up to our newsletter:\nyi=1(\u00afu+asi+bbi+ei\u00150) (2)\nsi=Z0\nigs+e0\ni (3)\nbi=Z0\nigb+e00\ni (4)\nHere, we instrument respondents\u2019 perceptions with a saturated set of treatment arm\nindicators, Zi(see equations 3 and 4). We use Stata\u2019s ivprobit routine to estimate\nthe parameters of interest using the ef\ufb01cient estimator proposed by Newey (1987). In\nspeci\ufb01cations where we pool both Democrats and Republicans, the set of instruments,\nZi, also includes interactions between the treatment arm indicators and a binary indicator\nfor whether the respondent voted for Trump to account for heterogeneous \ufb01rst-stage\neffects on beliefs across political groups. We also include a binary indicator for whether\nthe respondent voted for Trump as a control variable to allow for differences in the\noutside option ( \u00af u) across political groups in the pooled speci\ufb01cation.\nThe main advantage of this estimation strategy is that we exploit only exogenous\nvariation in perceptions to disentangle people\u2019s accuracy and belief con\ufb01rmation mo-\ntives: While the bias treatments in both experiments decrease the perceived accuracy\nrelative to the no bias treatment, the right-wing bias treatment in Experiment 1 shifts\nthe perceived bias to the right, while the left-wing bias treatment in Experiment 2 shifts\nthe perceived bias to the left.\nDiscussion of assumptions First, we focus only on the accuracy-belief con\ufb01rmation\ntrade-off. While demand for our newsletter could also re\ufb02ect other motives\u2014such as\na demand for entertainment\u2014our estimation strategy remains valid if these motives\ndo not differentially affect demand across treatment arms.15Second, we assume\n15The open-ended data from the mechanism experiment (Experiment 3), which we present in Section\n4.2, suggests that the treatment indeed mostly sparked thoughts about bias and accuracy and furthermore\n18\nthat there is no internal saturation point in terms of the newsletter\u2019s political bias.\nAn alternative approach would be to assume that people receive disutility from the\ndifference between their preferred level of media bias, b\u0003, and the perceived bias of\nthe newsletter,\u0000bjb\u0000b\u0003j. This is equivalent to equation (1) if b\u0003\nD\u0014b\u0014b\u0003\nR, i.e.,\nwhenever the newsletter is perceived to be more centric than the preferred level of\nbias among Biden voters ( b\u0003\nD) and Republicans ( b\u0003\nR). In practice, we expect this to\nhold as our respondents perceive The Boston Herald as a relatively unbiased news\noutlet to begin with. Third, we assume that our survey measures of accuracy and bias\ncapture underlying perceptions well and are comparable to each other. We designed\nour survey measures to be as comparable as possible by eliciting them both on the\nsame type of scale with the same number of response options. In addition, we only use\nz-scored perceptions in our estimation to further ensure the comparability of our survey\nmeasures.\nResults Table 2 presents the parameter estimates of the discrete choice model. Con-\nsistent with the predictions of standard models, the estimates using the full sample\nsuggest a preference for accurate news ( p< 0.01, column 1). At the same time, the\nmodel estimates suggest that the demand for news is also driven by a preference for\nbelief con\ufb01rmation ( p< 0.01, column 1), which corroborates our reduced form re-\nsults. Indeed, our estimates imply a relative weight on accuracy of a=(a+b) =\n0:241=(0:241+0:345) =0:412, and we cannot reject the null hypothesis that respon-\ndents assign equal weights to accuracy and belief con\ufb01rmation ( p> 0.10). Columns 2\nand 3 of Table 2 examine heterogeneity in preferences between Biden and Trump voters.\nAmong Biden voters, we again \ufb01nd both a preference for accurate news ( p< 0.01) as\nwell as a preference for belief con\ufb01rmation ( p< 0.01, column 2). The estimates for\nTrump voters are qualitatively similar but more noisily estimated. If anything, we \ufb01nd\nthat Biden voters assign a smaller weight to accuracy compared to belief con\ufb01rmation\nmotives than Trump voters. However, we cannot reject the null hypothesis that both\ngroups care equally about accuracy and belief con\ufb01rmation ( p> 0.10). Our second\nmain result can thus be summarized as follows:\nResult 2. Both accuracy and belief con\ufb01rmation motives are important drivers of the\ndemand for news, and our model estimates suggest that people assign about equal\ndid not trigger many thoughts related to entertainment, cognitive constraints, or other features of news\narticles. This motivates an approach that focuses only on perceptions of accuracy and political bias.\n19\nweight to both motives in the context of our experiment.\n[Insert Table 2 here]\nRobustness We obtain similar structural estimates across a series of robustness checks.\nFirst, we obtain similar results when we re-estimate the model without z-scoring beliefs\nabout accuracy and belief con\ufb01rmation (as shown in Table B.6), which suggests that\nthis normalization procedure does not affect our estimates of the relative importance of\naccuracy concerns and belief con\ufb01rmation motives. Second, our results are robust to\nreplacing the z-scored accuracy belief measure with an index based on post-treatment\nbeliefs about accuracy, quality, and trustworthiness (as shown in Table B.7). Third, we\nalso estimate an analogous linear probability model using a two-stage least-squares\nestimator where we again use our treatment assignments as instruments (Angrist and\nKrueger, 1995; Inoue and Solon, 2010). Panel A of Table B.8 shows that we obtain\nquantitatively very similar estimates of the relative importance of accuracy compared to\nbelief con\ufb01rmation motives using a linear probability model. Thus, while the choice of\na linear versus a non-linear second stage model affects the scale of the coef\ufb01cients, the\nimplied relative magnitudes are quantitatively robust across speci\ufb01cations. Fourth, we\nmitigate concerns about consistency bias in survey responses affecting our structural\nestimates (Falk and Zimmermann, 2015). The results from a robustness exercise\naddressing this concern are presented in Panel B of Table B.8. Appendix Section C\nprovides more details about this concern and how we address it. Fifth, we examine\nwhether a preference for accuracy in combination with, (i), a preference for simplicity\nin reporting or, (ii), a preference for entertainment could also explain the treatment\neffects with plausible parameter values. To do so, we repeat our main estimation\nbut replace our measure of belief con\ufb01rmation with a measure of, (i), the perceived\nsimplicity (reverse-coded complexity belief) or, (ii), perceived entertainment value\nof the newsletter. Table B.9 provides the results from this exercise. The coef\ufb01cients\nare mostly statistically insigni\ufb01cant and unstable in sign across political subgroups.\nThis indicates that it is dif\ufb01cult to rationalise the patterns of treatment effects based on\npreferences for simplicity or entertainment on top of a preference for accuracy.\nTaken together, these \ufb01ve additional checks underscore the robustness of the main\n\ufb01ndings from the structural model.\n20\n4 Psychological mechanisms and robustness\nIn this section, we provide evidence on psychological mechanisms underlying our\ntreatment effects and discuss some potential alternative mechanisms.\n4.1 Motives for news demand\nOur experimental \ufb01ndings and our model-based preference estimates suggest that both\nBiden and Trump voters have a preference for reading like-minded news that sometimes\ncon\ufb02icts with their desire for accuracy in reporting. To examine how people justify\ntheir demand for news, we collect direct data on people\u2019s motives for subscribing to the\nnewsletter at the end of the main experiments. To get an unprompted response, we asked\nour respondents to answer an open-ended question on their motives for subscribing or\nnot subscribing to the newsletter (the full instructions are provided in Section F.1.4 of\nthe Online Appendix). This data provides us with a direct lens into people\u2019s reasoning\nabout the motives underlying their subscription decision.\nText analysis In a \ufb01rst step, we use the methodology proposed by Gentzkow and\nShapiro (2010) to identify phrases that are characteristic of responses to the open-ended\nquestions of subscribers and non-subscribers across treatment arms. Speci\ufb01cally, given\ntwo groups AandBof respondents, we calculate Pearson\u2019s c2statistic for each word w,\nc2\nwAB=(fwAf\u0018wB\u0000fwBf\u0018wA)2\n(fwA+fwB)(fwA+f\u0018wA)(fwB+f\u0018wB)(f\u0018wA+f\u0018wA)(5)\nwhere fwAandfwBdenote the total number of times that the word wwas mentioned by\nrespondents in group AandB, respectively. Similarly, f\u0018wAandf\u0018wBrefer to the total\nnumber of times words other than wwere mentioned. We then focus on the words with\nthe largest c2.\nFigure 3 presents the 50 words with the largest c2statistic for subscribers (Panel\nA) and non-subscribers (Panel B). Words that are more characteristic of justi\ufb01cations\nprovided by respondents in the left-wing orright-wing biased treatments are shown in\ngreen, while words more characteristic of respondents in the unbiased treatments are\nshown in red. Panel A reveals that words related to \u201cunbiased\u201d are more diagnostic for\nsubscribers in the no bias treatments, while subscribers in the bias treatments avoid\n21\nusing terms related to bias or accuracy. Panel B shows that these patterns are reversed for\nnon-subscribers: Non-subscribers in the bias treatments justify their non-subscription\nwith the political bias of the newsletter, while respondents in the no bias treatment arms\nagain bring up more generic reasons, such as wanting to follow the news cycle or their\ninterest in economic policy.\n[Insert Figure 3 here]\nFrequency of mentioning bias Motivated by the previous \ufb01ndings, we more closely\nexamine respondents\u2019 tendency to justify their decision by referring to the political\nbias of the newsletter. Table 3 presents OLS regression estimates pooling respondents\nfrom Experiments 1 and 2. The dependent variable in columns 1\u20133 is a binary indicator\ntaking value one if respondents mention the word \u201cunbiased\u201d or any of its synonyms\nin their responses to the open-ended question.16Subscribers who were assigned to\ntheleft-wing bias or the right-wing bias treatment arms are 4.1 percentage points less\nlikely to utilize synonyms of \u201cunbiased\u201d (column 1, p= 0.013), a substantial effect\ncompared to a no bias group mean of 7.8%. On the other hand, respondents in the\nbias treatments who did not subscribe to our newsletter are marginally more likely to\nmention synonyms of \u201cunbiased\u201d in their responses (column 2, p= 0.051). The opposite\npattern emerges once we consider synonyms of \u201cbiased\u201d and construct an analogous\ndependent variable taking value one if respondents utilized any of the following words:\n\u201cbiased\u201d, \u201cpartisan\u201d, \u201ctendentious\u201d, or \u201cslanted.\u201d Column 4 shows that subscribers are\nnot signi\ufb01cantly more likely to mention synonyms of \u201cbiased.\u201d Yet, non-subscribers\nare 4.4 percentage points more likely to mention terms related to \u201cbiased\u201d (column\n5,p< 0.001), which is a substantial effect compared to the no bias group mean of\n1.9%. Our data thus suggests that our treatments either affect the composition of\nrespondents selecting into the newsletter subscription, or that respondents \ufb02exibly\nadjust their rationales for subscription in response to our treatments (see Bursztyn et\nal., 2022a,b, for evidence on the role of rationales in justifying socially stigmatized\nbehavior). People\u2019s rationales for choosing to consume biased news do not actively\nfeature the political bias of the newsletter, consistent with people providing rationales\n16The synonyms were obtained from the website thesaurus.com and include: \u201cdisinterested\", \u201cdispas-\nsionate\", \u201cequitable\", \u201chonest\", \u201cimpartial\u201d, \u201cneutral\u201d, \u201cnonpartisan\u201d, \u201copen-minded\u201d, \u201caloof\u201d, \u201ccold\u201d,\n\u201cequal\u201d, \u201ceven-handed\u201d, \u201cfair\u201d, \u201cnondiscriminatory\u201d, \u201cobjective\u201d, \u201con-the-fence\", \u201cstraight\u201d, \u201cunbigoted\u201d,\n\u201cuncolored\u201d, \u201cuninterested\u201d, \u201cunprejudiced.\u201d\n22\nthat allow them to maintain a positive self-image (Benabou and Tirole, 2006).\n[Insert Table 3 here]\n4.2 Mechanism experiment: Interpretation of treatment\nTo shed light on the psychological mechanisms, we measure respondents\u2019 thoughts\nabout the motives behind different reporting decisions by the news outlet. For this pur-\npose, we conducted an additional pre-registered experiment on Proli\ufb01c (Experiment 3;\nsee Table B.10). The experiment was conducted in February 2022 with 388 respondents\n(240 Biden voters and 148 Trump voters).17\nDesign Half of the respondents are informed that the CBO evaluated the consequences\nof the \u201c$15 Minimum Wage Bill\u201d while the remaining half of the respondents are\ninformed that the CBO evaluated the consequences of the \u201cRepublican Healthcare\nPlan.\u201d We also tell our respondents about the competing claims made by Democrats\nand Republicans about the respective plans. We then randomly assign respondents to\nthe same bias andno bias treatments on the respective plans as described in sections\n2.2 and 2.3. We then elicit people\u2019s thoughts on why The Boston Herald reported only\none statistic (in the bias treatment) or both statistics (in the no bias treatment) using\nopen-ended text responses. To ensure high levels of effort, we ask our respondents to\nwrite two to three sentences. For example, respondents assigned to the $15 Minimum\nWage Bill and the right-wing bias treatment were asked:\nWhy do you think that The Boston Herald reported that the bill would\nreduce employment by 1.4 million jobs but not that it would lift 900,000\npeople out of poverty?\nRespondents assigned to the Republican Healthcare Plan received analogous instruc-\ntions tailored to that plan (see Section F of the Online Appendix for the instructions).\n17The median response time was four minutes and we excluded respondents from previous experi-\nments. We aimed for a politically balanced sample of Trump and Biden voters but we found it challenging\nto recruit enough Trump voters after excluding previous survey respondents from participation. As noted\npreviously, there are about six times as many Biden voters as Trump voters active on the Proli\ufb01c platform.\n23\nHand-coded data We hand-code the open-ended responses about the reporting strat-\negy using a pre-speci\ufb01ed procedure. We assign each response to one of the following\nthree categories: First, if respondents mention that the outlet was politically biased, we\nassign them to the \u201cbias\u201d category (for instance, the following example responses were\nclassi\ufb01ed as \u201cbiased\u201d: \u201cI think it\u2019s biased reporting,\u201d \u201cPerhaps they are a Republican\nnewspaper,\u201d \u201cI believe it is a left-leaning newspaper,\u201d or \u201cThey clearly support the\nDemocrats\u201d). Second, if respondents mention that the newspaper was trying to provide a\nbalanced view of the facts, we assign them to the \u201cbalanced\u201d category (for instance, the\nfollowing example responses were classi\ufb01ed as \u201cbalanced\u201d: \u201cThey were probably trying\nto report fairly without bias,\u201d \u201cThey were trying to give the full picture,\u201d and \u201cThey\ntried to report fairly and accurately\u201d would all be classi\ufb01ed). Third, all other responses\nare assigned the \u201cother\u201d category. In addition to the pre-speci\ufb01ed categories, we also\ncategorized responses that mentioned motives related to entertainment, complexity, or\nrational delegation.\nResults based on hand-coded data Figure 4 shows that respondents assigned to\nthebias treatments are 41.1 percentage points more likely to refer to political bias\n(p< 0.001) compared to a mean of 12.4% in the no bias treatments. Respondents\nassigned to the no bias treatment, on the other hand, are 20.1 percentage points more\nlikely to talk about balanced reporting ( p< 0.001) compared to a mean of 0% in\nthebias treatments. These effects are both statistically and economically signi\ufb01cant\nand highlight that respondents interpret the reporting decision to be either driven by\nmotives to deliver accurate or biased reporting.18Respondents\u2019 unprompted responses\nalso reveal that other perceived motives, such as rational delegation, entertainment, or\ncognitive constraints, only play a very minor role. Only four out of 388 respondents\nprovide responses consistent with rational delegation in which the newspaper selectively\nreports statistics considered more important by their readers. Another three respondents\nmention entertainment motives. Finally, two respondents thought the selective reporting\nwas motivated to reduce the complexity of the reporting. These \ufb01ndings thus corroborate\nthe idea that our experiment is well-suited to quantify the relative importance of accuracy\nconcerns and belief con\ufb01rmation motives in driving the demand for news.\n[Insert Figure 4 here]\n18We \ufb01nd consistent patterns for whether people mentioned balanced reporting in their open-ended\nresponses (as shown in Figure B.2).\n24\nText analysis As a complement to the hand-coded data, we also use a more unstruc-\ntured approach to analyze the text data. We use the methodology proposed by Gentzkow\nand Shapiro (2010), which we described in more detail in Section 4.1, to determine\nthe words that are most characteristic of being in the no bias or the bias treatment\narms. Figure 5 presents the 50 words that are most characteristic of responses by Biden\nvoters (Panel A) and by Trump voters (Panel B). We \ufb01nd that words related to \u201cbias\u201d\nare more characteristic of responses in the bias treatment arms, while words, such as\n\u201cnon-partisan\u201d, \u201cunbiased\u201d, \u201cfair\u201d, and \u201cfactual\u201d are more typical of responses in the no\nbias treatment arms. This corroborates the \ufb01ndings from the hand-coding exercise that\nour treatments seem to put thoughts about bias and accuracy on top of people\u2019s minds.\n[Insert Figure 5 here]\n4.3 Discussion of alternative mechanisms\nA key assumption for our structural model is that the treatments only affect people\u2019s\nnews demand through their impact on beliefs about accuracy and political bias. In\nthis section, we discuss potential alternative mechanisms driving our treatment effects,\nincluding cognitive constraints, cross-learning about entertainment, diversi\ufb01cation\nmotives, and experimenter demand effects.\nCognitive constraints Respondents in the no bias treatments might expect the ar-\nticles from The Boston Herald to be more cognitively demanding as these articles\nmay be more likely to cover more facts compared to respondents in any of the bias\ntreatments. Alternatively, respondents might associate the unbiased newsletter with\nhigher complexity if they think it is psychologically more costly to process and integrate\ncon\ufb02icting pieces of evidence.\nThe open-ended responses from Experiment 3 demonstrate that complexity was not\non top of people\u2019s minds when interpreting the treatment variation: Only two out of\n388 respondents thought The Boston Herald only reported one statistic to reduce the\ncomplexity of the article or to make it easier to understand. If we consider the structured\npost-treatment beliefs measures from the main experiments, there is some evidence\nthat Biden voters in the bias treatments associate the newsletter with lower complexity\n(as shown in Table B.11). However, since people did not talk about complexity in\n25\nthe open-ended responses, a likely explanation is that these respondents changed their\nbeliefs about the complexity of the newsletter only when prompted to think about it\nafter deciding whether to subscribe to the newsletter. Furthermore, several patterns\nin our data are inconsistent with cognitive constraints driving the treatment effects.\nFirst, explanations based on cognitive constraints predict a similarly sized decrease\nin demand irrespective of the direction of the political bias. As shown in columns 2\nand 6 of Table B.11, the magnitudes of treatment effects on perceptions of complexity\namong Biden voters are almost identical across the two experiments. Yet, inconsistent\nwith a story based on cognitive constraints, Biden voters only signi\ufb01cantly reduce their\ndemand for the newsletter in response to the right-wing bias treatment. Second, Trump\nvoters do not signi\ufb01cantly update their beliefs about the complexity of the newsletter\u2014\neven when prompted to think about it\u2014making it unlikely that cognitive constraints\ndifferentially affected newsletter demand across treatment groups.\nEntertainment motives It is conceivable that the treatments may affect perceptions\nof the newsletter\u2019s entertainment value. For instance, people might think that balanced\nreporting is less likely to lead to feelings of surprise and suspense (Ely et al., 2015).\nThe open-ended responses from Experiment 3 demonstrate that entertainment was not\non top of people\u2019s minds when interpreting the treatment variation: Only three out of\n388 respondents mentioned entertainment in their responses. Turning to the structured\npost-treatment beliefs measures, we \ufb01nd some evidence that respondents update about\nthe entertainment value of the newsletter (as shown in Table B.11). However, the\nlack of references to entertainment motives in Experiment 3 suggests that people\nonly adjusted their beliefs about entertainment ex-post when they were prompted to\nspeci\ufb01cally think about this dimension. Furthermore, the structured post-treatment\nbelief measures in Experiment 4 (see Appendix Section C) show that only Biden voters\nsigni\ufb01cantly updated their beliefs about the entertainment value of the newsletter when\nthere was less scope for ex-post rationalization of the newsletter subscription decision\n(Table B.13). Finally, conceptually disentangling belief utility and entertainment utility\nis not straightforward since part of the utility from belief con\ufb01rmation might relate to\nthe entertainment value of having your beliefs con\ufb01rmed. If biased news were perceived\nto be more entertaining in general, unrelated to any form of belief con\ufb01rmation utility,\nwe would not expect to see any political heterogeneity in treatment effects.\n26\nDiversi\ufb01cation motive People\u2019s news demand might be driven by the objective to\nread news articles from a diversi\ufb01ed portfolio of outlets with an average ideological bias\nthat is close to zero. Even if any individual outlet covers the news with a political bias,\ncombining the signals across sources might allow people to obtain a more objective\nassessment of the state of the world. Importantly, this motive hinges on people\u2019s news\nconsumption outside the experiment, but not on people\u2019s political views. To assess the\nplausibility of this mechanism, we asked respondents pre-treatment to indicate all news\noutlets from which they have received news over the past 12 months using a list of 21\npopular outlets across the political spectrum. We then classify each outlet as either\nleft-wing or right-wing biased, and then split the sample into respondents who, (i), do\nnot read news from any of these outlets, (ii), who read more left-wing than right-wing\nsources, and, (iii), those who read more right-wing than left-wing sources. We then\nseparately estimate treatment effects on people\u2019s newsletter demand in Experiment 1\nand 2 for each of these three subgroups (as shown in Table B.14).\nFirst, the diversi\ufb01cation motive would predict a positive treatment effect whenever\nthe perceived bias of The Boston Herald shifts away from the bias of the majority of\noutlets that a respondent currently reads. In contrast, column 2 shows a statistically\nsigni\ufb01cant decrease in demand among respondents who mainly read left-wing biased\noutlets in Experiment 1 where we increase the perceived right-wing bias of The Boston\nHerald. In the symmetric case in Experiment 2, we \ufb01nd a negative point estimate,\nalthough the small sample size limits the statistical power in this case (column 6).\nSecond, diversi\ufb01cation would predict a negative treatment effect among people who do\nnot read news from any other source, for which we only \ufb01nd mixed empirical support\n(columns 1 and 4). Taken together, this suggests that a diversi\ufb01cation motive alone is\ninsuf\ufb01cient to rationalize our patterns of treatment effects.\nExperimenter demand effects It is possible that respondents in the different treat-\nment groups hold different beliefs about the experimenter\u2019s expectations. However, we\ndo not believe that experimenter demand is a major concern in our setting. First, our\nexperimental manipulation is implicit in nature as we only factually state the newspaper\nreporting without framing it in terms of bias or accuracy. Second, the patterns of\nheterogeneity by political ideology and experiments suggest that our patterns could\nonly be explained by heterogeneously occurring demand effects. Third, trying to please\nthe experimenter by signing up for an unwanted newsletter is a costly action as it\n27\nentails receiving weekly emails with unwanted content for a month. Fourth, recent\nevidence suggests that experimental subjects respond only moderately to explicit signals\nabout the experimenter\u2019s expectations, indicating a limited quantitative importance of\nexperimenter demand effects (de Quidt et al., 2018; Mummolo and Peterson, 2018).\nTo further alleviate potential concerns about experimenter demand effects, we use\nalmost 5000 hand-coded responses based on participants\u2019 guesses about the study\u2019s\npurpose from an open-ended question, which we elicited in our main experiments.\nAs shown Appendix Figure B.3, only 4.1% of our respondents correctly guess the\nstudy\u2019s purpose (i.e., how perceptions of bias shape people\u2019s news consumption).\nMost participants guess that the purpose is related to understanding perceptions of\nbias (36.2%), measuring opinions and attitudes (16.9%), and studying political views\n(10.4%). A sizable fraction of respondents also express that they simply do not know\n(11.5%). As an additional robustness check, we re-run our main speci\ufb01cations for\nthe subsample of respondents who did not correctly guess the study purpose. This\nrobustness check shows that results are virtually unchanged for this subsample for\nwhich demand effects are particularly unlikely to confound treatment effects (Appendix\nTable B.15). Taken together, our hand-coded data on guesses of the study\u2019s purpose\nindicates that experimenter demand effects are unlikely to explain the treatment effects.\n5 Concluding remarks\nIn this paper, we conducted several large-scale experiments with American voters to\nquantify the relative importance of accuracy concerns and belief con\ufb01rmation motives\nin driving the demand for news. For this purpose, we designed experiments which\nvary whether a news outlet reports the news in a right-wing biased, left-wing biased,\nor politically unbiased way. We then study people\u2019s demand for a newsletter featuring\narticles from this outlet. Our main \ufb01nding is that both Biden and Trump voters strongly\nreduce their demand for politically biased news, but only if the bias is inconsistent with\ntheir own political views: Trump voters strongly reduce their demand for left-wing\nbiased news, but not for right-wing biased news. The reverse patterns hold for Biden\nvoters. The political heterogeneity is consistent with the predictions of behavioral\nmodels of news demand in which readers trade off accuracy concerns against belief\ncon\ufb01rmation motives. In a second step, we quantify the relative importance of accuracy\n28\nand belief con\ufb01rmation motives by using the experimental variation in perceptions\nof accuracy and political bias to estimate a parsimonious discrete-choice model. The\nestimates of the key preference parameters reveal that people attach about equal weights\nto accuracy and belief con\ufb01rmation motives, suggesting that both motives play a key\nrole in shaping news demand.\nThere are growing concerns that biased news contributes to increasing political\npolarization and the rise of populism (Guriev and Papaioannou, 2022; Sunstein, 2018).\nIt is thus important to understand why media bias occurs in equilibrium. Our \ufb01ndings\nimprove our understanding of the origins of media bias by providing direct experimental\nevidence on potential demand-side drivers: People value accuracy, but also demand\nnews that con\ufb01rms their existing beliefs. This result lends empirical support to demand-\nside explanations of media bias, such as behavioral models where \ufb01rms cater to people\u2019s\npreference for like-minded news by slanting their reporting towards the beliefs of their\nreaders. While other factors may also contribute to the origin of media bias, our \ufb01ndings\nsuggest that accounts that assume that consumers only value the accuracy of news are\nlikely to be incomplete.\nA preference for like-minded news has important implications for regulation and\nother efforts aimed at \ufb01ghting media bias and fake news. First, competition among\nmedia outlets should increase media bias in equilibrium if consumers have a demand for\nbiased news (Mullainathan and Shleifer, 2005). Regulatory efforts to increase the com-\npetitive pressure in media markets\u2014such as limiting ownership concentration\u2014may\nthus back\ufb01re. Second, a preference for biased news complicates the welfare analysis\nof efforts aimed at reducing media bias. Speci\ufb01cally, it creates a trade-off between\nsatisfying consumers\u2019 preference for like-minded news and mitigating the negative\npolitical externalities of media bias. Our \ufb01ndings thus demonstrate the complexity of\noptimal regulation.\nThis paper studies the demand for political news, where the relative importance of\naccuracy concerns and belief con\ufb01rmation motives is of particular interest as informed\ncitizens are a necessary input to the functioning of democratic institutions. However, it\nis plausible to expect that the relative preference for accuracy in reporting is stronger\nin news domains where the costs of being misinformed are primarily borne by the\nreader\u2014rather than arising in the form of a political externality. Future research should\nthus explore how people resolve the trade-off between accuracy concerns and belief\ncon\ufb01rmation in other domains, such as \ufb01nancial news.\n29\nReferences\nAllcott, Hunt and Dmitry Taubinsky , \u201cEvaluating behaviorally-motivated policy:\nExperimental evidence from the lightbulb market,\u201d The American Economics Review ,\n2015.\n, Matthew Gentzkow, and Lena Song , \u201cDigital addiction,\u201d Technical Report, Na-\ntional Bureau of Economic Research 2021.\nAngrist, Joshua D. and Alan B. Krueger , \u201cSplit-Sample Instrumental Variables Esti-\nmates of the Return to Schooling,\u201d Journal of Business & Economic Statistics , 1995,\n13(2), 225\u2013235.\nAugenblick, Ned, Muriel Niederle, and Charles Sprenger , \u201c Working over Time:\nDynamic Inconsistency in Real Effort Tasks,\u201d Quarterly Journal of Economics , 05\n2015, 130(3), 1067\u20131115.\nBaron, David P. , \u201cPersistent media bias,\u201d Journal of Public Economics , 2006, 90(1),\n1\u201336.\nBenabou, Roland and Jean Tirole , \u201cBelief in a Just World and Redistributive Politics,\u201d\nQuarterly Journal of Economics , 2006.\nBlackwell, David , \u201cComparison of Experiments,\u201d in Jerzy Neyman, ed., Proceedings\nof the Second Berkeley Symposium on Mathematical Statistics and Probability ,\nUniversity of California Press Berkeley, CA 1951, pp. 93\u2013102.\n, \u201cEquivalent Comparisons of Experiments,\u201d Annals of Mathematical Statistics , 1953,\npp. 265\u2013272.\nBursztyn, Leonardo, Aakaash Rao, Christopher P. Roth, and David H.\nYanagizawa-Drott , \u201cOpinions as Facts,\u201d Working Paper , 2021.\n, Georgy Egorov, Ingar Haaland, Aakaash Rao, and Christopher Roth , \u201cJusti-\nfying Dissent,\u201d University of Chicago, Becker Friedman Institute for Economics\nWorking Paper 2020-73 , 2022.\n,,,, and , \u201cScapegoating During Crises,\u201d American Economic Association\nPapers & Proceedings , 2022.\nCapozza, Francesco, Ingar Haaland, Christopher Roth, and Johannes Wohlfart ,\n\u201cStudying Information Acquisition in the Field: A Practical Guide and Review,\u201d\nDiscussion paper 124, ECONtribute 2021.\nChan, Jimmy and Wing Suen , \u201cA Spatial Theory of News Consumption and Electoral\nCompetition,\u201d Review of Economic Studies , 07 2008, 75(3), 699\u2013728.\n30\nChopra, Felix, Ingar Haaland, and Christopher Roth , \u201cDo People Demand Fact-\nchecked News? Evidence from U.S. Democrats,\u201d Journal of Public Economics , 2022,\n205, 104549.\nde Quidt, Jonathan, Johannes Haushofer, and Christopher Roth , \u201cMeasuring and\nBounding Experimenter Demand,\u201d American Economic Review , 2018, 108(11),\n3266\u20133302.\nDellaVigna, Stefano , \u201cStructural behavioral economics,\u201d in B. Douglas Bernheim,\nStefano DellaVigna, and David Laibson, eds., Handbook of Behavioral Economics ,\nV ol. 1 of Applications and Foundations 1 , North-Holland, 2018, pp. 613\u2013723.\nand Devin Pope , \u201cWhat Motivates Effort? Evidence and Expert Forecasts,\u201d Review\nof Economic Studies , 2018, 85(2), 1029\u20131069.\nand Ethan Kaplan , \u201cThe Fox News Effect: Media Bias and V oting,\u201d Quarterly\nJournal of Economics , 2007, 122(3), 1187\u20131234.\n, John A List, Ulrike Malmendier, and Gautam Rao , \u201cEstimating social prefer-\nences and gift exchange at work,\u201d American Economic Review , 2022.\nDurante, Ruben and Brian Knight , \u201cPartisan Control, Media Bias, and Viewer Re-\nsponses: Evidence from Berlusconi\u2019s Italy,\u201d Journal of the European Economic\nAssociation , 2012, 10(3), 451\u2013481.\nEly, Jeffrey, Alexander Frankel, and Emir Kamenica , \u201cSuspense and Surprise,\u201d\nJournal of Political Economy , 2015, 123(1), 215\u2013260.\nExley, Christine L. , \u201cExcusing Sel\ufb01shness in Charitable Giving: The Role of Risk,\u201d\nReview of Economic Studies , 2015.\nEyal, Peer, Rothschild David, Gordon Andrew, Evernden Zak, and Damer Ekate-\nrina, \u201cData quality of platforms and panels for online behavioral research,\u201d Behavior\nResearch Methods , 2021, pp. 1\u201320.\nFaia, Ester, Andreas Fuster, Vincenzo Pezone, and Basit Zafar , \u201cBiases in infor-\nmation selection and processing: Survey evidence from the pandemic,\u201d Technical\nReport, National Bureau of Economic Research 2021.\nFalk, Armin and Florian Zimmermann , \u201cConsistency as a Signal of Skills,\u201d Man-\nagement Science , 2015.\nand , \u201cBeliefs and Utility: Experimental Evidence on Preferences for Information,\u201d\nWorking Paper , 2017.\n31\nForos, \u00d8ystein, Hans Jarle Kind, and Lars S\u00f8rgard , \u201cMerger Policy and Regulation\nin Media Industries,\u201d in Simon P. Anderson, Joel Waldfogel, and David Str\u00f6mberg,\neds., Handbook of Media Economics , V ol. 1A of Handbook of Media Economics ,\nNorth-Holland, 2015, pp. 225\u2013264.\nFuster, Andreas, Ricardo Perez-Truglia, and Basit Zafar , \u201cExpectations with En-\ndogenous Information Acquisition: An Experimental Investigation,\u201d Review of Eco-\nnomics and Statistics , 2022.\nGanguly, Ananda and Joshua Tasoff , \u201cFantasy and Dread: The Demand for Informa-\ntion and the Consumption Utility of the Future,\u201d Management Science , 2016, 63(12),\n4037\u20134060.\nGarz, Marcel, Gaurav Sood, Daniel F. Stone, and Justin Wallace , \u201cThe supply of\nmedia slant across outlets and demand for slant within outlets: Evidence from US\npresidential campaign news,\u201d European Journal of Political Economy , 2020, 63,\n101877.\nGentzkow, Matthew A. and Jesse M. Shapiro , \u201cMedia Bias and Reputation,\u201d Journal\nof Political Economy , 2006, 114(2), 280\u2013316.\nand , \u201cWhat Drives Media Slant? Evidence From U.S. Daily Newspapers,\u201d\nEconometrica , 2010, 78(1), 35\u201371.\n, Michael B. Wong, and Allen T. Zhang , \u201cIdeological Bias and Trust in Information\nSources,\u201d Working Paper , 2018.\nGentzkow, Matthew, Jesse M. Shapiro, and Michael Sinkinson , \u201cCompetition and\nIdeological Diversity: Historical Evidence from US Newspapers,\u201d American Eco-\nnomic Review , 2014, 104(10), 3073\u20133114.\nGuriev, Sergei and Elias Papaioannou , \u201cThe political economy of populism,\u201d Journal\nof Economic Literature , 2022.\nHaaland, Ingar, Christopher Roth, and Johannes Wohlfart , \u201cDesigning Informa-\ntion Provision Experiments,\u201d Journal of Economic Literature , 2021.\nInoue, Atsushi and Gary Solon , \u201cTwo-Sample Instrumental Variables Estimators,\u201d\nReview of Economics and Statistics , 2010, 92(3), 557\u2013561.\nLoewenstein, George and Andras Molnar , \u201cThe renaissance of belief-based utility\nin economics,\u201d Nature Human Behaviour , 2018, 2(3), 166\u2013167.\nMontanari, Giovanni and Salvatore Nunnari , \u201cAudi alteram partem: An experiment\non selective exposure to information,\u201d Technical Report, Technical report, Working\nPaper 2019.\n32\nMullainathan, Sendhil and Andrei Shleifer , \u201cThe Market for News,\u201d American Eco-\nnomic Review , 2005, 95(4), 1031\u20131053.\nMummolo, Jonathan and Erik Peterson , \u201cDemand Effects in Survey Experiments:\nAn Empirical Assessment,\u201d American Political Science Review , 2018, 113(2), 517\u2013\n529.\nNewey, Whitney K. , \u201cEf\ufb01cient estimation of limited dependent variable models with\nendogenous explanatory variables,\u201d Journal of Econometrics , 1987, 36(3), 231\u2013250.\nNewman, Nic, Richard Fletcher, Anne Schulz, Simge Andi, and Rasmus Kleis\nNielsen , \u201cReuters Institute Digital News Report 2020,\u201d Technical Report, Reuters\nInstitute for the Study of Journalism 2020.\nNielsen, Kirby , \u201cPreferences for the resolution of uncertainty and the timing of infor-\nmation,\u201d Journal of Economic Theory , 2020, 189, 105090.\nPerego, Jacopo and Sevgi Yuksel , \u201cMedia Competition and Social Disagreement,\u201d\nEconometrica , 2022, 90(1), 223\u2013265.\nSchwardmann, Peter and Jo\u00ebl van der Weele , \u201cDeception and self-deception,\u201d Na-\nture Human Behavior , 2019, 3(10), 1055\u20131061.\n, Egon Tripodi, and Jo\u00ebl J Van der Weele , \u201cSelf-persuasion: Evidence from \ufb01eld\nexperiments at two international debating competitions,\u201d American Economic Review ,\n2022.\nSuen, Wing , \u201cThe Self-Perpetuation of Biased Beliefs,\u201d The Economic Journal , 2004,\n114(495), 377\u2013396.\nSunstein, Cass R ,# Republic: Divided Democracy in the Age of Social Media , Prince-\nton University Press, 2018.\nTella, Rafael Di, Ricardo Perez-Truglia, Andres Babino, and Mariano Sigman ,\n\u201cConveniently Upset: Avoiding Altruism by Distorting Beliefs About Others\u2019 Altru-\nism,\u201d American Economic Review , 2015, 105(11), 3416\u20133442.\nThaler, Michael , \u201cThe \u201cFake News\u201d Effect: An Experiment on Motivated Reasoning\nand Trust in News,\u201d Working Paper , 2019.\nZimmermann, Florian , \u201cClumped or Piecewise? Evidence on Preferences for Infor-\nmation,\u201d Management Science , 2015, 61(4), 740\u2013753.\n33\nMain \ufb01gures and tables\nFigure 1: Overview of the experimental design\n1/25/22, 2:58 PM Untitled Diagram\n1/1Pre-treatment beliefs about CBO reporting in the Boston\nHerald  \nExperiment 1 (right-wing bias) : $15 Minimum Wage Bill  \nExperiment 2 (left-wing bias) : Healthcare Plan  Consent form, attention check, background questions\nTreatment: Biased reporting \nExperiment 1 (right-wing bias) : The article, published in The \nBoston Herald on February 26 , 2021, reported that the bill would  \nreduce employment by 1.4 million jobs but not  that it would lift \n900,000 out of poverty  \nExperiment 2 (left-wing bias) : The Boston Herald article about  \nthe Senate Republican Healthcare Plan reported that the plan  \nwould leave over 20 million more people uninsured but not  that it \nwould decrease the deficit by over $100 billion.  Treatment: Unbiased reporting \nExperiment 1 (right-wing bias) : The article, published in The \nBoston Herald on March 2, 2021, reported that the bill would  \nreduce employment by 1.4 million jobs and that it would lift \n900,000 out of poverty   \nExperiment 2 (left-wing bias) : The Boston Herald article about  \nthe House Republican Healthcare Plan reported that the plan  \nwould leave over 20 million more people uninsured and that it \nwould decrease the deficit by over $100 billion.  \nNewsletter demand   \nOur Weekly Economic Policy Newsletter  will cover the top three \narticles about economic policy  published in The Boston Herald.\nIf you say \"Yes\" below, we will message you the newsletter on your  \nProlific account on a weekly basis over the next month.\nWould you like to subscribe to the newsletter?  \nPost-treatment beliefs about accuracy, trust, quality, and  \nother newsletter characteristics. \nOpen-ended motives for signing/not signing up for the  \nnewsletter. \nNote: This \ufb01gure provides an overview of the main design features of Experiment 1\n(right-wing bias) and Experiment 2 (left-wing bias). Appendix Section F contains the full\nexperimental instructions.\n34\nFigure 2: Newsletter demand by treatment and voting status\n(a) Biden voters: Right-wing bias\n p < 0.001\nn = 742 n = 727\n0.05.1.15.2Mean \u00b1 s.e.m.\nNo bias Right-wing bias (b) Trump voters: Right-wing bias\n p = 0.629\nn = 613 n = 623\n0.05.1.15.2Mean \u00b1 s.e.m.\nNo bias Right-wing bias\n(c) Biden voters: Left-wing bias\n p = 0.285\nn = 741 n = 728\n0.05.1.15.2Mean \u00b1 s.e.m.\nNo bias Left-wing bias (d) Trump voters: Left-wing bias\n p = 0.093\nn = 418 n = 432\n0.05.1.15.2Mean \u00b1 s.e.m.\nNo bias Left-wing bias\nNote: This \ufb01gure presents the share of respondents who chose to subscribe to the weekly\npolitics newsletter by treatment and voting status. Panel (a) and Panel (b) present results\nfrom Experiment 1. Panel (c) and Panel (d) present results from Experiment 2. Panel (a) and\nPanel (c) focus on the subsample of respondents who voted for Joe Biden, while Panel (b)\nand Panel (d) focus on respondents who voted for Donald Trump. The p-values are obtained\nfrom a two-sample t-test of equality of means. Standard errors of the mean are shown.\n35\nFigure 3: Motives for news demand: Most distinctive phrases\n(a) Distinctive phrases of subscribers by treatment status\ncompar\nview\nread\naffect\nattentcuriou\nnice\nbalancperspectreli\nunbiasfinanci\nbenefici\ncaoppos\ntrustinfo\npoliciherald nonbiassiteworld\nreceivboston qualitiopportundecid\nappear inbox\nworthaltern appeal\nborecatch\nencountentertain\nexamin\nexistfarm\nfutur\nhealthcarheard\ninclud judgmentleftistletter\nminimummonthorient\npay\n0.0000.0020.004Chi\u2212squared\n(b) Distinctive phrases of non-subscribers by treatment status\nbias\nreport\nemailstorihear\nprolifslant\nherald\nsitetruthleav\nbill\nqualitiinfo\nboston\nhalfplan\npostprovidbad\ndetailpick read\naccur\nmainstreamtold\ntypebase financi\nbegindistract\nfrankli know\nmagazin privacireadili\nwashingtonsourc left\ncommunfoundcboentir\nglass misinform agenda bia\njobworri familiar0.0000.0020.004Chi\u2212squared\nNote: This \ufb01gure uses respondents\u2019 answers to the open-ended question of why they\nsubscribed (or did not subscribe) to the newsletter from Experiment 1 and 2 (see Table B.10).\nPanel (a) uses responses to the open-ended questions from respondents who subscribed to\nthe newsletter on why they subscribed to the newsletter, while Panel (b) uses responses\nfrom respondents who did not subscribe on why they did not subscribe to the newsletter.\nEach panel displays the 50 phrases with the largest c2statistic using the method proposed\nby Gentzkow and Shapiro (2010). We exclude stop words and reduce all words to their\nstem using the Porter stemmer. Phrases with a positive c2statistic are more distinctive of\nopen-responses in the left-wing orright-wing biased treatment arms (in green). Phrases with\na negative c2statistic are more distinctive of responses in the unbiased treatment arm (in\nred).36\nFigure 4: Treatment effects on mentioning political bias in the open-ended responses\n(a) Biden voters: Right-wing bias\n p < 0.001\nn = 57 n = 57\n0.1.2.3.4.5.6.7Mean \u00b1 s.e.m.\nNo bias Right-wing bias (b) Trump voters: Right-wing bias\n p = 0.008\nn = 38 n = 42\n0.1.2.3.4.5.6.7Mean \u00b1 s.e.m.\nNo bias Right-wing bias\n(c) Biden voters: Left-wing bias\n p < 0.001\nn = 64 n = 62\n0.1.2.3.4.5.6.7Mean \u00b1 s.e.m.\nNo bias Left-wing bias (d) Trump voters: Left-wing bias\n p < 0.001\nn = 34 n = 34\n0.1.2.3.4.5.6.7Mean \u00b1 s.e.m.\nNo bias Left-wing bias\nNote: The \ufb01gure presents treatment effects on whether respondents mention political bias\nin their responses to the open-ended motives question in Experiment 3 (see Table B.10).\nSpeci\ufb01cally, respondents were asked why they think The Boston Herald reported in the way\nit did. Each panel displays the share of respondents whose responses were hand-coded to\nthe \u201cbias\u201d category (example responses: \u201cI think it\u2019s biased reporting,\u201d \u201cPerhaps they are a\nRepublican newspaper,\u201d \u201cI believe it is a left-leaning newspaper,\u201d or \u201cThey clearly support\nthe Democrats\u201d would all be classi\ufb01ed as \u201cbiased\u201d). Panel (a) and Panel (b) compare the\nright-wing bias treatment to the no bias treatment (analogous to Experiment 1). Panel (c)\nand Panel (d) compare the left-wing bias treatment to the no bias treatment (analogous to\nExperiment 2). Panel (a) and Panel (c) focus on the subsample of respondents who voted for\nJoe Biden, while Panel (b) and Panel (d) focus on respondents who voted for Donald Trump.\nThep-values are obtained from a two-sample t-test of equality of means. Standard errors of\nthe mean are shown.\n37\nFigure 5: Perceived motives for reporting: Most distinctive phrases\n(a) Biden voters\nestimfocu\nnonpartisanview\npublishbillleanbiasomit\nlive\nbusineg\nunbiasliberpriceleavreader\nagendadecreas\nfactualfairhappenexplainfeelhighlight\npotenti deficit paper\ndatafindaccur\ntrue minimumfoundafford attent directli list\nofficpoliciwordappeal approach caterchose\ndetaileditoripaint\nrun suspect\n\u22120.0050\u22120.00250.00000.0025Chi\u2212squared\n(b) Trump voters\ncost\nbias\nstatesupport\nlowerpositpoverti\npublishamountcreat\nhour raisreal\nliber\nappeal corpor\nhappenhealthsavesourcchosedifficult\ndueeffect\nfree\nintentlight\nservic neg\nbusijob\naccount\nassumattempt\ncongressioncurrentdata\nextrafairfewerhelp\nimprovincom\njournalistleft\nmiddl misrepresmultiploffic\noutlet\n\u22120.0020.0000.0020.004Chi\u2212squared\nNote: This \ufb01gure uses data from the mechanism experiment in which we measured perceived\nmotives for the reporting strategy of The Boston Herald using open-ended questions (Ex-\nperiment 3, see Table B.10). The \ufb01gure displays the 50 phrases with the largest c2statistic\nusing the method proposed by Gentzkow and Shapiro (2010). We exclude stop words and\nreduce all words to their stem using the Porter stemmer. Panel (a) uses responses to the\nopen-ended motives question from Biden voters, while Panel (b) uses responses from Trump\nvoters to calculate the c2statistics. Phrases with a positive c2statistic are more distinctive\nof responses in the biased treatment arms (in green). Phrases with a negative c2statistic are\nmore distinctive of responses in the unbiased treatment arm (in red). The terms \u201ccbo\u201d and\n\u201creport\u201d, which have c2values of\u00000:0126 and\u00000:0098 , were omitted to better scale the\nother phrases.\n38\nTable 1: Main results: The demand for biased news\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6)\nAccuracy Left-wing bias Demand Accuracy Left-wing bias Demand\nPanel A: Biden voters\nBias treatment (a) -0.903*** -0.849*** -0.086*** -0.720*** 0.305*** -0.026\n(0.057) (0.061) (0.017) (0.055) (0.059) (0.019)\nN 1,464 1,464 1,469 1,466 1,466 1,469\nZ-scored Yes Yes No Yes Yes No\nControls Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0.181 0 0 0.189\np-value: Ex. 1 = Ex. 2 0.026 0.000 0.017 0.026 0.000 0.017\nPanel B: Trump voters\nBias treatment (b) -0.165*** -0.490*** 0.005 -0.542*** 0.266*** -0.052**\n(0.056) (0.063) (0.020) (0.072) (0.072) (0.024)\nN 1,235 1,235 1,236 849 849 850\nZ-scored Yes Yes No Yes Yes No\nControls Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0.162 0 0 0.191\np-value: Ex. 1 = Ex. 2 0.000 0.000 0.072 0.000 0.000 0.072\np-value: a = b 0.000 0.005 0.001 0.073 0.947 0.395\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20133) and Experiment\n2 (columns 4\u20136) where the dependent variables are post-treatment beliefs about accuracy (columns 1 and 4), the\nperceived left-wing bias of the newsletter (columns 2 and 5), and newsletter demand (columns 3 and 6). Panel\nA and Panel B present results for Biden and Trump voters, respectively. \u201cBias treatment\u201d is a binary variable\ntaking value one for respondents assigned the right-wing bias (columns 1\u20133) or the left-wing bias (columns\n4\u20136) treatment arm, and zero for respondents in the no bias treatment arm. \u201cDemand\u201d is a binary variable\ntaking value one for respondents who said \u201cYes\u201d to receiving the weekly newsletter, and zero for those who\nsaid \u201cNo.\u201d \u201cAccuracy\u201d of the newsletter is measured on a 5-point Likert scale from \u201cVery inaccurate\u201d to \u201cVery\naccurate.\u201d \u201cLeft-wing bias\u201d is measured on a 5-point Likert scale from \u201cVery right-wing biased\u201d to \u201cVery left-\nwing biased.\u201d \u201cAccuracy\u201d and \u201cLeft-wing bias\u201d have been z-scored using the relevant no bias group mean and\nstandard deviation. \u201c p-value: Ex. 1 = Ex. 2\u201d provides p-values for tests of the equality of coef\ufb01cients between\nExperiment 1 and Experiment 2. \u201c p-value: a = b\u201d provides p-values for tests of the equality of coef\ufb01cients\nbetween Trump and Biden voters. All regressions include a set of basic control variables: gender, age, education,\nrace and ethnicity, log income, employment status, Census region, voting, political af\ufb01liation, ideology, interest\nin economic news, whether they have read any of a list of 21 newspapers during the last 12 months, whether\nthey have read The Boston Herald, whether they currently subscribe to any newsletters, and their pre-treatment\nbeliefs about how The Boston Herald reported about the CBO \ufb01ndings.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n39\nTable 2: Structural model: Preferences for accuracy and biased news\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPreference for accuracy ( a) 0.241*** 0.204** 0.266\n(0.076) (0.085) (0.190)\nPreference for belief con\ufb01rmation ( b) 0.345*** 0.374*** 0.190\n(0.081) (0.091) (0.160)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.412*** 0.353*** 0.583**\n(0.111) (0.131) (0.270)\nN 5,014 2,930 2,084\nNote: This table presents the parameter estimates of the discrete choice model outlined in equations 2,\n3 and 4 in Section 3.3. Column 1 presents parameter estimates for the full sample, while columns 2\nand 3 present estimates for Biden and Trump voters, respectively. Speci\ufb01cally, we estimate an IV\nprobit model using Newey\u2019s (1987) two-step estimator as implemented by Stata\u2019s ivprobit routine.\nWe use data from Experiments 1 and 2 where we elicit newsletter subscription choices and percep-\ntions within-subject. The dependent variable is a binary indicator taking value one for respondents\nwho choose to sign up to the newsletter. The endogenous regressors are z-scored perceptions of\nquality and belief con\ufb01rmation. We instrument these perceptions with a saturated set of treatment\nstatus indicators. In column 1, we also include interactions of the treatment assignment with a binary\nindicator for whether a respondent voted for Trump as instruments to capture differential \ufb01rst-stage\neffects of the treatments. We include a binary indicator for whether a respondent voted for Trump as\na control variable in column 1.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n40\nTable 3: Motives for subscription vs. non-subscription to the newsletter\nMentions at least one synonym of:\nUnbiased Biased\n(1) (2) (3) (4) (5) (6)\nBiased -0.041** 0.009* 0.009* 0.002 0.044*** 0.044***\n(0.016) (0.005) (0.005) (0.015) (0.006) (0.006)\nNews demand 0.061*** 0.026**\n(0.013) (0.010)\nBiased x News demand -0.049*** -0.042**\n(0.017) (0.016)\nN 789 4,052 4,841 789 4,052 4,841\nSample Subscriber Non-subscriber All Subscriber Non-subscriber All\nNo bias treatment mean 0.078 0.017 0.028 0.046 0.019 0.024\nNote: This table presents OLS regression estimates pooling respondents from Experiment 1 and 2 where the depen-\ndent variables are binary indicators for whether respondents mentioned synonyms of \u201cunbiased\u201d (columns 1\u20133) or\n\u201cbiased\u201d (columns 4\u20136). Speci\ufb01cally, the dependent variable in columns 1\u20133 is a binary indicator taking value one\nif respondents mention the word \u201cunbiased\u201d or any of its synonyms in their open response to the question why\nthey subscribed (did not subscribe) to the newsletter. The synonyms are \u201cdisinterested\", \u201cdispassionate\", \u201cequi-\ntable\", \u201chonest\", \u201cimpartial\u201d, \u201cneutral\u201d, \u201cnonpartisan\u201d, \u201copen-minded\u201d, \u201caloof\u201d, \u201ccold\u201d, \u201cequal\u201d, \u201ceven-handed\u201d,\n\u201cfair\u201d, \u201cnondiscriminatory\u201d, \u201cobjective\u201d, \u201con-the-fence\", \u201cstraight\u201d, \u201cunbigoted\u201d, \u201cuncolored\u201d, \u201cuninterested\u201d,\n\u201cunprejudiced.\u201d Synonyms are taken from the website thesaurus.com. The dependent variable in columns 4\u20136 is\nconstructed analogously using \u201cbiased\u201d and any of the following synonyms: \u201cpartisan\u201d, \u201ctendentious\u201d, \u201cslanted.\u201d\n\u201cBiased\u201d is a binary indicator taking value one for respondents assigned to the \u201cleft-wing bias\u201d or the \u201cright-wing\nbias\u201d treatment arms, and zero otherwise. \u201cNews demand\u201d is a binary variable taking value one for respondents\nwho said \u201cYes\u201d to receiving the weekly newsletter, and zero for those who said \u201cNo.\u201d Columns 1 and 4 focus on\nthe subsample respondents who subscribed to the newsletter, while columns 2 and 5 focus on those who did not\nsubscribe. Columns 3 and 6 include all respondents. All regressions include experiment \ufb01xed effects.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n41\nFor online publication only:\nThe Demand for News: Accuracy Concerns versus\nBelief Con\ufb01rmation Motives\nFelix Chopra, Ingar Haaland, and Christopher Roth\nSection A presents theoretical results.\nSection B contains additional tables and \ufb01gures.\nSection D contains information about our preregistration and the ethics approval.\nSection E contains additional details about the publication and distribution of our\nweekly newsletter.\nSection F provides screenshots of the experimental instructions.\n1\nA Theoretical appendix\nThis section formalizes the intuition that our active control designs in Experiment 1 and\n2 (see Section 2) should decrease the perceived Blackwell informativeness of Boston\nHerald articles in the narrow context of reporting about CBO \ufb01ndings. Proposition\n1 below outlines suf\ufb01cient conditions for the left-wing biased andright-wing biased\ntreatment to strictly decrease the perceived Blackwell informativeness compared to the\nrespective no bias treatment. As a result, this provides us with the empirical prediction\nthat for neoclassical agents that care only about the accuracy of news reporting, our\ntreatments should decrease newsletter demand.\nWhile it seems intuitively reasonable that reporting both statistics is more infor-\nmative than selectively reporting only one statistic in our context, it is important to\nemphasize that there is in general no normative benchmark on the reporting of facts\nwhen full disclosure is notpossible. For example, suppose that a newsletter receives\nthree signals, (s1;s2;s3) = ( L;R;R), about an unobserved state q, but can only report\none signal. From the reader\u2019s perspective, the optimal reporting rule will depend on the\nprior beliefs and the cost of making a Type I and Type II error when conditioning actions\non one\u2019s belief about q(a point made by Suen 2004). Thus, readers with different priors\nprefer different reporting rules, making it not possible to de\ufb01ne a complete ordering of\nreporting rules in terms of their informativeness.\nWe therefore chose to focus on a setting where it seems ex-ante very likely that news\noutlets are not constrained in whether they report only one or both of the main \ufb01ndings\nfrom the CBO reports.1Thus, when evaluating the reporting of The Boston Herald\nonly in the narrow sense of how it covers CBO reports, an increase in the probability of\nreporting both statistics necessarily increases its informativeness in the Blackwell sense.\nBelow, we outline the formal argument.\nSetup There is a binary state space Q=fL;Rgwith a typical element denoted by\nqand an agent with prior belief q2D(Q)about the hidden state. The agent has\nthe option to acquire information from a news outlet (The Boston Herald), which\n1For example, we veri\ufb01ed that all top 15 US newspapers by circulation (as of June 2019) reported\nboth \ufb01ndings from the CBO report about the Healthcare Plan, suggesting that news outlets do not face\nbinding constraints that would require them to choose between reporting either the effects on the de\ufb01cit\nor the effects on the number of uninsured.\n2\npublishes a newsletter nthat is informative about the state q. To introduce scope for\ninformation suppression, we assume that the news outlet receives a set of private signals\ns=fs1;:::; sKg2Sabout q. The set consists of Kbinary signals si2Q, where K2N\nis drawn randomly and independently of q. The signals, si, take value Lwith probability\npqwhere pR<pL, and value Rotherwise. The news outlet can disclose any subset of\nsin its newsletter n, i.e. n\u0012s. Note that this implies that it cannot distort individual\nsignals but only choose to suppress a subset of signals. In our experiments, The Boston\nHerald received two con\ufb02icting signals from the Congressional Budget Of\ufb01ce about the\nconsequences of the $15 Minimum Wage Bill (Experiment 1) or the consequences of\nthe healthcare plan (Experiment 2), i.e. s=fL;Rgin both experiments.\nInformativeness The source signal can thus be represented as an information struc-\nture (S;p)with state-dependent likelihood p:Q!D(S). We are agnostic about the\nnews outlet\u2019s incentives to suppress information, subsuming them in the reader\u2019s belief\nr:S!D(N)about how the news outlet reports conditional on s. From the agent\u2019s\nperspective, the informativeness of nis an invariant of the state-dependent distribution\nover news articles, s:Q!D(N), induced by the agent\u2019s belief about the quality of the\nnews outlet\u2019s source, p, and the belief about how the news outlet reports, r. Consider\ntwo articles nandn0with distributions s;s0:Q!D(N). We use Blackwell\u2019s (1951)\nnotion of informativeness and say that nis(Blackwell) more informative than n0if(n;s)\nissuf\ufb01cient for(n0;s0), that is: there is a stochastic transformation tsuch that n0and\nt(n)are identically distributed. Intuitively, we obtain n0by adding noise to n. This\nis the benchmark for evaluating the informativeness of an information structure: any\nagent with access to an article nthat is more informative than n0can attain an expected\npayoff at least as large as the maximal expected payoff attainable with n0, regardless\nof the prior qand the decision problem a2Awith payoffs u(a;q)(Blackwell, 1953).\nThis provides the prediction that the demand for news should be strictly increasing in\nthe perceived informativeness of the news.\nHow does strategic suppression of signals affect the informativeness of news?\nSuppose the news outlet received the signals s=fs1;:::; sKgand let s(s0js)denote the\nagents\u2019 belief that the news outlet would report s0\u0012safter receiving s. Intuitively, the\ninformativeness of the article nshould be strictly increasing in the probability of fully\nconveying the set of signals. Indeed, the Blackwell informativeness strictly increases\nif we decrease the probability s(s0js)of reporting a \ufb01ltered signal s0(sand instead\n3\nincrease the probability of full information transmission, s(sjs).\nProposition 1 (Informativeness) .Fixs=fs1;:::; sKg2Sand two reporting strategies\nr;r0:S!D(N). Let s;s0:Q!D(N)be the information structures induced by\ncombining the source signal p:Q!D(S)with the reporting strategies, respectively.\nSuppose that\n(i)r(sjs)\u0015r0(sjs),\n(ii)r(tjs)\u0014r0(tjs)for all t(s,\n(iii)r(\u0001js0) =r0(\u0001js0)for all s06=s.\nThen, the information structure sis Blackwell more informative than s0.\nProof. It suf\ufb01ces to show that the conclusion obtains if we strengthen the assumption by\nadditionally assuming that r(tjs)<r0(tjs)for some t(sand that for all other t0(s\nwith t06=t, we have r(t0js) =r0(t0js). The general case then follows by applying the\nresult to the sequence r=r1;:::;rL=r0where rkandrk+1differ at most on the set\nfs;s0gfor some s0\u0012sandL=jP(s)j. Suppose that n2Nis a random variable with\nstate-dependent distribution s. To show that sis Blackwell more informative than s0,\nit suf\ufb01ces to construct an n-measurable random variable n02Nwith state-dependent\ndistribution s0, thereby establishing statistical suf\ufb01ciency. We construct n0as follows:\nletn0=nwhenever n6=sand set b=r0(sjs)=r(sjs). Ifn=s, then n0takes value\nswith probability band value twith probability 1\u0000b. One can then verify that\nconditional on the state q2Q, the distribution of n0iss0(\u0001jq). This concludes the\nproof.\nIn our active no bias group designs, the right-wing bias and the left-wing bias\ntreatment exogenously decrease the probability r(sjs)of reporting both statistics from\nthe CBO report compared to the no bias treatment, while increasing the probability of\nselective reporting. By Proposition 1, this means that respondents in the right-wing\nbias and the left-wing bias should perceive the newsletter as strictly less informative\ncompared to respondents in the no bias treatment.\n4\nB Additional tables and \ufb01gures\nTable B.1: Summary statistics\n(1)\nUS pop.(2)\nExp 1(3)\nExp 2(4)\nExp 3(5)\nExp 4\nMale 0.492 0.468 0.436 0.479 0.481\nAge (years) 47.78 35.487 36.304 35.737 38.829\nWhite 0.763 0.834 0.840 0.827 0.821\nEmployed 0.620 0.681 0.724 0.724 0.715\nCollege 0.329 0.649 0.678 0.683 0.695\nHigh income 0.482 0.443 0.429 0.461 0.446\nNortheast 0.17 0.174 0.194 0.157 0.189\nMidwest 0.21 0.231 0.235 0.206 0.204\nSouth 0.38 0.389 0.398 0.412 0.396\nWest 0.24 0.206 0.173 0.224 0.211\nV ote Trump 0.469 0.457 0.367 0.381 0.493\nObservations 2,705 2,319 388 1,910\nNote: This table displays the mean value of basic covariates for the US population (column 1) as well\nas for each experiment (see Table B.10 for an overview of the experiments). We obtained population\ndata from the 2019 American Community Survey and the U.S. Census Bureau \u201cQuickFacts\u201d tool.\n\u201cMale\u201d is a binary variable taking value one for male respondents, and zero otherwise. \u201cAge\u201d is the\nnumerical age of the respondent in years. \u201cWhite\u201d is a binary variable taking value one if the re-\nspondent selected \u201cCaucasian/White,\u201d and zero otherwise. \u201cEmployed\u201d is a dummy variable taking\nvalue one if the respondent is employed full-time, part-time, or self-employed. \u201cHigh income\u201d is\na binary variable taking value one if the respondent has pre-tax household annual income above\n$75,000. \u201cCollege degree\u201d is a binary variable taking value one if the respondent has at least a bach-\nelor\u2019s degree. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with value one if the\nrespondent lives in the respective region, and zero otherwise. \u201cV oted for Trump\u201d is a binary variable\ntaking value one if the respondent voted for Donald Trump in the 2020 US presidential election, and\nzero if the respondent voted for Joe Biden.\n5\nTable B.2: Test of balance of treatment vs. control: Experiment 1\nBias (B) No bias (NB) P-value (B - NB) Observations\nMale 0.47 0.47 0.949 2705\nAge 35.80 35.18 0.239 2705\nWhite 0.84 0.83 0.789 2705\nIncome (midpoint) 72051.85 69667.90 0.169 2705\nCollege degree 0.65 0.65 0.817 2705\nFull-time employee 0.49 0.49 0.953 2705\nNortheast 0.17 0.18 0.795 2705\nMidwest 0.23 0.24 0.589 2705\nWest 0.21 0.20 0.774 2705\nSouth 0.39 0.39 0.666 2705\nNote: This table provides a balance test between the treatment and no bias group\nusing all respondents from Experiment 1 (see Table B.10 for an overview of the ex-\nperiments). \u201cMale\u201d is a binary variable taking value one for male respondents, and\nzero otherwise. \u201cAge\u201d is the numerical age of the respondent in years. \u201cWhite\u201d is a\nbinary variable taking value one if the respondent selected \u201cCaucasian/White,\u201d and\nzero otherwise. \u201cIncome (midpoint)\u201d is coded continuously as the income bracket\u2019s\nmidpoint (Less than $15,000, $15,000 to $24,999, $25,000 to $49,999, $50,000\nto $74,999, $75,000 to $99,999, $100,000 to $149,999, $150,000 to $200,000,\n$200,000 or more). \u201cCollege degree\u201d is a binary variable taking value one if the\nrespondent has a college degree, and zero otherwise. \u201cFull-time employee\u201d is a\nbinary variable taking value one if the respondent is a full-time employee, and zero\notherwise. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with\nvalue one if the respondent lives in the respective region, and zero otherwise.\n6\nTable B.3: Test of balance of treatment vs. control: Experiment 2\nBias (B) No bias (NB) P-value (B - NB) Observations\nMale 0.41 0.46 0.018 2319\nAge 36.24 36.36 0.837 2319\nWhite 0.85 0.83 0.456 2319\nIncome (midpoint) 68377.16 69221.31 0.643 2319\nCollege degree 0.67 0.68 0.514 2319\nFull-time employee 0.55 0.52 0.253 2319\nNortheast 0.20 0.19 0.682 2319\nMidwest 0.24 0.23 0.816 2319\nWest 0.17 0.18 0.776 2319\nSouth 0.39 0.40 0.754 2319\nNote: This table provides a balance test between the treatment and no bias group\nusing all respondents from Experiment 2 (see Table B.10 for an overview of the ex-\nperiments). \u201cMale\u201d is a binary variable taking value one for male respondents, and\nzero otherwise. \u201cAge\u201d is the numerical age of the respondent in years. \u201cWhite\u201d is a\nbinary variable taking value one if the respondent selected \u201cCaucasian/White,\u201d and\nzero otherwise. \u201cIncome (midpoint)\u201d is coded continuously as the income bracket\u2019s\nmidpoint (Less than $15,000, $15,000 to $24,999, $25,000 to $49,999, $50,000\nto $74,999, $75,000 to $99,999, $100,000 to $149,999, $150,000 to $200,000,\n$200,000 or more). \u201cCollege degree\u201d is a binary variable taking value one if the\nrespondent has a college degree, and zero otherwise. \u201cFull-time employee\u201d is a\nbinary variable taking value one if the respondent is a full-time employee, and zero\notherwise. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with\nvalue one if the respondent lives in the respective region, and zero otherwise.\n7\nTable B.4: Predictions of different models of the demand for news\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2)\nPanel A : Accuracy\nBiden voters YB\nR<YB\nN YB\nL<YB\nN\nTrump voters YT\nR<YT\nN YT\nL<YT\nN\nPanel B : Belief con\ufb01rmation\nBiden voters YB\nR<YB\nN YB\nL>YB\nN\nTrump voters YT\nR>YT\nN YT\nL<YT\nN\nPanel C : Both motives\nBiden voters YB\nR<YB\nN ambiguous\nTrump voters ambiguous YT\nL<YT\nN\nNote: This table summarizes the predictions outlined in Section 2.4. Panel A summarizes the\npredictions of models where people only care about accuracy. Panel B summarize the predic-\ntions of models where people only care about belief con\ufb01rmation. Panel C summarizes the\npredictions of models where both accuracy and belief con\ufb01rmation motives shape the demand\nfor news. Yg\nidenote the demand for news in treatment arm i2fL;N;Rgand political group\ng2fB;Tg, where Brepresents Biden voters, Trepresents Trump voters, and L,NandRdenote\ntheleft-wing bias ,no bias andright-wing bias treatment arm, respectively.\n8\nTable B.5: Treatment effects on perceptions of accuracy: Robustness\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6) (7) (8)\nAccuracy Trust Quality Index Accuracy Trust Quality Index\nPanel A: Biden voters\nBias treatment (a) -0.903*** -0.824*** -0.545*** -0.842*** -0.720*** -0.662*** -0.504*** -0.703***\n(0.057) (0.056) (0.054) (0.056) (0.055) (0.053) (0.053) (0.054)\nN 1,464 1,464 1,464 1,464 1,466 1,466 1,466 1,466\nZ-scored Yes Yes Yes Yes Yes Yes Yes Yes\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nPanel B: Trump voters\nBias treatment (b) -0.165*** -0.143** -0.135** -0.162*** -0.542*** -0.522*** -0.376*** -0.546***\n(0.056) (0.056) (0.057) (0.056) (0.072) (0.072) (0.069) (0.072)\nN 1,235 1,235 1,235 1,235 849 849 849 849\nZ-scored Yes Yes Yes Yes Yes Yes Yes Yes\nControls Yes Yes Yes Yes Yes Yes Yes Yes\np-value: a = b 0.000 0.000 0.000 0.000 0.073 0.166 0.304 0.118\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20134) and Experiment 2 (columns 4\u20138)\nwhere the dependent variables are post-treatment beliefs about the newsletter. Panel A and Panel B show results for Biden and\nTrump voters, respectively. \u201cBias treatment\u201d is a binary variable taking value one for respondents assigned to the right-wing biased\n(columns 1\u20134) or left-wing biased (columns 5\u20138) treatment arm. \u201cAccuracy\u201d of the newsletter is measured on a 5-point Likert scale\nfrom \u201cVery inaccurate\u201d to \u201cVery accurate.\u201d \u201cTrust\u201d is the trustworthiness of the newsletter and measured on a 5-point Likert scale\nfrom \u201cNot trustworthy at all\u201d to \u201cVery trustworthy.\u201d \u201cQuality\u201d of the newsletter is measured on a 5-point Likert scale from \u201cVery\nlow quality\u201d to \u201cVery high quality.\u201d \u201cIndex\u201d is a simple average of the accuracy, trust, and quality outcomes. All outcomes are z-\nscored using the relevant no bias group mean and standard deviation. All regressions include the standard set of control variables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n9\nTable B.6: Structural model: Preferences for accuracy and biased news \u2014 Robustness\nto using non-z-scored perceptions of accuracy and bias\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPreference for accuracy ( a) 0.337*** 0.297** 0.353\n(0.103) (0.117) (0.250)\nPreference for belief con\ufb01rmation ( b) 0.423*** 0.475*** 0.224\n(0.112) (0.136) (0.191)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.443*** 0.385*** 0.612**\n(0.113) (0.135) (0.264)\nN 5,014 2,930 2,084\nNote: This table presents parameter estimates that are analogous to the IV probit estimates presented\nin Table 2 except for one difference: Instead of using the z-scored post-treatment measure of per-\nceived accuracy and belief con\ufb01rmation (which are measured on 5-point Likert scales), we use\nnon-z-scored perceptions of accuracy and belief con\ufb01rmation.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n10\nTable B.7: Structural model: Preferences for accuracy and biased news \u2014 Robustness\nto using an index of accuracy-related beliefs\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPreference for accuracy ( a) 0.270*** 0.244*** 0.274\n(0.082) (0.093) (0.191)\nPreference for belief con\ufb01rmation ( b) 0.367*** 0.412*** 0.192\n(0.097) (0.117) (0.166)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.424*** 0.372*** 0.588**\n(0.111) (0.130) (0.274)\nN 5,014 2,930 2,084\nNote: This table presents parameter estimates that are analogous to the IV probit estimates presented\nin Table 2 except for one difference: Instead of using the z-scored post-treatment measure of per-\nceived accuracy (which is measured on a 5-point Likert scale), we use a z-scored index based on the\nperceived accuracy, quality and trustworthiness of the newsletter. Trustworthiness of the newsletter\nis measured on a 5-point scale from \u201cNot trustworthy at all\u201d to \u201cVery trustworthy.\u201d Quality of the\nnewsletter is measured on a 5-point scale from \u201cVery low quality\u201d to \u201cVery high quality.\u201d\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n11\nTable B.8: Structural model: Robustness to using a linear probability model\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPanel A: 2SLS\nPreference for accuracy ( a) 0.053*** 0.047** 0.057\n(0.018) (0.021) (0.045)\nPreference for belief con\ufb01rmation ( b) 0.071*** 0.082*** 0.040\n(0.020) (0.024) (0.038)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.424*** 0.365** 0.588*\n(0.132) (0.153) (0.305)\nN 5,014 2,930 2,084\nPanel B: Two-sample 2SLS\nPreference for accuracy ( a) 0.054*** 0.055** 0.035\n(0.019) (0.024) (0.058)\nPreference for belief con\ufb01rmation ( b) 0.048** 0.059** 0.025\n(0.022) (0.030) (0.045)\nImplicit weight on accuracy\u0010\na\na+b\u0011\n0.527*** 0.481** 0.583**\n(0.194) (0.218) (0.229)\nN: Choice data 5,014 2,930 2,084\nN: Belief data 1,896 963 933\nNote: This table presents the parameter estimates of a linear probability model where the dependent\nvariable is a binary indicator taking value one for respondents who choose to sign up to the newsletter.\nColumn 1 presents parameter estimates for the full sample, while columns 2 and 3 present estimates\nfor Biden and Trump voters, respectively. Panel A (\u201c2SLS\u201d) presents two-stage least-squares esti-\nmates where we instrument the endogeneous regressors (z-scored perceptions of accuracy and belief\ncon\ufb01rmation) with a saturated set of treatment arm indicators. We use data from Experiments 1 and 2\nwhere we elicit newsletter subscription choices and perceptions within-subject. In column 1, we also\ninclude interactions of the treatment assignment with a binary indicator for whether a respondent\nvoted for Trump as instruments to capture differential \ufb01rst-stage effects of the treatments. We include\na binary indicator for whether a respondent voted for Trump as a control variable in column 1. Robust\nstandard errors are shown in parentheses. Panel B (\u201cTwo-sample 2SLS\u201d) presents analogous two-\nsample two-stage least squares estimates. The endogenous regressors are again z-scored perceptions\nof accuracy and belief con\ufb01rmation. However, to estimate the \ufb01rst stage, we only use the \u201cbelief data\u201d\nfrom Experiment 4 (where we only elicit perceptions) and regress z-scored perceptions of accuracy\nand belief con\ufb01rmation on a saturated set of treatment indicators. To estimate the second stage model,\nwe use data from Experiments 1 and 2 and estimate a linear probability model using the predicted\nperceptions based on the \ufb01rst-stage estimates. We use the same set of instruments and controls as in\nPanel A. Standard errors in Panel B are obtained from a bootstrap procedure that resamples both the\nchoice data (from Experiment 1 and 2) and the belief data (from Experiment 4) with replacement.\n*p< 0.10, ** p< 0.05, *** p< 0.01.\n12\nTable B.9: Structural model: Replacing the belief con\ufb01rmation measure with perceived\ncomplexity or perceived entertainment value\nParameter estimates:\n(1) (2) (3)\nFull sample Biden voters Trump voters\nPanel A: Complexity instead of bias\nPreference for accuracy ( a) 0.278 -0.080 0.364\n(0.240) (0.690) (0.260)\nPreference for simplicity ( b) -0.171 -1.257 0.701\n(0.696) (1.936) (1.102)\nWeight on accuracy\u0010\na\na+b\u0011\n2.598 0.060 0.342\n(20.312) (0.400) (0.283)\nN 5,014 2,930 2,084\nPanel B: Entertainment instead of bias\nPreference for accuracy ( a) 0.364*** 0.526*** 0.261\n(0.130) (0.177) (0.202)\nPreference for entertainment ( b) 0.009 -0.500 0.529\n(0.429) (0.528) (0.522)\nWeight on accuracy\u0010\na\na+b\u0011\n0.975 20.335 0.331\n(1.129) (306.462) (0.311)\nN 5,014 2,930 2,084\nNote: This table presents parameter estimates that are analogous to the IV probit estimates presented\nin Table 2 except for two differences: Panel A replaces the z-scored post-treatment measure of\nbelief con\ufb01rmation with a z-scored post-treatment measure of perceived simplicity (i.e., the reverse-\ncoded perception of complexity). Panel B replaces the z-scored post-treatment measure of belief\ncon\ufb01rmation with a z-scored post-treatment measure of entertainment value.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n13\nTable B.10: Overview of experiments\nExperiment Sample Treatment Arms Main Outcomes\nExperiment 1:\nRight-wing bias vs.\nno bias\n(November 2021)Proli\ufb01c:\nn= 2,705\nAsPredicted ID:\n#78800Right-wing bias treatment : Information\nabout how The Boston Herald covered only\none statistic from the CBO report on the\nMinimum Wage Bill\nNo bias treatment : Information about\nhow The Boston Herald covered both\nstatistics from the CBO report on the\nMinimum Wage BillDemand for a newsletter\ncovering the top 3\narticles from The Boston\nHerald;\nPost-treatment beliefs\nabout newsletter\ncharacteristics\nExperiment 2:\nLeft-wing bias vs.\nno bias\n(December 2021)Proli\ufb01c:\nn= 2,319\nAsPredicted ID:\n#80266Left-wing bias treatment : Information\nabout how The Boston Herald covered only\none statistic from the CBO report on the\nHealthcare Bill\nNo bias treatment : Information about\nhow The Boston Herald covered both\nstatistics from the CBO report on the\nHealthcare BillDemand for a newsletter\ncovering the top 3\narticles from The Boston\nHerald;\nPost-treatment beliefs\nabout newsletter\ncharacteristics\nExperiment 3:\nMechanisms on\ninterpretation of\ntreatment\n(February 2022)Proli\ufb01c:\nn= 388\nAsPredicted ID:\n#87947Bias treatments : Information about how\nThe Boston Herald covered one statistic\nfrom the CBO report on the Healthcare\nBill/Minimum Wage Bill\nNo bias treatments : Information about\nhow The Boston Herald covered both\nstatistics from the CBO report on the\nHealthcare Bill/Minimum wage billOpen-ended question on\nwhy The Boston Herald\nreported the statistics in\nthis particular way\nExperiment 4:\nFirst-stage\nExperiment\n(February 2022)Proli\ufb01c:\nn= 1,910\nAsPredicted ID:\n#89081Bias treatments : Information about how\nThe Boston Herald covered one statistic\nfrom the CBO report on the Healthcare\nBill/Minimum Wage Bill\nNo bias treatments : Information about\nhow The Boston Herald covered both\nstatistics from the CBO report on the\nHealthcare Bill/Minimum wage billPost-treatment beliefs\nabout accuracy and bias\nNote: This table provides an overview of all experiments.\n14\nTable B.11: Secondary results: Beliefs about other newsletter characteristics\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6) (7) (8)\nEntertainment Complexity Easy No outlet bias Entertainment Complexity Easy No outlet bias\nPanel A: Biden voters\nBias treatment (a) -0.306*** -0.281*** 0.118** -0.551*** -0.141*** -0.272*** 0.139*** -0.548***\n(0.051) (0.053) (0.052) (0.022) (0.050) (0.052) (0.053) (0.021)\nN 1,464 1,464 1,464 1,469 1,466 1,466 1,466 1,469\nZ-scored Yes Yes Yes No Yes Yes Yes No\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0 0.806 0 0 0 0.870\np-value: Ex. 1 = Ex. 2 0.017 0.902 0.715 0.912 0.017 0.902 0.715 0.912\nPanel B: Trump voters\nBias treatment (b) 0.150*** -0.076 -0.012 -0.359*** -0.155** -0.049 -0.105 -0.407***\n(0.058) (0.055) (0.057) (0.026) (0.067) (0.069) (0.069) (0.031)\nN 1,235 1,235 1,235 1,236 849 849 849 850\nZ-scored Yes Yes Yes No Yes Yes Yes No\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0 0.664 0 0 0 0.780\np-value: Ex. 1 = Ex. 2 0.001 0.736 0.279 0.232 0.001 0.736 0.279 0.232\np-value: a = b 0.000 0.008 0.094 0.000 0.706 0.008 0.005 0.000\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20134) and Experiment 2 (columns 5\u20138) where the\ndependent variables are post-treatment beliefs about the newsletter and The Boston Herald\u2019s reporting. Panel A and Panel B show results for\nBiden and Trump voters, respectively. \u201cBias treatment\u201d is a binary variable taking value one for respondents assigned to the right-wing biased\n(columns 1\u20133) or left-wing biased (columns 4\u20136) treatment arm. \u201cEntertainment\u201d of the newsletter is measured on a 5-point Likert scale from \u201cNot\nentertaining at all\u201d to \u201cVery entertaining.\u201d \u201cComplex\u201d is the belief about the complexity of the newsletter and measured on a 5-point Likert scale\nfrom \u201cVery simple\u201d to \u201cVery complex.\u201d \u201cEasy\u201d is the belief about the dif\ufb01culty of understanding the newsletter and measured on a 5-point Likert\nscale from \u201cVery easy\u201d to \u201cVery dif\ufb01cult.\u201d \u201cNo outlet bias\u201d is a binary variable taking value one for respondents who think that The Boston Herald\nwould disclose both key \ufb01ndings from a CBO report, and zero otherwise (see Section F.1.1 for the instructions we used). The outcome variables in\ncolumns 1\u20133 and 5\u20137 are z-scored using the relevant no bias group mean and standard deviation. All regressions include the standard set of control\nvariables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n15\nTable B.12: Experiment 4: Treatment effects on perceptions of accuracy and bias\nLeft-wing bias Right-wing bias\n(1) (2) (3) (4)\nAccuracy Left-wing bias Accuracy Left-wing bias\nPanel A: Biden voters\nBias treatment -0.576*** 0.384*** -0.980*** -0.822***\n(0.093) (0.111) (0.095) (0.107)\nN 486 486 477 477\nControls Yes Yes Yes Yes\nPanel B: Trump voters\nBias treatment -0.477*** 0.408*** -0.231** -0.308***\n(0.094) (0.100) (0.096) (0.104)\nN 473 473 460 460\nControls Yes Yes Yes Yes\nNote: This table presents OLS regression estimates using data from Experiment 4 (see Table B.10\nfor an overview of experiments) where the dependent variables are perceptions of the newsletter\u2019s\naccuracy (columns 1 and 3) and the perceived left-wing bias of the newsletter (columns 2 and 4).\nPanel A shows results for Biden voters and Panel B shows results for Trump voters. \u201cBias treatment\u201d\nis a binary indicator for whether respondents were informed that The Boston Herald reported the\nnews in a left-wing biased way (columns 1 and 2) or in a right-wing biased way (columns 3 and 4),\nand zero otherwise. \u201cAccuracy\u201d of the newsletter is measured on a 5-point Likert scale from \u201cVery\ninaccurate\u201d to \u201cVery accurate.\u201d \u201cLeft-wing bias\u201d is measured on a 5-point Likert scale from \u201cVery\nright-wing biased\u201d to \u201cVery left-wing biased.\u201d All outcomes have been z-scored using the relevant\nno bias group mean and standard deviation. All regressions include standard control variables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n16\nTable B.13: Experiment 4: Treatment effects on secondary outcomes\nLeft-wing bias Right-wing bias\n(1) (2) (3) (4) (5) (6) (7) (8)\nTrust Quality Entertainment Complexity Trust Quality Entertainment Complexity\nPanel A: Biden voters\nBias treatment -0.495*** -0.313*** -0.157* -0.300*** -0.980*** -0.825*** -0.419*** -0.271***\n(0.088) (0.092) (0.084) (0.091) (0.100) (0.095) (0.093) (0.089)\nN 486 486 486 486 477 477 477 477\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nPanel B: Trump voters\nBias treatment -0.393*** -0.446*** -0.123 -0.064 -0.145 -0.081 0.039 -0.037\n(0.093) (0.102) (0.099) (0.093) (0.094) (0.099) (0.100) (0.095)\nN 472 472 472 472 460 460 460 460\nControls Yes Yes Yes Yes Yes Yes Yes Yes\nNote: This table shows OLS regression estimates using data from Experiment 4 where the dependent variables are post-treatment beliefs about\nthe newsletter (see Table B.10 for an overview of experiments). Panel A shows results for Biden voters and Panel B shows results for Trump\nvoters. \u201cBias treatment\u201d is a binary indicator for whether respondents were informed that The Boston Herald reported the news in a left-wing\nbiased way (columns 1\u20134) or in a right-wing biased way (columns 5\u20138). \u201cTrust\u201d is the trustworthiness of the newsletter and measured on a\n5-point Likert scale from \u201cNot trustworthy at all\u201d to \u201cVery trustworthy.\u201d \u201cQuality\u201d of the newsletter is measured on a 5-point Likert scale from\n\u201cVery low quality\u201d to \u201cVery high quality.\u201d \u201cEntertainment\u201d of the newsletter is measured on a 5-point Likert scale from \u201cNot entertaining at all\u201d\nto \u201cVery entertaining.\u201d \u201cComplex\u201d is the belief about the complexity of the newsletter and measured on a 5-point Likert scale from \u201cVery\nsimple\u201d to \u201cVery complex.\u201d All outcomes are z-scored using the relevant no bias group mean and standard deviation. All regressions include\nthe standard set of control variables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n17\nTable B.14: Heterogeneity in effects by news demand outside the experiment\nDependent variable: Newsletter demand\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6)\nRespondents who read:No other\noutletMainly\nleft-wingMainly\nright-wingNo other\noutletMainly\nleft-wingMainly\nright-wing\nBias treatment 0.004 -0.047*** -0.129*** -0.035 -0.049** -0.031\n(0.023) (0.018) (0.042) (0.029) (0.020) (0.052)\nN 599 1,515 340 447 1,408 241\nControls Yes Yes Yes Yes Yes Yes\nMean of dep. var. 0.093 0.158 0.179 0.107 0.196 0.187\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20133) and\nExperiment 2 (columns 4\u20136) where the dependent variable is a binary indicator taking value one for\nrespondents who said \u201cYes\u201d to receiving the weekly newsletter, and zero for those who said \u201cNo.\u201d\nColumns 1 and 4 restrict to respondents who indicated pre-treatment that they do not read news from\nany of the 21 news outlets that we listed. Columns 2 and 5 restrict to respondents who read more left-\nwing than right-wing biased outlets, while columns 3 and 6 restrict to respondents who read more\nright-wing than left-wing biased outlets. We used a classi\ufb01cation of outlet ideology from the website\nmediabiasfactcheck.com as of January 26, 2022. \u201cTreatment\u201d is a binary variable taking value one\nfor respondents assigned the right-wing biased (Experiment 1) or the left-wing biased treatment arm\n(Experiment 2), and zero otherwise. All regressions include the standard set of control variables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n18\nTable B.15: Robustness: Results among respondents who did not correctly guess the study\npurpose\nExperiment 1: Right-wing bias Experiment 2: Left-wing bias\n(1) (2) (3) (4) (5) (6)\nAccuracy Left-wing bias Demand Accuracy Left-wing bias Demand\nPanel A: Biden voters\nBias treatment (a) -0.876*** -0.815*** -0.077*** -0.695*** 0.266*** -0.018\n(0.058) (0.063) (0.017) (0.056) (0.061) (0.020)\nN 1,376 1,376 1,376 1,393 1,393 1,393\nZ-scored Yes Yes No Yes Yes No\nControls Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0.178 0 0 0.186\np-value: Ex. 1 = Ex. 2 0.022 0.000 0.023 0.022 0.000 0.023\nPanel B: Trump voters\nBias treatment (b) -0.164*** -0.456*** 0.006 -0.537*** 0.254*** -0.057**\n(0.057) (0.064) (0.021) (0.074) (0.072) (0.025)\nN 1,191 1,191 1,191 814 814 814\nZ-scored Yes Yes No Yes Yes No\nControls Yes Yes Yes Yes Yes Yes\nNo bias treatment mean 0 0 0.159 0 0 0.195\np-value: Ex. 1 = Ex. 2 0.000 0.000 0.051 0.000 0.000 0.051\np-value: a = b 0.000 0.004 0.002 0.126 0.721 0.220\nNote: This table presents OLS regression estimates using data from Experiment 1 (columns 1\u20133) and Experiment\n2 (columns 4\u20136) where the dependent variables are post-treatment beliefs about accuracy (columns 1 and 4),\nthe perceived left-wing bias of the newsletter (columns 2 and 5), and newsletter demand (columns 3 and 6).\nPanel A and Panel B present results for Biden and Trump voters, respectively. The regressions only include the\nsubset of respondents who did not correctly guess the hypothesis of the study at the open-ended question about\nstudy purpose at the end of the survey. \u201cBias treatment\u201d is a binary variable taking value one for respondents\nassigned the right-wing bias (columns 1\u20133) or the left-wing bias (columns 4\u20136) treatment arm, and zero for\nrespondents in the no bias treatment arm. \u201cDemand\u201d is a binary variable taking value one for respondents who\nsaid \u201cYes\u201d to receiving the weekly newsletter, and zero for those who said \u201cNo.\u201d \u201cAccuracy\u201d of the newsletter is\nmeasured on a 5-point Likert scale from \u201cVery inaccurate\u201d to \u201cVery accurate.\u201d \u201cLeft-wing bias\u201d is measured\non a 5-point Likert scale from \u201cVery right-wing biased\u201d to \u201cVery left-wing biased.\u201d \u201cAccuracy\u201d and \u201cLeft-\nwing bias\u201d have been z-scored using the relevant no bias group mean and standard deviation. \u201c p-value: Ex. 1\n= Ex. 2\u201d provides p-values for tests of the equality of coef\ufb01cients between Experiment 1 and Experiment 2.\n\u201cp-value: a = b\u201d provides p-values for tests of the equality of coef\ufb01cients between Trump and Biden voters.\nAll regressions include a set of basic control variables: gender, age, education, race and ethnicity, log income,\nemployment status, Census region, voting, political af\ufb01liation, ideology, interest in economic news, whether\nthey have read any of a list of 21 newspapers during the last 12 months, whether they have read The Boston\nHerald, whether they currently subscribe to any newsletters, and their pre-treatment beliefs about how The\nBoston Herald reported about the CBO \ufb01ndings.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n19\nTable B.16: Test of balance of treatment vs. control: Experiment 3\nBias (B) No bias (NB) P-value (B - NB) Observations\nMale 0.49 0.47 0.758 388\nAge 35.12 36.36 0.364 388\nWhite 0.82 0.84 0.533 388\nIncome (midpoint) 71576.92 67551.81 0.383 388\nCollege degree 0.70 0.67 0.540 388\nFull-time employee 0.54 0.53 0.845 388\nNortheast 0.16 0.16 0.924 388\nMidwest 0.23 0.18 0.230 388\nWest 0.19 0.26 0.102 388\nSouth 0.42 0.40 0.744 388\nNote: This table provides a balance test between the treatment and no bias group\nusing all respondents from Experiment 3 (see Table B.10 for an overview of the ex-\nperiments). \u201cMale\u201d is a binary variable taking value one for male respondents, and\nzero otherwise. \u201cAge\u201d is the numerical age of the respondent in years. \u201cWhite\u201d is a\nbinary variable taking value one if the respondent selected \u201cCaucasian/White,\u201d and\nzero otherwise. \u201cIncome (midpoint)\u201d is coded continuously as the income bracket\u2019s\nmidpoint (Less than $15,000, $15,000 to $24,999, $25,000 to $49,999, $50,000\nto $74,999, $75,000 to $99,999, $100,000 to $149,999, $150,000 to $200,000,\n$200,000 or more). \u201cCollege degree\u201d is a binary variable taking value one if the\nrespondent has a college degree, and zero otherwise. \u201cFull-time employee\u201d is a\nbinary variable taking value one if the respondent is a full-time employee, and zero\notherwise. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with\nvalue one if the respondent lives in the respective region, and zero otherwise.\n20\nTable B.17: Test of balance of treatment vs. control: Experiment 4\nBias (B) No bias (NB) P-value (B - NB) Observations\nMale 0.48 0.48 0.968 1910\nAge 38.63 39.03 0.554 1910\nWhite 0.82 0.82 0.680 1910\nIncome (midpoint) 71156.05 68794.64 0.248 1910\nCollege degree 0.69 0.70 0.954 1910\nFull-time employee 0.51 0.48 0.272 1910\nNortheast 0.18 0.20 0.240 1910\nMidwest 0.20 0.21 0.855 1910\nWest 0.22 0.21 0.665 1910\nSouth 0.40 0.39 0.465 1910\nNote: This table provides a balance test between the treatment and no bias group\nusing all respondents from Experiment 4 (see Table B.10 for an overview of the ex-\nperiments). \u201cMale\u201d is a binary variable taking value one for male respondents, and\nzero otherwise. \u201cAge\u201d is the numerical age of the respondent in years. \u201cWhite\u201d is a\nbinary variable taking value one if the respondent selected \u201cCaucasian/White,\u201d and\nzero otherwise. \u201cIncome (midpoint)\u201d is coded continuously as the income bracket\u2019s\nmidpoint (Less than $15,000, $15,000 to $24,999, $25,000 to $49,999, $50,000\nto $74,999, $75,000 to $99,999, $100,000 to $149,999, $150,000 to $200,000,\n$200,000 or more). \u201cCollege degree\u201d is a binary variable taking value one if the\nrespondent has a college degree, and zero otherwise. \u201cFull-time employee\u201d is a\nbinary variable taking value one if the respondent is a full-time employee, and zero\notherwise. \u201cNortheast,\u201d \u201cMidwest,\u201d \u201cWest\u201d and \u201cSouth\u201d are binary variables with\nvalue one if the respondent lives in the respective region, and zero otherwise.\n21\nTable B.18: Heterogeneity by ideology\nDependent variable: Newsletter demand\n(1) (2)\nExperiment 1 Experiment 2\nBias treatment 0.018 -0.023\n(0.035) (0.043)\nBias treatment\u0002Liberal -0.020** -0.003\n(0.010) (0.012)\nLiberal 0.030** 0.030**\n(0.013) (0.014)\nN 2705 2319\nNote: This table presents OLS regression estimates using data from Experiment 1 (column 1) and\nExperiment 2 (column 2) where the dependent variables are newsletter demand. \u201cNewsletter de-\nmand\u201d is a binary variable taking value one for respondents who said \u201cYes\u201d to receiving the weekly\nnewsletter, and zero for those who said \u201cNo.\u201d \u201cBias treatment\u201d is a binary variable taking value one\nfor respondents assigned the right-wing bias (column 1) or the left-wing bias (column 2) treatment\narm, and zero for respondents in the no bias treatment arm. \u201cLiberal\u201d is measured on a 5-point Likert\nscale from 1: Very conservative to 5: Very liberal . All regressions include the standard set of control\nvariables.\n*p< 0.10, ** p< 0.05, *** p< 0.01. Robust standard errors in parentheses.\n22\nFigure B.1: Pre-treatment beliefs about bias by treatment status\n(a) Experiment 1: Beliefs about the coverage of the Healthcare Plan\n0.2.4.6\nPoverty onlyEmployment onlyBoth statistics Poverty onlyEmployment onlyBoth statisticsUnbiased BiasedFraction\nP-value obtained from a Kolmogorov-Smirnov test for equality of distribution functions: 0.927\n(b) Experiment 2: Beliefs about the coverage of the $15 Minimum Wage Bill\n0.2.4.6\nUninsured onlyDeficit only Both statistics Uninsured onlyDeficit only Both statisticsUnbiased BiasedFraction\nP-value obtained from a Kolmogorov-Smirnov test for equality of distribution functions: 0.899\nNote: Panel (a) and Panel (b) display the distribution of pre-treatment beliefs about reporting\nby The Boston Herald for Experiment 1 and 2, respectively. Each panel displays the\ndistribution of pre-treatment beliefs separately for respondents in the no bias treatment arm\n(\u201cunbiased\u201d) and the biased treatment arm (\u201cbiased\u201d), i.e., the right-wing bias treatment\nin Experiment 1 and the left-wing bias treatment in Experiment 2 (see Table B.10 for an\noverview of experiments).\n23\nFigure B.2: Treatment effects on mentioning balanced reporting in the open-ended\nresponses\n(a) Biden voters: Right-wing bias\n p = 0.012\nn = 57 n = 57\n0.1.2.3.4Mean \u00b1 s.e.m.\nNo bias Right-wing bias (b) Trump voters: Right-wing bias\n p = 0.001\nn = 38 n = 42\n0.1.2.3.4Mean \u00b1 s.e.m.\nNo bias Right-wing bias\n(c) Biden voters: Left-wing bias\n p < 0.001\nn = 64 n = 62\n0.1.2.3.4Mean \u00b1 s.e.m.\nNo bias Left-wing bias (d) Trump voters: Left-wing bias\n p = 0.005\nn = 34 n = 34\n0.1.2.3.4Mean \u00b1 s.e.m.\nNo bias Left-wing bias\nNote: The \ufb01gure presents treatment effects on whether respondents mentioned balanced\nreporting in their responses to the open-ended motives question in Experiment 3 (see\nTable B.10). Speci\ufb01cally, respondents were asked why they think The Boston Herald\nreported in the way that it did. Each panel displays the share of respondents whose responses\nwere hand-coded to the \u201cno bias\u201d category (e.g., \u201cThey were probably trying to report fairly\nwithout bias,\u201d \u201cThey were trying to give the full picture,\u201d and \u201cThey tried to report fairly\nand accurately\u201d would all be classi\ufb01ed as \u201cbalanced\u201d). Panel (a) and Panel (b) compare the\nright-wing bias treatment to the no bias treatment (analogous to Experiment 1). Panel (c)\nand Panel (d) compare the left-wing bias treatment to the no bias treatment (analogous to\nExperiment 2). Panel (a) and Panel (c) focus on the subsample of respondents who voted for\nJoe Biden, while Panel (b) and Panel (d) focus on respondents who voted for Donald Trump.\nThep-values are obtained from a two-sample t-test of equality of means. Standard errors of\nthe mean are shown.\n24\nFigure B.3: Perceived study purpose\n36.2\n4.111.5\n0.39.016.9\n11.7\n10.4\n010203040Percent\nbias correct dk junk media opinion other politics\nNote: This \ufb01gure shows the distribution of the perceived study purpose among our respondents\nin Experiment 1 and 2. Speci\ufb01cally, at the end of the main experiments, respondents were\nasked the following open-ended question: \u201cIf you had to guess, what would you say was\nthe purpose of this study?\u201d A team of research assistants hand-coded the responses based\non the following coding scheme: bias: Explicit mentions of bias in the media. Example\nresponse: \u201cAssessing social perceived bias towards left wing vs ring wing media sources.\u201d\ncorrect : People correctly guessing the study\u2019s hypothesis (how perceptions of bias shape\npeople\u2019s news consumption). Example response: \u201cI believe that the purpose of the study\nis to see how participants respond to bias in news stories in the media.\u201d junk: Nonsensical\nresponses. media : Generic mentions that the study is about perceptions of media (without\nexplicitly mentioning bias). Example response: \u201cHow perceptions of news organizations\nmesh with reality.\u201d opinion : Generic mentions that the study tries to assess opinions and\nattitudes. Example response: \u201cTo see how people judge a news source such as the Boston\nHerald.\u201d dk(don\u2019t know): People expressing uncertainty. Example response: \u201cI don\u2019t know.\u201d\nother : Responses that do not \ufb01t into any of the other categories. politics : People generically\ntalking about politics. Example response: \u201csomething about political parties.\u201d\n25\nC Structural estimates: Robustness\nOne potential concern is that perceptions of accuracy and bias are endogenous to\nchoices. Speci\ufb01cally, respondents might have a taste for providing survey responses\nthat are internally consistent (Falk and Zimmermann, 2015). In our main experiments,\nwe elicit demand for our newsletter before asking respondents to state their beliefs\nabout the newsletter\u2019s accuracy and political bias. A taste for consistency would thus\nimply that the act of subscribing to our newsletter has an effect on respondents\u2019 stated\nbelief that is independent from our treatments, which would imply that we do not\nobtain unbiased estimates of respondents\u2019 beliefs in our main experiments. This could\npotentially bias our structural estimates of the relative importance of accuracy compared\nto belief con\ufb01rmation motives. However, note that this cannot affect our results if\nthe magnitude of the consistency bias in survey responses is identical for the survey\nmeasures eliciting beliefs about accuracy and beliefs about the political bias.\nTo address this concern, we conducted an additional pre-registered experiment on\nProli\ufb01c in February 2022 (Experiment 4; see Table B.10).2In this experiment, we\nadminister the same treatments as in Experiment 1 and 2 but respondents are not offered\nthe chance to subscribe to the newsletter. Instead, we inform them about the existence of\nthe newsletter and then elicit respondents\u2019 post-treatment beliefs about the newsletter\u2019s\naccuracy and bias using the same survey measures as in our previous experiments.\nWhile this addresses concerns about consistency bias in survey responses, the absence\nof an active choice might lower engagement with the survey. We therefore view this as\na complementary robustness check.\nWe then use a two-sample instrumental variables strategy to estimate a linear\nprobability model where the binary dependent variable is the decision to sign up to\nour newsletter (Angrist and Krueger, 1995; Inoue and Solon, 2010). Speci\ufb01cally, we\nuse OLS to estimate equations the \ufb01rst-stage effect of our treatments on perceptions\nof accuracy and belief con\ufb01rmation (see equations 3 and 4) using the belief data\nfrom Experiment 4 (where we only elicit perceptions). We then use the choice data\nfrom Experiments 1 and 2 and estimate a linear probability model using the predicted\nperceptions of accuracy and belief con\ufb01rmation obtained from the \ufb01rst-stage regression\n2Our sample includes 968 Biden voters and 942 Trump voters. The median response time was 3.5\nminutes. To recruit enough Trump voters, we allowed 624 Trump voters who had participated in the\nmain experiments three to four months prior to participate in Experiment 4. Reassuringly, we see no\ntreatment heterogeneity based on the original treatment assignment.\n26\nas regressors. For inference, we obtain standard errors using a bootstrap procedure\nthat resamples the choice data (from Experiments 1 and 2) and the belief data (from\nExperiment 4) with replacement.\nPanel B of Table B.8 presents the parameter estimates from this robustness exercise.\nThe estimates using the full sample support the quantitative importance of people\u2019s\npreference for belief con\ufb01rmation ( p< 0.01, column 1). Again, the implied weight on\naccuracy is close to and not statistically signi\ufb01cantly different from 0.5, corroborating\nthe robustness of our model estimates. If anything, the point estimates are closer to 0.5\nand exhibit less heterogeneity across political groups (columns 2 and 3).\nThis suggests that consistency bias in survey responses is unlikely to account for\nour structural \ufb01nding that accuracy and belief con\ufb01rmation motives are approximately\nequally important drivers of people\u2019s demand for news.\n27\nD Research transparency\nPreregistration Our experiments were all preregistered in the AsPredicted registry\n(#78800, #80266, #87947, #89081). The preregistration includes details on the experi-\nmental design, the sampling process, planned sample size, exclusion criteria, and the\nmain analyses. Below, we document deviations from the preregistration for our main\nexperiments (Experiments 1 and 2):\n\u2022The set of control variables speci\ufb01ed in our pre-analysis plan erroneously omitted\nrespondents\u2019 pre-treatment belief about how The Boston Herald reported the\nnews (two indicators). In our main speci\ufb01cation, we control for pre-treatment\nbeliefs.\n\u2022In both experiments, Proli\ufb01c\u2019s subject pool was not large enough to achieve the\ntargeted sample size of 1,500 Trump voters within the pre-speci\ufb01ed sampling\nperiod of \ufb01ve days. In Experiment 1, we managed to recruit 1,236 Trump voters,\nwhile we managed to recruit 850 Trump voters in Experiment 2.\n\u2022In our main analysis, the treatment indicator takes value one for respondents in\nthe \u201cright-wing biased\u201d or \u201cleft-wing biased\u201d treatment arm, and value zero for\nrespondents in the \u201cunbiased\u201d treatment arm. This is numerically equivalent to\nthe speci\ufb01cation we speci\ufb01ed on AsPredicted.\nEthical approval The experimental study received ethics approval from the German\nAssociation for Experimental Economic Research, and the ethics committee of the\nUniversity of Cologne.\nData and code availability The experimental data and the analysis code will be\nmade available upon publication.\n28\nE Newsletter\nThis section provides additional details about how we published our newsletter.\nSelection of news articles We employed the following procedure to select three\narticles for each edition of the weekly newsletter. On Mondays, when the next edi-\ntion of the newsletter is to be published, we used a Firefox browser and went on\nhttps://duckduckgo.com . The advantage of this search engine over other engines,\nsuch as Google, is that search results are not biased by the researcher\u2019s own search his-\ntory or interests. After setting the search engine\u2019s settings to \u201cRegion: US (English)\u201d and\n\u201cTime: Past week\u201d, we used the following search query: site:bostonherald.com\neconomic policy . We then selected the top three articles matching the newsletter\u2019s\nfocus on economic policy from the results page.\nNewsletter editions Each edition of our newsletter had the same basic structure.\nAcross editions, we exchanged the article headlines and links to the articles. The\ntemplate we used for our newsletter editions is presented below:\nThank you very much for participating in our survey [last week, two weeks\nago, three weeks ago, four weeks ago] . According to our records, you\nalso wanted to subscribe to our weekly newsletter featuring articles related\nto economic policy over the next month. This is the [\ufb01rst, second, third,\nfourth and \ufb01nal] of four editions of our newsletter. The newsletter includes\nthe top three articles published in The Boston Herald based on readership.\nIndividual links to the articles included this week are included below.\nArticle 1: Biden\u2019s climate plan aims to reduce methane emissions\nLink: https://www.bostonherald.com/2021/11/02/bidens-climate-plan-aims-\nto-reduce-methane-emissions/\nArticle 2: Fed pulls back economic aid in face of rising uncertainties\nLink: https://www.bostonherald.com/2021/11/03/fed-pulls-back-economic-\naid-in-face-of-rising-uncertainties/\nArticle 3: Biden hails infrastructure win as \u2019monumental step forward\u2019\nLink: https://www.bostonherald.com/2021/11/06/biden-hails-infrastructure-\nwin-as-monumental-step-forward/\n29\nLogistics We released the newsletter on Mondays on the following dates in 2022 at\nabout 6 am Eastern Time: Nov 8, Nov 15, Nov 22, Nov 29, Dec 7, Dec 13, Dec 20.\nTo provide respondents with our newsletter, we used the capability of Proli\ufb01c to send\ndirect messages to respondents on Proli\ufb01c\u2019s platform. This allows us to distribute the\nnewsletter without having to elicit any personally identi\ufb01able information. This, in turn,\nensures that we can measure newsletter demand irrespective of privacy concerns. If\nrespondents indicated that they wish to unsubscribe from our newsletter, we did not\nsend them any additional editions of our newsletter in the following weeks.\nArticles Below is a complete list of all articles we included across newsletters.\n\u2022 Biden\u2019s climate plan aims to reduce methane emissions\n\u2022 Fed pulls back economic aid in face of rising uncertainties\n\u2022 Biden hails infrastructure win as \u2019monumental step forward\u2019\n\u2022 Yellen says quashing COVID is key to lowering in\ufb02ation\n\u2022 Biden bill would give local news outlets \u2018shot in the arm\u2019\n\u2022 Biden bill includes boost for union-made electric vehicles\n\u2022 House OKs $2T social, climate bill in Biden win; Senate next\n\u2022 Biden signs $1T infrastructure deal with bipartisan crowd\n\u2022 No settlement for separated migrant families amid criticism\n\u2022 Biden Administration approves 2nd large US offshore wind farm\n\u2022 Some fear China could win from US spat with Marshall Islands\n\u2022 Will Maine\u2019s anti-mining laws keep needed minerals underground?\n\u2022 Massive $4 billion ARPA, surplus tax revenue bill set for passage\n\u2022 Biden, Putin square off as tension grows on Ukraine border\n\u2022Auditor: Feds gave nearly $4 billion in pandemic relief to businesses that were probably ineligible\n\u2022 New in\ufb02ation number feeds angst about Democrats\u2019 $2T bill\n\u2022 Job listings and new quitting remain near record highs\n\u2022 In\ufb02ation hits a 39-year high and isn\u2019t going away\n\u2022 SALT in the wound: Expanded state and local tax deduction stranded as bill dies\n\u2022 People pressure governments worldwide to act on in\ufb02ation\n\u2022 Here come the rate hikes: Fed sees 3 in 2022\n30\nF Screenshots\nF.1 Experiment 1: Right-wing biased news\nF.1.1 Pre-treatment questions\n31\n32\n33\n34\n35\n36\n37\nF.1.2 Treatment: Right-wing biased news\n38\n39\nF.1.3 Treatment: Unbiased news\n40\n41\nF.1.4 Post-treatment outcomes\n42\n43\n44\n45\n46\n47\n48\n49\nF.2 Experiment 2: Left-wing biased news\nF.2.1 Treatment: Left-wing biased news\n50\n51\nF.2.2 Treatment: Unbiased news\n52\n53\nF.3 Post-treatment outcomes\n54\n55\nF.4 Experiment 3: Open-ended motives\nF.4.1 Treatment 1: No bias (minimum wage bill)\n56\nF.4.2 Treatment 2: Right-wing bias (minimum wage bill)\n57\nF.4.3 Treatment 3: No bias (healthcare plan)\n58\nF.4.4 Treatment 4: Left-wing bias (healthcare plan)\n59\nF.5 Experiment 4: Beliefs about newsletter characteristics\nF.5.1 Left-wing bias: Prior (control)\n60\nF.5.2 Left-wing bias: Prior (treatment)\n61\nF.5.3 Right-wing bias: Prior (control)\n62\nF.5.4 Right-wing bias: Prior (treatment)\n63\nF.5.5 Left-wing bias: Information provision (control)\nF.5.6 Left-wing bias: Information provision (treatment)\nF.5.7 Right-wing bias: Information provision (control)\n64\nF.5.8 Right-wing bias: Information provision (treatment)\nF.5.9 Post-treatment outcomes\n65\n66\n67", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "The demand for news: Accuracy concerns versus belief confirmation motives", "author": ["F Anand Chopra", "I Haaland", "C Roth"], "pub_year": "2022", "venue": "NA", "abstract": "We examine the relative importance of accuracy concerns and belief confirmation motives  in driving the demand for news. In experiments with US respondents, we first vary beliefs"}, "filled": false, "gsrank": 476, "pub_url": "https://www.econstor.eu/handle/10419/262101", "author_id": ["", "Pss00QoAAAAJ", "J4gmk2MAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:h4HSyLiQv2QJ:scholar.google.com/&output=cite&scirp=475&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D470%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=h4HSyLiQv2QJ&ei=W7WsaJnZIo6IieoP0sKRuAk&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:h4HSyLiQv2QJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://www.econstor.eu/bitstream/10419/262101/1/ECONtribute-157-2022.pdf"}}, {"title": "The covid-19 infodemic: Can the crowd judge recent misinformation objectively?", "year": "2020", "pdf_data": "The COVID-19 Infodemic: Can the Crowd Judge\nRecent Misinformation Objectively?\nKevin Roitero\nroitero.kevin@spes.uniud.it\nUniversity of Udine\nUdine, ItalyMichael Soprano\nsoprano.michael@spes.uniud.it\nUniversity of Udine\nUdine, ItalyBeatrice Portelli\nportelli.beatrice@spes.uniud.it\nUniversity of Udine\nUdine, ItalyDamiano Spina\ndamiano.spina@rmit.edu.au\nRMIT University\nMelbourne, Australia\nVincenzo Della Mea\nvincenzo.dellamea@uniud.it\nUniversity of Udine\nUdine, ItalyGiuseppe Serra\ngiuseppe.serra@uniud.it\nUniversity of Udine\nUdine, ItalyStefano Mizzaro\nmizzaro@uniud.it\nUniversity of Udine\nUdine, ItalyGianluca Demartini\ndemartini@acm.org\nThe University of\nQueensland, Australia\nABSTRACT\nMisinformation is an ever increasing problem that is difficult to\nsolve for the research community and has a negative impact on\nthe society at large. Very recently, the problem has been addressed\nwith a crowdsourcing-based approach to scale up labeling efforts:\nto assess the truthfulness of a statement, instead of relying on a few\nexperts, a crowd of (non-expert) judges is exploited. We follow the\nsame approach to study whether crowdsourcing is an effective and\nreliable method to assess statements truthfulness during a pandemic.\nWe specifically target statements related to the COVID-19 health\nemergency, that is still ongoing at the time of the study and has\narguably caused an increase of the amount of misinformation that\nis spreading online (a phenomenon for which the term \u201cinfodemic\u201d\nhas been used). By doing so, we are able to address (mis)information\nthat is both related to a sensitive and personal issue like health and\nvery recent as compared to when the judgment is done: two issues\nthat have not been analyzed in related work.\nIn our experiment, crowd workers are asked to assess the truth-\nfulness of statements, as well as to provide evidence for the assess-\nments as a URL and a text justification. Besides showing that the\ncrowd is able to accurately judge the truthfulness of the statements,\nwe also report results on many different aspects, including: agree-\nment among workers, the effect of different aggregation functions,\nof scales transformations, and of workers background / bias. We\nalso analyze workers behavior, in terms of queries submitted, URLs\nfound / selected, text justifications, and other behavioral data like\nclicks and mouse actions collected by means of an ad hoc logger.\nACM Reference Format:\nKevin Roitero, Michael Soprano, Beatrice Portelli, Damiano Spina, Vincenzo\nDella Mea, Giuseppe Serra, Stefano Mizzaro, and Gianluca Demartini. 2020.\nThe COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation\nObjectively?. In The 29th ACM International Conference on Information and\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM \u201920, October 19\u201323, 2020, Virtual Event, Ireland\n\u00a92020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-6859-9/20/10. . . $15.00\nhttps://doi.org/10.1145/3340531.3412048Knowledge Management (CIKM \u201920), October 19\u201323, 2020, Virtual Event, Ire-\nland. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3340531.\n3412048\n1 INTRODUCTION\n\u201cWe\u2019re concerned about the levels of rumours and misinfor-\nmation that are hampering the response. [. . . ] we\u2019re not just\nfighting an epidemic; we\u2019re fighting an infodemic. Fake news\nspreads faster and more easily than this virus, and is just as\ndangerous. That\u2019s why we\u2019re also working with search and me-\ndia companies like Facebook, Google, Pinterest, Tencent, Twitter,\nTikTok, YouTube and others to counter the spread of rumours\nand misinformation. We call on all governments, companies\nand news organizations to work with us to sound the appropri-\nate level of alarm, without fanning the flames of hysteria. \u201d\nThese are the alarming words used by Dr. Tedros Adhanom Ghe-\nbreyesus, the WHO (World Health Organization) Director General\nduring his speech at the Munich Security Conference on 15 Febru-\nary 2020.1It is telling that the WHO Director General chooses to\ntarget explicitly misinformation related problems.\nIndeed, during the still ongoing COVID-19 health emergency,\nall of us have experienced mis- and dis-information. The research\ncommunity has focused on several COVID-19 related issues [ 4],\nranging from machine learning systems aiming to classify state-\nments and claims on the basis of their truthfulness [ 23], search\nengines tailored to the COVID-19 related literature, as in the ongo-\ning TREC-COVID Challenge2[26], topic-specific workshops like\nthe NLP COVID workshop at ACL\u201920,3and evaluation initiatives\nlike the TREC Health Misinformation Track 2020.4More than the\nacademic research community, commercial social media platforms\nalso have looked at this issue.5Among all the approaches, in some\nvery recent work, Roitero et al . [27] , La Barbera et al . [17] , Roitero\net al. [29] have studied if crowdsourcing can be used to identify\nmisinformation. As it is well known, crowdsourcing means to out-\nsource to a large mass of unknown people (the \u201ccrowd\u201d), by means\n1https://www.who.int/dg/speeches/detail/munich-security-conference\n2https://ir.nist.gov/covidSubmit/\n3https://www.nlpcovid19workshop.org/\n4https://trec-health-misinfo.github.io/\n5https://www.forbes.com/sites/bernardmarr/2020/03/27/finding-the-truth-about-\ncovid-19-how-facebook-twitter-and-instagram-are-tackling-fake-news/ and\nhttps://spectrum.ieee.org/view-from-the-valley/artificial-intelligence/machine-\nlearning/how-facebook-is-using-ai-to-fight-covid19-misinformationarXiv:2008.05701v1  [cs.IR]  13 Aug 2020\nof an open call, a task that is usually performed by a few experts.\nThat recent work [ 17,27,29] specifically crowdsource the task of\nmisinformation identification, or rather assessment of the truthful-\nness of statements made by public figures (e.g., politicians), usually\non political, economical, and societal issues. That the crowd is able\nto identify misinformation might sound implausible at first\u2014isn\u2019t\nthe crowd the very mean to spread misinformation? However, on\nthe basis of recent research [ 17,27,29], it appears that the crowd\ncan provide high quality truthfulness labels, provided that adequate\ncountermeasures and quality assurance techniques are employed.\nIn this paper we address the very same problem, but focusing on\nstatements about COVID-19. This is motivated by several reasons.\nFirst, COVID-19 is of course a hot topic but, although there is a\ngreat amount of researchers efforts worldwide devoted to its study,\nthere is no study yet using crowdsourcing to assess truthfulness\nof COVID-19 related statements. To the best of our knowledge, we\nare the first to report on crowd assessment of COVID-19 related\nmisinformation. Second, the health domain is particularly sensitive,\nso it is interesting to understand if the crowdsourcing approach is\nadequate also in such a particular domain. Third, in the previous\nwork [ 17,27,29] the statements judged by the crowd were not re-\ncent. This means that evidence on statement truthfulness was often\navailable out there (on the Web), and although the experimental\ndesign prevented to easily find that evidence, it cannot be excluded\nthat the workers did find it, or perhaps they were familiar with the\nparticular statement because, for instance, it had been discussed\nin the press. By focusing on COVID-19 related statements we in-\nstead naturally target recent statements: in some cases the evidence\nmight be still out there, but this will happen more rarely. Fourth, an\nalmost ideal tool to address misinformation would be a crowd able\nto assess truthfulness in real time, immediately after the statement\nbecomes public: although we are not there yet, and there is a long\nway to go, we find that targeting recent statements is a step forward\nin the right direction. Fifth, our experimental design differs in some\naspects from that used in previous work, and allows us to address\nnovel research questions.\n2 BACKGROUND\n2.1 COVID-19 Infodemic\nThe number of initiatives to apply Information Access\u2014and, in gen-\neral, Artificial Intelligence\u2014techniques to combat the COVID-19\ninfodemic has been rapidly increasing (see Bullock et al . [4, p. 16 ]\nfor a survey). There is significant effort on analyzing COVID-19\ninformation on social media, and linking to data from external fact-\nchecking organizations to quantify the spread of misinformation\n[7,11,32]. Mejova and Kalimeri [19] analyzed Facebook advertise-\nments related to COVID-19, and found that around 5% of them\ncontain errors or misinformation. Crowdsourcing methodologies\nhave also been used to collect and analyze data from patients with\ncancer who are affected by the COVID-19 pandemic [ 8]. To the\nbest of our knowledge, there is no work addressing the COVID-19\ninfodemic using crowdsourcing.\n2.2 Crowdsourcing Truthfulness\nRecent work has focused on the automatic classification of truth-\nfulness or fact checking [ 2,9,15,20,22,24]. Zubiaga and Ji [33]investigated, using crowdsourcing, the reliability of tweets in the\nsetting of disaster management. CLEF developed a Fact-Checking\nLab [ 9,22] to address the issue of ranking sentences according to\nsome fact-checking property.\nThere is recent work that studies how to collect truthfulness\njudgments by means of crowdsourcing using fine grained scales\n[17,27,29]. Samples of statements from the PolitiFact dataset\u2014\noriginally published by Wang [31]\u2014have been used to analyze the\nagreement of workers with labels provided by experts in the origi-\nnal dataset. Workers are asked to provide the truthfulness of the\nselected statements, by means of different fine grained scales. Roi-\ntero et al . [27] compared two fine grained scales: one in the [0,100]\nrange and one in the (0,+\u221e)range, on the basis of Magnitude\nEstimation [ 21]. They found that both scales allow to collect reli-\nable truthfulness judgments that are in agreement with the ground\ntruth. Furthermore, they show that the scale with one hundred\nlevels leads to slightly higher agreement levels with the expert\njudgments. On a larger sample of PolitiFact statements, La Bar-\nbera et al . [17] asked workers to use the original scale used by the\nPolitiFact experts and the scale in the [0,100]range. They found\nthat aggregated judgments (computed using the mean function\nfor both scales) have a high level of agreement with expert judg-\nments. Recent work by Roitero et al . [29] found similar results in\nterms of external agreement and its improvement when aggregat-\ning crowdsourced judgments, using statements from two different\nfact-checkers: PolitiFact and RMIT ABC Fact Check ( ABC). Pre-\nvious work has also looked at internal agreement , i.e., agreement\namong workers [ 27,29]. Roitero et al . [29] found that scales have\nlow levels of agreement when compared with each other: corre-\nlation values for aggregated judgments on the different scales are\naround \u03c1=0.55\u22120.6forPolitiFact and \u03c1=0.35\u22120.5forABC,\nand\u03c4=0.4forPolitiFact and\u03c4=0.3forABC. This indicates that\nthe same statements tend to be evaluated differently in different\nscales.\nThere is evidence of differences on the way workers provide\njudgments, influenced by the sources they examine, as well as the\nimpact of worker bias. In terms of sources, La Barbera et al . [17]\nfound that that the vast majority of workers (around 73% for both\nscales) use indeed the PolitiFact website to provide judgments.\nDifferently from La Barbera et al . [17] , Roitero et al . [29] used a cus-\ntom search engine in order to filter out PolitiFact andABCfrom\nthe list of results. Results show that, for all the scales, Wikipedia\nand news websites are the most popular sources of evidence used\nby the workers. In terms of worker bias, La Barbera et al . [17] and\nRoitero et al . [29] found that worker political background has an\nimpact on how workers provide the truthfulness scores. More in\ndetail, they found that workers are more tolerant and moderate\nwhen judging statements from their very own political party.\n3 AIMS AND RESEARCH QUESTIONS\nWhen compared to previous work, in this paper we aim to focus on\nseveral novel aspects. With respect to La Barbera et al . [17] , Roitero\net al . [27,29], we focus on claims about COVID-19, which are\nrecent and interesting for the research community, and are arguably\non a more relevant/sensitive topic to the workers. We investigate\nwhether the health domain makes a difference in the ability of\ncrowd workers to identify and correctly classify (mis)information,\nand if the very recent nature of COVID-19 related statements has\nan impact as well. We focus on a single truthfulness scale, given the\nevidence that the scale used does not make a significant difference\n[17,27,29]. Another important difference is that we ask workers to\nprovide a textual justification for their decision: we analyze them\nto better understand the process followed by workers to verify\ninformation, and we investigate if they can be exploited to derive\nuseful information. Finally, we also exploit and analyze worker\nbehavior.\nWe investigate the following specific Research Questions:\nRQ1 Are the crowd workers able to detect and objectively catego-\nrize online (mis)information related to the medical domain\nand more specifically to COVID-19? Which are the rela-\ntionship and agreement between the crowd and the expert\nlabels?\nRQ2 Can the crowdsourced and/or the expert judgments be trans-\nformed or aggregated in a way that it improves the abil-\nity of workers to detect and objectively categorize online\n(mis)information?\nRQ3 Which is the effect of workers\u2019 political bias in objectively\nidentifying online misinformation? And the effect of work-\ners\u2019 background and Cognitive Reflection Test (CRT) perfor-\nmances?\nRQ4 Which are the signals provided by the workers while per-\nforming the task that can be recorded? To what extent are\nthese signals related to workers\u2019 accuracy? Can these signals\nbe exploited to improve accuracy and, for instance, aggregate\nthe labels in a more effective way?\nRQ5 Which sources of information does the crowd consider when\nidentifying online misinformation? Are some sources more\nuseful? Do some sources lead to more accurate and reliable\nassessments by the workers?\n4 METHODS\nIn this section we present the dataset used to carry out our experi-\nments (Section 4.1), and the crowdsourcing task design (Section 4.2).\nOverall, we considered one dataset annotated by experts, one crowd-\nsourced dataset, one judgment scale (the same for the expert and\nthe crowd judgments), and a total of 60 statements.\n4.1 Dataset\nWe considered as primary source of information the PolitiFact\ndataset [ 31] that was built as a \u201cbenchmark dataset for fake news\ndetection\u201d [ 31] and contains over 12k statements produced by pub-\nlic appearances of US politicians. The statements of the datasets are\nlabeled by expert judges on a six-level scale of truthfulness (from\nnow on referred to as E 6):pants-on-fire ,false ,mostly-false ,\nhalf-true ,mostly-true , and true . Recently, the PolitiFact web-\nsite (the source from where the statements of the PolitiFact\ndataset are taken) created a specific section related to the COVID-19\npandemic.6For this work, we selected 10 statements for each of the\nsixPolitiFact categories, belonging to such a COVID-19 section\nand with dates ranging from February 2020 to early April 2020.\nTable 1 contains some examples of the statements we used.\n6https://www.politifact.com/coronavirus/Table 1: Examples of COVID-19 fact-checked statements.\nStatement Source Year Label\n\u201cWe inherited a broken test for COVID-19.\u201d Donald\nTrump2020 pants-on-fire\n\u201cChurch services cannot resume until we are\nall vaccinated, says Bill Gates.\u201dBloggers 2020 false\n\u201cSays a 5G law passed while everyone was\ndistracted with the coronavirus pandemic\nand lists 20 symptoms associated with 5G\nexposure.\u201dFacebook\nPost2020 mostly-false\n\u201cSays a California surfer was alone, in the\nocean, when he was arrested for violating\nthe state\u00e2\u0102\u0179s stay-at-home order.\u201dFacebook\nPost2020 mostly-true\n\u201cPhoto shows a crowded New York City\nsubway train during stay-at-home order.\u201dViral\nImage2020 true\n4.2 Crowdsourcing Experimental Setup\nTo collect our judgments we used the crowdsourcing platform\nAmazon Mechanical Turk (MTurk). Each worker, upon accepting\nour HIT, is redirected to an external server to complete the task;\nwe set the payment to $1.5 for a set of 8 statements7. The task\nitself is as follows: first, a (mandatory) questionnaire is shown to\nthe worker, to collect his/her background information such as age\nand political views. Then, the worker needs to provide answers to\nthree Cognitive Reflection Test (CRT) questions, which are used to\nmeasure the personal tendency to answer with an incorrect \u201cgut\u201d\nresponse or engage in further thinking to find the correct answer\n[10].8After the questionnaire and CRT phase, the worker is asked to\nasses the truthfulness of 8 statements: 6 from the dataset described\nin 4.1 (one for each of the six considered PolitiFact categories)\nand 2 special statements called Gold Questions , one clearly true\nand the other clearly false, manually written by the authors of this\npaper and used as quality checks. We used a randomization process\nwhen building the HITs to avoid all the possible source of bias, both\nwithin each HIT and considering the overall task.\nTo assess the truthfulness of each statement, the worker is shown:\ntheStatement , the Speaker/Source , and the Year in which the state-\nment was made. We asked the worker to provide the following\ninformation: the truthfulness value for the statement using the\nsix-level scale adopted by PolitiFact , from now on referred to\nas C 6(presented to the worker using a radio button containing\nthe label description for each category as reported in the original\nPolitiFact website), a URL that s/he used as a source of infor-\nmation for the fact checking, and a textual motivation for her/his\nresponse (which can not include the URL, and should contain at\nleast 15 words). In order to prevent the user from using PolitiFact\nas primary source of evidence, we implemented a custom search\nengine, which is based on the Bing Web Search APIs9and filters\noutPolitiFact from the returned search results.\nWe logged the user behavior using a custom logger [ 12,13], and\nwe implemented in the task the following quality checks: (i) the\njudgments assigned to the gold questions have to be coherent (i.e.,\nthe judgment of the clearly false question should be lower than the\n7Before deploying the task on MTurk, we investigated the average time spent to\ncomplete the task, and we related it to the minimum US hourly wage.\n8We used the same CRT settings as Roitero et al. [29].\n9https://azure.microsoft.com/services/cognitive-services/bing-web-search-api/\none assigned to true question); and (ii) the cumulative time spent\nto perform each judgment should be of at least 10 seconds. Note\nthat the CRT (and the questionnaire) answers were not used for\nquality check, although the workers were not aware of that.\nOverall, we used 60 statements in total (10 for each PolitiFact\ncategory), and each statement has been evaluated by 10 distinct\nworkers. Thus, we deployed 100 MTurk HITs and we collected\n800 judgments in total. The crowd task was launched on May 1st,\n2020 and it completed on May 4th, 2020. All the data used to carry\nout our experiments can be downloaded at https://github.com/\nKevinRoitero/crowdsourcingTruthfulness.\n5 RESULTS AND ANALYSIS\nWe first report some descriptive statistics about the population of\nworkers and the data collected in our experiment (Section 5.1). Then,\nwe address crowd accuracy (i.e., RQ1) in Section 5.2, transformation\nof truthfulness scales (RQ2) in Section 5.3, worker background and\nbias (RQ3) in Section 5.4, worker behavior (RQ4) in Section 5.5;\nfinally, we study information sources (RQ5) in Section 5.6.\n5.1 Descriptive Statistics\n5.1.1 Worker Background, Abandonment, and Bias.\nQuestionnaire . Overall, 1113 workers resident in the United States\nparticipated in our experiment.10In each HIT, workers were first\nasked to complete a demographics questionnaire with questions\nabout their gender, age, education and political views. By analyzing\nthe answers to the questionnaire we derived the following demo-\ngraphic statistics. The majority of workers are in the 26\u201335 age\nrange (44%), followed by 36\u201350 (25%), and 19\u201325 (18%). The ma-\njority of workers are well educated: 47% of them have a four year\ncollege degree or a bachelor degree, 21% have a college degree, and\n17% have a postgraduate or professional degree. Only about 15%\nof workers have a high school degree or less. Concerning political\nviews, we had 28% of workers that identified themselves as liberals,\n28% as moderate, 24% as conservative, 11% as very conservative and\n9% as very liberal. Moreover, 44% of workers identified themselves\nas being Democrat, 31% as being Republican, and 22% as being\nIndependent. Finally, 46% of workers agree on building a wall on\nthe southern US border, and 42% of them disagree. Overall we can\nsay that our sample is well balanced.\nCRT Test . Analyzing the CRT scores, we found that: 31% of workers\ndid not provide any correct answer, 34% answered correctly to 1\ntest question, 18% answered correctly to 2 test questions, and only\n17% answered correctly to all 3 test questions.\nAbandonment . When considering the abandonment ratio (measured\naccording to the definition provided by Han et al . [12] , Han et al .\n[13]), we found that 100of the workers (about 9%) successfully\ncompleted the task, 991(about 87%) abandoned (i.e., voluntarily\nterminated the task before completing it), and 45(about 4%) failed\n(i.e., terminated the task due to failing the quality checks too many\ntimes). Most of the abandonment (80% of the 1091 workers, 85%\nof the 991workers that abandoned) happened before judging the\nfirst statement (i.e., before really starting the task); about 7.52% of\n10Workers provide proof that they are based in US and have the eligibility to work.\n012345\nScore04080120160Frequency\n0175350525700\nCumulative\n012345\nScore020406080Frequencylow\nhigh\n012345\nScore02468Frequency\n020406080\nCumulativeFigure 1: Distribution (in blue) and the cumulative distribu-\ntion (in red) of the individual (left), gold (middle), and aggre-\ngated with mean (right).\nthe 1091 workers (8% of the 991of the workers that abandoned)\nabandoned after the last statement (most likely once failed the\nquality check).\n5.1.2 Crowdsourced Scores. Figure 1 shows the distribution (in\nblue) and the cumulative distribution (in red) of the individual (left),\ngold (middle), and aggregated with mean (right) scores provided\nby the workers for the considered PolitiFact statements.\nIf we focus on the distribution of the individual scores (left plot),\nwe can see that the distribution is quite well balanced, just lightly\nskewed towards higher truthfulness values, represented in the right-\nmost part of the plot. This behavior is also remarked when focusing\non the red line representing the cumulative distribution, which\ndisplays almost evenly spaced steps. This is a first indication that\nsuggests that crowd judgments are overall of a decent quality; in\nfact, our empirical distribution is not distant from the ideal one:\nsince we considered 10 statements for each PolitiFact category,\nthe perfect distribution would have been the uniform distribution.\nTurning to the distribution of the gold scores (i.e., the two special\nstatements used for quality check, shown in the middle plot), we\nsee that the large majority of workers (i.e., 70% for High and 43%\nforLow) used the extreme values of the scale (i.e., pants-on-fire\nandtrue ); furthermore, we see that overall the High gold question\nhas been judged correctly more times than the Lowgold question,\nsuggesting the probably the workers found the former easier to\njudge than the latter.\nWe now turn to analyze the distribution of the scores when\naggregated using the mean function (shown in the right plot). The\ndistribution for the aggregated scores becomes roughly bell-shaped,\nand slightly skewed towards high truthfulness values\u2014this behavior\nis consistent with the findings of Roitero et al . [29] . In the following\nwe discuss both the external (i.e., between workers and experts)\nand internal (i.e., among workers) agreement of our dataset.\n5.2 RQ1: Crowd Accuracy\n5.2.1 External Agreement. To answer RQ1, we start by analyzing\nthe so called external agreement, i.e., the agreement between the\ncrowd collected labels and the experts ground truth. Figure 2 shows\nthe agreement between the PolitiFact experts (x-axis) and the\ncrowd judgments (y-axis). In the first plot, each point is a judgment\nby a worker on a statement, i.e., there is no aggregation of the\nworkers working on the same statement. In the next plots all work-\ners redundantly working on the same statement are aggregated\nusing the mean (second plot), median (third plot), and majority\nvote (right-most plot). If we focus on the first plot (i.e., the one\n012345\nE012345C\n012345\nE012345C\n012345\nE012345C\n012345\nE012345C\nFigure 2: The agreement between the PolitiFact experts (x-axis) and the crowd judgments (y-axis). From left to right: C 6\nindividual judgments; C 6aggregated with mean; C 6aggregated with median; C 6aggregated with majority vote.\nwith no aggregation function applied), we can see that, overall, the\nindividual judgments are in agreement with the expert labels, as\nshown by the median values of the boxplots, which are increasing\nas the ground truth truthfulness level increases. Concerning the\naggregated values, it is the case that for all the aggregation func-\ntions the pants-on-fire andfalse categories are perceived in a\nvery similar way by the workers; this behavior was already shown\nin previous work [ 17,29], and suggests that indeed workers have\nclear difficulties in distinguishing between the two categories; this\nis even more evident considering that the interface presented to the\nworkers contained a textual description of the categories\u2019 meaning\nin every page of the task.\nIf we look at the plots as a whole, we see that within each plot\nthe median values of the boxplots are increasing when going from\npants-on-fire totrue (i.e., going from left to right of the x-axis of\neach chart). This indicates that the workers are overall in agreement\nwith the PolitiFact ground truth, thus indicating that workers are\nindeed capable of recognizing and correctly classifying misinforma-\ntion statements related to the COVID-19 pandemic. This is a very\nimportant and not obvious result: in fact, the crowd (i.e., the work-\ners) is the primary source and cause of the spread of disinformation\nand misinformation statements across social media platforms [ 6].\nBy looking at the plots, and in particular focusing on the median\nvalues of the boxplots, it appears evident that the mean (second plot)\nis the aggregation function which leads to higher agreement levels,\nfollowed by the median (third plot) and the majority vote (fourth\nplot). Again, this behavior has already been noticed [ 17,28,29], and\nall the cited works used the mean as primary aggregation function.\nTo validate the external agreement, we measured the statis-\ntical significance between the aggregated rating for all the six\nPolitiFact categories; we considered both the Mann-Whitney\nrank test and the t-test, applying Bonferroni correction to account\nfor multiple comparisons. Results are as follows: when considering\nadjacent categories (e.g., pants-on-fire andfalse ), the difference\nbetween categories are never significant, for both tests and for all\nthe three aggregation functions. When considering categories of dis-\ntance 2 (e.g., pants-on-fire andmostly-false ), the differences\nare never significant, apart from the median aggregation function,\nwhere there is statistical significance to the p< .05level in 2/4\ncases for both Mann-Whitney and t-test. When considering cate-\ngories of distance 3, the differences are significant, for the mean, in\n3/3cases for the Mann-Whitney and 3/3cases for the t-test, for the\nmedian, in 2/3cases for the Mann-Whitney and 3/3cases for the\nt-test, for the majority vote, in 0/3cases for the Mann-Whitney and\npants-on-fire false mostly-false half-true mostly-true true\u22120.10.00.10.20.30.4agreement\nOverall  \u03b1\nOverall  \u03a6 \u03b1  \u03a6\n0.0 0.2 0.4 0.6 0.8 1.0\nPairwise Agreement0.000.250.500.751.00Freq.\nC6Figure 3: Workers agreement: \u03b1[16] and \u03a6[5] (top plot); pair-\nwise unit agreement (bottom plot).\n1/3cases for the t-test. When considering categories of distance 4\nand 5, the differences are always significant to the p>0.01level\nfor all the aggregation functions and for all the tests, apart from\nthe case of the majority vote function and the Mann-Whitney test,\nwhere the significance is at the p> .05level. In the following we\nuse the mean as being the most commonly used approach for this\ntype of data [29].\n5.2.2 Internal Agreement. Another standard way to address RQ1\nand to analyze the quality of the work by the crowd is to compute\nthe so called internal agreement (i.e., the agreement among the\nworkers). Figure 3 shows in the first plot the agreement measured\nwith \u03b1[16] and\u03a6[5], two popular measures often used to compute\nworkers\u2019 agreement in crowdsourcing tasks [ 18,27\u201329]. The x-axis\ndetails the PolitiFact categories, while the y-axis the level of\nagreement measured; while \u03b1is a punctual measure, \u03a6allows to\ncompute confidence intervals for the agreement measure; the plot\nshows the upper 97% and lower 3% confidence intervals as thinner\nlines. As we can see from the plot, the agreement levels measured\nwith the two scales is very similar for the pants-on-fire ,false ,\nmostly-false , and half-true categories: note that the \u03b1measure\nalways falls in the \u03a6confidence interval, and the little oscillations\nin the agreement value are not always indication of a real change in\nthe agreement level, especially when considering \u03b1[5]. Having said\nso, it appears that for all the two metrics the overall agreement falls\nin the[0.15,0.3]range, and the agreement level is similar for all\nthePolitiFact categories, with the exception of \u03a6, which shows\nhigher agreement levels for the mostly-true andtrue categories.\nThis confirms the finding, derived from Figure 2, that workers\nseem most effective in identifying and categorizing statements with\na higher truthfulness level. This remark is also supported by [ 5]\nwhich shows that \u03a6is better in distinguishing agreement levels\nin crowdsourcing than \u03b1, which is more indicated as a measure of\ndata reliability in non crowdsourced settings.\nFigure 3 also shows in the second plot a measure of the agreement\nat the HIT level (i.e., in the set of 8 statements judged by each\nworker) as detailed in [ 18,29]. More in detail, the plot shows the\nCCDF (Complementary Cumulative Distribution Function) of the\nrelative frequencies for the agreement of the 100 HITs considered\nin this experiment. The plot shows that around 20% of the hits\nhave a pairwise agreement which is very close to 1; this indicates\nthat around 20% of the workers judged statements almost in the\nsame way as the expert judges. Moreover, we see that 60% of the\nworkers have a pairwise agreement greater than 0.5. Again, this\nresult indicates a good overall agreement between crowd and expert\njudgments, confirming that the crowd is able to correctly identify\nand classify misinformation related to the COVID-19 pandemic.\n5.3 RQ2: Transforming Truthfulness Scales\nGiven the positive results presented above, it appears that the an-\nswer to RQ1 is overall positive, even if with some exceptions. There\nare many remarks that can be made: first, there is a clear issue that\naffects the pants-on-fire andfalse categories, which are very\noften mis-classified by workers. Moreover, while PolitiFact used\na six-level judgment scale, the usage of a two- (e.g., True/False) and\na three-level (e.g., False / In between / True) scale is also common\nwhen assessing the truthfulness of statements [ 17,29]. Finally, cat-\negories can be merged together to improve accuracy, as done for\nexample by Tchechmedjiev et al . [30] . All these considerations lead\nus to RQ2, addressed in the following.\n5.3.1 Merging Ground Truth Levels. For all the above reasons, we\nperformed the following experiment: we group together the six\nPolitiFact categories (i.e., E 6) into three (referred to as E 3) or two\n(E2) categories, which we refer respectively with 01,23, and 45for\nthe three level scale, and 012and234for the two level scale.\nFigure 4 shows the result of such a process. As we can see from\nthe plots, the agreement between the crowd and the expert judg-\nments can be seen in a more neat way. As for Figure 2, the median\nvalues for all the boxplots is increasing when going towards higher\ntruthfulness values (i.e., going from left to right within each plot);\nthis holds for all the aggregation functions considered, and it is\nvalid for both transformations of the E 6scale, into two and three\nlevels. Also in this case we computed the statistical significance\nbetween categories, applying the Bonferroni correction to account\nfor multiple comparisons. Results are as follows. For the case of\nthree groups, both the categories at distance one and two are always\nsignificant to the p<0.01level, for both the Mann-Whitney and\nthe t-test, for all three aggregation functions. The same behavior\nholds for the case of two groups, where the categories of distance 1\nare always significant to the p<0.01level.\n012345\nE012345C\n012345\nE012345C\n012345\nE012345C\n012 345\nE012345C\n012 345\nE012345C\n012 345\nE012345C\nFigure 4: The agreement between the PolitiFact experts\nand the crowd judgments. From left to right: C 6aggregated\nwith mean; C 6aggregated with median; C 6aggregated with\nmajority vote. First row: E 6to E 3; second row: E 6to E 2. Com-\npare with Figure 2.\nSummarizing, we can now conclude that by merging the ground\ntruth levels we obtained a much stronger signal: the crowd can\neffectively detect and classify misinformation statements related to\nthe COVID-19 pandemic.\n5.3.2 Merging Crowd Levels. Having reported the results on merg-\ning the ground truth categories we now turn to transform the crowd\nlabels (i.e., C 6) into three (referred to as C 3) and two (C 2) categories.\nFor the transformation process we rely on the approach detailed by\nHan et al . [14] , that also present a complete and exhaustive discus-\nsion on the effectiveness of the scale transformation methods. This\napproach has many advantages [ 14]: we can simulate the effect of\nhaving the crowd answers in a more coarse-grained scale (rather\nthan C 6), and thus we can simulate new data without running the\nwhole experiment on MTurk again. As we did for the ground truth\nscale, we choose to select as target scales the two- and three- levels\nscale, driven by the same motivations. Having selected C 6as being\nthe source scale, and having selected the target scales as the three-\nand two- level ones (C 3and C 2), we perform the following experi-\nment. We perform all the possible cuts11from C 6to C 3and from\nC6to C 2; then, we measure the internal agreement (using \u03b1and\u03a6)\nboth on the source and on the target scale, and we compare those\nvalues. In such a way, we are able to identify, among all the possible\ncuts, the cut which leads to the highest possible internal agreement.\nAlso in this case, a detailed discussion on the relationships between\ninternal agreement, effectiveness, and all the possible cuts can be\nfound in Han et al. [14].\nFigure 5 shows the results. The x-axis shows the cut performed\nto transform C 6into the target scale (C 3in the left-most plot, C 2in\nthe right-most plot), while the y-axis shows the internal agreement\nscore by means of either \u03b1or\u03a6. As we can see by inspecting the\nleft-most plot, (i.e., C 6to C 3) we can see that there is, both for \u03b1\n11C6can be transformed into C 3in 10 different ways, and C 6can be transformed into\nC2in 5 different ways.\n[[0], [1], [2, 3, 4, 5]]\n[[0], [1, 2], [3, 4, 5]]\n[[0], [1, 2, 3], [4, 5]]\n[[0], [1, 2, 3, 4], [5]]\n[[0, 1], [2], [3, 4, 5]]\n[[0, 1], [2, 3], [4, 5]]\n[[0, 1], [2, 3, 4], [5]]\n[[0, 1, 2], [3], [4, 5]]\n[[0, 1, 2], [3, 4], [5]]\n[[0, 1, 2, 3], [4], [5]]\ncut\u22120.3\u22120.2\u22120.10.00.10.20.3\u03b1/\u03a6\n\u03b1\n\u03a6\n[[0], [1, 2, 3, 4, 5]]\n[[0, 1], [2, 3, 4, 5]]\n[[0, 1, 2], [3, 4, 5]]\n[[0, 1, 2, 3], [4, 5]]\n[[0, 1, 2, 3, 4], [5]]\ncut\u22120.4\u22120.3\u22120.2\u22120.10.00.10.20.3\u03b1/\u03a6\n\u03b1\n\u03a6Figure 5: \u03b1and\u03a6cuts. From left to right: C 6cut into three\nlevels (C 3), C 6cut into two levels (C 2). The cut is detailed in\nthe x-label. The dotted line is \u03b1/\u03a6on C 6, the continuous\nline is the average \u03b1/\u03a6score measured among all the cuts.\n012345\nE0.00.51.01.52.0C\n012345\nE0.00.51.01.52.0C\n012345\nE0.00.20.40.60.81.0C\n012345\nE0.00.20.40.60.81.0C\nFigure 6: Comparison with E 6. C 6to C 3(first row) and to\nC2(second row), then aggregated with the mean function.\nBest cut selected according to \u03b1(left column) and \u03a6(right\ncolumn) (see Figure 5). Compare with Figure 2.\nand\u03a6, a single cut which leads to higher agreement levels with\nthe original C 6scale. On the contrary, if we focus on the rightmost\nplot (i.e., C 6to C 2), we can see that there is a single cut for \u03b1which\nleads to similar agreement levels as in the original C 6scale, and\nthere are no cuts with such a property when using \u03a6.\nHaving identified the best possible cuts for both transformations\nand for both agreement metrics, we now measure the external\nagreement between the crowd and the expert judgments, using\nthe selected cut. Figure 6 shows such a result when considering\nthe judgments aggregated with the mean function. As we can see\nfrom the plots, it is again the case that the median values of the\nboxplots is always increasing, for all the transformations. Never-\ntheless, inspecting the plots we can state that the overall external\nagreement appears to be lower than the one shown in Figure 2.\nMoreover, we can also state that also in the case of the transformed\nscales, the categories pants-on-fire andfalse are still not sep-\narable. Summarizing, we show that it is feasible to transform the\n01 23 45\nE0.00.51.01.52.0C\n01 23 45\nE0.00.51.01.52.0C\n012 345\nE0.00.20.40.60.81.0C\n012 345\nE0.00.20.40.60.81.0C\nFigure 7: C 6to C 3(first row) and to C 2(second row), then ag-\ngregated with the mean function. First row: E 6to E 3. Second\nrow: E 6to E 2. Best cut selected according to \u03b1(left column)\nand\u03a6(right column) (see Figure 5). Compare with Figures 2,\n4, and 6.\njudgments collected on a C 6level scale into two new scales, C 3and\nC2, obtaining judgments with a similar internal agreement as the\noriginal ones, and with a slightly lower external agreement with\nthe expert judgments.\n5.3.3 Merging both Ground Truth and Crowd Levels. It is now natu-\nral to combine the two approaches. Figure 7 shows the comparison\nbetween C 6transformed into C 3and C 2, and E 6transformed into\nE3and E 2. As we can see form the plots, also in this case the me-\ndian values of the boxplots are increasing, especially for the E 3case\n(shown in the first row). Furthermore, the external agreement with\nthe ground truth is present, even if for the E 2case (shown in the\nsecond row) the classes appear to be not separable. Summarizing,\nall these results show that it is feasible to successfully combine\nthe aforementioned approaches, and transform into a three- and\ntwo-level scale both the crowd and the expert judgments.\n5.4 RQ3: Worker Background and Bias\nTo address RQ3 we study if the answers to questionnaire and CRT\ntest have any relation to worker quality.\n5.4.1 Questionnaire. Table 2 (top) shows in the rows the answer\nto the workers political views, while on the columns the number of\ncorrectly classified statements (columns, max is 6). As we can see\nfrom the table, there is only one worker who successfully classified\nall 6 statements. Many workers correctly classified 1 or 2 statements\n(28 and 28, respectively). The next column summarizes, using Accu-\nracy (i.e., the fraction of exactly classified statements), the quality of\nworkers in each group. The number and fraction of correctly classi-\nfied statements are however rather crude measures of worker\u2019s qual-\nity, as small misclassification errors (e.g, pants-on-fire in place of\nfalse ) are as important as more striking ones (e.g., pants-on-fire\nin place of true ). Therefore, to measure the ability of workers to\nTable 2: Count of the number of workers depending on:\nnumber of statements correctly classified (columns, max is\n6), vs. (top table) the answers to the Political views question\n(rows) and vs. (bottom table) the number of correct answers\nto the CRT test (rows, max is 3). The last two columns show\nthe Accuracy (the fraction of correctly identified statements\nfor each group) and the CEMORDscore.\nCorrectly classified statements Acc CEMORD\n0 1 2 3 4 5 6 Sum Mean\nVery conservative 4 3 1 0 0 0 1 9 .13 .46\nConservative 0 9 2 3 1 0 0 15 .21 .51\nModerate 6 6 6 7 0 1 0 26 .20 .50\nLiberal 2 8 13 4 4 2 0 33 .16 .50\nVery Liberal 0 2 6 6 2 1 0 17 .21 .51\nSum 12 28 28 20 7 4 1 100\nCorrectly classified statements Acc CEMORD\n0 1 2 3 4 5 6 Sum Mean\nCRT 0 5 11 9 4 0 1 1 31 .14 .48\ncorrect 1 5 10 12 6 1 0 0 34 .22 .53\nanswers 2 1 6 1 6 3 1 0 18 .21 .51\n3 1 1 6 4 3 2 0 17 .15 .47\nSum 12 28 28 20 7 4 1 100\ncorrectly classify the statements, we also compute CEMORD, an effec-\ntiveness metric recently proposed for the specific case of ordinal\nclassification [ 1] (see Roitero et al . [29, \u00a73.3 ]for a more detailed\ndiscussion of these issues). The last column in the table shows the\naverage CEMORDvalue for the workers in each group. By looking\nat both Accuracy and CEMORD, it is clear that \u2018Very conservative\u2019\nworkers provide lower quality labels. The Bonferroni corrected two\ntailed t-test on CEMORDconfirms that \u2018Very conservative\u2019 workers\nperform statistically significantly worse than both \u2018Conservative\u2019\nand \u2018Very liberal\u2019 workers. The workers\u2019 political views affect the\nCEMORDscore, even if in a small way and mainly when considering\nthe extremes of the scale. An initial analysis of the other answers\nto the questionnaire (not shown due to space limitations) does not\nseem to provide strong signals; a more detailed analysis is left for\nfuture work.\n5.4.2 CRT Test. We now investigate the effect of the CRT test on\nthe worker quality. Table 2 (bottom) shows the count of the number\nof workers depending on: number of statements correctly classified\n(columns, max is 6), versus the number of correct answers to the\nCRT test (rows, max is 3). Concerning CRT scores, we see that the\nminority of workers (17) answered in a correct way to all the three\nquestions, and the majority of them answered correctly to only 1\nCRT question (34) or none (31). Although there is some variation in\nboth Accuracy and CEMORD, this is never statistically significant; it\nappears that the number of correct answers to the CRT test is not\ncorrelated with worker quality. We leave for future work a more\ndetailed study of this aspect.\n5.5 RQ4: Worker Behavior\nWe now turn to RQ4, and analyze the behavior of the workers while\nperforming the task.\n1 2 3 4 5 6 7 8\nstatement index150200250300350time elapsed (sec)\n012345\nE012345C\nFigure 8: Position of the statement in the task vs. time\nelapsed, cumulative on each single statement (left). Compar-\nison between E 6and C 6where the aggregation function is\nthe weighted mean and the weights are the political views\n(see Table 2 top) normalized to [0.5,1](right).\n5.5.1 Time. Figure 8 (left) shows that the amount of time spent\non average by the workers on the first statements is considerably\nhigher than on the last statements. This, combined with the fact\nthat the quality of the assessment provided by the workers does\nnot decrease for the last statements ( CEMORDscores per position are\n1: .61, 2: .60, 3: .64, 4: .58, 5: .59, 6: .54, 7: .61, 8: .62), is an indication\nof a learning effect: the workers learn how to assess truthfulness in\na faster way.\n5.5.2 Exploiting Worker Signals to Improve Quality. We have shown\nthat, while performing their task, workers provide many signals\nthat to some extent correlate with the quality of their work. These\nsignals could in principle be exploited to aggregate the individual\njudgments in a more effective way (i.e., giving more weight to work-\ners that possess features indicating a higher quality). For example,\nthe relationships between worker background / bias and worker\nquality (Section 5.4) could be exploited to this aim.\nWe thus performed the following experiment: we aggregated\nC6individual scores, using as aggregation function a weighted\nmean, where the weights are represented by the political views,\nnormalized to[0.5,1]. Figure 8 (right) shows the results. We also\naggregated C 6individual scores using as aggregation function the\nweighted mean function where the weights are represented by the\nnumber of correct answers to CRT, normalized to [0.5,1], which\nlead to similar results. Thus, it seems that leveraging quality-related\nbehavioral signals, like questionnaire answers or CRT scores, to\naggregate results does not provide a noticeable increase in the exter-\nnal agreement, although it does not harm. We have only scratched\nthe surface, though, as there are many other signals, and aggrega-\ntion functions, that can be tried; we leave for future work the in\ndepth analysis of how such behavioral signals can be leveraged to\nimprove external agreement.\n5.5.3 Queries. Table 3 shows query statistics for the 100 workers\nwhich finished the task. As we can see, the higher the statement\nposition, the lower the number of queries issued: 3.52 queries on\naverage for the first statement, down to 2.3 for the last statement.\nThis can indicate the attitude of workers to issue fewer queries the\nmore time they spend on the task, probably due to fatigue, boredom,\nor learning effects. Nevertheless, we can see that on average, for all\nthe statement positions each worker issues more than one query,\ni.e., workers often reformulate their initial query. This provides\nfurther evidence that they put effort in performing the task. The\nTable 3: Statement position in the task versus: number of\nqueries issued (top) and number of times the statement has\nbeen used as a query (bottom).\nStatement\nPosition1 2 3 4 5 6 7 8 Sum Mean\nNumber\nof Queries352\n16.8%280\n13.4%259\n12.4%255\n12.1%242\n11.6%238\n11.3%230\n11.0%230\n11.4%2095 261.9\nStatement\nas Query22\n9%32\n13%31\n12.6%33\n13.5%34\n13.9%30\n12.2%29\n11.9%34\n13.9%245 30.6\n0.0 0.1 0.2 0.3 0.4\nfrequency1\n2\n3\n4\n5\n6\n7\n8\n9\n10rank\nURL Percentage %\nsnopes.com 11.79%\nmsn.com 8.93%\nfactcheck.org 6.79%\nwral.com 6.79%\nusatoday.com 5.36%\nstatesman.com 4.64%\nreuters.com 4.64%\ncdc.gov 4.29%\nmediabiasfactcheck.com 4.29%\nbusinessinsider.com 3.93%\nFigure 9: On the left, distribution of the ranks of the URLs\nselected by workers, on the right, websites from which work-\ners chose URLs to justify their judgments.\nthird row of the table shows the number of times the worker used\nas query the whole statement. We can see that the percentage is\nrather low (around 13%) for all the statement positions, indicating\nagain that workers spend effort when providing their judgments.\n5.6 RQ5: Sources of Information\n5.6.1 URL Analysis. Figure 9 shows on the left the distribution\nof the ranks of the URL selected as evidence by the worker when\nperforming each judgment. URLs selected less than 1% times are\nfiltered out from the results. As we can see from the plot, about 40%\nof workers selected the first result retrieved by our search engine,\nand selected the remaining positions less frequent, with an almost\nmonotonic decreasing frequency (rank 8 makes the exception).\nWe also found that 14% of workers inspected up to the fourth\npage of results (i.e., rank =40). The breakdown on the truthfulness\nPolitiFact categories does not show any significant difference.\nFigure 9 shows on the right part the top 10 of websites from\nwhich the workers choose the URL to justify their judgments. Web-\nsites with percentage \u22643.9%are filtered out. As we can see from\nthe table, there are many fact check websites among the top 10\nURLs (e.g., snopes: 11.79%, factcheck 6.79%). Furthermore, medical\nwebsites are present, although in small percentage (cdc: 4.29%). This\nindicates that workers use various kind of sources as URLs from\nwhich they take information. Thus, it appears that they put effort\nin finding evidence to provide a reliable truthfulness judgment.\n5.6.2 Justifications. As a final result, we analyze the textual justifi-\ncations provided, their relations with the web pages at the selected\nURLs, and their links with worker quality. 54% of the provided\njustifications contain text copied from the web page at the URL\nselected for evidence, while 46% do not. Furthermore, 48% of the\n0 1 2 3 4 5\nJudgment absolute error0.00.10.20.30.40.5Percentage\ncopied\nno copied\n0.00.20.40.60.81.0\nCumulative\n-5 -4 -3 -2 -1 0 1 2 3 4 5\nJudgment error0.00.10.20.30.4Percentage\ncopied\nno copiedFigure 10: Effect of the origin of a justification (text\ncopied/not copied from the URL selected) on: the absolute\nvalue of the prediction error (left; cumulative distributions\nshown with thinner lines and empty markers), and the pre-\ndiction error (right).\njustification include some \u201cfree text\u201d (i.e., text generated and writ-\nten by the worker), and 52% do not. Considering all the possible\ncombinations, 6% of the justifications used both free text and text\nfrom web page, 42% used free text but no text from the web page,\n48% used no free text but only text from web page, and finally 4%\nused neither free text nor text from web page, and either inserted\ntext from a page of a different (not selected) web page or inserted\npart of the instructions we provided or text from the user interface.\nConcerning the preferred way to provide justifications, each\nworker seems to have a clear attitude: 48% of the workers used only\ntext copied from the selected web pages, 46% of the workers used\nonly free text, 4% used both, and 2% of them consistently provided\ntext coming from the user interface or random web pages.\nWe now correlate such a behavior with the workers quality. Fig-\nure 10 shows the relations between different kinds of justifications\nand the worker accuracy. The plots show the absolute value of the\nprediction error on the left, and the prediction error on the right.\nThe lines in the plots indicate if the text inserted by the worker\nwas copied or not from the web page selected. We did the same\nanalysis to investigate if the worker used or not free text, and the\nplots where almost indistinguishable.\nAs we can see from the plot , statements on which workers make\nless errors (i.e., where x-axis =0) tend to use text copied from the\nweb page selected. On the contrary, statements on which workers\nmake more errors (values close to 5 in the left plot, and values\nclose to +/- 5 in the right plot) tend to use text not copied from\nthe selected web page. The differences are small, but it might be\nan indication that workers of higher quality tend to read the text\nfrom selected web page, and report it in the justification box. To\nconfirm this result, we computed the CEMORDscores for the two\nclasses considering the individual judgments: the class \u201ccopied\u201d has\nCEMORD=0.62, while the class \u201cnot copied\u201d has a lower value, CEMORD\n=0.58. The behavior is consistent for what concerns the usage of\nfree text (not shown).\nBy looking at the right column of Figure 10 we can see that\nthe distribution of the prediction error is not symmetrical, as the\nfrequency of the errors is higher on the positive side of the x-\naxis ([0,5]). These errors correspond to workers overestimating\nthe truthfulness value of the statement (with 5 being the result of\nlabeling a pants-on-fire statement as true ). This is consistent\nwith what observed in Sect. 5.1.2. It is also noticeable that the\njustifications containing text copied from the selected URL have a\nlower rate of errors in the negative range, meaning that workers\nwhich directly quote the text avoid underestimating the truthfulness\nof the statement. These could be other useful signals to be exploited\nin future work to obtain more effective aggregation methods.\n6 CONCLUSIONS AND FUTURE WORK\nThe work presented in this paper is, to the best of our knowledge,\nthe first one investigating the ability of crowd workers to identify\nand correctly categorize recent health statements related to the\nCOVID-19 pandemic. The workers performed a task consisting\nof judging the truthfulness of 8 statements using our customized\nsearch engine, which allows us to control worker behavior. We\nanalyze workers background and bias, as well as workers cognitive\nabilities, and we correlate such information to the worker quality.\nWe publicly release the collected data to the research community.\nThe answers to our research questions can be summarized as\nfollows. We found evidence that the workers are able to detect\nand objectively categorize online (mis)information related to the\nCOVID-19 pandemic (RQ1). We found that while the agreement\namong workers does not provide a strong signal, aggregated work-\ners judgments show high levels of agreement with the expert labels,\nwith the only exception of the two truthfulness categories at the\nlower end of the scale ( pants-on-fire andfalse ). We found that\nboth crowdsourced and expert judgments can be transformed and\naggregated to improve label quality (RQ2). We found that worker\npolitical background, self-reported in a questionnaire, is indicative\nof label quality (RQ3). We found several promising behavioral sig-\nnals that are clearly related with worker quality (RQ4). Such signals\nmay effectively inform new ways of aggregating crowd judgments\n(e.g., see [ 3,25]), which we believe is a promising direction for\nfuture work. Finally, we found that workers use multiple sources\nof information, and they consider both fact-checking and health-\nrelated websites. We also found interesting relations between the\njustifications provided by the workers and the judgment quality\n(RQ5). Future work also includes reproducing our experiments in\nother crowdsourcing platforms to target other cohorts of workers.\nACKNOWLEDGMENTS\nThis work is partially supported by a Facebook Research award, by\nan Australian Research Council Discovery Project (DP190102141),\nand by the project HEaD \u00e2\u0102\u015e Higher Education and Development -\n1619942002 / 1420AFPLO1 (Region Friuli \u00e2\u0102\u015e Venezia Giulia).\nREFERENCES\n[1]Enrique Amig\u00f3, Julio Gonzalo, Stefano Mizzaro, and Jorge Carrillo-de-Albornoz.\n2020. An Effectiveness Metric for Ordinal Classification: Formal Properties and\nExperimental Results. In Proceedings of ACL . 3938\u20133949.\n[2]Pepa Atanasova, Preslav Nakov, Llu\u00eds M\u00e0rquez, Alberto Barr\u00f3n-Cede\u00f1o, Georgi\nKaradzhov, Tsvetomila Mihaylova, Mitra Mohtarami, and James Glass. 2019.\nAutomatic Fact-Checking Using Context and Discourse Information. J. Data and\nInformation Quality 11, 3, Article 12 (2019), 27 pages.\n[3]Yukino Baba and Hisashi Kashima. 2013. Statistical Quality Estimation for General\nCrowdsourcing Tasks. In Proceedings of KDD . 554\u2013562.\n[4]Joseph Bullock, Alexandra Luccioni, Katherine Hoffmann Pham, Cynthia Sin Nga\nLam, and Miguel Luengo-Oroz. 2020. Mapping the Landscape of Artificial Intelli-\ngence Applications against COVID-19. arXiv:2003.11336\n[5]Alessandro Checco, Kevin Roitero, Eddy Maddalena, Stefano Mizzaro, and Gian-\nluca Demartini. 2017. Let\u2019s Agree to Disagree: Fixing Agreement Measures for\nCrowdsourcing. In Proceedings of HCOMP . 11\u201320.[6]Xinran Chen, Sei-Ching Joanna Sin, Yin-Leng Theng, and Chei Sian Lee. 2015.\nWhy Students Share Misinformation on Social Media: Motivation, Gender, and\nStudy-level Differences. Journal of Academic Librarianship 41, 5 (2015), 583\u2013592.\n[7]Matteo Cinelli, Walter Quattrociocchi, Alessandro Galeazzi, Carlo Michele Valen-\nsise, Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo, and\nAntonio Scala. 2020. The COVID-19 Social Media Infodemic. arXiv:2003.05004\n[8]Aakash Desai, Jeremy Warner, Nicole Kuderer, Mike Thompson, Corrie Painter,\nGary Lyman, and Gilberto Lopes. 2020. Crowdsourcing a Crisis Response for\nCOVID-19 in Oncology. Nature Cancer 1, 5 (2020), 473\u2013476.\n[9]Tamer Elsayed, Preslav Nakov, Alberto Barr\u00f3n-Cede\u00f1o, Maram Hasanain, Reem\nSuwaileh, Giovanni Da San Martino, and Pepa Atanasova. 2019. Overview of the\nCLEF-2019 CheckThat! Lab: Automatic Identification and Verification of Claims.\nInProceedings of CLEF . 301\u2013321.\n[10] Shane Frederick. 2005. Cognitive Reflection and Decision Making. Journal of\nEconomic Perspectives 19, 4 (December 2005), 25\u201342.\n[11] Riccardo Gallotti, Francesco Valle, Nicola Castaldo, Pierluigi Sacco, and Manlio De\nDomenico. 2020. Assessing the Risks of \u201cInfodemics\u201d in Response to COVID-19\nEpidemics. arXiv:2004.03997\n[12] Lei Han, Kevin Roitero, Ujwal Gadiraju, Cristina Sarasua, Alessandro Checco,\nEddy Maddalena, and Gianluca Demartini. 2019. All those wasted hours: On task\nabandonment in crowdsourcing. In Proceedings of WSDM . 321\u2013329.\n[13] Lei Han, Kevin Roitero, Ujwal Gadiraju, Cristina Sarasua, Alessandro Checco,\nEddy Maddalena, and Gianluca Demartini. 2019. The Impact of Task Abandon-\nment in Crowdsourcing. IEEE TKDE (2019), 1\u20131.\n[14] Lei Han, Kevin Roitero, Eddy Maddalena, Stefano Mizzaro, and Gianluca Demar-\ntini. 2019. On Transforming Relevance Scales. In Proceedings of CIKM . 39\u201348.\n[15] Jooyeon Kim, Dongkwan Kim, and Alice Oh. 2019. Homogeneity-Based Trans-\nmissive Process to Model True and False News in Social Networks. In Proceedings\nof WSDM . 348\u2013356.\n[16] Klaus Krippendorff. 2011. Computing Krippendorff\u2019s Alpha-Reliability. (2011).\n[17] David La Barbera, Kevin Roitero, Damiano Spina, Stefano Mizzaro, and Gianluca\nDemartini. 2020. Crowdsourcing Truthfulness: The Impact of Judgment Scale\nand Assessor Bias. In Proceedings of ECIR . 207\u2013214.\n[18] Eddy Maddalena, Kevin Roitero, Gianluca Demartini, and Stefano Mizzaro. 2017.\nConsidering Assessor Agreement in IR Evaluation. In Proceedings of ICTIR . 75\u201382.\n[19] Yelena Mejova and Kyriaki Kalimeri. 2020. Advertisers Jump on Coronavirus\nBandwagon: Politics, News, and Business. arXiv:2003.00923\n[20] Tsvetomila Mihaylova, Georgi Karadjov, Pepa Atanasova, Ramy Baly, Mitra\nMohtarami, and Preslav Nakov. 2019. SemEval-2019 Task 8: Fact Checking in\nCommunity Question Answering Forums. In Proceedings of SemEval . 860\u2013869.\n[21] Howard R Moskowitz. 1977. Magnitude Estimation: Notes on What, How, When,\nand Why to Use It. Journal of Food Quality 1, 3 (1977), 195\u2013227.\n[22] Preslav Nakov, Alberto Barr\u00f3n-Cede\u00f1o, Tamer Elsayed, Reem Suwaileh, Llu\u00eds\nM\u00e0rquez, Wajdi Zaghouani, Pepa Atanasova, Spas Kyuchukov, and G Da San Mar-\ntino. 2018. Overview of the CLEF-2018 CheckThat! Lab on Automatic Identifica-\ntion and Verification of Political Claims. In Proceedings of CLEF . 372\u2013387.\n[23] Gordon Pennycook, Jonathon McPhetres, Yunhao Zhang, and David Rand. 2020.\nFighting COVID-19 Misinformation on Social Media: Experimental Evidence for\na Scalable Accuracy Nudge Intervention. PsyArXiv (2020). psyarxiv.com/uhbk9\n[24] Kashyap Kiritbhai Popat. 2019. Credibility Analysis of Textual Claims with Ex-\nplainable Evidence . Ph.D. Dissertation. Saarland University, Saarbr\u00fccken.\n[25] Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles\nFlorin, Luca Bogoni, and Linda Moy. 2010. Learning From Crowds. Journal of\nMachine Learning Research 11, 43 (2010), 1297\u20131322.\n[26] Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, Kyle Lo,\nIan Soboroff, Ellen Voorhees, Lucy Lu Wang, and William R Hersh. 2020. TREC-\nCOVID: Rationale and Structure of an Information Retrieval Shared Task for\nCOVID-19. Journal of the American Medical Informatics Association (2020).\n[27] Kevin Roitero, Gianluca Demartini, Stefano Mizzaro, and Damiano Spina. 2018.\nHow Many Truth Levels? Six? One Hundred? Even More? Validating Truthful-\nness of Statements via Crowdsourcing. In Proceedings of the CIKM Workshop on\nRumours and Deception in Social Media (RDSM\u201918) .\n[28] Kevin Roitero, Eddy Maddalena, Gianluca Demartini, and Stefano Mizzaro. 2018.\nOn Fine-Grained Relevance Scales. In Proceedings of SIGIR . 675\u2013684.\n[29] Kevin Roitero, Michael Soprano, Shaoyang Fan, Damiano Spina, Stefano Mizzaro,\nand Gianluca Demartini. 2020. Can The Crowd Identify Misinformation Objec-\ntively? The Effects of Judgment Scale and Assessor\u2019s Background. In Proceedings\nof SIGIR . 439\u2013448.\n[30] Andon Tchechmedjiev, Pavlos Fafalios, Katarina Boland, Malo Gasquet, Matth\u00e4us\nZloch, Benjamin Zapilko, Stefan Dietze, and Konstantin Todorov. 2019. ClaimsKG:\nA Knowledge Graph of Fact-Checked Claims. In Proceedings of ISWC . 309\u2013324.\n[31] William Yang Wang. 2017. \u201cLiar, Liar Pants on Fire\u201d: A New Benchmark Dataset\nfor Fake News Detection. In Proceedings of ACL . 422\u2013426.\n[32] Kai-Cheng Yang, Christopher Torres-Lugo, and Filippo Menczer. 2020. Prevalence\nof Low-Credibility Information on Twitter During the COVID-19 Outbreak.\narXiv:2004.14484\n[33] Arkaitz Zubiaga and Heng Ji. 2014. Tweet, but Verify: Epistemic Study of Infor-\nmation Verification on Twitter. SNAM 4, 1 (2014), 1\u201312.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "The covid-19 infodemic: Can the crowd judge recent misinformation objectively?", "author": ["K Roitero", "M Soprano", "B Portelli", "D Spina"], "pub_year": "2020", "venue": "Proceedings of the 29th \u2026", "abstract": "Misinformation is an ever increasing problem that is difficult to solve for the research community  and has a negative impact on the society at large. Very recently, the problem has been"}, "filled": false, "gsrank": 479, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3340531.3412048", "author_id": ["1xd52jMAAAAJ", "ocK0qRUAAAAJ", "HQZd6wsAAAAJ", "sLzYrNYAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:UU28XknhprMJ:scholar.google.com/&output=cite&scirp=478&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D470%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=UU28XknhprMJ&ei=W7WsaJnZIo6IieoP0sKRuAk&json=", "num_citations": 59, "citedby_url": "/scholar?cites=12945281884161330513&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:UU28XknhprMJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2008.05701"}}, {"title": "Propagation of the QAnon conspiracy theory on Facebook", "year": "2021", "pdf_data": "Propagation of the QAnon Conspiracy Theory on Facebook  \nSoojong Kim * \nsjkim97@ stanford.edu  \nStanford Un iversity  Jisu Kim  \njisu.kim@yale.edu   \nYale Law School   \nABSTRACT  \nThere has been concern about the proliferation of the \u201cQAnon\u201d \nconspiracy theory on Facebook, but little is known about how \nits misleading narrative propagated on the world\u2019s largest \nsocial media platform. Thus, the present research analyzed \ncontent generated by 2,813 Facebook pages and groups that \ncontributed to promoting the conspiracy narrative between \n2017 and 2020. The result demonstrated that activities of \nQAnon pages and groups started a significant surge months \nbefore the 2020 U.S. Presidential Election. We found that these \npages and groups increasingly relied on internal sources, i.e., \nFacebook accounts or their content on the platform, while their \ndependence on external information sources decreased \ncontinuously since 2017. It was also found that QAnon posts \nbased on the Facebook internal sources attracted significantly \nmore shares and comments compared with other QAnon pos ts. \nThese findings suggest that QAnon pages and groups \nincreasingly isolated themselves from sources outside \nFacebook while having more internal interactions within the \nplatform, and the endogenous creation and circulation of \ndisinformation might play a si gnificant role in boosting the \ninfluence of the misleading narrative within Facebook. The \nfindings imply that the efforts to tackle down disinformation on \nsocial media should target not only the cross -platform \ninfiltration of falsehood but also the intra -platform production \nand propagation of disinformation.  \nCCS CONCEPTS  \n\u2022 Human -centered computing \u2192 Collaborative and social \ncomputing  \u2022 Social and professional topics \u2192 Computing / \ntechnology policy  \nKEYWORDS  \nSocial media, Disinformation, Misinformation, Conspiracy \ntheory, Online community   1 Introduction  \nThere has been growing concern about the proliferation of \ndisinformation and conspiracy theories on social media [1, 18, \n24, 25, 33, 35] . Participating in and endorsing online conspiracy \ntheory discussions is known to lead to  rejection of science, \nunwillingness to involve in prosocial behaviors, and even \nextreme offline behaviors and violen ce [32, 34] . Especially, \n\u201cQAnon,\u201d a far -right conspiracy theory based on a set of \nunsubstantiated claims in favor of the former U.S. President \nDonald J. Trump, is known to have widely spread on social \nmedia platforms since 2017 [15, 52, 60] . QAnon is centered on \nunsubstantiated and false claims made by an anonymous \nindividual known as \u201cQ,\u201d that  a global pedophile cabal, \nincluding top Democrats, operates a child sex -trafficking ring, \nand Donald J. Trump would finally unmake and punish its \nmembe rs [43] . It has been reported that Facebook has been \nparticularly plagued with the conspiracy narrative [15, 40] , but \nthere has been a lack of systematic investigation of how QAnon \npropagated on the world\u2019s largest social media platform.  \nThis resea rch investigated the propagation of the QAnon \nnarrative based on, to the best of our knowledge, the most \nextensive dataset about the conspiracy theory on Facebook. \nScholars have pointed out the need to understand the \necosystem in which disinformation is pr oduced and propagated \nwithin and across online platforms [19, 31, 50, 52, 55, 60] . The \nQAnon conspiracy theory i s known to encourage its audience to \nassemble and remix different pieces of information [60] , so it is \neven more crucial to examine how onl ine communities \ndiscussing QAnon interacted with the broader ecosystem by \ndrawing information from various sources to construct and \nsupport their narratives.  \nThus, in this work, we examined the activities of Facebook \npages and groups that contributed to pr omoting the conspiracy \ntheory and explored how they constructed the misleading \nnarrative. Specifically, we focused on how Facebook pages and \ngroups used information sources internal or external to \nFacebook in generating QAnon -related content. This study \naimed to answer the following questions: How did the activities \nof the online groups promoting the QAnon narrative on \nFacebook change over time? What information sources were \nused in discussing the conspiracy theory and how the sources \nof information influen ced users\u2019 engagement with the social \nmedia content?  Addressing these questions not only  help s \nunderstand  the information ecosystem of QAnon on Facebook  \n \n \n * To whom correspondence may be addressed  \nPermission to make digital or hard copies of part or all of this work for personal \nor classroom use is granted without fee provided that copies are not made or \ndistributed for profit or commercial ad vantage and that copies bear this notice \nand the full citation on the first page. Copyrights for third -party components of \nthis work must be honored. For all other uses, contact the owner/author(s).  \n \n\u00a9  20 22 Copyright held by the owner/author(s).   \n \n  \n \n \n but also sheds light on  how misleading  narratives are produced \nand promoted in digital spaces and what factors should be \nconsidered in developing strategies to reduce or prevent the \nspread of disinformation.  \n2 Related Works  \n2.1  Prior Research on Misinformation  \nThere has been growing academic interest in the production \nand consump tion of  online misinformation and conspiracy \ntheory content [4, 17, 19 \u201321, 30, 33, 35, 49] . In a study  based \non a nationally representative sample of Twitter users , \nGrinberg et al. [19]  examined the fake news propagation during \nthe 2016 U.S. Presidential Election  and found that most \nmisinformation was shared by a small number of \u201csuper -\nspreaders.\u201d  Leveraging  extensive access to Twitter\u2019s historical  \ndatabase , Vosoughi et al. [54]  showed  that false news stories \npropagated faster and mor e broadly than true ones. Starbird et \nal. [51]  investigated URL li nks shared by Twitter users and \nfound that the majority of the shared domains was alternative \nmedia or blogs. They explained that the users attempted to \nconstruct conspiracy narratives by referencing these sources.  \nScholars  also examined factors affecting  the spread of \nmisinformation [7, 9, 25, 44, 51, 54] . Buchanan [7] found that \nparticipants were more likely to share misinformation when \nthey regarded it  as true or consistent with their pre -existing \nattitudes. Considering the platform -level factors, Hussein et al. \n[25]  examine d the effect of personalization attributes on the \nextent of misinformation in future searches and \nrecommendations on YouTube. Focusing on the strategies of \nconspiracy theorists, Kou et al. [30]  identified that Reddit users  \ntried to defend conspiracy theories by selectively citing \nauthoritative information, proposing unknowable risks, casting \ndoubts, and deflecting the burden of proof.  \nPioneering studies on  the conspiracy theory that had \nimmense i mpacts on American  politics and society, QAnon , \nstarted to appear  recently.  One of the most extensive and \nrigorous studies  on this issue was  conducted by Aliapoulios et \nal. [1]. Based on large -scale multi -platform datasets, including \n4chan, 8kun, Reddit, Voat, and other aggregation sites, the \nauthors explored how the QAnon theory evolved and spread on \nthe Web and found that the initial dissemination of the \nconspiracy theory was limited to a few sub -communities.  \nPapasavva et al. [35]  explored the activities of QAnon \ncommunities on Voat.co, an alternative social network service, \nand reported that the conspiracy narrative revolved around \nDonald Trump and other political matters.   \n2.2  Misinformation and Conspiracy Theory \non Facebook  \nFacebook has been criticized for failing to suppress misleading \nand unsubstantiated narratives on its platform [13, 15, 27] . It \nhas been suspected that Facebook pages and groups have produce d and promote d conspiracy theories, unverified \nrumors, and inaccurate information [6, 9, 27, 28, 45] . However, \nmost of the past research focused on Twitter and other \nplatforms, as discussed above. Despite its importance as the \nworld\u2019s biggest social media, the number of empirical \ninvestigations of Facebook is still insufficie nt, and , more \nimportantly,  research on how disinformation and conspiracy \ntheor ies are discussed and exchanged  on Facebook is lacking . \nPast  studies  about  misinformation and conspiracy theory  \non Facebook  have been based on  (1) Facebook\u2019s API, which is \nnot widely used by researchers  currently  due to its limitations, \n(2) access to Facebook\u2019s database as an insider  of the company , \nwhich is not a feasible option for academic researchers , or (3) \nmanual data collection, which is not a n optimal solution  for \nlarge -scale quantitative investigations [4, 9, 16] . For example, \nFriggeri et al. [16]  examined how fact -checking affected rumor \npropagation. Del Vicario et al . [9] and Bessi et al. [4] compared \nFacebook pages posting science news and those disseminating \nconspiracy theories. Johnson et al. [29]  analyzed the pa tterns of \nFacebook pages with pro - and anti -vaccination viewpoints and \nargued that the growth of anti -vaccination pages was faster \nthan the pro -vaccination pages.  Recently, researchers started \npaying attention to  CrowdTangle, a public insights tool owned \nand operated by Facebook , for Facebook data collection ( e.g. \nYang et al. [58] ). \nThe present study focuses on e xamining the use of \ndifferent information sources in promoting disinformation  and \nits impact on user engagement , which could  shed  light on the \necosystem o f online misleading narratives and on  how the se \nnarratives are amplified  in digital spaces . A few p revious \ninvestigations provide findings  that are relevant to  the current \nresearch . Bessi et al. [4] reported that conspiracy content  about \nscientific issues  received more likes and shares on Facebook \ncompared with mainstream news content.  Mocanu et al. [33]  \nstudied how alternative news, mainstream news, and self -\norganized online political movements were associated with \nuser engagement on Facebook. They discovered that those who \npreferred alternative information sources were more \nsusceptible to misinformation .  \n3 Method and Data  \nIn the present study, QAnon posts were defined as Facebook \nposts containing terms and slogans used in the context of the \nQAnon conspiracy theory, and QAnon clusters were defined as \npublic Facebook pages and groups that created multiple  QAnon \nposts. QAnon clusters contributed to the spread of the \nconspiracy narrative by repeating the terms and phrases \nrelated to QAnon in the posts they generated.  \nIn total, we identified 2,813 QAnon clusters that were \nactive during the period between Nove mber 1st, 2017, and \nNovember 30th, 2020, using a multi -staged data collection \nmethod that enabled us to obtain a comprehensive Facebook \ndataset. These QAnon clusters created 117,553 QAnon posts, \nPropagation of the QAnon Conspiracy Theory on Facebook   \n \n and Facebook users following the clusters received QAnon \npost s in their social media timelines over 3.1 billion times in \ntotal ( See Appendix  1.2 in Supplementary Information  for \ndetails ). Example QAnon posts are shown in Table 1. \n \nTable 1. Examples of QAnon Posts  \n# Text content  \n1 \u201c#PatriotsUnite Stand together! Where we go One We go All \u2026 \n#WWG1WGA\u201d  \n2 \u201cTRUMP SENDS OUT THE STORM TROOPERS \u2026 \n@realDonaldTrump #WWG1WGA\u201d  \n3 \u201c\u2026 CNN to frame upcoming Pizzagate arrests of politician \npedophiles \u2026 #Pedophiles #DCElites\u201d  \n4 \u201c\u2026 We want America First, We do not want a world bank .Hey \nSchummer ,Pillosi ,Warren Sanders, Watters, Gram , ,Mconnell, \nand all the other NWO (New World Order ) puppets in the \nDemocrat and Republican Parties \u2026 George Sorros should be \ninvestigated by the \u201cFederal Government\u201d #maga\u201d  \n5 \u201c\u2026 the Glory awaiting humanity! \u2026 #RedeemingTheFall \n#WWG1WGA\u201d  \nNote.  The text was partly removed to reduce the possibility of \nthe identification of specific communities and users. \nRemoved parts were replaced with \u201c\u2026\u201d.  \n3.1  Data Collection  \n3.1.1 Data Collection Method.   To collect data on QAnon \nclusters, we implemented a multi -staged data collection \nmethod combining data retrieval via CrowdTangle  (CT)  and \ncomputational techniques to discover new Facebook accounts. \nResearchers and journalists using CT can search and download \ndata on public Facebook pages and g roups. For researchers, CT \noffers two major advantages. First, it provides a powerful \nsearch function for Facebook content. CT users can search for \nFacebook posts that contain specific keywords in the text and \nimages included in the posts. Second, CT users  can access the \nentire historical record of Facebook accounts tracked by CT. CT \ntracks over 7 million influential Facebook pages, groups, and \nverified accounts but does not track all public Facebook \naccounts, which limits the scope of Facebook data retriev ed \nfrom CT [8, 56] . Thus, we implemented a new data collection \nmethod to overcome the limitation.  \nCT allows users to add and track new Facebook accounts \nwhich do not exist in its system [56]. Based on this feature, we \ndesigned a \u201csnowball\u201d data collection method that iterates data \ndownloading, link analysis, and data request, which enabled us \nto obtain sizeable data beyond the limited scope of the CT \ndatabase. (The Facebook accounts we i dentified during data \ncollection are now on the CT database and accessible to other \nCT users.)  \nThe data collection started on December 24th, 2020, and \nwas completed on January 8th, 2021. The timing of data \ncollection ensured that the collected data was not significantly \ninfluenced by Facebook\u2019s platform -level takedown of content \ncreated by Trump supporters and QAnon theorists attempting to delegitimize the outcome of the 2020 Presi dential Election \n[26] . \nThis study was approved by the Institutional Review \nBoard of [Removed for the blind review process].  \n3.1.2 Keyword Selection .  The QAnon conspiracy theory has \nbeen built upon and closely related to other conspiracy \ntheories, such as the \u201cpizzagate\u201d conspiracy theory and the \n\u201cdeep state\u201d conspiracy theory [43] . The QAnon narrative is \nbased on its own characteristic slogans and dialects, such as \n\u201cqarmy,\u201d but it also adopts terms from the related conspiracy \ntheories, such as \u201cpedogate.\u201d Thus, we developed two sets of \nkeywords to capture not only Facebook pages and groups \nrelying on QAnon -specific phrases but also those referencing \nthe QAnon context in its broad relationship with other relat ed \nconspiracy theories.  \nWe defined the two sets of keywords by expanding and \nmodifying keywords selected in previous studies [14, 53] . First, \nthe core  set of keywords consists of 14 terms used specifically \nin the QAnon co ntext . Second, the extended  set of keywords \nincludes all terms in the core set and additional terms used \nbroadly in the QAnon and related conspiracy context (40 terms \nin total). The data collection process aimed to identify Facebook \npages and groups which mentioned one or more core keywords \nand multiple extended keywords across multiple posts, as \ndescribed in 3.1.3 in detail.  \n1. Core  keywords: \u201cqanon, \u201d \u201cqarmy\", \u201cweareq ,\u201d \u201cweareallq ,\u201d \n\u201c17anon ,\u201d \u201cqdrop ,\u201d \u201cqpost ,\u201d \u201cqsentme ,\u201d \u201cqproof ,\u201d \n\u201casktheq ,\u201d \u201cgreatawakening ,\u201d \u201cwwg1wga ,\u201d \u201cthestorm ,\u201d \nand \u201ctheperfectstorm \u201d (14 terms).  \n2. Extended  keywords:  \u201cqanon, \u201d \u201cqarmy\", \u201cweareq ,\u201d \n\u201cweareallq ,\u201d \u201c17anon ,\u201d \u201cqdrop ,\u201d \u201cqpost ,\u201d \u201cqsentme ,\u201d \n\u201cqproof ,\u201d \u201casktheq ,\u201d \u201cgreatawakening ,\u201d \u201cwwg1wga ,\u201d \n\u201cthestorm ,\u201d and \u201ctheperfectstorm ,\u201d \u201cpainiscoming ,\u201d \n\u201cdarktolight ,\u201d \u201cenjoytheshow ,\u201d \u201ctaketheoath ,\u201d \n\u201ctrusttheplan ,\u201d \u201cquestioneverything ,\u201d  \u201cdraintheswamp ,\u201d \n\u201csheepnomore ,\u201d \u201ctakebackcontrol ,\u201d \u201cdigitalsoldiers ,\u201d \n\u201cwearethenews ,\u201d \u201ctruthseeker ,\u201d \u201cfollowthewhiterabbit ,\u201d \n\u201csavethechildren ,\u201d \u201csaveourchildren ,\u201d \u201cdeepstate ,\u201d  \n\u201cpedowood ,\u201d \u201cpedogate ,\u201d \u201cpedovores ,\u201d \u201cpizzagate ,\u201d \n\u201cadrenochrome ,\u201d \u201cspiritcooking ,\u201d \u201cthepeoplearesick ,\u201d \n\u201cepsteindidntkillhimself ,\u201d \u201cchildtrafficking ,\u201d and \n\u201chumantrafficking \u201d (40 terms).  \nAlthough the keywords used in this rese arch do not \nconstitute an exhaustive list of all words and phrases \nrepresenting the QAnon narrative, the selection of keywords \nlisted above and the filtering  rule s explained in 3.1.3  \nconsidered both the key characteristics of the QAnon narrative \nand its relationship s with other conspiracy theories, adopting \nand extending previous research on this issue [1, 14, 35, 53] . \n3.1.3 Iterative Data Collection Process .  We conducted \nthree rounds of data collection. In the first round, we used CT\u2019s \nPost Search API (Application Programming Interface) to \ndownload all English posts created between January 1st, 2010 \n  \n \n \n and December 23rd, 2020 that contained at least one keyword \nin the extended set. Based on the downloaded post data, we \ncreated a list of Facebook pages and groups that (i) generated \nat least 5 posts in the search results, (ii) stated at least 5 \ndifferent extended keywords in their posts appearing in the \nsearch results, and (iii) stated at least one core keyword in their \nposts appearing in the search results. These three criteria \nensure that selected pages and groups repeatedly created \ncontent discussing details of the QAnon narrative. For example, \npages and groups that uploaded few QAnon -related news \narticles cannot satisfy the first and second criteria. Pages  \noperated by legitimate news media, such as the Fox News page, \nare unlikely to satisfy the second criteria. The accounts that \npassed the filtering process of the first round were called the \n\u201cfirst -round accounts,\u201d and their posts included in the search \nresults were called the \u201cfirst -round posts.\u201d  \nIn the second round of data collection, we started by \nanalyzing all Facebook internal URLs included in the first -\nround posts and discovering new Facebook accounts that were \nlinked with the first -round accounts via these URLs. We then \nrequested CT to retrieve data from these newly identified \nFacebook accounts. After that, we utilized the Post Search API \nto retrieve all English posts that were generated by these new \naccounts between January 1st, 2010 and December 23rd, 2020, \nand contained at least one keyword in the extended set. Based \non the downloaded data, we created a list of Facebook accounts \nsatisfying the same three conditions used in the first round. The \naccounts that passed the filtering process of the second round \nwere called the \u201csecond -round accounts,\u201d and their posts \nincluded in the search results were called the \u201csecond -round \nposts.\u201d  \nThe third round started by analyzing all Facebook internal \nURLs included in the second -round posts and identifying new \naccou nts that were linked with the second -round accounts via \nURLs but were not included in the first -round and second -\nround accounts. We then requested CT to retrieve data on these \nnewly identified Facebook accounts and retrieved all English \nposts which were cr eated by the accounts between January 1st, \n2010 and December 23rd, 2020, and contained at least one \nkeyword in the extended set. From the downloaded content \ndata, we identified Facebook pages and groups satisfying the \nsame three conditions for the QAnon ac count selection. Only \none new QAnon cluster was identified in the third round, and \nthus we stopped the iterative data collection process after \nfinishing this round.  \nLastly, all post data retrieved during the three rounds were \ncombined, and then posts crea ted before November 1st, 2017, \nor after November 30th, 2020 were removed from the combined \ndata, since the QAnon conspiracy theory allegedly started in \nOctober 2017.  \nIt is known that QAnon supporters hijacked some of the \nslogans that had long been used by  human rights and anti -\ntrafficking groups [42] . Therefore, we conducted additional \ndata cleaning by reviewing 39 Facebook pages and groups in the collected data that passed the filtering with only 5 \nkeywords: \u201cqanon,\u201d \u201csavethechildren ,\u201d \u201csaveourchildren ,\u201d \n\u201cchildtrafficking ,\u201d and \u201chumantrafficking. \u201d Two researchers \nreviewed these pages and groups and identified that 38 of them \nwere pure human rights and anti -trafficking communities. \nThese 38 pages and groups could pass ou r keyword filtering \ncriteria because (i) they had been using the anti -trafficking \nslogans related to human rights purposes and (ii) they had \nstated \u201cqanon\u201d while denying their relationships with or \nexpressing their concern about the conspiracy theory. We \nremoved the 38 pages and their posts from the collected data.  \nThe resulting data contained 117,553 QAnon posts \ngenerated by 2,813 QAnon clusters (2,070 Facebook pages, 743 \nFacebook groups). Although, without access to Facebook\u2019s \ninternal knowledge about its  website architecture and database \nstructure, we cannot assert that the collected data include a \ncomplete set of Facebook pages and groups satisfying our \nsearch criteria, this method enabled researchers to observe an \nextensive number of accounts beyond the  limited pool offered \nby CrowdTangle, one of the very few ways allowed for \nquantitative researchers investigating Facebook.  \n3.2  Preliminary Classification of QAnon \nClusters  \nTo understand the topics generally discussed in QAnon \nclusters, we analyzed the to p 200 clusters based on the number \nof QAnon posts generated in the given time period. Excluding \n46 clusters that were not accessible as of April 2021, two \nsubject -matter experts manually accessed and reviewed the \nQAnon cluster webpages and identified 7 dif ferent types of \nQAnon clusters. These types include supporters of politicians \nor journalists, general discussion groups, human right s groups, \nconspiracy theory groups, and spiritual discussion groups. \nExample QAnon clusters of each type are shown in Table 2. \n \nTable 2. Preliminary Classification of QAnon Clusters  \nCategory  N Example page/group names  \nSupporters of politicians and \njournalists  51 \u201c\u2026 Trump Only \u2026\u201d,   \n\u201c\u2026 Keep America Great \u2026\u201d  \nIndependent news organization  24 \u201c\u2026News and Views\u201d  \nForums ( Groups sharing \ninformation about general \nissues ) 24 \u201c\u2026 Truth Movement\u2026\u201d  \nHuman right s group and non -\nprofit org  19 \u201c\u2026 Family Rights\u201d,  \n\u201c\u2026 Against Trafficking\u201d, \n\u201cSaveourchildren\u2026\u201d  \nConspiracy theory group  18 \u201c\u2026 Truth Seekers\u201d, \u201cExposing \nthe Lies of \u2026\u201d  \nSpiritual discussion group  8 \u201c\u2026 Cosmic News\u201d,  \n\u201cGalactic \u2026\u201d  \nMiscellaneous  10  \nNote . Names were partly removed to reduce the possibility of the \nidentification of specific pages and groups. Removed parts were \nreplaced with \u201c\u2026\u201d. T he top 200 clusters based on the number of \nQAnon posts  were chosen for this preliminary classification, and \namon g them, 46 clusters that were not accessible as of April 2021  \nwere excluded.  \nPropagation of the QAnon Conspiracy Theory on Facebook   \n \n 3.3  URLs and Information Sources  \nWe explored the information ecosystem that enabled QAnon \nclusters by analyzing URLs (Unified Resource Locators) \nincluded in QAnon posts and iden tifying information sources \nused by the clusters. Facebook pages and groups tend to rely on \nmultiple information sources in generating their content. \nInformation sources used in a Facebook post can be traced back \nby analyzing URLs included in the post. Eac h URL corresponds \nto a source from which a piece of information or content was \ndrawn, such as a photo included in a post, a newspaper article \nlinked in a post, and a website address referenced in a post. A \npost can include multiple URLs, and CrowdTangle pr ovides \ninformation on all URLs included in a post. For example, if a post \ncreated by Cluster A includes a hyperlink to a Fox News Article \nwhile resharing another post from Cluster B, and if the post \nfrom B included a video uploaded by B while mentioning a CNN \narticle by including a hyperlink to it, the data on the Cluster A\u2019s \npost will include the URL of the Fox News article, the URL of the \nphoto uploaded by Cluster B, and the URL of the CNN article. At \nthe web domain level, the post in Cluster A is drawing  \ninformation from three different sources: foxnews.com, \nfacebook.com, and cnn.com.  \nFacebook users often use link shortening services to create \na new URL and include it in their social media content instead \nof its original full URL. Link shortening services  remove some \nof the information available in a full URL while creating a \nshorter version of it. For example, bit.ly, a URL shortening \nservice, shortens https://en.wikipedia.org/wiki/QAnon  into \nhttps://bit.ly/3hKCAOQ . In this example, while the original full \nURL shows the actual domain (wikipedia.org), its shortened \nversion does not.  Thus, we identified shortened URLs included \nin Facebook posts, retrieved their full UR Ls, and identified their \nactual domains. To identify shortened URLs, we built a list of 55 \npopular link shortening services by supplementing the list of \nservices reported by Yang et al. [58]  and adding 6 more services \nto the list: fb.me, is.gd, chng.it, tobtr.com, eepurl.com, cutt.ly, \nand gf.me. The full list of 55 shortening services covers all \nshortening services responsible for at least 30 URLs in our \ndataset. Based on the list of link sho rtening services, we \ndetected shortened URLs in Facebook posts and identified their \noriginal URLs and actual domains. For shortened URLs whose \noriginal URLs were not identifiable, domains of their link \nshortening services were considered as their actual do mains.  \n3.4  Classification of URLs  \nWe identified top -level domains of all URLs. For example, the \ndomain of an URL, www.nytimes.com/article/what -is-\nqanon.html , is nytimes.com. Each URL was then categorized \ninto one of the two source types according to its domain.  \n3.4.1 Facebook Internal Sources .  URLs whose domains are \nfacebook.com, fb.com, and fb.watch were categorized as \nFacebook internal sources. The existence of Facebook internal \nsources in a post indicates that the posting user included or \nreferenced Facebook in -house content (e.g., photos, vi deos, and notes stored on the platform), accounts (e.g., pages, groups, and \nindividual profiles), or services.  \n3.4.2 External Sources .  URLs that are not Facebook \ninternal sources were categorized as \u201cexternal sources.\u201d Three \nsubgroups of external sources were further identified: social \nmedia sources, news sources, and low credibility sources, \nwhich are known to be critical in understanding the creation \nand propagation of disinformation [3, 19, 21, 57] . The detection \nof subgroups of external sources is explained in Appendix 4  in \nSupplement ary Information.  \n4 Results and Discussion  \n4.1  The Activity of QAnon Clusters  \nA descriptive overview showed that the activity of QAnon \nclusters  included in the current dataset  had significantly \nintensified in 2020.  \n \nFigure 1. The activity of QAnon clusters on Facebook . A. The \nnumber of active clusters. Active clusters are defined as \nclusters that created at least one post in a certain time period. \nA dot indicates the number of active clusters in a given year. B. \nThe number of posts generated by an a ctive cluster each year. \nA green triangle indicates a mean. Each grey dot represents a \ncluster. C. Cluster lifetime and the average number of weekly \nposts. The lifetime of a cluster is defined as the time duration \nbetween its first and last QAnon posts. Ea ch line corresponds to \na cluster, starting at the date of its first post and ending at the \ndate of its last post. The color of a line represents the average \nnumber of posts per week created by the cluster, N w. Black: N w \n> 1.5, Brown: N w > 1.0, Red: N w > 0.5, Yellow: N w > 0.1, Grey: N w \n\u2264 0.1. D. The average weekly posts generated by a cluster \nduring its lifetime. Clusters were grouped into four different \ncohorts based on the year that their first QAnon post was \ngenerated. E. The number of posts generated by a cluster in its \nfirst month.  \nFirst, the number of active clusters in our data drastically \nincreased since 2017 ( Figure  1A). Second, active clusters in \n\n  \n \n \n 2020 generated 2.0 and 1.9 times more posts than those in \n2019 and 2018, respectively ( Figure  1B). Third, the slope of the \ncumulative curve and the distribution of colors in Figure  1C \ndemonstrate a surge in the number of clusters that newly \nparticipated in 2020 and intense activities of these new \nclusters. Specifically, when all clusters were clas sified into four \ndifferent cohorts based on the year that their first QAnon post \nwas created, the 2020 cohort produced 7.1 QAnon posts per \nweek on average, which was 8.4 and 20.5 times greater than the \n2019 and 2018 cohorts, respectively ( Figure  1D). Lastl y, the \nnumber of posts generated within the first month was the \nhighest for the 2020 cohort, and it was 2.1 and 3.6 times greater \nthan the 2019 and 2018 cohorts, respectively ( Figure  1E).  See \nAppendix 1 for detail ed analyses . \n  \nFigure 2. The volume of Fac ebook content containing a \nspecific keyword  \nIt is worth emphasiz ing that we report  these observations  \nto provide preliminary descriptive information about  the main \ndataset of the current  resea rch, before  discussing , in the \nfollowing sections  of 4.2 and 4.3,  detailed patterns and \nassociations  within the dataset.  Although we believe that these \ndescriptive findings are informative, i t is not our goal to claim \nthat the total number  of the problematic Facebook QAnon \ncontent  and online groups  showed a n exclusive  pattern of a \nsurge  in the entire platform  or to reveal  what  caused th is \npattern of growth . Let alone  our intent, rigorous validation of \nthis hypothetical claim is nearly impossible for now, largely \nbecause of the limited scope and scale of Face book data that \nacademic researchers can access . \nHowever, due to potential social and policy implications \nthat these surging patterns have, one may still question if  the \nincreasing volume  of Facebook content  over time  shown in this \nstudy  is a general  trend that  Facebook posts containing any \nkeywords  show  in the given  period . Another potential \nargument is that the volume of Facebook content containing \nany keywords had increased since the beginning of the \npandemic  (e.g., due to the increased screen time during \nlockdown s). We attempted to briefly explore  these claims by \ncheck ing if the number of posts containing other keywords \nshows a longitudinal pattern similar to  those containing \n\u201cqanon .\u201d Although , considering our limited time and resources,  we did not repeat the iterative  procedure that we followed in \ncollect ing the main dataset  of the current research , we used \nCT\u2019s Post Search API to retrieve  content from public Facebook \npages and groups  and count ed the monthly number of all posts  \ncontaining  each of  the following keywords : keywords \nrepresenting relatively general political issues (\u201cgun control ,\u201d \n\u201cimmigration ban\u201d ), political figures (\u201cbarack obama,\u201d \u201chillary \nclinton ,\u201d \u201cdonald trump\u201d ), pandemic -related terms  (\u201ccovid -19,\u201d \n\u201csocial distancing\u201d),  non-political issues related to conspiracy \ntheories (\u201cflat earth,\u201d \u201cchemtrail\u201d), and general non -political \ntopics  (\u201cweather ,\u201d \u201cdinner recipe,\u201d  \u201cbeyonce ,\u201d and \n\u201cbasketball\u201d ). For each keyword, the normalized number of \nFacebook posts was computed in each mont h by dividing the \nnumber of all Facebook posts containing a keyword in a given \nmonth with the 107 -month average number of posts containing \nthe keyword. As shown in Figure 2 , except for the posts \ncontaining \u201cqanon,\u201d Facebook posts  including a keyword  did \nnot display an increasing pattern over time . Also, significant \nsurges during the pandemic were not identified except for the \nposts containing \u201cqanon\u201d and the two other pandemic -related \nkeywords, \u201ccovid -19\u201d and \u201csocial distancing.\u201d  \n4.2  Information Sources for the QAnon \nNarrative  \nTo investigate the information ecosystem that enabled the \nproduction and distribution of the QAnon narrative, we \nanalyzed information sources used by QAnon clusters.  \n \nFigure 3 . Information sources for QAnon clusters . A. The \naverage proportion of posts in a cluster using different types of \nsources. \u201cAny source\u201d refers to the proportion of posts in a \ncluster containing one or more URLs. Bars and error bars \nindicate mean \u00b1 95% confidence interval. Each gra y dot \nrepresents an active cluster in a given year. Four bars in each \ngroup represent different years: 2017 (the leftmost), 2018, \n2019, and 2020 (the rightmost), respectively. B. The average \nproportion of posts in a cluster using different subtypes of \nexte rnal sources. C. The distribution of proportions of posts in \na cluster using Facebook internal sources. D. Posts in a cluster \nusing external sources. E. The average proportion of posts in a \n\nPropagation of the QAnon Conspiracy Theory on Facebook   \n \n cluster using the two different video sources. Error bars \nindicate mean \u00b1 95% confidence interval.  \nWe examined the proportion of posts using information \nsources within the Facebook platform (i.e., Facebook internal \nsources) and other sources outside the p latform (i.e., external \nsources). The prevalence of posts using Facebook internal \nsources had increased significantly each year from 30.0% in \n2017 to 50.0% in 2020, while the use of external sources \nsteadily decreased over time from 74.4% in 2017 to 52.7% in \n2020. These results show that the misleading narrative \nincreasingly more relied on information sources within \nFacebook , drawing less and less information from outside . It is \npossible that t he increasing separation is a platform -level shift , \nnot limited to the QAnon narrative, but this observation  still has \nsignificant implication s, regarding  the role played by the \nplatform and potential interventions to reduce the conspiracy \ntheory narrative.  Figure 3 C and D visualize the drastic decrease \nand increase in  detail. These changes are discussed in more \ndetail in Appendix 2 . \nWe further investigated three subcategories of external \nsources: social media sources, news sources, and low credibility \nsources ( Figure 3 B). The proportion of posts using social media \nsour ces sharply declined from 40.6% in 2017 to 24.0% in 2020. \nIt was also found that less than 5% of posts in a cluster \nreferenced low credibility sources, and the proportion even \ndecreased each year from 5.0% to 1.3% between 2017 and \n2020. The proportion of p osts utilizing news sources ranged \nbetween 6 and 9%.  Detailed numeric statistics  of subcategories \nare provided  in 4.2. 1 - 4.2.3. \nWe compared Facebook internal sources with the most \ndominant external source of QAnon clusters, YouTube \n(Detailed i nformation is available in  Appendix 2 .4). Consistent \nwith the aforementioned trends, the proportion of posts using \nYouTube videos in a cluster has decreased between 2017 and \n2020, in contrast to the increase in the prevalence of posts \nusing Facebook videos  over time ( Figure 3 E). \n4.2.1 Social Media Sources .  Posts using social media \nsources other than Facebook decreased every year significantly \nfrom 2017 to 2020 (Fig ure 2B). The proportion of posts using \nnon-Facebook social media sources in a cluster on average was \n40.6% ( SD = 42.3%, N =524) in 2017, 32.1% ( SD = 32.9%, N = \n1,464) in 2018, 26.7% ( SD = 31.4%, N = 1,804) in 2019, and \n24.0% ( SD = 23.1%, N = 2,714). Differenc es in median between \n2017 and 2018 (Mann -Whitney U = 359655.0, P = .015), \nbetween 2018 and 2019 ( U = 1190729.5, P < .001), and \nbetween 2019 and 2020 ( U = 2334970.0, P = .004) were \nstatistically significant. The mean differences between 2017 \nand 2018 (Welch \u2019s t(1986) = -4.17, P < .001), between 2018 and \n2019 (Welch\u2019s t(3266) = -4.82, P < .001), and between 2019 and \n2020 (Welch\u2019s t(4516) = -3.11, P = .002) were all statistically \nsignificant.  \n4.2.2 News  Sources .  The proportion of posts including \nnews sources was 6.5% ( SD = 20.8%, N = 524) in 2017, 8.3% (SD = 19.0%, N = 1,464) in 2018, 9.0% ( SD = 19.8%, N = 1,804) \nin 2019, and 8.3% ( SD = 14.1%, N = 2,714) in 2020, as shown in \nFigure 3 B. Differences between 2017 and 2018 (Mann -Whitney \nU = 320218.0, P < .001) and between 2019 and 2020 ( U = \n2142255.5, P < .001) were statistically significant, while the \ndifference between 2018 and 2019 was not significant ( U = \n1301024.5, P = .193). The mean differences were not \nstatistically significant: between 2017 and 201 8 (Welch\u2019s \nt(1986) = 1.78, P = .076), between 2018 and 2019 (Welch\u2019s \nt(3266) = 0.98, P = .328), and between 2019 and 2020 (Welch\u2019s \nt(4516) = -1.30, P = .192).  \n4.2.3 Low Credibility Sources .  QAnon clusters\u2019 reliance on \nlow credibility sources was 5.0% ( SD = 18.2%, N = 524) in 2017, \n3.8% ( SD = 12.2%, N = 1,464) in 2018, 3.4% ( SD = 12.3%, N = \n1,804) in 2019, and 1.3% ( SD = 6.1%, N = 2,714) in 2020, as \nshown in Figure 3 B. Differences between 2017 and 2018 \n(Mann -Whitney U = 346044.0, P < .001), between 2018 and \n2019 ( U = 1273279.5, P = .006), and between 2019 and 2020 ( U \n= 2356655.5, P < .001) were statistically significant. The mean \ndifference between 2019 and 2020 (Welch\u2019s t(4516) = -6.48, P \n< .001) was statistically significant, while the differences \nbetwe en 2017 and 2018 (Welch\u2019s t(1986) = -1.42, P = .156) and \nbetween 2018 and 2019 (Welch\u2019s t(3266) = -0.99, P = .324) \nwere not significant.  \n4.3  Association between Information Source \nUse and User Engagement  \nWe investigated whether the use of a certain type of \ninformation source in a post was positively or negatively \nassociated with its user engagement. We found that QAnon \nposts using Facebook internal sources produced more shares \nand comments than those not using internal sources, while \nposts with exter nal sources induced fewer shares, comments, \nand reactions than posts without external sources ( Appendix \n3.1). \n \nFigure 4 . Information sources and QAnon content \nengagement.  Incidence rate ratios (IRRs) were calculated \nusing the results of regression models fit to the number of \nshares, comments, and reactions of a post, respectively, \n\n  \n \n \n controlling for covariates. All standard errors were clustered at \nthe QAnon cluster level, and all models were estimated with \ncluster -robust standard error at the QAnon cluster l evel. The \ndots and lines represent estimated mean IRRs \u00b1 95% \nconfidence intervals. A. Associations between the inclusion of \neach information source type in a post and the number of \nshares of the post. B. Information source types and the number \nof comments.  C. Information source types and the number of \nreactions.   \nWhen the three subtypes of external sources were \nconsidered ( Figure 4 ), QAnon posts with news sources received \nmore shares ( IRR = 1.59, 95% CI [1.12, 2.27], P = .010), \ncomments ( IRR = 1.66, 95% CI [1.42, 1.95], P < .001), and \nreactions ( IRR = 1.49, 95% CI [1.24, 1.79], P < .001) compared \nwith those not including news sources. Also , QAnon posts using \nFacebook internal sources appeared to attract more shares \n(IRR = 1.62, 95% CI [1.19, 2.21], P = .002)  and comments ( IRR = \n1.84, 95% CI [1.18, 2.89], P = .008) than other QAnon posts.  \nThese results show that (1) referencing legitimate news \nsources within the conspiracy theory context and combining \ntheir news content with the misleading narrative, and (2) u sing \nFacebook internal sources in producing and supporting the \nconspiracy theory content were associated with greater \nattention and reactions from users . On the other hand, t he \ninclusion of non -Facebook social media sources was negatively \nassociated with shares ( IRR = 0.52, 95% CI [0.39, 0.69], P < \n.001), comments ( IRR = 0.50, 95% CI [0.35, 0.71], P < .001), and \nreactions ( IRR = 0.46, 95% CI [0.37, 0.58], P < .001).  (The \nnegative associations  could  be because  social media  links  \nrouted  Faceb ook users to other platforms , reducing  their \nengagements with Facebook post s as a result . Examining \npotential mechanisms  like this  one is , however,  outside  the \nscope of th is research  and also  beyon d the capabilit ies of the \ncurrent data .) See Appendix 3.2 for the detailed model \nspecification and results . \nThe significant associations between user engagement, \nand Facebook internal sources and non -Facebook social media \nsources were still identified in a different statistical model \nchosen for a robustness check ( Appendix 3 .3). Evidence \nsupporting significant associations between the inclusion of \nlow credibility sources and user engagement measures was not \nfound in this analysis.  It is  also  worth mentioning that we are \nnot making  claims on causa l relationships or causal \nmechanisms in th ese analys es. For example, an association can \nshow  that  \u201cposts including news sources received  more \nshares, \u201d but saying \u201cincluding news sources led to  the \ndifference  in shares \u201d based on the association  inevitably \naccompanies certain levels of speculation.  \n5 Discussion and Conclusion  \nDespite  significant  concerns about disinformation and \nconspiracy theories  encroaching  the world\u2019s largest social \nmedia,  the lack of systematic resea rch has prevent ed us from understanding the status and developing appropriate  \nstrategies  against  the social problem . The current study \nexamined the activities and the information ecosystem of \nonline communities that contributed to promoting the QAnon \nnarrative , one of the most influential conspiracy theor ies in the \npast  deca de. The activities of QAnon pages and groups  in the \npresent dataset  significantly surged in the year of the 2020 \nPresidential Election. By analyzing their use of information \nsources, we further discovered the following. First, QAnon \npages and groups increasingly relied on resources within \nFacebook and disconnected themselves from the ecosystem \noutside the platform. It suggests that QAnon's narrative \nproliferated within their self -sustaining Facebook environment \nwhich is separated from other platforms. Because QAnon posts \nbased on internal sources induce d higher levels of user \nengagement, this trend might also help boost the influence of \nthe QAnon narrative on Facebook. Second, the results imply \nthat combining legitimate news media sources with misguiding \nnarratives to create \u201calternative facts\u201d [40]  might effectively \nstimulate public attention and reaction to conspiracy theory \ncontent [49, 60] . Lastly, previous studies reported that the \nprevalence of information from low credibility sources is \ngenerally limited  [19, 21] , and the current study reveals that it \nstill holds even among communities on social media discussing \nconspiracy theories.  \nThe present research is not without limitations. Although \nwe could identify  a large number of public pages and groups \ndiscussing QAnon, data on accounts and content removed by \nFacebook, and information about private accounts and \nindividual profiles are not available to researchers. Especially,  \nbecause  Facebook ha d allegedly  deleted  QAnon posts, pages , \nand groups  since August 2021  [15] , the current dataset might \noverrepresent  QAnon content  and cluster s that had not been \nremoved  by Facebook's algorithms  and content moderation.  \nMore industry -academia collaboration and data sharing for \nresearch purposes would be essential in examining the role \nplayed by algorithms and platform -wide policies in the spread \nof disinformation.  Also , generalizing the findings of th is \nresearch  should be done with caution  because, despite its scale \nand sc ope, the current dataset is not a representative sample \nfrom the entire platform . Researchers  should also be careful in \nexpanding the implications of the present study to other \nmisleading narratives  and other social media services.  \nDespite the limitations, this study provides empirical \nevidence urgently needed to understand and suppress \ndisinforma tion on social media. Tackling the intra -platform \ncreation and circulation of disinformation could be imperative \nconsidering the increasing importance of these endogenous \nprocesses and the disproportional impacts that internally \nforged false narratives mig ht have on users. This study also \nsuggests that strategies focusing only on the potential intrusion \nof content from suspicious external sources may not be \nsufficient nor efficient in minimizing the impacts of \ndisinformation.  \nPropagation of the QAnon Conspiracy Theory on Facebook   \n \n SUPPLEMENTARY INFORMATION  \nAppendix 1: The Activity of QAnon Clusters  \n1.1 The Number of Active Clusters  \nActive clusters are defined as clusters that created at least one \nQAnon post in a certain time period. The number of active \nclusters in 2017 was 524, and it increased to 1,464, 1, 804, and \n2,714 in 2018, 2019, and 2020, respectively ( Figure  1A).  \nMonthly, the number of active clusters was 367 in \nNovember 2017 and steadily increased, peaking at 2,131 in \nAugust 2020, and then decreased afterward. The number of \nactive clusters was 1,029 in November 2020. This pattern \nmight reflect the fact that Facebo ok started cracking down \naccounts spreading QAnon and their posts in August 2020 [15] . \n1.2  The Number of Users who Received \nQAnon Posts Directly  \nThe number of users who received QAnon posts directly was \ncalculated by retrieving the number of followers of a cluster \nthat created each post and summing the numbers of followers \nacross all posts in a given month. The sum can be viewed as the \nappr oximated total number of users who received QAnon posts \non their social media timeline in a given month. (Users who \nwere subscribing to two or more QAnon pages or groups need \nto be considered for more accurate estimation, but information \nabout these users is not available to researchers.) The result \nshowed that the number of users directly exposed to QAnon \nposts was 85 million in November 2017, 365 million in August \n2020, and 76 million in November 2020. The total number of \nusers receiving QAnon posts in th e entire time period was 3.1 \nbillion.  \n1.3  The Number of Posts in a Year \nFor each year, we counted the number of posts generated by \neach active cluster ( Figure  1B). On average, an active cluster \ngenerated 4.03 posts in 2017 ( SD = 8.46, N = 524), 14.05 posts \nin 2018 ( SD = 30.57, N = 1,464), 13.10 posts in 2019 ( SD = \n61.49, N = 1,804), and 26.25 posts in 2020 ( SD = 50.30, N = \n2,714).  \n1.4 The Number of Active Clust ers \nActive clusters are defined as clusters that created at least one \nQAnon post in a certain time period. The number of active \nclusters in 2017 was 524, and it increased to 1,464, 1,804, and \n2,714 in 2018, 2019, and 2020, respectively ( Figure  1A).  \nMonthl y, the number of active clusters was 367 in \nNovember 2017 and steadily increased, peaking at 2,131 in \nAugust 2020, and then decreased afterward. The number of \nactive clusters was 1,029 in November 2020. This pattern \nmight reflect the fact that Facebook sta rted cracking down \naccounts spreading QAnon and their posts in August 2020 [15] . 1.5  Weekly Posts by Cohort  \nA cluster cohort was defined as a group of QAnon clusters that \ncreated their first post in the same year. All clusters were \nclassified into four different cohorts: 2017, 2018, 2019, and \n2020 cohorts. The average weekly number of posts was \ncalculated by div ing the number of posts from a cluster by the \nnumber of hours between its first and last post, and multiplying \nthe result by 168 hours (24 hours \u00d7 7 days). This metric was \ncalculated only for clusters that generated at least two QAnon \nposts. The average we ekly number of posts was 0.45 for the \n2017 cohort ( SD = 0.58, N = 524), 0.35 posts for the 2018 cohort \n(SD = 0.95, N = 965), 0.85 posts for the 2019 cohort ( SD = 2.30, \nN = 549), and 7.13 posts for the 2020 cohort ( SD = 151.63, N = \n767). See Figure  1D. \n1.6  Early Activity by Cohort  \nThe average number posts created by a cluster within the first \nmonth after the creation of its first QAnon post was 3.23 in the \n2017 cohort ( SD = 6.15, N = 524), 2.48 in the 2018 cohort ( SD = \n6.24, N = 968), 4.15 in the 2019 cohort ( SD = 19.18, N = 552), \nand 8.81 in the 2020 cohort ( SD = 20.88, N = 769). The results \nare shown in Figure  1E. \nAppendix 2: Information Sources for the QAnon \nNarrative  \n2.1  Descriptive Statistics  \nIn total, 91.4% of the posts included at least one URL, while only \n8.6% did not include any URL. When aggregated by clusters, \n90.9% ( SD = 11.6%, N = 2,813) of posts in a QAnon cluster \nincluded one or more URLs. 75.8% of posts in a cluster ( SD = \n18.6%, N = 2,813) used at least one of the four types of \ninfor mation sources: Facebook internal sources, social media \nsources, news sources, and low credibility sources.  \nAn average cluster had 47.8% ( SD = 25.5%, N = 2,813) of \nposts using Facebook internal sources. The average \nproportions of posts in a cluster using n on-Facebook social \nmedia sources, news sources, and low credibility sources were \n25.8% ( SD = 21.7%, N = 2,813), 8.1% ( SD = 12.1%, N = 2,813), \nand 2.1% ( SD = 6.0%, N = 2,813), respectively.  \nThe 20 most frequently used domains of the URLs were the \nfollowing:  facebook.com (31.2%), youtube.com (20.6%), \ntwitter.com (6.3%), google.com (1.4%), amazon.com (1.3%), \npatreon.com (1.1%), gab.ai (0.7%), nativate3d.no (0.7%), \npaypal.me (0.6%), blogspot.com (0.6%), \nthemillennialbridge.com (0.5%), foxnews.com (0.4%), \nbitchu te.com (0.4%), wikipedia.org (0.4%), google.no (0.4%), \nwordpress.com (0.3%), nbcnews.com (0.3%), instagram.com \n(0.3%), stillnessinthestorm.com (0.3%), and \nconspiracydailyupdate.com (0.3%).  \n2.2  Facebook Internal Sources  \n  \n \n \n The proportion of posts using Facebook internal sources \nsignificantly increased every year between 2017 and 2020. As \nshown in Figure 3 A, the proportion of posts using Facebook \ninternal sources in a cluster on average was 30.0% ( SD = 40.1%, \nN = 524) in 2017 , 37.2% ( SD = 34.6%, N = 1,464) in 2018, 44.7% \n(SD = 36.3%, N = 1,804) in 2019, and 50.0% ( SD = 27.5%, N = \n2,714). Differences in median between 2017 and 2018 (Mann -\nWhitney U = 310445.0, P < .001), between 2018 and 2019 ( U = \n1169646.5, P < .001), and between 2019 and 2020 ( U = \n2176166.5, P < .001) were statistically significant. The mean \ndifferences between 2017 and 2018 (Welch\u2019s t(1986) = 3.67, P \n< .001), between 2018 and 2019 (Welch\u2019s t(3266) = 5.96, P < \n.001), and between 2019 and 2020  (Welch\u2019s t(4516) = 5.33, P < \n.001) were also statistically significant.  \n2.3  External  Sources  \nPosts using external sources significantly decreased every year \nfrom 2017 to 2020 ( Figure 3 A). The proportion of posts using \nthese sources in a cluster on averag e was 74.4% ( SD = 37.7%, \nN = 524) in 2017, 65.8% ( SD = 33.5%, N = 1,464) in 2018, 59.1% \n(SD = 35.2%, N = 1,804) in 2019, and 52.7% ( SD = 27.7%, N = \n2,714) in 2020. Differences in median between 2017 and 2018 \n(Mann -Whitney U = 295141.5, P < .001), between 2018 and \n2019 ( U = 1181651.0, P < .001), and between 2019 and 2020 ( U \n= 2102013.5, P < .001) were statistically significant. The mean \ndifferences between 2017 and 2018 (Welch\u2019s t(1986) = -4.61, P \n< .001), between 2018 and 2019 (Welch\u2019s t(3266) = -5.50, P < \n.001), and between 2019 and 2020 (Welch\u2019s t(4516) = -6.50, P \n< .001) were all statistically significant.  \n2.4  Comparison Between Facebook Videos and \nYouTube Videos  \nThe two most frequently used sources for QAnon clusters were \nFacebook and YouTube, as reported in 4.1.1, and these two \nsources accounted for 51.9% of all URLs. We compared these \ntwo sources as an important case analysis showing the \ndiffe rence between internal information sources within \nFacebook and the most dominant external source, YouTube.  \nFor this purpose, we examined the proportion of posts based \non Facebook videos and YouTube videos. Facebook categorizes \neach post into one of the fol lowing types: photo, link, \nnative_video, youtube, video, status, live_video_complete, \nlive_video, live_video_scheduled, and album. We called posts in \nthe native_video, video, live_video_complete, live_video, or \nlive_video_scheduled categories as \u201cposts usi ng Facebook \nvideos.\u201d On the other hand, \u201cposts using YouTube videos\u201d \nreferred to posts in the \u201cyoutube\u201d category.  \nAs depicted in Figure 3E, the proportion of posts using \nYouTube videos in a cluster decreased from 2017 to 2020: \n21.8% in 2017 ( SD = 35.1%, N = 524), 16.7% in 2018 ( SD = \n26.3%, N = 1,464), 11.0% in 2019 ( SD = 22.3%, N = 1,804), and \n10.1% in 2020 ( SD = 15.4%, N = 2,714). Differences between \n2017 and 2018 (Mann -Whitney U = 338553.5, P < .001) and \nbetween 2019 and 2020 ( U = 1943618.0, P < .001) were statistically significant, while the difference between 2018 and \n2019 was not significant ( U = 1309299.5, P = .331).  \nContrarily, the proportion of posts using Facebook videos \nin a cluster de clined. It was 21.8% in 2017 ( SD = 35.2%, N = \n524), 22.0% in 2018 ( SD = 29.2%, N = 1,464), 22.6% in 2019 \n(SD = 29.2%, N = 1,804), and 27.0% in 2020 ( SD = 22.9%, N = \n2,714). Differences between 2018 and 2019 (Mann -Whitney U \n= 1146920.0, P < .001) and betwee n 2019 and 2020 ( U = \n2133890.5, P < .001) were statistically significant, while the \ndifference between 2017 and 2018 was not ( U = 369576.5, P = \n.087).  \nAppendix 3: Association between Information \nSources and User Engagement  \nTo estimate the association betwe en information source types \non user engagement, we fit negative binomial regression \nmodels with maximum likelihood estimation (MLE) to consider \noverdispersion [5, 23] . For one of the most prominent \nexamples, Brady et al. [5] used negative binomial regression \nmodels with MLE to analyze how user engagement on Twitter \nwas influenced by the inclusion of moral -emotional language in \nsocial media messages. We also fit Poisson models with MLE to \ncheck the robustness o f the findings. The dependent variables \nwere the number of shares, the number of comments, and the \nnumber of reactions to a post. The number of reactions was \ndefined as the sum of 8 different reaction counters of a post: \nlike, love, wow, haha, sad, angry, thankful, and care.  \nIn this section, we present the specifications and results of two \nanalyses. First, among posts with at least one information \nsource, we examined the association between user \nengagement, and the inclusion of Facebook internal sources \nand the inclusion of external sources in a post, respectively. \nSecond, among posts with at least one information source, we \nestimated the association between the subtypes of information \nsources (Facebook internal, social media, news, and low \ncredibility sourc es) and user engagement and additional \nresults for a robustness check. All standard errors were \nclustered at the QAnon cluster level, and all models were \nestimated with cluster -robust standard error at the QAnon \ncluster level. Incidence rate ratios ( IRRs) were calculated by \nexponentiating the coefficients of each regression model. SciPy \n(version 1.6.1), a package based on the Python programming \nlanguage, was used for all analyses. The data and the script for \nregression analyses are available online [41] . \n3.1  Asso ciation Between Internal/ External \nSources and User Engagement  \nAmong posts with at least one information source ( N = \n107,392), we examined the associations between the existence \nof Facebook internal sources and user engagement and \nbetween the existence of external sources and user \nengagement. The first model predicting the number of shares \n(52% of posts were shared mo re than once) included two \nPropagation of the QAnon Conspiracy Theory on Facebook   \n \n binary predictors indicating the inclusion of Facebook internal \nsources in a post and the inclusion of external sources, and five \ncovariates: (1) cluster size (a counting variable representing \nthe number of Facebook users followi ng the cluster), (2) \nposting month (a continuous variable ranging between 0 and \n106), (3) the existence of videos in a post (a binary variable \nindicating whether or not a post includes at least one video), \n(4) the existence of photos in a post (a binary va riable \nindicating whether or not a post includes at least one photo), \n(5) cluster type (a categorical variable indicating the type of a \ngiven cluster: a page or a group). The second model predicting \nthe number of comments in a post (36% of posts had more t han \none comment) and the third model predicting the number of \nreactions in a post (70% of posts had more than one reaction) \nhad the same two predictors and five covariates as the first \nmodel.  \nThe regression results showed that QAnon posts using \nFacebook in ternal sources attracted more shares ( IRR = 1.68, \n95% CI [1.20, 2.35], P = .002) and comments ( IRR = 1.86, 95% \nCI [1.09, 3.19], P = .024). Statistical significance was not \nidentified for the association between the internal sources and \nreactions ( IRR = 1.2 4, 95% CI [0.87, 1.75], P = .231). QAnon \nposts using external sources, on the other hand, attracted fewer \nshares ( IRR = 0.61, 95% CI [0.46, 0.82], P = .001), comments \n(IRR = 0.59, 95% CI [0.38, 0.91], P = .017), and reactions ( IRR = \n0.50, 95% CI [0.38, 0.6 5], P < .001) than those not using \nexternal sources.  \n3.2  Association Between Source Subtypes \nand User Engagement  \nAmong posts with at least one information source ( N = \n107,392), we examined the association between information \nsource types and user engageme nt. The first model predicting \nthe number of shares included four binary predictors \nindicating the inclusion of different information source types in \na post (Facebook internal source, social media source, news \nsource, and low credibility content source) an d the same five \ncovariates explained in 2.1. The second model predicting the \nnumber of comments in a post and the third model predicting \nthe number of reactions in a post had the same predictors and \ncovariates as the first model.  The regression results are \nreported in Table 3 and visualized in Figure 4 . Results of \nrobustness checks are available in [41]  \n \nTable 3. The Association Between the Use of Information \nSource s and User Engagement  \nIndependent \nvariables  Dependent variable  \nShare  Comment  Reaction  \nFB internal   0.482  **  0.611 **   0.280  \nNews   0.465  *  0.509 ***   0.400 ***  \nSocial media  -0.653  *** -0.695 ***  -0.772 ***  \nLow credibility   0.271  -0.112  -0.049  \nMonth   0.038  ***  0.033 ***   0.037 ***  \nFB group (ref: \npage)  -1.498  *** -0.676 **  -1.118 ***  Subscriber   5.61e -6 ***  7.14e -6 ***   8.98e -6 ***  \nPhoto  -0.006  -0.112   0.378  \nVideo  -0.068  -0.058  -0.048  \nNote . Results of negative binomial models estimating the number of \nshares, comments, and reactions of a post as a function of variables \nshown in the leftmost column. The values are regression \ncoefficients. All standard errors were clustered at the QAnon cluster  \nlevel, and all models were estimated with cluster -robust standard \nerror at the QAnon cluster  level. N = 107,392. * P < .05, ** P < .01, *** P \n< .001  \nAppendix 4: Subgroups of External Sources  \nURLs whose domains were included in the social media domain \ngroup were categorized as \u201csocial media sources.\u201d The social \nmedia domain group consists of the domains of social media \nservices that were included in Pew Research Center\u2019s reports \non social media  from 2012 to 2020, except Facebook [10\u201312, \n18, 24, 37, 46 \u201348]. This group consisted of the domains of \nInstagram, Reddit, Twitter, YouTube, TikTok, Tumblr, Google+, \nTwitch, Vine, WhatsApp, and Sn apchat. (For Google+, URLs had \nto match with plus.google.com at the subdomain level. For \nother social media services, URLs were matched at the top -level \ndomain, e.g., twitter.com and instagram.com.)  \nURLs whose domains were included in the news domain \ngroup  were categorized as \u201cnews sources.\u201d The news domain \ngroup includes domains that were included in the following \nlists: \u201cHard news domains\u201d on Bakshy et al. [3], \u201cNews media \nsites\u201d  on Yang et al. [59] , \u201cGreen\u201d and \u201cYellow\u201d domains on \nGrinberg et al. [19] , and \u201cNewspapers\u201d and \u201cDigital -native news \noutlets\u201d labeled by Pew Research Center [39] . Domains in the \nlow credibility domain group were excluded from the news \ndomain group. This group consists of 529 domains.  \nURLs whose domains were included in the low credibility \ndomain group were categorized as \u201clow cr edibility sources.\u201d \nThe low credibility domain group included domains \ncategorized as \u201cBlack,\u201d \u201cRed,\u201d or \u201cOrange\u201d sources in Grinberg \net al. [19], which extended previous lists of fake news sources \ncurated by Guess et al. [22] , Allcott and Gentzkow [2], \nSnopes.com, and Buzzfeed; and domains identified as \u201cvery low \ncredibility\u201d and \u201clow credibility\u201d sites by Media Bias and Fact \nCheck (mediabiasfactcheck.com) . The number of domains in \nthis group was 889.  \nREFERENCES  \n[1] Aliapoulios, M. et al. 2021. The Gospel According to Q: Understanding the \nQAnon Conspiracy from the Perspective of Canonical Information. \narXiv:2101.08750 [cs] . (May 2021).  \n[2] Allcott, H. and Gentzkow, M. 2017. Social Media and Fake News in the 2016 \nElect ion. Journal of Economic Perspectives . 31, 2 (May 2017), 211 \u2013236. \nDOI:https://doi.org/10.1257/jep.31.2.211.  \n[3] Bakshy, E. et al. 2015. Exposure to ideologically diverse news and opinion \non Facebook. Science . 348, 6239 (Jun. 2015), 1130 \u20131132. \nDOI:https://d oi.org/10.1126/science.aaa1160.  \n[4] Bessi, A. et al. 2015. Science vs Conspiracy: Collective Narratives in the Age \nof Misinformation. PLOS ONE . 10, 2 (Feb. 2015), e0118093. \nDOI:https://doi.org/10.1371/journal.pone.0118093.  \n[5] Brady, W.J. et al. 2017. Emot ion shapes the diffusion of moralized content \nin social networks. Proceedings of the National Academy of Sciences . 114, 28 \n(Jul. 2017), 7313 \u20137318. DOI:https://doi.org/10.1073/pnas.1618923114.  \n  \n \n \n [6] Broniatowski, D.A. et al. 2020. Facebook Pages, the \u201cDisneyl and\u201d Measles \nOutbreak, and Promotion of Vaccine Refusal as a Civil Right, 2009 \u20132019. \nAmerican Journal of Public Health . 110, S3 (2020), S312 \u2013S318.  \n[7] Buchanan, T. 2020. Why do people spread false information online? The \neffects of message and viewer chara cteristics on self -reported likelihood of \nsharing social media disinformation. PLOS ONE . 15, 10 (Oct. 2020), \ne0239666. DOI:https://doi.org/10.1371/journal.pone.0239666.  \n[8] Citing CrowdTangle Data: 2021. \nhttp://help.crowdtangle.com/en/articles/3192685 -citing-crowdtangle -\ndata . Accessed: 2021 -05-09. \n[9] Del Vicario, M. et al. 2016. The spreading of misinformation online. \nProceedings of the National Academy of Sciences . 113, 3 (Jan. 2016), 554 \u2013\n559. DOI:https://doi.org/10.1073/pnas.1517441113.  \n[10]  Duggan, M. 2 015. Mobile Messaging and Social Media 2015. Pew Research \nCenter: Internet, Science & Tech . \n[11]  Duggan, M. et al. 2015. Social Media Site Usage 2014. Pew Research Center: \nInternet, Science & Tech . \n[12]  Duggan, M. and Brenner, J. 2013. The Demographics of Social Media Users \n\u2014 2012. Pew Research Center: Internet, Science & Tech . \n[13]  Facebook banned big anti -vaccination pages. Researchers warn it\u2019s too \nlittle, too late.: 2020. https://www.nbcnews.com/tech/tech -news/covid -19-\nvaccines -face -varied -powerful -misi nformation -movement -online -n1249378 . \nAccessed: 2021 -04-23. \n[14]  Ferrara, E. 2017. Disinformation and Social Bot Operations in the Run Up \nto the 2017 French Presidential Election. First Monday . (2017), 33.  \n[15]  Frenkel, S. 2020. QAnon is still spreading on Facebook, despite a ban. The \nNew York Times . \n[16]  Friggeri, A. et al. 2014. Rumor Cascades. ICWSM  (2014).  \n[17]  Geeng, C. et al. 2020. Fake News on Facebook and Twitter: Investigating \nHow People (Don\u2019t) Investigate. Proceedings of the 2020 CHI Conference on  \nHuman Factors in Computing Systems  (New York, NY, USA, Apr. 2020), 1 \u2013\n14. \n[18]  Greenwood, S. et al. 2016. Demographics of Social Media Users in 2016. Pew \nResearch Center: Internet, Science & Tech . \n[19]  Grinberg, N. et al. 2019. Fake news on Twitter during the 2016 U.S. \npresidential election. Science . 363, 6425 (Jan. 2019), 374 \u2013378. \nDOI:https://doi.org/10.1126/science.aau2706.  \n[20]  Guess, A. et al. 2020. Exposure to untrustworthy websites in the 2016 US \nelection. Nature Human Behaviour . 4, 5 (May 2020), 472 \u2013480. \nDOI:https://doi.org/10.1038/s41562 -020-0833 -x. \n[21]  Guess, A. et al. 2019. Less than you think: Prevalence and predictors of fake \nnews dissemination on Facebook. Science Advances . 5, 1 (Jan. 2019), \neaau4586. DOI:https://doi.org/10.1126/sciadv.aau4586.  \n[22]  Guess, A. et al. 2018. Selective Exposure to Misinformation: Evidence from \nthe consumption of fake news during the 2016 U.S. presidential campaign. \n(2018).  \n[23]  Hilbe, J.M. 2011. Negative Binomial Regression . Cambridge University Press.  \n[24]  Holcomb,  J. et al. 2013. News Use Across Social Media Platforms. Pew \nResearch Center\u2019s Journalism Project . \n[25]  Hussein, E. et al. 2020. Measuring Misinformation in Video Search \nPlatforms: An Audit Study on YouTube. Proceedings of the ACM on Human -\nComputer Interac tion . 4, CSCW1 (May 2020), 1 \u201327. \nDOI:https://doi.org/10.1145/3392854.  \n[26]  Isaac, M. 2021. Facebook will remove \u2018Stop the Steal\u2019 misinformation. The \nNew York Times . \n[27]  Jankowicz, N. and Otis, C. 2020. Facebook Groups Are Destroying America. \nWired . \n[28]  Johnson, N.F. et al. 2019. Hidden resilience and adaptive dynamics of the \nglobal online hate ecology. Nature . (Aug. 2019), 1 \u20135. \nDOI:https://doi.org/10.1038/s41586 -019-1494 -7. \n[29]  Johnson, N.F. et al. 2020. The online competition between pro - and anti -\nvaccination views. Nature . (May 2020). \nDOI:https://doi.org/10.1038/s41586 -020-2281 -1. \n[30]  Kou, Y. et al. 2017. Conspiracy Talk on Social Media: Collective Sensemaking \nduring a Public Health Crisis. Proceedings of the ACM on Human -Computer \nInteraction . 1, CSCW (Dec. 2017), 1 \u201321. \nDOI:https://doi.org/10.1145/3134696.  \n[31]  Lazer, D.M.J. et al. 2018. The science of fake news. Science . 359, 6380 (Mar. \n2018), 1094 \u20131096. DOI:https://d oi.org/10.1126/science.aao2998.  \n[32]  van der Linden, S. 2015. The conspiracy -effect: Exposure to conspiracy \ntheories (about global warming) decreases pro -social behavior and science \nacceptance. Personality and Individual Differences . 87, (Dec. 2015), 171 \u2013\n173. DOI:https://doi.org/10.1016/j.paid.2015.07.045.  \n[33]  Mocanu, D. et al. 2015. Collective attention in the age of (mis)information. \nComputers in Human Behavior . 51, (Oct. 2015), 1198 \u20131204. \nDOI:https://doi.org/10.1016/j.chb.2015.01.024.  [34]  Munn, L. 2021 . More than a mob: Parler as preparatory media for the U.S. \nCapitol storming. First Monday . (Feb. 2021). \nDOI:https://doi.org/10.5210/fm.v26i3.11574.  \n[35]  Papasavva, A. et al. 2021. \u201cIs it a Qoincidence?\u201d: An Exploratory Study of \nQAnon on Voat. Proceedings of the Web Conference 2021  (Ljubljana \nSlovenia, Apr. 2021), 460 \u2013471.  \n[36]  Pennycook, G. and Rand, D.G. 2019. Fighting misinformation on social \nmedia using crowdsourced judgments of news source quality. Proceedings \nof the National Academy of Sciences . 116, 7 (Feb. 2019), 2521 \u20132526. \nDOI:https://doi.org/10.1073/pnas.1806781116.  \n[37]  Perrin, A. and Anderson, M. Share of U.S. adults using social media, \nincluding Facebook, is mostly unchanged since 2018. Pew Research Center . \n[38]  Persily, N. and Tucker, J.A. eds.  2020. Social Media and Democracy: The State \nof the Field, Prospects for Reform . Cambridge University Press.  \n[39]  Pew Research Center 2019. Methodology: State of the News Media. Pew \nResearch Center\u2019s Journalism Project . \n[40]  QAnon shows that the age of alt ernative facts will not end with Trump: \n2020. https://www.cjr.org/opinion/qanon -trump -alternative -facts.php . \nAccessed: 2021 -05-31. \n[41]  Replication data for \u201cThe Information Ecosystem of Conspiracy Theory: \nExamining the QAnon Narrative on Facebook\u201d: 2021. \nhttps://osf.io/ba6et/?view_only=19124f4918c04acfb5c3607765e1f038 . \n[42]  Roose, K. 2020. QAnon Followers Are Hijacking the #SaveTheChildren \nMovement. The New York Times . \n[43]  Roose, K. 2021. What Is QAnon, the Viral Pro -Trump Conspiracy Theory? \nThe New York Times . \n[44]  Scheufele, D.A. and Krause, N.M. 2019. Science audiences, misinformation, \nand fake news. Proceedings of the National Academy of Sciences . 116, 16 \n(Apr. 2019), 76 62\u20137669. DOI:https://doi.org/10.1073/pnas.1805871115.  \n[45]  Schmidt, A.L. et al. 2018. Polarization of the vaccination debate on \nFacebook. Vaccine . 36, 25 (Jun. 2018), 3606 \u20133612. \nDOI:https://doi.org/10.1016/j.vaccine.2018.05.040.  \n[46]  Shearer, E. and Gottfr ied, J. 2017. News Use Across Social Media Platforms \n2017. Pew Research Center\u2019s Journalism Project . \n[47]  Shearer, E. and Mitchell, A. 2021. News Use Across Social Media Platforms \nin 2020. Pew Research Center\u2019s Journalism Project . \n[48]  Smith, A. and Anders on, M. 2018. Social Media Use 2018: Demographics and \nStatistics. Pew Research Center: Internet, Science & Tech . \n[49]  Starbird, K. et al. 2019. Disinformation as Collaborative Work: Surfacing the \nParticipatory Nature of Strategic Information Operations. Proceedings of \nthe ACM on Human -Computer Interaction . 3, CSCW (Nov. 2019), 1 \u201326. \nDOI:https://doi.org/10.1145/3359229.  \n[50]  Starbird, K. 2019. Disinformation\u2019s spread: bots, trolls and all of us. Nature . \n571, 7766 (Jul. 2019), 449 \u2013449. DOI:https://doi.org/10.1 038/d41586 -\n019-02235 -x. \n[51]  Starbird, K. 2017. Examining the Alternative Media Ecosystem Through the \nProduction of Alternative Narratives of Mass Shooting Events on Twitter. \nICWSM  (2017).  \n[52]  Tollefson, J. 2021. Tracking QAnon: how Trump turned conspirac y-theory \nresearch upside down. Nature . 590, 7845 (Feb. 2021), 192 \u2013193. \nDOI:https://doi.org/10.1038/d41586 -021 -00257 -y. \n[53]  Torres -Lugo, C. et al. 2020. The Manufacture of Political Echo Chambers by \nFollow Train Abuse on Twitter. arXiv:2010.13691 [cs] . (Oct. 2020).  \n[54]  Vosoughi, S. et al. 2018. The spread of true and false news online. Science . \n359, 6380 (Mar. 2018), 1146 \u20131151 . \nDOI:https://doi.org/10.1126/science.aap9559.  \n[55]  Watts, D.J. et al. 2021. Measuring the news and its impact on democracy. \nProceedings of the National Academy of Sciences . 118, 15 (Apr. 2021). \nDOI:https://doi.org/10.1073/pnas.1912443118.  \n[56]  What data i s CrowdTangle tracking? 2021. \nhttp://help.crowdtangle.com/en/articles/1140930 -what -data -is-\ncrowdtangle -tracking . Accessed: 2021 -05-09. \n[57]  Wilson, T. and Starbird, K. 2020. Cross -platform disinformation campaigns: \nLessons learned and next steps. Harvard K ennedy School Misinformation \nReview . 1, 1 (Jan. 2020). DOI:https://doi.org/10.37016/mr -2020 -002.  \n[58]  Yang, K. -C. et al. 2021. The COVID -19 Infodemic: Twitter versus Facebook. \nBig Data & Society . 8, 1 (Jan. 2021), 20539517211013860. \nDOI:https://doi.org/10. 1177/20539517211013861.  \n[59]  Yang, T. et al. 2020. Exposure to news grows less fragmented with an \nincrease in mobile access. Proceedings of the National Academy of Sciences . \n117, 46 (Nov. 2020), 28678 \u201328683. \nDOI:https://doi.org/10.1073/pnas.2006089117.  \n[60] Zuckerman, E. 2019. QAnon and the Emergence of the Unreal. Journal of \nDesign and Science . 6 (Jul. 2019). \nDOI:https://doi.org/10.21428/7808da6b.6b8a82b9.  \n ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Propagation of the QAnon conspiracy theory on Facebook", "author": ["S Kim", "J Kim"], "pub_year": "2021", "venue": "OSF Preprint. https://doi. org/10.31219/osf. io/wku5b", "abstract": "There has been concern about the proliferation of the \u201cQAnon\u201d conspiracy theory on Facebook,  but little is known about how its misleading narrative propagated on the world\u2019s largest"}, "filled": false, "gsrank": 481, "pub_url": "https://files.osf.io/v1/resources/wku5b/providers/osfstorage/60da5a7731881a012263a4e7?action=download&direct&version=3", "author_id": ["5rEBYxUAAAAJ", ""], "url_scholarbib": "/scholar?hl=en&q=info:X6XADoFp0AUJ:scholar.google.com/&output=cite&scirp=480&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D480%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=X6XADoFp0AUJ&ei=XbWsaM-eLcDZieoPqdqh8QU&json=", "num_citations": 6, "citedby_url": "/scholar?cites=418950768364660063&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:X6XADoFp0AUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://files.osf.io/v1/resources/wku5b/providers/osfstorage/60da5a7731881a012263a4e7?action=download&direct&version=3"}}]