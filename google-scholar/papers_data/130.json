[{"title": "Findings of the NLP4IF-2019 shared task on fine-grained propaganda detection", "year": "2019", "pdf_data": "Findings of the NLP4IF-2019 Shared Task\non Fine-Grained Propaganda Detection\nGiovanni Da San Martino1Alberto Barr \u00b4on-Cede \u02dcno2Preslav Nakov1\n1Qatar Computing Research Institute, HBKU, Qatar\n2Universit `a di Bologna, Forl `\u0131, Italy\nfgmartino, pnakov g@hbku.edu.qa a.barron@unibo.it\nAbstract\nWe present the shared task on Fine-Grained\nPropaganda Detection, which was organized\nas part of the NLP4IF workshop at EMNLP-\nIJCNLP 2019. There were two subtasks. FLC\nis a fragment-level task that asks for the iden-\nti\ufb01cation of propagandist text fragments in a\nnews article and also for the prediction of the\nspeci\ufb01c propaganda technique used in each\nsuch fragment (18-way classi\ufb01cation task).\nSLC is a sentence-level binary classi\ufb01cation\ntask asking to detect the sentences that con-\ntain propaganda. A total of 12 teams submit-\nted systems for the FLC task, 25 teams did\nso for the SLC task, and 14 teams eventu-\nally submitted a system description paper. For\nboth subtasks, most systems managed to beat\nthe baseline by a sizable margin. The leader-\nboard and the data from the competition are\navailable at http://propaganda.qcri.\norg/nlp4if-shared-task/ .\n1 Introduction\nPropaganda aims at in\ufb02uencing people\u2019s mindset\nwith the purpose of advancing a speci\ufb01c agenda.\nIn the Internet era, thanks to the mechanism\nof sharing in social networks, propaganda cam-\npaigns have the potential of reaching very large\naudiences (Glowacki et al., 2018; Muller, 2018;\nTard\u00b4aguila et al., 2018).\nPropagandist news articles use speci\ufb01c\ntechniques to convey their message, such as\nwhataboutism ,red Herring , and name calling ,\namong many others (cf. Section 3). Whereas\nproving intent is not easy, we can analyse the\nlanguage of a claim/article and look for the use\nof speci\ufb01c propaganda techniques. Going at this\n\ufb01ne-grained level can yield more reliable systems\nand it also makes it possible to explain to the user\nwhy an article was judged as propagandist by an\nautomatic system.With this in mind, we organised the shared\ntask on \ufb01ne-grained propaganda detection at the\nNLP4IF@EMNLP-IJCNLP 2019 workshop. The\ntask is based on a corpus of news articles anno-\ntated with an inventory of 18 propagandist tech-\nniques at the fragment level. We hope that the\ncorpus would raise interest outside of the commu-\nnity of researchers studying propaganda. For ex-\nample, the techniques related to fallacies and the\nones relying on emotions might provide a novel\nsetting for researchers interested in Argumentation\nand Sentiment Analysis.\n2 Related Work\nPropaganda has been tackled mostly at the arti-\ncle level. Rashkin et al. (2017) created a corpus\nof news articles labelled as propaganda, trusted,\nhoax, or satire. Barr \u00b4on-Cede \u02dcno et al. (2019) ex-\nperimented with a binarized version of that cor-\npus: propaganda vs. the other three categories.\nBarr\u00b4on-Cedeno et al. (2019) annotated a large bi-\nnary corpus of propagandist vs. non-propagandist\narticles and proposed a feature-based system for\ndiscriminating between them. In all these cases,\nthe labels were obtained using distant supervision,\nassuming that all articles from a given news out-\nlet share the label of that outlet, which inevitably\nintroduces noise (Horne et al., 2018).\nA related \ufb01eld is that of computational argu-\nmentation which, among others, deals with some\nlogical fallacies related to propaganda. Habernal\net al. (2018b) presented a corpus of Web forum\ndiscussions with instances of ad hominem fallacy.\nHabernal et al. (2017, 2018a) introduced Argo-\ntario , a game to educate people to recognize and\ncreate fallacies, a by-product of which is a corpus\nwith 1:3karguments annotated with \ufb01ve fallacies\nsuch as ad hominem ,red herring andirrelevant\nauthority , which directly relate to propaganda.arXiv:1910.09982v1  [cs.CL]  20 Oct 2019\nUnlike (Habernal et al., 2017, 2018a,b), our cor-\npus uses 18 techniques annotated on the same set\nof news articles. Moreover, our annotations aim\nat identifying the minimal fragments related to a\ntechnique instead of \ufb02agging entire arguments.\nThe most relevant related work is our own,\nwhich is published in parallel to this paper at\nEMNLP-IJCNLP 2019 (Da San Martino et al.,\n2019) and describes a corpus that is a subset of\nthe one used for this shared task.\n3 Propaganda Techniques\nPropaganda uses psychological and rhetorical\ntechniques to achieve its objective. Such tech-\nniques include the use of logical fallacies and ap-\npeal to emotions. For the shared task, we use 18\ntechniques that can be found in news articles and\ncan be judged intrinsically, without the need to\nretrieve supporting information from external re-\nsources. We refer the reader to (Da San Martino\net al., 2019) for more details on the propaganda\ntechniques; below we report the list of techniques:\n1. Loaded language. Using words/phrases with\nstrong emotional implications (positive or nega-\ntive) to in\ufb02uence an audience (Weston, 2018, p. 6).\n2. Name calling or labeling. Labeling the ob-\nject of the propaganda as something the target au-\ndience fears, hates, \ufb01nds undesirable or otherwise\nloves or praises (Miller, 1939).\n3. Repetition. Repeating the same message over\nand over again, so that the audience will eventually\naccept it (Torok, 2015; Miller, 1939).\n4. Exaggeration or minimization. Either rep-\nresenting something in an excessive manner: mak-\ning things larger, better, worse, or making some-\nthing seem less important or smaller than it ac-\ntually is (Jowett and O\u2019Donnell, 2012, p. 303),\ne.g., saying that an insult was just a joke.\n5. Doubt. Questioning the credibility of some-\none or something.\n6. Appeal to fear/prejudice. Seeking to build\nsupport for an idea by instilling anxiety and/or\npanic in the population towards an alternative,\npossibly based on preconceived judgments.\n7. Flag-waving. Playing on strong national feel-\ning (or with respect to a group, e.g., race, gender,\npolitical preference) to justify or promote an ac-\ntion or idea (Hobbs and Mcgee, 2008).8. Causal oversimpli\ufb01cation. Assuming one\ncause when there are multiple causes behind an\nissue. We include scapegoating as well: the trans-\nfer of the blame to one person or group of people\nwithout investigating the complexities of an issue.\n9. Slogans. A brief and striking phrase that may\ninclude labeling and stereotyping. Slogans tend to\nact as emotional appeals (Dan, 2015).\n10. Appeal to authority. Stating that a claim\nis true simply because a valid authority/expert on\nthe issue supports it, without any other supporting\nevidence (Goodwin, 2011). We include the special\ncase where the reference is not an authority/expert,\nalthough it is referred to as testimonial in the liter-\nature (Jowett and O\u2019Donnell, 2012, p. 237).\n11. Black-and-white fallacy, dictatorship.\nPresenting two alternative options as the only pos-\nsibilities, when in fact more possibilities exist\n(Torok, 2015). As an extreme case, telling the\naudience exactly what actions to take, eliminating\nany other possible choice ( dictatorship ).\n12. Thought-terminating clich \u00b4e.Words or\nphrases that discourage critical thought and mean-\ningful discussion about a given topic. They are\ntypically short and generic sentences that offer\nseemingly simple answers to complex questions\nor that distract attention away from other lines of\nthought (Hunter, 2015, p. 78).\n13. Whataboutism. Discredit an opponent\u2019s\nposition by charging them with hypocrisy without\ndirectly disproving their argument (Richter, 2017).\n14. Reductio ad Hitlerum. Persuading an au-\ndience to disapprove an action or idea by suggest-\ning that the idea is popular with groups hated in\ncontempt by the target audience. It can refer to\nany person or concept with a negative connota-\ntion (Teninbaum, 2009).\n15. Red herring. Introducing irrelevant mate-\nrial to the issue being discussed, so that every-\none\u2019s attention is diverted away from the points\nmade (Weston, 2018, p. 78). Those subjected to a\nred herring argument are led away from the issue\nthat had been the focus of the discussion and urged\nto follow an observation or claim that may be as-\nsociated with the original claim, but is not highly\nrelevant to the issue in dispute (Teninbaum, 2009).\nFigure 1: The beginning of an article with annotations.\n16. Bandwagon. Attempting to persuade the\ntarget audience to join in and take the course of\naction because \u201ceveryone else is taking the same\naction\u201d (Hobbs and Mcgee, 2008).\n17. Obfuscation, intentional vagueness, con-\nfusion. Using deliberately unclear words, to let\nthe audience have its own interpretation (Supra-\nbandari, 2007; Weston, 2018, p. 8). For instance,\nwhen an unclear phrase with multiple possible\nmeanings is used within the argument and, there-\nfore, it does not really support the conclusion.\n18. Straw man. When an opponent\u2019s proposi-\ntion is substituted with a similar one which is then\nrefuted in place of the original (Walton, 1996).\n4 Tasks\nThe shared task features two subtasks:\nFragment-Level Classi\ufb01cation task (FLC).\nGiven a news article, detect all spans of the text\nin which a propaganda technique is used. In\naddition, for each span the propaganda technique\napplied must be identi\ufb01ed.\nSentence-Level Classi\ufb01cation task (SLC). A\nsentence is considered propagandist if it contains\nat least one propagandist fragment. We then de-\n\ufb01ne a binary classi\ufb01cation task in which, given a\nsentence, the correct label, either propaganda or\nnon-propaganda , is to be predicted.5 Data\nThe input for both tasks consists of news articles\nin free-text format, collected from 36 propagandist\nand 12 non-propagandist news outlets1and then\nannotated by professional annotators. More de-\ntails about the data collection and the annotation,\nas well as statistics about the corpus can be found\nin (Da San Martino et al., 2019), where an earlier\nversion of the corpus is described, which includes\n450 news articles. We further annotated 47 addi-\ntional articles for the purpose of the shared task\nusing the same protocol and the same annotators.\nThe training, the development, and the test par-\ntitions of the corpus used for the shared task con-\nsist of 350, 61, and 86 articles and of 16,965,\n2,235, and 3,526 sentences, respectively. Fig-\nure 1 shows an annotated example, which con-\ntains several propaganda techniques. For ex-\nample, the fragment babies on line 1 is an in-\nstance of both Name Calling andLabeling .\nNote that the fragment not looking as though\nTrump killed his grandma on line 4 is an instance\nofExaggeration orMinimisation and it\noverlaps with the fragment killed his grandma ,\nwhich is an instance of Loaded Language .\nTable 1 reports the total number of instances per\ntechnique and the percentage with respect to the\ntotal number of annotations, for the training and\nfor the development sets.\n1We obtained the gold labels about whether a given news\noutlet was propagandistic from the Media Bias Fact Check\nwebsite: http://mediabiasfactcheck.com/\nTechnique Train (%) Dev (%)\nAppeal to Authority 116 (1.92) 50 (5.92)\nAppeal to fear / prejudice 239 (3.96) 103 (12.19)\nBandwagon 13 (0.22) 3 (0.36)\nBlack and White Fallacy 109 (1.80) 17 (2.01)\nCausal Oversimpli\ufb01cation 201 (3.33) 22 (2.60)\nDoubt 490 (8.11) 39 (4.62)\nExaggeration, Minimisation 479 (7.93) 59 (6.98)\nFlag Waving 240 (3.97) 63 (7.46)\nLoaded Language 2,115 (35.10) 229 (27.10)\nName Calling, Labeling 1,085 (17.96) 87 (10.30)\nObfuscation, Intentional\nVagueness, Confusion 11 (0.18) 5 (0.59)\nRed Herring 33 (0.55) 10 (1.18)\nReductio ad hitlerum 54 (0.89) 9 (1.07)\nRepetition 571 (9.45) 101 (11.95)\nSlogans 136 (2.25) 26 (3.08)\nStraw Men 13 (0.22) 2 (0.24)\nThought-terminating Cliches 79 (1.31) 10 (1.18)\nWhataboutism 57 (0.94) 10 (1.18)\nTable 1: Statistics about the gold annotations for the\ntraining and the development sets.\n6 Setup\nThe shared task had two phases: In the develop-\nment phase, the participants were provided labeled\ntraining and development datasets; in the testing\nphase, testing input was further provided.\nPhase 1. The participants tried to achieve the best\nperformance on the development set. A live\nleaderboard kept track of the submissions.\nPhase 2. The test set was released and the partici-\npants had few days to make \ufb01nal predictions.\nIn phase 2, no immediate feedback on the submis-\nsions was provided. The winner was determined\nbased on the performance on the test set.\n7 Evaluation\nFLC task. FLC is a composition of two sub-\ntasks: the identi\ufb01cation of the propagandist text\nfragments and the identi\ufb01cation of the techniques\nused (18-way classi\ufb01cation task). While F 1mea-\nsure is appropriate for a multi-class classi\ufb01cation\ntask, we modi\ufb01ed it to account for partial match-\ning between the spans; see (Da San Martino et al.,\n2019) for more details. We further computed an F 1\nvalue for each propaganda technique (not shown\nbelow for the sake of saving space, but available\non the leaderboard).\nSLC task. SLC is a binary classi\ufb01cation task\nwith imbalanced data. Therefore, the of\ufb01cial eval-\nuation measure for the task is the standard F 1mea-\nsure. We further report Precision and Recall.8 Baselines\nThe baseline system for the SLC task is a very sim-\nple logistic regression classi\ufb01er with default pa-\nrameters, where we represent the input instances\nwith a single feature: the length of the sentence.\nThe performance of this baseline on the SLC task\nis shown in Tables 4 and 5.\nThe baseline for the FLC task generates spans\nand selects one of the 18 techniques randomly.\nThe inef\ufb01cacy of such a simple random baseline\nis illustrated in Tables 6 and 7.\n9 Participants and Approaches\nA total of 90 teams registered for the shared task,\nand 39 of them submitted predictions for a total\nof 3,065 submissions. For the FLC task, 21 teams\nmade a total of 527 submissions, and for the SLC\ntask, 35 teams made a total of 2,538 submissions.\nBelow, we give an overview of the approaches\nas described in the participants\u2019 papers. Tables 2\nand 3 offer a high-level summary.\n9.1 Teams Participating in the\nFragment-Level Classi\ufb01cation Only\nTeam newspeak (Yoosuf and Yang, 2019)\nachieved the best results on the test set for the FLC\ntask using 20-way word-level classi\ufb01cation based\non BERT (Devlin et al., 2019): a word could be-\nlong to one of the 18 propaganda techniques, to\nnone of them, or to an auxiliary (token-derived)\nclass. The team fed one sentence at a time in\norder to reduce the workload. In addition to ex-\nperimenting with an out-of-the-box BERT, they\nalso tried unsupervised \ufb01ne-tuning both on the 1M\nnews dataset and on Wikipedia. Their best model\nwas based on the uncased base model of BERT,\nwith 12 Transformer layers (Vaswani et al., 2017),\nand 110 million parameters. Moreover, oversam-\npling of the least represented classes proved to be\ncrucial for the \ufb01nal performance. Finally, careful\nanalysis has shown that the model pays special at-\ntention to adjectives and adverbs.\nTeam Stalin (Ek and Ghanimifard, 2019)\nfocused on data augmentation to address the\nrelatively small size of the data for \ufb01ne-tuning\ncontextual embedding representations based\non ELMo (Peters et al., 2018), BERT, and\nGrover (Zellers et al., 2019). The balancing of\nthe embedding space was carried out by means of\nsynthetic minority class over-sampling. Then, the\nlearned representations were fed into an LSTM.\nTeam BERT LSTM Word Emb. Char. Emb. Features Unsup. Tuning\nCUNLP /check_sign /check_sign /check_sign\nStalin /check_sign /check_sign\nMIC-CIS /check_sign /check_sign /check_sign\nltuorp /check_sign\nProperGander /check_sign /check_sign\nnewspeak /check_sign /check_sign\nTable 2: Overview of the approaches for the fragment-level classi\ufb01cation task.\nTeam BERT LSTM logreg USE CNN Embeddings Features Context\nNSIT /check_sign /check_sign\nCUNLP /check_sign /check_sign /check_sign\nJUSTDeep /check_sign /check_sign /check_sign /check_sign\nTha3aroon /check_sign /check_sign\nLIACC /check_sign /check_sign /check_sign\nMIC-CIS /check_sign /check_sign /check_sign /check_sign /check_sign\nCAUnLP /check_sign /check_sign\nYMJA /check_sign\njinfen /check_sign /check_sign /check_sign\nProperGander /check_sign\nTable 3: Overview of the approaches used for the sentence-level classi\ufb01cation task.\n9.2 Teams Participating in the\nSentence-Level Classi\ufb01cation Only\nTeam CAUnLP (Hou and Chen, 2019) used two\ncontext-aware representations based on BERT. In\nthe \ufb01rst representation, the target sentence is fol-\nlowed by the title of the article. In the sec-\nond representation, the previous sentence is also\nadded. They performed subsampling in order to\ndeal with class imbalance, and experimented with\nBERT BASE and BERT LARGE\nTeam LIACC (Ferreira Cruz et al., 2019) used\nhand-crafted features and pre-trained ELMo em-\nbeddings. They also observed a boost in perfor-\nmance when balancing the dataset by dropping\nsome negative examples.\nTeam JUSTDeep (Al-Omari et al., 2019) used\na combination of models and features, including\nword embeddings based on GloVe (Pennington\net al., 2014) concatenated with vectors represent-\ning affection and lexical features. These were\ncombined in an ensemble of supervised models:\nbi-LSTM, XGBoost, and variations of BERT.\nTeam YMJA (Hua, 2019) also based their ap-\nproach on \ufb01ne-tuned BERT. Inspired by kaggle\ncompetitions on sentiment analysis, they created\nan ensemble of models via cross-validation.\nTeam jinfen (Li et al., 2019) used a logistic re-\ngression model fed with a manifold of representa-\ntions, including TF.IDF and BERT vectors, as well\nas vocabularies and readability measures.Team Tha3aroon (Fadel and Al-Ayyoub, 2019)\nimplemented an ensemble of three classi\ufb01ers: two\nbased on BERT and one based on a universal sen-\ntence encoder (Cer et al., 2018).\nTeam NSIT (Aggarwal and Sadana, 2019) ex-\nplored three of the most popular transfer learning\nmodels: various versions of ELMo, BERT, and\nRoBERTa (Liu et al., 2019).\nTeam Mindcoders (Vlad et al., 2019) combined\nBERT, Bi-LSTM and Capsule networks (Sabour\net al., 2017) into a single deep neural network and\npre-trained the resulting network on corpora used\nfor related tasks, e.g., emotion classi\ufb01cation.\nFinally, team ltuorp (Mapes et al., 2019) used\nan attention transformer using BERT trained on\nWikipedia and BookCorpus.\n9.3 Teams Participating in Both Tasks\nTeam MIC-CIS (Gupta et al., 2019) participated\nin both tasks. For the sentence-level classi\ufb01ca-\ntion, they used a voting ensemble including lo-\ngistic regression, convolutional neural networks,\nand BERT, in all cases using FastText embeddings\n(Bojanowski et al., 2017) and pre-trained BERT\nmodels. Beside these representations, multiple\nfeatures of readability, sentiment and emotions\nwere considered. For the fragment-level task, they\nused a multi-task neural sequence tagger, based\non LSTM-CRF (Huang et al., 2015), in conjunc-\ntion with linguistic features. Finally, they applied\nsentence- and fragment-level models jointly.\nSLC Task: Test Set (Of\ufb01cial Results)\nRank Team F 1 Precision Recall\n1ltuorp 0.6323 0.6028 0.6648\n2 ProperGander 0.6256 0.5649 0.7009\n3 YMJA 0.6249 0.6252 0.6246\n4 MIC-CIS 0.6230 0.5735 0.6818\n5 TeamOne 0.6183 0.5778 0.6648\n6 Tha3aroon 0.6138 0.5309 0.7274\n7 JUSTDeep 0.6112 0.5792 0.6468\n8 CAUnLP 0.6109 0.5180 0.7444\n9 LIPN 0.5962 0.5241 0.6914\n10 LIACC 0.5949 0.5090 0.7158\n11 aschern 0.5923 0.6050 0.5800\n12 MindCoders 0.5868 0.5995 0.5747\n13 jinfen 0.5770 0.5059 0.6712\n14 guanggong 0.5768 0.5039 0.6744\n15 Stano 0.5619 0.6666 0.4856\n16 nlpseattle 0.5610 0.6250 0.5090\n17 gw2018 0.5440 0.4333 0.7306\n18 SDS 0.5171 0.6268 0.4400\n19 BananasInPajamas 0.5080 0.5768 0.4538\n20 Baseline 0.4347 0.3880 0.4941\n21 NSIT 0.4343 0.5000 0.3838\n22 Stalin 0.4332 0.6696 0.3202\n23 Antiganda 0.3967 0.6459 0.2863\n24 Debunkers 0.2307 0.3994 0.1622\n25 SBnLP 0.1831 0.2220 0.1558\n26 Sberiboba 0.1167 0.5980 0.0646\nTable 4: Of\ufb01cial test results for the SLC task.\nTeam CUNLP (Alhindi et al., 2019) considered\ntwo approaches for the sentence-level task. The\n\ufb01rst approach was based on \ufb01ne-tuning BERT. The\nsecond approach complemented the \ufb01ne-tuned\nBERT approach by feeding its decision into a lo-\ngistic regressor, together with features from the\nLinguistic Inquiry and Word Count (LIWC)2lexi-\ncon and punctuation-derived features. Similarly to\nGupta et al. (2019), for the fragment-level problem\nthey used a Bi-LSTM-CRF architecture, combin-\ning both character- and word-level embeddings.\nTeam ProperGander (Madabushi et al., 2019)\nalso used BERT, but they paid special attention to\nthe imbalance of the data, as well as to the differ-\nences between training and testing. They showed\nthat augmenting the training data by oversampling\nyielded improvements when testing on data that\nis temporally far from the training (by increasing\nrecall). In order to deal with the imbalance, they\nperformed cost-sensitive classi\ufb01cation, i.e., the er-\nrors on the smaller positive class were more costly.\nFor the fragment-level classi\ufb01cation, inspired by\nnamed entity recognition, they used a model based\non BERT using Continuous Random Field stacked\non top of an LSTM.\n2http://liwc.wpengine.com/SLC Task: Development Set\nRank Team F 1 Precision Recall\n1 Tha3aroon 0.6883 0.6104 0.7889\n2 KS 0.6799 0.5989 0.7861\n3 CAUnLP 0.6794 0.5943 0.7929\n4 ProperGander 0.6767 0.5774 0.8173\n5 JUSTDeep 0.6745 0.6234 0.7347\n6 ltuorp 0.6700 0.6351 0.7090\n7 TeamOne 0.6649 0.6198 0.7171\n8 aschern 0.6646 0.6104 0.7293\n9 jinfen 0.6616 0.5800 0.7699\n10 YMJA 0.6601 0.6338 0.6887\n11 SBnLP 0.6548 0.5674 0.7740\n12 guanggong 0.6510 0.5737 0.7523\n13 LIPN 0.6484 0.5889 0.7212\n14 Stalin 0.6377 0.5957 0.6860\n15 Stano 0.6374 0.6561 0.6197\n16 BananasInPajamas 0.6276 0.5204 0.7902\n17 Kloop 0.6237 0.5846 0.6684\n18 nlpseattle 0.6201 0.6332 0.6075\n19 gw2018 0.6038 0.5158 0.7280\n20 MindCoders 0.5858 0.5264 0.6603\n21 NSIT 0.5794 0.6614 0.5155\n22 Summer2019 0.5567 0.6724 0.4749\n23 Antiganda 0.5490 0.6609 0.4695\n24 Cojo 0.5472 0.6692 0.4627\n25 Baseline 0.4734 0.4437 0.5074\n26 gudetama 0.4734 0.4437 0.5074\n27 test 0.4734 0.4437 0.5074\n28 Visionators 0.4410 0.5909 0.3518\n29 MaLaHITJuniors 0.3075 0.4694 0.2286\nTable 5: Results for the SLC task on the development\nset at the end of phase 1 (see Section 6).\n10 Evaluation Results\nThe results on the test set for the SLC task are\nshown in Table 4, while Table 5 presents the re-\nsults on the development set at the end of phase\n1 (cf. Section 6).3The general decrease of the F 1\nvalues between the development and the test set\ncould indicate that systems tend to over\ufb01t on the\ndevelopment set. Indeed, the winning team ltuorp\nchose the parameters of their system both on the\ndevelopment set and on a subset of the training set\nin order to improve the robustness of their system.\nTables 6 and 7 report the results on the test and\non the development sets for the FLC task. For\nthis task, the results tend to be more stable across\nthe two sets. Indeed, team newspeak managed to\nalmost keep the same difference in performance\nwith respect to team Antiganda . Note that team\nMIC-CIS managed to reach the third position de-\nspite never having submitted a run on the develop-\nment set.\n3Upon request from the participants, we reopened the sub-\nmission system for the development set for both tasks after\nthe end of phase 2; therefore, Tables 5 and 7 might not be up\nto date with respect to the online leaderboard.\nFLC Task: Test Set (Of\ufb01cial Results)\nRank Team F 1 Precision Recall\n1newspeak 0.2488 0.2862 0.2200\n2 Antiganda 0.2267 0.2882 0.1868\n3 MIC-CIS 0.1998 0.2234 0.1808\n4 Stalin 0.1453 0.1920 0.1169\n5 TeamOne 0.1311 0.3234 0.0822\n6 aschern 0.1090 0.0715 0.2294\n7 ProperGander 0.0989 0.0651 0.2056\n8 Sberiboba 0.0450 0.2974 0.0243\n9 BananasInPajamas0.0095 0.0095 0.0095\n10 JUSTDeep 0.0011 0.0155 0.0006\n11 Baseline 0.0000 0.0116 0.0000\n12 MindCoders 0.0000 0.0000 0.0000\n13 SU 0.0000 0.0000 0.0000\nTable 6: Of\ufb01cial test results for the FLC task.\n11 Conclusion and Further Work\nWe have described the NLP4IF@EMNLP-\nIJCNLP 2019 shared task on \ufb01ne-grained\npropaganda identi\ufb01cation. We received 25 and 12\nsubmissions on the test set for the sentence-level\nclassi\ufb01cation and the fragment-level classi\ufb01cation\ntasks, respectively. Overall, the sentence-level\ntask was easier and most submitted systems\nmanaged to outperform the baseline. The\nfragment-level task proved to be much more\nchallenging, with lower absolute scores, but most\nteams still managed to outperform the baseline.\nWe plan to make the schema and the dataset\npublicly available to be used beyond NLP4IF. We\nhope that the corpus would raise interest outside\nof the community of researchers studying propa-\nganda: the techniques related to fallacies and the\nones relying on emotions might provide a novel\nsetting for researchers interested in Argumentation\nand Sentiment Analysis.\nAs a kind of advertisement, Task 11 at SemEval\n20204is a follow up of this shared task. It features\ntwo complimentary tasks:\nTask 1 Given a free-text article, identify the pro-\npagandist text spans.\nTask 2 Given a text span already \ufb02agged as pro-\npagandist and its context, identify the speci\ufb01c\npropaganda technique it contains.\nThis setting would allow participants to focus\ntheir efforts on binary sequence labeling for Task 1\nand on multi-class classi\ufb01cation for Task 2.\n4http://propaganda.qcri.org/\nsemeval2020-task11/FLC Task: Development Set\nRank Team F 1 Precision Recall\n1 newspeak 0.2422 0.2893 0.2084\n2 Antiganda 0.2165 0.2266 0.2072\n3 Stalin 0.1687 0.2312 0.1328\n4 ProperGander 0.1453 0.1163 0.1934\n5 KS 0.1369 0.2912 0.0895\n6 TeamOne 0.1222 0.3651 0.0734\n7 aschern 0.1010 0.0684 0.1928\n8 gudetama 0.0517 0.0313 0.1479\n9 AMT 0.0265 0.2046 0.0142\n10 esi 0.0222 0.0308 0.0173\n11 ltuorp 0.0054 0.0036 0.0107\n12 Baseline 0.0015 0.0136 0.0008\n13 CAUnLP 0.0015 0.0136 0.0008\n14 JUSTDeep 0.0010 0.0403 0.0005\nTable 7: Results for FLC tasl on the development set.\nThe values refer to the end of phase 1 (see section 6)\nAcknowledgments\nThis research is part of the Propaganda Analy-\nsis Project,5which is framed within the Tanbih\nproject.6The Tanbih project aims to limit the ef-\nfect of \u201cfake news\u201d, propaganda, and media bias\nby making users aware of what they are reading,\nthus promoting media literacy and critical think-\ning, which is arguably the best way to address dis-\ninformation and \u201cfake news.\u201d The project is de-\nveloped in collaboration between the Qatar Com-\nputing Research Institute (QCRI), HBKU and the\nMIT Computer Science and Arti\ufb01cial Intelligence\nLaboratory (CSAIL).\nThe corpus for the task was annotated by A Data\nPro,7a company that performs high-quality man-\nual annotations.\nReferences\nKartik Aggarwal and Anubhav Sadana. 2019.\nNSIT@NLP4IF-2019: Propaganda detection from\nnews articles using transfer learning. In (Feldman\net al., 2019).\nHani Al-Omari, Malak Abdullah, Ola AlTiti, and\nSamira Shaikh. 2019. JUSTDeep at NLP4IF 2019\nTask 1: Propaganda detection using ensemble deep\nlearning models. In (Feldman et al., 2019).\nTariq Alhindi, Jonas Pfeiffer, and Smaranda Muresan.\n2019. Fine-tuned neural models for propaganda de-\ntection at the sentence and fragment levels. In (Feld-\nman et al., 2019).\n5http://propaganda.qcri.org\n6http://tanbih.qcri.org\n7http://www.aiidatapro.com\nAlberto Barr \u00b4on-Cede \u02dcno, Giovanni Da San Martino, Is-\nraa Jaradat, and Preslav Nakov. 2019. Proppy: A\nsystem to unmask propaganda in online news. In\nProceedings of the 33rd AAAI Conference on Arti\ufb01-\ncial Intelligence , AAAI \u201919, pages 9847\u20139848, Hon-\nolulu, HI, USA.\nAlberto Barr \u00b4on-Cedeno, Israa Jaradat, Giovanni Da\nSan Martino, and Preslav Nakov. 2019. Proppy:\nOrganizing the news based on their propagandistic\ncontent. Information Processing & Management ,\n56(5):1849\u20131864.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics , 5:135\u2013146.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\n2018. Universal sentence encoder. CoRR ,\nabs/1803.11175.\nGiovanni Da San Martino, Seunghak Yu, Alberto\nBarr\u00b4on-Cede \u02dcno, Rostislav Petrov, and Preslav\nNakov. 2019. Fine-grained analysis of propaganda\nin news articles. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing , EMNLP-\nIJCNLP 2019, Hong Kong, China.\nLavinia Dan. 2015. Techniques for the Translation of\nAdvertising Slogans. In Proceedings of the Interna-\ntional Conference Literature, Discourse and Multi-\ncultural Dialogue , LDMD \u201915, pages 13\u201323, Mures,\nRomania.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies , NAACL-HLT \u201919, pages 4171\u20134186, Min-\nneapolis, MN, USA.\nAdam Ek and Mehdi Ghanimifard. 2019. Synthetic\npropaganda embeddings to train a linear projection.\nIn (Feldman et al., 2019).\nIbrahim Fadel, Ali Tuffaha and Mahmoud Al-Ayyoub.\n2019. Pretrained ensemble learning for \ufb01ne-grained\npropaganda detection. In (Feldman et al., 2019).\nAnna Feldman, Giovanni Da San Martino, Alberto\nBarr\u00b4on-Cede \u02dcno, Chris Brew, Chris Leberknight, and\nPreslav Nakov, editors. 2019. Proceedings of the\n2019 Workshop on Natural Language Processing for\nInternet Freedom (NLP4IF): censorship, disinfor-\nmation, and propaganda . Hong Kong, China.Andr \u00b4e Ferreira Cruz, Gil Rocha, and Henrique Lopes\nCardoso. 2019. On sentence representations for pro-\npaganda detection: From handcrafted features to\nword embeddings. In (Feldman et al., 2019).\nMonika Glowacki, Vidya Narayanan, Sam Maynard,\nGustavo Hirsch, Bence Kollanyi, Lisa-Maria Neud-\nert, Phil Howard, Thomas Lederer, and Vlad Barash.\n2018. News and political information consumption\nin Mexico: Mapping the 2018 Mexican Presidential\nelection on Twitter and Facebook. Technical Report\nCOMPROP DATA MEMO 2018.2, Oxford Univer-\nsity, Oxford, UK.\nJean Goodwin. 2011. Accounting for the force of the\nappeal to authority. In Proceedings of the 9th Inter-\nnational Conference of the Ontario Society for the\nStudy of Argumentation , OSSA \u201911, pages 1\u20139, On-\ntario, Canada.\nPankaj Gupta, Khushbu Saxena, Usama Yaseen,\nThomas Runkler, and Hinrich Schutze. 2019. Neu-\nral architectures for \ufb01ne-grained propaganda detec-\ntion in news. In (Feldman et al., 2019).\nIvan Habernal, Raffael Hannemann, Christian Pol-\nlak, Christopher Klamm, Patrick Pauli, and Iryna\nGurevych. 2017. Argotario: Computational argu-\nmentation meets serious games. In Proceedings\nof the Conference on Empirical Methods in Natu-\nral Language Processing , EMNLP \u201917, pages 7\u201312,\nCopenhagen, Denmark.\nIvan Habernal, Patrick Pauli, and Iryna Gurevych.\n2018a. Adapting serious game for fallacious argu-\nmentation to German: pitfalls, insights, and best\npractices. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation , LREC \u201918, Miyazaki, Japan.\nIvan Habernal, Henning Wachsmuth, Iryna Gurevych,\nand Benno Stein. 2018b. Before name-calling: Dy-\nnamics and triggers of ad hominem fallacies in web\nargumentation. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies , NAACL-HLT \u201918, pages 386\u2013\n396, New Orleans, LA, USA.\nRenee Hobbs and Sandra Mcgee. 2008. Teaching\nabout propaganda: An examination of the historical\nroots of media literacy. Journal of Media Literacy\nEducation , 6(62):56\u201367.\nBenjamin D Horne, Sara Khedr, and Sibel Adali. 2018.\nSampling the news producers: A large news and\nfeature data set for the study of the complex media\nlandscape. In Proceedings of the International AAAI\nConference on Web and Social Media , ICWSM \u201918,\npages 518\u2013527, Stanford, CA, USA.\nWenjun Hou and Ying Chen. 2019. CAUnLP\nat NLP4IF 2019 shared task: Context-dependent\nBERT for sentence-level propaganda detection. In\n(Feldman et al., 2019).\nYiqing Hua. 2019. Understanding BERT performance\nin propaganda analysis. In (Feldman et al., 2019).\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-\nrectional LSTM-CRF models for sequence tagging.\nCoRR , abs/1508.01991.\nJohn Hunter. 2015. Brainwashing in a large group\nawareness training? The classical conditioning hy-\npothesis of brainwashing. Master\u2019s thesis, Uni-\nversity of Kwazulu-Natal, Pietermaritzburg, South\nAfrica.\nGarth S. Jowett and Victoria O\u2019Donnell. 2012. What is\npropaganda, and how does it differ from persuasion?\nInPropaganda & Persuasion , chapter 1, pages 1\u201348.\nSage Publishing.\nJinfen Li, Zhihao Ye, and Lu Xiao. 2019. Detection of\npropaganda using logistic regression. In (Feldman\net al., 2019).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv , abs/1907.11692.\nHarish Tayyar Madabushi, Elena Kochkina, and\nCastelle Michael. 2019. Cost-sensitive BERT for\ngeneralisable sentence classi\ufb01cation on imbalanced\ndata. In (Feldman et al., 2019).\nNorman Mapes, Anna White, Radhika Medury, and\nSumeet Dua. 2019. Divisive language and pro-\npaganda detection using multi-head attention trans-\nformers with deep learning BERT-based language\nmodels for binary classi\ufb01cation. In (Feldman et al.,\n2019).\nClyde R. Miller. 1939. The Techniques of Propaganda.\nFrom \u201cHow to Detect and Analyze Propaganda,\u201d an\naddress given at Town Hall. The Center for learning.\nRobert Muller. 2018. Internet Research Agency Indict-\nment. http://www.justice.gov/file/\n1035477/download .\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP \u201914, pages 1532\u20131543, Doha, Qatar.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies , NAACL-HLT \u20192018, pages\n2227\u20132237, New Orleans, LA, USA.\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nV olkova, and Yejin Choi. 2017. Truth of varyingshades: Analyzing language in fake news and polit-\nical fact-checking. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing , EMNLP \u201917, pages 2931\u20132937, Copen-\nhagen, Denmark.\nMonika L Richter. 2017. The Kremlin\u2019s platform for\n\u2018useful idiots\u2019 in the West: An overview of RT\u2019s ed-\nitorial strategy and evidence of impact. Technical\nreport, Kremlin Watch.\nSara Sabour, Nicholas Frosst, and Geoffrey E. Hinton.\n2017. Dynamic routing between capsules. In Pro-\nceedings of the Annual Conference on Neural Infor-\nmation Processing Systems 2017 , NIPS \u201917, pages\n3856\u20133866, Long Beach, CA, USA.\nFrancisca Niken Vitri Suprabandari. 2007. Ameri-\ncan propaganda in John Steinbeck\u2019s The Moon is\nDown. Master\u2019s thesis, Sanata Dharma University,\nYogyakarta, Indonesia.\nCristina Tard \u00b4aguila, Fabr \u00b4\u0131cio Benevenuto, and Pablo\nOrtellado. 2018. Fake news is poisoning Brazilian\npolitics. WhatsApp can stop it. https://www.\nnytimes.com/2018/10/17/opinion/\nbrazil-election-fake-news-whatsapp.\nhtml .\nGabriel H Teninbaum. 2009. Reductio ad Hitlerum:\nTrumping the judicial Nazi card. Michigan State\nLaw Review , page 541.\nRobyn Torok. 2015. Symbiotic radicalisation strate-\ngies: Propaganda tools and neuro linguistic pro-\ngramming. In Proceedings of the Australian Se-\ncurity and Intelligence Conference , pages 58\u201365,\nPerth, Australia.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the Conference on\nNeural Information Processing Systems , NIPS \u201917,\npages 5998\u20136008, Long Beach, CA, USA.\nGeorge-Alexandru Vlad, Mircea-Adrian Tanase, Cris-\ntian Onose, and Dumitru-Clementin Cercel. 2019.\nSentence-level propaganda detection in news ar-\nticles with transfer learning and BERT-BiLSTM-\nCapsule model. In (Feldman et al., 2019).\nDouglas Walton. 1996. The straw man fallacy . Royal\nNetherlands Academy of Arts and Sciences.\nAnthony Weston. 2018. A rulebook for arguments .\nHackett Publishing.\nShehel Yoosuf and Yin Yang. 2019. Fine-grained pro-\npaganda detection with \ufb01ne-tuned BERT. In (Feld-\nman et al., 2019).\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. CoRR , abs/1905.12616.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Findings of the NLP4IF-2019 shared task on fine-grained propaganda detection", "author": ["GDS Martino", "A Barr\u00f3n-Cede\u00f1o", "P Nakov"], "pub_year": "2019", "venue": "arXiv preprint arXiv \u2026", "abstract": "We present the shared task on Fine-Grained Propaganda Detection, which was organized  as part of the NLP4IF workshop at EMNLP-IJCNLP 2019. There were two subtasks. FLC is a"}, "filled": false, "gsrank": 202, "pub_url": "https://arxiv.org/abs/1910.09982", "author_id": ["URABLy0AAAAJ", "0q0QVG4AAAAJ", "DfXsKZ4AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:9OJoXqZMcbwJ:scholar.google.com/&output=cite&scirp=201&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D200%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=9OJoXqZMcbwJ&ei=KbWsaMZzjoiJ6g_SwpG4CQ&json=", "num_citations": 2, "citedby_url": "/scholar?cites=13578718628930970356&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:9OJoXqZMcbwJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/1910.09982"}}, {"title": "Effects of perceived reach on ratings of media bias, quality, and agreement", "year": "2022", "pdf_data": "Univ ersity of Nor thern Iowa Univ ersity of Nor thern Iowa \nUNI ScholarW orks UNI ScholarW orks \nDisser tations and Theses @ UNI Student W ork \n2022 \nEffects of per ceived reach on r atings of media bias, quality , and Effects of per ceived reach on r atings of media bias, quality , and \nagreement agreement \nMatthew Sedlacek \nUniv ersity of Nor thern Iowa \nLet us know how access t o this document benefits y ou \nCopyright \u00a92022 Matthew Sedlacek \nFollow this and additional works at: https:/ /scholar works.uni.edu/etd \n Part of the Journalism Studies Commons , and the Other Psy chology Commons \nRecommended Citation Recommended Citation \nSedlacek, Matthew , \"Effects of per ceived reach on r atings of media bias, quality , and agr eement\" (2022). \nDisser tations and Theses @ UNI . 1237. \nhttps:/ /scholar works.uni.edu/etd/1237 \nThis Open Access Thesis is br ought t o you for fr ee and open access b y the Student W ork at UNI ScholarW orks. It \nhas been accepted for inclusion in Disser tations and Theses @ UNI b y an authoriz ed administr ator of UNI \nScholarW orks. F or mor e information, please contact scholar works@uni.edu . \nOffensiv e Materials Statement:  Materials located in UNI ScholarW orks come fr om a br oad r ange of sour ces and \ntime periods. Some of these materials ma y contain off ensiv e ster eotypes, ideas, visuals, or language. \nCopyright by  \nMATTHEW SEDLACEK  \n2022 \nAll Rights Reserved\n \nEFFECTS OF PERCEIVED REACH ON RATINGS  \nOF MEDIA BIAS, QUALITY, AND AGREEMENT  \n \n \n \n \n \n \nAn Abstract of a Thesis  \nSubmitted  \nin Partial Fulfillment  \nof the Requirements for the Degree  \nMaster of Arts  \n \n \n \n \n \n \nMatthew Sedlacek  \nUniversity of Northern Iowa  \nJuly 2022  \n \nABSTRACT  \nThe hostile media effect refers to individuals\u2019 tendencies to perceive seemingly neutral \nnews coverage as biased against their stance (Vallone et al., 1985). Research has shown \nthis effect in partisan politics, with liberals and conservatives perceiving bi as in \ninformation presented from the opposing side. The current study examined the effects of \nliberal or conservative and small or large audience news source manipulations on \nliberals\u2019 and conservatives\u2019 perceptions of media bias and the relationship betwe en \npartisan identity and perceptions of bias. Three hundred and sixty participants read a \nnews article presented with either a Fox News, CNN, PatriotNewsDaily , or The \nProgressive  news heading  and completed a questionnaire assessing perceptions of article \nbias, article quality, and their agreement with the article\u2019s content. I hypothesized that  \nliberals and conservatives would perceive hostile media bias when reading an article with \nan opposing source heading and that the effect would be greater when partici pants were \npresented with the high -reach source headings (i.e., Fox News, CNN). Liberals and \nconservatives did not perceive the article to be biased against their stance based on the \npolitical stance of the news source or its perceived reach. They also did  not differ in their \narticle quality ratings and ratings of agreement with the article\u2019s content depending on the \nnews source or perceived reach. Exploratory correlations between partisan identity article \nbias, article quality, and agreement with the artic le\u2019s content showed small correlations \noverall. Individuals may be less inclined to focus as much on the source of information in \na society increasingly centered on sharing of content on social media. They also may \nfocus less on a source\u2019s political stance  and more on the article content itself. This \n \nresearch highlights the effects of the way news outlets present their content as well as an \nindividual\u2019s biased interpretations of the news they are receiving.   \nKeywords :  hostile media effect, liberal, conser vative, psychology, study  \n  \n \nEFFECTS OF PERCEIVED REACH ON RATINGS  \nOF MEDIA BIAS, QUALITY, AND AGREEMENT  \n \n \n \n \n \n \nA Thesis \nSubmitted  \nin Partial Fulfillment  \nof the Requirements for the Degree  \nMaster of Arts  \n \n \n \n \n \n \nMatthew Sedlacek  \nUniversity of Northern Iowa  \nJuly 2022\nii \n \n This Study by:  Matthew Sedlacek  \nEntitled: Effect of Perceived Reach on Ratings of Media Bias, Quality, and Agreement  \n \nhas been approved as meeting the thesis requirement for the  \nDegree of Master of Arts  \n \n \n \n    \nDate Dr. Helen Harton , Chair, Thesis Committee  \n \n \n    \nDate Dr. Jiuqing Cheng , Thesis Committee Member  \n \n \n    \nDate Dr. Nicholas Schwab , Thesis Committee Member  \n \n \n    \nDate Dr. Gabriela Olivares , Interim Dean, Graduate College  \n \n  \niii \n \n ACKNOWLEDGEMENTS  \n I would like to thank my advisor, Dr. Helen Harton, for her guidance and support \nthroughout this entire process. Getting to the finish line on this research was no easy task, \nand I am grateful for her help in seeing this thesis through to the end.  I would also like to \nthank my committee members, Dr. Nicholas Schwab and Dr. Jiuqing Cheng, for their \nfeedback and contributions to this research.  \n I would also like to thank my partner and biggest support, Kayla, for sticking with \nme through thick and thin, and inspiring me to reach my fullest potential.  To my parents \nand my brother, I appreciate everything you have done for me and for your \nencouragement along this journey. I can\u2019t express enough how much all those closest to \nme have meant to this research. Once again, thank you all.     \n \n  \niv \n \n TABLE OF CONTENTS  \nPAGE \nLIST OF TABLES  ................................ ................................ ................................ ............ vii \nLIST OF FIGURES  ................................ ................................ ................................ ........... ix \nNeutrality and Bias in News Media  ................................ ................................ ................  3 \nFoundations of the Hostile Media Effect  ................................ ................................ ........ 5 \nHostile Media Effect Review  ................................ ................................ ......................  5 \nTheory and Processes  ................................ ................................ ................................ ...... 7 \nMotivated Reasoning  ................................ ................................ ................................ .. 8 \nCognitive Dissonance  ................................ ................................ ................................ . 9 \nConfirmation Bias  ................................ ................................ ................................ ..... 11 \nBiased Assimilation  ................................ ................................ ................................ .. 12 \nSummary  ................................ ................................ ................................ ...................  13 \nFactors Affecting the Hostile Media Effect  ................................ ................................ .. 14 \nNews Source  ................................ ................................ ................................ ............. 14 \nPerceived Reach of Source  ................................ ................................ .......................  15 \nPartisan Identity  ................................ ................................ ................................ ........ 15 \nCurrent Study  ................................ ................................ ................................ ................  18 \nPilot Study  ................................ ................................ ................................ .....................  22 \nv \n \n Method ................................ ................................ ................................ ......................  23 \nResults and Discussion  ................................ ................................ .............................  25 \nMethod ................................ ................................ ................................ ..........................  27 \nDesign ................................ ................................ ................................ .......................  27 \nParticipants  ................................ ................................ ................................ ................  28 \nProcedure  ................................ ................................ ................................ ..................  29 \nMeasures  ................................ ................................ ................................ ...................  30 \nPlan of Analysis  ................................ ................................ ................................ ........ 33 \nResults ................................ ................................ ................................ ...........................  34 \nData Cleaning  ................................ ................................ ................................ ........... 34 \nTests of Assumptions  ................................ ................................ ................................  34 \nManipulation check  ................................ ................................ ................................ ... 35 \nHypothesis Testing  ................................ ................................ ................................ ... 36 \nExploratory Analyses  ................................ ................................ ................................  45 \nDiscussion  ................................ ................................ ................................ .....................  48 \nPossible Explanations  ................................ ................................ ...............................  48 \nLimitations and Ideas for Futur e Research  ................................ ...............................  51 \nImplications  ................................ ................................ ................................ .............. 55 \nREFERENCES  ................................ ................................ ................................ .................  57 \nvi \n \n APPENDIX A  ................................ ................................ ................................ ...................  69 \nAPPENDIX B  ................................ ................................ ................................ ...................  72 \nAPPENDIX C  ................................ ................................ ................................ ...................  74 \nAPPENDIX D  ................................ ................................ ................................ ...................  77 \nAPPENDIX E  ................................ ................................ ................................ ...................  79 \nAPPENDIX F  ................................ ................................ ................................ ...................  81 \nAPPENDIX G  ................................ ................................ ................................ ...................  82 \nAPPENDIX H  ................................ ................................ ................................ ...................  83 \n \n  \nvii \n \n LIST OF TABLES  \nPAGE \nTable 1  Article Bias Ratings, Perceived Source Audience Size, and Perceived Article \nViews (Percentages) ................................ ................................ ................................ .......... 26 \nTable 2 Results of Moderation of News Source Reach on Relationship Between Source \nPO, Participant PO, and Article Bias Ratings Against a Conservative Viewpoint  .......... 42 \nTable 3 Results of Moderation of News Source Reach on Relationship Between Source \nPO, Participant PO, and Article Bias Ratings Against a Liberal View point ...................  43 \nTable 4 Results of Moderation of News Source Reach on Relationship Between Source \nPO, Participant PO, and Article Quality  Ratings................................ .............................  44 \nTable 5 Results of Moderation of News Source Reach on Relationship Between Source \nPO, Participant PO, and Ratings of Agreement  ................................ ...............................  45 \nTable 6 Within-cell Correlations Table  ................................ ................................ ............ 47 \nTable 7 Re-run of Article Bias Ratings Against a Conservative Viewpoint Depending on \nSource Political Orientation  ................................ ................................ .............................  83 \nTable 8 Re-run of Article Bias Ratings Against a Liberal Viewpoint Depending on Source \nPolitical Orientation  ................................ ................................ ................................ ......... 83 \nTable 9 Re-run of Article Quality Ratings Depending on Source Political Orientation  .. 84 \nTable 10 Re-run of Ratings of Agreement with Article Content Depending on Source \nPolitical Orientation  ................................ ................................ ................................ ......... 84 \nTable 11 Re-run of Moderation of News Source Reach on Relationship Between Source \nPO, Participant PO, and Article Bias Ratings Against a Conservative Viewpoint  .......... 85 \nviii \n \n Table 12 Re-run of Moderation of News Source Reach on Relationship Between Source \nPO, Participant PO, and Article Bias Ratings Against a Liberal View point ...................  85 \nTable 13 Re-run of Moderation of News Source Reach on Relationship Between Source \nPO, Participant PO, and Article Quality  Ratings................................ .............................  86 \nTable 14 Re-run of Moderation of News Source Reach on Relationship Between Source \nPO, Participant PO, and Ratings of Agreement  ................................ ...............................  86 \n  \n \n \n  \nix \n \n LIST OF FIGURES  \nPAGE \nFigure 1. Article Bias Ratings Against a Conservative Viewpoint.  ................................ . 37 \nFigure 2. Article Bias Ratings Against a Liberal Viewpoint.  ................................ ........... 38 \nFigure 3. Article Quality Ratings  ................................ ................................ ......................  39 \nFigure 4. Agreement with Article content  ................................ ................................ ........ 41 \n \n \n \n1 \n \nEffects of Reach and Partisan Identity on the Hostile Media Effect  \nSince 1994, the divide between individuals who identify as Democrat and \nindividuals who identify as Republican on major political issues has more than doubled  \n(Doherty, 2017) . There are several aspects of news media d irectly tied to this widening \npartisan divide, including the biased presentation of media coverage on political issues.  \nThis biased news presentation has coincided with t he proliferation of coverage as a result \nof the 24-hour news cycle and the growth of mainstream cable news networks. News \nmedia, especially cable television,  that has a liberal slant may amplify viewpoints of \nother liberals, creating an \u201cecho chamber \u201d effect (Carmich ael et al., 2017). This biased \nnews coverage encourages supporters of a partisan position to operate in a close d system \nwhere they only believe arguments on their side to be true, and their overall viewpoints \nare strengthened.  \nThis growing divide between the positions of Democrats and Republicans on \nmajor issues contributes to greater feelings of negativity toward s the opposing party. \nIdentified as affective polarization, this is a growing trend of animosity between the two \nparties where individuals on both sides feel negatively towards the other and perceive \nthem as unwilling to cooperate across political lines (I yengar et al., 2018). Startlingly, the \nU.S. has increased in affective polarization significantly more than eight other major \ncountries studied for this phenomenon, including the U.K., Canada, German, and \nAustralia (Boxell et al., 2021). With feelings towa rds members of the opposing party \ngrowing increasingly negative, it  is important to examine what contributes to this effect.  \n2 \n \n The political slant of  news coverage has played a significant role in this growing \nrift between liberals and conservatives , and it occurs through the strategic ways news \nnetworks present their coverage. All three major cable news networks (i.e., FOX  News, \nCNN, MSNBC) engaged in several noteworthy bias tactics  in their coverage of the 2012 \nelection (Rosell, 2013). Programs were biased in many ways, including  their story \nselections, their deliberate omissions of other sides of a story, and their strategic selection \nof guests who offer ed supportive poin ts to their overall agenda.  Other networks have also \ndisplayed partisan bias in their coverage, with CBS stories favoring the Democratic \ncandidate in the 2000 and 2004 presidential elections and ABC stories favoring the \nRepublican candidate (Zeldes et al., 2008). News coverage with a political slant has \nplayed a role in the growing divide between Democrats and Republicans, but individual \nperceptions of bias also play a role, even when the news coverage is largely unbiased.  \nPartisan evaluations  are not only affected by the presentati on of news coverage, \nbut also the individual interpretations and perceptions of this coverage. Even unbiased \nnews can be perceived to be biased by people if it goes against their personal beliefs or \nworldview. The hostile media effect refers to an individu al\u2019s tendency to perceive even \nobjective news coverage of a particular issue as biased against their stance (Vallone  et al., \n1985). Democrats are likelier to perceive a Republican bias in mainstream media , \nwhereas Republicans are likelier to perceive a Dem ocratic bias, regardless of the \nobjectivity of the news reported (Morris, 20 07). Individuals on both sides of a debate \nhave a stake in the outcome, and \u2013 because of their stance \u2013 may interpret news coverage \n3 \n \n in a biased way , despite the accuracy of the inf ormation reported. Personal values and \nbeliefs may impact an individual \u2019s perceptions of bias, even in unbiased news coverage.   \nThis study examined the hostile media effect in online news coverage . In the \nfollowing sections, I first discuss what \u201cneutral\u201d or non-biased media consists of. Then, I \nreview the  foundations of the hostile media effect, including hostile media effect research \nas well as  several components of motivated reasoning, including  cognitive dissonance \nand confirmation bias . Then, I discuss the effects of news source heading manipulations, \nthe strength of an individual \u2019s identification with their partisan position, and the \nperceived influence of the news source on hostile media bias  in individuals.  Finally, I \ndescribe a study that assesse d perceived hostile media bias and its relationship with these \ncomponents.  \nNeutrality and Bias in News Media  \nWhen news reporting was establishing its footing as a more desirable profession \nin the 1800s, realism and unbiased reporting was widespread (Gershon, 2019) . The most \nsuccessful journalists were those whose work was entirely based on factual reporting  \n(Gershon, 2019). The turn of the 20th century saw a shift, where reporters became more \naware of the effects of propaganda. With this, news reporting with a slant, or an \nallegiance to a certain side, became more mainstream. Walter Lippmann, one of the most \nprominent journalists of the 20th century, described the dangers that biase d news reporting \ncauses for democracy and stated that journalism should find a common ground rooted in \nfacts (Lippmann  et al., 1920). Despite Lippmann \u2019s influential commentary, journalistic \nobjectivity continues to be a  topic of concern. There is no clear agreement between \n4 \n \n liberals and conservatives on what news \u201cfalls in the middle \u201d between the two sides. In \nfact, 56% of Americans stated that they cannot name a news source that reports news \nobjectively (Jones  & Ritter, 2018).  \nIn a world where perce ptions of a hostile med ia are increasingly apparent, some \njournalists have become more conscientious in their objective reporting of the news \n(Solomon, 2018 ). They recognize neutral and fair reporting of the news as being a core \nvalue the profession of jou rnalism was built on. However, there seems to be a tradeoff \nbetween objective reporting of the news and producing informative content. For example, \nreporting news in partisan politics that is objective, or covers both the liberal and \nconservative sides in an equal manner, can actually be counterproductive. Reporting news \nin partisan politics in an objective manner that does not question either side may actually \nundermine the public \u2019s ability to make an informed decision and their ability to make \nsignificant  distinctions between two candidates (Dunaway et al., 2015; Patterson, 2013). \nIn fact, articles with no slant that did not pick a side in a debate were found to be less \ninformative as a whole (Dunaway et al., 2015).   \nMedia bias refers to the subjectivity o f journalists and all those who report the \nnews in the ways they select and cover events. Biased reporting is accomplished through \nthe process of framing, which refers to selecting elements of a perceived or implied \nreality and organizing them in a narrati ve to achieve a desired interpretation of the story \n(Entman, 2007). Creating a narrative that promotes a particular interpretation of a story \nhas often been referred to in media bias research as agenda setting (Entman, 2007 ; \nMorstatter et al., 2018). News reporters who bias their coverage want their audience to \n5 \n \n respond to information in a particular way, and framing allows them to achieve this goal. \nThe concept of slant goes hand in hand with framing, and the overall concept of media \nbias, as it indicates a n allegiance to a particular side. Slant is often achieved through the \nparticular language used in a story  as well as the elements of a story that are included \n(Dunaway et al., 2015).  In the political landscape, slant is interpreted through the ways \npolicy and various others pieces of information, such as candidate trait and issue \npreferences, are discussed in a story. Skeptics have  been aware of biased reporting for \nmore than a century, but it has only been recently that researchers have  begun to examine \nthe mechanisms through which it is achi eved.   \nFoundations of the Hostile Media Effect  \nHostile Media Effect  Review  \nThe hostile media effect refers to how even neutral news information can be \nperceived as biased against one \u2019s stance (Vallone et al., 1985). This effect has been \ndemonstrated in perceptions of news coverage related to areas such as sports  (Arpan & \nRaney, 2003) , the use of GMOs  (Gunther et al., 2009), various political issues  (Gunther \n& Liebhart, 2006) , and the 1997 UPS strike  (Christen et al., 2002). For example, in \nviewing the same news story about the Beirut massacre of 1982 from a major TV \nnetwork, pro -Israeli students and pro -Arab students perceived the coverage to be biased \nagainst their side (Vallone et al., 1985). A meta-analysis on hostile media effect research  \nshowed a medium effect size across all thirty-four of the studies ( Hansen & Kim, 2011) . \nThis meta -analysis investigated the various mediums through which the hostile media \neffect was explored, finding 50% of the studies examined the effect through newspaper \n6 \n \n content, wh ereas the rest examined the effect through television or general media \nexposure.  \nThe hostile media effect has been studied extensively in politic al communication . \nIn a study of reactions to the John Kerry/George W. Bush debates preceding the 2004 \nPresidential election, Republicans were more likely to perceive the moderator of the \ndebate to be hostile against George W. Bush, whereas Democrats were more likely to \nperceive the  moderator to be hostil e against John Kerry (Richardson et al., 2008) . \nIndividuals perceive less bias against their position in news co verage if the host of a show \nhas similar political views, and more bias against their position when they do not \n(Feldman, 2011). Although Democrats and Republicans differ on their opinions, values, \nand beliefs, it appears hostile media bias affects supporters of both political parties.  \n Perceptions of bias in political news coverage cover a wide range of political \nissues. An individual \u2019s partisan stance impacts their perception of global warming, and \nthey may perceive opposing coverage as biased against their stance  (Feldman  et al., \n2017). Democrats and Republicans also perceived media bias in newspapers they read \nduring a gubernatorial race (Huge & Glynn, 2010). Interestingly, Republicans \u2019 \nperceptions of bias grew much stronger as the campaign wore on as opposed to \nDemocrats \u2019 perceptions . This finding could be attributed to the fact the Democratic \ncandidate had an increasingly larger lead in opinion polls as the election drew closer, so \nthe Democrats may have been less apprehensive about biased coverage swaying voters \nwho were unsure of t heir candidate choice  (Huge & Glynn, 2010). Th ese results indicate \n7 \n \n that individual perceptions of bias may be fluid throughout an election cycle depending \non factors such as opinion polls.  \nAs the social media age has progressed, the hostile media effect research has \ntranslated to online settings . One of these settings is the social media platform Twitter, \nwhich is a major  media source for consumers of news. Individuals who identified as \nDemocrat perceived hostile media bias in a tweet from another Twitter user who \nidentified as Republican (Lee  et al., 2018). News articles that are embedded in shared \nposts on social media also seem to induce biased perceptions. Liberals and conservatives \nperceived bias in a news article included in a blog post when it was p resented from a \nsource with the opposing political position (Yun  et al., 2016). This bias may emerge even \nif individuals do not read the article . Partisans perceived bias in a Facebook post about an \narticle when they were exposed to comments underneath it that disagreed with their \nstance (Gearhart  et al., 2020). There were no biased perceptions in partisans who were \nexposed to comments that agreed with their opinions.  Even in an online setting such as \nsocial media, partisans demonstrate  hostile media perce ptions, and these could be \ninfluenced by perceptions of the sourc e and comments  on the social media posts.  \nTheory and Processes  \n The hostile media effect is a subset of a larger area of research in social cognition , \ncalled motivated reasoning . Motivated reasoning refers to how individuals are driven to \nbe accurate in their thoughts and behavior, and this process influences the way they \nconstruct and access their  internal beliefs (Kunda, 1990). Other aspects of motivated \nreasoning include cognitive dissonance , confirmation bias , and selective exposure . These \n8 \n \n terms compose the foundation of the hostile media effect and the mechanisms that \ncontribute to perceptions of media bias.  \nMotivated Reasoning  \nIndividuals \u2019 behaviors are heavily impacted by in ternal beliefs they hold to be true \nand their motivations to adhere to these beliefs. Th e motivation to defend one \u2019s beliefs is \nmanifested in the way people make decisions and develop or change their attitudes. This \nidea has been applied to explain people \u2019s behavior in a variety of ways, with one example \nbeing the use of stereotypes (Kunda & Sinclair, 1999). When individuals believe a \nstereotype fits the initial impression they have of another person, motivated reasoning \nleads them to activate this stereoty pe, and they will interact with them in a constrained \nmanner. In the same light, when a stereotype does not fit the initial impression, motivated \nreasoning dictates the individual will inhibit that stereotype (Kunda & Sinclair, 1999). \nThis effect was demon strated through individuals \u2019 activation of a stereotype against \nAfrican-American individuals in a study where either a White or African -American \nindividual reviewed participants \u2019 responses  on an employee exercise. Participants \nresponded to criticism more h arshly when they received a negative review from a n \nAfrican-American  individual rather than a White individual (Kunda & Sinclair, 1999). \nParticipants were motivated to undermine the evaluator because they had activated the \nBlack stereotype, and this motiva tion stemmed from the desire to defend their own \ninternal beliefs and self -views.  \nPeople also use motivated reasoning to support decisions they want to make. \nWhen smokers had the urge to smoke, they accessed their preexisting beliefs about the \n9 \n \n positive as pects of smoking and suppressed the negative aspects. With these positive \naspects of smoking more salient to them, they reinforced these preexisting beliefs by \nsatisfying their urge to smoke (Sayette & Hufford, 1997). Motivated reasoning impacts \ndecision-making and interactions with other individuals.  \nMotivated reasoning translates to politics in that individuals with strong prior \npolitical attitudes may selectively interpret information in a way that supports their \ndesired conclusio n. Taber and Lodge (2016) posit  that motivated reasoning produces an \nautomatic response, and exposure to items that engender a strong opinion \u2014such as a \npolitician \u2019s speech or news coverage of a major event \u2014will trigger an immediate \nreaction after exposure to the information . This immediate reaction is tied to hot \ncognition, which refers to cognition that is influenced by an individual \u2019s current \nemotional state (Abelson, 1963). Regarding  political news media coverage, Democrats \nand Republicans may harbor strong positive or neg ative emotions about certain policies. \nMotivated reasoning through hot cognition dictates that any time they are exposed to \nnews coverage on these hot -button issues, they will be triggered by their existing \nemotional response to have an automatic reaction either in favor or against the coverage. \nIndividuals are motivated to adhere to their beliefs, and this motivation heavily influences \ntheir cognition. One of the ways this motivation impacts individuals \u2019 thought processes \noccurs when they are exposed to in formation inconsistent with their beliefs.  \nCognitive Dissonance  \nThe theory of cognitive dissonance states that when individuals are  exposed to \ninformation inconsistent with their beliefs , they are motivated to resolve the discomfort \n10 \n \n that arises from this inconsistency  (Festinger, 1957) . People reduce cognitive dissonance \nthrough either changing one of the  inconsistent beliefs to agree with the other, integrating \nnew information that outweighs the dissonant belief, or decreasing  the importance of the \nbelief altogether. A  classic study on cognitive dissonance showed that individuals \nchanged their previous personal ratings of a menial task from \u201cboring\u201d to \u201cenjoyable ,\u201d \nwhen they were paid $1 to tell another group of participants th at the task they performed \nwas pleasant. They changed their personal ratings of the task to ensure their behaviors \nand beliefs were not in conflict with one another, demonstrating a desire to reduce \nfeelings of dissonance (Festinger & Carlsmith, 1959).  \n The effects of cognitive dissonance have  been shown in political contexts. One \nstudy of cognitive dissonance in a political context  asked participants to write counter -\nattitudinal essays about the two most recent U.S. Presidents (Nam  et al., 2013). \nSpecifically, c onservatives had to write a counter -attitudinal essay about Obama (liberal) \nand liberals had to write one about Bush (conservative). Conservatives were more likely \nto refuse to write the essay than liberals were , indicating that conservative s may be more \nguarded towards feelings of dissonance . In another study, p articipants were asked \nwhether they would like to hear arguments from  opposing sides of the debate on same-\nsex marriage and earn $10 or hear arguments that were compatible with their position on \nthe debate for only $7 (Frimer  et al., 2017). Motivation to avoid cognitive dissonance was \nevident in participants \u2019 responses, with 63% of the participants choosing to forgo the \nchance at the additional $3 given for hearing attitudes from the o pposing side of the \ndebate. Individuals are more comfortable taking in information that is consistent with \n11 \n \n their position, and liberals and conservatives are both inclined to avoid feelings of \ndissonance. Not only are individuals motivated to avoid inconsi stent information, but \nthey also seek out information that is consistent with their beliefs.  \nConfirmation Bias  \n People tend to seek out information that agrees with their beliefs , a phenomenon \ntermed \u201cconfirmation bias \u201d (Nickerson, 1998) . In a classic study of confirmation bias, \nparticipants created a sequence pattern of three numbers in a row. After being given an \nexample of  a sequence by the researcher , participants developed constrained  beliefs about \nthe rule, almost as if they developed  their own understanding of the rule. They then only \ncreated patterns that fit their understanding of the rule (Wason, 1960).  \nConfirmation bias is related to the  concept of  selective exposure, which refers to \nhow individuals tend to favor and seek out information that is consistent with their beliefs \nand intentionally avoid information that opposes them (Klapper, 1960). In the 1940 \nPresidential election,  more than 75% of voters had consumed media propaganda from \ntheir own party, whereas only 20% had consumed media propaganda from an opposing \nparty (Lazarsfeld  et al., 1948). Selective exposure is a way for individuals to avoid \ndissonance -inducing information.  Individuals are also more likely to spend time \ninteracting with news information that is consistent with their beliefs  than information \nthat goes against their beliefs , and this finding occurs  across cultures (Jonas et al., 2003; \nWesterwick  et al., 2013). Individuals are motivated to avoid or discredit information that \nis contrary to their beliefs and only accept agreeable information as fact  (Knobloch -\n12 \n \n Westerwick  et al., 2017). Individuals seek out information that confirms their beliefs, but \nthey also process incoming information in a biased way to fit their beliefs as well.  \nBiased Assimilation  \n The ways in which people intake and interact with new information often de pends \non their existing beliefs.  Biased assimilation refers to the tendency for  \nindividuals to interpret new information that supports their position more positively  \nthan information that does not (Lord  et al., 1979). Sometimes referred to as \u201cmotivated \nskepticism ,\u201d when assimilating information in a biased way, individuals tend to \nemphasize incoming information that supports their beliefs and weaken or undermine \narguments against it. Additionally, individuals \u2019 attitudes may become more polarized \nbecause of  this biased assimilation of information (Lord et al., 1979; Taber & Lodge, \n2006). The biased processing of information is reflective of individuals \u2019 ties to their \npreexisting beliefs, and the ways in which they defend or bolster them.  \n Biased assimilation of information exists in the political landscape, particularly in \nthe ways in which liberals and conservatives evaluate arguments from either side. \nConservatives and lib erals each read information relating to one of three political issues \n(i.e., abortion, illegal immigration, economic inequality) and viewed articles with \nopposing arguments as more negative than articles with supporting ones (Suhay & \nErisen, 2018). They we re also more inclined to provide counterarguments  when exposed \nto information from the opposing position. Both liberals and conservatives also perceived \ntheir candidate (i.e., Kerry, Bush) to be the winner of a 2004 Presidential debate, \nindicating they emp hasized or focus on supporting information to their position \n13 \n \n (Richardson et al., 2008). Both liberals and conservatives are inclined to process political \ninformation in a biased way and will emphasize information supporting their viewpoint \nand minimize or counter the information against it.  \nSummary  \n Cognitive dissonance suggests that individuals are motivated to resolve the \nfeelings of discomfort that arise from conflicting cognitions and beliefs. Confirmation \nbias suggests that individuals intentionally s eek out information that is consistent with \ntheir beliefs. Selective exposure is related to confirmation bias due to it being the act of \navoiding information that is disconfirming of one \u2019s beliefs. Biased assimilation indicates \nindividuals interact with incoming information in a way that supports their position, \nwhich includes highlighting supporting arguments and minimizing or dismissing the \nopposing ones. These terms all fit under the overall umbrella term of motivated \nreasoning, where people are driven  by preexisting beliefs to act in a certain way.  \n Each of these phenomena are evident in politics through the ways people interact \nand interpret information. Individuals avoid or dismiss information contrary to their \nbeliefs, actively seek out information  that confirms them, and interpret incoming \ninformation supporting their stance in a more positive light than information that goes \nagainst it. With the growing partisan divide on political issues, individuals are more \nmotivated than ever to act in ways th at support or defend their political stance. In this \nregard, people respond to  information in a way that aligns with their beliefs, and one of \nthese ways is by perceiving neutral information as biased . This tendency is the essence of \nthe hostile media effe ct. The strength of the hostile media effect is influenced by factors \n14 \n \n such as the  news source presenting the information, the perceived reach of the news \nsource, and the strength of an individual \u2019s partisan identity.  \nFactors Affecting the Hostile Media Effect \nNews Source  \nPerceptions of the news source contribute to the presence of the hostile media \neffect. Individuals who believe a particular news source to be partial to the opposing side \nmay perceive  bias in the story they present (Giner -Sorolla & Chai ken, 1994). Liberals \nand conservatives each viewed taped news stories from the Fox and CNN networks, and \nliberals perceived more bias in the Fox News network story , whereas conservatives \nperceived more bias in the CNN network story (Coe et al., 2008). Supporters of a home -\ntown sports team perceived more bias in a neutral news article about an NCAA \ninvestigation into their school when the article was presented with the rival team\u2019s \nnewspaper source heading than when the article was presented with the home -town \nteam\u2019s newspaper source heading (Arpan & Raney, 2003). Native Americans perceived \nmore hostile media bias in an article \u2013 written on the genetic modification of natively \ncultivated wild rice \u2013 from an opposing source than from a source that closely iden tified \nwith their position  (Gunther et al., 2009). The manipulations of the news source s \npresenting the article have elicited perceptions of bias against one \u2019s own stance.  \nBiased news source perceptions are also impacted by beliefs about the source \u2019s \ncredibility. Individuals perceive a message more favorably when it is presented from a \nnews source they deem to have \u201chigh credibility ,\u201d (Gunther, 1992) . Credibility attached \nto a source may depend on the agreement of the source\u2019s content with the individual\u2019s \n15 \n \n stance. Individuals tend to rely on the news sources they deem the most credible,  which is \ntypically the source that  aligns with their position , and this tendency leads to biased \nperceptions of news information when it is presented in a rival or oppositional source.   \nPerceived Reach of Source      \n  The perceived reach of the source \u2014how widespread and disseminated its \ninformation is \u2014may impact hostile media perceptions because it leads people to perceive \nthat others will be influenced  by the media message. People were more likely to perceive \nthe hostile media effect when information was presented from a \u201chigh reach \u201d news source \n(e.g., a prominent news source) than from a  \u201clow reach \u201d news source (e.g., a college \nessay; Gunther & Liebhart, 2006 ; Gunther & Schmitt, 2004).  Republicans perceived \nmore hostile media bias in a tweet when it came from an account with 503,000 followers \ncompared to when it was from an account with 21 followers (Lee  et al., 2018). This \nimpact is of greater importance when factoring in the thoughts and beliefs  of partisans. \nPartisans are more concerned with public issues that are of great importance, which are \nusually covered by large media sources, a nd so they may be more cognizant of the impact \nmass media has on popular opinion (Gunther & Liebhart, 2006). Perceived reach of a \nsource has considerable influence  on the hostile media effect  given that individuals may \nperceive more bias in news coverage p resented from a source with a large audience as \nopposed to one with a small audience .  \nPartisan Identity   \nThe strength of an individual \u2019s allegiance to their partisan side , which includes \nthe level of identification with their in -group as well as animosity towards the out -group, \n16 \n \n appear to be directly tied to their perceptions of hostile media bias.  Partisans with \nstronger in -group identification s are more likely to perceive information presented from \nan outgroup source more negatively  and are also more likely to denigrate the source \n(Daily, 2014).  Partisans may activate an ingroup -versus-outgroup schema when exposed \nto information from an opposing source that does  not align with their stance. These \neffects may be explained by self-categorization theory  (Turner et al., 1987) , which asserts \nthat a key component of one\u2019s self -concept is their identification with social groups \n(Reid, 2012 ). Partisans who had their identities activated by a precursor message to an \narticle perceived more media b ias against their political position than the control group, \nindicating a self -categorization effect (Reid, 2012). Partisans indicate less agreement , and \nmore bias,  with a source that does not align  with their political views  due to this source \nbeing identified with the outgroup.  \nHostile media perceptions are also stronger among individuals with a stronger \nidentification with their in -group (Ariyanto  et al., 2007; Matheson & Dursun, 2001).  \nIndividuals with strong in -group identificat ions with political positions seek out so -called \n\u201cvillains\u201d and \u201cvictims\u201d to distinguish between their in -group and outgroup  members. A \nstronger in -group identification may be activated when individuals are exposed to \npartisan issues that are more importan t to them. This in-group identification would in turn \nstrengthen their opinions and beliefs, causing them to become more extreme (Harton & \nLatan\u00e9, 1997).  Individuals who demonstrate a strong in-group identification are far more \nlikely to perceive hostile media bias in news coverage (Matheson & Dursun, 2001).  \n \n17 \n \n An individual \u2019s allegiance to a certain political position can impact how they \nperceive bias in the media, as well as its perceived in fluence on others. Individuals with \npartisan positions inconsistent with a partisan news source perceived more bias in the \nsource than those whose position was consistent with the source (Kim, 2016).  \nAdditionally, individuals voted more strongly against or  in favor of political initiatives, \nsuch as clean energy efforts, when the topics  were directly referenced by highly \ninfluential political leaders, such as the President (Crowe, 2020). A higher profile, or \nhigher reach, source may pose more of a threat to one\u2019s own partisan identity due to the \nincreased number of people that could be swayed by the source. I ndeed, individuals with \na stronger identity to their group are more likely to perceive bias in a news source not \naffiliated with their group. Individuals  who identify as nationalists perceived more hostile \nbias towards the U.S. in news stories from international news sources (Golan et al., \n2021). A stronger partisan identity may amplify these perceived media biases overall, as \nindividuals are more motivate d to defend their in -group from information that is \ndisseminated from opposing news sources.  \nThere may be differences between liberals and conservative s in how partisan \nidentity impacts their perceptions of hostile media bias. Conservatives  believe that t he \nmedia has more of a liberal spin and is thus biased against them ( Domke et al. , 1999). \nConversely, liberals may believe that news media has a conservative slant (Mayer, 2005). \nThe truth is that media bias exists on the part of both partisan political positions. The rise \nof 24-hour news networks has created platforms for both liberal and cons ervative-leaning \nnews sources to build their influence and reach. There are prominent conservative TV \n18 \n \n networks (e.g., Fox News) and liberal TV networks (e.g., CNN), and numerous online \npolitical sources (e.g., Breitbart, The Blaze, The Atlantic, Politico).  With the \ndissemination of a wide range of information, which may be biased towards a liberal or \nconservative viewpoint, it is crucial to study the hostile media effect and its relationship \nwith the reach of news sources. This research will help individual s recognize bias in the \nway information is presented, and researchers  develop strategies to counteract it.  \nCurrent Study \nThe hostile media effect refers to how individuals perceive even neutral news \ncoverage as biased against their stance  (Vallone et al.,  1985). This effect is a subset of the \nfield of motivated reasoning, with components such as cognitive dissonance, \nconfirmation bias, and selective exposure all providing a foundation for the phenomenon. \nThe hostile media effect  transpires in political con texts with perceptions  by Democrat s \nand Republican s of bias in media coverage , and can depend on the news source \npresenting the coverage  (Arpan & Raney, 2003; Gunther et al., 2009) , the perceived reach \nof the source  (Gunther & Schmitt, 200 4), and the strength of partisan identity in the \nindividual perceiving the coverage (Daily, 2014).  \nIn a previous study, I assessed level of perceived bias in an article depending on \nthe source heading (Sedlacek & Harton, 2019) . One hundred fifty  Amazon Mech anical \nTurk (mTurk)  participants read an article detailing the recent passage of the U.S. Farm \nBill with an NBC News heading (liberal source) or a Fox News heading (conservative \nsource) and rated the bias and quality of the article. Those who identified as conservative \nrated the article as low er quality when they were presented with the liberal source \n19 \n \n heading, and those who identified as liberal rated the article as lower quality when they \nwere presented with the conservative source heading. Political orientation and source did \nnot affect ratin gs of liberal/conservative bias in the article. Finding differences in \nperceived article quality based on the source indicated individuals recognized  the source. \nHowever, the finding of no  differences in perceived liberal or conservative bias  was \ncontrary to this idea and it needed to be examined in future research. I t is not clear \nwhether the results may have been affected by low power in some conditions or \nparticipants being unfamiliar with the NBC News header and its liberal stance (Sedlacek \n& Harton, 20 19).  \nThe current study used similar methodology to Sedlacek and Harton (2019), but \nalso assessed the moderation of news source reach on the relationship between an \nindividual \u2019s political orientation and their perceptions of bias. The current study was  a 2 \n(liberal/conservative source) x 2 (low/high perceived reach) x 2 (liberal/conservative \nparticipant) between groups factorial design.  \nThe current study address ed several gaps in hostile media effect research. There is \na need in hostile media effect resear ch to synthesize indicators of perceived bias such as \nagreement with the article and quality of the article. These variables are often assessed in \nseparate studies from one another, and the current study addresses each of them as \nindicators of perceived bi as. There is also a need in hostile media effect research to \nfurther address social media metrics (e.g., likes, shares, views) and how they impact \nperceptions of source reach , because consumption of news has shifted largely from print \nnews to websites and social media platforms. Specifically, it is important to assess \n20 \n \n whether these metrics can be manipulated in studies of perceived source reach, so that \nparticipants can identify whether a news source has a low or a high reac h based on these \nnumbers. Research conducted in the mid -2000s by Gunther and colleagues, that used \nprint news sources  (e.g., newspapers) , needs to be translated to today \u2019s digital news \nmedia society. Therefore, present day indicators of online news consump tion (i.e., social \nmedia metrics) should be applied to the methodology previously used in hostile media \neffect research to examine whether the  findings have changed.  \nThe current study addressed these gaps by investigating perceptions of the \narticle\u2019s quality and ratings of an individual \u2019s agreement with the content presented in the \narticle as indicators of perceived bias . Additionally, the current study investigate d how \nsocial media metrics that are often included on online news articles directly influence  \nindividual perceptions of the reach of the source. Participants read an article with a liberal \n(i.e., The Progressive, CNN) or conservative (i.e., PatriotNewsDaily, Fox) news source. I \nmanipulated perceived reach by having participants read an article wit h a smaller -\naudience source heading (i.e., The Progressive, PatriotNewsDaily) or a larger -audience \nsource heading (i.e., CNN, Fox). I also manipulated perceived reach using the social \nmedia metrics  of article views, article shares, and website subscriptions . Social media \nmetrics are a more modern indicator of perceived reach beyond traditional print media \nsources (Stavrositu & Kim, 2014).  All \u201chigh-reach\u201d sources had article views and website \nsubscription numbers over 1,000, whereas the \u201clow-reach\u201d sources had all of their social \nmedia metrics under 1,000. Two groups (i.e., liberal, conservative) were compared in \nregards to their perceptions of hostile media bias, their perceptions of article quality , and \n21 \n \n their agreement with the article. Partisan identity \u2019s relationships with the three dependent \nvariables were assessed in exploratory analyses, given that the strength of an individual \u2019s \npartisan identity has been previously associated with perceptions  of bias (Ariyanto et al., \n2007; Matheson & Dursun, 2001 ; Reid, 2012 ). \nI also evaluated perceived objectivity versus subjectivity of each article as well as \nthe perceived informativeness of the article; however, these evaluations were conducted \nin an exploratory manner. I evaluated r atings of objectivity versus subjectivity on a slider \nasking participants whether the article contained \u201cmostly fact -based\u201d or \u201cmostly opinon -\nbased\u201d content. I evaluated r atings of informativeness by assessing the strength of \nparticipants \u2019 agreement with how informative the article was.  There are several websites \nthat employ independent researchers to dissect media content of various political sources \nin order to most accurately depict their partisan slant to concerned news con sumers. One \nof these sources, mediabiasfactcheck.com, was used in order to identify the source \ncontent adapted for the current study.  The following hypotheses  and research questions  \nwere tested in the current  study:  \n\u2022 H1: Conservatives will rate an article as more biased against conservatives \nwhen presented  with a liberal news source heading. Liberals will rate an \narticle as more biased against liberals when presented with a conservative \nnews source heading.    \n\u2022 H2: Conservatives will rate an article as lower quality when presented \nwith a liberal news source heading, wh ereas liberals will rate an article as \nlower quality when presented with a conservative news source heading.  \n22 \n \n \u2022 H3: Conservatives will indicate more agreement with an article when it is \npresented with a conservative news source heading than when it is \npresented with a liberal news source heading. Liberals will indicate more \nagreement  with an article when it is presented with a liberal news source \nheading than when it is presented with a conservative news source \nheading.  \n\u2022 H4: Perceived reach of the news source will moderate these effects , such \nthat ratings of th e article will be more negative  when conservatives view \nthe article with the high -reach liberal news source heading (CNN) than \nwhen they view the article with the low -reach liberal news source heading \n(The Progressive). Likewise, ratings of the article will be more negative  \nwhen liberals view the article with the high -reach conservative news \nsource heading (Fox News) than when they view the article with the low -\nreach conservative news source heading (PatriotNewsDaily).  \nIn exploratory analyses, I assessed partisan identity \u2019s correlation with the three dependent \nvariables (ratings of article bia s, quality, and agreement with the article)  using the \nPartisan Identity Scale (Huddy & Bankert, 2017 ).    \nPilot Study  \nI conducte d a pilot study  to test a number of important aspects for the primary \nstudy. The pilot study assessed the perceived neutrality of the two different articles used \nin the primary study (i.e., U.S. Farm Bill article, military housing article), the perceived \nliberal or conservative stances of sources, and perceived reach  of these sources. \n23 \n \n Additionally, this tested whether the sources were perceived to have a \u201chigh\u201d or \u201clow\u201d \nreach. The results of the pilot study determined which article was used in the primary \nstudy, and which four source headings were manipulated with tha t article.  \nMethod \nParticipants . I recruited 90 participants (59% Male, 39% Female; Age M = \n24.22, SD = 14.05; 81.1 % White/Caucasian, 7.8% Hispanic/Latin x, 6.7% Black/African \nAmerican, 4.4% Asian/Pacific Islander , 1.1% Multiracial; 61.6% liberal, 31.1% \nconservative, 7.8% Moderate)  through Amazon Mechanical Turk  for the pilot study. I set \nCloudResearch  to block duplicate IP addresses and suspicious or duplicate geocodes. I \nrequired participants  to have a 99% hit approval rate and between 5,000 -500,000 \napproved hits. I limited the sample  to US citizens only. Additionally, I use d the \nCloudResearch  panels to only include people in my study who identified as Democrat or \nRepublican  in their political views , and exclude those who identified as Independent . I \ncompensated participants $0.50 for completing the study.  \nProcedure . Participants either read an article on the U.S. Farm Bill or a military \nbill with low or high reach metrics. These articles were adapted from articles published \non Reuters.com. This website w as determined to have little to no partisan bias, and more \nmoderate reporting overall  (mediabiasfactcheck.com ). The articles were modified to \nensure they were as neutral as possible, with the number of liberal and conservative \narguments being balanced . Specifically, there were three arguments on both sides in the \nU.S. Farm Bill article, and one argument on each side in the military housing bill article. \nThese articles had no source headings , and low versus high reach was manipulated \n24 \n \n through using differe nt social media metrics at the bottom of each article. The low-reach \narticles had less than 5,000 views, less than 1,000 shares and less than 1,000 website \nsubscriptions. The high -reach articles had more than 1,000,000 views, more than 100,000 \nshares, and more than 1,000,000 website subscriptions.  \nAfter reading the article,  participants completed a questionnaire  assessing their \nperceptions of the article quality and their perceptions of  liberal or conservative bias in \nthe article content itself  (see Append ix A).   \nThen, participants completed a manipulation check that was multiple choice, \nasking them to estimate the number of views the article had . This item assessed  whether \nthe participants  paid attention to the social media metrics at the bottom of the ar ticle (see \nAppendix B). Participants also completed an item assessing whether they agreed the \narticle was informative or not  and an item asking them to indicate whether the article was \nfact-based or opinion -based (see Appendix A). \nParticipants then rated their perceptions of the political stances (liberal or \nconservative) , their ratings of the size of the audience (i.e., \u201cSmall\u201d, \u201cMedium -sized\u201d, \n\u201cLarge\u201d), and their ratings of trustworthiness  of several news outlets (e.g., Fox News, \nCNN, Wall Street Journal ) (See Table 1) . The questionnaire also included an adapted \nform of the Partisan Identity Scale (Huddy & Bankert, 2017).  This scale was created to \nmeasure partisanship as a social identity, and the converging of one\u2019s partisan ident ity \nwith their sense of self  (see Appendix C).  \n25 \n \n Results and Discussion  \nAn independent samples t-test revealed participants who read the article on the \nU.S. Farm Bill indicated that it was closer to moderate  politically  (M = 3.12, SD = 1.41) \nthan those who read the article on the military housing bill ( M = 2.17, SD = 1.25; 1-7 \nscale). There were 16 participants who were not able to identify the article they just read. \nRoughly 39% of all participants had stated the number of views in t he article they read \nwas in the smaller range of 1,000 -100,000. Due to a coding error, I was not able to \nidentify the differences between low -reach and high -reach views manipulations. In open-\nended responses, participants indicated they had to guess on the  number of views the \narticle had. Therefore, in the main study I moved the metrics to the top of the article \ninstead of the bottom in order to make them more noticeable for the reader. I also used \nthe U.S. Farm Bill as the article I manipulated with differ ent headings since participants \nperceived it to be more neutral than the military housing bill arti cle.  \n  \n26 \n \n Table 1  \nArticle Bias Ratings, Perceived Source Audience Size, and Perceived Article Views \n(Percentages)  \nBias Ratings  CNN PatriotNewsDaily  Fox News  The Progressive  \nLiberal 32.4 4.5 2.7 30.6 \nSlightly Liberal  22.5 3.6 4.5 17.1 \nNeutral 14.4 26.1 3.6 27 \nSlightly Conservative  11.7 23.4 14.4 5.4 \nConservative  5.4 28.8 60.4 7.2 \nMean (1-5 Scale) 2.25 3.79 4.46 2.33 \n \nAudience Size CNN PatriotNewsDaily  Fox News  The Progressive  \nSmall Audience  9.9 67.6 8.1 63.1 \nMedium-sized Audience  6.3 10.8 9.9 16.2 \nLarge Audience  66.7 4.5 65.8 5.4 \n \nNumber of \nViews Less \nthan \n1,000 1,000-\n100,000 100,000-\n500,000 500,000-\n1,000,000  More than \n1,000,000  Mean \n(1-5 \nscale)  \n 9.9 38.7 15.3 4.5 18 2.79  \n \nTests of perceived  liberal or conservative bias showed that 63% of the partic ipants \nidentified CNN as having a liberal bias  and 74.8% of participants identified Fox News as \nhaving a conservative bias. This demonstrated that the two largest mainstream media \nsources were perceived to have their liberal and conservative stances , respectively. The \ntwo smaller -scale sources that had the highest percentages of perceived bias in the liberal \nor conservati ve direction were The Progressive and PatriotNewsDaily, with 56% of \n27 \n \n participants identifying The Progressive as having a liberal bias , and 52.2% of \nparticipants identifying PatriotNewsDaily as having a conservative bias.  \nIn tests of the perceived source re ach, both conservative sources were identified \nas having their correct reach , with 67.6% of participants identifying PatriotNewsDaily as \nhaving a small audience and 65.8% of participants identifying Fox News as having a \nlarge audience . Similar results were  found with the two liberal sources, with 76% \nviewing The Progressive  as having a small audience and 80% of participants identifying \nCNN as having a large audience.  These findings led me to use The Progressive as the \nlow-reach liberal source, CNN as the hi gh-reach liberal source, PatriotNewsDaily as the \nlow-reach conservative source, and Fox News as the high -reach conservative source  in \nthe primary study.   \nMethod \n All materials used in the procedure  and the planned analyses  were preregistered \non the Open Science Framework website : \nhttps://osf.io/atjzf/?view_only=330a3b87c6b942fca63c9f2648985a70  (Open Science \nCollaboration, 2015).    \nDesign \n This study used  a 2 (political affiliation of news source ; liberal/conservative ) x 2 \n(perceived reach of news source ; Low/High ) x 2 (individual\u2019s political affiliation ; \nliberal/conservative ) between groups factorial design .  \n28 \n \n Participants   \n Three hundred sixty  participants (42.9% Male, 56% Female; Age M = 28.28, SD \n= 13.52; 77.3% White/Caucasian, 10. 2% Black/African American, 7. 8% Asian/Pacific \nIslander, 4. 7% Hispanic/Latin x, 1.7% Multiracial; 59.6% liberal, 34.6% conservative, \n5.3% Moderate)  were recruited using CloudResearch , an online crowdsourcing platform , \nto participate in a study assessing their evaluations of the quality of news articles . \nCloudResearch  was set to block duplicate IP addresses and suspicious or duplicate \ngeocodes. Participants were required to have a 99% hit approval rate and between 5,000 -\n500,000 approved hit s. The sample was limited to US citizens only. Like the pilot study, I \nused the CloudResearch  panels to only include people in my study who identified as \nDemocrat or Republican  in their political views , and exclude those who identified as \nIndependent . Participants were compensated $0.50 for their completion of the study.   \nAfter all data cleaning and the exclusion of participants who identified as having a \nmoderate political orientation from analyses, 143 participants (38.4% Male, 60.9% \nFemale; Age M = 28.74, SD = 13.59; 76.8% White/Caucasian, 10. 6% Black/African \nAmerican, 7. 3% Asian/Pacific Islander, 4. 6% Hispanic/Latin x, 3.3% Multiracial; 61.6% \nliberal, 33.1% conservative)  were included in data analyses.  \nI determined a suggested sample  size of 360 participants through conducting a \npower analysis using G*power (Faul et al., 2007). Gunther and Liebhart\u2019s (2006) study \nassessing perceived reach of source\u2019s moderation of the hostile media effect yielded an \neffect size of partial \uf0682=.01. However, Gunther et al., (2009) yielded an effect size of \npartial \uf0682=.02. Given this discrepancy, and that multiple studies similar to my design \n29 \n \n have yielded results in a range of small to medium effects, I decided to split the \ndifference between a small effect size ( partial \uf0682=.01) and medium effect size ( partial \uf0682= \n.025) and inputted the effect size partial n2 =.015 into my power analysis. This effect size \nwas used to find the desired sample size for a two-tailed ANOVA assessing group \ndifferences between liberals and conservatives , depending on the political orientation of \nthe source and the source reach. The desired power in this power analysis calculation was \n.90 with an alpha level set at .05 , number of groups set at 8, and a numerator df set at 1. \nThe power analysis  yielded a recommended sample size of 327 participants .  \nProcedure  \n Participants  first read an electronic consent form , and then read that the study \nwould assess their evaluations of the quality of news information. They were randomly \nassigned to read an article about the recently passed U.S. Farm Bill, with either a Fox \nNews heading used from their site, foxnews.com (high-reach conservative condition), \nCNN news heading used from their site, cnn.com (high-reach liberal condition), \nPatriotNewsDaily  news heading used from their site, patriotnewsdaily.com (low-reach \nconservative condition), or The Progressive  news heading from their site, progressive.org \n(low-reach liberal condition ; see Appendix D). Although  the legislation exists and did \npass, some details about the bill in the article were falsified, such as the impact the bill \nhad on food stamps rec ipients. Specifically, the number of arguments that would be \nclassified as more liberal were balanced with the number of arguments that would be \nclassified as conservative ( see Appendix E). This manipulation was done in order to \nbalance the number of liberal and conservative arguments in the article  and contribute to \n30 \n \n its overall neutrality. In addition, the low -reach news source conditions included smaller \nsocial media metrics (i.e., 4,754 views, 3 46 shares, 897 website subscriptions) , whereas \nthe high-reach news source conditions included larger social media metrics (i.e., \n3,859,567 views, 302,781 shares, 5,067,435 website subscriptions).  The survey also \nrecorded the time participants spent reading the article .  \n Participants then complete d a questionnaire assessing the ir perceptions of  the \narticle along with two  manipulation check s. The items assessing participants\u2019 perceptions \nof bias against a conservative or liberal viewpoint were adapted from similar items used \nin Gunther and Liebhart (2006). Then, participant s completed a questionnaire addressing \ntheir political orientation, level of partisan identity , and basic demographic questions.  I \nassessed partisan identity using an adapted subscale of the Partisan Identity Scale created \nby Huddy and Bankert (2017). Participants completed  two open-ended response items \nasking them to write what the article was about in their own words and whether they had \ncomments for  the researchers.  \nI debriefed pa rticipants on the true focus and goals of the study  and indicated to \nthem that the article was falsified in order to accomplish the study\u2019s goals. I included this \ndebriefing on the \u201cend of survey\u201d screen and then directed p articipants  out of the survey \nafter completion  (see Appendix F ).  \nMeasures \nArticle bias against conservative viewpoint  and liberal viewpoint items. Two \nitems adapted from those used by Gunther and Liebhart (2006)  assessed perceived bias  \non a 7-point Likert scale  (1=Strongly Disagree and 7=Strongly Agree). I analyzed these \n31 \n \n items separately due to there being a low correlation between the two items ( r = -.27). \nThe first item assessed perceived article bias against conservatives : \u201cI feel the article was \nbiased against a conservative  viewpoint.\u201d The second item on perceived bias assessed \nperceived article bias against liberals: \u201cI feel the article was biased against a liberal \nviewpoint.\u201d (see Appendix A).  \nQuality of article item.  One item assess ed perceived quality of the article  on a 7-\npoint Likert scale  (1=Very poor quality  and 7=Very high quality ): \u201cPlease rate the quality \nof the article you just read.\u201d (see Appendix A).  \nAgreement item. One item assess ed agreement with the article \u2019s content  on a 7-\npoint Likert scale  (1=Strongly Disagree  and 7=Strongly Agree ): \u201cPlease indicate the \nextent to which you agree with the content of the article you read.\u201d (see Appendix A).  \nFact-Based versus Opinion -Based Item. One item assessed  the level of \nperceived  fact-based information or opinion based information  in the article  on a graphic \nslider from 1 -100, with the bar starting in the middle of the slider for the participants to \ndrag to either side : \u201cOn the slider below, please indicate the extent to which you thought \nthe article you read contained opinion -based information or fact -based information\u201d  (see \nAppendix A).  \nInformative Item.  One item assessed the informativeness of the article on a 7-\npoint Likert scale  (1=Strongly Disagree  and 7=Strongly Agree ): \u201cPlease indicate the \nextent to which, if any, you agree with the following statement: The article was \ninformative\u201d  (see Appendix A).  \n32 \n \n Manipulation chec k item. One item served as  a perceived reach manipulation \ncheck on a 7-point Likert scale (1= hardly any people  and 7=extremely large number of \npeople): \u201cOn a scale of 1 -7, with 1 meaning hardly any people  and 7 meaning an \nextremely large number of people , indicate the size of the message\u2019s audience \u201d (see \nAppendix B). \nAttention check items.  Participants completed two attention checks throughout \nthe study, with the first one as follows: \u201cPlease identify the source of the a rticle you just \nread.\u201d Participants chose from a list of five sources to answer this check. The second \nattention check w as the following:  \u201cIn your own words, please describe what you thought \nthe article you read was about. \u201d This item was an open-ended response item with a \ncomment box. (see Appendix B).  \nPartisan identity scale. I asked participants to indicate the political party, if any, \nthey identif ied most with, and to choose from the following options: Democrat ( liberal), \nRepublican ( conservative), Inde pendent, Green Party, Libertarian, No Affiliation, and \nOther (please specify) with a comment box. They completed  four items in an adapted \nsubscale of the Partisan Identity Scale, which assess ed participants\u2019 level s of partisan \nidentity relative to the party they indicated they identified the most with (Huddy & \nBankert, 2017). This scale has greater predictive validity than single -item assessments of \npartisan identity. These items were answered on a 7 -point Likert scale  (1=strongly \ndisagree and 7=strongly agree ). An example of an item on this subscale is , \u201cWhen I \nspeak about thi s party, I usually say \u2018we\u2019 instead of \u2018they\u2019\u201d (\uf061=.85; see Appendix C).  \n33 \n \n Demographic questions . Participants answered  basic demographic questions \nincluding gender, race/ethnicity, and educational attainmen t (see Appendix G). \nHonesty check.  I asked participants the following question:  \u201cHow honest were \nyou in answering the questions on this survey? You will receive compensation regardless \nof your answer\u201d ( see Appendix G).  \nComment box.  I asked participants if they had any  additional comments for the \nresearchers, and I provided them  a text box for their answer.  \nPlan of Analys is \n To test hypotheses 1  to 3, I conducted four ANOVAs to compare the differences \nbetween liberals and conservatives , depending on the political orientation of the source \n(i.e., liberal, conservative) , on the four dependent variables (i.e.,  ratings of bias against a \nconservative viewpoint, ratings of bias against a Liberal viewpoint, ratings of article \nquality, ratings of agreement with the article\u2019s content ). To test hypothesis 4 , I did a \nsecond set of ANOVAs adding perceived reach (i.e., low, high) as an additional \nindependent variable. Because of unequal cell sizes, I chose to run two separate sets of \nanalyses because of potential differences in results between the 2 x 2 and the  2 x 2 x 2 \nANOVA. In itially, I had pre-registered that I would use the Process macro (Hayes, 2013) \nto test moderation, but because of program limitations and because all variables were \ncategorical, I did ANOVAs instead. I conducted exploratory analyses to assess the \nwithin-cell correlations between each of the four dependent variables .  \n34 \n \n Results \nData Cleaning  \n Based on pre -registration criteria, I excluded 199 cases due to participants  failing \nthe attention check  asking them to identify the source they read . Participants did not fail \nthe other attention check (i.e., open -ended response describing what the article was \nabout). I excluded  9 cases due to participants not meeting the three -minute minimum time \nrequirement , and 2 cases due to participants indicatin g they were \u201cnot at all honest\u201d or \n\u201cslightly honest\u201d on the honesty check. After these exclusions, there were 151 remaining \ncases.  \nTo conduct the analyses comparing the differences between liberals and \nconservatives, participants were coded as \u201c1 = conservative\u201d if they identified their \npolitical orientation anywhere from 1 -3 (1=extremely conservative  and 3=lean \nconservative) and \u201c2 = liberal\u201d if they identified their political orientation anywhere from \n5-7 (5=lean liberal  and 7=extremely liberal ). Participants who identified themselves in \nthe middle (i.e., 4= moderate ) were excluded from analyses. There were 8 participants \nwho identified themselves as moderates. After exclusion of these cases  and all other \nexclusions, there were 143 participants included in the analyses.  \nTests of Assumptions  \n Due to unequal n\u2019s in each condition, I used Bartlett\u2019s test of homogeneity to test \nthe assumption of equal variances. The assumption of homogeneity of variance was not \nviolated f or article bias ratings against a conservative viewpoint based on political \norientation ( \uf063\uf032= .14, p = .71) or source reach ( \uf0632 = 2.02, p = .16), for article bias ratings \n35 \n \n against a liberal viewpoint based on political orientation ( \uf0632 = .055, p = .81) or source \nreach (\uf0632 = .12, p =.73), for article quality ratings based on political orientation ( \uf0632 = \n2.41, p =.12) or source reach ( \uf0632  = .03, p = .86), or for agreement with article\u2019s content \nbased on political orientation ( \uf0632 = 1.23, p = .27) or source reach ( \uf0632 = .665, p =.42). The \nassumption of homogeneity of variance was not violated for article bias ratings against a \nconservative viewpoint based on source political orientation ( \uf0632 = .03, p = .86), for article \nbias ratings against a Liberal vie wpoint based on source political orientation ( \uf0632 = .00, p \n= .99), for article quality ratings based on source political orientation ( \uf0632 = 1.32, p = .25), \nor for agreement with the article\u2019s content based on source political orientation ( \uf0632 = .09, \np = .77).  \nManipulation check  \n To test the effectiveness of the manipulation, I conducted an independent samples \nt-test to assess the differences in participants\u2019 perceived number of views of the article \nthey read. Participants perceived the article in the high-reach condition ( M = 3.60, SD = \n1.48)  as having a larger number of views than the article in the low -reach condition ( M = \n1.97, SD = .55), suggesting the manipulation was valid , t(141) = -8.58, p < .001, d = 1.46, \n95% CI: [1.114, 1.857].  \n Additionally, I set a timer to check how much time participants spent reading the \narticle. On average, participants included in analyses spent 576 seconds (about 10 \nminutes) reading the article in the study. There were 9 participants excluded from \nanalyses due to viewing the article for less than 180 seconds (3 minutes). For participants \nincluded in analyses, the shortest amount of time spent reading the article was 194 \n36 \n \n seconds (a little over 3 minutes), while the longest amount of time spent was 7,433 \nseconds (a little over 2 hours).  \nHypothesis Testing    \nHypothesis 1.  The first ANOVA assessed differences between liberals and \nconservatives on article bias ratings against a conservative viewpoint, depending on the \nsource political orientation . Participants who were politically conservative rated the \narticle as being more biased against conservatives (M = 3.50, SD = 1.71) than did liberals  \n(M = 2.58, SD = 1.28), F(1, 143) = 6.539, p = .012, partial \uf0682 = .05, 95% CI: [.002, .124]. \nThere was not a main effect of sour ce political orientation , F(1, 143) = 1.511, p = .22, \npartial \uf0682 = .01. There was not a significant interaction between source political \norientation and the participant political orientation , F(1, 143) = .143, p = .71, partial \uf0682 = \n.001 (see Figure 1) .  \nThe second ANOVA assessed differences between liberals and conservatives on \narticle bias ratings against a liberal viewpoint, depending on the source political \norientation . Participants who were politically liberal rated the article as be ing more biased \nagainst liberals (M = 3.83, SD = 1.65) than did conservatives (M = 2.64, SD = 1.43), F(1, \n143) = 4.586, p = .034, partial \uf0682 = .03, 95% CI: [0, .105]. Participants who read the \narticle with a conservative source heading rated the article as  being more biased against \nliberals (M = 3.55, SD = 1.56) than those who read the article with a liberal source \nheading (M = 2.91, SD = 1.56), F(1, 143) = 5.037, p = .026, partial \uf0682 = .04, 95% CI: [0, \n.109]. There was not a significant interaction between  source political orientation and \nparticipant political orientation , F(1, 143) = .417, p > .05, partial \uf0682 = .003 (see Figure 2). \n37 \n \n Overall, the hypothesis that conservatives will perceive more bias in an article with a \nliberal source heading, and that liberals would do so when the article is presented with a \nconservative source heading, was not supported.  \n \nFigure 1. Article Bias Ratings Against a Conservative  Viewpoint.  \nMean Article Bias Ratings Against a Conservative Viewpoint Depending on Source \nPolitical Orientation  \n \n \n \n  1234567\nconservative (n = 22) liberal (n = 46) conservative (n = 28) liberal (n = 47)\nliberal source conservative sourceMean article bias ratings\nNote. Error bars represent standard error of each value. \u201cConservative\u201d and \u201cliberal\u201d labels \nrefer to participant political orientation unless otherwise specified.  Higher ratings indicate \nstronger perceptions of bias.  \n \n38 \n \n Figure 2. Article Bias Ratings Against a Liberal Viewpoint.  \nMean Article Bias Ratings Against a Liberal Viewpoint Depending on Source Political \nOrientation  \n \n \n \n \nHypothesis 2.  The third ANOVA assessed differences between liberals and \nconservatives on article quality ratings, depending on the source political orientation. \nThere was  not a main effect of source political orientation , F(1, 143) = .009, p = .93, \npartial \uf0682  = .000. There was not a main effect of participant political orienta tion, F(1, \n143) = .061, p = .81, partial \uf0682 = .000. There was  not a significant interaction between \nsource political orientation and participants \u2019 political orientation , F(1, 143) = .378, p = 1234567\nconservative (n = 22) liberal (n = 46) conservative (n = 28) liberal (n = 47)\nliberal source conservative sourceMean article bias ratings\nNote. Error bars represent standard error of each value. \u201cConservative\u201d and \u201cliberal\u201d labels \nrefer to participant political orientation unless otherwise specified.  Higher ratings indicarte \nstronger perceptions of bias.  \n \n39 \n \n .54, partial \uf0682  = .003 (see Figure 3). Overall, the hypothesis that conservatives will rate \nan article as lower quality when it is presented with a liberal source heading, and that \nliberals would do so when the article is presented with a conservative source heading, \nwas not supported.   \n \nFigure 3. Article Quality Ratings  \nMean Article Quality Ratings Depending on Source Political Orientation  \n \n \n \nHypothesis 3.  The fourth ANOVA assessed differences between liberals and \nconservatives on agreement with the article\u2019s content, depending on the source political \norientation. There was a main effect of participant political orientation, as p articipants 1234567\nconservative (n = 22) liberal (n = 46) conservative (n = 28) liberal (n = 47)\nliberal source conservative sourceMean article quality ratings\nNote. Error bars represent standard error of each value. \u201cConservative\u201d and \u201cliberal\u201d labels \nrefer to participant political orientation unless otherwise specified.  Higher ratings indicate \nhigher perceived article quality.  \n \n40 \n \n who were politically conservative had higher ratings of agreemen t (M = 4.58, SD = 1.05) \nwith the article than did liberals (M = 4.11, SD = 1.21) regardless of the source political \norientation , F(1, 143) = 5.240, p = .024, partial \uf0682 = .04, 95% CI: [0, . 111]. There was \nnot a main effect of source political orientation , F(1, 143) = .487, p = .49, partial \uf0682 = \n.003. There was not a significant interaction between source political orientation and \nparticipant political orientation , F(1, 143) = 1.001, p = .32, partial \uf0682 = .007 (see Figure \n4). Overall, the hypothesis that conservatives will indicate more agreement with an article \nwhen it is presented with a conservative source heading rather than a liberal source \nheading, and that liberals would do so when it is presented with a liberal source h eading \nversus a conservative one, was not supported.  \n  \n41 \n \n Figure 4. Agreement with Article content  \nMean Ratings of Agreement with Article Content Depending on Source Political \nOrientation  \n \n \n \nHypothesis 4.  The first moderation  model was a 2 x 2 x 2 ANOVA with \nparticipant political orientation , source political orientation, and source reach  as the \nindependent  variables and article bias ratings against a conservative viewpoint as the \ndependent variable in the analysis. Participants who were politically conservative rated \nthe article as being more biased against conservatives ( M = 3.50, SD = 1.71) than did \nliberals (M = 2.58, SD = 1.28), F(1, 143) = 6. 246, p = .014, partial \uf0682 = .04, 95% CI: \n[.002, .121]. The interaction between participant political orientation , source political 1234567\nconservative (n = 22) liberal (n = 46) conservative (n = 28) liberal (n = 47)\nliberal source conservative sourceMean agreement ratings\nNote. Error bars represent standard error of each value. \u201cConservative\u201d and \u201cliberal\u201d labels \nrefer to participant political orientation unless otherwise specified.  Higher ratings indicate \nstronger agreement with the article\u2019s content.  \n \n42 \n \n orientation, and news source reach was not significant  (see Table 2) . Contrary to the \nhypothesis,  this result did not identify news source reach as a moderator of the \nrelationship between participant poli tical orientation , source political orientation  and \narticle bias ratings against a conservative viewpoint.  \n \nTable 2 \nResults of Moderation of News Source Reach on Relationship Between Source PO, \nParticipant  PO, and Article Bias Ratings Against a Conservative Viewpoint \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  7 3.420 1.787 .095 .085 \nIntercept 1 1132.005 591.704 .000 .814 \nSourcePO  1 2.368 1.238 .268 .009 \nSourceReach  1 6.793 3.551 .062 .026 \nPartPO 1 11.949 6.246 .014* .044 \nSourcePO*Source Reach  1 .735 .384 .536 .003 \nSourcePO*PartPO  1 .574 .300 .585 .002 \nSourceReach*PartPO  1 .096 .050 .823 .000 \nSourcePO*SourceReach*PartPO  1 .164 .086 .770 .001 \nNote: * = correlation is significant at the .05 level. PO = political orientation.   \n \nThe second moderation  model included participant political orientation, source \npolitical orientation, and source reach as the independent  variables and article bias ratings \nagainst a liberal viewpoint as the dependent variable in the analysis . There was a main \neffect of participant political orientation, as participants who were politically liberal rated \nthe article as being more biased against liberals (M = 3.44, SD = 1.67) than did \nconservatives ( M = 2.88, SD = 1.37) regardless of the source political orientation , F(1, \n43 \n \n 143) = 4.751, p = .031, partial \uf0682 = .03, 95% CI: [0, .106]. Liberals rated the conservative \nsource heading as more biased ( M = 3.83, SD = 1.65) against lib erals than did \nconservatives  (M = 3.07, SD = 1.30), F(1, 143) = 4.568, p = .034, partial \uf0682 = .03, 95% \nCI: [0, .104]. The interaction between participant political orientation, source political \norientation, and news source reach was not significant ( see Table 3). Contrary to the \nhypothesis, t his result did not identify news source reach as a moderator of the \nrelationship between participant political orientation  and source political orientation on \narticle bias ratings against a liberal viewpoint.  \n \nTable 3 \nResults of Moderation of News Source Reach on Relationship Between Source PO, \nParticipant PO, and Article Bias Ratings Against a Liberal Viewpoint \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  7 5.411 2.279 .032 .106 \nIntercept 1 1244.571 524.139 .000 .795 \nSourcePO  1 10.847 4.568 .034* .033 \nSourceReach  1 .580 .244 .622 .002 \nPartPO 1 11.282 4.751 .031* .034 \nSourcePO*Source Reach  1 .027 .011 .916 .000 \nSourcePO*PartPO  1 .809 .341 .560 .003 \nSourceReach*PartPO  1 8.190 3.449 .065 .025 \nSourcePO*SourceReach*PartPO  1 1.803 .759 .385 .006 \nNote: * = correlation is significant at the .05 level. PO = political orientation.  \n \nThe third moderation  model tested participant political orientation, source \npolitical orientation, and source reach as the independent variables and article quality \n44 \n \n ratings as the dependent  variable in the analysis. The interaction between participant \npolitical orientation, source politica l orientation, and news source reach was not \nsignificant ( see Table 4). Contrary to the hypothesis, this result did not identify news \nsource reach as a moderator of the relationship between participant political orientation, \nsource political orientation an d article quality ratings.  \n \nTable 4 \nResults of Moderation of News Source Reach on Relationship Between Source PO, \nParticipant PO, and Article Quality Ratings \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  7 .241 .196 .986 .010 \nIntercept 1 2454.694 1994.407 .000 .937 \nSourcePO  1 7.475E-5 .000 .994 .000 \nSourceReach  1 .019 .015 .901 .000 \nPartPO 1 .083 .067 .796 .000 \nSourcePO*Source Reach  1 .260 .211 .646 .002 \nSourcePO*PartPO  1 .440 .357 .551 .003 \nSourceReach*PartPO  1 .365 .297 .587 .002 \nSourcePO*SourceReach*PartPO  1 .378 .307 .580 .002 \nNote: * = correlation is significant at the .05 level. PO = political orientation.  \n \nThe fourth moderation  model tested participant political orientation, source \npolitical orientation, and source reach as the independent variables and ratings of \nagreement as the dependent variable in the analysis. Participants who were politically \nconservative  indicated higher ratings of agreement with the article  (M = 4.58, SD = 1.05) \nthan did liberals (M = 4.11, SD = 1.21) regardless of the source political orientation , F(1, \n45 \n \n 143) = 4.865, p = .029, partial \uf0682 = .04, 95% CI: [0, .108]. The interaction between \nparticipant political orienta tion, source political orientation, and news source reach was \nnot significant (See Table 5). Contrary to the hypothesis, t his result did not identify news \nsource reach as a moderator of the relationship between participant political orientation, \nsource political orientation and ratings of agreement.  Overall, the hypothesis that news \nsource reach would moderate the relationship between participant PO and article bias \nratings, article quality ratings, and ratings of agreement, was not supported.  \n \nTable 5 \nResults of Moderation of News Source Reach on Relationship Between Source PO, \nParticipant PO, and Ratings of Agreement \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  7 2.231 1.666 .122 .080 \nIntercept 1 2420.681 1808.018 .000 .931 \nSourcePO  1 .558 .417 .520 .003 \nSourceReach  1 .111 .083 .774 .001 \nPartPO 1 6.514 4.865 .029* .035 \nSourcePO*Source Reach  1 .377 .282 .596 .002 \nSourcePO*PartPO  1 1.344 1.004 .318 .007 \nSourceReach*PartPO  1 1.458 1.089 .299 .008 \nSourcePO*SourceReach*PartPO  1 2.530 1.890 .171 .014 \nNote: * = correlation is significant at the .05 level. PO = political orientation.  \n \nExploratory Analyses  \nIn exploratory analyses, I conducted within -cell correlations between each of the \ndependent variables (i.e., bias ratings against a liberal viewpoint, bias ratings against a \n46 \n \n conservative viewpoint, article quality ratings, article agreement ratings) within  each of \nthe eight conditions. Article quality ratings had a strong positive association  with article \nagreement ratings  in the Low -reach liberal/conservative PO condition ( r = .736, p = \n.006). Those who rated the article as having a higher quality also ind icated higher ratings \nof agreement with the article. Article bias ratings against a liberal viewpoint had a strong \nnegative association with quality ratings in the High -reach liberal/conservative PO \ncondition ( r = -.751, p = .012). Those who indicated high er ratings of bias against a \nliberal viewpoint indicated lower quality ratings in this condition.  Overall, the two \ndependent variables with the highest correlations across all conditions were quality and \nagreement ratings. The full table of correlations is  displayed below ( see Table 6). \n Many participants failed the manipulation check asking them to identify the \nsource of the article they read.  This failed check resulted in more than half of the cases\u2019 \ndata being excluded from analyses.   Analyses were re -run with none of the cases \nexcluded in order to test the hypotheses using the full sample size (see Appendix H).  The \npattern of results using all participants was similar as with the reduced sample; \nconservatives perceived that the article was more biased against conservatives, whereas \nliberals perceived the article as more biased against liberals. Conservatives agreed with \nthe article more than did liberals. Using the full sample, there was no longer an overall \ndifference by source on perceptions of bias against liberals. There were no ot her \nsignificant effects, suggesting that the lack of differences by conditions w as not likely \ndue to lack of statistical power. \n \n \n 47 Table 6 \nWithin-cell Correlations Table \n* = correlation is significant at the .05 level  ** = correlation is significant at the .01 level  \n \n  Low-reach \nlib-libPO \n(n = 27) High-\nreach lib-\nlibPO \n(n = 19) Low-reach \ncons-\nlibPO \n(n = 21) High-\nreach \ncons-\nlibPO \n(n = 26) Low-reach \ncons-\nconsPO \n(n = 15) High-reach \ncons-\nconsPO \n(n = 13) Low-reach \nlib-consPO \n(n = 12) High reach \nlib-consPO \n(n = 10) \nBias ag lib -bias \nag conserv  -.181 .676** .080 -.093 .501 .126 -.147 .139 \nBias ag lib -\nquality .039 .012 -.253 -.413* .103 -.322 -.326 -.751* \nBias ag lib -\nagreement  -.387* -.183 -.487 -.536** -.310 .014 -.159 -.196 \nBias ag \nconserv-quality -.216 .106 .125 .092 -.149 -.278 -.065 -.313 \nBias ag \nconserv-\nagreement  .385* -.014 .282 .267 -.334 -.506 -.216 -.668* \nQuality-\nagreement  -.010 .085 .483* .496** .397 .506 .736** .653* \n48 \n \n 48 Discussion  \nParticipant political orientation impact ed how partisans perceived bias in a neutral \nnews article, as conservatives indicated higher bias ratings against conservatives in the \narticle they read, wh ereas liberals indicated higher bias ratings ag ainst liberals , \ndemonstrating the hostile media effect ( Vallone et al., 1985 ). Contrary to hypotheses, \nhowever, this effect was not exacerbated by the perceived political orientation of the \nnews source, although p articipants of both political orientations perceived the article as \nmore biased against liberals when it had a conservative heading. Conservatives also \nagreed more with the article. Unexpectedly, perceived reach of the article did not \nmoderate these effect s.  \nPossible Explanations  \nConservatives perceived more bias against their viewpoint  and liberals perceived \nmore biased against their own viewpoint, regardless of the source. This finding provides \nevidence of the hostile media effect at play, but it does no t support the manipulation of \nsource as the factor. The activation of the individual\u2019s partisan identity may be an \nexplanation for this result. Consistent with the self -categorization theory, an individual \nwho identifies with a partisan position may have t his identity become salient to them \nthrough the presentation of political news content (Reid, 2012). This effect can occur \nwithout the recognition of the source\u2019s political orientation, but rather by the exposure to \nthe political content itself.  \nThere wer e few news source effects in the study. Participants may not have been \nas focused on the source as what was predicted . A 2018 Pew research survey suggests \n49 \n \n 49 that online news consumers may have difficulties recalling the news source that they \nobtained their i nformation from (Mitchell  et al., 2017). Specifically, online news \nconsumers between the ages of 18 and 29 recall ed a news soure they accessed two hours \nprior about 47 percent of the time. On average across all age groups, people were able to \nrecall the ne ws source only 56 percent of the time. This finding translates to social media \nplatforms, as i ndividuals tend to show a lack of memory of a source when the source\u2019s \nheadline is presented on Twitter (Bourne et al., 2020).  When a news article with a CNN \nheadline is presented on Twitter, there is less source memory than when reading the \nsource on the actual website. Individuals separating a news source from its content is \nproblematic in terms of looking at source effects in research on hostile media perception s \nbecause manipulating the source itself may not influence perceptions of bias as strongly \nas previous research suggests.  \nAdditionally, news presentation on social media platforms  does not display the \nsource as prominently as they do in print , which may lead to less focus on the source \nitself. Previous hostile media effect research that used source manipulations  used print \nnews source manipulations in the early to mid -2000s (Gunther & Liebhart, 2006; Gunther \net al., 2009) . People may be more inclined to f ocus on the news source when looking at a \nprint newspaper than an online article, as the individual could continually see the source \non each page , whereas in an online article they could immediately scroll down to the \ncontent. Another explanation for the l ack of source effects is the way in which \nindividuals are conditioned to respond to information that is more educational or \nexplanatory  of the issue at hand  as opposed to information that is less direct  and more \n50 \n \n 50 focused on affective assoications with the i nformation . A central route of persuasion \nwould dictate that individuals are more focused on information directly related to the \nissue as opposed to the peripheral route, which focuses more on affective cues or ones \nthat are not necessarily directly tied t o the root of the issue (Petty & Cacioppo, 1986). If \nindividuals are more focused on the information itself, such as the reporting that \nlegislation was passed or vetoed, they may be less inclined to pay attention to the source \npresenting the information, a nd thus perceive bias.  \nOne final possible explanation for the lack of source effects is the idea that certain \nindividuals are conditioned to see news articles as biased regardless of the source. There \nare well-supported claims that conservatives tend to s ee all media as biased, and that they \ngenerally perceive it to have a liberal slant (Domke et al., 1999). Individuals who \ngenerally perceive the media to be biased would be more likely to indicate stronger \nperceptions of bias in the news information they i nteract with. Gauging the general \nperceptions of media bias that an individual has prior to even reading a news article could \nindicate whether a manipulation of source impacts the hostile media effect.  \nNews source reach also did not affect impressions of bias, quality, or agreement. \nA possible explanation for the lack of effects of news source reach  is the lack of \nrecognition of the two low -reach sources (i.e., The Progressive , PatriotNewsDaily ). This \nwas reflected in the open -ended response data, as many participants indicated they  \nguessed on the metrics associated with the se news articles due to not being familiar with \nthe source. Individuals may also be more inclined to associate the political stance with a \nsource, but not as inclined to associate its in fluence on others. In the pilot study,  a large \n51 \n \n 51 number (i.e., over 60% for each source) of participants were able to identify the \nconservative stance for Fox News and the liberal stance for CNN, but fewer participants \nidentified the sources as having a \u201chigh\u201d reach.  Using the metrics , such as views and \nsubscriptions , did not particularly affect perceptions of bias. The lack of source effects \nmay have contributed to less of a focus on influence of the source, and more of a focus on \nthe content only. It is  plausible that with a larger number of participants associating the \nsource with the content, the perceptions of its reach would be stronger.   \nAn additional explanation of the lack of effect of news source reach is that \nindividuals may perceive the smaller  or lesser-known sources as more biased. Lower -\nreach sources and those that do not have as large of an influence would not have to abide \nby some of the standards in place in mainstream media. Sources that reach a much \nsmaller audience may go a bit more unc hecked in headlines and content that directly \nopposes the other side. An assessment of the overall content being presented on a source \nsuch as Fox News versus PatriotNewsDaily may indicate how deliberate one source \nwould be over the other in biased present ation of content.  \nLimitations  and Ideas for Future Research  \nA major limitation of this study is power. There were a large number of \nparticipants who did not correctly identify the source they read and had their data \nexcluded from analysis, resulting in a sample size that was less than half of what was \nintended for 90% power. There were an unequal number of participants in each of the \nfour conditions (i.e., Low -Reach liberal, High -Reach liberal, Low -Reach conservative, \nHigh-Reach conservative) after data cleaning, which may have compromised the \n52 \n \n 52 comparison of differen ces between liberals and conservatives across the independent \nvariables. In future research using a similar methodology, perceptions of bias could be \nexplored in a different way where attention checks on source are not as crucial to the \noutcomes and overal l goals of the study. For example, open -ended response items could \nbe used where individuals could explain why they perceived a source to be bias or \nneutral. In this way, researchers could detect whether the recognition of the source truly \nimpacts percepti ons of bias, or rather if the content itself is most important.  \nAn additional limitation to the study is the potential bias in the article content \nitself. There were steps taken to ensure the article was as neutral as possible and that it \ndid not have a s lant in either the conservative or liberal direction. Some of these steps \nincluding balancing the arguments made in favor of either side, as well as omitting \ninformation that was a direct indication of bias in either direction. The article chosen for \nthe study was the modified version of the U.S. Farm Bill article, and this was due to \nparticipants in the pilot study rating it closely to moderate (i.e., M = 3.12 on a 1 -5 scale, \n1-3 indicating a liberal bias, 3 -5 indicating a conservative bias, and 3 indicati ng neutral or \nmoderate). Since the ratings were an average of 3.12, a limitation is that the article \ncontent may have been slightly biased in the conservative direction from the start. Future \nstudies using a similar methodology could ask participants to el aborate further on their \nratings and why they perceived the content as biased in any particular direction. This \nway, specific points in the article could be targeted and altered to fit more of a neutral \nstance.  \n53 \n \n 53 Another limitation  is a nonrepresentative sa mple of the population. A byproduct \nof using Amazon Mechanical Turk is that demographics of participants vary \ntremendously, and depending on the samples there may be a very uneven distribution of \nwhite/caucasian, african american, and latinx participants.  There is also the limitation of \ntypically a Liberally-skewed sample in terms of political orientation  in Amazon  \nMechnical Turk participants (Paolacci & Chandler, 2014). This limitation was reflected \nin my research, with nearly double the number of Liberal participants as Conservative \nparticipants.  Other studies have shown that a recurring limitation to  Amazon Mechanical \nTurk samples is a lack of participant naivet \u00e9 (Chandler  et al., 2014; Fort et al., 2011). The \ngeneralizability of the results to other popu lations is compromised, so it will be important \nto conduct further research using a different sampling method.  \nThe shift in the dynamics surrounding online news in general presents an \nadditional limitation to this study. Individuals are consuming a large amount of news \nthrough social media, which often is done through the sharing of embedded links that \nmay already have comments/critiques on the story (Carlson, 2016). Additionally, people \nmay be paying more attention to the individual who shares the article  rather than the \nsource (Lee et al., 2018). As a result  of the sharing of news by third parties on social \nmedia, there is now a prevalence of mundane media criticism, which may be a phrase or \nseveral sentences provided by the sharer that offer criticism or  support towards a news \narticle. This may cause the consumer to detach the article from the website or news \nsource itself, compromising their evaluation of the article within its original context \n(Carlson, 2016). It is possible that individuals are perceiv ing news in an entirely different \n54 \n \n 54 way, and less consideration is being given to the news outlet. This provides a limitation \nto this study due to the effectiveness of the manipulation depending on perceptions of the \nsource itself as well as perceptions of i ts audience. Future research should examine news \nsharing in  the social media landscape, and how the hostile media effect translates to this  \nresearch.  \nLastly, there are several limitations to this study related to the timing and \nmotivations of participant s who completed this study. Participants may have been less \nmotivated to pay attention to the article and the questions due to the low compensation \nfor completion of the study ($0.50). This lowered motivation may also relate to the lack \nof time spent readi ng the article.  The timer set on the article page yielded a wide range of \nresults in participants\u2019 time spent reading the article itself. It is reasonable to question \nhow well they were able to decipher bias in the article and any political slant in either  \ndirection, given that some participants spent so little time reading the article. Future \nresearch could offer higher compensation for the study, such as in the $1.00 to $5.00 \nrange for completion . This may lead to more motivated participants and even rais e the \ntime spent reading any articles or other materials during the study, and may also lead to \nmore thoughtful responses on the questionnaires.  \nFuture research could look at other factors influencing the hostile media effec t, \nsuch as the diversity and nu mber of news outlets an individual follows. If they show \nallegiance to only one or two outlets, will they perceive more bias in coverage from \noutlets they do not follow or outlets that are contrary to the stance of the outlets they \nfollow? Additionally, future research could further assess the association between \n55 \n \n 55 partisan identity and the hostile media effect  by examining whether partisan identity \nmoderates the relationship between news source and hostile media effect.  \nMotivated reasoning encompasses different phenomena that govern how people \ninteract with and the ways people seek out information (Kunda, 1990). Biased \nassimilation refers to an individual\u2019s selective interpretation o f new or incoming \ninformation (Lord et al., 1979). Since biased assimilation and hostile media bias both \nrefer to perceptions of incoming information, it is plausible that they may be more closely \nrelated and their relationship should be examined further. Separating or at least \nemphasizing certain parts of bias could help determine the root causes and potential ways \nto counteract it. Interestingly, asking individuals to consider bias in a news report prior to \nreading it helped to counteract overall percepti ons of hostile media bias (Litovsky, 2021). \nDefining or separating out the types of bias that underlie hostile media perceptions could \naid in understanding of its root causes and where biases lie in the news content people \ninteract with daily.  \nImplication s  \nThis research contributes to the growing discussion on the hostile media effect in \na digital news society, the partisan divide , and the steps society should take  to reduce it. \nThe key outcome of this study is that individuals perceived bias in the artic les they read, \nbut it was not the source that contributed to these perceptions of bias, or the level of its \nreach. The hostile media effect is evident in individual perceptions of news coverage in \ngeneral topic areas as well as the political landscape , however, the contributing factors \ntowards hostile media perceptions need to be explored further. Previous studies have \n56 \n \n 56 detected partisan identity salience as a mediator ( For a review, see Kim & H wang, 2019; \nReid, 2012), however there could be additional facto rs influencing an individual\u2019s \nperceptions of bias .  \nAnother overarching implication of this study is that it  reflects a change in \ndynamics of news consumption and how perceptions of bias fit into this new context. A \nkey takeaway from this research is that  news sharing has diminished source perceptions, \nand it is imperative to investigate how the hostile media effect plays a role in how we \ninteract with news on social media. With social media offering individuals the \nopportunity to offer commentary and pres ent news in a way that fits their beliefs and \ncognitive representations, dissecting whether we are receiving accurate news is a fair \nquestion. It  is crucial in an increasingly polarizing political climate \u2013 where individuals \nare more divided on issues than  ever before \u2013 that individuals are actively seeking out the \nmost accurate news information and multiple sources to develop their opinions. This \ncould impact choices at the voting booths and subsequent policies that affect millions of \nAmericans.  \n  \n57 \n \n 57 REFERENCES  \nAbelson, R.  P. (1963). Computer simulation of \u201chot cognition.\u201d In S.  S. Tomkins & S. \nMessick (Eds.), Computer simulation of personality: Frontier of psychological \ntheory (pp. 277\u2013302). Wiley & Sons.  \nAriyanto, A., Hornsey, M. J., & Gallois, C. (2 007). Group allegiances and perceptions of \nmedia bias. Group Processes & Intergroup Relations , 10(2), 266\u2013279.  \n  https://doi.org/10.1177/1368430207074733  \nArpan, L. M., & Raney, A. A. (2003). An experimental investigation of news source and \nthe hostile media effect. Journalism & Mass Communication Quarterly , 80(2), \n265\u2013281. https://doi.org/10.1177/1 07769900308000203   \nBourne, K. A., Boland, S. C., Arnold, G. C., & Coane, J. H. (2020). Reading the news on \nTwitter: Source and item memory for social media in younger and older adults. \nCognitive Research: Principles and Implications , 5(1), 11-11. \nhttps://doi.org/10.1186/s41235 -020-0209-9  \nBoxell, L., Gentzkow, M., & Shapiro, J. M. (202 1). Cross-country trends in affective \npolarization . National Bureau of Economic Research . \nhttp://www.nber.org/papers /w26669 \nCarlson, M. (2016). Embedded links, embedded meanings. Journalism Studies , 17(7), \n915\u2013924. https://doi.org/10.1080/1461670x.2016.1169210   \nCarmichael, J. T., Brulle, R. J., & Huxster , J. K. (2017). The great divide: Understanding \nthe role of media and other drivers of the partisan divide in public concern over \n58 \n \n 58 climate change in the USA, 2001 \u20132014. Climatic Change , 141(4), 599\u2013612. \nhttps://doi.org/10.1007/s10584 -017-1908-1  \nChandler, J., Mueller, P., & Paolacci, G. (201 4). Nonna\u00efvet\u00e9 among Amazon Mechanical \nTurk workers: Consequences and solutions for behavioral researchers. Behavior \nResearch Methods , 46(1), 112\u2013130. https://doi.org/10.3758/s13428 -013-0365-7  \nChristen, C. T., Kannaovakun, P., & Gunther, A. C. (2002). Hostile media perceptions: \nPartisan assessments of press and public during the 1997 United Parcel Service \nstrike. Political Communication , 19(4), 423\u2013436. \nhttps://doi.org/10.1080/10584600290109988   \nCoe, K., Tewksbury, D., Bond, B. J., Drogos, K. L., Porter, R. W., Yahn, A., & Zhang, \nY. (2008). Hostile news: Partisan use and perceptions of cable news \nprogramming.  Journal of Communication , 58(2), 201\u2013219. \nhttps://doi.org/10.1111/j.1460 -2466.2008.00381.x   \nCrowe, J. A. (2020). The effect of partisan cues on support for solar and wind energy in \nthe United States.  Social Science Quarterly (Wiley -Blackwell) , 101(4), 1461\u2013\n1474. https://doi.org/10.1111/ssqu.12799   \nDaily, K. (2014). Explicating the hostile media perception: How source credibility \ninfluences partisans\u2019 responses to balanced news coverage of health policies . \nDigital Repository of the University of Maryland , 301(314-1328), 1-255. \nhttps://doi.org/10.13016/M2MW4Q  \n59 \n \n 59 Doherty, C. (2017, October 5). Americans' growing partisan divide: 8 key findings. \nhttps://www.pewresearch.org/fact -tank/2017/10/05/takeaways -on-americans -\ngrowing-partisan-divide-over-political-values/. \nDomke, D., Wat ts, M. D., Shah, D. V., & Fan, D. P. (1999). The politics of conservative \nelites and the \u201c liberal media\u201d argument. Journal of Communication , 49(4), 35\u201358. \nhttps://doi.org/10.1111/j.1460 -2466.1999.tb02816.x   \nDunaway, J. L., Davis, N. T., Padgett, J., & Scholl, R. M. (2015). Objectivity and \ninformation bias in campaign news. Journal of Communication , 65(5), 770\u2013792. \nhttps://doi.org/10.1111/jcom.12172   \nEntman, R. M. (2007). Framing bias: Media in the distribution of power. Journal of \nCommunication , 57(1), 163\u2013173. https://doi.org/10.1111/j.1460 -\n2466.2006 .00336.x  \nFaul, F., Erdfelder, E., Lang, A. -G., & Buchner, A. (2007). G*Power 3: A flexible \nstatistical power analysis program for the social, behavioral, and biomedical \nsciences. Behavior Research Methods , 39(2), 175\u2013191. \nhttps://doi.org/10.3758/bf03193146   \nFeldman, L. (2011). Partisan differences in opinionated news perceptions: A test of the \nhostile media e ffect. Political Behavior , 33(3), 407\u2013432. \nhttps://doi.org/10.1007/s11109 -010-9139-4  \nFeldman, L., Hart, P. S., Leiserowitz, A., Maibach, E., & Roser -Renouf, C. (2017). Do \nhostile media perceptions lead to action? The role of hostile media perceptions, \npolitical efficacy, and ideology in predicting climate change \n60 \n \n 60 activism. Communication Research , 44(8), 1099\u20131124. \nhttps://doi.org/10.1177/0093650214565914   \nFestinger, L. (19 57). A theory of cognitive dissonance.  Stanford University Pre ss. \nFestinger, L., & Carlsmith, J. M. (1959). Cognitive consequences of forced compliance. \nThe Journal of Abnormal and Social Psychology , 58(2), 203\u2013210. \nhttps://doi.org/10.1037/h0041593   \nFort, K., Adda, G. , & Cohen, K. B. (2011). Amazon Mechanical Turk: Gold mine or coal \nmine? Computational Linguistics , 37(2), 413\u2013420. \nhttps://doi.org/10.1162/coli_a_00057   \nFrimer, J. A., Skitka, L. J., & Motyl , M. (2017). Liberals and conservatives are similarly \nmotivated to avoid exposure to one another\u2019s opinions. Journal of Experimental \nSocial Psychology , 72(1), 1\u201312. https://doi.org/10.1016/j.jesp. 2017.04.003   \nGearhart, S., Moe, A., & Zhang, B. (2020). Hostile media bias on social media: Testing \nthe effect of user comments on perceptions of news bias and credibility. Human \nBehavior and Emerging Technologies , 2(2), 140\u2013148. \nhttps://doi.org/10.1002/hbe2.185   \nGershon, L. (2019). The Invention of Journalistic Objectivity . JSTOR Daily. \nhttps://daily.jstor.org/the -invention -of-journalistic -objectivity/  \nGiner-Sorolla, R., & Chaiken, S. (1994). The causes of hostile media judgments. Journal \nof Experimental Social Psychology , 30(2), 165\u2013180. \nhttps://doi.org/10.1006/jesp.1994.1008   \n61 \n \n 61 Golan, G. J., Waddell, T. F., & Barnidge, M. (2021). Competing identity cues in the \nhostile media phenomenon: Source, nationalism, and perceived bias in news \ncoverage of foreign affairs. Mass Communication and Society , 24(5), 676\u2013700. \nhttps://doi.org/10.1080/1520543 6.2021.1884263   \nGunther, A. C. (1992). Biased press or biased public? Attitudes toward media coverage \nof social groups. Public Opinion Quarterly , 56(2), 147. \nhttps://doi.org/10.1086/269308   \nGunther, A. C. , & Liebhart, J. L. (2006). Broad reach or biased source? Decomposing the \nhostile media e ffect. Journal of Communication , 56(3), 449-466. \nhttps://doi.org/10.1111/j.1460 -2466.2006.00295.x   \n Gunther, A. C., Miller, N., & Liebhart, J. L. (2009). Assimilation and contrast in a test of \nthe hostile media effect. Communication Research , 36(6), 747\u2013764. \nhttps://doi.org/10.1177/0093650209346804   \nGunther, A. C., & Schmitt, K. M. (2004). Mapping boundaries of the hostile media \neffect. Journal of Communication , 54(1), 55-70. https://doi.org/10.1111/j.1460 -\n2466.2004.tb02613.x   \nHansen, G. J., & Kim, H. (2011). Is the media biased against me? A meta-analysis of the \nhostile media effect research. Communication Research Reports , 28(2), 169-\n179. https://doi.org/10.1080/08824096.2011.565280   \nHarton, H. C., & Latan\u00e9, B. (1997). Information - and thought-induced polarization: The \nmediating role of involvement in making attitudes extreme. Journal of Social \nBehavior & Personality , 12(2), 271\u2013299. \n62 \n \n 62 http://web.b.ebscohost.com.proxy.lib.uni.edu/ehost/detail/detail?vid=0&sid=bbfb\n25a4-5ed1-4302-b096-\nb31516af6742%40sessionmgr103&bdata=JnNpdGU9ZWhvc3QtbGl2ZQ%3d%3\nd#db=afh&AN=9709236093  \nHayes, A. F. (2013). Introduction to mediation, moderation, and conditional process \nanalysis: A regression -based approach. The Guilford Press  \nHuddy, L., & Bankert, A.  (2017, May 24). Political partisanship as a social \nidentity. Oxford Research Encyclopedia of Politics.  \nhttp://oxfordre.com/politics/view/10.1093/acrefore/ 9780190228637.001.0001/acr\nefore-9780190228637 -e-250. \nHuge, M., & Glynn, C. J. (2010). Hostile media and the campaign trail: Perceived media \nbias in the race for governor. Journal of Communication , 60(1), 165\u2013181. \nhttps://doi.org/10.1111/j.1460 -2466.2009.01473.x   \nIyengar, S., Lelkes, Y., Levendusky, M., Malhotra, N., & Westwood, S. J. (2019). Th e \norigins and consequences of affective polarization in the United States. Annual \nReview of Political Science , 22(1), 129\u2013146. https://doi.org/10.1146/annurev -\npolisci-051117-073034  \nJonas, E., Graupmann, V., Fischer, P., Greitemeyer, T., & Frey, D. (2003). Slush fund, \nclean slate? Confirmation bias in the context  of the CDU political donations \nscandal. Zeitschrift f\u00fcr Sozialpsychologie , 34(1), 47-61. \nhttps://doi.org/10.1024//0044 -3514.34.1.47  \n63 \n \n 63 Jones, J., & Ritter, Z. (2018). Americans see more news bias; Most can't name neutral \nsource. Gallup. https://news.gallup.com/poll/225755/americans -news-bias-name-\nneutral-source.aspx  \nKim, M. (2016). The role of partisan sources and audiences\u2019 involvement in bias \nperceptions of controversial news. Media Psychology , 19(2), 203\u2013223. \nhttps://doi.org/10.1080/15213269.2014.1002941   \nKim, Y., & Hwang, H. (2019). When partisans see media coverage as hostile: The effect \nof uncivil online comments on hostile media effect.  Media Psychology , 22(6), \n845\u2013866. https://doi.org/10.10 80/15213269.2018.1554492   \nKlapper, J. T. (1960). The effects of mass communication . The Free Press. \nKnobloch -Westerwick, S., Mothes, C., & Polavin, N. (2017). Confirmation bias, ingroup \nbias, and negativity bias in selective exposure to political information. \nCommunication Research , 47(1), 104-124. \nhttps://doi.org/10.1177/0093650217719596   \nKunda, Z. (1990). The case for motivated reasoning. Psychological Bulletin , 108(3), \n480\u2013498. https://doi.org/10.1037/0033 -2909.108.3.480   \nKunda, Z., & Sinclair, L. (1999). Motivated reasoning with stereotypes: Activation, \napplication, and inhibition. Psychological Inquiry , 10(1), 12\u201322. \nhttps://doi.org/10.1207/s15327965pli1001_2   \nLazarsfeld, P. F., Berelson, B., & Gaudet, H. (1948). The people\u2019s choice . Columbia \nUniversity Press.  \n64 \n \n 64 Lee, T. K., Kim, Y., & Coe, K. (2018). When social media becomes hostile media: An \nexperimental examination of news sharing, partisanship, and follower count. Mass \nCommunication and Society , 21(4), 450\u2013472. \nhttps://doi.org/10.1080/15205436.2018.1429635   \nLippmann, W., Merz, C., & Lippmann, F. (1920). A test of the news: An examination of \nthe news reports in the New York Times on aspects of the Russian revolution of \nspecial importance to Americans, March 1917 -March 1920 . New Republic. \nhttps://books.google.com/books?id=noZCpwAACAAJ  \nLitovsky, Y. (2021). (Mis)perception of bias in print media: How depth of content \nevaluation affects the perception of ho stile bias in an objective news report. PLoS \nONE, 16(5), e0251355 \u2013e0251355. https://doi.org/10.1371/journal.pone.0251355   \nLord, C. G., Ross, L., & Lepper, M. R. (1979). Biased assimilation and attitude \npolarization: The effects of prior theories on subsequently considered evidence. \nJournal of Personality and Social Psychology , 37(11), 2098\u20132109. \nhttps://doi.org/10.1037/0022 -3514.37.11.2098   \nMatheson, K., & Dursun, S. (2001). Social identity precursors to the hostile media \nphenomenon: Partisan perceptions of coverage of the Bosnian conflict. Group \nProcesses & Intergr oup Relations , 4(2), 116\u2013125. \nhttps://doi.org/10.1177/1368430201004002003   \nMayer, W. G. (2005). What conservative media? The unproven case for conservative \nmedia bias. Critical Review , 17(3-4), 315\u2013338. \nhttps://doi.org/10.1080/08913810508443642   \n65 \n \n 65 Mitchell, A., Gottfried, J., Shearer, E., & Lu, K. (2017). How Americans encounter, \nrecall and act upon digital news. Pew Research Center\u2019s Journa lism Project . \nhttps://www.journalism.org/2017/02/09/how -americans -encounter -recall-and-act-\nupon-digital-news/ \nMorris, J. S. (2007). Slanted objectivity? Perceived media bias, cable news exposure, and \npolitical attitudes. Social Science Quarterly , 88(3), 707\u2013728. \nhttps://doi.org/10.1111/j.1540 -6237.2007.00479.x   \nMorstatter, F., Wu, L., Yavanoglu, U., Corman, S. R., & Liu, H. (2018). Identifying \nframing bias in online news. ACM Transactions  on Social Computing , 1(2), 1\u201318. \nhttps://doi.org/10.1145/3204948   \nNam, H. H., Jost, J. T., & Van Bavel, J. J. (2013). \u201cNot for all the tea in China!\u201d Political \nideology and the avoidance of dissonance -arousing situations. PLoS ONE , 8(4), \ne59837. https://doi.org/10.1371/journal.pone.0059837   \nNickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many \nguises. Review of General Psychology , 2(2), 175\u2013\n220. https://doi.org/10.1037/1089 -2680.2.2.175   \nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological  \nscience. Science, 349(6251), aac4716. https://doi.org/10.1126/science.aac4716   \nPaolacci, G., & Chandler, J. (2014). Inside the Turk: Understanding Mechanical Turk as \na participant pool. Current Directions in Psychological Science,  23(3), 184-188. \nhttps://doi.org/10.1177/0963721414531598   \n66 \n \n 66 Patterson, T. E. (2013). Informing the news . Vintage Books  \nPetty, R. E., & Cacioppo, J. T. (1986). The elaboration likelihood model of persuasion. \nAdvances in Experimental Social Psychology , 19(1), 123\u2013205. \nhttps://doi.org/10.1016/s0065 -2601(08)60214 -2  \nReid, S. A. (2012). A self -categorization explanation for the hostile media e ffect. Journal \nof Communication , 62(3), 381\u2013399. https://doi.org/10.1111/j.1460 -\n2466.2012.01647.x   \nRichardson, J. D., Huddy, W. P., & Morgan, S. M. (2008). The hostile media effect, \nbiased assimilation, and perceptions of a presidential debate. Journal of Applied \nSocial Psychology , 38(5), 1255-1270. https://doi.org/10.1111/j.1559 -\n1816.2008.00347.x   \nRosell, K. (2013). Bias in election coverage: The top three cable news networks\u2019 framing \nof content, and their reporting on the 2012 United States presidential election. \nhttp://www.diva -portal.org/smash/record.jsf?pid=diva2%3A633141&dswid= -\n8538 \nSayette, M. A., & Hufford, M. R. (1997). Effects of smoking urge on generation of \nsmoking-related information. Journal of Applied Social Psychology , 27(16), \n1395\u20131405. https://doi.org/10.1111/j.1559 -1816.1997.tb01604.x   \nSedlacek, M.  B., & Harton, H.  C. (2019, April). The effect of news source on biased \nperceptions of political news. Poster presented at  12th Annual Graduate \nSymposium . Symposium conducted at the University of Northern Iowa, Cedar \nFalls.  \n67 \n \n 67 Solomon, J. (2018, November 23). The greatest threat to American journalism: The loss \nof neutral reporting. The Hill. https://thehill.com/opinion/white -house/417921 -\nthe-greatest-threat-to-american-journalism -the-loss-of-neutral-reporting \nStavrositu, C. D., & Kim, J. (2014). Social media metrics: Third -person perceptions of \nhealth information. Computers in Human Behavior , 35(1), 61\u201367. \nhttps://doi.org/10.1016/j.chb.2014.02.025   \nSuhay, E., & Erisen, C. (2018). The role of anger in the biased assimilation of political \ninformation. Political Psychology , 39(4), 793\u2013810. \nhttps://doi.org/10.1111/pops.12463   \nTaber, C. S., & Lodge, M. (2006). Motivated skepticism in the evaluation of political \nbeliefs. American Journal of Political Science , 50(3), 755\u2013769. \nhttps://doi.org/10.1111/j.1540 -5907.2006.00214.x   \nTaber, C. S., & Lodge, M. (2016). The illusion of choice in democratic politics: The \nunconscious impact of motivated political reasoning. Political Psychology , 37(1), \n61\u201385. https://doi.org/10.1111/pops.12321   \nTurner, J. C., Hogg, M. A., Oakes, P. J., Reicher, S. D. , & Wetherell, M. S. (1987). \nRediscovering the social group: A self -categorization theory. Blackwell  \nVallone, R. P., Ross, L., & Lepper, M. R. (1985). The hostile media phenomenon: Biased \nperception and perceptions of media bias in coverage of the Beirut massacre. \nJournal of Personality and Social Psychology , 49(3), 577\u2013585. \nhttps://doi.org/10.1037/0022 -3514.49.3.577   \n68 \n \n 68 Wason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. \nQuarterly Journal of Experimental Psychology , 12(3), 129\u2013140. \nhttps://doi.org/10.1080/17470216008416717   \nWesterwick, A., Kleinman, S. B., & Knobloch -Westerwick, S. (2013). Turn a blind eye if \nyou care: Impacts of attitude consistency, importance, and credibility on seeking \nof political information and implications for attitudes. Journal of Communication , \n63(3), 432\u2013453. https://doi.org/10.1111/jcom.12028   \nYun, G. W., Park, S. -Y., Lee, S., & Flynn, M. A. (2016). Hostile media or hostile source? \nBias perception of shared news. Social Science Computer Review , 36(1), 21\u201335. \nhttps://doi.org/10.1177/0894439316684481   \nZeldes, G. A., Fico, F., Carpenter, S., & Diddi, A. (2008). P artisan balance and bias in \nnetwork coverage of the 2000 and 2004 Presidential elections. Journal of \nBroadcasting & Electronic Media , 52(4), 563\u2013580. \nhttps://doi.org/10.1080/08838150802437354   \n \n \n  \n69 \n \n 69 APPENDIX A  \nHostile media effec t measures, Quality, and Agreement items (Gunther & Liebhart, 2006; \nGunther et al., 2009), Fact -Based versus Opinion -based item, and Informative item.  \nHostile media effect items  \nPlease indicate how much you agree or disagree  with the following statements:  \nI feel the article I just read was biased against a Liberal viewpoint  (HME) \n1=Strongly Disagree  \n2 \n3 \n4 \n5 \n6 \n7=Strongly Agree  \nI feel the article I just read was biased against a Conservative viewpoint  (HME)  \n1=Strongly Disagree \n2 \n3 \n4 \n5 \n6 \n7=Strongly Agree  \n  \n70 \n \n 70  \nQuality Item  \nPlease rate the quality of the article you just read.  \n1=Very poor quality  \n2 \n3 \n4 \n5 \n6 \n7=Very high quality  \nAgreement Item  \nPlease indicate the extent to which you agree with the content of the article you read.   \n1=Strongly Disagree  \n2 \n3 \n4 \n5 \n6 \n7=Strongly Agree  \nFact-Based versus Opinion -Based item.   \nOn the slider below, please indicate the extent to which you thought the article  you read \ncontained opinion -based information or fact -based information.   \n \n71 \n \n 71  \n 0 10 20 30 40 50 60 70 80 90 100 \n \nInformative item  \nPlease indicate the extent to which, if any, you agree with the following statement:  \nThe article was informative.  \n1 = Strongly Disagree  \n2 \n3 \n4 \n5 \n6 \n7 = Strongly Agree  \n \n \n \n \n   Mostly Opinion -based Mostly Fact -based  \n\n72 \n \n 72 APPENDIX B  \nAttention checks and Manipulation check  \nAttention check items  \nWhat was the news source of the article you just read?  \n1. CNN \n2. Fox News  \n3. PatriotNewsDaily     \n4. The Progressive  \n5. NBC News  \n6. Not Sure \nIn your own words, please describe what the article was about? (open -ended response  \nwith comment box )  \n \nManipulation check item s \nPilot Study Item \nAbout how many views did the article you read have?   \n1. Less than 1,000  \n2. 1,000-100,000 \n3. 100,000-500,000 \n4. 500,000-1,000,000  \n5. More than 1,000,000  \nPrimary Study Item  \n73 \n \n 73 On a scale of 1 -7, with 1 meaning \u201chardly any people\u201d and 7 meaning \u201can extremely \nlarge number of people\u201d, indicate the size of the message\u2019s audience.  (PR)  \n1. Hardly any people  \n2.  \n3.  \n4.  \n5.  \n6.  \n7. An extremely large number of people  \n \n \n \n \n  \n74 \n \n 74 APPENDIX C  \nPartisan Identity Scale (Huddy & Bankert, 2017)  \nWhat political party, if any, do you identify most with?  \n1. Democrat (Liberal)  \n2. Republican (Conservative)  \n3. Independent  \n4. Green Party  \n5. Libertarian  \n6. No Affiliation  \n7. Other (please specify ) ________________  \nWhen answering the following statements, please think about the political party  you \nindicated you identify most with\u2026  \nWhen I speak about this political party, I usually say \u201cwe\u201d instead of \u201cthey\u201d  \n1=strongly disagree  \n2 \n3 \n4 \n5 \n6 \n7=strongly agree  \nWhen people criticize this party, it feels like a personal insult.    \n1=strongly disagree  \n75 \n \n 75 2 \n3 \n4 \n5 \n6 \n7=strongly agree  \nI have a lot in common with other supporters of this party.  \n1=strongly disagree  \n2 \n3 \n4 \n5 \n6 \n7=strongly agree  \nWhen I meet someone who supports this party, I feel connected to this person.  \n1=strongly disagree  \n2 \n3 \n4 \n5 \n6 \n7=strongly agree \n76 \n \n 76 When people praise this party, it makes me feel good.  \n1=strongly disagree  \n2 \n3 \n4 \n5 \n6 \n7=strongly agree  \nIf this party does badly in opinion polls, my day is ruined.  \n1=strongly disagree  \n2 \n3 \n4 \n5 \n6 \n7=strongly agree  \n \n77 \n \n 77 APPENDIX D  \nNews Source Manipulations\n \n \n \n \n \n  \n \nArticle Statistics  \nViews: 3,859,567  \nShares: 302,781 \nWebsite Subscriptions: 5,067,435  \n \n \n \n \n \n \nArticle Statistics  \nViews: 4,754 \nShares: 346 \nWebsite Subscriptions: 897 \n  \n(CNN) - U.S. lawmakers recently passed the Agriculture Improvement Act, otherwise known as the U.S. 2018 \nfarm bill, in order to establish new funding for the U.S. Department of Agriculture and several other key programs \nfor the nation\u2019 s farmers. Perhaps the most controversial component to the bill, or lack thereof, is the proposed \nchanges to the Supplemental Nutrition Assistance Program (SNAP), more commonly known as food stamps.  \nRepublicans exhausted their efforts in lobbying for tighter food stamps criteria but were once again unsuccessful \nin doing so.  President Trump and others hastily fought for lowering the income criteria below the threshold it sits \nat currently and also enacting mandatory strict drug testing of all individuals receiving the benefit s .   If included \nin the bill, individuals who failed drug tests were to have their benefit s  stripped form them for a minimum of \n1-year and be subjected to rigorous drug counseling.  \nDemocrats had advocated for an increase in food stamp provisions in the bill, wanting the inclusion of items such \nas medications and birth control on the list.  Unsurprisingly , they were unsuccessful in getting these changes \nenacted to the bill.  In addition, the Democrats in the House of Representatives came under fir e  for deliberately \nstalling the passage of the bill in a time when the fin a nci a l  security of farmers was in the palm of their hands.  \nWhile the various agricultural programs were passed under the bill, the food stamp changes were tabled for the \ntime being and more clarity should come when discussions on a new bill commence halfway through 2020.  \nTrump signs Agriculture Improvement Act, Republicans \nnot ready to drop stricter food stamp regulations\nBy Todd Alexander , CNN\nUpdated 5:08 PM ET, February 14, 2019\nU.S. lawmakers recently passed the Agriculture Improvement Act, otherwise known as the U.S. 2018 \nfarm bill, in order to establish new funding for the U.S. Department of Agriculture and several other key \nprograms for the nation\u2019s farmers. Perhaps the most controversial component to the bill, or lack thereof, is \nthe proposed changes to the Supplemental Nutrition Assistance Program (SNAP), more commonly known as \nfood stamps.  Republicans exhausted their effo r ts in lobbying for tighter food stamps criteria but were once \nagain unsuccessful in doing so.  President Trump and others hastily fought for lowering the income criteria \nbelow the threshold it sits at currently and also enacting mandatory strict drug testing of all individuals \nreceiving the benefits .  If included in the bill, individuals who failed drug tests were to have their benefit s  \nstripped form them for a minimum of 1-year and be subjected to rigorous drug counseling.  \nDemocrats had advocated for an increase in food stamp provisions in the bill, wanting the inclusion of items \nsuch as medications and birth control on the list.  Unsurprisingly, they were unsuccessful in getting these \nchanges enacted to the bill.  In addition, the Democrats in the House of Representatives came under fir e  for \ndeliberately stalling the passage of the bill in a time when the fin a ncial security of farmers was in the palm \nof their hands.  While the various agricultural programs were passed under the bill, the food stamp changes \nwere tabled for the time being and more clarity should come when discussions on a new bill commence \nhalfway through 2020.  Dec. 20, 2018,  5:06 p.m.\nBy Todd Alexander\nTrump signs Agriculture Improvement Act, Republicans \nnot ready to drop stricter food stamp regulationsHome / Dispatches\nHigh-reach Liberal  \nLow-reach Liberal  \n78 \n \n 78  \n \n \n  \n \nArticle Statistics  \nViews: 3, 859,567 \nShares: 30 2,781 \nWebsite Subscriptions: 5, 067,435 \n \n \n \n \n \nArticle Statistics  \nViews: 4,754 \nShares: 346 \nWebsite Subscriptions: 897\nU.S. lawmakers recently passed the Agriculture Improvement Act, otherwise known as the U.S. 2018 \nfarm bill, in order to establish new funding for the U.S. Department of Agriculture and several other key \nprograms for the nation\u2019s farmers. Perhaps the most controversial component to the bill, or lack thereof, \nis the proposed changes to the Supplemental Nutrition Assistance Program (SNAP), more commonly \nknown as food stamps.  Republicans exhausted their efforts in lobbying for tighter food stamps criteria \nbut were once again unsuccessful in doing so.  President Trump and others hastily fought for lowering the \nincome criteria below the threshold it sits at currently and also enacting mandatory strict drug testing of \nall individuals receiving the benefit s .   If included in the bill, individuals who failed drug tests were to have \ntheir benefits\n s t rip ped fo r m  th em  fo r  a  m i n i m u m  o f  1 - y ear  a nd b e s ubj e ct e d to  rig or o us d r u g c ounsel in g.   \nDemocrats had advocated for an increase in food stamp provisions in the bill, wanting the inclusion of \nitems such as medications and birth control on the list.  Unsurprisingly, they were unsuccessful in getting \nthese changes enacted to the bill.  In addition, the Democrats in the House of Representatives came \nunder fir e  for deliberately stalling the passage of the bill in a time when the fin a nci a l  security of farmers \nwas in the palm of their hands.  While the various agricultural programs were passed under the bill, the \nfood stamp changes were tabled for the time being and more clarity should come when discussions on a \nnew bill commence halfway through 2020.  \nPresident Trump signs 2018 farm bill, despite \nintentional Democratic stalling\nHigh-reach Conservative  \nLow-reach Conservative  \n79 \n \n 79 APPENDIX E  \nOriginal news article text with changes/edits highlighted  \n*underlined text = paraphrased and/or edited to balance arguments in study article*  \n*strikethrough text = omitted from study article*  \nOriginal Article source: https://www.reuters.com/article/us -usa-farmbill/senate -approves-\nfarm-bill-compromise -that-avoids-food-stamp-cuts-idUSKBN1OA0BY  \nOriginal Article:  \nU.S. lawmakers have reached an agreement on a farm bill that leaves out a \nproposal to tighten food stamps criteria backed by President Donald Trump,  and offers \nsome financial certainty to farmers suffering from the U.S. trade war with China.   \nThe bill passed the Senate 87 -13. Congressional staffers are expecting the House \nto vote by Thursday and send the bill to Trump for his signature before Friday.  \nThe agreement between Republicans and Democrats on the crucial piece of \nlegislation caps a bitter, months -long debate on the bill, which covers $867 billion worth \nof food and agriculture programs including crop subsidies and support to growers seeking \naccess to export markets.  \nThe final text shows Republicans in the lame duck Congress had to walk back \nfrom some of their demands, the biggest being the proposal, championed by Trump, to \nimpose stricter requirements for recipients of food stamps.   \n80 \n \n 80 Speaking to r eporters at the White House, Trump, who had accused the \nDemocrats of stalling the bill, said the progress on it was bipartisan.  \u201cWe think the farm \nbill is in very good shape. A lot of good things are happening with it, and out farmers are \nwell taken care o f,\u201d he said.  \nThe debate had delayed the legislation beyond the most recent version\u2019s \nexpiration in September, and was finalized only after Democrats won a majority in the \nHouse of Representatives in elections in November.  \nFood stamps, or the Supplemental N utrition Assistance Program, is a voucher -\ntype free food program used by more than 40 million Americans, or about 12 percent of \nthe total U.S. population.  \n \n \n \n \n \n \n  \n81 \n \n 81 APPENDIX F  \nDebriefing  \n There was a debriefing message for the primary study discussing the true focus \nand goals of the study incorporated into the \u201cEnd of Survey\u201d message. The debriefing \nmessage was as follows:  \n\u201cThank you for participating in this study!  \nWhile the article you read was based on a real piece of U.S. legislation, th e article was \ncreated for this study and contained some false information. The article was also not \ndistributed from the source you read it from in the study. We didn\u2019t tell you that to start \nwith because we wanted to see how you would respond to an articl e that you believed \nwas printed in a particular news source. We are interested in whether people\u2019s political \nbeliefs affect their interpretations of news from the mass media, as well as whether or not \nthe perceptions of the size of the news source\u2019s influe nce will impact those \ninterpretations.  Please do not discuss the details of this study with any other individuals \nthat could participate \u2013 this is very important for the accuracy of the study.  \nIf you have further questions about this study, please contact either Matthew Sedlacek at \nsedlamab@uni.edu or Helen Harton at helen.harton@uni.edu.  \nThank you for your time and effort! \u201d \n \n  \n82 \n \n 82 APPENDIX G  \nDemographic questions and honesty check  \nWhat is your age? (dropdown list 18 -99)  \nWhat is your gender?  \nWhat is your education level?      \n1. 8th grade or lower      4. Bachelor\u2019s Degree   \n2. High School diploma or GED     5. Graduate Degree  \n3. Associate\u2019s Degree  \nWhat is your race/ethnicity? Select all that apply .  \n1. Asian/Pacific Islander  \n2. Black/African American  \n3. Hispanic/Latin x \n4. Multiracial \n5. Native American/American Indian  \n6. White/Caucasian  \n7. Not listed (please specify _________)  \nHow honest were you in answering the questions on this survey? You will receive \ncompensation regardless of your answer.  \n1. Not at all honest  \n2. Slightly honest  \n3. Somewhat honest \n4. Very honest  \n83 \n \n 83 APPENDIX H  \nResults without exclusion of cases for mis -identification of source  \n \nTable 7 \nRe-run of Article Bias Ratings Against a Conservative Viewpoint Depending on Source \nPolitical Orientation  \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  6 2.809 1.587 .150 .027 \nIntercept 1 105.469 59.588 <.001* .148 \nSourcePO  2 1.510 .853 .427 .005 \nPartPO 2 5.526 3.122 .045* .018 \nSourcePO*PartPO  2 1.320 .746 .475 .004 \nNote: * = correlation is significant at the .05 level. PO = political orientation.  \n \nTable 8 \nRe-run of Article Bias Ratings Against a Liberal Viewpoint Depending on Source \nPolitical Orientation  \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  6 8.821 3.739 .001* .061 \nIntercept 1 118.254 50.129 <.001* .128 \nSourcePO  2 5.022 2.129 .121 .012 \nPartPO 2 11.092 4.702 .010* .027 \nSourcePO*PartPO  2 .628 .266 .766 .002 \nNote: * = correlation is significant at the .05 level. PO = political orientation.   \n  \n84 \n \n 84 Table 9 \nRe-run of Article Quality Ratings Depending on Source Political Orientation  \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  6 2.253 1.821 .094 .031 \nIntercept 1 409.914 317.276 <.001* .481 \nSourcePO  2 1.990 1.540 .216 .009 \nPartPO 2 2.627 2.034 .132 .012 \nSourcePO*PartPO  2 3.428 2.653 .072 .015 \nNote: * = correlation is significant at the .05 level. PO = political orientation.   \n \nTable 10 \nRe-run of Ratings of Agreement with Article Content Depending on Source Political \nOrientation  \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  6 3.331 2.353 .031* .040 \nIntercept 1 386.059 272.688 <.001* .443 \nSourcePO  2 1.572 1.111 .331 .006 \nPartPO 2 7.224 5.103 .007* .029 \nSourcePO*PartPO  2 1.118 .790 .455 .005 \nNote: * = correlation is significant at the .05 level. PO = political orientation.   \n  \n85 \n \n 85 Table 11 \nRe-run of Moderation of News Source Reach on Relationship Between Source PO, \nParticipant PO, and Article Bias Ratings Against a Conservative Viewpoint \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  12 2.818 1.609 .087 .054 \nIntercept 1 137.897 78.746 <.001* .189 \nSourcePO  1 1.273 .727 .394 .002 \nSourceReach  1 6.336 3.618 .058 .011 \nPartPO 2 6.692 3.822 .023* .022 \nSourcePO*Source Reach  1 5.432 3.102 .079 .009 \nSourcePO*PartPO  2 2.041 1.165 .313 .007 \nSourceReach*PartPO  2 1.882 1.074 .343 .006 \nSourcePO*SourceReach*PartPO  2 4.333 2.474 .086 .015 \nNote: * = correlation is significant at the .05 level. PO = political orientation.  \n \nTable 12 \nRe-run of Moderation of News Source Reach on Relationship Between Source PO, \nParticipant PO, and Article Bias Ratings Against a Liberal Viewpoint \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  12 5.212 2.197 .012* .073 \nIntercept 1 146.203 61.626 <.001* .155 \nSourcePO  1 1.595 .672 .413 .002 \nSourceReach  1 .027 .011 .915 .000 \nPartPO 2 10.293 4.339 .014* .025 \nSourcePO*Source Reach  1 .730 .308 .579 .001 \nSourcePO*PartPO  2 .899 .379 .685 .002 \nSourceReach*PartPO  2 1.828 .771 .464 .005 \nSourcePO*SourceReach*PartPO  2 .027 .011 .989 .000 \nNote: * = correlation is significant at the .05 level. PO = political orientation.  \n \n86 \n \n 86 Table 13 \nRe-run of Moderation of News Source Reach on Relationship Between Source PO, \nParticipant PO, and Article Quality Ratings \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  12 1.797 1.390 .168 .047 \nIntercept 1 474.496 367.008 <.001* .521 \nSourcePO  1 1.819 1.407 .236 .004 \nSourceReach  1 .009 .007 .932 .000 \nPartPO 2 3.271 2.530 .081 .015 \nSourcePO*Source Reach  1 2.008 1.553 .214 .005 \nSourcePO*PartPO  2 3.879 3.000 .051 .018 \nSourceReach*PartPO  2 1.275 .986 .374 .006 \nSourcePO*SourceReach*PartPO  2 2.014 1.558 .212 .009 \nNote: * = correlation is significant at the .05 level. PO = political orientation.  \n \nTable 14 \nRe-run of Moderation of News Source Reach on Relationship Between Source PO, \nParticipant PO, and Ratings of Agreement \nSource df Mean \nSquare F p-value partial \n\uf0682 \nCorrected Model  12 2.559 1.816 .044* .061 \nIntercept 1 432.799 307.138 <.001* .477 \nSourcePO  1 .220 .156 .693 .001 \nSourceReach  1 .769 .546 .461 .002 \nPartPO 2 6.957 4.937 .008* .029 \nSourcePO*Source Reach  1 .118 .083 .773 .000 \nSourcePO*PartPO  2 1.152 .818 .442 .005 \nSourceReach*PartPO  2 1.183 .839 .433 .005 \nSourcePO*SourceReach*PartPO  2 1.761 1.250 .288 .007 \nNote: * = correlation is significant at the .05 level. PO = political orientation.  \n ", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Effects of perceived reach on ratings of media bias, quality, and agreement", "author": ["M Sedlacek"], "pub_year": "2022", "venue": "NA", "abstract": "The hostile media effect refers to individuals\u2019 tendencies to perceive seemingly neutral news  coverage as biased against their stance (Vallone et al., 1985). Research has shown this"}, "filled": false, "gsrank": 204, "pub_url": "https://scholarworks.uni.edu/etd/1237/", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:30-2Lrwi2n8J:scholar.google.com/&output=cite&scirp=203&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D200%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=30-2Lrwi2n8J&ei=KbWsaMZzjoiJ6g_SwpG4CQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:30-2Lrwi2n8J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://scholarworks.uni.edu/cgi/viewcontent.cgi?article=2239&context=etd"}}, {"title": "Exploring the Potential of the Large Language Models (LLMs) in Identifying Misleading News Headlines", "year": "2024", "pdf_data": "arXiv:2405.03153v1  [cs.CL]  6 May 2024Exploringthe Potentialofthe Large Language Models(LLMs) in\nIdentifyingMisleadingNews Headlines\nMdMainUddin Rony\nUniversity of Maryland\nCollegePark,Maryland,USA\nmrony@umd.eduMdMahfuzulHaque\nUniversity ofMaryland\nCollegePark,Maryland,USA\nmhaque16@umd.eduMohammadAli\nUniversity ofMaryland\nCollegePark,Maryland,USA\nmali24@umd.edu\nAhmedShatilAlam\nUniversity of Oklahoma\nNorman,Oklahoma,USA\nasalam@ou.eduNaeemulHassan\nUniversity ofMaryland\nCollegePark,Maryland,USA\nnhassan@umd.edu\nABSTRACT\nIn the digital age, the prevalence of misleading news headli nes\nposes a signi\ufb01cant challenge to information integrity, nec essitat-\ning robustdetectionmechanisms. This studyexplores thee\ufb03 cacy\nof Large Language Models (LLMs) in identifying misleading v er-\nsus non-misleading news headlines. Utilizing a dataset of 6 0 arti-\ncles, sourced from both reputable and questionable outlets across\nhealth, science & tech, and business domains, we employ thre e\nLLMs\u2014ChatGPT-3.5,ChatGPT-4,andGemini\u2014forclassi\ufb01cati on.Our\nanalysis reveals signi\ufb01cant variance in model performance , with\nChatGPT-4 demonstrating superior accuracy, especially in cases\nwithunanimousannotatoragreementonmisleadingheadline s.The\nstudyemphasizestheimportanceofhuman-centeredevaluat ionin\ndevelopingLLMsthatcannavigatethecomplexitiesofmisin forma-\ntiondetection,aligningtechnicalpro\ufb01ciencywithnuance dhuman\njudgment. Our \ufb01ndings contribute to the discourse on AI ethi cs,\nemphasizing theneed for models that are not only technicall y ad-\nvanced butalso ethically aligned and sensitive tothe subtl eties of\nhuman interpretation.\n1 INTRODUCTION\nNewsheadlinesareprecursorstocomprehensivestoriesand serve\nas persuasive messages, making their accuracy and authenti city\ncrucial. Gabielkov et al. note that many readers may not proc eed\nbeyondtheheadlinestoreadthefullcontent[7];however,t heycan\nstill receive misleading information if these headlines do not ac-\ncurately represent thecontent. We use theterm Misleading N ews\nHeadlinestodescribethisparticularphenomenon.Mislead ingNews\nHeadlines arise when the headline of a news article fails to r ep-\nresent its content accurately. Consider the following exam ple for\nillustration.\nHeadline: Hottealinkedtoincreasedriskofesophageal\ncancer1\nContent: Peoplewho like hot tea may want to wait\nuntilit gets coolerbeforetaking that\ufb01rst sip.Drink-\ning more than 700 milliliters of tea at higher than\n60 degrees Celsius, or 140 degrees Fahrenheit, was\n1https://tinyurl.com/misleading-headline-example1\n1st HEAL Workshop at CHI Conference on Human Factorsin Compu ting Systems,May\n12,Honolulu, HI, USA\n2024.linked to a 90 percent increased risk of esophageal\ncancer, accordingtoa study...\n\u201cManypeopleenjoydrinkingtea,co\ufb00ee,orotherhot\nbeverages. However, according to our report, drink-\ning very hot tea can increase the risk of esophageal\ncancer,\" said lead author Farhad Islami, a researcher\nat the American Cancer Society and study lead au-\nthor, in a news release. ... In 2016, the International\nAgency for Research on Cancer said that drinking\nanydrinkover65degreesCelsiusmakesitacarcino-\ngen or something likely to cause cancer. Other stud-\nies have linked drinking hot tea and drinking exces-\nsive amounts of alcohol daily to esophageal cancer,\nas well.\nThe headline Hot tea linked to increased risk of esophageal can-\ncerismisleadingbecauseitspeci\ufb01callysinglesouthottea,de spite\nthe article indicating that the risk is associated with cons uming\nany very hot beverage. This narrow focus on hot tea could lead\nreaders to incorrectly believe that only hot tea poses this c ancer\nrisk, potentiallycausing them to overlookthe similar risk s associ-\natedwithotherhotbeverages. Consequently,readers might make\nuninformed decisions about their beverage choices, errone ously\nassuming that switching from hot tea to another hot drink, li ke\nco\ufb00ee, would mitigate their risk of esophageal cancer when t he\ntemperature,notthetypeof beverage, is crucial.\nIf the headline is misleading, it may cause a wrong impressio n,\nleading to uninformed decision-making [6]. Addressing the issue\nofMisleadingNewsHeadlinesiscriticaltorebuildingtrus tinjour-\nnalism and combatingmisinformation. Manual evaluations, while\ne\ufb00ective, are impractical due to the sheer volume and speed o f\nnews dissemination, necessitating automated solutions. H owever,\nconstructingsuchsystemspresentschallenges,particula rlytheneed\nforextensive,high-qualitydata.Large-scale,represent ativedatasets\nare essential for training robust machine learning models. This\ncomplexity underscores the signi\ufb01cance of leveraging adva nced\ntechniques likeLarge Language Models(LLMs) todetect and c las-\nsifymisleadingheadlinesaccurately,ultimatelyenhanci ngjournal-\nism\u2019s credibilityand its abilitytocounteractmisinforma tion.\nRecentadvancesinnaturallanguageprocessinghaveledtop ow-\nerfulLargeLanguageModels(LLMs)capableofunderstandin gcom-\nplexlanguages intricately[8].TheseLLMs have beensucces sfully\n1\n1stHEAL Workshop atCHI Conference onHuman Factorsin Compu ting Systems, May 12,Honolulu, HI,USA Rony etal.\napplied to identify and rectify vaccine misinformation, sh owcas-\ning their potential in public health communication and info rma-\ntionvalidation[3].However,it\u2019sessentialtoacknowledg ethatmis-\nleading headlines di\ufb00er from other forms of misinformation . Mis-\nleading headlines oftenstraddlea \ufb01ne line, potentiallypr esenting\nskewed or exaggerated information without being entirely f alse.\nThis distinct nature adds complexity to the task of utilizin g LLMs\nto detect and address misleading headlines accurately. In l ight of\nthis gray area, designing detectionmechanisms canbe morec om-\nplex,resulting inthefollowingresearch question:\nRQ:TowhatextentcanLargeLanguageModelsaccuratelyiden tify\nheadlines as misleading?\n2 RELATED WORK\n2.1 Overview ofMisleadingHeadline Detection\nMisleadingheadlinescreateadisconnectbetweenthetitle andthe\narticle\u2019scontent,leadingtopotentialmisinterpretatio nbyreaders.\nThese headlines may present overrated, false, or unsupport ed in-\nformation,aiming toattractattentionordrivewebtra\ufb03cth rough\nexaggerated or sensational content [2, 17]. They often leve rage\nemotionallanguage,biasingreadersevenbeforetheyengag ewith\nthe article, and may omit key information or emphasize less r ele-\nvant details, leading to confusion and misinformation [6, 1 5].The\nchallenge lies intheheadlines\u2019 abilitytoreinforce exist ing beliefs,\nmaking misinformation appear more credible and di\ufb03cult to c or-\nrect, thus signi\ufb01cantly impacting reader understanding an d opin-\nionformation[1,11].Akeychallengehighlightedinthelit erature\nfor automated misleading headline detectionis the inadequ acy of\nexisting datasets and NLP methods in capturing incongruenc e be-\ntween headlines and article content. This gap necessitates the de-\nvelopmentofmorenuanceddatasetsandmethodologiesthatg obe-\nyond simple agreement or disagreement models [2]. Addition ally,\nthe variability in dataset creation methods and the limitat ions of\nexistingdatasetsinrepresenting thefullscopeofmislead inghead-\nlinesposesigni\ufb01cantchallengestodevelopinge\ufb00ectiveau tomated\ndetectionsystems [9,10].\n2.2 Overview ofLLMs in NLP and\nMisinformation\nTheproliferationofLargeLanguageModels(LLMs)innatura llan-\nguage processing represents a signi\ufb01cant leap forward, ena bling\nthese models to grasp complex language structures with rema rk-\nable depth [8]. Demonstrated by their e\ufb00ectiveness in comba ting\nvaccine misinformation, LLMs hold promise for enhancing pu b-\nlic health communication and ensuring the accuracy of infor ma-\ntion[3].Nonetheless,thechallengeofmisleadingheadlin es,which\nmayconveyskewedorexaggeratedinformationwithoutbeing out-\nright false, underscores a unique dilemma. This subtlety co mpli-\ncatestheuseofLLMsfordetectingandaddressingmisleadin gcon-\ntent, revealing a gap in their application. This nuanced cha llenge\nunderlines the need for research into the capabilities of LL Ms to\ndiscern and classify misleading headlines, highlighting a critical\narea forexploration.3 METHOD\n3.1 DataCollection\nIn our study, we collected news articles from 12 sources, cat ego-\nrized into reliable (e.g., ABC News, NY Times, Washington Po st)\nand unreliable (e.g., Infowars, Lifezette) groups based on assess-\nmentsfromMediaBias/FactCheck(MBFC)2,athird-partywebsite\nthat evaluates media source credibility. Our focus was on ar ticles\nwithin the Health, Science & Tech, and Business domains. Thr ee\ndomain-knowledgeableannotatorsselected\ufb01vearticlesfr omeach\ndomain from four sources, starting from March 31st, 2022, as sess-\ning if headlines were misleading by reviewing both the headl ine\nandthecontent.Thisprocessyieldedabalancedpreliminar ydataset\nof 60 articles, comprising 30 misleading and 30 non-mislead ing\nheadlines.\nDuring annotation, each annotator independently reviewed 40\narticles (20 misleading and 20 non-misleading) compiled by the\nothers, without source identi\ufb01ers to avoid bias. The review pro-\ncessinvolvedthreeroundsofdetailedexaminationtolabel articles\nasmisleadingornon-misleading,withannotationsre\ufb02ecti ngvary-\ningcon\ufb01dencelevels. Consensus was reached on18articles b eing\nunanimously misleading, while at least two annotators agre ed on\n27 articles. Given our rigorous criterion that a headline is consid-\nered misleading if it could potentially mislead at least one reader,\nthe\ufb01naldatasetconsistedof37misleadingand23non-misle ading\nheadlines.\n3.2 LLM Evaluation\nChatGPT(version 3.5 and 4) and Gemini evaluated the collect ed\nheadlines for labels and explanations, aiming to understan d their\ncapabilitytoidentify misleading headlines.\nTheheadlinesandrelevantnewscontentweresubmittedtoLL Ms\nfor assessment. The LLMs determined if the headline is misle ad-\ning and explain their decisions. The API requests were sent t o\ntheLLMs,which evaluatedthenews content\u2019srepresentatio n and\nprovided a decision and an explanation.A sample request pro mpt\nwouldbeas follows:\nprompt=\u201cEvaluateifthefollowingheadlineismislead-\ningbasedonthenewscontentprovided.Headline:[Your\nHeadlineHere]NewsContent:[YourNewsContentHere]\nIs this headline misleading? Please explain your deci-\nsion.\"\n4 RESULTS\nThisstudyaimedtoassessthecapabilityoflargelanguagem odels\n(LLMs) \u2014 speci\ufb01cally ChatGPT-3.5, ChatGPT-4, and Gemini \u2014 t o\ndetectand explain misleading news headlines accurately.E mploy-\ning a dataset of 60 news articles, where human annotators ide n-\nti\ufb01ed 37 as having misleading headlines, we explored how the se\nLLMs could align with human judgment in identifying misinfo r-\nmation.\n4.1 LLM\u2019s Classi\ufb01cation Performance Analysis\nThissectionpresentsthe\ufb01ndingsfromevaluatingthreeLar geLan-\nguage Models (LLMs) \u2014 ChatGPT-3.5, ChatGPT-4.0, and Gemini\nbasedona binary classi\ufb01cationtask.\n2https://mediabiasfactcheck.com/\n2\nIdentifying Misleading Headlines withLLM 1stHEAL Worksho p at CHI Conference on Human Factorsin Computing Systems, Ma y 12,Honolulu, HI, USA\nTable1:PerformanceofLLMsinDetectingMisleadingNews\nHeadlines\nModel Non-misleading Misleading Accuracy\nPrecision Recall F1 Precision Recall F1\nChatGPT-3.5 1.00 0.09 0.16 0.46 1.00 0.63 0.48\nChatGPT-4 0.85 0.97 0.90 0.95 0.77 0.85 0.88\nGemini 0.68 0.79 0.73 0.65 0.50 0.57 0.67\n4.1.1 LLMs\u2019OverallPerformanceAnalysis. EachLLMwasassessed\nthrough precision, recall, f1-score, and overall accuracy metrics,\nproviding insight into their e\ufb00ectiveness in addressing th eRQ1.\nChatGPT-3.5Performance TheperformanceofChatGPT-3.5\nshowed a high level of precision in identifying non-mislead ing\nheadlines(precision:1.00)butwithanotablylowrecallra te(recall:\n0.09), indicating a tendency to misclassify non-misleadin g head-\nlinesasmisleading.Conversely,formisleadingheadlines ,themodel\ndemonstrated a lower precision (0.46) but a perfect recall s core\n(1.00), suggesting it was e\ufb00ective in identifying misleadi ng head-\nlines but with a considerable rate of false positives. The ac curacy\nofChatGPT-3.5stoodat48%,withamacro-averagef1-scoreo f0.39,\nindicating a moderate level of imbalance in its classi\ufb01cati on capa-\nbility,skewed towards identifying misleading headlines.\nChatGPT-4.0Performance ChatGPT-4.0signi\ufb01cantlyimproved\noveritspredecessor,achieving anaccuracyof88%.Itshowe dhigh\nprecisionandrecallinidentifyingbothmisleading(preci sion:0.95,\nrecall: 0.77) and non-misleading headlines (precision: 0. 85, recall:\n0.97),re\ufb02ectedinabalancedf1-scorefornon-misleading( 0.90)and\nmisleading (0.85) headlines. The macro and weighted averag e f1-\nscores were both close to 0.88, illustrating a robust capabi lity in\naccuratelyclassifyingheadlines whilemaintainingabala ncedper-\nformance across bothclasses.\nGemini Performance Gemini\u2019s performance presented a bal-\nanced approach between the two extremes of ChatGPT-3.5 and\nChatGPT-4.0,withanoverallaccuracyof67%.Itdemonstrat edmod-\nerate precision and recall for non-misleading (precision: 0.68, re-\ncall: 0.79) and misleading headlines (precision: 0.65, rec all: 0.50),\nleadingtoanf1-scoreof0.73and0.57,respectively.Thema croand\nweightedaveragef1-scoreswere0.65and0.66,indicatinga reason-\nable butnot optimalbalance in classi\ufb01cationcapability ac ross the\ntwocategories.\n4.1.2 LLM\u2019sPerformancebyConsensusLevel. Thee\ufb03cacyofLLMs\ninidentifyingmisleadingcontentwasexaminedincontexts ofunan-\nimousconsensus byannotatorsversusmixedconsensus(Majo rity\nand MinorityMisleading) ( SeeinTable2).\nUnanimousConsensus Inscenarioswherehumanannotators\nunanimouslyagreedonthenatureoftheheadlines(eithermi slead-\ning or not misleading), ChatGPT-4 exhibited the highest per for-\nmance, accurately classifying misleading headlines with a n accu-\nracy of 83.3% and non-misleading headlines with 95.7%. Gemi ni\nfollowed with 61.1% accuracy for misleading and 73.9% for no n-\nmisleadingheadlines.ChatGPT-3.5showedatopmostaccura cy,with\n94.4%formisleadingbutperformedpoorlyfornon-misleadi nghead-\nlines with 8.7% accuracy. These results indicate a potentia l align-\nment between advanced LLM judgments and unanimous human\nconsensus.\nMixedConsensus(Majority&Minority Misleading)Table 2: LLMs\u2019 PerformancebyHumanConsensusLevel\nConsensuslevel Model Is Misleading? #of Headlines\nUnanimous NotMisleadingGeminiYes 6\nNo 17\nChatGPT-4Yes 1\nNo 22\nChatGPT-3.5Yes 21\nNo 2\nMinorityMisleadingGeminiYes 2\nNo 8\nChatGPT-4Yes 2\nNo 8\nChatGPT-3.5Yes 9\nNo 1\nMajorityMisleadingGeminiYes 2\nNo 7\nChatGPT-4Yes 3\nNo 6\nChatGPT-3.5Yes 8\nNo 1\nUnanimous MisleadingGeminiYes 11\nNo 7\nChatGPT-4Yes 15\nNo 3\nChatGPT-3.5Yes 17\nNo 1\n\u2022Majority Misleading: When a majority (but not all) of the\nannotatorsidenti\ufb01ed headlines as misleading, ChatGPT-4\u2019 s\nperformancesigni\ufb01cantly decreased to33.33%accuracyfor\nmisleadingheadlines.WhileGeminiexperiencedamorepro-\nnounced dropto22.2%,ChatGPT-3.5 demonstrateda better\nperformancewithanaccuracyratingof88.9%,whichisgen-\nerally due to the tendency to misclassify non-misleading\nheadlines as misleading. The results of this study suggest\nthat there may be challenges in cases where there is less\nclear-cuthuman agreement.\n\u2022Minority Misleading: For headlines deemed misleading by\na minority of annotators, ChatGPT-4\u2019s accuracy was 20%.\nAlthoughGemini exhibitedthesameaccuracyasChatGPT-\n4,ChatGPT-3.5performedsigni\ufb01cantlybetter(90%)thanit s\ncounterpartmodels,whichunderscores thedi\ufb03cultyLLMs\nhave when thereis a lack ofstronghuman consensus.\n5 DISCUSSION\nTheevaluationofLargeLanguageModels(LLMs)indistingui shing\nmisleadingnewsheadlinesrevealsessentialinsightsinto theinter-\nsection of arti\ufb01cial intelligence and media integrity. Thi s discus-\nsiondelvesintotheimplicationsofour\ufb01ndingswithintheb roader\ncontext of human-centered evaluation and auditing methods for\nLLMs,highlightingthenuancedrolethesemodelsplayinsup port-\ningstakeholders across thedigital informationlandscape .\n5.1 Integrating Human-Centered Evaluation in\nLLM Auditing\nAspresentedinour\ufb01ndings,theexplorationofthee\ufb00ective nessof\nlargelanguagemodels(LLMs)indiscerningmisleadingnews head-\nlinesemphasizestheimperativeforincorporatinghuman-c entered\nevaluationandauditingframeworks.Thisapproachnotonly bench-\nmarkstheperformanceofLLMsagainsthumanjudgmentbutals o\n3\n1stHEAL Workshop atCHI Conference onHuman Factorsin Compu ting Systems, May 12,Honolulu, HI,USA Rony etal.\nalignswiththebroaderdiscourseonenhancingAIinterpret ability\nand reliability inmedia contexts [4].\n5.2 LLM Performance and Human Consensus\n5.2.1 AlignmentwithUnanimousConsensus. Thebetterperformance\nofChatGPT-4ininstancesofunanimoushumanconsensusonmi s-\nleading headlines highlights the advancements in AI\u2019s capa bility\nto parallel human reasoning in clear-cut scenarios. This ob serva-\ntionresonateswiththeliteratureemphasizingtheneedfor AIsys-\ntems to understand and replicate human-like judgment in tas ks\nrequiring nuanced interpretation [13]. Such alignment is c rucial\nfor stakeholders, including media professionals and conte nt mod-\nerators, who rely on AI to \ufb01lter through vast amounts of data f or\npotentialmisinformation.\n5.3 Navigating Mixed Consensus\nThenuancedchallengepresentedbymixedhumanconsensushi gh-\nlights a frontier in AI development. The di\ufb00erential perfor mance\nof LLMs, particularlyin majorityand minority misleading s cenar-\nios, re\ufb02ects the complexity of human cognition and the subje c-\ntivenatureofmisinformation.Thisobservationalignswit htheAI\nethics community\u2019s push for models that are not just technic ally\nadvanced but also attuned to the nuances of human thinking an d\nethical concerns [12,16].\n5.4 Implications forStakeholders\nThepracticalimplicationsofthese\ufb01ndings aremanifold.F orjour-\nnalists andmediaoutlets,thedeploymentofLLMsthataccur ately\nidentifymisleadingheadlinescouldrepresentasigni\ufb01can tstepfor-\nward in maintaining informational integrity. For develope rs and\nAI researchers, ourstudyhighlights theimportanceofembe dding\nhuman-centereddesignprinciplesinthedevelopmentofLLM s,en-\nsuring these toolsare bothe\ufb00ective and ethically aligned w ith so-\ncietal norms [14].\nMoreover, for policymakers and regulators, understanding the\ncapabilitiesandlimitationsofLLMsinidentifyingmisinf ormation\nis crucial for crafting guidelines that promote responsibl e AI use\nin journalism and beyond. This aligns with ongoing discussi ons\nabout the regulatory frameworks necessary to govern AI\u2019s ap pli-\ncationin sensitive societal domains [5].\n5.5 FutureResearch Direction\nFutureresearchshouldaimtobridgethegapbetweenLLMperf or-\nmance and the diverse ranges of human judgment, particularl yin\nambiguous or controversial scenarios. This includes inves tigating\nmethodologiesfor incorporatingethical reasoning and bia s recog-\nnition into LLM training processes. Additionally, expandi ng the\nscopeofLLMtrainingtoencompassmultimodalcontentcould en-\nhance their applicability across various media formats, o\ufb00 ering a\nmoreholistic approachtomisinformation detection.\nA critical area for future exploration is the examination of ex-\nplanationsgeneratedbyLLMsinidentifyingmisleadinghea dlines\nand how these explanations align with human rationale. Unde r-\nstanding the logic and reasoning behind LLM decisions is ess en-\ntial for improving their reliability and trustworthiness. Analyzing\nLLM-generated explanationscanprovideinsightsintothem odels\u2019\ninterpretive processes, identifying areas where they may d iverge\nfrom human thought patterns. This line of inquiry not only co n-\ntributestothedevelopmentofmoresophisticatedandhuman -likeLLMsbutalsosupportsthecreationofAIsystemswhosedecis ion-\nmaking processes are transparent, explainable, and, most i mpor-\ntantly,aligned withethical standards and societalexpect ations.\n6 CONCLUSION\nOur investigation into the capabilities of Large Language M odels\n(LLMs)toidentifymisleadingnewsheadlineshighlightsth epoten-\ntial and challenges inherent in aligning AI with human judgm ent\nand ethical considerations. The study reveals that while mo dels\nlike ChatGPT-4 show promise in closely mirroring human deci -\nsions,particularlyinclear-cutcases,discrepancies inp erformance\nacross varying levels of human consensus highlight the comp lex-\nity of misleading headline detection. The \ufb01ndings advocate for a\nhuman-centered approach in the development and evaluation of\nLLMs, emphasizing the need for models that are not only techn i-\ncally adept but also sensitive to the nuances of human ethics and\nreasoning. Future research directions, including examini ng LLM-\ngeneratedexplanationsandexpandingtrainingtomultimod alcon-\ntent, promise to further bridge the gap between AI and human\njudgment, paving the way for more reliable, ethical, and e\ufb00e ctive\ntoolsincombatingmisinformation.\nREFERENCES\n[1] 2015. Thepsychologyofmisinformation. Australasianscience 36,2(2015),21\u2013.\n[2] Sophie Chesney,MariaLiakata,MassimoPoesio, andMatt hewPurver.2017. In-\ncongruent headlines:Yetanotherwaytomisleadyourreader s.InProceedingsof\nthe 2017emnlp workshop:Natural language processingmeets journalism . 56\u201361.\n[3] GiovannaDeiana,MarcoDettori,Antonella Arghittu, An tonio Azara,Giovanni\nGabutti,andPaoloCastiglia.2023.Arti\ufb01cialIntelligenc eandPublicHealth:Eval-\nuatingChatGPTResponsestoVaccinationMythsandMisconce ptions.Vaccines\n11,7 (2023), 1217.\n[4] Nicholas Diakopoulos and Michael Koliska. 2017. Algori thmic transparencyin\nthe news media. Digitaljournalism 5,7 (2017), 809\u2013828.\n[5] VirginiaDignum.2019. Responsible arti\ufb01cialintelligence: howtodevelop and use\nAI ina responsible way . Vol. 2156. Springer.\n[6] Ullrich KH Ecker,Stephan Lewandowsky,Ee PinChang,and RekhaPillai. 2014.\nThee\ufb00ectsofsubtlemisinformationinnewsheadlines. Journal of experimental\npsychology:applied 20,4 (2014), 323.\n[7] Maksym Gabielkov, Arthi Ramachandran, Augustin Chaint reau, and Arnaud\nLegout. 2016. Social clicks: What and who gets read on Twitte r?. InProceed-\nings of the 2016ACMSIGMETRICS international conference on measurementand\nmodeling ofcomputer science .179\u2013192.\n[8] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veys eh, Thien Huu\nNguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Rot h. 2023. Recent\nadvancesinnaturallanguageprocessingvialargepre-trai nedlanguagemodels:\nA survey. Comput.Surveys 56, 2(2023), 1\u201340.\n[9] Rahul Mishra, Piyush Yadav, Remi Calizzano, and Markus L eippold. 2020.\nMuSeM:Detectingincongruentnewsheadlinesusingmutuala ttentivesemantic\nmatching. In 2020 19th IEEE International Conference on Machine Learnin g and\nApplications (ICMLA) . IEEE, 709\u2013716.\n[10] Kunwoo Park, Taegyun Kim, Seunghyun Yoon, Meeyoung Cha , and Kyomin\nJung. 2020. BaitWatcher: A lightweight web interface for th e detection of in-\ncongruent news headlines. Disinformation, Misinformation, and Fake News in\nSocial Media: EmergingResearchChallenges and Opportunit ies(2020), 229\u2013252.\n[11] Michal Piksa, Karolina Noworyta, Jan Piasecki, Pawel G wiazdzinski, Alek-\nsander B Gundersen, Jonas Kunst, and Rafal Rygula. 2022. Cog nitive Processes\nand Personality Traits Underlying Four Phenotypes of Susce ptibility to (Mis)\nInformation. Frontiersin Psychiatry 13(2022), 1142.\n[12] Iyad Rahwan, Manuel Cebrian, Nick Obradovich, Josh Bon gard, Jean-Fran\u00e7ois\nBonnefon, Cynthia Breazeal, Jacob W Crandall, Nicholas A Ch ristakis, Iain D\nCouzin, Matthew O Jackson, et al. 2019. Machine behaviour. Nature568, 7753\n(2019), 477\u2013486.\n[13] Cynthia Rudin. 2019. Stop explaining black box machine learning models for\nhigh stakesdecisions and use interpretable models instead .Nature machine in-\ntelligence 1, 5(2019), 206\u2013215.\n[14] BenShneiderman.2020. Human-centeredarti\ufb01cialinte lligence: Reliable,safe&\ntrustworthy. International Journal of Human\u2013Computer Interaction 36, 6 (2020),\n495\u2013504.\n[15] WeiWeiandXiaojunWan.2017.Learningtoidentifyambi guousandmisleading\nnews headlines. arXiv preprintarXiv:1705.06031 (2017).\n4\nIdentifying Misleading Headlines withLLM 1stHEAL Worksho p at CHI Conference on Human Factorsin Computing Systems, Ma y 12,Honolulu, HI, USA\n[16] JessWhittlestone,RuneNyrup,AnnaAlexandrova,andS tephenCave.2019.The\nroleandlimitsofprinciplesinAIethics:Towardsafocuson tensions.In Proceed-\ningsof the2019AAAI/ACMConferenceon AI,Ethics,and Socie ty. 195\u2013200.[17] Seunghyun Yoon, Kunwoo Park, Joongbo Shin, Hongjun Lim , Seungpil Won,\nMeeyoung Cha, and Kyomin Jung. 2019. Detecting incongruity between news\nheadline and body text via a deep hierarchical encoder. In Proceedings of the\nAAAIconference onarti\ufb01cial intelligence , Vol. 33.791\u2013800.\n5\nThis figure \"acm-jdslogo.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2405.03153v1", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Exploring the Potential of the Large Language Models (LLMs) in Identifying Misleading News Headlines", "author": ["MMU Rony", "MM Haque", "M Ali", "AS Alam"], "pub_year": "2024", "venue": "arXiv preprint arXiv \u2026", "abstract": "In the digital age, the prevalence of misleading news headlines poses a significant challenge  to information integrity, necessitating robust detection mechanisms. This study explores the"}, "filled": false, "gsrank": 205, "pub_url": "https://arxiv.org/abs/2405.03153", "author_id": ["FOLUQWYAAAAJ", "vtx7jw0AAAAJ", "5Lh1i7UAAAAJ", "idlnt2oAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:vCCedoP_0pUJ:scholar.google.com/&output=cite&scirp=204&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D200%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=vCCedoP_0pUJ&ei=KbWsaMZzjoiJ6g_SwpG4CQ&json=", "num_citations": 3, "citedby_url": "/scholar?cites=10795972196856373436&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:vCCedoP_0pUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2405.03153"}}, {"title": "On Context-aware Detection of Cherry-picking in News Reporting", "year": "2024", "pdf_data": "On Context-aware Detection of Cherry-picking in News Reporting\nIsraa Jaradat , Haiqi Zhang, Chengkai Li\nDepartment of Computer Science and Engineering\nUniversity of Texas at Arlington\nArlington, TX, USA\n{israa.jaradat,haiqi.zhang}@mavs.uta.edu, cli@uta.edu\nAbstract\nCherry-picking refers to the deliberate selection\nof evidence or facts that favor a particular view-\npoint while ignoring or distorting evidence that\nsupports an opposing perspective. Manually\nidentifying cherry-picked statements in news\nstories can be challenging. In this study, we\nintroduce a novel approach to detecting cherry-\npicked statements by identifying missing im-\nportant statements in a target news story using\nlanguage models and contextual information\nfrom other news sources. Furthermore, this re-\nsearch introduces a novel dataset specifically\ndesigned for training and evaluating cherry-\npicking detection models. Our best performing\nmodel achieves an F-1 score of about 89% in\ndetecting important statements. Moreover, re-\nsults show the effectiveness of incorporating\nexternal knowledge from alternative narratives\nwhen assessing statement importance.\n1 Introduction\nThe erosion of citizens\u2019 trust in the media has\nreached a critical stage, posing significant harm\nto the foundations of democracy (Dan et al., 2021;\nDruckman and Parkin, 2005; Chiang and Knight,\n2011). Research shows evidence of a decline in me-\ndia trust among citizens (Str\u00f6mb\u00e4ck et al., 2020).\nMedia losing trust can be attributed to the percep-\ntion of political bias in media reporting (Lee, 2010)\nand consumers\u2019 exposure to disinformation (Lee\net al., 2023; Hameleers et al., 2022).\nCherry-picking , a widely used but often imper-\nceptible practice in news reporting, is a device\nthat aids bias and disinformation (Vincent, 2019;\nPaul and Elder, 2019). It is defined as a logical\nfallacy that involves deliberately selecting (or ex-\naggerating) facts that bolster a certain argument\nand omitting (or under-emphasizing) facts that sup-\nport the opposing viewpoint (Best, 2004). Its com-\nmon aliases in literature include one-sideness (Paul\nand Elder, 2008), argument by half-truth (Sproule,\n2001), and case-making (Lee, 1945).In news reporting, cherry-picking can be ob-\nserved across scales as large as in selecting which\nevents to cover, and as subtle as in choosing specific\nwords for a particular story. Furthermore, cherry-\npicking can manifest in various types of data such\nas charts (Asudeh et al., 2020) or when particu-\nlar statistics (Best, 2004) are chosen to bolster a\ncertain claim (Wu et al., 2017).\nThis study\u2019s focus is cherry-picking statements\nwithin news articles, particularly when only certain\nstatements are included while others are omitted.\nA multitude of media outlets, including prominent\nand highly regarded ones, exhibit biases in var-\nious topics within domains such as politics and\nscience (Baron, 2006). These media outlets often\nexercise caution in curating the statements included\nin their stories covering controversial topics (Paul\nand Elder, 2019, 2008). This selection of state-\nments is capable of influencing public opinion on\npivotal subjects, such as climate change, vaccina-\ntion, or elections, by misleading readers to perceive\npartial truth as holistic truth (Fleming, 1995).\nWe introduce a novel method for identifying\ncherry-picking in news stories by leveraging con-\ntext derived from other narratives. Our method\nemploys language models, including fine-tuned\nembedding models and few/zero shot prompting\nof generative models, to assess statements\u2019 impor-\ntance based on a given context to identify cherry-\npicked ones. The fine-tuned models are trained\non the novel Cherry dataset we curated. The pro-\nposed method shows promising results in inferring\na statement\u2019s importance given external contextual\ninformation. Our top-performing model achieves\nan F-1 score of 89% and an accuracy of 90%.\nTo the best of our knowledge, this is the first\nstudy that addresses the detection of cherry-picking\nin text using computational approach. The key\ncontributions of this work are as follows: (a) The\nformulation and modeling of the problem from a\ncomputational perspective. (b) The novel end-to-arXiv:2401.05650v2  [cs.CL]  20 Jul 2024\nend cherry-picking detection approach that infuses\ncontextual information with statements through dif-\nferent language modeling techniques. (c) A new\ncherry-picking annotated dataset which can prove\nuseful in not only detection but also other lines\nof research related to cherry-picking. (d) Thor-\nough experimentation to determine the most ef-\nfective context to consider when assessing cherry-\npicking. All artifacts of this work are released\nunder the GNU General Public License v3.0 at our\nanonymous Github repository https://github.\ncom/cherry-pic/Cherry .\n2 Related Work\nResearch on media bias can be classified into\nthree levels based on text granularity: word-level,\nsentence-level, and article-level (Chen et al., 2020).\nDistinguishing between these levels is not always\nstraightforward, as they often intersect and influ-\nence each other. For instance, word-level bias\nis dependent on sentence or article context, and\nsimilarly sentence-level bias is contingent on the\nbroader article context.\n\u2022Word-level bias. The choice of words in news arti-\ncles can influence the audience\u2019s opinions towards\na particular event (Hamborg, 2020). Word-level\nbias encompasses two primary categories: fram-\ning bias and epistemological bias (Recasens et al.,\n2013). The connotations associated with the same\nword can vary by the specific context in which they\nare employed (Spinde et al., 2021b).\n\u2022Sentence-level bias. Numerous studies focus on\ndetecting bias at sentence level, considering it as\nthe fundamental level that can be combined with\narticle-level bias (Spinde et al., 2021a). Using lexi-\ncon of bias words, Hube and Fetahu (2018) intro-\nduced a method for identifying biased statements\nfrom Wikipedia. In Lei et al. (2022), biased sen-\ntences were identified in news articles, serving to\nexplain the overall bias present throughout articles.\nFocusing on media\u2019s sentiment toward target enti-\nties, Fan et al. (2019) examined sentence-level bias\nalong with article context.\n\u2022Article-level bias. Baly et al. (2020) constructed\na dataset of news articles annotated with left, cen-\nter, and right leanings, then used machine learning\nmodels to predict articles\u2019 political ideology.\nPrior research on media bias primarily focused\non predisposition towards particular viewpoints,\nleaving a notable gap in exploring media bias re-\nlated to cherry-picking in news articles. Some stud-ies have examined the impact of cherry-picking in\ncomputational fact-checking (Wu et al., 2017; Lin\net al., 2021) and the detection of cherry-picking\nin trendlines (Asudeh et al., 2020). Nevertheless,\nthere has been no specific investigation into end-to-\nend cherry-picking detection in news articles.\n3 Cherry-picking Facts in News Stories:\nProblem Definition and Modeling\nMedia outlets cherry-pick in news reporting by\nprioritizing less crucial statements that align with\ntheir biases to appear in their narrative, or neglect-\ning more important relevant statements, especially\nthose against their biases (Paul and Elder, 2008;\nVincent, 2019; Brown, 1963). To offer a more\nillustrative portrayal of cherry-picked statements,\nTable 5 in Appendix A provides a juxtaposition\nof three authentic news stories from distinctively\nbiased outlets covering the same event.\nThe example in Table 5, specifically rows 3, 7,\nand 8 demonstrates two important insights that\nguide our approach. First , cherry-picked state-\nments can be potentially discerned by their impor-\ntance to the event being covered. Therefore, we\nexploit statement importance in detecting cherry-\npicking. We opt for assessing cherry-picking on the\nstatement (i.e., sentence) level since it is a common\npractice in news reporting to represent a singular\nidea in one sentence (Mencher and Shilton, 1997).\nNote that stories often face constraints on space due\nto factors such as reader\u2019s attention span, which re-\nsults in prioritizing the most important statements\nto appear in an article. Second , the importance of a\nstatement is relevant, contextualized by whether\nit is discussed or not in other stories about the\nsame event\u2014simply put, together all stories de-\npict a comprehensive picture. For a comprehensive\nand representative context, it is crucial to incorpo-\nrate narratives from other sources with different\nbiases (Lee and Lee, 1939).\nWe categorize cherry-picking into two forms:\noveremphasis andunderemphasis . Overemphasis\noccurs when the coverage of a particular event fo-\ncuses on non-essential statements and magnifies\nthem, creating the impression that they are signifi-\ncant components of the event (Fleming, 1995; Lee,\n1945). Conversely, underemphasis occurs when a\nstory entirely excludes or slants a crucial fact from\nan event coverage (Sproule, 2001; Lakomy, 2020).\nThe detection of overemphasis requires not only\nevaluating a statement\u2019s importance and presence\nbut also analyzing other factors, such as exagger-\nation cues, sentiment, opinion, in addition to mea-\nsuring repetition and the amount of space allocated\nto the statements within a report. Studying the pres-\nence of a statement requires a different approach\nthan studying its absence. For instance, detecting\nexaggeration requires computing lexical features\nwhich is not applicable to the analysis of textual ab-\nsence since there is no text to begin with. We will\naddress cherry-picking by overemphasis in future\nwork, and we focus on cherry-picking by under-\nemphasis in this study. To identify underempha-\nsis, analysis of textual absence using context from\nother narratives and sources is required. To find\nimportant statements in an event, one needs to read\nstories from other distinctively biased sources, then\nevaluate the importance of each statement within\nthe story based on their understanding of the event.\nDrawing from the discussion above, the prob-\nlem of detecting cherry-picking through under-\nemphasis is formulated as follows. Consider an\nevent e={d1, ..., d n}comprising various narratives\n(i.e., documents). Each document di={s1, ..., s m}\ncontains a collection of statements (i.e., sentences,\nas we do not consider multiple sentences collec-\ntively as a single statement in this study). The state-\nments from all documents in ecollectively form\nthe universal set of statements Se={d1\u222ad2...\u222adn}\nfor the entire event. Our goal is to determine the set\nof important statements that are missing from each\ndocument, i.e., ci=Ie\u2212diin which Ie\u2282Serepre-\nsents the important statements regarding e, among\nall statements. The problem then reduces to finding\nIebased on the event\u2019s context. In this study we\nuse documents from distinctively biased sources\ncovering event eas its context. This is based on\nthe need for alternative narratives (i.e., the other\nperspective) to assess cherry-picking (Lee and Lee,\n1939). To determine outlets biases, we utilized cate-\ngorization of media outlet bias based on unanimous\nagreement from three sources\u2014Media Bias Fact\nCheck (MBFC),1AllSides,2and Ad Fontes Me-\ndia.3These sources are widely utilized in media\nbias analysis, providing researchers with tools and\nratings to evaluate the political bias and credibility\nof news sources. The details of how they assess\nbias in news outlets can be found in Appendix B.\nFinally, we utilize cito assess cherry-picking of\n1https://mediabiasfactcheck.com/\n2https://www.allsides.com/\n3https://adfontesmedia.com/a given outlet by counting the number of cherry-\npicked statements in each document difrom the\noutlet, followed by averaging this number across a\ntime span of events.\n4 Dataset\nTo train and evaluate our models for gauging state-\nment importance given event context, we curated\na novel cherry-picking detection dataset Cherry .\nOur dataset is composed of 3,346 examples in total.\nEvery example contains a statement s\u2208Se(i.e.,\na single sentence from a news report of event e),\nan event context d\u2208ewhich contains an article\ncovering the event collected from another source,\nand an importance label Ythat indicates whether\nthe statement is important to the event or not.\n4.1 Data Collection\nTo generate the examples in the dataset, we com-\npiled a list of 41 noteworthy news sources, such\nas CNN.com, Reuters.com, and FoxNews.com, se-\nlected for their high publication frequency and dis-\ntinct political biases as determined by the ratings\nfrom the aforementioned three websites.4Subse-\nquently, we employed GDELT\u2019s API (Leetaru and\nSchrodt, 2013) to gather all news articles except\nfor opinion articles and editorials from the chosen\nsources. The collected news articles cover the time\nspan between December 2019 and January 2021.\nNext, we used DBSCAN (Ester et al., 1996) to\ncluster the collected articles into events based on\nthe cosine similarity calculated between the vectors\nof the articles, with each article being vectorized\nthrough the concatenation of BERT (Devlin et al.,\n2019) and TF-IDF representations derived from its\nheadline and initial paragraph. We set DBSCAN\u2019s\nparameters to 0.04 for the radius of neighborhood\naround data points ( \u03b5) and 2 for the minimum clus-\nter size.\nAfter a manual inspection of the generated clus-\nters, we curated a subset of 82 clusters, each corre-\nsponding to a distinct controversial event. Exam-\nples of such events include the January 6th, 2021\nU.S. Capitol attack and the alleged foreign inter-\nvention in the 2020 U.S. presidential elections.5\nFor each event, we used NLTK (Bird et al., 2009)\nsentence tokenizer to segment the articles into state-\nments, and then clustered the statements based on\n4The full list of the 41 sources can be found at https:\n//github.com/cherry-pic/Cherry .\n5The full list of the 82 events is available at https://\ngithub.com/cherry-pic/Cherry .\nsemantic similarity. Similar to how articles were\nclustered, we used BERT and TF-IDF to represent\nstatements and applied DBSCAN with an value of\n0.07 for \u03b5, a minimum cluster size of 2, and co-\nsine similarity of statement vectors as the similarity\nfunction. Table 6 in Appendix C shows a sample\nof clustered statements within a single event. State-\nment clustering facilitates data augmentation on\nthe dataset, where the collected label for a single\nstatement was cast over all statements in the same\ncluster. As a result, multiple examples were labeled\nwith a single example labeling effort.\nThe data collection, clustering, and segmentation\npipeline is depicted in Figure 1. After clustering the\nstatements, we fed them along with their respective\ncontext into our custom data annotation tool, as\nexplained in Section 4.2.\nFigure 1: Data collection and preparation pipeline.\n4.2 Data Annotation\nWe developed a data annotation tool6to collect\nlabels for statements that reflect the statements\u2019\nimportance to corresponding events. The annota-\ntors comprise one faculty member and nine Ph.D.\nstudents from a university research lab. They are\ndiverse in terms of cultural background, nationality,\ngender, and age. The ensuing discussion in this\nsection provides a brief explanation of the data an-\nnotation tool and process, while more details can\nbe found in Appendix D.\nFor each event, the annotators are presented\nwith a news article from a least-biased source (e.g.,\nReuters.com), along with clusters of statements re-\nlated to the event, as shown in Figure 5 in Appendix\nD. The annotators are required to thoroughly read\n6Public URL withheld for anonymity.the articles and gain a comprehensive understand-\ning of the event covered. They are instructed to\nassign one of the following five labels to the clus-\nter: (1) very important , (2) kind of important , (3)\nnot very important , (4) the excerpts might be incor-\nrect, and (5) I am not sure . These labels convey\nthe perceived level of importance of the statements\nin the cluster. Following the collection of labels\nfrom annotators, we filtered out examples with less\nthan 3 annotators or less than 75% agreement ratio\n(i.e., number of majority votes divided by the total\nnumber of annotators for an example).\n4.3 Dataset Statistics\nConsidering the inherent difficulty in delineating\nthe decision boundary between the different labels\ndescribed in Section 4.2), we opted to explore vari-\nous binary and multinomial classification configura-\ntions as depicted in Table 1. This involved merging\ncertain labels into a single class or excluding spe-\ncific labels from the dataset. The class distribution\nfor each of the four classification configurations is\nprovided in Table 1. For each configuration, we\nsplit the dataset randomly into 85% for training and\n15% for testing.\nEach example in the dataset consists of labels,\ncontext collected from other articles, and state-\nments. Multiple statements stem from the same\nevent, and consequently the context repeats within\nthe dataset examples. This leads to half of the\nexamples in the dataset sharing the same context.\nHence, we split the dataset between testing and\ntraining by events instead of stratification to avoid\nany data leakage (i.e., model\u2019s exposure to event\ncontexts from the testing dataset during training).\nConf. Class 1 Class 2 Class 3\n1 {(1)} {(2),(3),(4),(5)} -\n2175 (64%) 1232 (36%) -\n2 {(1)} {(2),(3)} -\n2175 (65%) 1171 (35%) -\n3 {(1)} {(2),(3)} {(4),(5)}\n2175 (64%) 1171 (34%) 61 (2%)\n4 {(1)} {(2)} {(3)}\n2175 (65%) 667 (20%) 504 (15%)\nTable 1: Label combinations and class distribution (num-\nber of examples and ratio with regard to the whole\ndataset) for each of the four classification configura-\ntions. The label for each index from (1) to (5) can be\nfound in Section 4.2.\n5 Models\nWe develop cherry-picking models that, given an\nevent and a statement, estimate an importance\nscore for the statement with regard to the event.\nOur models emulate the approach taken by human\nannotators\u2014reading the context about the event\nand then assessing the statement\u2019s importance con-\nsidering the context. This section describes how\nvarious techniques can be employed for this task,\nincluding supervised, unsupervised, and zero-shot\ndemonstration methods.\n5.1 Fine-tuned Supervised Models\nTo consider context while estimating a statement\u2019s\nimportance, we fine-tuned supervised language\nmodels using the sequence pair classification task\non our dataset (refer to Figure 6 in Appendix E for\na detailed illustration). A sequence pair consists\nof a statement and the corresponding context. The\ncontext is taken from other articles within the event\ncollection. If the token limit of a certain language\nmodel is exceeded, the sequence is right-truncated.\nThe encoded sequence is represented as a vector\nobtained from the output of the [CLS] token. This\nvector is subsequently passed through a linear layer\nto produce a class probability.\nWe employed both BERT (Devlin et al., 2019)\nand Longformer (Beltagy et al., 2020) as our su-\npervised models. Unlike BERT, Longformer ex-\nhibits linear scaling with input sequence length.\nAs a result, it enables the processing of lengthier\ndocuments, accommodating approximately 4096\ntokens compared to BERT\u2019s limit of 512 tokens.\nThis capability aligns well with our requirement\nfor typically long input sequences of context (i.e.,\nfull news articles). The primary factor that distin-\nguishes Longformer as a computationally efficient\nvariant from BERT is its novel self-attention mecha-\nnism. Longformer applies self-attention over a slid-\ning window or dilated window of tokens, instead\nof every token in the input sequence. Additionally,\nto learn task-specific representations, Longformer\nuses global attention on preselected tokens form\nthe input sequence. These tokens attend to every\nother token in the sequence, and every token in the\nsequence attends only to these preselected tokens,\nwhich facilitates the learning of task-specific repre-\nsentations (Beltagy et al., 2020), further enhancing\nits suitability for our needs.\n5.2 Large Language Models with Zero-shot or\nFew-shot Demonstration\nThe advantage of zero-shot and few-shot demon-\nstration is that it does not require large amount of\ndata or human supervision. In utilizing the genera-tive language model GPT (Brown et al., 2020) to\nassess a statement\u2019s importance, our approach imi-\ntates the way human coders annotated the Cherry\ndataset. We use the same prompt in the annota-\ntion interface (i.e., the annotation instruction) with\nslight modifications (refer to Appendix G for the\nfull template). We first let the model read the con-\ntext collected from other articles about an event.\nWe then prompt the model to answer whether the\nstatement is important to mention in a news story\ncovering the event. Alternatively, a few demon-\nstrations can be embedded within the prompt for\nthe model to learn from, where each demonstration\nexample contains the context article, the statement\nto assess, the question asking the model if the state-\nment is important considering the context article,\nand the answer to the question (i.e., Yes/No). The\nfull prompt template is included in Appendix G.\n5.3 Unsupervised Methods\nExisting unsupervised text summarization algo-\nrithms such as LexRank (Erkan and Radev, 2004)\ncan facilitate finding Ie(important statements re-\ngarding event e) with less cost and effort compared\nto supervised approaches. LexRank is fit by first\nconstructing a graph over a given text, where each\nsentence in the text is represented as a node. Edges\nbetween nodes are weighted based on the similarity\nbetween sentences, using common measures such\nas cosine similarity of TF-IDF vectors. LexRank\nthen constructs a stochastic matrix of these weights\nand uses a power method to find the eigenvector\ncentrality of each sentence. After fitting LexRank,\nit is given a new set of sentences from which\nLexRank extracts a summary by ranking the sen-\ntences based on their centrality scores which reflect\ntheir importance. Only the top n(i.e., summary\nsize) sentences appear in the summary. For the task\nof cherry-picking detection, we employ LexRank\nto find statements\u2019 importance after fitting it (i.e.,\nconstructing its graph) on the context, then pass\nit a list of sentences (i.e., a news article) to rank\nthem based on importance (i.e., extract a summary).\nOnly sentences selected by the algorithm to appear\nin the summary are considered important.\n5.4 End-to-end Cherry-picking Detection\nThe cherry-picking detection pipeline identifies\ncherry-picked statements ciwithin a document di\ngiven an event e, as follows:\nci={\u2203s\u2208Se:f(s, d) = 1} \u2212di (1)\nwhere fis any of our aforementioned methods\nwhich assign a class of 1 for important statements\nand 0 for unimportant ones based on the probability\nscore for that class. The pipeline finds ciby first\nsegmenting all documents in event eto obtain the\nuniversal set of statements Se. Then each statement\ninSeis scored for importance using a model and\nthe event context article d. Finally, each impor-\ntant statement is verified for its presence in each\narticle using semantic similarity. If the statement\nis absent in an article, it is appended to the list of\ncherry-picked (i.e., underemphasized) statements\nassociated with that article.\n6 Experimentation and Results7\nWe used the dataset described in Section 4 to eval-\nuate the performance of the models proposed in\nSection 5.\nContext contribution to performance To un-\nderstand the contribution of context in inferring\nstatement importance, we utilized a BERT-base8\nmodel as the baseline, with a statement alone (i.e.,\nno context) as its input sequence. We also cre-\nated a variant of the model by forming the input\nsequence as the statement and its context concate-\nnated and separated by the [SEP] token. Context in\nthis experiment is collected from a neutral source\n(i.e., Reuters.com). We use macro F-1 and accu-\nracy as models\u2019 performance measures in this and\nsubsequent experiments. Our hypothesis is that,\njust as humans can make more accurate judgments\nregarding the importance and cherry-picking of a\nstatement when provided with an unbiased con-\ntext (Lee and Lee, 1939), models are anticipated to\nexhibit similar capacity. The baseline model scored\n0.619 and 0.617 in accuracy and F-1, respectively.\nWhen the same model was given 100 words of con-\ntext to attend to when classifying statements, its\naccuracy and F-1 were significantly improved to\n0.846 and 0.870, respectively. Note that language\nmodels can still capture some signals pertinent to\nstatements\u2019 importance from their content without\ncontext (thus better than a random guesser).\nModel comparison Given the importance of con-\ntext as established above, we compared the per-\nformance of various models at different context\nlengths. Toward this, we trimmed the context in\n7All the code base, experiments and results are available\nathttps://github.com/cherry-pic/Cherry .\n8https://huggingface.co/google-bert/\nbert-base-uncasedboth the training and test sets at different lengths\nmeasured in words, and we then fine-tuned the\nsupervised models and evaluated all models. In\naddition to comparing the various proposed meth-\nods, this experiment aims to gain insights on how\nmuch context should be fed to the models. Similar\nto the experiment above, context in this experiment\nis collected from a neutral source.\nIn this experiment, for supervised models, we\nused the smaller variant BERT-base consisting\nof 12 transformer layers, 12 attention heads, and\n110M parameters, in addition to Longformer-base\n9consisting of 12 transformer layers, 12 attention\nheads and 149M parameters. We fixed the learning\nrate at 2e-05, the batch size at 8, and the classifica-\ntion threshold at 0.5. To maintain performance and\nefficiency, we set global attention in Longformer\nmodels at the classification and statement tokens\nonly (refer to Appendix F for details of experimen-\ntation on global attention locations). Additionally,\nwe set the number of epochs for training the su-\npervised models to 5, which required at most five\nminutes of training with the aforementioned pa-\nrameters on an NVIDIA H100 PCIe GPU with 80\nGB memory. For zero and few-shot demonstration\nmodels, we experimented with GPT, specifically\ngpt-3.5-turbo-16k via OpenAI\u2019s Chat Completion\nAPI10with temperature fixed at 0 for a determinis-\ntic behavior. We prompted GPT using the prompts\ndiscussed in Section 5.2. For unsupervised models,\nwe used LexRank with summary size (in terms of\nsentences) for each event to be equal to the number\nof positive examples in the event from the test set,\nand we fixed the cosine similarity threshold at 0.1.\nResults in Figure 2 show variation in perfor-\nmance as the context size increases from 100 to\n500 words. The majority of the models demon-\nstrated heightened F-1 and accuracy within a con-\ntext length of 400 words. This amount corre-\nsponds to approximately 12 paragraphs sourced\nfrom Reuters.com. This observation indicates that a\nneutral news report of this specified length pertain-\ning to a given event is sufficient for discerning in-\nstances of cherry-picking within a biased narrative.\nMoreover, results show that the best performing\nmodels for this task are the fine-tuned supervised\nmodels at context length of 500 words. However,\nonly 10 demonstrations fed to the generative LLM\n9https://huggingface.co/allenai/\nlongformer-base-4096\n10https://platform.openai.com/docs/guides/\ntext-generation/chat-completions-api\ncould give decent results.\nLexRank underperformed compared to other\nmodels, primarily due to its dependence on text rep-\netition and the frequency of similar content within\nthe document set. Unsupervised summarizers, such\nas LexRank, encounter challenges in capturing im-\nportant statements when they occur infrequently in\nthe document collection. A statement\u2019s importance\nis determined by its critical role as a key piece\nof evidence, rather than its frequency in the news\narticles.\nFor further improvement, we tested the large\nvariant of Longformer (i.e., Longformer-large),\nconsisting of 24 transformer layers, 16 attention\nheads, and 435M parameters. Longformer-large\nachieved the best performance compared to all\nother models given context size of 500, with an\naccuracy of 0.897 and an F-1 of 0.887. We used\nthis model to perform the ensuing experiments.\nFigure 2: Effect of context size (measured in words) on\nmodels\u2019 performance.\nBiased non-neutral context In addition to con-\ntext collected from least-biased sources, we experi-\nmented with context from biased sources. Partic-\nularly, for each event we collected context from a\nleft-biased source and a right-biased source. We\nthen concatenated and fed the two articles to a gen-\nerative LLM, specifically GPT 3.5, to summarize\nthem in a single text of varying lengths. The reason\nfor summarizing the two articles is to make sure\ntheir gist fits the upper limits on the input sequence\nlength of the different models. We ran all mod-\nels against the data sets with summarized contexts.\nAdditionally, we asked the generative LLM to sum-\nmarize the context collected from biased sources in\n500 words, however, this time we trimmed the 500\nwords summary at different lengths. We then ran\nall models against the data sets with summarized-\nthen-trimmed contexts. In these two experiments\n(i.e., summarized in different lengths vs. summa-\nrized at 500 words then trimmed) we used the same\nsettings and hyper-parameters as in the previous\nexperiments. Results in Figure 3 show that our pro-posed methodology does not rely on the existence\nof a neutral context to perform well. Instead, con-\ntext articles from two sources with different biases\ncan neutralize each other and lead to performance\ncomparable to the least-biased context. Note how\nperformance becomes more stable when context is\nsummarized then introduced to the models at dif-\nferent lengths as Figure 4 shows. One explanation\nis that the most important statements are present in\nthe beginning of an article and thus its summary.\nThis means that once a model sees all the important\nstatements within the context its performance either\nstops improving or degrades as it gets distracted by\nadditional insignificant text.\nFigure 3: Model performance when using a context col-\nlected and summarized in different lengths from biased\nnews sources instead of a neutral source.\nFigure 4: Model performance when using a context\ncollected from biased news sources and summarized in\n500 words then trimmed at different lengths.\nImportance granularity. We anticipate that the\nperformance of models trained on varying levels\nof statement importance granularity will differ. For\ninstance, we expect supervised models to be more\nprecise in drawing a boundary between the labels\nvery important versus everything else, compared\nto drawing a boundary between more fine-grained\nimportance labels such as very important versus\nkind of important . To examine our models\u2019 capa-\nbilities across different levels of importance gran-\nularity, we trained our top-performing model (i.e.,\nLongformer-large) using the four classification con-\nfigurations outlined in Table 1. The results in Ta-\nble 2 show that the difficulty of learning a decision\nboundary decreases when classes are merged and\nthus label granularity decreases. This trend is par-\nticularly notable in the case of kind of important\nandnot very important . When these two classes are\nseparate (configuration #4 in Table 1), the perfor-\nmance is significantly inferior, compared to other\nconfigurations where these two classes are merged.\nThis is attributed to the high fuzziness between the\ntwo classes. Therefore, we utilize configuration #1\nfrom Table 1 in all remaining experiments.\nConf. # Acc. F-1\n1 0.897 0.887\n2 0.853 0.828\n3 0.875 0.898\n4 0.728 0.717\nTable 2: Performance of the Longformer model in the\nfour classification configurations.\nEnd-to-end cherry-picking detection. To assess\nthe performance of the complete cherry-picking\ndetection pipeline, we examined the relationship\nbetween two variables, x: Cherry-picking score\ncalculated on the detected cherry-picking by our au-\ntomatic pipeline in each news outlet and y: the bias\nscores of the outlet collected from MBFC and All-\nSides. Our hypothesis is that, since cherry-picking\nis a form of bias, there should be some level of\ncorrelation between the two variables.\nTo test the relationship between variables x\nandy, we ran the full cherry-picking detection\npipeline described in Formula 1 employing our\ntop-performing supervised model on 2,453 events\ncomprised of about 97k statements collectively. We\nensured that these events did not overlap with the\nCherry dataset utilized for training our supervised\nmodels. Next, we calculated the average number\nof cherry-picked statements per outlet across all\nevents, resulting in a final score of x. Finally,\nsince variable yis ordinal, we calculated the Spear-\nman\u2019s correlation coefficient rbetween xandy.\nResults in Table 3 show a positive moderate corre-\nlation approaching statistical significance when the\nbias scores come from AllSides.com. Additionally,\nthere is a positive weak quasi-significant correla-\ntion when the bias scores come from MBFC.11\nA strong correlation between the two variables is\nnot evident. It could be due to a few reasons. First,\nthe bias scores provided by MBFC and AllSides\nwere calculated by assessing multiple bias forms\nincluding those beyond cherry-picking, e.g., lexi-\n11Further insights into the interpretation of correlation co-\nefficient, including what is considered moderate and what is\nweak, can be found in (Ratner, 2009).cal cues embedded in text. Furthermore, MBFC\nand AllSides continually update their bias scores\nusing fresh article samples. On the country, the\ninference of our models was applied on static data\nfrom each outlet collected between December 2019\nand January 2021.\nBias score source r P-value\nMBFC 0.28 0.10\nAllSides 0.32 0.06\nTable 3: Correlation between x: cherry-picking detec-\ntion pipeline scores and y: bias scores from MBFC and\nAllSides.\nLastly, we hypothesize that the average cherry-\npicking score xfor biased outlets, belonging to a\nparticular bias band, should align with their bias\nintensity. For instance, we anticipate observing\na higher average cherry-picking score among out-\nlets categorized as \u201cleft bias\u201d compIared to those\nclassified as \u201cleft-center bias.\u201d To test this, we\ncalculated the mean and standard deviation of the\ncherry-picking score of all outlets under each bias\ncategory as Table 4 shows. Results show an uptrend\nin cherry-picking scores that aligns with the inten-\nsity of bias in the analyzed outlets. Nevertheless,\nthe observed pattern is affected by the limited sam-\nple sizes, highlighting the need for future analysis\ninvolving larger samples.\nBias category Mean STD Sample size\nLeft 15.12 4.39 6\nLeft center 10.32 3.05 15\nRight 8.91 4.09 7\nRight center 8.33 1.21 5\nCenter 8.44 0.30 2\nTable 4: The mean and standard deviation of cherry-\npicking scores aggregated by bias category.\n7 Conclusions and Future Work\nManually spotting cherry-picking statements in\nnews reports is challenging due to the need to ex-\namine other narratives. This study introduces a\nnovel approach to automate the detection of cherry-\npicking in news reports. Our approach focuses\non comparing multiple news articles that pertain\nto the same event and identifying the omission of\ncrucial statements. To facilitate our research, we\nhave constructed the first cherry-picking detection\ndataset. The results of our models demonstrate\npromising outcomes. Currently, our work specif-\nically addresses the detection of cherry-picking\nbased on underemphasis of statements. For future\nwork, we plan to also considering overemphasis.\nLimitations\nDependence on semantic similarity thresholds\nand adjustable parameters. A notable limita-\ntion of the end-to-end cherry-picking pipeline, as\noutlined in Section 5, is its dependence on seman-\ntic similarity thresholds. This can be observed in\nthe application of clustering techniques during the\ncreation of events from a set of articles. If a news\nsource covers an event through multiple articles and\none of these articles is not clustered into the correct\nevent, the pipeline may overlook important infor-\nmation from the coverage of that outlet. Moreover,\nwhen determining whether an article mentions a\nspecific statement, if the threshold for semantic\nsimilarity is not well-tuned, similar statements in\nthe article may not be detected and consequently\nbe falsely considered cherry-picked.\nDefinition and tokenization of statements. To\nensure the precise identification of cherry-picked\nstatements, it is essential to have a clear defini-\ntion and an appropriate tokenization method for\nsegmenting text into statements. Currently we con-\nsider every sentence as a statement, but this ap-\nproach may not always be accurate since multiple\nsentences can collectively form a single statement.\nEthics Statement\nWhile our work focuses on detecting bias by omis-\nsion, it inevitably incorporates a human element\nin two stages. First, At the data collection stage,\nhuman coders are asked to indicate whether a state-\nment is important to the event. This may result\nin a dataset that contains the inherent subjectivity\nand personal perspectives of the human annotators\nthemselves. We are aware that annotators\u2019 implicit\nbiases can influence the annotations they provide.\nTherefore, we tried to alleviate this influence by\nhiding sources names from segments in the annota-\ntion tasks, in addition to providing annotators with\ncontext to read before they assign labels. Second,\nbias categorizations we use from MBFC, AllSides,\nand Ad Fontes Media rely on human annotators as\nwell. Hence, the bias of these sources can be inher-\nently embedded into our dataset. While we hold\nthe belief that the sources we rely on exhibit high\nquality, it is important to recognize that their reli-\nability and credibility may decline due to various\nunforeseen factors. At last, we assert that our usage\nof all artifacts this work including pretrained mod-\nels (e.g., Longformer and BERT) and derived data\n(e.g., GDELT events data and labels from MBFC,Ad Fontes Media and AllSides) is purely for re-\nsearch purposes and consistent with their intended\nuse.\nReferences\nAbolfazl Asudeh, Hosagrahar Visvesvaraya Jagadish,\nYou Wu, and Cong Yu. 2020. On detecting cherry-\npicked trendlines. Proceedings of the VLDB Endow-\nment , 13(6):939\u2013952.\nRamy Baly, Giovanni Da San Martino, James Glass,\nand Preslav Nakov. 2020. We can detect your bias:\nPredicting the political ideology of news articles. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 4982\u20134991, Online. Association for Computa-\ntional Linguistics.\nDavid P Baron. 2006. Persistent media bias. Journal of\nPublic Economics , 90(1-2):1\u201336.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150 .\nJoel Best. 2004. More damned lies and statistics: How\nnumbers confuse public issues . University of Califor-\nnia Press.\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Nat-\nural language processing with Python: analyzing text\nwith the natural language toolkit . \" O\u2019Reilly Media,\nInc.\".\nJames Alexander Campbell Brown. 1963. Techniques\nof Persuasion, from propaganda to brainwashing ,\nvolume 604.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems , 33:1877\u20131901.\nWei-Fan Chen, Khalid Al Khatib, Henning Wachsmuth,\nand Benno Stein. 2020. Analyzing political bias\nand unfairness in news articles at different levels of\ngranularity. In Proceedings of the Fourth Workshop\non Natural Language Processing and Computational\nSocial Science , pages 149\u2013154, Online. Association\nfor Computational Linguistics.\nChun-Fang Chiang and Brian Knight. 2011. Media bias\nand influence: Evidence from newspaper endorse-\nments. The Review of economic studies , 78(3):795\u2013\n820.\nMatteo Cinelli, Walter Quattrociocchi, Alessandro\nGaleazzi, Carlo Michele Valensise, Emanuele Brug-\nnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo,\nand Antonio Scala. 2020. The COVID-19 social\nmedia infodemic. Scientific reports , 10(1):1\u201310.\nViorela Dan, Britt Paris, Joan Donovan, Michael\nHameleers, Jon Roozenbeek, Sander van der Linden,\nand Christian von Sikorski. 2021. Visual mis-and dis-\ninformation, social media, and democracy. Journal-\nism & Mass Communication Quarterly , 98(3):641\u2013\n664.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJames N Druckman and Michael Parkin. 2005. The\nimpact of media bias: How editorial slant affects\nvoters. The Journal of Politics , 67(4):1030\u20131049.\nG\u00fcnes Erkan and Dragomir R Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text sum-\nmarization. Journal of artificial intelligence research ,\n22:457\u2013479.\nMartin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, and Xi-\naowei Xu. 1996. A density-based algorithm for dis-\ncovering clusters in large spatial databases with noise.\nInProceedings of the Second International Confer-\nence on Knowledge Discovery and Data Mining , vol-\nume 96, pages 226\u2013231.\nLisa Fan, Marshall White, Eva Sharma, Ruisi Su, Pra-\nfulla Kumar Choubey, Ruihong Huang, and Lu Wang.\n2019. In plain sight: Media bias through the lens of\nfactual reporting. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) ,\npages 6343\u20136349, Hong Kong, China. Association\nfor Computational Linguistics.\nCharles A Fleming. 1995. Understanding propaganda\nfrom a general semantics perspective. ETC.: A Re-\nview of General Semantics , 52(1):3\u201313.\nFelix Hamborg. 2020. Media bias, the social sciences,\nand NLP: Automating frame analyses to identify bias\nby word choice and labeling. In Proceedings of the\n58th Annual Meeting of the Association for Com-\nputational Linguistics: Student Research Workshop ,\npages 79\u201387, Online. Association for Computational\nLinguistics.\nMichael Hameleers, Anna Brosius, and Claes H\nde Vreese. 2022. Whom to trust? media exposure pat-\nterns of citizens with perceptions of misinformation\nand disinformation related to the news media. Euro-\npean Journal of Communication , 37(3):237\u2013268.\nChristoph Hube and Besnik Fetahu. 2018. Detecting\nbiased statements in wikipedia. In Companion pro-\nceedings of the the web conference 2018 , pages 1779\u2013\n1786.Miron Lakomy. 2020. Between the \u201cCamp of False-\nhood\u201d and the \u201cCamp of Truth\u201d: Exploitation of Pro-\npaganda Devices in the \u201cDabiq\u201d Online Magazine.\nStudies in Conflict & Terrorism , pages 1\u201326.\nAlfred Lee and Elizabeth Briant Lee. 1939. The Fine\nArt of Propaganda; A Study of Father Coughlin\u2019s\nSpeeches, Edited by Alfred Mcclung Lee & Elizabeth\nBrian Lee .\nAlfred McClung Lee. 1945. The analysis of propaganda:\na clinical summary. American Journal of Sociology ,\n51(2):126\u2013135.\nSangwon Lee, Homero Gil de Z\u00fa\u00f1iga, and Kevin\nMunger. 2023. Antecedents and consequences of\nfake news exposure: a two-panel study on how news\nuse and different indicators of fake news exposure\naffect media trust. Human Communication Research .\nTien-Tsung Lee. 2010. Why they don\u2019t trust the media:\nAn examination of factors predicting trust. American\nbehavioral scientist , 54(1):8\u201321.\nKalev Leetaru and Philip A Schrodt. 2013. Gdelt:\nGlobal data on events, location, and tone, 1979\u20132012.\nInISA annual convention , volume 2, pages 1\u201349.\nCiteseer.\nYuanyuan Lei, Ruihong Huang, Lu Wang, and Nick\nBeauchamp. 2022. Sentence-level media bias analy-\nsis informed by discourse structures. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing , pages 10040\u201310050,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nYin Lin, Brit Youngmann, Yuval Moskovitch, HV Ja-\ngadish, and Tova Milo. 2021. On detecting cherry-\npicked generalizations. Proceedings of the VLDB\nEndowment , 15(1):59\u201371.\nMelvin Mencher and Wendy P Shilton. 1997. News re-\nporting and writing . Brown & Benchmark Publishers\nMadison, WI.\nRichard Paul and Linda Elder. 2008. How to Detect Me-\ndia Bias & Propaganda: In the National and World\nNews: Based on the Critical Conepts & Tools .\nRichard Paul and Linda Elder. 2019. The Thinker\u2019s\nGuide for Conscientious Citizens on How to De-\ntect Media Bias and Propaganda in National and\nWorld News: Based on Critical Thinking Concepts\nand Tools .\nBruce Ratner. 2009. The correlation coefficient: Its\nvalues range between+ 1/- 1, or do they? Journal of\ntargeting, measurement and analysis for marketing ,\n17(2):139\u2013142.\nMarta Recasens, Cristian Danescu-Niculescu-Mizil, and\nDan Jurafsky. 2013. Linguistic models for analyzing\nand detecting biased language. In Proceedings of the\n51st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages\n1650\u20131659.\nFilipe Ribeiro, Lucas Henrique, Fabricio Benevenuto,\nAbhijnan Chakraborty, Juhi Kulshrestha, Mah-\nmoudreza Babaei, and Krishna Gummadi. 2018. Me-\ndia bias monitor: Quantifying biases of social media\nnews outlets at large-scale. In Proceedings of the\nInternational AAAI Conference on Web and Social\nMedia , volume 12.\nTimo Spinde, Manuel Plank, Jan-David Krieger, Terry\nRuas, Bela Gipp, and Akiko Aizawa. 2021a. Neu-\nral media bias detection using distant supervision\nwith BABE - bias annotations by experts. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021 , pages 1166\u20131177, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nTimo Spinde, Lada Rudnitckaia, Jelena Mitrovi \u00b4c, Felix\nHamborg, Michael Granitzer, Bela Gipp, and Karsten\nDonnay. 2021b. Automated identification of bias\ninducing words in news articles using linguistic and\ncontext-oriented features. Information Processing &\nManagement , 58(3):102505.\nJ Michael Sproule. 2001. Authorship and origins of the\nseven propaganda devices: A research note. Rhetoric\n& Public Affairs , 4(1):135\u2013143.\nJesper Str\u00f6mb\u00e4ck, Yariv Tsfati, Hajo Boomgaarden,\nAlyt Damstra, Elina Lindgren, Rens Vliegenthart,\nand Torun Lindholm. 2020. News media trust and its\nimpact on media use: Toward a framework for future\nresearch. Annals of the International Communication\nAssociation , 44(2):139\u2013156.\nRichard C Vincent. 2019. Global communication and\npropaganda . Rowman & Littlefield Lanham, MD.\nGalen Weld, Maria Glenski, and Tim Althoff. 2021. Po-\nlitical bias and factualness in news sharing across\nmore than 100,000 online communities. In Proceed-\nings of the International AAAI Conference on Web\nand Social Media , volume 15, pages 796\u2013807.\nYou Wu, Pankaj K Agarwal, Chengkai Li, Jun Yang,\nand Cong Yu. 2017. Computational fact checking\nthrough query perturbations. ACM Transactions on\nDatabase Systems (TODS) , 42(1):1\u201341.\nAppendix A Cherry-picking Example\nTable 5 illustrates cherry-picking in journalism by\nshowcasing statements from authentic news articles\nby distinctively biased news outlets, all covering\nthe same event: U.S. President Donald Trump\u2019s\nattendance at the annual Davos economic forum.\nThe three news outlets included in Table 5 are\nReuters.com (center), NewsMax.com (right-bias),\nand CNBC.com (left-bias). The bias categorization\nof these media outlets is determined by assessments\nfrom Media Bias Fact Check (MBFC). The table\norganizes corresponding statements from the threearticles in rows based on their semantic similarity.\nIf a statement is not mentioned by a particular news\nsource, the corresponding cell in the table is left\nempty. Note that Reuters.com and NewsMax.com\npublished identical articles for this event.\nBy examining Table 5, we can observe instances\nof cherry-picking. For example, in the third row,\ntwo of the news outlets mentioned that President\nTrump attended the Davos forum last year (i.e.,\n2019). However, CNBC.com (left-bias) failed to\nmention it, and focused instead on details about\nother issues including placing tariffs on cars im-\nported from European Union and French special-\nties, as shown in the eighth row of the table.\nAppendix B Media Bias Rating Sources\nMBFC, AllSides, and Ad Fontes Media employ\ndiverse methods to evaluate bias in news outlets.\nLiterature addressing bias and misinformation de-\ntection heavily relies on these three sources for\nbias categorization in tasks such as social media\nsource annotation (Weld et al., 2021), comparing\nthe diffusion of news from reliable and non-reliable\nsources (Cinelli et al., 2020), and benchmarking au-\ntomatic media bias monitors (Ribeiro et al., 2018).\nMBFC utilizes a comprehensive methodology to\nassess the ideological leanings and factual accuracy\nof media outlets.12The bias assessment includes\ncategories such as Biased Wording/Headlines, Fac-\ntual/Sourcing, Story Choices, and Political Af-\nfiliation. The scoring mechanism categorizes\nsources into Least Biased, Left/Right Center Bias,\nLeft/Right Bias, or Extreme Bias. The method-\nology also assesses factual reporting with ratings\nranging from Very High to Very Low based on the\nreliability and commitment to factual accuracy.\nAllSides employs a comprehensive methodol-\nogy to rate media bias, including various factors\nand perspectives.13AllSides Editorial Reviews in-\nvolve a multipartisan panel assessing news reports\nand bias indicators. Blind Bias Surveys gather\nopinions from average Americans across political\nspectrums. AllSides reviewers assess content in-\ndependently, considering common bias indicators\nand transparency about political leaning. AllSides\nalso incorporates third-party data and allows Com-\nmunity Feedback for additional perspectives.\nAd Fontes Media generates scores for news\n12https://mediabiasfactcheck.com/methodology/\n13https://www.allsides.com/media-bias/\nmedia-bias-rating-methods\nReuters.com (Center) /NewsMax.com (Right-bias) CNBC.com (Left-bias)\nU.S. President Donald Trump plans to attend the annual\nDavos economic forum in January.After skipping a gathering of the world\u2019s most elite in Davos,\nSwitzerland last January, President Donald Trump will attend\nthe World Economic Forum in 2020.\nTrump had to cancel his plan to attend the annual gathering of\nglobal economic leaders early this year due to a government\nshutdown.Trump blamed his no-show last time on the partial govern-\nment shutdown that was triggered by a funding dispute over\na proposed wall along the United States\u2019 southern border.\nHe attended the Davos forum last year. -\nThe exact date of when Trump would participate was unclear. A spokesperson for the White House did not immediately\nrespond to a request for comment about the president\u2019s plans\nto attend next year.\nDavos may be one of the few foreign trips that Trump takes\nin 2020.-\nThe World Economic Forum in the Swiss ski resort town of\nDavos is scheduled to run January 21-24.The 50th annual forum, held Jan. 21 - Jan. 24 will focus on\n\u201cStakeholders for a Cohesive and Sustainable World.\u201d\n- Meantime, while the Davos agenda will look to \u201ccreate\nbridges to resolve conflicts in global hotspots,\u201d Trump has\nmade jabs at a number of European allies.\n- While he has said he was just joking about placing tariffs on\ncars imported from the European Union, he is still threaten-\ning to place tariffs on French specialties like champagne and\ncheese.\nTable 5: A juxtaposition of three news stories covering the same event, published by three sources with different\npolitical biases. Bias categorization of these media outlets is based on MediaBiasFactCheck.com (MBFC).\nsources based on the ratings of individual articles\nor episodes, using over 60 trained analysts from\ndiverse backgrounds who undergo initial and on-\ngoing training.14Each source is rated by at least\nthree analysts with different political leanings. The\nanalysts independently rate the content, compare\nscores, and discuss any discrepancies. The overall\nrating is the average of the analysts\u2019 ratings. In\nsome cases, more analyses may be used for rating\narticles with outlier scores.\nAppendix C Clustering of Se\nTable 6 shows two statement clusters from two\ndifferent events.\nAppendix D Annotation Interface\nUpon logging into the interface, the annotators en-\nter an assigned event ID. The annotators are in-\nstructed to assign one of five labels to the statement\ncluster based on their understanding of the event de-\nscribed in the news article (Figure 5). A statement\nis considered important if it articulates a fact essen-\ntial to understanding the event, and vice versa. If\nthe statement cluster is unrelated to the event, lacks\nmeaningfulness, or contains facts with conflicting\nmeanings, the annotators are expected to label it as\n(4)the excerpts might be incorrect . The annotators\ncan select (5) I am not sure if they feel uncertain\nabout the statements\u2019 importance. After selecting\n14https://adfontesmedia.com/\nhow-ad-fontes-ranks-news-sources/a label, the annotators can proceed by clicking the\n\u201cSubmit and go to next example\u201d button to annotate\nthe subsequent statement clusters associated with\nthe same event. Once the annotators completes all\nstatement clusters of the event, they will receive a\nprompt to enter a new event ID and proceed to the\nnext event.\nAppendix E Importance Assessment as a\nSequence-pair Classification\nTask\nThe supervised BERT and Longformer models are\ntrained using the sequence-pair classification task,\nwith each input consisting of a statement and the\ncorresponding context derived from the given event,\nas shown in Figure 6. The output embedding vector\nof the classification token is softmax\u2013ed to produce\nthe positive class probability.\nAppendix F Global Attention in\nLongformer\nTo evaluate the influence of the global attention\nmechanism on predicting a statement\u2019s importance,\nwe tested our best model (Longformer-large) with\nall possible combinations of global attention loca-\ntions within the input sequence, i.e., the classifica-\ntion token, the statement tokens, and the context\ntokens. We maintained the sequence length of 512\nand set the batch size to 4 in this experiment to en-\nsure memory can handle applying global attention\non all the sequence tokens. Results in Table 7 show\nCluster #1\n\u00b7President-elect Joe Biden plans to release nearly all available doses of the COVID-19 vaccines after he takes office.\n\u00b7President-elect Joe Biden plans to release almost all vaccine doses immediately.\n\u00b7President-elect Joe Biden will aim to release every available dose of the coronavirus vaccine when he takes office.\n\u00b7Joe Biden will release most available Covid-19 vaccine doses to speed delivery to more people when he takes office.\nCluster #2\n\u00b7The House voted to override President Trump\u2019s veto of a $740 billion defense spending and policy bill.\n\u00b7The House of Representatives on Monday voted to override President Trump\u2019s veto of the National Defense Authorization\nAct for Fiscal Year 2021.\n\u00b7House V otes to Override Trump\u2019s Veto of 2021 Defense Policy Bill.\n\u00b7The House voted late Monday to override President Donald Trump\u2019s veto of a defense spending bill for 2021.\nTable 6: Two examples of statement clusters.\nFigure 5: Cherry-picking data annotation interface.\nFigure 6: Automatic cherry-picking detection using\nsequence-pair classification architecture.\nthat we can still maintain the same performance\nbut optimize memory and inference time (Beltagy\net al., 2020) by applying global attention on thestatement tokens and classification tokens only. For\nthis reason, we fixed global attention to these two\nlocations in all of our experiments. Moreover, we\ncan optimize further and trade off more efficiency\nat the expense of a slight decrease in performance\nby applying global attention on the classification\ntoken only.\nGlobal attention locations Acc. F-1\n[CLS] 0.819 0.820\n[CLS] + statement 0.831 0.837\n[CLS] + context 0.683 0.680\n[CLS] + statement + context 0.837 0.838\nTable 7: Performance of the Longformer-large model\nusing different global attention locations.\nFigure 7: Template used to prompt GPT.\nAppendix G GPT Prompts\nTo prompt GPT, we experimented with various\nprompts, including simple ones such as the ques-\ntion\u201cIs the above sentence important to mention\nin a news article that covers the story mentioned in\nthe above article? Answer with \u2018yes\u2019 or \u2018no\u2019 only. \u201d\nUsing the prompt as shown in Figure 7, which is\ncrafted based on the annotation interface, yielded\nthe best GPT-based performance on the test dataset.", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "On Context-aware Detection of Cherry-picking in News Reporting", "author": ["I Jaradat", "H Zhang", "C Li"], "pub_year": "2024", "venue": "arXiv preprint arXiv:2401.05650", "abstract": "Cherry-picking refers to the deliberate selection of evidence or facts that favor a particular  viewpoint while ignoring or distorting evidence that supports an opposing perspective."}, "filled": false, "gsrank": 206, "pub_url": "https://arxiv.org/abs/2401.05650", "author_id": ["r2NMH4oAAAAJ", "nq-oPFkAAAAJ", "v8ZQDf8AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:qhN9PY5UH8sJ:scholar.google.com/&output=cite&scirp=205&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D200%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=qhN9PY5UH8sJ&ei=KbWsaMZzjoiJ6g_SwpG4CQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:qhN9PY5UH8sJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2401.05650"}}, {"title": "Topic Models on Biased Corpora", "year": "2018", "pdf_data": "People and Knowledge NetworksWeSTLeibniz Institute for Faculty 4: Computer Science Institute for Web Science\nthe Social Sciences and Technologies\nTopic Models on Biased Corpora\nMaster\u2019s Thesis\nin partial ful\ufb01llment of the requirements for\nthe degree of Master of Science (M.Sc.)\nin Informatik\nsubmitted by\nMarcel Reif\nFirst supervisor: JProf. Dr. Claudia Wagner\nInstitute for Web Science and Technologies\nSecond supervisor: Dr. Christoph Carl Kling\nInstitute for Web Science and Technologies\nKoblenz, January 2018\n\nStatement\nI hereby certify that this thesis has been composed by me and is based on my own\nwork, that I did not use any further resources than speci\ufb01ed \u2013 in particular no\nreferences unmentioned in the reference section \u2013 and that I did not submit this\nthesis to another examination before. The paper submission is identical to the\nsubmitted electronic version.\nYes No\nI agree to have this thesis published in the library. \u0003 \u0003\nI agree to have this thesis published on the Web. \u0003 \u0003\nThe thesis text is available under a Creative Commons\nLicense (CC BY-SA 4.0). \u0003 \u0003\nThe source code is available under a GNU General Public\nLicense (GPLv3). \u0003 \u0003\nThe collected data is available under a Creative Commons\nLicense (CC BY-SA 4.0). \u0003 \u0003\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n(Place, Date) (Signature)\niii\n\nZusammenfassung\nTopic Models sind ein beliebtes Werkzeug um Themen in gro\u00dfen Textkorpora zu\nidenti\ufb01zieren. Diese Textkorpora enthalten oft versteckte Meta-Gruppen. Das Gr\u00f6-\n\u00dfenverh\u00e4ltnis zwischen diesen Gruppen variiert meist stark. Die Pr\u00e4senz dieser\nGruppen wird in der Praxis oft ignoriert. Diese Masterarbeit erforscht daher ob die-\nse Gruppen Ein\ufb02uss auf ein Topic Model haben.\nUm den Ein\ufb02uss zu testen, wird LDA auf Samples mit unterschiedlichen Grup-\npengr\u00f6\u00dfen trainiert. Die Samples werden von Textkorpora mit gro\u00dfen Gruppen-\nunterschieden (d.h. Sprachunterschieden) und kleinen Gruppenunterschieden (d.h.\nUnterschiede in der politische Orientierung) generiert. Die Leistungsf\u00e4higkeit von\nLDA wird per \u201cPerplexity\u201d evaluiert.\nDer Ein\ufb02uss von Gruppen auf die generelle Leistungsf\u00e4higkeit von Topic Models\nh\u00e4ngt von verschiedenen Faktoren der Gruppen ab, z.B. der Vorhersagbarkeit der\nSprache generell. Die Leistungsf\u00e4higkeit der Topic Models f\u00fcr die einzelnen Grup-\npen wird von der Variation der relativen Gruppengr\u00f6\u00dfen beein\ufb02usst. Allerdings ist\nder Effekt f\u00fcr alle Datens\u00e4tze verschieden.\nLDA kann die Gruppen intern unterscheiden, wenn die Unterschiede der Grup-\npen gro\u00df genug sind (z.B. Sprachunterschiede). Der Anteil der Topics, die explizit\nf\u00fcr eine Gruppe gelernt werden, ist jedoch unterproportional zu dem Anteil der\nGruppe im Trainingskorpus. Dieser Effekt verst\u00e4rkt sich f\u00fcr kleinere Minderheiten.\nAbstract\nTopic models are a popular tool to extract concepts of large text corpora. These\ntext corpora tend to contain hidden meta groups. The size relation of these groups\nis frequently imbalanced. Their presence is often ignored when applying a topic\nmodel. Therefore, this thesis explores the in\ufb02uence of such imbalanced corpora on\ntopic models.\nThe in\ufb02uence is tested by training LDA on samples with varying size relations.\nThe samples are generated from data sets containing a large group differences i.e\nlanguage difference and small group differences i.e. political orientation. The pre-\ndictive performance on those imbalanced corpora is judged using perplexity.\nThe experiments show that the presence of groups in training corpora can in\ufb02u-\nence the prediction performance of LDA. The impact varies due to various factors,\nincluding language-speci\ufb01c perplexity scores. The group-related prediction perfor-\nmance changes for groups when varying the relative group sizes. The actual change\nvaries between data sets.\nLDA is able to distinguish between different latent groups in document corpora\nif differences between groups are large enough, e.g. for groups with different lan-\nguages. The proportion of group-speci\ufb01c topics is under-proportional to the share\nof the group in the corpus and relatively smaller for minorities.\nv\n\nContents\n1 Introduction 1\n1.1 Research Topic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2 Background and Related Work 3\n2.1 Topic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.2 Latent Dirichlet Allocation . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.3 Topic Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.3.1 Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.3.2 Alternative Evaluation Metrics . . . . . . . . . . . . . . . . . . 6\n2.4 Current State of Research . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.5 Topic Models for Groups in Document Corpora . . . . . . . . . . . . 7\n2.5.1 Hierarchical Dirichlet Process . . . . . . . . . . . . . . . . . . . 8\n2.5.2 Topic Models for Known Groups . . . . . . . . . . . . . . . . . 8\n2.5.3 Topic Models for Unknown Groups . . . . . . . . . . . . . . . 9\n3 Methodology 9\n3.1 Sample Seed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2 Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.3 Training and Test Corpora . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.4 Training LDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.5 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4 Data Sets 16\n4.1 Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.1.1 Data Acquisition and Formatting . . . . . . . . . . . . . . . . . 17\n4.1.2 Article Pairing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.1.3 Link Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.1.4 Article Pair Filtering . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.2 Event Registry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.2.1 Source Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.2.2 Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n4.2.3 Article Pair Filtering . . . . . . . . . . . . . . . . . . . . . . . . 27\n4.3 Stemming and Stopword Removal . . . . . . . . . . . . . . . . . . . . 27\n4.4 Data Set Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.4.1 Wikipedia Data Set . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.4.2 Event Registry UK Data Set . . . . . . . . . . . . . . . . . . . . 31\n4.4.3 Event Registry US Data Set . . . . . . . . . . . . . . . . . . . . 34\n5 Experimental Results 38\n5.1 Vocabulary Mismatch . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\nvii\n5.2 Overall Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n5.2.1 Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n5.2.2 Event Registry UK . . . . . . . . . . . . . . . . . . . . . . . . . 42\n5.2.3 Event Registry US . . . . . . . . . . . . . . . . . . . . . . . . . 42\n5.3 Group-Speci\ufb01c Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . 45\n5.3.1 Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n5.3.2 Event Registry UK . . . . . . . . . . . . . . . . . . . . . . . . . 47\n5.3.3 Event Registry US . . . . . . . . . . . . . . . . . . . . . . . . . 49\n5.4 Topic Assignment per Group . . . . . . . . . . . . . . . . . . . . . . . 49\n5.4.1 Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n5.4.2 Event Registry . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n6 Conclusion 55\nviii\nList of Tables\n1 Categorisation UK online news . . . . . . . . . . . . . . . . . . . . . . 25\n2 Categorisation US online news . . . . . . . . . . . . . . . . . . . . . . 25\n3 Overview of Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nList of Figures\n1 Outline of the experimental setup . . . . . . . . . . . . . . . . . . . . . 10\n2 Example of a sample seed . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3 Sample Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 Example of a 40/60 sample . . . . . . . . . . . . . . . . . . . . . . . . 14\n5 Wikipedia\u2019s language link schema . . . . . . . . . . . . . . . . . . . . 18\n6 Ambiguous Wikipedia link patterns . . . . . . . . . . . . . . . . . . . 20\n7 Unambiguous Wikipedia link patterns . . . . . . . . . . . . . . . . . . 20\n8 Political Orientation \u2013 YouGov Survey . . . . . . . . . . . . . . . . . . 23\n9 Ideological Pro\ufb01le of Each Source\u2019s Audience \u2013 Pew Research Center 24\n10 Article length distribution \u2013 Wikipedia Corpus . . . . . . . . . . . . . 29\n11 Distribution of Unique and Shared Words \u2013 Wikipedia Corpus . . . . 30\n12 Word Frequency \u2013 Wikipedia Corpus . . . . . . . . . . . . . . . . . . . 31\n13 Source Distribution \u2013 UK Corpus . . . . . . . . . . . . . . . . . . . . . 32\n14 Article Length Distribution \u2013 UK Corpus . . . . . . . . . . . . . . . . 33\n15 Distribution of Unique and Shared Words \u2013 UK Corpus . . . . . . . . 33\n16 Word Frequency \u2013 UK Corpus . . . . . . . . . . . . . . . . . . . . . . . 34\n17 Source Distribution \u2013 US Corpus . . . . . . . . . . . . . . . . . . . . . 35\n18 Article length distribution \u2013 US Corpus . . . . . . . . . . . . . . . . . 36\n19 Distribution of Unique and Shared Words \u2013 US Corpus . . . . . . . . 37\n20 Word Frequency \u2013 US Corpus . . . . . . . . . . . . . . . . . . . . . . . 37\n21 Vocabulary Mismatch \u2013 Wikipedia Corpus . . . . . . . . . . . . . . . 39\n22 Perplexity \u2013 Wikipedia Corpus . . . . . . . . . . . . . . . . . . . . . . 41\n23 Perplexity \u2013 UK Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n24 Perplexity \u2013 US Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n25 Perplexity per Group \u2013 Wikipedia . . . . . . . . . . . . . . . . . . . . 46\n26 Perplexity per Group \u2013 UK Corpus . . . . . . . . . . . . . . . . . . . . 48\n27 Perplexity per Group \u2013 US Corpus . . . . . . . . . . . . . . . . . . . . 50\n28 Topic Assignment \u2013 Wikipedia Corpus . . . . . . . . . . . . . . . . . . 52\n29 Topic Assignment per Training Corpus Share \u2013 Wikipedia . . . . . . 53\n30 Topic Assignment \u2013 UK and US Corpus . . . . . . . . . . . . . . . . . 54\nix\n\n1 Introduction\nText mining methods which help users to gain insights into larger document corpora\nbecome increasingly popular. Larger document corpora (e.g. Wikipedia database\ndumps) consist of millions of documents and can cover a variety of different topics\n(say politics or sports). While a human reader could read and understand only a\nlimited number of documents, automated methods process large amounts of docu-\nments.\nTo process larger data sets, sophisticated algorithms are required. Depending on\nthe task, the right method has to be chosen and in many cases, parameters have to\nbe set. It is crucial to understand the behaviour of methods to make a well-informed\nchoice on the appropriate method.\nLarge corpora usually provide additional information on documents: metadata,\nwhich store information on the document such as the language of a document or\nthe source of a news article. Categorical metadata variables can be interpreted as\ninformation on group memberships of documents. For instance, language informa-\ntion might group documents into German and English documents. In practice these\ngroups are often of unequal size e.g. the documents that are written by men exceed\nthe amount of documents written by women in a computer science corpus.\nOne important class of text mining methods are topic models, which use statistical\nmeans to detect latent topics in documents. The most-popular topic models focus\non the actual content of the documents and ignore any attached metadata.\nThe presence of groups in corpora might in\ufb02uence the topic detection process in\ndocuments. To illustrate, imagine you want to detect the topics of a science publica-\ntion corpus. 80% of the articles are written by British scientists and only 20% of the\narticles are written by German scientists. Will the topic model build topics for the\ndocuments of the German scientists? How well can it predict their documents?\nIt is currently unknown if and to what extend such group imbalance in\ufb02uences\nthe quality of topic models. While one might be tempted to predict that the minority\ngroup will have a worse prediction performance, the topic model could also detect\ntopics for both groups proportionally to their relative share, and the group-speci\ufb01c\nmodel performance could be unaffected.\n1.1 Research Topic\nThis thesis examines the following four questions:\n(i)Does the presence of groups in corpora in\ufb02uence the prediction performance\nof topic models?\n(ii)Does the prediction performance of topic models change when varying the\nrelative group sizes?\n(iii) Is the topic model able two distinguish between different latent groups in im-\nbalanced corpora?\n1\n(iv) If the model can distinguish the latent groups, is the proportion of group-\nspeci\ufb01c topics under-proportional to the share of the group in the corpus?\nThese are important questions to ask since data mining practitioners often train\ntopic models on corpora that consist of different groups of documents and the group\nsizes can be imbalanced. However, this is often unknown or neglected. It is unclear\nto what extend the quality of topics and assignment of topics to documents is af-\nfected.\nA reasonable assumption is, that the presence of groups in a training corpus in\ufb02u-\nences the overall prediction performance negatively. If one of these groups is forced\ninto a minority position, this group will additionally suffer from lower prediction\nperformance.\nThis thesis should shed light on how relative group size differences affect the\nperformance and the parameters of a topic model.\nIn order to answer questions (i) \u2013 (iv), multiple LDA topic models [2] are trained\non imbalanced corpora. LDA or \u201cLatent Dirichlet Allociation\u201d is the most com-\nmonly used topic model. The imbalanced corpora are samples of a larger data set.\nEach sample contains two predetermined groups . The relative size relation of those\ngroups is manipulated while keeping the concepts within the sample consistent.\nThe quality of topic models trained on these samples is evaluated using the per-\nplexity on held-out data. Perplexity is based on the likelihood of held-out docu-\nments and explains how well the topic model can predict the test data. Therefore,\nperplexity is used to measure the prediction performance of LDA and answer ques-\ntions (i) and (ii). The remaining questions (iii) and (iv) are evaluated by examining\nthe predicted topic distributions over the test documents.\n1.2 Thesis Structure\nSection 2 describes background information of this thesis and gives an overview of\nthe current state of research. A short introduction to topic models and an explana-\ntion of Latent Dirichlet Allocation is given. Approaches for the evaluation of topic\nmodels are presented, including the de\ufb01nition of perplexity. Additionally, problems\nrelated to the evaluation of semantic cohesiveness of topic models are discussed.\nSection 3 illustrates the experiment. It explains in detail how the experiment is set\nup. This section describes how the samples are created and how the concepts are\ncontrolled when changing the relative size of groups. It provides information about\nthe training and evaluation process which was built around the gensim [24] module.\nSection 4 describes the data sets used during the experiment. It explains the thoughts\nbehind their selection and the setup used to create them. In total, three different data\nsets are described: an article data set extracted from Wikipedia and two news arti-\ncle data sets, built using Event Registry. The Wikipedia data set contains German\nand English articles while the Event Registry data sets differ by using American and\nBritish news sources.\n2\nSection 5 shows the results of the experiment and answers the four main questions\nof this thesis.\nSection 6 summarises the \ufb01ndings of the thesis and points out implications for the\napplication of topic models on corpora containing groups.\n2 Background and Related Work\nTopic models are a common tool in natural language processing and machine learn-\ning. They are statistical models that try to capture the hidden concepts of a docu-\nment corpus. They capture these concepts as so called \u201ctopics\u201d. The \ufb01rst part of this\nsection will explain the general ideas behind topic models.\nAfterwards, the most common topic model, LDA, is de\ufb01ned as it is the topic\nmodel of choice during the experiment. The model itself is explained together with\nthe reasons why it was chosen above others.\nThe predictive performance of LDA will be quanti\ufb01ed using perplexity. A de\ufb01ni-\ntion of perplexity is given as well as a quick overview of other possible evaluation\nmetrics. These metrics are not used during this thesis but might give some inspira-\ntion to future evaluation setups.\nLastly, a short overview about group-speci\ufb01c topic models is given. These topic\nmodels were explicitly created to cope with groups in corpora.\n2.1 Topic Models\nTopic models are a class of algorithms which exploit co-occurrences of words in doc-\numents in order to uncover hidden sets of words which explain the co-occurrence\npatterns and are referred to as topics . Probabilistic topic models explain observed\ndocuments with an underlying, hidden probabilistic model. The observed docu-\nments are assumed to be random samples from this model. In a probabilistic topic\nmodel, each document is associated with a probability distribution over a set of top-\nics, and topics are associated with a probability distribution over the set of words.\nSimilar documents share a similar topic distribution.\nTopic models are often employed for text mining tasks, e.g. for understanding\nand visualising the content of large document corpora or for detecting relations be-\ntween topics and other variables of interest. Additionally, topic models can be em-\nployed as a mean for dimensionality reduction (documents are mapped to a lower-\ndimensional topic space) [16], as input for prediction tasks, in recommender systems\n(e.g. for predicting semantically related tags) [14] or in information retrieval (e.g. to\nunderstand and disambiguate the topic of query terms) [33].\nExample. A paper about \u201cData Science\u201d might contain words from topics \u201cCom-\nputer Science\u201d, \u201cStatistics\u201d and \u201cData Visualization\u201d. While a news article about\n\u201cGerman Politics\u201d might contain words from topics \u201cPolitics\u201d and \u201cEurope\u201d. Both\n3\ndocuments are described by a distribution over all topics. But the shape of the dis-\ntribution will differ between the two documents. The topic distribution of a paper\nabout \u201cMachine Learning\u201d might show similarities with the \u201cData Science\u201d paper\nbecause it can contain words from topics \u201cComputer Science\u201d and \u201cStatistics\u201d.\nA single topic is the model of a hidden concept. It is a group of words that ap-\npear together in multiple documents. Regarding the previous example: words like\n\u201cRegression\u201d and \u201cCorrelation\u201d might appear together in the \u201cData Science\u201d and\nthe \u201cMachine Learning\u201d papers, so the topic model decides that \u201cRegression\u201d and\n\u201cCorrelation\u201d belong in the same topic.\nThe meaning of a topic is not determined by the algorithm itself. The algorithm\ndoes not know the semantic connections between words so it cannot attach a top\nlevel de\ufb01nition (e.g. \u201cStatistics\u201d or \u201cComputer Science\u201d) to describe the topic. The\ntop level description is usually attached by a human based on the topic models\u2019\nselection of words. For \u201cRegression\u201d and \u201cCorrelation\u201d one might attach the topic\nname \u201cStatistics\u201d. In some cases the labelling can be hard because the topic detection\nonly works on co-occurrence of words. So the algorithm might form a topic that is\nnot interpretable for humans.\n2.2 Latent Dirichlet Allocation\nLatent Dirichlet Allocation is a probabilistic topic model by Blei et al. [2, 9, 1].\nIn the context of text mining LDA model\u2019s variables are de\ufb01ned as follows. As-\nsume there are Mdocuments d1;:::;d M. A speci\ufb01c Document mis made out of Nm\nwords labelled wm;1;:::;w m;Nm. LDA assumes that a document is a bag-of-words.\nHence, only the presence of words in a document is relevant, not its position. All\nunique words form the vocabulary Vof the model.\nLDA assumes that each document contains words from different topics. There-\nfore, a document is a mixture of various topics. A document can be represented\nas a distribution \u0012mover all possible topics K. The total amount of topics in LDA\nKneed to be given as a parameter. Each single topic of index kis a multinomial\ndistribution \u001ekover all words in the vocabulary V.\nThe parameters \u000band\fcontrol prior beliefs of the model. Parameter \u000bis a \u2013\ntypically symmetric \u2013 vector of length Kand controls the prior probabilities in the\ntopic distribution of a documents. Parameter \fis a \u2013 typically symmetric \u2013 vector of\nlengthVand controls the prior probabilities in the word distribution of the topics.\nA Dirichlet distribution depending on \u000bencodes the intuition that documents\nonly have a signi\ufb01cant probability for a limited number of topics. A Dirichlet distri-\nbution depending on \fencodes that a topic can only have a signi\ufb01cant probability\nfor a limited number of words.\nWith these prerequirements, a text corpus Dconsisting of Mdocuments, each of\nlengthNm, can then be created with the following generative process:\n4\n1. For each document m, draw a multinomial distribution over the Ktopics:\n\u0012m\u0018Dirichlet (\u000b), wherem21;:::;M (1)\n2. For each topic k, draw a word distribution:\n\u001ek\u0018Dirichlet (\f), wherek21;:::;K (2)\n3. For all corpus word positions i;j, whereiindicates the ith document and j\nindicates the jth position of the word in this document i:\na) Draw the topic the word originates from:\nzi;j\u0018Multinomial (\u0012i) (3)\nb) Draw the word based on the chosen topic zi;j:\nwi;j\u0018Multinomial (\u001e(zi;j)) (4)\n2.3 Topic Model Evaluation\nThe evaluation step is crucial to interpret the results of the experiment. To rate the\nprediction performance of a trained LDA model, perplexity is used. Perplexity is\na common measure used to evaluate topic models. It describes the likelihood of\nheld-out documents regarding the trained topic model.\nUnfortunately, a good perplexity value does not necessarily indicate that the de-\ntected topics are interpretable for humans. Often the rating differs to human judge-\nment [7]. So, additional evaluation metrics are discussed.\n2.3.1 Perplexity\nPerplexity is a popular measure for the ability of a probabilistic model to predict new\nobservations [17, 2]. Because models which are able to accurately predict new events\nare typically the desired outcome of probabilistic modelling, perplexity scores are\nwidely used to evaluate probabilistic models. This also includes topic models.\nPerplexity is an intrinsic evaluation measurement based on the log-likelihood of\nheld-out documents. For a document d, which contains nwordsw1;:::;w n, and a\ntopic model that already learned Tand the topic distribution \u000b, the log-likelihood\nis de\ufb01ned as the following formula:\nL(d) =nX\ni=0lnp(wijT;\u000b) (5)\nTo calculate the actual perplexity for a document dthe log-likelihood is normalised\nwith regards to the words nin the document dand transformed back from logarith-\nmic space:\nPP=exp\u0012\n\u0000L(d)\nn\u0013\n(6)\n5\nA small perplexity indicates that a topic model does well in predicting the test sam-\nple. A high perplexity indicates that topic model is \u201csurprised\u201d to \ufb01nd the word\ncombination of the test sample. Thus, perplexity is well suited to compare different\ntopic models that are trained on various imbalanced corpora.\nPerplexity does not capture the quality of the topics though. It provides no infor-\nmation of the semantic context between words. Only the predictability of a docu-\nment by the topic model is known.\nExample. A topic can assign high probabilities to the words: \u201capple, car, banana,\norange\u201d. A human would judge these words as semantically incoherent.\nHence, even if the perplexity on the test documents is low the topics can be not\ninterpretable for a human. Chang et al. showed that perplexity often produces\ndifferent results than human judgement [7].\n2.3.2 Alternative Evaluation Metrics\nAnother way to evaluate topic models is rating the quality of the topics. A good\ntopic shows semantic cohesion between the words that were assigned to it. Eval-\nuating semantic cohesion between words is a dif\ufb01cult problem for an algorithm.\nTherefore, the quality of a topic is often evaluated by humans. A popular task given\nto human raters is to detect an intruder word in the set of the top-k words of a\ntopic [7].\nUnfortunately, these tests will not work for this thesis. The experiment produces\nhundreds of different models, each with several different topics. The evaluation\nby humans is not feasible for such a large number of topics. Only an automated\nevaluation would be reasonable.\nEven though evaluating semantic cohesion between words is a dif\ufb01cult problem\nfor machines, measures have been created to rate the topic quality automatically.\nPopular measures are the UCI coherence [21] and the UMass coherence [19].\nBoth measures use an external reference corpus to estimate the general word oc-\ncurrence and co-occurrence probabilities. A common choice for this corpus is the\nEnglish version of Wikipedia, as it contains a massive amount of documents that\nshow the natural use of the English language. The measures differ in their approach\nto combine the word probabilities for a topic.\nThe UMass coherence uses these probabilities to compute the mean of all log-\nprobabilities between a word given the next lower ranked word in a topic. The\nUCI coherence computes the mean point-wise mutual information [4] between all\npossible word pairs in a topic. For the calculation, a topic is often represented by its\ntop-10 most probable words.\nA study by R\u00f6der et al. [25] showed that the UMass coherence performs worse\nthan the UCI coherence when ranking the topics by their semantic cohesion. The\nUCI coherence displayed a higher correlation to human judgement on all tested six\n6\ndata sets. An even higher performance can be achieved when substituting the point-\nwise mutual information with the normalised point-wise mutual information.\nUnfortunately, none of these metrics would be meaningful for the experiments\nconducted in this thesis. During small scale test runs of the experiment several\nproblems occurred. One data set contains two different languages. To the best\nknowledge of the author, neither of the two measures have ever been applied to\nnon-English reference corpora. Therefore, the measures were not applied to the\nbilingual data set because it is uncertain if the measures produce equal results with\na non-English reference corpus.\nThe other two data sets indicated that the groups could not be clearly separated by\nthe model, which makes the comparison of topic quality across groups impossible.\nThus, the evaluation of topic quality is only feasible for corpora of English language\nwith strictly separable topics \u2013 a condition which will hardly be ful\ufb01lled by real-\nworld corpora. Even corpora with groups of different language do not produce a\nclear separation of topics, as discovered later in this thesis.\n2.4 Current State of Research\nData sets that contain an imbalanced set of groups appear frequently in the real\nworld. Examples include posts of male and female members on Twitter or contribu-\ntions on Wikipedia by citizens of different countries. Typically, some of the groups\nshow more activity than others, creating majority and minority factions. This im-\nbalance has to be taken into account when data is modelled, which is a well-known\nissue from text categorisation.\nThere exist various approaches to solve issues caused by group imbalance; a pop-\nular solution is to increase the amount of documents by oversampling documents\nof underrepresented groups or to change the weighting scheme of the classi\ufb01ers\n[15] [13].\nAn alternative approach by Chen et al. consists in undersampling and oversam-\npling documents using probabilistic topic models [8]. The authors show that the\nclassi\ufb01cation performs better on the minority documents if the samples are gener-\nated using a topic model. But even though they have shown that topic models can\nbe useful in terms of re-sampling, it remains unknown how relative group sizes\nin\ufb02uence topic models in general.\n2.5 Topic Models for Groups in Document Corpora\nThere exist two kind of topic models that cope with groups in corpora: Topic mod-\nels which require the explicit assignment of documents to groups and models which\nlearn about groups in the corpus by statistical evidence. Both kind of models can\nbe realised with groups-speci\ufb01c topic distributions, for instance with a hierarchy of\nDirichlet distributions. A very popular alternative to hierarchical Dirichlet distribu-\ntions is a hierarchy of Dirichlet processes, the Hierarchical Dirichlet Process (HDP).\n7\nIn the following, HDP-based topic models are introduced and an overview of topic\nmodels for known and unknown group information is given.\n2.5.1 Hierarchical Dirichlet Process\nThe hierarchical Dirichlet process (HDP) topic model was introduced in 2004 by Teh\net al. [29, 30]. Unlike other topic models it does not need to receive the amount of\ntopics as a parameter.\nIn the context of text mining the HDP model works as follows. Assume there are J\ndocuments d1;:::;d J. A document jis made out of Njwords labelled wj;1;:::;w j;Nj.\nEach document contains words from different topics. A document can be repre-\nsented as a distribution Gjover the in\ufb01nitely many topics \u0012ji. Each topic is a multi-\nnomial distribution over a set of words.\nDocuments Gican be created by repeating the following process njtimes: decide\non a topic and each time choose exactly one word based on the word distribution of\nthe chosen topic F(\u0012j;i). The topic decision process is different for each document.\nThis can be described with following formulas:\n\u0012j;ijGj\u0018Gj (7)\nwj;ij\u0012j;i\u0018F(\u0012j;i) (8)\nEven though each document should be different in terms of their topic distribution\nall documents should still have the same set of topics to choose from. To model this,\nthe authors used two Dirichlet processes:\nG0j\r;H\u0018Dirichlet (\r;H) (9)\nGjj\u000b;G 0\u0018Dirichlet (\u000b;G 0) (10)\nThe documents Gjshare the same base distribution G0which is another Dirich-\nlet process. The concentration parameter \u000bvaries for each document. Thus each\ndocument is conditionally independent given G0[29].G0is the global probabil-\nity measure dependent on the base distribution Hand the concentration parameter\n\r. Collapsed inference [31] and stochastic online-inference [11] allow for ef\ufb01cient\nparameter inference even for large corpora.[3].\n2.5.2 Topic Models for Known Groups\nA straight-forward way of modelling group-speci\ufb01c parameters is to model differ-\nent prior distributions over topics for different groups. The three level HDP [30]\nis an example of such a model. It is almost identical to the standard HDP topic\nmodel, except for additional, group-speci\ufb01c base-measures over topics which are\ndrawn from the global topic measure G0and which serve as input for the document-\nspeci\ufb01c Dirichlet processes.\n8\nAnother class of topic models which explicitly model group information are poly-\nlingual topic models [18, 5], which use given information about the language of doc-\numents. The model proposed by Mimno et al. [18] requires known pairs of trans-\nlated documents in the corpus, while other models, like the model proposed by\nBoyd-Graber et al. [5], are able to detect topic translations even without such trans-\nlation pairs. In their paper Boyd-Graber et al. claimed that standard LDA trained\non a corpus with multiple languages e.g. German and English would be able to dis-\ntinguish the two languages and assign language speci\ufb01c topics. This claim will be\nreviewed during this thesis. It will be examined if LDA can build group-assigned\ntopics for English and German groups in a corpus.\nFor settings where authorship information for documents is available, author-\ntopic models were developed by Rosen-Zvi et al. [26, 28]. It models and mixes\nauthor-speci\ufb01c topic distributions to explain the creation process of documents.\nAnother example for mixed group-speci\ufb01c topic distributions is the Multi Dirich-\nlet Process (MDP) topic model by Kling [12]. It \ufb01rst maps documents from so-called\ncontext-spaces such as time or geographical location to a set of groups which then are\nexplicitly modelled similar to the three-level-HDP . Additionally, relations between\ngroups are modelled.\n2.5.3 Topic Models for Unknown Groups\nThe 3-level HDP model [30] can be extended for learning about a-priori unknown\ngroups: One could treat the group-assignment as an unknown variable which has\nto be learned during parameter estimation. Canini et al. presented a HDP-based\nmodel which even allows for learning more complex hierarchies, i.e. n-level HDP\nmodels [6].\n3 Methodology\nThe goal of this thesis is to investigate the in\ufb02uence of imbalanced corpora on the\nmost popular topic model, LDA. While some probabilistic models \u2013 such as cluster-\ning methods \u2013 explicitly model the presence of latent groups, standard topic models\nsuch as LDA do not model group-speci\ufb01c parameters. This thesis investigates if a\ntopic model can still differentiate between the two latent groups and will rate the\nprediction performance in terms of perplexity.\nOne could hypothesise that corpora with latent groups of unequal size will in-\n\ufb02uence the prediction quality of a trained topic model for minority and majority\ngroups. This section explains the experimental setup used to investigate this hy-\npothesis.\nFigure 1 depicts the overall setup of the experiment. It can be split up into \ufb01ve\nessential steps: (I) creation of a sample seed, (II) article sampling, (III) assignment\nof documents to test and training corpora, (IV) training of LDA and (V) param-\neter inference and model evaluation. Each step is described in detail in the \ufb01ve\n9\nFigure 1: Outline of the experimental setup for the Wikipedia data set. Start with\ntwo groups in a data set: DE and EN. The documents in these groups have already\nbeen paired and are comparable. (I) Decide which pairs to use in a sample (sam-\nple seed). (II) Create imbalanced samples using the documents of the chosen pairs.\n(III) Split the documents into training and test documents by a ratio of 70/30. (IV)\nTrain LDA using the training corpus. (V) Evaluate LDA using the test sets. Repeat\nsteps (II)-(V) for varying relative group size relations to examine the impact of dif-\nferent group sizes. Repeat the whole setup 50 times to ensure that the results are not\nin\ufb02uenced by the chosen sample seed.\n10\nfollowing sections.\nThe experiment is repeated using three different data sets. Their construction is\nexplained in section 4. Regarding the experimental setup the corpora are treated\nalmost identically. In Figure 1 and in the following sections, the Wikipedia data set\nis employed to explain the data set creation process. This data set\u2019s group division is\nmore evident than the group division of the other tested data sets. The two groups\nthat divide the Wikipedia data set are German articles (DE) and English articles\n(EN).\n3.1 Sample Seed\nDuring the experiment the size relation between two groups will be manipulated.\nAt the same time, the high level concepts across these groups will be controlled.\nThat is, during one iteration of the experiment the concepts covered in the data will\nstay the same. This is not possible with traditional data sets.\nTherefore, during the construction of each data set, the documents have been\npaired (e.g. for each English Wikipedia article the German version of the article is in-\ncluded). This makes the root of the experiment a data set of pairs and not a data set\nof plain articles. Articles of a pair will be called partners throughout the thesis. Dur-\ning the construction of the data sets, it is ensured that the partners are comparable\nand contain the same concepts.\nBased on the pairs of the initial data set a sample seed is created. The sample seed\nis a random selection of article pairs in the original data set. Sample seeds are the\nfoundation of the samples. For each data set 50 different sample seeds are created.\nThe variation in each sample seed ensures that the experiment tests a wide range of\ndifferent topic distributions and the results do not occur by chance.\nFigure 2: Example of a Sample Seed. Red articles are English articles. Blue articles\nare German articles. The numbers indicate the pair id.\nFigure 2 shows an example of a sample seed of size 10. Each red or blue box is an\narticle. A red box represents an English article while a blue box represents a German\narticle. The numbers indicate the \u201cpair id\u201d. They indicate to which pair each article\nbelongs e.g. red article 175 is the partner article of blue article 175. The used pair ids\nand their order are determined randomly.\nIn this thesis, a sample seed size of 20,000 pairs is used for the Wikipedia corpus.\nThe UK corpus contains 10,000 pairs and the US corpus contains 4.000 pairs.\n11\n3.2 Sampling\nArticles have been paired in the initial data sets such that the partners contain the\nsame high level concepts. In the sample seed, all articles in one group, e.g. EN\n175, contain a partner article in the other group e.g. DE 175. Therefore, the concept\ndistribution of English articles in the sample seed is equal to the concept distribution\nof German articles in the sample seed. In fact, the concept distribution will always\nstay equal as long as you select exactly as many articles as there are pairs contained\nin the sample seed and you ensure that there are no articles with the same pair id.\nExample. If the sample seed contains 10 pairs, one can select 10 articles with dif-\nferent pair ids. Which group each article belongs to is irrelevant. The hidden concept\ndistribution stays the same. This property allows building samples with different\ngroup size relations while controlling the covered concepts.\nTo build the actual samples a sliding threshold is introduced that indicates which\narticles should be included in a sample at which size relation. Figure 3 shows the\nconcept of this threshold.\nThe red bar indicates English documents of the sample seed, the blue bar indi-\ncates German documents of the sample seed. The white box is the threshold. It is\nplaced between two pair ids. Bright documents are included in a sample, darkened\ndocuments are excluded. Pair ids that occur before the threshold provide their En-\nglish article while pair ids beyond the threshold will provide their German article.\nThe sample seed contains the pair ids in a random order. Therefore, the documents\nthemselves do not need to be selected randomly to form a random sample.\nThe threshold can be freely adjusted to obtain various imbalanced samples. Fig-\nure 3a shows an example of a sample that contains 40% English documents and 60%\nGerman documents. When moving the threshold further along the sample seed, a\nsample as depicted in Figure 3b, which contains 80% English documents and 20%\nGerman documents, can be created. During this thesis, such samples are referred to\nas, say, 40/60 or 80/20 samples based on size relation of the groups. The following\nfractions are tested: 10/90, 20/80, 30/70, 40/60, 50/50, 60/40, 70/30, 80/20 and\n90/10.\n3.3 Training and Test Corpora\nAfter sampling a sample with the desired group size relation, a training and two\ntest corpora are created. The training corpus is used to train the topic model. The\ntwo test corpora are used to evaluate the topic model.\nA part of each document is removed from the training set to use it as a test doc-\nument in the test corpus. Throughout this thesis, 70% of each document are used\nfor training and 30% of documents for testing. This ensures that the concepts that\nappear in the test corpus have already been seen by the topic model in the training\n12\n(a)Example of a 40% EN and 60%\nDE sample\n(b)Example of a 80% EN and 20%\nDE sample\nFigure 3: Creation of samples from a sample seed. Red bars describe English doc-\numents, blue bars describe German documents in the sample seed. The white box\nindicates the position of the threshold. Only the bright parts are contained in the\nsample. Darkened parts are discarded.\ncorpus. At the same time it prevents testing the topic model on the same documents\nthat it was trained on.\nTo illustrate the process of the test and training corpus creation, Figure 4 shows\nthe transformation of the sample seed of Figure 4 into training and test data for a\n40/60 split.\nBars represent unique documents. For convenience they all share the same length\nin this example. In reality the size varies. Coloured bars have been selected to\nbe part of the sample, the grey bars have been removed. The green parts of a bar\nshow the parts that are assigned to the training corpus. The red parts of documents\n175, 127, 201 and 532 are assigned to the English test corpus while the blue parts\nof documents 24, 12, 125, 250, 52 and 310 are used in the German test corpus. To\nprevent unintentionally capturing reoccurring patterns in documents, the position\nof the test data is chosen randomly.\n3.4 Training LDA\nThe training corpus is used to train the topic model. The topic model used in this\nthesis is Latent Dirichlet Allocation (LDA). A general introduction to topic models\nand the de\ufb01nition of LDA can be found in section 2.\n13\nFigure 4: Example of a 40/60 sample, whose documents are split into training and\ntest documents. Coloured bars are contained in the sample. Green parts are used\nduring training. Red parts are English test documents. Blue parts are German test\ndocuments.\nTo train LDA on the test documents, the text corpus needs to be transformed into a\nnumerical corpus. For this task the gensim module [24] is used. Gensim is a scalable\nand robust python module that allows ef\ufb01cient use of various topic models. Gensim\nprovides an implementation of LDA, together with the methods to transform the\ntext corpora into numerical corpora such that they can be understood by the model.\nGensim\u2019s methods are used to translate the test and training corpora. To translate\ntext corpora into numerical corpora, a dictionary is created based of the training\ncorpus. The dictionary maps each word in the training corpus to a number. Using\nthe dictionary the training corpus can be transformed into a matrix. The same dic-\ntionary is used to also transform the test sets of a sample. Unique dictionaries will\nbe computed for each sample.\nThe dictionaries are often very large. A large dictionary slows the training and\nevaluation process of a topic model signi\ufb01cantly. So, it is reasonable to reduce the\namount of words in the dictionary. It is common to remove words with low fre-\nquency in the training corpus. In this thesis, words that do not appear in a certain\namount of documents are removed from the dictionary. This threshold has been ad-\njusted to each data set\u2019s sample size. The Wikipedia sample\u2019s threshold is at 20 docu-\nments, the UK sample\u2019s threshold is at 10 documents and the US sample\u2019s threshold\n14\nis at 5 documents.\nWords that appear in the test set but do not appear in the dictionary cannot be\nprocessed by the model. A dictionary might not contain a word because it was not\nin the training corpus or it has been removed because it does not appear in enough\ndocuments. To evaluate the impact of this loss, the vocabulary mismatch is reviewed\nin section 5.1.\nThe transformed training corpus is used to train gensim\u2019s LDA model. The im-\nplementation is based on the publication \u201cOnline Learning for Latent Dirichlet Al-\nlocation\u201d by Hoffman et al. [10]. It uses an online variational Bayes algorithm over\nchunks of the training corpus to estimate the variational posterior of LDA.\nLDA is a topic model that requires the user to specify the amount of topics a\nmodel should learn. To determine if the amount of topics in\ufb02uences the results,\nthree different topic models are trained during the experiment. The different topic\nmodels learn 64, 128 and 256 topics.\n3.5 Evaluation\nThe trained LDA model is used to compute the perplexity of each groups held-out\ntest documents. A de\ufb01nition of perplexity can be found in section 2.3.1. The per-\nplexity captures how likely the test documents are given the trained LDA model. It\nindicates how well the topic model predicts the test corpus. A low value of perplex-\nity shows a high prediction performance. It is reasonable to expect that if a group is\nforced into a minority role the test documents will show a higher value of perplexity\nbecause they can be predicted worse.\nGensim\u2019s LDA model provides a method to compute the perplexity on a test cor-\npus. The results of this function are used during the perplexity evaluations in sec-\ntions 5.2 and 5.3. They answer the questions (i) if the presence of groups in\ufb02uences\nthe predictive performance of topic models and (ii) if the predictive performance of\nthe groups changes with varying the relative group sizes.\nTo answer the question (iii) if a topic model is able to distinguish the two latent\ngroups, the topic predictions of the test documents are inferred from the trained\nLDA model. A model is able to distinguish the two groups if it is able to assign\ntopics to a group unambiguously for both groups. A topic is assigned to a group, if\nthe probability of belonging to a group Ggiven a topic Tis larger than 90%. That is,\nregarding the Wikipedia samples, for G2fEN;DEg:\nP(GjT)>90% (11)\nIf the model is able to distinguish the latent groups the question (iv) if the propor-\ntion of group-speci\ufb01c topics is under-proportional relative to the share of the group\nin the training corpus can be examined.\nThe evaluation step completes the setup of the experiment depicted in Figure 1.\nThe experiment is repeated 50 times for 3 different data sets, each iteration testing 9\ndifferent size relations each. Hence, the whole experiment trains and evaluates 450\ndifferent models for each data set, that is 1350 models in total.\n15\n4 Data Sets\nAfter de\ufb01ning the used methods and describing the experimental setup, the data\nsets will be described. The data sets are the foundation of the experiment. Three\ndifferent data sets are used. All data sets are constructed such that they can be\nseparated into two groups. The separation into two groups allows the creation of\nsamples in which the fraction of each group can be manipulated. Leading to the\ncreation of imbalanced corpora.\nIn the samples, the high level concepts across the groups should be controlled.\nIn order to make this possible, the documents in the data set are be paired. That\nis, a document of one group has a partner article in the other group. Both of these\ndocuments refer to the same high level concepts but use different words to do so.\nThe choice of words to describe these concepts depends on the group of documents\nthey belong to.\nThe \ufb01rst data set is constructed such that its groups show a high difference in the\nchoice of words. The groups will differ by using different languages. This data set\nis built using a subset of Wikipedia articles in German and English language. The\narticles are paired using Wikipedia\u2019s language links. These links connect all different\nlanguage versions of the same article. They ensure connected articles always cover\nthe same concepts.\nThe other two data sets represent a more subtle scenario where the choice of\nwords is rather similar. One data set contains US news articles, the other data set\ncontains UK news articles. The document groups in these data sets differ in their\npolitical orientation. One group contains articles that represent a political left wing\nopinion while the other group contains articles that represent a political right wing\nopinion. The news sources and their political orientation are hand-picked.\nThe documents are selected using \u201cEvent Registry\u201d. \u201cEvent Registry\u201d is a service\nthat tracks news outlets and groups news articles to certain events. The articles are\npaired based on these events. Similar to the Wikipedia language links, this ensures\nthat the articles in a pair reference the same overall concepts.\n4.1 Wikipedia\nWikipedia is the largest free online encyclopedia available. It was launched in 2001\nby Jimmy Wales and Larry Sanger. Wikipedia is available in nearly 300 different\nlanguages. The English Wikipedia alone counts above 5 million articles. The articles\nare created by users.\nWikipedia is an interesting corpus to investigate since it is one of the largest text\ncorpora available. There are a lot of different documents covering a large variety of\nconcepts. At the same time, it provides the tools and meta data such that documents\ncan be divided into two groups while controlling the top level concepts.\nThe articles are divided based on their language. In this thesis, the Wikipedia data\nset has a group of English documents and a group of German documents. Other lan-\nguage combination might form interesting corpora as well. To \ufb01nd the documents\n16\nthat cover the same concepts, Wikipedia\u2019s \u201clanguage links\u201d are used. Almost all\narticles contain \u201clanguage links\u201d which connect two articles that cover the same\nconcepts in a different language. For example: the English article \u201cGermany\u201d is\nconnected to the German article \u201cDeutschland\u201d.\n4.1.1 Data Acquisition and Formatting\nWikipedia offers regular text dumps of their articles, links and meta data. There are\nseparate dumps for each language that Wikipedia offers.1The Wikipedia data set\nused in this thesis should contain German and English articles, hence the starting\npoint are all articles of the English and German Wikipedia. These articles are ex-\ntracted as a raw text corpus. In this thesis the latest dumps of May 2017 have been\nused.\nWikipedia article dumps still contain the Wikipedia speci\ufb01c formatting2i.e. HTML\nor Markup formatting. HTML and Markup formatting text is removed to retrieve\na clean text corpus. To achieve this goal the WikiExtractor3by Giuseppe Attardi\nis used. The tool removes everything besides the section headers and actual text\ncontent of the dump and stores them together with the article title and id.\nDuring that cleaning process several pages have been completely removed by the\nWikiExtractor by default. These pages were mainly forwarding pages or special\npages like category or help pages.\nThe forwarding pages e.g. \u201cwikipedia.org/wiki/Bombay\u201d are empty after the\nWikiExtractor\u2019s format cleaning because they only include forwarding information\nand no actual text. The category pages, help pages, etc. \u2013 which show the URL\nschema \u201cTag:XYZ\u201d e.g \u201cwikipedia.org/wiki/Category:Germany\u201d \u2013 are removed be-\ncause they are often only a list of links. These links refer to pages that belong to the\nparticular category. Help pages describe how the user should act on the Wikipedia\nplatform and are no real articles. So their removal will not matter with respect to\nthe experiment because only real articles should be analysed.\n4.1.2 Article Pairing\nDuring the experiment, articles need to be paired across groups. The pairs ensure\nthat the high level concepts in a sample can be controlled while freely varying the\ngroup size relation. Paired articles are be called partner articles orpartners throughout\nthe thesis. That means, each English article will have a German partner article in the\ndata set. Both articles will contain the same high level concepts.\nTo pair the articles Wikipedia\u2019s language links are used. Each Wikipedia dump\ncontains a list of links to the different language versions of an article (\u201clanglinks\u201d).\nWikipedia\u2019s language links do not follow a direct ID-to-ID mapping. They follow\n1https://dumps.wikimedia.org/enwiki/latest/\nhttps://dumps.wikimedia.org/dewiki/latest/\n2https://www.mediawiki.org/wiki/Help:Formatting\n3https://github.com/attardi/wikiextractor\n17\nFigure 5: Wikipedia\u2019s language link schema. Blue boxes contain a language\u2019s title\ninformation. Red boxes contain a language\u2019s article ID. Based on these information\nthe links can be resolved in the article data bases.\nan ID-to-title mapping. For example, the German language links contain the lan-\nguage speci\ufb01c ID of the German article, a language identi\ufb01er in ISO 639-1 standard\n(\u201cen\u201d,\u201cde\u201d,...) and the title of the foreign partner article. A visualisation of this\nlinking scheme can be found in Figure 5.\nThe links are not necessarily bidirectional i.e. there can be a link from a German\nto an English article while the link from the English to the German article is absent.\nFigure 5 shows that it is possible to pair the documents based on language links.\nNevertheless, the ID-to-title mapping makes it unintuitive to create article pairs.\nTherefore, a title look-up is created. It maps a title of an article to the corresponding\nEnglish or German ID. With its help, most of the language links can be resolved such\nthat the articles can be paired.\nSome links can not be resolved. During the data acquisition and formatting step\nthe forwarding pages, help pages and category pages have been deleted. The lan-\nguage links can still link to these pages. Then no text could be attached and the link\nis dropped. The forwarding links could have been resolved to the real pages but this\ncan introduce new errors. It cannot be predicted to which page the forwarding page\nrefers. Hence, the lookup could violate the ability to control the high level topics in\nan article pair. The following example illustrates the possible error.\nExample. The English page \u201cSquare_kilometre\u201d does not have a direct German\nequivalent. The language link leads to a forwarding page that links to \u201cQuadrat-\nmeter\u201d. \u201cQuadratmeter\u201d is the German translation of \u201csquare meter\u201d. \u201cSquare\nkilometer\u201d only has a minor subsection in the German \u201csquare meter\u201d article.\n18\nWhile in this case the error would only be minor because it is generally the same\nconcept, one cannot assume that this is always the case when there is a link between\nan actual article and a minor subsection.\nExample. Imagine an article about a football tournament covering all teams and\ngames. If the language link of this page forwards to a German page of \u201cFootball\u201d\nwhere the tournament is only mentioned in a subsection or a list of tournament.\nThen the articles are not comparable anymore. These pages might be sharing a few\nof the core concepts but the concept distribution differs too much overall.\nSo forwarding links should not be used in the experiment and the links to forward-\ning pages will be dropped.\n4.1.3 Link Resolution\nResolving the language links ensured that every remaining article has at least one\npartner. However, the partners are not always unique. Each Wikipedia page can\nonly have one outgoing language link but it can have multiple in-links. Sometimes\narticles would be assigned to multiple pairs. This section explains which language\nlink patterns can be used safely while others might introduce errors.\nFigure 6 and Figure 7 summarise all possible language link patterns that can in-\n\ufb02uence a pair. A node represents an article. The blue link represents the edge on\nwhich a pair is built. The direction of the edge follows the direction of Wikipedia\u2019s\nlanguage link. In all patterns the pair is based on the language link from article A1\nto articleB1. In general, A1is called the referring article, as it is the start point of\nthe link.B1is the referred article. The labels \u201cA\u201d and \u201cB\u201d represent that the arti-\ncles belongs to different language groups. If \u201cA\u201d represents a German article, \u201cB\u201d\nrepresents an English article and vice versa.\nThe patterns depicted in Figure 6 make it impossible to build unique pairs. Fig-\nure 6a shows a scenario where the referred article B1does not link back to the re-\nferring article A1. Instead it links to a different page A2. AsA1andA2are possible\npartners for B1this case cannot be resolved without further knowledge.\nIn Figure 6b the linked article B1is recipient of multiple links ( A1andA2). Again,\nin this case it cannot be determined which of the articles A1orA2should be the\npartner ofB1.\nFigure 6c depicts a case where the referring article A1has an incoming link from\nan additional article B2. This time B1orB2could be possible partners of A1.\nEach of these patterns suggests multiple possible pairs and will need further in-\nvestigation to actually \ufb01nd the right pair. For this thesis, nodes involved in these\npatterns will be removed.\nThe remaining language link patterns are depicted in Figure 7 and are used to\nbuild pairs.\nFigure 7a describes a situation where the articles A1andB1are only connected by\nthe outgoing language link from article A1. As long as there is no evidence against\n19\n(a)Referred article is referencing a different article\n(b)Referred article is referenced by additional articles\n(c)Referring article is referenced by additional articles\nFigure 6: Ambiguous Wikipedia link patterns. These links will not be used to build\npairs. Articles contained in these link patterns are discarded.\n(a)Unique link without any additional referring article\n(b)Bidirectional link (disregarding additional articles\nthat might reference an article of this pair)\nFigure 7: Unambiguous Wikipedia link patterns . These links will be used to build\npairs in the Wikipedia dataset.\nthis connection (e.g. a case from Figure 6) both articles are paired.\nThe \ufb01nal case, depicted in Figure 7b, is the optimal case. Both articles are refer-\n20\nencing each other. If this bidirectional connection is present the articles A1andB1\nare paired regardless of other articles linking to these pages. Even if there are other\narticles linking to the pair from the outside it is more likely that the bidirectional\nconnection captures the actual pair.\n4.1.4 Article Pair Filtering\nEven if the articles can be paired, not all article pairs are suitable for the experiment.\nTo ensure that the articles contain enough information and that articles in a pair are\ncomparable in terms of their content, two restrictions will be applied:\n1. If one or both articles of a pair contain less than 50 words the pair is removed\n2. If one article is more than twice as long as its assigned partner the pair is\nremoved\nThe \ufb01rst restriction forces a minimal amount of information in each article. If an\narticle is too short it only contains few information. This is especially meaningful\nwhen splitting the documents into training and test document. The test document\ncontains 30% of the original document. Hence, a short article might not contain\nenough information to build a reasonable test document. Therefore, a threshold of\n50 words is introduced. If a pair contains at least one article that is shorter than 50\nwords it will be removed. Both groups, German and English articles, contain such\ndocuments. In total 262,415 pairs fall below this threshold and are discarded.\nThe second restriction applies to the length difference between the articles of a\npair. If the length of article partners differs too much then the concepts covered\nin both articles might differ. Varying concepts between partnered articles violate\nthe pre-requirement which is necessary to control the concepts in a sample. Big\ndifferences between the concepts can cause \ufb02awed results.\nExample. Imagine a pair where the German article has 5000 words and the En-\nglish article has 100 words. Both articles will share the core concepts but the longer\narticle will go much more into detail and contain additional concepts. Consequently,\nthe articles are not comparable.\nTo prevent this phenomenon, the size difference between partnered articles is re-\nstricted. If one article has twice the size of its partner article (or longer), the pair will\nbe removed. In the whole data set 557,875 pairs do not meet this requirement. In\ntotal 651,092 pairs do not meet one or both restrictions and are removed.\nThe documents in these pairs are stemmed and stop words are removed. A de-\nscription of this process can be found in section 4.3. The \ufb01nal data set is described\nin section 4.4.1.\n21\n4.2 Event Registry\nEvent Registry is a service that tracks news articles. The company tracks more than\n100.000 news sources in 15 languages. Each new article that is released by one of\nthese news sources is analysed. During the analysis, Event Registry detects article\ngroups that report the same event using machine learning.\nThese event-assigned article groups allow building article pairs based on these\nevents. The assumption is, that articles that report the same event refer to the same\nconcepts. Therefore, the event assignment can be treated like a language link in the\nWikipedia data set.\nThe group assignment in the Event Registry data sets is done based on the polit-\nical orientation of the news sources. The data set differentiates between right-wing\nand left-wing sources. The \ufb01rst data set will focus on American sources and will\nbe referred to as the US data set. The second data set will focus on British sources\nand will be referred to as the UK data set. The creation process of both data sets\nonly differs in the initial selection of news sources. Thus, this section describes the\ncreation of both data sets.\n4.2.1 Source Selection\nEvent Registry tracks articles from multiple languages. In this thesis only articles\nwritten in English language are used. American and British articles are treated sep-\narately which leads to the creation of two different data sets, the US data set and the\nUK data set.\nEach data set contains groups that differ in their political orientation. A data set\ncontains articles with a right-wing orientation and articles with a left-wing orienta-\ntion. The political orientation of a single article cannot be determined per se. Never-\ntheless, the general political orientation of the news sources that publish the articles\ncan be assessed. The assignment of political orientation is based on two surveys that\ntried to determine the political orientation of several news sources.\nThe \ufb01rst survey was conducted by YouGov [27]. YouGov is an international mar-\nket research and data analytics company. It\u2019s survey will be used as the foundation\nof the UK data set. The survey does have no in\ufb02uence on the US data set.\nIn 2017 YouGov conducted a survey in which they asked a sample of 2040 citizens\nof the UK how they would rate the political orientation of 8 mainstream newspa-\npers. The results (excluding between 39-49% of respondents who answered with\n\u201cdon\u2019t know\u201d) are displayed in Figure 8.\nFigure 8 shows a left-wing orientation for \u201cThe Guardian\u201d and \u201cThe Mirror\u201d.\nTherefore, they will be categorised as left-wing in the data set. \u201cThe Times\u201d, \u201cThe\nTelegraph\u201d, \u201cThe Sun\u201d, \u201cThe Daily Express\u201d and \u201cThe Daily Mail\u201d show a right-\nwing orientation and will be categorised as such in the data set. \u201cThe Independent\u201d\nis a corner case. It is not as left as the other two sources but based on its difference to\nthe \u201cright-wing\u201d articles it will still be considered \u201cleft-wing\u201d. This categorisation\nwill be extended by the second survey.\n22\nFigure 8: Political Orientation based on YouGov survey [27]. This survey is the\nfoundation of the UK news source categorisation. \u201cThe Guardian\u201d, \u201cThe Mirror\u201d\nand \u201cThe Independent\u201d are categorised as \u201cleft\u201d; the remaining news sources are\ncategorised as \u201cright\u201d.\nThe second survey was conducted by the Pew Research Center [20]. The Pew\nResearch Center is an American nonpro\ufb01t, nonpartisan \u201cfact tank\u201d. They conduct\ndata-driven social science research. While extending the UK data set, this survey\nwill serve as the foundation of the US data set. It is used to extend the UK data set\nbecause it contains sources that are commonly perceived as British news sources.\nIn 2014 the Pew Research Center asked a sample of 2901 web respondents a series\nof 10 political values questions. They analysed the audience of 36 news sources\nand created an ideological pro\ufb01le for each audience. Figure 9 shows the average\nideological orientation of each audience on a scale comparing it to the ideological\nplacement of the average respondent.\nThe group of news sources in the range of \u201cMSNBC\u201d to \u201cWall Street Journal\u201d\nhave been considered as too central and are not used in this thesis. All sources\nwhose ideological orientation is left of this group will be considered \u201cleft-wing\u201d, all\nsources that lay right of this group will be considered \u201cright-wing\u201d.\nEven though this survey was targeted for American audiences \u201cBBC\u201d and \u201cThe\n23\nFigure 9: Ideological Pro\ufb01le of Each Source\u2019s Audience based on Survey of Pew\nResearch Center [20]. This survey builds is the foundation of the US news source\ncategorisation. Sources assigned left from \u201cMSNBC\u201d are categorised as \u201cleft\u201d;\nsources assigned right from \u201cWall Street Journal\u201d are categorised as \u201cright\u201d.\nGuardian\u201d are primarily British news sources and will thus be used in the UK data\nset. They are not used in the US data set. \u201cThe Huf\ufb01ngton Post\u201d has two different\nweb appearances. One tailored for the UK market and another appearance for the\nAmerican market. The respective versions are used as news sources for the accord-\ning data set. This leads to the \ufb01nal assignment of news sources for the UK data set\ndepicted in table 1.\nThe remaining news sources are used to create the American data set as far as they\nare available at Event Registry. Using just these sources, there is a heavy overhead of\nleft sources. To even the count between left-wing and right-wing sources additional\nright sources are added. These sources are hand-picked.\n24\nTable 1: Categorisation of UK online news into left and right political orientation.\nArticles from a left and a right news source on the same subject are paired in the\nexperiment to construct the UK data set.\nLEFT RIGHT\ntheguardian.com thetimes.co.uk\nindependent.co.uk telegraph.co.uk\nbbc.co.uk thesun.co.uk\nbbc.com dailymail.co.uk\nhuf\ufb01ngtonpost.co.uk express.co.uk\nmirror.co.uk\nThe following news sources: \u201cNew York Post\u201d, \u201cWorld News Daily\u201d, \u201cNews-\nmax\u201d and \u201cTownhall\u201d. To assure that these source have a right bias, the news out-\nlets were checked on \u201cmediabiasfactcheck.com\u201d regarding their political tendencies.\nThe website \u201cmediabiasfactcheck.com\u201d aims to de\ufb01ne the credibility and political\norientation of news outlets. They are funded by advertising and individual dona-\ntions. The credibility of this website is not comparable to of\ufb01cial survey companies\nlike YouGov or the Pew Research Center but their judgement matched the results of\nthe two surveys. The \ufb01nal list of news sources for the US data set is shown in table\n2.\nThe source lists 1 and 2 are used to create article pairs for their respective data set.\nA pair will contain an article from a left source and a right source. The Assumption\nis, that that left-wing articles differ from right-wing articles and that the \u201cstrength\u201d\nof the political orientation within each group is equally pronounced. With this as-\nsumption all pairs can be treated equally e.g. an article pair from \u201cThe Guardian\u201d\nand \u201cThe Times\u201d will be treated the same as an article pair from \u201cBBC\u201d and \u201cDaily\nMail\u201d.\nTable 2: Categorisation of US online news into left and right political orientation.\nArticles from a left and a right news source on the same subject are paired in the\nexperiment to construct the US data set.\nLEFT RIGHT\nnewyorker.com breitbart.com\nslate.com foxnews.com\nnytimes.com rushlimbaugh.com\nnpr.org theblaze.com\npbs.org nypost.com\nwashingtonpost.com wnd.com\nbuzzfeed.com newsmax.com\npolitico.com townhall.com\nhuf\ufb01ngtonpost.com\n25\n4.2.2 Data Acquisition\nWith the news source lists the data sets can each be divided into two groups based\non their political orientation. This section explains how these sources\u2019 news articles\nare acquired to build the UK and US data set.\nThe articles are requested from Event Registry. Event Registry offers an API that\nlets the user search for events and articles respectively. For this thesis, the Event\nRegistry team provided the author with a 5000 request token license. Using this\nlicense data in a time frame of about two and a half month , starting at 10.07.2017\nuntil 25.9.2017, was captured. The goal was the selection of events that contain at\nleast one article from a left source and an article from right source.\nThe author did not \ufb01nd a way to extract the necessary data with a single re-\nquest. You can request events that meet the condition that they contain left and\nright sources but you cannot retrieve articles associated with this event in the same\nstep. To solve this issue the request has been divided into two steps:\n1. Finding events that contain at least one left source and one right source (dis-\nregarding US and UK separation at this point)\n2. Retrieving the articles of the according events\nThe \ufb01rst step can be implemented in a single request. The request retrieves all\nevent ids that contain articles from a left and right source in a time frame of one\nweek. Each week containes between 1500-2500 events. Each of these events has to\nbe called separately to retrieve their articles. The maximum amount of articles that\ncan be retrieved in one request is 200.\nSince the total amount of available requests was limited for this thesis, the main\ngoal of the data extration was, to minimise the amount of requests necessary for\neach event while still extracting the articles needed to build a pair. This led to the\nfollowing extraction process:\n1. Pick an event id from the events that contain a left and right article\n2. Fetch 200 articles using the event id\n3. If the fetched articles contain a left and right article pair, you are done\n4. Else: Go to 2 (or mark as unsolvable if request threshold is reached)\nThe process tries to keep the amount of requests per event as small as possible by\nstopping early when at least one article pair can be build. This is a reasonable step\nto take as only a single article pair per event necessary. If it is possible to create addi-\ntional pairs, these pairs are created but they are not mandatory for the experiment.\nIn general, this process aims to create article pairs that cover a lot of events. It does\nnot try to retrieve all articles of an event.\nTo reduce the amount of requests per event even further, the order in which the\narticles are received is adjusted. Articles are received in order of their popularity.\n26\nEvent Registry offers four different sorting methods that decide in which order the\narticles will be returned: \u201cDate\u201d, \u201cRelevance\u201d, \u201cShares on Social Media\u201d and \u201cStory\nCentrality\u201d. For this thesis \u201cShares on Social Media\u201d is used. This sorting method\nincreases the chances of getting a left and a right article in the \ufb01rst requested 200\narticles due to the fact that all chosen news sources are rather popular or controver-\nsial.\nThis choice introduces some degree of selection bias as it alters the order in which\nthe news articles received. Popular news sources are covered more frequently be-\ncause unpopular news sources are sometimes not included in the \ufb01rst 200 received\narticles. However, based on the previous assumption that articles from news sources\nwith the same political orientation are equal in their \u201cstrength\u201d of ideological faith,\nthe sources are interchangeable and the in\ufb02uence can be disregarded. Nevertheless,\nif there is no request limit, receiving the articles sorted by their \u201cDate\u201d is preferred\nas it will contain less bias.\nAfter the articles of an event have been retrieved and at least one pair of left and\nright articles has been found, the articles can be paired. If exactly one left and one\nright article is found, they are paired directly and the next event is processed. If\nmultiple pairs are possible, then the left and right articles will be paired randomly.\nEvent Registry sometimes contains articles that are assigned to multiple events.\nFor sampling only single occurrences of an article are desired. So if articles appear in\nmultiple pairs, one pair is picked randomly while the remaining pairs are discarded.\n4.2.3 Article Pair Filtering\nThe article pairs of the Event Registry data sets are treated with the same restrictions\nthat were introduced in section 4.1.4. An article has to have a minimum length of\n50 words to ensure that an article contains enough information to be split into a\ntraining and test document. Furthermore, in a pair of articles one article can only\nbe at maximum two times as large as its partner. If the length difference between\narticles in a pair is too large, the contained information and the high level concepts\nwill vary such that the articles are not comparable anymore. Pairs that do not meet\nthese requirements will be dropped.\nOnly 69 article pairs in the UK data set contain an article with less than 50 words.\nThe US data set contains only one such article. In the UK data set 11.827 pairs do not\nmeet the length difference restriction. In the US data set 3.975 pairs do not ful\ufb01l it.\nIn total, the restrictions leads to the removal of 11.857 UK pairs and 3.975 US pairs.\nThe documents in the remaining pairs are stemmed and stop words are removed.\nThe description of this process will follow in section 4.3. The \ufb01nal data sets are\ndescribed in section 4.4.2 and 4.4.3.\n4.3 Stemming and Stopword Removal\nAfter building the three data sets all articles still contain a lot of words that do not\ncontain any information about the content of the article e.g. \u201cthe\u201d,\u201cand\u201d, etc. These\n27\nwords are called stop words and appear in almost every document. Due to their\nhigh occurrence frequency they often in\ufb02uence the topics that a topic model builds.\nIn this case, the topic model builds topics that are based on syntactic conventions\nand not on semantic connection. To prevent this effect stop words are removed\nfrom the corpora.\nBefore removing words, the initial structure of the document is saved. This infor-\nmation is important to correctly split the documents into a test and training docu-\nment during the experiment (section 3.3). The test split will take 30% of each doc-\numents at a random position. Therefore, the the initial structure of the document\nshould stay intact.\nTo save the structure, the original length of each document is stored and a position\nidenti\ufb01er is added to each word e.g. \u201cI love data\u201d will be transformed to [(\u201ci\u201d,0),\n(\u201clove\u201d,1), (\u201cdata\u201d,2)].\nTo remove the stop words prede\ufb01ned stop lists are used [16]. Two different stop\nlists are used, one for English and one for German documents. The foundation of\nthese stop lists are the stop lists provided by the natural language toolkit (nltk) [34].\nThe nltk module is a powerful module to process text corpora developed by Steven\nBird and Edward Loper. When using only the nltk lists there were words remaining\nthat could be considered stop words so these stop lists have been extended with\nadditional, external German4and English5stop lists.\nAnother issue is the amount of different words used in the data sets. The topic\nmodel will need a vocabulary that matches every unique word to a number. So the\nvocabulary and the according test and training corpora will be extremely large and\nthe subsequent computation is very slow. Therefore, we stem all words in the data\nset.\nStemming reduces words to a base form [16]. The base form does not necessary\nmatch the linguistic root of a word. For example, a stemmer can match the words\n\u201cfamilies\u201d and \u201cfamily\u201d to the base form \u201cfamili\u201d. An aggressive stemmer e.g. the\nLancaster stemmer [22] can sometimes reduce the words too much such that they\nare not easily interpretable for humans. In this thesis nltk\u2019s implementation of the\nSnowball stemmer developed by Martin Porter [23] is used. It can be applied on\nboth German and English corpora and keeps word stems interpretable.\n4.4 Data Set Description\nThis section will describe the \ufb01nal corpora of the Wikipedia data set, the UK data set\nand the US data set. This data is used during the experiment to investigate the four\nlead questions (presented in section 1.1). The pairs in all corpora have been \ufb01ltered\nsuch that the partner articles are comparable. Stopwords have been removed and all\nwords have been stemmed. A brief overview of the datasets can be found in Table 3.\n4Additional German stop word list: https://github.com/solariz/german_stopwords/\nblob/master/german_stopwords_full.txt\n5Additional English stop word list: http://xpo6.com/list-of-english-stop-words/\n28\nTable 3: Overview of data sets used in the experiments. Documents of each group\nwere paired with documents of the other group on the same subject.\nName #Pairs Description Groups\nWikipedia 385,306 Wikipedia articles German and English\nUK 18,170 UK news articles Political orientation (left/right)\nUS 4,721 US news articles Political orientation (left/right)\n4.4.1 Wikipedia Data Set\nAfter building article pairs the original corpus contained 1.036.398 million pairs.\nA lot of them did not meet the established length requirements such that the \ufb01nal\ncorpus only contains 385.306 pairs of English and German Wikipedia articles. The\nEnglish documents have a mean length of 522 words and the German documents\nhave a mean length of 476 words. The cumulative document length distribution is\ndepicted in Figure 10. Both length distributions are similar. Hence, there was no\nneed to balance the document lengths across groups.\n101102103104105\nDocument Length102103104105106Cumulative Amount\nde\nen\nFigure 10: Article length distribution of English and German Wikipedia after\nlength reduction. The red graph describes English articles (en) and the blue graph\ndescribes German articles (de). The green line shows the minimum length of 50\nwords we required for all articles. A lot for short articles up until 1000 words for\nboth groups. Longer articles are rare. Both article length distributions are very sim-\nilar.\nLooking at the pairs in a corpus, an English document is longer than his German\ncounterpart in 56.1% of the cases. The German article is longer in 43.4% of the pairs.\n29\nOnly in 0.4% of all cases the German and English article are of equal size. In the\naverage pair, the English article is 6% longer than the German partner.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportionen exclusive shared de exclusive\nFigure 11: Distribution of unique and shared words in the Wikipedia corpus. The\nred bar shows words that appear only in the English group, the blue bar shows the\nwords that only appear in the German group. The purple bar shows the words that\nappear in both groups. The German exclusive vocabulary is signi\ufb01cantly larger than\nthan the English exclusive vocabulary. Indicating that German documents will be\nharder to predict for a topic model.\nThe stemmed and stop word removed Wikipedia data set contains 2.725.922 unique\nwords. Their distribution in German and English articles is shown in Figure 11.\n1.173.116 of these words are used in the English articles. 2.128.509 of these words\nare used in the German articles. Both groups share 575.703 of all words, which is\nabout 21% of the whole corpus. The vocabulary of both languages is clearly sepa-\nrated. On average a pair shared around 40 words.\nThe German vocabulary is signi\ufb01cantly larger than the English vocabulary. An\nexplanation for this are for example compound words, which are frequent in the\nGerman language and cannot be reduced to the base form by a stemmer. It is rea-\nsonable to expect that the increased vocabulary makes it harder for a topic model to\npredict the German documents during the experiment.\nFigure 12 describes the word frequencies of English and German words. The\nwords have been ranked based on their frequency beforehand. Both graphs resem-\nble a power law function. This phenomenon is typical for natural-language docu-\nments and known as Zipf\u2019s law [17]. The highly ranked English words are more\nfrequent than the highly ranked German words. With increasing rank the word fre-\nquency decreases faster for the English documents. Approximately, beyond rank\n10.000 the rare German words are more frequent than the English words.\nThus, German documents contain more unique words in general and the rare\nwords show a higher frequency than the English rare words. The increased fre-\nquency of rare words can have an in\ufb02uence on the created topics, as the topic model\nneeds to group more words. It is expected that these additional challenges make\nGerman articles harder to predict for a topic model.\n30\n100101102103104105106107\nRank100101102103104105106Frequencyen Words\nde WordsFigure 12: Word frequency in English and German Wikipedia articles sorted by\nword rank. The red graph represents the English words. The blue graph represents\nthe German words. English high rank words are more frequent than the German\nhigh rank words. Words ranked beyond rank 10.000 are more frequent in German.\n4.4.2 Event Registry UK Data Set\nThe UK data set contains 18.170 pairs divided in articles of left and right political\norientation. The left and right articles were build from different combinations of\nleft and right sources. The mean document length for both political orientations are\nsimilar. The documents of the left sources have an average length of 476 words,\nwhile the documents of the right sources have an average length of 480 words.\nFigure 13 shows the distribution of sources over all article pairs. The most used\nleft source was \u201cThe Mirror\u201d. The most common right source was \u201cThe Daily Mail\u201d.\nBoth make up the most frequent source pair of all article pairs. Combinations of less\ncommon sources are very rare in general e.g. \u201cThe Telegraph\u201d and \u201cThe Indepen-\ndent\u201d. The frequency difference will not matter for the experiment because the as-\nsumption is that all articles inside of the left or right source list are interchangeable.\nThe cumulative length distribution is depicted in Figure 14. The majority of docu-\nments has a length below 1000 words. Documents longer than 1000 words are rare.\nThe right articles show an irregularity at about 125 to 150 words in a document.\nThese documents are surprisingly frequent. It is probable that this is caused by a\nlength threshold of one or more of the news sources. As it did not interfere with the\nexperiment, it was not investigated further.\nIn 51.7% of all pairs a right article is longer than a left one and in 48.1% the left\narticle is longer. Only in 0.2% of all cases, both articles are of equal size. In the\naverage pair, the right article is longer by about 2%.\n31\nFigure 13: Source distribution of left and right UK articles. Red shows a high fre-\nquency of a pair. Blue indicates low frequency. \u201cThe Daily Mail\u201d and \u201cThe Mirror\u201d\nare the most common pair.\n32\n101102103104\nDocument Length100101102103104105Cumulative Amount\nright\nleftFigure 14: Article length distribution of left and right UK articles. The red graph\nshows the left oriented articles while the blue graph shows the right oriented ar-\nticles. The green line represents the minimum length of 50 words imposed on all\narticles. Right articles show a sudden rise in documents of length 125 to 150 words,\nprobably caused by length threshold of a news source.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportionleft exclusive shared right exclusive\nFigure 15: Unique and Shared words in the UK corpus. The red bar shows words\nthat appear only in the left group, the blue bar shows the words that only appear in\nthe right group. The purple bar shows the words that appear in both groups. Com-\npared to Wikipedia data set, signi\ufb01cantly bigger proportion of shared words. Right\ndocuments show a slightly larger exclusive vocabulary than the left documents.\n33\nFigure 15 shows the vocabulary distribution of the stemmed UK corpus. In con-\ntrast to the Wikipedia data set the Event Registry articles share the same language.\nTherefore, the articles share a lot more words and the difference in word diversity\nis not as large as in the Wikipedia data set. There were 79,813 unique words used\nin the stemmed UK data set. The left articles used 59,715 of these words while the\nright articles used 63,145 unique words. 43,047 words appear in both vocabularies.\nOn average a pair shared around 117 words which almost triples the amount shared\nin an average Wikipedia pair.\n100101102103104105\nRank100101102103104105Frequencyleft Words\nright Words\nFigure 16: Word frequency of left and right articles from the UK. The red graph\nshows the left words, the blue graph represents the right words. The word fre-\nquency per rank is very similar as both groups use the same language.\nFigure 16 shows the word frequency relative to the words frequency rank. In con-\ntrast to Figure 12 the word frequency distribution is almost identical for both groups\nas they share the same language. The word frequency per rank of both groups are\nvery close together and almost overlap. There are no blatant inconsistencies be-\ntween both distributions.\n4.4.3 Event Registry US Data Set\nThe US corpus is the smallest of all three corpora. Even though there is a higher\namount of different sources during the extraction, the corpus contains the least\namount of pairs. The US corpus contains 4,721 pairs and consists of articles with left\nand right political orientation. A left article contains on average 514 words while a\nright article has on average a length of 468 words.\nThe source distribution across all pairs have been depicted in Figure 17. The most\nfrequent left source was the \u201cWashington Post\u201d. The most frequent right source was\n34\nFigure 17: Source distribution of left and right US articles. Red shows a high\nfrequency of a pair. Blue indicates a low frequency. \u201cWashington Post\u201d and \u201cFox\nNews\u201d are the most common pair.\n35\n\u201cFox News\u201d closely followed by the \u201cNew York Post\u201d. It is interesting that \u201cNew\nYork Post\u201d gets paired a lot more frequently with the \u201cNew York Times\u201d than the\nmost frequent source \u201cFox News\u201d. This can probably be explained with the locality\nof both newspapers.\n101102103104\nDocument Length100101102103104Cumulative Amount\nright\nleft\nFigure 18: Article length distribution of left and right articles from the US. The\nred graph describes the left articles. The blue graph describes the right articles. The\ngreen line shows the minimum article length of 50 words. This time, left articles\nshow a sudden rise in documents of length 125 to 150 words, probably caused by\nlength threshold of a news source.\nThe cumulative article length distribution is displayed in Figure 18. Similar to the\nUK corpus, the distributions are similar and do not show a lot of difference across\ngroups. This time the left documents show a quick rise in documents at the 125 to\n150 word mark. Again, this might be explained by a policy of one or multiple news\nsources and was not investigated further.\nWhen comparing the articles in a pair, a left article is longer in 58.5% of all cases\nwhile a right article is longer in only 41.2%. Compared to the UK data, where the\nright articles were longer, the situation has turned and is more pronounced. In the\naverage pair, the left article is longer by about 10%.\nFigure 15 shows the vocabulary distribution of the stemmed US corpus. Over-\nall there are 41,321 unique words in the articles of the US data set. This amount\ncannot be compared to the UK data, because of the smaller sample size. The left\narticles used 33.177 different words while the right articles used 30,655 different\nwords. Both groups share 22,511 of their vocabulary. This time the left articles show\na larger vocabulary over the right articles. Similar to the UK pairs, the articles of a\npair have on average 112 words in common.\n36\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportionleft exclusive shared right exclusiveFigure 19: Unique and Shared words in the US corpus. The red bar shows words\nthat appear only in the left group, the blue bar shows the words that only appear in\nthe right group. The purple bar shows the words that appear in both groups. Similar\nto the UK corpus, most words are shared across groups. The amount of exclusive\nwords in the left group is higher than the amount of exclusive words in the right\ngroup.\n100101102103104105\nRank100101102103104105Frequencyleft Words\nright Words\nFigure 20: Word frequency of the left and right US articles. The red graph shows\nthe left words, the blue graph represents the right words. Equal to the results of the\nUK corpus, the word frequency per rank is very similar as both groups use the same\nlanguage.\n37\nThe word frequency distribution is displayed in Figure 20. The observations\nmatch the observations of the UK word frequency plot. As both groups use the\nsame language the word frequency distribution is very similar and almost overlaps.\nThere are no obvious inconsistencies.\n5 Experimental Results\nThis section presents the results of the four different experimental settings. The\nsettings were explained in section 3.5. They give answers to the four lead questions\nposed in section 1.1.\nDuring the experiments multiple LDA models were built with three different\ntopic parameter settings \u2013 64, 128 and 256 topics \u2013 to ensure reproducibility of re-\nsults. The resulting models represent a small scale, medium scale and large scale\nmodel trained on these training corpora. Thus, each experiment produces three\ndifferent plots. The perplexity range varies depending on amount of topics LDA\nlearns due to gensim\u2019s perplexity implementation. They are not directly compara-\nble. Hence, each model is evaluated in its personal scope of values.\nBefore examining the actual results, this section will give insights into the mean-\ningfulness of the test results. To evaluate if the trained topic models predict test\ndocuments based on a suf\ufb01cient amount of words, the vocabulary mismatch is cal-\nculated. It shows if a test corpus contains a high share of words that are unknown\nto the topic model. The prediction of words in the test phase is based on words the\nmodel saw during the training phase. Unseen words would have a very low prob-\nability, but are excluded in the experiments. Therefore, if a model only knows 10%\nof the words of test documents but it can assign them well, the prediction quality in\nthe experiment is good but the result is rather \ufb02awed.\n5.1 Vocabulary Mismatch\nTo ensure that the performance measure perplexity \u2013 which only uses known words\nfrom the training corpus \u2013 is meaningful, the fraction of words in the test corpus\nthat have not been captured in the training corpus are examined for each group.\nThis fraction will be called vocabulary mismatch . A high vocabulary mismatch indi-\ncates that a large proportion of words used in the test documents is unknown to the\nmodel. The topics have been assigned based on a few known words, while discard-\ning most of the actual content. This is mainly important for the Wikipedia Corpus\nas it contains two different languages.\nFigure 21 shows the vocabulary mismatch relative to the proportion of English\ndocuments in the training corpus. English documents have on average a lower vo-\ncabulary mismatch in their documents than German documents. Even if only 10%\nEnglish documents are contained in the training corpus the test corpus only has a\nmismatch of 25%. At this point the German proportion of documents is at its high-\nest point with 90%. Nevertheless, the German test documents already show higher\n38\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nPercentage of \u2019en\u2019 documents in the training corpus0.00.20.40.60.81.0Proportion of Vocabulary Missmatch in a Test Documenten mean en std de mean de stdFigure 21: Average vocabulary mismatch in Wikipedia samples. The red area\nrefers to the documents of the English test group and the blue are refers to the\ndocuments of the German test group. The continuous, middle line describes the\nmean proportion of vocabulary mismatch. The area between the two dashed line\ndescribes the area within the sample standard deviation of the mean. The vocab-\nulary mismatch of a group decreases when adding documents of this group to the\ntraining corpus. With only 10% German documents in the training corpus the vo-\ncabulary mismatch almost reaches 50% and should not be increased further.\nmean vocabulary mismatch. The mean vocabulary mismatch is at 26%.\nWhen increasing the fraction of English documents in the training corpus the\nvocabulary mismatch of the English test documents decreases slowly. With 90%\nEnglish test documents the average vocabulary mismatch of the English test doc-\numents decreases to 15%. The average vocabulary mismatch of the German docu-\nments increases to 43,0% and is at its maximum.\nThe German test documents contain many words that are unknown to the model.\nOne can assume that German is a more complex language than English as it con-\ntains several different grammatical cases and compound words [32]. Figure 11 in\nsection 4.4.1 showed that the German vocabulary is signi\ufb01cantly larger than the En-\nglish vocabulary while describing the same overall concepts. Therefore, the chance\nof not seeing a word in the training corpus is higher.\nThe amount of known words in a document is still suf\ufb01cient to evaluate the per-\nformance of the topic model though, because at least half of these words are known.\n39\nNevertheless, it is not recommended to increase the size gap between both groups\nany further as the mismatch will only rise and weaken the prediction results. It\nis worth to note that a real corpus often contains more than two languages or a\nvery complex language such that the vocabulary mismatch will probably rise sig-\nni\ufb01cantly in such a corpora.\nThe plots of vocabulary mismatch for the Event Registry corpora have been omit-\nted. The vocabulary mismatch of both groups shows a stable mean below 10% as all\ndocuments share the same language. The mismatch is signi\ufb01cantly lower than the\nvocabulary mismatch of the Wikipedia samples, such that they are well suited for\ninference and likelihood calculations.\n5.2 Overall Perplexity\nThis section answers the research question (i) if the presence of groups in the train-\ning corpus of a topic model in\ufb02uences its prediction performance . It is unknown\nif a topic model\u2019s prediction performance on a test corpus is in\ufb02uenced by latent\ngroups. There are two realistic scenarios for the effect of groups on the perplexity in\nthe test corpus:\n\u000fThe perplexity could rise: The model could mix topics of both groups together,\nlearning topics which never occur within a single document. It also could\nlearn topics for the majority group, and neglect the minority which is most\nimportant for a good model \ufb01t. In this case it would be necessary to split the\ncorpus based on these groups to achieve better results.\n\u000fThe perplexity could not change at all, if the topic model would be unaffected\nby groups \u2013 group-speci\ufb01c topics could be detected proportionally to the rela-\ntive share of the group. This would indicate that a topic model can be trained\non imbalanced corpora without special treatment of the groups, without af-\nfecting the topic quality.\nThis section evaluates the perplexity over the complete test corpus relative to each\nrelative group size. The complete test corpus contains held-out words from all test\ndocuments that belong to the training documents used to train the topic model at\nthe given relative group size.\n5.2.1 Wikipedia\nFigure 22 describes the median perplexity development over all test documents of\nthe Wikipedia samples relative to the fraction of English documents in the training\ncorpus. For all tested models, the perplexity is higher when trained on a lot of\nGerman documents and decreases monotonically when more English documents\nare added. At 90% English documents the model can predict the test documents\nbest.\n40\n280300320340360380400420440460Perplexity64 TopicsComplete Corpus 25/75th Percentile Complete Corpus Median\n350400450500550600Perplexity128 Topics\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nPercentage of \u2019en\u2019 documents in the training corpus10001500200025003000Perplexity256 TopicsFigure 22: Perplexity over all test documents of the Wikipedia samples relative to\nthe relative English group size. The continuous black lines describes the median\nperplexity of the test corpus. The dashed black lines describe the 25th and 75th per-\ncentiles. Perplexity is at its peak with 90% German documents and monotonically\ndecreases when adding English documents. The perplexity adapts to the perplexity\nof the 100% corpus of each group.\n41\nThis underlines the assumption made in section 5.1 and section 4.4.1 that Ger-\nman is a more complex language than English. Its vocabulary contains more unique\nwords overall. Consequently, the model struggles more when predicting the Ger-\nman test corpus.\nThe experiment on the Wikipedia corpus shows that the presence of latent groups\nin training corpora does in\ufb02uence the prediction performance of a standard topic\nmodel. The overall prediction performance depends on how well each group can\nbe predicted and how big its share is . A corpus with a majority of German docu-\nments is harder to predict than a corpus with a majority of English documents. The\nmixture proportion of both groups controls the overall perplexity.\n5.2.2 Event Registry UK\nFigure 23 describes the perplexity development over all test documents of the UK\ncorpus relative to the fraction of left documents in the training corpus for LDA mod-\nels. All graphs of LDA trained on different topic sizes show different results. The\nmean perplexity of LDA with 64 and 128 topics barely changes when adding or\nremoving left documents. The mean perplexity of LDA with 256 topics changes sig-\nni\ufb01cantly more. But when taking into account the overall scope and inter-percentile\nrange of perplexity values of this model, the values stay rather stable as well.\nThe results of the experiment on the UK corpus do not show an in\ufb02uence of\ngroups on the overall prediction performance . All graphs move rather unpre-\ndictably . The only common feature is that a 90% right corpus is slightly worse to\npredict than a 90% left corpus. But the effect is rather insigni\ufb01cant as the perplexity\nbetween those extremes varies too much. Hence, it is safe to assume that the predic-\ntion performance of these samples depends on other in\ufb02uences than the presence\nof groups.\n5.2.3 Event Registry US\nFigure 24 describes the perplexity development over all test documents of the UK\ncorpus relative to the fraction of left documents in the training corpus. LDA models\nthat learned 64 or 128 show a declining trend when increasing the amount of left\ndocuments. This trend cannot be found in LDA when learning 256 topics. The\nmean perplexity in LDA with 256 topics stays rather stable at a very high perplexity\nvalue but can deviate a lot. This effect might be caused by the topic model learning\ntoo many topics on a rather small sample (4000 docs), such that it cannot reasonably\nassign the words to topics. Hence, the results of LDA with 256 topics are disregarded\nfor now.\nThe results of the experiment on the US corpus show a minor in\ufb02uence of groups\non the overall prediction performance . Figure 19 in section 4.4.3 shows a small\noverhead in vocabulary size for the right articles. So, similar to the declining trend\nin the Wikipedia samples, the declining trend in overall perplexity can be explained\nwith the vocabulary size of the group . The right articles use more unique words in\n42\n444446448450452454456Perplexity64 TopicsComplete Corpus 25/75th Percentile Complete Corpus Median\n708710712714716718720722724726Perplexity128 Topics\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nPercentage of \u2019left\u2019 documents in the training corpus3650370037503800385039003950400040504100Perplexity256 TopicsFigure 23: Perplexity over all test documents of the UK samples relative to the left\ngroup size proportion. The continuous black lines describes the median perplex-\nity of the test corpus. The dashed black lines describe the 25th and 75th percentiles.\nPerplexity does only vary in a small perplexity window \u2013 besides 256 topics, the per-\nplexity varies more and the inter-percentile distance is larger. There are no common\npatterns between all three topic sizes.\n43\n1200121012201230124012501260Perplexity64 TopicsComplete Corpus 25/75th Percentile Complete Corpus Median\n40504100415042004250430043504400Perplexity128 Topics\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nPercentage of \u2019left\u2019 documents in the training corpus375003800038500390003950040000405004100041500Perplexity256 TopicsFigure 24: Perplexity over all test documents of the US samples relative to the left\ngroup size proportion. The continuous black lines describes the median perplexity\nof the test corpus. The dashed black lines describe the 25th and 75th percentiles.\nTrend of decreasing perplexity when increasing the share of left documents when\ntraining on 64 and 128 topics. Not as pronounced as in the Wikipedia samples.\nResults of 256 topics rather stable but with a larger inter-percentile range.\n44\ntheir documents, therefore they are harder to predict for the topic model and the\noverall perplexity is higher.\n5.3 Group-Speci\ufb01c Perplexity\nThis section will answer (ii) if the group-wise prediction performance changes\nwhen the relative group size relation in the training corpus of a topic model is\nvaried.\nIt is reasonable to expect that the prediction performance of a group suffers when\nthe group is pushed further into a minority position, because the minority group\u2019s\nshare in the training corpus is decreasing. There might even be a point of imbalance\nat which a topic model can not cover the minority group properly anymore, because\nthe group\u2019s share of documents in the training corpus is too small. It is unknown\nwhen and if this point occurs.\nTo test this question, this section evaluates the perplexity over each group\u2019s test\ncorpus relative to the relative group size.\n5.3.1 Wikipedia\nFigure 25 shows the perplexity development over each groups\u2019 test documents in\nthe Wikipedia corpus relative to the fraction of English documents in the training\ncorpus. In almost all cases, English documents can be predicted better than Ger-\nman documents. This gap can be reasoned with each languages complexity. Even\nthough English documents are on average longer than German documents, the Ger-\nman documents use more unique words (see section 4.4.1). Therefore, a suf\ufb01cient\namount of German documents is necessary to build topics that can predict the test\ndocuments well. English corpora can build topics with better prediction perfor-\nmance with a lower share of documents because English documents use less unique\nwords.\nWhen increasing the amount of English documents in the training corpus, the\nperplexity on English test documents decreases monotonically. At the same time the\nperplexity of German documents increases monotonically. The English documents\nshow an increase in perplexity when only 20% or less English documents are in the\ntraining corpus, the German documents show a similar increase when only 40% or\nless are German documents in the training corpus. The increase in perplexity can\nbe explained by the fading amount of the minority language\u2019s words in the sample\nvocabulary.\nThe results when training on 256 topics differ from the other results. It is the only\nresult within the test range where the English documents can be predicted worse\nthan the German documents at one point. A possible explanation of why this case\noccurs within the test range only when training 256 topics is that the other models\ncould not keep track of enough topics to increase the prediction performance of\nGerman documents to the level of English documents. When more topics can be\ntrained, the more complex German documents can be covered more appropriately.\n45\n250300350400450500550Perplexity64 Topics\u2019en\u2019 Corpus 25/75th Percentile\n\u2019en\u2019 Corpus Median\nComplete Corpus Median\u2019de\u2019 Corpus 25/75th Percentile\n\u2019de\u2019 Corpus Median\n300400500600700800Perplexity128 Topics\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nPercentage of \u2019en\u2019 documents in the training corpus05000100001500020000Perplexity256 TopicsFigure 25: Perplexity over test documents of the Wikipedia samples per group.\nThe red line shows the median perplexity of the English test corpus, the blue line\nshows the median perplexity of the German test corpus. The black line shows the\nperplexity of the complete test corpus (from Figure 22). The dashed line shows the\n25th/75th percentile of perplexity of each test group. In general, German documents\nare harder to predict than English documents. English documents show a signi\ufb01cant\nperplexity increase when they provide 20% or less of the training corpus. German\ndocument show a perplexity increase when providing less than 40% of the training\ncorpus. The rate of growth increases with decreasing share of documents.\n46\nThis case occurs when only 10% of the test corpus consist of English documents. It\nis possible this case occurs in a range below 10% for models trained on less topics \u2013\nas long as the vocabulary mismatch does not prevent such a prediction. These cases\nhave not been investigated during this thesis.\nIn general, all results show that the prediction performance is not solely depen-\ndant on a minority position of a group. It is true that the prediction performance\nworsens when pushed further into a minority position, but the graphs demonstrate\nthat the English minority group frequently shows a better prediction performance\nthan the German majority. The amount of topics learned in\ufb02uences the relative pre-\ndiction performance between both groups.\nThere have to be additional factors that in\ufb02uence the predictability besides the\nrelative group size. As already assumed in section 5.2 the complexity of a language\nis a reasonable explanation. German is a more complex language [32], such that\nthe topic model cannot predict it as well as the English documents even if German\ndocuments are the majority during training.\n5.3.2 Event Registry UK\nFigure 26 shows the perplexity development over each groups\u2019 test documents in\nthe UK corpus relative to the fraction of left documents in the training corpus. The\nperplexity on right test documents shows a decreasing trend when increasing the\namount of left documents in the training corpus when LDA trained 64 or 128 topics.\nAt the same time the perplexity of right documents show an increasing trend. The\nperplexity decreases faster for left documents than the perplexity of right documents\nrises.\nWhen LDA is trained on 256 topics, both perplexity values are almost equal. The\ngraphs are barely interpretable. It shows an irregularity which is not occurring in\nLDA trained with 64 or 128 topics. It is reasonable to assume that the increased\namount of topics is suf\ufb01cient to train topics that cover both groups, such that the\nperplexity equalises.\nWhen LDA is trained on 64 topics both groups almost have the same perplexity\nvalue when the training corpus contains 60% left documents. When trained on 128\ntopics this point moves towards 90% left documents in the training corpus. Before\nthis point, right documents show a better prediction performance, beyond this point\nleft documents show a better prediction performance. Similar to the Wikipedia cor-\npus, the break-even point of perplexity is in\ufb02uenced by the amount of topics.\nThe two graphs of LDA with 64 and 128 topics show that, in a same language cor-\npus, the prediction performance of each group suffers if forced into a minority role.\nWhen there are are enough topics to learn e.g. 256 topics, this effect slowly nulli\ufb01es.\nIt is worth to note that the test sets of both groups still show different perplexity\nvalues when trained on lower amounts of topics. This behaviour is surprising, as it\nexhibits language differences beyond language or covered topics (which were con-\ntrolled for).\n47\n440445450455460465470Perplexity64 Topics\u2019left\u2019 Corpus 25/75th Percentile\n\u2019left\u2019 Corpus Median\nComplete Corpus Median\u2019right\u2019 Corpus 25/75th Percentile\n\u2019right\u2019 Corpus Median\n700710720730740750760Perplexity128 Topics\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nPercentage of \u2019left\u2019 documents in the training corpus36003700380039004000410042004300Perplexity256 TopicsFigure 26: Perplexity over test documents of the UK samples per group. The red\nline shows the median perplexity of the left test corpus and a blue line shows the\nmedian perplexity of the right test corpus. The dashed line shows the 25th/75th\npercentile of perplexity of each groups test corpus. The black line shows the per-\nplexity of the complete test corpus (from Figure 23). Models trained on 64 and 128\ntopics show similar results. Perplexity on left documents decreases when adding\nleft documents to the training corpus. Perplexity on right documents increases, but\nat a slower rate. Models trained on 256 topics show a similar perplexity for all doc-\numents without showing common patterns with the other two models.\n48\n5.3.3 Event Registry US\nFigure 27 shows the perplexity development of each groups\u2019 test documents in the\nUS corpus relative to the fraction of left documents in the training corpus. The\nresults differ from the results of the UK and Wikipedia corpus.\nBoth groups\u2019 perplexity increases monotonically with increasing amount of left\ndocuments in the training corpus. The rate of growth is almost equal for both\ngroups. It is worth to note that this keeps the perplexity of the complete corpus\nat a stable level.\nRight articles always show a larger perplexity than left articles. The reason be-\nhind this difference is currently unexplained. Different to the Wikipedia corpus, left\narticles \u2013 which are easier to predict \u2013 show a larger vocabulary. It is possible that\ncertain words in left articles are used together more consistently such that the left-\noriented topics are better at predicting new articles. The offset of both groups shows\nthough, that both groups have internal properties that in\ufb02uence the prediction per-\nformance differently.\nThe fact that both groups perplexity increases monotonically and at the same rate\nwhen increasing the amount of left documents in the training corpus indicates that\nthese samples are not in\ufb02uenced by minority-majority relations at all. If a topic\nmodel treats both groups fairly, then there would be two lines parallel to the x-axis,\nbecause for each relative group size relation the perplexity will stay stable. These\ngraphs are parallel, but are rising with increasing left document share.\nThis means that there is an additional hidden parameter that changes when adding\nleft documents. A reasonable parameter \u2013 given the corpus statistics in section 4.4.3\n\u2013 is the average document length. It shows a signi\ufb01cant difference between 514\nwords in left articles and 468 words in the right articles. So, when adding left ar-\nticles the average document length in the sample rises and the documents of both\ngroups in the sample are harder to predict for LDA.\nThe perplexity of the complete test corpus stays stable throughout this process\nbecause the right test documents at 90% right documents are predicted almost as\ngood as the left test documents at 90% left documents. The change relative group\nsize relation then counter-acts the overall increase in perplexity such that it stays at\nan overall stable level.\n5.4 Topic Assignment per Group\nThis section gives the answer to the question (iii) if a topic model can distinguish\nbetween two latent groups in the training corpus . It is unclear if a topic model\ntrained on a imbalanced corpus builds topics that are used especially for one group\nor if the topics are build for both groups.\nThis section evaluates the proportion of topics in a topic model that can be as-\nsigned to one of the groups relative to each relative group size. A topic Tis assigned\nto a groupGif the formula P(GjT)>90% holds.\n49\n1050110011501200125013001350140014501500Perplexity64 Topics\u2019left\u2019 Corpus 25/75th Percentile\n\u2019left\u2019 Corpus Median\nComplete Corpus Median\u2019right\u2019 Corpus 25/75th Percentile\n\u2019right\u2019 Corpus Median\n3000350040004500500055006000Perplexity128 Topics\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nPercentage of \u2019left\u2019 documents in the training corpus200003000040000500006000070000Perplexity256 TopicsFigure 27: Perplexity over test documents of the US samples per group. The red\nline shows the median perplexity of the left test corpus and a blue line shows the\nmedian perplexity of the right test corpus. The black line shows the perplexity of\nthe complete test corpus (from Figure 24) The dashed line shows the 25th/75th per-\ncentile of perplexity of each groups test corpus. Both group\u2019s perplexity monotoni-\ncally increases with the share of left documents in the training corpus. Both change\nalmost analogously, keeping the median perplexity of the whole test corpus stable.\n50\nIf a topic can be assigned to a group unambiguously, this section also answers (iv)\nif the proportion of group-speci\ufb01c topics is under-proportional to the proportion\nof topics in the training corpus .\nIf a topic model builds separate topics for each group, it is unknown if a topic\nmodel builds e.g. 70% of its topics for a group, if this group provides 70% of the\ntraining corpus. It is expected that the amount of topics built is under-proportional,\nas the assignment rule is set rather strictly, but the extent of under-proportionality\nis unclear.\n5.4.1 Wikipedia\nFigure 28 shows the topic assignment for LDA trained on the Wikipedia corpus. As\nall graphs are similar, this section focuses on the data of LDA trained on 128 topics.\nLDA trained on Wikipedia samples can distinguish both groups .\nAt 10% English documents in the training corpus there are about 83% German\ntopics and below 3% English topics. The case is reversed when there are 90% En-\nglish documents in the training corpus, there are 80% English topics and 3% German\ntopics. As the corpus gets more balanced the amount of topics that can not be as-\nsigned to one group rises quickly.\nAt the peak, 51% of all topics cannot be assigned unambiguously. That means in\nthe balanced training corpus at 50% English documents only about 20% to 25% of\ntopics can be assigned to a group. It is surprising that in the balanced corpus less\nthan 50% of the topics can be assigned to one of the groups even though it contains\ntwo separate languages.\nThis refutes the claim of Boyd-Graber et al. [5] \u2013 mentioned in section 2.5.2 \u2013 which\nstated that LDA, when trained on poly-lingual corpora, learns topics that belong to\na single language only. This thesis is the \ufb01rst to show, that the learned topics are\nused by both corpora. Even though the most frequent terms appear to share the\nsame language, these topics cannot be assigned to one language unambiguously\nas both corpora are using these topics .\nNevertheless, the usage of shared topics is usually not desired in a topic model.\nThus it is advised to not use LDA on poly-lingual corpora and to either split the\ncorpus by language to achieve clean topics or to use a poly-lingual model [5, 18]\nTo further analyse the connection between the proportion of group assigned top-\nics and the proportion of a group in the training corpus, their relation is depicted in\nFigure 29. It shows that minority groups are always signi\ufb01cantly under-represented :\nThe share of group-speci\ufb01c topics grows relative to the share of documents of a\ngroup in the corpus. As seen in Figure 29, the perplexity of small minorities of\n10% or 20% of the corpus is signi\ufb01cantly worse than the perplexity of the major-\nity group . For minorities of 30% and larger, this effect is considerably smaller.\n51\n0.00.20.40.60.81.0Proportion64 Topicsen unassigned de\n0.00.20.40.60.81.0Proportion128 Topics\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nPercentage of \u2019en\u2019 documents in the training corpus0.00.20.40.60.81.0Proportion256 TopicsFigure 28: Topic Assignment per group in the Wikipedia corpus. Red bars show\nthe mean proportion of topics assigned to the English group while the blue bars\nshow the mean proportion of topics assigned to the German group. Assigned topics\nwill be called English and German topics. Purple bars show the mean proportion\nof topics that cannot be assigned to either group. LDA can differentiate between\nboth groups. The amount of assigned topics per group depends on the share of this\ngroup in the training corpus. Close to the balanced state, the model creates a lot of\nshared topics that cannot be assigned unambiguously.\n52\n0.00.20.40.60.81.0Topic/Corpus Ratio64 Topicsen de\n0.00.20.40.60.81.0Topic/Corpus Ratio128 Topics\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nPercentage of group\u2019s documents in the trainings corpus0.00.20.40.60.81.0Topic/Corpus Ratio256 TopicsFigure 29: Ratio of the share of group-assigned topics to the share of group\ndocuments in the training corpus. Smaller minorities get represented in under-\nproportionally many topics. They are under-represented.\n53\n0.00.20.40.60.81.0Proportion64 Topicsleft\nunassigned\nright\n0.00.20.40.60.81.0Proportion128 Topics\n0.10.20.30.40.50.60.70.80.9\nPerc. of \u2019left\u2019 docs in training corpus0.00.20.40.60.81.0Proportion256 Topics(a)UK corpus\n0.00.20.40.60.81.0Proportion64 Topicsleft\nunassigned\nright\n0.00.20.40.60.81.0Proportion128 Topics\n0.10.20.30.40.50.60.70.80.9\nPerc. of \u2019left\u2019 docs in training corpus0.00.20.40.60.81.0Proportion256 Topics (b)US corpus\nFigure 30: Topic Assignment per group in the Event Registry corpora. Red bars\nshow the mean proportion of topics assigned to the left group while the blue bars\nshow the mean proportion of topics assigned to the right group. Assigned topics\nwill be called left and right topics. Purple bars show the mean proportion of topics\nthat cannot be assigned to either group. In both data sets almost all topics are shared.\nThe documents that could be assigned to a group can only be assigned due choice\nof the assignment threshold.\n54\n5.4.2 Event Registry\nFigure 30 shows the topic assignments to groups for LDA trained on the US and\nUK corpus. Since both corpora contain groups that do not differ by language but by\npolitical orientation, LDA generates almost no group-speci\ufb01c topics. In the range of\n20% to 80% of English documents in the training corpus almost no groups can be\nassigned to a group. That means, the topic model uses these topics in both groups.\nOnly the corner cases of 10% and 90% English documents show a clear assign-\nment. This assignment happens due to the interaction between the threshold and\nthe construction of the corpus though. The threshold to assign a topic is set to 90%.\nTherefore, if the training corpus contains 90% documents of a group, it will auto-\nmatically assign about 50% of the topics to that group by chance.\nIt appears that the differences between both groups are too small to clearly assign\nthem to a group. As decent group assignments are not possible , an investigation\nof the relation between share of topic group assignment and share of group in the\ntraining corpus is impossible.\n6 Conclusion\nThis thesis investigated the in\ufb02uence of an imbalanced training corpus on LDA, the\nmost-popular topic model. The main \ufb01ndings of this thesis are:\n(i)The presence of groups in training corpora can in\ufb02uence the prediction per-\nformance of topics models as measured by perplexity due to various factors,\nincluding increased group-speci\ufb01c perplexity scores.\n(ii)The prediction performance of topic models changes for all groups when vary-\ning the relative group sizes.\n(iii) Basic topic models are able to distinguish between different latent groups in\ndocument corpora to a certain extent if differences between groups are large\nenough, e.g. for groups with different languages.\n(iv) The proportion of group-speci\ufb01c topics is under-proportional to the share of\nthe group in the corpus and relatively smaller for minorities.\nTo achieve these \ufb01ndings, three different data sets were built: a Wikipedia data\nset, a data set with UK articles and a data set with US articles. Using these data\nsets, imbalanced training corpora were sampled. The imbalanced training corpora\ncontained documents of two distinct groups. The relative group size relation was\nmanipulated. In order to remove the in\ufb02uence of the semantics of sampled docu-\nments when varying the relative group size, the covered concepts in each sample\nwere controlled for. The training corpora were used to train a LDA model. Using\nheld-out test data of each group the topic models were evaluated regarding their\nprediction performance.\n55\nBoth the UK and the US data sets were constructed with English speaking articles.\nThe latent groups in this corpus differed by political orientation. The vocabulary\ndifference across these groups is rather small. LDA (iii), (iv) did not build topics\nfor groups speci\ufb01cally as the difference was too small.\nWhen changing the relative size relation between these groups (i) they did not\nshow any clear results regarding the prediction performance of the topic model in\ngeneral. It stayed rather stable.\nWhen investigating the perplexity of the group speci\ufb01c test corpora (ii) both cor-\npora showed completely different results. In the UK corpus, the prediction per-\nformance of a group increased when adding documents of that group . The point\nwhere both groups could be predicted equally well depended on the amount of\ntopics LDA learns.\nIn the US corpus, the prediction performance of both groups improved when in-\ncreasing the amount of right documents . It suggests that both groups were treated\nfairly, but the perplexity depends on other group speci\ufb01c parameter i.e. the article\nlength of each group.\nThe Wikipedia data set contains German documents and English documents. It\nshows a clear vocabulary difference between both groups. Therefore, the results\ndiffered from the Event Registry corpora. The Wikipedia data showed (i) that the\nprediction performance varies between languages . English documents were easier\nto predict than German documents \u2013 due to a signi\ufb01cantly smaller vocabulary \u2013\nsuch that an increasing the amount of English documents in the corpus bene\ufb01ted\nthe prediction performance.\nWhen analysing the group-speci\ufb01c test sets (ii) English documents showed in-\ncreasing perplexity values when providing less than 20% of the training corpus,\nGerman documents showed an even higher increase when providing less than 40%\nof the training corpus. In General, the English test documents showed a better\nprediction performance than the German test documents . Only when learning\n256 topics, 10% English documents could be predicted worse than 90% German\ndocuments. Similar to the UK results, the results of the Wikipedia data set were\nin\ufb02uenced by the chosen amount of topics LDA learns.\nThe language separation in the Wikipedia data set was clear enough such that\nLDA (iii) could differentiate between the two groups and build group assigned\ntopics . The amount of topics assigned to each group increases with the proportion\nof a group in the training data. It did not assign all topics though. There was always\na share of topics that was assigned to both groups. This share grew, if the corpus got\nmore balanced.\nThe share of topics (iv) was always under-proportional relative to the share of\ndocuments in the training data. The effect of under-proportionality increases dra-\nmatically for small minorities of 10% or 20% of the corpus.\nOverall the in\ufb02uence of minority and majority depends on the properties of the\ncontained groups. If the group difference is rather small e.g. political orientation,\nthen the groups do not alter the overall prediction performance of the model and the\n56\nmodel does not learn speci\ufb01c topics for each group. The in\ufb02uence on each groups\ntest corpus is signi\ufb01cant and differs though, even though the overall prediction on\nthe test corpus might look similar.\nIf the group difference is large e.g. languages, then the topic model prediction\nperformance adapts to the majorities prediction performance. That is, if a training\ncorpus contains 80% articles that are easy to predict, then the perplexity will increase\ncompared to a balanced corpus. The topic model even learns speci\ufb01c topics for each\nlanguage. At the same time it is surprising that a rather large share of topics stays\nunassigned and is used by both language groups.\nThese unassignable topics contradict a claim of Boyd-Graber et al. [5]. They\nstated that LDA, when trained on poly-lingual corpora, would learn only language-\nspeci\ufb01c topics (e.g. English and German topics). In the experiments of this thesis,\nthe behaviour of LDA on poly-lingual corpora was examined for the \ufb01rst time and it\ncould be shown that not all learned topics are language-speci\ufb01c. A signi\ufb01cant share\nof the topics in the Wikipedia data set could not be assigned unambiguously to one\nlanguage.\nThese \ufb01ndings have practical implications for the application of LDA: When\nworking with topic models, it is necessary to test for groups with signi\ufb01cant vocab-\nulary differences (e.g. different languages) in the corpus. These groups should be\nidenti\ufb01ed, and group-speci\ufb01c parameters should be learned \u2013 either by using a topic\nmodel which models groups (e.g. poly-lingual topic models [5, 18]) or by learning a\nspeci\ufb01c topic model for each group. This will prevent topics of mixed groups such\nas mixed-language topics, and circumvent majority-minority situations which can\nhave serious impact on the predictability of the minority group.\nStandard corpora, with common vocabularies or small unique vocabularies do\nnot pose such a large problem for LDA. Even though the perplexity values differ\nacross groups, they stay close together such that the impact is not as in\ufb02uential as\nthe impact of a language difference.\nThe variety in all results shows that there exist additional group-related features\nthat control the prediction performance, e.g. the complexity of a language and the\nlength of articles in each group. It is up to further research how these features impact\nthe prediction performance and if they enhance or reduce the effect of a minority\nrole. Suggestions for possible features that could have an in\ufb02uence are: the sample\nsize, the amount of vocabulary overlap across groups, the article length difference\nacross groups and the complexity of the language used in a group. Especially the\nimpact of various languages on the perplexity of topic models is interesting, because\na complexity rank of languages can be created.\n57\nReferences\n[1] David M. Blei, Lawrence Carin, and David B. Dunson. Probabilistic topic mod-\nels.IEEE Signal Processing Magazine , 27:55\u201365, 2010.\n[2] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation.\nJournal of machine Learning research , 3(Jan):993\u20131022, 2003.\n[3] Arnim Bleier. Practical collapsed stochastic variational inference for the hdp.\nCoRR , abs/1312.0412, 2013.\n[4] Gerlof Bouma. Normalized (pointwise) mutual information in collocation ex-\ntraction. Proceedings of GSCL , pages 31\u201340, 2009.\n[5] Jordan L. Boyd-Graber and David M. Blei. Multilingual topic models for un-\naligned text. CoRR , abs/1205.2657, 2012.\n[6] Kevin Robert Canini and Thomas L. Grif\ufb01ths. A nonparametric bayesian model\nof multi-level category learning. In Wolfram Burgard and Dan Roth, editors,\nAAAI . AAAI Press, 2011.\n[7] Jonathan Chang, Jordan L Boyd-Graber, Sean Gerrish, Chong Wang, and\nDavid M Blei. Reading tea leaves: How humans interpret topic models. In\nNips , volume 31, pages 1\u20139, 2009.\n[8] Enhong Chen, Yanggang Lin, Hui Xiong, Qiming Luo, and Haiping Ma. Ex-\nploiting probabilistic topic models to improve text categorization under class\nimbalance. Information Processing & Management , 47(2):202\u2013214, 2011.\n[9] Gregor Heinrich. Parameter estimation for text analysis. Tech-\nnical report, arbylon.net and Fraunhofer Computer Graphics Institute,\nhttp://www.arbylon.net/publications/text-est.pdf, August 2005.\n[10] Matthew Hoffman, Francis R Bach, and David M Blei. Online learning for\nlatent dirichlet allocation. In advances in neural information processing systems ,\npages 856\u2013864, 2010.\n[11] Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic\nvariational inference. Journal of Machine Learning Research , 14:1303\u20131347, 2013.\n[12] Christoph Kling. Probabilistic models for context in social media . PhD thesis, 2016.\n[13] Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas, et al. Handling\nimbalanced datasets: A review. GESTS International Transactions on Computer\nScience and Engineering , 30(1):25\u201336, 2006.\n[14] Ralf Krestel, Peter Fankhauser, and Wolfgang Nejdl. Latent dirichlet allocation\nfor tag recommendation. In Proceedings of the third ACM conference on Recom-\nmender systems , pages 61\u201368. ACM, 2009.\n58\n[15] Ying Liu, Han Tong Loh, and Aixin Sun. Imbalanced text classi\ufb01cation: A term\nweighting approach. Expert systems with Applications , 36(1):690\u2013701, 2009.\n[16] Christopher D Manning, Prabhakar Raghavan, Hinrich Sch\u00fctze, et al. Introduc-\ntion to information retrieval , volume 1. Cambridge university press Cambridge,\n2008.\n[17] Christopher D Manning and Hinrich Sch\u00fctze. Foundations of statistical natural\nlanguage processing . MIT press, 1999.\n[18] David Mimno, Hanna M Wallach, Jason Naradowsky, David A Smith, and An-\ndrew McCallum. Polylingual topic models. In Proceedings of the 2009 Conference\non Empirical Methods in Natural Language Processing: Volume 2-Volume 2 , pages\n880\u2013889. Association for Computational Linguistics, 2009.\n[19] David Mimno, Hanna M Wallach, Edmund Talley, Miriam Leenders, and An-\ndrew McCallum. Optimizing semantic coherence in topic models. In Proceed-\nings of the conference on empirical methods in natural language processing , pages\n262\u2013272. Association for Computational Linguistics, 2011.\n[20] Amy Mitchell, Jeffrey Gottfried, Jocelyn Kiley, and Kate-\nrina Eva Matsa. Political polarization & media habits.\nhttp://www.journalism.org/2014/10/21/political-polarization-media-\nhabits/, 2017. Accessed: 2017-01-22.\n[21] David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. Automatic\nevaluation of topic coherence. In Human Language Technologies: The 2010 An-\nnual Conference of the North American Chapter of the Association for Computational\nLinguistics , pages 100\u2013108. Association for Computational Linguistics, 2010.\n[22] Chris D. Paice. Another stemmer. SIGIR Forum , 24(3):56\u201361, November 1990.\n[23] Martin F Porter. Snowball: A language for stemming algorithms, 2001.\n[24] Radim \u02c7Reh \u02da u\u02c7 rek and Petr Sojka. Software Framework for Topic Modelling with\nLarge Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for\nNLP Frameworks , pages 45\u201350, Valletta, Malta, May 2010. ELRA.\n[25] Michael R\u00f6der, Andreas Both, and Alexander Hinneburg. Exploring the space\nof topic coherence measures. In Proceedings of the eighth ACM international con-\nference on Web search and data mining , pages 399\u2013408. ACM, 2015.\n[26] Michal Rosen-Zvi, Thomas Grif\ufb01ths, Mark Steyvers, and Padhraic Smyth. The\nauthor-topic model for authors and documents. In Proceedings of the 20th Con-\nference on Uncertainty in Arti\ufb01cial Intelligence , UAI \u201904, pages 487\u2013494, Arlington,\nVirginia, United States, 2004. AUAI Press.\n59\n[27] Matthew Smith. How left or right-wing are the uk\u2019s newspapers?\nhttps://yougov.co.uk/news/2017/03/07/how-left-or-right-wing-are-uks-\nnewspapers/, 2017. Accessed: 2017-01-22.\n[28] Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi, and Thomas Grif\ufb01ths.\nProbabilistic author-topic models for information discovery. In Proceedings of\nthe tenth ACM SIGKDD international conference on Knowledge discovery and data\nmining , pages 306\u2013315. ACM, 2004.\n[29] Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Sharing\nclusters among related groups: Hierarchical dirichlet processes. In NIPS , pages\n1385\u20131392, 2004.\n[30] Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. Hi-\nerarchical dirichlet processes. Journal of the American Statistical Association ,\n101(476):1566\u20131581, 2006.\n[31] Yee Whye Teh, Kenichi Kurihara, and Max Welling. Collapsed variational in-\nference for hdp. In Advances in Neural Information Processing Systems (NIPS) ,\n2008.\n[32] Mark Twain. The awful German language . BVK, 1880.\n[33] Xing Wei and W Bruce Croft. Lda-based document models for ad-hoc retrieval.\nInProceedings of the 29th annual international ACM SIGIR conference on Research\nand development in information retrieval , pages 178\u2013185. ACM, 2006.\n[34] www.nltk.org. Natural language toolkit, 2012.\n60", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Topic Models on Biased Corpora", "author": ["M Reif"], "pub_year": "2018", "venue": "NA", "abstract": "Topic models are a popular tool to extract concepts of large text corpora. These text corpora  tend to contain hidden meta groups. The size relation of these groups is frequently"}, "filled": false, "gsrank": 207, "pub_url": "https://d-nb.info/115226446X/34", "author_id": [""], "url_scholarbib": "/scholar?hl=en&q=info:8bY8UC9wgpAJ:scholar.google.com/&output=cite&scirp=206&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D200%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=8bY8UC9wgpAJ&ei=KbWsaMZzjoiJ6g_SwpG4CQ&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:8bY8UC9wgpAJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://d-nb.info/115226446X/34"}}, {"title": "Computational assessment of hyperpartisanship in news titles", "year": "2024", "pdf_data": "Computational Assessment of Hyperpartisanship in News Titles\nHanjia Lyu*, Jinsheng Pan*, Zichen Wang*, Jiebo Luo\nUniversity of Rochester\n{hlyu5, jpan24, zwang189}@ur.rochester.edu, jluo@cs.rochester.edu\nAbstract\nThe growing trend of partisanship in news reporting can have\na negative impact on society. Assessing the level of parti-\nsanship in news headlines is particularly crucial, as they are\neasily accessible and frequently provide a condensed sum-\nmary of the article\u2019s opinions or events. Therefore, they can\nsignificantly influence readers\u2019 decision to read the full arti-\ncle, making them a key factor in shaping public opinion. We\nfirst adopt a human-guided machine learning framework to\ndevelop a new dataset for hyperpartisan news title detection\nwith 2,200 manually labeled and 1.8 million machine-labeled\ntitles that were posted from 2014 to the present by nine repre-\nsentative media organizations across three media bias groups\n-Left, Central, and Right in an active learning manner.\nA fine-tuned transformer-based language model achieves an\noverall accuracy of 0.84 and an F1score of 0.78 on an exter-\nnal validation set. Next, we conduct a computational analysis\nto quantify the extent and dynamics of partisanship in news\ntitles. While some aspects are as expected, our study reveals\nnew or nuanced differences between the three media groups.\nWe find that overall the Right media tends to use propor-\ntionally more hyperpartisan titles. Roughly around the 2016\nPresidential Election, the proportions of hyperpartisan titles\nincreased across all media bias groups, with the Left media\nexhibiting the most significant relative increase. We identify\nthree major topics including foreign issues, political systems,\nandsocietal issues that are suggestive of hyperpartisanship in\nnews titles using logistic regression models and the Shapley\nvalues. Through an analysis of the topic distribution, we find\nthat societal issues gradually gain more attention from all me-\ndia groups. We further apply a lexicon-based language anal-\nysis tool to the titles of each topic and quantify the linguistic\ndistance between any pairs of the three media groups, uncov-\nering three distinct patterns. Codes and data are available at\nhttps://github.com/VIStA-H/Hyperpartisan-News-Titles.\nIntroduction\nExposure to hyperpartisan news online such as the coverage\nof growing polarization could lead individuals to perceive\nthat both the political system and the public are highly po-\nlarized (Fiorina, Abrams, and Pope 2005). In this study, we\naim to understand the partisanship in news. To facilitate a\nquantitative understanding of the extent of partisanship,\n*These authors contributed equally.\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.Vincent and Mestre (2018) and Kiesel et al. (2019) curated\nahyperpartisan news dataset. According to their definition,\nhyperpartisan articles \u201cmimic the form of regular news arti-\ncles, but are one-sided in the sense that opposing views are\neither ignored or fiercely attacked\u201d. In the same vein, Pot-\nthast et al. (2018) considered hyperpartisan news as \u201ctypi-\ncally extremely one-sided, inflammatory, emotional, and of-\nten riddled with untruths.\u201d\nWe believe that a more comprehensive understanding of\nhyperpartisanship can be achieved by considering not only\n(1) the news that contains one-sided opinions but also (2)\nthe news that describes conflicts and the underlying po-\nlitically polarized climate because both of them could lead\nto an increase in the public\u2019s perceived polarization (Yang\net al. 2016; Fiorina, Abrams, and Pope 2005; Levendusky\nand Malhotra 2016). Additionally, the quantity of coverage\nitself can be considered as a particular form of bias (Lin,\nBagrow, and Lazer 2011). Specifically, we seek to extend\nthe definitions of hyperpartisan news to include news that\ncovers partisan conflicts and confrontations.\nTo this end, we first develop a dataset to measure the par-\ntisanship in news titles. We focus on titles because they are\neasily accessible to online users and often summarize the\nopinions or events of the full article, thereby influencing the\ndecision to read the news content. Furthermore, carefully\ncrafted headlines achieve an optimal balance between cap-\nturing the reader\u2019s attention and conveying the key message\nwith minimal cognitive effort, thus enabling readers to easily\ncomprehend and synthesize the information presented (Dor\n2003). Titles that are readily available and circulated in like-\nminded online communities tend to become anchor points\nfor individual opinions (Yang et al. 2016; Zillmann 1999),\nespecially when individuals do not directly experience the\ncovered events (Busselle and Shrum 2003).\nYang et al. (2016) conducted a questionnaire-based study\non the relationship between media use and perceived politi-\ncal polarization. However, one of the limitations is the lack\nof examinations in real-world social platforms where media\nexposure is more complicated because of the combination of\nplatforms\u2019 recommendation algorithms and sharing behav-\niors of friends and followers (Bakshy, Messing, and Adamic\n2015). In other words, respondents may behave differently\nwhen they are finishing a questionnaire than when they are\nexposed to news on social platforms without knowing they\nProceedings of the Eighteenth International AAAI Conference on Web and Social Media (ICWSM 2024)\n999\nare surveyed (Krumpal 2013). A study on the relationship\nbetween their opinions and hyperpartisanship in news titles\ncan be conducted given a dataset that quantitatively mea-\nsures hyperpartisanship in news titles. Therefore, we first\ndevelop a new dataset that complements previous hyper-\npartisan news datasets and then conduct an initial quantita-\ntive comparative analysis of more generalized hyperpartisan\nnews titles. To summarize, our study consists of two parts:\n\u2022Hyperpartisan title detection Based on the experimen-\ntal results of our preliminary study on an existing hyper-\npartisan news dataset (details in Material and Method ),\nwe find that class imbalance, task-label misalignment,\nanddistribution shift issues may hinder us from directly\napplying it to hyperpartisan title detection. Therefore,\nwe adopt a human-guided machine learning framework\nto develop a new hyperpartisan news titles dataset\nwith 2,200 manually labeled and 1.8 million machine-\nlabeled titles that were posted by nine representative me-\ndia organizations across three media bias groups - Left,\nCentral, and Right. The detection achieves an over-\nall accuracy of 0.84 and an F1score of 0.78. We build\nit upon the ideas of a previously proposed hyperparti-\nsan news dataset (Kiesel et al. 2019; Vincent and Mestre\n2018) and further include a second type of news that\ncould also affect perceived polarization. Another distinc-\ntion between our dataset and the previous dataset is that\nwe only focus on titles instead of full articles.\n\u2022Computational analysis of hyperpartisan titles We\nconduct a computational analysis of 1.8 million news ti-\ntles that were posted by nine representative media organi-\nzations from 2014 to 2022. For simplicity, we categorize\nand refer to them as Left, Central, and Right me-\ndia. However, additional attention should be paid when\ninterpreting the results as they are based on nine instead\nof all media organizations. We find that overall Right\nmedia tends to use proportionally more hyperpartisan ti-\ntles, followed by Left media and then Central media.\nMoreover, roughly before the 2016 Presidential Election,\na rise in the proportion of hyperpartisan titles is observed\nin all three media bias groups. In particular, the relative\nincrease in the proportion of hyperpartisan titles of Left\nis the greatest, while the relative changes of Right and\nCentral are more similar and moderate. After Biden\nwas elected the 46thPresident, the proportions of hy-\nperpartisan titles dropped and seemed to gradually return\nto the level before the 2016 Presidential Election. More-\nover, three topics including foreign issues, political sys-\ntems, and societal issues are important in understanding\nthe usage of hyperpartisan titles. A topic divergence in-\ncluding the focus on the choice of topics and linguistic\ndifferences is further observed among the media groups.\nIn the subsequent sections, we provide an overview of re-\nlated work, discuss more details about our motivations, de-\nscribe the development and labeling of our dataset in Mate-\nrial and Method, present the results of our computational\nanalysis, and discuss these results, limitations, and direc-\ntions for future research. Finally, we explore broader per-\nspectives and ethical considerations.Related Work\nNews reports are not always free from bias. For instance,\nBourgeois, Rappaz, and Aberer (2018) discovered selection\nbiases in news coverage. Additionally, political partisanship\nbias is widespread in news media, with headlines, article\nsizes, and framings often manipulated strategically to align\nwith particular ideologies (D\u2019Alessio and Allen 2000; Bu-\ndak, Goel, and Rao 2016; Gentzkow and Shapiro 2010). This\ncan lead to skewed reporting, making it crucial for readers\nto be mindful of this potential bias when consuming news.\nUnderstanding partisanship in texts is an important re-\nsearch topic as it is closely related to the studies of political\npolarization (Monroe, Colaresi, and Quinn 2008; Gentzkow\nand Shapiro 2010). One of its subtasks - hyperpartisan news\ndetection, has drawn great attention from the research com-\nmunity. Potthast et al. (2018) conducted an analysis of the\nwriting style of 1,627 manually labeled hyperpartisan news\narticles and found that the writing style can help discriminate\nhyperpartisan news from non-hyperpartisan news. Kiesel\net al. (2019) raised a contest in hyperpartisan news detection.\nThey developed a hyperpartisan news dataset with 1,273\nmanually labeled and 754,000 machine-labeled news arti-\ncles. Forty-two teams completed the contest. Deep learn-\ning methods as well as the more traditional models that use\nhand-crafted features show promising performance.\nMore recently, researchers have discovered the domi-\nnant performance of transformer-based pre-trained language\nmodels in a variety of natural language processing tasks (De-\nvlin et al. 2019; Chen et al. 2021; Zhang et al. 2021). Zhang\net al. (2021) created a fusion classifier that combines deep\nlearning model scores, other psychological textual features,\nand user demographics to detect depression signals in social\nmedia posts. Card et al. (2022) applied RoBERTa (Liu et al.\n2019) to 200,000 congressional speeches and 5,000 presi-\ndential communications regarding immigration and discov-\nered a significant shift towards more positive language in po-\nlitical discourse around immigration, compared to the past.\nIn this study, we intend to leverage a transformer-based\nlanguage model to quantify the extent and dynamics of\nhyperpartisanship in news titles at a larger scale (i.e., 1.8\nmillion titles of nine representative news media from 2014\nto 2022) compared to previous studies. Furthermore, while\nmost studies focus on the comparison between left-leaning\nand right-leaning media, we expand our analysis to include\nthe comparison with central media.\nMaterial and Method\nHyperpartisan Title Detection\nTask Definition Inspired by Kiesel et al. (2019), we de-\nfine hyperpartisan title detection as \u201cGiven the title of an on-\nline news article, decide whether the title is hyperpartisan\nor not \u201d. A news title is considered hyperpartisan if it either\n(1) expresses a one-sided opinion (e.g., denouncement, crit-\nicism) against a policy, a political party, or a politician in a\nbiased and aggressive tone, or (2) describes a confrontation\nor a conflict between opposing parties indicating the under-\nlying political climate is polarized.\n1000\nCase Accuracy Precision\nRecall F1\n1Original 0.51 0.36 0.28\n0.51\n2Downsampling 0.52 0.48 0.44 0.53\n3Testing year = 2016 0.47 0.38 0.92 0.61\n4Testing year = 2017 0.48 0.87 0.47 0.61\n5Testing year = 2018 0.66 0.67 0.59 0.63\nTable 1: Hyperpartisan title detection on SemEval.\nMotivating Analysis The most closely related task to our\ngoal is hyperpartisan news detection as we have discussed\ninIntroduction. The hyperpartisan news dataset curated by\nVincent and Mestre (2018) and Kiesel et al. (2019) contains\n1,273 manually labeled news articles that were published by\nboth active hyperpartisan and mainstream media outlets. The\narticles were labeled 1 if they contained a fair amount of hy-\nperpartisan content, or 0 if not. For simplicity, we refer to\nthis dataset as SemEval. One primary reason we cannot di-\nrectly apply it to our task is that it only considers one type of\nhyperpartisan news which mainly conveys one-sided opin-\nions. However, we aim to include an extra type of hyper-\npartisan news that describes an underlying politically polar-\nized climate. Moreover, we perform a preliminary analysis\nto show that there are three more major issues that prevent\nus from directly using it for hyperpartisan title detection.\n\u2022Class imbalance Following the same train-test split\nof SemEval, we fine-tune a pre-trained BERT-base\nmodel (Devlin et al. 2019) to predict whether a news ti-\ntle is hyperpartisan based solely on the title itself. How-\never, it achieves a poor performance (Row 1 of Table 1).\nWe hypothesize that one potential cause is the class im-\nbalance issue as the positive samples only account for\n36.90% in the training set. In Row 2, we downsample the\ndata of the majority class in the training set and conduct\nthe same experiment again. We observe that the perfor-\nmance improves when the imbalance issue is alleviated.\n\u2022Task-label misalignment Even after we apply down-\nsampling to SemEval, the improvement is still limited.\nOne reason we hypothesize is that the task and label are\nnotaligned. Our task is to predict whether a news title is\nhyperpartisan, however, the labels in SemEval are rated\nbased on the full article and its metadata (e.g., publisher).\n\u2022Distribution shift A majority of the articles of SemEval\nwere posted from 2016 to 2018. To verify if hyperpar-\ntisan title detection is robust across different years, we\nfurther conduct three experiments. For each experiment,\nwe use the articles from one year as the testing set, and\nall the other articles as the training set. The same fine-\ntuning process is performed. Rows 3, 4, and 5 of Ta-\nble 1 show the performance of using the articles posted\nin 2016, 2017, and 2018, respectively, as the testing set.\nThe results suggest a poor and inconsistent performance\nacross different years. It may be because of a distribution\nshift in the textual patterns such as tones, context, and so\non (which is later confirmed by our analysis results).\nThe experimental results provide insights into the con-\nstruction of our dataset for hyperpartisan news title detec-\nLabeled data\nCorpus\nHyperpartisan Title ClassifierInitial labeled dataNewly labeled data\nUnlabeled \nranked data\nUnlabeled dataTrain\nSampleAnnotate\nRank\nFigure 1: The human-guided machine learning framework.\ntion. The dataset should (1) be class-balanced in order to\nsupport robust learning for language models (Japkowicz\net al. 2000; Chawla, Japkowicz, and Kotcz 2004); (2) have\nlabels that are rated solely based on titles; and (3) contain\ntitles of different years so that the prediction performance\nis consistent. Most importantly, the dataset should also con-\nsider both two types of hyperpartisan news. Next, we de-\nvelop a dataset that satisfies these conditions.\nData collection We collect news from nine representative\nmedia outlets of different media biases - Left, Central,\nandRight. The media bias is judged and assigned by\nallsides.com and mediabiasfactcheck.com. The New York\nTimes, CNN, NBC, and Bloomberg are included in Left\nmedia. Wall Street Journal and Christian Science Monitor\nare included in Central media. The Federalist, Reason,\nand Washington Times are included in Right media. Fox\nis not included because access to its history news titles is\nnot publicly available. Two methods are employed to col-\nlect news: (1) using the official web API provided by the\nnews media, and (2) crawling the archive/sitemap page that\nis publicly available on the media\u2019s official websites. News\nfrom The New York Times and Bloomberg is collected using\nthe first approach. News from the remaining media outlets is\ncollected using the second approach. As aforementioned, we\nfocus on the titles of the articles. In total, we have collected\nover 1.8 million titles from January 2014 to September 2022.\nHuman-guided machine learning After a preliminary\nexploration of the collected titles, we observe a class imbal-\nance issue, with only a small number of titles being hyper-\npartisan. Consequently, we will have to label more data to\nhave enough hyperpartisan samples. To label titles more ef-\nficiently, we develop our dataset in an active learning man-\nner, where a machine learning model and human annotators\nare guided by each other to iteratively seek samples of the\nminority class and improve the performance of the machine\nlearning model. We adopt this strategy because it has shown\neffectiveness in developing datasets and handling class im-\nbalance in multiple studies (Sadilek et al. 2013; Lyu et al.\n2022; Zhou et al. 2023).\nFigure 1 shows a diagram of the human-guided machine\nlearning framework we adopt from Lyu et al. (2022). In gen-\neral, the learning process is composed of multiple iterations.\nIn iteration i, annotators first manually label a small batch\nof data Di\nranked unlabled . Next, we add this new batch of\nlabeled data Di\nlabled into the labeled corpus Ctrain and re-\n1001\n2014 2015 2016 2017 2018 2019 2020 2021 2022\nManually\nlabeled# hyperpartisan titles 71 84 87 119 124 114 116 105 78\n# non-hyperpartisan titles 149 173 171 143 130 132 140 127 137\nModel\nlabeled# hyperpartisan titles 24,258 24,706 34,626 34,181 32,259 33,326 34,766 28,075 22,495\n# non-hyperpartisan titles 229,029 207,176 193,419 162,657 155,651 145,964 155,173 160,403 144,460\nTable 2: Summary statistics of the dataset. The total number of manually labeled titles is 2,200. The total number of titles that\nare labeled by the model is 1.8 million.\nIteration Accuracy Precision Recall F1\n1 0.76 0.79 0.52 0.63\n2 0.78 0.77 0.62 0.67\n3 0.82 0.78 0.75 0.76\n4 0.84 0.81 0.76 0.78\nTable 3: Performance of hyperpartisan title detection of each\niteration of our model.\ntrain the machine learning model H. The trained model is\nused to look for the samples of the minority class - hyper-\npartisan titles from corpus Cunlabeled . The samples that are\nrated as most likely hyperpartisan by the model make up\nDi+1\nranked unlabledwhich will be manually labeled by the an-\nnotators in the next iteration.\nMore specifically, Cunlabeled is comprised of 1.8 million\ntitles we collected. For each iteration, three annotators inde-\npendently read a new batch of 500 titles and rate them as\nhyperpartisan or non-hyperpartisan. A news title is labeled\nhyperpartisan if (1) expresses a one-sided opinion (e.g., de-\nnouncement, criticism) against a policy, a political party, or\na politician in a biased and aggressive tone, or (2) describes\na confrontation or a conflict between opposing parties indi-\ncating the underlying political climate is polarized. Other-\nwise, it is labeled non-hyperpartisan. After the annotation of\nthe first iteration, the three annotators read each other\u2019s la-\nbeling results and discuss the disagreements which mainly\ncome from the less familiarity with the language and poli-\ntics of different periods. The final label is assigned with the\nconsensus votes from three annotators. We choose the pre-\ntrained BERT-base as our machine learning model H.\nIn the first iteration, we fine-tune Husing SemEval and\napply it to 2,000 titles we randomly sample from Cunlabled .\nThese 2,000 titles are ranked based on the likelihood of be-\ning hyperpartisan predicted by H. The new batch is com-\nposed of 90% (n = 450) most likely hyperpartisan titles and\n10% (n = 50) randomly sampled titles. The 10% random\nsamples are included for diversity. The specified proportions\nof the most likely hyperpartisan and random samples allow\nus actively look for the samples of the minority class which\nalleviates the class imbalance issue. Moreover, to address\nthe potential distribution shift problem we discuss before,\nwe add a time constraint. The 90% (n = 450) titles are com-\nposed of the 90% most likely hyperpartisan titles of each\nyear between 2014 to 2022. As a result, the number of poten-\ntial hyperpartisan titles has a relatively uniform distributionacross different years. In the following iterations, all settings\nare the same, except that we no longer need SemEval to fine-\ntune our model. Our model His fine-tuned iteratively with\nthe aggregate labeled data from our own collected titles.\nWe conduct the labeling and model training for four it-\nerations. Inter-annotator agreement measured by Cohen\u2019s\nKappa is 0.47, suggesting a moderate to substantial agree-\nment (Landis and Koch 1977). More details on the annota-\ntion process can be found in the Appendix. We have also la-\nbeled an external validation set with 200 titles, with an equal\nsplit of 50% hyperpartisan and 50% non-hyperpartisan ti-\ntles. Note that these 200 titles are from the same corpus of\n1.8 million news titles but are labeled at the beginning of the\nlabeling session. They are not used during the active learn-\ning process. The same annotation schema is applied. Table 2\nshows the summary statistics of the 2,200 manually labeled\ntitles. The performance of His evaluated on the external\nvalidation set at each iteration (Table 3). We find that the\nperformance on hyperpartisan title detection is improved it-\neration by iteration. After four iterations, the model achieves\nan overall accuracy of 0.84 and an F1score of 0.78.\nMoreover, we compare the performance of other repre-\nsentative language models with the BERT-base model (Ta-\nble 4). In general, the BERT-base model outperforms other\nlanguage models. TF-IDF has a slightly higher recall score\nand a much lower precision score, suggesting poor perfor-\nmance. The superiority of the BERT-base model over bag-\nof-words, TF-IDF, and word2vec is consistent with its per-\nformance in other classification tasks (Devlin et al. 2019;\nZhang et al. 2021; Chen et al. 2021). The preprocessing\nprocedures, implementation details, and hyperparameter set-\ntings are discussed in the Appendix.\nIt is noteworthy that the 1.8 million titles we collect are\nfrom nine media outlets. It is possible that the trained classi-\nfier may not generalize to other outlets. To verify the gener-\nalizability of our model, we conduct nine more experiments.\nFor each experiment, the titles of one of the nine media out-\nlets are completely removed from the training set and the\ntesting set is only composed of the titles of this particular\nmedia outlet. The results, shown in Table 5, demonstrate that\nthe performance of the BERT-based model is consistent and\ncan generalize well to unseen outlets.\nResults\nHyperpartisanship in News Titles\nThe model is fine-tuned using our labeled data and applied\nto 1.8 million titles that were posted by Left, Central,\n1002\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n202300.10.20.30.40.5\nleft media\nright media\ncenter mediaMonthly Hyper-title percentageObama Trump Biden(a) Percentage of hyperpartisan titles.\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n202311.522.533.5\nleft media\nright media\ncenter mediaRelative Monthly Hyper-title shiftObama Trump Biden (b) Relative change in percentage.\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n202300.10.20.30.40.50.60.70.8\nleft media\nright media\ncenter mediaMonthly Hyper-title percentageObama Trump Biden\n(c) Percentage of hyperpartisan titles in the politics subset.\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n202311.21.41.61.82\nleft media\nright media\ncenter mediaRelative Monthly Hyper-title shiftObama Trump Biden (d) Relative change in percentage of the politics subset.\nFigure 2: Usage of hyperpartisan titles of Left, Central, and Right media from 2014 to 2022.\nModel Accuracy Precision Recall F1\nBag-of-words 0.70 0.30 0.77 0.43\nTF-IDF 0.73 0.39 0.79 0.52\nword2vec 0.78 0.60 0.77 0.67\nBERT-base 0.84 0.81 0.76 0.78\nTable 4: Performance of hyperpartisan title detection of dif-\nferent language models.\nandRight media between 2014 and 2022. Table 2 shows\nthe summary statistics of the titles that are labeled by our\nmodel. Overall, 14.7% titles are classified as hyperpartisan.\nFigure 2a shows the percentage of hyperpartisan titles. To\ncalculate the relative change in percentage, the percentage at\neach time stamp is divided by the initial percentage in 2014.\nThe result is shown in Figure 2b. The Appendix includes the\nresults of an alternative model and a validity verification.\nFirst, we find that overall Right media tends to use pro-\nportionally more hyperpartisan titles, followed by Left me-\ndia and then Central media (Figure 2a). Of all the titles\nbetween 2014 to 2022, 14.20%, 8.91%, and 31.65% are hy-\nperpartisan in Left, Central, and Right, respectively.\nSecond, we discover a rise in the proportion of hyperpar-\ntisan titles in all three media bias groups (Figure 2b). Specif-\nically, the relative increase in the proportion of hyperparti-Target Media Accuracy Precision Recall F1\nBloomberg 0.83 0.79 0.78 0.78\nCNN 0.84 0.80 0.77 0.78\nNBC 0.85 0.81 0.81 0.81\nThe New York Times 0.84 0.77 0.86 0.81\nChristian Science Monitor 0.84 0.82 0.77 0.84\nWall Street Journal 0.83 0.82 0.72 0.77\nReason 0.84 0.85 0.73 0.78\nThe Federalist 0.84 0.81 0.78 0.79\nWashington Time 0.85 0.82 0.81 0.81\nTable 5: Generalizability verification for our model.\nsan titles of Left media is the greatest, while the relative\nchanges of Right andCentral media are more similar\nand moderate. Compared to 2014, the average proportion\nof hyperpartisan titles that Left used in 2016 increased by\n101.77% (22.19% for Central and 43.53% for Right).\nThe proportions of hyperpartisan titles remained high dur-\ning the Trump Administration (2017-2020).\nThird, after Biden was elected the 46thPresident, the pro-\nportions of hyperpartisan titles dropped and seemed to grad-\nually return to the level before the 2016 Presidential Elec-\ntion. For instance, the relative change in hyperpartisan title\nproportion of Left media dropped by 39.74% by compar-\ning November 2020 and November 2021.\n1003\nNote that the increased proportion of hyperpartisan titles\nmay be due to the increased coverage of political events\nsuch as presidential elections. To better understand the rea-\nson for the rise in the usage of hyperpartisan titles, we create\napolitics subset which only contains titles regarding\npolitics from the 1.8 million corpora. A separate labeling-\ntraining process is conducted (more details in the Appendix ).\nFigures 2c and 2d show the percentage of hyperpartisan ti-\ntles in the politics subset and relative change in per-\ncentage of the politics subset, respectively. We observe\nthat the overall pattern remains the same. However, a no-\ntable difference is that although the relative change in per-\ncentage of Left media is still the highest, it is smaller in\nthepolitics subset than that in the overall set, suggest-\ning that a great portion of the increased hyperpartisan titles\nposted by the Left media is the coverage of political events.\nInfluential Terms in Predicting Hyperpartisanship\nSimilar to Card et al. (2022), we train L1 logistic regression\nmodels to identify the most important words that are sug-\ngestive of hyperpartisan and non-hyperpartisan titles used by\nLeft, Central, and Right media. Based on the observa-\ntions in Figure 2, we categorize the titles into three periods -\nbefore the 2016 Presidential Election, during the Trump Ad-\nministration, and after the 2020 Presidential Election. The\ninput to the logistic regression models is the words in each\ntitle. The output label - whether or not the title is hyperparti-\nsan, is assigned based on the inference of our BERT-base\nmodel. We only include the words that at least appear in\n0.5% of all collected titles. Stop words are removed using\nthenltk package. Word counts are binarized.\nThe importance of each word is measured using the Shap-\nley values (Lundberg and Lee 2017). Each model is trained\nand evaluated using 5-fold cross-validation (Table 6). In\neach validation, the 50 most important features (i.e., words)\nare kept. After five validations, the features that occur the\nmost are reported in Table 7. Similar to the BERT-base\nmodel, logistic regression may suffer from the generalizabil-\nity issue. To verify this, we conduct a set of additional ex-\nperiments where the input of one media outlet is completely\nremoved from the training set and the testing set is only com-\nposed of the input of this outlet. Table 6 shows that the lo-\ngistic regression models generalize well to unseen outlets.\nIt is noteworthy that the accuracy appears to be lower for\nright-leaning outlets. This could be attributed to the limited\ntraining data available for titles from these outlets.\nOur first observation is that across three periods, three\ngroups of words representing foreign countries, domestic\npolitical systems, and societal issues are influential in pre-\ndicting hyperpartisan and/or non-hyperpartisan titles.\nSince the pre-2016 election period only covers the news\nposted between 2014 to 2016, it is expected the words\nthat are suggestive of either hyperpartisanship or non-\nhyperpartisanship are election-related (e.g., \u201ccampaign\u201d and\n\u201cdebate\u201d), although there is a subtle difference where in spite\nof the similarity in terms of the election, \u201ccampaign\u201d and\n\u201cdebate\u201d are more important in hyperpartisanship prediction\nwhile \u201cvote\u201d and \u201celection\u201d are more important in the non-\nhyperpartisan prediction of Left media. Another notewor-thy finding is that \u201cBiden\u201d is important in predicting hyper-\npartisan titles of Right media while \u201cTrump\u201d is influential\nin predicting non-hyperpartisan titles.\nDuring the Trump Administration, words indicating so-\ncietal issues were more important in predicting hyperparti-\nsan titles of Left media. Interestingly, although COVID-19\nwas announced as a pandemic in 2020 (late in the Trump\nAdministration), the related words (i.e., \u201ccoronavirus\u201d and\n\u201ccovid\u201d) are still influential at the aggregate level. Moreover,\ncoverage of foreign issues becomes important in predicting\nhyperpartisanship of Right media.\nAfter the 2020 Presidential Election, some election-\nrelated words are still considered influential. However, it is\nnot surprising as there is still an argument from Trump and\nhis allies about the election result.\nTopic Divergence among Media Groups\nTopic distribution Based on the important terms we iden-\ntify in Table 7, we discover three major topics - \u201cforeign\nissue\u201d, \u201cpolitical system\u201d, and \u201csociety issue\u201d, of three me-\ndia groups across different periods. To better understand the\ntopic divergence among the three media groups, we first\nmeasure the difference in topic distributions. For each topic,\nwe curate a list of keywords from the most important terms\nin Table 7 (see Appendix for details). We then use the key-\nword list to identify the corresponding topic of each news\ntitle. To analyze the distribution divergence among differ-\nent media groups, the log of frequency ratio is calculated as\nEq. 1 and shown in Figure 3. Following Card et al. (2022),\nwe use the log of frequency ratio instead of frequency ratio\nfor a better visualization.\nlogFLHS\nFRHS(1)\nFLHS andFRHS represent the frequency of the media on\nthe left-hand side and right-hand side, respectively. A larger\nlog of frequency ratio means that this topic is mentioned\nmore frequently by the media on the right-hand side and\nvice versa. The circle size indicates the overall frequency\nof this topic. Figure 3 shows the relative frequency for each\nof the three topics - \u201cforeign issue\u201d, \u201cpolitical system\u201d, and\n\u201csocietal issue\u201d among different media groups (Figure 3a:\nLeft versus Right, Figure 3b: Left versus Central,\nFigure 3c: Central versus Right). To ensure the robust-\nness of our method, we leave one word out of the keyword\nlist and conduct the same analysis again. The new values are\nplotted as black dots in Figure 3.\nIn almost all comparisons, we find much difference in the\ntopic distributions. Compared to Left andCentral me-\ndia,Right media pays more attention to the political sys-\ntem and societal issues between 2014 to 2022. Central\nmedia emphasizes the coverage of foreign issues.\nWhen comparing the difference in topic distributions\nacross the three periods, we observe that the attentions to\nthese three topics of all media groups tend to be more simi-\nlar where the log of relative frequency is closer to zero.\nIn terms of the general topic distribution, we find all me-\ndia groups cover more news about societal issues. More im-\nportantly, the difference among the distributions of societal\nissue topic is reduced in the post-2020 election period.\n1004\nPre-2016 election\n(2014-2016)Trump Administration\n(2017-2020)Post-2020 election\n(2021-2022)\nLeft 0.92\u00b10.00 0.89 \u00b10.00 0.90 \u00b10.00\n- Bloomberg 0.94\u00b10.00 0.94 \u00b10.00 0.92 \u00b10.00\n- CNN 0.86\u00b10.00 0.85 \u00b10.00 0.88 \u00b10.00\n- NBC 0.90\u00b10.00 0.86 \u00b10.00 0.86 \u00b10.00\n- The New York Times 0.94 \u00b10.00 0.91 \u00b10.00 0.91 \u00b10.00\nCentral 0.93\u00b10.00 0.92 \u00b10.00 0.92 \u00b10.00\n- Christian Science Monitor 0.88 \u00b10.00 0.88 \u00b10.00 0.85 \u00b10.01\n- Wall Street Journal 0.94\u00b10.00 0.93 \u00b10.00 0.92 \u00b10.00\nRight 0.79\u00b10.00 0.79 \u00b10.00 0.79 \u00b10.00\n- Reason 0.79\u00b10.00 0.81 \u00b10.00 0.84 \u00b10.00\n- The Federalist 0.77\u00b10.01 0.75 \u00b10.01 0.70 \u00b10.01\n- Washington Time 0.79\u00b10.01 0.79 \u00b10.00 0.78 \u00b10.00\nTable 6: Classification accuracy of the logistic regression models under 5-fold cross-validation. Each row indicated by the media\nname represents the classification accuracy of the generalizability verification. For instance, the row of Bloomberg reports the\nclassification accuracy of the logistic regression model that is trained without the input from Bloomberg but is tested with the\ninput from Bloomberg under 5-fold cross-validation.\nSocietal IssuePolitical SystemForeign IssuePre-2016 election\nL  R\nSocietal IssuePolitical SystemForeign IssueTrump Administration\nL  R\n-1 0 1\nlog(Frequency ratio)Societal IssuePolitical SystemForeign IssuePost-2020 election\nL  R\n(a) Left versus Right\nSocietal IssuePolitical SystemForeign IssuePre-2016 election\nL  C\nSocietal IssuePolitical SystemForeign IssueTrump Administration\nL  C\n-1 0 1\nlog(Frequency ratio)Societal IssuePolitical SystemForeign IssuePost-2020 election\nL  C\n (b) Left versus Central\nSocietal IssuePolitical SystemForeign IssuePre-2016 election\nC  R\nSocietal IssuePolitical SystemForeign IssueTrump Administration\nC  R\n-1 0 1\nlog(Frequency ratio)Societal IssuePolitical SystemForeign IssuePost-2020 election\nC  R\n (c) Central versus Right\nFigure 3: Relative frequency for each of three major topics by different media groups. Farther to the right on each plot represents\nhigher frequency by the media on the right-hand side, and vice versa. The circle size represents the overall frequency of the\ntopic. For each topic, we leave one word out and plot the log of frequency ratio again which is then plotted in black.\nLinguistic difference To estimate how different media\ngroups portray the same topic, we leverage Linguistic In-\nquiry and Word Count (LIWC) (Pennebaker, Francis, and\nBooth 2001). LIWC is a lexicon-based language analysis\ntool that measures the linguistic features of people\u2019s lan-\nguage. In a nutshell, LIWC takes a piece of text data and\ncounts the number of words that fall into each of the pre-\ndefined categories such as emotions, thinking styles, and so-\ncial concerns that characterizes authors\u2019 psychological states\nbased on its dictionary of words and their associated linguis-\ntic scores. For instance, LIWC may categorize a word as\nindicating a positive emotion, such as \u201chappy\u201d. If the num-\nber of \u201chappy\u201d is large in that piece of text data, LIWC will\noutput a higher score for positive emotion. In total, LIWC\noutputs 95 different linguistic features. In our study, we use\nall of them. Specifically, for each title, we employ LIWC to\noutput a 95-d vector of linguistic features.To ensure the robustness of LIWC on short text such as\nnews titles, we concatenate all the titles of one media group\nabout one topic in a certain period, following Chen et al.\n(2021). Next, because of the scale differences among the lin-\nguistic features, we standardize the features. The linguistic\ndifference between any pair of the three media groups is then\nmeasured using the cosine distance. The results smoothed\nusing a 7-month moving average are shown in Figure 4.\nOverall, we find three distinct patterns in the linguistic dif-\nference between the titles of these topics.\nWith respect to foreign issues, we find that the linguistic\ndifference between Right andCentral media becomes\nsmaller (green line) while both of them are linguistically\nmore distinct from Left media. In contrast, around 2014-\n2015, there was a smaller difference between Left and\nCentral than between either of them and Right.\nWe identify two major patterns in the linguistic differ-\n1005\nPre-2016 election\n(2014-2016)Trump Administration\n(2017-2020)Post-2020 election\n(2021-2022)Left\nHpower, Democrat,\nparty, policy,\ntime, lead, crisis, star, push,\ncampaign, kill, Obama,\nlose, GOP, debateparty, star, government, election,\ntest, crisis, rate, gun, sale,\npower, school, coronavirus, law,\nworld, womanBlack, talk, kill, game, Democrat,\nprotest, campaign, call, right,\nformer, debate, group, power,\nRussian, todayNon-Hgame, thing, return,\nprotest, law,\nBlack, Russian, White, company,\nvote, rate, market, election, womanClinton, lose, rate, next, push,\nman, kill, climate, live, senate,\nObama, debate, Republican,\ncompany, attackAmerican, Republican, shoot,\nattack, love, rate, climate, state,\ncovid, bank, Ukraine, America,\nbill, buy, manCentral\nHWhite, Ukraine, crisis, protest,\nDemocrat,\ndebate, party, Black,\nreview, attack, government,\nRepublican, rate, supreme, Iranrate, Clinton, covid, right, lose,\ncall, senate, China, review,\nTrump, push, north, bill, life,\nsellTexas, Republican, new, right,\nUkraine, car, kill, big, attack,\nTrump, coronavirus, push,\nBiden, GOPNon-Hdeath, car, meet,\ngame, election,\nCEO, school, news, Korea, sale,\njob, tech, test, star, tradeAmerica, protest, cut, car, school,\nBlack, price, kill, supreme, work,\nWhite, set, meet, stop, buyformer, work, plan, child, vote,\nlook, covid, record, rate, ban,\ntech, stock, policy, president, healthRight\nHright, fight, record, seek,\nleave,\nlose, book, risk, party, time,\nschool, push, protest, gun, Bidenbook, big, test, review, attack,\nstop, policy, Russian, tax, new,\nTexas, Ukraine, star, court, warpower, want, state, protest,\npandemic, make, Democrat,\ngovernment, Black, Texas, time,\nClinton, ban, judge, formerNon-Hcar, company\n, Ukraine, day, Trump,\nRussian, market, return, America,\nBlack, leader, new, home,\ngovernment, oldelection, Democrat, work, home,\nsupreme, business, Obama, old,\nreturn, win, White, take, timeBiden, find, school, vote, election,\nstar, company, bill, party, start,\nfight, meet, trade, old, seek\nTable 7: Most influential words for hyperpartisan and non-hyperpartisan titles, in three periods, when approximating the\npredicted hyperpartisanship from our BERT-base model with logistic regression models. H: Hyperpartisan, Non-H: Non-\nhyperpartisan. We find words representing foreign countries (e.g., \u201cRussia\u201d and \u201cChina\u201d), domestic political systems (e.g.,\n\u201cDemocrat\u201d and \u201cRepublican\u201d), and societal issues (e.g., \u201cgun\u201d and \u201cschool\u201d).\nence in the coverage of political systems. First, except for\nthe linguistic difference between Left andCentral, the\nlinguistic difference between the other pairs of the three me-\ndia groups gradually decreases over time. Second, roughly\naround the 2016 Presidential Election, the linguistic differ-\nence between Left andRight is almost consistently the\nlowest compared to the difference between either of them\nandCentral, indicating a more similar linguistic style be-\ntween Left andRight when they cover news about the\npolitical systems (e.g., election and politicians).\nThe linguistic difference in terms of societal issues\ndemonstrates a seasonal pattern where the peak in the lin-\nguistic difference occurs approximately after each election\n(2015, 2017, 2019, 2021). We hypothesize that it is because\nthe societal issues are mainly discussions about policies that\nare closely related to voters\u2019 rights. The different focuses in-\nside the policies are captured by LIWC through the titles.\nFurther, the difference between Left andCentral me-\ndia is smaller before the 2016 Presidential Election (orange\nline) compared to the other two cases. However, since then,\nthe differences between the three cases evolve similarly.\nDiscussions and Conclusions\nIncreased partisanship in news can result in enhanced po-\nlarization which undermines democracy and has a substan-tial negative impact across the entire society (Vincent and\nMestre 2018; Minar and Naher 2018; Recuero, Soares, and\nGruzd 2020; Conover et al. 2011; Weld, Glenski, and Althoff\n2022; Lyu and Luo 2022). For instance, researchers found\nthat online partisan media can lead to a lasting and meaning-\nful decrease in trust in the mainstream media (Guess et al.\n2021). Recuero, Soares, and Gruzd (2020) investigated the\nrole of hyperpartisanship and polarization on Twitter during\nthe 2018 Brazilian presidential Election and found that as\nthe centrality of the hyperpartisan news outlets grows, the\nmainstream news outlets become more and more biased.\nIn this study, we have developed a new dataset for hyper-\npartisan title detection that allows us to quantify the extent\nand dynamics of hyperpartisanship in news titles. We con-\nsider two types of news titles: (1) the ones that express a one-\nsided opinion against a policy, apolitical party, or a politi-\ncian in a biased, inflammatory, and aggressive tone, and (2)\nthe ones that describe a confrontation or a conflict between\nopposing parties indicating the underlying political climate\nis polarized. Different from previous studies that only con-\nsider the first type of news (Kiesel et al. 2019; Vincent and\nMestre 2018; Potthast et al. 2018), we include both types\nas they could both lead to an increase in perceived polar-\nization (Yang et al. 2016; Fiorina, Abrams, and Pope 2005;\nLevendusky and Malhotra 2016).\n1006\n0.40.50.60.70.80.91.01.1DistanceForeign Issue\nL  R\nL  C\nC  R\n0.50.60.70.80.9DistancePolitical System\nL  R\nL  C\nC  R\n2014 2015 2016 2017 2018 2019 2020 2021 20220.40.50.60.70.80.9DistanceSocietal Issue\nL  R\nL  C\nC  R\nFigure 4: Linguistic difference among different media\ngroups about foreign issues, political systems, and societal\nissues. L: Left, C: Central, R: Right.\nWe find that the Right media more often use hyper-\npartisan titles compared to the Left andCentral me-\ndia. The proportions of hyperpartisan titles of all media bias\ngroups increased roughly around the 2016 Presidential Elec-\ntion and decreased after the 2020 Presidential Election. A\nstrong association between elections and hyperpartisanship\nis observed, which is also reported by Lin, Bagrow, and\nLazer (2011). Interestingly, the relative change in the pro-\nportion of hyperpartisan titles of the Left media is the most\ncompared to the Right andCentral media.\nUsing the logistic regression models with the Shapley val-\nues, we find that although the specific words that are sugges-\ntive of hyperpartisanship or non-hyperpartisanship in news\ntitles across three media bias groups are not necessarily the\nsame, three common topics are identified including foreign\nissues, political systems, and societal issues.\nComparisons between any pairs of the Left, Central,\nandRight media are made in terms of topic frequencies.\nSocietal issues have drawn more attention from all media\ngroups since the start of the Biden Administration. We hy-pothesize that it is related to the Biden-Harris Administra-\ntion Immediate Priorities which is intended to address so-\ncietal issues such as the COVID-19 pandemic, education,\nclimate change, and so on.1\nFurther linguistic analysis reveals three distinct patterns\nin how different media portrays these topics. First, when it\ncomes to foreign issues, the linguistic difference between\nLeft and either Right orCentral media becomes larger\nwhile the difference between Right andCentral media\nis smaller. We hypothesize that the linguistic difference may\nbe suggestive of the conflicting partisan priorities for U.S.\nforeign policy. For instance, Republicans are more likely\nto prioritize reducing illegal immigration while Democrats\ntend to prioritize dealing with global climate change, ac-\ncording to a survey by Pew Research Center.2Second, the\nlinguistic difference in the titles about political systems be-\ncomes smaller among all media groups except for the dif-\nference between Left andCentral. Third, seasonality is\nobserved in the linguistic difference when depicting societal\nissues. The difference grows around the start of elections and\ndecreases after the election results are confirmed. We think it\nis expected since societal issues (e.g., education, health) are\nclosely related to voters\u2019 rights. It is noteworthy that the lin-\nguistic difference between the Left andCentral groups\nincreased for all three topics.\nOur computational assessment reveals several new or nu-\nanced aspects about both the extent and dynamics of hyper-\npartisanship in news titles. According to the results of the\ntopic analysis, we derive insights into the reasons for the\nlong- and short-term changes. Short-term changes are found\nrelated to elections while long-term changes might imply the\nunderlying shifting priorities for the wide range of policies.\nWe use mainstream media news as our study sample. It is\nworth noting that a similar divergence between different po-\nlitically leaning entities is also identified in studies that are\nbased on social media data (Waller and Anderson 2021) or\ncongressional and presidential speeches (Card et al. 2022).\nLimitations\nOur study has several limitations. First, there are more and\nnewer news organizations that could be worth investigating.\nHowever, given our focus on analyzing the news headlines\nover time, it is essential that the media we consider should\nprovide publicly available access to their news titles for the\nentire study period. Other media outlets may not provide\nsuch online archives or APIs. It is important to acknowl-\nedge that these alternative news sources often disproportion-\nately feature hyperpartisan titles. Despite this limitation, our\nexperiments suggest that analyzing the behavior of newer\noutlets would provide a more holistic understanding of me-\ndia hyperpartisanship and represent a promising avenue for\nfuture research. Second, while online news is an important\nsource of news for many people, there are other ways in\nwhich individuals consume news. It is crucial to examine\nmedia hyperpartisanship in other forms of media as well.\n1https://www.whitehouse.gov/priorities/\n2https://www.pewresearch.org/politics/2018/11/29/conflicting-\npartisan-priorities-for-u-s-foreign-policy/\n1007\nAcknowledgments\nWe are grateful to Ying Zhou, Weihong Qi, Prof. Mayya\nKomisarchik, and Prof. Anson Kahng for their constructive\nsuggestions.\nReferences\nBakshy, E.; Messing, S.; and Adamic, L. A. 2015. Expo-\nsure to ideologically diverse news and opinion on Facebook.\nScience, 348(6239): 1130\u20131132.\nBird, S.; Klein, E.; and Loper, E. 2009. Natural language\nprocessing with Python: analyzing text with the natural lan-\nguage toolkit. \u201d O\u2019Reilly Media, Inc.\u201d.\nBourgeois, D.; Rappaz, J.; and Aberer, K. 2018. Selection\nbias in news coverage: learning it, fighting it. In Companion\nProceedings of the The Web Conference 2018, 535\u2013543.\nBudak, C.; Goel, S.; and Rao, J. M. 2016. Fair and balanced?\nQuantifying media bias through crowdsourced content anal-\nysis. Public Opinion Quarterly, 80(S1): 250\u2013271.\nBusselle, R. W.; and Shrum, L. 2003. Media exposure and\nexemplar accessibility. Media Psychology, 5(3): 255\u2013282.\nCard, D.; Chang, S.; Becker, C.; Mendelsohn, J.; V oigt, R.;\nBoustan, L.; Abramitzky, R.; and Jurafsky, D. 2022. Com-\nputational analysis of 140 years of US political speeches\nreveals more positive but increasingly polarized framing of\nimmigration. Proceedings of the National Academy of Sci-\nences, 119(31): e2120510119.\nChawla, N. V .; Japkowicz, N.; and Kotcz, A. 2004. Special\nissue on learning from imbalanced data sets. ACM SIGKDD\nexplorations newsletter, 6(1): 1\u20136.\nChen, L.; Lyu, H.; Yang, T.; Wang, Y .; and Luo, J. 2021.\nFine-grained analysis of the use of neutral and controversial\nterms for COVID-19 on social media. In International Con-\nference on Social Computing, Behavioral-Cultural Model-\ning and Prediction and Behavior Representation in Model-\ning and Simulation, 57\u201367. Springer.\nConover, M.; Ratkiewicz, J.; Francisco, M.; Gonc \u00b8alves, B.;\nMenczer, F.; and Flammini, A. 2011. Political polarization\non twitter. In Proceedings of the international aaai confer-\nence on web and social media, volume 5, 89\u201396.\nD\u2019Alessio, D.; and Allen, M. 2000. Media bias in presiden-\ntial elections: A meta-analysis. Journal of communication,\n50(4): 133\u2013156.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Burstein, J.; Doran, C.; and\nSolorio, T., eds., Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\nume 1 (Long and Short Papers), 4171\u20134186. Association for\nComputational Linguistics.\nDor, D. 2003. On newspaper headlines as relevance opti-\nmizers. Journal of pragmatics, 35(5): 695\u2013721.\nFiorina, M. P.; Abrams, S. J.; and Pope, J. C. 2005. Culture\nwar. The myth of a polarized America, 3.FORCE11. 2020. The FAIR Data principles. https://force11.\norg/info/the-fair-data-principles/. Accessed: 2024-04-03.\nGebru, T.; Morgenstern, J.; Vecchione, B.; Vaughan, J. W.;\nWallach, H.; Iii, H. D.; and Crawford, K. 2021. Datasheets\nfor datasets. Communications of the ACM, 64(12): 86\u201392.\nGentzkow, M.; and Shapiro, J. M. 2010. What drives media\nslant? Evidence from US daily newspapers. Econometrica,\n78(1): 35\u201371.\nGuess, A. M.; Barber \u00b4a, P.; Munzert, S.; and Yang, J. 2021.\nThe consequences of online partisan media. Proceedings of\nthe National Academy of Sciences, 118(14): e2013464118.\nJapkowicz, N.; et al. 2000. Learning from imbalanced data\nsets: a comparison of various strategies. In AAAI workshop\non learning from imbalanced data sets, volume 68, 10\u201315.\nAAAI Press Menlo Park, CA.\nKiesel, J.; Mestre, M.; Shukla, R.; Vincent, E.; Adineh, P.;\nCorney, D.; Stein, B.; and Potthast, M. 2019. SemEval-\n2019 Task 4: Hyperpartisan News Detection. In Proceed-\nings of the 13th International Workshop on Semantic Evalu-\nation, 829\u2013839. Minneapolis, Minnesota, USA: Association\nfor Computational Linguistics.\nKingma, D. P.; and Ba, J. 2017. Adam: A Method for\nStochastic Optimization. arXiv:1412.6980.\nKrumpal, I. 2013. Determinants of social desirability bias\nin sensitive surveys: a literature review. Quality & quantity,\n47(4): 2025\u20132047.\nLandis, J. R.; and Koch, G. G. 1977. The measurement of\nobserver agreement for categorical data. biometrics, 159\u2013\n174.\nLevendusky, M.; and Malhotra, N. 2016. Does media cover-\nage of partisan polarization affect political attitudes? Politi-\ncal Communication, 33(2): 283\u2013301.\nLin, Y .-R.; Bagrow, J.; and Lazer, D. 2011. More voices than\never? quantifying media bias in networks. In Proceedings of\nthe International AAAI Conference on Web and Social Me-\ndia, volume 5, 193\u2013200.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv:1907.11692.\nLundberg, S. M.; and Lee, S.-I. 2017. A unified approach\nto interpreting model predictions. Advances in neural infor-\nmation processing systems, 30.\nLyu, H.; and Luo, J. 2022. Understanding Political Polar-\nization via Jointly Modeling Users, Connections and Multi-\nmodal Contents on Heterogeneous Graphs. In Proceedings\nof the 30th ACM International Conference on Multimedia ,\n4072\u20134082.\nLyu, H.; Wang, J.; Wu, W.; Duong, V .; Zhang, X.; Dye, T. D.;\nand Luo, J. 2022. Social media study of public opinions\non potential COVID-19 vaccines: informing dissent, dispar-\nities, and dissemination. Intelligent medicine, 2(01): 1\u201312.\nMinar, M. R.; and Naher, J. 2018. Violence originated from\nFacebook: A case study in Bangladesh. arXiv:1804.11241.\n1008\nMonroe, B. L.; Colaresi, M. P.; and Quinn, K. M. 2008.\nFightin\u2019words: Lexical feature selection and evaluation for\nidentifying the content of political conflict. Political Analy-\nsis, 16(4): 372\u2013403.\nPedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V .;\nThirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss,\nR.; Dubourg, V .; Vanderplas, J.; Passos, A.; Cournapeau, D.;\nBrucher, M.; Perrot, M.; and Duchesnay, E. 2011. Scikit-\nlearn: Machine Learning in Python. Journal of Machine\nLearning Research, 12: 2825\u20132830.\nPennebaker, J. W.; Francis, M. E.; and Booth, R. J. 2001.\nLinguistic inquiry and word count: LIWC 2001. Mahway:\nLawrence Erlbaum Associates, 71(2001): 2001.\nPotthast, M.; Kiesel, J.; Reinartz, K.; Bevendorff, J.; and\nStein, B. 2018. A Stylometric Inquiry into Hyperpartisan\nand Fake News. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), 231\u2013240. Melbourne, Australia: Association\nfor Computational Linguistics.\nRecuero, R.; Soares, F. B.; and Gruzd, A. 2020. Hyperparti-\nsanship, disinformation and political conversations on Twit-\nter: The Brazilian presidential election of 2018. In Proceed-\nings of the international AAAI conference on Web and social\nmedia, volume 14, 569\u2013578.\nRehurek, R.; and Sojka, P. 2011. Gensim\u2013python framework\nfor vector space modelling. NLP Centre, Faculty of Infor-\nmatics, Masaryk University, Brno, Czech Republic, 3(2).\nSadilek, A.; Brennan, S.; Kautz, H.; and Silenzio, V . 2013.\nnEmesis: Which restaurants should you avoid today? In\nFirst AAAI Conference on Human Computation and Crowd-\nsourcing.\nVincent, E.; and Mestre, M. 2018. Crowdsourced Measure\nof News Articles Bias: Assessing Contributors\u2019 Reliability.\nInSAD/CrowdBias@ HCOMP, 1\u201310.\nWaller, I.; and Anderson, A. 2021. Quantifying social orga-\nnization and political polarization in online platforms. Na-\nture, 600(7888): 264\u2013268.\nWeld, G.; Glenski, M.; and Althoff, T. 2022. Political Bias\nand Factualness in News Sharing across more than 100,000\nOnline Communities. arXiv:2102.08537.\nYang, J.; Rojas, H.; Wojcieszak, M.; Aalberg, T.; Coen, S.;\nCurran, J.; Hayashi, K.; Iyengar, S.; Jones, P. K.; Mazzoleni,\nG.; et al. 2016. Why are \u201cothers\u201d so polarized? Perceived\npolitical polarization and media use in 10 countries. Journal\nof Computer-Mediated Communication, 21(5): 349\u2013367.\nZhang, Y .; Lyu, H.; Liu, Y .; Zhang, X.; Wang, Y .; Luo, J.;\net al. 2021. Monitoring depression trends on twitter dur-\ning the COVID-19 pandemic: observational study. JMIR in-\nfodemiology, 1(1): e26769.\nZhou, E.; Liu, Y .; Lyu, H.; and Luo, J. 2023. A Fine-\nGrained Analysis of Public Opinion toward Chinese Tech-\nnology Companies on Reddit. In 2023 IEEE International\nConference on Big Data (BigData), 5951\u20135959. Los Alami-\ntos, CA, USA: IEEE Computer Society.\nZillmann, D. 1999. Exemplification theory: Judging the\nwhole by some of its parts. Media psychology, 1(1): 69\u201394.Paper Checklist\n1. For most authors...\n(a) Would answering this research question advance sci-\nence without violating social contracts, such as violat-\ning privacy norms, perpetuating unfair profiling, exac-\nerbating the socio-economic divide, or implying disre-\nspect to societies or cultures? Yes.\n(b) Do your main claims in the abstract and introduction\naccurately reflect the paper\u2019s contributions and scope?\nYes.\n(c) Do you clarify how the proposed methodological ap-\nproach is appropriate for the claims made? Yes.\n(d) Do you clarify what are possible artifacts in the data\nused, given population-specific distributions? Yes.\n(e) Did you describe the limitations of your work? Yes.\n(f) Did you discuss any potential negative societal im-\npacts of your work? Yes.\n(g) Did you discuss any potential misuse of your work?\nYes.\n(h) Did you describe steps taken to prevent or mitigate po-\ntential negative outcomes of the research, such as data\nand model documentation, data anonymization, re-\nsponsible release, access control, and the reproducibil-\nity of findings? Yes.\n(i) Have you read the ethics review guidelines and en-\nsured that your paper conforms to them? Yes.\n2. Additionally, if your study involves hypotheses testing...\n(a) Did you clearly state the assumptions underlying all\ntheoretical results? NA.\n(b) Have you provided justifications for all theoretical re-\nsults? NA.\n(c) Did you discuss competing hypotheses or theories that\nmight challenge or complement your theoretical re-\nsults? NA.\n(d) Have you considered alternative mechanisms or expla-\nnations that might account for the same outcomes ob-\nserved in your study? NA.\n(e) Did you address potential biases or limitations in your\ntheoretical framework? NA.\n(f) Have you related your theoretical results to the existing\nliterature in social science? NA.\n(g) Did you discuss the implications of your theoretical\nresults for policy, practice, or further research in the\nsocial science domain? NA.\n3. Additionally, if you are including theoretical proofs...\n(a) Did you state the full set of assumptions of all theoret-\nical results? NA.\n(b) Did you include complete proofs of all theoretical re-\nsults? NA.\n4. Additionally, if you ran machine learning experiments...\n(a) Did you include the code, data, and instructions\nneeded to reproduce the main experimental results (ei-\nther in the supplemental material or as a URL)? Yes.\n1009\n(b) Did you specify all the training details (e.g., data splits,\nhyperparameters, how they were chosen)? Yes.\n(c) Did you report error bars (e.g., with respect to the ran-\ndom seed after running experiments multiple times)?\nNA.\n(d) Did you include the total amount of compute and the\ntype of resources used (e.g., type of GPUs, internal\ncluster, or cloud provider)? NA.\n(e) Do you justify how the proposed evaluation is suffi-\ncient and appropriate to the claims made? Yes.\n(f) Do you discuss what is \u201cthe cost\u201c of misclassification\nand fault (in)tolerance? NA.\n5. Additionally, if you are using existing assets (e.g., code,\ndata, models) or curating/releasing new assets, without\ncompromising anonymity...\n(a) If your work uses existing assets, did you cite the cre-\nators? Yes.\n(b) Did you mention the license of the assets? NA.\n(c) Did you include any new assets in the supplemental\nmaterial or as a URL? Yes.\n(d) Did you discuss whether and how consent was ob-\ntained from people whose data you\u2019re using/curating?\nYes.\n(e) Did you discuss whether the data you are using/cu-\nrating contains personally identifiable information or\noffensive content? Yes.\n(f) If you are curating or releasing new datasets, did you\ndiscuss how you intend to make your datasets FAIR\n(see FORCE11 (2020))? Yes.\n(g) If you are curating or releasing new datasets, did you\ncreate a Datasheet for the Dataset (see Gebru et al.\n(2021))? Yes.\n6. Additionally, if you used crowdsourcing or conducted\nresearch with human subjects, without compromising\nanonymity...\n(a) Did you include the full text of instructions given to\nparticipants and screenshots? NA.\n(b) Did you describe any potential participant risks, with\nmentions of Institutional Review Board (IRB) ap-\nprovals? NA.\n(c) Did you include the estimated hourly wage paid to\nparticipants and the total amount spent on participant\ncompensation? NA.\n(d) Did you discuss how data is stored, shared, and dei-\ndentified? NA.\nAppendix\nAdditional Dataset Statistics\nTable 8 shows the breakdown of the titles posted by the nine\nmedia outlets. Numbers in parentheses indicate percentages.\nPolitics Subset\nTo curate the training set for political title detection, the\nthree annotators who have labeled our hyperpartisan newstitle dataset further independently label each title as political\nor not. They are asked to label 1 if the title summarizes po-\nlitical news, and 0 if not. A total of 2,200 titles are labeled.\nThe final label of each title is assigned with the consensus\nvotes from three annotators. Inter-annotator agreement mea-\nsured by Cohen\u2019s Kappa is 0.69, suggesting a substantial\nagreement (Landis and Koch 1977). A different BERT-base\nmodel is fine-tuned using this labeled dataset with the same\nimplementation details. On the 200 external validation titles,\nthe accuracy, precision, recall, and F1scores are 0.91, 0.91,\n0.93, and 0.92, respectively. We apply the fine-tuned model\nto the remaining titles and create the politics subset. Ta-\nble 9 shows the summary statistics.\nImplementation Details\nText preprocessing We clean our dataset with the follow-\ning process. First, we remove all Unicode characters and\npunctuation symbols. We then transform all the text into\nlowercase. Additionally, we tokenize and lemmatize each\nword. Finally, the stop words are removed. Tokenization and\nlemmatization are integrated by using functions from the\nnltk library (Bird, Klein, and Loper 2009).\nBERT-base model We fine-tune a vanilla BERT-base\nmodel on 2,000 manually labeled titles for 15 epochs and\nevaluate our model on an external validation set with 200\ntitles. We choose 32 as the batch size for the training set\nand 200 as the batch size for the validation set. We select\nthe Adam optimizer (Kingma and Ba 2017) with \u03b21= 0.9,\n\u03b22= 0.999, and \u03f5= 10\u22128. The learning rate is 2\u00d710\u22125.\nBag-of-words model We first identify all the bigrams in\nour training set. We then build the co-occurrence matrix. The\nfeature is represented by the mean of word vectors, which\nare the row vectors from the co-occurrence matrix. After ex-\ntracting the feature representation of the title, we train the\nmodel using an XGBoost classifier with 200 estimators.\nTF-IDF model We exploit the TF-IDF feature extractor\nfrom the sklearn library (Pedregosa et al. 2011). Next,\nthe TF-IDF features are calculated and fed into an XGBoost\nclassifier with 200 estimators.\nWord2Vec model We implement the Word2Vec model us-\ning the Gensim library (Rehurek and Sojka 2011), which\noffers open-source natural language processing models. The\ntitle representation is the average of each word vector. We\ntrain the model using an XGBoost classifier with 200 esti-\nmators.\nValidity Verification for Hyperpartisan Title\nDetection\nPre-trained models may introduce unknown bias from the\npre-training data (Card et al. 2022). To verify that our results\n(Figure 2 in the main paper) are not significantly influenced\nby BERT-base, we repeat the hyperpartisanship inference for\nall the 1.8 million titles using XGBoost classifiers with bag-\nof-words features and replicate Figure 2. The replication is\nshown in Figure 5. We find that the overall characteristics\nand trends do not change.\n1010\nHyperpartisan Non-hyperpartisan Total\nBloomberg 10,964 (0.08) 120,451 (0.92) 131,415\nCNN 56,519 (0.21) 213,554 (0.79) 270,073\nNBC 26,063 (0.20) 104,585 (0.80) 130,648\nThe New York Times 61,492 (0.11) 492,763 (0.89) 554,255\nChristian Science Monitor 11,674 (0.16) 62,985 (0.84) 74,659\nWall Street Journal 35,104 (0.08) 415,174 (0.92) 450,278\nReason 17,511 (0.26) 49,611 (0.74) 67,122\nThe Federalist 13,372 (0.39) 21,080 (0.61) 34,452\nWashintong Time 35,993 (0.33) 73,729 (0.67) 109,722\nTable 8: Dataset statistics.\n2014 2015 2016 2017 2018 2019 2020 2021 2022\nManually\nlabeled# political titles 135 153 162 176 168 167 175 157 135\n# non-political titles 85 104 96 86 86 79 81 75 80\nModel\nlabeled# political titles 61,734 62,371 72,226 65,675 62,170 61,875 64,156 57,067 52,447\n# non-political titles 191,553 169,511 155,819 131,163 125,740 117,415 125,783 131,411 114,508\nTable 9: Summary statistics of the dataset (political and non-political). The total number of manually labeled titles is 2,200.\nThe total number of titles that are labeled by the model is 1.8 million.\nSearch Keywords of Each Topic\nFor each of the three topics, we curate a list of related key-\nwords and then use this list to classify each title into its\ncorresponding topic. The noun and adjective of each for-\neign country3are used as the search keywords to identify\ntheforeign issue topic. For political system andsocietal is-\nsue, one researcher read the words listed in Table 7 and their\ncorresponding sample titles. The researcher then categorizes\nthem into three groups:(1) political system-related, (2) so-\ncietal issue-related, and (3) neither. In particular, for each\nword, twenty titles that contain this word are sampled. If\nmore than 75% of the samples (i.e., titles) belong to either\npolitical system-related or societal issue-related, this word\nis categorized in that group. If neither category is dominant\n(i.e., whose proportion of titles is equal to or greater than\n75%) in these samples, the word will not be included in any\nkeyword lists. As a result, the keyword list used to detect\nthe titles about political system is composed of \u201cTrump\u201d,\n\u201cObama\u201d, \u201cObamacare\u201d, \u201cClinton\u201d, \u201cdemocrat\u201d, \u201crepubli-\ncan\u201d, \u201cGOP\u201d, \u201cdebate\u201d, \u201ccampaign\u201d, \u201cparty\u201d, \u201cvote\u201d, \u201celec-\ntion\u201d, \u201cBiden\u201d, \u201csenate\u201d. The keyword list used to detect\nthe titles regarding societal issue includes \u201cCOVID\u201d, \u201cpan-\ndemic\u201d, \u201cCoronavirus\u201d, \u201cgun\u201d, \u201claw\u201d, \u201csupreme\u201d, \u201cschool\u201d,\n\u201ctax\u201d, \u201cclimate\u201d. Note that all words are converted to lower-\ncase during the search so it is not case-sensitive.\nTo verify the performance of this keyword search method,\ntwo annotators independently label 50 sample titles of each\ntopic. The Cohen\u2019s Kappa for foreign issue andsocietal is-\nsueis 0.88 and 0.43, respectively. When the two annotators\ndisagree on a title, we ask them to engage in a thorough\ndiscussion to reach a consensus. These labels are then used\n3https://www.englishclub.com/vocabulary/world-countries-\nnationality.phpModel Accuracy Precision Recall F1\nforeign issue 0.96 0.92 1.00 0.96\npolitical system 0.92 0.96 0.89 0.92\nsocietal issue 0.72 0.60 0.79 0.68\nTable 10: Performance of the keyword search method for\ntopic detection.\nto verify the keyword search performance. Note that the la-\nbels we use to verify the performance of detecting titles be-\nlonging to political system are from the politics subset\nwe construct before. Table 10 shows the performance of the\nkeyword search method. Although the accuracy for societal\nissue detection is not very high, the keyword search method\nperforms well in foreign issue andpolitical system detection.\nTo further ensure the robustness of our method, we leave one\nword out of the keyword list and conduct the same analysis\non topic distribution again. The new log of frequency ratio\nis plotted as black dots in Figure 3. We can observe that the\npatterns mostly remain the same.\nFurther Discussion on Potential Broader Impact\nand Ethical Considerations\nDifferent from previous studies of hyperpartisanship in\nnews, we introduce a second type of news which describes\nevents that indicate an underlying polarized climate. One po-\ntential negative outcome could be that the findings of our\nstudy may be interpreted or quoted based on partial state-\nments or without caution. Therefore, we discuss the limita-\ntions and how to interpret the results of our study. In terms\nof the ethical considerations about our dataset, all raw data\nfrom the selected mainstream media are publicly available.\n1011\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n202300.050.10.150.20.250.30.35\nleft media\nright media\ncenter mediaMonthly Hyper-title percentageObama Trump Biden(a) Percentage of hyperpartisan titles.\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023123456\nleft media\nright media\ncenter mediaRelative Monthly Hyper-title shiftObama Trump Biden (b) Relative change in percentage.\nFigure 5: Usage of hyperpartisan titles of Left, Central, and Right media between 2014 to 2022. Replication of Figure 2 in the\nmain paper, using XGBoost classifiers with bag-of-words features.\nWe develop this dataset to complement the hyperpartisan\nnews datasets of previous studies.\n1012", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Computational assessment of hyperpartisanship in news titles", "author": ["H Lyu", "J Pan", "Z Wang", "J Luo"], "pub_year": "2024", "venue": "\u2026 of the International AAAI Conference on \u2026", "abstract": "The growing trend of partisanship in news reporting can have a negative impact on society.  Assessing the level of partisanship in news headlines is particularly crucial, as they are"}, "filled": false, "gsrank": 208, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/31368", "author_id": ["tPhwyYsAAAAJ", "zUWKW5AAAAAJ", "13MQzsgAAAAJ", "CcbnBvgAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:da1Uw76MIx8J:scholar.google.com/&output=cite&scirp=207&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D200%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=da1Uw76MIx8J&ei=KbWsaMZzjoiJ6g_SwpG4CQ&json=", "num_citations": 9, "citedby_url": "/scholar?cites=2243791790309420405&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:da1Uw76MIx8J:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/31368/33528"}}, {"title": "Higher ground? How groundtruth labeling impacts our understanding of fake news about the 2016 US presidential nominees", "year": "2020", "pdf_data": "Proceedings of the Fourteenth International AAAI Conference on Web and Social Media (ICWSM 2020)\nHigher Ground? How Groundtruth Labeling Impacts\nOur Understanding of Fake News about the 2016 U.S. Presidential Nominees\nLia Bozarth, Aparajita Saraf, Ceren Budak\nUniversity of Michigan, School of Information\n105 S State St\nAnn Arbor, MI 48109\nlbozarth,apsaraf,cbudak@umich.edu\nAbstract\nThe spread of fake news on social media platforms has gar-\nnered much public attention and apprehension. Consequently,\nboth the tech industry and academia alike are investing in-creased effort to understand, detect, and curb fake news. Yet,\nresearchers differ in what they consider to be fake news sites.\nIn this paper, we \ufb01rst aggregate 5 lists of fake and 3 ofmainstream news sites published by experts and reputable or-\nganizations. Then, focusing on tweets about the democratic\n(Hillary Clinton) and republican (Donald Trump) nomineesin the 2016 U.S. presidential election, we use each pair of fake\nand traditional news lists as an independent \u201cgroundtruth\u201d to\nexamine i) the prevalence, ii) temporal characteristics and iii)the agenda-setting differences between fake and traditional\nnews sites. We observe that depending on the groundtruth,\nthe prevalence of fake news varies signi\ufb01cantly. However,the temporal trends and agenda-setting differences between\nfake and mainstream news sites remain moderately consistent\nacross different groundtruth lists.\nIntroduction\nFollowing the 2016 U.S. presidential election, fake news\nswiftly became a topic of interest and scrutiny for politicalpundits, media scholars, and the general public (Silverman\n2016; Guo and V argo 2018)\u2014driving increased research ef-\nforts on fake news. The research community has been strug-gling to de\ufb01ne fake news. While there is currently no con-sensus on the topic, leading scholars advocate \u201c... focusingon the original sources\u2014the publishers\u2014rather than indi-vidual stories, because we view the de\ufb01ning element of fakenews to be the intent and processes of the publisher.\u201d (Lazer,Baum, and others 2018). Yet, there is currently no agree-ment on which news producers are fake news producers ei-ther (Tandoc Jr, Lim, and Ling 2018).\nConsequently, there are a number of lists with opaque\ngeneration processes (Zimdars 2016; Guo and V argo 2018;Allcott and Gentzkow 2017; V an Zandt, Dave 2018; Poli-tifact staff 2018; Shao et al. 2016) being used by studieswith important implications such as examining fake newscascading behavior (Allcott and Gentzkow 2017; Allcott,\nCopyright c/circlecopyrt2020, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.Gentzkow, and Y u 2018), assessing agenda-setting powers\nof fake and traditional news sites (V argo, Guo, and Amazeen2018; Guo and V argo 2018; Mukerji 2018) or characteriz-ing changes in fake news trends (Allcott, Gentzkow, and Y u2018). How robust are these studies, particularly the ones fo-cused on the 2016 presidential elections, with respect to thechoice of groundtruth lists that de\ufb01ne which publishers areproducers of fake or traditional news? We set out to answerthis question through meta-analysis\u2014a methodology used toovercome the limitations of any single study by consolidat-ing multiple data sources or studies that aim to address thesame research questions, and determining their similaritiesand differences (Boulianne 2015).\nHere, we aggregate 5 lists of fake and 3 of mainstream\nnews sites contributed by both the academia and otherreputable sources (Poynter Institute 2019; Zimdars 2016;\nWang 2017; White 2018; Leetaru and Schrodt 2013; V anZandt, Dave 2018). We \ufb01rst review the labeling processesof these lists, assess their similarities and temporal changes.We then determine how selection impacts prevalence, tem-poral trends, and agenda-setting analysis of fake news aboutthe 2016 presidential nominees.\nWe \ufb01rst examine prevalence given the divergent \ufb01nd-\nings in recent work (Silverman 2016; Allcott and Gentzkow2017; Bovet and Makse 2019)\n1. A careful analysis of preva-\nlence can also help lawmakers/platforms in better prioritiz-ing anti-misinformation actions (Lazer, Baum, and others2018). Next, we investigate the robustness of trend anal-ysis since having an accurate assessment of temporal pat-terns of fake news can assist lawmakers/platforms in eval-uating whether their efforts to curtail fake news is success-ful (Allcott, Gentzkow, and Y u 2018). Finally, we turn totopic analysis. Agenda-setting theory (McCombs and Shaw1972) postulates that the most frequently covered topics arewhat the general public considers the most important. Re-latedly, fake news sites could have led voters to re-evaluateissue importance and nominee viability by prioritizing cer-\n1For instance, Silverman (2016) suggests that fake news articles\ngarnered \u201c...sometimes more than twice as many as legitimate newsscoops in major outlets\u201d. Whereas, Allcott and Gentzkow (2017)\nsuggests that an average adult only saw and remembered 1.14 fake\nnews articles during the 2016 presidential election.\n48\ntain topics over others (Guo and V argo 2018). Determining\nthe robustness of fake news agenda-setting effects is conse-\nquential to media effects research.\nOur paper makes the following contributions:\n\u2022We demonstrate that existing fake news lists share very\nfew domains in common. Additionally, popular fake newssites are more likely to be included (and included earlier)than unpopular ones. Further, domains in hate ,junksci ,\nclickbait subcategories are less likely to be included by\nlists compared to domains in the fake subcategory.\n\u2022Based on the groundtruth choice, the prevalence of fake\nnews varies considerably (2%-to-40%). This discrepancyis mostly due to the inclusion or exclusion of domainswith mixed factualness.\n\u2022We show that the time-series correlation between most\nlists is high, especially for the general election periodwhere we observe an increase in fake news prevalenceregardless of groundtruth choice. Further, we also show\nthat scheduled events contribute to a temporary drop in\nfake news prevalence. Observations for scandals are notas robust and are dependent on selection.\n\u2022Studying the agenda-setting priority difference between\nfake and traditional news sites, we observe that whether atopic (e.g., immigration) was more central to the coveragefrom fake news outlets compared to the traditional newssites is robust to the choice of groundtruth.\n\u2022Finally, groundtruth selection of mainstream news lists\nhas a very limited impact on all downstream analyses.\nTo summarize, through meta-analysis, we characterize\nwhat makes a domain a fake news producer to some but tonot others. We show that the use of different groundtruthsets can account for diverging fake news prevalence \ufb01nd-ings. Further, despite the varied labeling and validation pro-\ncedures used and domains listed by fake news annotators,\nthe groundtruth selection has a limited to modest impact onstudies reporting on the behaviors of fake news sites (e.g.,agenda-setting).\nRelated Work\nResearchers have extensively documented the negative im-pact fake news has on the quality of civic engagement,healthcare, markets, and disaster management (Rapoza2017; Marcon, Murdoch, and Caul\ufb01eld 2017; Palen andHughes 2018), both within the United States (Silver-man 2016; Main 2018; Starbird 2017) and internation-ally (Kucharski 2016; Alimonti and V eridiana 2018).\nMany studies aim to distinguish false content from credi-\nble news articles at scale. Prior studies have identi\ufb01ed differ-ences in i) linguistic patterns such as punctuation and wordchoices (Potthast et al. 2017), ii) auxiliary data (Shu et al.2017; Shu, Wang, and Liu 2018), iii) network cascading at-tributes such as depth, breadth, and speed (Shao et al. 2017;Allcott, Gentzkow, and Y u 2018; V osoughi, Roy, and Aral2018), and iv) agenda-setting priorities (V argo, Guo, andAmazeen 2018). These differences are then used to buildautomated fake news detection platforms (Horne and Adali2017) in an effort to curtail fake news.However, efforts to study fake news and to diminish its\nspread are dif\ufb01cult (Budak, Agrawal, and El Abbadi 2011),partly because scholars do not have a consistent de\ufb01nitionfor fake news (Tandoc Jr, Lim, and Ling 2018; Wardle2017). For instance, Tandoc et al. identify 2 primary dimen-sions of fake news: levels of facticity and deception. War-dle, on the other hand, conceptualizes fake news using 3 dis-tinct dimensions: type of content, motivation, and dissemi-nation method. Moreover, existing fake news labelsets (Poli-\ntifact staff 2018; Zimdars 2016; V an Zandt, Dave 2018;White 2018; Leetaru and Schrodt 2013) have considerably\ndifferent annotation and categorization procedures.\nWe \ufb01rst consolidate existing groundtruth labelsets of fake\nand mainstream news sites that have been generated by var-ious groups. We then assess whether and to what extent dif-ferences in groundtruth selection affect downstream studies.\nData\nWe use 3 types of data: i) lists of fake and traditional newssites, ii) tweets about the two nominees during the 2016U.S. presidential election, and iii) webpages, or news arti-cles, corresponding to the URLs shared in those tweets.\nFake and Traditional News Site Lists: We collect 5 dis-\ntinct fake news lists and 3 traditional news lists from boththe academia and the press (Zimdars 2016; Guo and V argo2018; Allcott and Gentzkow 2017; V an Zandt, Dave 2018;Politifact staff 2018; Shao et al. 2016), resulting in 1884 ag-gregated fake news sites and 8238 traditional news sites. Wedescribe and evaluate these lists in Section .\nTwitter Data: The social media dataset is described in de-\ntail in Bode et al. ( 2020). The data collection was performed\nusing Sysomos MAP - a social media search engine that in-cludes access to all tweets (Twitter \ufb01rehose) going back oneyear. For any given day between May 23, 2014, and January1, 2017, our dataset includes i.) 5,000 tweets randomly sam-pled from all tweets that included the keyword \u201cTrump\u201d, andii) 5,000 tweets similarly sampled from all that mentioned\u201cClinton\u201d. The resulting dataset includes approximately 4.8million tweets each about Donald Trump and Hillary Clin-ton respectively.\nWebpages (News Articles): The webpages dataset (Budak\n2019) includes the content of the webpages shared in theTwitter dataset described above. For each tweet with an ex-ternal URL, the dataset includes a record with: i) the short-ened URL, ii) the original URL, iii) domain name, iv) ti-tle of the document, v) body of the document, (vi) the dateof the tweet, vii) Twitter account id of the user sharing theURL, and viii) a binary categorization that indicates whetherthis tweet is about Clinton or Trump. We remove the recordswith domains not listed in the aforementioned 10K+ newssites and \ufb01lter out the tweets posted before 12/01/2015 orafter 01/01/2017. We derive approximately 244K unique ar-ticles shared by 1M Tweets on Twitter.\nMeta-review\nIn this section, we \ufb01rst examine the characteristics and ap-plications of the available lists of fake and traditional newswebsites. Then, focusing on fake news lists, we assess their\n49\ncommonalities and differences and explore the characteris-\ntics of websites that are correlated with them being included\nin or excluded from any given list. Finally, we explore fake\nnews domains\u2019 likelihood of becoming defunct.\nLists of Fake News Sites: We collect 5 fake news lists.\n1.ZDR : We refer to the set of fake news websites annotated\nby Zimdars et al. (2016) as ZDR .ZDR tags each website\nwith at most 3 of the following 10 subcategories: fake,\nsatire, bias, conspiracy, rumor , state, junksci, hate, click-\nbait, and unreliable2. Among these subcategories, un-\nreliable and clickbait are noted to have \u201cmixed\u201d factual-\nness.\n2.MBFC : The set of sites labeled by Media Bias/Fact\nCheck \u2013an independent online media outlet maintained by\na small team of researchers and journalists (V an Zandt,Dave 2018)\u2013will be referred to as MBFC . Similar to ZDR ,\nMBFC assigns domains to subcategories: fake, conspir-\nacy, satire . Moreover, it also labels websites with political\nideology ( extreme left, left, center , right, extreme right,\nunlabeled ) and rates websites by their factualness ( low,\nmixed, high ).\n3.POLIT : The staff of PolitiFact, in collaboration with\nFacebook, identi\ufb01ed the list of most-shared fake newssites on Facebook during the 2016 election (Politifact\nstaff 2018). This list\u2013referred to as POLIT \u2013labels sites\ntofake, imposter , some fake ,o r parody .\n4.DDOT : This list is shared by the Daily Dot , a mainstream\nonline news site (White 2018). This list is largely createdby referencing other pre-existing fake news lists and doesnot contain subcategories.\n5.AGZ: Allcott et al. (2018) aggregated the following \ufb01ve\nlists:POLIT , Grinberg et al. (2018), Silverman (2016),\nSchaedel (2018), and Guess et al.( 2018). This list is re-ferred to as AGZ . The subcategorization process in AGZ is\nsomewhat complex. For instance, POLIT subcategories\nwere ignored and all the domains were relabeled as fake .\nHowever, the subcategories black, red, orange (black:\ncompletely false, red/orange: has unreliable claims) ofGrinberg et al. (2018) were maintained. Finally, all do-mains from other referenced lists were labeled as fake .\nA synthesis of these lists reveals that 4 out of the 5 lists\nshare 2 common subcategories: i) a subcategory containingdomains with mixed factualness, and ii) a fake subcategory\n(entirely fabricated). This consistency suggests that mixed\norfake domains are conceptually distinct from others. Thus,\nstudies should take this distinction into consideration.\nLists of Traditional News Sites: We consider the follow-\ning 3 traditional news lists.\n1.ALEXA : Alexa is an online domain directory owned by\nAmazon (Wikipedia contributors 2019). We crawl for allthe websites listed under Alexa\u2019s News category.\n2Zimdars et al. also list a small subset of domains as political,\nreliable and unidenti\ufb01ed which are not fake news sites and there-\nfore removed from subsequent analyses.2.MBFC(T) :Media Bias/Fact Check also lists a large set of\ntraditional news sites. We refer to this list as MBFC(T) .\n3.VARGO : This list contains fact-based news websites com-\npiled through manual content analysis of the top newsmedia websites found in GDELT\u2019s global knowledgegraph (V argo, Guo, and Amazeen 2018).\nConsidering fake news domain list quantities, DDOT has\nthe fewest with 175 domains, followed by POLIT (327) andAGZ (673). ZDR (786) and MBFC (1183) are the largestlists. Traditional news site list quantities are MBFC(T)(1685), V ARGO (2649) and ALEXA (5497). Table 1 pro-vides a summary of the annotation processes and the usesof these lists. As is evident from the second column ( An-\nnotation and Quality ), most lists do not have a transparent\nannotation and quality evaluation procedure. Perhaps due tothe absence of such robust procedures, there is no consen-sus on which of these lists should be treated as the ultimategroundtruth. This is clear from the third column ( Applica-\ntions ). More than 20 studies have used these lists of fake\nand traditional news sites. The lists are used for various im-portant purposes such as building automated fake news clas-si\ufb01ers or assessing the impact of fake news on the 2016 elec-tion. This highlights the importance of identifying similari-ties and differences between the lists.\nThus, we conduct downstream analysis using differ-\nent groundtruth pairs (f,t)where f\u2208{ZDR,MBFC ,POLIT ,\nDDOT ,AGZ}, and t\u2208{ALEXA ,MBFC (T),VARGO}.\nList Overlap: Here, we identify the overlap among the 5\nfake news lists using 2 metrics. We \ufb01rst calculate the fractionof websites being present in at least 2 of the 5 lists, then 3,then 4. We observe that close to 50% of all domains are onlyincluded in a single list. In fact, only 5.7% of the domainsare included by all fake lists. Second, we also calculate theJaccard similarity score (Goodall 1966) of each pair of lists.We observe that more than half of the 15 pairs of fake newslists have a similarity of <=0.1. We note that MBFC and\nDDOT have the lowest Jaccard similarity score of 0.08, and\nAGZ andPOLIT have the highest score of 0.48.\nThe extent of dissimilarity between the lists is surprising,\nand we identify four potential measures: i) popularity , de-\n\ufb01ned as the number of times a URL from a given domain isshared in the Twitter dataset, ii) age (we collect data using\nwhois.com, an online domain registration service), iii) sub-\ncategory , as de\ufb01ned by Zimdars et al. (2016)\n3, and \ufb01nally\nvi) ideology , as de\ufb01ned by Media Bias/Fact Check (V an\nZandt, Dave 2018)4. The details of the regression model and\nanalysis are provided in the Appendix. We observe that thepopularity of a website is positively correlated with beingincluded in lists (though the variable is not signi\ufb01cant for\n3Zimdars et al. (2016) have the most comprehensive subcate-\ngories and a coherent labeling guideline. Subcategory is unknown\nif a domain is not listed by Zimdars et al. (2016).\n4Ideology is unknown if the domain is not listed by Media\nBias/Fact Check (V an Zandt, Dave 2018) or if Media Bias/Fact\nCheck didn\u2019t mark it with an ideological label (approximately\n18.6% domains). Here we collapse MBFC \u2019sextreme left and left\ncategories into single liberal class. Same for conservative .\n50\nList Annotation and Quality Applications\nDDOT no information build automated fake news trackers (Shao et al. 2016; Helmstetter and Paulheim 2018), assess agenda-\nsetting powers of fake and traditional news sites (V argo, Guo, and Amazeen 2018; Guo and V argo 2018;Mukerji 2018)\nAGZ\nauthors aggregate lists generated by\nothers, and then use various combina-tions of these list for result robustness\ncheckassess impact on election, examine fake news cascading behavior (Allcott and Gentzkow 2017); exam-\nining fake news trend (Allcott, Gentzkow, and Y u 2018)\nMBFC annotated by staff; authors examine\nwording, source, story selection, and\npolitical af\ufb01liationstudies of the Alt-right (Main 2018), globalism (Starbird 2017), the virality of fake news (Darwish,\nMagdy, and Zanouda 2017), information literacy (Farmer 2017), polarization (Croft and Moore 2017),\nand information quality (Nelimarkka, Laaksonen, and Semaan 2018)\nPOLIT no information study the diffusion of fake news on social media (Allcott, Gentzkow, and Y u 2018), information liter-\nacy (Mukerji 2018), automate fake news detection (Granskogen 2018)\nZDR annotated by scholars and librarians;domain name, about us page, writing\nstyle, aesthetics, and social media ac-\ncounts are among the examined charac-teristicsexamine network cascading behavior difference between fake and real news articles during the 2016Election (Allcott and Gentzkow 2017; Allcott, Gentzkow, and Y u 2018), build fake news classi-\n\ufb01ers (Shao et al. 2016; Horne and Adali 2017; Horne et al. 2018), assess agenda-setting powers of fake\nand real news sites (V argo, Guo, and Amazeen 2018; Guo and V argo 2018), impact assessment (Rini2017; Figueira and Oliveira 2017; Doshi et al. 2018), ethics and policy (Farte and Obada 2018;\nKoulolias et al. 2018)\nMBFC-T see MBFC see MBFC\nALEXA no information examine cascading behavior differences between fake and traditional news articles (Allcott andGentzkow 2017; Allcott, Gentzkow, and Y u 2018), news sharing behavior in right-leaning echo cham-\nbers (Lima et al. 2018)\nV ARGO\nannotated by authors; intercoder relia-bility of 0.988 Krippendorff\u2019s alpha. assess agenda-setting power of fake and real news sites (Guo and V argo 2018; V argo, Guo, and Amazeen2018)\nTable 1: Traditional and Fake News Lists and Their Applications. Some studies below use multiple sources.\nDDOT andPOLIT ). Further, ideology is not predictive of\nwhether a domain will be included by lists except for AGZ\n(conservative-leaning domains are more likely to be listed).Finally, we observe that compared to domains subcatego-rized as fake byZDR , domains that belong to other subcate-\ngories are uniformly less likely to be present in other lists.\nDomain Addition and Removal through Time: We fur-\nther examined how the lists changed over time and foundthe types of changes to be largely consistent. For the listswe have temporal information for ( MBFC ,ZDR , andDDOT ),\nwe observe the following: i) they include more popular do-mains earlier on\u2013adding the less popular ones later, ii.) theyinclude the sites that publish fake news earlier compared tosites that publish less problematic categories such as click-\nbait and bias , and iii.) interestingly, sites labeled as satirical\nare added early on to the lists, perhaps due to the ease ofidenti\ufb01cation. For the regression model for temporal analy-sis, we refer the reader to the Appendix.\nBesides the addition of domains through time, we also\nlooked into i) domain removals and ii) domains withchanged subcategories. We observe very few to no re-movals\n5; same for changes of subcategories.\nActive and Defunct Domains: Once \ufb02agged as fake news\nwebsites, these publishers may aim to bypass fact-checkingsystems by using simple tricks such as abandoning their do-mains and migrating to new ones (Funke 2019). We observethat 68.9% of all websites listed under POLIT are no longer\nactive\u2014the highest defunct rate among all lists\n6. Further,\n5The exception being DDOT : in late 2016, DDOT contained 98\nwebsites; it then removed a substantial number of sites and reducedits size to 25 in mid-2017; its latest version has a size of 175. No\nexplanation was given for each change.\n6We use scrapy (Mitchell 2018), a Python crawler library, to\nscrape website homepages. Domains timed-out during scraping, orreturned 404 errors (Not Found), 502 (Bad Gateway), 503 (ServiceUnavailable), et cetera are labeled as defunct.AGZ andDDOT have comparable defunct rates of 64% and\n62% respectively. In comparison, ZDR andMBFC have con-\nsiderably lower rates of 40 .6% and 30 .9%. Similar to the\nprevious section, we assess a domain\u2019s likelihood of beingdefunct as a function of its popularity, age, subcategory , and\nideology (see Appendix).\nWe show that older, more popular, and ideologically con-\nservative or ambiguous domains are less likely to be defunct.Further, compared to domains subcategorized by ZDR as\nfake , domains with other subcategories (e.g., junksci ,satire )\nare less likely to be defunct. Thus, one possible explanationthatZDR andMBFC have lower defunct rates is that both\nsources include more domains that do not belong to the sub-category fake (e.g., unreliable and conspiracy websites), and\nthese types of domains are targeted less frequently by fact-checking platforms and thus have less incentive to migrate.\nGroundTruth Selection and Downstream\nConsequences\nA meta-review of the fake news lists in the previous sec-\ntion demonstrates marked differences between these lists.How do these differences affect the downstream analysis?We aim to answer this question in this section. To that end,we \ufb01rst assess how groundtruth selection impacts the per-ceived prevalence of fake news during the 2016 election.Next, we measure the similarities or dissimilarities of fakenews time-series generated using different groundtruth pairs(f,t). Finally, we determine whether there are any marked\ndifferences in agenda-setting priorities of fake and real newssites due to choice in groundtruth.\nPrevalence\nHere, we de\ufb01ne prevalence as the fraction of tweets con-\ntaining URLs that are from fake news sites. We examine towhat extent groundtruth difference impacts perceived per-vasiveness of fake news using 3 distinct boundary con-ditions (strictness in de\ufb01nition) for each fake news list:\n51\nall,all-except-mixed , and fake . More speci\ufb01cally, given a\ngroundtruth pair (f,t), we write fallas the entire set of do-\nmains in f,fmixed and ffa k e as the set of domains in fallthat\nbelong to subcategories with mixed factualness and the sub-category fake respectively. We then calculate prevalence as\n|fall|s\n|fall|s+|tall|s,|fall|s\u2212|fmixed|s\n|fall|s+|tall|s, and|ffa k e|1|s\n|fall|s+|tall|swhere|fall|sis the\nnumber of tweets, or shares, contributed by fall.\nResults are shown in Figure 1. For the allcondition, based\non(f,t), fake news could amount to be more than 40% of\ntotal news shares or as low as less than 3%. Further, for ro-bustness check (details in Section ), we also rede\ufb01ne preva-lence as the fraction of unique accounts that posted at least1 fake news tweet and observe comparable results.\nAdditionally, if we discard all domains with mixed factu-\nalness, prevalence drops substantially to between 1 .3% and\n20.1%. Further, the fraction of fake news are comparable for\nthe conditions all-except-mixed and fake except for ZDR .I n\nother words, domains that are low in quality but not neces-sarily fake, (i.e. mixed ), contribute to a large fraction of total\narticles shared, and domains that are neither fake nor mixed\nare not as popular on Twitter. To further illustrate this point,we calculate the average number of tweet shares per domainfor each type of subcategories. We observe that mixed do-\nmains have an average of 0.7K to 2.4K tweet shares, 4 to 5times that of the average of all subcategories for each list;in fact, mixed domains, on average, are considerably more\npopular than traditional news outlets which had an averagetweet share of 0.15K to 0.66K.\nOur analysis helps explain the divergent \ufb01ndings in the\nliterature. While some studies raise signi\ufb01cant concernsabout the prevalence of fake news (Silverman 2016), oth-ers claimed limited prevalence (Allcott, Gentzkow, and Y u2018)\n7. Here, similar to work by Grinberg et al. (2018)\nwhich showed that the analysis on fake news exposure issigni\ufb01cantly dependent on whether domains of mixed factu-alness were included, we see that drastically divergent con-clusions can be reached even with the same Twitter data as afunction of the fake and traditional news lists and fake-nessde\ufb01nitions (e.g., fake, mixed) one chooses to use. In sum,the more comprehensive a fake news list is, the higher thefake news prevalence.\nTime-Series Analysis\nIn this section, we \ufb01rst construct a time-series represent-ing the fraction of fake news over all available news perday for each (f,t)from 3 different time periods (pri-\nmary, general election, and after election) accounting foronly Clinton tweets, only Trump tweets, and all tweets(for both nominee). Speci\ufb01cally, for each election phase\niwhere i\u2208{ primary ,general election ,a f ter election },\n7More speci\ufb01cally, Silverman (2016) selected the top 20 highest\nperforming fake news stories from hundreds of known fake news\nsites and demonstrated, on aggregate, they had a larger number of\ntweet shares compared to the top 20 news stories selected from\nthe top 13 traditional news sites. In comparison, Allcott et al. (All-cott, Gentzkow, and Y u 2018) aggregated 673 fake news sites and\nshowed that an average adult saw and remembered a single fake\nnews story.\nFigure 1: Fraction of Fake News. The x-axis indicates fake\nnews lists. Each list is divided into subsets (marked by color)ofall,all-except-mixed (not including domains in mixed\nsubcategory), and fake (only domains in fake subcategory).\nThe shape of each point denotes mainstream news lists, andy-axis is the fraction of tweets contain fake news.\ngiven a groundtruth pair (f,t)and nominee n(where n\u2208\n{clinton ,trump ,both}), we write |f|\ns\n0,n, and|t|s\n0,nas the to-\ntal number of tweets, or shares, that mention nand contain\nURLs from fortat day 08. We then derive the time-series\nPi(f,t,n)={|f|s\n0,n\n|f|s\n0,n+|t|s\n0,n,|f|s\n1,n\n|f|s\n1,n+|t|s\n1,n...}.\nNext, we then compare these time-series from 3 distinct\ndimensions: i) correlation, ii) trend, and iii) effects of exter-nal events. For example, one might be interested to knowhow consistent the fake news trend is over time for dis-cussions about Clinton ( n) during the primary ( i) when us-\ning(MBFC ,ALEXA )or(AGZ,ALEXA )as the groundtruth pair.\nFor that, we can use P\nprimary(MBFC ,ALEXA ,Clinton )and\nPprimary(AGZ,ALEXA ,Clinton ). Furthermore, instead of com-\nparing only 2 pairs, we can compute and contrast the \ufb01nd-ings for all 15 pairs to examine overall consistency of Clin-ton conversations during the primary season.\nTime-series Correlation: We calculate correlation sepa-\nrately for each time period and nominee. For each nand i,\ngiven 2 groundtruth pairs (f1,t1)and(f2,t2)where f\n1/negationslash=f2\nort1/negationslash=t2, we compute the maximum normalized cross cor-\nrelation coef\ufb01cient and the corresponding time lag (Haugh1976) of P\ni(f1,t1,n)and Pi(f2,t2,n).\nWe observe that the highest correlation scores of all pair-\nwise comparisons occur at 0 lag, indicating that no sin-gle time-series is \u201cahead\u201d or \u201cbehind\u201d others. Correlationscores are plotted in Figure 2a. Normalized coef\ufb01cientshave a range between {\u22121,1}. As shown, correlation for\nP(f\n1,t1,n)and P(f2,t2,n)is the highest when f1\u2261f2but\nt1/negationslash=t2, indicating traditional news list selection (choosing\nALEXA ,MBFC(T) ,o rVARGO ) has little impact here. Fur-\nther, we also note that certain fake news lists have consid-erably high correlation (e.g., ZDR andMBFC have correla-\ntion consistently higher than 0.9). Yet, DDOT diverges sig-\nni\ufb01cantly from others.\nWe further observe that the correlation is highest for the\ngeneral election season (median correlation between the\n8Here, we pick 2015-12-01, 2016-06-15, and 2016-11-09 as\nday 0 for primary, general election, and after election; and 2016-\n06-21, 2016-11-15, and 2017-01-01 as the last day.\n52\n(a) Cross correlation coef\ufb01-\ncient between pairs of lists foreach time period. Circle and\ntriangle points are the median\ncoef scores when including orexcluding DDOT .\n(b) Level effect of scandals and\nscheduled events.\nFigure 2: Time-series analysis results for correlation and ef-\nfects of external events .\npairs for each nominee nare all above 0.8). Most efforts\nin fake news detection were motivated by the spread of fakenews during the 2016 presidential election. This providesone potential explanation\u2014fact-checkers and scholars couldhave had a stronger emphasis on the publishers that were ac-tive in this time frame, resulting in higher agreement.\nperiod nominee majority\ntrendmajority\nfracmedian \u03b21 least.congruent\nprimary trump positive 0.67 0.04% DDOT ,POLTI\nclinton stationary 0.60 NA\nboth stationary,\npositive0.47 NA, 0.02%\ngeneral trump positive 1.00 0.03% NA\nelection clinton positive 1.00 0.09%\nboth positive 1.00 0.07%\nafter trump negative 0.67 -0.01% AGZ ,POLTI\nelection clinton positive 0.80 0.07%\nboth positive 0.67 0.05%\nTable 2: Fraction of Fake News Per Day Time-series TrendUsing Different GroundTruth. Column majority trend de-\nnotes the trend observed by a majority of pairs, column ma-\njority frac is the fraction of pairs in majority.\nTrend: Similar to prior work (Allcott, Gentzkow, and Y u\n2018; Lazer, Baum, and others 2018), we are also inter-ested in assessing whether there was an increase in fakenews prevalence and to what degree \ufb01ndings would dependon the choice of groundtruth pairs. Here, we \ufb01rst employseasonal decomposition using moving averages (Beveridgeand Nelson 1981) to deconstruct each time-series P\ni(f,t,n)\ninto its components trend ,seasonal , and residual . This is\nto remove the seasonality and residuals from the originaltime-series. Next, we apply both Augmented Dickey-Fuller(ADF) and Kwiatkowski Phillips Schmidt Shin (KPSS), 2commonly used methods to test for stationarity (Charemza\nand Syczewska 1998) on the trend component of P\ni(f,t,n).\nIf any one of the tests show that unit root is non-stationary,we run the linear regression model yi(f,t,n)=\u03b20+\u03b21\u2217T+\n\u03b5(where yi(f,t,n)contains the values from trend ,a n dTi s\nthe time elapsed since the start of the time-series). Here, apositive \u03b2\n1suggests a rise of fake news. Finally, we assess\nwhether trend analysis results for each nominee nand time\nperiod iusing all pairs (f,t)are consistent.\nResults are on Table 2. Column \u201cmajority trend\u201d shows\nthe trend result shared by the largest fraction of groundtruthpairs and column \u201cmajority frac\u201d is the size of that frac-tion. Additional, median \u03b2\n1scores indicate the median esti-\nmated percentage increase, over all congruent pairs, of fakenews per day. As shown, conclusions for the general elec-\ntion are remarkably consistent: all lists pairs indicate an in-\ncrease in fake news. In other words, regardless of whethergroundtruth choice is (MBFC ,ALEXA ),(AGZ,VARGO ),o ra n y\nof the other combinations, we repeatedly see a positive trendfor fake news in the general elections. Results for other elec-tion phases are less congruent (e.g., 80% pairs show a pos-itive trend for Clinton-related fake news after the election,but the other 20% show stationarity or a negative trend). Weobserve that DDOT andPOLIT disagree the most with other\nfake news lists (measured by the number of times a list di-verges from \u201cmajority vote\u201d) in primary , whereas it\u2019s AGZ\nandPOLIT inafter election .\nIn sum, similar to time-series correlation analysis, we see\na higher consistency for the general election period com-\npared to primary and after election . Accurate trend analysis\nis vital given that it impacts platform owners and policymak-ers\u2019 decision-making. Facebook\u2019s fact-checking system tar-gets domains listed in POLIT andAGZ , and consequently\n(Allcott, Gentzkow, and Y u 2018) shows signi\ufb01cantly re-duced content from these domains on Facebook over time.We do not have access to Facebook data and therefore can-not check the robustness of their curtailing efforts. Yet, wedo demonstrate that caution must be taken when examiningfake news spread outside of the general election period.\nEffects of External Events: Many prior studies exam-\nined media coverage of i) unexpected political events suchas scandals (Puglisi and Snyder Jr 2011) as well as ii)scheduled high-pro\ufb01le events such as the presidential de-bates (Scheufele, Kim, and Brossard 2007). Such events areshown to have important effects on campaign news cover-age. Here, we examine whether these 2 distinct categoriesof events have a temporary effect on the prevalence of fakenews, speci\ufb01cally in the general election period.\nWe \ufb01rst obtain a list of scandals and planned key events of\nTrump, Clinton, or both that occurred in the general electionfrom ABC News and The Guardian . The list, ordered chron-\nically, includes: Republican nomination (07/18), Democratnomination (07/28), Clinton \u201cdeplorable\u201d and \u201cpneumonia\u201dscandals (09/09), \ufb01rst debate (09/26), Clinton email involv-ing Wikileaks and Trump Hollywood tape scandals (10/07),second debate (10/09), Clinton email scandals involvingthe FBI (10/28, 11/06), and \ufb01nally election day (11/08).Here, nominations, debates, and election day are assignedtoscheduled and others to scandal .\nNext, we use the autoregressive integrated moving aver-\nage (ARIMA) time-series model (Stock, Watson, and oth-\n53\ners 2003) to run interrupted time-series analysis and iden-\ntify whether scandals and scheduled events are associated\nwith level changes in the fraction of fake news per day for\nxdays where x\u2208{3,5,7}. In our paper, we use auto.arima ,\na common ARIMA model selection function (Makridakis,Wheelwright, and Hyndman 2008) from R\u2019s forecast library.Given a time-series, P\ni(f,t,n), and a set of external regres-\nsors (i.e., events), auto.arima selects the best ARIMA model\nbased on the corrected Akaike information criterion (AIC).Here, we have 2 external regressors for each n. We de-\nnote xreg\n1\nn,T={0,0,...1,1,1...}where xreg1\nn,t=1i fd a y t\nis within x days of the nearest scandal (after it has occurred)\ninvolving n. Similarly, we write xreg2\nn,Tforscheduled9.\nA positive coef\ufb01cient returned by auto.arima forxreg1\nn,T\nwould mean that scandals temporarily increase the fraction\nof fake news per day. As shown in Figure 2b10, regard-\nless of the groundtruth selection, scheduled events generally\ncontribute to a reduction of fake news. This does not meanplanned events reduced the absolute volume of fake news.One possible explanation is mainstream media simply cov-\nered scheduled events much more, thus\n|f|s\n|f|s+|t|sis smaller.\nResults for scandal are, however, more varied, suggesting\nthat groundtruth pair selection has an impact on perceivedeffects of scandals. For instance, we see that scandals con-\ntributed to a short-term increase in the fraction of fake news\nshared per day when given groundtruth pair (ZDR,ALEXA ),\nbut a decrease if pair is (POLIT ,ALEXA ). This discrepancy is\nparticularly important to studies that examine how scandalsand negative media coverage diminish voter turnout in the2016 election, particularly for Clinton (Faris et al. 2017).\nAgenda-setting Priorities\nIn this section, we \ufb01rst use an iterative topic modeling pro-cess to extract issues, or topics, being covered by both fakeand traditional news sites and assign each news article to itscorresponding topic. Next, we examine whether the choiceof groundtruth pairs impacts agenda-setting conclusions.\nTopic Modeling of News Articles Using Guided LDA:\nWe use Guided LDA for topic modeling. It is an extensionof the base LDA that allows sets of keywords to guide doc-ument topic assignment by increasing their \u201ccon\ufb01dence\u201d orweights (Jagarlamudi, Daum \u00b4e III, and Udupa 2012).\nFirst, we use base LDA and manual labeling to extract\nseed words from news articles\n11. More speci\ufb01cally, we use\ngensim (Rehurek and Sojka 2011) to generate several base\nLDA models12. We then select the model which has the op-\ntimal coherence score13. From it, we obtain the top 30 most\n9Ifnisboth , we only use events that involve both nominees.\n10Trend results for when x=7 is omitted due to space.\n11We remove stop words, lemmatize and perform stemming. Fi-\nnally, we remove all articles that have <100 or >800 word tokens.\n12The number of topics are {50,75,100,125,150}respectively\nfor the models. In addition, we set all models to ignore words and\nbigrams that have a frequency of less than 100 or occur in more\nthan 50% of total documents.\n13Coherence score for a topic is the average of the pairwise\nword-similarity scores of its words (Newman et al. 2010). Atopic doc\nfracmost weighted tokens f1\nabortion 0.96% woman abort life plan parenthood issu punish\nfemal0.87\nbenghazi 0.60% attack benghazi libya committe report secre-\ntari secur0.75\nc-health 0.86% medic doctor releas report mental suffer pneu-\nmonia0.75\nclimate 1.40% climat coal environment industri land admin-\nistr regul0.89\nwst 0.30% speech wall street talk ask issu transcript re-\nleas0.82\nd&i 0.75% commun lgbt issu equal woman discrimin anti\nmarriag0.78\neconomy 4.4% trade job china deal compani manufactur\neconom0.79\nelection 20.3% sander berni primari voter percent poll voter\ncruz0.77\nemail 5.76% email depart investig server classi\ufb01 comey sec-\nretari0.84\nborder 2.28% immigr border mexico wall illeg deport mexi-\ncan build0.85\nmid-east 3.86% muslim islam israel isi terror terrorist attack\nunit syria0.76\nreligion 1.14% christian evangel church faith religi leader pas-\ntor pope0.78\nrussia 1.81% russia russian putin intellig hack of\ufb01ci govern 0.76\nsecurity 1.70% iran china nuclear polici foreign deal nato se-cur 0.78\nsexual 1.93% woman accus alleg rape husband sexual claim\nsexual assault0.82\nTable 3: List of Topics, Fraction of Total Documents Ac-counted for, Most Weighted Keywords, and F1\nrepresentative words for each topic. Next, we manually in-\nspect words and categorize them into coherent sets (i.e., top-ics). Using this approach, we obtain 409 unique seed wordsdivided into 33 different sets. Next, we run the guided us-ing the derived seed word sets\n14. We \ufb01lter out the subset\nof topics that lacked coherent themes and collapse topicsthat share the same human-interpretable theme into a sin-gle topic. This process results in 19 distinct topics. Finally,we assign each document into a single topic according to the\nmaximum probability of its topic distribution. This topic is\nlater referred to as the document\u2019s predicted topic label.\nTopic Modeling Quality Assessment and Selection: For\neach topic, we randomly sample 0 .2% of its documents (or\n10 if the size of a topic is small). This gives us 434 unique\ndocuments. We also sample 0 .2% documents from the arti-\ncles not included in the 19 topics. This results in 525 doc-uments. Finally, we shuf\ufb02e and publish the 1K (434 +525)\ndocuments on MTurk for crowdsourced labeling\n15.\nWe assign 3 independent workers to categorize each doc-\nument16and mark the manual topic of each article according\nmodel\u2019s coherence score is the sum over its topic coherence scores.\n14We adjust model\u2019s seed con\ufb01dence to 0.25 and set the number\nof total topics to 125. We use perplexity score (Misra, Capp \u00b4e, and\nYvon 2008) to determine the optimal number of topics given that\ngensim does not support coherence calculation for guided LDA.\n15The success of a crowdsourcing task relies heavily on the right\nmechanisms to ensure worker quali\ufb01cations. We require that work-\ners: 1) reside in the U.S. 2) have successfully completed at least\n1,000 HITs; and 3) have an approval rate of at least 98%.\n16Workers are given a list of categories (19 topics listed in Table\n3+1 none of the above option) to choose from and are instructed to\nselect a single primary category of a given article. We use Krippen-\ndorff\u2019s alpha (Hayes and Krippendorff 2007) to measure interrater\nreliability. It is 0.62, which means a moderate agreement.\n54\nFigure 3: Relative agenda priority difference between fake\nand traditional news. Y -axis is fraction of fake news articleson topic isubtracted by the fraction of traditional news arti-\ncles on i. Topics colored in green indicate a higher priority\nby fake news.\nto the majority vote\n17. Next, for each topic, we calculate its\nprecision, recall, and f1 scores using the manual and pre-\ndicted topic labels. We \ufb01lter out the topics that have an f1\nscore of <0.75. This process produces 15 distinct topics\naccounting for 49% of total articles. Table 3 provides thislist of topics, their names, prevalence across domains thatare listed by at least one fake or traditional news list, mostweighted keywords, and f1 score. As shown, election is the\nmost prevalent topic accounting for 20.3% of total news arti-cles, followed by Clinton\u2019s email scandal, and the economy.\nAgenda-setting Priorities: Next, we assess whether\ngroundtruth choice affects the perceived agenda-setting dif-ference between fake and mainstream news.\nFor a groundtruth pair (f,t), we derive the following\ntopic distributions K\n(f,t)={k1\n(f,t),k2\n(f,t)...k16\n(f,t)}and L(f,t)=\n{l1\n(f,t),l2\n(f,t)...l16\n(f,t)}where ki\n(f,t)and li\n(f,t)are the fractions of\nfake and traditional news articles on topic irespectively, and\n\u221116\ni=1ki\n(f,t)=1,\u221116\ni=1li\n(f,t)=1.\nThen, for each topic iinI(where Iis the entire\nset of topics), and all groundtruth pairs (F,T),w ea p -\nply Student\u2019s T-test on Ki(F,T)and Li(F,T)to deter-\nmine whether the difference in mean is statistically sig-ni\ufb01cant between these 2 distributions (here, K\ni(F,T)=\n{Ki(f1,t1),Ki(f2,t1)...Ki(f5,t3)}). In other words, we as-\nsess whether fake news sites have published signi\ufb01cantlymore or fewer articles (measured using normalized frac-tions) on certain topics than traditional news sites and viceversa. We observe a signi\ufb01cant difference in 9 topics. Forinstance, the average fraction of traditional news articles fo-cusing on election is 22.5%, while the average is less than\n15% for fake news articles. Traditional news sites are alsomore concentrated on topics including economy and cli-\nmate . Fake news sites, on the other hand, spend a consider-\nable fraction, approximately 10%, of all articles on Clinton\u2019semail scandal alone, twice that of traditional news sites.\nFake news sites also place a stronger emphasis on topicssuch as sexual scandals (mostly related to Bill Clinton), and\nHillary\u2019s pneumonia and claims of early onset dementia.\n17Articles that do not have a majority is labeled as unknown .W e\nobserve 46, or 8.6% unknown documents. Note, unknown docu-\nments differ from none of the above .\nFigure 4: PCA plot for topic fractional difference distribu-\ntion between fake and traditional news described in Section.Fake news lists are marked by shape.\nFor each pair (f,t), we calculate the difference distri-\nbution D\n(f,t)={d1\n(f,t),d2\n(f,t)...d9\n(f,t)}where d1\n(f,t)=k1\n(f,t)\u2212\nl1\n(f,t). We plot DI\n(F,T)in Figure 3. Notably, the data points\nofDI\n(f,t)consistently stay above or below the horizontal\ny=0 line. For instance, given groundtruth (AGZ,VARGO ),\n13.1% and 23.6% of fake and traditional news articles cov-ered the election. The negative difference is statistically sig-ni\ufb01cant, suggesting that fake news places less priority on thehorse-race coverage compared to traditional news. Further,the negative difference persists for all pairs (f,t). Similarly,\nfor all pairs (f,t), we consistently see a higher fraction cov-\nerage of Clinton\u2019s email scandal by fake news outlets. Inother words, the assessment as to whether a topic was morecentral to the coverage of fake vs. traditional news outletsis robust to the choice of groundtruth pairs. This is goodnews for studies that are focused on misinformation pub-lishers\u2019 agenda-setting functions (V argo, Guo, and Amazeen2018): fake news domains commonly prioritize hyperpolar-izing and hyperpartisan issues, and including more or fewerdomains in a study is unlikely to change the overall results.\nGroundtruth Difference Using Factor Analysis: Here,\nwe provide an analysis of how topics contribute to the vari-ance in agenda-setting across groundtruth pairs. We applyPCA (Wold, Esbensen, and Geladi 1987) on D\nS\nF,Tand extract\nthe \ufb01rst 2 principal components (the \ufb01rst and second compo-\nnent explains 68% and 23% of the total variance). The result-ing biplot is shown in Figure 4. We see that MBFC ,ZDR , and\nAGZ are more similar in their topic distributions. In compari-\nson, fake sites in POLIT have a higher fraction of articles on\nelection . One possible explanation is that this list is speci\ufb01-\ncally created to reduce election-related fake news (Politifactstaff 2018). Additionally, we also see that fake news sites in\nDDOT have a higher priority for scandals and controversial\nissues including benghazi and sexual , perhaps due to Daily\nDot being a social news site focused on fake news sites that\nwrote entertaining false content.\nRobustness Checks\nIn this section, we conduct additional analysis to ensurethat our results on the prevalence, temporal attributes,and agenda-setting priorities of fake news with respect togroundtruth choice are robust.\n55\nFake News Characteristics Measured Using User Partic-\nipation: Thus far, we have approximated prevalence using\nthe number of tweets. Yet, it\u2019s possible to have a few exceed-\ningly active and concentrated accounts post a large amountof tweets containing fake news without gaining traction inthe general population (Grinberg et al. 2018). Here, we re-examine fake news characteristics using the number of users.We observe comparable results.\nFirst, we rede\ufb01ne prevalence as the fraction of accounts\nthat posted at least 1 tweet containing fake news and observethat, depending on the groundtruth choice, the prevalence offake news ranges from 3.9% to 55.7% (compared to from1.3% to 43.7% when measured using tweets).\nFocusing on temporal patterns, we again see a consistent\npositive trend on the fraction of users who shared fake newsduring the general election period. That is, regardless ofgroundtruth choice, we observe that the closer the time wasto the general election date, the higher the fraction of userswho shared fake news. Further, scheduled events are consis-\ntently associated with a short-term decrease in the fractionof users who shared fake news, whereas results for scandals\nare dependent on groundtruth choice (e.g., scandals are cor-related with a short-term decrease in fake news when the\ngroundtruth pair is (AGZ,ALEXA ), but a short-term increase\nif pair is (MBFC ,ALEXA )). These observations are compara-\nble to prior results obtained using tweets.\nFinally, agenda-setting priority differences between fake\nand traditional news media measured by user participation(i.e., de\ufb01ning priority as how many unique accounts postedabout a given topic versus how many tweets were postedabout that topic) result in comparable conclusions. We ob-serve that, for all combinations of fand t, topics including\nemail ,mid-east , and sexual have the highest priority in fake\nnews, whereas climate ,economy , and election have the high-\nest priority in traditional news. In sum, we arrive at simi-lar results when conducting analysis using user participation\ncompared to when using tweets.\nAddressing Potential Biases in Keywords-based Data\nCollection Another concern lies with data incompleteness\nleading to biased observations. Thus far, we only use key-words \u201ctrump\u201d and \u201cclinton\u201d to collect tweets concerning\neach of the two presidential nominees respectively. There-fore, a tweet about Hillary Clinton that only includes the\ufb01rst name \u201chillary\u201d is absent from our original data. Here,we expand our dataset to include the 2 additional randomsample of tweets that contain the keywords \u201chillary\u201d and\u201chillary clinton\u201d respectively\u2013collected using the SysomosMAP pipeline (see \u201cData\u201d section). We then repeat ourprior analysis. While the additional data increases the to-tal number of tweets for Clinton to 13.3M, 2.8 times thesize of the original dataset, downstream results generally re-main the same. For instance, fake news prevalence rangesfrom 2 .2% to 47 .7% when using the expanded dataset\u2014\nsimilar to the range of 1 .3% to 43 .7% when using the\noriginal dataset. Further, time-series generated using the 2datasets are also highly correlated (e.g., the median normal-ized cross-correlation for the time-series on Clinton is 0.94).In fact, the expanded dataset only resulted in 306 additionalnumber of unique articles (a mere 0.13% increase from the\ntotal 244K).\nOverall, the results suggest that our analysis are robust.\nHowever, we note that our dataset and assessments remainonly focused on the two 2016 presidential nominees. Ourdata do not include other related subjects, or personalities,such as political parties and congressional candidates, and\nthe study of these subjects is outside the scope of this paper.\nConclusion and Discussion\nIn this paper, we \ufb01rst provided a comprehensive overview\nof the publicly available lists of fake and traditional newssites. We showed that these lists have divergent labelingprocesses and very few domains in common. In addition,we illustrated that the perceived prevalence of fake newsvaries substantially based on groundtruth choice. Despitethese initially discouraging results, we were able to reachseveral important robust conclusions. We noted an increasein fake news during the general election season regardlessof the groundtruth selection and a temporary reduction offake news due to scheduled events (conclusions for scandalswere more mixed). Finally, after an iterative topic model-ing process, we showed that agenda-setting priority differ-ences between fake and mainstream news sites are relativelyrobust to the groundtruth pair choice. Overall, our resultssuggest groundtruth selection has a sizable impact on preva-lence analysis and limited impact on downstream analysis ini) temporal characteristics, and ii) agenda-setting priorities.\nThere are several caveats to our study. First, our analy-\nsis of groundtruth difference and its impact is limited todomain-level labels. There are more granular datasets thatannotate content at article\u2013or even sentence\u2013level. Second,while the focus of our meta-analysis\u2014prevalence, tempo-ral characteristics, and agenda-setting priorities\u2014asks im-portant research questions, future work should also reviewexisting literature on similarly signi\ufb01cant issues, such asfake news exposure in different demographics (Grinberg etal. 2018) or supervised fake news detection (Shao et al.2016), identify similarities and potentially contradictory re-sults, and determine whether groundtruth choice contributesto the observed differences (e.g., how groundtruth affect theperformance of automated fake news classi\ufb01ers).\nThird, our dataset and analysis are only focused on the\nsubset of fake news surroundings the two presidential nomi-nees in the 2016 presidential election. Future work should\naddress how the study of fake news in other \ufb01elds (e.g.,misinformation concerning vaccination) could also be po-tentially impacted by groundtruth choice.\nWhere do we go from here? How can we make progress as\na research community despite the lack of agreement betweenfake news lists and domains with potential to be consideredfake? Our \ufb01ndings can be leveraged to provide guidance.\nGuidance on List Expansion and Maintenance for List\nCreators: Both fake news websites and groundtruth labels\nare indeed changing through time. List creators should in-\nclude methods that track and evaluate these changes.\nFor ef\ufb01cient and timely list expansion, one key road-\n56\nModel 1 Model 2 Model 3\n(DDOT) (POLIT) (AGZ) (MBFC) (ZDR) (ZDR) (MBFC) (defunct)\nindependent variables\nideology conservative \u22120.007 0.049 0.125\u22170.006 0.124\u2217\u22170.033 \u22120.130\u2217\nideology unknown \u22120.006 0.027 0.170\u2217\u22170.126 0.064 \u22120.066 \u22120.142\u2217\u2217\nsubtype bias \u22120.664\u2217\u2217\u2217\u22120.480\u2217\u2217\u2217\u22120.375\u2217\u2217\u22170.008 0.094\u2217\u2217\u22120.229\u2217\u2217\u2217\nsubtype clickbait \u22120.677\u2217\u2217\u2217\u22120.462\u2217\u2217\u2217\u22120.373\u2217\u2217\u2217\u22120.128\u22170.124\u2217\u22120.150\u2217\nsubtype conspiracy \u22120.686\u2217\u2217\u2217\u22120.442\u2217\u2217\u2217\u22120.370\u2217\u2217\u22170.021 \u22120.047 \u22120.001 \u22120.219\u2217\u2217\u2217\nsubtype hate \u22120.703\u2217\u2217\u2217\u22120.452\u2217\u2217\u2217\u22120.348\u2217\u2217\u2217\u22120.251\u2217\u2217\u22170.172\u2217\u2217\u22120.034\nsubtype junksci \u22120.705\u2217\u2217\u2217\u22120.460\u2217\u2217\u2217\u22120.388\u2217\u2217\u22170.055 \u22120.032 \u22120.297\u2217\u2217\u2217\nsubtype rumor \u22120.704\u2217\u2217\u2217\u22120.420\u2217\u2217\u2217\u22120.408\u2217\u2217\u2217\u22120.148 0.012 \u22120.058\nsubtype satire \u22120.704\u2217\u2217\u2217\u22120.345\u2217\u2217\u2217\u22120.284\u2217\u2217\u22170.047 \u22120.136\u2217\u2217\u2217\u22120.115\u2217\u22120.271\u2217\u2217\u2217\nsubtype unknown \u22120.703\u2217\u2217\u2217\u22120.412\u2217\u2217\u2217\u22120.268\u2217\u2217\u22170.295\u2217\u2217\u2217\u22120.186\u2217\u2217\u2217\nsubtype unreliable \u22120.703\u2217\u2217\u2217\u22120.512\u2217\u2217\u2217\u22120.511\u2217\u2217\u2217\u22120.178\u2217\u2217\u22170.068 \u22120.205\u2217\u2217\u2217\npopularity \u22120.0004 \u22120.002 0.023\u2217\u2217\u22170.048\u2217\u2217\u22170.042\u2217\u2217\u2217\u22120.018\u2217\u2217\u2217\u22120.046\u2217\u2217\u2217\u22120.021\u2217\u2217\u2217\nage inyear \u22120.0001 \u22120.009\u2217\u2217\u2217\u22120.024\u2217\u2217\u22170.008\u2217\u2217\u2217\u22120.003 0.001 0.005\u2217\u22120.021\u2217\u2217\u2217\nObservations 1,644 1,644 1,644 1,644 1,644 695 724 1,644\np-value 0.62 0.21 0.18 0.17 0.053 0.057 0.067 0.173\nNote:\u2217p<0.1;\u2217\u2217p<0.05;\u2217\u2217\u2217p<0.01\nTable 4: Model 1 assesses a domain\u2019s likelihood of being listed by a source ( DDOT ,POLIT ,AGZ,MBFC ,ZDR ) given its i) ideology,\nii) subcategory, iii) age, and iv) popularity. Model 2 examines characteristics that contribute to a domain\u2019s time of inclusion insources ZDR andMBFC . Model 3 analyzes attributes correlated with the likelihood of a domain being defunct.\nblock is the amount of manual labor required\n18. List creators\ncan reduce workload by using supervised machine learn-ing models to classify unlabeled news domains into fake ormainstream provided that potential model biases are exam-ined and understood\n19.\nSecond, for list maintenance, we urge researchers to un-\ndertake the following tasks. First, it\u2019s valuable to i) doc-ument the exact timestamp when a domain is added, re-moved, updated (e.g., change of subcategory), or defunct in\nthe list. Further, if a change is unusual (e.g., subcategorymodi\ufb01cation), creators should also ii) underline reasons forthe change. Next, if the annotation process is updated (e.g.,ZDR introduced more subcategories as the list expands), it\u2019s\nalso integral to iii) keep both the initial and updated proce-dures separate, highlight the differences, and note the timeof change. These tasks not only generate useful metadatathat is required by various studies, they also make the main-tenance process much more transparent, which can enhancethe list\u2019s credibility and help researchers identify potentialdiscrepancies or errors early on.\nLastly, given that top fake news stories in 2016 ostensibly\ntarget white, older conservative men and favor Trump overClinton (Grinberg et al. 2018; Allcott and Gentzkow 2017),\n18For instance, for MBFC , unaf\ufb01liated individuals \ufb01rst sub-\nmit questionable websites which automatically go into pend-\ning status, then the staff will review each pending domain and\nreach a decision using existing annotation procedure. Given thatmediabiasfactcheck.com currently has a backlog of 500+ domainsin pending, it suf\ufb01ces to say that the process is painstakingly slow.\n19For instance, creators can assess whether a model is biased\nagainst domains\u2019 i) ideology-leaning, ii) popularity, iii) age and iv)\nsubcategory. Biases in a model may not automatically disqualifyit from being employed, but documenting these biases can help\nfuture scholars using these lists and models better conceptualize\nhow potential limitations may impact the validity of their studies.we posit that the ideological-leaning of fake news sites will\nbe undoubtedly valuable to future work in this \ufb01eld, and pro-pose that creators also include the meta-data and the relevantannotation process in the lists.\nGuidance on Groundtruth Selection for List Users:\nFirst, researchers need to consider whether an analysis is di-rectly affected by list size, as in the case of prevalence. Othertypes of analysis that depend on the nature of the fake newsdomain (as opposed to counts) are more robust to the choice\n(e.g., temporal and topical analysis).\nThe second consideration relates to which lists one should\nuse for evaluation. We \ufb01rst observe that the choice of tradi-tional news lists seems to not matter, thus reducing the effortto carry out research. Second, we also see consistent cluster-ing of fake news lists across different analyses and we rec-ommend selecting a list from each cluster. MBFC ,AGZ , and\nZDR are commonly clustered together (e.g., topic analysis\nlatent space and prevalence). POLIT andDDOT are rather\ndistinct from the rest. By selecting a list from each (e.g.,\nMBFC andPOLIT ), researchers can determine informative\nbounds on their analyses. Finally, if the \ufb01ndings diverge, ex-panding the set of lists used as a function of (i) annotationand quality measure described in the meta-analysis and (ii)list clustering, i.e., considering the next most distinct list,can help explore this data space systematically.\nAcknowledgement\nThis research was partly supported by Michigan Institute for\nData Science (MIDAS) at the University of Michigan and\nthe National Science Foundation (Grant IIS-1815875).\nAppendix\nRegression Model for Domain Inclusion F o rag i v e n\ndomain ithat\u2019s listed by at least one fwhere f\u2208\n{ZDR,MBFC ,AGZ,DDOT ,POLIT}, let the binary variable yi,s=\n57\n{0,1}denote whether domain iexists in the list of fake news\nsites f. We \ufb01t model for each fusing ideology ,subcategory ,\npopularity , and age as the explanatory variables.\nyf=\u03b20+\u03b21ideology +\u03b22subtype +\u03b23popularity +\u03b24age+\u03b5i\nResults are summarized on Table 4 (Model 1).\nRegression Model for the Time of Addition We \ufb01rst use\nweb.archive.org and authors\u2019 websites to obtain 3 times-\ntamped snapshots20ofZDR ,MBFC , and DDOT . Let ibe a\nwebsite that was added to ZDR in one of its 3 snapshots and\nremained on the list thereafter, we determine i\u2019s preferred\nideology ,subcategory ,popularity , and age. Let the variable\nyi,zdr={0,1,2}denote whether domain iwas added in the\n1st, 2nd, or 3rd version of ZDR , we \ufb01t the following:\nyzdr=\u03b20+\u03b21ideology +\u03b22subtype +\u03b23popularity +\u03b24age+\u03b5i\nWe repeat the same procedure for DDOT andMBFC . Regres-\nsion results are summarized on Table 4 (Model 2)21.\nRegression Model for Active and Defunct Domains For\na given domain ithat\u2019s listed by at least one fwhere f\u2208\n{ZDR,MBFC ,AGZ,DDOT ,POLIT}, let the binary variable yi,s=\n{0,1}denote whether domain iis defunct (i.e. y=1 when i\nis no longer active). We \ufb01t model:\nyf=\u03b20+\u03b21ideology +\u03b22subtype +\u03b23popularity +\u03b24age+\u03b5i\nResults are summarized on Table 4 (Model 3).\nReferences\nAlimonti, K. R., and V eridiana. 2018. \u201cfake news\u201d offers\nlatin american consolidated powers an opportunity to censoropponents.\nAllcott, H., and Gentzkow, M. 2017. Social Media and Fake\nNews in the 2016 Election.\nJournal of Economic Perspectives\n31(2):211\u2013236.\nAllcott, H.; Gentzkow, M.; and Y u, C. 2018. Trends in the\ndiffusion of misinformation on social media. arXiv preprint\narXiv:1809.05901 .\nBeveridge, S., and Nelson, C. R. 1981. A new approach\nto decomposition of economic time series into permanentand transitory components.\nJournal of Monetary economics\n7(2):151\u2013174.\nBode, L.; Budak, C.; Ladd, J. M.; Newport, F.; Pasek, J.;\nSingh, L. O.; Soroka, S. N.; and Traugott, M. W. 2020.\nWords That Matter: How the News and Social Media Shaped the\n2016 Presidential Campaign . Washington, D.C.: Brookings In-\nstitution Press.\nBoulianne, S. 2015. Social media use and participation: A\nmeta-analysis of current research. Information, communication\n& society 18(5):524\u2013538.\nBovet, A., and Makse, H. A. 2019. In\ufb02uence of fake news\nin twitter during the 2016 us presidential election. Nature\ncommunications 10(1):7.\n20December 2016, June 2017, and December 2017 with each\nseparated by 6 months\n21Results for DDOT are removed given none of the variables are\nsigni\ufb01cant.Budak, C.; Agrawal, D.; and El Abbadi, A. 2011. Lim-\niting the spread of misinformation in social networks. In\nProceedings of the 20th International Conference on World Wide\nWeb, WWW \u201911, 665\u2013674. New Y ork, NY , USA: ACM.\nBudak, C. 2019. What happened? the spread of fake news\npublisher content during the 2016 u.s. presidential election.WWW \u201919.\nCharemza, W. W., and Syczewska, E. M. 1998. Joint appli-\ncation of the dickey-fuller and kpss tests.\nEconomics Letters\n61(1):17\u201321.\nCroft, M., and Moore, R. 2017. Checking what students\nknow about checking the news.\nDarwish, K.; Magdy, W.; and Zanouda, T. 2017. Trump\nvs. hillary: What went viral during the 2016 us presidentialelection. In\nInternational Conference on Social Informatics .\nDoshi, A. R.; Raghavan, S.; Weiss, R.; and Petitt, E. 2018.\nThe impact of the supply of fake news on consumer behaviorduring the 2016 us election.\nFaris, R.; Roberts, H.; Etling, B.; Bourassa, N.; Zuckerman,\nE.; and Benkler, Y . 2017. Partisanship, propaganda, anddisinformation: Online media and the 2016 us presidentialelection.\nFarmer, L. S. 2017. Don\u2019t get faked out by the news: Be-\ncoming an informed citizen. In\nThe Fifth European Conference\non Information Literacy (ECIL) , 174.\nFarte, G.-I., and Obada, D.-R. 2018. Reactive public rela-\ntions strategies for managing fake news in the online envi-ronment.\nFigueira, \u00b4A., and Oliveira, L. 2017. The current state of\nfake news: challenges and opportunities.\nProcedia Computer\nScience 121:817\u2013825.\nFunke, D. 2019. Want to get away with posting fake news\non facebook? just change your website domain.\nGoodall, D. W. 1966. A new similarity index based on\nprobability. Biometrics 882\u2013907.\nGranskogen, T. 2018. Automatic detection of fake news in\nsocial media using contextual information. Master\u2019s thesis,NTNU.\nGrinberg, N.; Joseph, K.; Friedland, L.; Swire-Thompson,\nB.; and Lazer, D. 2018. Fake news on twitter during the2016 us presidential election. Technical report.\nGuess, A.; Nyhan, B.; and Rei\ufb02er, J. 2018. Selective expo-\nsure to misinformation: Evidence from the consumption offake news during the 2016 us presidential campaign.\nEuro-\npean Research Council 9.\nGuo, L., and V argo, C. 2018. \u201cFake News\u201d and Emerg-\ning Online Media Ecosystem. Communication Research\n009365021877717.\nHaugh, L. D. 1976. Checking the independence of\ntwo covariance-stationary time series: a univariate residualcross-correlation approach.\nJournal of ASA 71.\nHayes, A. F., and Krippendorff, K. 2007. Answering the call\nfor a standard reliability measure for coding data. Communi-\ncation methods and measures 1(1):77\u201389.\n58\nHelmstetter, S., and Paulheim, H. 2018. Weakly supervised\nlearning for fake news detection on twitter. 274\u2013277. IEEE.\nHorne, B. D., and Adali, S. 2017. This Just In.\narXiv:1703.09398 [cs] . arXiv: 1703.09398.\nHorne, B. D.; Dron, W.; Khedr, S.; and Adali, S. 2018. As-\nsessing the News Landscape. Lyon, France: ACM Press.\nJagarlamudi, J.; Daum \u00b4e III, H.; and Udupa, R. 2012. Incor-\nporating lexical priors into topic models. 204\u2013213. Associ-\nation for Computational Linguistics.\nKoulolias, V .; Jonathan, G. M.; Fernandez, M.; and Sotir-\nchos, D. 2018. Combating misinformation: An ecosystemin co-creation.\nKucharski, A. 2016. Post-truth: Study epidemiology of fake\nnews.\nNature 540(7634):525.\nLazer, D. M.; Baum, M. A.; et al. 2018. The science of fake\nnews. Science 359(6380):1094\u20131096.\nLeetaru, K., and Schrodt, P . A. 2013. Gdelt: Global data on\nevents, location, and tone, 1979\u20132012. In ISA annual conven-\ntion, 1\u201349. Citeseer.\nLima, L.; Reis, J.; Melo, P .; Murai, F.; Ara \u00b4ujo, L.; Vikatos,\nP .; and Benevenuto, F. 2018. Inside the right-leaning echo\nchambers.\nMain, T. J. 2018. The Rise of the Alt-Right . Brookings Institu-\ntion Press.\nMakridakis, S.; Wheelwright, S.; and Hyndman, R. 2008.\nF orecasting methods and applications . John wiley & sons.\nMarcon, A. R.; Murdoch, B.; and Caul\ufb01eld, T. 2017. Fake\nnews portrayals of stem cells and stem cell research. Regen-\nerative medicine 765\u2013775.\nMcCombs, M. E., and Shaw, D. L. 1972. The agenda-setting\nfunction of mass media. Public opinion quarterly 36(2).\nMisra, H.; Capp \u00b4e, O.; and Yvon, F. 2008. Using lda to detect\nsemantically incoherent documents. 41\u201348. Association forComputational Linguistics.\nMitchell, R. 2018.\nWeb Scraping with Python: Collecting More\nData from the Modern Web . \u201d O\u2019Reilly Media, Inc.\u201d.\nMukerji, N. S. 2018. A conceptual analysis of fake news.Nelimarkka, M.; Laaksonen, S.-M.; and Semaan, B. 2018.\nSocial media is polarized. 957\u2013970. ACM.\nNewman, D.; Lau, J. H.; Grieser, K.; and Baldwin, T. 2010.\nAutomatic evaluation of topic coherence. 100\u2013108. Associ-ation for Computational Linguistics.\nPalen, L., and Hughes, A. L. 2018. Social media in disaster\ncommunication. In\nHandbook of Disaster Research . Springer.\nPolitifact staff. 2018. Politifact guide to fake news websites\nand what they peddle.\nPotthast, M.; Kiesel, J.; Reinartz, K.; Bevendorff, J.; and\nStein, B. 2017. A stylometric inquiry into hyperpartisanand fake news.\narXiv preprint arXiv:1702.05638 .\nPoynter Institute. 2019. International Fact Checking Net-\nwork. [Online; accessed -04-November-2019].\nPuglisi, R., and Snyder Jr, J. M. 2011. Newspaper coverage\nof political scandals. The Journal of Politics 73(3):931\u2013950.Rapoza, K. 2017. Can \u2018fake news\u2019 impact the stock market?\nRehurek, R., and Sojka, P . 2011. Gensim\u2013python framework\nfor vector space modelling.\nRini, R. 2017. Fake news and partisan epistemology.\nKennedy Institute of Ethics Journal 27(2):E\u201343.\nSchaedel, S. 2018. Websites that post fake and satirical\nstories. factcheck.\nScheufele, D. A.; Kim, E.; and Brossard, D. 2007. My\nfriend\u2019s enemy: How split-screen debate coverage in\ufb02uencesevaluation of presidential debates.\nCommunication Research\n34(1):3\u201324.\nShao, C.; Ciampaglia, G. L.; Flammini, A.; and Menczer, F.\n2016. Hoaxy. WWW \u201916 Companion . arXiv: 1603.01511.\nShao, C.; Ciampaglia, G. L.; V arol, O.; Flammini, A.; and\nMenczer, F. 2017. The spread of fake news by social bots.96\u2013104.\nShu, K.; Sliva, A.; Wang, S.; Tang, J.; and Liu, H. 2017. Fake\nnews detection on social media: A data mining perspective.\nACM SIGKDD Explorations Newsletter 19(1):22\u201336.\nShu, K.; Wang, S.; and Liu, H. 2018. Understanding user\npro\ufb01les on social media for fake news detection. In 2018\nIEEE Conference on (MIPR) , 430\u2013435. IEEE.\nSilverman, C. 2016. Here are 50 of the biggest fake news\nhits on facebook from 2016.\nStarbird, K. 2017. Examining the alternative media ecosys-\ntem through the production of alternative narratives of massshooting events on twitter. In\nICWSM , 230\u2013239.\nStock, J. H.; Watson, M. W.; et al. 2003. Introduction to\neconometrics , volume 104. Addison Wesley Boston.\nTandoc Jr, E. C.; Lim, Z. W.; and Ling, R. 2018. De\ufb01n-\ning \u201cfake news\u201d a typology of scholarly de\ufb01nitions. Digital\nJournalism 6(2):137\u2013153.\nV an Zandt, Dave. 2018. Media bias/fact check (mbfc news).V argo, C. J.; Guo, L.; and Amazeen, M. A. 2018. The\nagenda-setting power of fake news.\nnew media & society\n20(5):2028\u20132049.\nV osoughi, S.; Roy, D.; and Aral, S. 2018. The spread of true\nand false news online. Science 359(6380):1146\u20131151.\nWang, W. 2017. \u201dLiar, Liar Pants on Fire\u201d. arXiv:1705.00648 .\nWardle, C. 2017. Fake news. it\u2019s complicated. First Draft .\nWhite, N. 2018. The daily dot.\nWikipedia contributors. 2019. Alexa internet \u2014 Wikipedia,\nthe free encyclopedia. [Online; accessed 4-May-2019].\nWold, S.; Esbensen, K.; and Geladi, P . 1987. Principal com-\nponent analysis. Chemom. Intell. Lab. Syst. 2(1-3):37\u201352.\nZimdars, M. 2016. My \u201cfake news list\u201d went viral. but\nmade-up stories are only part of the problem. The Washington\nPost.\n59", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Higher ground? How groundtruth labeling impacts our understanding of fake news about the 2016 US presidential nominees", "author": ["L Bozarth", "A Saraf", "C Budak"], "pub_year": "2020", "venue": "\u2026 of the International AAAI Conference on Web \u2026", "abstract": "The spread of fake news on social media platforms has garnered much public attention and  apprehension. Consequently, both the tech industry and academia alike are investing"}, "filled": false, "gsrank": 210, "pub_url": "https://aaai.org/ojs/index.php/ICWSM/article/view/7278", "author_id": ["", "3JMfe8kAAAAJ", "wIhJS60AAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:-MxtOiJpuNgJ:scholar.google.com/&output=cite&scirp=209&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D200%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=-MxtOiJpuNgJ&ei=KbWsaMZzjoiJ6g_SwpG4CQ&json=", "num_citations": 51, "citedby_url": "/scholar?cites=15616347303637273848&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:-MxtOiJpuNgJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://aaai.org/ojs/index.php/ICWSM/article/download/7278/7132"}}, {"title": "Weakly supervised learning for analyzing political campaigns on facebook", "year": "2023", "pdf_data": "Weakly Supervised Learning for Analyzing Political Campaigns on Facebook\nTunazzina Islam*, Shamik Roy, Dan Goldwasser\nDepartment of Computer Science, Purdue University\nWest Lafayette, Indiana 47907\n{islam32, roy98, dgoldwas}@purdue.edu\nAbstract\nSocial media platforms are currently the main channel for po-\nlitical messaging, allowing politicians to target specific demo-\ngraphics and adapt based on their reactions. However, making\nthis communication transparent is challenging, as the messag-\ning is tightly coupled with its intended audience and often\nechoed by multiple stakeholders interested in advancing spe-\ncific policies. Our goal in this paper is to take a first step\ntowards understanding these highly decentralized settings. We\npropose a weakly supervised approach to identify the stance\nand issue of political ads on Facebook and analyze how po-\nlitical campaigns use some kind of demographic targeting by\nlocation, gender, or age. Furthermore, we analyze the temporal\ndynamics of the political ads on election polls.\nIntroduction\nOver the last decade, social media has impacted public\ndiscourse and communication, particularly in the political\ncontext (Kushin and Yamamoto 2010; Wattal et al. 2010;\nRatkiewicz et al. 2011; Stieglitz and Dang-Xuan 2013; Jensen\n2017; Marozzo and Bessi 2018; Badawy, Ferrara, and Ler-\nman 2018; Ferrara et al. 2020; Sharma, Ferrara, and Liu\n2021). Social media has a transformative effect on how po-\nlitical candidates interact with potential voters by adapting\ntheir messaging to different demographic groups\u2019 specific\nconcerns and interests. This process, known as microtarget-\ning (Hersh 2015), relies on data-driven campaigning tech-\nniques that exploit the rich information collected by social\nnetworks about their users. By measuring the users\u2019 engage-\nment with political content, candidates can identify the issues,\nand even the specific phrases and slogans, that resonate with\neach demographic group. Furthermore, political campaigns\non social media are highly distributed, with multiple stake-\nholders and interest groups using the platforms to advance\ntheir interests and show support for different candidates by\nspecifically focusing on agenda items relevant for their in-\nterests (e.g., the National Rifle Association (NRA) might\nemphasize the track-record of each candidate on protecting\ngun rights).\nOur goal in this paper is to take a first step towards ana-\nlyzing and monitoring the landscape of political advertising\n*Corresponding author. Email: islam32@purdue.edu\nCopyright \u00a9 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.on social media. We focus our experiments on the U.S.\n2020 presidential elections, analyzing content supporting\neither the Biden-Harris or the Trump-Pence campaigns.\nOur goal is twofold: first to characterize the different\nstakeholders and analyze their content, and second to build\non this characterization to analyze political messaging across\ndifferent demographics. We deal with the decentralized\nnature of political advertising on social media. We analyze\nover 5Kadvertisers (referred to as funding entities) that\nfunded over 800K political ads on Facebook\u2020, associating\nadvertisers with a binary label (Pro-Biden or Pro-Trump)\nand ads with four categories capturing positive or negative\nmessaging and its target. We also identify the specific policy\nissue discussed in the ad, a 13-class classification problem.\nTo clarify, consider the following two ads:\nAd1: From COVID-19 to the environment to racial justice, Donald\nTrump has failed. Joe Biden and Kamala Harris can set us on a\nnew course. The stakes for Pennsylvanians could not be higher.\nAd2: President Trump PROTECTED Social Security and\nMedicare. Joe Biden tried to cut them MULTIPLE times. President\nTrump LOWERED drug costs, and Medicare Advantage Premiums\nfell 34%. Under Biden, drug prices SKYROCKETED. Joe Biden\nand Kamala Harris\u2019s FAR left plan threatens private insurance and\nlimits choices.\nAd1 has Anti-Trump and Pro-Biden stances focusing on the\nmultiple issues covid, climate, racial justice. On the other\nhand, Ad2 has Pro-Trump and Anti-Biden stances focusing\non the healthcare issue.\nIn this paper, we suggest a weakly supervised graph-\nembedding based framework in which ads and advertisers\nare learned jointly. While some cases, the name of the ad-\nvertisers capture their bias, e.g., \u2018BIDEN FOR PRESIDENT\u2019,\n\u2018TRUMP MAGA COMMITTEE\u2019 and we mention them as explicit\nadvertisers. Some advertisers do not explicitly mention any\ncandidate name/party affiliation in their names, e.g., \u2018Union\n2020\u2019, \u2018Plains PAC\u2019 and we call them implicit advertisers. We\nleverage weak supervision from explicit advertisers for the\nstance prediction. Our embedding objective is derived from\ndedicated lexicons developed for identifying policy issues,\nand by identifying the political position of a small number of\nadvertisers (i.e., the position of the Trump-Pence campaign is\n\u2020https://www.facebook.com/ads/library/api\nProceedings of the Seventeenth International AAAI Conference on Web and Social Media (ICWSM 2023)\n411\nFigure 1: Political advertising graph capturing relations\namong ads, funding entities, stances, issues, issue lexicons.\nA funding entity connects to Ad1, which has both pro-biden\u2019\nand \u2018anti-trump\u2019 stances, on issues related to \u2018climate\u2019 and\n\u2018coronavirus\u2019.\nknown). During learning, our model learns the associations\nbetween advertisers and their positions and the content they\npublish, as well as the issues they are mostly concerned with.\nFig. 1 represents the embedding graph of our framework\ncontaining nodes i.e., ads, funding entities, stances, issues,\nissue lexicons and edges representing their relationships,\ni.e. as Ad1 has Pro-Biden stance, there is an edge between\nnode Ad1 andPro-Biden. We learn a graph embedding to\nmaximize the similarity between neighboring nodes (Perozzi,\nAl-Rfou, and Skiena 2014; Tang et al. 2015; Grover and\nLeskovec 2016). Fig. 1 shows that Ad1 andAd2 have weak\nlabels for stances and issues obtaining from their funding\nentities and issue lexicons respectively. Initially, Ad3 doesn\u2019t\nhave a label. It has edge with Funding Entity1 and issue lex-\nicons named \u2018covid\u2019 and \u2018mask\u2019. We exploit these resources\nto train our embedding model, which captures the context\nand, as a result, can generalize and grasp stances and issues\nin new ads (i.e., Ad3 in Fig. 1).\nThe learned model allows us to analyze how political can-\ndidates and stakeholders micro-target specific demographics.\nIn this work, we examine a novel dataset of advertisements\nposted to Facebook during the 2020 U.S presidential election\nand make the full dataset available to the research community.\nUsing the information provided by Facebook Political Ads\nAPI, we analyze the issues used for supporting (and attack-\ning) each candidate on ads targeting different geographical\nregions, age, and gender groups. We evaluate the quality of\nthe learned model by applying it to detect the stance of im-\nplicit funding entities and compare it with their views (ground\ntruth). Further, we discuss how election polls affect political\ncampaigns using Granger causality (Granger 1988). We focus\non the following research questions (RQ) to analyze political\ncampaigns:\n\u2022RQ1. Can we analyze political campaigns without direct\nsupervision? (Section Results and Analysis)\u2022RQ2. Are messages distinctive in ads? (Subsection De-\nscriptive Insights)\n\u2022RQ3. Which demographics are reached by advertisers?\n(Subsection Audience Demographics)\n\u2022RQ4. How specific region is reached by advertisers and\ntheir messages? (Subsection State-wise Issue and Demo-\ngraphics)\n\u2022RQ5. Are election polls represented in ad campaigns?\n(Subsection Granger Causality with Polls)\nOur contributions are summarized as follows:\n1.We formulate a novel problem of exploiting weak super-\nvision to analyze the landscape of political advertising on\nsocial media.\n2.We propose a weakly supervised graph embedding based\nframework to identify political stance of advertisers as\nwell as the published content and issues of the content.\nWe show that our model outperforms the baselines.\n3.We conduct quantitative and qualitative analysis on real-\nworld dataset to demonstrate the effectiveness of our pro-\nposed model.\nOur code and data are publicly available here\u2021.\nRelated Work\nDuring elections, political candidates use social media for\ntheir campaigns. Recent works show monitoring and analysis\nof targeted advertising on social media (Andreou et al. 2019;\nSilva et al. 2020b; Serrano et al. 2020). Islam and Goldwasser\n(2022b) analyzed Covid-19 vaccine campaign on Facebook.\nRibeiro et al. (2019) analyzed political ads on Facebook that\nare linked to a Russian propaganda group: Internet Research\nAgency (IRA). Silva et al. (2020a) showed Facebook ad\n(created by IRA) engagement targeting the 2016 U.S. general\nelection. Silva et al. (2020b) designed a system to monitor\npolitical ads on Facebook in Brazil and deployed during\nthe Brazilian 2018 elections. Capozzi et al. (2020, 2021)\nexamined advertising concerning the issue of immigration\nin Italy. Our paper detects the stance and issue of political\nads on Facebook. It analyzes political campaigns for both\ncandidates based on the target audience\u2019s demographic and\ngeographic information as well as presents temporal analysis\nfor the 2020 U.S presidential election.\nRecent works frame the issue of perspective detection\nas a text categorization problem (Greene and Resnik 2009;\nKlebanov, Beigman, and Diermeier 2010; Recasens et al.\n2013; Iyyer et al. 2014; Johnson and Goldwasser 2016). It\nis typically studied as a supervised learning task (Lin et al.\n2006; Durant and Smith 2006; Greene and Resnik 2009). In\ncontrast, our approach relies on weak supervision and lex-\nicon based approaches (Roy and Goldwasser 2020; Field,\nKliger et al. 2018). Weakly supervised methods reduce de-\npendence on labeled texts. Graph based semi-supervised al-\ngorithms achieved considerable attention over the years (Zhu\nand Ghahramani 2002; Belkin, Niyogi, and Sindhwani 2006;\nSubramanya and Bilmes 2008; Talukdar et al. 2008; Sind-\nhwani and Melville 2008; Yang, Cohen, and Salakhudinov\n\u2021https://github.com/tunazislam/weaklysup-FB-ad-political\n412\n2016; Hisano 2018). Tang, Qu, and Mei (2015); Zhang et al.\n(2020) used graph-based methods to build text networks with\nwords, documents and labels and propagate labeling informa-\ntion along the graph via embedding learning. Han and Shen\n(2016) encoded weakly supervised information in positive\nunlabelled learning tasks into pairwise constraints between\ntraining instances imposing on graph embedding. Recently,\nIslam and Goldwasser (2022a) proposed weakly supervised\ngraph embedding based EM-style framework to characterize\nuser types on social media. Our embedding model is similar\nto contrastive learning-based embedding (Wu et al. 2020;\nGiorgi et al. 2020). However, contrastive learning is self-\nsupervised, where labels are generated from the data without\nany manual or weak label sources. In our case, we generate\nthe label using weak supervision. Our work is also closely\nrelated to the entity-targeted sentiment analysis (Mohammad\net al. 2016; Field and Tsvetkov 2019; Mitchell et al. 2013;\nMeng et al. 2012). In our work, we use weak supervision to\nidentify stance and issue of political ads and analyze political\ncampaigns. To the best of our knowledge, this is the first\nwork to utilize a weakly supervised graph embedding based\nframework to analyze political campaigns on social media.\nData\nWe collect around 0.8million political ads from January-\nOctober 2020 using Facebook Ad Library API with the search\nterm \u2018biden\u2019, \u2018harris\u2019, \u2018trump\u2019, \u2018pence\u2019. All advertisements\nare written in English. For each ad, the API provides the ad\nID, title, ad body, and URL, ad creation time and the time\nspan of the campaign, the Facebook page authoring the ad,\nfunding entity, the cost of the ad (given as a range). The API\nalso provides information on the users who have seen the\nad (called \u2018impressions\u2019): the total number of impressions\n(given as a range and we take the average of the end points\nof the range), distribution over impressions broken down by\ngender (male, female, unknown), age ( 7groups), and location\ndown to states in the USA. We have duplicate content among\nthose collected ads because the same ad has been targeted to\ndifferent regions and demographics with unique ad id. We\nhave 35327 ads with different contents, 5431 unique fund-\ning entities, among them 537explicitly mention candidate\nnames and/or party affiliations, e.g., BIDEN FOR PRESIDENT,\nDONALD J. TRUMP FOR PRESIDENT, INC.\nHoldout Data\nFor validation purpose, we manually annotate 667ads for\nstances and issues. We consider 4stances \u2018pro-biden\u2019, \u2018pro-\ntrump\u2019, \u2018anti-biden\u2019, \u2018anti-trump\u2019 and 13issues\u00a7called \u2018abor-\ntion\u2019, \u2018covid\u2019, \u2018climate\u2019, \u2018criminal justice reform, race, law\n& order\u2019, \u2018economy and taxes\u2019, \u2018education\u2019, \u2018foreign pol-\nicy\u2019, \u2018guns\u2019, \u2018healthcare\u2019, \u2018immigration\u2019, \u2018supreme court\u2019,\n\u2018terrorism\u2019, \u2018lgbtq\u2019. We also mark \u2018non-stance\u2019, \u2018non-issue\u2019\nads. Two annotators from the Computer Science department\nmanually annotate a subset of ads to calculate inter-annotator\nagreement using Cohen\u2019s Kappa coefficient (Cohen 1960).\nThis subset has inter-annotator agreements of 77.50% for\n\u00a7https://ballotpedia.org/ISS\nUE(UNI,BI,TRI) ISS\nUE(UNI,BI,TRI)\nAbortion (56,\n20, 1) Foreign\npolicy (95, 31, 6)\nCovid (52, 23, 5) Guns (92,\n20, 6)\nClimate (66, 22, 3) Healthcare (62,\n21, 4)\nCriminal justice reform, race Immigration (78,\n25, 3)\nlaw & order (93, 26, 5) Supreme court\n(80, 25, 4)\nEconomy & taxes (41, 16, 2) Terrorism\n(73, 19, 3)\nEducation (62, 22, 2) LGBTQ (55,\n12, 1)\nTable 1: Number of unigram, bigram, trigram in each issue.\nstance and 69.60% for the issue, which are substantial agree-\nments. In case of a disagreement, we resolve it by discussion.\nThe rest of the data was annotated by one graduate student\nfrom the Computer Science department.\nMethodology\nWe represent political advertising activity on social media as\na graph, connecting funding entities to their ads. We repre-\nsent the outcome of our analysis, stance and issue predictions,\nas separate label nodes in the graph connected via edges to\nads and funding entities. Each issue label-node is associated\nwith an n-gram lexicon, a set of nodes representing lexical\nindicators for the issue. Based on known associations be-\ntween funding entities and stances, we associate 10% of the\nfunding entities and their ads with stance labels. The lexicon\nand observed stance relations act as a weak form of super-\nvision for graph embedding. Our model learns to generalize\nthe stance predictor to new ads, and by contextualizing the\nlexicon n-grams based on their occurrence in ads, we learn\nto associate other ads with the relevant issue even when the\nlexicon items are not present. These settings are described\nin Fig. 1. Note that each ad can be associated with multiple\nissues and stances (e.g., pro-biden and anti-trump).\nIssue Lexicon\nTo create the issue lexicon, we collect 30news articles\ncovering each issue from left leaning, right leaning, and\nneutral news media. We know the news source bias from\nhttps://mediabiasfactcheck.com/. We calculate the Pointwise\nMutual Information (PMI) (Church and Hanks 1990) to iden-\ntify issue-specific lexicons. We calculate the PMI for an n-\ngram, wwith issue, iasPMI (w, i) =logP(w|i)\nP(w). To com-\nputeP(w|i), we take all news articles related to an issue i\nand computecount (w)\ncount (allngrams ). We have 30news articles per\nissue. P(w)is computed by counting n-gram, wover the\nwhole corpus ( 390news articles). We assign each n-gram\nto the issue with the highest PMI and build an n-gram lexi-\ncon for each issue. Table 1 shows the number of unigrams,\nbigrams, and trigrams with PMI \u22650.5per issue. In this pa-\nper, we use only unigrams, resulting in 905issue-indicating\nwords.\nModel\nTo identify stances and issues, we do the followings:\nInferring Stance Labels Using Knowledge. In some cases,\n413\nModel Accuracy Macro-avg F1\nBiLSTM Glove 0.54 0.42\nFine-tuned BERT 0.56 0.41\nRule-based 0.38 0.33\nOur Model 0.73 0.63\nTable 2: Model performance for the stance prediction task.\nthe names of funding entities capture their bias. For example:\n\u2018Biden Victory Fund\u2019, \u2018Keep Trump in office\u2019 clearly state\ntheir position. We extract all funding entities mentioning the\ncandidates or their party. In addition, if the funding entity\nname also includes the words {\u2018dump\u2019, \u2018lie\u2019, \u2018out\u2019, \u2018fail\u2019,\n\u2018against\u2019 }, we assume the funding agency has stance against\nthat candidate. In this manner, we annotate 537funding enti-\nties. We annotate the ads generated by those funding entities.\nIf a \u2018pro-trump\u2019 funding entity addresses \u2018biden\u2019, the ad is\n\u2018anti-biden\u2019 (and the vice-versa). We use this approach to pro-\nvide labels for 5343 ads, and use them as weak supervision\nfor our model.\nPrediction of Stances and Issues of Ads and Funding Enti-\nties. We embed the following instances in a common embed-\nding space - (a) Ads (b) Funding Entities; (c) Issue Lexicon;\n(d) Issue Labels; (e) Stance Labels. We maximize the similar-\nity between two instances in the embedding space if \u2013 (1) A\nfunding entity has a stance. (2) An ad has a stance. (3) An\nad has a word from the issue lexicon. (4) An issue lexicon\nword has an annotated issue. We follow a negative sampling\napproach to learn the embeddings. Given an instance o, a\npositive example mpand a negative example mn, where ois\nplaced closer to mpand far from mnin the embedding space,\nthe embedding loss designed to place ocloser to mpthan\nmnisEr(o, mp, mn) =l(sim( o, mp),sim(o, mn))Here,\nErdefines the embedding loss for objective type r(for ex-\nample, ad to stance). Our goal is to maximize the similarity\nof a node embedding with a positive example and minimize\nthe similarity with a negative example. We call a stance a\npositive example for an ad, if the ad has the stance, otherwise,\nthe stance is called a negative example. We randomly sample\n2negative examples for each instance (ad) for the objectives\nnumbered (1) and (2) and 5negative examples for (3) and (4).\nThe number of negative examples provided differs by type\nof objectives because there are multiple stances and issues\nfor the same instance. For example, let\u2019s assume an ad has\nstances both \u2018pro-biden\u2019 and \u2018anti-trump\u2019 which are positive\nexamples for the ad, and negative examples for that would\nbe \u2018pro-trump\u2019 and \u2018anti-biden\u2019 stances \u2013 this is the reason\nfor choosing 2negative examples randomly for objectives\nnumbered (1) and (2). If the ad has multiple words from the\nissue lexicon, that means the ad has multiple issues that are\nconsidered positive examples. The rest would be the negative\nexamples for the ad. Therefore, we randomly pick 5negative\nexamples for the objectives numbered (3) and (4) for our data.\nsim()is the dot product and l()is the cross-entropy loss,\nl(sim( o, mp),sim(o, mn)) = \u2212log(esim(o,mp)\nesim(o,mp)+esim(o,mn))\nFor all kind of objectives, we minimize the summed lossP\nr\u2208R\u03bbrEr, where Ris the set of all objective functions\n(a) pro-biden\n (b) anti-trump\n(c) pro-trump\n (d) anti-biden\nFigure 2: Wordcloud for stances. Importance of each word\nis shown with font size and color. Pro-biden and anti-trump\nstances are talking about vote, ballot, chip. Pro-trump has\nwords like president, vote. Anti-biden has noticeable words\nsuch as radical left, sleepy, illegal immigrant.\nand\u03bbris the weight for objective function of type r. We\ninitialize \u03bbr= 1, for all.\nExperimental Setup\nIn this section, we present the baselines to evaluate the effec-\ntiveness of our model and hyperparameter tuning.\nBaselines\nFor baseline comparison, we use rule-based ad stance pre-\ndiction, based on 4simple paraphrases \u201cWe support Donald\nTrump\u201d, \u201cWe support Joe Biden\u201d, \u201cWe do not support Don-\nald Trump\u201d, \u201cWe do not support Joe Biden\u201d. We predict the\nstance of all ads based on the similarity with the paraphrases.\nFor the supervised baseline, we train a model using the weak\nlabels assigned by the knowledge from explicit funding en-\ntities and predict the stance of the ads having no weak la-\nbel (test data). From the weakly labeled training data, we\nrandomly choose 20% data as the validation set. Our first\nsupervised baseline is referred to as BiLSTM Glove model.\nWe use 300d Glove word embeddings to obtain the ad embed-\ndings and pass them to bidirectional LSTM (Hochreiter and\nSchmidhuber 1997). For the second supervised baseline, we\nfine-tune the pre-trained BERT-base-uncased (Devlin et al.\n2019) model. For the BERT\u2019s input, we tokenize the text\nusing BERT\u2019s wordpiece tokenizer. For both supervised base-\nlines, we use cross-entropy loss, 5-fold cross-validation and\nreport the average test result (accuracy and macro-avg F1\nscore) considering only the manually annotated ads as ground\ntruth.\nHyperparameter Details\nAd embeddings are obtained by running a Bi-LSTM (Schus-\nter and Paliwal 1997) over the Glove (Pennington, Socher,\nand Manning 2014) word embeddings of the words of the ad.\nWe concatenate the hidden states of the two opposite direc-\ntional LSTMs to get representation over one time-stamp and\naverage the representations of all time-stamps to get a single\n414\nFE GT\nPred M Acc M(%) FE GT\nPred M Acc M(%)\nPlanned P\narenthood V otes L L 100.0 Women\nSpeak Out PAC C C 95.5\nUnion 2020 L L 82.8 America First\nAction C C 98.3\nUnited We Dream Action L L 100.0 PRESER VE\nAMERICA PAC C C 100.0\nIndependence USA PAC L L 99.9 PRO\nTECT FREEDOM PAC C C 68.5\nPACRONYM L L 92.6 American Potential\nFund C C 76.9\nAlliance for a Better Minnesota L L 98.4 Plains P\nAC C C 100.0\nBlack PAC L L 100.0 Citizens for\nFree Enterprise C C 100.0\nThe Lincoln Project L L 100.0 AG\nTOGETHER PAC C C 100.0\nDEFEAT DISINFO PAC L L 93.9 FREEDOM THR\nOUGH TRUTH C C 63.2\nUNITED FOR PROGRESS PAC L L 92.1 Family\nPolicy Alliance C C 75.0\nVeterans For Responsible Leadership L L 100.0 REST ORA\nTION PAC C C 75.0\nDream Defenders Fight PAC L L 100.0 AMERICANS FOR\nPROSPERITY C L 22.4\nWorking America L L 100.0 Championing America\nat Her Best C L -\nUNITE THE COUNTRY L L 100.0 C3P\nAC C L 46.6\nFOR OUR FUTURE L L 100.0 Honoring American\nLaw Enforcement PAC C C 100.0\nBEST DAYS AHEAD L L 100.0 CONGRESSION AL\nLEADERSHIP FUND C C 72.5\nREALLY AMERICAN PAC L L 84.3 WEARE\nGREAT AGAIN PAC C C 72.9\nWin Justice L L 84.8 KEEP AMERICA\nGREAT COMMITTEE C C 73.2\nVOTE V ALUES L L 82.0 GREA T\nAMERICA PAC C C 100.0\nNARAL Freedom Fund L L 90.9 Keep\nKentucky Great C C 100.0\nCOMMON DEFENSE ACTION FUND L L 100.0 STOP\nSOCIALISM NOW PAC C C 100.0\nDEFEAT BY TWEET L L 100.0 Wisconsin\nRight to Life C C 75.0\nNJ7 CITIZENS FOR CHANGE L L 100.0 Te\nxas Right to Life C C 100.0\nNEW POWER PAC L L 100.0 FLORID A\n8TH PAC C C 100.0\nQUESTION PAC L L 100.0 CatholicV ote C\nC 100.0\nTable 3: Model performance on implicit funding entities for predicting stance. Bold funding entities have prediction error (see\ndetails in the Qualitative Analysis section). L= Liberal; C= Conservative; FE= Funding entity; GT= Ground truth stance of\nfunding entity; Pred M= Predicted stance by our model; Acc M= Predicted accuracy by our model.\nrepresentation of the ad. All the embeddings are initialized\nin a300d space. We use single layer Bi-LSTM which takes\n300d Glove word embeddings as inputs and maps to a 150d\nhidden layer. We train this Bi-LSTM jointly with the embed-\nding learning. Adam (Kingma and Ba 2014) optimizer with\nlearning rate 0.001is used. We initialize the embeddings of\nall of the other instances randomly. we use validation loss as\na stopping criteria. We run the embedding learning at most\n100epochs or stop the learning if the loss does not decrease\nfor10consecutive epochs. For BiLSTM Glove baseline, the\nsingle layer Bi-LSTM takes 300d Glove word embeddings\nas inputs and maps to a 150d hidden layer with optimizer=\nAdam , learning rate= 0.01, batch size = 32, epochs = 20. For\nBERT fine-tuning, we use maximum sentence length = 200,\nbatch size = 32, learning rate = 2e\u22125, optimizer= AdamW\n(Loshchilov and Hutter 2018), epochs = 3, epsilon parameter\n=1e\u22128. We chose the lowest validation loss as a stopping\ncriterion for the two supervised baselines.\nResults and Analysis\nAfter learning the embeddings from our weakly supervised\ngraph embedding approach, we can infer the stances and is-\nsues of unlabeled ads and funding entities by looking at the\nembedding similarity of the ads and funding entities having\nstance and issue labels. The stance or issue label having the\nmaximum similarity is inferred as the predicted issues and\nstances. We discard \u2018non-stance\u2019 and \u2018non-issue\u2019 cases for\nthe stance and issue prediction evaluation, resulting in 544\nads and 174ads, respectively. We use accuracy and macro-\naverage F1 score as the evaluation metrics. We obtain accu-\nracy of 73.71% and macro-average F1 score of 62.81% forstance prediction of the ads. For the issue prediction tasks,\nwe achieve 68.39% accuracy and 54.76% macro-average\nF1 score. For rule-based baseline of stance prediction we\nget38.42% accuracy and 33.17% macro-average F1 score\n(Table. 2). We compare our model with two supervised base-\nlines. BiLSTM Glove obtains 53.55% accuracy and 41.62%\nmacro-average F1 score for stance prediction (Table. 2). Sec-\nond supervised baseline (Fine-tuned BERT) gets 55.91%\naccuracy and 40.70% macro-average F1 score (Table. 2). Ta-\nble 2 shows that weakly supervised graph embedding model\nachieves the highest performance in stance prediction com-\npare to the rule-based as well as supervised baselines which\nanswers the RQ1.\nQualitative Analysis\nAs we do not have ground truths for implicit funding enti-\nties, for qualitative evaluation, we consider 50funding en-\ntities (containing 23881 ads) that do not explicitly mention\nany candidate name/party affiliation in their name. To under-\nstand their stance as ground truth, we look for their views\nat OpenSecrets.org. We report results from 25liberal and\n25conservative advertisers. For each entity, we take the ma-\njority vote over the predictions. In Table. 3, we show the\npredicted stance with accuracy using our model and compare\nwith ground truth.\nFrom Table. 3, we observe \u2018liberal\u2019 view for the conserva-\ntive advertiser \u2018Championing America at Her Best\u2019\u00b6. The\ngoal of this funding entity is to prevent the reelection of\nDonald Trump and founding director of this funding entity\n\u00b6https://www.championingamerica.com/\n415\nPro-biden Anti-trump Pr\no-trump Anti-biden\nTrigrams PCor T\nrigrams PCor T\nrigrams PCor T\nrigrams PCor\nvote\njoe biden 0.763 defeat donald trump 0.594 president trump need 0.667 joe biden democratic 0.713\npresidential election held 0.470 request ballot today 0.526 trump need vote 0.729 dont let america 0.357\njoe kamala democrat 0.579 time running out 0.238 vote november 3rd0.539 taken joe biden 0.600\ntoday vote democrat 0.583 not authorized candidate 0.353 3rdpresident trump 0.510 radical left taken 0.431\nvote democrat joe 0.683 affordable care act 0.382 fake news medium 0.303 democratic party dont 0.524\ndefeat donald trump 0.535 new trumpcare plan 0.572 president trump spent 0.523 million illegal immigrants 0.295\nsure joe biden 0.659 health insurance affordable 0.206 poll sleepy joe 0.396 trillions new taxes&amnesty 0.460\nendorse joe biden 0.740 save big health 0.199 live american dream 0.381 biden embraced policy 0.575\njoe biden president 0.689 defeat trump gop 0.608 forgotten men woman 0.185 policy far left 0.466\nkamala democrat country 0.407 condemn donald trump 0.446 equal opportunity justice 0.206 reduction police funding 0.231\nTable 4: Most frequent trigrams and their Pearson correlation coefficient with ads stance. P Cor: Pearson correlation coefficient.\nPCor ranges from \u22121to1, with a value of \u22121meaning a total negative linear correlation, 0being no correlation, and +1\nmeaning a total positive correlation.\n(a) trigrams from pro-biden covid ads\n (b) trigrams from anti-trump covid ads\n(c) trigrams from pro-trump covid ads\n (d) trigrams from anti-biden covid ads\nFigure 3: Correlation heatmaps for issue-specific (coronavirus) most frequent trigrams and ads stances. The heatmap cell colors\nrepresent the percentage of times trigrams appear in the same context.\n416\nis Matthew Mattern who is a Republican committed to the\nideals of the party of Lincoln and Reagan. Our model predict\nanti-trump stance for all ads ( 100% accuracy) sponsored by\nthis advertiser in our data.\nWe notice that our model provides \u2018liberal\u2019 view for conser-\nvative advertisers \u2018AMERICANS FOR PROSPERITY\u2019 and\n\u2018C3 PAC\u2019 based on majority vote over the predictions (Ta-\nble. 3) by predicting \u2018anti-trump\u2019 stance mostly. For adver-\ntiser \u2018AMERICANS FOR PROSPERITY\u2019, we observe that\nour model predict anti-trump stance where ads contain text\nlike \u2013 \u201cMake your voice heard today by sending a letter to\ntell Senator Y\u2013 I support the confirmation of Justice Amy\nConey Barrett!\u201d, where Y= any Republican senators. Our\nmodel predicts anti-trump stance for ads sponsored by \u2018C3\nPAC\u2019 where it contains criticism of Trump inside quote, i.e.,\nWe can\u2019t let Nancy Pelosi get away with more baseless lies\nand sham conspiracy theories. Join us in condemning her\nrecent \u201cTrump and GOP lawmakers are Enemies of the Peo-\nple\u201d remarks. Though our model can predict \u2018liberal\u2019 views\nfor for all liberal advertisers based on majority vote (Table.\n3), we notice that our model predicts pro-trump stance for\nthe ads having rhetorical question \u2013 \u201cJoe Biden worked with\nObama to save the auto industry and bring back jobs. What\nhas Trump done for American workers?\u201d\nDescriptive Insights\nIn our data, we have more pro-trump stance ads. We notice\nthatsupreme court is the most and lgbtq is the least prominent\nissues in the ad content. To answer RQ2, we focus on how\ndistinctive the contents of \u2018 X\u2212trump \u2019 and \u2018 X\u2212biden \u2019 ads,\nwhere X=pro, anti . To understand what kind of words ads\nuse to represent positive and negative stance towards the can-\ndidates, we show most frequent trigrams for each category\n(Table. 4). To compute the statistical significance, we calcu-\nlate Pearson correlation coefficient (Pearson 1895) between\nads of each stance category and each trigram generated by\ncorresponding category of ads ( pearson corr(ti, sj), where\nti= trigrams generated from sjandsj= ads related to each\nstance category). To calculate correlation, we provide the\nembedding for each trigrams and embedding of ads for each\nstance category. For embedding, we use Sentence-BERT em-\nbeddings (Reimers and Gurevych 2019). Trigrams and corre-\nsponding Pearson correlation coefficient are shown in Table.\n4. From Table. 4, we notice that the ads having pro-biden\nstance has common trigrams like \u2018vote joe biden\u2019, \u2018today vote\ndemocrat\u2019, \u2018joe kamala democrat\u2019 etc, whether anti-trump\nhas trigrams like \u2018defeat donald trump\u2019, \u2018request ballot to-\nday\u2019, \u2018new trumpcare plan\u2019 etc. On the other hand, ads with\npro-trump stance has common trigrams like \u2018president trump\nneed\u2019, \u2018vote november 3rd\u2019, \u2018live american dream\u2019, \u2018equal\nopportunity justice\u2019 etc, whether anti-biden ads contains tri-\ngrams like \u2018radical left taken\u2019, \u2018million illegal immigrants\u2019,\n\u2018trillions new taxes&amnesty\u2019, \u2018reduction police funding\u2019 etc.\nBiden\u2019s economy & tax plans and immigration policies are\noften the target of anti-biden attacks, and as such are often\nmentioned and accused of raising tax on middle class fam-\nilies, providing illegal immigrants amnesty and healthcare,\nembracing far left policy. On the other side, pro-trump mes-\nsages focus on Trump\u2019s vow never to forget the \u2018forgottenmen and women\u2019. They also advertise about how every single\ncitizen have a chance to live their American dream and have\nequal opportunity justice under Trump administration.\nTo show the noticeable qualitative differences, we create\nwordcloud with the most frequent words from the whole data.\nFig. 2 shows the visual representation of the text for each\nstance category. Words are usually single words, and the\nimportance of each tag is shown with font size and color. Ads\nwith pro-biden and anti-trump stances are talking about vote,\nchip, donation, win, defeat, elect, ballot, win, support (Fig.\n2a and 2b). Positive stance regarding Trump has following\nwords president, vote, fighting, fake news, real american\n(Fig. 2c). Anti-biden wordcloud has noticeable words such\nasradical left, sleepy, washington swamp, illegal immigrant,\nfar left (Fig. 2d).\nIssue-specific Ads To analyze how do ads talk about an\nissue based on stances, i.e., pro trump vs. anti-biden ads on\nimmigration e.g., \u201cTrump will build the wall\u201d vs. \u201cBiden\nis weak on immigration\u201d, we put a condition on meaning-\nful words that characterize an issue. Then, we calculate the\ncorrelation between ads and the most frequent trigrams in\neach categorty ( pearson corr(ti,k, sj,k), where ti,k= tri-\ngrams generated from sjonkissue and sj= ads related\nto each stance category on kissue). To better understand\nhow each stance represents messaging, we analyze their co-\noccurrence and convey this information in heatmaps. Each\nrow in the heatmap captures the association strength between\nissue-specific trigrams ( y-axis) and all the stances ( x-axis).\nThe heatmap cell colors represent the percentage of times tri-\ngrams appear in the same context. The heatmaps showing the\ncorrelation of trigrams and stances on \u2018coronavirus\u2019 issue are\nshown in Fig. 3. It demonstrates how trigrams are correlated\nto each stance category. Fig. 3a shows the top 5most frequent\ntrigrams from pro-biden ads focusing on the \u2018coronavirus\u2019 is-\nsue and their Pearson correlation coefficients for each stance\ncategory. For example, there is a higher correlation between\ntrigrams that focuses on Biden\u2019s plan for tackling coronavirus\nby putting effort to develop vaccine, rebuilding country and\npro-biden stance. On the other hand, anti-trump ads correlate\nmore with the trigrams focusing on rising covid cases and\ndeaths in Florida and Trump\u2019s ignorance regarding health\nexperts (Fig. 3b). Trump\u2019s plan for reopening country is one\nof the prominent messages of pro-trump ads (Fig. 3c). From\nFig. 3d we notice that anti-biden ads highly correlate with\ncoronavirus stimulus. A more thorough qualitative examina-\ntion of the content of these ads is left for future work, and\ninstead we focus on their audience reach.\nAudience Demographics\nIn this section, we focus on the audience of these ad cam-\npaigns. As Facebook Ads Library provides summary of de-\nmographic statistics on \u2018impressions\u2019 received by each ad, as\na distribution over 3 genders (male, female, unknown) and\n7 age groups. This metric describes the views of each ad,\nwhich may be different from users exposed to the ad, as the\nsame user may be exposed multiple times. To answer RQ3,\nwe analyze 1) Targeted demographics by the advertisers, 2)\nAd impressions by the demographics.\n417\n(a)\n (b)\nFigure 4: demographic distribution (a. age and b.gender) over percentages of ads of each stance category. Chi-square test results\nindicate an association between audience\u2019s demographics and advertisers\u2019 stances (p-value <0.05). Pro-biden and anti-trump\nads mostly target 35\u221254age group and the female gender. Advertisers with pro-trump and anti-biden stances target the 55\u221264\nage group and the male population mostly. The age group 45\u221254is equally targeted by all four stances.\nNull Hypothesis, H0 Alter\nnate Hypothesis, Ha T-test statistics P-value\nMore females\nthan males of all ages More females than males from all ages 4.768\u2217\u22170.005\ndo not watch pro-biden ads. watch pro-biden ads.\nMore females than males of all ages More females than males from all ages 4.847\u2217\u22170.004\ndo not watch anti-trump ads. watch anti-trump ads.\nMore males than females from age More males than females from age 4.841\u22170.017\nrange 18-54 do not view pro-trump ads. range 18-54 view pro-trump ads.\nMore males than females from age More males than females from age 4.131\u22170.026\nrange 18-54 do not view anti-biden ads. range 18-54 view anti-biden ads.\nMore females than males from older More females than males from older 1.255ns0.428\nage (55+) do not watch pro-trump ads. age (55+) watch pro-trump ads.\nMore females than males from older More females than males from older 1.057ns0.483\nage (55+) do not watch anti-biden ads. age (55+) watch anti-biden ads.\nTable 5: T-test of the influence of audiences\u2019 age and gender on ad impressions. ** = highly statistically significant at p-value\n<0.01; * = statistically significant at p-value <0.05; ns = statistically not significant (p-value >0.05).\nFigure 5: Distribution of impressions over age and gender in each stance category. Pro-biden and anti-trump ads are viewed\nmainly by female audiences and it\u2019s highly statistically significant (p-value <0.01). Pro-trump and anti-biden ads are mostly\nviewed by males from age range 18\u221254(p-value <0.05). Results of t-test hypothesis testing are shown in Table 5.\n418\nTargeted Demographics We have 3.4%pro-biden ads,\n55% pro-trump, 13.4%anti-biden, and 28.2%anti-trump\nads targeting different demographics. To answer the question\n\u201cIs there any association between audience\u2019s demographics\nand advertisers\u2019 stances?\u201d, we perform chi-square test of\ncontingency (Cochran 1952). The chi-square test provides\na method for testing the association between the row and\ncolumn variables in a two-way table called contingency table.\nThe null hypothesis H0assumes that there is no association\nbetween the variables, while the alternative hypothesis Ha\nclaims that some association does exist. The chi-square test\nstatistic is computed as \u03c72=P(observed \u2212expected )2\nexpected. The\ndistribution of the statistic \u03c72is denoted as \u03c72\n(d f), where d f\nis the number of degrees of freedom. d f= (r\u22121)(c\u22121),\nwhere rrepresents the number of rows and crepresents the\nnumber of columns in the contingency table. The p-value for\nthe chi-square test is the probability of observing a value at\nleast as extreme as the test statistic for a chi-square distribu-\ntion with (r\u22121)(c\u22121)degrees of freedom. To perform a\nchi-square test, we take age distribution over stance and gen-\nder distribution over stance separately to build contingency\ntables correspondingly. We choose value of significance level,\n\u03b1= 0.05. The p-value for both cases is <0.05, which is\nstatistically significant and we reject the null hypothesis H0,\nindicating that there is some association between the audi-\nence\u2019s demographics and advertisers\u2019 stances. Fig. 4 shows\nthe comparison of the percentages of stances by age group\n(age group 13\u221217has been dropped due to no activity)\n(Fig. 4a) and gender (Fig. 4b). We notice that the ads with\npro-biden and anti-trump stances mostly target the age group\n35\u221254, whereas pro-trump ads mainly target age rage 55\u221264\nand anti-trump ads focus 45\u221265+ age group. Interestingly\nage group 45\u221254is equally targeted by \u2018 X\u2212trump \u2019 and\n\u2018X\u2212biden \u2019 ads, where X=pro, anti (Fig. 4a). However,\nads with pro-biden and anti-trump stances target the female\npopulation mostly, while pro-trump and anti-biden ads target\nthe male population mostly (Fig. 4b). We can conclude that\nfunding agencies supporting Biden in election targeting the\nage group 35\u221254and women and funding agencies support-\ning Trump in election targeting older people and men are not\ndue to random variation.\nDemographic Ad Impression Among the 12.5Bimpres-\nsions received by the ads we have collected, ads with pro-\nbiden stance has 5.3%impressions, 41.6%for pro-trump,\nanti-biden 12.6%, anti-trump has 40.4%impressions. Fig.\n5 shows the distribution of impressions over demographics\n(age group 13\u221217and unknown gender have been dropped\nbecause of having almost no activity). Considering all ads,\npro-trump advertising has the majority of impressions and\nit is mostly viewed by younger male than female audiences.\nBoth pro-trump and anti-biden ads have more views from\nmales than females in age range 18\u221254. Fig. 5 shows that\nmore women from all age groups, compared to men from all\nage groups, watch pro-biden and anti-trump ads. We note that\nthe female audience of ads is even more skewed towards older\nage than males. We perform t-test (Student 1908) hypothesis\ntesting to provide statistical evidence of our study. Table 5shows the null hypothesis ( H0), alternate hypothesis ( Ha),\nt-test statistics with p-value for each tested variables. In the\nt-test, our level of significance, \u03b1= 0.05. Ifp\u2212value > \u03b1 ,\nwe accept H0; otherwise we reject H0. No statistical signifi-\ncance is found when we test whether more females compared\nto males from older age watch pro-trump and anti-biden ads\nas p-value >0.05and we accept H0(last2rows of Table 5).\nResults from top 4rows in Table 5 show that pro-biden and\nanti-trump ads are viewed mostly by females than males re-\ngardless of age group, whereas more males than females from\nage group 18\u221254tend to watch pro-trump and anti-biden\nads (p-value <0.05and reject H0).\nState-wise Issue and Demographics\nWe investigate state-wise issue and demographic distribution\nover stances to answer RQ4. We use historical voting data||\nto determine red/blue/battleground status of the states. We\nconsider battleground state Pennsylvania ( 451576 ads), blue\nstate New York ( 389788 ads), and red state Idaho ( 306476\nads) to analyze their stances, issues, and corresponding de-\nmographics. To narrow down our analysis, we consider ads\nhaving >10% regional impressions in these states. We per-\nform chi-square test by taking issue distribution over stance\nto build contingency tables separately for PA, NY , and ID.\nWe choose the value of significance level, \u03b1= 0.05, and our\ntest results show that the p-value <0.05for all three states.\nTherefore, we reject the null hypothesis, indicating some as-\nsociation between advertisers\u2019 issues and stances focusing on\nthe region. Fig. 6a shows issue distribution over stances in the\nswing state PA. In PA, we notice the highest pro-biden stance\non \u2018supreme court\u2019 issue, and a higher anti-trump issue on\n\u2018covid\u2019 and \u2018supreme court\u2019. We show issue distribution with\nstances in the blue state NY and red state ID in Fig. 6b and\nFig. 6c correspondingly. Issue like \u2018economy & taxes\u2019 has\nmore pro-biden and anti-trump stances both in NY and ID\n(Fig. 6c). On the other hand, anti-biden ads mostly focus on\n\u2018law & order\u2019 and \u2018economy & taxes\u2019 issues in New York\n(Fig. 6b). It\u2019s noticeable that \u2018supreme court\u2019 is the most\nprominent issue in pro-trump ads in PA, NY , and ID. In ad-\ndition, anti-biden stances are apparent in ads related to the\n\u2018criminal justice reform, race, law & order\u2019 issue for all three\nstates. To analyze age and gender distribution with stances\nbased on the three states, we again perform chi-square test by\ntaking age and gender distribution over stance to build con-\ntingency tables separately. Our test results show the p-value\n<0.05for age and gender in PA, NY , and ID. Therefore,\nwe reject the null hypothesis, indicating some association\nbetween advertisers\u2019 stances and demographics focusing on\nthe region. In PA, pro-biden and anti-trump ads primarily tar-\nget the younger population ( 25\u221244). In contrast, pro-trump\nand anti-biden ads mainly target the people of 45\u221265+ in\nPA. We notice that ads with pro-trump and anti-biden stances\ntarget the age group 55\u221264in NY . Interestingly in ID, pro-\nbiden ads are more prominent, and the target age group is\n55\u221264. After analyzing gender distribution with stances, we\nnotice that more men than women are targeted by pro-trump\n||https://www.visualcapitalist.com/u-s-presidential-voting-\nhistory-by-state/\n419\n(a) Pennsylvania\n(b) New York\n(c) Idaho\nFigure 6: Issue distribution over percentages of ads of each stance category in (a) PA, (b) NY , (c) ID considering ads having\n>10% regional impressions. Chi-square test results indicate an association between advertisers\u2019 issue and stances focusing\non a specific region (p-value <0.05). Pro-biden and anti-trump ads focus on various issues, e.g., \u2018covid\u2019, \u2018economy & taxes\u2019,\n\u2018supreme court\u2019, depending on region. Conversely, pro-trump and anti-biden stances are noticeable in \u2018supreme court\u2019 and \u2018law &\norder\u2019 related ads, respectively, targeting all three states.\n420\nFigure 7: Granger causality tests comparing polling averages\nfor each day and ad impressions. On the x-axis, we report the\nnumber of lags in days we consider for the delay between the\ntwo time series; on the y-axis, we show the sum-of-square\nF\u2212test statistics for corresponding lag. We highlight those\nwith p-value <0.05with black diamond box.\nand anti-biden ads in all three states. In PA, both females and\nmales are targeted equally by pro-biden and anti-trump ads.\nMore females compared to males are targeted by anti-trump\nads in NY . On the other hand, more female populations (60%)\nthan males are targeted by pro-biden ads in Idaho.\nGranger Causality with Polls\nTo start off, toward answering RQ5, we collect poll data of\nyear2020 from https://fivethirtyeight.com/. We exclude from\nthe analysis the period after 3rdNovember 2020. We com-\npute two time series:\n(1) For poll data, we take the sum of average poll count of\neach presidential candidate for each day called Polls (t).\n(2) For ads, we calculate the total number of impressions of\n\u2018X\u2212trump \u2019 and \u2018 X\u2212biden \u2019 ads, where X=pro, anti\nfor a given day, AdImpressions (t).\nWe compute the following two Granger causality tests with\nthese two time series to check (1) Does Polls (t)Granger\ncause AdImpressions (t)? (2) Does AdImpressions (t)\nGranger cause Polls (t)? The null hypothesis (H0)assumed\nby the first test is that Polls (t)does not Granger cause\nAdImpressions (t)and the alternative hypothesis (HA)is\nthatPolls (t)Granger causes AdImpressions (t). For the\nsecond test, H0isAdImpressions (t)does not Granger\ncause Polls (t)andHA=AdImpressions (t)Granger\ncauses Polls (t). We reject H0if p-value is <0.05for these\ntests. We report results for these two tests in Fig. 7. We notice\na significant F-test for the hypothesis of Polls (t)Granger\ncauses AdImpressions (t)for ads with both stances (Left\nside of Fig. 7). Conversely, we find no significant Granger\ncausality from ad impressions to polls (Right side of Fig. 7).\nOur finding from this analysis is when polls lean towards\none candidate, Facebook ads sponsored by the advertisers\nsupporting that candidate get more attention.\nConclusion\nWe suggest a weakly supervised approach for analyzing po-\nlitical campaigns on social media, and show the difference\nin the frequency of positive and negative ads and their main\npolicy issue, as a function of the group targeted. By answer-ing our research questions and providing statistical tests to\ndiscern the most significant findings, we inform empirical\nunderstandings of how polarized political environments are\nlinked to the underlying funding structure for election ads.\nThough our approach is tailored for the specific use case, this\ncan be extended to analyze any kind of ads on social media.\nEthical Impact\nThe data collected in the paper was made publicly available\nby Facebook Ads API and does not contain any personal in-\nformation. Any qualitative result that we report is an outcome\nfrom a machine learning model that does not represent the\nauthors\u2019 personal views.\nAcknowledgements\nWe are thankful to the anonymous reviewers for their insight-\nful comments. This work was partially supported by Purdue\nGraduate School Summer Research Grant (to TI) and an NSF\nCAREER award IIS-2048001.\nReferences\nAndreou, A.; et al. 2019. Measuring the Facebook advertising\necosystem. In NDSS.\nBadawy, A.; Ferrara, E.; and Lerman, K. 2018. Analyzing the\nDigital Traces of Political Manipulation: The 2016 Russian\nInterference Twitter Campaign. In ASONAM.\nBelkin, M.; Niyogi, P.; and Sindhwani, V . 2006. Manifold\nregularization: A geometric framework for learning from\nlabeled and unlabeled examples. JMLR.\nCapozzi, A.; et al. 2020. Facebook Ads: Politics of Migration\nin Italy. In ICSI.\nCapozzi, A.; et al. 2021. Clandestino or Rifugiato? Anti-\nimmigration Facebook Ad Targeting in Italy. In CHI.\nChurch, K. W.; and Hanks, P. 1990. Word association norms,\nmutual information, and lexicography. Computational lin-\nguistics.\nCochran, W. G. 1952. The \u03c72 test of goodness of fit. The\nAnnals of mathematical statistics.\nCohen, J. 1960. A coefficient of agreement for nominal\nscales. EPM.\nDevlin, J.; et al. 2019. BERT: Pre-training of Deep Bidirec-\ntional Transformers for Language Understanding. In HLT-\nNAACL.\nDurant, K. T.; and Smith, M. D. 2006. Predicting the political\nsentiment of web log posts using supervised machine learning\ntechniques coupled with feature selection. In KDWEB.\nFerrara, E.; et al. 2020. Characterizing social media manipu-\nlation in the 2020 US presidential election. First Monday.\nField, A.; Kliger, D.; et al. 2018. Framing and Agenda-setting\nin Russian News: a Computational Analysis of Intricate Po-\nlitical Strategies. In EMNLP.\nField, A.; and Tsvetkov, Y . 2019. Entity-Centric Contextual\nAffective Analysis. In ACL.\nGiorgi, J. M.; et al. 2020. Declutr: Deep contrastive learning\nfor unsupervised textual representations. arXiv:2006.03659.\n421\nGranger, C. W. 1988. Some recent development in a concept\nof causality. Journal of econometrics.\nGreene, S.; and Resnik, P. 2009. More than words: Syntactic\npackaging and implicit sentiment. In HLT-NAACL.\nGrover, A.; and Leskovec, J. 2016. node2vec: Scalable Fea-\nture Learning for Networks. In KDD.\nHan, Y .; and Shen, Y . 2016. Partially Supervised Graph\nEmbedding for Positive Unlabelled Feature Selection. In\nIJCAI.\nHersh, E. D. 2015. Hacking the electorate: How campaigns\nperceive voters. Cambridge University Press.\nHisano, R. 2018. Semi-supervised graph embedding ap-\nproach to dynamic link prediction. In CompleNet.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation.\nIslam, T.; and Goldwasser, D. 2022a. Twitter User Repre-\nsentation Using Weakly Supervised Graph Embedding. In\nICWSM.\nIslam, T.; and Goldwasser, D. 2022b. Understanding COVID-\n19 Vaccine Campaign on Facebook using Minimal Supervi-\nsion. In IEEE Big Data.\nIyyer, M.; et al. 2014. Political ideology detection using\nrecursive neural networks. In ACL.\nJensen, M. J. 2017. Social media and political campaigning:\nChanging terms of engagement? IJPP.\nJohnson, K.; and Goldwasser, D. 2016. \u201cAll I know about\npolitics is what I read in Twitter\u201d: Weakly Supervised Models\nfor Extracting Politicians\u2019 Stances From Twitter. In COLING.\nKingma, D.; and Ba, J. 2014. Adam: A method for stochastic\noptimization. arXiv:1412.6980.\nKlebanov, B. B.; Beigman, E.; and Diermeier, D. 2010. V o-\ncabulary choice as an indicator of perspective. In ACL.\nKushin, M. J.; and Yamamoto, M. 2010. Did social media\nreally matter? College students\u2019 use of online media and\npolitical decision making in the 2008 election. MCS.\nLin, W.; et al. 2006. Which side are you on?: identifying\nperspectives at the document and sentence levels. In CoNLL.\nLoshchilov, I.; and Hutter, F. 2018. Decoupled Weight Decay\nRegularization. In ICLR.\nMarozzo, F.; and Bessi, A. 2018. Analyzing polarization of\nsocial media users and news sites during political campaigns.\nSNAM.\nMeng, X.; et al. 2012. Entity-centric topic-oriented opinion\nsummarization in twitter. In ACM SIGKDD.\nMitchell, M.; et al. 2013. Open domain targeted sentiment.\nInEMNLP.\nMohammad, S.; et al. 2016. Semeval-2016 task 6: Detecting\nstance in tweets. In SemEval.\nPearson, K. 1895. Notes on Regression and Inheritance in\nthe Case of Two Parents Proceedings of the Royal Society of\nLondon, 58, 240-242.\nPennington, J.; Socher, R.; and Manning, C. 2014. Glove:\nGlobal vectors for word representation. In EMNLP.Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk:\nOnline learning of social representations. In KDD.\nRatkiewicz, J.; et al. 2011. Detecting and tracking political\nabuse in social media. In ICWSM.\nRecasens, M.; et al. 2013. Linguistic models for analyzing\nand detecting biased language. In ACL.\nReimers, N.; and Gurevych, I. 2019. Sentence-bert: Sentence\nembeddings using siamese bert-networks. arXiv:1908.10084.\nRibeiro, F. N.; et al. 2019. On microtargeting socially divisive\nads: A case study of russia-linked ad campaigns on facebook.\nInACM FAccT.\nRoy, S.; and Goldwasser, D. 2020. Weakly Supervised Learn-\ning of Nuanced Frames for Analyzing Polarization in News\nMedia. In EMNLP.\nSchuster, M.; and Paliwal, K. K. 1997. Bidirectional recurrent\nneural networks. IEEE transactions on Signal Processing.\nSerrano, J. C. M.; et al. 2020. The Political Dashboard: A\nTool for Online Political Transparency. In ICWSM.\nSharma, K.; Ferrara, E.; and Liu, Y . 2021. Characterizing\nOnline Engagement with Disinformation and Conspiracies\nin the 2020 US Presidential Election. arXiv:2107.08319.\nSilva, M.; et al. 2020a. Facebook ad engagement in the rus-\nsian active measures campaign of 2016. arXiv:2012.11690.\nSilva, M.; et al. 2020b. Facebook Ads Monitor: An Inde-\npendent Auditing System for Political Ads on Facebook. In\nWWW.\nSindhwani, V .; and Melville, P. 2008. Document-word co-\nregularization for semi-supervised sentiment analysis. In\nIEEE ICDM.\nStieglitz, S.; and Dang-Xuan, L. 2013. Social media and\npolitical communication: a social media analytics framework.\nSNAM.\nStudent. 1908. Probable error of a correlation coefficient.\nBiometrika.\nSubramanya, A.; and Bilmes, J. 2008. Soft-supervised learn-\ning for text classification. In EMNLP.\nTalukdar, P.; et al. 2008. Weakly-supervised acquisition\nof labeled class instances using graph random walks. In\nEMNLP.\nTang, J.; Qu, M.; and Mei, Q. 2015. PTE: Predictive Text Em-\nbedding Through Large-scale Heterogeneous Text Networks.\nInKDD.\nTang, J.; et al. 2015. Line: Large-scale information network\nembedding. In WWW.\nWattal, S.; et al. 2010. Web 2.0 and politics: the 2008 US\npresidential election and an e-politics research agenda. MIS\nquarterly.\nWu, Z.; et al. 2020. Clear: Contrastive learning for sentence\nrepresentation. arXiv:2012.15466.\nYang, Z.; Cohen, W.; and Salakhudinov, R. 2016. Revisiting\nsemi-supervised learning with graph embeddings. In ICML.\nZhang, Y .; et al. 2020. Minimally supervised categorization\nof text with metadata. In ACM SIGIR.\nZhu, X.; and Ghahramani, Z. 2002. Learning from Labeled\nand Unlabeled Data with Label Propagation. Technical re-\nport.\n422", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Weakly supervised learning for analyzing political campaigns on facebook", "author": ["T Islam", "S Roy", "D Goldwasser"], "pub_year": "2023", "venue": "\u2026 of the International AAAI Conference on \u2026", "abstract": "Social media platforms are currently the main channel for political messaging, allowing  politicians to target specific demographics and adapt based on their reactions. However, making"}, "filled": false, "gsrank": 214, "pub_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/22156", "author_id": ["YNChCGMAAAAJ", "qbbGZ8EAAAAJ", "u8358QgAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:7GORPFOiW-sJ:scholar.google.com/&output=cite&scirp=213&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D210%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=7GORPFOiW-sJ&ei=KrWsaO_cK8DZieoPqdqh8QU&json=", "num_citations": 12, "citedby_url": "/scholar?cites=16959327300175881196&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:7GORPFOiW-sJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://ojs.aaai.org/index.php/ICWSM/article/download/22156/21935"}}, {"title": "Framing analysis of health-related narratives: Conspiracy versus mainstream media", "year": "2024", "pdf_data": "Framing Analysis of\nHealth-Related\nNarratives:\nConspiracy versus\nMainstream MediaJournal Title\nXX(X):1\u2013 ??\n\u00a9The Author(s) 2023\nReprints and permission:\nsagepub.co.uk/journalsPermissions.nav\nDOI: 10.1177/ToBeAssigned\nwww.sagepub.com/\nSAGE\nMarkus Reiter-Haas1, Beate Kl\u00f6sch2, Markus Hadler2, and\nElisabeth Lex1\nAbstract\nUnderstanding how online media frame issues is crucial due to their impact on\npublic opinion. Research on framing using natural language processing techniques\nmainly focuses on specific content features in messages and neglects their\nnarrative elements. Also, the distinction between framing in different sources\nremains an understudied problem. We address those issues and investigate\nhow the framing of health-related topics, such as COVID-19 and other diseases,\ndiffers between conspiracy and mainstream websites. We incorporate narrative\ninformation into the framing analysis by introducing a novel frame extraction\napproach based on semantic graphs. We find that health-related narratives in\nconspiracy media are predominantly framed in terms of beliefs, while mainstream\nmedia tend to present them in terms of science. We hope our work offers new ways\nfor a more nuanced frame analysis.\nKeywords\nnatural language understanding, abstract meaning representations, framing theory,\nconspiracy narratives, pretrained language models, online media\n1Institute of Interactive Systems and Data Science, Graz University of Technology\n2Department of Sociology, University of Graz\nPrepared using sagej.cls [Version: 2017/01/17 v1.20]arXiv:2401.10030v1  [cs.CL]  18 Jan 2024\n2 Journal Title XX(X)\nprevent-01instance\nARG0ARG1ARG3time\ndoctor spread-03 vaccinate-01\ncompany virus\nnamePfizerdate-entity\n2021instance instance\nARG1instance\nARG1ARG0\ninstance\nyear instanceinstance name\ninstanceop1In 2021, doctors prevented the spread of the virus by vaccinating with Pfizer .\nFigure 1. Example sentence (top) with its extracted AMR graph using a BART -based\nmodel. Given this representation, we can identify the narrative elements, while syntactical\ninformation such as tenses is omitted. Within the narrative, three characters are present,\ni.e., a doctor who acts twice (i.e., two ARG0 relations) as character (orange), as well as a\ncompany and a virus (both with ARG1 relations). The plot (blue; predicates with word\nsenses) revolves around three frames, namely prevent ,vaccinate , and spread . Additionally,\nthe year 2021 (i.e., date-entity = setting; green) and the company name Pfizer are depicted\nas entities (purple).\nIntroduction\nThe perception of reality in human communication heavily relies on how messages\nare framed, leading to significant effects on human behavior. In their groundbreaking\nstudy, Tversky and Kahneman (1985) demonstrate that altering the formulation of\na problem impacts people\u2019s decision making. Hence, understanding the role of\nframing in textual communication is a critical research direction. While many existing\ncomputational framing research studies have primarily focused on narrow topics, such\nas war (Wicke & Bolognesi, 2020), terrorists (Demszky et al., 2019), morality (Reiter-\nHaas, Kopeinik, & Lex, 2021), or blame (Shurafa, Darwish, & Zaghouani, 2020), these\nworks often overlook the narrative content embedded within the frames. However,\nunderstanding the narrative content is essential as it plays an important role in\ntransmitting the underlying information. Furthermore, framing in textual content is\ndefined as promoting particular aspects of information through the selection and\nsalience of content (Entman, 1993). Hence, a comprehensive framing analysis needs to\nextend beyond the identification of frames themselves and interpret why frames were\nused, such as supporting a particular narrative.\nNarrative framing is especially critical in domains where the differences in the\nmessages can be subtle, as exemplified in conspiracy theories. According to McLeod\nPrepared using sagej.cls\nReiter-Haas et al. 3\net al. (2022), narrative frames are abstract constructs that refer to entire messages\nrather than individual content features, which makes them difficult to identify. Fong,\nRoozenbeek, Goldwert, Rathje, and van der Linden (2021) find that despite conflicting\nnarratives of conspiracy theories versus scientific narratives, the language employed\nwithin them has similarities in terms of word frequencies (using LIWC - Linguistic\nInquiry and Word Count; Tausczik & Pennebaker, 2010). Contrary to Fong et al.\nhypotheses, these similarities also extend to causal (e.g., \u201chow,\u201d \u201cwhy,\u201d \u201cbecause\u201d)\nand outgroup language (e.g., \u201cthey,\u201d \u201cthem\u201d). Still, the authors highlight consistent\ndifferences in specific linguistic patterns, such as the prevalent use of anger-based\nwording to convey negative emotion within conspiratorial discourse. In our work, we\naim to bridge the gap between word-frequency of messages and their framing. To\nthat end, we propose to incorporate narrative information into the framing analysis\nof conspiracy theories.\nWe argue that most conventional text analysis methods employed in framing\nresearch, including various types of topic modeling (see Ali & Hassan, 2022, for\nan overview) do not accurately capture narrative information. Therefore, we propose\nthe extraction of semantic graphs from textual data to conduct frame analysis. Our\napproach draws on the recent work of Jing and Ahn (2021), in which semantic\nrelations are mined from textual data in the form of triples, i.e., subject, predicate, and\nobject. In this representation, subjects and objects correspond to semantic roles such\nas agents and patients, while the predicate (i.e., the verb) connects these roles. In our\nwork, however, we leverage semantic graphs as they allow for a more comprehensive\nrepresentation of concepts, entities, and relations in textual data. This enables us to\ncapture a broader range of semantic information beyond roles and predicates.\nTo that end, we utilize abstract meaning representations (AMR; Banarescu et al.,\n2013) as a means to transform textual content into semantic graphs. AMR gives us a\nstructured representation of textual content in the form of AMR graphs, from which\nwe extract their inherent semantic frames using edge information; thus considering\nthe embedded narrative content contained within them. To validate the effectiveness\nof our approach, we apply it to health-related narratives mined from the language of\na conspiracy corpus (LOCO; Miani, Hills, & Bangerter, 2021). LOCO contains text\ndocuments from mainstream and conspiracy websites from a time period of 2004\nuntil 2020. The study of Miani et al. (2021) shows that the detection of conspiracy\ncontent compared to mainstream content can be challenging for humans, primarily\ndue to ambiguity, highlighting the potential benefits of algorithmic support tools\nfor humans. Our work aims to address this challenge by showing the difference in\nnarrative elements, such as setting, characters, plot, and the moral of the story. Figure 1\nexemplarily shows an extracted AMR graph on COVID-19 and its narrative elements.\nIt depicts how narrative elements can be determined by edge traversal of the given\nsemantic information. This approach allows for a more effective analysis of narratives,\nas compared to traditional methods relying on syntactical or word-based extraction\ntechniques, which would prove considerably more challenging in this context.\nThrough our analysis, we uncover a distinctive pattern in the narrative framing\nemployed by conspiracy media compared to mainstream media. Specifically,\nconspiracy media tend to employ belief-based rather than science-based arguments.\nConversely, mainstream media shows the opposite tendency (i.e., towards science\nPrepared using sagej.cls\n4 Journal Title XX(X)\nrather than beliefs). This disparity in narrative framing underpins the contrasting\napproaches to information dissemination between both media types. Hence, our\napproach advances the narrative understanding of textual content by providing\na comprehensive and holistic view of embedded narrations. Consequently, our\nmethodology enables a nuanced frame extraction, facilitating future works in framing\napplications.\nIn sum, our main contributions are:\nC1: We present a novel approach based on AMR and use it to extract frames\nimbued with narrative information. Our approach is flexible while also being\nconceptually simple to employ.\nC2: We demonstrate that within LOCO, the framing of health-related narratives\n(i.e., on COVID-19, diseases in general, and pharmacology) of conspiracy media\nfocuses on beliefs compared to mainstream media, which focuses on science.\nBackground\nIn this section, we provide the background of narrative framing analysis. We\ndescribe the theory behind narrations, review the related work of computational frame\nextraction, and introduce the background of AMR, i.e., frame semantics.\nNarration. According to Piper, So, and Bamman (2021), narrations contain multiple\nelements, such as a setting (in the work of Piper et al., setting refers to spatial location\nexclusively; in contrast, our work also considers temporality), characters (referred to\nas agents and potential objects), a plot (referred to as events), a reason (which we refer\nto as moral of the story), as well as a perspective (i.e., information about the teller and\nrecipient). In this work, we focus mainly on the content and consider the perspective\nimplicitly by contrasting two different sources. In line with Entman\u2019s (1993) basic\nassumptions, we assume that the tellers within each source have different motivations.\nSimilar to Piper et al.\u2019s definition of narration, the narrative policy framework (Jones &\nMcBeth, 2010), on which we ground our work, defines a set of four narrative elements.\nIt comprises a setting or context, a plot, the characters, and the moral of the story. In\nthis framework, the perspective is also implicit by considering the trust and credibility\nof the source (i.e., narrator).\nIn our work, we leverage abstract meaning representations (AMR) (Banarescu et al.,\n2013) to extract frames. AMR is a graph-based approach to representing the semantic\ncontent of textual information. AMR parsing transforms textual information into a\ndirected acyclic graph, whose nodes correspond to concepts (Xu, Li, Zhu, Zhang, &\nZhou, 2020). These concepts are connected via edges reflecting semantic relations,\nsuch as, e.g., the role that they occupy. AMR parsers are trained on an annotated corpus\nconsisting of structured semantic information, which is based on a strict specification\nof how AMR graphs are constructed by humans (Banarescu et al., 2012). We use AMR\nbecause it results in simple representations; also, it allows us to extract frames in a\nflexible, exploratory manner.\nComputational frame extraction. A body of recent research focuses on\ncomputational frame extraction (see Ali & Hassan, 2022, for an overview). Herein,\nframing detection is often presented as a classification problem, such as in the SemEval\nPrepared using sagej.cls\nReiter-Haas et al. 5\n2023 shared task (Piskorski, Stefanovitch, Da San Martino, & Nakov, 2023). In another\nexample, Tourni et al. (2021) consider gun violence as portrayed in news headlines and\nlead images. In their work, the authors formalize the notion of frame concreteness\nderived from the tangibility of words within their headlines. They relate it to the\nrelevance of images to the given headline. Their experiments show that news about\npolitics has a high concreteness and relevance, whereas news about society/culture is\nlow on both. Huguet Cabot, Dankers, Abadi, Fischer, and Shutova (2020) consider\nthe frames security and defense, morality and fairness, and equality in the context of\nimmigration, gun control, and death penalty. Their approach is based on RoBERTa (Liu\net al., 2019), and they predict the framing of policy issues based on a joint model\nof emotion, metaphors, and political rhetoric. From a methodological perspective,\nmany works also employ unsupervised learning to extract frames, such as clustering\nand sentiment analysis (e.g., Burscher, Vliegenthart, & Vreese, 2016, uses both).\nHerein, Jing and Ahn (2021) extract frames in partisan tweets related to COVID-19\nby combining BERT with semantic role labeling (Shi & Lin, 2019).\nWe similarly position our paper as frame extraction but focus on narration instead.\nIn our work, we extract frames related to COVID-19, albeit from conspiracy and\nmainstream media. Different from previous works (e.g., Jing & Ahn, 2021), we use\nAMR instead of topic modeling or semantic role labeling1. By structuring concepts\nwithin a text rather than tagging text spans, AMR allows for more flexible extraction\nof semantic information, which in turn benefits the interpretability of the data. For\ninstance, extracted roles (i.e., agents and patients) are often long sequences of text\nin semantic role labeling, as modifiers are also included in the tagged spans of text.\nHence, the full sequence of the doctors who only recently graduated is tagged as an\nagent instead of just extracting the doctor concept.\nFrame Semantics. Frame semantics has a long history in the natural language\nprocessing community since its initial introduction by Fillmore (1976). The use of\nframe semantics gained momentum with the FrameNet project (Baker, Fillmore, &\nLowe, 1998) and aided their adoption in natural language understanding (Fillmore &\nBaker, 2001). PropBank (short for proposition bank; Palmer, Gildea, & Kingsbury,\n2005) provided an annotated corpus of frames and the relations to their arguments,\nand hence, paved the way for widely used computational methods in natural\nlanguage understanding, such as semantic role labeling (Shi & Lin, 2019) and AMR\nparsing (Banarescu et al., 2013). Our work uses the latter due to its matching properties\nfor the task.\nAs shown in Figure 1, AMR is a graph-based representation of the semantic\ncontent in the text without explicit syntax. To be more concise, AMR is a rooted,\ndirected, acyclic graph with labeled edges and leaf nodes. AMR parsing converts\ntexts to structured information beyond the capabilities of simple textual extraction\nmethods. Firstly, it enriches the textual information with semantic information about\ndata types (e.g., date-entity ) and information (e.g., name ). It also simplifies the\nsemantic information by normalization (e.g., removing tenses \u2013 prevented toprevent ,\nsingularizing nouns \u2013 doctors todoctor , considering word senses \u2013 spread-03 to\nindicate distribution instead of smearing, omitting the distinction between nouns and\nPrepared using sagej.cls\n6 Journal Title XX(X)\nListing 1: Penman Notation of AMR graph\n( p / p r e v e n t \u221201\n:ARG0 ( d / d o c t o r )\n:ARG1 ( s / spread \u221203\n:ARG1 ( v / v i r u s ) )\n:ARG3 ( v2 / v a c c i n a t e \u221201\n:ARG0 d\n:ARG1 ( c / company\n: name ( n / name\n: op1 \" P f i z e r \" ) ) )\n: t im e ( d2 / date \u2212 e n t i t y\n: y e a r 2 0 2 1 ) )\nverbs \u2013 converting both vaccinating andvaccination to the common form vaccinate-03 ,\nand even substituting for named entities \u2013 company instead of using its name Pfizer ).\nNotation and Definitions\nAs the term \"frame\" is used in various ways in the literature (e.g., compare Entman\n(1993) and Fillmore (1976)), we briefly clarify at this point the specific meaning of the\nunderlying representation and most important terms for the remainder of the paper. Our\ndefinitions are adapted to be specific for the framing analysis imbued with narrative\ninformation .\nAs an alternative to the graph-based representation, AMR graphs can also be\nrepresented as serialized text using the Penman notation (Kasper, 1989). The Penman\nnotation applies to connected, rooted, directed, acyclic, and labeled graphs, such as\nAMR, which is often even used synonymously (Goodman, 2019). The notation has\na recursive structure concerning its relations denoted by parenthesis, typically also\nindicated using newlines and indentation as a convention for human readability. As\nan example, Figure 1 is equivalent to the Penman notation in Listing 1.\nIn the following, we will clarify the definitions using the AMR annotation\nguideline (Banarescu et al., 2012) and the provided example. We highlight the main\nbuilding blocks in bold, the references to the Penman example in italic, and \u2019lexical\ndefinitions\u2019 with single quotation marks.\nSemantic frames are defined in a language resource (here, PropBank2), which\ncomprises a predefined set of predicates including their sense and associated frame\narguments . For instance, consider the first two lines in Listing 1. The frame (p /\nprevent-01 :ARG0 (d / doctor)) comprises a predicate ( prevent-01 ), with doctor as\nframe argument. Frame arguments have a semantic role assigned to them (e.g., ARG0\nfordoctor ). In the given example, the prevent predicate only has a single sense (i.e.,\n01with the meaning of \u2019stopping in advance\u2019). However, when considering the next\ntwo lines ( :ARG1 (s / spread-03 :ARG1 (v / virus)) ),spread-03 refers to spread in\nthe third sense - \u2019cause to be widely located or distributed\u2019 rather than referring\nto \u2019smear\u2019 (i.e., spread-01) or \u2019extend\u2019 (i.e., spread-02). Also, frame arguments can\nPrepared using sagej.cls\nReiter-Haas et al. 7\nthemselves be frames, with spread-03 being an argument of prevent-01 as denoted\nby the ARG1 relation. Therefore, frames can contain substructures, such as virus\nbelonging to spread-03 as argument. Alternatively, frame arguments can be concepts\ncomprising words and phrases (e.g., doctor orvirus ). Furthermore, Penman uses\nvariables (equivalent to nodes in the graph) to distinguish between their instances and\ndenote instance relations with a slash. Hence, in the example, the dvariable of the\nsemantic frame (v2 / vaccinate-01 :ARG0 d) refers to the same doctor as in the (p /\nprevent-01 :ARG0 (d / doctor)) . Finally, nodes can have associated attributes (e.g.,\ncompany has the name attribute of \"Pfizer\" ).\nHere, we want to emphasize the subtle difference between semantic frames (also\ncalled \"linguistic frames\") and narrative framing (i.e., a form of communicative\nframes), which operate on different levels of language and communication,\nrespectively (Sullivan, 2023)3. In the task at hand, language is essential for studying\nnarrative framing, and therefore depend on semantic frames (which is not necessarily\nthe case for other types of communicative frames, such as art (Sullivan, 2023)).\nConsequently, we use the precisely defined semantic frames as a basis to study more\ncomplex communicative frames (i.e., narrative frames).\nWhen considering the narrative information in the framing analyses, i.e., narrative\nframing , we refer to AMR-subgraphs as potential narratives, such as (p / prevent-\n01 :ARG0 (d / doctor) :ARG1 (s / spread-03 :ARG1 (v / virus))) , and instances, as\nwell as attributes, as narrative elements . In the remainder of the paper, we refer to\nsemantic frames as frames , frame arguments as arguments , and narrative elements\naselements . For brevity, we omit semantic roles and use subject-verb-object notation\nwhere applicable (e.g., doctor prevent-01 spread-03 ).\nMethod\nWe present our approach for frame mining in text-based content based on AMR,\ncomprising a pipeline of three main components (i.e., contribution C1):\n1.AMR parsing with a pretrained BART model and Penman decoder.\n2.Mining narrative elements , such as characters, plot, setting, and the moral of the\nstory.\n3.Analysis of narrative information concerning differences in word usage,\nembedding spaces, and subgraphs.\nWe first describe the main components in detail, before providing a complete\nconceptual description of the pipeline from a technical perspective.\nAMR Parsing\nFor AMR parsing, i.e., the conversion from text to AMR graphs, we use the\nAMRlib4with a pretrained BART-based model (i.e., parse_xfm_bart_base-v0_1_0 ;\nbased on Lewis et al., 2019). The model was trained on the AMR Annotation Release\n3.0 (LDC2020T02; Knight et al., 2021) based on the PropBank annotations (Palmer\net al., 2005) and has a SMATCH score of 82.3, which is a semantic matching score\nPrepared using sagej.cls\n8 Journal Title XX(X)\nbased on F1-measure (refer to Cai & Knight, 2013, for details). AMR parsing also has\nthe advantage of applying multiple linguistic tasks simultaneously, such as co-reference\nresolutions via reentrants in the graph (refer to Szubert, Damonte, Cohen, & Steedman,\n2020, for an overview of different types of reentrants) and named entity recognition (via\nthe name attribute). Hence, it alleviates the need for building sophisticated processing\npipelines. The output of the AMR parser is in PENMAN notation, which is transformed\ninto a graph for mining via the Penman library (Goodman, 2020).\nMining Narrative Elements\nWe first introduce the narrative policy framework (Jones & McBeth, 2010), which\ndescribes an empirical approach to studying policy narratives. Thereby, a narrative\nstructure consists of characters (e.g., heroes/villains or victims), a plot (i.e., actions),\na setting or context, as well as the moral of the story. The narrative policy framework\nprovides the theoretical grounding for mining the narrative information. Specifically,\nwe extract the narrative elements, i.e., characters and plots, by considering the AMR\nedge information.\nCharacters and plots are described as simple (<subject>, <verb/predicate>, <object>)\ntriples, such as, e.g., we protect them . A more general representation of the plot and\nits corresponding characters is as a variable-length tuple of the format: (<predicate>,\n<argument0>, <argument1>, . . . , <argumentN>), which resembles PropBank frames.\nFrame arguments can be other frames (e.g., vaccinate-01 or spread-03), concepts (e.g.,\nnouns such as doctor, company, or virus), or attributes (e.g., named entities such as\nPfizer or a year such as 2021).\nCharacters. For the characters (orange), we consider instances of ARG0 orARG1\nroles. While frames can have more than these two arguments (i.e., ARG2 and beyond),\nthey tend to appear less often and hence play a less important role, as the highest-\nranked (i.e., the lowest number) argument precedes according to the PropBank\nguidelines (Babko-Malaya, 2005). Hence, we focus on the first two arguments for\nsimplicity. Due to reentrants in the graph, characters can assume multiple (possibly\neven different) roles. This is exemplified in Figure 1 as seen by the doctor in the\nexample, who acts twice as a character.\nAccording to the narrative policy framework, characters can be categorized as\nheroes, villains, or victims. In the present work, we do not distinguish between these\nsubtypes of characters5.\nPlot. For the plot (blue), we use the predicates of the semantic frames directly. To\nfind the frames (i.e., predicates), we reverse the traversal of the graph (i.e., go up\nfrom ARG0 orARG1 arguments to parent nodes and towards their instances). One\nobservation is that the plot is driven by verbs and indicated by other words that can be\nencoded in frames. In the example given in Figure 1, the spread is part of the plot, as it\nsuggests the distribution of a virus (i.e., ARG1 ) but does not detail who the spreader is\n(i.e., misses an ARG0 ).\nSetting. For the setting (green), we consider the special time andlocation relations.\nThese represent the context in which the narration is embedded and are typically\nassociated with attributes (purple), such the specific year. Compared to the characters\nPrepared using sagej.cls\nReiter-Haas et al. 9\nand plot, the attributes can be more diverse as they are not bounded by the number\nof common words and their normalization. For instance, considering the range of\npharmaceutical companies researching vaccinations for COVID-19, some associated\nnamed entities are more commonly portrayed (e.g., Pfizer), while others are only\nrarely mentioned (e.g., Sanofi). Similarly, certain temporal or spatial information might\nappear more frequently in relation to particular topics (e.g., 2021 for COVID-19).\nNevertheless, these types of information are unbounded by definition. In the analysis,\nwe thus differentiate between types of the settings, such as the narrative refer to a year,\nand their specific attributes, e.g., 2021.\nMoral of the Story. Similar to the setting, the moral of the story (i.e., reason) relies\non specific relations, i.e., purpose andcause . Unlike the setting, these relations often\ncomprise concepts or even complete subgraphs. Here, we use the top element (i.e.,\nroot of the subgraph), which carries the most meaningful information. Moreover, as\nmany sentences do neither include a purpose nor cause, such relations are only sparsely\navailable. Nevertheless, they provide important narrative information.\nAnalysis of Narrative Information\nWe compare the narrative information extracted between the mainstream and\nconspiracy corpus. Herein, we use the log-odds ratio to diminish the influence of\npredominant characters and plot devices in terms of relative frequency to each other,\ne.g., similar to Jing and Ahn (2021). However, we leverage smoothed log-odds ratio\ninstead of informative Dirichlet priors (Monroe, Colaresi, & Quinn, 2008), and thus do\nnot require a separate background corpus. The complete equation, which also includes\nZ-score normalization, is given by:\nzw=logfi(w)+1\nni\u2212fi(w)+1\u2212logfj(w)+1\nnj\u2212fj(w)+1q\n1\nfi(w)+1+1\nfj(w)+1(1)\n, where fi(w)andfj(w)represent the frequency of a given word win its\ncorresponding sub-corpus, while niandnjrepresent the total number of words per\nsub-corpus (i.e., ni=P\nw\u2208Vfi(w)with Vcontaining all words and similarly for\nnj=P\nw\u2208Vfj(w)). Hence, the enumerator of Equation 1 corresponds to a relative\nprobability that is symmetric due to the log transformation, while the denominator\naccounts for the variance. Consequently, over-represented words in the given sub-\ncorpus get a high absolute value. The sign indicates the dominant sub-corpus, while\nthe magnitude of the score (i.e., absolute value) is equivalent for both sub-corpora.\nHence, negative values show the over-representativeness in the alternative sub-corpus\nwithout requiring recalculation.\nVisualization for elements. We plot the over-represented words (indicating plot,\ncharacters, setting, and moral of the story) in a shared two-dimensional embedding\nspace using UMAP-reduced embeddings (McInnes, Healy, & Melville, 2018) of\nthe model\u2019s input layer side by side for comparison6. The positioning of the plot\nimproves the analysis by positioning semantically similar words in a similar region, and\nthus improves the subsequent interpretation. For readability, we use a force-adjusted\nPrepared using sagej.cls\n10 Journal Title XX(X)\npositioning for the labels. To declutter the plot, we simplify the labels by removing\nthe sense tags (as a distinction between word senses is typically not necessary anyway\nin this particular case). In a similar vein, we only keep the first part of compound\nconcepts, e.g., government instead of government-organization , which follows the\nsame rationale.\nNotation for narratives. For readability, we also provide a short notation to represent\nframes and their corresponding arguments, as well as their associated z-score. We\ndenote the frames with ARG01.0\u2190 \u2212 \u2212 FRAME-01 andFRAME-011.0\u2212 \u2212 \u2192 ARG1 respectively.\nSpecifically, ARG0 appears left of the frame with a left arrow, while ARG1 appears on\nthe right with a right arrow. Consequently, the two relations can be combined to form\nan ARG0\u2013FRAME\u2013ARG1 triplet. For instance, doctor1.0\u2190 \u2212 \u2212 prevent-011.0\u2212 \u2212 \u2192 spread-03\nreads similar to the well-known subject\u2013verb\u2013object structure. Above the arrows, we\nprovide the z-score of the log-odds-ratio between the two corpora.\nPipeline Description\nThe text is tokenized and fed into an embedding layer of a pretrained BART model.\nThe input token embeddings are combined with positional embeddings and fed into a\nbidirectional encoder stack comprising multiple encoder layers for text understanding.\nThe resulting representation is then in turn fed into an autoregressive decoder stack\n(again comprising multiple decoder layers) to iteratively generate the PENMAN\nrepresentation. A PENMAN decoder then creates a graph-based representation. By\ntraversing the graph from its root, the Frame Miner component extract the relevant\ninformation (narrative elements). The aggregation of the information is then divided\ndepending on the label. Using the frequency information, we can compare the\noccurrences and calculate a score over-representative elements for each label. We use\nthe top-N (positive score) and bottom-N (negative score) elements and plot them in a\nword embedding space for analysis. Here, we want to stress that we distinguish between\ndifferent word types, such as Frame and ARG0. Note that we also provide a detailed\ndiagram of the approach in the supplemental materials.\nAdditionally, we emphasize that the approach is easily extensible. For instance, we\ncould inject sentiment information to distinguish between the usage of words from the\nword frequencies, i.e., to derive the character sentiment for a hero vs. villain distinction.\nSimilar, other information could be extracted by including dictionaries, e.g., for a value-\nbased analysis. However, this goes beyond the scope of work, i.e., pure AMR-based\nanalysis of narrative information.\nExperiments and Results\nWe present our analysis and empirical results of health-related framing (i.e.,\ncontribution C2). Specifically, we investigate health-related narratives and report our\nfindings in three topics (i.e., Covid-19, general diseases, and pharmacology). To that\nend, we leverage a publicly available dataset (i.e., LOCO) containing media content\nfrom various online information sources.\nPrepared using sagej.cls\nReiter-Haas et al. 11\nDataset and Preprocessing\nWe use the LOCO dataset (Miani et al., 2021), which contains documents collected\nfrom English-speaking news websites concerning both mainstream and conspiracy\nmedia7. The documents were collected from May to July 2020 via web scraping\n(thus, also including older documents dating back to 2004 for the oldest conspiracy\ndocument) using a combination of predefined sources and manual seed selection while\nexcluding non-English domains from the collection (refer to Miani et al. (2021) for the\ncomplete data collection and processing details). Documents are labeled as conspiracy\nif they originate from a website known to publish \"unverifiable information that is not\nalways supported by evidence\" (as determined by the Media Bias/Fact Check list8) and\nmainstream otherwise. The corpus comprises 72,806mainstream documents from 92\nwebsites and 23,937conspiracy documents from 58websites on 47seeds.\nWe consider three health-related subcorpora. First, we focus on the documents\non COVID-19-related topics, i.e., we use the following seeds as defined by LOCO:\nvaccine.covid ,covid.19 , and coronavirus . Second, we consider documents related to\ndisease with the seeds aids,cancer ,zika.virus , and ebola . Third, the pharmacology\nLOCO subset comprises documents with the seeds vaccine ,pharma , and drug .\nConsidering the time (see Figure 2a), we observe that the majority of documents for\nCOVID-19 and pharmacology appear in 2020 with peaks in May and June (COVID-\n19 specific peak) just before the end of data collection on July 3rd, 2020. Note,\nhowever, that both disease and pharmacology have more documents overall compared\nto COVID-19, which is in turn more clumped in 2020. This in turn also result in a\ngreater number of graphs and narrative elements.\nIn Figure 2b, we observe that both mainstream and conspiracy media resemble a\nlognormal distribution in terms of document length (we only depict the distribution for\nthe number of characters, but observe similar distributions for the number of words and\nsentences). On average, each document consists of 5455 characters, 1009 words, and\n38sentences. However, conspiracy documents are more concentrated near the median\n(i.e., red line at 3805 ). We extract the AMR graphs using the methodology described\nin the Method Section9. The detailed statistics are described in Table 1.\nAnalysis of Narrative Information\nWe contrast the narrative framing of COVID-19 in the mainstream and conspiracy\ncorpus. In Figure 3a (we also provide the Table with the top 15 over-represented words\nper corpus per narrative element type with their associated score in the supplementary\nmaterials.), we observe that conspiracy media tends to focus on argumentation frames\nas plot, such as believe ,claim ,lie, and oppose . Conversely, mainstream media focuses\non action-oriented frames like develop ,spread , and reopen . Similarly, mainstream\nmedia uses science-related characters such as scientists ,vaccines ,antibodies , and\nproteins . In comparison, conspiracy media use typical characters that suggest large-\nscale conspiracies, such as world ,elites ,truth , and power . When considering the\ncontexts, we note that conspiracy media is more focused on the now, such as today\nortomorrow , rather than specific weeks ormonths as is the case for mainstream media.\nFinally, when considering the moral of the story, conspiracy media reasons more\nconcerning alarm , while mainstream uses information in its narratives.\nPrepared using sagej.cls\n12 Journal Title XX(X)\n(a)Number of documents in LOCO per subcor-\npus since 2020. We observe an increasing trend\nfor COVID-19 and Pharmacology, with two sepa-\nrate peaks in May and June. Hence, it resembles\nthe COVID-19 waves in English-speaking coun-\ntries (Mathieu et al., 2020). The relative peaks\nare about equal for COVID-19, while the first\npeak is far more pronounced in Pharmacology.\nNo such trend is present for the disease dataset.\nWe omitted plotting the data before 2020 due to\na lack of noteworthy peaks.\n(b)Histogram of document length, i.e., number\nof characters, in the three subcorpora of LOCO.\nThe X-axis is on a log scale. Hence, both\nconspiracy and mainstream media resemble\nlognormal distributions, but conspiracy media is\nmore pronounced in the median (as depicted\nby the red line). This observation is consistent\nboth when considering individual subcorpora,\nand concerning the number of words and\nsentences. Overall, there are more mainstream\nthan conspiracy documents.\nFigure 2. Details of the LOCO dataset in terms of temporality and distribution.\nAnother difference is the war-focused framing in conspiracy media (e.g., using\ndestroy as frame, military as character, and counter as moral of the story). Whereas\nmainstream media has a more health-oriented framing (e.g., infect being used both\nfor the plot and as character, while treat acts as rationale). Besides, we also briefly\ninvestigated the associated attributes (see Table in supplementary materials10), such as\nnamed entities, where we observe that narratives in conspiracy media revolve about\npeople, such as Gates andTrump , as well as religion (e.g., Jews andChristians ) and\nhave a focus on US/China . In comparison, mainstream media focus on institutions,\nsuch as universities and the NHS .\nIn the disease dataset (see Figure 3b), we observe many similarities to the COVID-19\ndataset. However, we also notice a shift, especially in the entities of mainstream media,\ntoward global south countries where the diseases are more prevalent. Furthermore, in\nconspiracy media, the narrations shift toward non-natural origins such as engineering ,\nweapons , and chemical .\nIn the pharmacology dataset (see Figure 3c), the mainstream media uses the drug\ncompany names as entities and the development and manufacturing as plots with\ntreatment-related characters such as dose. Conspiracy media shows its mistrust with\nterms like corrupt ,kill, and control .\nWhile all three datasets exhibit similarities, we also observe specific differences.\nMost notably, the mainstream disease dataset has a stronger emphasis on the role of\nwomen due to female-associated elements (e.g., she,woman ,pregnancy ,care).\nPrepared using sagej.cls\nReiter-Haas et al. 13\nCOVID-19 Disease Pharma\ntotal # conspiracy mainstream conspiracy mainstream conspiracy mainstream\nDocuments 2,414 6,308 2,877 8,296 3,914 9,839\nGraphs 99,728 255,622 150,189 287,897 215,294 364,979\nPlots 436,780 1,193,282 622,873 1,332,290 921,102 1,758,036\n,\u2192unique 6,577 7,656 7,573 8,439 8,303 9,398\nCharacters 419,764 1,143,711 598,708 1,270,719 882,955 1,689,006\n,\u2192unique 12,500 15,981 15,354 17,890 16,845 20,177\nSettings 68,807 195,564 96,141 221,733 128,206 248,783\n,\u2192unique 3,042 4,243 3,861 4,720 4,103 4,999\nMoral o.t.S. 9,433 25,899 12,562 25,772 19,022 38,179\n,\u2192unique 1,769 2,371 2,069 2,339 2,438 2,945\nEntities 164,993 395,068 250,144 510,859 341,763 596,920\n,\u2192unique 17,339 36,379 27,102 40,754 30,613 48,659\nTable 1. Dataset statistics regarding the number of extracted elements. Each document\ncontains several graphs, which in turn contains elements of different types. We also report\nthe number of unique elements per type.\nAnalysis of Narratives. To gain a clearer picture of how the frames are used, we\ninvestigate the differences in arguments (i.e., ARG0 andARG1 ) in three frames from\nthe initial example (i.e., prevent-01 ,spread-03 , and vaccinate-01 ). In general, we find\nthatARG1 is more suitable for the frames, as they have the highest scores. Here, we\nhighlight noteworthy examples of narratives.\nIn COVID-19, conspiracy media mainly invokes prevent-013.7\u2212 \u2212 \u2192 violence , but also\ninvokes the government-organization3.6\u2190 \u2212 \u2212 prevent-012.9\u2212 \u2212 \u2192 individual . Hence, their\nfocus does not lie in the prevention of the virus. In comparison, mainstream media\nfocus on the infection with prevent-015.1\u2212 \u2212 \u2192 infect-01 . For spread-03 , conspiracy\ntheories often focus on spreading rumors but also invoke vaccine4.2\u2212 \u2212 \u2192 spread-03 ,\nsuggesting that the vaccine spreads the disease. In contrast, mainstream media has a\nclear focus on the viral spread with person1.0\u2190 \u2212 \u2212 spread-033.3\u2212 \u2212 \u2192 virus . For vaccinate-\n01,military2.9\u2190 \u2212 \u2212 vaccinate-01 is common for conspiracy media, whereas, vaccinate-\n011.8\u2212 \u2212 \u2192 person is common for mainstream media. We also analyzed differences in\nframe arguments. As a noteworthy example, conspiracy media is less concerned about\npreventing the virus and that the vaccine might spread the disease.\nWe observe similar patterns for diseases in general and pharmacology. Regarding\nthe usage of the prevent-01 frame in pharmacology, we observe person4.3\u2190 \u2212 \u2212 prevent-01\nin conspiracy and prevent-016.8\u2212 \u2212 \u2192 infect-01 in mainstream media as the top (i.e., over-\nrepresentative) narratives. Similarly, the usage of spread-03 frame in other diseases,\nvaccine4.3\u2190 \u2212 \u2212 spread-03 andspread-034.1\u2212 \u2212 \u2192 virus are dominant for conspiracy and\nmainstream media, respectively. Hence, the narratives are mostly mirrored between\nthe different sub-datasets.\nPrepared using sagej.cls\n14 Journal Title XX(X)\ndestroywar\ncause\nfreebelieve\nwordopposemainstream\nnatural\nnote\nlieclaimtrue\nstatelegal\nnothing\ntheytruthpower\nmedia\nmilitary\nworldquestion\nlaw\niarticlepolitical\nyoubook\ntodayabove\njustpublication\nplanet\ntomorrowhereformer\nlist finalbelow\nnoweveralarm\nbattle\ncounter\nnewspossible\npurposesuppressattempt\nsavebenefit\nkeepresist\ndistractConspiracy\nPlot\nCharacters\nSetting\nMoral of the Story\ncare\nspreadinfect\ntest\ncasepossible\ndevelop\nhaverisk\ncontactwear\nwork\nreopen\nnewsaydrug\nmask\nsymptomvaccine\nantibodyviruspatient\ndisease\ncompany\nhome\nmonth\nweekas\nbydatecommunity\nsurfacesetting\nsocityafterhospital\nearly\ntreatadvise\nassess\ncheckslow\nseesupport\ntravelhelporganization allowensure\nreduceinformationMainstream\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n(a)COVID-19\ncreateengineer\nactualtrue\nbelieve\nliestatereadnatural\ndestroyoppose\nclaimlegal\ngovernmainstream\nlawmedia government oilpower\ntheyarticle\nmilitary\nyou\niweapon\nwarbookquestion\ntruth\ntodaybelowbehind\nplanetformerever\nnow\nworldfacility\nabove\nall\nhereprove\njustifyalarm\neffort\nsavebenefit\nseat\nforcepurpose\ncounterprofit\nexpand\nfreeexperimentConspiracy\nPlot\nCharacters\nSetting\nMoral of the Story\ndetect\nspreadprevent\ninfectrisk\ntraveldiagnose\ntreatcase\nnewsayoutbreak\ncare\ntransmitpatient\ndiseasemosquito\nshecellhealthwoman\nvirus drugcity\ncountrypregnancy\nsince\nmonthcurrent\nstageregion\nyear eastarea\nworld earlyafterfight\nsupport\nhelp\ncheckinformation\naddress\nscreencombatensure\nassess\nrespondreduceMainstream\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n(b)Diseases\nnatural\ncause\nliegovern wartruecorrupt\nmainstream\ncontrolengineerkill\nreaddestroy\nknow\nfreetruth\ni\nyouarticlegovernment\ntheyquestion\ncitizenmedia\nmilitarybookpower\ncorporationman\nthenbehindeverrealm\nnowplanetbelow\nabout world\ntodayurlabove\nhereexperiment\nlivecounter\nnewssavebenefit\nrightprofit\ndefendjustify\nversionalarmConspiracy\nPlot\nCharacters\nSetting\nMoral of the Story\nhaveprice\nlow\nnewresearch\nmanufacturedevelopcost\ntreatworksayinfect possible\naccess\ntestantibodypatient\ncandidate proteindrug\ntrydisease\ncompanyvaccine\nafter\nsoglobe\nweekcity\nearly\nby\ndate\nyearmonthcommunityuniversity\ncountry\nstateaddressassessensure\nmakemeetimproveinformationmeasles\nhelpMainstream\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\n(c)Pharmacology\nFigure 3. Over-represented narrative elements (i.e., plot, characters, setting, moral of the\nstory) on COVID-19 in conspiracy versus mainstream media. Positioning is according to\n2-dimensional UMAP embedding of the AMR input layer (i.e., semantically similar words\nappear in similar locations), and labels are force-adjusted for readability (with lines indicating\ntheir associated positioning if moved beyond a threshold).\nDiscussion\nWe now briefly discuss the implications of our findings on five distinct aspects.\nNarrative Themes. Apart from the well-known belief and faith focus of conspiracy\noutlets, our analyses of the different narratives highlights that the conspiracy sites\nemphasize an urgency of the social problem (\"Today\", \"Now\", etc.) and also an\nimmediacy of the issue (\"You\", \"I\", and \"We\"). These characterizations are in line\nPrepared using sagej.cls\nReiter-Haas et al. 15\nwith the description of conspiracy adherer (Douglas et al., 2019). Interestingly,\nconspiracy sites also see the mainstream and media as characters and part of the game,\nwhereas mainstream does not refer to conspiracists - which points to a non-reciprocity.\nSimilarly, we find a corroborative emphasis (\"truth\", \"true\", \"actual\"), which might\nsuggest another demarcation to mainstream media. Finally, while war-framing is often\npresent in health-related discourse (e.g., as shown in Wicke & Bolognesi, 2020), we\nobserve a one-sided tendency towards war-framing in conspiracy media.\nSocietal Implications. Our work shows a clear distinction between the narratives\nin conspiracy and mainstream media. While this finding on its own is expected, we\ncan draw parallels to prior studies. For instance, Shelton (2020) suggests that the\nCOVID-19 pandemic was the first post-truth pandemic . Our work complements this\nfinding, as a belief-oriented framing in conspiracy media competes with a science-\noriented framing in mainstream media. Thus, we see that the media have different\nlines of argumentation on the same issue, which can influence and bias people\u2019s\nattitudes and drive polarization, e.g., through diverging assessments of the consensus in\nsociety regarding the seriousness of the COVID-19 pandemic (Logemann & Tomczyk,\n2023). A deeper understanding of the main competing narratives is therefore essential\nfor improving the ability to spot fake news (Porshnev, Miltsov, Lokot, & Koltsova,\n2021) and for automatic detection to identify and combat conspiracy theories in the\nmedia (Shahsavari, Holur, Wang, Tangherlini, & Roychowdhury, 2020). Knowledge of\nconspiracy narratives can thus be used to improve the dissemination guidelines of social\nmedia platforms or handbooks developed by policymakers, e.g., the \u2019Check Before\nYou Share Toolkit\u2019 in the UK (Bloomfield, Magnusson, Walsh, & Naylor, 2021). It\ncan also be used to educate society at an individual level, e.g., through \u2019fake news\ngames\u2019 where players learn to identify manipulation techniques commonly used in\nconspiracy theories (Basol et al., 2021). Yet, we need to acknowledge that our analysis\nis only highlighting the structure of arguments, but does not consider the reach of\nthese sources. However, based on previous research (Reiter-Haas, Kl\u00f6sch, Hadler, &\nLex, 2022), we can estimate that COVID-19 conspiracy beliefs are more widespread\nthan belief in other conspiracy theories (Uscinski et al., 2022). Assessing content and\narguments in online media is thus of utmost importance.\nMethodological Advancements. Our work is based on the premise that text\nanalyses are challenging, especially when considering more abstract concepts, such\nas framing (partly also due to a lack of a clear definition (Entman, 1993)). Hence,\ngraph representations such as AMR allow for a more comprehensive analysis, which\nis supported by our approach. Most text processing tasks are directly handled by\nAMR parsing, which is conceptually easy to employ using pretrained models based on\nthe Transformer architecture (Vaswani et al., 2017). Our approach demonstrates that\nnarrative elements can directly be mapped onto AMR and thus extracted. For instance,\nthe four elements of the narrative policy framework (Jones & McBeth, 2010) are\ndirectly applicable. Moreover, similar elements as described by Piper et al. (2021) can\nbe extracted (besides the perspective, as it is typically not part of the textual content).\nFinally, we show that we can perform a wide range of analyses on the extracted\ninformation.\nPrepared using sagej.cls\n16 Journal Title XX(X)\nTechnical Limitations. We recognize two main limitations of our work: First,\nwhile AMR provides an expressive semantic representation of narrations, more subtle\ninformation, such as sentiment, cannot be extracted directly. Hence, AMR graphs\nwould require external resources for sentiment analysis (e.g., sentiment polarity\nlexicons); moreover, while AMR encodes direct negations, considering indirect\nnegations (e.g., via prevent-01 frame) would require yet another external resource.\nSecond, while we show that AMR provides understandable narrative elements on our\nEnglish-based dataset, the generalizability to other datasets, domains, languages, and\nmore complex narratives is yet subject to more research. Large-scale and rigorous\nexperiments (e.g., a linguistic evaluation of the constructs by experts) would be\nrequired to further validate AMR graphs\u2019 explainability, effectiveness, reliability, and\naccuracy in narrative extraction.\nEthical Considerations. As conspiracy theories are a sensitive societal topic, we\noutline three primary ethical considerations of our research. First, our analyses are\nbased on a publicly available dataset that includes information from publicly available\nnews media. The presented results are highly aggregated and do not allow the\nidentification of any individual website or person. The harm to human subjects is thus\nnegligible. Second, as we leverage pretrained language models, we are also subject\nto their inherent biases. Third, our approach aims to better understand conspiracy\nnarratives rather than advocating any of the knowledge attained. A better understanding\ncould counteract conspiracy theories, but coincidentally also enable a better framing of\nconspiracy theories. Still, an improved understanding of diverging/competing frames in\nconspiracy and mainstream media can, in general, be seen as having a positive impact\non society.\nConclusion\nIn the present work, we discussed how semantics derived from AMR graphs relate\nto the framing of narrative content. We showed that AMR is an ideal fit to analyze\nnarrative frames, as we can directly extract context, characters, and plot from its\ngraph representation. Using AMR, we introduced a conceptually simple to employ\nbut flexible approach ( C1). We demonstrated the merits of our approach for framing\nanalysis by contrasting conspiracy to mainstream media on three health-related topics\n(C2), i.e., COVID-19, diseases, and pharmacology.\nWe observe that all three topics paint a similar picture of conspiracy media (i.e.,\na tendency towards beliefs instead of science). Hence, our approach provides a more\nholistic view of conspiracy narratives than previous research. We hope that our work\ninspires future research related to nuanced framing analysis.\nPrepared using sagej.cls\nReiter-Haas et al. 17\nNotes\n1. We also experimented with a BERT variant for both topic modeling and semantic role\nlabeling but found richer AMR representations better suited for the task at hand.\n2.https://propbank.github.io/v3.4.0/frames/\n3. Both, in turn, rely on a third type of cognitive frames, which operate at the level of thought.\nWhile implicitly required, cognitive frame are not the focus in the present work and thus\nomitted.\n4.https://github.com/bjascob/amrlib\n5. A naive approach to model the subtypes, is to use sentiment analysis to distinguish between\nheroes (positive) and villains (negative) portrayed characters. However, sentiment is not part\nof AMR (only sentence polarity) and thus would require external resources (e.g., dictionaries\nor models). As our work focuses on AMR for narrative analysis, we omit such analysis for\nbrevity and leave it as future work.\n6. We also experimented with PCA for dimensionality reduction and pretrained GloVe\nembeddings, which are two other often used approaches, respectively. However, we find that\nUMAP better preserve semantic similarity, while using the model\u2019s inherent embeddings\nallows for a better mapping, as it avoids a domain shift.\n7. In a pre-study, we analyzed whether similar topics are discussed in the mainstream\nand conspiracy corpus using BERTopic (Grootendorst, 2022). We observed differences,\nespecially regarding the discussed nouns, as conspiracy media are more concerned with\ntopics such as COVID-19 origin, vaccination, and President Trump. In contrast, mainstream\nmedia focuses on drug trials, testing, and the economy. Hence, such a method is too limited\nto analyze narratives as it gives us mainly the context of a story; we, however, are interested\nin the characters, plot, and the moral of the story.\n8.https://mediabiasfactcheck.com/conspiracy/\n9. We ran the calculation on a shared SLURM-managed server using a single Nvidia Quadro\nRTX 8000. The calculation for each of the three subsets took approximately a day but could\ndiffer depending on server utilization.\n10. As attributes can be arbitrary, such as names of entities, their embeddings cannot directly be\nextracted from the AMR model. Hence, they do not possess a specific position in the graph,\nwhich is why we omitted plotting them and refer to the data instead.\nPrepared using sagej.cls\n18 Journal Title XX(X)\nReferences\nAli, M., & Hassan, N. (2022). A survey of computational framing analysis approaches.\nProceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pages 9335\u20139348, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics. .\nBabko-Malaya, O. (2005). Propbank annotation guidelines. URL: http://verbs.\ncolorado. edu .\nBaker, C. F., Fillmore, C. J., & Lowe, J. B. (1998). The berkeley framenet project.\nInColing 1998 volume 1: The 17th international conference on computational\nlinguistics.\nBanarescu, L., Bonial, C., Cai, S., Georgescu, M., Griffitt, K., Hermjakob, U., . . .\nSchneider, N. (2012). Abstract meaning representation (amr) 1.0 specification.\nInParsing on freebase from question-answer pairs. in proceedings of the 2013\nconference on empirical methods in natural language processing. seattle: Acl\n(pp. 1533\u20131544).\nBanarescu, L., Bonial, C., Cai, S., Georgescu, M., Griffitt, K., Hermjakob, U., . . .\nSchneider, N. (2013). Abstract meaning representation for sembanking. In\nProceedings of the 7th linguistic annotation workshop and interoperability with\ndiscourse (pp. 178\u2013186).\nBasol, M., Roozenbeek, J., Berriche, M., Uenal, F., McClanahan, W. P., & Linden,\nS. v. d. (2021). Towards psychological herd immunity: Cross-cultural evidence\nfor two prebunking interventions against covid-19 misinformation. Big Data &\nSociety ,8(1), 20539517211013868.\nBloomfield, P. S., Magnusson, J., Walsh, M., & Naylor, A. (2021). Communicating\npublic health during covid-19, implications for vaccine rollout. Big Data &\nSociety ,8(1), 20539517211023534.\nBurscher, B., Vliegenthart, R., & Vreese, C. H. d. (2016). Frames beyond words:\nApplying cluster and sentiment analysis to news coverage of the nuclear power\nissue. Social Science Computer Review ,34(5), 530\u2013545.\nCai, S., & Knight, K. (2013). Smatch: an evaluation metric for semantic feature\nstructures. In Proceedings of the 51st annual meeting of the association for\ncomputational linguistics (volume 2: Short papers) (pp. 748\u2013752).\nDemszky, D., Garg, N., V oigt, R., Zou, J., Gentzkow, M., Shapiro, J., & Jurafsky, D.\n(2019). Analyzing polarization in social media: Method and application to tweets\non 21 mass shootings. arXiv preprint arXiv:1904.01596 .\nDouglas, K. M., Uscinski, J. E., Sutton, R. M., Cichocka, A., Nefes, T., Ang, C. S., &\nDeravi, F. (2019). Understanding conspiracy theories. Political psychology ,40,\n3\u201335.\nEntman, R. M. (1993). Framing: Towards clarification of a fractured paradigm.\nMcQuail\u2019s reader in mass communication theory ,390, 397.\nFillmore, C. J. (1976). Frame semantics and the nature of language. In Annals of\nthe new york academy of sciences: Conference on the origin and development of\nlanguage and speech (V ol. 280, pp. 20\u201332).\nFillmore, C. J., & Baker, C. F. (2001). Frame semantics for text understanding. In\nProceedings of wordnet and other lexical resources workshop, naacl (V ol. 6).\nPrepared using sagej.cls\nReiter-Haas et al. 19\nFong, A., Roozenbeek, J., Goldwert, D., Rathje, S., & van der Linden, S. (2021). The\nlanguage of conspiracy: A psychological analysis of speech used by conspiracy\ntheorists and their followers on twitter. Group Processes & Intergroup Relations ,\n24(4), 606\u2013623.\nGoodman, M. W. (2019). Amr normalization for fairer evaluation. arXiv preprint\narXiv:1909.01568 .\nGoodman, M. W. (2020, July). Penman: An open-source library and tool for\nAMR graphs. In Proceedings of the 58th annual meeting of the association\nfor computational linguistics: System demonstrations (pp. 312\u2013319). Online:\nAssociation for Computational Linguistics. Retrieved from https://\naclanthology.org/2020.acl-demos.35 doi: 10.18653/v1/2020.acl\n-demos.35\nGrootendorst, M. (2022). Bertopic: Neural topic modeling with a class-based tf-idf\nprocedure. arXiv preprint arXiv:2203.05794 .\nHuguet Cabot, P.-L., Dankers, V ., Abadi, D., Fischer, A., & Shutova, E. (2020,\nNovember). The Pragmatics behind Politics: Modelling Metaphor, Framing\nand Emotion in Political Discourse. In Findings of the association for\ncomputational linguistics: Emnlp 2020 (pp. 4479\u20134488). Online: Association\nfor Computational Linguistics. Retrieved from https://aclanthology\n.org/2020.findings-emnlp.402 doi: 10.18653/v1/2020.findings\n-emnlp.402\nJing, E., & Ahn, Y.-Y . (2021). Characterizing partisan political narrative frameworks\nabout covid-19 on twitter. EPJ data science ,10(1), 53.\nJones, M. D., & McBeth, M. K. (2010). A narrative policy framework: Clear enough\nto be wrong? Policy studies journal ,38(2), 329\u2013353.\nKasper, R. T. (1989). A flexible interface for linking applications to Penman\u2019s\nsentence generator. In Speech and natural language: Proceedings of a workshop\nheld at philadelphia, Pennsylvania, February 21-23, 1989. Retrieved from\nhttps://aclanthology.org/H89-1022\nKnight, K., Badarau, B., Baranescu, L., Bonial, C., Bardocz, M., Griffitt, K., . . . others\n(2021). Abstract meaning representation (amr) annotation release 3.0. Abacus\nData Network.\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., . . .\nZettlemoyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehension. arXiv preprint\narXiv:1910.13461 .\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., . . . Stoyanov, V . (2019).\nRoberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692 .\nLogemann, H. T., & Tomczyk, S. (2023). How media reports on covid-19 conspiracy\ntheories impact consensus beliefs and protective action: A randomized controlled\nonline trial. Science Communication ,45(2), 145\u2013171.\nMathieu, E., Ritchie, H., Rod\u00e9s-Guirao, L., Appel, C., Giattino, C., Hasell, J., . . .\nRoser, M. (2020). Coronavirus pandemic (covid-19). Our World in Data .\n(https://ourworldindata.org/coronavirus)\nMcInnes, L., Healy, J., & Melville, J. (2018). Umap: Uniform manifold approximation\nPrepared using sagej.cls\n20 Journal Title XX(X)\nand projection for dimension reduction. arXiv preprint arXiv:1802.03426 .\nMcLeod, D. M., Choung, H., Su, M.-H., Kim, S.-J., Tao, R., Liu, J., & Lee, B. (2022).\nNavigating a diverse paradigm: A conceptual framework for experimental\nframing effects research. Review of Communication Research ,10.\nMiani, A., Hills, T., & Bangerter, A. (2021). Loco: The 88-million-word language of\nconspiracy corpus. Behavior research methods , 1\u201324.\nMonroe, B. L., Colaresi, M. P., & Quinn, K. M. (2008). Fightin\u2019words: Lexical feature\nselection and evaluation for identifying the content of political conflict. Political\nAnalysis ,16(4), 372\u2013403.\nPalmer, M., Gildea, D., & Kingsbury, P. (2005). The proposition bank: An annotated\ncorpus of semantic roles. Computational linguistics ,31(1), 71\u2013106.\nPiper, A., So, R. J., & Bamman, D. (2021, November). Narrative theory for\ncomputational narrative understanding. In Proceedings of the 2021 conference\non empirical methods in natural language processing (pp. 298\u2013311). Online and\nPunta Cana, Dominican Republic: Association for Computational Linguistics.\nRetrieved from https://aclanthology.org/2021.emnlp-main.26\ndoi: 10.18653/v1/2021.emnlp-main.26\nPiskorski, J., Stefanovitch, N., Da San Martino, G., & Nakov, P. (2023, July). Semeval-\n2023 task 3: Detecting the category, the framing, and the persuasion techniques\nin online news in a multi-lingual setup. In Proceedings of the 17th international\nworkshop on semantic evaluation. Toronto, Canada.\nPorshnev, A., Miltsov, A., Lokot, T., & Koltsova, O. (2021). Effects of\nconspiracy thinking style, framing and political interest on accuracy of fake news\nrecognition by social media users: evidence from russia, kazakhstan and ukraine.\nInInternational conference on human-computer interaction (pp. 341\u2013357).\nReiter-Haas, M., Kl\u00f6sch, B., Hadler, M., & Lex, E. (2022). Polarization of opinions on\ncovid-19 measures: integrating twitter and survey data. Social Science Computer\nReview , 08944393221087662.\nReiter-Haas, M., Kopeinik, S., & Lex, E. (2021). Studying moral-based differences in\nthe framing of political tweets. arXiv preprint arXiv:2103.11853 .\nShahsavari, S., Holur, P., Wang, T., Tangherlini, T. R., & Roychowdhury, V . (2020).\nConspiracy in the time of corona: automatic detection of emerging covid-19\nconspiracy theories in social media and the news. Journal of computational\nsocial science ,3(2), 279\u2013317.\nShelton, T. (2020). A post-truth pandemic? Big Data & Society ,7(2),\n2053951720965612.\nShi, P., & Lin, J. (2019). Simple bert models for relation extraction and semantic role\nlabeling. arXiv preprint arXiv:1904.05255 .\nShurafa, C., Darwish, K., & Zaghouani, W. (2020). Political framing: Us covid19\nblame game. In International conference on social informatics (pp. 333\u2013351).\nSullivan, K. (2023). Three levels of framing. Wiley Interdisciplinary Reviews:\nCognitive Science , e1651.\nSzubert, I., Damonte, M., Cohen, S. B., & Steedman, M. (2020, November). The\nrole of reentrancies in Abstract Meaning Representation parsing. In Findings\nof the association for computational linguistics: Emnlp 2020 (pp. 2198\u20132207).\nOnline: Association for Computational Linguistics. Retrieved from https://\nPrepared using sagej.cls\nReiter-Haas et al. 21\naclanthology.org/2020.findings-emnlp.199 doi: 10.18653/v1/\n2020.findings-emnlp.199\nTausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words:\nLiwc and computerized text analysis methods. Journal of language and social\npsychology ,29(1), 24\u201354.\nTourni, I., Guo, L., Daryanto, T. H., Zhafransyah, F., Halim, E. E., Jalal, M., . . . Wijaya,\nD. T. (2021, November). Detecting frames in news headlines and lead images\nin U.S. gun violence coverage. In Findings of the association for computational\nlinguistics: Emnlp 2021 (pp. 4037\u20134050). Punta Cana, Dominican Republic:\nAssociation for Computational Linguistics. Retrieved from https://\naclanthology.org/2021.findings-emnlp.339 doi: 10.18653/v1/\n2021.findings-emnlp.339\nTversky, A., & Kahneman, D. (1985). The framing of decisions and the psychology of\nchoice. In Behavioral decision making (pp. 25\u201341). Boston, MA: Springer.\nUscinski, J., Enders, A., Klofstad, C., Seelig, M., Drochon, H., Premaratne, K., &\nMurthi, M. (2022). Have beliefs in conspiracy theories increased over time?\nPLoS One ,17(7), e0270429.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . .\nPolosukhin, I. (2017). Attention is all you need. Advances in neural information\nprocessing systems ,30.\nWicke, P., & Bolognesi, M. M. (2020). Framing covid-19: How we conceptualize and\ndiscuss the pandemic on twitter. PloS one ,15(9), e0240010.\nXu, D., Li, J., Zhu, M., Zhang, M., & Zhou, G. (2020). Improving amr parsing with\nsequence-to-sequence pre-training. In Proceedings of the 2020 conference on\nempirical methods in natural language processing (emnlp) (pp. 2501\u20132511).\nPrepared using sagej.cls\n22 Journal Title XX(X)\nSupplemental Material\nw0 w1 w2 ... wnx1 x2 x3 xn\nBidirectional Encoder Stack\nEmbedding LookupAutoregressive Decoder Stack\np0 p1 p2 ... pmt1 t2 t3 tn\nx1 x2 x3 xn\nshared Embedding Lookupt1 t2 t3 tn\np0 p1 p2 ... pm pm+1BART modelPenman decoderFrame MinerAMR Graph\nlabelInputType\nFrame\nARG0\nARG1\n...\nnameWord\nprevent-01\ndoctor\nvirus\n...\nPfizer\nIn 2021, doctors prevented the spread of the virus by vaccinating with Pfizer . MainstreamExampleFreq\n500\n200\n400\n...\n50Table for Label 1 CorpusType\nFrame\nARG0\nARG1\n...\nnameWord\nkill-01\ngoverment-\norganization\nvaccine\n...\nU.S.Freq\n350\n120\n330\n...\n60Table for Label 2 CorpusComparative Occurrences\n(Over-/Underrepresentativeness)Type\nFrame\n...\nARG1\n...\nFrameWord\nprevent-01\n...\nvaccine\n...\nkill-01Score\n+2.5\n...\n0.05\n...\n-1.5\nEmbedding Space\n(UMAP-reduced)TopN -\nhighest\nBottomN\n- lowest\nOverrepresented for label 2 Overrepresented for label 1Comparative Score Table\nAggregate\nby Label\nAMR-based framing analysis\nFigure 4. AMR-based Framing Analysis Approach Overview\nPrepared using sagej.cls\nReiter-Haas et al. 23Plot Characters Setting Moral of the Story Entities\nConspiracy Mainstream Conspiracy Mainstream Conspiracy Mainstream Conspiracy Mainstream Conspiracy Mainstream\nCOVID-19 1 destroy (24.91) say (43.95) i (39.65) case-04 (29.38) ever (27.06) early (11.73) counter-01 (9.03) ensure-01 (7.48) \"America\" (35.88) \"COVID-19\" (47.36)\n2 claim (23.10) test (29.09) book (29.93) test-01 (28.29) now (17.16) pandemic (10.15) alarm-01 (8.62) information (7.21) \"Gates\" (34.28) \"Covid-19\" (23.09)\n3 free (20.90) develop (21.72) military (25.37) vaccine (26.00) planet (14.88) week (10.14) free-01 (6.07) test-01 (6.83) \"God\" (28.94) \"SARS-CoV-2\" (21.57)\n4 state (20.59) infect (21.17) article (24.81) patient (24.83) today (13.47) by (9.25) keep-up-05 (5.97) treat-03 (6.83) \"Wuhan\" (28.81) \"University\" (19.35)\n5 believe (20.14) possible (20.98) amr-unknown (23.68) disease (24.56) above (12.25) date-entity (8.95) news (5.14) help-01 (5.95) \"Bill\" (28.22) \"Health\" (18.17)\n6 true (19.88) work (18.38) you (23.17) risk-01 (22.47) book (12.15) hospital (8.93) attempt-01 (5.01) reduce-01 (4.43) \"US\" (24.03) \" coronavirus\" (16.85)\n7 lie (18.90) reopen (17.32) law (22.56) infect-01 (21.64) below (11.67) after (8.68) benefit-01 (4.52) assess-01 (3.85) \"Israel\" (23.92) \"Oxford\" (14.73)\n8 war (18.86) case (16.54) media (21.88) company (21.43) world (11.49) home (8.35) possible-01 (4.50) allow-01 (3.84) \"China\" (21.81) \"Zealand\" (13.30)\n9 mainstream (18.55) wear (16.28) truth (21.42) symptom (20.21) final (11.30) month (8.10) battle-01 (4.45) advise-01 (3.80) \"Earth\" (19.88) \"Brazil\" (12.72)\n10 legal (17.90) risk (16.20) political-party (21.35) drug (19.70) just (10.46) city (8.07) save-02 (4.44) slow-01 (3.69) \"Trump\" (19.84) \"Moderna\" (12.29)\n11 cause (17.18) care (15.91) world (19.66) virus (18.30) publication (9.94) as-of (7.24) purpose (4.26) see-01 (3.48) \"Big\" (18.91) \"and\" (11.91)\n12 oppose (16.70) new (15.61) power (19.32) mask (18.23) tomorrow (9.90) community (7.23) resist-01 (4.22) organization (3.28) \"Christian\" (18.78) \"NHS\" (11.38)\n13 note (16.50) spread (15.59) war-01 (19.26) antibody (17.48) former (9.39) surface (6.97) suppress-01 (4.12) support-01 (3.18) \"Iran\" (18.76) \"England\" (11.19)\n14 natural (16.45) have (15.58) they (19.10) care-03 (17.08) list (8.97) so-far (6.68) destroy-01 (4.05) travel-01 (3.07) \"Bible\" (18.35) \"UK\" (10.57)\n15 word (16.39) contact (15.37) nothing (19.04) contact-01 (16.62) here (8.89) setting (6.59) distract-01 (3.99) check-01 (3.04) \"West\" (17.90) \"AstraZeneca\" (10.36)\nDisease 1 govern (28.01) say (49.44) you (43.43) disease (80.28) ever (26.35) country (16.90) alarm-01 (7.38) treat-03 (11.21) \"US\" (36.60) \"HIV\" (63.26)\n2 claim (27.98) infect (47.20) government-organization (37.47) infect-01 (40.25) here (20.92) early (15.86) free-01 (7.23) prevent-01 (8.93) \"America\" (32.58) \"Ebola\" (49.01)\n3 true (25.36) treat (35.50) military (30.23) virus (38.44) now (18.49) after (14.60) counter-01 (6.31) fight-01 (6.30) \"Israel\" (30.29) \"Zika\" (47.78)\n4 state (25.32) transmit (31.94) i (29.88) treat-03 (35.35) planet (17.88) outbreak-29 (13.49) effort-01 (5.55) ensure-01 (6.24) \"Russia\" (30.14) \"cancer\" (39.39)\n5 natural (25.06) spread (30.48) media (26.96) outbreak-29 (33.95) below (16.17) area (12.38) profit-01 (5.53) information (6.05) \"God\" (24.22) \"AIDS\" (27.75)\n6 engineer (23.70) diagnose (26.98) article (26.71) case-04 (33.57) former (14.31) world-region (12.28) purpose (5.40) assess-01 (5.85) \"Earth\" (24.21) \"Congo\" (26.01)\n7 actual (23.19) risk (25.07) amr-unknown (26.68) risk-01 (31.23) above (14.23) region (11.82) save-02 (5.40) reduce-01 (5.65) \"Trump\" (23.01) \"HIV/AIDS\" (25.46)\n8 legal (23.01) impregnate (23.84) book (25.71) woman (29.97) world (13.63) pregnancy (10.37) justify-01 (5.39) help-01 (4.88) \"Jew\" (22.44) \"Health\" (24.94)\n9 destroy (22.19) prevent (23.63) they (25.33) drug (28.02) all-over (11.31) year (10.14) benefit-01 (4.95) address-02 (4.82) \"China\" (21.80) \"WHO\" (23.82)\n10 mainstream (21.16) outbreak (23.26) truth (23.57) transmit-01 (27.80) article (10.90) east (9.95) create-01 (4.88) screen-01 (4.78) \"Iraq\" (21.63) \"Z Zika\" (21.38)\n11 lie (20.86) care (23.14) weapon (22.89) patient (27.74) war (10.65) since (9.51) experiment-01 (4.87) check-01 (4.67) \"Gates\" (20.82) \"Africa\" (19.98)\n12 create (20.72) detect (22.88) law (22.08) she (26.49) book (10.62) stage (9.50) seat (4.74) respond-01 (4.49) \"Monsanto\" (20.80) \"Uganda\" (18.89)\n13 oppose (20.43) case (22.66) power (21.90) mosquito (25.80) today (10.16) current (9.16) expand-01 (4.67) combat-01 (4.38) \"War\" (20.75) \"Organization\" (17.70)\n14 read (20.19) new (22.41) oil (21.62) health (25.37) behind (10.01) city (8.10) prove-01 (4.66) support-01 (4.33) \"Syria\" (20.50) \"DRC\" (17.47)\n15 believe (20.19) travel (22.32) war-01 (21.30) cell (25.03) facility (9.91) month (7.94) force-01 (4.61) care-03 (4.27) \"Iran\" (19.55) \"Dengue\" (16.10)\nPharma 1 destroy (33.30) say (71.64) you (58.11) company (53.50) ever (34.17) year (23.63) profit-01 (9.52) disease (9.50) \"America\" (44.91) \"COVID-19\" (39.24)\n2 natural (28.75) develop (45.22) i (38.97) drug (52.56) now (20.76) date-entity (17.03) alarm-01 (7.13) ensure-01 (8.98) \"Big\" (35.03) \"UK\" (23.26)\n3 mainstream (28.57) price (37.93) media (31.46) vaccine (37.57) below (19.15) week (16.19) news (7.06) treat-03 (7.90) \"God\" (32.06) \"India\" (23.20)\n4 govern (26.98) cost (26.99) government-organization (30.76) price-01 (36.62) here (18.45) month (16.12) free-01 (6.70) develop-02 (7.40) \"Gates\" (30.30) \"Covid-19\" (21.92)\n5 know (26.79) new (25.10) truth (30.75) disease (30.94) planet (16.44) by (13.70) destroy-01 (6.57) test-01 (6.80) \"CDC\" (27.00) \"NHS\" (21.81)\n6 true (26.55) infect (23.70) article (30.21) cost-01 (29.57) above (15.82) city (10.91) right-05 (5.66) information (5.98) \"Bush\" (26.98) \"University\" (21.73)\n7 lie (25.39) possible (21.89) they (29.98) develop-02 (26.54) today (14.25) pandemic (10.82) save-02 (5.15) help-01 (5.82) \"Bill\" (26.41) \"Pfizer\" (21.67)\n8 free (24.79) have (20.15) amr-unknown (29.50) patient (23.93) url-entity (13.70) early (10.57) defend-01 (5.13) address-02 (5.55) \"Russia\" (23.84) \"Moderna\" (20.98)\n9 read (23.66) low (19.72) corporation (29.00) try-01 (23.70) behind (12.70) country (9.95) benefit-01 (5.02) meet-01 (5.04) \"Iraq\" (22.77) \"measles\" (20.54)\n10 corrupt (23.49) treat (19.61) military (28.80) protein (23.00) realm (10.38) after (9.05) experiment-01 (4.98) improve-01 (4.71) \"Obama\" (22.74) \"Wakefield\" (20.49)\n11 kill (23.08) test (19.28) book (28.78) vaccinate-01 (22.19) article (10.28) university (8.69) cause-01 (4.88) measles (4.56) \"Iran\" (22.64) \"Medicare\" (20.26)\n12 control (22.97) access (19.01) man (24.60) antibody (21.78) war (10.03) so-far (8.47) justify-01 (4.70) immunize-01 (4.31) \"Wuhan\" (22.23) \"Health\" (20.12)\n13 cause (22.95) research (18.82) war-01 (24.60) treat-03 (21.75) then (9.94) community (8.00) live-01 (4.68) assess-01 (4.21) \"CIA\" (21.48) \"Oxford\" (19.84)\n14 engineer (21.77) work (18.47) power (22.68) infect-01 (21.70) world (9.83) globe (7.98) version (4.67) try-02 (4.05) \"Pharma\" (21.01) \"AstraZeneca\" (19.11)\n15 war (21.60) manufacture (18.22) citizen (22.56) candidate (21.61) about-to (9.52) state (7.62) counter-01 (4.66) make-01 (4.00) \"War\" (20.92) \"SARS-CoV-2\" (18.22)\nTable 2. Overrepresented elements.\nPrepared using sagej.cls", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Framing analysis of health-related narratives: Conspiracy versus mainstream media", "author": ["M Reiter-Haas", "B Kl\u00f6sch", "M Hadler", "E Lex"], "pub_year": "2024", "venue": "arXiv preprint arXiv \u2026", "abstract": "Understanding how online media frame issues is crucial due to their impact on public opinion.  Research on framing using natural language processing techniques mainly focuses on"}, "filled": false, "gsrank": 215, "pub_url": "https://arxiv.org/abs/2401.10030", "author_id": ["IN-hA_gAAAAJ", "p65WHAIAAAAJ", "yvdOcLUAAAAJ", "kEzfvdYAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:rXAL6xk4PBUJ:scholar.google.com/&output=cite&scirp=214&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D210%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=rXAL6xk4PBUJ&ei=KrWsaO_cK8DZieoPqdqh8QU&json=", "num_citations": 8, "citedby_url": "/scholar?cites=1530159657367859373&as_sdt=40005&sciodt=0,10&hl=en", "url_related_articles": "/scholar?q=related:rXAL6xk4PBUJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://arxiv.org/pdf/2401.10030"}}, {"title": "Automatic Large-scale", "year": "2024", "pdf_data": "Automatic Large-scale Political\nBias Detection of News Outlets\nAuthors Thelen-R\u00f6nnback,Ronja; Emmery,Chris; Brighton,Henry\nPublished in PLOS ONE\nDOI 10.1371/journal.pone.0321418 M\nPublication Date 2024-11-29\nDocument Version publishersversion\nLink https://research.tilburguniversity.edu/en/publications/b97ded90-\nf03d-4df3-908f-1d1831685f2a\nCitation Thelen-R\u00f6nnback, R, Emmery, C & Brighton, H 2024, 'Automatic Large-scale\nPolitical Bias Detection of News Outlets', PLOS ONE. https://doi.org/10.1371/\njournal.pone.0321418 M\nDownload Date 2025-08-25 19:41:29\nRights General rights\nCopyright and moral rights for the publications made accessible in the public\nportal are retained by the authors and/or other copyright owners and it is a\ncondition of accessing publications that users recognise and abide by the legal\nrequirements associated with these rights.\n- Users may download and print one copy of any publication from the public\nportal for the purpose of private study or research.\n- You may not further distribute the material or use it for any profit-making\nactivity or commercial gain\n- You may freely distribute the URL identifying the publication in the public\nportal\"\nTake down policy\nIf you believe that this document breaches copyright please contact us\nproviding details, and we will remove access to the work immediately and\ninvestigate your claim.\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 1 \u2014 #1\nPLOS ONE\nOPEN ACCESS\nCitation:R\u00f6nnbackR, EmmeryC, Brighton\nH(2025)Automaticlarge-scalepoliticalbias\ndetectionofnewsoutlets.PLoSOne20(5):\ne0321418. https://doi.org/10.1371/journal.pone.\n0321418\nEditor:ShadyElbassuoni,AmericanUniversity\nofBeirut,LEBANON\nReceived: November28,2024\nAccepted: March07,2025\nPublished: May12,2025\nPeer Review History: PLOSrecognizesthe\nbenefitsoftransparencyinthepeerreview\nprocess;therefore,weenablethepublicationof\nallofthecontentofpeerreviewandauthor\nresponsesalongsidefinal,publishedarticles.\nTheeditorialhistoryofthisarticleisavailable\nhere:https://doi.org/10.1371/journal.pone.\n0321418\nCopyright: \u00a92025 R\u00f6nnbacketal.Thisisan\nopenaccessarticledistributedundertheterms\noftheCreativeCommonsAttributionLicense ,\nwhichpermitsunrestricteduse,distribution,\nandreproductioninanymedium,providedthe\noriginalauthorandsourcearecredited.\nData availability statement: Weuseopenly\navailabledatasetsavailablevia https://www.\ngdeltproject.org/data.htmlfortheGDELTdataset ,\nhttps://mediabiasfactcheck.com/mbfcs-data-api/RESEARCH ARTICLE\nAutomatic large-scale political bias\ndetection of news outlets\nRonja R\u00f6nnback\n  \n\u2217, Chris Emmery\n  \n, Henry Brighton\nDepartment of Cognitive Science and Artificial Intelligence, Tilburg University, Tilburg, The Netherlands\n\u2217r.g.i.ronnback@tilburguniversity.edu\nAbstract\nPolitical bias is an inescapable characteristic in news and media reporting, and under-\nstanding what political biases people are exposed to when interacting with online news\nis of crucial import. However, quantifying political bias is problematic. To systematically\nstudy the political biases of online news, much of previous research has used human-\nlabelled databases. Yet, these databases tend to be costly, and cover only a few thou-\nsand instances at most. Additionally, despite the wide recognition that bias can be\nexpressed in a multitude of ways, many have only examined narrow expressions of bias.\nFor example, most have focused on biased wording in news articles, but ignore bias\nexpressed when an outlet avoids reporting on certain topics or events. In this article, we\nintroduce a data-driven approach that uses machine learning techniques to analyse mul-\ntiple forms of bias, and that can estimate the political leaning of hundreds of thousands\nof Web domains with high accuracy. Crucially, this approach also allows us to provide\ndetailed explanations for why a news outlet is assigned a particular political bias. Our\nwork thereby presents a scalable and comprehensive approach to studying political bias\nin news on a larger scale than ever before.\nIntroduction\nThe proper functioning of a democratic system presumes that its citizens have the tools to\nmake well-informed decisions. Yet, bias in news is unavoidable. Understanding how people\nare exposed to political bias when interacting with Web technologies like social media, search\nengines, or other sources is crucial for contributing to better-informed societies. This can be\nespecially important when the bias of the source cannot easily be anticipated, such as when\nthe source is unfamiliar. It raises the concrete question of how to study news bias. An imme-\ndiate challenge lies in how to measure political bias in the first place; most would agree that\nFox News and the Guardian behave differently and occupy different locations on the polit-\nical spectrum. However, the assumption of a left-to-right political spectrum is by no means\nuncontroversial [ 1\u20133]. It is a simplifying one that many (our work included) choose to make\nto be able to systematically study political biases in online services. Many researchers mea-\nsure bias by using such labels, which detail properties like reliability or political leaning of\nnews. These labels are generally derived in one of two ways: human labelling or computational\nlabelling.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 1/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 2 \u2014 #2\nPLOS One Automatic political bias detection of news outlets\nHuman labelling can allow researchers to deal with ambiguity and contextual information,\nfortheMediaBiasfactCheckdata,and\nhttps://personalization.ccs.neu.edu/Projects/\nPartisanship/ forRobertsonetal.\u2019sdata.Our\ncodeusedtoprocessthesedatasetsisavailable\nathttps://github.com/rtronnback/automatic_\nnews_monitoring_with_GDELT .\nFunding:Theauthor(s)receivednospecific\nfundingforthiswork.\nCompeting interests: Theauthorshave\ndeclaredthatnocompetinginterestsexist.but this approach is often slow, laborious and, ironically, can be subject to bias as well [ 4]. On\nthe other hand, data-driven approaches enable fast and efficient analysis of news through Nat-\nural Language Processing (NLP) and Machine Learning (ML), yet fail to provide the same\nlevel of insight as manual labelling, falling short of actually increasing understanding of the\nphenomenon [ 4].\nMoreover, many simplify even further by focusing on narrow expressions of bias in, for\ninstance, word choice in headlines. Yet, some forms of bias cannot be detected unless exam-\nining the holistic behaviour of a news outlet, rather than individual news items. For exam-\nple, an outlet systematically avoiding a topic, or only covering it very briefly despite societal\nrelevance, can be a clear sign of bias. Yet, this is often not considered in existing research.\nGiven these two challenges, our work proposes an approach that uses automatic labelling\nof news web-domains\u2019 bias on a global scale. To do this, we use ML to predict web-domain\nbias using the Global Database of Events, Language, and Tone (GDELT). GDELT tracks and\nanalyses global news, making it an ideal source for this task [ 5]. It enables us to: i) focus on\naggregated outlet information rather than article-, sentence- or word-level analysis (pre-\ndominant in related work), ii) differentiate between multiple types of bias to review their\nimpact, whereas much of previous research studies one sub-type, and iii) evaluate model per-\nformance against the provenance of the true bias labels (either computationally derived or\nhuman-annotated). Finally, we combine these computational methods with techniques for\nmodel explainability to extract the approximate reasoning behind why a news web-domain\nin question is deemed to be politically biased. We believe our analyses may prove meaningful\nfor establishing recurrent problematic behaviour on the part of news outlets in an automatic\nmanner, and, if developed further, could help citizens inform themselves as to the partiality of\ntheir news sources.\nBarriers of news bias studies: Narrow focus, scope and lack of\ninsight\nPolitical bias is challenging to define and more often than not considered to be subjective, but\ngenerally refers to a recurring (intentional or unintentional) attempt to influence a reader [ 4,\n6]. There are many ways that bias can manifest itself in news media. This can range from the\nselection of what events to cover, where an article should be placed on the homepage, how\nmuch space to give it, or whether to (as the classic example goes) refer to \u201cfreedom fighters\u201d\nas opposed to \u201cterrorists\u201d [ 4,7]. As a consequence, measuring bias raises a lot of practical\nproblems, and many have focused on studying very specific types of bias to simplify the task.\nWhat follows is a succinct overview of biases identified in Hamborg, Donnay and Gipp\u2019s [ 4]\nliterature review, and that we focus on in our analyses.\n\u2022Event selection bias or coverage bias involves choosing which events merit report. Nat-\nurally, not all stories can, nor should be published. Yet, intentional and consistent avoid-\nance of or focus on a topic can influence or mislead audiences. This is a well-studied phe-\nnomenon in crime reporting [ 8\u201310].\n\u2022Labelling andword choice bias are a major focus of study. This concerns framing events or\nhighlighting a certain perspective by choosing labels or particular words that, while similar,\nwill convey different meanings to audiences: for example, referring to something as a \u201cspe-\ncial military operation\u201d or \u201cintervention\u201d instead of \u201cinvasion\u201d may change perceptions of\nevents.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 2/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 3 \u2014 #3\nPLOS One Automatic political bias detection of news outlets\n\u2022Size allocation bias concerns the length of articles. The amount of text written on some\ntopics may introduce certain outlet biases. For example, it is possible that news outlets\nreport consistently but only at brief length on certain topics while dedicating a lot of work\nand space to others. This is a relatively straightforward form of bias to study, though it has\nnot received much attention [ 4].\n\u2022Picture selection andexplanation bias concern what pictures are chosen to accompany\ncertain articles, and how those pictures are described. Images have been shown to affect\nreaders\u2019 perceptions of news articles [ 11,12], therefore selecting and describing them is\nsusceptible to potential biases.\nThese subtypes of bias have, to varying extents, been examined in previous literature. How-\never, it is rare that a single study encompasses more than one form of bias. Furthermore,\nmany have focused on making article-level inferences (trained directly on the content of the\narticles, and therefore often fixating on word choice bias), rather than outlet-level inferences\n(based on meta-data of multiple articles, which could encompass multiple of the bias subtypes\noutlined above).\nOn article-level, studies have used computational (NLP) tools such as Term Frequency-\nInverse Document Frequency [ 13,14] or doc2vec [ 15] as feature representation meth-\nods, but many have had limited success [ 16,17] and rely on costly resources [ 16\u201318]. Gan-\ngula et al. [ 18], for example, aimed to predict news bias towards five local political parties\nbased on headlines, articles and a combination of the two. They achieved an accuracy of 89%\nwith an attention-based model. However, the narrow focus and reliance on very specific\nhuman annotations limits the work\u2019s ability to scale to a wider context and to provide deeper\nunderstanding of political bias on the whole. Spinde et al. [ 19], on the other hand, use exist-\ning labelled datasets from Reddit comments, movie reviews, Wikipedia, and two general lan-\nguage datasets. These were combined to train a DistilBERT model [ 20] in a Multitask Learn-\ning setting. While the results look promising (F1-score of 0.77), their results are only partially\ntransferable to news due to the data being only indirectly related to news bias, as they them-\nselves note. A follow-up study compiled a dataset of 3,700 sentence-level expert annotations\non a broad range of topics in lieu of the usual crowd-sourced annotations. BERT-based mod-\nels [21] detected sentence bias, achieving a maximum F1-score of 0.80 [ 22]. This constitutes\nan improvement, though backs off to extensive manual annotation and still limits the focus\nto word choice bias on an article-level. Finally, some previous work has focused on detecting\na dramatically wider range of broad bias subtypes on sentence-level (ad hominem or circular\nreasoning bias, for example) [ 23]. These distinctly focus on political bias as a subtype, how-\never, rather than as a nuanced subject that can be expressed in a number of different ways [ 4],\nas the current work does.\nNot all existing work focuses on article- or sentence-level bias, or even uses ML to esti-\nmate website or news political bias, however. For example, using the Twitter accounts of users\nwho were registered as either Republican or Democrat voters, Le, Shafiq and Srinivasan [ 24]\napproximated bias based on how often users shared articles from outlets or websites. Articles\nshared frequently by Republicans would thus be assumed to stem from web-domains with a\nright-wing political leaning, and vice versa. Given this method, Robertson et al. [ 25] assigned\nand validated political leaning scores for over twenty thousand websites. This approach\nscales well and provides follow-up studies with validated political bias scores. Nevertheless,\nit presents an approximated measure of bias and does not delve deeper into what makes a\nparticular outlet more biased towards a political audience.\nWork that bears resemblance to our own is MediaRank [ 26], which also opts for a source-\nlevel analysis to create quality rankings of the world\u2019s most prominent news sources. Using\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 3/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 4 \u2014 #4\nPLOS One Automatic political bias detection of news outlets\nmetrics such as reputation, reporting bias, financial pressure, and popularity, they evalu-\nate over 50 thousand news sources in 68 countries. There are some important distinctions,\nhowever: firstly, they rely on an outlet\u2019s average sentiment regarding celebrity Democrats\nand Republicans. In contrast, our approach makes use of the millions of GDELT themes and\nready-made NLP features that have been selected in an entirely data-driven fashion. This\nmakes our approach more comprehensive and globally applicable (also for alternative divides\nof the political spectrum), since American politicians or celebrities will not be extensively dis-\ncussed everywhere. As such, our project can provide an assessment of news sources that is\nmore focused on an in-depth examination of political bias using multiple related features and\nthemes. Therefore, while MediaRank focuses on a generalist perspective of news quality, our\napproach offers a more in-depth study of political bias, the numerous ways it can manifest,\nand does this at a globally applicable scale.\nIt is worth noting that large language models (LLMs) present a promising avenue for large-\nscale sentence- and article-level bias classification. LLMs offer a low barrier of entry to inter-\nfacing with large amounts of textual data, and especially to extract information from complex\nstructures. They have therefore been used to for example automatically detect misinformation\nor fake news [ 27\u201329], and various forms of bias [ 28,30,31], and might therefore be deemed\nas relevant for our project as well. However, LLMs-based approaches face some serious chal-\nlenges. Namely, LLMs are subject to various internal political biases [ 32,33], and seem to con-\nsistently differ from human judgement [ 27], which presents a serious complication for LLM-\nbased applications [ 34]. Additionally, their inherent stochasticity implies that the accuracy of\noutputs may differ significantly despite receiving the same prompts, and has been shown to\nproduce contradicting results when dealing with political disinformation [ 29]. Previous work\nhas also aptly noted that LLMs do not receive regular updates, and that this may present a\nproblem in the rapidly evolving news cycle [ 31]. Finally, the quality of LLM classifications has\nalso been shown to not match the performance of fine-tuned supervised models on numerous\napplications (social understanding [ 35], media bias detection [ 28], as well as other social sci-\nence tasks [ 36,37]). Despite these limitations, we provide two naive zero-shot LLM baselines\nfor comparative purposes. This is relevant given the aforementioned potential of LLMs to\nreplace manual labelling and ease of use, but importantly also allows us to examine if the per-\nformance limitations found in previous work are repeated here. It is worth noting that, while\nLLMs offer advantages like ease of use, there are also trade-offs to consider, such as, hardware\ncosts, output fabrication, and energy consumption, especially if used on a large scale, among\nother challenges [ 38].Appendix E details the implementation and prompting used.\nThe aforementioned examples all demonstrate the potential of data-driven news bias\ndetection but also the existing limitations. Focus on article- or sentence-level bias excludes\nimportant patterns that emerge from a news web-domain\u2019s behaviour as a whole, such as\ncoverage, story placement and size patterns (i.e., are some topics avoided, only given lim-\nited space, or only reported upon very briefly). These could play a key role in demonstrat-\ning and explaining bias. Additionally, many studies rely on costly expert or crowd-sourced\nannotations [ 18,22], or on tangential datasets [ 19].\nCrucially, to identify media bias of news outlets at scale, it is not sufficient to rely on spe-\ncific topics, small-scale datasets or only on sentence- or article-level classifications. Instead,\nnews bias monitoring should ideally cover a multitude of topics and be applicable to (nearly)\nany web-domain, whether this be from well-known sources such as breitbart.com or cnn.com\nor from lesser known organisations. Therefore, our work aims to develop a system that relies\non a broader data source which specifically covers news, facilitates global-scale coverage,\nand is capable of examining multiple facets of media bias more thoroughly. As a result, this\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 4/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 5 \u2014 #5\nPLOS One Automatic political bias detection of news outlets\napproach is entirely data-driven and reliant on automated techniques, rather than time-\nconsuming manual approaches. We believe this could hugely benefit the field and ensure a\nholistic coverage of news bias.\nAn approach for large-scale bias labelling\nTo make inferences about web-domains on a global scale, one of course needs a data source\nwith global coverage. This is possible thanks to the Global Database of Events, Language, and\nTone (GDELT), as well as some features from the independent media bias tracking organi-\nsation Media Bias Fact Check (MBFC). For further details of implementation, please refer to\nAppendix B .\nGDELT [5] is an open platform for monitoring global news, and the most extensive\ndatabase covering news in existence, to the best of the authors\u2019 knowledge. It has been used\nin related previous work examining the use of images in news [ 39], as well as the rise fake\nnews [ 40]. Compared to similar datasets, it has been found to contain a broader set of unique\nnews outlets [ 41], making it ideal for our application. The main dataset of interest here is\nthe Global Knowledge Graph (GKG), which records \u201clatent dimensions, geography and\nnetwork structure of the global news\u201d [ 5, p. 1]. It contains various automatically identified\nthemes associated with articles (these are extensive, covering topics ranging from immigra-\ntion to gasoline prices or even specific currencies or mammals) and the results of various\nanalyses, such as tone. Given the size of the dataset (one year\u2019s worth of data corresponds to\n2.5TB [ 42]), our experiments use a limited sample consisting of English articles from the year\n2022. This excludes articles that have been translated, and subsequently focuses on Western\ncountries which approximately follow the bipartisan political spectrum. Though there may\nbe some slight selection bias given the outbreak of the COVID-19 pandemic, we expect that\nthe breadth of GDELT alleviates concerns of generalisability. We filtered all features that we\ndeemed unlikely to reflect bias, resulting in features detailed in Table 1 .\nWe found more than 30,000 unique themes in our sample. To reduce their sparsity, we\nopted to filter over-specific taxonomic items (e.g., specific birds, mammals, or fish), and\nother themes appearing less than 1000 times and more than one standard deviation from\nthe mean of the log transform of theme frequency. In addition to maintaining a data-driven\napproach, this method does not limit the sample to known politically controversial themes\nTable 1. GDELT features included in analysis, accompanied by short description.\nGDELT Feature Description\nTone The positive and negative tone of the article as a whole.\nPolarity Proportion of words that matched a tonal dictionary to indicate how polarized the\ntext is. For example, a high polarity but similar scores for positive and negative\ntones indicates that the article contains roughly the same amount of positively and\nnegatively charged words.\nActivity reference density Percentage score of active words, like active verbs, in the article and is supposed to act\nas a proxy for \u201cactiveness\u201d in the text as compared to merely descriptive text.\nSelf or group reference density Percentage of pronouns present. The GDELT documentation states that this \u201ccan be\nused to distinguish certain classes of news media and certain contexts\u201d [ 43].\nWord count Number of words in the article.\nVisual content Presence of cover images, embedded social media images or videos related to the\narticle. This was noted merely as either \u201cpresent\u201d or \u201cabsent\u201d for the purposes of out\nwork,\nThemes A list of the themes detected in the article as by GDELT\u2019s analysis.\nhttps://doi.org/10.1371/journal.pone.0321418.t001\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 5/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 6 \u2014 #6\nPLOS One Automatic political bias detection of news outlets\n(such as immigration, abortion, or climate change [ 22,44]) thereby potentially including\nunder-explored indicators of bias.\nAdditionally, to account for other forms of bias beyond those found in tone or sentiment\nanalyses, we added a set of outlet-level features per theme: the proportion of articles (as this\nmight allow for examination of selection bias, and whether some topics are ignored or exces-\nsively focused on), the average word count per article (which might reveal size bias), and the\npresence of images or videos (which could also be indicative of under-explored forms of bias).\nCrucially, this aggregation based on themes makes it easy to notice if some theme is largely\nignored or alternatively excessively focused on by a source (for example, if a source very rarely\nand only briefly covers news related to global warming, the proportion and average word\ncount of articles for that theme would be very low). This preprocessing yields one row per\nweb-domain, with associated bias features from GDELT per theme. Due to the high num-\nber of features this resulted in, we also eliminated some features (for details, see Appendix B ).\nFig 1 provides an overview of these preprocessing steps.\nAside from GDELT, we also used some information from Media Bias Fact Check ( MBFC ).\nMBFC provides information about the factuality, traffic, country of origin, press freedom,\nmedia type, and credibility of news web-domains. As such information is relevant but not\navailable through GDELT, we extended the training dataset with these features (see Appendix\nAfor a full explanation of the features). These additional features were however only included\nin one of the experiments, as is further detailed in the sections below.\nLabelling: Human reliance or fully automated?\nGiven the laborious nature of human labelling, it would be cheaper and faster to rely on auto-\nmatic methods, but only if performance is somewhat comparable. To better examine this,\nwe considered model performance on two datasets of ground truth labels. One is composed\nof human evaluations from MBFC, whereas the other is automatically derived by Robert-\nson et al. [ 25].\nMBFC is an independent organisation that estimates media bias based on human eval-\nuations [ 45]. Their main aim is to promote awareness of bias and misinformation. For each\nnews web-domain, they provide a political leaning label and some other noteworthy metrics\n(these are described in detail in Appendix A ). The bias label is determined based on a set of\ntopics like, to only name a few, immigration, economic policy, or social views [ 44]. They do\nhowever note that these topics are derived from an American perspective, and may therefore\nFig 1.Overview of data aggregation process. Overview of process to aggregate GDELT data from article- to outlet-level instances, containing themes and their\nrespective average GDELT features.\nhttps://doi.org/10.1371/journal.pone.0321418.g001\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 6/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 7 \u2014 #7\nPLOS One Automatic political bias detection of news outlets\nnot perfectly apply to all countries. While there are numerous entities collecting similar rat-\nings of political media bias and we acknowledge that no ground truth is perfectly unbiased or\naccurate, MBFC\u2019s methodology is thoroughly documented [ 46], and their dataset is open and\nextensive ( Fig 2 ). Their labels serve as a ground truth value, and has been used for this same\npurpose in previous work as well [ 26].\nThe other set of labels stems from research by Robertson et al. [ 25], who built a dataset\nof bias scores for nearly twenty thousand websites (henceforth referred to as PABS,\nretrieved from github.com/gitronald/domains/tree/master/data/bias_\nscores ). By relying on Twitter users who were officially registered as either Republican or\nDemocrat voters, they collected all links to web-domains that these users shared on the plat-\nform. Operating on the assumption that users would predominantly share links to domains\nthey agreed with, they create a proxy score of the political bias of a web-domain based on the\nproportion of times it was shared by Democrat versus Republican users. Scores range from -1\nto 1, wherein -1 indicates that the source was shared exclusively by Democrats (left-learning\nbias), and a score of 1 indicates it was shared exclusively by Republicans (right-lean bias).\nAutomatically derived scores such as these are cheaper to obtain but might not be as accurate\nas human-made labels. We examine whether this is the case by comparing whether results dif-\nfer between such proxy-labels and human-made labels. The data was preprocessed by binning\nthe continuous values into the five bias classes.\nClassification with machine-learning models\nWe trained various ML models to classify web-domain bias based on the preprocessed\nGDELT features, using either MBFC or PABS data as a ground truth [ 25]. Serving as a point\nof comparison, a majority baseline model was implemented (i.e., one that invariably classifies\nall instances as the most common class: \u201cleast biased\u201d). The other models included a feed-\nforward neural network that was trained using Pytorch [ 47], and Support Vector Machine\n(SVM, [ 48,49]), AdaBoost [ 50,51], and XGBoost classifiers [ 52] We lastly also include two\nFig 2. Frequency of political lean classes per dataset, on a logarithmic scale.\nhttps://doi.org/10.1371/journal.pone.0321418.g002\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 7/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 8 \u2014 #8\nPLOS One Automatic political bias detection of news outlets\nbaselines using a naive zero-shot LLM with Llama 3.1 [53] and GPT-4o mini [54],\ndetailed further in Appendix E .\nWe empirically determined a two-linear-layered network to perform best (with ReLU acti-\nvation, batch normalization and a dropout-rate of 0.5 between each layer [ 55\u201357]). Training\nwas done via the Adam optimizer [ 58] and negative log-likelihood loss. Additionally, as one\nof the experiments involved categorical variables from MBFC, we embedded these [ 59,60].\nThe complete structure of the networks can be found in Appendix C . All other models are\ntrained using ten-fold cross-validation and optimized using halving grid search [ 61] for tun-\ning; the hyper-parameters used in the grid search are detailed in Appendix B andC.\nModel explainability for insight into web-domain bias\nHamborg, Donnay and Gipp [ 4] criticize computational analyses of media bias of lacking\ninsight into how bias is manifested. Indeed, numerous previous studies have merely focused\non determining whether it is present [ 16,18,19,22]. We therefore opted to address this through\nthe use of computational methods that provide explanations of model decisions. Specifi-\ncally, we used Shapley Additive Explanations (SHAP), which expands upon six pre-existing\nmethods [ 62] (examples include LIME [ 63] and Layer-wise Relevance Propagation [ 64]). The\nSHAP framework provides model-agnostic explanations, meaning it can be applied to tradi-\ntionally inscrutable black-box models. SHAP averages the differences in the model\u2019s output\nwith and without a particular feature; the resulting set of differences is then used to approx-\nimate the Shapley values for each feature, representing the contribution of that feature to a\nprediction. This allows us to provide thorough outlet-specific explanations rather than sim-\nply model-level insight (as is the case for other traditional explainability frameworks). Con-\nsequently, we can scrutinize any news domain to understand why a model classifies it as left-\nor right-wing biased, providing direct insight into the manifestation of bias and therefore\naddressing the critique of Hamborg, Donnay and Gipp [ 4].\nExperimental setup and testing\nOur first three experiments were repeated using either MBFC (human-made labels)\nor PABS scores (automatically derived labels) as ground truth labels. This analysis\nserved to determine whether either labelling method is more successful; as automati-\ncally derived labels are easier to obtain but may be less accurate, such a comparison is\ninformative. Finally, a post-hoc analysis examined the difference between the bias labels\nby MBFC and PABS. The code necessary to replicate our experiments is available at\ngithub.com/rtronnback/automatic_news_monitoring_with_GDELT . The\nfollowing paragraphs provide an overview of each experiment:\nTraditional Bias Experiment trained models on data related solely to word bias, mean-\ning it covered features related to tone, polarity, activity- and self/group reference density\u2014\nfeatures that have been more extensively studied in prior work.\nAlternative Bias Experiment used word-, article-counts, and image or video presence,\naiming to better glean the significance of lesser-studied forms of bias; namely, size, selection,\nand picture bias respectively (the content of the images is not accounted for, thus this feature\nonly approximates picture bias).\nFull Bias Experiment used all features. This structure aimed to allow for a better examina-\ntion of the information value of different forms of bias and to extend the analysis beyond the\ntraditional focus of word bias.\nFull Bias & Categorical Features Experiment was conducted adding various categorical\nfeatures provided by MBFC such as credibility, factuality ratings, traffic estimates, country\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 8/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 9 \u2014 #9\nPLOS One Automatic political bias detection of news outlets\npress freedom index, and media type (see Appendix A for full list). We expected these fea-\ntures to be informative and thus conducive to improved performance. This experiment was\nonly done for the full bias dataset using MBFC as the ground truth. For an overview of all\nexperiments, see Fig 3 .\nResults\nModels were evaluated based on how well they predict a news web-domain\u2019s political bias,\nand results are detailed in Table 2 . The overall best-performing model was the neural network\ntrained on the full dataset supplemented with categorical features of MBFC (see Appendix A\nfor the full descriptions). Some examples of news domains, the ground truth and the model\u2019s\npredictions are shown in Table Table 3 . It classified web-domains with an accuracy of 76%,\nand an AUC score of 81%, compared to the baseline model which achieved 45% and an AUC\nof 50%. The LLM baseline performed similarly to this naive baseline. Models using multiple\nmanifestations of bias generally achieved better performance compared to those using tradi-\ntional or alternative forms of bias only. The confusion matrices of the best performing model\nunder each experimental condition (traditional bias, alternative bias or both) are shown in\nFig 4 . Models trained on MBFC as ground truth outperformed models trained on PABS,\nwhich achieved only a maximum accuracy of 58.2% and an AUC of 70% with the neural\nnetwork trained on the alternative bias dataset.\nWe performed some simple error analysis to examine the strengths and limitations of the\nbest performing model. Detailed results can be found in Appendix F . Considering the dif-\nferent classes, the model performs best at classifying right-wing sources, followed by least\nbiased, and right-centre. Left and left-leaning results were harder for the model to correctly\ndetect. Furthermore, the error rates of the model were lowest for outlets with minimal and\nmedium traffic. This is interesting given that low-traffic websites are often more challenging\nFig 3.Overview of experiments used to test models. To test the impact of different bias-related data, models were trained on subsets of\nthe data: traditional bias data (features related to tone, polarity, activity and self/group reference density); alternative bias data (features\nof word-, article-counts, image- or video presence); and the combination of all these features: full bias data. An additional experiment\ntested model performance on the full dataset when supplemented with categorical features from the MBFC data.\nhttps://doi.org/10.1371/journal.pone.0321418.g003\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 9/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 10 \u2014 #10\nPLOS One Automatic political bias detection of news outlets\nTable 2. Model results per experiment.\nDataset Model TB Dataset AB Dataset FB Dataset FB&C Dataset\nAcc. AUC Acc. AUC Acc. AUC Acc. AUC\nMBFC\nMBL 0.454 0.500 0.454 0.500 0.454 0.500 0.454 0.500\nSVM 0.471 0.510 0.521 0.570 0.681 0.701 0.750 0.790\nAdaB 0.681 0.730 0.579 0.680 0.723 0.770 0.750 0.780\nXGB 0.622 0.700 0.546 0.640 0.655 0.720 0.648 0.690\nNN 0.689 0.730 0.630 0.710 0.664 0.750 0.765 0.813\nLlama 3.1 - - - - - - 0.531 0.622\nGPT-4o mini - - - - - - 0.555 0.627\nPABS\nMBL 0.408 0.500 0.408 0.500 0.408 0.500 - -\nSVM 0.537 0.630 0.537 0.650 0.569 0.740 - -\nAdaB 0.556 0.650 0.592 0.660 0.547 0.70 - -\nXGB 0.444 0.580 0.531 0.640 0.579 0.660 - -\nNN 0.479 0.620 0.582 0.700 0.527 0.680 - -\nLlama 3.1 - - - - - - 0.495 0.632\nGPT-4o mini - - - - - - 0.505 0.623\nThe test set performance of models classifying political leaning of media web-domains under various experimental\nconditions: traditional bias features (TB Dataset; tone, polarity, activity- and self/group reference density), alternative\nbias features (AB Dataset; word- and article count, image- or video presence) and full bias features (FB Dataset). The\nlast column to the right presents the performance of a model trained on the full data, supplemented with categori-\ncal features derived from MBFC\u2019s dataset (FB&C Dataset). Evaluation metrics are accuracy (Acc.) and AUC-score,\nand the best-performing model is highlighted in bold for each experiment. The tested models include a baseline\nmodel (MBL), a support vector machine (SVM), an AdaBoost model (AdaB), an XGBoost model (XGB) and a neu-\nral network (NN). Note that the LLM baselines using Llama 3.1 andGPT-4o mini follow a slightly different\nimplementation (see Appendix E ), and are displayed under FB&C merely for the sake of brevity.\nhttps://doi.org/10.1371/journal.pone.0321418.t002\nTable 3. Output examples. Examples of domains with corresponding predictions and ground truths. Predictions\nwere made using the best performing NN model.\nDomain Ground Truth Prediction\ninvestmentwatchblog.com right right\nirishtimes.com left left\nwishtv.com left-leaning least biased\n12news.com least biased least biased\nkhou.tv least biased least biased\nbicesteradviser.com least biased least biased\ndailyprogress.com least biased least biased\ndailysignal.com right right-leaning\nisraelnationalnews.com right-leaning left-leaning\n12news.com least biased least biased\nheraldpalladium.com right-leaning right-leaning\nhttps://doi.org/10.1371/journal.pone.0321418.t003\nwhen trying to determine political bias, but therefore also of central importance. Readers are\nexpectedly aware of the political orientation of popular news sources like Fox News or The\nGuardian, and can therefore anticipate the slant of the information. The main difficulties arise\nwhen a reader encounters a lesser-known source, where the potential bias is unknown. This\ncan be polarising in spheres where information is uncertain and moves at a rapid pace; as\ntends to be the case online, and especially on social media. A recent well-known case of this\nis the rebranding of Twitter to X, and the subsequent shift in tonality and bias. As such, our\nmodel\u2019s increased performance on lower-traffic websites is highly encouraging for dealing\nwith unfamiliar sources\u2019 biases.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 10/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 11 \u2014 #11\nPLOS One Automatic political bias detection of news outlets\nFig 4.Confusion matrices. Confusion matrices of the predictions by the best performing models per task.\nhttps://doi.org/10.1371/journal.pone.0321418.g004\nModel explanations\nSHAP decision plots can be made for any given web-domain, so some representative exam-\nples were selected for visualisation. The results can be found in Figs 5 \u20139(due to limitations\nin the SHAP library, these pertain the comparably performing SVM model). Generally, the\ncategorical features from MBFC frequently appear in the top most important features, with\nthe exception of the Press Freedom Index. Geographical location also structurally appears as\nan informative feature in the provided examples. It is unclear what precisely about the coor-\ndinates influences the model. As the they refer to countries, rather than regions, this may\nreflect an approximate correlation with democracies and autocratic regimes. However, this\nshould reasonably also be reflected in the Press Freedom Index, which seems to have been\ndisregarded.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 11/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 12 \u2014 #12\nPLOS One Automatic political bias detection of news outlets\nFig 5.Decision plot of Breitbart, a right-wing political news source. The twenty most influential features are plotted in descending order. The range at the top of the\ngraph represents the political bias labels as predicted by the model.\nhttps://doi.org/10.1371/journal.pone.0321418.g005\nFig 6.Decision plot of Forbes, a right-leaning political news source.\nhttps://doi.org/10.1371/journal.pone.0321418.g006\nAside from the categorical features, polarising themes previously highlighted in the litera-\nture are also prevalent (e.g., inequality, environmental issues, election fraud, firearm owner-\nship, and social movements). Interestingly, however, the model also picks up on themes not\nappearing in earlier research (e.g., natural disasters).\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 12/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 13 \u2014 #13\nPLOS One Automatic political bias detection of news outlets\nFig 7.Decision plot of the Economist, a centre-leaning political news source.\nhttps://doi.org/10.1371/journal.pone.0321418.g007\nFig 8.Decision plot of the Guardian, a left-leaning political news source.\nhttps://doi.org/10.1371/journal.pone.0321418.g008\nRegarding the question of whether different types of bias impact model performance, arti-\ncle count features are often shown to be informative. This confirms the suggestion that alter-\nnative bias features could be more informative than previously credited: coverage bias, as rep-\nresented by the number of articles published per theme, can be recognised and used to inform\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 13/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 14 \u2014 #14\nPLOS One Automatic political bias detection of news outlets\nFig 9.Decision plot of CNN, a left-wing political news source.\nhttps://doi.org/10.1371/journal.pone.0321418.g009\nclassifications. However, it is worth noting that other forms of alternative bias (such as word\ncount or image presence) rarely appear in our decision plots, whereas traditional bias features\nare consistently informative.\nAside from gathering insight into how bias may manifest, SHAP can also be used to anal-\nyse misclassifications by the model ( Fig 10 ). For example, the domain theconservative-\ntreehouse.com was labelled as left-leaning despite actually being right-wing. Using the\nSHAP decision plot, we can see that the model was drastically influenced by the number of\narticles related to hate crime, causing it to output a left-centre label. Examining the dataset\nreveals that this domain has a high article count for this theme, and that this likely resulted\nin the misclassification. As such, SHAP plots can be helpful for analysing errors as well as\nunderstanding bias.\nPost-hoc analysis: Ground truth label comparisons\nIn light of the difference in performance between the models trained on PABS or MBFC data,\nwe conducted additional analyses to examine this closer. A quick comparison revealed that\nthere is a sizeable mismatch between labels. For all web-domains present in both PABS and\nMBFC\u2019s data, 46% of ratings agree with each other, and the AUC score is 69%. Fig 11 displays\na confusion matrix to compare prediction errors, showing that neighbouring labels tend to be\nmisclassified. Notably, there are some more significant disagreements; e.g., 32 left-wing web-\ndomains are classified as \u201cleast biased\u201d by PABS, and similarly for 14 right-wing web-domains.\nThese larger gaps in labelling are problematic and raise an important issue regarding the valid-\nity of bias ratings in general, but especially in the disparity of results between computationally\ndetermined results and human-made labels. However, this particular labelling task is complex\nand prone to such disagreement even when based on human annotators. For example, MBFC\nand another bias rating website using human annotators, AllSides [ 65], show a slightly greater\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 14/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 15 \u2014 #15\nPLOS One Automatic political bias detection of news outlets\nFig 10. SHAP decision plots of misclassified web-domain. Example of a misclassified web-domain, theconservativetreehouse.com , which is a right-wing\ndomain that was falsely classified as left-leaning by the model.\nhttps://doi.org/10.1371/journal.pone.0321418.g010\nFig 11. PABS and MBFC label agreement. A confusion matrix comparison of MBFC labels with those of PABS.\nhttps://doi.org/10.1371/journal.pone.0321418.g011\ndegree of agreement between themselves (57% of 293 web-domains in common agree, AUC\nscore of 74%). Given that this is still quite a low degree of agreement, it demonstrates that the\ncomputationally determined labels are only slightly less reliable compared to the realistically\nachievable upper bound set by human annotations, while retaining the benefits of speed and\nefficiency.\nDiscussion\nThe current work sought to automatically classify the political bias of news outlets, with a\nparticular focus on scalability, minimal human intervention, and transparency. As GDELT\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 15/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 16 \u2014 #16\nPLOS One Automatic political bias detection of news outlets\nincludes data from 1979 until the present, our approach allows for automatic labelling at any\npoint in time, anywhere. Furthermore, it addresses limitations of previous research, such as\nthe focus on constrained topics or types of bias [ 4,18,19,22,26]. Altogether, this allows for a\nmuch broader applicability than any previous research (to the best of our current knowledge),\nand is a fast and cost-effective method to recurrently obtain political bias estimates. Any user\nmay employ our approach to analyse any given online website indexed by GDELT.\nThe leading question of this work was whether one can infer the political leaning of news\nweb-domains based on GDELT data. As suggested by the results, this was indeed relatively\nsuccessful. The highest performing model (a neural network) achieved an accuracy of 75%\nand an AUC score of 81%, compared to a 45% and 50% AUC score for the baseline model.\nThis is comparable to previous work [ 19,22], though there is still room for improvement when\ncompared to some of the more specific applications of media bias classification (though such\nimplementations do not compare to ours in terms of scope [ 18]). Nevertheless, the fact that\nthis approach could achieve such performance without using custom, optimized language\nmodels, but instead only using GDELT\u2019s relatively basic features is highly promising. Our\nresults demonstrate the efficacy and value of our approach: even with a standard set of models\nand experiments, we achieved commendable performance.\nInterestingly, despite the impressive capabilities usually ascribed to LLM models, the\nLlama 3.1 andGPT-4o mini baselines performed poorly, assigning the \u201dleast biased\u201d\nlabel to most items. These results mirror issues mentioned in previous work [ 27,28,31,32,35\u2013\n37]. Our results are perhaps not surprising, as many of the news web-domains are not very\nwell-known and therefore unlikely to have been sufficiently represented in the training data.\nIn light of this, we would like to emphasize that our NN model performs best for outlets with\nlow- or medium-traffic, as revealed during our error analysis, which sets it in an ideal position\nto detect bias for unfamiliar sources. This is crucial in the opaque and rapidly evolving online\ninformation sphere.\nAnother issue our work aimed to address was the focus on narrow types of bias in previous\nstudies [ 4,18,19,22,26]. We compared the performance of models trained on the more com-\nmonly studied forms of bias related to word choice and general tone of articles (traditional\nbias dataset) to models trained on features related to under-explored patterns of bias (alter-\nnative bias dataset). The inclusion of alternative forms of bias improves performance, demon-\nstrating that automatic bias detection benefits from expanding its focus. Indeed, features such\nas article counts per theme (a proxy for coverage bias) were particularly informative, as is\napparent in Figs 5 -9.\nIn addition to these advantages of the current approach, transparency and explainability of\nresults were also central to our work. SHAP was used to provide detailed explanations of fea-\nture impact for any web-domain of interest. As an example, Fig 5 shows how Breitbart, a polit-\nically right-wing outlet, was accurately labelled thanks to features related to crime (including\nthemes like cartels, kidnapping, black markets, organized crime, and robbery). The feature\nvalues can be interpreted to make sense of the result: for instance, many articles discuss crime\ncartels (0.545, where the maximum possible value is 1 due to the scaling of the data) and that\nthe articles about black markets will tend to have a negative tone (0.409). Thus, one can inter-\npret these results as meaning that Breitbart\u2019s focus on crime is indicative of right-wing bias,\nwhich is more intuitive and informative than a simple classification result or model-level\nfeature importances.\nFurthermore, the SHAP decision plots also lend credence to previous results. For exam-\nple, alternative forms of bias such as article counts tend to be in the top twenty most impactful\nfeatures in Figs 5 -9. Nevertheless, the plots also raise some questions. Some themes are part\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 16/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 17 \u2014 #17\nPLOS One Automatic political bias detection of news outlets\nof the \u201cusual suspects\u201d of politically polarising themes in previous work (inequality, environ-\nment, social movements, firearms, and election fraud). Other themes, however, are more dif-\nficult to explain. For instance, features related to exhumation, sanitation and natural or man-\nmade disasters of varying kinds are also included in the top twenty features, though these are\nmore difficult to fully interpret.\nAside from this, we should also highlight that models performed better when trained on\nhuman-made labels [ 45] compared to computationally derived ones [ 25]. A potential reason\nfor the drop in performance is that these labels are inherently an approximation, and there-\nfore add uncertainty for the model. This is particularly detrimental when such approxima-\ntions compound. Robertson et al. [ 25] used Twitter sharing patterns of registered voters to\nattribute a score to each website when creating the PABS labels we used as ground truth. How-\never, as they themselves note, this assumes that people only share articles that agree with their\nown political opinions, which is not always the case (they do perform various tests for valid-\nity with other existing bias labels that suggest their results are an adequate replacement, how-\never). Nevertheless, the present study\u2019s results suggest that models trained on such computa-\ntionally determined labels can still be used to some extent, should manual labels be unavail-\nable. Furthermore, it is worth noting the degree to which the models generalize, indicating\nthat while manual labour was initially required, we can now partially rely on these models for\nsubsequent analyses even without human-made labels.\nAltogether, bias detection following an approach similar to ours could hopefully be\nmore informative for the public, offering a transparent examination of overall web-domain\nbehaviour. This can be done in a cost-effective and recurrent way, allowing for systematic esti-\nmates of political bias in the online news media environment. This might contribute to citi-\nzens\u2019 ability to make decisions in an informed manner about various topics important in the\ncurrent political climate [ 66].\nLimitations and future considerations\nDespite addressing many drawbacks of previous research, such as the manual annotations,\nlimited applicability, and focus on narrow forms of bias, there are remaining limitations of the\ncurrent approach as well.\nFirstly, the best-performing model was trained on GDELT and supplemented with categor-\nical features from MBFC. These categorical features are, however, only available for a subset\nof web-domains present in GDELT, meaning that this particular model is not applicable to all\nweb-domains. Nevertheless, other models trained only on GDELT data achieved compara-\nble performance, so it is possible to get accurate predictions and SHAP explanations for any\nGDELT website with a minimal drop in accuracy.\nSecondly, better features representing the various forms of bias could be constructed in\nthe future. Picture and explanation bias was, for example, only indirectly examined here,\nas the current approach only accounted for the presence or absence of images. Ideally, the\nactual content of the images would be included, as has been done in previous studies using\nGDELT [ 39]. Additionally, some forms of bias were excluded from this analysis despite poten-\ntial relevance (e.g. placement bias).\nFurthermore, it was noted during post-hoc analysis that the ground truth labels display\nremarkable disagreement with each other. This raises questions regarding what can be consid-\nered acceptable ground truths, as even expert labels tend to disagree with each other. Future\nwork might want to consider using other labels of political bias, as the bipartisan scoring\ndoes not necessarily lend itself well to all global political systems. Indeed, it has been noted\nthat what is considered left-leaning in one country would not be so in another one [ 3]. As\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 17/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 18 \u2014 #18\nPLOS One Automatic political bias detection of news outlets\nsuch, future applications of our approach should remain aware that the labels we use may not\nperfectly transfer when considering another country. Helpfully, however, MBFC provides\nan overview of criteria for how their bias labels were determined (for example, the outlet\u2019s\nstance on taxation, abortion, or the climate), which can be used to evaluate whether the scale\nis applicable for any particular use case [ 44]. Given previous research on the persistence of the\nleft-right political divide [ 1] and the pervasiveness of US political structures in its media and,\nimportantly, its social media, we expect that these ground truth labels will be appropriate in\nthe majority of, at the very least, Western countries. Should future work wish to forgo the left-\nright political divide for another kind of distinction, our approach nevertheless remains help-\nful, since the model can quite simply be retrained on the same data but using a different set\nof ground truth labels. This might enhance the applicability of our approach to countries that\ndo not neatly follow the left-right political spectrum, while retaining the benefits of a system-\natic and in-depth analysis of political bias in news. Alternatively, future work may forgo using\nlabels altogether, opting for unsupervised models instead. Given that there are no perfectly\nunbiased benchmarks, this may be a preferable approach depending on the context. Future\nwork may also examine the possibility of using GDELT\u2019s data to examine an outlet\u2019s overall\nstance on a particular theme. This could provide a robust and extensive perspective on news\noutlets, and show potentially unexpected biases.\nThe rise in popularity of LLM-based methods also presents a promising avenue of research,\ndespite the challenges mentioned in our section on related work. Given the unprecedented\npotential for nuanced model output explanations that LLMs offer, they certainly merit fur-\nther study of whether they can provide the nuance commonly reserved to expert-based\napproaches.\nLastly, our use of SHAP is explorative; future work in online bias might focus on including\nmore detailed information, such as relevant excerpts of articles, to concretely provide insight\ninto model predictions. This might give insight into about why some of the more surprising\nthemes were deemed to be informative, such as waterways, for instance. Generally, it can help\ninform the field of themes that are not usually considered in online bias research.\nConclusion\nThe current work proposes an approach to classify news outlet political bias. Crucially, we\nintended to expand the scope beyond what had previously been done by ensuring global cov-\nerage and by focusing on multiple forms of bias. Our results indicate that the method indeed\nprovides a fully automatic and scalable approach to detecting news bias, and that enlarging\nthe focus to multiple forms of bias could help the field advance. Finally, the SHAP explana-\ntions allow for interpretation of why a particular web-domain is considered politically biased\nand show which topics and behaviours influence the classification. Interestingly, many themes\ncommonly considered divisive reappear, but some informative features have not been pre-\nviously considered in the literature. This may help address gaps or future avenues in current\nresearch.\nAll in all, the current work extends existing research to be more widely applicable and\ninformative for the field. The increased transparency may be helpful for adequately informing\nthe public about its news consumption, as well as providing more insight into the underlying\nmechanisms of bias to a more granular extent than what is traditionally attempted by com-\nputational methods. Considering the immense impact of news on global political climates,\nour hope is that increased understanding and trustworthiness of media might contribute to a\nbetter-informed society and a healthier political environment.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 18/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 19 \u2014 #19\nPLOS One Automatic political bias detection of news outlets\nSupporting information\nS1 Appendix. MBFC features. This appendix describes in more detail the features provided\nby MBFC. For each news web-domain, they provide a set of data points of interest.\n(PDF)\nS2 Appendix. Details of implementation and data pre-processing. This appendix describes\nin the implementation and other details related to the data pre-processing.\n(PDF)\nS3 Appendix. Grid search model parameters. The appendix details the hyperparameters\nused for grid search when optimizing the various models.\n(PDF)\nS4 Appendix. Neural network architecture. This section notes the architecture of the\nPyTorch neural network for all experiments.\n(PDF)\nS5 Appendix. Large language model baseline. This section notes the implementation details\nof the LLM baseline.\n(PDF)\nS6 Appendix. Error analysis. This section details some error analysis per political leaning\nlabel and the website traffic.\n(PDF)\nS7 Appendix. Example of a GDELT news item. Shows an excerpt from a news story as it is\nshown on GDELT.\n(PDF)\nAuthor contributions\nConceptualization: Ronja R\u00f6nnback, Chris Emmery, Henry Brighton.\nData curation: Ronja R\u00f6nnback, Chris Emmery.\nFormal analysis: Ronja R\u00f6nnback.\nInvestigation: Ronja R\u00f6nnback.\nMethodology: Ronja R\u00f6nnback, Chris Emmery, Henry Brighton.\nProject administration: Ronja R\u00f6nnback.\nSoftware: Ronja R\u00f6nnback, Chris Emmery.\nSupervision: Chris Emmery, Henry Brighton.\nValidation: Ronja R\u00f6nnback, Chris Emmery, Henry Brighton.\nVisualization: Ronja R\u00f6nnback, Henry Brighton.\nWriting \u2013 original draft: Ronja R\u00f6nnback.\nWriting \u2013 review & editing: Ronja R\u00f6nnback, Chris Emmery, Henry Brighton.\nReferences\n1.Le Gall C, Berton R. Left-Right vs. traditional and new cleavages: Testing durability of an old\npolitical category. Cambridge Scholars Publishing; 2013. p. 255\u201368.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 19/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 20 \u2014 #20\nPLOS One Automatic political bias detection of news outlets\n2.Lewis H, Lewis V. The myth of left and right: How the political spectrum misleads and harms\nAmerica. Oxford University Press; 2023. p. 9\u201316.\nhttps://doi.org/10.1093/oso/9780197680216.001.0001\n3.Huber J, Inglehart R. Expert interpretations of party space and party locations in 42 societies. Party\nPolit. 1995;1(1):73\u2013111. https://doi.org/10.1177/1354068895001001004\n4.Hamborg F, Donnay K, Gipp B. Automated identification of media bias in news articles: An\ninterdisciplinary literature review. Int J Digit Libr. 2019;20(4):391\u2013415.\nhttps://doi.org/10.1007/s00799-018-0261-y\n5.Leetaru K, Schrodt PA. GDELT: Global data on events, location, and tone. In: ISA annual\nconvention; 2013.\n6.Rodrigo-Gin\u00e9s FJ, de Albornoz JC, Plaza L. A systematic review on media bias detection: What is\nmedia bias, how it is expressed, and how to detect it. Expert Syst Applic. 2024;237:121641.\nhttps://doi.org/10.1016/j.eswa.2023.121641\n7.Kelsey D. News, discourse, and ideology. In: The handbook of journalism studies. Routledge; 2019.\np. 246\u201360.\n8.Ditton J, Duffy J. Bias in the newspaper reporting of crime news. Brit J Criminol. 1983;23:159.\n9.Gilliam Jr FD, Iyengar S, Simon A, Wright O. Crime in black and white: The violent, scary world of\nlocal news. Harvard Int J Press/Polit. 1996;1(3):6\u201323.\n10.Paybarah A. Media matters: New York TV News over-reports on crimes with black suspects.\nPOLITICO; 2015. Available from: https://www.politico.com/states/new-york/city-hall/story/2015/03/\nmedia-matters-new-york-tv-news-over-reports-on-crimes-with-black-suspects-020674 (Accessed\n06 May 2022).\n11.Madrigal G, Soroka S. Migrants, caravans, and the impact of news photos on immigration attitudes.\nInt J Press/Polit. 2023;28(1):49\u201369. https://doi.org/10.1177/19401612211008430\n12.Soroka S, Loewen P, Fournier P, Rubenson D. The impact of news photos on support for military\naction. Polit Commun. 2016;33(4):563\u201382.\n13.Jones K. A statistical interpretation of term specificity and its application in retrieval. J Doc.\n1972;28(1):11\u201321. https://doi.org/10.1108/eb026526\n14.Jones KS. Some thoughts on classification for retrieval. Journal of Documentation.\n1970;26(2):89\u2013101. https://doi.org/10.1108/eb026488\n15.Le QV, Mikolov T. Distributed representations of sentences and documents. In: Proceedings of the\n31th international conference on machine learning, ICML 2014, Beijing, China, 21\u201326 June 2014.\nvol. 32 of JMLR workshop and conference proceedings. JMLR.org; 2014. p. 1188\u201396. Available\nfrom: http://proceedings.mlr.press/v32/le14.html\n16.Spinde T, Hamborg F, Gipp B. Media bias in German news articles: A combined approach. In:\nECML PKDD 2020 Workshops \u2013 Workshops of the European conference on machine learning and\nknowledge discovery in databases (ECML PKDD 2020): SoGood 2020, PDFL 2020, MLCS 2020,\nNFMCP 2020, DINA 2020, EDML 2020, XKDD 2020 and INRA 2020, Ghent, Belgium, September\n14-18, 2020, Proceedings. vol. 1323 of communications in computer and information science.\nSpringer; 2020. p. 581\u201390. Available from: https://doi.org/10.1007/978-3-030-65965-3_41\n17.Baraniak K, Sydow M. News articles similarity for automatic media bias detection in Polish news\nportals. In: Ganzha M, Maciaszek LA, Paprzycki M, editors. Proceedings of the 2018 federated\nconference on computer science and information systems, FedCSIS 2018, Poznan, Poland,\nSeptember 9-12, 2018. vol. 15 of annals of computer science and information systems; 2018. p.\n21\u20134. Available from: https://doi.org/10.15439/2018F359\n18.Gangula RRR, Duggenpudi SR, Mamidi R. Detecting political bias in news articles using headline\nattention. In: Linzen T, Chrupala G, Belinkov Y, Hupkes D, editors. Proceedings of the 2019 ACL\nworkshop BlackboxNLP: Analyzing and interpreting neural networks for NLP; 2019. p. 77\u201384.\nAvailable from: https://doi.org/10.18653/v1/w19-4809\n19.Spinde T, Krieger J, Ruas T, Mitrovic J, G\u0308otz-Hahn F, Aizawa A, et al. Exploiting transformer-based\nmultitask learning for the detection of media bias in news articles. In: Smits M, editor. Information for\na better world: Shaping the global future \u2013 17th international conference, iConference 2022, virtual\nevent, February 28-March 4, 2022, Proceedings, Part I. vol. 13192 of lecture notes in computer\nscience. Springer; 2022. p. 225\u201335. Available from: https://doi.org/10.1007/978-3-030-96957-8_20\n20.Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled version of BERT: Smaller, faster,\ncheaper and lighter. CoRR. 2019;abs/1910.01108.\n21.Devlin J, Chang M, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In: Burstein J, Doran C, Solorio T, editors. Proceedings of the 2019\nconference of the North American chapter of the association for computational linguistics: Human\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 20/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 21 \u2014 #21\nPLOS One Automatic political bias detection of news outlets\nlanguage technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, Volume 1 (long and\nshort papers). Association for Computational Linguistics; 2019. p. 4171\u201386.\nhttps://doi.org/10.18653/v1/n19-1423\n22.Spinde T, Plank M, Krieger J, Ruas T, Gipp B, Aizawa A. Neural media bias detection using distant\nsupervision with BABE \u2013 Bias annotations by experts. CoRR. 2022.\nhttps://doi.org/10.48550/ARXIV.2209.14557\n23.Menzner T, Leidner JL. Improved models for media bias detection and subcategorization. In: Rapp\nA, Caro LD, Meziane F, Sugumaran V, editors. Natural language processing and information\nsystems \u2013 29th international conference on applications of natural language to information systems,\nNLDB 2024, Turin, Italy, June 25-27, 2024, Proceedings, Part I. vol. 14762 of lecture notes in\ncomputer science. Springer; 2024. p. 181\u201396. Available from:\nhttps://doi.org/10.1007/978-3-031-70239-6_13\n24.Le HT, Shafiq Z, Srinivasan P. Scalable news slant measurement using Twitter. In: Proceedings of\nthe eleventh international conference on web and social media, ICWSM 2017, Montreal, Quebec,\nCanada, May 15-18, 2017. AAAI Press; 2017. p. 584\u20137. Available from:\nhttps://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15668\n25.Robertson RE, Jiang S, Joseph K, Friedland L, Lazer D, Wilson C. Auditing partisan audience bias\nwithin Google search. Proc ACM Hum-Comput Interact. 2018;2(CSCW):1\u201322.\nhttps://doi.org/10.1145/3274417\n26.Ye J, Skiena S. MediaRank: Computational ranking of online news sources. In: Teredesai A, Kumar\nV, Li Y, Rosales R, Terzi E, Karypis G, editors. Proceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining, KDD 2019, Anchorage, AK, USA, August 4-8,\n2019. ACM; 2019. p. 2469\u201377. Available from: https://doi.org/10.1145/3292500.3330709\n27.Li Z, Zhang H, Zhang J. A revisit of fake news dataset with augmented fact-checking by ChatGPT.\nCoRR. 2023; abs/2312.11870. https://doi.org/10.48550/ARXIV.2312.11870\n28.Wen Z, Younes R. ChatGPT v.s. Media bias: A comparative study of GPT-3.5 and fine-tuned\nlanguage models. CoRR. 2024; abs/2403.20158. https://doi.org/10.48550/ARXIV.2403.20158\n29.Makhortykh M, Sydorova M, Baghumyan A, Vziatysheva V, Kuznetsova E. Stochastic lies: How\nLLM-powered chatbots deal with Russian disinformation about the war in Ukraine. HKS Misinfo\nRev. 2024. https://doi.org/10.37016/mr-2020-154\n30.Szwoch J, Staszkow M, Rzepka R, Araki K. Can LLMs determine political leaning of Polish news\narticles? In: 2023 IEEE Asia-Pacific conference on computer science and data engineering (CSDE).\nIEEE; 2023. p. 1\u20136.\n31.Menzner T, Leidner JL. Experiments in news bias detection with pre-trained neural transformers. In:\nGoharian N, Tonellotto N, He Y, Lipani A, McDonald G, Macdonald C, et al., editors. Advances in\ninformation retrieval \u2013 46th European conference on information retrieval, ECIR 2024, Glasgow, UK,\nMarch 24-28, 2024, Proceedings, Part IV. vol. 14611 of lecture notes in computer science. Springer;\n2024. p. 270\u201384. Available from: https://doi.org/10.1007/978-3-031-56066-8_22\n32.Lin L, Wang L, Guo J, Wong K. Investigating bias in LLM-based bias detection: Disparities between\nLLMs and human perception. CoRR. 2024; abs/2403.14896.\nhttps://doi.org/10.48550/ARXIV.2403.14896\n33.Motoki F, Pinho Neto V, Rodrigues V. More human than human: Measuring ChatGPT political bias.\nPubl Choice. 2024;198(1):3\u201323. https://doi.org/10.1007/s11127-023-01097-2\n34.van Dis EAM, Bollen J, Zuidema W, van Rooij R, Bockting CLH. ChatGPT: Five priorities for\nresearch. Nature. 2023;614:224\u20136. https://doi.org/10.1038/d41586-023-00288-7 PMID: 36737653\n35.Choi M, Pei J, Kumar S, Shu C, Jurgens D. Do LLMs understand social knowledge? Evaluating the\nsociability of large language models with SocKET benchmark. In: Bouamor H, Pino J, Bali K,\neditors. Proceedings of the 2023 conference on empirical methods in natural language processing.\n2023. Singapore, December 6-10. Association for Computational Linguistics; 2023. p. 11370\u2013403.\nhttps://doi.org/10.18653/v1/2023.emnlp-main.699\n36.Mu Y, Wu BP, Thorne W, Robinson A, Aletras N, Scarton C, et al. Navigating prompt complexity for\nzero-shot classification: A study of large language models in computational social science. In:\nCalzolari N, Kan M, Hoste V, Lenci A, Sakti S, Xue N, editors. Proceedings of the 2024 joint\ninternational conference on computational linguistics, language resources and evaluation,\nLREC/COLING 2024, 20-25 May, 2024, Torino, Italy. ELRA and ICCL; 2024. p. 12074\u201386. Available\nfrom: https://aclanthology.org/2024.lrec-main.1055\n37.Y Qiang, S Nandi, N Mehrabi, GV Steeg, A Kumar, A Rumshisky, et al. Prompt perturbation\nconsistency learning for robust language models. In: Y Graham, M Purver, editors. Findings of the\nassociation for computational linguistics: EACL 2024, St. Julian\u2019s, Malta, March 17-22. Association\nfor computational linguistics; 2024. p. 1357\u201370. Available from:\nhttps://aclanthology.org/2024.findings-eacl.91\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 21/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 22 \u2014 #22\nPLOS One Automatic political bias detection of news outlets\n38.Bender EM, Gebru T, McMillan-Major A, Shmitchell S. On the dangers of stochastic parrots. In:\nElish MC, Isaac W, Zemel RS, editors. FAccT \u201921: 2021 ACM conference on fairness, accountability,\nand transparency, 2021, virtual event/Toronto, Canada, March 3-10. ACM; 2021. p. 610\u201323.\nhttps://doi.org/10.1145/3442188.3445922\n39.Kwak H, An J. Revealing the hidden patterns of news photos: Analysis of millions of news photos\nthrough GDELT and deep learning-based vision APIs. In: An J, Kwak H, Benevenuto F, editors.\nNews and public opinion, papers from the 2016 ICWSM workshop, Cologne, Germany, May 17,\n2016. vol. WS-16-18 of AAAI Technical Report. AAAI Press; 2016. p. 339\u201343. Available from:\nhttp://aaai.org/ocs/index.php/ICWSM/ICWSM16/paper/view/13191\n40.Vargo CJ, Guo L, Amazeen MA. The agenda-setting power of fake news: A big data analysis of the\nonline media landscape from 2014 to 2016. New Media Soc. 2018;20(5):2028\u20132049.\nhttps://doi.org/10.1177/1461444817712086\n41.El Ouadi A, Beskow D. Comparison of common crawl news & GDELT. In: 2024 IEEE international\nsystems conference (SysCon); 2024. p. 1\u20133.\n42.GDELT Project. GDELT Project; 2018. Available from: https://www.gdeltproject.org/\n43.GDELT Project. The GDELT Global Knowledge Graph (GKG) data format codebook V2.1; 2015\nAvailable from:\nhttp://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf\n44.Media Bias Fact Check. Left vs. right bias: How we rate the bias of media sources; 2021. Available\nfrom: https://mediabiasfactcheck.com/left-vs-right-bias-how-we-rate-the-bias-of-media-sources/\n45.Media Bias Fact Check. About media bias fact check; 2021. Available from:\nhttps://mediabiasfactcheck.com/about/\n46.Media Bias Fact Check. MBFC methodology; 2023. Available from:\nhttps://mediabiasfactcheck.com/methodology/\n47.Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: An imperative style,\nhigh-performance deep learning library. In: Wallach HM, Larochelle H, Beygelzimer A, d\u2019Alch\u00b4e-Buc\nF, Fox EB, Garnett R, editors. Advances in neural information processing systems 32: Annual\nconference on neural information processing systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada; 2019. p. 8024\u201335. Available from:\nhttps://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html\n48.Boser BE, Guyon I, Vapnik V. A training algorithm for optimal margin classifiers. In: Proceedings of\nthe fifth annual ACM conference on computational learning theory. 1992:144\u2013152.\nhttps://doi.org/10.1145/130385.130401\n49.Cortes C, Vapnik V. Support-vector networks. Mach Learn. 1995;20(3):273\u201397.\nhttps://doi.org/10.1007/BF00994018\n50.Freund Y, Schapire RE. A decision-theoretic generalization of on-line learning and an application to\nboosting. J Comput Syst Sci. 1997;55(1):119\u201339. https://doi.org/10.1006/JCSS.1997.1504\n51.Li X, Wang L, Sung E. AdaBoost with SVM-based component classifiers. Eng Applic Artif Intell.\n2008;21(5):785\u201395. https://doi.org/10.1016/j.engappai.2007.07.001\n52.Chen T, Guestrin C. XGBoost: A scalable tree boosting system. In: Proceedings of the 22nd ACM\nSIGKDD international conference on knowledge discovery and data mining. San Francisco, CA,\nUSA, August 13-17. ACM; 2016. p. 785\u201394. Available from:\nhttps://doi.org/10.1145/2939672.2939785\n53.Dubey A, Jauhri A, Pandey A, Kadian A, Al-Dahle A, Letman A, et al. The Llama 3 herd of models.\nCoRR. 2024; abs/2407.21783. https://doi.org/10.48550/ARXIV.2407.21783\n54.OpenAI. GPT-4o-mini; 2024. Available from:\nhttps://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n55.Fukushima K. Visual feature extraction by a multilayered network of analog threshold elements.\nIEEE Trans Syst Sci Cybern. 1969;5(4):322\u201333. https://doi.org/10.1109/TSSC.1969.300225\n56.Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. In: Bach FR, Blei DM, editors. In: Proceedings of the 32nd international conference\non machine learning, ICML 2015, Lille, France, 6-11 July 2015. vol. 37 of JMLR Workshop and\nConference Proceedings. JMLR.org; 2015. p. 448\u201356. Available from:\nhttp://proceedings.mlr.press/v37/ioffe15.html\n57.Srivastava N, Hinton GE, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: A simple way to\nprevent neural networks from overfitting. J Mach Learn Res. 2014;15(1):1929\u201358.\nhttps://doi.org/10.5555/2627435.2670313\n58.Kingma DP, Ba J. Adam: A method for stochastic optimization. CoRR. 2014; abs/1412.6980.\n59.Guo C, Berkhahn F. Entity embeddings of categorical variables. CoRR. 2016; abs/1604.06737.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 22/ 23\nID: pone.0321418 \u2014 2025/5/10 \u2014 page 23 \u2014 #23\nPLOS One Automatic political bias detection of news outlets\n60.Brownlee J. Three ways to encode categorical variables for deep learning; 2019. Available from:\nhttps://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/\n61.Scikit Learn. Tuning the hyper-parameters of an estimator: Searching for optimal parameters with\nsuccessive halving; 2023. Available from:\nhttps://scikit-learn.org/stable/modules/grid_search.html#successive-halving-user-guide\n62.Lundberg SM, Lee S. A unified approach to interpreting model predictions. In: Guyon I, von Luxburg\nU, Bengio S, Wallach HM, Fergus R, Vishwanathan SVN, et al., editors. Advances in neural\ninformation processing systems 30: Annual conference on neural information processing systems\n2017, December 4-9, 2017, Long Beach, CA, USA; 2017. p. 4765\u201374. Available from: https:\n//proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html\n63.Ribeiro MT, Singh S, Guestrin C. \u201cWhy Should I Trust You?\u201d: Explaining the predictions of any\nclassifier. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge\ndiscovery and data mining; 2016.\n64.Bach S, Binder A, Montavon G, Klauschen F, M\u00fcller K-R, Samek W. On pixel-wise explanations for\nnon-linear classifier decisions by layer-wise relevance propagation. PLoS ONE. 2015;10:e0130140.\nhttps://doi.org/10.1371/journal.pone.0130140 PMID: 26161953\n65.AllSides. AllSides methodology; 2023. Available from:\nhttps://www.allsides.com/media-bias/media-bias-rating-methods\n66.Lewandowsky S, Smillie L, Garcia D, Hertwig R, Weatherall J, Egidy S, et al.. Technology and\ndemocracy: Understanding the influence of online technologies on political behaviour and\ndecision-making. Publications Office of the European Union; 2020.\nPLOS One https://doi.org/10.1371/journal.pone.0321418 May 12, 2025 23/ 23", "raw_data": {"container_type": "Publication", "source": "PUBLICATION_SEARCH_SNIPPET", "bib": {"title": "Automatic Large-scale", "author": ["R Thelen-R\u00f6nnback", "C Emmery"], "pub_year": "2024", "venue": "pone \u2026", "abstract": "Political bias is an inescapable characteristic in news and media reporting, and understanding  what political biases people are exposed to when interacting with online news is of"}, "filled": false, "gsrank": 216, "pub_url": "https://repository.tilburguniversity.edu/server/api/core/bitstreams/eb79d6ca-1bd8-4119-b15a-f0f41565a982/content", "author_id": ["", "Ot4Z-qkAAAAJ"], "url_scholarbib": "/scholar?hl=en&q=info:S2IGJCC8y9QJ:scholar.google.com/&output=cite&scirp=215&hl=en", "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2522mediabiasfactcheck.com%26hl%3Den%26start%3D210%26as_sdt%3D0,10&citilm=1&update_op=library_add&info=S2IGJCC8y9QJ&ei=KrWsaO_cK8DZieoPqdqh8QU&json=", "num_citations": 0, "url_related_articles": "/scholar?q=related:S2IGJCC8y9QJ:scholar.google.com/&scioq=%22mediabiasfactcheck.com&hl=en&as_sdt=0,10", "eprint_url": "https://repository.tilburguniversity.edu/server/api/core/bitstreams/eb79d6ca-1bd8-4119-b15a-f0f41565a982/content"}}]